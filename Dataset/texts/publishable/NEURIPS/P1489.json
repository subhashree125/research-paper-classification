{
  "Abstract": "In open-world scenarios, where both novel classes and domains may exist, an idealsegmentation model should detect anomaly classes for safety and generalize to newdomains. However, existing methods often struggle to distinguish between domain-level and semantic-level distribution shifts, leading to poor out-of-distribution(OOD) detection or domain generalization performance. In this work, we aim toequip the model to generalize effectively to covariate-shift regions while preciselyidentifying semantic-shift regions. To achieve this, we design a novel generativeaugmentation method to produce coherent images that incorporate both anomaly(or novel) objects and various covariate shifts at both image and object levels. Fur-thermore, we introduce a training strategy that recalibrates uncertainty specificallyfor semantic shifts and enhances the feature extractor to align features associatedwith domain shifts. We validate the effectiveness of our method across benchmarksfeaturing both semantic and domain shifts. Our method achieves state-of-the-art per-formance across all benchmarks for both OOD detection and domain generalization.Code is available at",
  "Introduction": "Semantic segmentation, a fundamental task in computer vision, has become indispensable in variousreal-world applications, such as autonomous driving . Recent progress in deep learning-basedsemantic segmentation has exhibited promising results under the assumption of consistent distribu-tions between the training and testing data. However, these models often falter when faced withdistributional shifts. Consequently, research on semantic segmentation under distributional shiftshas garnered significant attention in recent years. Some studies approach this challenge from ageneralization perspective, aiming to train networks to adapt to data with covariate distribution shifts,such as novel domains . Another line of research focuses on training models to discern (ordetect) test data exhibiting semantic distributional shifts, such as anomalies or unfamiliar objects, toensure reliable predictions . In real-world situation, both types of distribution shifts often occurjointly. This leaves us with the question: Can a model jointly handle both kinds of distribution shift? To address this question, we assess the ability of current domain generalization techniques todetect unknown objects and that of out-of-distribution detection techniques to generalizeto unknown domains. Interestingly, we find that models trained using domain generalization tech-niques, such as domain randomization or whitening transformation, often fail to identify unknownobjects, and sometimes even perform worse than the baseline without domain generalization. Fur-thermore, we observe that models trained using out-of-distribution detection techniques struggle togeneralize to unknown domains, exhibiting overly high uncertainty towards objects experiencingdomain shifts compared to baseline methods without OOD training. While one intuitive approach is",
  "(c)": ": We study semantic segmentation with both semantic-shift and covariate-shift regions. (a)Training for Out-of-distribution (OOD) detection alone yields high uncertainty for both types ofshifts, whereas training for domain generalization (DG) alone tends to produce low uncertaintyfor both. Our method effectively differentiates between the two, generating high uncertainty onlyfor semantic-shift regions. (b) We achieve strong performance in both OOD detection and domain-generalized semantic segmentation. (c) This is achieved by coherently augmenting original images(first row) with both covariate and semantic shifts (second row). to combine existing anomaly segmentation and domain generalization techniques during training,we note that current domain generalization strategies primarily address image-level shifts, whereasanomaly segmentation focuses on object-level semantic differences. Consequently, the resultingmodels tend to generalize well to image-level variations, such as changes in weather but strugglewith object-level shifts. They often misinterpret any object-level distribution shift as a semanticanomaly, assigning high uncertainty scores to known objects that exhibit covariate changes, such ascolor variations in cars or changes in pedestrian attire, as demonstrated in .These experimentsunderscore the challenge of differentiating and jointly handling different types of distribution shifts. In this work, we jointly study both semantic and covariate distribution shifts. That is, we aim to equipthe model to generalize effectively to covariate-shift regions while precisely identifying semantic-shiftregions. To achieve this, we design a novel generative augmentation method to produce coherentimages that incorporate both anomaly (or novel) objects and various covariate shifts at both image andobject levels. Furthermore, we introduce a training strategy that re-calibrates uncertainty specificallyfor semantic shifts and enhances the feature extractor to align features associated with domain shifts 1. Specifically, we first introduce a novel data augmentation technique that employs a semantic-to-imagegeneration model to create data that encompasses both covariate and semantic shifts at variouslevels, allowing the model to learn the essential differences between the shift types. Additionally, weintroduce a learnable, semantic-exclusive uncertainty function trained using a relative contrastive loss.We adopt a two-stage training paradigm designed to balance the integration of these enhancementswhile minimizing their potential interference. A noise-aware training strategy further complementsthis approach, employing online, pixel-wise selection to mitigate noise in the generated images.Altogether, our approach not only boosts the models generalization across domain shifts but alsoensures a high level of uncertainty in response to semantic shifts. We validate the effectiveness of our method across benchmarks featuring both semantic and domainshifts, including RoadAnomaly , SMIYC , ACDC-POC and MUAD benchmarks.Our results demonstrate that our method achieves state-of-the-art performance across all benchmarks,employing different segmentation backbones for both OOD detection and known class segmentation. In summary, our contributions are: (1) We study semantic segmentation under both semantic anddomain shifts, revealing limitations in methods focused on a single shift; (2) We introduce a coherent-generative augmentation method that augments training data with both shifts; (3) We propose atwo-stage, noise-aware training pipeline to optimally leverage augmented data, learning a semantic-exclusive uncertainty function while aligning features for domain shifts.",
  "In this work, we use domain shift and covariate shift interchangeably": "models to learn training distributions and detect anomalies through reconstruction differences ,but this often requires additional networks, resulting in slower inference. Other methods use auxiliaryOOD data to train models to distinguish known from unknown instances .Among these, Entropy Maximization uses entire images from COCO as OOD proxies,maximizing softmax entropy on these samples. PEBAL improves upon this by cutting out OODobject instances, pasting them into training images, and using an energy function as the uncertaintyscore. To reduce artifacts in the pasted OOD region, proposes using a style transfer model toalign the pasted region with the background. RPL further regularizes embedding similaritybetween COCO background pixels and training images. Beyond improvements in OOD proxiesand uncertainty functions, recent methods explore the use of the Mask2Former architecture ,such as RbA , Mask2Anomaly , and EAM . Our method follows this second approach,generating OOD data with a semantic-to-image model to reduce artifacts and further introducing alearnable uncertainty function to enhance both OOD detection and known class segmentation. It isarchitecture-agnostic, compatible with both pixel-based and mask-based segmentation backbones. Domain Generalization for Semantic SegmentationThe task aims to train a model on one ormore source domains that can perform well on unseen target domains. Existing techniques focus eitheron introducing specialized model architectures, such as those incorporating normalization orwhitening transformations , or on designing domain randomization techniques .Most domain randomization methods rely on image transformation rules or style transfer .Recently, Jia et al. proposed a semantic-to-image model that generates images across diversedomains. Orthogonally, Bi et al. explore architectural changes with Mask2Former. Our approachbelongs to the domain randomization category, generating images with both domain and semanticshifts simultaneously to improve the models ability to distinguish between these shifts. Segmentation Under Multiple Distribution ShiftsEarly works demonstrated the necessityand feasibility of addressing both semantic segmentation under domain shifts and anomaly segmen-tation. However, these problem settings remain in their early stages (e.g., image-level anomalies)and may not fully capture the true challenges. More recent benchmarks, such as RoadAnomaly ,SMIYC , and MUAD , include domain and semantic shifts that better reflect real-world sce-narios. Some recent studies have explored the effects of domain shifts on anomaly segmentationbenchmarks and proposed a test-time adaptation pipeline to address the problem. In this work, weaim to further bridge the gap by investigating the core challenges of adopting domain generalizationtechniques and simultaneously enhancing model performance in both areas. Generative-based Data AugmentationThis technique is widely used to expand training datasetsand prevent overfitting . Unlike rule-based augmentation methods, which focuson image-level changes, generative methods can introduce more object-level variations. Amongthese works, are the most related to ours, using a text-guided inpainting pipeline to generateanomalies or novel objects. However, this local generation process risks creating inconsistenciesbetween the patch and its background. Additionally, they either focus on generating novel objectswithin the same domain or use separate pipelines for domain and semantic shifts . In contrast,our method generates multiple distribution shifts in a single process, preserving the global context ofthe image and ensuring a more natural integration of novel objects.",
  "Problem Formulation and Method Overview": "We consider the problem of semantic segmentation under multiple distribution shifts. Formally, wedefine the training distribution as PXY in X YHWin, where X = R3HW represents the three-dimensional input space of images with H W pixels, and Yin = [1, C] denotes the semantic labelspace at each pixel. The test distribution is denoted as QXY X YHWtest. There are two commontypes of distribution shifts: covariate shiftswhere the input distribution changes (QX = PX) butthe label space remains the same and semantic shifts, which involve alterations to the label space,including the introduction of novel categories (Ytest = Yin). We consider the possibility of bothtypes of distribution shifts occurring during testing.",
  "Place, OOD": ": Method Overview: (a) A novel generative-based data augmentation strategy that supple-ments training data with both covariate and semantic shifts in a coherent manner. (2) A semantic-exclusive uncertainty function with two-stage noise-aware training to encourage invariant featurelearning for covariate-shift regions while maintaining high uncertainty for semantic-shift regions. Our goal is to learn a model capable of jointly identifying semantic-shift regions and generalizing wellunder covariate shifts. This involves two primary challenges: (1) Enabling the model to distinguishbetween the two types of distribution shifts, and (2) ensuring the model responds appropriately to eachtype. To address the first challenge, we introduce a novel generative-based data augmentation strategythat supplements training data with both covariate and semantic shifts in a coherent manner. To tacklethe second challenge, we propose a semantic-exclusive uncertainty function with a decoupled trainingstrategy. This encourages the model to learn invariant features for covariate-shift regions whilemaintaining high uncertainty for semantic-shift regions. Below, we first introduce our generative-based augmentation strategy (.2), followed by the model training pipeline (.3).",
  "Coherent Generative-based Augmentation": "To distinguish between covariate and semantic shifts, we design a coherent generative-based dataaugmentation (CG-Aug) pipeline that enriches the training data with realistic and diverse distributionshifts. The pipeline consists of two stages: The first stage uses zero-shot semantic-to-image generationto create a variety of synthetic data, while the second stage automatically filters out low-qualitysynthetic data. We describe the details of each stage below. Zero-Shot Semantic-to-Image Generation.To generate more realistic and diverse OOD datafor segmentation, we propose a generation process that first cut-and-pastes the semantic mask ofnovel objects to the training labels and then leverages a semantic-to-image generation model to createcorresponding augmentation images. By exploiting powerful image generation models, this processis able to produce images with a wide range of covariate shifts and augments the training images withboth covariate and semantic shifts in a coherent manner. We detail our process below. Formally, given training set Dtr := {(xn, yn)}Ntn=1 with (xn, yn) PXY , we introduce an auxiliaryOOD set Do := {yom}Nam=1 with object masks yom YHWout. Subsequently, using a pretrainedsemantic-to-image generative model G : (Yin Yout)HW RHW , we generate an augmentedimage asxaug = G(yaug, t)withyaug = y yo,(1)where t is a text prompt and denotes the pasting operation. Here we adopt a pretrained Control-Net as G to instantiate the semantic-to-image generation process. Thanks to the powerful priorencoded in Stable Diffusion , this process allows us to generate images with more diverse stylesthan a task-specific semantic-to-image generation model, therefore creating rich covariate shifts.Moreover, we leverage the text prompt t to produce more diversity in the augmented images byspecifying the space, time, and weather, and to enhance the OOD object generation by indicating theclass of the pasted objects, via a set of templates (see Appendix A.1 for details). Auto-Filtering.While generative-based augmentation can produce more diverse and realisticdistribution shifts than rule-based augmentation, we observed it to often yield inaccurate or noisyrendering for the OOD objects. This might be caused by the fact that these objects appear rarely, ortheir cut masks are inconsistent with the surroundings. To cope with this, we design an automaticfiltering process that identifies generation failures where no object is generated, or a known-categoryobject is incorrectly generated. To achieve this, we leverage pretrained segmentation models to checkthe region size or its semantic class, and assign a quality score to each generated image. We thenfilter out the images with low-quality scores (see Appendix A.2 for details). We perform the above image generation process offline before model training, and the resulting syn-thetic data is used alongside standard augmentation strategies, such as mixup and AnomalyMix ,during training. Below, we denote our augmented dataset as Daug = {(xn, yn, xaugn, yaugn)}Ntn=1.",
  "Model Training": "Given the augmented dataset, we aim to train a segmentation model with recalibrated uncertaintyoutput, generating high OOD scores for semantic-shift regions and performing robustly undercovariate shifts. To achieve this, we propose a learnable uncertainty function and develop a stage-wise learning strategy that initializes the uncertainty function before fine-tuning the entire model.Our training process integrates a relative contrastive loss and a noise-aware data selection scheme,enabling the model to effectively align both the feature space and the OOD output scores. We note thatour method is generic and can be applied to pixel-wise models (e.g. DeepLabv3+ ) or mask-wisemodels (e.g. Mask2Former ).",
  "Semantic-Exclusive Uncertainty Recalibration": "Learnable Uncertainty Function.Suppose we have a neural network with its feature extractorf(x) RMF , where M is the number of pixels (or masks), and F is the feature dimension. Weintroduce a learnable linear projection W o RF C, with W oc denotes W o[: c] for short. For apixel-wise prediction model, we adopt the energy function form and parameterize it into a learnableuncertainty functionu(x) = log",
  "softmax (f(x)W oc )T g(x).(3)": "Here g(x) (0, 1)MHW is the sigmoid output of the mask head. For both cases, we initializethe projection function W o as the class weight W in of the pretrained segmentation network. Thecorresponding uncertainty score corresponds to the original energy score (or MSP score). Relative Contrastive Loss.We train the uncertainty function using a novel relative contrastive loss,which encourages higher uncertainty in unknown-class regions compared to known-class regions,while ensuring that regions with and without covariate shifts exhibit similar levels of uncertainty. Formally, for each batch of data {(xn, yn, xaugn , yaugn )}Bn=1, where B is the batch size, we define thefollowing pixel index sets: in = {i : y(i) Yin}, representing inlier pixel indices from the originaltraining images; aug = {i : yaug(i) Yin}, representing inlier pixel indices from the augmentedtraining images (covariate-shift set); and out = {i : y(i) / Yin} {i : yaug(i) / Yin}, representingoutlier pixel indices from both original and augmented images (semantic-shift set). Here, y(i) (oryaug(i)) denotes the label of pixel i. Our contrastive loss is defined as",
  "caug,iinmc,i3((ucui)), (4)": "where (x) = max( x, 0) is the margin-based contrastive loss, which encourages the input valueto exceed . The first two terms promote larger uncertainty gaps between unknown-class and known-class regions, while the third term encourages smaller uncertainty gaps between covariate-shifted and original data. For the third term, we calculate gaps only between pairs of original and augmentedimages, with mc,i {0, 1} indicating whether pixel (c, i) is paired in the dataset. The three marginvalues (1, 2, 3) introduce priors on the uncertainty gaps, and are set based on the initial averagedistance. Our method remains robust across a wide range of margin values (cf. ). Compared to existing OOD losses that either maximize uncertainty only for unknown data orsupervise known and unknown data separately , our loss supervises the relative distancebetween them, making it more robust to hyperparameters and simpler to train (cf. Sec.4.5).",
  "Two-Stage Noise-Aware Training": "We now present our two-stage training procedure, which sequentially learns the uncertainty functionand the feature extractor f(x) of the segmentation network. Specifically, we first freeze the pre-trained segmentation network and learn the semantic-exclusive uncertainty function using the relativecontrastive loss defined in Eq. 4. We then fine-tune the feature extractor with both the contrastive andstandard segmentation loss to improve the feature representations of both known and OOD classes. Despite the offline filtering process, the generated images may still contain regions that are inconsistentwith the label masks. To address this, we introduce a pixel-wise sample selection scheme duringtraining, based on the small loss criterion . Specifically, we compute and rank the cross-entropyloss for each pixel, selecting pixels with smaller losses for backpropagation while ignoring those withlarger losses. Formally, our selective cross-entropy loss is defined as",
  "cyci log pci ,(5)": "where pi and yi represent the pixel-wise softmax score and one-hot label, respectively, and i {0, 1}indicates whether a pixel is selected for backpropagation. We determine the percentage of selectedpixels per batch by visualizing the selection map of a small number of samples, ensuring that visiblyincorrect patterns are excluded (see for an example). For models using the Dice loss, suchas mask-based ones, we use a similar scheme to remove pixels with a large loss (cf. Appendix A.3). For the original data, which we assume to be noise-free, we set i = 1 for all pixels. This correspondsto using the standard cross-entropy loss. We denote the segmentation loss for the original data asLinseg and for the generated augmentation data as Laugseg. The overall loss function can be written as",
  "Here, 1 and 2 ensure that the three loss terms are on the same scale": "In summary, our semantic-exclusive uncertainty function, trained through a decoupled parametertraining approach and relative contrastive loss, enables the model to fully leverage the generateddistribution-shift data. Our noise-aware learning strategy enhances the models robustness againstgeneration errors. Together, these components of our training pipeline equip the model to effectivelylearn both domain generalization and accurate OOD detection, ensuring robust performance indynamic open-world scenarios.",
  "Experiments": "In this section, we evaluate our methods performance in jointly handling anomaly segmentationand domain generalization using several datasets that include both domain and semantic shifts:RoadAnomaly , SMIYC , ACDC-POC , and MUAD . We first introduce the datasetsin Sec.4.1 and describe the experimental setup in Sec.4.2. The results are presented in Sec.4.3 andSec.4.4, followed by an ablation study in Sec. 4.5.",
  "-79.7013.4588.714.6093.30.20RbA -85.426.9290.9011.6091.800.50M2F-EAM -69.407.7093.754.0992.870.52Ours97.9490.177.5491.927.9495.290.07": "and covariate shifts. (b) The SMIYC benchmark consists of RoadAnomaly21 (10 validation, 100test images) and RoadObstacle21 (30 validation, 327 test images), with anomaly objects and domainshifts. These datasets provide masks for anomaly objects, allowing us to evaluate our methodsperformance on anomaly segmentation under distribution shifts. Joint Anomaly Segmentation and Domain Generalization Datasets: (a) The ACDC-POCdataset is based on the original ACDC Validation set with generated anomaly objectsvia inpainting . It contains 200 images with domain shifts including various weather and nightscenes. (b) The MUAD dataset is a synthetic dataset containing various driving environmentsand anomaly objects. We use the challenge test set as in , which contains 240 images withdomain shifts at both object and image levels, and anomaly objects such as animals and trash cans.2These two datasets contain both known-class annotations and unknown object masks, enabling us toevaluate our method jointly for anomaly segmentation and domain generalization.",
  "Experimental Setup": "Performance Measure: For evaluation of anomaly segmentation, we use the Area Under the ReceiverOperating Characteristics curve (AUROC), the Average Precision (AP), and the False Positive Rateat a True Positive Rate of 95% (FPR95). For evaluation of known class segmentation, we use themean intersection-over-union (mIoU) and the mean accuracy (mAcc). Implementation Details: We build our method on two segmentation backbones: (a) DeepLabv3+ and (b) Mask2Former . We maintain the network architecture, pretrained models, segmentationloss, and training pipeline the same as in previous work to make a fair comparison. We usethe SMIYC validation set for model selection and maintain the same model for evaluation across alltest sets. We refer the reader to Appendix A for other training details.",
  "Results on Anomaly Segmentation Benchmarks": "We present the performance of our method on anomaly segmentation benchmarks, including Road-Anomaly and SMIYC (RA21 and RO21). As shown in , our method achieves state-of-the-artperformance on both DeepLabv3+ and Mask2Former-based models. With the same backbone, it out-performs RPL by 3% on RoadAnomaly and 5% on SMIYC, and surpasses Mask2Anomaly by 10% on RoadAnomaly and 3% on SMIYC. Recent methods, M2F-EAM and RbA ,use a more powerful Swin Transformer backbone, while ours uses ResNet-50, as Mask2Anomaly.M2F-EAM also uses Mapillary Vistas as additional dataset for training. Despite these unfaircomparisons, our method still outperforms both on most metrics, demonstrating its effectiveness.",
  "Since we use Cityscapes as the training set, the unknown object set differs from that used in": ": Results on ACDC-POC and MUAD. Our model achieves the best performance in bothanomaly segmentation (AP , FPR ) and domain-generalized segmentation (mIoU , mAcc). Anomaly segmentation methods typically perform worse than the baseline for known classsegmentation, while domain generalization methods fall below the baseline on OOD detection. (Bestresults are in bold; results below baseline are in blue.)",
  "LargeSmall": ": Comparison of Uncertainty Maps. Our method robustly detects anomalies under covariateshifts across five datasets (first five columns) and generated data (last column). The previous methodRPL failed to distinguish domain from semantic shifts, producing high uncertainty in both cases. In , we visualize the uncertainty map output by our method using the DeepLabv3+ architecture.Compared to the previous state-of-the-art method, RPL , our model assigns higher uncertaintyscores to anomalous objects and lower uncertainty scores to covariate shifts. This highlights theefficacy of our method in distinguishing between domain shifts and semantic shifts.",
  "Results on ACDC-POC and MUAD": "We then extend our evaluation to the ACDC-POC and MUAD datasets, assessing both anomalysegmentation performance and known-class domain generalization performance. For a comprehen-sive comparison, we include both previous state-of-the-art OOD detection techniques and domaingeneralization techniques . Additionally, we trained a DG+OOD combination method bycombining naive OOD training with contrastive loss and rule-based data augmentation (denoted asOOD+RuleAug). A DeepLabv3+ model with standard training is used as a baseline method. 3 The results are shown in , where our model achieves the best results for both out-of-distributiondetection and domain generalization, demonstrating its capacity in jointly handling both types ofdistribution shifts. By comparison, previous methods fall short in either known class segmentationor OOD detection. Specifically, we find that: (a) Previous works that mainly focus on domaingeneralization (RobustNet , RuleAug ) generally improve the known class segmentationresults, but their performance in OOD detection is affected, sometimes worse than the baseline.(b) Previous works that mainly focus on OOD detection (such as PEBAL ) perform poorly 3For all compared methods, except RuleAug, we used the official pretrained models provided by the respectiveauthors and performed re-inference to obtain the results. For RuleAug, we applied a combination of colorjittering, Gaussian blur, etc., as suggested in . For further details, please refer to the Appendix A.5 : Impact of CG-Aug and Training Strategy. The proposed coherent generative-basedaugmentation consistently enhances the previous OOD method, Mask2Anomaly (M2A forshort). Our fine-tuning strategy makes better use of the data and further boosts the performance.",
  "M2A Default79.7013.4594.503.3088.600.30M2A Ours85.4722.3897.961.5589.800.12OursOurs90.177.5497.311.0493.240.14": "on domain generalization, sometimes worse than the baseline. Furthermore, their OOD detectionperformance may also be affected by the domain shift. (c) Previous works that jointly handle image-level DG and OOD (RPL and OOD+RuleAug) may not fully distinguish object-level domainshifts. Our method leveraging diverse augmentations and a dedicated decoupled training strategyenables the model to jointly handle OOD detection and domain generalization. In Appendix C.1, we provide additional results on individual domain shifts (fog, rain, snow, night) andper-class evaluation. Furthermore, we compare our method with other DG methods on the originalACDC dataset in Appendix C.4, where we show superior domain generalization performance.",
  "Analysis and Ablation Study": "We conduct ablation studies to evaluate the design of our components. We begin by analyzing theeffectiveness of our proposed modules: the coherent generative-based augmentation (CG-Aug) andour model training strategy. We then proceed with a detailed examination of each modules design. Impact of CG-Aug and Training StrategyWe evaluate the decoupled contributions of our dataaugmentation and training strategies in . Starting with a recent anomaly segmentation method,Mask2Anomaly , we first replace its original OOD data, which utilizes cut-and-pasted COCOimages, with our proposed CG-Aug. As shown in Row #2, this substitution results in consistentperformance improvements across all datasets. This demonstrates the efficacy of introducing datawith both semantic and domain shifts in a coherent way. Next, we replace their training strategywith ours, leading to further gains in performance. This indicates that our training strategy is moreeffective in leveraging the generated data. Additionally, we present and discuss similar experimentsusing RPL as a baseline. For more details, please refer to Appendix . : Ablation Study of CG-Aug. Generatingdata with both Semantic-shift (SS) and Domain-shift (DS) in a coherent manner achieves betterresults than other variations. The experiments wereconducted using the Mask2Former backbone andevaluated on the RoadAnomaly dataset.",
  "CG-Aug (Ours)97.9490.177.54": "Ablation Study of our CG-AugThe pro-posed CG-Aug generate semantic-shift anddomain-shift jointly in a coherent way. To eval-uate the design, we compare with three varia-tions: (1) Semantic-Shift Only (SS): Generateimages with semantic shift using POC . (2)Domain-shift or Semantic-shift (DS or SS): Cre-ate a mixed dataset with either domain shifts(DS) using our semantic-mask-to-image processor semantic shifts (SS) using POC. (3) Domain-shift and Semantic-shift (DS and SS): First gener-ate DS data, then inpaint unknown objects. Thesecond and third methods can be seen as apply-ing to our problem in two ways. Resultsin show that: adding domain shift datasignificantly improves performance over semantic-shift-only data. Jointly generating DS and SS inone image yields better results than generating them separately. Our method, which generates bothDS and SS in one step, achieves the best performance, ensuring more coherence without artifactsand outperforming the two-step approach. We include more comparison results with POC inAppendix C.3. : Abaltion Study of Our Training Pipeline: Learnable Uncertainty Function (Learnable-UF),Relative Contrastive Loss (RelConLoss), and Noise-aware Sample Selection (Selection). Experimentsare conducted under DeepLabv3+ architecture.",
  "BaselineOurs-First StageOurs-Second StageSingle Stage": ": (a) Visualization of Our Selection Maps. Our selection strategy effectively identifiesand removes generation errors (highlighted with boxes). (b) Analysis of Our Two-Stage Training.The first stage of training the uncertainty function boosts baseline performance, and second-stagefine-tuning further improves performance, achieving better results than single-stage training. Ablation study of our Training DesignWe evaluate our training design in . In Row #1,we replace our Learnable Uncertainty Function (Learnable-UF) with a fixed energy function. InRow #2, we substitute our Relative Contrastive Loss (RelConLoss) with an absolute contrastiveloss , which directly supervises the uncertainty score value rather than the relative gapbetween two uncertainty scores. In Row #3, we remove the Sample Selection module. Compared toour complete method, presented in the final row, these modifications result in decreased performancein both OOD detection and domain generalization, highlighting the effectiveness of our moduledesign. A visualization of the sample selection process is shown in (a). In (b), we evaluate the effectiveness of our Stage-wise Training pipeline. Starting from a pre-trained baseline model, our first stagefine-tuning only the learnable uncertainty functiondoublesthe performance on SMIYC-RA/RO datasets, demonstrating that the initial uncertainty functionis often sub-optimal and can be significantly improved using fixed features . A second-stagefeature fine-tuning further boosts performance. Additionally, our two-stage approach outperformssingle-stage fine-tuning with a learnable uncertainty function, showing that training directly withuncalibrated uncertainties can disrupt feature learning and degrade OOD detection performance.",
  "Conclusion": "In this work, we have studied semantic segmentation under multiple distribution shifts, finding thatprior methods focusing separately on domain generalization and anomaly segmentation may noteffectively handle these complex shifts. To tackle this, we have introduced a coherent generative dataaugmentation approach that enriches training data with both domain and semantic shifts. Additionally,we have proposed a learnable uncertainty function, trained in a stage-wise manner, to fully utilize thedata and produce uncertainty scores specifically for semantic shifts. One limitation of our method isits reliance on the quality of the generative model. While we mitigate generation failures throughoffline autofiltering and online sample selection, some impact remains, such as lower performancefor classes the generative model struggles with and potential limitations in scaling up the generateddata (see Appendix E for details).",
  "Acknowledgement": "This work was supported by NSFC 62350610269, Shanghai Frontiers Science Center of Human-centered Artificial Intelligence, and MoE Key Lab of Intelligent Perception and Human-MachineCollaboration (ShanghaiTech University). Devansh Arpit, Stanisaw Jastrzebski, Nicolas Ballas, David Krueger, Emmanuel Bengio,Maxinder S Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, et al. Acloser look at memorization in deep networks. In International conference on machine learning.PMLR, 2017. Petra Bevandic, Ivan Kreo, Marin Oric, and Sinia egvic. Simultaneous semantic seg-mentation and outlier detection in presence of domain shift. In Pattern Recognition: 41stDAGM German Conference, DAGM GCPR 2019, Dortmund, Germany, September 1013, 2019,Proceedings 41. Springer, 2019. Qi Bi, Shaodi You, and Theo Gevers. Learning content-enhanced mask transformer for domaingeneralized urban-scene segmentation. In Proceedings of the AAAI Conference on ArtificialIntelligence, volume 38, pages 819827, 2024.",
  "Robin Chan, Matthias Rottmann, and Hanno Gottschalk. Entropy maximization and metaclassification for out-of-distribution detection in semantic segmentation.arXiv preprintarXiv:2012.06575, 2020": "Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam.Encoder-decoder with atrous separable convolution for semantic image segmentation. In Proc.of the European Conference on Computer Vision (ECCV), pages 801818, 2018. Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and Rohit Girdhar.Masked-attention mask transformer for universal image segmentation. In Proceedings of theIEEE/CVF conference on computer vision and pattern recognition, 2022. Sungha Choi, Sanghun Jung, Huiwon Yun, Joanne T. Kim, Seungryong Kim, and Jaegul Choo.Robustnet: Improving domain generalization in urban-scene segmentation via instance selectivewhitening. In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR), pages 1158011590, June 2021. Sungha Choi, Sanghun Jung, Huiwon Yun, Joanne T Kim, Seungryong Kim, and JaegulChoo. Robustnet: Improving domain generalization in urban-scene segmentation via instanceselective whitening. In Proceedings of the IEEE/CVF conference on computer vision andpattern recognition, pages 1158011590, 2021. Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, RodrigoBenenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semanticurban scene understanding. In Proc. of IEEE conference on computer vision and patternrecognition (CVPR), pages 32133223, 2016. Pau de Jorge, Riccardo Volpi, Puneet K Dokania, Philip HS Torr, and Gregory Rogez. Plac-ing objects in context via inpainting for out-of-distribution segmentation. arXiv preprintarXiv:2402.16392, 2024. Giancarlo Di Biase, Hermann Blum, Roland Siegwart, and Cesar Cadena. Pixel-wise anomalydetection in complex driving scenes. In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 1691816927, 2021. Lisa Dunlap, Alyssa Umino, Han Zhang, Jiezhi Yang, Joseph E Gonzalez, and Trevor Darrell.Diversify your vision datasets with automatic diffusion-based augmentation. Advances in neuralinformation processing systems, 36:7902479034, 2023. Gianni Franchi, Xuanlong Yu, Andrei Bursuc, Angel Tena, Rmi Kazmierczak, SverineDubuisson, Emanuel Aldea, and David Filliat. Muad: Multiple uncertainties for autonomousdriving, a benchmark for multiple uncertainty types and tasks. In 33rd British Machine VisionConference 2022, BMVC 2022, London, UK, November 21-24, 2022. BMVA Press, 2022.",
  "Zhitong Gao, Shipeng Yan, and Xuming He. Atta: Anomaly-aware test-time adaptation forout-of-distribution detection in segmentation. Advances in Neural Information ProcessingSystems, 2023": "Matej Grcic, Petra Bevandic, and Sinia egvic. Densehybrid: Hybrid anomaly detection fordense open-set recognition. In Computer VisionECCV 2022: 17th European Conference, TelAviv, Israel, October 2327, 2022, Proceedings, Part XXV, pages 500517. Springer, 2022. Matej Grcic, Josip aric, and Sinia egvic. On advantages of mask-level recognition foroutlier-aware segmentation. In Proceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 29372947, 2023. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for imagerecognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,pages 770778, 2016. Sobhan Hemati, Mahdi Beitollahi, Amir Hossein Estiri, Bassel Al Omari, Xi Chen, and GuojunZhang. Cross domain generative augmentation: Domain generalization with latent diffusionmodels. arXiv preprint arXiv:2312.05387, 2023.",
  "Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distributionexamples in neural networks. arXiv preprint arXiv:1610.02136, 2016": "Lei Huang, Yi Zhou, Fan Zhu, Li Liu, and Ling Shao. Iterative normalization: Beyondstandardization towards efficient whitening. In Proceedings of the IEEE/CVF conference oncomputer vision and pattern recognition, pages 48744883, 2019. Yuru Jia, Lukas Hoyer, Shengyu Huang, Tianfu Wang, Luc Van Gool, Konrad Schindler, andAnton Obukhov. Dginstyle: Domain-generalizable semantic segmentation with image diffusionmodels and stylized semantic control. In European Conference on Computer Vision, pages91109. Springer, 2025. Sanghun Jung, Jungsoo Lee, Daehoon Gwak, Sungha Choi, and Jaegul Choo. Standardized maxlogits: A simple yet effective approach for identifying unexpected road obstacles in urban-scenesegmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision,2021.",
  "Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprintarXiv:1412.6980, 2014": "Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple unified framework fordetecting out-of-distribution samples and adversarial attacks. In Proc. of the Advances in NeuralInformation Processing Systems (NeurIPS), pages 71677177, 2018. Yumeng Li, Dan Zhang, Margret Keuper, and Anna Khoreva. Intra-source style augmentationfor improved domain generalization. In Proceedings of the IEEE/CVF Winter Conference onApplications of Computer Vision, pages 509519, 2023. Shiyu Liang, Yixuan Li, and Rayadurgam Srikant.Enhancing the reliability of out-of-distribution image detection in neural networks. In Proc. of the International Conferenceon Learning Representations (ICLR), 2018. Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays,Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Dollr. Microsoft coco: Commonobjects in context. arXiv preprint arXiv:1405.0312, 2014.",
  "Krzysztof Lis, Krishna Nakka, Pascal Fua, and Mathieu Salzmann. Detecting the unexpectedvia image resynthesis. In Proc. of IEEE international conference on computer vision (ICCV),2019": "Yuyuan Liu, Choubo Ding, Yu Tian, Guansong Pang, Vasileios Belagiannis, Ian Reid, andGustavo Carneiro. Residual pattern learning for pixel-wise out-of-distribution detection insemantic segmentation. In Proceedings of the IEEE/CVF International Conference on ComputerVision, 2023. Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and BainingGuo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedingsof the IEEE/CVF international conference on computer vision, pages 1001210022, 2021.",
  "S Minaee, YY Boykov, F Porikli, AJ Plaza, N Kehtarnavaz, and D Terzopoulos. Imagesegmentation using deep learning: A survey. IEEE Transactions on Pattern Analysis andMachine Intelligence, 2021": "Nazir Nayal, Misra Yavuz, Joao F Henriques, and Fatma Gney. Rba: Segmenting unknownregions rejected by all. In Proceedings of the IEEE/CVF International Conference on ComputerVision, pages 711722, 2023. Gerhard Neuhold, Tobias Ollmann, Samuel Rota Bulo, and Peter Kontschieder. The mapil-lary vistas dataset for semantic understanding of street scenes. In Proceedings of the IEEEinternational conference on computer vision, 2017. Mehrdad Noori, Milad Cheraghalikhani, Ali Bahri, Gustavo Adolfo Vargas Hakim, DavidOsowiechi, Moslem Yazdanpanah, Ismail Ben Ayed, and Christian Desrosiers. Feedback-guideddomain synthesis with multi-source conditional diffusion models for domain generalization.arXiv preprint arXiv:2407.03588, 2024. Xingang Pan, Ping Luo, Jianping Shi, and Xiaoou Tang. Two at once: Enhancing learning andgeneralization capacities via ibn-net. In Proceedings of the european conference on computervision (ECCV), 2018. Xingang Pan, Xiaohang Zhan, Jianping Shi, Xiaoou Tang, and Ping Luo. Switchable whiteningfor deep representation learning. In Proceedings of the IEEE/CVF international conference oncomputer vision, pages 18631871, 2019.",
  "Duo Peng, Yinjie Lei, Munawar Hayat, Yulan Guo, and Wen Li. Semantic-aware domaingeneralized segmentation. In Proceedings of the IEEE/CVF conference on computer vision andpattern recognition, 2022": "Shyam Nandan Rai, Fabio Cermelli, Dario Fontanel, Carlo Masone, and Barbara Caputo. Un-masking anomalies in road-scene segmentation. In Proceedings of the IEEE/CVF InternationalConference on Computer Vision, 2023. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjrn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVFconference on computer vision and pattern recognition, 2022. Christos Sakaridis, Dengxin Dai, and Luc Van Gool. ACDC: The adverse conditions dataset withcorrespondences for semantic driving scene understanding. In Proceedings of the IEEE/CVFInternational Conference on Computer Vision (ICCV), October 2021. Manuel Schwonberg, Fadoua El Bouazati, Nico M Schmidt, and Hanno Gottschalk.Augmentation-based domain generalization for semantic segmentation. In 2023 IEEE In-telligent Vehicles Symposium (IV). IEEE, 2023. Yu Tian, Yuyuan Liu, Guansong Pang, Fengbei Liu, Yuanhong Chen, and Gustavo Carneiro.Pixel-wise energy-biased abstention learning for anomaly segmentation on complex urbandriving scenes. In Computer VisionECCV 2022: 17th European Conference, Tel Aviv, Israel,October 2327, 2022, Proceedings, Part XXXIX, pages 246263. Springer, 2022. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural informationprocessing systems, 30, 2017. Yingda Xia, Yi Zhang, Fengze Liu, Wei Shen, and Alan L Yuille. Synthesize then compare:Detecting failures and anomalies for semantic segmentation. In Computer VisionECCV 2020:16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part I 16, 2020. Xuanlong Yu, Yi Zuo, Zitao Wang, Xiaowen Zhang, Jiaxuan Zhao, Yuting Yang, LichengJiao, Rui Peng, Xinyi Wang, Junpei Zhang, et al. The robust semantic segmentation uncv2023challenge results. In Proceedings of the IEEE/CVF International Conference on ComputerVision, 2023. Xiangyu Yue, Yang Zhang, Sicheng Zhao, Alberto Sangiovanni-Vincentelli, Kurt Keutzer, andBoqing Gong. Domain randomization and pyramid consistency: Simulation-to-real general-ization without accessing target domain data. In Proceedings of the IEEE/CVF InternationalConference on Computer Vision, pages 21002110, 2019. Oliver Zendel, Katrin Honauer, Markus Murschitz, Daniel Steininger, and Gustavo FernandezDominguez. Wilddash-creating hazard-aware benchmarks. In Proceedings of the EuropeanConference on Computer Vision (ECCV), pages 402416, 2018. Dan Zhang, Kaspar Sakmann, William Beluch, Robin Hutmacher, and Yumeng Li. Anomaly-aware semantic segmentation via style-aligned ood augmentation.In Proceedings of theIEEE/CVF International Conference on Computer Vision, 2023.",
  "Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-imagediffusion models. In Proceedings of the IEEE/CVF International Conference on ComputerVision, 2023": "Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba.Scene parsing through ade20k dataset. In Proceedings of the IEEE conference on computervision and pattern recognition, pages 633641, 2017. Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, and AntonioTorralba. Semantic understanding of scenes through the ade20k dataset. International Journalof Computer Vision, 127:302321, 2019.",
  "A.1Zero-Shot Semantic-to-Image Generation": "We adopt a pretrained semantic-to-image generation model, ControlNet 1.0 , provided by theofficial GitHub repository4, for our generation process. This model is based on Stable Diffusion and fine-tuned on ADE20K . It takes two main inputs: a semantic mask and a text prompt. To obtain our pasted semantic mask, we first convert the masks from Cityscapes labels to ADE20Klabels, then overlay this with auxiliary out-of-distribution (OOD) object masks. Specifically, weuse the mask labels from ADE20K that belong to the thing categories, excluding those with labelsshared with Cityscapes. Our text prompts have two parts: one part specifies the domain shifts, and the other specifies theOOD objects. For domain-shift prompts, we use the template An image sampled from variousstereo video sequences taken by dash cam in {PLACE} in a {WEATHER} {TIME},where we define PLACE as a set of 100 cities worldwide, WEATHER as [cloudy, rainy,snowy, foggy, clear], and TIME as [day, night]. Additionally, we improvethe OOD object generation by indicating the specific class of pasted objects in the prompt withthe template: There is a {OOD} accidentally staying on the road., where OOD is theclass name of the pasted object. This further contextualizes the generated scene to reflect realisticanomaly scenarios.",
  "A.2Auto-Filtering of Failed Generations": "The generation process can be noisy, particularly when generating image regions for pasted OODobject masks. By nature, these objects are anomalies within the scenes, appearing rarely, and their cutmasks may exhibit shapes or poses inconsistent with their surroundings. As a result, the generatedobjects may deviate significantly in shape from the intended mask, be overlooked (blending intothe surroundings), or be incorrectly generated as more common objects within the scene. Suchdiscrepancies make the raw augmented image-label pairs too noisy for direct training. To address these issues, we design an automatic filtering process to identify generation failures, suchas cases where no object is generated or a known-category object is incorrectly produced. If thegenerated object is present and does not belong to a known category, we retain the image whilerevising the corresponding mask for the generated novel object. Otherwise, we discard the imageand regenerate it. To implement this, we use the Segment-Anything Model (SAM), providing thebounding box location of the pasted mask as input to obtain a segmentation. We then compare theSAM output with the original mask, identifying it as a failure case if the Intersection over Union(IoU) is very low (below 0.7). Additionally, we employ a pre-trained segmentation model to producean uncertainty score for the generated objects, filtering out those with very low uncertainty scores,as these are likely to have been misgenerated into a known-category object. This comprehensivefiltering process effectively enhances the quality of the training dataset, making it better suited foreffective model training.",
  "A.3Training Details on Mask2Former Backbone": "Following Mask2Anomaly , we train Mask2Former using a combination of dice loss andbinary cross-entropy (BCE) loss for the mask prediction head, and cross-entropy loss for the classprediction head. For the dice and BCE losses, we modify the sampling strategy for generated imagesto implement the sample selection process described in Sec. 3.3. Specifically, we compute thepixel-level BCE loss and select pixels with lower BCE losses for backpropagation in both the diceand BCE loss calculations. Since most generation errors occur at the pixel level, we do not applysample selection for the mask-wise class prediction in the mask prediction head. We maintain the same model architecture as Mask2Anomaly, which includes a ResNet-50 backbone, a pixel decoder, a Transformer decoder, and a global mask attention mechanism thatindependently distributes attention between foreground and background. As in Mask2Anomaly, wekeep the ResNet backbone frozen while training the remaining model components, and we employ the AdamW optimizer with its default learning rate and scheduler. For our method, the uncertaintyloss margins are set to 1 = 0.7, 2 = 0.5, and 3 = 0.2, and we use a selection ratio = 0.8. Theloss weights 1 and 2 are set to 10 for both. We use a batch size of 8 for all experiments and trainthe model on a single NVIDIA A40 48GB GPU.",
  "A.4Training Details on DeepLabv3+ Backbone": "For the DeepLabv3+ backbone, we follow the setup in RPL , using DeepLabv3+ with WideRes-Net38 pretrained by Nvidia. The backbone remains fixed, and only the ASPP layers are fine-tuned.We use the Adam optimizer with a learning rate of 1.0e-6. For our method, the contrastivemargins 1, 2, 3 are set to 10,5,5, the selection ratio is 0.8. The loss weights are 50 and 10 for 1and 2 respectively. The batch size is set to 8, and all experiments are conducted on two NVIDIAA40 48GB GPUs.",
  "A.5Rule-Based Augmentation": "As a typical approach to domain generalization, we implement Rule-Based Augmentation as outlinedin , using a set of image transformations. Specifically, we apply the following transformations,with their application probabilities indicated in parentheses: color jittering (0.5), Gaussian blur (0.5),random sharpness adjustment (0.5), random contrast adjustment (0.5), random equalization (0.5),random resizing (0.5), random rotation (0.5), random horizontal flipping (0.75), and random cropping(1.0).",
  "B.1Impact of Hyperparameters": "Loss MarginsOur relative contrastive loss 4 includes three terms, each with a margin value controlling the distance penalty limits. These margins are set based on the average uncertainty scoresfrom the training set. Specifically, we compute the differences in uncertainty scores between unknownvs. original known data, unknown vs. augmented known data, original known vs. augmented knowndata, and set the differences as margins for these distance respectively. Moreover, our two-stagetraining framework first trains the uncertainty function based on the existing model, allowing thisfunction to adapt to different scales. This provides flexibility in parameter setting even without priorknowledge. In , we evaluate the models robustness across a wide range of hyperparameter variations.Our default loss margins are = . We start by scaling them by 0.1 and 10 andconduct experiments using (1, 0.5, 0.5) and (100, 50, 50), respectively. As shown in (a), theuncertainty function adjusted to these scales with minimal impact on the results. Further analysis,such as changing the second and third parameters individually, showed that while the relative sizes ofthe three contrastive losses have some impact, the effect remains minor(see (b) and (c)).Ensuring that the parameters are set within an order of magnitude does not affect the results much. : Impact of Loss Margins. We examine model robustness across various loss margins byevaluating margin scale impacts in (a) and analyzing effects of individual margins in (b) and (c).Results are reported on SMIYC-RA Val(AP & FPR) and MUAD (mIoU) using the DeepLabv3+architecture.",
  "3.2732.24593.82 3.9431.33792.45 3.2131.7310 92.08 3.9631.56": "Selection RatioWe determine the selection ratio for our sample selection process by visualizing theselection map of a small batch of data under several choices to ensure that visibly incorrect patternsare removed. To examine the impact of selection ratio to our method, we conducted experimentswith selection ratios ranging from 0.6 to 0.9, as detailed in (a). The results show that whileincluding too many pixels (1.0) introduces noise, and including too few (0.6) removes useful regions,the model performance is stable within a wide range (0.7 to 0.9), demonstrating the robustness of themodel to this hyperparameter.",
  "Anomaly Score Histogram": ":(a) Impact of Sample Selection Ratio.We report both anomaly segmentationperformance(AP, FPR on SMIYC-RA Val) and known class segmentation performance (mIoUon MUAD). Experiments are conducted under DeepLab v3+ architecture. (b) Impact of GeneratedData Size. We observe an improvement of performance with the increase of generated data size withthe same evaluation under Mask2Former architecture.",
  "B.2Impact of the Size of Generated Dataset": "By default, we generate our distribution-shift dataset at the same size as Cityscapes. To analyze theimpact of the generated dataset size, we scale it to 2x and 3x the size of the Cityscapes training set.As shown in (b), there is a significant improvement from dataset sizes 0 to 1, demonstrating theeffectiveness of our generated data, with further gains observed as the dataset size increases.",
  "B.3Impact of CG-Aug and Training Strategy for RPL": "We evaluate the decoupled contributions of our data augmentation and training strategies withRPL in . Similar to , we first replace its original OOD data, which utilizes cut-and-pasted COCO images, with our proposed CG-Aug in Row#2. However, we find the improvementis not as significant. This may be due to certain aspects of RPLs loss and training design being lesssuitable for our scenario. Firstly, RPL relies on the original networks predictions to supervise alearnable residual part. Since the original network does not generalize well to data with domain-shift, this results in imprecise supervision. Secondly, the RPL uncertainty loss focuses solely onincreasing uncertainty for unknowns, without adequately addressing the known classes, particularlyfor augmented images. Additionally, restricting the trainable parameters to a residual block may limitthe models ability to learn more complex patterns, thereby reducing overall effectiveness. Next, we replace their training strategy with ours, leading to consistent performance improvement.Those results demonstrate that effectively utilizing the generated training data with multiple distribu-tion shifts remains an open question. Our work takes a step towards analyzing the shortcomings ofexisting training designs, offering novel and effective strategies for better handling this data.",
  "C.1Additional Results on ACDC-POC": "Performance under Individual Domain ShiftsIn addition to the main , we present theACDC-POC results with domain-specific splits to provide a more detailed analysis of our methodacross different types of domain shifts. As shown in , our method outperforms previousstate-of-the-art methods (RPL and Mask2Anomaly ) across four domainsfog, rain, snow,and nighton most metrics in both OOD detection and known-class segmentation. : Impact of CG-Aug and Training Strategy. We evaluate our proposed coherent generative-based augmentation on the previous OOD method, RPL , the improvement is not obvious.However, with our training strategy, the performance has largely improved. This demonstrates ourtraining method can effectively utilize the generated training data with multiple distributions.",
  "M2A 83.90.967.394.3 75.91.753.291.0 71.02.345.286.4 75.83.929.561.8Ours (Mask2Former) 90.50.969.794.2 91.30.354.591.2 90.70.651.586.7 88.70.431.861.6": "Per-Class Segmentation ResultsWe evaluated per-class segmentation results and compared themwith the baseline DeepLabv3+ model. Results are presented in . Our method improvessegmentation performance (mIoU) across most categories. However, performance in fence, pole,and traffic sign remains similar (with differences of less than 1%), and performance on vegetationdecreases by 3%, likely due to poor generation quality for this class. : Per-class segmentation results. We present the segmentation performance (mIoU) for eachknown class on the ACDC-POC dataset. Compared to the baseline model (DeepLabv3+ ), ourmethod improves performance in most categories.",
  "C.3Comparison with POC": "We present additional comparison results of our CG-Aug method against POC across sixdatasets. Following the experimental setup in POC , we replace the default OOD data (COCO) inMask2Anomaly with our CG-Aug. As shown in , our method outperforms both POCvariations on most datasets. Notably, for FS-Static , where OOD objects are introduced through : Comprehensive Metric Results on SMIYC. We present the results of our method acrosspixel-wise metrics (AP, FPR) and component-wise metrics (sIoU, PPV, and F1). Compared to recentmethods RPL and Mask2Anomaly , our method achieves superior or comparable results.",
  "C.4Comparison with DG Methods on the Original ACDC Dataset": "To assess the effectiveness of our method in domain generalization, we conducted additional com-parisons with several recent approaches, including IBN , IterNorm , IW , ISW ,ISSA , and CMFormer , on the ACDC dataset. As shown in , our method outper-forms all ResNet-based methods in the Fog, Rain, and Snow domains, achieving comparable resultsin the Night domain. Among Mask2Former-based methods, our approach also surpasses ISSA ,which similarly uses a ResNet backbone. However, there remains a notable performance gap betweenour method and CMFormer , likely due to CMFormers use of the Swin Transformer as thebackbone for Mask2Former. : Domain generalization performance comparison between our method and other DGmethods. Results from other methods are taken from CMFormer . All methods are trained on theCityscapes dataset and tested on the ACDC dataset. Results are shown in mIoU (%).",
  "DVisualization of Generated Data": "In , we provide additional visualization examples of our generated images (row 2), correspondingselection maps (row 3), and the loss maps used to produce the selection map (row 4). Below eachcolumn, we display the weather, time, and location prompts that guide the model in generating diverse covariate shifts, as well as the OOD prompts used to generate objects. As shown, our methodeffectively generates images with both domain and semantic shifts, with the novel objects blendingseamlessly into the background (e.g. pose and lighting). Additionally, our sample selection processeffectively filters out some generation errors (highlighted in red boxes).",
  "HL": ": Visualization of Generated Images. Row 1: Original images from Cityscapes. Row 2:Generated images featuring both semantic and domain shifts. Row 3: Selection map used to calculateselected cross-entropy loss during training. Row 4: Cross-entropy loss map used to produce theselection map (excluding the OOD regions, which are not involved in known class segmentation losscalculation). Below each column, we display the weather, time, and location prompts that guide themodel in generating diverse covariate shifts, along with the OOD prompts for object generation. Redboxes highlight generation errors.",
  "EDiscussion of Generation Failures and Their Impact": "A limitation of our method is its reliance on the quality of the generative model. Although we applyoffline auto-filtering and online sample selection to minimize the impact of generation failures duringtraining, some issues may still arise. Specifically, we observe that generation failures typically occurin the following scenarios: (a) remote scenes, (b) small objects, and (c) text-related elements. Theselimitations highlight the current constraints of generative models and suggest areas for future research.Below, we discuss the impact of generation failures: Impact on Class-Specific Learning:Generation failures can adversely affect specific classes. Asshown in , we evaluated per-class segmentation results and compared them with the baselinemodel on Cityscapes. Performance in categoriessuch as fence, pole, and traffic signremainssimilar (differences of less than 1%), and vegetation shows a 3% decrease, likely due to lowergeneration quality for these classes. Performance Saturation:We observe that performance tends to saturate with increasing amountsof generated data. Experiments with dataset scaling from 1.0x to 2.0x and 3.0x Cityscapes sizes, asshown in , indicate that while performance improves with larger dataset size, it eventuallyplateaus. This saturation may result from an interplay between the benefits of additional data and theadverse effects of generation failures.",
  "FSocietal Impacts": "Enhancing OOD detection in autonomous vehicles can significantly improve safety by enabling thesesystems to better recognize and respond to novel and unexpected situations, thereby reducing the riskof accidents. Improved robustness to domain shifts also contributes to greater resilience and safetyacross diverse driving scenarios. However, improved OOD detection may lead to an over-relianceon autonomous systems, potentially reducing the vigilance of human drivers or passengers in semi-autonomous vehicles. Additionally, unintended biases in OOD detection systems could result inunsafe responses to certain situations, particularly if training data does not sufficiently cover diversescenarios, potentially compromising safety in rare but critical cases.",
  "The answer NA means that the paper has no limitation while the answer No means thatthe paper has limitations, but those are not discussed in the paper": "The authors are encouraged to create a separate \"Limitations\" section in their paper. The paper should point out any strong assumptions and how robust the results are toviolations of these assumptions (e.g., independence assumptions, noiseless settings,model well-specification, asymptotic approximations only holding locally). The authorsshould reflect on how these assumptions might be violated in practice and what theimplications would be. The authors should reflect on the scope of the claims made, e.g., if the approach wasonly tested on a few datasets or with a few runs. In general, empirical results oftendepend on implicit assumptions, which should be articulated. The authors should reflect on the factors that influence the performance of the approach.For example, a facial recognition algorithm may perform poorly when image resolutionis low or images are taken in low lighting. Or a speech-to-text system might not beused reliably to provide closed captions for online lectures because it fails to handletechnical jargon.",
  "If applicable, the authors should discuss possible limitations of their approach toaddress problems of privacy and fairness": "While the authors might fear that complete honesty about limitations might be used byreviewers as grounds for rejection, a worse outcome might be that reviewers discoverlimitations that arent acknowledged in the paper. The authors should use their bestjudgment and recognize that individual actions in favor of transparency play an impor-tant role in developing norms that preserve the integrity of the community. Reviewerswill be specifically instructed to not penalize honesty concerning limitations.",
  ". Experimental Result Reproducibility": "Question: Does the paper fully disclose all the information needed to reproduce the main ex-perimental results of the paper to the extent that it affects the main claims and/or conclusionsof the paper (regardless of whether the code and data are provided or not)?Answer: [Yes]Justification:Guidelines: The answer NA means that the paper does not include experiments. If the paper includes experiments, a No answer to this question will not be perceivedwell by the reviewers: Making the paper reproducible is important, regardless ofwhether the code and data are provided or not.",
  "If the contribution is a dataset and/or model, the authors should describe the steps takento make their results reproducible or verifiable": "Depending on the contribution, reproducibility can be accomplished in various ways.For example, if the contribution is a novel architecture, describing the architecture fullymight suffice, or if the contribution is a specific model and empirical evaluation, it maybe necessary to either make it possible for others to replicate the model with the samedataset, or provide access to the model. In general. releasing code and data is oftenone good way to accomplish this, but reproducibility can also be provided via detailedinstructions for how to replicate the results, access to a hosted model (e.g., in the caseof a large language model), releasing of a model checkpoint, or other means that areappropriate to the research performed. While NeurIPS does not require releasing code, the conference does require all submis-sions to provide some reasonable avenue for reproducibility, which may depend on thenature of the contribution. For example(a) If the contribution is primarily a new algorithm, the paper should make it clear howto reproduce that algorithm.",
  "(b) If the contribution is primarily a new model architecture, the paper should describethe architecture clearly and fully": "(c) If the contribution is a new model (e.g., a large language model), then there shouldeither be a way to access this model for reproducing the results or a way to reproducethe model (e.g., with an open-source dataset or instructions for how to constructthe dataset). (d) We recognize that reproducibility may be tricky in some cases, in which caseauthors are welcome to describe the particular way they provide for reproducibility.In the case of closed-source models, it may be that access to the model is limited insome way (e.g., to registered users), but it should be possible for other researchersto have some path to reproducing or verifying the results.",
  "Guidelines:": "The answer NA means that the paper does not include experiments. The authors should answer \"Yes\" if the results are accompanied by error bars, confi-dence intervals, or statistical significance tests, at least for the experiments that supportthe main claims of the paper. The factors of variability that the error bars are capturing should be clearly stated (forexample, train/test split, initialization, random drawing of some parameter, or overallrun with given experimental conditions).",
  ". Experiments Compute Resources": "Question: For each experiment, does the paper provide sufficient information on the com-puter resources (type of compute workers, memory, time of execution) needed to reproducethe experiments?Answer: [Yes]Justification:Guidelines: The answer NA means that the paper does not include experiments. The paper should indicate the type of compute workers CPU or GPU, internal cluster,or cloud provider, including relevant memory and storage.",
  "Question: Does the paper discuss both potential positive societal impacts and negativesocietal impacts of the work performed?Answer: [Yes]Justification:Guidelines:": "The answer NA means that there is no societal impact of the work performed. If the authors answer NA or No, they should explain why their work has no societalimpact or why the paper does not address societal impact. Examples of negative societal impacts include potential malicious or unintended uses(e.g., disinformation, generating fake profiles, surveillance), fairness considerations(e.g., deployment of technologies that could make decisions that unfairly impact specificgroups), privacy considerations, and security considerations. The conference expects that many papers will be foundational research and not tiedto particular applications, let alone deployments. However, if there is a direct path toany negative applications, the authors should point it out. For example, it is legitimateto point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point outthat a generic algorithm for optimizing neural networks could enable people to trainmodels that generate Deepfakes faster. The authors should consider possible harms that could arise when the technology isbeing used as intended and functioning correctly, harms that could arise when thetechnology is being used as intended but gives incorrect results, and harms followingfrom (intentional or unintentional) misuse of the technology. If there are negative societal impacts, the authors could also discuss possible mitigationstrategies (e.g., gated release of models, providing defenses in addition to attacks,mechanisms for monitoring misuse, mechanisms to monitor how a system learns fromfeedback over time, improving the efficiency and accessibility of ML).",
  ". Safeguards": "Question: Does the paper describe safeguards that have been put in place for responsiblerelease of data or models that have a high risk for misuse (e.g., pretrained language models,image generators, or scraped datasets)?Answer: [NA]Justification:Guidelines: The answer NA means that the paper poses no such risks. Released models that have a high risk for misuse or dual-use should be released withnecessary safeguards to allow for controlled use of the model, for example by requiringthat users adhere to usage guidelines or restrictions to access the model or implementingsafety filters.",
  ". Licenses for existing assets": "Question: Are the creators or original owners of assets (e.g., code, data, models), used inthe paper, properly credited and are the license and terms of use explicitly mentioned andproperly respected?Answer: [Yes]Justification:Guidelines: The answer NA means that the paper does not use existing assets. The authors should cite the original paper that produced the code package or dataset. The authors should state which version of the asset is used and, if possible, include aURL. The name of the license (e.g., CC-BY 4.0) should be included for each asset. For scraped data from a particular source (e.g., website), the copyright and terms ofservice of that source should be provided. If assets are released, the license, copyright information, and terms of use in thepackage should be provided. For popular datasets, paperswithcode.com/datasetshas curated licenses for some datasets. Their licensing guide can help determine thelicense of a dataset.",
  "According to the NeurIPS Code of Ethics, workers involved in data collection, curation,or other labor should be paid at least the minimum wage in the country of the datacollector": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with HumanSubjectsQuestion: Does the paper describe potential risks incurred by study participants, whethersuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)approvals (or an equivalent approval/review based on the requirements of your country orinstitution) were obtained?Answer: [NA]Justification:Guidelines:",
  "The answer NA means that the paper does not involve crowdsourcing nor research withhuman subjects": "Depending on the country in which research is conducted, IRB approval (or equivalent)may be required for any human subjects research. If you obtained IRB approval, youshould clearly state this in the paper. We recognize that the procedures for this may vary significantly between institutionsand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and theguidelines for their institution."
}