{
  "Abstract": "The human brain encodes stimuli from the environment into representations thatform a sensory perception of the world. Despite recent advances in understandingvisual and auditory perception, olfactory perception remains an under-exploredtopic in the machine learning community due to the lack of large-scale datasetsannotated with labels of human olfactory perception. In this work, we ask thequestion of whether pre-trained transformer models of chemical structures en-code representations that are aligned with human olfactory perception, i.e., cantransformers smell like humans? We demonstrate that representations encodedfrom transformers pre-trained on general chemical structures are highly alignedwith human olfactory perception. We use multiple datasets and different types ofperceptual representations to show that the representations encoded by transformermodels are able to predict: (i) labels associated with odorants provided by experts;(ii) continuous ratings provided by human participants with respect to pre-defineddescriptors; and (iii) similarity ratings between odorants provided by human partic-ipants. Finally, we evaluate the extent to which this alignment is associated withphysicochemical features of odorants known to be relevant for olfactory decoding.",
  "Introduction": "The human brain receives sensory input from the environment and encodes it into a high-dimensionalrepresentation space, forming a perception of the world . Recent studies have significantlyimproved our understanding of the underlying mechanisms of visual, linguistic, and auditory percep-tion . Indeed, there is a significant level of alignment between human response (from neuron tobehavior) and activations of deep neural networks when provided with the same stimuli . Despite these recent advances, human olfactory perception remains an under-explored topic. There isno single organizing principle that determines the dimensions of odor space, making the characteriza-",
  "CC(O)CN CC(=O)O": ": Evaluating representational alignment between human and pre-trained transformers.Human participants are stimulated with two odorant substances and asked to rate the perceptualsimilarity between them (Left). We encode representations of the same pair of odorants usingMoLFormer and compute the similarity between pairs of representations (Right). Finally, we measurethe alignment between the two systems. tion of odor perception and its relation to chemical compounds an open and complex problem . Alack of universally accepted methods to describe odorants either quantitatively or qualitatively makesthis problem even more challenging. There are very few studies that have explored the mappingof chemical structures to olfactory perception . In addition, processing chemical olfactorystimuli using deep neural networks has not been extensively investigated. Nevertheless, training theexisting supervised models usually requires an extensive effort by experts to label data. Transformer-based models are a breakthrough in machine learning, surpassing the need forextensive labeling by utilizing implicit supervision without the necessity for direct labels. Thesemodels have demonstrated impressive performance in various tasks such as image , video ,and natural language processing . More recently, they have also shown promising results inencoding chemical structures . In this work, we ask the question of whether representations of odorant chemical structures extractedfrom transformers pre-trained on chemical structures align with human olfactory perception or, inother words, can transformers smell like humans? We employ MoLformer , a state-of-the-arttransformer, which is pre-trained on chemical structures and we show that representations of odorantsextracted from this model:",
  "The availability of larger datasets, together with advances in predictive methods, has led to anincreasing interest in the prediction of olfactory perception from molecular structures": "Olfactory perception prediction. Learning predictive models of olfactory from molecular structureshas been addressed mostly by the neuroscience community. Several works used standard chemoinfor-matic representations of molecules to model olfactory perception . Specifically, Snitz etal. proposed a computational framework and algorithm based on structural features of moleculesto predict perceptual similarities between odorant pairs. This algorithm leverages feature engineeringto identify the most relevant subset of features among 1433 physicochemical descriptors to predictpair-wise odorant perceptual similarities. Later, Ravia et al. extended this model to also include the perceived intensity of molecularcomponents. They employed 21 physicochemical descriptors discovered in previous works andproposed a weighting approach for multicomponent odorants (MC-odorants) based on their perceivedintensity. They reported a higher correlation when employing the weighting approach compared tousing the same model without it. However, the representation and generalization capabilities of thesemodels are quite limited and unexplored. Deep neural networks for odorants. Recently, Lee et al. proposed a novel representationlearning model of odorants, based on a message-passing graph neural network , which theyrefer to as Principal Odor Map (POM). To train this model, they curated and merged data fromLeffingwell and GoodScent databases to compile a dataset of about 5000 molecules with138 expert-labeled odor descriptors. This model outperforms the baselines in multiple odor predictiontasks and shows a relatively high alignment with human ratings in describing odorants. Nevertheless,training this model requires labeled data, relying on subjective evaluations of numerous odorants byexperts. Besides being time-consuming and laborious, this process can introduce subjective biasesinto the model, a concern magnified by our incomplete understanding of the foundational factors ofolfactory perception. Large-scale molecular models. Large-scale pre-trained models, often known as foundation mod-els , have been recently explored to perform diverse tasks by leveraging large amounts of unlabeleddata. MoLFormer model has been proposed in the context of chemical prediction tasks, able toextract rich representations from chemical structures. MoLFormer consists of a transformer-basedarchitecture, with linear attention and relative positional encodings. This model is trained using aself-supervised approach, on multiple datasets (e.g., the PubChem and ZINC datasets) on amasked token prediction loss.",
  "Method": "In this section, we provide a detailed description of the datasets utilized in this study and outline themethodology for extracting both odorant (machine) and perceptual (human) representations. Addition-ally, we present the main model and baseline methods employed, along with the evaluation metricsused to assess their performance. Our experiments do not require significant computational resources:we mostly train linear models that do not involve GPU usage or models that can be trained on a singlecommercially available GPU under one hour. All computational code to reproduce our results isavailable at",
  "We use the publicly available version of the following datasets provided by Pyrfume repository": "Leffingwell-Goodscent (GS-LF) .We employ a curated and merged version of theGoodscents and Leffingwell datasets, provided by , following the procedure intro-duced by Lee et al. . This dataset contains 4983 molecules with 138 expert-labeled descriptors(e.g. creamy, grassy), where each odorant may be linked to multiple descriptors. Sagar . This dataset contains the rating of 160 odorants by 3 human participants, with respect to15+3 perceptual descriptors. In addition to 15 common descriptors among participants, there are 3more descriptors that vary among them. We excluded these variable descriptors and focused solelyon the common descriptors among the participants. The provided ratings were already normalizedwithin the range of and the mean ratings across all the subjects are computed for subsequentanalysis. Keller . This dataset contains ratings of 480 structurally and perceptually diverse molecules by55 human participants, evaluating 23 descriptors. Participants were instructed to adjust a slider torate odorants according to individual descriptors, with the slider position subsequently translated intoa scale ranging from 0 to 100. Ratings were then averaged across all participants for further analysis. Ravia . This dataset contains similarity ratings of 195 unique pairs of MC-odorants and mono-molecules by 94 participants. The similarity values were averaged across all the participants. In thiswork, we disregarded the factor of odorant intensity and averaged similarity ratings based on theunique pairs of odorants. Snitz . This dataset includes similarity ratings from 139 participants and 359 unique pairs ofodorants. In each trial, participants were presented with two distinct odorants and asked to rate thedegree of similarity in their smells. These ratings were then averaged across all participants.",
  "Odorant representations": "Odorants can be described as a single molecule or a mixture of molecules, which we denote asmulticomponent odorants (MC-odorants). In this section we describe the method to extract odorantrepresentations from the main pre-trained model (MoLFormer) and our baseline models (DAM andOpen-POM). MoLFormer. We employ MoLFormer to encode SMILES strings associated with a singlemolecule and extract a 768-dimensional vector from the last layer of the model. SMILES (simplifiedmolecular-input line-entry system) is a string-based representation that encodes relevant chemicalinformation such as the type of atoms, their bonds, and the substructures present in the molecule.For MC-odorants, we average the extracted representations across all available mono-moleculecomponents within that MC-odorant. The odorant representation for each dataset is a matrix ofXi Rn768 where n is the number of unique odorants. Open-POM. The principal odor map (POM) is a supervised-learning model, based on a message-passing graph neural network , which is trained on the GS-LF datasets to predict odorant percep-tual labels. We employ a publicly available version of this model, which we denote by Open-POM .We train Open-POM for 150 epochs, using 30 different train-test splits, and we extract representationsfrom the penultimate layer of this model. The odorant representation for each dataset is a matrixof Xi Rn256 where n is the number of unique odorants. For MC-odorants, we average therepresentations extracted for each individual molecule within the mixture. Distance Angle Model (DAM). Snitz et al. proposed a distance angle model (DAM) that uses 21physicochemical descriptors to predict the similarities between pairs of odorants. We extract these 21descriptors for each odorant using AlvaDesc and discard 6 of them due to NaN values producedby this software. We use the remaining subset of 15 physicochemical descriptors out of 21 to measuresimilarity between odorants or train a linear mapping from them to the perceptual representationspace. The odorant representation for each dataset is a matrix of Xi Rn15 where n is the numberof unique odorants. As suggested by Ravia et al. , we average the representations extracted foreach individual molecule within the mixture to compute representations for MC-odorants.",
  "Perceptual representations of odorants are provided by human participants when exposed to odorantstimuli. Perceptual olfactory data were collected in one of the following ways:": "1. Experts label the odorants, where each odorant may be linked to multiple labels (e.g., ).The perceptual representation is a matrix of yi {0, 1}nd where n is the number of uniqueodorants and d is the number of classes. 2. Non-expert participants provided ratings with regards to a set of predefined descriptors (e.g.,.) In this case, the averaged perceptual representations over participants and replicasform a matrix of yi [a, b]nd, where n is the number of unique odorants, d is the numberof descriptors, and a and b denote the minimum and maximum values participants can use todescribe the odorants with respect to these descriptors. 3. Participants evaluated the perceived similarity between pairs of odorants (e.g., ). Inthis case, the averaged perceptual representations over participants and replicas are a vector ofyi [a, b]n1 where n is the number of unique \"pair of odorants\" and a and b indicate the rangeof values participants can use to rate the odorants similarity with respect to the descriptors.",
  "(c) DAM": ": ROC curve for linear classifiers trained on GS-LF representations extracted from threedifferent models. Each curve corresponds to a separate test split, with the thicker curve representingthe average performance across all splits. We highlight that MoLFormer outperforms DAM, despitenot being trained to predict perceptual labels but does not achieve the performance level of Open-POM, which demonstrates the highest performance. The chance level is shown with red dashed line.",
  "In this section, we introduce the main evaluation metrics to measure the alignment in this paper": "Micro-averaged ROC-AUC score. The micro-averaged ROC-AUC score was computed to assessthe performance of each model for the multi-label classification task. The micro-averaged ROC-AUCscore is computed by aggregating true positive, false positive, true negative, and false negative valuesacross all classes. Normalized Root Mean Squared Error (NRMSE). The root mean squared error (RMSE) is thedifference between the observed values and predicted ones for the regression task. Here, we normalizeit by the range of true observations i.e., NRMSE = RMSE/(max(y) min(y)). Pearson Correlation Coefficient (CC). We report the Pearson correlation coefficient betweenpredicted results and real values. It measures the linear correlation between two sets of data and isthe ratio between the covariance of two variables and the product of their standard deviations.",
  "Results": "In this section, we evaluate whether the representations encoded by pre-trained models of chemicaldata can predict the human olfactory experience despite not being explicitly trained for this purpose.First, we focus on a subset of experiments aimed at predicting expert-assigned labels from odorantsthrough linear mapping from representations to perceptual descriptors (.1). Subsequently,we aim to predict continuous scores provided by human participants (.2). Finally, we seekto predict the direct similarity scores from the representations extracted from odorants (.3).Additionally, we provide insights into the potential reasons underlying the observed alignments(.4).",
  "Expert-assigned labels classification": "To assess the performance of MoLFormer in predicting expert-assigned labels for odorants, weimplemented a linear mapping from the representations extracted by MoLFormer to the odorant rep-resentations extracted from GS-LF dataset. First, the dimensionality of the extracted representationsis reduced to 20 using PCA, followed by z-scoring of each feature. Then, we train individual logisticregression models for each descriptor. This process was repeated 30 times, each with a differenttrain-test split, to quantify the uncertainty of the results. Principal Component 1 Principal Component 2 floralmuguetlavenderjasmin meaty savorybeefyroasted ethereal cognacfermentedalcoholic ethereal cognacfermentedalcoholic",
  "(b) Open-POM": ": Visualization of odorant representations encoded by different models on the GS-LFdataset using the figure layout suggested by . We plot the first and second principal components(PCs) of the representation spaces. Areas dense with molecules that have broad category labels(floral, meaty, or ethereal) are shaded, while areas dense with narrow category labels are outlined.MoLFormer captures the perceptual relationship between different odorants in its representationspace, despite not being explicitly trained for this purpose. We apply the same procedure without dimensionality reduction to DAM model representations. Forthe Open-POM model, which is already trained end-to-end and supervised on the same dataset, wedirectly extracted the predictions for the test set without retraining the model. As shown in the MoLFormer model achieves high ROC-AUC scores in odorant classification, outperforming theDAM model, which is trained using 15 physicochemical descriptors. However, the performance ofMoLFormer is lower than that of Open-POM, which is trained end-to-end with supervision on thesame dataset. An additional experiment is conducted to understand the degree of perceptual details captured in theodorant representation space of MoLFormer by comparing odorant representations encoded by thismodel with the representations encoded by Open-POM. In we depict the first two principalcomponents of the representations. We highlight the similarity between the representations encodedby both Open-POM and MoLFormer and observe that the latter is able to capture the perceptualrelationship between different odorants despite not using any perceptual labels during training (unlikethe supervised Open-POM Model).",
  "Continues perceptual rating prediction": "To evaluate the capabilities of the MoLFormer model to predict continuous rating scores with respectto pre-defined descriptors, provided by human participants, we train separate linear regressionmodels with regularization applied using the Lasso penalty for each descriptor. Once again, thedimensionality of the extracted representations is reduced to 20 using PCA (for MoLFormer andOpen-POM), followed by z-scoring of each feature. This procedure is repeated using 30 differenttrain-test splits. The results of these experiments are shown in and . shows the averagePearson correlation coefficient and NRMSE across all descriptors, while presents the resultsfor each individual descriptor. As shown in , overall, none of the models exhibit a highcorrelation. Nevertheless, MoLFormer slightly underperforms compared to Open-POM in bothdatasets. However, it performs better than DAM for the Keller dataset but worse than DAM for theSagar dataset, where DAM even outperforms Open-POM. : Performance of the models to predict continuous ratings averaged across all perceptualdescriptors. We compute the average Pearson correlation coefficient (CC) and normalized root meansquared error (NRMSE) across all descriptors. MoLFormer shows slightly worse performance thanOpen-POM but better than DAM for the Keller dataset and worse than DAM for the Sagar dataset,where DAM outperforms Open-POM.",
  "(b) Sagar": ": Performance of the models to predict continuous ratings per descriptor. We computedCorrelation and NRMSE between predicted and actual ratings per perceptual descriptor. Despitenot being trained to predict human olfactory labels, the MoLFormer model performs on par with theOpen-POM and DAM models. According to MoLFormer model performs on par with the Open-POM and DAM models,which are trained with supervision in predicting the rating for each descriptor. In summary, although,on average, MoLFormer performs slightly worse than Open-POM, it still demonstrates a similardegree of alignment, especially despite the absence of supervision in its training process.",
  "Representational similarity analysis": "In order to evaluate the direct alignment between the odorant similarities encoded by MoLFormerand those obtained from human participants, we separately encode each odorant by MoLFormer(and the baseline models) and compute the cosine similarity between the extracted representations.Subsequently, we compute the Pearson correlation between the similarity scores computed by themodels and those provided by human participants in the Ravia and Snitz datasets. The results arepresented in a. These results show that the MoLFormer is able to extract representations that encode informationrelated to the human olfactory perception, despite not having access to that information during modeltraining. We highlight a significant high correlation between perceptual and odorant representationfor the Snitz (r = 0.64, p < 0.0001) and Ravia datasets (r = 0.66, p < 0.0001). Ravia 2020Snitz 2013",
  "(b)": ": Representational similarity analysis for Snitz and Ravia datasets: a) Correlation coeffi-cients between similarity scores provided by human participants and computed using representationsencoded by the different models ; b) Correlation coefficients considering odorant representationsextracted from different layers of the MoLFormer model. The comparison with the baseline models indicates that it performs on par with the Open-POMmodel and significantly outperforms the DAM model. These results suggest that, despite beingtrained with some form of supervision, these models may struggle to effectively extract similaritiesbetween odorants. Additionally, the findings demonstrate that MoLFormer is more proficient atidentifying similarities between pairs of odorants than mapping them to a set of predefined descriptors.This superior performance may be due to the models ability to capture a measure of similarity, asperceived by humans, rather than introducing subjective language bias associated with pre-defineddescriptors. Finally, we aim to evaluate whether the depth of the layer in the MoLFormer model, from whichwe extract the odorant representations, affects the representational alignment. To assess this, werepeat the described procedure in this section for each layer separately. As shown in b,representational alignment improves with increasing layer depth, indicating that deeper layers of thetransformer are more aligned with high-level perceptual representations.",
  "Decoding relevant physicochemical features from pre-trained representations": "To evaluate whether MoLformer effectively extracts features from chemical structures relevant toolfactory perception, we evaluate the alignment of MoLFormer with physicochemical descriptorsthat are used in the DAM model. To do so, we train 15 linear regression models, each one to predicta single physicochemical descriptor from the extracted representations of the MoLFormer. Wesubsequently evaluate the correlation between the predicted and true values. As shown in ,MoLformer demonstrates a high degree of alignment in predicting these values. Out of the 15physiochemical descriptors, MoLformer successfully predicts the values for 13 descriptors as well asor better than the Open-POM model. Next, we evaluate whether this alignment changes across the layers of MoLformer. Therefore, werepeat the same procedure for each layer separately. As illustrated in , the alignment withthe identified chemical features decreases with increasing layer depth. However, as demonstrated inb, the alignment with perception improves. These results collectively are consistent withwell-known principles in vision models, where the lower layers typically capture low-level, localizedfeatures like edges and textures, while deeper layers gradually shift toward higher-level, abstractrepresentations, such as shapes and objects. Nonetheless, additional investigation is required tofully reveal and comprehend this potential hierarchical structure. nCIR ZM1 GNar S1K piPC08 MATS1v MATS7v GATS1v Eig05_AEA(bo) SM02_AEA(bo) SM03_AEA(dm) SM10_AEA(dm) SM13_AEA(dm) SpMin3_Bh(v) nRCOSR 0.2 0.4 0.6 0.8",
  "NRMSE": "MoLFormerOpen-POM : Performance of the models to predict relevant physicochemical descriptors. We com-puted Correlation and NRMSE between the predicted and actual values of descriptors. MoLFormeris able to predict 13 out of 15 physicochemical descriptors related to smell as well as or better thanthe Open-POM model, demonstrating high alignment with physicochemical descriptors. 0.2 0.6 1nCIRZM1GNarS1KpiPC08 0.2 0.6 1MATS1vMATS7vGATS1vEig05_AEA(bo)SM02_AEA(bo) 2 4 6 8 1012 0.2 0.6 1 SM03_AEA(dm) 2 4 6 8 1012 SM10_AEA(dm) 2 4 6 8 1012 SM13_AEA(dm) 2 4 6 8 1012 SpMin3_Bh(v) 2 4 6 8 1012 nRCOSR",
  "Discussion": "In this study, we investigated the alignment between odorant representations encoded by the MoL-Former, a self-supervised transformer model pre-trained on chemical structures, and human olfactoryperception. We evaluated the alignment between these representations by analyzing the similaritybetween them or finding a linear mapping between the representations. Additionally, we offeredinsights into the potential reasons behind the observed alignments by exploring relevant chemicalfeatures extracted by the model. Perceptual prediction from pretrained models. We demonstrate for the first time that representa-tions extracted from pre-trained large models, solely trained on chemical structures, align closelywith the perceptual representations of odorants. This finding suggests that odorant perception can beaccurately predicted from chemical structures. Furthermore, we show that this model can predict asubset of physicochemical descriptors known to be relevant to olfactory perception. Together, theseresults offer valuable predictions for chemists and neuroscientists to explore in future research. Evaluating alignment across multiple datasets. To evaluate alignment from various perspectives,we designed three different experiments. First, we leveraged a dataset with expert-provided labels forodorants, assessing the models ability to independently predict multi-target binary labels for eachodorant. This task did not involve variability from human participants or continuous odorant ratings.MoLFormer exhibited relatively high performance in predicting these binarized labels. Second,we used datasets containing average continuous ratings from human participants, which inherentlypresent more challenges due to variability among non-expert participants ratings. Our evaluationrevealed that while all models performed poorly on this task, MoLFormer performed comparably tosupervised models. Lastly, we evaluated direct similarity scores between odorants from two datasets,examining the alignment between human-provided similarity scores and those computed from therepresentations encoded by models. MoLFormer showed a high alignment, highlighting its ability topredict similarity between odorants rather than relying on human-made descriptors. This suggests thatpre-defined descriptors for describing odorants may need to be more carefully chosen, and modelstrained with these descriptors might not accurately reflect the true similarity between odorants. Reduction in alignment with physicochemical descriptors across layers of the models. Weconducted a complementary analysis to identify potential reasons underlying the observed perceptualalignment. Our focus was on the subset of features previously identified as significant for decodingolfactory perception from chemical structures. Our findings indicate that MoLFormer representationsexhibit a high degree of alignment with these features. While most features show strong alignment,a few demonstrate less alignment (such as nRCOSR). These results collectively suggest that whilethese features are important, their significance varies. Additionally, our analysis of the predictabilityof these features across the different layers of the model shows that as we go through the layers, weobserve a decrease in alignment with physicochemical descriptors despite an increase in alignmentwith perception. This observation aligns well with established principles in vision models, wherelower layers have been shown to capture low-level, local features such as edges and textures, whileprogressively transitioning to align with higher-level, abstract representations, such as shapes andobjects, in deeper layers . However, further exploration is needed to fully uncover and understandthis potential hierarchy. Limitations. Our work is perhaps best understood in the context of its limitations. We do not directlytake into consideration the intensity or concentration of each individual molecule within a mixtureduring the encoding of odorants. Incorporating these intensity factors in future work could potentiallyimprove the alignment. Additionally, our research was constrained by the available datasets, whichtypically lack sufficient variations in different odorants, particularly for continuous rating regressiontasks. Furthermore, we only considered the average rating scores and did not evaluate the alignmenton a per-subject basis. Future Work. We aim to leverage these findings to develop improved models of olfactory perception.Specifically, we plan to utilize unsupervised models trained exclusively on chemical structuresto identify which chemical features are crucial for predicting perception, thereby avoiding theintroduction of biases from human subjective perception. Additionally, we intend to investigatethe mechanisms underlying olfactory perceptions decoded from chemical features. The observedalignment trends across different layers of the model may provide key insights into this process.Finally, evaluating representational alignment between the extracted representations from transformerstrained on chemical structures and fMRI data from the brain can provide deeper insights into theunderlying mechanisms of olfactory perception.",
  "Dota Tianai Dong and Mariya Toneva. Interpreting multimodal video transformers using brainrecordings. In ICLR 2023 Workshop on Multimodal Representation Learning: Perks and Pitfalls,2023": "Charles F Cadieu, Ha Hong, Daniel LK Yamins, Nicolas Pinto, Diego Ardila, Ethan A Solomon,Najib J Majaj, and James J DiCarlo. Deep neural networks rival the representation of primate itcortex for core visual object recognition. PLoS computational biology, 10(12):e1003963, 2014. Martin Schrimpf, Jonas Kubilius, Ha Hong, Najib J Majaj, Rishi Rajalingham, Elias B Issa,Kohitij Kar, Pouya Bashivan, Jonathan Prescott-Roy, Franziska Geiger, et al. Brain-score:Which artificial neural network for object recognition is most brain-like? BioRxiv, page 407007,2018.",
  "Daniel LK Yamins and James J DiCarlo. Using goal-driven deep learning models to understandsensory cortex. Nature neuroscience, 19(3):356365, 2016": "Mariya Toneva and Leila Wehbe. Interpreting and improving natural-language processing (inmachines) with natural language-processing (in the brain). Advances in neural informationprocessing systems, 32, 2019. Juliette Millet, Charlotte Caucheteux, Yves Boubenec, Alexandre Gramfort, Ewan Dunbar,Christophe Pallier, Jean-Remi King, et al. Toward a realistic model of speech processing inthe brain with self-supervised learning. Advances in Neural Information Processing Systems,35:3342833443, 2022.",
  "Mario Pannunzi and Thomas Nowotny. Odor stimuli: not just chemical identity. Frontiers inphysiology, 10:1428, 2019": "Brian K Lee, Emily J Mayhew, Benjamin Sanchez-Lengeling, Jennifer N Wei, Wesley W Qian,Kelsie Little, Matthew Andres, Britney B Nguyen, Theresa Moloy, Jane K Parker, et al. Aprincipal odor map unifies diverse tasks in human olfactory perception. BioRxiv, pages 202209,2022. Aharon Ravia, Kobi Snitz, Danielle Honigstein, Maya Finkel, Rotem Zirler, Ofer Perl, LaviSecundo, Christophe Laudamiel, David Harel, and Noam Sobel. A measure of smell enablesthe creation of olfactory metamers. Nature, 588(7836):118123, 2020.",
  "A Vaswani. Attention is all you need. Advances in Neural Information Processing Systems,2017": "Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney vonArx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On theopportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprintarXiv:2010.11929, 2020. Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. Videomae: Masked autoencoders aredata-efficient learners for self-supervised video pre-training. Advances in neural informationprocessing systems, 35:1007810093, 2022. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models arefew-shot learners. Advances in neural information processing systems, 33:18771901, 2020. Jerret Ross, Brian Belgodere, Vijil Chenthamarakshan, Inkit Padhi, Youssef Mroueh, and PayelDas. Large-scale chemical language representations capture molecular structure and properties.Nature Machine Intelligence, 4(12):12561264, 2022. Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neuralmessage passing for quantum chemistry. In International conference on machine learning,pages 12631272. PMLR, 2017.",
  "ANoise Ceiling": "In order to evaluate the quality of data and the upper limit of the models, we computed the noiseceiling (Equateion 1, 2) for the Sagar and Keller datasets as these are the only ones that havemultiple evaluators for each odorant, and those are publicly available. The results show that we havethe average noise ceilings of 0.28 0.1 for the Keller dataset and 0.7 0.05 for the Sagar dataset(Table S.1, S.2). The results show that the data of the Sagar dataset is less noisy, and there is stillroom for the models to increase the alignment. However, the Keller dataset alignment results arerelatively close to the noise ceiling value.",
  "BRepresentational Similarity Matrix (RSM)": "In order to better visualize how the models and humans represent different odors, we visualizedrepresentational similarity matrices for humans participants, Open-Pom, and MoLFormer across pairsof odorants for Ravia dataset in Figure S.1. The white cells show the pair of odorants for which nosimilarity score is available. MC-odorants corresponding to each mixture are provided in Table S.3",
  "DFine-tuned MolFormer": "We fine-tuned MoLFormer using GS-LF dataset which is a large and inclusive dataset of odorants.Then we extracted representations for all the datasets and tasks. Figure S.3 shows ROC-AUC curvefor GS-LF dataset. Figure S.4 demonstrates the results for the continuous rating prediction tasks forKeller and Sagar datasets, and FigureS.5 shows the results for RSA for Ravia and Snitz datasets. 0 1 2 3 4 5 6 7 8 9 101112131415161718192021222324252627 0 1 2 3 4 5 6 7 8 9 101112131415161718192021222324252627",
  "(a) MoLFormer(b) Open-POM": "Figure S.2: t-SNE Visualization of odorant representations encoded by different models on theGS-LF dataset using the figure layout suggested by . We reduced dimensionality using t-SNE.Areas dense with molecules that have broad category labels (floral, meaty, or ethereal) are shaded,while areas dense with narrow category labels are outlined. MoLFormer captures the perceptualrelationship between different odorants in its representation space, despite not being explicitly trainedfor this purpose.",
  "The answer NA means that the paper has no limitation while the answer No means thatthe paper has limitations, but those are not discussed in the paper": "The authors are encouraged to create a separate \"Limitations\" section in their paper. The paper should point out any strong assumptions and how robust the results are toviolations of these assumptions (e.g., independence assumptions, noiseless settings,model well-specification, asymptotic approximations only holding locally). The authorsshould reflect on how these assumptions might be violated in practice and what theimplications would be. The authors should reflect on the scope of the claims made, e.g., if the approach wasonly tested on a few datasets or with a few runs. In general, empirical results oftendepend on implicit assumptions, which should be articulated. The authors should reflect on the factors that influence the performance of the approach.For example, a facial recognition algorithm may perform poorly when image resolutionis low or images are taken in low lighting. Or a speech-to-text system might not beused reliably to provide closed captions for online lectures because it fails to handletechnical jargon.",
  "If applicable, the authors should discuss possible limitations of their approach toaddress problems of privacy and fairness": "While the authors might fear that complete honesty about limitations might be used byreviewers as grounds for rejection, a worse outcome might be that reviewers discoverlimitations that arent acknowledged in the paper. The authors should use their bestjudgment and recognize that individual actions in favor of transparency play an impor-tant role in developing norms that preserve the integrity of the community. Reviewerswill be specifically instructed to not penalize honesty concerning limitations.",
  "If the contribution is a dataset and/or model, the authors should describe the steps takento make their results reproducible or verifiable": "Depending on the contribution, reproducibility can be accomplished in various ways.For example, if the contribution is a novel architecture, describing the architecture fullymight suffice, or if the contribution is a specific model and empirical evaluation, it maybe necessary to either make it possible for others to replicate the model with the samedataset, or provide access to the model. In general. releasing code and data is oftenone good way to accomplish this, but reproducibility can also be provided via detailedinstructions for how to replicate the results, access to a hosted model (e.g., in the caseof a large language model), releasing of a model checkpoint, or other means that areappropriate to the research performed. While NeurIPS does not require releasing code, the conference does require all submis-sions to provide some reasonable avenue for reproducibility, which may depend on thenature of the contribution. For example(a) If the contribution is primarily a new algorithm, the paper should make it clear howto reproduce that algorithm.",
  "(b) If the contribution is primarily a new model architecture, the paper should describethe architecture clearly and fully": "(c) If the contribution is a new model (e.g., a large language model), then there shouldeither be a way to access this model for reproducing the results or a way to reproducethe model (e.g., with an open-source dataset or instructions for how to constructthe dataset). (d) We recognize that reproducibility may be tricky in some cases, in which caseauthors are welcome to describe the particular way they provide for reproducibility.In the case of closed-source models, it may be that access to the model is limited insome way (e.g., to registered users), but it should be possible for other researchersto have some path to reproducing or verifying the results.",
  "Guidelines:": "The answer NA means that the paper does not use existing assets. The authors should cite the original paper that produced the code package or dataset. The authors should state which version of the asset is used and, if possible, include aURL. The name of the license (e.g., CC-BY 4.0) should be included for each asset. For scraped data from a particular source (e.g., website), the copyright and terms ofservice of that source should be provided. If assets are released, the license, copyright information, and terms of use in thepackage should be provided. For popular datasets, paperswithcode.com/datasetshas curated licenses for some datasets. Their licensing guide can help determine thelicense of a dataset.",
  ". Experiments Compute Resources": "Question: For each experiment, does the paper provide sufficient information on the com-puter resources (type of compute workers, memory, time of execution) needed to reproducethe experiments?Answer: [Yes]Justification: We describe sufficient information on the computer resources needed toproduce the experiments in the supplementary material section.Guidelines: The answer NA means that the paper does not include experiments. The paper should indicate the type of compute workers CPU or GPU, internal cluster,or cloud provider, including relevant memory and storage.",
  ". Code Of Ethics": "Question: Does the research conducted in the paper conform, in every respect, with theNeurIPS Code of Ethics [Yes]Justification: The research conducted in the paper conform, in every respect, with theNeurIPS Code of Ethics.Guidelines: The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. If the authors answer No, they should explain the special circumstances that require adeviation from the Code of Ethics.",
  "If the authors answer NA or No, they should explain why their work has no societalimpact or why the paper does not address societal impact": "Examples of negative societal impacts include potential malicious or unintended uses(e.g., disinformation, generating fake profiles, surveillance), fairness considerations(e.g., deployment of technologies that could make decisions that unfairly impact specificgroups), privacy considerations, and security considerations. The conference expects that many papers will be foundational research and not tiedto particular applications, let alone deployments. However, if there is a direct path toany negative applications, the authors should point it out. For example, it is legitimateto point out that an improvement in the quality of generative models could be used togenerate deepfakes for disinformation. On the other hand, it is not needed to point outthat a generic algorithm for optimizing neural networks could enable people to trainmodels that generate Deepfakes faster. The authors should consider possible harms that could arise when the technology isbeing used as intended and functioning correctly, harms that could arise when thetechnology is being used as intended but gives incorrect results, and harms followingfrom (intentional or unintentional) misuse of the technology. If there are negative societal impacts, the authors could also discuss possible mitigationstrategies (e.g., gated release of models, providing defenses in addition to attacks,mechanisms for monitoring misuse, mechanisms to monitor how a system learns fromfeedback over time, improving the efficiency and accessibility of ML).",
  "The answer NA means that the paper does not involve crowdsourcing nor research withhuman subjects": "Depending on the country in which research is conducted, IRB approval (or equivalent)may be required for any human subjects research. If you obtained IRB approval, youshould clearly state this in the paper. We recognize that the procedures for this may vary significantly between institutionsand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and theguidelines for their institution."
}