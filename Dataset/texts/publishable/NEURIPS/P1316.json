{
  "Abstract": "Deep generative models have achieved promising results in image generation, andvarious generative model hubs, e.g., Hugging Face and Civitai, have been developedthat enable model developers to upload models and users to download models.However, these model hubs lack advanced model management and identificationmechanisms, resulting in users only searching for models through text matching,download sorting, etc., making it difficult to efficiently find the model that bestmeets user requirements. In this paper, we propose a novel setting called GenerativeModel Identification (GMI), which aims to enable the user to identify the mostappropriate generative model(s) for the users requirements from a large number ofcandidate models efficiently. To our best knowledge, it has not been studied yet.In this paper, we introduce a comprehensive solution consisting of three pivotalmodules: a weighted Reduced Kernel Mean Embedding (RKME) framework forcapturing the generated image distribution and the relationship between images andprompts, a pre-trained vision-language model aimed at addressing dimensionalitychallenges, and an image interrogator designed to tackle cross-modality issues.Extensive empirical results demonstrate the proposal is both efficient and effective.For example, users only need to submit a single example image to describe theirrequirements, and the model platform can achieve an average top-4 identificationaccuracy of more than 80%.",
  "Introduction": "Recently, stable diffusion models have achieved state-of-the-art performance in imagegeneration and become one of the popular topics in artificial intelligence. Various model hubs, e.g.,Hugging Face and Civitai, have been developed to enable model developers to upload and sharetheir generative models. However, existing model hubs provide trivial methods such as tag filtering,text matching, and download volume ranking , to help users search for models. However, thesemethods cannot accurately capture the users requirements, making it difficult to efficiently identifythe most appropriate model for users. As shown in , the user should submit their requirementsto the model hub and subsequently, they must download and evaluate the searched model one by oneuntil they find the satisfactory one, causing significant time and computing resources. The above limitation of existing generative model hubs inspires us to consider the following question:Can we describe the functionalities and utilities of different generative models more precisely in some",
  ": Performance evaluated by av-erage accuracy and rank metrics": "format that enables the model can be efficiently and accurately identified in the future by matchingthe models functionalities with users requirements? We call this novel setting Generative ModelIdentification (GMI). To the best of our knowledge, this problem has not been studied yet. It is evident that two problems need to be addressed to achieve GMI, the first is how to describe thefunctionalities of different generative models, and the second is how to match the user requirementswith the models functionalities. Inspired by the learnware paradigm , which proposes to assign aspecification to each model that reflects the models utilities, we enhance the Reduced Kernel MeanEmbedding (RKME) to tackle the intractability of modeling generative tasks instead ofclassification tasks. To this end, we propose a novel systematic solution consisting of three pivotalmodules: a weighted RKME framework for capturing not only the generated image distribution butalso the relationship between images and prompts, a pre-trained vision-language model aimed ataddressing dimensionality challenges, and an image interrogator designed to tackle cross-modalityissues. For the second problem, we assume the user can present one image as an example todescribe the requirements, and then we can match the model specification with the example image tocompute how well each candidate generative model matches users requirements. provides acomparison between previous model search methods and the new solution. The goal is to identify themost suitable generative model with only one image as an example to describe the users requirements. To evaluate the effectiveness of our proposal, we construct a benchmark platform consisting of 16tasks specifically designed for GMI using stable diffusion models. The experiment results showthat our proposal is both efficient and effective. For example, users only need to submit one singleexample image to describe their requirements, and the model platform can achieve an average top-4identification accuracy of more than 80%, indicating that recommending four models can satisfyusers needs in major cases on the benchmark dataset.",
  "Problem Setup and Notions": "In this paper, we explore a novel problem setting called GMI, where users identify the most appro-priate generative models for their specific purposes using one image. We assume there is a modelplatform, consisting of M generative models {fm}Mm=1. Each model is associated with a specificationSm to describe its functionalities for future model identification. The platform consists of two stages:the submitting stage for model developers and the identification stage for users, respectively. In the submitting stage, the model developer submits a generative model fm to the platform. Then,the platform assigns a specification Sm to this model. Here, the specification Sm = As (fm, P)is generated by a specification algorithm As using the model fm and a prompt set P = {pk}Nk=1.If the model developer can provide a specific prompt set for the uploaded model, the generatedspecification would be more precise in describing its functionalities. In the identification stage, theusers identify models from the platform using only one image x. When users upload an imagex to describe their purposes, the platform automatically calculates the pseudo-prompt p and thengenerates requirements R = Ar(x, p) using a requirement algorithm Ar. Users can optionallyprovide corresponding prompt p, setting pr = p, to more precisely describe their purposes. During the identification process, the platform matches requirement R with model specifications {Sm}Mm=1using a evaluation algorithm Ae and compute similarity score s,m = Ae(Sm, R) for each modelfm. Finally, the platform returns the best-matched model with the maximum similarity score or a listof models sorted by {s,m}Mm=1 in descending order. There are two main challenges for addressing GMI setting: 1) In the submitting stage, how to designAs to fully characterize the generative models for identification? 2) In the identification stage, how todesign Ar and Ae to effectively identify the most appropriate generative models for user needs?",
  "Proposed Method": "In this section, we present our solution for the GMI setting. Due to space limitations, we explain theRKME framework and its failure in GMI in the Appendix B. Our solution adopts a novel weightedterm to capture the relationship between images and prompts in RKME, thereby enabling the modelto be more precise in describing the functionalities of generative models. However, there are twoissues remain: 1) High dimensionality of images brings intractability of efficiency and similaritymeasurement; 2) Cross-modality issue causes difficulties in calculating weight. To address thesechallenges, we employ a large pre-trained vision model G() to map images from image space toa common feature space. Subsequently, an image interrogator I() is adopted to convert x tocorresponding pseudo prompt p, thereby mitigating the cross-modality issues. Consequently, thesimilarity in the common feature space can be computed with the help of a large pre-trained languagemodel T (). We provide a detailed description of our proposal as follows. Submitting StageThe algorithm As first samples images from the generative model fm using theprompt set: Xm = {fm(p)|p P}. The developer can optionally replace P with a specific promptset to generate a more precise specification. Then, the large pre-trained vision model G() is adoptedto encode Xm as follows. The obtained feature representation Zm is efficient and robust to computethe similarity between images, i.e., Zm = {G(x)|x Xm}. Subsequently, As encodes prompt set Pto the common feature representation using T (): Qm = {T (p)|p P}. Finally, the specificationSm of generative model fm is defined as follows: Sm = As(fm; Pm) = {Zm; Qm}. Note that Smis automatically computed inside the platform, which is very convenient for developers to use anddeduce their burden of uploading models. Additionally, the specification does not occupy a largeamount of storage space on the platform since the only feature representation is storage. Identification StageThe users upload one single image x to describe their requirements and theplatform describes the requirements with R from x. Specifically, the requirement algorithm Arfirst generates feature representations of x using G(), i.e., z = G(x). Subsequently, the pseudo-prompt p is generated by I(), i.e., p = I(x), and converted to feature representations using T (),i.e., q = T (p). The user can optionally replace p with a prompt p built on his understandingto precisely describe the requirement. Finally, the requirement is:R = Ar(x) = {z; q}. Notethat R is automatically computed inside the platform, which is very easy to use for users. Afterthe platform generates the requirement R, it will calculates the similarity score for each model fmusing evaluation algorithm Ae:",
  "Hk(1)": "where the weighted term is defined as the cosine similarity between platform prompts qm,i Qm andpseudo-prompt q. Wm encodes the structure information of x within Pm during the identification,which successfully captures the relation between images and prompts. The platform returns a list ofmodels sorted in increasing order of similarity score obtained by Equation 1.",
  "Discussion": "It is evident that our proposal for the GMI scenario achieves a higher level of accuracy and efficiencywhen compared to model search techniques employed by existing model hubs. For accuracy, ourproposal elucidates the functionalities of generated models by capturing both the distribution ofgenerated images and prompts, which allows for more accurate identification compared to the",
  "Proposal0.4550.6140.7340.812": "traditional model search method that relies on download ranks. For efficiency, our proposal achievesO(Tr +MTs) time for one identification, where generating requirement costs Tr time and calculatingsimilarity score costs Ts time. Moreover, with accurate identification results, users can save the effortsof browsing and selecting models, as well as reducing the consumption of network and computing.Additionally, our approach also has the potential to achieve further acceleration through the use of avector database such as Faiss .",
  "In this section, we briefly introduce the experiment settings and main results. Detailed informationabout experiments is additionally provided in Appendix C": "SettingsWe conduct experiments on a benchmark dataset described in subsection C.1. Our proposalis compared with three baseline methods: 1) Download: The model is ranked based on the downloadvolume , representing methods that ignore model capabilities. 2) RKME-Basic: The model isidentified using the basic RKME paradigm . 3) RKME-CLIP: The model is identified based onthe combination of the RKME paradigm and CLIP model . Two metrics, i.e., accuracy and rank,are adopted for evaluation. Accuracy evaluates the ability of methods to identify the most appropriatemodel, the higher the better. Rank evaluates the users efforts in identifying the most appropriatemodels, the lower the better. Additionally, Top-k accuracy is reported to indicate how many attemptscan users find their satisfied models in major cases. Empirical ResultsAs shown in , our proposal achieves the best performance in bothaverage accuracy and average rank, which demonstrates the effectiveness of our proposal. TheDownload and RKME-Basic methods cannot work in our setting because they do not consider thechallenges of GMI. The performance of the RKME-CLIP method improves significantly, indicatingthat the CLIP model can address the high dimensionality issue. Our proposal captures the relationbetween images and prompts, thereby giving the best performance. presents the resultsof Top-k accuracy. These results show that our proposal achieves 80% top-4 accuracy on thebenchmark dataset, indicating that user only requires four attempts to satisfy their needs in majorcases using our proposal to identify generative models. Finally, we show the visualization in .The requirements are shown in the first column, and the generated images of each method usingpseudo-prompts are shown in the remaining columns. Our proposal gives the most similar images.",
  "Conclusion": "In this paper, for the first time, we propose a novel problem called Generative Model Identification.The objective of GMI is to describe the functionalities of generative models precisely and enable themodel to be accurately and efficiently identified in the future by users requirements. To this end,we present a systematic solution including a weighted RKME framework to capture the generatedimage distributions and the relationship between images and prompts, a large pre-trained vision-language model aimed at addressing dimensionality challenges, and an image interrogator designed totackle cross-modality issues. Moreover, we built and released a benchmark platform based on stablediffusion models for GMI. Extensive experiment results on the benchmark clearly demonstrate theeffectiveness of our proposal. For example, our proposal achieves more than 80% top-4 identification",
  "accuracy using just one example image to describe the users requirements, indicating that users canefficiently identify the best-matched model within four attempts in major cases": "In future work, we will endeavor to develop a novel generative model platform based on the tech-niques presented in this paper, aiming to provide a more precise description of generative modelfunctionalities and user requirements. This will assist users in efficiently discovering models that alignwith their specific requirements. We believe this could facilitate the development and widespreadusage of generative models.",
  "Martn Arjovsky, Soumith Chintala, and Lon Bottou. Wasserstein generative adversarial networks. InProceedings of the 34th International Conference on Machine Learning, pages 214223, 2017": "Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high fidelity naturalimage synthesis. In Proceedings of the 7th International Conference on Learning Representations, 2019. Yunjey Choi, Youngjung Uh, Jaejun Yoo, and Jung-Woo Ha. Stargan v2: Diverse image synthesis formultiple domains. In Proceedings of IEEE/CVF Conference on Computer Vision and Pattern Recognition,pages 81858194, 2020.",
  "Prafulla Dhariwal and Alexander Quinn Nichol. Diffusion models beat GANs on image synthesis. InAdvances in Neural Information Processing Systems, pages 87808794, 2021": "Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,Aaron C. Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural InformationProcessing Systems, pages 26722680, 2014. Lan-Zhe Guo, Zhi Zhou, Yu-Feng Li, and Zhi-Hua Zhou. Identifying useful learnwares for heterogeneouslabel spaces. In Proceedings of the 40th International Conference on Machine Learning, pages 1212212131, 2023.",
  "Peng Tan, Zhi-Hao Tan, Yuan Jiang, and Zhi-Hua Zhou. Towards enabling learnware to handle heteroge-neous feature spaces. Machine Learning, pages 122, 2022": "Peng Tan, Zhi-Hao Tan, Yuan Jiang, and Zhi-Hua Zhou. Handling learnwares developed from heteroge-neous feature spaces without auxiliary data. In Proceedings of the 32nd International Joint Conference onArtificial Intelligence, pages 42354243, 2023. Anh T Tran, Cuong V Nguyen, and Tal Hassner. Transferability and hardness of supervised classificationtasks. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 13951405,2019.",
  "ARelated Work": "Generative modeling is a field of machine learning that focuses on learning the underlyingdistribution and generation of new samples for corresponding distribution. Recently, significantprogress has been made in image generation with various methods. Generative Adversarial Networks(GANs) apply an adversarial approach to learn the data distribution. It consists of a generatorand a discriminator playing a min-max game during the training process. Variational Autoencoders(VAEs) is a variant of Auto-Encoder (AE) , where both consist of the encoderand decoder networks. The encoder in AE learns to map an image into a latent representation.Then, the decoder aims to reconstruct the image from that latent representation. Diffusion Models(DMs) leverages the concept of the diffusion process, consisting of forward and reversediffusion processes. Noise is added to an image during the forward process and the diffusion modellearns to denoise and reconstruct the image. With the development of the generative model, variousgenerative model hubs/pools, e.g., HuggingFace, Civitai, have been developed. However, they lackmodel management and identification mechanisms, resulting in inefficiency for users to find the mostsuitable model. Lu et al. adopts a contrastive learning method to explore the search for deepgenerative models in terms of their contents. Assessing the transferability of pre-trained models is related to the problem studied in this paper.Negative Conditional Entropy (NCE) proposed an information-theoretic quantity to studythe transferability and hardness between classification tasks. LEEP is primarily developedwith a focus on supervised pre-trained models transferred to classification tasks. You et al. designs a general algorithm, which is applicable to vast transfer learning settings with supervisedand unsupervised pre-trained models, downstream tasks, and modalities. However, these methodsare not suitable for our GMI problem because they impose significant computational overheadin terms of model inference during the identification process. Learnware presents a generaland realistic paradigm by assigning a specification to models to describe their functionalities andutilities, making it convenient for users to identify the most suitable models. Model specificationis the key to the learnware paradigm. Recent studies are designed on Reduced Kernel MeanEmbedding (RKME) , which aims to map the training data distributions to points in ReproducingKernel Hilbert Space (RKHS), and achieves model identification by comparing similarities in theRHKS. Subsequently, Guo et al. improves existing RKME specifications for heterogeneous labelspaces. Tan et al. make their efforts to solve heterogeneous feature spaces. However, thesestudies primarily focus on classification tasks, overlooking the relationship between images andprompts, which is crucial for identifying generative models. Therefore, existing techniques areinadequate for addressing the GMI problem, underscoring the pressing need for the development ofnew technologies specifically tailored to generative models.",
  "(2)": "where k(, ) is the reproducing kernels associated with RKHS Hk. This baseline method fails tocapture the interplay between generated images Xm and the prompt set P, which is the probabilitydistribution pm(x0:T |p) inside the generative model fm. We present an example to show thisinterplay is important otherwise the specification cannot distinguish two models in specific cases,resulting in unsatisfactory identification results.Example B.1. Suppose that there are two simplified generative models f1 and f2 on the platform.f1 generates scatter points following x = cos (p), y = sin (p). f2 generates scatter points following x = sin (p), y = cos (p). The prompt set p follows U(1, 1). The user wants todeploy the identified model conditioned on prompts p following distribution U(0.5, 0). In ,we show that the baseline method in Equation 2 fails to distinguish two models f1 and f2 for theuser. However, the two models function differently with p. a and b show that",
  ": Baseline method in Equation 2 fails to distinguish two different models for users": "although models f1 and f2 function differently, the data distribution X1 f1(p) and X2 f2(p),conditioned on the default prompt distribution p, could be identical. Therefore, the specificaionsSRKME1and SRKME2are identical, resulting in the same similarity scores ARKMEe(SRKME1, RRKME) andARKMEe(SRKME2, RRKME). However, c shows that two models f1 and f2 generate differentdata distributions f1(p) and f2(p) conditioned on the user prompt distribution p. Remark. Example B.1 shows us that overlooking the interplay between images and prompts leads toimpossible cases for distinguishing generative models effectively. Existing RKME studies mainlyfocus on classification tasks, which can implicitly model the tasks through data distribution since theclass space is discrete and small. For generative models, we have to explicitly model the modelsfunctionality, i.e., the relation between images and prompts, to achieve satisfied identification results.",
  "Hk(3)": "where Wm = {wm,i}Nmi=1 are required to measure the relation between user image x and promptset P. Here, we make the simplifications RWeighted= x and SWeightedm= Xm in Equation 3. Thisraises challenges inherent in dimensionality since stable diffusion models produce high-qualityimages. Moreover, measuring the relation using Wm is also a challenging problem and encounterscross-modality issues.",
  "C.1Model Platform and Task Construction": "In practice, we expect model developers to submit their models and corresponding prompts to themodel platform. And we expect users to identify models for their real needs. In our experiments, weconstructed a model platform and user identification tasks respectively to simulate the above situation.For the construction of the model platform, we manually collect M = 16 different stable diffusionmodels {f1, . . . , fM} from one popular model platform, CivitAI, as uploaded generative models onthe platform. Note that these collected models belong to the same category to simulate the real processin which users first trigger category filters and then select the models. We construct 55 prompts",
  "Proposal0.4550.6142.852": "{p1, . . . , p55} as default prompt set P of platform. For task construction, we construct 18 evaluationprompts {p1, . . . , p18} for each model on the platform to generate testing images with randomseed in {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}, forming N = 18 16 10 = 2880 different identification tasks{(xi, ti)}Ni=1, where each testing image xi is generated by model fti and its best matching modelindex is ti. Here, we ensure that there is no overlap between {p1, . . . , p55} and {p1, . . . , p18} toensure the correctness of the evaluation.",
  "C.2Comparison Methods": "Initially, we compare it with the traditional model search method called Download. This method isused to simulate how users search generative models according to their downloading volumes ,where users will try models with high downloading volume first. This baseline method can representa family of methods that employ statistical information without regard to model capabilities. We alsoconsider the basic implementation of the RKME specification as a baseline method RKME-Baiscfor our GMI problem. The details of generating specifications, and identifying models are presentedin subsection B.1. Furthermore, we compare our proposed method with a variant of the basic RKMEspecification, that is, RKME-CLIP, which calculates specifications in the feature representationspace encoded by the CLIP model . The results obtained from RKME-CLIP further support ourviewpoint on the critical challenges posed by dimensionality.",
  "C.3Implementation Details": "We adopt the official code in Wu et al. to implement the RKME-Basic method and theofficial code in Radford et al. to implement the CLIP model.For RKME-Basic andRKME-CLIP methods, we follow the default hyperparameter setting of RKME in previousstudies .We set the size of the reduced set to 1 and choose the RBF kernel forRKHS. The hyperparameter for calculating RBF kernel and similarity score is tuned from{0.005, 0.006, 0.007, 0.008, 0.009, 0.01, 0.02, 0.03, 0.04, 0.05} and set to 0.02 in our experiments.Experiment results below show that our proposal is robust to .",
  "C.4Ablation Study": "In order to comprehensively evaluate the effectiveness of our proposal, we investigate whether eachcomponent contributes to the final performance. We additionally compare our proposal with twovariants, called RKME-CLIP and RKME-Concat. RKME-CLIP adopts the CLIP model to extract thefeature representation for constructing RKME specifications. RKME-Concat adopts both vision andtext branches of the CLIP model to extract representations of images and prompts. It combines twomodes of representation for constructing RKME specifications. We report accuracy and rank metricsin . The performance of RKME-CLIP demonstrates that employing large pre-trained modelsis an effective approach for addressing dimensionality issues. The performance of RKME-Concatdemonstrates the benefits of considering both images and prompts for model identification. Ourresults achieve the best performance, and demonstrate the effectiveness of our weighted formulationin Equation 3 and our specifically designed algorithm in Equation 1. 0.0050.0060.0070.0080.0090.010.020.030.040.05 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45",
  "C.5Hyperparameter Robustness": "We evaluate the robustness of each method to the hyperparameter in . The resultsdemonstrate that our proposed method exhibits robust performance across a wide range of values.However, as continues to increase, the performance of both our proposal and the baseline methodsbegins to degrade. This observation highlights the importance of tuning the hyperparameter beforedeploying our method in practical applications. Once is properly tuned, our method can operaterobustly due to its hyperparameter robustness within a broad range."
}