{
  "Abstract": "There is strong agreement that generative AI should be regulated, but strong dis-agreement on how to approach regulation. While some argue that AI regulationshould mostly rely on extensions of existing laws, others argue that entirely newlaws and regulations are needed to ensure that generative AI benets society. Inthis paper, I argue that the debates on generative AI regulation can be informed bythe debates and evidence on social media regulation. For example, AI companieshave faced allegations of political bias regarding the images and text their modelsproduce, similar to the allegations social media companies have faced regardingcontent ranking on their platforms. First, I compare and contrast the affordancesof generative AI and social media to highlight their similarities and differences.Then, I discuss specic policy recommendations based on the evolution of socialmedia and their regulation. These recommendations include investments in: ef-forts to counter bias and perceptions thereof (e.g., via transparency, researcheraccess, oversight boards, democratic input, research studies), specic areas of reg-ulatory concern (e.g., youth wellbeing, election integrity) and trust and safety,computational social science research, and a more global perspective. Applyinglessons learnt from social media regulation to generative AI regulation can saveeffort and time, and prevent avoidable mistakes.",
  "Introduction": "When Googles generative AI model Gemini produced images of racially diverse Nazis in early2024, it led to a public outcry and allegations of anti-conservative bias (Robertson, 2024). Almosta decade earlier, the rst allegations of anti-conservative bias were made against social media plat-forms like Facebook (Barrett and Sims, 2021), and have continued to persist e.g. during Senatehearings (Romm, 2019) and when former President Trump was banned from Twitter (now X) andFacebook (Barrett and Sims, 2021). This shows that the content moderation challenges that emerg-ing technologies face are not entirely new. Media scholars have called attention to the fact that newtechnologies often elicit similar questions and concerns as their predecessors (Wartella and Reeves,1985). Generative AI is the latest technology to garner widespread attention and raise societal andregulatory concerns, but so have social media and other technologies before it. The aim of this paper is to show that what has been learnt with regards to social media regulationin the past two decades can inform the regulation of generative AI going forward. While there isstrong agreement that generative AI should be regulated, there is strong disagreement on how toapproach regulation. Some argue that AI regulation should mostly rely on extensions of existinglaws (Huttenlocher, Ozdaglar and Goldston, 2023), while others argue that entirely new laws andregulations are needed and have proposed laws and regulations such as the EU AI Act, the White",
  "2nd Workshop on Regulatable ML at NeurIPS 2024": "House Executive Order on AI, or Californias AI Safety Bill SB 1047. Analyzing the evolution ofsocial media regulation can provide insights into which approaches to regulation are promising whenit comes to generative AI, which in turn can save effort and time, and prevent avoidable mistakes. The focus of this paper is on content moderation, i.e. how to design and regulate the content thatis generated by a generative AI model or shown on a social media platform. Further, the paper fo-cuses on regulation in a broad sense, which can include self-regulation of industry players to ensureharmless output or avoiding bias, and formal laws such as the General Data Protection Regulationor the White House Executive Order on AI. The rst section compares and contrasts the affordancesof generative AI and social media to highlight their similarities and differences. The second sec-tion discusses specic policy recommendations based on the evolution of social media and theirregulation.",
  "Affordances of generative AI and social media": "To shed light on the similarities and differences between specic media, we can analyze their affor-dances. Affordances are the features that characterize a medium. Both generative AI, e.g. in theform of a chatbot like OpenAIs ChatGPT or Anthropics Claude, and social media, e.g. in the formof Metas Facebook or X (formerly Twitter), can be considered media that allow to create and dis-tribute content and are shaped by specic features. The features discussed here pertain to a mediumin general, but may not apply to every instance, that is, a specic generative AI application or socialmedia platform may differ from the norm in terms of its affordances. Based on an analysis of commonly used generative AI applications (e.g., ChatGPT and Claude) aswell as social media applications (e.g., Facebook and X), I identied key features that generativeAI and social media share or that differentiate them. The analysis of features is grounded in workby Clark (1996), who discusses several features of media that fall into three categories: medium,control, and immediacy. Since Clarks features were originally meant to capture affordances offace-to-face communication,1 I added new features and removed features that are less relevant tothe comparison of generative AI and social media. I will point out each feature that is adapted fromClark.",
  "Generative AI and social media are not perfectly comparable": "By denition, an analogy is not a perfect match, otherwise the objects of comparison would be thesame thing. As Jacob Stern puts it: [T]his is just the nature of analogies: They are illuminating butincomplete (Stern, 2023). reveals differences in affordances between generative AI and social media. With regards tofeatures of the medium, generative AI and social media show some variation. While generative AIsuch as ChatGPT constitutes a conversation partner for the user and interacts in a dialogue with theuser, social media are merely mediating between the user and their human conversation partner (e.g.,when a social media algorithm displays one users post on another users feed) and tend to involve asequence of one-off actions. Relatedly, while social media foster direct connections between users,generative AI is usually used by a single person at a time. While generative AI tends to respondto prompts, usually with a single output instead of multiple outputs, and does not continue to servecontent unless the user requests it, social media often feature innite scroll or auto-play that servecontent as long as the user is on the platform. The purpose of social media tends to be limited tosocial communication, while generative AI is considered a general purpose technology that couldserve various functions, including as a text writer or reviewer, a calculator, a programmer and muchmore.",
  "With regards to control features, a feature Clark (1996) proposed is simultaneity, which is the usersability to receive and produce content concurrently. Simultaneity is given for social media e.g.,": "1There are contextual differences between face-to-face communication on the one hand and generative AIand social media on the other, such as where and why they may be used. This paper focuses on the comparisonof generative AI and social media, and therefore focuses on features in Clarks model that are pertinent togenerative AI and social media, but not the comparison to other media.",
  "FeatureDenitionGenerative AISocial Media": "MediumSpatial separationContent is generated in different locationsYesYesDirect connectionMedium is conversation partnerYesNoUser connectionsMedium connects user to other usersNoYesDialogue-by-defaultActions occur in a dialogueYesNoRecordingUser actions are recordedYesYesPersonalizationUser context and preferences are learnt over timeYesYesSingle outputMedium presents usually just a single outputYesNoInnite contentContent is served innitelyNoYesGeneral contentContent can pertain to any domainYesYesGeneral purposeMedium serves many functionsYesNoUse of AIMedium learns patterns from dataYesYesAbstractionMedium hides its complexityYesYesBlack-boxHow algorithmic decisions are made is intransparentYesYesControlContent moderationContent is moderated at allYesYesInvisible content moderationMost content moderation is not visible to the userYesNoContent moderation pre-generationContent is moderated before it is received by the userYesNoSelf-determinationUser can decide themselves how to actYesYesSelf-expressionUser can express themselvesYesYesSimultaneityUser can receive and produce content concurrentlyNoYesImmediacyInstantaneityActions are perceived almost immediatelyYesYesEvanescenceMedium quickly recedes to the backgroundYesYes Note: The features spatial separation, recording, self-determination, self-expression, simultaneity, instantaneity,and evanescence, as well as the categories medium, control and immediacy are based on Clark (1996). Instanceswhere features of generative AI are similar to features of social media are highlighted in bold. one user might send a message at the same time as another user is sending them a message , but notfor generative AI, which operates in a sequential dialogue of user input and model output. Importantdifferences between generative AI and social media concern content moderation: Even though bothgenerative AI and social media feature content moderation, content moderation in generative AItends to be less visible than on social media. Social media platforms may occasionally take hardlyvisible actions such as downranking posts, but many social media content moderation actions suchas removal of a post or user are clearly visible. Generative AI models, on the other hand, are builtand ne-tuned to moderate content in a certain way (e.g., to avoid providing dangerous information)without the user necessarily becoming aware of the moderation. Generative AI content moderationmay be invisible to the user because the model will usually respond, and not necessarily providea reason if it refuses to respond to a prompt directly, which makes moderation less obvious than amissing response or a refused response citing the reason for refusal. Relatedly, generative AI modelstend to moderate before the content is shown to the user, e.g. by refusing to reply to a prompt, whilesocial media content moderation tends to occur only after content made it onto a platform, e.g. whena post was reported as harmful misinformation. Beyond specic features of generative AI and social media, there are differences in their contextand potential consequences. In terms of business model, most social media companies rely on rev-enue from advertisements, while prominent generative AI companies have so far leaned towardsfreemium subscription models. While the potential harm of social media to democracy and societyhas been an important focus of scholarly and public attention (Persily and Tucker, 2020), some ar-gue that the destructive potential of AI may be at another level since it may present a larger threat orstronger geopolitical advantage (Stern, 2023). Generative AI and social media differ also in the levelof uncertainty they bring. For example, auditing and discovering vulnerabilities in systems that areprobabilistic (Cattell, Ghosh and Kaffee, 2024), like generative AI models, implies new complexi-ties that traditional, deterministic social media algorithms do not entail. Finally, generative AI andsocial media may differ in areas that have so far remained legally uncertain, such as questions ofliability (e.g., for harms results from media use) and copyright.",
  "imply that both of these media necessarily moderate content and thus face complex content modera-tion challenges and public scrutiny": "shows key similarities between generative AI and social media when it comes to the featuresof each medium. Both generative AI and social media allow for spatial separation, that is, the conver-sation partners usually generate content in different physical spaces e.g., in a home ofce and ata data center for generative AI and are not copresent (copresence is one of the features of face-to-face communication in Clark (1996)). Both generative AI and social media are recording user data(the recording feature is adapted from Clarks recordlessness feature). Both media can learn abouta users context and their preferences over time to personalize their output, e.g. by updating thechatbots memory or personalizing a recommendation algorithm. Further, both generative AI andsocial media can feature content on all kinds of domains (e.g., hobbies, jobs, politics). Both are pow-ered by articial intelligence (AI), that is, they rely on learning patterns from data to perform well ontasks such as generating text or recommending content, although generative AI relies on more recentdeep learning models while social media tends to rely on traditional machine learning approachessuch as recommender systems. Both media also feature abstraction, that is, they hide the complextechnical implementation details from the user behind a simple user interface. Further, generativeAI and social media algorithms tend to be black-box, that is, algorithmic decisions are intransparent almost always for users, but often also for experts because mechanistic interpretability that canexplain why a deep learning model made a certain decision is in its infancy. With regards to control features (Clark, 1996), both generative AI and social media feature contentmoderation, that is, the medium shapes what content is allowed to appear. Both media also meetClarks criteria for self-determination, i.e. a users ability to decide themselves how to act, andself-expression, i.e. a users ability to express themselves on a medium. With regards to immediacy (Clark, 1996), both generative AI and social media share instantaneity(Clark, 1996), i.e. that actions are perceived almost immediately, and evanescence (Clark, 1996), i.e.that the medium recedes to the background quickly once it is not actively used anymore.",
  "Learnings from social media regulation for generative AI regulation": "As the review of the affordances has shown, generative AI and social media share important features,including the use of AI and content moderation. Although generative AI and social media differon some dimensions, these differences suggest, for the most part, differences in degree, and notdifferences in kind when it comes to regulation. Thus, lessons learnt from social media regulationmay be relevant to generative AI regulation. This paper addresses four policy recommendations forgenerative AI regulation based on the evolution of social media regulation: (1) counter bias andperceptions thereof (e.g., via transparency, oversight boards, researcher access, democratic input,multidisciplinary research), (2) address specic regulatory concerns (e.g., youth wellbeing, electionintegrity) and invest in trust and safety, (3) promote computational social science research, and (4)take on a more global perspective.",
  "Counter bias and perceptions thereof": "Given that both generative AI and social media share the key features content moderation, use ofAI, that they are black-box and abstract the complexity of algorithmic decision-making away suchthat much of the decision-making is intransparent, it is no surprise that both generative AI com-panies and social media companies have faced allegations of bias, including allegations of anti-conservative political bias (Robertson, 2024; Barrett and Sims, 2021). While there is no evidenceof anti-conservative bias for social media (Barrett and Sims, 2021), multiple studies have shownpolitical bias in generative AI. For example, compared to representative opinion polls, large lan-guage models were found to output biased opinions (Durmus et al., 2023; Santurkar et al., 2023),and multiple studies showed left-leaning bias in generative AI such as ChatGPT (Rozado, 2023;Rttger et al., 2024).",
  "stereotypical depictions of race, gender, age, nationality, and socioeconomic status (Nangia et al.,2020)": "Addressing such biases is as important as it is challenging. It is important to address biases becausebiases can harm users by leading to lower-quality output, they can entrench historical biases andstereotypes, and they can undermine trust in model developers, model deployers, and regulators ofgenerative AI. It is challenging to address biases because they are challenging to measure accurately(e.g., they may be sensitive to the specic prompt design (Rttger et al., 2024)) and because it is notclear where exactly biases stem from. Biases can arise at different points in the development anddeployment of generative AI, including training and data curation, ne-tuning, evaluation and feed-back, real-time moderation, customization and control of models (Suresh and Guttag, 2019; Ferrara,2023). Social media companies have taken different approaches to address biases or perceptions thereof thatmainly focus on transparency about algorithms and decision-making, gathering input from users andlearning from case studies, and increasing user choice.",
  "Increase transparency and researcher access": "The features content moderation, use of AI, black-box and abstraction also give rise to transparencychallenges for social media and generative AI. Generative AI transparency has been poor as shownin the Foundation Model Transparency Index (Bommasani et al., 2023, 2024). Social media com-panies have pursued multiple different approaches to increase transparency and generative AI canlearn from this playbook. For example, Facebooks parent company Meta introduced features suchas Why am I seeing this ad? that allowed users to understand why they were served certain adcontent (Thulasi, 2019), created blog posts and a Transparency Center providing some informa-tion on the role of AI and other factors in content recommendation (Clegg, 2023; Meta, 2024a),and established an independent oversight board of experts that adjudicates particularly contentiouscontent moderation decisions (Meta, 2024b). These initiatives do not come without problems. Inresponse to the launch of Facebooks oversight board, The real Facebook Oversight Board was cre-ated, which brought experts together to argue for more independence, transparency and regulation(The Real Facebook Oversight Board, 2022). An important aspect of transparency is allowing for third-party evaluations.Efforts to cre-ate APIs accessible to researchers, such as the Facebook Open Research and Transparency Re-searcher API and the TikTok Research API, or to design academic-industry collaboration suchas the Facebook and Instagram Election Study are helpful but imperfect (Wagner, 2023). TheCoalition for Independent Technology Research was founded after researchers at different insti-tutions faced difculty maintaining or gaining access to social media data for research purposes(Coalition for Independent Technology Research, 2022). Importantly, we can learn from these short-comings. Researcher access programs to evaluate technology should be characterized by sufcientresources (including stafng, infrastructure, and funding), incentives that are compatible with aca-demic research (e.g., data retention policies, persistent API access and publication permission forresearchers), sound knowledge sharing processes between internal and external researchers to helpunderstand data availability and feasibility, helpful documentation, privacy preserving measures(e.g., aggregation of user data) and timeliness in terms of publication or addressing of issues thatresearchers discovered. To protect researchers involved, researchers have called for safe harbors,that is, legal protection for researchers pursuing legitimate research purposes, initially for socialmedia (Abdo et al., 2022) and more recently for generative AI (Longpre et al., 2024). Additionalproposals to facilitate external generative AI research include data donations (Sanderson, 2024). Regulations like the Digital Services Act prescribe transparency by requiring audits of social mediacompanies (European Commission, 2023), and similar auditing efforts are imaginable for generativeAI. In fact, some scholars suggest to extend and adapt DSA rules for social media platforms togenerative AI (Hacker, Engel and Mauer, 2023). While the specic implementation of these transparency efforts may be contentious and requiresnuance, ideas such as short and accessible explanations of the technology, independent oversightmechanisms, researcher access and mandatory audits are viable options for increasing transparencyvia generative AI regulation.",
  "Gather democratic input to inform technology": "Generative AI and social media share features that make them complex, including that the contentthey feature can pertain to a variety of domains, that there is potential for personalization, and thatcontent could be moderated in various different ways. Given the vast set of choices that develop-ers face, one approach is to gather input directly from users to determine what a good system maylook like. In terms of gathering input from users to enable democratic decisions about the nature ofregulation and content moderation, different initiatives have been launched over the past few yearsto deliberate issues ranging from cyberbulling on social platforms to the rules and constitutions thatinform generative AI models (Wetherall-Grujic, 2023), relying on the much older idea of deliber-ative democracy (Eagan, 2016). Social media also offers case studies of networks where contentmoderation seems to be broadly accepted and deliver productive results, such as in the case of thedeliberation platform vTaiwan (Miller, 2019) or a neighborhood-focused social network (Oremus,2024). Finally, social media researchers have studied how to embed important societal values into AI(Bernstein et al., 2023), which could also inform how such values can be embedded into generativeAI.",
  "Promote user choice": "Another option to empower users to make choices in the face of features like content moderationand the varied nature of content is to enable users to set up rules for a subset of the system. Thesocial media platform Mastodon is a prominent example in terms of increasing user choice in sucha way. Mastodon is built on the idea that different communities can create their own servers and setand enforce their own content moderation rules (Mastodon, 2024). This highlights that the featureof personalization may be a potential route for resolving content moderation dilemmas. Contentmoderation questions with regards to generative AI and social media are similar and it is not clearwhat opinion representation should be the default (Redpoint, 2020). This suggests that increasedpersonalization of models may be an answer (Redpoint, 2020).",
  "Address specic regulatory concerns and invest in trust and safety": "The feature of content moderation that generative AI and social media share comes with especiallythorny issues such as preventing the spread of harmful misinformation and protecting user wellbe-ing. Social media companies have invested in teams that address these specic regulatory concerns.Examples include teams at companies like Google, Meta and Microsoft working on youth wellbeingand mental health in general, election integrity, preventing spam, preventing the spread of child sex-ual abuse material, preventing harmful misinformation, detecting deceptive campaigns, and ensuringtrust in the platform and safety of its users overall. Generative AI chatbots have already been rated on AI-related principles that apply just as much tosocial media. Common Sense Media published rankings of different generative AI models with re-gards to the following principles: put people rst, prioritize fairness, be trustworthy, keep kids andteens safe, be effective, help people connect, use data responsibly, and be transparent and account-able (Common Sense Media, 2024). Yet, generative AI companies do not have teams at the samescale as social media companies to address these issues. Generative AI companies are much smaller and younger than some of the social media giants, thusit is not surprising that they do not have as much dedicated staff to work on these issues. Goingforward, however, adding diverse staff beyond engineers that can bring in expertise to address issuessuch as user mental health or combating misinformation seems important. Investment in trust andsafety teams seems particularly crucial, and it is encouraging to see that companies like OpenAI andAnthropic are investing in this area, with OpenAI publishing the rst-ever report on the activity ofdeceptive campaigns on generative AI platforms in May 2024 (Nimmo, 2024). The policies socialmedia companies have put in place to decide how and when to moderate individual users, and thebest practices they have developed to uncover abuse such as deceptive campaigns that try to interferewith elections or spam users, could inform the approaches generative AI companies take. This in-cludes developing a repertoire of content moderation approaches, which could include bans, but alsomore cautious interventions such as warnings and strikes for misbehavior, putting more guardrails inplace or throttling usage for users that tried to abuse generative AI models in the past. Social mediacompanies also gained experience in involving the user community in content moderation decisions",
  "Promote computational social science research": "Both generative AI and social media allow users to express themselves and allow for a connection,be it to other users or to an AI with a vast pool of knowledge. These features suggest that bothof these media are so important and powerful because of how they interact with users. They areneither purely technical, nor purely social systems. This suggests that multidisciplinary study computational social science is needed to understand, evaluate and shape these systems. In fact, the recommendations above, whether regarding measures to reduce bias or enhance userwellbeing, all require computational social science research to test their effectiveness. Social mediacompanies have hired researchers from many disciplines, including computer science, psychology,political science, communication, law and others, to better understand how their platforms impactsociety, and how certain interventions inuence society and their revenue. Whether research is conducted in-house or via access to the platforms for external researchers, rigor-ous evaluations are key to ensure that media like generative AI and social media meet their goal ofbeing helpful and not harmful to society. Further investment in research is needed because genera-tive AI does have features that differ from previous technologies, so its impact and user preferences(e.g., with regards to privacy, personalization or content moderation) are not clear. Even the im-pact of previous technologies like social media has not yet been comprehensively evaluated andneeds further investment. Rigorous research can inform platform and public policy when it comesto regulation, and it can enhance user trust. This implies investing in diverse research teams that understand the interaction of humans an a givenmedium and that can evaluate the societal implications of a product. While AI company recruitingoften focuses heavily on engineers, and some companies are more concerned with extreme risksin the more distant future, social media companies have shown the value of addressing currentrisks such as biases and of creating multidisciplinary teams to do so. This allows companies totest different product features and interventions effectively, e.g. to reduce spam or misinformationspread. Computational social scientists from any background, data scientists and user experienceresearchers would be especially helpful to address questions at the intersection of technology andhumans, such as which emotional bonds may be formed between humans and AI, and what type ofpersonalization could be implemented. While content moderation on social media is far from a resolved issue, there is a large and growingbody of academic literature that speaks to promising approaches and could inform content modera-tion for generative AI (e.g. Persily and Tucker, 2020; Kozyreva et al., 2024).",
  "Take on a more global perspective": "As the features spatial separation, general content, and use of AI imply, both generative AI andsocial media can be used in a variety of contexts. Generative AI companies have grown rapidlyand are serving users around the world, similar to social media companies. However, comparedto social media companies, generative AI companies, at least the startups among them, seem moreheavily focused on the US due to their location (with exceptions like Google DeepMind in the UKand Mistral AI in France). To address problems like biases, it is crucial that even small companiestake on a global perspective and become global companies with local expertise in multiple countries.This could take the form of local ofces and a focus on hiring internationally. The stakes are high.If companies fail to invest in taking user preferences and risk factors outside of the US seriously, thetechnology may serve large numbers of users worse (e.g., due to under-investment in non-Englishlanguage content generation) and could even result in catastrophes such as promoting violence inconict regions (Amnesty International, 2022). Given the increasing amount of national and localregulations on generative AI, global expertise is also important to keep up with local laws. For effective regulation, local expertise needs to be integrated into a global perspective. For example,the former Prime Minister of New Zealand suggested that a model for governing AI could follow theChristchurch Call, which is a multinational, multi-stakeholder effort bringing together governments,",
  "Conclusion": "There are strong disagreements about the approach that should be taken to regulate generative AI.This paper argued that the regulation of generative AI can be informed by the evolution of the reg-ulation of social media. While social media is by far not the only analogy proposed for generativeAI (Maas, 2023), and by no means a perfect analogy, generative AI and social media share key fea-tures that make a comparison of the two worthwhile. Taking a close look at social media regulationefforts including self-regulation and laws reveals interesting approaches and best practices.This paper outlined recommendations regarding transparency, researcher access, gathering demo-cratic input, promoting user choice, addressing specic regulatory concerns, increasing investmentsinto computational social science research, and taking on a more global perspective. In the caseof social media, self-regulation did not always work, which has resulted in multiple new laws be-ing proposed in the past few years. These laws, but also the forms of self-regulation that were putin place, including specic approaches to increasing transparency, enhancing user choice, and in-vesting in research, can be valuable pointers for those looking to regulate generative AI. Analyzingsocial media regulation may speed up the process of developing generative AI regulation. The EUAI Act may not have been able to address general purpose models as fast as it did had it not alreadybeen concerned with other forms of machine learning much earlier. Regulation takes time and ef-fort, so where possible, resources should be saved and mistakes avoided by looking at social mediaregulation and research.",
  "Ardern, Jacinda. 2023. There s a model for governing AI. Here it is..URL:": "Barrett, Paul M. and J. Grant Sims. 2021. False Accusation: The Unfounded Claim that Social Media Compa-nies Censor Conservatives. Technical Report February NYU Stern Center for Business and Human Rights.URL: Bernstein, Michael S, Angle Christin, Jeffrey T Hancock, Tatsunori Hashimoto, Chenyan Jia, Michelle Lam,Nathaniel Persily, Tiziano Piccardi, Martin Saveski, Jeanne L Tsai, Johan Ugander and Chunchen Xu. 2023.Embedding Societal Values into Social Media Algorithms. Journal of Online Trust and Safety 2(1):113.",
  "Huttenlocher, Dan, Asu Ozdaglar and David Goldston. 2023. A Framework for U.S. AI Governance: Creatinga Safe and Thriving AI Sector. Technical report MIT Schwarzman College of Computing.URL:": "Kozyreva, Anastasia, Philipp Lorenz-Spreen, Stefan M. Herzog, Ullrich K.H. Ecker, Stephan Lewandowsky,Ralph Hertwig, Ayesha Ali, Joe Bak-Coleman, Sarit Barzilai, Melisa Basol, Adam J. Berinsky, CorneliaBetsch, John Cook, Lisa K. Fazio, Michael Geers, Andrew M. Guess, Haifeng Huang, Horacio Larreguy,Rakoen Maertens, Folco Panizza, Gordon Pennycook, David G. Rand, Steve Rathje, Jason Reier, PhilippSchmid, Mark Smith, Briony Swire-Thompson, Paula Szewach, Sander van der Linden and Sam Wineburg.2024. Toolbox of individual-level interventions against online misinformation. Nature Human Behaviourpp. 19. Longpre, Shayne, Sayash Kapoor, Kevin Klyman, Ashwin Ramaswami, Rishi Bommasani, Borhane Blili-Hamelin, Yangsibo Huang, Aviya Skowron, Zheng Xin Yong, Suhas Kotha, Yi Zeng, Weiyan Shi, XianjunYang, Reid Southen, Alexander Robey, Patrick Chao, Diyi Yang, Ruoxi Jia, Daniel Kang, Alex Pentland,Arvind Narayanan, Percy Liang and Peter Henderson. 2024. Position: A Safe Harbor for AI Evaluationand Red Teaming. In Proceedings of the 41st International Conference on Machine Learning, ed. Rus-lan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett and FelixBerkenkamp. Vol. 235 of Proceedings of Machine Learning Research PMLR pp. 3269132710.URL:"
}