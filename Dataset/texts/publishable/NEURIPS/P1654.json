{
  "Abstract": "Instruction tuning is a supervised fine-tuning approach that significantly improvesthe ability of large language models (LLMs) to follow human instructions. Forprogramming tasks, most models are finetuned with costly human-annotatedinstruction-response pairs or those generated by large, proprietary LLMs, whichmay not be permitted. We propose SelfCodeAlign, the first fully transparent andpermissive pipeline for self-aligning code LLMs without extensive human annota-tions or distillation. SelfCodeAlign employs the same base model for inferencethroughout the data generation process. It first extracts diverse coding conceptsfrom high-quality seed snippets to generate new tasks. It then samples multipleresponses per task, pairs each with test cases, and validates them in a sandboxenvironment. Finally, passing examples are selected for instruction tuning. In ourprimary experiments, we use SelfCodeAlign with CodeQwen1.5-7B to generatea dataset of 74k instruction-response pairs. Finetuning on this dataset leads to amodel that achieves a 67.1 pass@1 on HumanEval+, surpassing CodeLlama-70B-Instruct despite being ten times smaller. Across all benchmarks, this finetunedmodel consistently outperforms the original version trained with OctoPack, theprevious state-of-the-art method for instruction tuning without human annotationsor distillation. Additionally, we show that SelfCodeAlign is effective across LLMsof various sizes, from 3B to 33B, and that the base models can benefit more fromalignment with their own data distribution. We further validate each componentseffectiveness in our pipeline, showing that SelfCodeAlign outperforms both directdistillation from GPT-4o and leading GPT-3.5-based distillation methods, suchas OSS-Instruct and Evol-Instruct. SelfCodeAlign has also led to the creationof StarCoder2-Instruct, the first fully transparent, permissively licensed, and self-aligned code LLM that achieves state-of-the-art coding performance. Overall,SelfCodeAlign shows for the first time that a strong instruction-tuned code LLMcan result from self-alignment rather than distillation.",
  "Introduction": "Recent studies have demonstrated the outstanding performance of large language models (LLMs) in various code-related tasks, e.g., program synthesis , programrepair , code optimization , code completion , code transla-tion , software testing , and software agents . The reason isthat modern LLMs are pre-trained over trillions of code tokens in the wild using various training ob-jectives (as such next-token prediction ), making the base models natively good at understandingand generating code snippets. Furthermore, to fully unleash the power of LLMs, the base models are",
  "arXiv:2410.24198v2 [cs.CL] 1 Nov 2024": "typically further fine-tuned on high-quality instruction-following data to boost their performance infollowing natural language instructions and solving more general software engineering tasks .This step is known as instruction tuning . Curating high-quality data for instruction tuning is crucial yet challenging. One source of acquiringinstruction data is to employ human annotation . For example, Llama-3 uses a corpusof 10 million human-annotated examples in instruction tuning. Due to the high cost of humanannotation, knowledge distillation is widely adopted to train a weaker LLM with outputs generated bystronger LLMs . However, distillation may violate the terms of service of proprietaryLLMs and the prerequisite of using a stronger LLM limits its generalizability. Therefore, recentproposals focus on instruction tuning without relying on human annotation or distillation .One cornerstone work along this direction is SELF-INSTRUCT , which finetunes GPT-3 withself-generated instruction data using in-context learning. There is a growing number of instruction-tuned open-source LLMs in the code domain. However,some models, such as DeepSeek-Coder , Llama-3 , and CodeQwen1.5 , either use propri-etary data or do not disclose their instruction-tuning strategies. Others, including WizardCoder ,Magicoder , WaveCoder , and OpenCodeInterpreter , rely on knowledge distillation. Theonly exception is OctoCoder , which is instruction-tuned over heavily filtered GitHub commits,with commit messages as instructions and the changed code as responses, as well as data from Ope-nAssistant, a human-generated corpus of user-assistant conversations . Despite its transparencyand permissive licensing, OctoCoders performance, at 32.9 HumanEval+ pass@1, lags behindother mainstream code LLMs. Meanwhile, previous attempts at applying SELF-INSTRUCT for codegeneration have resulted in performance degradation over training on natural instruction-responsepairs . Our findings imply that effective self-alignment requires a combination of data diversitycontrol and response validation, which is not present in the traditional SELF-INSTRUCT approach. In this paper, we propose SelfCodeAlign, the first fully transparent pipeline to successfully self-alignbase code LLMs with purely self-generated instruction data. First, SelfCodeAlign extracts diversecoding concepts from high-quality seed functions in The Stack V1 , a large corpus of permissivelylicensed code. Next, using these concepts, we prompt the base model to generate new coding tasksthrough in-context learning. We then instruct the base model to produce multiple responses for eachtask, each paired with test cases for self-validation. Finally, we select only the instruction-responsepairs that pass the test cases. This method ensures the model practices various coding concepts andvalidates the consistency between instructions and responses. To evaluate our method, we train CodeQwen1.5-7B, a state-of-the-art open-source base LLM forcode, on both a dataset generated with SelfCodeAlign and OctoPack, a naturally-generated andmeticulously-filtered dataset used for training OctoCoder . We benchmark both, along withOctoCoder and other models, on a series of tasks: code generation (both function- and class-level) , data science programming , and code editing . On all tasks, trainingCodeQwen with SelfCodeAlign significantly improves performance over the base model and overtraining it on OctoPack. For instance, on HumanEval+, our model achieves a pass@1 score of 67.1,21.4 points higher than CodeQwen1.5-7B and 16.5 points higher than CodeQwen1.5-7B-OctoPack.This highlights the effectiveness of our synthetic data generation method compared to natural data inenhancing the capabilities of code LLMs. In the component analysis, we justify the different components of the pipeline. We demonstrate thatSelfCodeAlign is general to different LLMs whose sizes range from 3B to 33B. In particular, we findthat a base LLM could learn more effectively from data within its own distribution than a shifteddistribution from a teacher LLM. Additionally, we show that seed selection, concept generation,and execution filtering all contribute positively to the pipeline. Furthermore, on HumanEval+, Self-CodeAlign (67.1 pass@1) outperforms state-of-the-art, GPT-3.5-Turbo-based distillation methods,including OSS-Instruct (61.6) and Evol-Instruct (59.1), as well as direct output distillationfrom GPT-4o (65.9). SelfCodeAlign has also led to the creation of StarCoder2-Instruct, the first fully transparent, permis-sively licensed, and self-aligned code LLM that achieves state-of-the-art coding performance. Wediscuss StarCoder2-Instruct in Appendix A.",
  "Overall, we make the following main contributions: (i) We introduce SelfCodeAlign, the first fullytransparent and permissive pipeline for self-aligning code LLMs to follow instructions. Our method": "does not rely on extensive human annotations or distillation from larger models. (ii) We generate aseries of datasets using SelfCodeAlign and train multiple models on these datasets, which will all bereleased to the public. (iii) We thoroughly evaluate our method on a multitude of tasks, showing strongperformance across all the evaluated models. (iv) Our experiments demonstrate that training modelson their own data can be more effective than using data from stronger, but distributionally different,teacher models when they dont have a huge performance gap. (v) Finally, we run a comprehensivecomponent analysis that verifies the positive contribution of each component in SelfCodeAlign.",
  "SelfCodeAlign: Self-Alignment for Code Generation": "illustrates an overview of our SelfCodeAlign technique. It first generates diverse in-structions by extracting coding concepts from high-quality seed snippets. This process resemblesOSS-Instruct , which employs GPT-3.5-Turbo to convert random snippets into instructions.However, our method uses the base model exclusively and incorporates a separate concept generationphase that we prove beneficial in 4.3. SelfCodeAlign then generates several responses for eachtask, pairing each with test cases for sandbox execution, and finally chooses passing examples forinstruction tuning. Example outputs from each step are listed in Appendix D.1. In the followingsections, we provide detailed explanations of these steps.",
  "Seed Snippets Collection": "SelfCodeAlign starts by collecting a set of seed code snippets from The Stack V1. In this step,its crucial to ensure that the seed snippets are diverse and high-quality, as they will be used as thestarting point for generating instructions and responses. To collect the seed snippets, we extract allPython functions with docstrings from The Stack V1, and then apply a series of filtering rules toensure the quality of the seed snippets. In total, we collect 250k Python functions from 5M functionswith docstrings in The Stack V1, which were filtered by running the Pyright type checker, removingbenchmark items, filtering out functions with poor documentation, and removing near-duplicates.Appendix B details this process in depth.",
  "Diverse Instruction Generation": "After collecting the seed functions, we perform Self-OSS-Instruct, our adaptation of OSS-Instruct for self-alignment, to generate diverse instructions. In detail, we employ in-context learning to letthe base model self-generate instructions from the given seed code snippets. This process utilizes 21carefully designed few-shot examples listed in Appendix E. The instruction generation procedure isdivided into the following two steps: Concepts extraction: For each seed function, we prompt the base model to produce a list ofcode concepts present within the function. Code concepts refer to the foundational principles andtechniques used in programming, such as pattern matching and data type conversion. Instruction generation: We then prompt the base model to self-generate a coding task conditionedon the identified code concepts and two additional attributes, difficulty (easy/medium/hard) andcategory (function/class/program implementation), which we randomly sample to enrich thediversity of the generated instructions.",
  "Given the instructions generated from Self-OSS-Instruct, our next step is to match each instructionwith a high-quality response. Prior practices commonly rely on distilling responses from stronger": "teacher models, such as GPT-4, which hopefully exhibit higher quality. However, distilling proprietarymodels leads to non-permissive licensing and a stronger teacher model might not always be available.More importantly, teacher models can be wrong as well, and the distribution gap between teacher andstudent can be detrimental. We propose to self-align the base model by explicitly instructing the model to generate tests forself-validation after it produces a response interleaved with natural language. This process is similarto how developers test their code implementations. Specifically, for each instruction, the base modelsamples multiple outputs of the format (response, tests), and we filter out those responses falsified bythe test execution under a sandbox environment. We then randomly select one passing response perinstruction to the final instruction tuning dataset.",
  "SelfCodeAlign-CQ-7BSelf-generated67.165.2": "HumanEval+ and MBPP+. HumanEval and MBPP are the two most widely-used benchmarksfor function-level code generation. We use their test augmented versions, i.e., HumanEval+ andMBPP+, with 80/35 more test cases for rigorous evaluation . As baselines, we consider a diverse set of state-of-the-art instruction-tuned models over variousdimensions, including weight openness, data openness, transparency, and performance. compares the pass@1 of the self-aligned SelfCodeAlign-CQ-7B against other baseline models on HumanEval+ and MBPP+. Among those trained using a fully transparent pipeline without anyproprietary data or distillation, SelfCodeAlign-CQ-7B stands out as the best LLM by drasticallyoutperforming the base model, OctoCoder-16B, StarCoder2-15B, and CodeQwen1.5-7B-OctoPack.Meanwhile, compared to much larger models, SelfCodeAlign-CQ-7B outperforms Arctic, Command-R+, and Mixtral-8x7B-Instruct, while closely matching Mixtral-8x22B-instruct. Even comparedto LLMs trained using proprietary data (e.g., manually annotated), SelfCodeAlign-CQ-7B remainscompetitive, surpassing Gemini Pro, Mistral Large, and CodeLlama-70B-Instruct. Additionally,SelfCodeAlign-CQ-7B, fine-tuned on purely self-generated data, closely rivals models finetuned withdistillation-based or non-transparent synthetic data. LiveCodeBench. In subsequent evaluations, we benchmark our model against state-of-the-artopen-source LLMs of similar sizes for a fair comparison. LiveCodeBench is a benchmark forcontamination-free evaluation. It features 400 recent Python algorithm challenges from May 2023 toFebruary 2024. These tasks are curated from online judge websites such as Codeforce and LeetCode,each with over 20 test cases on average. While LiveCodeBench is a holistic benchmark covering fourproblem types, we focus on the code generation task for assessing LLM function generation. reports the pass@1 results for problem subsets created after three specific start dates. Itshows that SelfCodeAlign-CQ-7B consistently outperforms most baseline models and closely matchesCodeQwen1.5-7B-Chat. In addition, moving the start date forward has minimal impact on the pass@1of SelfCodeAlign-CQ-7B, indicating that our pipeline is less likely to suffer from contamination.",
  "SelfCodeAlign-CQ-7B22.422.823.4": "EvoEval. To mitigate the impact of potential data contamination, EvoEval includes 828programming problems created by prompting GPT-4 to evolve original HumanEval tasks across 5semantic-altering and 2 semantic-preserving benchmarks. Following the leaderboard of EvoEval, weuse the 5 semantic-altering benchmarks, each of which has 100 problems. shows that SelfCodeAlign-CQ-7B achieves the best pass@1 score among all transparentlyfinetuned models. Meanwhile, it also surpasses most open LLMs (except CodeQwen1.5-7B-Chat)trained on unknown instruction-tuning data.",
  "SelfCodeAlign-CQ-7B43.63340602065": "EvalPerf. While the earlier benchmarks focus on code correctness, we use EvalPerf to evaluatethe efficiency of LLM-generated code. EvalPerf includes 118 performance-exercising tasks withcomputation-intensive test inputs to fully exercise the efficiency of LLM-generated code. Since code efficiency only matters when the generated code is correct, in we only evaluatebaselines that can achieve a decent pass@1 (i.e., over 50%) on HumanEval+. Specifically, we runEvalPerf by following its default settings: (i) Each model generates 100 samples per task at thetemperature of 1.0; (ii) We evaluate the efficiency of up to 20 correct samples per model for taskswhere it can at least generate 10 passing samples; and (iii) Finally we rank the models based ontheir win rates, where each model pair compares their differential performance score (DPS) over thecommon set of passing tasks. Notably, DPS is a LeetCode-inspired metric that indicates the overallefficiency ranking of submissions. For example, shows that SelfCodeAlign-CQ-7B achievesa DPS of 79.9, indicating that its correctly generated solutions can overall outperform or match theefficiency 79.9% of reference solutions across various efficiency levels. shows that SelfCodeAlign-CQ-7B ranks second among the evaluated models of comparablesize. Specifically, SelfCodeAlign-CQ-7B is only next to DeepSeek-Coder-6.7B-Instruct whosetraining data is not disclosed. Surprisingly, the efficiency of SelfCodeAlign-CQ-7B-generated codesurpasses many recent open models trained using private data, including the latest Llama-3.1-8B-Instruct. : Ranking of model code efficiency based on the EvalPerf win rates, which are computed overthe common set of passing tasks for each model pair. Each model generates 100 samples per task at atemperature 1.0. To exemplify differential performance score (DPS) with SelfCodeAlign-CQ-7B, itmeans its generations if correct can match the efficiency of 79.9% LLM samples.",
  "Class-level Code Generation": "We evaluate code LLMs on class-level code generation using ClassEval , a collection of 100class-level Python code generation tasks, covering 100 classes and 410 methods, with an average of33 tests per class and 8 tests per method. Following the ClassEval paper , we set the maximum model context size as 2048 tokens andreport the best class-level pass@1 (and corresponding method-level pass@1) of each model amongthree generation strategies: (i) Holistic Generation: generating the entire class given a class skeleton,(ii) Incremental Generation: generating class methods iteratively by putting earlier generated methodsin the prompt, and (iii) Compositional Generation: generating each class method independentlywithout looking at other methods. Specifically, class-level pass@1 in refers to the pass rateof generated classes given both the method- and class-level tests. In contrast, method-level pass@1is computed by only checking if the generated methods can pass the method-level tests. shows, in terms of class-level performance, SelfCodeAlign-CQ-7B is the best transparently finetunedmodel, surpassing the second-best transparent model (i.e., CodeQwen1.5-7B-OctoPack) by 28%,while performing no worse than those using unknown or proprietary instruction-tuning data. Formethod generation, SelfCodeAlign-CQ-7B also stands out in transparently finetuned models.",
  "Data Science Programming": "DS-1000 is a benchmark of 1000 realistic data science challenges across 7 popular Python datascience libraries. In DS-1000, a model must complete a partial code snippet to solve the problem.The solution is then evaluated through test execution. shows that SelfCodeAlign-CQ-7B,despite being trained on limited data science code, stands out as the best in the transparent modelcategory, while remaining competitive among the other evaluated baselines.",
  "Code Editing": "We further evaluate LLMs on code editing tasks using the CanItEdit benchmark , comprised of210 code editing tasks from three change kinds (70 tasks each): corrective (fixing bugs), adaptive(adding new features), and perfective (improving existing features). The tasks are evaluated based onthe correctness of the generated code changes, according to a set of hidden test cases. For each task,the model is given as input the original code snippet and a natural-language instruction describingthe desired code change; then it is expected to produce an updated code snippet that satisfies theinstruction. We follow the setting from the original benchmark to generate 20 completions pertask at a temperature of 0.2. reports the pass@1 for each change kind and the average pass@1across all tasks. Despite not being specifically tuned for code editing, SelfCodeAlign-CQ-7B exhibitsstrong performance on CanItEdit, achieving a pass@1 of 39.0%, outperforming all other modelsexcept CodeQwen1.5-Chat, whose instruction tuning details are not disclosed.",
  "Self-Alignment with Different Models": "To assess whether SelfCodeAlign is generalizable and how performance varies with finetuning datagenerated by different models, we run the same data generation pipeline end to end with differentLLMs. We include four diverse state-of-the-art model architectures and sizes ranging from 3B to 33Bto observe how SelfCodeAlign performs across small, medium, and large-scale LLMs. shows the comparison and guides us to reach the following findings. Looking at the diagonalcells, SelfCodeAlign consistently improves the performance of the base models with varying sizes,from 3B to 33B. Comparing each diagonal cell and the cell immediately to its right (i.e., using basemodels with slightly better HumanEval+ performance as the teacher models), we can see that a basemodel may benefit more from self-generated data than a stronger teacher model, when they donthave a large performance gap. However, when the teacher model is clearly stronger, the base modellearns better by distilling the teachers knowledge. For example, StarCoder2-3B achieves higherpass@1 trained on its own data (35.4) compared to Llama-3-8B data (34.1), but when tuned withstronger models, StarCoder2-3B further improves (e.g., 42.1 with DeepSeek-Coder-33B data). Also,the last row shows that a stronger model can still learn from a weaker model, but less effectively. Weprovide qualitative examples in Appendix D.2.",
  "Effectiveness of Execution-based Filtering": "The SelfCodeAlign pipeline samples multiple responses for an instruction and each response isequipped with self-generated test cases. Responses with failing tests are filtered out and eachinstruction will be paired with a randomly selected passing response. To answer the question of towhat extent is execution information helpful, in , we conduct 4 controlled experiments byvarying how responses are selected while keeping the other components unchanged: Random selection (all): pair each instruction with a random response without response filtering. Random selection (subset): 15.6k subset of Random selection (all) for a consistent data amount. Failures only: pair each instruction with a failing response. Passes only: pair each instruction with a passing response.",
  "Importance of Seed Selection and Concepts Generation": "For instruction generation, SelfCodeAlign applies Self-OSS-Instruct that first selects high-qualityseed code snippets, then mines code concepts from the seeds, and finally generates the instructions.To validate the usefulness of concept generation and high-quality seeds, we compare two variants ofSelfCodeAlign in : 1) directly generating instructions from seeds, where the model producesan instruction based solely on a seed snippet, and 2) using the default pipeline except for the initialseeds, where random snippets are sampled from different code documents in The Stack V1.",
  "Filtered functionsSeed instruction59.8Random snippetsSeed concepts instruction64.0Filtered functionsSeed concepts instruction65.2": "It is shown that directly generating instructions from seeds leads to the poorest performance. This isbecause a direct generation from seeds requires the seed snippet to be presented in the context, whoseformat is not well represented in the wild and may not be in distribution for the model. The generatedinstructions will then be distracted and thus be of lower quality. Concept generation neutralizes thiseffect and produces more realistic and natural instructions. Using random snippets produces a morediverse but less coherent set of concepts, leading to slightly worse performance compared to usinghigh-quality seeds. Appendices D.3 and D.4 illustrate some qualitative examples.",
  "Evol-Instruct74kGPT-3.5-Turbo59.1OSS-Instruct74kGPT-3.5-Turbo61.6Direct distillation74kGPT-4o65.9SelfCodeAlign74kCodeQwen1.5-7B67.1": "To compare self-alignment with distillation, we evaluate SelfCodeAlign against two state-of-the-artdistillation methods for code instruction tuning: OSS-Instruct and Code Evol-Instruct .We use the official OSS-Instruct dataset. As the official implementation of Code Evol-Instruct isunavailable, we opt for the most popular open-source version on Hugging Face. Both datasetsare generated using GPT-3.5-Turbo and we randomly select their subsets to match the 74ksamples generated by SelfCodeAlign. shows that SelfCodeAlign substantially outperformsboth methods, indicating the strength and promising future of self-alignment for code. Additionally,SelfCodeAlign outperforms direct distillation, where we use the same set of SelfCodeAlign instruc-tions but rely on GPT-4o to generate each response at temperature 0. This suggests that weakermodels, combined with more post-validation compute, can produce higher-quality responses.",
  "Related Work": "Instruction tuning for code. To build more powerful code assistants, pre-trained code models arefine-tuned over a small amount of high-quality instruction-response pairs that are either collected fromreal-world or synthetically generated . This step is known as instruction tuning.OctoPack compiles a large set of real-world Git commits which are partially used for codefine-tuning. Code Alpaca applies SELF-INSTRUCT to the code domain, which prompts ChatGPTto generate synthetic instruction data for code. Similarly, the instruction data for CODELLAMA includes coding problems generated by prompting LLAMA 2 and solutions and tests by promptingbase CODELLAMA. Code Evol-Instruct uses harder programming challenges as instructiondata to fine-tune more capable models. Specifically, Code Evol-Instruct prompts ChatGPT withheuristics to evolve existing instruction data to more challenging and complex ones. Besides datacomplexity, the widely-adopted OSS-INSTRUCT looks at the data diversity andquality dimension. Specifically, given a source code snippet, OSS-INSTRUCT prompts ChatGPTto get inspired and imagine potential instruction-response pairs, which inherit the diversity andquality of sampled code snippets. Besides instruction tuning, recent work on training code LLMsfor performance improvement also explores multi-turn code generation , model merging ,preference tuning , and reinforcement learning . Recently, various strong instruction-tunedcode models have been released by major organizations . However, their instruction-tuningrecipes (e.g., data and strategies) are not fully disclosed. This lack of transparency underscores theneed for fully transparent and permissive instruction-tuning methods to advance the field. Self-alignment. Self-alignment is an approach to instruction tuning that utilizes an LLM to learn fromits own output without depending on an existing well-aligned teacher LLM. SELF-INSTRUCT is one of the first endeavors that allow GPT-3 to improve itself by generating new instructions andresponses for instruction-tuning using its in-context learning capability. SELF-ALIGN , based onin-context learning, utilizes topic-guided SELF-INSTRUCT for instruction generation and pre-definesprinciples to steer the LLM towards desired responses. These instruction-response pairs are used tofine-tune the base model, followed by a final refinement stage to ensure the model produces in-depthand detailed responses. Instruction backtranslation offers an alternative self-alignment methodby initially training a backward model to generate instructions from unlabeled web documents usinglimited seed data. It then iteratively produces new instructions from new web documents and selectshigh-quality data for self-training. Most code LLM work targets knowledge distillation. Haluptzok etal. share a relevant idea to our work but only consider program puzzles specified in symbolicforms. This setting cannot be generalized to real-world tasks with natural language involved.",
  "Limitations and Future Work": "We limit our data generation within a 3000 window, skewing our distribution towards medium-sizedsamples. Therefore, generating and training on long-context instruction-response pairs can be apromising avenue . Second, we gather several negative samples during response generation, whichare currently filtered out. These negatives could be used in a reinforcement-learning loop to steer themodel away from incorrect responses . Furthermore, the good responses are labeled by testexecution, while the generated unit tests might be erroneous, calling for research to study and improvethe generation of valid test cases. Finally, we plan to apply SelfCodeAlign to more challengingdomains such as complex program generation and agentic software engineering .",
  "Conclusion": "We introduce SelfCodeAlign, the first fully transparent and permissive pipeline for self-aligningcode LLMs without extensive human annotations or distillation. SelfCodeAlign-CQ-7B, finetunedfrom CodeQwen1.5-7B using SelfCodeAlign, outperforms the 10 larger CodeLlama-70B-Instructon HumanEval+ and consistently surpasses CodeQwen1.5 trained with OctoPack on all studiedbenchmarks. We evaluate SelfCodeAlign across various model sizes, illustrating that stronger basemodels benefit more from self-alignment than distillation. We also examine the effectiveness ofdifferent components in the pipeline, showing that SelfCodeAlign is better than GPT-3.5 and GPT-4odistillation. Overall, we demonstrate for the first time that a strong instruction-tuned code LLM canbe created through self-alignment, without expensive human annotations or distillation.",
  "Y. Bai, X. Lv, J. Zhang, Y. He, J. Qi, L. Hou, J. Tang, Y. Dong, and J. Li. Longalign: A recipefor long context alignment of large language models, 2024": "F. Cassano, J. Gouwar, F. Lucchetti, C. Schlesinger, A. Freeman, C. J. Anderson, M. Q. Feldman,M. Greenberg, A. Jangda, and A. Guha. Knowledge transfer from high-resource to low-resourceprogramming languages for Code LLMs, 2024. F. Cassano, L. Li, A. Sethi, N. Shinn, A. Brennan-Jones, A. Lozhkov, C. J. Anderson, andA. Guha. Can It Edit? Evaluating the Ability of Large Language Models to Follow Code EditingInstructions. In The First International Workshop on Large Language Model for Code, 2024.",
  "S. Chaudhary. Code alpaca: An instruction-following llama model for code generation. 2023": "M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda,N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry,P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter,P. Tillet, F. P. Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H.Guss, A. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders,C. Hesse, A. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight,M. Brundage, M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish,I. Sutskever, and W. Zaremba. Evaluating large language models trained on code, 2021. C. Cummins, V. Seeker, D. Grubisic, M. Elhoushi, Y. Liang, B. Roziere, J. Gehring, F. Gloeckle,K. Hazelwood, G. Synnaeve, et al. Large language models for compiler optimization. arXivpreprint arXiv:2309.07062, 2023.",
  "Y. Deng, C. S. Xia, H. Peng, C. Yang, and L. Zhang. Large language models are zero-shotfuzzers: Fuzzing deep-learning libraries via large language models, 2023": "Y. Ding, H. Ding, S. Wang, Q. Sun, V. Kumar, and Z. Wang. Horizon-length prediction:Advancing fill-in-the-middle capabilities for code generation with lookahead planning. arXivpreprint arXiv:2410.03103, 2024. Y. Ding, J. Liu, Y. Wei, and L. Zhang. XFT: Unlocking the power of code instruction tuning bysimply merging upcycled mixture-of-experts. In L.-W. Ku, A. Martins, and V. Srikumar, editors,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics(Volume 1: Long Papers), pages 1294112955, Bangkok, Thailand, Aug. 2024. Association forComputational Linguistics. X. Du, M. Liu, K. Wang, H. Wang, J. Liu, Y. Chen, J. Feng, C. Sha, X. Peng, and Y. Lou.Classeval: A manually-crafted benchmark for evaluating llms on class-level code generation.arXiv preprint arXiv:2308.01861, 2023. A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten,A. Yang, A. Fan, A. Goyal, A. Hartshorn, A. Yang, A. Mitra, A. Sravankumar, A. Korenev,A. Hinsvark, A. Rao, A. Zhang, A. Rodriguez, A. Gregerson, A. Spataru, B. Roziere, B. Biron,B. Tang, B. Chern, C. Caucheteux, C. Nayak, C. Bi, C. Marra, C. McConnell, C. Keller,C. Touret, C. Wu, C. Wong, C. C. Ferrer, C. Nikolaidis, D. Allonsius, D. Song, D. Pintz,D. Livshits, D. Esiobu, D. Choudhary, D. Mahajan, D. Garcia-Olano, D. Perino, D. Hupkes,E. Lakomkin, E. AlBadawy, E. Lobanova, E. Dinan, E. M. Smith, F. Radenovic, F. Zhang,G. Synnaeve, G. Lee, G. L. Anderson, G. Nail, G. Mialon, G. Pang, G. Cucurell, H. Nguyen,H. Korevaar, H. Xu, H. Touvron, I. Zarov, I. A. Ibarra, I. Kloumann, I. Misra, I. Evtimov,J. Copet, J. Lee, J. Geffert, J. Vranes, J. Park, J. Mahadeokar, J. Shah, J. van der Linde, J. Bil-lock, J. Hong, J. Lee, J. Fu, J. Chi, J. Huang, J. Liu, J. Wang, J. Yu, J. Bitton, J. Spisak, J. Park,J. Rocca, J. Johnstun, J. Saxe, J. Jia, K. V. Alwala, K. Upasani, K. Plawiak, K. Li, K. Heafield,K. Stone, K. El-Arini, K. Iyer, K. Malik, K. Chiu, K. Bhalla, L. Rantala-Yeary, L. van der Maaten, L. Chen, L. Tan, L. Jenkins, L. Martin, L. Madaan, L. Malo, L. Blecher, L. Landzaat,L. de Oliveira, M. Muzzi, M. Pasupuleti, M. Singh, M. Paluri, M. Kardas, M. Oldham, M. Rita,M. Pavlova, M. Kambadur, M. Lewis, M. Si, M. K. Singh, M. Hassan, N. Goyal, N. Torabi,N. Bashlykov, N. Bogoychev, N. Chatterji, O. Duchenne, O. elebi, P. Alrassy, P. Zhang, P. Li,P. Vasic, P. Weng, P. Bhargava, P. Dubal, P. Krishnan, P. S. Koura, P. Xu, Q. He, Q. Dong,R. Srinivasan, R. Ganapathy, R. Calderer, R. S. Cabral, R. Stojnic, R. Raileanu, R. Gird-har, R. Patel, R. Sauvestre, R. Polidoro, R. Sumbaly, R. Taylor, R. Silva, R. Hou, R. Wang,S. Hosseini, S. Chennabasappa, S. Singh, S. Bell, S. S. Kim, S. Edunov, S. Nie, S. Narang,S. Raparthy, S. Shen, S. Wan, S. Bhosale, S. Zhang, S. Vandenhende, S. Batra, S. Whitman,S. Sootla, S. Collot, S. Gururangan, S. Borodinsky, T. Herman, T. Fowler, T. Sheasha, T. Geor-giou, T. Scialom, T. Speckbacher, T. Mihaylov, T. Xiao, U. Karn, V. Goswami, V. Gupta,V. Ramanathan, V. Kerkez, V. Gonguet, V. Do, V. Vogeti, V. Petrovic, W. Chu, W. Xiong,W. Fu, W. Meers, X. Martinet, X. Wang, X. E. Tan, X. Xie, X. Jia, X. Wang, Y. Goldschlag,Y. Gaur, Y. Babaei, Y. Wen, Y. Song, Y. Zhang, Y. Li, Y. Mao, Z. D. Coudert, Z. Yan, Z. Chen,Z. Papakipos, A. Singh, A. Grattafiori, A. Jain, A. Kelsey, A. Shajnfeld, A. Gangidi, A. Victoria,A. Goldstand, A. Menon, A. Sharma, A. Boesenberg, A. Vaughan, A. Baevski, A. Feinstein,A. Kallet, A. Sangani, A. Yunus, A. Lupu, A. Alvarado, A. Caples, A. Gu, A. Ho, A. Poulton,A. Ryan, A. Ramchandani, A. Franco, A. Saraf, A. Chowdhury, A. Gabriel, A. Bharambe,A. Eisenman, A. Yazdan, B. James, B. Maurer, B. Leonhardi, B. Huang, B. Loyd, B. D. Paola,B. Paranjape, B. Liu, B. Wu, B. Ni, B. Hancock, B. Wasti, B. Spence, B. Stojkovic, B. Gamido,B. Montalvo, C. Parker, C. Burton, C. Mejia, C. Wang, C. Kim, C. Zhou, C. Hu, C.-H. Chu,C. Cai, C. Tindal, C. Feichtenhofer, D. Civin, D. Beaty, D. Kreymer, D. Li, D. Wyatt, D. Adkins,D. Xu, D. Testuggine, D. David, D. Parikh, D. Liskovich, D. Foss, D. Wang, D. Le, D. Holland,E. Dowling, E. Jamil, E. Montgomery, E. Presani, E. Hahn, E. Wood, E. Brinkman, E. Ar-caute, E. Dunbar, E. Smothers, F. Sun, F. Kreuk, F. Tian, F. Ozgenel, F. Caggioni, F. Guzmn,F. Kanayet, F. Seide, G. M. Florez, G. Schwarz, G. Badeer, G. Swee, G. Halpern, G. That-tai, G. Herman, G. Sizov, Guangyi, Zhang, G. Lakshminarayanan, H. Shojanazeri, H. Zou,H. Wang, H. Zha, H. Habeeb, H. Rudolph, H. Suk, H. Aspegren, H. Goldman, I. Damlaj,I. Molybog, I. Tufanov, I.-E. Veliche, I. Gat, J. Weissman, J. Geboski, J. Kohli, J. Asher,J.-B. Gaya, J. Marcus, J. Tang, J. Chan, J. Zhen, J. Reizenstein, J. Teboul, J. Zhong, J. Jin,J. Yang, J. Cummings, J. Carvill, J. Shepard, J. McPhie, J. Torres, J. Ginsburg, J. Wang, K. Wu,K. H. U, K. Saxena, K. Prasad, K. Khandelwal, K. Zand, K. Matosich, K. Veeraraghavan,K. Michelena, K. Li, K. Huang, K. Chawla, K. Lakhotia, K. Huang, L. Chen, L. Garg, L. A,L. Silva, L. Bell, L. Zhang, L. Guo, L. Yu, L. Moshkovich, L. Wehrstedt, M. Khabsa, M. Avalani,M. Bhatt, M. Tsimpoukelli, M. Mankus, M. Hasson, M. Lennie, M. Reso, M. Groshev, M. Nau-mov, M. Lathi, M. Keneally, M. L. Seltzer, M. Valko, M. Restrepo, M. Patel, M. Vyatskov,M. Samvelyan, M. Clark, M. Macey, M. Wang, M. J. Hermoso, M. Metanat, M. Rastegari,M. Bansal, N. Santhanam, N. Parks, N. White, N. Bawa, N. Singhal, N. Egebo, N. Usunier,N. P. Laptev, N. Dong, N. Zhang, N. Cheng, O. Chernoguz, O. Hart, O. Salpekar, O. Kalinli,P. Kent, P. Parekh, P. Saab, P. Balaji, P. Rittner, P. Bontrager, P. Roux, P. Dollar, P. Zvyagina,P. Ratanchandani, P. Yuvraj, Q. Liang, R. Alao, R. Rodriguez, R. Ayub, R. Murthy, R. Nayani,R. Mitra, R. Li, R. Hogan, R. Battey, R. Wang, R. Maheswari, R. Howes, R. Rinott, S. J. Bondu,S. Datta, S. Chugh, S. Hunt, S. Dhillon, S. Sidorov, S. Pan, S. Verma, S. Yamamoto, S. Ra-maswamy, S. Lindsay, S. Lindsay, S. Feng, S. Lin, S. C. Zha, S. Shankar, S. Zhang, S. Zhang,S. Wang, S. Agarwal, S. Sajuyigbe, S. Chintala, S. Max, S. Chen, S. Kehoe, S. Satterfield,S. Govindaprasad, S. Gupta, S. Cho, S. Virk, S. Subramanian, S. Choudhury, S. Goldman,T. Remez, T. Glaser, T. Best, T. Kohler, T. Robinson, T. Li, T. Zhang, T. Matthews, T. Chou,T. Shaked, V. Vontimitta, V. Ajayi, V. Montanez, V. Mohan, V. S. Kumar, V. Mangla, V. Albiero,V. Ionescu, V. Poenaru, V. T. Mihailescu, V. Ivanov, W. Li, W. Wang, W. Jiang, W. Bouaziz,W. Constable, X. Tang, X. Wang, X. Wu, X. Wang, X. Xia, X. Wu, X. Gao, Y. Chen, Y. Hu,Y. Jia, Y. Qi, Y. Li, Y. Zhang, Y. Zhang, Y. Adi, Y. Nam, Yu, Wang, Y. Hao, Y. Qian, Y. He,Z. Rait, Z. DeVito, Z. Rosnbrick, Z. Wen, Z. Yang, and Z. Zhao. The llama 3 herd of models,2024.",
  "Google. Generative ai terms of service, 8 2023. Accessed: August 17, 2023": "S. Gunasekar, Y. Zhang, J. Aneja, C. C. T. Mendes, A. D. Giorno, S. Gopi, M. Javaheripi,P. Kauffmann, G. de Rosa, O. Saarikivi, A. Salim, S. Shah, H. S. Behl, X. Wang, S. Bubeck,R. Eldan, A. T. Kalai, Y. T. Lee, and Y. Li. Textbooks are all you need, 2023. D. Guo, Q. Zhu, D. Yang, Z. Xie, K. Dong, W. Zhang, G. Chen, X. Bi, Y. Wu, Y. K. Li, F. Luo,Y. Xiong, and W. Liang. Deepseek-coder: When the large language model meets programming the rise of code intelligence, 2024.",
  "P. Haluptzok, M. Bowers, and A. T. Kalai. Language models can teach themselves to programbetter. In The Eleventh International Conference on Learning Representations, 2023": "N. Jain, K. Han, A. Gu, W.-D. Li, F. Yan, T. Zhang, S. Wang, A. Solar-Lezama, K. Sen, andI. Stoica. Livecodebench: Holistic and contamination free evaluation of large language modelsfor code. arXiv preprint arXiv:2403.07974, 2024. A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. de las Casas, F. Bressand,G. Lengyel, G. Lample, L. Saulnier, L. R. Lavaud, M.-A. Lachaux, P. Stock, T. L. Scao, T. Lavril,T. Wang, T. Lacroix, and W. E. Sayed. Mistral 7b, 2023. A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary, C. Bamford, D. S. Chaplot,D. de las Casas, E. B. Hanna, F. Bressand, G. Lengyel, G. Bour, G. Lample, L. R. Lavaud,L. Saulnier, M.-A. Lachaux, P. Stock, S. Subramanian, S. Yang, S. Antoniak, T. L. Scao,T. Gervet, T. Lavril, T. Wang, T. Lacroix, and W. E. Sayed. Mixtral of experts, 2024. N. Jiang, K. Liu, T. Lutellier, and L. Tan. Impact of code language models on automatedprogram repair. In 2023 IEEE/ACM 45th International Conference on Software Engineering(ICSE), pages 14301442. IEEE, 2023.",
  "C. E. Jimenez, J. Yang, A. Wettig, S. Yao, K. Pei, O. Press, and K. Narasimhan. Swe-bench:Can language models resolve real-world github issues?, 2023": "M. Jin, S. Shahriar, M. Tufano, X. Shi, S. Lu, N. Sundaresan, and A. Svyatkovskiy. Inferfix:End-to-end program repair with llms. In Proceedings of the 31st ACM Joint European SoftwareEngineering Conference and Symposium on the Foundations of Software Engineering, pages16461656, 2023. D. Kocetkov, R. Li, L. B. Allal, J. Li, C. Mou, C. M. Ferrandis, Y. Jernite, M. Mitchell, S. Hughes,T. Wolf, D. Bahdanau, L. von Werra, and H. de Vries. The stack: 3 tb of permissively licensedsource code, 2022. A. Kpf, Y. Kilcher, D. von Rtte, S. Anagnostidis, Z. R. Tam, K. Stevens, A. Barhoum, D. M.Nguyen, O. Stanley, R. Nagyfi, S. ES, S. Suri, D. A. Glushkov, A. V. Dantuluri, A. Maguire,C. Schuhmann, H. Nguyen, and A. J. Mattick. Openassistant conversations - democratizing largelanguage model alignment. In Thirty-seventh Conference on Neural Information ProcessingSystems Datasets and Benchmarks Track, 2023.",
  "Y. Lai, C. Li, Y. Wang, T. Zhang, R. Zhong, L. Zettlemoyer, S. W. tau Yih, D. Fried, S. Wang,and T. Yu. Ds-1000: A natural and reliable benchmark for data science code generation, 2022": "H. Le, Y. Wang, A. D. Gotmare, S. Savarese, and S. Hoi. CodeRL: Mastering code genera-tion through pretrained models and deep reinforcement learning. In A. H. Oh, A. Agarwal,D. Belgrave, and K. Cho, editors, Advances in Neural Information Processing Systems, 2022. C. Lemieux, J. P. Inala, S. K. Lahiri, and S. Sen. Codamosa: Escaping coverage plateaus intest generation with pre-trained large language models. In 2023 IEEE/ACM 45th InternationalConference on Software Engineering (ICSE), pages 919931. IEEE, 2023. R. Li, L. B. Allal, Y. Zi, N. Muennighoff, D. Kocetkov, C. Mou, M. Marone, C. Akiki, J. Li,J. Chim, Q. Liu, E. Zheltonozhskii, T. Y. Zhuo, T. Wang, O. Dehaene, M. Davaadorj, J. Lamy-Poirier, J. Monteiro, O. Shliazhko, N. Gontier, N. Meade, A. Zebaze, M.-H. Yee, L. K. Umapathi,J. Zhu, B. Lipkin, M. Oblokulov, Z. Wang, R. Murthy, J. Stillerman, S. S. Patel, D. Abulkhanov,M. Zocca, M. Dey, Z. Zhang, N. Fahmy, U. Bhattacharyya, W. Yu, S. Singh, S. Luccioni,P. Villegas, M. Kunakov, F. Zhdanov, M. Romero, T. Lee, N. Timor, J. Ding, C. Schlesinger,H. Schoelkopf, J. Ebert, T. Dao, M. Mishra, A. Gu, J. Robinson, C. J. Anderson, B. Dolan-Gavitt,D. Contractor, S. Reddy, D. Fried, D. Bahdanau, Y. Jernite, C. M. Ferrandis, S. Hughes, T. Wolf,A. Guha, L. von Werra, and H. de Vries. Starcoder: may the source be with you!, 2023. X. Li, P. Yu, C. Zhou, T. Schick, O. Levy, L. Zettlemoyer, J. E. Weston, and M. Lewis. Self-alignment with instruction backtranslation. In The Twelfth International Conference on LearningRepresentations, 2024. X. Li, P. Yu, C. Zhou, T. Schick, O. Levy, L. Zettlemoyer, J. E. Weston, and M. Lewis. Self-alignment with instruction backtranslation. In The Twelfth International Conference on LearningRepresentations, 2024.",
  "OpenAI. Chatgpt: Optimizing language models for dialogue. 2022": "OpenAI. Gpt-4 technical report, 2023. OpenAI. Terms of service, 3 2023. Accessed: August 17, 2023. OpenAI. Gpt-4o system card. 2024. L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal,K. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder,P. Christiano, J. Leike, and R. Lowe. Training language models to follow instructions withhuman feedback, 2022. R. Pan, A. R. Ibrahimzada, R. Krishna, D. Sankar, L. P. Wassi, M. Merler, B. Sobolev, R. Pavu-luri, S. Sinha, and R. Jabbarvand. Understanding the effectiveness of large language models incode translation. arXiv preprint arXiv:2308.03109, 2023.",
  "A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever. Improving language understandingby generative pre-training. 2018": "R. Rafailov, A. Sharma, E. Mitchell, C. D. Manning, S. Ermon, and C. Finn. Direct preferenceoptimization: Your language model is secretly a reward model. In Thirty-seventh Conferenceon Neural Information Processing Systems, 2023. S. Rajbhandari, J. Rasley, O. Ruwase, and Y. He. Zero: memory optimizations toward trainingtrillion parameter models. In Proceedings of the International Conference for High PerformanceComputing, Networking, Storage and Analysis, SC 20. IEEE Press, 2020.",
  "B. Roziere, M.-A. Lachaux, L. Chanussot, and G. Lample. Unsupervised translation of pro-gramming languages. Advances in neural information processing systems, 33:2060120611,2020": "B. Rozire, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y. Adi, J. Liu, T. Remez,J. Rapin, A. Kozhevnikov, I. Evtimov, J. Bitton, M. Bhatt, C. C. Ferrer, A. Grattafiori, W. Xiong,A. Dfossez, J. Copet, F. Azhar, H. Touvron, L. Martin, N. Usunier, T. Scialom, and G. Synnaeve.Code llama: Open foundation models for code, 2023. N. Shazeer and M. Stern. Adafactor: Adaptive learning rates with sublinear memory cost, 2018. A. Shypula, A. Madaan, Y. Zeng, U. Alon, J. Gardner, M. Hashemi, G. Neubig, P. Ranganathan,O. Bastani, and A. Yazdanbakhsh. Learning performance-improving code edits. arXiv preprintarXiv:2302.07867, 2023. Z. Sun, Y. Shen, H. Zhang, Q. Zhou, Z. Chen, D. D. Cox, Y. Yang, and C. Gan. SALMON:Self-alignment with instructable reward models. In The Twelfth International Conference onLearning Representations, 2024. Z. Sun, Y. Shen, Q. Zhou, H. Zhang, Z. Chen, D. D. Cox, Y. Yang, and C. Gan. Principle-driven self-alignment of language models from scratch with minimal human supervision. InThirty-seventh Conference on Neural Information Processing Systems, 2023. C. Team, H. Zhao, J. Hui, J. Howland, N. Nguyen, S. Zuo, A. Hu, C. A. Choquette-Choo,J. Shen, J. Kelley, K. Bansal, L. Vilnis, M. Wirth, P. Michel, P. Choy, P. Joshi, R. Kumar,S. Hashmi, S. Agrawal, Z. Gong, J. Fine, T. Warkentin, A. J. Hartman, B. Ni, K. Korevec,K. Schaefer, and S. Huffman. Codegemma: Open code models based on gemma, 2024.",
  "G. Team. Gemini: A family of highly capable multimodal models, 2024. Q. Team. Code with codeqwen1.5, April 16 2024. Accessed: 2024-05-20. theblackcat102. The evolved code alpaca dataset. 2023": "H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra,P. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. C. Ferrer, M. Chen, G. Cucurull, D. Esiobu,J. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini,R. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura, M.-A.Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra,I. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M.Smith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan,I. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, andT. Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023. X. Wang, B. Li, Y. Song, F. F. Xu, X. Tang, M. Zhuge, J. Pan, Y. Song, B. Li, J. Singh, H. H.Tran, F. Li, R. Ma, M. Zheng, B. Qian, Y. Shao, N. Muennighoff, Y. Zhang, B. Hui, J. Lin,R. Brennan, H. Peng, H. Ji, and G. Neubig. Openhands: An open platform for ai softwaredevelopers as generalist agents, 2024. Y. Wang, Y. Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi, and H. Hajishirzi. Self-instruct: Aligning language models with self-generated instructions. In A. Rogers, J. Boyd-Graber, and N. Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers), pages 1348413508, Toronto, Canada,July 2023. Association for Computational Linguistics.",
  "Y. Wang, H. Le, A. D. Gotmare, N. D. Q. Bui, J. Li, and S. C. H. Hoi. Codet5+: Open codelarge language models for code understanding and generation, 2023": "Y. Wang, W. Wang, S. Joty, and S. C. Hoi. CodeT5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation. In M.-F. Moens, X. Huang, L. Specia,and S. W.-t. Yih, editors, Proceedings of the 2021 Conference on Empirical Methods in NaturalLanguage Processing, pages 86968708, Online and Punta Cana, Dominican Republic, Nov.2021. Association for Computational Linguistics.",
  "Y. Wei, Z. Wang, J. Liu, Y. Ding, and L. Zhang. Magicoder: Source code is all you need. arXivpreprint arXiv:2312.02120, 2023": "Y. Wei, C. S. Xia, and L. Zhang. Copiloting the copilots: Fusing large language models withcompletion engines for automated program repair. In Proceedings of the 31st ACM JointEuropean Software Engineering Conference and Symposium on the Foundations of SoftwareEngineering, ESEC/FSE 2023, page 172184, New York, NY, USA, 2023. Association forComputing Machinery.",
  "T. Zheng, G. Zhang, T. Shen, X. Liu, B. Y. Lin, J. Fu, W. Chen, and X. Yue. Opencodeinterpreter:Integrating code generation with execution and refinement. arXiv preprint arXiv:2402.14658,2024": "T. Y. Zhuo, M. C. Vu, J. Chim, H. Hu, W. Yu, R. Widyasari, I. N. B. Yusuf, H. Zhan, J. He, I. Paul,S. Brunner, C. Gong, T. Hoang, A. R. Zebaze, X. Hong, W.-D. Li, J. Kaddour, M. Xu, Z. Zhang,P. Yadav, N. Jain, A. Gu, Z. Cheng, J. Liu, Q. Liu, Z. Wang, D. Lo, B. Hui, N. Muennighoff,D. Fried, X. Du, H. de Vries, and L. V. Werra. Bigcodebench: Benchmarking code generationwith diverse function calls and complex instructions, 2024.",
  "AStarCoder2-Instruct: Fully Transparent and Permissive Self-Alignment forCode Generation": "StarCoder2-Instruct is the very first entirely self-aligned code LLM created with an earlier versionof SelfCodeAlign. The pipeline uses StarCoder2-15B to generate thousands of instruction-responsepairs, which are then used to finetune StarCoder2-15B itself without any human annotations ordistilled data from huge and proprietary LLMs. StarCoder2-15B-Instruct achieves a 72.6 HumanEvalscore, surpassing the 72.0 score of CodeLlama-70B-Instruct. More details are explained in the blog:",
  "Listing 2: The Tree-sitter query utilized for extracting Python functions with docstrings": "Our seed gathering process starts off by extracting all Python functions with docstrings from TheStack v1, which is a large dataset of code from GitHub. We accomplish this by utilizing the Tree-sitterparser and the query language it provides. Listing 2 provides the query utilized for matching eachfunction, assuring that the functions live at the top-level of the program, and that they indeed containa docstring. Utilizing this query, we extracted a total of 5,359,051 Python functions with docstrings.Interestingly, we found that roughly only 20% of Python functions contain any docstring.",
  "B.2Quality Filtering and Transformations": "After gathering our 5M Python functions, we apply a series of filtering and transformations steps.After all of our filtering rules, we are left with a dataset of 248,934 high-quality Python functions.These steps are a generalization of the dataset building pipeline in MultiPL-T , which only managesto produce half as many functions (those without imports). We detail each step of this process below. Import predictionBy naively extracting the functions from Python files, we may have lost importstatements of external libraries that are utilized inside the function. To remedy this, we utilize theautoimport library to infer potential import statements for unbound identifiers in the function. Removing benchmark dataTo enable a fair evaluation of our method, we decontaminate our seeddataset from examples that resemble prompts and solutions of items in the benchmarks on which weevaluate on. We accomplish this by checking if either the substring of the solution or prompt to eachbenchmark item exists in any function in the dataset. Return filteringTo aid in our self-validation step, we aim to include only functions that return avalue, such that potential responses will contain test cases with complex expected values. We utilizeTree-sitter to filter any function that does not contain at least one return statement with an argumentvalue. Type-checkingTo further ensure the quality of our Python functions, we apply Pyright, a heuristictype-checker for Python, on all of our functions, and keep only ones passing the check. This step alsoensures that no unbound identifiers are referenced inside the function. Docstring quality filteringWe find that several Python functions, while having defined a doc-string, contain poor or misleading documentation. In aims of removing such functions, we employStarCoder2-15B with a simple binary classification prompt, tasking the model to detect functionswith poor docstrings. We remove all functions that were deemed poor quality by this classifier. Near-deduplicationAs a final step, we wish to increase the diversity of seeds in our dataset.To accomplish this, we utilize MinHash with Locality-Sensitive Hashing and a Jaccard Similaritythreshold of 0.5 to identify duplicate groups of functions in our seed dataset. We then only pick asingle function from each group, and add it to our final dataset. We note that this is the same processutilized to deduplicate the pre-training dataset of StarCoder and StarCoder2 .",
  "We implement 21 few-shot examples of the form (seed, property, instruction, response, tests),where coding concepts are encoded in the property of each example. Besides coding concepts and": "programming language, a property includes a task category and a difficulty level that are randomlysampled during data generation. We use eight-shot for concept and instruction generation, and one-shot for response generation. During response generation, we explicitly guide the model to generatetests by concatenating the response and tests in the one-shot example. For the main experiment, ifthe test case follows a specified format, we additionally include it in the instruction body with a fiftypercent chance to boost diversity. shows the estimated cost for end-to-end data generationwith different models.",
  "C.3Training": "Our overall hyperparameter choices are derived from existing good practices . Weset the initial learning rate at 1e-5 for training on self-generated data and 2e-5 for training on datagenerated from other models. Empirically, we find this to be the optimal setting for both cases. Weadopt a 0.05 warmup ratio and a linear scheduler. We use Adafactor as our optimizer and choosea batch size of 64 with a sequence truncation length of 1280.",
  "C.4Computater Resources": "We primarily conduct data generation, training, and evaluation on a node equipped with 4 NVIDIAA100 PCI-E GPUs, 128 cores, and 512 GB of memory. For experiments involving DeepSeek-Coder,we use a node with 8 NVIDIA H100 GPUs. For DeepSeek-Coder, we utilize DeepSpeed ZeRO-3 for training. For StarCoder2-15B, we use one A100 for training since otherwise it cannot fit theGPU memory due to the extra overhead caused by inter-GPU communication. For all the otherexperiments, we do a 4-GPU training using PyTorchs Distributed Data Parallel (DDP) module.",
  "The answer NA means that the paper has no limitation while the answer No means thatthe paper has limitations, but those are not discussed in the paper": "The authors are encouraged to create a separate \"Limitations\" section in their paper. The paper should point out any strong assumptions and how robust the results are toviolations of these assumptions (e.g., independence assumptions, noiseless settings, modelwell-specification, asymptotic approximations only holding locally). The authors shouldreflect on how these assumptions might be violated in practice and what the implicationswould be. The authors should reflect on the scope of the claims made, e.g., if the approach was onlytested on a few datasets or with a few runs. In general, empirical results often depend onimplicit assumptions, which should be articulated. The authors should reflect on the factors that influence the performance of the approach.For example, a facial recognition algorithm may perform poorly when image resolution islow or images are taken in low lighting. Or a speech-to-text system might not be usedreliably to provide closed captions for online lectures because it fails to handle technicaljargon.",
  "If applicable, the authors should discuss possible limitations of their approach to addressproblems of privacy and fairness": "While the authors might fear that complete honesty about limitations might be used byreviewers as grounds for rejection, a worse outcome might be that reviewers discoverlimitations that arent acknowledged in the paper. The authors should use their bestjudgment and recognize that individual actions in favor of transparency play an importantrole in developing norms that preserve the integrity of the community. Reviewers will bespecifically instructed to not penalize honesty concerning limitations.",
  "Answer: [Yes]": "Justification: In addition to the general technique in 2, we detailed our experimental config-urations of data generation, code execution, model training, and evaluation in Appendix C,such as the temperature, maximum length of newly generated tokens, etc. We have includeda pipeline to reproduce the model training and evaluation steps in the supplemental materialand will also open-source it.",
  "If the contribution is a dataset and/or model, the authors should describe the steps taken tomake their results reproducible or verifiable": "Depending on the contribution, reproducibility can be accomplished in various ways.For example, if the contribution is a novel architecture, describing the architecture fullymight suffice, or if the contribution is a specific model and empirical evaluation, it may benecessary to either make it possible for others to replicate the model with the same dataset,or provide access to the model. In general. releasing code and data is often one goodway to accomplish this, but reproducibility can also be provided via detailed instructionsfor how to replicate the results, access to a hosted model (e.g., in the case of a largelanguage model), releasing of a model checkpoint, or other means that are appropriate tothe research performed. While NeurIPS does not require releasing code, the conference does require all submis-sions to provide some reasonable avenue for reproducibility, which may depend on thenature of the contribution. For example(a) If the contribution is primarily a new algorithm, the paper should make it clear how toreproduce that algorithm.",
  "(b) If the contribution is primarily a new model architecture, the paper should describethe architecture clearly and fully": "(c) If the contribution is a new model (e.g., a large language model), then there shouldeither be a way to access this model for reproducing the results or a way to reproducethe model (e.g., with an open-source dataset or instructions for how to construct thedataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authorsare welcome to describe the particular way they provide for reproducibility. In thecase of closed-source models, it may be that access to the model is limited in someway (e.g., to registered users), but it should be possible for other researchers to havesome path to reproducing or verifying the results.",
  ". Open access to data and code": "Question: Does the paper provide open access to the data and code, with sufficient instruc-tions to faithfully reproduce the main experimental results, as described in supplementalmaterial?Answer: [Yes] Justification: We have included the source code, data, and corresponding instructions to usethe artifact in the supplemental material.Guidelines: The answer NA means that paper does not include experiments requiring code. Please see the NeurIPS code and data submission guidelines ( for more details. While we encourage the release of code and data, we understand that this might not bepossible, so No is an acceptable answer. Papers cannot be rejected simply for notincluding code, unless this is central to the contribution (e.g., for a new open-sourcebenchmark).",
  ". Experimental Setting/Details": "Question: Does the paper specify all the training and test details (e.g., data splits, hyper-parameters, how they were chosen, type of optimizer, etc.) necessary to understand theresults?Answer: [Yes]Justification: We detailed the configurations and rationales for model finetuning in Ap-pendix C, including data amounts, hyperparameters, optimizers, etc.Guidelines: The answer NA means that the paper does not include experiments. The experimental setting should be presented in the core of the paper to a level of detailthat is necessary to appreciate the results and make sense of them.",
  ". Experiment Statistical Significance": "Question: Does the paper report error bars suitably and correctly defined or other appropriateinformation about the statistical significance of the experiments?Answer: [No]Justification: Admittedly, we did not draw error bars for all evaluations as we managedto align our experimental settings with prior work. However, except for DS-1000 in and CanItEdit in , all other evaluations use greedy decoding to compute pass@1, making the results in theory deterministic. pass@1 is commonly used in codeLLM papers as it assumes in code completion most users would either accept or reject acompletion in one shot.Guidelines: The answer NA means that the paper does not include experiments. The authors should answer \"Yes\" if the results are accompanied by error bars, confidenceintervals, or statistical significance tests, at least for the experiments that support the mainclaims of the paper. The factors of variability that the error bars are capturing should be clearly stated (forexample, train/test split, initialization, random drawing of some parameter, or overall runwith given experimental conditions).",
  ". Experiments Compute Resources": "Question: For each experiment, does the paper provide sufficient information on the com-puter resources (type of compute workers, memory, time of execution) needed to reproducethe experiments?Answer: [Yes]Justification: We reported our compute configurations in Appendix C.Guidelines: The answer NA means that the paper does not include experiments. The paper should indicate the type of compute workers CPU or GPU, internal cluster, orcloud provider, including relevant memory and storage.",
  ". Code Of Ethics": "Question: Does the research conducted in the paper conform, in every respect, with theNeurIPS Code of Ethics [Yes]Justification: We have read the NeurIPS Code of Ethics and believe our work does notviolate the terms.Guidelines: The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. If the authors answer No, they should explain the special circumstances that require adeviation from the Code of Ethics.",
  "Justification: Our technique is neutral in not implying clear positive or negative impacts onsociety": "The answer NA means that there is no societal impact of the work performed. If the authors answer NA or No, they should explain why their work has no societal impactor why the paper does not address societal impact. Examples of negative societal impacts include potential malicious or unintended uses(e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g.,deployment of technologies that could make decisions that unfairly impact specific groups),privacy considerations, and security considerations. The conference expects that many papers will be foundational research and not tied toparticular applications, let alone deployments. However, if there is a direct path to anynegative applications, the authors should point it out. For example, it is legitimate to pointout that an improvement in the quality of generative models could be used to generatedeepfakes for disinformation. On the other hand, it is not needed to point out that ageneric algorithm for optimizing neural networks could enable people to train models thatgenerate Deepfakes faster. The authors should consider possible harms that could arise when the technology is beingused as intended and functioning correctly, harms that could arise when the technology isbeing used as intended but gives incorrect results, and harms following from (intentionalor unintentional) misuse of the technology. If there are negative societal impacts, the authors could also discuss possible mitigationstrategies (e.g., gated release of models, providing defenses in addition to attacks, mecha-nisms for monitoring misuse, mechanisms to monitor how a system learns from feedbackover time, improving the efficiency and accessibility of ML).",
  "Guidelines:": "The answer NA means that the paper does not use existing assets. The authors should cite the original paper that produced the code package or dataset. The authors should state which version of the asset is used and, if possible, include a URL. The name of the license (e.g., CC-BY 4.0) should be included for each asset. For scraped data from a particular source (e.g., website), the copyright and terms of serviceof that source should be provided. If assets are released, the license, copyright information, and terms of use in the packageshould be provided. For popular datasets, paperswithcode.com/datasets has curatedlicenses for some datasets. Their licensing guide can help determine the license of adataset.",
  ". Institutional Review Board (IRB) Approvals or Equivalent for Research with HumanSubjects": "Question: Does the paper describe potential risks incurred by study participants, whethersuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)approvals (or an equivalent approval/review based on the requirements of your country orinstitution) were obtained?Answer: [NA]Justification: This work does not involve crowdsourcing nor research with human subjects.Guidelines: The answer NA means that the paper does not involve crowdsourcing nor research withhuman subjects. Depending on the country in which research is conducted, IRB approval (or equivalent)may be required for any human subjects research. If you obtained IRB approval, youshould clearly state this in the paper. We recognize that the procedures for this may vary significantly between institutionsand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and theguidelines for their institution."
}