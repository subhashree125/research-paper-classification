{
  "Abstract": "We introduce a novel approach for detecting distribution shifts that negativelyimpact the performance of machine learning models in continuous productionenvironments, which requires no access to ground truth data labels. It builds uponthe work of Podkopaev and Ramdas , who address scenarios where labels areavailable for tracking model errors over time. Our solution extends this frameworkto work in the absence of labels, by employing a proxy for the true error. This proxyis derived using the predictions of a trained error estimator. Experiments show thatour method has high power and false alarm control under various distribution shifts,including covariate and label shifts and natural shifts over geography and time.",
  "Introduction": "When deploying a machine learning model in production, it is common to encounter changes inthe data distribution, such as shifts in covariates [Shimodaira, 2000], labels [Saerens et al., 2002,Lipton et al., 2018] or concepts [Gonalves Jr et al., 2014]. Many methods exist for detecting suchdistribution shifts. However, a distinct but equally important challenge is assessing whether a shift hasa harmful impact on the prediction error of a given model, which may necessitate interventions suchas ceasing production or retraining the model. Not all distribution shifts are harmful, but traditionalmethods for shift detection are unable to distinguish harmful and benign shifts. While some approaches address the specific issue of performance shift, most require access to groundtruth data labels in the production environment [Gama et al., 2013, 2014, Bayram et al., 2022]. Inscenarios where predictions concern future outcomes, such as medical diagnosis or credit scoring,immediate access to labels in production is not feasible. This work focuses on the challenge ofdetecting harmful distribution shifts those that increase model error in production withoutrequiring access to labels. As Trivedi et al. note, current methods for harmful shift detectionwithout labels rely on disparate heuristics, often lacking a solid theoretical foundation. Such methodsinclude proxies based on aggregate dataset-level statistics [Deng and Zheng, 2021], optimal transportmappings between training and production distributions [Koebler et al., 2023], and model-specificmetrics such as input margins [Mouton et al., 2023], perturbation-sensitivity [Ng et al., 2023],disagreement-metrics [Chen et al., 2023, Ginsberg et al., 2022], and prediction confidence [Guilloryet al., 2021, Garg et al., 2022]. While such methods may be practically effective in certain contexts,they rely on assumptions and correlations that do not hold universally, so can provide no guarantees. Furthermore, conventional methods rely on two-sample or batch testing, which involves comparingthe statistical properties of a production dataset with those of a control sample. These methods haveinherent limitations, as the sample size is prespecified. This is a problem because the necessaryamount of data to detect any given shift is unknown beforehand. Furthermore, in real-world scenarios,data typically arrive sequentially over time and shifts may occur either suddenly or gradually. In suchscenarios, it may be desirable to detect harmful shifts as early as possible. Batch testing is ill-suited tothe sequential context [Maharaj et al., 2023], as it does not accommodate the collection of additionaldata for retesting without adjusting for multiple testing, leading to diminished power.",
  "arXiv:2412.12910v1 [stat.ML] 17 Dec 2024": "The most principled and relevant work to our problem is that of Podkopaev and Ramdas ,which tackles the problem of sequential harmful shift detection with false alarm control but assumesthe availability of ground truth labels in production. Our work builds on the foundation establishedby Podkopaev and Ramdas , extending the methodology to detect harmful shifts in unlabeledproduction data while effectively managing false alarms. Our approach leverages a secondary model to estimate the errors of the primary model. Whilelearning such a model might seem challenging at first, consider a situation where the primary modelperforms well overall but struggles with specific data subgroups. Sagawa et al. demonstratethat this phenomenon can occur in natural distributions. In such cases, learning to predict errorgiven X might be easier than the primary task of predicting Y given X, because the error estimatoronly needs to identify those subgroups where the primary model struggles. This approach has shownpromise in recent studies [Zrnic and Cands, 2024, Amoukou and Brunel, 2023]. More generally,Zrnic and Cands note that predicting the magnitude of the error, rather than its direction,is often easier. Furthermore, our approach is based on estimating the proportion of high-errorobservations over time. For this task, the error estimator does not need to be very accurate; it onlyneeds to assign higher values to observations with higher errors. That is, the estimator only needs tocorrectly order most observations from low to high error, which is easier than precisely predictingthe error itself. We demonstrate in .1 that even a relatively inaccurate error estimator canbe effective at identifying high-error observations, and thus provides the functionality required byour framework. Although this paper uses a learned error estimators predictions as a proxy for error,we note that any scalar function correlated with error could suffice to isolate high-error observations.For example, for a well-calibrated binary classification model, we could instead use that modelspredicted probability, tracking observations with predictions near 0.5 to identify uncertain predictions.",
  "ALARM": ": Overview of the proposed approach.Left: calibrating an estimated error threshold toseparate low/high true errors. Right: sequentiallytracking production data exceeding the thresholdand raising an alarm upon a significant increase. gives an overview of our approach. Wefirst fit the secondary error estimator model topredict the error of the primary model, thenuse labeled data to calibrate an estimated er-ror threshold (---) that separates observationswith low (x) and high (x) true error as fullyas possible. We run the error estimator on allobservations encountered in production and con-tinually monitor the proportion of observationswhose estimated error falls above the threshold.We raise an alarm when this exceeds the rateof high-error observations (x) in the calibrationset plus a tolerance threshold tol and correctionterms to deal with the sequential setting and ac-count for uncertainty in the estimates. In theexample shown, this occurs at time t = 10. The rest of this paper is organized as follows. outlines the problem definition and provides an overview of the foundational work of Podkopaev and Ramdas as background. is dedicated to the presentation and theoretical analysis of our sequential statistical test. Sec-tion 5 demonstrates the empirical efficacy of our method, showcasing its strong detection capabilitiesand controlled false alarm rates across various types of harmful shift.",
  "Problem Definition": "Let X and Y be input and label spaces, f : X Y be a predictive model, and : Y2 E be ameasurable and bounded error function that is selected for monitoring purposes. The models erroron a specific observation (X, Y ) X Y, drawn from a joint distribution P(X,Y ) = PXPY |X, isrepresented by the random variable E = (f(X), Y ). The probability distribution of the error isdenoted by PE. As discussed above, our focus is not on detecting shifts in covariates or labels per se,but rather changes in the error distribution PE. Error changes can be caused by various types of shiftin the underlying joint distribution, including changes in PX while the conditional label distributionPY |X remains constant (covariate shift) or changes in PY while PX|Y remains constant (label shift). We assume access to a dataset Dn = {(X0i , Y 0i )}ni=1, sampled independently from a source distribu-tion P 0(X,Y ). In addition, we have a sequence of data (Xt, Yt)t1 drawn independently from a time- varying distribution encountered by the model in production, P t(X,Y ). We model the ocurrence of ashift in production by assuming this distribution is equal to the source before some time T N {}(i.e., P t(X,Y ) = P 0(X,Y ), t < T) and different thereafter (i.e., P t(X,Y ) = P 0(X,Y ), t T). Our goal whenever there is a shift, (i.e., T < ), is to decide if this shift is harmful to the model error.To formalize this, we introduce : P(E) R+ as a mapping from probability distributions on theerror space E to a real-valued parameter. This mapping could, for instance, map the distribution to itsmean or a certain quantile. We aim to construct a sequential test for the following pair of hypotheses:",
  "where P kE denotes the error distribution at at time k, the running risk 1": "ttk=1 (P kE) is the averagevalue of the error parameter up to time t, and tol 0 is a tolerance level. Intuitively, H0 holds if therunning risk remains below that of the source distribution (+tol) for all time throughout production,and H1 holds if this condition is violated. Objective. Construct a -level sequential test, defined by an alarm function : k=1X k {0, 1},which at time t uses the first t observations X1, . . . , Xt to output 0 (no harmful shift so far) or 1(harmful shift; raise an alarm) with a controlled false alarm rate and high power, i.e.,",
  "SHSD with Production Labels": "A work closely related to ours is that of Podkopaev and Ramdas , which offers a solution forscenarios where the ground truth labels of the production data are available. This method leveragesconfidence sequences [Darling and Robbins, 1967, Jennison and Turnbull, 1984, Johari et al., 2015,Jamieson and Jain, 2018], which are time-uniform (i.e., valid for any time) confidence intervals,allowing for the ongoing monitoring of any bounded random variable. With access to labels, it ispossible to calculate the true errors on the production data over time and monitor the running risk.Choosing the mean as the error parameter i.e. (P kE) = EP k[E], Podkopaev and Ramdas use the empirical production errors E1 = (f(X1), Y1), . . . , Et = (f(Xt), Yt), to construct aconfidence sequence lower bound L for the running risk, satisfying a chosen miscoverage levelprod (0, 1):",
  "Pt 1,1ttk=1(P kE) L(E1, . . . , Et) 1 prod.(4)": "This equation guarantees that the lower bound remains valid over time with high probability. Fur-thermore, given the errors on the source data E01 = (f(X01), Y 01 ), . . . , E0n = (f(X0n), Y 0n ), eitheranother confidence sequence or a traditional confidence interval method [Howard et al., 2021, Waudby-Smith and Ramdas, 2020] can be used to construct a fixed-time upper confidence bound U for themean error (P 0E). For a miscoverage level source (0, 1), U satisfies the following condition:",
  "Sequential Harmful Shift Detection without Production Labels": "This section consists of two subsections, each detailing one of the two stages of our proposal. Theinitial stage consists of fitting an error estimator and calibrating it to identify high-error observationswith few mistakes. Following this, we apply confidence sequence methods to track the proportion ofhigh errors over time in production, and develop a test for raising an alarm based on this proportion.",
  "Fitting and Calibrating the Error Estimator": "The primary drawback of the Podkopaev and Ramdas method is its reliance on havingground truth labels for the production data, which are often unavailable in real-world scenarios. Astraightforward solution is to use a plug-in approach: replace the true error in production with anestimated error obtained from a secondary predictive model, denoted as r : X E. This modeltrained to predict the true error of the primary model using any available labeled data. We can thenreformulate the alarm function of Equation 6 to deal with unlabeled production data as follows:",
  "m(X1, . . . , Xt) = 1L(r(X1), . . . , r(Xt)) > U(E01, . . . , E0n) + tol(8)": "If r() is sufficiently accurate, the performance of this alarm mechanism should align closely withwhat would be achieved if ground truth labels were available. However, even if the estimator rexhibits strong performance on its training distribution, the absence of labels in production makes itdifficult to conclusively determine the alarms reliability in a shifting production environment. Our strategy to address this issue consists of using a calibration step to derive a more reliable statisticfrom the imperfect estimator r(). Specifically, we propose to track the proportion of observationsabove a carefully-selected quantile of estimated error, rather than the mean value as in the originalmethod of Podkopaev and Ramdas . The fundamental hypothesis here is that an estimator, evenif not particularly accurate at predicting error magnitudes, may still effectively distinguish betweenthe lowest and highest errors across a dataset, thereby preserving most ordinal relationships betweenobservations. For example, if r() has correctly represented some underlying patterns to predict theerrors, and if k-th and l-th ranked errors are significantly different, then it is highly probable thatr(X(k)) r(X(l)). Focusing on the aggregate distinction of low and high errors rather than theprediction of specific magnitudes allows us to utilize an imperfect estimator r more effectively. Our proposed calibration process is as follows. Given the labeled source data Dn and a trained errorestimator r, we identify an empirical quantile of the true errors, q = Q(p, {E0i }ni=1), p [0.5, 1),and an empirical quantile for the estimated errors q = Q(p, {r(X0i ) : X0i Dn)}, p (0, 1), suchthat the selector function Sr,q(X) = 1{r(X) > q} reliably distinguishes between observations withtrue error below and above q. Specifically, we seek to balance the statistical power and false discoveryproportion (FDP) of the selector, which are defined as follows:",
  "Power =ni=1 Sr,q(X0i ) 1{E0i > q}ni=1 1{E0i > q};FDP =ni=1 Sr,q(X0i ) 1{E0i q}ni=1 Sr,q(X0i ).(9)": ": Calibration toy example. Left: threshold grid created bysweeping p [0.5, 0.95] at increments of 0.05 and p [0.1, 0.9]at increments of 0.1. Middle: FDP of selector for each (p, p)pair. Black outline indicates pairs for which FDP < 0.2. Right:selector power for each (p, p) pair. Green dotted outline indicatesthe pair that maximises power subject to the FDP < 0.2 limit.Corresponding thresholds (q, q) shown as thick lines in left plot. We search over a uniform gridof quantile pairs (p, p), computethe associated thresholds (q, q),and identify those that achievean FDP below a maximum value.Among these qualifying pairs,we select the one that maximizesthe power. illustratesthis process for a toy example.In this case, thresholds are foundthat achieve a selector power of0.72 while keeping FDP belowthe specified maximum of 0.2.",
  "We now present empirical ev-idence that it is possible toachieve high power and a con-trolled FDP in realistic settings,": "using the California house prices [Dua and Graff, 2017], Bike sharing demand [Fanaee-T, 2013],HELOC [FICO, 2018] and Nhanesi [CDC, 1999-2022] datasets. We partition each dataset intotraining (60%), test (20%) and calibration (20%) sets and use the training data to train randomforests (RFs) as the primary models. However, we first ablate the training data in various ways toensure the models perform poorly on certain subgroups. The ablation is done on a per-feature basis.For continuous features, we exclude 80% of observations with values either above or below themedian. For categorical features, we exclude data from one category. We then simulate productionenvironments by gradually reintroducing these previously excluded observations alongside the testset. For each dataset, the number of distribution shifts studied equals the number of features times thenumber of splits: two for continuous features and the number of categories for discrete ones. We usehalf of the calibration sets to train RF regressors as the error estimators, then use the remainder tocalibrate true and estimated error thresholds using the grid search process described above.",
  ": Selector FDP (left) and power (right) vsestimator accuracy. Results on source data in blue;results on production data in red": "In , we present the distribution of theFDP and power across all datasets and shifts,relative to the performance of the error estima-tor, as measured by the R-squared score on thesource/calibration data. The R-squared score isbinned into quantiles, with 10 bins used. Weobserve that the estimators are generally highlyimperfect, with R-squared values consistentlybelow 0.3. Despite these low predictive accura-cies, we can still find threshold pairs that achievean FDP below 0.2 in the source data (shown nextto the red boxplot). The power ranges from 0.4for the least accurate estimators to 0.9 for themost accurate. Crucially, when we apply the calibrated thresholds in the production environments, weachieve similarly low FDP values (shown in red), almost always below 0.25 (though some reach 0.4),while the power remains similar to the source data, ranging from 0.4 to 0.9. This consistency of theFDP/power even when error estimators are not particularly accurate is promising for shift detection.",
  "ttk=1PP k(E > q) > PP 0(E > q) + tol,(11)": "where PP k denotes a probability taken under distribution P k. Note that this is a special case of thegeneral test in Equations 1 and 2, with the probability (P kE) = PP k(E > q) as the error parameter. Since we cannot observe production errors directly, we use the selector function Sr,q(X) as a proxyfor a check on the true error E > q. The effectiveness of the sequential test under this substitutiondepends on how well the selectors power and FDP properties generalize from the source distributionto the production environment. In particular, we can show that the method outlined below provablycontrols the false alarm rate given in Equation 3 if the following assumption holds:",
  "ttk=1 PP k (Sr,q(X) = 1, E q) PP 0 (Sr,q(X) = 1, E q)": "Referring back to the example in (left), this assumption implies that at all times duringproduction, the proportion of data observed so far falling the quadrant above and to the left of thecalibrated thresholds (---) does not exceed that observed under the source distribution. While we donot claim that this assumption always holds exactly, we find that it is only violated to a small extent inrealistic settings (see Appendix A for more discussion and experimental analysis). If this is the case,and thresholds (q, q) have been found that yield a small number of false discoveries in calibration,PP 0 (Sr,q(X) = 1, E q), then the number of false discoveries in production will also remain low.A substantial increase in false discoveries in production would require a shift specifically targetingthose rare observations with low error but high estimated error.",
  "ttk=1 PP k(Sr,q(X) = 1) PP 0(Sr,q(X) = 1, E q).(15)": "The last inequality uses Assumption 4.1 to substitute the probability of a false discovery in productionwith the probability on the source. As we can empirically estimate both PP k(Sr,q(X) = 1) andPP 0(Sr,q(X) = 1, E q) (via the labeled source data Dn), we can use a confidence sequence toconstruct a valid time-uniform lower bound of their sum. Specifically, we define the bound Lq asLq = 1",
  "PH0 (t 1 : q(X1, . . . , Xt) = 1) source + prod.(21)": "While a controlled false alarm rate is a desirable property, the power of q may be limited ifthe degree of error change is not large. Noting that (1/t) tk=1 PP k(E > q) is lower-boundedby (1/t) tk=1 PP k(Sr,q(X) = 1, E > q), detecting a change requires this probability to exceedPP 0(E > q). Thus, we also propose to compare 1",
  "Experiments": "In this section, we compare the performance of the plug-in approach of Podkopaev and Ramdass method (Equation 8), which is designed to detect a change in the mean error, and ourapproach, which determines if an increasing number of observations fall beyond a certain quantile.We focus on the second test (Equation 24) to simplify the comparison with the mean detector andbecause it consistently outperforms the first statistics. Results for the first test are reported in AppendixE. We conduct three experiments using a variety of datasets and setups. The first experiment aimsto illustrate the different approaches and demonstrate the applicability of our method to image dataand deep learning models. The second experiment returns to the tabular datasets studied in .1, going into more detail by comparing the mean and quantile detection approaches in terms ofpower and FDP on the numerous generated shifts. The final experiment also consists of a large-scaleevaluation of the approaches, in this case on natural shifts due to temporal and geographical changes.Although the focus of this paper is on the sequential or online setting, we provide an analysis usingstate-of-the-art methods in the batch setting in Appendix F.",
  "Illustrative Example on an Image Dataset": "The first experiment replicates the setup of Saerens et al. using the CelebA dataset [Liu et al.,2015]. They demonstrate that a ResNet50 model [He et al., 2016] trained on this dataset performspoorly on males with blond hair due to spurious correlations. We split this dataset into a training set(60%), test set (20%) and calibration set (20%), and train a ResNet50 on the training set. Using halfof the calibration set, we train another ResNet50 (with a regression head) as an error estimator. Theremaining half is employed to determine the empirical quantiles p [0.5, 1), p (0, 1) at which weachieve maximum power while keeping the FDP below 0.2. We create a harmful shift in productionas follows. For each time step up to t = 4990, we sample an observation uniform-randomly from thetest set. Thereafter, we begin to oversample instances of males with blond hair, sampling such anobservation with probability t = 1/(1 + exp((t 4990))), and a random observation otherwise. The objective of this experiment is to visually observe how the methods can be used to monitorperformance shift over time and to evaluate how each method compares to an idealised version withaccess to true production errors. Both Podkopaev and Ramdas s method (mean detector)and our approach (quantile detector) involve comparing a lower bound to an upper bound. For bothmethods, displays the lower bound in blue and the version calculated with true productionerrors in gray. For the quantile detector, the blue line corresponds to Lq of Equation 16, whichis the estimated lower bound of 1 ttk=1 PP k(E > q) with estimated production errors. The grayline represents the lower bound of this same quantity, except computed using the true errors. Theblue lower bound of the plug-in approach of the mean detector is defined as L(r(X1), . . . , r(Xt)).The gray line represents L(E1, . . . , Et), the lower bound of the original mean detector using trueerrors. The upper bound that needs to be surpassed for each method to raise an alarm is depictedin red. For the quantile detector, this is the second lower bound L2q, and for the mean detector, it isU(r(X1), . . . , r(Xt)). For the quantile detector, we also plot in pink the upper bound of the firststatistic q (Equation 20).",
  ": Evolution of bounds in production for mean detector (left) and quantile detector (right)": "The R-squared value of the error estimator on the source distribution is 0.35, which is not especiallyhigh. By analyzing the upper bounds for each method in , we observe that all bounds remainroughly constant before the shift starts. Unsurprisingly, we observe the mean detector using true production errors quickly detects the shift (gray line). In contrast, its plug-in version raises an alarmwith a significantly delayed detection. For the quantile detector, there is a much smaller differencebetween the lower bound of the plug-in and the one using true production errors. This observationvalidates our expectation that the FDP remains relatively stable post-shift. Additionally, as expected,the plot shows that the lower bound of the quantile detector crosses the upper bound of the secondstatistic (red line) much earlier than that of the first statistic (pink line). This experiment suggests that in scenarios with a less accurate error estimator, targeting quantilechanges is more effective for detecting harmful shifts than focusing on mean change. Additionalexperiments on image datasets confirming this observation can be found in Appendix D. Larger-scaleanalyses in the following subsections examine the advantages of the quantile detector in more depth.",
  "Synthetic Shifts on Tabular Datasets": "In this section, we conduct a large-scale experiment to evaluate the effectiveness of both methodsin detecting harmful shifts while maintaining their ability to control false alarms. We also analyzehow these metrics relate to the performance of the error estimator. We use two regression datasets(California house prices and bike sharing demand) as well as two classification datasets (HELOC andNhanesi). We follow the feature-splitting setup of .1 to generate synthetic distribution shifts,excluding splits that result in subsets with fewer than 10 observations, and repeat each split 50 timeswith different random seeds.",
  "california621048bike212957961heloc33857741283nhanesi1697377679": "shows the number of generated shiftsand the number of harmful shifts detected byeach method using the true errors (H-M formean detector and H-Q for quantile detector).A shift is considered harmful by each methodas soon as the lower bound exceeds the upperbound plus tol = 0. The left plot of displays the aggregated results across all distribution shifts for mean detection(red) and quantile detection (green) on the different datasets. The points labeled all-[method]represent the average results across the datasets. The quantile method achieves a significantly betterpower-FDP balance: (power 0.83, FDP 0.11) compared to the mean method: (power 0.67, FDP 0.41)across all experiments. An exception is observed for the Nhanesi dataset, where the mean detectionshows slightly better power. However, overall, the quantile detection demonstrates a superior trade-offbetween power and false alarms. A similar trend is observed in the middle plot, which analyzesthe absolute difference in detection time between each method using estimated errors and the samemethod with access to true errors. In the right plot, we compute how the power across datasets varieswhen we increase the threshold at which we consider the true shift as harmful (tol). Across varyingintensities of shift, the quantile detector consistently outperforms the mean detector, with false alarmrates at tol = 0 being 0.41 and 0.11, respectively.",
  ": Left: Power/FDP when tol = 0 for all datasets. Middle: Absolute detection timedifference vs. the methods using true errors. Right: Power values for different harmfulness thresholds(tol)": "In , we further investigate the relationship between the power (top row) and FDP (bottom row)of each method and the error estimators performance binned into 10 quantiles for each dataset. Theerror estimator performance, measured by R-squared values, is generally low across all experiments(0.10 - 0.26). Notably, the quantile detector consistently maintains a lower FDP compared to the",
  "Natural Shifts on a Tabular Dataset": "In the last experiment, we conduct another large-scale evaluation of our approach on natural shiftswithin the Folktables dataset [Ding et al., 2021]. This dataset is derived from US Census data spanningall fifty states within the US (plus Puerto Rico), each with a unique data distribution. Furthermore, itincludes data from multiple years (from 2014 to 2018), introducing a form of temporal distributionshift in addition to the variations between states. We select the income feature as the target label,specifically predicting whether income exceeds $50, 000. We first split the dataset of each state inthe year 2014 into training (50%), and calibration (50%). Then, we train a separate RF classifierin each state in the year 2014, and an RF regressor to learn the error of the primary model on thecalibration set. Subsequently, we evaluate the models error on all the remaining 50 states over 5years, effectively creating 250 production datasets. We consider a shift to be harmful if the modelserror in production exceeds the error on the calibration dataset plus tol = 0. We introduce the shift inall datasets starting at time t = 3300.",
  "Conclusion": "We have introduced an approach to identifying harmful distribution shifts in continuous productionenvironments where ground truth labels are unavailable. Utilizing a plug-in strategy that substitutestrue errors with estimated errors, alongside a threshold calibration step, our method effectivelycontrols false alarms without relying on perfect error predictions. Experiments on real-world datasetsdemonstrate that our approach is effective in terms of detection power, false alarm control anddetection time across various shifts, including covariate, label, and temporal shifts. In future work,we plan to apply interpretability techniques to the quantile detector to understand where and how thedata are shifting in the input space, and to use this information to improve the primary model itself.",
  "Disclaimer": "This paper was prepared for informational purposes by the Artificial Intelligence Research group of JPMorganChase & Co and its affiliates (J.P. Morgan) and is not a product of the Research Department of J.P. Morgan.J.P. Morgan makes no representation and warranty whatsoever and disclaims all liability, for the completeness,accuracy or reliability of the information contained herein. This document is not intended as investment researchor investment advice, or a recommendation, offer or solicitation for the purchase or sale of any security, financialinstrument, financial product or service, or to be used in any way for evaluating the merits of participating in anytransaction, and shall not constitute a solicitation under any jurisdiction or to any person, if such solicitationunder such jurisdiction or to such person would be unlawful.",
  "Paulo M Gonalves Jr, Silas GT de Carvalho Santos, Roberto SM Barros, and Davi CL Vieira. Acomparative study on concept drift detectors. Expert Systems with Applications, 41(18):81448156,2014": "Devin Guillory, Vaishaal Shankar, Sayna Ebrahimi, Trevor Darrell, and Ludwig Schmidt. Predictingwith Confidence on Unseen Distributions. In 2021 IEEE/CVF International Conference onComputer Vision (ICCV), pages 11141124, Montreal, QC, Canada, October 2021. IEEE. ISBN978-1-66542-812-5. doi: 10.1109/ICCV48922.2021.00117. URL Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for imagerecognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,pages 770778, 2016.",
  ": Distribution of across the different shifts and datasets of .2 (a) and the naturaldistribution shifts of .3 (b)": "We observe that Assumption 4.1 is valid approximately 50% of the time for both experimental setups,corresponding to the cases where is negative. In the other half of the cases where the assumption isnot verified, we note that is very small, often less than 0.01. It should be noted that Assumption 4.1 allows for controlling false alarms when is zero or negative.To control false alarms when is positive, it is sufficient to always add to the lower bound Lq(Eq. 16) to have the false alarm guarantee. Specifically, under the assumption that for all t 1,1ttk=1 PP k (Sr,q(X) = 1, E q) PP 0 (Sr,q(X) = 1, E q) + we can show, similar toTheorem 4.2, that the corrected bounds Lcorrq= Lq used in the following statistic:",
  "DAdditional Experiments on Image Datasets": "Here, we conduct two experiments using image datasets, specifically CIFAR-10 [Krizhevsky et al.]and Fashion MNIST [Xiao et al., 2017]. Similar to previous experiments, we remove some partof the data during training phase, here 90% of the observations with label 3 for both datasets, andreintroduce them gradually during the production phase. In , we observe the same behavior as in .1. The quantile detector detects changesmore quickly than the mean detector, and the performance of the former is closer to the true versionthan that of the latter.",
  "EComparison Between q and 2q": "In this section, we will revisit the main experiments from sections 5.2 and 5.3, incorporatingcomparisons with the quantile detector using the first statistic q. As expected, in figure 10, weconsistently observe that the first statistic q achieves a better FDP than the second statistic 2q at thecost of a much smaller power. In addition, q fails to detect any shift in the California dataset andhas a much higher detection time.",
  "FEvaluations in Batch Setting": "In this section, we compare our approach with a leading method, Detectron, proposed by Ginsberget al. , in a batch setting. Directly comparing our method (SHSD) to those in Ginsberg et al. presents challenges due to fundamental differences in their design. Their methods are tailoredfor an offline batch setup, which requires a complete batch of production data to compute statisticsand trigger alarms. In contrast, our approach is optimized for an online setting where shifts may occurgradually and continuously, necessitating real-time decisions without the ability to observe an entireunlabelled batch upfront. Our methodology is designed to detect harmful performance shifts on thefly, processing each observation sequentially without requiring access to the full production dataset. Applying offline methods like Detectron in an online setting would be both impractical and unfair, asthese methods rely on training a model or computing statistics from a batch of data. Additionally, itwould be computationally expensive since Detectron requires training a new model for each batch ofproduction data. Consequently, deploying this approach online would entail training a number ofmodels proportional to the production data size. To provide a meaningful comparison, we evaluated our method alongside Detectron in a batch setting,progressively increasing the size of the production or out-of-distribution (OOD) data. We generatedshifts in line with the setup described in .2, ensuring no shift within the first 1300 samples of production data. We utilized the NHANESI classification dataset, as Detectron is specificallydesigned for classification tasks. Our experiments were replicated 50 times, yielding a total of 10,200shift instances.",
  "N/A1.00N/A0.001000N/A1.00N/A0.0020000.960.610.400.0230000.980.600.630.0235000.980.600.670.0285930.980.600.740.04": "The results, summarized in , demonstrate that for smaller sample sizes (100 and 1000), ourmethod did not detect any shifts, as expected given the lack of shifts in the initial 1300 samples.However, Detectron raised a significant number of alarms (1126 out of 1700 for sample size 100 and1493 for sample size 1000), all of which were false alarms. For larger sample sizes, while Detectronshows high power in detecting shifts, it also produces a high false discovery proportion (FDP). Incontrast, our method exhibits lower power but significantly better control over false alarms, consistentwith our objective of minimizing false positives.",
  ": Illustration of a Disagreement-Based Detector Failure Case": "The primary concept behind Detectron is to train a disagreement classifier that performs comparably tothe original model on the training distribution while disagreeing with the original models predictionson production data. This approach is highly sensitive to the base models performance, the choice offunction class, and the size and nature of the production data. Although Detectron shows high powerin detecting harmful shifts (as evidenced by our experiments), it may raise false alarms when the shiftis benign. In , we illustrate a failure case for the disagreement-based detector. In this example, trainingdata points are represented in red and green, with the ground truth shaded accordingly. The solid blackline represents the decision boundary of a base model, which we assume to be a perfect classifier.The data has shifted to the right, resulting in unlabeled production data that is still correctly classifiedby the base model. Weve also depicted the potential learnable classifier as a dashed line, representing the boundary of allpossible functions, which depends on the model type, complexity used for the disagreement classifier,and the nature and size of the data. We have shown a potential disagreement classifier in orange thatperforms similarly to the original model on training data but disagrees with the predictions of the baseclassifier in the production data. As shown, even with a benign shift, we can still find a disagreementclassifier that performs well on training data but disagrees significantly in production, raising a falsealarm.",
  "HImpact Statement": "This research, focusing on developing algorithms to detect harmful distribution shifts in machinelearning models, has significant and diverse practical impacts. It offers a solution to a key challengein the safe deployment of AI across various industries by detecting shifts without needing labeleddata. For instance, in healthcare, the ability to identify harmful shifts in predictive models enhancesthe accuracy and reliability of diagnostic tools, which is especially vital as patient data continuouslychanges due to new diseases or demographic shifts. In finance, the algorithms can detect markettrends or consumer behaviour changes that might negatively impact forecasting models, leadingto more adaptive and resilient economic models, improved risk management, and better-informeddecision-making processes.",
  ". Limitations": "Question: Does the paper discuss the limitations of the work performed by the authors?Answer: [Yes]Justification: On lines 40-51, we discussed the potential difficulty of learning a secondestimator and provided rationale and examples of when it is possible. We presented theassumptions under which our method should have false alarm control and discussed itsvalidity in practice in appendix A.Guidelines:",
  "The answer NA means that the paper has no limitation while the answer No means thatthe paper has limitations, but those are not discussed in the paper": "The authors are encouraged to create a separate \"Limitations\" section in their paper. The paper should point out any strong assumptions and how robust the results are toviolations of these assumptions (e.g., independence assumptions, noiseless settings,model well-specification, asymptotic approximations only holding locally). The authorsshould reflect on how these assumptions might be violated in practice and what theimplications would be. The authors should reflect on the scope of the claims made, e.g., if the approach wasonly tested on a few datasets or with a few runs. In general, empirical results oftendepend on implicit assumptions, which should be articulated. The authors should reflect on the factors that influence the performance of the approach.For example, a facial recognition algorithm may perform poorly when image resolutionis low or images are taken in low lighting. Or a speech-to-text system might not beused reliably to provide closed captions for online lectures because it fails to handletechnical jargon.",
  "If applicable, the authors should discuss possible limitations of their approach toaddress problems of privacy and fairness": "While the authors might fear that complete honesty about limitations might be used byreviewers as grounds for rejection, a worse outcome might be that reviewers discoverlimitations that arent acknowledged in the paper. The authors should use their bestjudgment and recognize that individual actions in favor of transparency play an impor-tant role in developing norms that preserve the integrity of the community. Reviewerswill be specifically instructed to not penalize honesty concerning limitations.",
  ". Experimental Result Reproducibility": "Question: Does the paper fully disclose all the information needed to reproduce the main ex-perimental results of the paper to the extent that it affects the main claims and/or conclusionsof the paper (regardless of whether the code and data are provided or not)? Answer: [Yes]Justification: We fully disclose all the information needed to reproduce the main experi-mental results, but we also plan to release the code to use the methods and replicate theexperiments.",
  "If the contribution is a dataset and/or model, the authors should describe the steps takento make their results reproducible or verifiable": "Depending on the contribution, reproducibility can be accomplished in various ways.For example, if the contribution is a novel architecture, describing the architecture fullymight suffice, or if the contribution is a specific model and empirical evaluation, it maybe necessary to either make it possible for others to replicate the model with the samedataset, or provide access to the model. In general. releasing code and data is oftenone good way to accomplish this, but reproducibility can also be provided via detailedinstructions for how to replicate the results, access to a hosted model (e.g., in the caseof a large language model), releasing of a model checkpoint, or other means that areappropriate to the research performed. While NeurIPS does not require releasing code, the conference does require all submis-sions to provide some reasonable avenue for reproducibility, which may depend on thenature of the contribution. For example(a) If the contribution is primarily a new algorithm, the paper should make it clear howto reproduce that algorithm.",
  "(b) If the contribution is primarily a new model architecture, the paper should describethe architecture clearly and fully": "(c) If the contribution is a new model (e.g., a large language model), then there shouldeither be a way to access this model for reproducing the results or a way to reproducethe model (e.g., with an open-source dataset or instructions for how to constructthe dataset). (d) We recognize that reproducibility may be tricky in some cases, in which caseauthors are welcome to describe the particular way they provide for reproducibility.In the case of closed-source models, it may be that access to the model is limited insome way (e.g., to registered users), but it should be possible for other researchersto have some path to reproducing or verifying the results.",
  "Guidelines:": "The answer NA means that the paper does not include experiments. The authors should answer \"Yes\" if the results are accompanied by error bars, confi-dence intervals, or statistical significance tests, at least for the experiments that supportthe main claims of the paper. The factors of variability that the error bars are capturing should be clearly stated (forexample, train/test split, initialization, random drawing of some parameter, or overallrun with given experimental conditions).",
  ". Experiments Compute Resources": "Question: For each experiment, does the paper provide sufficient information on the com-puter resources (type of compute workers, memory, time of execution) needed to reproducethe experiments?Answer: [Yes]Justification: We have provided in Section G the type of compute workers we used.Guidelines: The answer NA means that the paper does not include experiments. The paper should indicate the type of compute workers CPU or GPU, internal cluster,or cloud provider, including relevant memory and storage.",
  ". Broader Impacts": "Question: Does the paper discuss both potential positive societal impacts and negativesocietal impacts of the work performed?Answer: [Yes]Justification: We discuss broader impacts in Section H.Guidelines: The answer NA means that there is no societal impact of the work performed. If the authors answer NA or No, they should explain why their work has no societalimpact or why the paper does not address societal impact. Examples of negative societal impacts include potential malicious or unintended uses(e.g., disinformation, generating fake profiles, surveillance), fairness considerations(e.g., deployment of technologies that could make decisions that unfairly impact specificgroups), privacy considerations, and security considerations. The conference expects that many papers will be foundational research and not tiedto particular applications, let alone deployments. However, if there is a direct path toany negative applications, the authors should point it out. For example, it is legitimateto point out that an improvement in the quality of generative models could be used togenerate deepfakes for disinformation. On the other hand, it is not needed to point outthat a generic algorithm for optimizing neural networks could enable people to trainmodels that generate Deepfakes faster. The authors should consider possible harms that could arise when the technology isbeing used as intended and functioning correctly, harms that could arise when thetechnology is being used as intended but gives incorrect results, and harms followingfrom (intentional or unintentional) misuse of the technology. If there are negative societal impacts, the authors could also discuss possible mitigationstrategies (e.g., gated release of models, providing defenses in addition to attacks,mechanisms for monitoring misuse, mechanisms to monitor how a system learns fromfeedback over time, improving the efficiency and accessibility of ML).",
  ". Safeguards": "Question: Does the paper describe safeguards that have been put in place for responsiblerelease of data or models that have a high risk for misuse (e.g., pretrained language models,image generators, or scraped datasets)?Answer: [NA]Justification: [NA]Guidelines: The answer NA means that the paper poses no such risks. Released models that have a high risk for misuse or dual-use should be released withnecessary safeguards to allow for controlled use of the model, for example by requiringthat users adhere to usage guidelines or restrictions to access the model or implementingsafety filters.",
  ". Licenses for existing assets": "Question: Are the creators or original owners of assets (e.g., code, data, models), used inthe paper, properly credited and are the license and terms of use explicitly mentioned andproperly respected?Answer: [Yes]Justification: [NA]Guidelines: The answer NA means that the paper does not use existing assets. The authors should cite the original paper that produced the code package or dataset. The authors should state which version of the asset is used and, if possible, include aURL. The name of the license (e.g., CC-BY 4.0) should be included for each asset. For scraped data from a particular source (e.g., website), the copyright and terms ofservice of that source should be provided. If assets are released, the license, copyright information, and terms of use in thepackage should be provided. For popular datasets, paperswithcode.com/datasetshas curated licenses for some datasets. Their licensing guide can help determine thelicense of a dataset.",
  ". New Assets": "Question: Are new assets introduced in the paper well documented and is the documentationprovided alongside the assets?Answer: [Yes]Justification: We plan to release a well-documented code.Guidelines: The answer NA means that the paper does not release new assets. Researchers should communicate the details of the dataset/code/model as part of theirsubmissions via structured templates. This includes details about training, license,limitations, etc.",
  "According to the NeurIPS Code of Ethics, workers involved in data collection, curation,or other labor should be paid at least the minimum wage in the country of the datacollector": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with HumanSubjectsQuestion: Does the paper describe potential risks incurred by study participants, whethersuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)approvals (or an equivalent approval/review based on the requirements of your country orinstitution) were obtained?Answer: [NA]Justification: [NA]Guidelines:",
  "The answer NA means that the paper does not involve crowdsourcing nor research withhuman subjects": "Depending on the country in which research is conducted, IRB approval (or equivalent)may be required for any human subjects research. If you obtained IRB approval, youshould clearly state this in the paper. We recognize that the procedures for this may vary significantly between institutionsand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and theguidelines for their institution."
}