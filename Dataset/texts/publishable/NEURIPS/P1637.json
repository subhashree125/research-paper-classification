{
  "Abstract": "The crucial role of convolutional models, both as standalone vision models andbackbones in foundation models, necessitates effective acceleration techniques.This paper proposes a novel method to learn semi-structured sparsity patterns forconvolution kernels in the form of maskings enabling the utilization of readilyavailable hardware accelerations. The approach accelerates convolutional modelsmore than two-fold during inference without decreasing model performance. Atthe same time, the original model weights and structure remain unchanged keepingthe model thus easily updatable. Beyond the immediate practical use, the effect ofmaskings on prediction is easily quantifiable. Therefore, guarantees on model pre-dictions under maskings are derived showing stability bounds for learned maskingseven after updating the original underlying model.1",
  "Introduction": "The increasing complexity of deep learning models , their deployment in applications , andthe adoption of reflection incurring several inference passes per query, e.g., as in the O1 models fromthe GPT family , shifts the relative amounts of resources spent during the model lifetime from thetraining to the inference stage . It therefore becomes imperative to make models more efficient. One way of achieving this is by spending a comparatively small, additional share of resourcesduring training to learn a one-time modification of the model that lowers the models inferenceand thus lifetime cost . First and foremost, such a modification is effective if it decreasesthe models computational and time cost at a relatively low additional training overhead while notaffecting the prediction performance of the model negatively . Additionally, there are otherdesirable properties of such one-time modifications: From an application perspective the achievablegain in efficiency is only useful if it can be leveraged easily, a well-known challenge, e.g., withsparsifying models . Taking into consideration the increasing popularity of large, expensiveto train, foundation models or models employed in an online setting subject to continuousupdates the proposed change should not affect the possibility to update the model, e.g., by changingthe weights or architecture underlying the model. Ideally, if such a model is updated, the learnedmodification can even be reused under the constraint of the magnitude of change imposed by updatingthe model. Semi-structured sparse maskings satisfy the above properties by replacing the dense matrix operationsusually required during inference by cheaper and faster operations on semi-structured sparse matrices. While many works have demonstrated that sparse (pruned) submodels can solve the same task atalmost no loss of performance the sparsity of the models does not necessarily have to adhereto a specific pattern making it difficult to turn theoretically obtained computational speedups bysaving on data loading and computational operations into practical efficiency gains . Regular",
  "arXiv:2411.00288v1 [cs.LG] 1 Nov 2024": "patterns are more machine-friendly inducing the desired efficiency a priori but limiting the choicesfor the sparse patterns, which thus need to be chosen carefully with the goal of minimizing the loss ofinference performance in mind. This paper proposes a novel method of learning regularly sparse masking patterns for convolutions,key building blocks for state-of-the art Computer Vision (CV) models and foundation modelsbuilding on CV models as their backbone . The proposed method",
  "shows how to effectively use readily available hardware accelerations for semi-structuredsparse matrices in convolution kernels to accelerate inference,": "outperforms available heuristics for semi-structured sparsity showing that semi-structuredsparsity masks can be learned with a fraction of the original training resources whileincurring a negligible performance loss in CV classification tasks, provides the additional advantage of not changing the original set of trained weights keepingmodels updatable and rendering the method especially attractive for use in large models,e.g., foundation models and in online settings,",
  "induces an easily quantifiable change to the models prediction behavior and thus lends itselfto settings where hard guarantees on model predictions are of interest": "In the following section the adoption of semi-structured sparsity and sparsity in convolutionalmodels are addressed. of the paper covers modeling semi-structured sparsity in general, inconvolutional models, and the theoretical implications of such model alterations in inference. Theresults of empirically testing the method on widely used convolutional architectures are presented in followed up by a discussion of the method presented and a conclusion.",
  "Related Work": "In the following the notion and adoption of semi-structured sparsity is introduced. Then, theimplications on prediction performance and the computational challenges of sparsifying ConvolutionalNeural Networks (CNNs) are highlighted. Semi-Structured SparsitySemi-structured sparsity to accelerate network inference has beenintroduced in as N:M-sparsity requiring N out of M elements of a contiguous block to be zero(s. a. 3.1). Beyond the general case of N:M sparsity, the practically interesting special case of 2:4sparsity has been considered in more detail in which exactly half of the weights are prunedas illustrated in . This setting enables hardware acceleration via NVIDIA sparse tensor coresavailable from the NVIDIA Ampere architecture on via the TensorRT v8.0 library . Since half theelements are zeroed out and thus negligible, the amount of data to load from memory is almost halvedwith the number of Floating Point Operations (FLOPs) needed to conduct an operation on the sparsematrix also decreasing, e.g., linearly for addition and element-wise operations and super-linearly formultiplication, decomposition etc. . This way 2:4 sparse matrix operations compute the sameeffective operation while reducing the time needed by a factor of two . The difficulty in turninga dense matrix into a 2:4 sparse matrix, however, lies in selecting the most useful two of the fourweights in each quadruple. To this end propose a permutation regime that allows for preservingthe weights based on magnitude and assess the found pattern via a scoring mechanism, the efficacyscore. The functionality is available via NVIDIAs Apex library . Notably, pruning via Apexrequires finetuning the network again after pruning to achieve an inference performance comparableto that of the dense network in CV tasks, e.g., classification , and therefore changes theoriginal pretrained weights. Sparsity in CNNsThe state-of-the-art performance of CNNs in image-based and other tasks comesat the cost of a large memory footprint and computational cost. Pruning to obtain sparse networks istherefore a popular technique to decrease the computational and storage cost of deep neural networks. Pruning techniques include magnitude-based pruning , iterative pruning , and dynamicpruning . Although theoretically any sparsity reduces the computational costs of such networks,irregularity in the sparsity patterns makes it difficult to map the required computations to (parallel)processor operations . Even extremely high levels of (irregular) sparsity, i.e., > 97%, often yieldno inference acceleration suffering from lack of library support . As visualized in , in : A 2:4 sparse matrix of floating point values obtained from a dense matrix and a sparse bitmask and its equivalent structured representation containing only the non-zero entries and a 2-bitindex preserving the structure taking up roughly only half the space. the case of CNNs different granularities of sparsity emerge naturally with more regular patterns beingmore machine-friendly effectively inducing smaller, still dense models . Structured pruningapproaches pruning filters or even entire channels at once , however, quickly deteriorateprediction performance . This motivates semi-structured sparsity, a fine-grained yetstructured sparsity pattern, to maintain a large degree of freedom in the selection of sparsity patternsto not impede performance while also observing some (machine-)usable regular structure.",
  "Modeling Semi-Structured Sparsity": "N:M sparsity divides a matrix into non-overlapping blocks of M contiguous elements requiring thatN of these elements be zero as these elements can subsequently be ignored in many matrix operations,e.g., addition or multiplication. There are exactly n = (M N)!/N! ways of choosing N of theM elements without replacement and ignoring permutations, yielding n unique sparsity patterns.Selecting one of the n patterns can be modeled via a categorical variable z with class probabilities1, ..., n s.t. each probability denotes the probability of selecting the corresponding N:M sparsitypattern. Sampling the choice vector z, a n-dimensional one-hot vector on the simplex n1, fromsuch a categorical distribution can be performed efficiently via the Gumbel-Max trick",
  "{0, 1}hNwwhereb = Dz {0, 1}N, z Categorical(1, ..., n) (2)": "Since the arg max operator employed to sample z according to (1) is discrete and thus not differen-tiable this method cannot be used directly in a neural network to optimize the parameters (i)i[n]via backpropagation. Instead a differentiable approximation is constructed by replacing the arg maxoperator with a softmax. The choice vector z can now be drawn as follows",
  "k exp((gk + log k)/)(3)": "yielding a Gumbel-Softmax (GS) distribution over the n choices additionally parameterized bythe temperature parameter . While not identical to a categorical distribution, the GS distributionapproximates a categorical distribution over the choices for small temperature values, e.g., = 0.1.This distribution allows for expressing the gradient as a deterministic function of the choice weights and an independent source of random noise and thus gradient propagation through the stochastic node. By updating the class probabilities in the distribution in respect to the classification objectivethe choice weights can be optimized for the classification task and the optimal choice is selected.After convergence the choice weights are frozen and the inference bit mask is obtained by samplingthe blocks one final time from the GS distributions yielding the sparse bit mask. An element-wisemultiplication of the bit mask with the dense weight matrix results in the desired semi-structuredsparse matrix and, by extension, CNN.",
  "Effects of Maskings on Classifier Class Predictions": "Understanding how sparsity-inducing maskings affect a classifiers predictions is crucial to effectivelytrade off inference acceleration and potential performance inhibitions. The following Lemma containsthe classifier definition and states a useful property. This definition is used in all subsequent theoreticalresults and models the architectures considered for experiments closely. All proofs in this section aredeferred to Appendix A. Lemma 3.1. Let f(x) = (softmax fd ... f1)(x) be a compositional classifier of depth d withfi(x) = (Wix + bi) predicting the class probabilities of an input sample x X across c classes.Let be a non-linear element-wise activation function and L-Lipschitz. Then f(x) is Lf-Lipschitzwith Lf Ld",
  "i yi = 1, 0 yi 1 i into a classprediction. Given a prediction y Rc by a classifier f(x) on a sample x X let the confidence0 f(x) 1": "2 + , > 0 of a classifier be defined as the minimum change minRc s.t. y + is a valid probability vector and (y) = (y + ). Intuitively, this minimum change vector shiftsthe probability mass from the class with the highest probability to the class with the second highestprobability to change the discretized class prediction () with a minimal shift. Let W be any weightmatrix in a classifier and W an additive perturbation of the weights yielding W = W + W. Leta classifier be stable in respect to a perturbation and a given sample x if the perturbation doesntaffect the classifiers prediction on x, i.e., (fW (x)) = (fW (x)). Lemma 3.2. Let W be any weight matrix in a compositional (Lf-)Lipschitz classifier f(x) and Wan additive perturbation of the weights yielding W j = Wj + W and a perturbed compositionalclassifier f (x). Then f (x) is Lf -Lipschitz with Lf Lf + L W di=1,i=j Wi. A masking of a matrix W, i.e., zeroing out some (or all) of the entries, can be modeled as an element-wise product of the matrix W with a bit mask B. However, any masking can always equivalently bedescribed as an additive perturbation. Let (B, W) be the masking function yielding this additiveperturbation.",
  "Lemma 3.3. For any bit mask B and a matrix W the additive perturbation W = (B, W) s.t.W + W = B W always exists and fulfills W W. The bound is tight": "The model of a compositional classifier from Lemma 3.1 yields the following result guaranteeingstability of the classifier as a function of the perturbation and prediction confidence.Lemma 3.4. Let Wj be the j-th weight matrix in a compositional Lipschitz classifier f(x) and Wjan additive perturbation of the weights W j = Wj + Wj. Then the classifier is guaranteedto be stable in respect to such a perturbation and a sample x predicted with a confidence off(x) > Ld W x di=1,i=j Wi. Combining the statements made in Lemma 3.4 above with the reformulation of maskings of weightsas additive perturbations from Lemma 3.3 yields the following result left without proof.Lemma 3.5. Let Wj be the j-th weight matrix in a compositional Lipschitz classifier f(x) andWj = (B, W) an additive perturbation of the weights W j = Wj +Wj induced by any maskingB. Then the classifier is guaranteed to be stable in respect to such a perturbation and a sample xpredicted with a confidence of f(x) > Ld x di=1 Wi The looser bound in Lemma 3.5 addresses the general case in which any masking is considered andthus a worst case assumption. For a specific masking or a constrained class of maskings a tighterbound as in Lemma 3.4 can be derived. Lastly, consider the case in which one such specific maskinghas been learned for a classifier, which has since been updated, e.g., due to new data available.Applying the same masking to the updated classifier yields the following bound.Lemma 3.6. Let Wj be the j-th weight matrix in a compositional Lipschitz classifier f(x), Wj =(B, W) an additive perturbation of the weights W j = Wj + Wj induced by any maskingB yielding the masked classifier f (x). Let the update Uj be an additive perturbation of theweights Vj = Wj + Uj yielding the updated classifier fU(x). Then the masked and updatedclassifier f U(x) obtained from applying the mask B to the updated weight matrix Vj is guaranteedto be stable in respect to a sample x predicted with a confidence of f(x) > Ld (Wj +Uj) xdi=1,i=j Wi. Statements of the kind made above are always theoretical in nature and gauge worst case effects ofalterations of a model on the considered outcome. The weak, yet tight, bound on the norm of additiveperturbations obtained from masking weights propagates through subsequent statements and thuslimit the obtainable guarantees. As the results in the following section of the paper show, however,applying the proposed method yields highly promising results in application.Furthermore, the bounds obtained from above should be considered useful in two regards. Firstly,they yield an easily quantifiable estimate of what can still be guaranteed when applying the proposedmethod of introducing semi-structured sparsity to a model via maskings. In fact, commonly usedregularization techniques such as weight decay or norms on weights in loss functions directly yieldsmaller bounds on the norms of the mask-induced perturbations. Secondly, understanding in whatways sparse masks affect model performance can guide a practitioner to develop heuristics thatminimize the downside effect on (guaranteed) model performance, e.g., by specifically maskingweights of low magnitude and by bounding the maximum magnitude of weights to be masked.",
  "Semi-Structured Sparse Convolutions": "Since CV models commonly rely on two-dimensional convolutions to process the (image) inputs, theapplication of semi-structured sparsity on convolutions is illustrated in two dimensions. The methodextends to other dimensions in an analogue fashion. A discrete two-dimensional convolution witha kernel H Rcinhw convolves an input tensor X Rcinbd into an output tensor y Rbdassuming zero padding. In the below formulation the functions f and g handle the padding for invalidcombinations of input values, i.e., out of range values, else return the sum of the two input values:",
  "s=1HcusXcf(i,u)g(j,s)(4)": "Usually such a convolution is conducted with cout kernels to obtain an output Y Rcoutbd.Alternatively, this convolution can also be expressed as a matrix multiplication between the sameinput X Rcinbd and a weight matrix W Rcout(cinwh) constructed from the cout kernels in theconvolutional layer:Y = WU(X)(5) The unfold operator U() turns the matrix X into a flattened matrix X RcinwhL where L =(b + 2p1 w 1)(d + 2p2 h 1) denotes the number of blocks in the input. In the case ofzero padding, i.e., full padding, the padding sizes in the respective dimensions for uneven kerneldimensions are p1 = w",
  "and p2 = h": "2 and thus L = bd. Reshaping Y recovers the exact same Y asin (4). Note, that the described reformulation of (4) as (5) does not change the mathematical operationsconducted but rather makes the non-contiguous memory access of the convolution explicit. The costof the convolution (neglecting the details of data loading) therefore does not change. To achieve a2:4 sparse convolution compatible with the accelerated matrix multiplication for 2:4 sparse matricesa mask M {0, 1}coutcinwh whose (block) entries are sampled according to a GS distribution asdescribed in (2) is multiplied entry-wise to the weight matrix.",
  "Y = (M W) U(X)(6)": "The corresponding masking layer therefore learns as many GS distributions as there are blocks offour elements in the matrix. Note, that this assumes that the product cout cinwh is a multiple of four,since M can only contain a multiple of four entries to account for the size of the sparsity pattern. Ifthis is not the case the matrix needs to be augmented column- or row-wise to contain a multiple of 4entries. A schematic illustration of the two views on convolutions is illustrated in . : Simplified visualizations of convolutions on single channel input X of unspecified widthand height and a single filter H as (a) standard convolution with a moving filter and (b) as a matrixproduct between an unfolded input X and a weight matrix W derived from the filter",
  "Results": "In the following the architectures considered for experimental evaluation of the proposed methodand the empirical results are reported. The figures were obtained by evaluating the models on theImageNet-1K multi-class classification challenge . The details of the dataset and comparisonsbetween reported metrics on validation and test set performance are deferred to Appendix A.6. Thedetails of the training procedure employed to train the masking layers of the modified architecturesare reported in Appendix A.7. ArchitecturesTo empirically evaluate whether the proposed performance gain via semi-structuredsparsity can in fact be achieved without any significant loss in inference performance the followingarchitectures were considered: ResNet , and ConvNeXt . The models were obtained fromPyTorchs Torchvision module . In each variant of the models considered, the convolutionallayers, unless grouped, were reformulated as in (5) yielding a matrix product in which the weightsof k contiguous blocks of entries could be masked to yield a 2:4 sparse matrix as described in (6).To obtain the mask a trainable masking layer learning k GS distributions modeling the maskingchoices for the k blocks per layer was added from which the mask can be drawn. Since groupedconvolutions constitute sparse operations themselves, they induce highly sparse large weight matricesin a high-level reformulation as described in (5) and would have slowed down model training andinference beyond feasibility on the available hardware while only promising negligible efficiencygains at the same time. They were thus not altered. Likewise, linear dense layers were not consideredfor a modification of the above kind as they do not contribute to the compute cost of the modelssignificantly. E.g., in ResNet-50, linear layers account for only 0.3% of total FLOPs despiteaccounting for roughly 8% of the parameters of the model . This further shows, that the number of parameters itself is not a reliable measure of the computational cost incurred by a layer. Thepretrained weights of the original architectures were copied into the modified models initializingthe choice weights of the masking layers randomly using Glorot initialization and a normaldistribution with = 106 for weights and biases respectively. Only the weights associated with themasking layers were configured to be trainable leaving the set of pretrained weights unchanged. Inference Performance summarizes the results obtained for the ResNet and ConvNeXtarchitectures indicating the training effort needed to converge to a top-1 accuracy comparable to orbetter than the originally reported figures. The results show that both the ResNet-based as well asthe ConvNeXt-based architectures converged to the non-sparse performance levels with negligibleto no loss in performance, both in top-1 and top-5 accuracy. Convergence was reached for allarchitectures after training periods of a fraction of the length of the original training periods showingempirically that only a comparatively small share of additional resources is needed to learn theproposed efficiency modification. This neglects the fact that modern training recipes makeheavy use of augmentation diminishing the additionally spent share even further in comparison.Further training beyond convergence to the reported performance, reported in , showed furtherimprovement both in the top-1 and top-5 accuracy commonly beyond the reported performance forthe unmodified networks. : Validation classification performance of the 2:4 sparse networks measured as the top-kaccuracy on ImageNet-1K and the number of epochs needed to converge to a comparable orbetter top-1 accuracy than reported in contrast to the original number of training epochs",
  "ConvNeXt-T82.5296.1530085.6396.0930ConvNeXt-S83.6296.6530087.5396.7130": "To compare the results of the proposed method to a state of the art method of computing a 2:4 sparsesubnetwork two variants of the heuristic proposed in , the so-called efficacy score to evaluatepruning patterns, available via NVIDIAs Apex library were used in the same regime. To conformto the idea of not altering the original weights the networks were not retrained2 after pruning asoriginally proposed in and the results are aggregated in . As such, no significant computeresources needed to be spent. It can be observed that for all variants of ResNet and ConvNeXt the lossin performance is significant even in the better performing variant allowing for channel permutationsbefore selecting the 2:4 sparse subnetwork. The method proposed in this paper always manages tolearn a significantly better sparse pattern while spending less than a tenth of the resources.",
  "Discussion": "Spending only a fraction of the resources invested to pretrain the network the results show that itis possible to learn semi-structured 2:4 sparsity patterns that can accelerate CNN inference whilenot impeding or even improving classification performance. This shows that extending the nativesupport of 2:4 sparse matrix operations to 2:4 sparse convolutional kernels is a highly promisingavenue towards more efficiency that is achievable today. The results presented were obtained using a simple, generic training procedure. However, recentworks indicate the high relevance of the training recipe, which could go as far as being the sole reasonwhy (vision) transformers outperformed CNNs in image classification tasks in recent years . Theeffect of more sophisticated training procedures including, e.g., data augmentation ,needs to be studied offering potential to accelerate convergence and reaching even higher levels ofclassification performance. Furthermore, the proposed method does not yet make use of availableheuristics with patterns still being randomly initialized. In the case of, e.g., ConvNeXt, in which morethan one epoch was needed to converge, a meaningful initialization cheaply obtainable, e.g., ,could serve as an improved starting point and reduce lifetime resource spending even further. Lastly,while working exceptionally well, the work thus far only explores image classification. However,CV models are also frequently employed for object detection and segmentation tasks. Futureexperiments could be aimed at surveying all CV tasks relevant for CV models employed as backbonesin foundation models. From a theoretical viewpoint more assumptions, e.g., on the distribution ofweights could lead to more constrained yet tighter bounds. While losing some generality, this couldlead to results that are even more interesting from a practical viewpoint to guide effective trades offbetween inference acceleration and model performance. Beyond the aforementioned proposals for future work several additional avenues come into con-sideration: Firstly, the proposed architectural change introduces the temperature parameter ofthe GS distribution to the reformulated convolutional models, but the effects on performance andconvergence are yet to be studied. Secondly, the work can be extended to cover more models, bothconvolutional and non-convolutional, as well as other frequently used, costly layer types. Lastly,more detailed insights into what information the network loses when modified as proposed couldprove valuable.",
  "Conclusion": "In this paper, a novel method for accelerating inference in CV architectures has been presented. Byexpressing convolutions as semi-structured sparse matrix operations existing hardware accelerationsfor semi-structured sparsity can be used to directly translate model sparsity into significant savingsin regards to data loading and FLOPs spent during inference. The proposed use of semi-structuredsparsity patterns bridges the gap between practical requirements induced by compute hardwareand the theoretical desire to not limit the choice of sparse models to not affect model performancenegatively. To obtain the sparsity patterns, a semi-structured sparse masking of the pretrained models weights islearned from the training data optimizing for the same goal as the unchanged model. The resourcesspent on learning the maskings constitute a fraction of the resources spent during the original trainingof the model effectively reducing the resources spent during the models lifetime. At the same time,despite dropping out half of the weights in each convolutional layer, the performance of the model isnot affected negatively, even increasing in many instances as the classification experiments conductedshow. From a theoretical perspective, the effects of masking the weights of a classifier are quantifiedin the paper in the form of guarantees on class predictions under maskings. Combining these resultswith model changes induced by updates of the pretrained weights guarantees for reusing learnedsparsity patterns can be derived. In conclusion, the proposed method demonstrates that extending the support of readily availableacceleration techniques to natively support convolutional kernels is a promising avenue to accelerateconvolutional models more than two-fold while retaining the pretrained performance. I would like to thank Dr. Peter Belck for the insightful discussions on the presented work, and Prof.Dr. Roger Wattenhofer and the Distributed Computing (DISCO) Group at ETH Zrich for providingthe necessary resources without which this work would not have been possible. J. Ansel, E. Yang, H. He, N. Gimelshein, A. Jain, M. Voznesensky, B. Bao, P. Bell, D. Berard,E. Burovski, G. Chauhan, A. Chourdia, W. Constable, A. Desmaison, Z. DeVito, E. Ellison,W. Feng, J. Gong, M. Gschwind, B. Hirsh, S. Huang, K. Kalambarkar, L. Kirsch, M. Lazos,M. Lezcano, Y. Liang, J. Liang, Y. Lu, C. K. Luk, B. Maher, Y. Pan, C. Puhrsch, M. Reso,M. Saroufim, M. Y. Siraichi, H. Suk, S. Zhang, M. Suo, P. Tillet, X. Zhao, E. Wang, K. Zhou,R. Zou, X. Wang, A. Mathews, W. Wen, G. Chanan, P. Wu, and S. Chintala. PyTorch 2:Faster Machine Learning Through Dynamic Python Bytecode Transformation and GraphCompilation. In Proceedings of the 29th ACM International Conference on ArchitecturalSupport for Programming Languages and Operating Systems, Volume 2, pages 929947, LaJolla CA USA, Apr. 2024. ACM.",
  "D. Blalock, J. J. Gonzalez Ortiz, J. Frankle, and J. Guttag. What is the state of neural networkpruning? Proceedings of machine learning and systems, 2:129146, 2020": "T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan,P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child,A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray,B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei.Language Models are Few-Shot Learners. arXiv preprint arXiv:2005.14165, July 2020.",
  "T. Gale, E. Elsen, and S. Hooker. The state of sparsity in deep neural networks. arXiv preprintarXiv:1902.09574, 2019": "X. Glorot and Y. Bengio. Understanding the difficulty of training deep feedforward neuralnetworks. In Proceedings of the Thirteenth International Conference on Artificial Intelligenceand Statistics, pages 249256. JMLR Workshop and Conference Proceedings, 2010. M. Grimaldi, D. C. Ganji, I. Lazarevich, and S. S. Deeplite. Accelerating Deep Neural Networksvia Semi-Structured Activation Sparsity. In 2023 IEEE/CVF International Conference onComputer Vision Workshops (ICCVW), pages 11711180, Paris, France, Oct. 2023. IEEE.",
  "K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages770778, 2016": "T. Hoefler, D. Alistarh, T. Ben-Nun, N. Dryden, and A. Peste. Sparsity in deep learning: Pruningand growth for efficient inference and training in neural networks. Journal of Machine LearningResearch, 22(241):1124, 2021. S. Hooker. The hardware lottery. Communications of the ACM, 64(12):5865, Dec. 2021. E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. LoRA:Low-Rank Adaptation of Large Language Models. arXiv preprint arXiv:2106.09685, Oct. 2021.",
  "W. Huang, Y. Hu, G. Jian, J. Zhu, and J. Chen. Pruning Large Language Models with Semi-Structural Adaptive Sparse Training. arXiv preprint arXiv:2407.20584, Aug. 2024": "I. Hubara, B. Chmiel, M. Island, R. Banner, J. Naor, and D. Soudry. Accelerated sparse neuraltraining: A provable and efficient method to find n: M transposable masks. Advances in neuralinformation processing systems, 34:2109921111, 2021. E. Iofinova, A. Peste, M. Kurtz, and D. Alistarh. How Well Do Sparse ImageNet ModelsTransfer? In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),pages 1225612266, New Orleans, LA, USA, June 2022. IEEE.",
  "E. Jang, S. Gu, and B. Poole. Categorical Reparameterization with Gumbel-Softmax. arXivpreprint arXiv:1611.01144, Aug. 2017": "D. Justus, J. Brennan, S. Bonner, and A. S. McGough. Predicting the Computational Cost ofDeep Learning Models. In 2018 IEEE International Conference on Big Data (Big Data), pages38733882, Seattle, WA, USA, Dec. 2018. IEEE. D. Li, X. Chen, M. Becchi, and Z. Zong. Evaluating the Energy Efficiency of Deep ConvolutionalNeural Networks on CPUs and GPUs. In 2016 IEEE International Conferences on Big Dataand Cloud Computing (BDCloud), Social Computing and Networking (SocialCom), SustainableComputing and Communications (SustainCom) (BDCloud-SocialCom-SustainCom), pages477484, Atlanta, GA, USA, Oct. 2016. IEEE.",
  "NVIDIA Corporation.Apex (A PyTorch Extension) Apex 0.1.0 documentation. 2018": "NVIDIA Corporation. DALI (nvidia-dali-cudaX.X), 2024. J. Park, M. Naumov, P. Basu, S. Deng, A. Kalaiah, D. Khudia, J. Law, P. Malani, A. Malevich,S. Nadathur, J. Pino, M. Schatz, A. Sidorov, V. Sivakumar, A. Tulloch, X. Wang, Y. Wu, H. Yuen,U. Diril, D. Dzhulgakov, K. Hazelwood, B. Jia, Y. Jia, L. Qiao, V. Rao, N. Rotem, S. Yoo,and M. Smelyanskiy. Deep Learning Inference in Facebook Data Centers: Characterization,Performance Optimizations and Hardware Implications. arXiv preprint arXiv:1811.09886, Nov.2018.",
  "J. Pool and C. Yu. Channel Permutations for N:M Sparsity. Advances in neural informationprocessing systems, 34:1331613327, 2021": "A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell,P. Mishkin, J. Clark, G. Krueger, and I. Sutskever. Learning Transferable Visual Models FromNatural Language Supervision. arXiv preprint arXiv:2103.00020, Feb. 2021. O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy,A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual RecognitionChallenge. International Journal of Computer Vision, 115(3):211252, Dec. 2015.",
  "W. Wen, C. Wu, Y. Wang, Y. Chen, and H. Li. Learning structured sparsity in deep neuralnetworks. Advances in neural information processing systems, 29, 2016": "H. You, C. Li, P. Xu, Y. Fu, Y. Wang, X. Chen, R. G. Baraniuk, Z. Wang, and Y. Lin. DrawingEarly-Bird Tickets: Towards More Efficient Training of Deep Networks.arXiv preprintarXiv:1909.11957, Feb. 2022. S. Yun, D. Han, S. J. Oh, S. Chun, J. Choe, and Y. Yoo. Cutmix: Regularization strategy totrain strong classifiers with localizable features. In Proceedings of the IEEE/CVF InternationalConference on Computer Vision, pages 60236032, 2019.",
  "A.1Proof of Lemma 3.1": "Proof. The claim follows from the fact that the Lipschitz constant propagates through each layer ofthe compositional function. Consider a single layer fi(x) = (Wix + bi). Since is L-Lipschitz itfollows that fi is Li-Lipschitz with Li := LWi as indicated below where the first inequality usesthe Lipschitz-continuity of and the last inequality uses the sub-multiplicative property of norms.",
  "fk(fk1 ... f1(x1)) fk(fk1 ... f1(x2))(11) Lkfk1 ... f1(x1) fk1 ... f1(x2)(12) LkLk1...L1x1 x2(13)": "Thus, the function fk ... f1 is LkLk1...L1-Lipschitz. Finally, since the softmax functionsoftmax(z) for a vector z is a normalized exponential function it is known to be 1-Lipschitz. Iffd ... f1 is L-Lipschitz, where L = L1L2...Ld, then the overall classifier f(x) is consequentlyLf-Lipschitz with Lf = L 1 = L.",
  "A.2Proof of Lemma 3.2": "Proof. The claim follows from the fact that the masking changes the Lipschitz constant of the maskedlayer which then propagates through each layer of the compositional function. Let the i-th layer bethe masked layer, i.e., W i = Wi + W and f i(x) = (W ix + bi). Since is L-Lipschitz it followsthat f i is (Li + LW)-Lipschitz with Li := LWi as indicated below where the first inequalityuses the Lipschitz-continuity of and the latter two inequalities use the sub-multiplicative propertyof norms and the triangle inequality respectively.",
  "=: (Li + LW) =: Li(18)": "Reusing the results from the proof of Lemma 3.1 on the Lipschitz constant of a compositional functioncomposed with the softmax function it follows: If fd ... fi ... f1 is L-Lipschitz, where L =L1...Li...Ld = L1...(Li + LW)...Ld = L + L1...WLd = L + LW di=1,i=j Wi(cf. proof of Lemma 3.1), then the overall classifier f(x) is consequently Lf-Lipschitz with Lf =L 1 = L.",
  "A.4Proof of Lemma 3.4": "Proof. Let f(x) = (softmax fd ... fj ... f1)(x) be a compositional classifier of depth d withfi(x) = (Wix + bi). Let W j = Wj + W be the additive perturbation yielding the perturbed layerf j(x) = (W jx + bj) and classifier f (x) = (softmax fd ... f j ... f1)(x). Let xj1 be theinput to the j-th layer, i.e., xj1 = (fj1 ... f1)(x) = fj1,1(x). Then the the following can beobserved about the variance in the j-th layer, where the first inequality uses the Lipschitz-continuityof and the last inequalities use the triangle inequality and the sub-multiplicative property of normsrespectively.fj(xj1) f j(xj1) = (Wjxj1 + bj) ((Wj + W)xj1 + bj)(23) LWjxj1 + bj ((Wj + W)xj1 + bj)(24)= LWjxj1 + bj (Wjxj1 + bj) Wxj1(25) L(Wjxj1 + bj (Wjxj1 + bj) + Wxj1)(26) LWxj1(27)This can be extended to a statement on the entire classifier as follows:f(x) f (x) = (fd,j+1 fj fj1,1)(x) (fd,j+1 f j fj1,1)(x)(28)",
  "=(Wj)ik (Wj)ik + (Uj)ik (Uj)ikifBik = 0(Wj)ik 0 + (Uj)ik 0ifBik = 1(36)": "All perturbations shown in the diagram are additive perturbations. From the associative property ofmatrix addition it follows that for any two or more perturbations applied to a weight matrix the inducedchange can be expressed via a single perturbation. This yields Wj +Uj +V = Wj +(Uj +V ) =Wj +(Uj +(B, Wj +Uj)). From Lemma 3.4 it follows directly that f U is stable respect to a samplex predicted with a confidence of f(x) > Ld Uj + (B, Wj + Uj) xdi=1,i=j Wj.Using the result from Lemma 3.3 the following bound can be obtained:",
  "[Uj + (B, Uj)]ik =(Uj)ik + 0ifBik = 0(Uj)ik (Ui)ikifBik = 1(38)": "and thus Uj + (B, Wj + Uj) Uj (cf. argument in the proof of Lemma 3.3). Thisyields Wj + W + (Uj + (B, Wj + Uj)) = Wj + (W + Uj + (B, Wj + Uj)) implyingfor the stability of f U is stable respect to a sample x predicted with a confidence of f(x) >Ld W + Uj + (B, Wj + Uj) xdi=1,i=j Wi. The following bound obtained fromthe auxiliary result above yields",
  "A.6Classification Task and Reported and Validation Classification Performance": "The pretrained networks were retrained and evaluated on the multi-class classification challengeprovided by the ImageNet-1K dataset . The data set contains approximately 1.2 million trainingimages, 50.000 validation and 100.000 test images of 1000 object classes. To ensure consistencybetween the performance on the validation set and the reported accuracies on the test set the unmodi-fied architectures were evaluated showing no significant differences (cf. ). To assess whetherreformulating the layers induced changes due to alteration of the floating point arithmetic, likewisecomparisons for the unmodified architectures and the modified architectures without masking wereconducted showing no change in predictions and performance as expected and are thus not reported. The below experimental results can be reproduced with the provided code and were obtained ona NVIDIA GeForce RTX 3090 with 24 GB of RAM spending less about 3 to 3:30 minutes pervariant of ResNet or ConvNeXt respectively.",
  "ConvNeXt-T28.6M82.5296.1582.1895.96ConvNeXt-S50.2M83.6296.6583.2696.94": "default resolution of 2242, validation images to 2562. The pixel values were normalized with a meanof = (0.485, 0.456, 0.406) and a standard deviation of = (0.229, 0.224, 0.225). Image loadingand processing was Graphics Processing Unit (GPU)-accelerated using the DALI library . Thelearning rate was adjusted by a step scheduler with no warm-up period adjusting the learning rateevery 3 epochs by a factor of = 0.1. The training did not make use of augmentation of the trainingdata showing each sample in every epoch exactly once. All experiments were run on a compute nodeequipped with 8 processor cores of 8 GB RAM each and a single NVIDIA GeForce RTX 3090with 24 GB of RAM spending roughly from 1-2 hours per epoch for all (row) entries in and2."
}