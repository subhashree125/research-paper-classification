{
  "Abstract": "Modern time-series forecasting models often fail to make full use of rich unstruc-tured information about the time series themselves. This lack of proper conditioningcan lead to \"obvious\" model failures; for example, models may be unaware ofthe details of a particular product, and hence fail to anticipate seasonal surgesin customer demand in the lead up to major exogenous events like holidays forclearly relevant products. To address this shortcoming, this paper introduces anovel forecast post-processor which we call LLMForecaster that fine-tuneslarge language models (LLMs) to incorporate unstructured semantic and contextualinformation and historical data to improve the forecasts from an existing demandforecasting pipeline. In an industry-scale retail application, we demonstrate thatour technique yields statistically significantly forecast improvements across severalsets of products subject to holiday-driven demand surges.",
  "Introduction": "Time series forecasting has a broad variety of uses across industry today, including in transportation,weather and retail settings. In modern retail settings, accurate demand forecasts are key to running anefficient supply chain. Improvements in forecast quality directly affect inventory efficiency, reducingstockouts, and enhancing customer satisfaction. In recent years, deep neural networks have become a powerful tool for forecasting at scale. Deeplearning models such as Recurrent Neural Networks (RNNs) [1; 2], Convolutional Neural Networks(CNNs) , and attention-mechanisms , have shown promising results as they extract complexfeatures and adapt to various time series patterns. These architectures are often designed to learnauto-correlations and cross-correlations from data [5; 6; 7]. These kinds of models have successfullyintegrated exogenous features (e.g. past covariates, known future information, and static covariates)and achieved remarkable success in real-world problems, including traffic forecasting [8; 9], retaildemand prediction [10; 11], power generation prediction , and energy consumption modeling .",
  "arXiv:2412.02525v1 [cs.LG] 3 Dec 2024": ": Aggregated demand and forecast for groups of products: (i) Mothers Day products; and(ii) Easter products. The vertical dashed lines mark the week prior to the holiday in question. Ingreen, we show time segments where the production model anticipates event-driven demand surges specifically large shopping events like Christmas. In red, we show time segments where theproduction model fails to anticipate event-driven demand surges. While these models can effectively incorporate numerical or categorical exogenous features, othervaluable features like product descriptions or customer reviews exist only as unstructured text.Because this information exists only as unstructured text which is difficult to featurize, these sourcesof information have been largely neglected or featurized in simple ways [7; 14; 15; 16]. Nonetheless, descriptive and contextual information about the time series may meaningfully enhanceforecast quality. In the retail setting, unstructured text describing the use and design of a productcontains valuable information about whether this product will see surges in demand for upcomingseasonal events (holidays, back-to-school, etc.). This information is often difficult or impossible tolearn from the time series themselves, for several reasons. Many products are new, with perhapsonly one (or fewer) years of sales history. Sales history at the product level is also noisy, subject tospurious spikes and stockouts which distort the historical sales. These factors mean that the availablehistorical sales are often insufficient to predict upcoming seasonal patterns reliably. In , we illustrate this problem, using forecasts from an MQ-Transformer model trainedon a retail dataset. We focus on products which are relevant to two key holiday related seasonalevents: Mothers Day and Easter. For both groups of products, we see that the existing forecastingmodel appropriately anticipates surges in demand during the holiday season (between Black Fridayand Christmas). The model responds reasonably to the holiday season (green bands), where wesee elevated sales across many products, high customer traffic and numerous price discounts. Bycontrast, the model fails to anticipate surges in demand during the Mothers Day and Easter holidayperiods themselves (red bands). Notably, the demand surges during Mothers Day and Easter arelocalized to small groups of products, aggregate customer traffic does not spike, and products areinfrequently discounted. The failure to anticipate these localized surges in demand increases the riskof stockouts for key products during these events. Today, these defects must be addressed throughhuman intervention, relying on human analysts to use their judgment and knowledge to identifyproducts relevant to an upcoming event and adjust the forecasts accordingly. Recent advancements in Large Language Models (LLMs) offer a promising avenue to address thesechallenges, enhancing predictive accuracy by combining rich textual information with traditionalcovariates. Gruver et al. first showed that even LLMs with text-based pre-training can performimpressive zero-shot time series forecasting. Moreover, pre-trained LLMs have the capability to per-form domain-specific predictive tasks by directly querying them with domain-specific instructions andknowledge [18; 19; 20; 21]. Xue et al. extended this approach by generalizing prediction tasks totime series data, incorporating context and semantic information from historical data. LLM4TSutilizes a two-stage fine-tuning process to improve the models ability to handle time series data, evenwith limited data availability. Similarly, TEMPO applies a Generative Pre-trained Transformer(GPT) to time series forecasting, using a prompt-based approach that tailors the model to complextemporal patterns and non-stationary data. Most existing research in time series forecasting hasfocused on using time series data as input to LLMs, exploring methods like tokenization of timeseries data [7; 15]. Despite these advancements, it remains an open question how to develop LLMs to integrate descrip-tive information about the time series themselves for example, the description of the productcorresponding to the sales in question. Further, these methods are often stand-alone forecastingmodels. This can be powerful, but in other real-world use cases we may already have a good enoughforecasting solution in place. In those cases, we do not want to completely replace the existingsolution instead we only want to modify the forecasts to fix areas where the existing models hasclear deficiencies. Our work addresses both of these areas. Here we introduce a procedure, the LLMForecaster, toincorporate unstructured textual information and recommend forecast adjustments to improve theaccuracy of an existing forecasting pipeline. Our procedure utilizes fine-tuned LLMs that incorporateboth historical forecasts as well as unstructured information. This allows us to systematically improveforecast quality in cases where the existing model fails to anticipate holiday related demand surges.We show that our approach enhances the accuracy of demand forecasts in retail environments,empowering businesses to better manage seasonal fluctuations and optimize their operations. 2MethodologyWe introduce the LLMForecaster, designed to automate forecast adjustments to correct biases inexisting demand forecasting models. Here, the existing model is an MQ-Transformer model trainedon a large dataset of retail sales. This existing model generates initial predictions, denoted as fi,tfor product i and target date t. We then train an additional model, incorporating unstructured textinformation such as product titles and descriptions (xi,t,text) and numeric features such as the price orforecast (xi,t,num) via prompts to an LLM. The model architecture is shown in .",
  "f i,t = e(xi,t,text,xi,t,num)fi,t(2)": "The model depends on an input prompt, which contains product information, forecast values, andother contextual information formatting using a predefined template. An example of such a prompt isshown in Appendix A.1. The fine-tuned LLM generates an embedding vector, which is then adaptedto the specific forecasting task using Low-Rank Adaptation (LoRA) . This adapted embeddingis concatenated with numerical features and fed into a Multi-Layer Perceptron (MLP) head, whichoutputs the scaling factor i,t. By fine-tuning the LLMForecaster on historical forecasts and demand,",
  "Experiment Results": "We apply the LLMForecaster to refine predictions made by the existing global model,MQ-Transformer (MQT), at a lead time of 12 weeks. This lead time is selected to provide suf-ficient time to procure inventory in advance of holiday surges. The various features are processed intoa prompt and fed into the pre-trained MPT7b-Instruct model, which is further fine-tuned forthe forecasting task. Performance is evaluated using the weighted p50 quantile loss (wQL), defined as:wQL =",
  "i,t yi,t where i is index of product and t is the forecast target date": "In this experiment, we train a single model capable of calibrating demand predictions across multipleholidays. We focus on five holidays known for strong seasonality: Halloween, Easter, Mothers Day,Fathers Day, and Valentines Day. The training dataset spans 88 weeks, from August 29th, 2021, toApril 30th, 2023, including both holiday-related and non-holiday products. A holiday-related productis one with the holiday name in the item name or product description. The test period covers 48weeks from May 7th, 2023, to March 3rd, 2024, and is divided into five distinct test sets, one for eachtarget holiday. To ensure the LLM knows when events happen, we use a \"Holiday-Encoding Prompt\"that provides the LLM with the proximity of the target date to the relevant holiday (A.2).",
  ": Example of aggregated forecasts on Easter products": "compares the aggregated forecasted demand with actual demand for Easter products. It showsseveral LLMForecaster models (r16, r64, r128, and r256, representing varying LoRA ranks), theemb baseline (where we do not apply LoRA fine-tuning) and the original MQT baseline. All iterationsof the LLMForecaster approach anticipate the Easter demand surge, while our two baselines fail todo so. presents wQL improvement results for the 48-week test sets demonstrating that thefine-tuned LLMForecaster models (r16, r64, r128, and r256) consistently outperform the baselineMQT and emb models across all five holiday datasets. We also conduct statistical significance testingof the improvement throughout the year - most of these improvements are statistically significant.More detailed empirical results are available in Appendix A.3, and discussion about Valentines Dayare available in Appendix A.4 . By contrast, the emb model, in which we do not do the LoRA fine-tuning, shows no improvement overthe MQT baseline. This underscores the importance of the LLMForecasters sophisticated fine-tuningapproach, leveraging LoRA fine-tuning to effectively learn the holiday-specific demand patterns.",
  "emb85111416": "4Conclusion and Future WorkWe introduced the LLMForecaster, a procedure which incorporates unstructured product-level infor-mation into numerical time series forecasts and implements forecast adjustments where they are likelyto add value; and we demonstrated that the LLMForecaster model leads to statistically significantimprovements to product-level demand forecast in large scale backtests in a retail setting. In future work, we plan to experiment with a broader variety of prompting techniques, as well ashyperparameter optimization. We are actively exploring similar techniques to use LLMs as a tool tofeaturize data as inputs to deep learning models, rather than as a post-processor. We will also exploremultimodal inputs like product images to further enhance forecast accuracy.",
  "Thomas Fischer and Christopher Krauss. Deep learning with long short-term memory networksfor financial market predictions. European journal of operational research, 270(2):654669,2018": "Yann LeCun, Bernhard Boser, John S Denker, Donnie Henderson, Richard E Howard, WayneHubbard, and Lawrence D Jackel. Backpropagation applied to handwritten zip code recognition.Neural computation, 1(4):541551, 1989. Bryan Lim, Sercan Ark, Nicolas Loeff, and Tomas Pfister. Temporal fusion transformersfor interpretable multi-horizon time series forecasting. International Journal of Forecasting,37(4):17481764, 2021. Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and WancaiZhang. Informer: Beyond efficient transformer for long sequence time-series forecasting. InProceedings of the AAAI conference on artificial intelligence, volume 35, pages 1110611115,2021. Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Autoformer: Decomposition trans-formers with auto-correlation for long-term series forecasting. Advances in neural informationprocessing systems, 34:2241922430, 2021.",
  "Carson Eisenach, Yagna Patel, and Dhruv Madeka. Mqtransformer: Multi-horizon forecastswith context dependent and feedback-aware attention. arXiv preprint arXiv:2009.14799, 2020": "David Salinas, Valentin Flunkert, Jan Gasthaus, and Tim Januschowski. Deepar: Probabilis-tic forecasting with autoregressive recurrent networks. International journal of forecasting,36(3):11811191, 2020. Hanyu Zhang, Mathieu Tanneau, Chaofan Huang, V Roshan Joseph, Shangkun Wang, andPascal Van Hentenryck. Asset bundling for hierarchical forecasting of wind power generation.Electric Power Systems Research, 235:110771, 2024. Peijun Zheng, Heng Zhou, Jiang Liu, and Yosuke Nakanishi. Interpretable building energyconsumption forecasting using spectral clustering algorithm and temporal fusion transformersarchitecture. Applied Energy, 349:121607, 2023.",
  "A.1Prompting details": "The following template is used to apply on the provided text features and numerical features. Thepart that is being inserted based upon data is indicated with blue square brackets, []. As discussed inA.2, we also include information if the target date is in the near vicinity of a particular holiday.",
  "A.2Holiday encoding prompt": "To improve the LLMForecasters ability to capture holiday-driven demand patterns, we have imple-mented a \"Holiday-Encoding Prompt\". This prompt provides the model with contextual informationabout the temporal relationship between the target forecast date and surrounding holidays. Forexample, for a forecast targeting the week of June 1, 2024, the prompt would include:",
  "The fathers day at 2024-06-16 is 2 weeks after 2024-06-01": "By including these details about the proximity of the target date to relevant holidays, we aim to helpthe model better identify the appropriate demand patterns. This helps by ensuring the LLM knowsprecisely when a given event will take place. This is especially important for \"moving holidays\"like Easter, where the exact date can vary by as much as 35 days from year to year. This Holiday-Encoding Prompt is expected to significantly improve the LLMForecasters performance in accuratelypredicting sales for various holiday periods. As shown in , the proposed LLMForecaster withthe Holiday-Encoding Prompt is able to identify the Easter spike at the end of March 2024, whileremoving the prompt fails to capture the Easter-related demand surge.",
  "A.3Accuracy results throughout the year": "Here we show how the LLMForecaster changes forecast accuracy for different groups of productsthroughout the year. First, we show the results from a t-test measuring the weekly change in quantileloss, over the 48 weeks in the test period. Across the various product sets, we generally see statisticallysignificant improvements over the MQT baseline. The only exception is for Valentines Day products,which show improvements which are not statistically significant. Next, we show the change in forecast accuracy from our LLMForecaster model versus the MQTbaseline. Positive numbers indicate weeks in which the LLMForecaster model improved accuracyover the existing baseline. For each group examined, the LLMForecaster generally improves accuracythroughout the year.",
  "A.4Valentines Day": "The results shown in Tables 1 and 2 indicate a relatively small and sometimes not statisticallysignificant improvement for the Valentines Day products compared to the other holiday categories.This can be attributed to the unique nature of the Valentines Day holiday and its shifting positionwithin the calendar week. For other holidays like Halloween, Easter, Mothers Day, and FathersDay, the dates are fixed on either Saturdays or Sundays, so the overall demand distribution during theholiday period remains relatively consistent. By contrast, Valentines Day is fixed on February 14th,which can fall on different days of the week. That means that last-minute shopping, for example,may take place in the week of Valentines Day or the prior week. In this experiment, the trainingdataset contained Valentines Day falling on Monday or Tuesday - with minimal time to shop duringthe week of the holiday, last-minute shopping took place primarily in the prior week. In the test set,Valentines Day occurring on a Wednesday, so consumers had more time to shop for the holidayduring the week itself. When plotting the total prediction and demand on , this effect isclearly observed. While the LLMForecaster models are able to capture the Valentines Day demandspike compared to the baseline models, they tend to over-predict the weeks before Valentines Day",
  ": Forecast accuracy change for Easter products": "and significantly under-predict the demand during the actual Valentines Day week in 2024. Thisissue could potentially be addressed by training the model with data spanning multiple years, or byincorporating daily demand patterns into the training process. This would help the LLMForecasterbetter account for the shifting position of Valentines Day within the calendar week and improve itsability to accurately predict the associated demand patterns."
}