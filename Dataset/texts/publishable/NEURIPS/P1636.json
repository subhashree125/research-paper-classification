{
  "Abstract": "Federated learning (FL) is a distributed learning framework that leverages com-monalities between distributed client datasets to train a global model. Underheterogeneous clients, however, FL can fail to produce stable training results. Per-sonalized federated learning (PFL) seeks to address this by learning individualmodels tailored to each client. One approach is to decompose model traininginto shared representation learning and personalized classifier training. Nonethe-less, previous works struggle to navigate the bias-variance trade-off in classifierlearning, relying solely on limited local datasets or introducing costly techniquesto improve generalization. In this work, we frame representation learning as agenerative modeling task, where representations are trained with a classifier basedon the global feature distribution. We then propose an algorithm, pFedFDA, thatefficiently generates personalized models by adapting global generative classifiersto their local feature distributions. Through extensive computer vision benchmarks,we demonstrate that our method can adjust to complex distribution shifts withsignificant improvements over current state-of-the-art in data-scarce settings. Oursource code is available on GitHub1.",
  "Introduction": "The success of deep learning models relies heavily on access to large, diverse, and comprehensivetraining data. However, communication constraints, user privacy concerns, and government regula-tions on centralized data collection often pose significant challenges to this requirement .To address these issues, Federated Learning (FL) has gained considerable attention as a dis-tributed learning framework, especially for its privacy-preserving properties and efficiency in trainingdeep networks. The FedAvg algorithm, introduced in the seminal work , remains one of the most widelyadopted algorithms in FL applications . It utilizes a parameter server tomaintain a global model, trained through iterative rounds of distributed client local updates and serveraggregation of client models. While effective under independent and identically distributed (i.i.d.)client data, its performance deteriorates as client datasets become more heterogeneous (non-i.i.d.).Data heterogeneity leads to the well-documented phenomenon of client drift , where distinctlocal objectives cause the model to diverge from the global optimum, resulting in slow convergence and suboptimal local client performance . Despite extensive efforts toenhance FedAvg for non-i.i.d. clients, the use of a single global model remains too restrictive formany FL applications.",
  "arXiv:2411.00329v1 [cs.LG] 1 Nov 2024": "balancing the bias introduced by using global knowledge that may not generalize to individualclients, and the variance inherent in learning from limited local datasets. Popular PFL techniquesinclude regularized local objectives , local-global parameter interpolation , meta-learning, and representation learning . While these techniques have shown significantimprovements for clients under limited types of synthetic data heterogeneity (e.g., imbalancedpartitioning of an otherwise i.i.d. dataset), we find that current methods still struggle to navigate thebias-variance trade-off with the additional challenge of feature distribution shift and data scarcity,conditions commonly encountered in cross-device FL. As such, we look to design a method capable of handling real-world distribution shifts, e.g., covariateshift caused by weather conditions or poor camera calibration, (see clients 1 and 2 in ) withlimited local datasets. To this end, we approach PFL through shared representation learning guidedby a global, low-variance generative classifier. Specifically, we select a probability density p withdesirable statistical properties (e.g., one that admits an efficient Bayesian classifier) and iterativelyestimate the global parameters of this distribution and representation layers to produce features fromthe estimated distribution (a). To further navigate the bias-variance trade-off, we introduce a local-global interpolation methodto adapt the global estimate to the distribution of each client. At inference time, clients use theiradaptive local distribution estimate in a personalized Bayesian classifier (b). Contributions. We propose a novel Personalized Federated Learning method based on FeatureDistribution Adaptation (pFedFDA). We contextualize our algorithm using a class-conditionalmultivariate Gaussian model of the feature space in a variety of computer vision benchmarks. Ourempirical evaluation demonstrates that our proposed method consistently improves average modelaccuracy in benchmarks with covariate shift or client data scarcity, obtaining over 6% in multiplesettings. At the same time, our method remains competitive with current state-of-the-art (oftenwithin 1%) on more general benchmarks with more moderate data heterogeneity. To summarize, ourcontributions are three-fold:",
  "Related Work": "Federated Learning with Non-i.i.d. Data. Various studies have worked to understand and improvethe ability of FL to serve heterogeneous clients. In non-i.i.d. scenarios, the traditional FedAvg method is susceptible to client drift , resulting in slow convergence and poor local client accuracy. To tackle this challenge, proposed the use of regularized local objectives toreduce the bias on the global model after local training. Another approach focuses on rectifying thebias of local updates through techniques such as control variates. Other strategies includeloss-balancing , knowledge distillation , prototype learning , and contrastivelearning . Despite promising results on non-i.i.d. data, their reliance on a single global modelposes limitations for highly heterogeneous clients . Personalized Federated Learning. In response to the limitations of a single global model, PFLseeks to overcome heterogeneity by learning models tailored to each client. In this framework,methods attempt to strike a balance between being flexible enough to fit the local distribution andrelying on global knowledge to prevent over-fitting on small local datasets. Popular strategies includemeta-learning an initialization for client adaptation , multi-task learning with local modelregularization , local and global model interpolation , personalized model aggregation, client clustering , and decoupled representation and classifier learning .Our work focuses on this latter approach, in which the neural network is typically decomposed intothe first L 1 layers used for feature extraction, and the final classification layer. Existing works in this category share feature extraction parameters between clients and rely on clientclassifiers for personalization. These approaches differ primarily in the acquisition of client classifiers",
  "(b) Local Distribution Adaptation": ": Overview of pFedFDA. (Left) Heterogeneous clients collaboratively train representation parametersunder a generative classifier derived from a global estimate of class feature distributions. (Right) At test time,clients adapt the generative classifier to their feature distributions to obtain personalized classifiers. during training, which influences representation learning. For example, FedRep sequentially trainsa strong local classifier while holding the representation fixed, then updates the representation underthe fixed classifier. FedBABU proposes to use fixed dummy classifiers to align client objectives,only fine-tuning the classifier layers after the representation parameters have converged. Similarly,FedRoD aims to train a generic representation model and classifier in tandem via balanced softmaxloss, later obtaining personalized classifiers through fine-tuning or hypernetworks. FedPAC adopts the learning algorithm of FedRep, but additionally regularizes the feature space to be similaracross clients, before learning a personalized combination of classifiers across clients to improvegeneralization. However, this collaboration comes with an additional computational overhead thatscales with the number of active clients. pFedGP leverages a shared feature extractor as a kernelfor client Gaussian processes. Although this approach offers improved sample efficiency, it comes atthe cost of increased computational complexity and reliance on an inducing points set. In a similar spirit to our method, FedEM estimates the latent data distribution of clients inparallel to training classification models. FedEM estimates each client data distribution as a mixtureof latent distributions, where personalized models are a weighted average of mixture-specific models.Notably, this introduces a significant overhead in both communication and computation as separatemodels are trained for each mixture. In contrast, our work estimates the distribution of client featuresin parallel to training a global representation model.",
  "Problem Formulation": "FL System and Objective. We consider an FL system where a parameter server coordinates with Mclients to train personalized models i, i = 1, 2, , M. Each client i has a local training datasetDi = {(xji, yji )}nij=1, where x Rm and y {1, , C}. The model training objective in PFL is:",
  "i=1Fi(i),(1)": "where Q is feasible set of model parameters, Fi(i) = E(x,y)Di[L(i(x), y)] is the empirical risk ofdataset Di, and L is a loss function of the prediction errors (e.g., cross-entropy). The client populationM in FL can be large, resulting in partial client participation . Let q denote the participation rate,meaning that in each round, a client participates in model training with probability q. Following , we approach this as a problem of global representation learning and local clas-sification, in which each i consists of a shared backbone, , responsible for extracting low-levelfeatures (z Rd = (x)), and a local classifier, hi, for learning a client-specific mapping betweenfeatures and labels. Considering this decomposition of parameters i = (hi ), we can rewrite theoriginal PFL objective as the following:",
  "where and H are the feasible sets of neural network and classifier parameters, respectively": "In our generative modeling framework, we consider H to be the probability simplex over {1, , C},and our algorithm uses approximations of the posterior distributions as classifiers hi. However, forfair comparison with existing work (as well as other nice properties, discussed in .1), weselect a generative model of the feature space such that hi can be represented with an equivalentlinear layer. Data Heterogeneity. The data distribution of each client i is a joint distribution on X Y, whichcan be written as pi(x, y), pi(y)pi(x|y), or pi(x)pi(y|x). Using the terminology of , we referto each case of data heterogeneity as follows: prior probability shift (pi(y) = pi(y)), conceptdrift (pi(x|y) = pi(x|y)), covariate shift (pi(x) = pi(x)), and concept shift (pi(y|x) = pi(y|x)).Furthermore, the local dataset volumes Di may have quantity skew, i.e., ni = ni.",
  "Algorithm 1 describes the workflow ofpFedFDA": "Our algorithm begins with a careful initial-ization of parameters for the feature extrac-tor , Gaussian means = {c}Cc=1, andcovariance (Lines 1-2). We initialize with established techniques (e.g., ) suchthat the output features follow a Gaussiandistribution with controlled variance. Wesimilarly use a spherical Gaussian to ensurea stable initialization of the correspondinggenerative classifier (see .1). At the start of each FL round r, the server broadcasts the current rg, rg, rg to each participatingclient. The local training of each client consists of two key components: (1) global representationlearning, in which clients train to maximize the likelihood of local features under the globalfeature distribution rg, rg (Line 7); (2) local distribution adaptation, in which clients obtain robustestimates of their local feature distribution ri , ri , using techniques for efficient low-sample Gaussianestimation (Line 8) and local-global parameter interpolation (Lines 9-11). After local training, clientssend their ri , ri , ri to the parameter server for aggregation (Line 14). In the following sections, we provide detailed explanations of each algorithmic component. In.1 we discuss the benefits of a generative modeling framework and provide the justificationfor our selected class-conditional Gaussian model. We outline how the resulting generative classifiercan be used to guide representation learning in .2 and describe how we obtain personalizedgenerative classifiers in .3.",
  "Generative Model of Feature Distributions": "Motivation for Generative Classifiers. A central theme in FL is exploiting inter-client knowledgeto train more generalizable models than any client could attain using only their local dataset. Thispresents an important bias-variance trade-off, as incorporating global knowledge naively can introducesignificant bias. Fortunately, under a generative modeling approach, this bias can be naturally handled,enabling efficient inter-client collaboration.",
  "cC nci:= ci , where nci is the number of local samples whose labels are c. This leaves the": "primary source of bias to the mismatch between local and global feature distributions pg(z|y) andpi(z|y). Crucially, it turns out that this bias is controllable due to the dependence of z on global repre-sentation parameters . Consequentially, we propose to minimize this bias through our classificationobjective, which we discuss further in .2. Class-Conditional Gaussian Model. In this work we approximate the distribution of latent repre-sentations using a class-conditional Gaussian with tied covariance, i.e., pi(z|y = c) = N(z|ci, i).We show the resulting generative classifier under this model in Eq. 4. Note that it has a closed formand results in a decision boundary that is linear in z. I.e., if we know the underlying local featuredistribution mean and covariance, we can efficiently compute the optimal header parameters hi forthe inner objective in Eq. 2. In addition to the convenient form of the Bayes classifier, we select this distribution as the Gaussianityof latent representations is likely to hold in practice. Notably, by adopting the common technique ofGaussian weight initialization (e.g., ), the resulting feature space is highly Gaussian at the startof training. It has also been observed that the standard supervised training of neural networks withcross-entropy objectives results in a feature space that is well approximated by a class-conditionalGaussian distribution , i.e., the corresponding generative classifier Eq. 4 has equal accuracy tothe learned discriminative classifier. We provide a further discussion of this modeling assumption inAppendix A.",
  "Global Representation Learning": "Next, we describe our process for training the shared feature extractor . Similar to existing works, our local training consists of training via gradient descent to minimize the cross-entropy lossof predictions from fixed client classifiers. We obtain our client classifiers through Eq. 4, using globalestimates of g, g and local estimated priors i. For computational efficiency, we avoid invertingthe covariance matrix by estimating 1c with the least-squares solution w = minw w c.",
  "c=1yc log p(yc|(x), c, , ).(5)": "Note that for a spherical Gaussian = I and uniform prior , we recover a nearest-mean classifierunder Euclidean distance. This resembles the established global prototype regularization , whichminimizes the Euclidean distance of features from their corresponding global class prototypes.Notably, FedPAC uses this prototype loss to align client features. However, this implicitlyassumes that all feature dimensions have equal variance, and additionally requires a hyperparameter to balance the amount of regularization with the primary objective. In contrast, our generativeclassifier naturally aligns the distribution of client features by training with our global generativeclassifier.",
  "cC1{yj=c}c.(8)": "Estimators Eq. 6 and Eq. 7 may be noisy on clients with limited local data. To illustrate this, considerthe common practical scenario where ni d. The feature covariance matrix i at client i will bedegenerate; in fact, it will have a multitude of zero eigenvalues. In these cases, we can add a smalldiagonal I to , and replace the non-positive-definite matrices with the nearest positive definitematrix with identical variance. This can be efficiently computed by clipping eigenvalues in thecorresponding correlation matrix and followed by converting it back to a covariance matrix withnormalization to maintain the initial variance. We refer readers to for a review of low-samplecovariance estimation. Local-Global Interpolation. We introduce this fusion because even with the aforementioned correc-tion to ill-defined covariances, the variance of the local estimates remains highly noisy, indicating thenecessity of leveraging global knowledge. It is essential to consider that in the presence of data het-erogeneity, clients with differing local data distributions and dataset sizes have varying requirementsfor global knowledge. For our Gaussian parameters , , we consider the introduction of global knowledge through apersonalized interpolation between local and global estimates, which can be viewed as a form ofprior. We provide an analysis of the high-probability bound on estimation error for an interpolatedmean estimate in simple settings in Theorem 1. The full derivation is deferred to Appendix E.Theorem 1 (Bias-Variance Trade-Off). Let C = 1. Define i as the sample mean of client is localfeatures i :=1ninij=1 zji , and g as the global sample mean using all N samples across M clients:",
  "g := 1": "NMi=1nij=1 zji . Assume client features are independent and distributed as zi N(i, i),with true global feature distribution N(g, g). We consider the use of global knowledge at client ithrough an interpolated estimate: i := i + (1 )g, where . For any (0, 1), withprobability at least 1 , it holds that",
  "(x,y)DkL(x, y, , k + (1 )g, k + (1 )g, i),(9)": "where Dk is the dataset consisting of the validation samples for the k-th fold, and (k, k) are thelocal distribution estimates Eq. 6 and Eq. 7 estimated using the training samples from the k-th fold.In our experiments, we avoid additional forward passes on the local dataset by preemptively storingthe feature-label pairs obtained over the latest round of training. We solve Eq. 9 using off-the-shelf quasi-newton methods (e.g., L-BFGS-B). We additionally exploreusing separate terms for the means and covariance (.3) and recommend the use of a single term for most applications. After obtaining , we set our local estimates of i, i to their interpolated versions. These estimatesare then sent to the server for aggregation. Notably, the server update rule can be viewed as a movingaverage between the previous round estimate and the client average scaled by , reducingthe influence of local noise in the global distribution estimate. At test time, clients use their localdistribution estimates for inference through the classification rule in Eq. 4.",
  "Experimental Setup": "Datasets, Tasks, and Models: We consider image classification tasks and evaluate our method onfour popular datasets. The EMNIST dataset is for 62-class handwriting image classification. TheCIFAR10/CIFAR100 datasets are for 10 and 10-class color image classification. The TinyIm-ageNet dataset is for 200-class natural image classification. For EMNIST and CIFAR10/100datasets, we adopt the 4-layer and 5-layer CNNs used in . On the larger TinyImageNet dataset,we use the ResNet18 architecture. Notably, the feature dimension d for EMNIST/CIFAR CNNsis 128, and 512 for ResNet. We provide additional details in Appendix C.1. Clients and Dataset Partitions: The EMNIST dataset has inherent covariate shifts due to theindividual styles of each writer. We partition the dataset by writer following , and train withM = 1000 total clients (writers), participating with rate q = 0.03. On CIFAR and TinyImageNetdatasets, we simulate prior probability shift and quantity skew by partitioning the dataset accordingto a Dirichlet distribution with parameters (0.1, 0.5), where lower indicates higher levels ofheterogeneity. On these datasets, we use M = 100 clients with participation rate q = 0.3. Additionaldetails of the partitioning strategy are provided in Appendix C.1.2.",
  "We split each clients data partition 80-20% between training and testing": "Covariate Shift and Data Scarcity: We introduce two modifications to client partitions to simulatethe challenges of real-world cross-device FL. We first consider common sources of input noisefor natural images, which may result from the qualities of the measuring devices (e.g., cameracalibration, lens blur) or environmental factors (e.g., weather, lighting). To simulate this, we selectten image corruptions at five levels of severity defined in , and corrupt the training and testingsamples of the first 50 clients in CIFAR10/100 with unique corruption-severity pairs. We leave theremaining 50 client datasets unchanged. We refer to these datasets with natural covariate shifts asCIFAR10-S/CIFAR100-S and detail the specific corruptions in Appendix C.1.1. Second, we perform uniform subsampling of client training sets, leaving them with (75%, 50%, or25%) of their original samples. These low-sample settings are more realistic for cross-device FL,where clients rely more on knowledge sharing. Baselines and Metrics: We compare pFedFDA to the following baselines: Local, in which eachclient trains its model in isolation; FedAvg and FedAvg with fine-tuning (FedAvgFT); APFL ;Ditto ; pFedMe ; FedRoD ; FedBABU ; FedPAC ; FedRep ; and LG-FedAvg. We report the average and standard deviation of client test accuracies. Model Training: We train all algorithms with mini-batch SGD for E = 5 local epochs and R = 200global rounds. We apply no data augmentation besides normalization into the range . ForpFedFDA, we use k = 2 cross-validation folds to estimate a single i term for each client. Additionaltraining details and hyperparameters for each baseline method are provided in Appendix C.2.",
  "pFedFDA.844(.10).902(.09).763(.07).523(.08).385(.07).384(.07).214(.04)": "the other methods in test accuracy, demonstrating the effectiveness of our method in adapting toheterogeneous client distributions. Additionally, pFedFDA has an increasing benefit relative to othermethods in data-scarce settings: on CIFAR10, we improve 4.2% over the second-best method with100% of training samples and 6.9% with 25%. On CIFAR100, the same improvements range from0.1% to 6.5%. This indicates the success of our method in navigating the bias-variance trade-off. Evaluation in more moderate scenarios. Our evaluation of all four datasets in the traditional setting(no added covariate shift, full training data) is presented in . We note that: (1) our method isstill competitive, always ranking within the top 3 methods, and (2) the gap between top methods issmaller than in the previous experimental setting. For example, on EMNIST/CIFAR10, we see thatFedAvgFT, FedPAC, and pFedFDA are within 1% accuracy. We observe larger performance gapsfor CIFAR100, with FedPAC and pFedFDA having the best results. Results under extreme data scarcity. We present additional results at the limits of data scarcity onCIFAR10/100 datasets in , where we assign a single mini-batch (50) of training examplesto each client. Notably, even as ni d, which poses a challenge to local covariance estimation,pFedFDA clients obtain the best test accuracy, indicating the robustness of our local-global adaptation. Generalization to new clients. We further analyze the ability of our generative classifiers togeneralize on clients unseen at training time. To simulate this setting, we first train the servermodel model using half of the client population. We then evaluate each method on the set ofclients not encountered throughout training, using their original input data, as well as their dataset",
  "CleanDataMotionBlurDefocusBlurGaussNoiseShotNoiseImpulseNoiseFrostFogJPEGComp.BrightnessContrast": "FedAvg.592(.07).584(.08).512(.09).554(.08).568(.07).575(.07).569(.07).465(.08).467(.08).580(.07).557(.08).359(.10)FedAvgFT.716(.08).709(.08).689(.08).704(.09).695(.09).699(.09).696(.09).680(.09).672(.09).711(.08).707(.08).688(.09) FedBABU.703(.10).691(.09).682(.08).685(.09).683(.09).680(.09).679(.09).651(.10).661(.09).690(.08).689(.09).670(.09)FedPAC.727(.09).724(.09).695(.09).708(.09).714(.09).712(.09).705(.09).682(.10).683(.09).716(.09).718(.09).667(.09)pFedFDA.738(.08).738(.08).702(.09).719(.09).729(.08).739(.07).725(.08).695(.09).684(.09).738(.08).733(.08).689(.09) transformed using each corruption from CIFAR-S. Further benchmark details, including fine-tuning(personalization) procedures, are provided in Appendix C.3. As demonstrated in , our methodgeneralizes well even on clients with covariate shifts not encountered at training time. Moreover,observe that pFedFDA has the highest accuracy on the original clients, highlighting the efficacy ofstructured generative classifiers when less training data is available (i.e., having 50 rather than 100clients).",
  "Ablation of Method Components": "We conduct two studies to verify the efficacy of our local-global interpolation method. In , wesee that our interpolated estimates always perform better than using only local data, indicating thebenefits of harnessing global knowledge. Learning separate terms for the means and covariancemay be beneficial in low-sample or covariate-shift settings when the local distribution estimate mayfluctuate further from the global estimate. However, using a single scalar appears sufficient andcomes with the lowest computational cost (associated with the time to solve Eq. 9). : Ablation study on CIFAR100 with Dir(0.1) partition. NB denotes clients using only local data toestimate their feature distribution (i = 1). SB denotes each client estimating a single i for both the meansand covariance, MB denotes clients computing i terms for the means and covariance separately. We show theaverage computational overhead across all settings.",
  "Communication and Computation": "The parameter count and relative communication load of our generative classifiers compared to asimple linear classifier varies depending on class count C and feature dimension d. In our experimentalconfigurations (datasets, architectures), the overhead in total parameter count ranges from 1.1% to6.8%. See Appendix D.3 for additional details. In , we compare the local training time (client-side computation) and total runtime of pFedFDAto baseline methods on CIFAR10. We observe a slight increase in training time relative to FedAvg,which can be attributed primarily to cost of learning our parameter interpolation coefficient .However, this increase is comparable to the existing methods and is lower than representation-",
  "Conclusion": "Balancing local model flexibility and generalization remains a central challenge in personalizedfederated learning (PFL). This paper introduces pFedFDA, a novel approach that addresses the bias-variance trade-off in client personalization through representation learning with generative classifiers.Our extensive evaluation on computer vision tasks demonstrates that pFedFDA significantly outper-forms current state-of-the-art methods in challenging settings characterized by covariate shift anddata scarcity. Furthermore, our approach remains competitive in more general settings, showcasing itsrobustness and adaptability. The promising results underline the potential of our method to improvepersonalized model performance in real-world federated learning applications. Future work willfocus on exploring the scalability of pFedFDA and its application to other domains.",
  "and Disclosure of Funding": "We gratefully acknowledge the support from the National Science Foundation CAREER awardunder Grant No. 2340482, the Army Research Laboratory under Cooperative Agreement NumberW911NF-23-2-0014, the Sony Faculty Innovation Award, and the National Defense & EngineeringGraduate (NDSEG) Fellowship Program. The views and conclusions contained in this documentare those of the authors and should not be interpreted as representing the official policies, eitherexpressed or implied, of the Army Research Laboratory, the National Science Foundation, or the U.S.government. The U.S. government is authorized to reproduce and distribute reprints for governmentpurposes notwithstanding any copyright notation herein. We also thank Ming Xiang for valuablediscussions and feedback on this work. Durmus Alp Emre Acar, Yue Zhao, Ramon Matas, Matthew Mattina, Paul Whatmough, andVenkatesh Saligrama. Federated learning based on dynamic regularization. In InternationalConference on Learning Representations, 2021.",
  "Yuyang Deng, Mohammad Mahdi Kamani, and Mehrdad Mahdavi. Adaptive personalizedfederated learning. arXiv preprint arXiv:2003.13461, 2020": "Zhaoyang Du, Celimuge Wu, Tsutomu Yoshinaga, Kok-Lim Alvin Yau, Yusheng Ji, and JieLi. Federated learning for vehicular internet of things: Recent advances and open issues. IEEEOpen Journal of the Computer Society, 1:4561, 2020. Moming Duan, Duo Liu, Xinyuan Ji, Yu Wu, Liang Liang, Xianzhang Chen, Yujuan Tan, andAo Ren. Flexible clustered federated learning for client-level data distribution shift. IEEETransactions on Parallel & Distributed Systems, 33(11):26612674, November 2022. Alireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar.Personalized federated learningwith theoretical guarantees: A model-agnostic meta-learning approach. Advances in NeuralInformation Processing Systems, 33:35573568, 2020. Liang Gao, Huazhu Fu, Li Li, Yingwen Chen, Ming Xu, and Cheng-Zhong Xu. Feddc:Federated learning with non-iid data via local drift decoupling and correction. In Proceedingsof the IEEE/CVF conference on computer vision and pattern recognition, pages 1011210121,2022.",
  "Yaqian Guo, Trevor Hastie, and Robert Tibshirani. Regularized linear discriminant analysis andits application in microarrays. Biostatistics (Oxford, England), 8:86100, 02 2007": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers:Surpassing human-level performance on imagenet classification. In Proceedings of the IEEEInternational Conference on Computer Vision, pages 10261034, 2015. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-age recognition. In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition, pages 770778, 2016.",
  "Yihan Jiang, Jakub Konecn`y, Keith Rush, and Sreeram Kannan. Improving federated learningpersonalization via model agnostic meta learning. arXiv preprint arXiv:1909.12488, 2019": "Peter Kairouz, H Brendan McMahan, Brendan Avent, Aurlien Bellet, Mehdi Bennis, Ar-jun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings,et al. Advances and open problems in federated learning. Foundations and Trends in MachineLearning, 14(12):1210, 2021. Georgios Kaissis, Alexander Ziller, Jonathan Passerat-Palmbach, Tho Ryffel, Dmitrii Usynin,Andrew Trask, Ionsio Lima, Jason Mancuso, Friederike Jungmann, Marc-Matthias Steinborn,Andreas Saleh, Marcus Makowski, Daniel Rueckert, and Rickmer Braren. End-to-end privacypreserving deep learning on multi-institutional medical imaging. Nature Machine Intelligence,3(6):473484, Jun 2021. Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, andAnanda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. InInternational Conference on Machine Learning, pages 51325143. PMLR, 2020. Ahmed Khaled, Konstantin Mishchenko, and Peter Richtrik. Tighter theory for local sgd onidentical and heterogeneous data. In International Conference on Artificial Intelligence andStatistics, pages 45194529. PMLR, 2020.",
  "Ximeng Liu, Lehui Xie, Yaopeng Wang, Jian Zou, Jinbo Xiong, Zuobin Ying, and Athanasios VVasilakos. Privacy and security issues in deep learning: A survey. IEEE Access, 9:45664593,2020": "Nathalie Majcherczyk, Nishan Srishankar, and Carlo Pinciroli. Flow-fl: Data-driven federatedlearning for spatio-temporal predictions in multi-robot systems. In 2021 IEEE InternationalConference on Robotics and Automation (ICRA), pages 88368842. IEEE, 2021. Othmane Marfoq, Giovanni Neglia, Aurlien Bellet, Laetitia Kameni, and Richard Vidal.Federated multi-task learning under a mixture of distributions. Advances in Neural InformationProcessing Systems, 34:1543415447, 2021. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.Communication-efficient learning of deep networks from decentralized data. In Artificialintelligence and statistics, pages 12731282. PMLR, 2017.",
  "Swaroop Ramaswamy, Rajiv Mathews, Kanishka Rao, and Franoise Beaufays. Federatedlearning for emoji prediction in a mobile keyboard. arXiv preprint arXiv:1906.04329, 2019": "Felix Sattler, Klaus-Robert Mller, and Wojciech Samek. Clustered federated learning: Model-agnostic distributed multitask optimization under privacy constraints. IEEE Transactions onNeural Networks and Learning Systems, 32(8):37103722, 2020. Micah J Sheller, G Anthony Reina, Brandon Edwards, Jason Martin, and Spyridon Bakas.Multi-institutional deep learning modeling without sharing patient data: A feasibility study onbrain tumor segmentation. In Brainlesion: Glioma, Multiple Sclerosis, Stroke and TraumaticBrain Injuries: 4th International Workshop, BrainLes 2018, Held in Conjunction with MICCAI2018, Granada, Spain, September 16, 2018, Revised Selected Papers, Part I 4, pages 92104.Springer, 2019.",
  "Roman Vershynin. High-dimensional probability: An introduction with applications in datascience, volume 47. Cambridge university press, 2018": "Chunnan Wang, Xiang Chen, Junzhe Wang, and Hongzhi Wang. Atpfl: Automatic trajectoryprediction model design under federated learning framework. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition (CVPR), pages 65636572, June 2022. Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H Vincent Poor. Tackling the objectiveinconsistency problem in heterogeneous federated optimization. Advances in Neural InformationProcessing Systems, 33:76117623, 2020.",
  "Jian Xu, Xinyi Tong, and Shao-Lun Huang. Personalized federated learning with featurealignment and classifier collaboration. In The Eleventh International Conference on LearningRepresentations, 2023": "Timothy Yang, Galen Andrew, Hubert Eichner, Haicheng Sun, Wei Li, Nicholas Kong, DanielRamage, and Franoise Beaufays. Applied federated learning: Improving google keyboardquery suggestions. arXiv preprint arXiv:1812.02903, 2018. Jianqing Zhang, Yang Hua, Hao Wang, Tao Song, Zhengui Xue, Ruhui Ma, and Haibing Guan.Fedala: Adaptive local aggregation for personalized federated learning. In Proceedings of theAAAI Conference on Artificial Intelligence, volume 37, pages 1123711244, 2023. Michael Zhang, Karan Sapra, Sanja Fidler, Serena Yeung, and Jose M. Alvarez. Personalizedfederated learning with first order model optimization. In International Conference on LearningRepresentations, 2021.",
  "ALimitations": "The selected class-conditional Gaussian distribution may not work well for all neural networkarchitectures. For example, if the output features are the result of an activation such as ReLU, atruncated Gaussian distribution may be a better model. Future work can look to exploit knowledgeof the neural network architecture to improve the accuracy of the feature distribution estimate. In this work, we leverage the insights from a fusion of global and local feature space. As in manyapplications there is often an underlying cluster structure between clients datasets, future worksmay explore the identification and efficient estimation of feature distributions of client clusters, inorder to reduce the degree of bias introduced in client collaboration.",
  "BBroader Impacts": "Federated learning has become the main trend for distributed learning in recent years and has deployedin many popular consumer devices such as Apples Siri, Googles GBoard, and Amazons Alexa.Our paper addresses the practical limitations of personalization methods in adapting to clients withcovariate shifts and/or limited local data, which is a central issue in cross-device FL applications. Weare unaware of any potential negative social impacts of our work.",
  "C.2Training Settings": "All methods are trained using mini-batch SGD for 200 global rounds with 5 local epochs of training.We use a fixed learning rate of 0.01, momentum of 0.5, and weight decay of 5e-4. The batch size isset to 50 for all experiments, except for EMNIST, where we use a batch size of 16. We sample theset of active clients uniformly with probability q=0.3 for CIFAR and TinyImageNet and q=0.03 forEMNIST. The last global round of training employs full client participation. We split the data of eachclient 80-20% between training and testing.",
  ": Comparison of Dirichlet Partitions on CIFAR10": "Hyper-parameters. For APFL, we tune over [0.25, 0.5, 0.75, 1.0], and set = 0.25. For pFedMe,we tune over [1.0, 5.0, 10.0, 15.0] and set = 5.0. For Ditto, we use five local epochs forpersonalization and tune over [0.05, 0.1, 0.5, 1.0, 2.0] and set = 1.0. For FedRep and FedBABU,we use five local epochs for training the head parameters. For FedPAC, we tune over [0.1, 0.5, 1.0,5.0, 10.0], and set = 1.0. FedPAC uses one local epoch for training head parameters with a higherlearning rate of 0.1, following the original implementation.",
  "C.3Evaluation on New Clients": "Our fine-tuning procedure on new clients largely follows the methodology above. For FedAvgFT,we fine-tune the global model for five local epochs. For FedBABU and FedPAC, we personalizethe model in 2 different ways and report the best result: (1) fine-tuning only the head for 5 localepochs, and (2) fine-tuning both the body and head for 5 local epochs. For pFedFDA, each new clientestimates their local interpolated statistics (i.e., lines 8-11 of Algorithm 1) to obtain a personalizedgenerative classifier.",
  "D.1Multi-Domain FL": "In , we present results on the DIGIT-5 domain generalization benchmark . This presentsan alternate form of covariate shift, as the data from each client is drawn from one of 5 datasets(SVHN, USPS, SynthDigits, MNIST-M, and MNIST). In particular, we use 20 clients trained withfull participation, and assign 4 clients to each domain. Within each domain, we use the Dirichlet(0.5)partitioning strategy to assign data to each client. We observe that pFedFDA is effective in all settings,but has the most significant benefits over prior work in the low-data regime.",
  "D.2Effect of Local Epochs": "In many FL settings, we would like clients to perform more local training between rounds to reducecommunication costs. However, too much local training can cause the model to diverge. In ,we compare the effect of the local amount of epochs for CIFAR100 and CIFAR100-S-25% sampledatasets. We observe that (1) pFedFDA outperforms FedAvgFT at all equivalent budgets of E, (2)",
  "D.4Runtime of Method Components": "In , we evaluate the proportion of each local iteration of pFedFDA associated with each line ofour algorithm. Network Passes refers to the time taken to train the base network parameters (Line 7of Alg. 1). Mean/Covariance Est. refers to the time taken to estimate the local mean and covariancefrom features extracted during model training (Line 8 of Alg. 1). Interpolation Optimization refersto the time taken to optimize the local coefficient (Line 9 of Alg. 1). Overall, we find that themajority of the overhead of our method comes from estimating the interpolation parameter .",
  "Proof of Theorem 1. For ease of exposition, we drop the time index and class index": "Let i be an arbitrary client with local dataset size ni of class c. Let N be the total data volume ofclass c over the entire FL system. Assuming that the distribution of client is features z follow a mul-tivariate Gaussian distribution N(i, i), and the global feature distribution follows N(, ) whereg := Mi=1 nii/(",
  "Tr(A2) if matrix A is symmetric, and that max{a, b} a + b, inequality (d)holds because Tr(A2) (Tr(A))2 for positive semidefinite matrix A and that Tr(i), 2 0, andTr(g) are by definition non-negative": "The first term (1 )2 g i22 is the bias introduced when client i uses global knowledge; thesmaller the , the more bias introduced. The last term reveals the interaction of and the tradeoffbetween local and global variance. When approaches 0, we have the global feature variance Tr()reduced by the average of N global samples. When approaches 1, we have local feature varianceTr(i) reduced by the average of only ni local data. Thus the bias-variance tradeoff on client icrucially depends on the degree of local-global distribution shift, g i22, the local data volumeni and its quality (i.e., i), and the volume and quality of the data across clients N, g."
}