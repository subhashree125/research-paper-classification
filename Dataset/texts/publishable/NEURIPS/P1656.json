{
  "Abstract": "Scaling has been a critical factor in improving model performance and gener-alization across various fields of machine learning. It involves how a modelsperformance changes with increases in model size or input data, as well as howefficiently computational resources are utilized to support this growth. Despitesuccesses in scaling other types of machine learning models, the study of scalingin Neural Network Interatomic Potentials (NNIPs) remains limited. NNIPs act assurrogate models for ab initio quantum mechanical calculations, predicting theenergy and forces between atoms in molecules and materials based on atomicconfigurations. The dominant paradigm in this field is to incorporate numerousphysical domain constraints into the model, such as symmetry constraints likerotational equivariance. We contend that these increasingly complex domain con-straints inhibit the scaling ability of NNIPs, and such strategies are likely to causemodel performance to plateau in the long run. In this work, we take an alternativeapproach and start by systematically studying NNIP scaling properties and strate-gies. Our findings indicate that scaling the model through attention mechanismsis both efficient and improves model expressivity. These insights motivate usto develop an NNIP architecture designed for scalability: the Efficiently ScaledAttention Interatomic Potential (EScAIP). EScAIP leverages a novel multi-headself-attention formulation within graph neural networks, applying attention at theneighbor-level representations. Implemented with highly-optimized attention GPUkernels, EScAIP achieves substantial gains in efficiencyat least 10x speed upin inference time, 5x less in memory usagecompared to existing NNIP models.EScAIP also achieves state-of-the-art performance on a wide range of datasetsincluding catalysts (OC20 and OC22), molecules (SPICE), and materials (MPTrj).We emphasize that our approach should be thought of as a philosophy rather thana specific model, representing a proof-of-concept towards developing general-purpose NNIPs that achieve better expressivity through scaling, and continue toscale efficiently with increased computational resources and training data.",
  "Introduction": "In recent years, the principle of scaling model size, data, and compute has become a key factor forimproving performance and generalization in machine learning (ML), across fields from naturallanguage processing (NLP) [Kaplan et al., 2020] to computer vision (CV) [Dosovitskiy et al., 2021,Zhai et al., 2022]. Scaling in ML is, in a large part, defined by the ability to best exploit GPUcomputing capabilities. This typically involves efficiently increasing model sizes to large parametercounts, as well as optimizing model training and inference to be optimally compute-efficient.",
  "arXiv:2410.24169v1 [cs.LG] 31 Oct 2024": "Parallel to these developments, ML models have also been rapidly developing for atomistic simulation,addressing problems in drug design, catalysis, materials, and more [Deringer et al., 2019, Unkeet al., 2021]. Among these, machine learning interatomic potentials, and particularly neural networkinteratomic potentials (NNIPs), have gained popularity as surrogate models for computationallyintensive ab initio quantum mechanical calculations like density functional theory. NNIPs aredesigned to predict the energies and forces of molecular systems with high efficiency and accuracy,allowing downstream tasks such as geometry relaxations or molecular dynamics to be carried out onsystems that would be intractable to simulate directly with density functional theory. Current NNIPs are predominantly based on graph neural networks (GNNs). The atomistic system isrepresented as a graph, where nodes correspond to atoms and edges representing interactions betweenatoms. Many effective models in this field have increasingly tried to embed physically-inspiredconstraints into the model, often justified by the belief that these constraints improve accuracy anddata efficiency. Common constraints include incorporating predefined symmetries into the NNarchitecture, such as rotational equivariance, as well as using complex input feature sets. NNIP models that integrate symmetry constraints [Batzner et al., 2022, Batatia et al., 2022, Liaoet al., 2024] often rely on computationally intensive tensor products of rotation order L [Geigerand Smidt, 2022] to maintain rotational equivariance. Although recent advancements have reducedthe computational complexity of these operations [Passaro and Zitnick, 2023a, Luo et al., 2024],the remaining computational overhead still significantly limits their scalability. Other approaches[Gasteiger et al., 2020a, 2021, 2022] use basis expansions of the edge directions, angles, and dihedralsas features. Generally, incorporating these constraints tends to be compute-inefficient. As a result,many of the models in the field remain highly-constrained and small, despite the availability of largerdatasets [Chanussot et al., 2021, Jain et al., 2013] and more computational resources. We contend that these increasingly complex domain constraints inhibit the scaling ability of an MLmodel, and such strategies are likely to plateau over time in terms of model performance. As thescale of the models increase, we hypothesize that imposing these constraints hinders the learning ofeffective representations, restricts the models ability to generalize, and impedes efficient optimization.Many of these feature-engineered approaches are not optimized for efficient parallelization on GPUs,further limiting their scalability and efficiency, especially when applied to larger systems. In many other fields of ML, general-purpose architectures that best exploit computing capabilitiesoutperform models with handcrafted, domain-specific constraints [Dosovitskiy et al., 2021, Zhaiet al., 2022]. These observations motivate us to ask: How can we develop principled methodsand design choices that enable the creation of general-purpose neural network interatomicpotentials that scale effectively with increased computational resources and training data? To answer this question, we conduct an initial ablation study to identify which components in NNIPsare most conducive to scaling. In NNIPs with built-in rotational equivariance, it is commonly believedthat increasing the rotation order (L) improves model performance, even though it incurs additionalcomputational cost. However, our investigations show that increasing the rotation order also addsmore parameters to the model, and NNIPs are not always adjusted to account for this difference inparameter count. Our investigations also show that how parameters are added to the model is critical,as different types of parameter increases can differently impact the models expressivity. We find thatincreasing the parameters of other components of the model besides the rotation orderparticularlythose involved in attention mechanismsgreatly improves model performance. Based on these insights, we develop the Efficiently Scaled Attention Interatomic Potential (EScAIP),an NNIP architecture explicitly designed for scaling by incorporating highly optimized attentionmechanisms. To the best of our knowledge, our model is the first to leverage attention mechanismson the neighbor representations of atoms rather than only the nodes, resulting in more expressivity.We also leverage advancements in attention mechanisms [Lefaudeux et al., 2022], which havecomputational and memory efficiencies for scaling on large datasets. Our model achieves the best performance on a wide range of chemical applications, includingthe top performance on the Open Catalyst 2020 (OC20), Open Catalyst 2022 (OC22), SPICEmolecules, and Materials Project (MPTrj) datasets. It also demonstrates a 10x speed up in inferencetime and 5x less in memory usage compared to existing NNIP models. To analyze rotationalequivariance, on the validation set, we 1) predict forces on a set of atomistic systems (A), 2) rotatethe atomistic systems and predict forces (B), and then 3) compute the cosine similarity between Inference Speed (Sample/Sec) Force MAE (meV/) 31M 153M 200M 216M 83M 146M317M EquiformerV2eSCNGemNet-OCEScAIP Inference Memory (GB/Sample) Force MAE (meV/) 31M 153M 200M 216M 83M 146M317M EquiformerV2eSCNGemNet-OCEScAIP : Efficiency, performance, and scaling comparisons between EScAIP and baseline models onthe Open Catalyst dataset (OC20). Force MAE Error (meV/ ) vs. Inference Speed (Sample/Sec )and Force MAE vs. Memory (GB/Sample ) is reported. Results with Energy MAE can be found inthe Appendix . EScAIP achieves better performance with smaller time and memory cost. the force predictions (B) and the rotated version of force predictions (A). After training EScAIPon different datasets, we find that the cosine similarity is consistently 0.99, meaning EScAIP isessentially always predicting the rotations correctly. We also provide evidence that EScAIP scaleswell with compute, and is designed in such a way that it will further improve in efficiency as advancesin GPU computing continue to increase. Our code and model checkpoints are publicly available at",
  "Related Works": "Neural Network Interatomic Potentials.There have been significant advancements in the devel-opment of neural network interatomic potentials (NNIPs), and we give a very general overview ofthe field. These models are usually trained to predict the system energy and per-atom force basedon system properties, including atomic numbers and positions. We classify these models into twocategories: (1) models that are based on Group Representation node features, and (2) models that arebased on node features represented by Cartesian Coordinates. In the former, the node features areequivariant to different groups acting on the atomic positions, such as rotations and translations. Inthe latter, most architectures obey basic group symmetries, such as rotation and translation invariance. Group Representation Architectures. The first model that used group representation nodefeatures was the Tensor Field Network [Thomas et al., 2018], followed by an improved version,NequIP [Batzner et al., 2022]. Then, MACE [Batatia et al., 2022] incorporated the AtomicCluster Expansion [Drautz, 2019] into the architecture. SCN [Zitnick et al., 2022] used sphericalfunctions to represent equivariant node features, followed by an efficiency improvement in thetensor products, eSCN [Passaro and Zitnick, 2023b]. Equiformer [Liao and Smidt, 2022, Liaoet al., 2024] incorporated graph attention into the architecture. Cartesian Coordinates Architectures. SchNet [Schtt et al., 2017] was the initial work thatonly used edge distances as input to maintain invariant node features. DimeNet [Gasteiger et al.,2020a,b] and GemNet [Gasteiger et al., 2021, 2022] added invariant bond direction feature setsas input. They designed an output head that maintains rotational equivariance with invariant nodefeatures. Another line of work tries to maintain equivariant features in Cartesian space by explicitlymodeling spherical functions [Frank et al., 2022, Bekkers et al., 2024, Chen and Ong, 2022, Cheng,2024, Haghighatlari et al., 2022]. Datasets for NNIP training.There has also been a growing focus in the NNIP domain on generatinglarger datasets with quantum mechanical simulations, and using this to train models. These datasetsspan domains such as molecules [Eastman et al., 2023, Smith et al., 2020], catalysts [Chanussot et al.,2021, Tran et al., 2023], and materials [Barroso-Luque et al., 2024, Yang et al., 2024, Merchant et al.,2023, Jain et al., 2013, Choudhary et al., 2020]. Liao et al., 2024 AT + SC ATSCMore AT BOO18 Force MAE (meV/) 26.2419M 21.3744M 20.4683M 21.1885M 20.8786M 20.8586M 20.3584M 22.5983M 21.6383M20.1384M 19.8785M 21.1885M 20.5495M L=0L=2L=4L=6 GemNet-OCAT: AttentionSC: Spherical Channels Liao et al., 2024 AT + SC ATSCMore AT BOO270 Energy MAE (meV) 29819M 28583M 28444M 28085M 27786M 27886M 27584M 28383M 27983M 27584M 27485M 27585M 27595M L=0L=2L=4L=6 GemNet-OCAT: AttentionSC: Spherical Channels : Results of ablation study of EquiformerV2 [Liao et al., 2024] on the OC20 2M dataset.Energy (eV) and force (eV/) mean absolute error (MAE) are reported, along with the modelsparameter counts. The leftmost column shows the original results from [Liao et al., 2024], wheredifferent L had a different number of trainable parameters. We look at scaling parameters through theattention mechanisms (AT) and spherical channels (SC) for the original L = 2 and L = 4 models,such that the number of parameters is approximately equal to the original L = 6 model. Scalingparameters in different ways affects the overall energy and forces error, and increasing attentionparameters is particularly effective in improving model performance (More AT). We also modifythe architecture to be invariant (L = 0), allowing us to examine the effects of excluding rotationalequivariance while controlling for the number of parameters (BOO). After controlling for parametercounts, many of the models have comparable error to the original L = 6 model. Constrained vs. Unconstrained Architectures.There has been a trend of incorporating physically-inspired constraints into NNIP model architectures, such as all Group Representation Architecturesthat incorporate symmetry constraints into the model. However, there have been other lines of workthat do not try to build in symmetry directly into the NN, and instead either try to approximatethe symmetry [Pozdnyakov and Ceriotti, 2023, Wang et al., 2022, Finzi et al., 2021] or learn thesymmetry via data augmentation techniques [Puny et al., 2022, Duval et al., 2023].",
  "Investigation on How to Scale Neural Network Interatomic Potentials": "We systematically investigate strategies for scaling neural network interatomic potential (NNIP)models through an ablation study. We examine how higher-order symmetries (rotation order L)impact scaling efficiency and identify the most effective methods for increasing model parameters(3.1). We also assess the importance of incorporating directional bond features (3.2). We conductexperiments using a leading NNIP architecture, the EquiformerV2 model [Liao et al., 2024], on theOpen Catalyst 2020 (OC20) Dataset [Chanussot et al., 2021] 2M split to evaluate the performance ofdifferent scaling strategies.",
  "Optimal Components for Scaling Neural Network Interatomic Potentials": "A prevalent approach to improve the capability of NNIP models with group representation featuresis to increase the order of representations (L). Liao et al. did a study on the EquiformerV2model, varying L to examine its impact on model performance. However, they did not control for thetotal number of trainable parameters in the model. This variation introduces discrepancies that canconfound the true effect of L on the models performance. Ablation Study Settings.To clarify the impact of increasing L on model performance and deter-mine the most effective strategy for increasing parameters in NNIP models, we conduct a parameter-controlled experiment using the EquiformerV2 model on the OC20 S2EF 2M dataset. We standardizethe number of trainable parameters across different values of L to isolate the effects of increasingL, and systematically add parameters to different components of the original L = 2 and L = 4EquiformerV2 models from Liao et al. . Our approach targets four distinct configurations:increasing parameters solely in the attention mechanisms (AT), solely in the spherical channels that acton all group representations in the NN (SC), evenly across both attention mechanisms and spherical",
  "channels (AT + SC), and a configuration where spherical channels are reduced while significantlyboosting attention parameters (More AT)": "Results of Ablation Study.The comparative analysis reveals a clear hierarchy in performancegains with different parameter scaling strategies. The More AT configuration yields the highestperformance improvement, followed by AT, AT + SC, and SC. The results, summarized in ,show that once the number of parameters across models are controlled, many of the models havecomparable error to the original L = 6 model. Increasing the parameters of the attention mechanismsis most beneficial and provides more substantial improvements than simply adding more parametersacross all components.",
  "Bond Directional Features": "We explore what the most minimal representations are of the atomistic system to enable the modelto learn a scalable, data-driven feature set, and find that incorporating bond directional informationis useful for NNIP models. As opposed to other domains, such as social networks, the edges (orbonds) in molecular graphs possess distinct geometric attributes, i.e., pairwise directions. However,the raw value of the bond direction changes with the rotation and translation of the molecule, makingit challenging to directly utilize these features in NNIP models. We propose a straightforward and data-driven approach to embed the bond directional information.To avoid the computational inefficiency of taking a tensor product, we aim to use the simplest possiblerepresentation of bond direction that is rotationally invariant. Inspired by Steinhardt et al. ,we use an embedding of the Bond-Orientational Order (BOO) to represent the directional features.Formally, for a node v, the BOO of order l is,",
  "(1)": "where duv is the normalized bond direction vector between node v and u, nv is the number ofneighbors of v, Nei(v) is the neighbors of v, Y (l)m is the spherical harmonics of order l and degree m.This can be interpreted as the minimum-order rotation-invariant representation of the l-th momentin a multipole expansion for the distribution of bond vectors bond(n) across a unit sphere. Inother words, BOO is the simplest way to encode the neighborhood directional information in arotationally invariant manner. The BOO features BOO(v) RL+1 for a node v is the concatenationof BOO(v)(l). In theory, the BOO feature contains all the directional information of the neighborhood,and the embedding network can learn to extract such information. Testing the bond-orientational order (BOO) features.We conduct a study to test the BOOfeatures. We modify the EquiformerV2 model to be L = 0 and replace the spherical harmonicsdirectional features with embeddings of the BOO features. The results are in . The L = 0 modelachieves comparable results with the L = 6 model. This finding suggests that the BOO features are astraightforward and effective way to incorporate bond directional information in NNIP models, andthat it is also possible to learn additional information solely through scaling.",
  "Efficiently Scaled Attention Interatomic Potential (EScAIP)": "We introduce a new NNIP architecture, Efficiently Scaled Attention Interatomic Potential (EScAIP),which leverages highly optimized self-attention mechanisms for expressivity, with design choicescentered around scalability and efficiency. To avoid costly tensor products, we operate on scalarfeatures that are invariant to rotations and translations. This enables us to take advantage the optimizedself-attention mechanisms from natural language processing, making the model substantially moretime and memory efficient than equivariant group representation models such as EquiformerV2 [Liaoet al., 2024]. An illustration of our model is shown in . We describe the key components of themodel and the motivation behind their design:",
  "(f)": ": Illustration of the Efficiently Scaled Attention Interatomic Potential (EScAIP) modelarchitecture. The model consists of B graph attention blocks (dashed box), each of which contains agraph attention layer, a feed forward layer, and two readout layers for node and edge features. Theconcatenated readouts from each block are used to predict per-atom forces and system energy. Input Block.The input to the model is a radius-r graph representation of the molecular system. Weuse three attributes from the molecular graph as input: atomic numbers [Zitnick et al., 2022], RadialBasis Expansion (RBF) of pairwise distances [Schtt et al., 2017], and Bond Order Orientation (BOO)features from 3.2. The atomic numbers embeddings are used to encode the atom type information,while the RBF and BOO embeddings are used to encode the spatial information of the molecularsystem. These input attributes are the minimal representations of the system, enabling the modelto learn a scalable, data-driven feature set. We also note that the attributes can be pre-computed,requiring minimal computational cost. The input features are then passed through a feed forwardneural network (FFN) to produce the initial edge and node features. Graph Attention Block.The core component of the model is the graph attention block, illustratedin . It takes node features and molecular graph attributes as input. All the features are projectedand concatenated into a large message tensor of shape (N, M, H), where N is the number of nodes,M is the max number of neighbor, and H is the message size. The message tensor is then processedby a multi-head self-attention mechanism. The attention is parallelized over each neighborhood,where M is the sequence length. By using customized Trition kernels [Tillet et al., 2019, Lefaudeuxet al., 2022], the attention mechanism is highly optimized for GPU acceleration. The output of theattention mechanism is aggregated back to the atom level. The aggregated messages are then passedthrough the node-wise Feed Forward Network (FFN) to produce the output node features. To the bestof our knowledge, this attention mechanism is unique because it acts on a neighborhood level, whichis more expressive than the graph attention architectures that only act on the node level. Readout Block.We use two readout layers for each graph attention block, which follows GemNet-OC [Gasteiger et al., 2022]. The first readout layer takes in the unaggregated messages from thegraph attention block and produces edge readout features. The second readout layer takes in theoutput node features from the node-wise FFN and produces node readout features. The node andedge readout features from all graph attention blocks are concatenated and passed into the outputblock for output prediction. Output Block.The output block takes the concatenated readout features and predicts the per-atomforces and system energy. The energy prediction is done by an FFN on the node readout features. Theforce prediction is divided into two parts: the force magnitude is predicted by an FFN on the nodereadout features, and the force direction is predicted by a transformation of the unit edge directionswith an FFN on the edge readout features. As opposed to GemNet [Gasteiger et al., 2022], the",
  "Length": ": Detailed illustration of the graph attention block. The input attributes are projected andconcatenated into a large message tensor. The tensor is fed into an optimized multi-head self-attentioncomputation, where the max number of neighbors dimension is the sequence length dimension. : EScAIP performance on the OC20 All+MD, OC20 2M, and OC22 datasets. The results arereported in Energy (eV) and Force (eV/) mean absolute error (MAE). EScAIP generally achievesthe best Energy and Force MAE among all current models. Due to its efficiency, EScAIP requiresless training time compared to the other models.",
  "GemNet-OC39M--70735.0EquiformerV2122M53126.7946227.1EScAIP-Medium146M51424.3247325.73": "transformation is not scalar but vector-valued. Thus, the predicted force direction is not equivariant torotations of the input data. In our experiments, we found this symmetry-breaking output block madethe model perform better. The reason could be that this formulation has more degrees of freedom andso is easier to optimize. We note that though the force direction is initially not equivariant, the trainedmodel is able to learn this symmetry from the data (See 5.4). We also note that predicting the force magnitude from node readout features is very helpful for energyprediction. The reason could be that the energy prediction is a global property of the molecularsystem, while the force magnitude is a local property of the atom. By guiding the model towards afine-grained force magnitude prediction, the model can learn a better representation of the system,which can in turn help it predict the system energy more accurately.",
  "Batch Size": "Memory (GB) 100 Atoms1000 Atoms : Inference runtime and memory usage comparison of EScAIP and baseline models on theOC20 dataset. Mean and standard deviation are reported across 16 randomly sampled batches perbatch size. Grey lines indicate the cumulative number of atoms in the batch. EScAIP not only scalesefficiently with batch size, but also exhibits minimal variation in performance across different batches.All reported results are tested on NVIDIA V100 32G.",
  "We evaluate on the S2EF task, which is the prediction of system energy and per-atomic force fromatomistic structure": "Settings.We use three variants of the EScAIP model: Small (83M), Medium (146M), and Large(317M). The models are trained to predict the energy and forces of each sample (S2EF task). Wetrain the model on the OC20 All+MD, OC20 2M, and OC22 splits. We evaluate the performance onthe four validation sets and test sets (both have 4M samples in total) and compare the results withEquiformerV2 [Liao et al., 2024], eSCN [Passaro and Zitnick, 2023b], SCN [Zitnick et al., 2022],and GemNet-OC [Gasteiger et al., 2022], the best performing models on this dataset. Results.The results of EScAIP on the Open Catalyst dataset are summarized in Tab. 1, whereEScAIP achieves state-of-the-art performance across all splits: OC20 2M, OC20 All+MD, and OC22.We note that we exclude models that train with a denoising objective (on OC22), as we focus onthe performance of the model architecture itself. There is a clear trend that increasing the modelsize improves the performance of EScAIP. Notably, even the Small model achieves competitiveperformance against other models while remaining significantly more efficient, making it suitablefor downstream, practical applications. More results on the scalability of the EScAIP model can befound in the Appendix B.1. Efficiency Comparisons.We provide the runtime and memory usage of EScAIP and other baselinemodels on the OC20 dataset in Tab. 2. EScAIP is approximately 10x faster and has 5x less memoryusage than an EquiformerV2 [Liao et al., 2024] model of comparable size. Additionally, as shown in, EScAIP scales effectively with batch size while maintaining minimal performance variationacross batches. This consistency is because EScAIPs input is padded to the maximum system size,enabling efficient use of PyTorchs compile feature. These qualities could make EScAIP well-suitedfor practical applications.",
  "Materials (MPTrj)": "Dataset.We evaluate EScAIPs performance on the Matbench-Discovery benchmark [Riebesellet al., 2023], a widely recognized benchmark for assessing models in new materials discovery. Themodel is trained on the MPTrj dataset [Deng et al., 2023], which consists of 1.6 million samples.This approach adheres to the compliant setting of the Matbench-Discovery benchmark. Settings.Given the relatively small dataset size, we use a small version of EScAIP with 45Mparameters. The model is trained to predict the energy, force, and stress of each sample. After trainingfor 100 training epochs, we increase the energy coefficient in the loss function and fine-tune the modelfor another 50 epochs. We evaluate the performance on the Matbench-Discovery benchmark andcompare the results with EquiformerV2 [Liao et al., 2024, Barroso-Luque et al., 2024], ORB MPTrj[Orbital-Materials, 2024], SevenNet [Park et al., 2024], and MACE [Batatia et al., 2023, 2022]-thetop compliant models on this benchmark. We note that we exclude models that train with a denoisingobjective, as we focus on the performance of the model architecture itself1. Results.The results of EScAIP on the Matbench-Discovery benchmark are summarized in Tab. 3.EScAIP achieves state-of-the-art performance on this benchmark, outperforming other models. In thecode release, we also provide the EScAIP model before the energy fine-tuning step, as this may bemore relevant for applications such as molecular dynamics simulations.",
  "Molecules (SPICE)": "Dataset.We evaluate the EScAIP models performance on the SPICE dataset [Eastman et al., 2023],which consists of approximately one million molecules across seven different categories. To ensurecomparability, we adopt the same training and evaluation settings as used for the MACE-OFF23model [Kovcs et al., 2023a]. Settings.We use a smaller EScAIP model with 45M parameters, trained to predict the energy andforces of each sample. The models performance is then evaluated on the different SPICE test datasetsand compared directly with MACE-OFF23 [Kovcs et al., 2023a].",
  "Based on the ORB technical report orb , it is possible that the ORB MPTrj model result reported herewas pre-trained with a denoising objective": ": To analyze rotational equivariance, on the validation set, we 1) predict forces on a set ofatomistic systems (A), 2) rotate the atomistic systems and predict forces (B), and then 3) compute thecosine similarity between the force predictions (B) and the rotated version of force predictions (A).After training EScAIP on different datasets, we find that the cosine similarity is consistently 0.99,meaning EScAIP is essentially always predicting the rotations correctly.",
  "Before Training0.21090.29400.21320.22870.2364After Training0.99810.99870.99940.99990.9999": "and passed through the trained model to obtain a force prediction (A). Next, we rotate the batch bya random angle and obtain a second force prediction (B) from the model. To quantify rotationalequivariance, we calculate the cosine similarity between prediction (B) and the rotated version ofprediction (A). This process is repeated for 128 batches, and we report the average cosine similarity. Results.The results of the rotational equivariance analysis are presented in Tab. 5, and the cosinesimilarity is consistently 0.99. These findings indicate that though EScAIP is not initially rota-tionally equivariant, after training it is able to correctly map input system rotations to output systempredictions. This could suggest that these symmetries can be effectively learned from the data.",
  "Conclusions": "We have investigated scaling strategies for developing neural network interatomic potentials (NNIPs)on large-scale datasets. Based on our investigations, we introduced a new NNIP architecture,Efficiently Scaled Attention Interatomic Potential (EScAIP), that leverages highly optimized self-attention mechanisms for scalability and expressivity. We demonstrated the effectiveness of EScAIPon a wide range of chemical datasets (OC20, OC22, MPTrj, SPICE) and showed that EScAIP achievestop performance on these different prediction tasks, while being much more efficient in training andinference runtime, as well as memory usage. We highlight some important takeaways from our workand the future of machine learning interatomic potentials more broadly: The sweet lesson.We note that our line of investigation in this work follows some of the generalprinciples of the bitter lesson [Sutton, 2019]. That is, strategies that focus on scaling and computetend to outperform those that try to embed domain knowledge into models. However, in this field, weprefer to think of this as a sweet lesson. Training large, constrained models requires significantlymore computational resources, making this feasible for only a limited number of researchers. Efficientscaling strategies thus democratize large-scale training and make it accessible to a broader community. Were still not giving enough credit to the data.Thus far, much of the effort in the NNIP fieldhas concentrated on model development. However, atomistic systems are far more complex thanthe domain-specific information being embedded into models. Predefined symmetry constraints andhandcrafted features offer only a simplistic representation of this complexity. A path forward tocapture these complexities is to focus on generating comprehensive datasets, ideally accompaniedby relevant evaluation metrics, allowing NNs to learn the rest of the information through gainingexpressivity via scaling. Future of NNIPs.As datasets continue to grow, training models from scratch on small datasets willlikely become unnecessary. While constraints may be beneficial in the very small data regime (thoughdata augmentation techniques can also help here), leveraging the representation of a pre-trained largemodel can serve as a starting point for fine-tuning on smaller datasets. This could make the verysmall dataset regime essentially a non-factor in the future, and it is likely that the need for NNIPswith built-in hard-constraints becomes even less necessary. Beyond focusing on data generation,other techniques are likely to gain importance in the NNIP domain. These include model distillation,general training and inference strategies that are model agnostic and can be applied to any NNIP, andapproaches to better connect with experimental results. Finally, more comprehensive strategies willbe important for evaluating NNIP accuracy and utility.",
  "and Disclosure of Funding": "This work was initially supported by Laboratory Directed Research and Development (LDRD)funding under Contract Number DE-AC02-05CH11231. It was then supported in part by the Office ofNaval Research (ONR) under grant N00014-23-1-2587. This research used resources of the NationalEnergy Research Scientific Computing Center (NERSC), a U.S. Department of Energy Office ofScience User Facility located at Lawrence Berkeley National Laboratory, operated under ContractNo. DE-AC02-05CH11231. We thank Ishan Amin, Sam Blau, Xiang Fu, Rasmus Hoegh, TobyKreiman, Jennifer Listgarten, Ryan Liu, Sanjeev Raja, and Brandon Wood for helpful discussionsand comments. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, ScottGray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models.arXiv preprint arXiv:2001.08361, 2020. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, ThomasUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit,and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale.In International Conference on Learning Representations, 2021. URL Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers.In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages1210412113, 2022.",
  "Volker L Deringer, Miguel A Caro, and Gbor Csnyi. Machine learning interatomic potentials asemerging tools for materials science. Advanced Materials, 31(46):1902765, 2019": "Oliver T Unke, Stefan Chmiela, Huziel E Sauceda, Michael Gastegger, Igor Poltavsky, Kristof TSchtt, Alexandre Tkatchenko, and Klaus-Robert Mller. Machine learning force fields. ChemicalReviews, 121(16):1014210186, 2021. Simon Batzner, Albert Musaelian, Lixin Sun, Mario Geiger, Jonathan P Mailoa, Mordechai Kornbluth,Nicola Molinari, Tess E Smidt, and Boris Kozinsky. E (3)-equivariant graph neural networks fordata-efficient and accurate interatomic potentials. Nature communications, 13(1):2453, 2022. Ilyes Batatia, David P Kovacs, Gregor Simm, Christoph Ortner, and Gbor Csnyi. Mace: Higherorder equivariant message passing neural networks for fast and accurate force fields. Advances inNeural Information Processing Systems, 35:1142311436, 2022. Yi-Lun Liao, Brandon M Wood, Abhishek Das, and Tess Smidt. Equiformerv2: Improved equivarianttransformer for scaling to higher-degree representations. In The Twelfth International Conferenceon Learning Representations, 2024. URL",
  "Johannes Gasteiger, Florian Becker, and Stephan Gnnemann. Gemnet: Universal directionalgraph neural networks for molecules. Advances in Neural Information Processing Systems, 34:67906802, 2021": "Johannes Gasteiger, Muhammed Shuaibi, Anuroop Sriram, Stephan Gnnemann, Zachary WardUlissi, C Lawrence Zitnick, and Abhishek Das. Gemnet-oc: Developing graph neural networksfor large and diverse molecular simulation datasets. Transactions on Machine Learning Research,2022. Lowik Chanussot, Abhishek Das, Siddharth Goyal, Thibaut Lavril, Muhammed Shuaibi, MorganeRiviere, Kevin Tran, Javier Heras-Domingo, Caleb Ho, Weihua Hu, Aini Palizhati, AnuroopSriram, Brandon Wood, Junwoong Yoon, Devi Parikh, C. Lawrence Zitnick, and Zachary Ulissi.Open catalyst 2020 (oc20) dataset and community challenges. ACS Catalysis, 11(10):60596072,2021. doi: 10.1021/acscatal.0c04525. Anubhav Jain, Shyue Ping Ong, Geoffroy Hautier, Wei Chen, William Davidson Richards, StephenDacek, Shreyas Cholia, Dan Gunter, David Skinner, Gerbrand Ceder, et al. Commentary: Thematerials project: A materials genome approach to accelerating materials innovation.APLmaterials, 1(1), 2013. Benjamin Lefaudeux, Francisco Massa, Diana Liskovich, Wenhan Xiong, Vittorio Caggiano, SeanNaren, Min Xu, Jieru Hu, Marta Tintore, Susan Zhang, Patrick Labatut, Daniel Haziza, LucaWehrstedt, Jeremy Reizenstein, and Grigory Sizov. xformers: A modular and hackable transformermodelling library. 2022. Nathaniel Thomas, Tess Smidt, Steven Kearnes, Lusann Yang, Li Li, Kai Kohlhoff, and Patrick Riley.Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point clouds.arXiv preprint arXiv:1802.08219, 2018.",
  "Bingqing Cheng. Cartesian atomic cluster expansion for machine learning interatomic potentials,2024": "Mojtaba Haghighatlari, Jie Li, Xingyi Guan, Oufan Zhang, Akshaya Das, Christopher J Stein, FarnazHeidar-Zadeh, Meili Liu, Martin Head-Gordon, Luke Bertels, et al. Newtonnet: a newtonianmessage passing network for deep learning of interatomic potentials and forces. Digital Discovery,1(3):333343, 2022. Peter Eastman, Pavan Kumar Behara, David L Dotson, Raimondas Galvelis, John E Herr, Josh THorton, Yuezhi Mao, John D Chodera, Benjamin P Pritchard, Yuanqing Wang, et al. Spice, adataset of drug-like molecules and peptides for training machine learning potentials. ScientificData, 10(1):11, 2023. Justin S Smith, Roman Zubatyuk, Benjamin Nebgen, Nicholas Lubbers, Kipton Barros, Adrian ERoitberg, Olexandr Isayev, and Sergei Tretiak. The ani-1ccx and ani-1x data sets, coupled-clusterand density functional theory properties for molecules. Scientific data, 7(1):134, 2020. Richard Tran, Janice Lan, Muhammed Shuaibi, Brandon M Wood, Siddharth Goyal, Abhishek Das,Javier Heras-Domingo, Adeesh Kolluru, Ammar Rizvi, Nima Shoghi, et al. The open catalyst 2022(oc22) dataset and challenges for oxide electrocatalysts. ACS Catalysis, 13(5):30663084, 2023. Luis Barroso-Luque, Muhammed Shuaibi, Xiang Fu, Brandon M. Wood, Misko Dzamba, MengGao, Ammar Rizvi, C. Lawrence Zitnick, and Zachary W. Ulissi. Open materials 2024 (omat24)inorganic materials dataset and models, 2024. Han Yang, Chenxi Hu, Yichi Zhou, Xixian Liu, Yu Shi, Jielan Li, Guanzhi Li, Zekun Chen, ShuizhouChen, Claudio Zeni, Matthew Horton, Robert Pinsler, Andrew Fowler, Daniel Zgner, Tian Xie,Jake Smith, Lixin Sun, Qian Wang, Lingyu Kong, Chang Liu, Hongxia Hao, and Ziheng Lu.Mattersim: A deep learning atomistic model across elements, temperatures and pressures, 2024.",
  "Paul J Steinhardt, David R Nelson, and Marco Ronchetti. Bond-orientational order in liquids andglasses. Physical Review B, 28(2):784, 1983": "Philippe Tillet, Hsiang-Tsung Kung, and David Cox. Triton: an intermediate language and compilerfor tiled neural network computations. In Proceedings of the 3rd ACM SIGPLAN InternationalWorkshop on Machine Learning and Programming Languages, pages 1019, 2019. Janosh Riebesell, Rhys EA Goodall, Anubhav Jain, Philipp Benner, Kristin A Persson, and Alpha ALee. Matbench discoveryan evaluation framework for machine learning crystal stability prediction.arXiv preprint arXiv:2308.14920, 2023. Bowen Deng, Peichen Zhong, KyuJung Jun, Janosh Riebesell, Kevin Han, Christopher J Bartel, andGerbrand Ceder. Chgnet as a pretrained universal neural network potential for charge-informedatomistic modelling. Nature Machine Intelligence, 5(9):10311041, 2023.",
  "Orbital-Materials. Orb forcefield models from orbital materialsorb forcefield models from orbitalmaterials. 2024. Accessed: 2024-10-29": "Yutack Park, Jaesun Kim, Seungwoo Hwang, and Seungwu Han. Scalable parallel algorithm for graphneural network interatomic potentials in molecular dynamics simulations. Journal of ChemicalTheory and Computation, 2024. Ilyes Batatia, Philipp Benner, Yuan Chiang, Alin M. Elena, Dvid P. Kovcs, Janosh Riebesell,Xavier R. Advincula, Mark Asta, William J. Baldwin, Noam Bernstein, Arghya Bhowmik,Samuel M. Blau, Vlad Carare, James P. Darby, Sandip De, Flaviano Della Pia, Volker L. De-ringer, Rokas Elijoius, Zakariya El-Machachi, Edvin Fako, Andrea C. Ferrari, Annalena Genreith-Schriever, Janine George, Rhys E. A. Goodall, Clare P. Grey, Shuang Han, Will Handley, Hendrik H.Heenen, Kersti Hermansson, Christian Holm, Jad Jaafar, Stephan Hofmann, Konstantin S. Jakob,Hyunwook Jung, Venkat Kapil, Aaron D. Kaplan, Nima Karimitari, Namu Kroupa, Jolla Kullgren,Matthew C. Kuner, Domantas Kuryla, Guoda Liepuoniute, Johannes T. Margraf, Ioan-BogdanMagdau, Angelos Michaelides, J. Harry Moore, Aakash A. Naik, Samuel P. Niblett, Sam WaltonNorwood, Niamh ONeill, Christoph Ortner, Kristin A. Persson, Karsten Reuter, Andrew S. Rosen,Lars L. Schaaf, Christoph Schran, Eric Sivonxay, Tams K. Stenczel, Viktor Svahn, ChristopherSutton, Cas van der Oord, Eszter Varga-Umbrich, Tejs Vegge, Martin Vondrk, Yangshuai Wang,William C. Witt, Fabian Zills, and Gbor Csnyi. A foundation model for atomistic materialschemistry. 2023.",
  "Richard Sutton. The bitter lesson. Incomplete Ideas (blog), 13(1):38, 2019": "Stefan Chmiela, Valentin Vassilev-Galindo, Oliver T Unke, Adil Kabylda, Huziel E Sauceda, Alexan-dre Tkatchenko, and Klaus-Robert Mller. Accurate global machine learning force fields formolecules with hundreds of atoms. Science Advances, 9(2):eadf0873, 2023. Yunyang Li, Yusong Wang, Lin Huang, Han Yang, Xinran Wei, Jia Zhang, Tong Wang, Zun Wang,Bin Shao, and Tie-Yan Liu. Long-short-range message-passing: A physics-informed frameworkto capture non-local interaction for scalable molecular dynamics simulation. In The TwelfthInternational Conference on Learning Representations, 2024. URL Dvid Pter Kovcs, Ilyes Batatia, Eszter Sra Arany, and Gbor Csnyi. Evaluation of the maceforce field architecture: From medicinal chemistry to materials science. The Journal of ChemicalPhysics, 159(4), 2023b. Sanjeev Raja, Ishan Amin, Fabian Pedregosa, and Aditi S Krishnapriyan. Stability-aware training ofneural network interatomic potentials with differentiable boltzmann estimators. arXiv preprintarXiv:2402.13984, 2024. Xiang Fu, Zhenghao Wu, Wujie Wang, Tian Xie, Sinan Keten, Rafael Gomez-Bombarelli, andTommi S Jaakkola. Forces are not enough: Benchmark and critical evaluation for machine learningforce fields with molecular simulations. Transactions on Machine Learning Research, 2023. 25%50%75%100% Portion of Training Set 2 101 3 101 Force MAE (meV/) AT + SC L=2 Slope=-0.32AT + SC L=4 Slope=-0.32Original L=6 Slope=-0.28More AT L=2 Slope=-0.40More AT L=4 Slope=-0.39 25%50%75%100% Portion of Training Set 2.8 102 2.9 102 3 102 3.1 102 3.2 102 Energy MAE (meV) AT + SC L=2 Slope=-0.10AT + SC L=4 Slope=-0.08Original L=6 Slope=-0.06More AT L=2 Slope=-0.11More AT L=4 Slope=-0.09 : Force MAE vs.Training Dataset Size for EquiformerV2 ablation study on the OC20 2Mdataset. Slope is fitted by linear regression. We scale the parameters of the original L = 2 and L = 4models from Liao et al. through the attention mechanisms and/or spherical channels, suchthat the number of parameters is approximately equal to the original L = 6 model. As the trainingdataset size increases, the scaled L = 2 and L = 4 models have a steeper slope, indicating fasterperformance improvement with increasing data.",
  "We provide additional details on our investigations from 3": "Results of Ablation Study comparing Force MAE vs. Training Dataset Size.To further inves-tigate how scaling efficiency varies as a function of training dataset size, we train the parameter-controlled Equiformer V2 models with different amounts of training data. The results in showthat the scaled L = 2 and L = 4 models exhibit a steeper performance improvement (log-log slope)compared to the original L = 6 model. In particular, the More AT configuration (more attention) hasa steeper log-log slope compared to the AT + SC configuration and the original L = 6 model. Thissuggests that increasing the complexity of the attention mechanisms is a more effective strategy forscaling with increasing training dataset size, rather than increasing L.",
  "B.1Catalysts (OC20 and OC22)": "To illustrate EScAIPs scalability, we train the model on varying sizes of training data and modelconfigurations. The results, shown in , indicate a clear trend: as model and data sizes grow,EScAIPs performance continues to improve. We also include results to complement : the sameefficiency, performance, and scaling comparisons between EScAIP and baseline models on the OpenCatalyst dataset for Energy MAE (meV ) vs. Inference Speed (Sample/Sec ) and Energy MAE vs.Memory (GB/Sample ). The trend is similar to the forces MAE results, and EScAIP achieves betterperformance with smaller time and memory cost.",
  "B.2Large Molecules (MD22)": "Dataset.We evaluate the performance of our EScAIP model on the MD22 dataset [Chmiela et al.,2023], which consists of seven molecular systems with varying sizes. It consists of energy and forcelabels calculated from DFT simulations. Settings.We use an EScAIP model 15M parameters on each system and evaluate the performanceon the test set (train-test split 95:5). The model is compared with MACE [Batatia et al., 2022], VisNet-LSRM [Li et al., 2024], and sGDML [Chmiela et al., 2023]. We note that there were discrepancy ofresults of VisNet-LSRM in the MACE [Kovcs et al., 2023b] paper and the VisNet-LSRM [Li et al.,2024], thus we reported both in the table. We use the same train/validation splits as the baselines[Chmiela et al., 2023], where the training set is hundreds to thousands of samples, and also applydata augmentation (randomly rotating each training sample 16 times). Inference Speed (Sample/Sec) Energy MAE (meV) 31M153M 200M 216M 83M 146M 317M EquiformerV2eSCNGemNet-OCEScAIP Inference Memory (GB/Sample) Energy MAE (meV) 31M153M 200M 216M 83M 146M 317M EquiformerV2eSCNGemNet-OCEScAIP : Efficiency, performance, and scaling comparisons between EScAIP and baseline models onthe Open Catalyst dataset. Energy MAE (meV ) vs. Inference Speed (Sample/Sec ) and EnergyMAE vs. Memory (GB/Sample ) is reported. EScAIP achieves better performance with smallertime and memory cost. 500k 1M2M172M All+MD",
  "Training Data Size": "Energy MAE (meV) EScAIP 45MEScAIP 83MEScAIP 146MEquiformerV2 31MEquiformerV2 83MEquiformerV2 134MEquiformerV2 153M : Scaling experiment of EScAIP on OC20. Forces MAE (meV/) and Energy (meV) across4 validation splits are reported. For 500k, 1M, and 2M split, the EScAIP model is trained for 30epochs; for All+MD, the EScAIP model is trained for 8 epochs. Force and Energy MAE consistentlydecreases as model size and training data size increases. After we train the model, we also run molecular dynamics (MD) simulations to check the stability ofthe potential and evaluate how well the simulation recovers the distribution of interatomic distances,h(r), in the simulation [Raja et al., 2024, Fu et al., 2023]. We use the simulation setup from Fu et al. and run the simulation for 200000 steps (100 ps) using the Langevin integrator with a frictioncoefficient of 0.5. The temperature is set to 500 K. Results.The results of EScAIP on the MD22 dataset are summarized in Tab. 6. EScAIP outper-forms other models in both energy and force prediction, especially for large molecules. The lowh(r) error in the MD simulation also indicates that the model is able to capture this observableaccurately. Interestingly, MD22 is not a particularly large dataset: the training dataset sizes are in thethousands. Despite this, a scalable architecture with high parameter counts is still able to achievegood performance.",
  "NeurIPS Paper Checklist": "The checklist is designed to encourage best practices for responsible machine learning research,addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not removethe checklist: The papers not including the checklist will be desk rejected. The checklist shouldfollow the references and precede the (optional) supplemental material. The checklist does NOTcount towards the page limit.",
  "Please provide a short (12 sentence) justification right after your answer (even for NA)": "The checklist answers are an integral part of your paper submission. They are visible to thereviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it(after eventual revisions) with the final version of your paper, and its final version will be publishedwith the paper. The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation.While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided aproper justification is given (e.g., \"error bars are not reported because it would be too computationallyexpensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering\"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, weacknowledge that the true answer is often more nuanced, so please just use your best judgment andwrite a justification to elaborate. All supporting evidence can appear either in the main paper or thesupplemental material, provided in appendix. If you answer [Yes] to a question, in the justificationplease point to the section(s) where related material for the question can be found.",
  "The answer NA means that the paper has no limitation while the answer No means thatthe paper has limitations, but those are not discussed in the paper": "The authors are encouraged to create a separate \"Limitations\" section in their paper. The paper should point out any strong assumptions and how robust the results are toviolations of these assumptions (e.g., independence assumptions, noiseless settings,model well-specification, asymptotic approximations only holding locally). The authorsshould reflect on how these assumptions might be violated in practice and what theimplications would be. The authors should reflect on the scope of the claims made, e.g., if the approach wasonly tested on a few datasets or with a few runs. In general, empirical results oftendepend on implicit assumptions, which should be articulated. The authors should reflect on the factors that influence the performance of the approach.For example, a facial recognition algorithm may perform poorly when image resolutionis low or images are taken in low lighting. Or a speech-to-text system might not beused reliably to provide closed captions for online lectures because it fails to handletechnical jargon.",
  "If applicable, the authors should discuss possible limitations of their approach toaddress problems of privacy and fairness": "While the authors might fear that complete honesty about limitations might be used byreviewers as grounds for rejection, a worse outcome might be that reviewers discoverlimitations that arent acknowledged in the paper. The authors should use their bestjudgment and recognize that individual actions in favor of transparency play an impor-tant role in developing norms that preserve the integrity of the community. Reviewerswill be specifically instructed to not penalize honesty concerning limitations.",
  "If the contribution is a dataset and/or model, the authors should describe the steps takento make their results reproducible or verifiable": "Depending on the contribution, reproducibility can be accomplished in various ways.For example, if the contribution is a novel architecture, describing the architecture fullymight suffice, or if the contribution is a specific model and empirical evaluation, it maybe necessary to either make it possible for others to replicate the model with the samedataset, or provide access to the model. In general. releasing code and data is oftenone good way to accomplish this, but reproducibility can also be provided via detailedinstructions for how to replicate the results, access to a hosted model (e.g., in the caseof a large language model), releasing of a model checkpoint, or other means that areappropriate to the research performed. While NeurIPS does not require releasing code, the conference does require all submis-sions to provide some reasonable avenue for reproducibility, which may depend on thenature of the contribution. For example(a) If the contribution is primarily a new algorithm, the paper should make it clear howto reproduce that algorithm.",
  "(b) If the contribution is primarily a new model architecture, the paper should describethe architecture clearly and fully": "(c) If the contribution is a new model (e.g., a large language model), then there shouldeither be a way to access this model for reproducing the results or a way to reproducethe model (e.g., with an open-source dataset or instructions for how to constructthe dataset). (d) We recognize that reproducibility may be tricky in some cases, in which caseauthors are welcome to describe the particular way they provide for reproducibility.In the case of closed-source models, it may be that access to the model is limited insome way (e.g., to registered users), but it should be possible for other researchersto have some path to reproducing or verifying the results.",
  ". Experimental Setting/Details": "Question: Does the paper specify all the training and test details (e.g., data splits, hyper-parameters, how they were chosen, type of optimizer, etc.) necessary to understand theresults?Answer: [Yes]Justification: Yes, we provide this in the experiments section.Guidelines: The answer NA means that the paper does not include experiments. The experimental setting should be presented in the core of the paper to a level of detailthat is necessary to appreciate the results and make sense of them.",
  ". Experiment Statistical Significance": "Question: Does the paper report error bars suitably and correctly defined or other appropriateinformation about the statistical significance of the experiments?Answer: [No]Justification: Due to computational cost, we are not able to provide error bars for everyexperiment.Guidelines: The answer NA means that the paper does not include experiments. The authors should answer \"Yes\" if the results are accompanied by error bars, confi-dence intervals, or statistical significance tests, at least for the experiments that supportthe main claims of the paper. The factors of variability that the error bars are capturing should be clearly stated (forexample, train/test split, initialization, random drawing of some parameter, or overallrun with given experimental conditions).",
  ". Experiments Compute Resources": "Question: For each experiment, does the paper provide sufficient information on the com-puter resources (type of compute workers, memory, time of execution) needed to reproducethe experiments?Answer: [Yes]Justification: Yes, we provide computational details.Guidelines: The answer NA means that the paper does not include experiments. The paper should indicate the type of compute workers CPU or GPU, internal cluster,or cloud provider, including relevant memory and storage.",
  ". Code Of Ethics": "Question: Does the research conducted in the paper conform, in every respect, with theNeurIPS Code of Ethics [Yes]Justification: We have reviewed this and the research conforms to this code.Guidelines: The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. If the authors answer No, they should explain the special circumstances that require adeviation from the Code of Ethics.",
  ". Broader Impacts": "Question: Does the paper discuss both potential positive societal impacts and negativesocietal impacts of the work performed?Answer: [Yes]Justification: Yes, this is also discussed in the introduction.Guidelines: The answer NA means that there is no societal impact of the work performed. If the authors answer NA or No, they should explain why their work has no societalimpact or why the paper does not address societal impact. Examples of negative societal impacts include potential malicious or unintended uses(e.g., disinformation, generating fake profiles, surveillance), fairness considerations(e.g., deployment of technologies that could make decisions that unfairly impact specificgroups), privacy considerations, and security considerations. The conference expects that many papers will be foundational research and not tiedto particular applications, let alone deployments. However, if there is a direct path toany negative applications, the authors should point it out. For example, it is legitimateto point out that an improvement in the quality of generative models could be used togenerate deepfakes for disinformation. On the other hand, it is not needed to point outthat a generic algorithm for optimizing neural networks could enable people to trainmodels that generate Deepfakes faster. The authors should consider possible harms that could arise when the technology isbeing used as intended and functioning correctly, harms that could arise when thetechnology is being used as intended but gives incorrect results, and harms followingfrom (intentional or unintentional) misuse of the technology. If there are negative societal impacts, the authors could also discuss possible mitigationstrategies (e.g., gated release of models, providing defenses in addition to attacks,mechanisms for monitoring misuse, mechanisms to monitor how a system learns fromfeedback over time, improving the efficiency and accessibility of ML).",
  "Guidelines:": "The answer NA means that the paper does not use existing assets. The authors should cite the original paper that produced the code package or dataset. The authors should state which version of the asset is used and, if possible, include aURL. The name of the license (e.g., CC-BY 4.0) should be included for each asset. For scraped data from a particular source (e.g., website), the copyright and terms ofservice of that source should be provided. If assets are released, the license, copyright information, and terms of use in thepackage should be provided. For popular datasets, paperswithcode.com/datasetshas curated licenses for some datasets. Their licensing guide can help determine thelicense of a dataset.",
  "According to the NeurIPS Code of Ethics, workers involved in data collection, curation,or other labor should be paid at least the minimum wage in the country of the datacollector": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with HumanSubjectsQuestion: Does the paper describe potential risks incurred by study participants, whethersuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)approvals (or an equivalent approval/review based on the requirements of your country orinstitution) were obtained?Answer: [NA]Justification: The paper does not involve this.Guidelines:",
  "The answer NA means that the paper does not involve crowdsourcing nor research withhuman subjects": "Depending on the country in which research is conducted, IRB approval (or equivalent)may be required for any human subjects research. If you obtained IRB approval, youshould clearly state this in the paper. We recognize that the procedures for this may vary significantly between institutionsand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and theguidelines for their institution."
}