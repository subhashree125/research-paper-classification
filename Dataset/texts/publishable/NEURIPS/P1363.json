{
  "Abstract": "The rise of bedroom producers has democratized music creation, while chal-lenging producers to objectively evaluate their work. To address this, we presentAI TrackMate, an LLM-based music chatbot designed to provide constructivefeedback on music productions. By combining LLMs inherent musical knowledgewith direct audio track analysis, AI TrackMate offers production-specific insights,distinguishing it from text-only approaches. Our framework integrates a MusicAnalysis Module, an LLM-Readable Music Report, and Music Production-OrientedFeedback Instruction, creating a plug-and-play, training-free system compatiblewith various LLMs and adaptable to future advancements. We demonstrate AITrackMates capabilities through an interactive web interface and present findingsfrom a pilot study with a music producer. By bridging AI capabilities with the needsof independent producers, AI TrackMate offers on-demand analytical feedback, po-tentially supporting the creative process and skill development in music production.This system addresses the growing demand for objective self-assessment tools inthe evolving landscape of independent music production.",
  "Introduction": "The advancement of music technology has revolutionized music production, consolidating variousroles into the bedroom producer . This shift has democratized music creation, allowingindividuals with limited musical background to produce finished pieces. However, this solitaryproduction environment presents challenges, particularly in objectively evaluating ones work anddeveloping critical listening skills . The lack of regular and objective feedback in isolated production environments threatens to limit thepotential of many talented independent producers . Traditionally, these skills are developed inhigher education or professional studio settings , which may be inaccessible to many bedroomproducers. Recent advances in AI technology, particularly Large Language Models (LLMs), offer apromising solution to this dilemma. LLMs have demonstrated impressive capabilities in various domains , including music. Recentworks have used LLMs for music captioning , understanding, and reasoning . However,these approaches face limitations when applied to providing constructive feedback to independentmusic producers. They often lack specialized production knowledge or focus more on musictheory with symbolic representation than production with audio track , with limited utility forproducers working primarily with audio tracks.",
  "arXiv:2412.06617v1 [cs.SD] 9 Dec 2024": "To address these limitations, we propose AI TrackMate, an LLM-based music chatbot designed toprovide objective and constructive feedback on music productions. Unlike previous approaches thatrequire the training of new models or the fine-tuning of existing ones , AI TrackMate lever-ages the inherent musical knowledge of LLMs , focusing on providing detailed track informationand guiding the LLM to think like a music producer. Our framework consists of three key components: Music Analysis Module, LLM Readable MusicReport, and Music Production-Oriented Feedback Instruction. This design creates a plug-and-play,training-free system compatible with any LLM , including those fine-tuned on open-source models and future iterations with enhanced music understanding capabilities. We demonstrate AITrackMate through an interactive web-based interface, allowing music producers to easily uploadtheir tracks and receive structured analytical feedback based on multiple musical aspects, supportingtheir production decision-making process. Moreover, we conducted a pilot study evaluating thesystems effectiveness, involving an in-depth qualitative interview with a real music producer. AI TrackMate represents a significant step forward in AI-assisted music production, providing de-tailed analytical feedback on submitted music tracks. By bridging the gap between AI capabilitiesand the specific needs of independent music producers, AI TrackMate has the potential to signif-icantly enhance the creative process and accelerate skill development in the evolving landscapeof music production. Our interactive demo can be checked in providing hands-on experience with comprehensive track analysis andinsights into its real-world application and impact.",
  "System": ": The system comprises three layers: (1) User Interface for audio upload, query input,and feedback reception; (2) Data Processing for handling raw audio and text; and (3) AI Analysis,featuring a Music Analysis Module that transforms raw audio into LLM-readable report, and an LLMthat processes these reports along with user queries. Guided by music production-oriented feedbackinstructions, the LLM generates insights comparable to those of a music producer. Our system delivers AI-generated feedback through four core components, as illustrated in :Music Analysis Module, LLM Readable Report, Music Production-Oriented Feedback Instruction,and Web-based UI. The Music Analysis Module examines audio input across multiple aspects,mimicking a producers detailed ear. LLM Readable Report structures this analysis, similar to howproducers organize their thoughts before giving feedback. Music Production-Oriented FeedbackInstructions guide the LLM, akin to a seasoned producer drawing on experience to form critiques.These components integrate into a user-friendly web UI for easy interaction. In the following section,we will explain the methodology and functionality of each component in detail, demonstrating howthey work together to provide comprehensive music analysis and production quality feedback.",
  "Music Analysis Module": "Rhythm, harmony and sound design constitute the core elements of music production. To facilitatean LLMs comprehension of these aspects, we conduct a comprehensive analysis of each element.For rhythm, this analysis encompasses onset detection, beat-and-downbeat tracking, and tempoestimation. With respect to harmony, we concentrate on key classification and chord recognition.In terms of sound design, we perform instrument recognition and extract timbral characteristics. In addition, to allow an LLM to grasp the expressive and subjective dimensions of music, we implementgenre, theme, and emotion classification. Recognizing that most tracks exhibit diverse emotionsand instruments across various sections, we extend our predictions of emotion and instrumentationbeyond the track level to include structure-level analysis. We employ All-In-One for detecting beats and downbeats, estimating tempo, and segmentingstructure. We utilize Madmom for onset detection and key classification, and autochord forchord recognition. To extract timbral characteristics, we rely on timbral models by AudioCommons 1.Additionally, Essentia is leveraged for instrument recognition, theme classification, and emotionclassification.",
  "LLM Readable Music Report": "We implement an iterative refinement process to optimize LLMs interpretation of music analysisresults, ensuring alignment with musicians needs. This process commences by feeding raw meta-data(including genre, chords, rhythm, and emotion metrics) directly to the LLM. Initial interpretationsare typically suboptimal due to the datas complexity. For example, in chord analysis, raw data mightinclude time-stamped chord labels (e.g., end: 4.37, label: G:min, start: 0.79). The LLMsinitial analysis often identifies dominant chords and general texture but lacks depth. Subsequently, weaugment the data based on identified gaps by calculating additional statistical metrics. In our chordanalysis example, we introduce metrics such as total chord changes, dominant chord identification,and major/minor chord counts. This refinement continues for 2-3 iterations, each enhancing theLLMs understanding. By the final iteration, we typically incorporate more nuanced metrics such asaverage chord duration and common progressions. This enables the LLM to generate comprehensiveinsights, including analyses of chord usage patterns, transition pacing, and emotional complexity.The process concludes with a secondary LLM that evaluates the output of each iteration for clarity,accuracy, and relevance, selecting the most insightful representation. This method demonstrablyenhances the LLMs capacity to generate meaningful, musician-relevant insights from complexmusical data",
  "Foundational Prompt Template": "We structure our prompt into three distinct parts: primary function, track scoring process, and trackimprovement suggestion. In the primary function component, we provide role-specific instructions,ensuring the LLM understands its role as a music feedback provider. We design the track scoringprocess to avoid excessive praise and identify areas needing improvement. To guide the LLM inassigning scores, we introduce predefined categories such as Creativity and Originality, Genre Fidelity,Conveyability, Musical Richness, and Track Memorability. This approach addresses LLMs tendencyto offer only positive assessments, which often lack critical suggestions for music improvement.Finally, we include a track improvement suggestion section, guiding the LLM through a process toprovide meaningful feedback. We will elaborate on the details of this track improvement suggestionprocess in the following section",
  "We apply several prompt engineering techniques and self-designed strategies to ensure the LLMsmusic understanding is meaningful to musicians": "Graph-of-Thought (GoT). The GoT prompting technique has demonstrated significant potential inenhancing LLMs reasoning capabilities . In our study, we apply this approach to the complexdomain of music production by deconstructing high-level thinking processes into several key factors.These factors encompass both objective and subjective elements of musical composition and analysis.We consider technical aspects such as instruments, rhythm, and timbre for the objective side. To complement these, we incorporate subjective elements, including musics emotion and theme. Bysystematically integrating these objective and subjective dimensions, we enable the LLM to interpretand analyze musical pieces with greater accuracy, thus bridging the gap between technical analysisand artistic interpretation. We present an illustrative example in . : Graph of Thoughts (GoT) approach applies to analyze dominant instruments impacton a tracks emotional tone. Our workflow progresses through generate (T1, T3), aggregate (T2,T4b), and refine (T4a, T5) transformations, adhering to the original GoT framework. We representanalytical steps as nodes (N1-N11), demonstrating how GoT decomposes complex musical analysisinto interconnected nodes. This structure elucidates how instrument interactions contribute to thetracks overall emotional impact. Through this application of GoT, we enable a systematic explorationof the relationship between instrumental composition and emotional resonance in music. Feedback Mechanism. Our feedback mechanism is designed to enhance the interaction betweenLLM and musicians, focusing on providing actionable insights and maintaining user engagement.We integrate data-driven analysis with contextual understanding, addressing key aspects of musicalcomposition such as melody, harmony, rhythm, and production techniques. Through iterative testing,we refine the instruction set to mitigate the LLMs tendency towards shallow data parroting, insteadpromoting critical thinking and deeper analysis. Our system employs clear, constructive language todeliver balanced critiques, ensuring feedback is both informative and motivating. To enhance userexperience, we implement a conversational tone reminiscent of a friendly rapper, utilizing metaphorsand analogies to elucidate complex musical concepts. This strategy is coupled with the integrationof tailored questions, significantly improving user engagement and facilitating a more dynamic,user-centered interaction. Our research demonstrates that this refined feedback mechanism not onlyprovides musicians with practical, applicable advice but also fosters a more engaging and productivedialogue between the AI system and its users. Person Switching.The person-switching strategy is incorporated in our instruction design tooptimize the LLMs performance in music evaluation tasks. We strategically employ differentgrammatical persons to delineate various aspects of the LLMs operation. We utilize the first-personperspective to foster an internalized evaluation process, encouraging the LLM to adopt a moreengaged and personal stance. This technique aims to enhance the depth of analysis and promoteactive reasoning. Concurrently, we employ second-person constructions when presenting objectiveinformation or factual content, creating a clear demarcation between the LLMs internal reasoningprocesses and external reference points. This distinction aids in maintaining accuracy and properattribution in the LLMs responses. We use imperative sentences to convey non-negotiable rules andguidelines, ensuring strict adherence to the established evaluation framework and maintaining theintegrity of the assessment process. Our person-switching methodology effectively differentiatesbetween the LLMs thought processes, factual knowledge, and operational constraints, resultingin more nuanced and contextually appropriate responses in music evaluation tasks. By carefully",
  "Web-based UI": "The web application is built using the Gradio framework. The typical workflow involves usersuploading audio files or pasting YouTube links for analysis. Our system conducts various analyses,generates a music report, and provides it to the LLM. The LLM then offers initial scoring andimprovement suggestions in the chat window. Users can pose follow-up questions through thetext input interface, facilitating an interactive, user-driven analysis process that transforms howindependent producers receive personalized feedback on their work. : The user interface consists of: (1) Audio input components for file upload or YouTubelink input. (2) A chat window displaying the LLMs scoring and suggestions. (3) A text input foruser questions and LLM responses.",
  "Pilot Study": "We conducted an exploratory pilot study with a music producer to gather initial insights about oursystems approach and potential impact. The study consisted of a questionnaire, an onboardingsession, an AI-assisted track analysis, and a recorded conversation, concluding with a semi-structuredinterview. Our analysis synthesizes observations from the producer profile, AI-producer dialogue,and interview feedback to understand how the system performs in a real-world scenario.",
  "Producer Profiles": "Our pilot study participant was a 24-year-old self-taught producer with four years of experience,representing an example of the \"bedroom producer\" demographic. His experience provided insightinto the challenges of skill development and self-evaluation in isolated environments. When discussingpeer feedback, he noted its subjective limitations, stating: Some peoples suggestions are completelybased on their own preferences, which only have a little reference value for me. This individualcase suggests potential value in more objective, intention-aligned feedback systems. The participantshigh comfort with music technology (10/10 rating for AI tools) indicated readiness to engage withAI-assisted systems, though broader studies would be needed to assess general producer attitudes.",
  "The AI-Producer dialogue provided several interesting observations about interaction patterns andsystem capabilities. The conversation naturally progressed from general queries (Why are my lyrics": "and melody disconnected?) to specific technical discussions (How should I modify my chordprogression?), indicating a progression toward more focused technical inquiries. This evolutionsuggests potential value in facilitating structured technical discussions for solitary producers. The dialogue culminated with the producer asking, If modifications were made based on yoursuggestions, how would the emotional flow of the track be after these changes? Can you help mecompare the emotional analysis of the modified track with the unmodified original version? Inresponse, the AI provided a comparative analysis: The existing version has relatively smoothemotional changes between verse and chorus, but lacks clear emotional highs and lows. The modifiedversion maintains overall warmth consistency while introducing richer chord changes, giving thechorus section more emotional layers and dynamism. This example shows how the system attemptsto link technical modifications with their potential emotional impact, illustrating an approach tointegrating technical and perceptual feedback in music production. Analysis of the dialogue content showed that 75% of AI responses combined technical suggestionswith emotional/perceptual feedback. This observation suggests a possible framework for balancingtechnical and artistic elements in AI-assisted music production feedback.",
  "Producer Opinion on AI Feedback": "Initial feedback from our pilot study participant offered exploratory insights into the systemscurrent capabilities and limitations. In examining the tools analysis approach, the participantnoted: It considers aesthetic aspects from a technical perspective and thinks about different partsin a comprehensive way, suggesting one possible method for connecting technical and aestheticfeedback. When discussing comparative experiences with peer feedback, they observed that thesystem gradually delves into the details that we music creators care about, indicating potentialadvantages of structured analytical approaches. The participant also noted the systems handling ofemotional themes and instrumentation analysis. Through the exploratory session, several observations emerged about the systems current implemen-tation. The participant found the feedback structure helpful for their creative process, noting potentialapplications for skill development. For novice musicians in particular, they suggested the systemmight offer guidance on conceptual aspects not commonly found in general tutorials. The feedbacksession also revealed several areas for potential enhancement, including more detailed mixing analy-sis, better DAW integration, improved lyrics and vocal analysis, and comparison capabilities withreference tracks. A key suggestion focused on workflow integration, specifically the possibility ofin-DAW feedback during the production process. While these observations from a single user study cannot be generalized, they provide useful directionsfor future investigation. The feedback highlights both promising aspects and areas needing refinement,offering specific technical and conceptual considerations for future development of AI-assisted musicproduction tools.",
  "Conclusion & Discussion": "In this paper, we propose AI TrackMate, an innovative LLM-based music chatbot for production-oriented feedback. Our system leverages LLMs inherent musical knowledge, integrating a MusicAnalysis Module, LLM Readable Music Report, and Music Production-Oriented Feedback Instruc-tions. The latter employs advanced techniques like Graph-of-Thought prompting to guide the LLMin thinking and responding like an experienced music producer, providing comprehensive, tailoredfeedback for bedroom producers. Our current implementation has several inherent limitations that point to future work directions.The system primarily focuses on conventional music structures, suggesting the need for expandedgenre coverage including classical and experimental music. Technical constraints in mixing analysis,vocal interpretation, and lyrical content analysis could be addressed through enhanced capabilities.Additionally, moving from the current per-submission model to real-time analysis could enable betterintegration with active production workflows. Our exploratory pilot study provided initial insightsinto these aspects, and we leave comprehensive evaluation studies across various genres and skilllevels as important directions for future research. Abubakar Abid, Ali Abdalla, Ali Abid, Dawood Khan, Abdulrahman Alfozan, and JamesZou. Gradio: Hassle-free sharing and testing of ml models in the wild. In Proceedings ofthe Machine Learning Conference, 2019. doi: 10.48550/arXiv.1906.02569. URL Christopher John Bayron. Autochord: Automatic chord recognition library and chord visu-alization app. In Proceedings of the International Society for Music Information RetrievalConference (ISMIR), Manila, Philippines, 2021. Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, LukasGianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, et al. Graphof thoughts: Solving elaborate problems with large language models. In Proceedings of theAAAI Conference on Artificial Intelligence, volume 38(16), pages 1768217690, 2024. Sebastian Bck, Filip Korzeniowski, Jan Schlter, Florian Krebs, and Gerhard Widmer. mad-mom: a new Python Audio and Music Signal Processing Library. In Proceedings of the 24thACM International Conference on Multimedia, pages 11741178, Amsterdam, The Netherlands,10 2016. doi: 10.1145/2964284.2973795. Dmitry Bogdanov, Nicolas Wack, Emilia Gmez, Sankalp Gulati, Perfecto Herrera, O. Mayor,Gerard Roma, Justin Salamon, J. R. Zapata, and Xavier Serra. Essentia: an audio analysislibrary for music information retrieval. In Proceedings of the International Society for MusicInformation Retrieval Conference (ISMIR), pages 493498, 2013.",
  "Darro Chea. Music production & self-producing: A case study. Masters thesis, Berklee Collegeof Music, Valencia Campus, 2017": "Qixin Deng, Qikai Yang, Ruibin Yuan, Yipeng Huang, Yi Wang, Xubo Liu, Zeyue Tian, JiahaoPan, Ge Zhang, Hanfeng Lin, Yizhi Li, Yinghao Ma, Jie Fu, Chenghua Lin, Emmanouil Benetos,Wenwu Wang, Guangyu Xia, Wei Xue, and Yike Guo. Composerx: Multi-agent symbolic musiccomposition. arXiv:2404.18081, 2024. SeungHeon Doh, Keunwoo Choi, Jongpil Lee, and Juhan Nam. Lp-musiccaps: Llm-basedpseudo music captioning. In Proceedings of the International Society for Music InformationRetrieval Conference (ISMIR), 2023. Josh Gardner, Simon Durand, Daniel Stoller, and Rachel M. Bittner. Llark: A multimodalinstruction-following language model for music. In Proceedings of the International Conferenceon Machine Learning, 2023. Taejun Kim and Juhan Nam. All-in-one metrical and functional structure analysis with neigh-borhood attentions on demixed audio. In IEEE Workshop on Applications of Signal Processingto Audio and Acoustics (WASPAA), 2023.",
  "Daniel Walzer. Towards an understanding of creativity in independent music production.Creative Industries Journal, 2021": "Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, XiaoyunZhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen W White, Doug Burger, andChi Wang. Autogen: Enabling next-gen llm applications via multi-agent conversation. arXivpreprint arXiv:2308.08155, 2023. Ruibin Yuan, Hanfeng Lin, Yi Wang, Zeyue Tian, Shangda Wu, Tianhao Shen, Ge Zhang,Yuhang Wu, Cong Liu, and Ziya Zhou. Chatmusician: Understanding and generating musicintrinsically with llm. arXiv:2402.16153, 2024."
}