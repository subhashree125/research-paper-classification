{
  "Abstract": "Mobile devices such as smartphones, laptops, and tablets can often connect tomultiple access networks (e.g., Wi-Fi, LTE, and 5G) simultaneously. Recent ad-vancements facilitate seamless integration of these connections below the transportlayer, enhancing the experience for apps that lack inherent multi-path support.This optimization hinges on dynamically determining the traffic distribution acrossnetworks for each device, a process referred to as multi-access traffic splitting.This paper introduces NetworkGym, a high-fidelity network environment simulatorthat facilitates generating multiple network traffic flows and multi-access trafficsplitting. This simulator facilitates training and evaluating different RL-basedsolutions for the multi-access traffic splitting problem. Our initial explorationsdemonstrate that the majority of existing state-of-the-art offline RL algorithms(e.g. CQL) fail to outperform certain hand-crafted heuristic policies on average.This illustrates the urgent need to evaluate offline RL algorithms against a broaderrange of benchmarks, rather than relying solely on popular ones such as D4RL.We also propose an extension to the TD3+BC algorithm, named Pessimistic TD3(PTD3), and demonstrate that it outperforms many state-of-the-art offline RL algo-rithms. PTD3s behavioral constraint mechanism, which relies on value-functionpessimism, is theoretically motivated and relatively simple to implement.",
  "Introduction": "There exists a general lack of standardized benchmarks for reinforcement learning (RL) in thedomain of computer networking. Whereas RL has shown promise in addressing various challenges incomputer networking, such as congestion control, routing, and resource allocation, the field lackswidely accepted benchmarks that would facilitate systematic evaluation and comparison of differentRL approaches. Hence, we propose NetworkGym, a high-fidelity, end-to-end, full-stack networkSimulation-as-a-Service framework that leverages open-source network simulation tools, such asns-3 Henderson et al. . Furthermore, NetworkGym offers a closed-loop machine learning (ML)algorithm development and training pipeline via open-source gym-like APIs. The components ofNetworkGym achieve the following objectives:",
  "arXiv:2411.04138v1 [cs.NI] 30 Oct 2024": ": GMA Protocol. A UE interfaces with the GMA gateway over UDP. \"APP\" refers to theapplication layer at the client or server level, \"IP\" refers to the Internet Protocol layer, facilitating theaddressing and routing of packets, and \"PHY\" refers to the physical layer in the network responsiblefor the actual transmission of data over the network medium. The GMA gateway handles multi-accesstraffic splitting at the edge. Open APIs for ML Training and Data Collection: The Agent is fully customizable and controlledby the developer. The network simulation Environment is hosted in the cloud. By utilizing theopen-source NetworkGym Client and APIs, an Agent can interact with an Environment to collectmeasurement data and take actions that allow training for the desired use case. Flexibility of Programming Language: The separation of Agent and Environment provides thefreedom to employ different programming languages for the ML algorithm and network simulation.For instance, a Python-based Agent can smoothly interact with a C++ (ns-3) based simulationEnvironment. This is a critical aspect of our framework, as previous networking frameworkswould have required modern ML algorithms to be coded in the same language(s) as the simulationenvironment. Independent and Modular Deployment: Such separation also allows the Agent and Environmentto be deployed on different machines or platforms, optimized for specific workloads. For example,when training online on-policy algorithms, such as PPO Schulman et al. and SAC Haarnojaet al. [2018a,b], it is often critical to parallelize environment instances to accelerate training andimprove generalization capability Wijmans et al. , Makoviychuk et al. . This wouldbe difficult to accomplish if the Agent and Environment were coupled. They can also be developedand maintained by different entities. Access to the Environment is controlled through NetworkGymAPIs to hide the details of how a network function or feature is implemented from developers. Motivation from Computer Networking. As the mobile industry evolves toward 6G, it is becomingclear that no single access technology will be able to meet the great variety of requirements forhuman and machine communications. Multi-access traffic management for integrating multipleheterogeneous wireless networks, e.g., Wi-Fi, cellular, satellite, etc., into a virtualized and unifiednetwork becomes vital for addressing todays ever-increasing performance requirements and futureapplications. Recently, the Generic Multi-Access (GMA) protocol has been proposed in the InternetEngineering Task Force (IETF) to address this need Zhu and Zhang , and the 3rd GenerationPartnership Project (3GPP) has also developed the access traffic steering, switching, and splitting(ATSSS) feature, which enables simultaneous use of one 3GPP and one non-3GPP connection todeliver data flows ats . We defer more technical details of these protocols to Appendix A. One effective method for managing multi-access traffic is through traffic splitting between differentnetwork types. Specifically, for each user equipment (UE), traffic is allocated between a 3GPPconnection (e.g., LTE) and a non-3GPP connection (e.g., Wi-Fi), with the ratio adjusted at frequentintervals, as in . It is natural to consider using RL for learning adaptive and data-drivendecision policies on the traffic-splitting ratios. Applying RL, however, is notoriously hard. One may run online RL on real networks, but the initialdecisions made by the algorithms can be suboptimal, leading to poor network traffic splitting anddiminished user experience. Notably, in applications such as robotic control over networks, it iscritical to ensure high reliability and low packet-loss ratios to maintain operational effectiveness. Analternative is to run offline RL on the logged data from real networks, but data coverage is a big issue.Even if the learned policy from offline RL improves over the baseline, one cannot know for sure untiltesting it online with real network traffic. Moreover, the networking environment is not static andmost challenging scenarios occur in the long tail of the data distribution. NetworkGym is timely as it allows us to not only evaluate any learned RL policies, but also stress-testthem in challenging scenarios. One could also use NetworkGym to simulate the entire workflow ofoffline RL for policy improvement before deploying the workflow on real networks. Frictionless Reproducibility for ML Researchers interested in Computer Networking. Theintended use of NetworkGym is to allow machine learning (especially RL) researchers to evaluatetheir algorithms on a faithfully simulated environment in computer networking without havingto understand the intricate networking protocols and their interactions in a multi-access trafficsplitting system. To facilitate frictionless reproducibility Donoho , we conduct preliminaryexperiments on NetworkGym with popular offline RL algorithms and make the code to setup suchexperiments available. Our results provide the following take-home messages:",
  "Related Work": "RL-based Network Optimization. RL has been used for network optimization in a variety ofcontexts Yang et al. , Mao et al. , Jay et al. , Jamil et al. , Zhang et al., Xia et al. , Gilad et al. , Boyan and Littman , Wei et al. , He et al., Liang et al. , Sadeghi et al. . For example, Yang et al. Yang et al. useoffline RL on a mixture of datasets from different behavior policies to maximize throughput viaradio resource management. Additionally, Mao et al. Mao et al. construct a system that cangenerate adaptive bitrate algorithms to maximize user quality of experience by training a deep RLmodel on client video player observations. Jay et al. Jay et al. employ deep RL to solve thecongestion control problem, whereas Jamil et al. Jamil et al. use deep RL to dynamicallydetermine the optimal number of TCP streams in parallel to maximize throughput while avoidingnetwork congestion. Despite the existing works, our use of offline RL for multi-access traffic splittingis novel and the first of its kind. RL Benchmarks. A wide variety of online and offline RL benchmarks have been proposed inthe research community in order to properly evaluate the performance and generalization of RLalgorithms. Popular online RL benchmarks include the OpenAI Gym Brockman et al. , Atari2600 games bel , and Mujoco Todorov et al. . These sets of environments offer a diverseselection of tasks to choose from, mostly involving classic control, continuous control of multi-jointbodies, or video game playing with high-dimensional input spaces. Common offline RL benchmarksinclude D4RL Fu et al. and RL Unplugged Gulcehre et al. , which provide similarenvironments to those in the referenced online RL benchmarks. Recent efforts have been made toconsolidate these offline RL benchmarks and have also reinforced the finding that the success of offline RL methods strongly depends on the training data distribution Kang et al. . Additionally,Voloshin et al. Voloshin et al. introduce the COBS off-policy evaluation benchmarking suiteto comprise a much wider variety of environments than simply the Mujoco-style or Atari-style ones.However, none of these benchmarks contains environments that focus on computer networkingapplications. Offline RL. Most approaches to offline RL involve some form of behavioral constraint or policyregularization to ensure that the actions chosen by the policy dont stray too far from the actionsin the dataset for corresponding states Levine et al. , Kostrikov et al. [2021b], Kumar et al., Kostrikov et al. [2021a], Yin and Wang , Li et al. . This is used to mitigatethe distribution shift between training and testing states. Certain algorithms seek to avoid off-policyevaluation (OPE) altogether, due to the inherent associated high variance which is compounded oneach training iteration Brandfonbrener et al. . Other algorithms use a form of divergenceconstraint to control the resulting behavior policy. For example, Conservative Q-Learning (CQL)modifies the actor-critic framework by selecting a policy whose expected value under a Q-functionlower-bounds its true value Kumar et al. . Implicit Q-Learning (IQL) seeks to avoid policyevaluation on unseen actions and instead treats the state value function as a random variable; thevalue of the best actions at a state can then be estimated by taking the upper expectile of the valuefunction conditioned on the state Kostrikov et al. [2021b]. Offline RL Using Online Algorithms. Other offline RL methods take advantage of the empiricalsuccess of state-of-the-art online RL methods; we include a discussion of some of these algorithmsin Appendix B. Additionally, Fujimoto et al. Fujimoto and Gu propose a minimal extensionto the popular online RL algorithm TD3 by augmenting the policy improvement step with a simplebehavioral cloning term. We note that while behavioral cloning is one way to prevent learned policiesfrom excessively favoring out-of-distribution (OOD) actions, another possibility is to incorporatesome form of pessimism into the Q-value estimates for these OOD actions. In particular, our work isinspired by that of Yin et al. Yin et al. in which the authors analyze the Pessimistic FittedQ-Learning (PFQL) algorithm and show that in the finite-horizon case, it is provably sample efficientunder certain assumptions. Their approach involves computing the Fisher information matrix onthe offline dataset with respect to the Q-function approximator and using that matrix to estimate theuncertainty of any state-action pair. In this way, they are able to compute policies that maximize alower-bound estimate of the state-action value function, improving the performance of the algorithmin the offline RL setting. In our work, we incorporate this idea of introducing pessimism into theQ-value estimates of TD3 in a similar way that Fujimoto et al. introduce behavioral cloning toTD3. Specifically, we adjust the policy improvement step to account for uncertainties present in theQ-values of specific state-action pairs and produce a resulting algorithm we denote as PessimisticTD3 (PTD3). We introduce PTD3 in Appendix C.",
  "Problem Setup": "Markov Decision processes. Let (S, A, R, p, ) define a Markov decision process (MDP) where S isthe state space, A is the action space, R : S A R is the scalar reward function, p : S A Sis the transition dynamics model where S is a set of probability distributions over S, and is the discount factor. S and A can both potentially be infinite or continuous. Typically, an RL agentchooses actions via a deterministic policy : S A or a stochastic policy : S A where A isa set of probability distributions over A. The goal of an RL agent is to find a policy that maximizes theexpected discounted return E [t=0 trt|s0 = s] from the starting state distribution. We denote thestate-action value function with respect to policy as Q(s, a) = E [t=0 trt|s0 = s, a0 = a]. Offline RL. In the offline RL setting, we assume that the agent does not have the ability to interactwith the environment and instead has access to an offline dataset D = {(sk, ak, rk, sk)}Kk=1 collectedby some unknown data-generating process (for example, a collection of different behavior policies).This makes the offline RL setting more challenging than the online RL setting. An online RL agentthat overestimates the Q-values at specific state-action pairs can quickly adapt after being punishedfor taking those actions in the environment, but an offline RL agent does not have the ability tointeract with the environment. This leads to the resulting problem of distribution shift in offline RL,which occurs due to extrapolation error in the Q-function approximators on state-action pairs that arepoorly represented by those in the offline dataset. Multi-Access Traffic Splitting Environment. In the Multi-Access Traffic Splitting environment,a predetermined number of UEs are randomly distributed on a 2-dimensional grid. When theenvironment is first instantiated, each UE is connected to a single LTE base station and the nearestWi-Fi access point. The location range of the UEs and the locations of the base station and accesspoints may be specified at environment initialization. If the RSSI-based handover is enabled in theNetworkGym environment configuration, then the Wi-Fi access point for each UE will dynamicallychange during the simulation to whichever has the highest received signal. Each time step in theenvironment consists of a time interval of 0.1 seconds. During this time interval, traffic-relatedmeasurements are taken, such as the one-way-delay and output traffic throughput. The goal of acentralized traffic splitting agent is to strategically split traffic over the Wi-Fi and LTE links, aimingto achieve high throughput and low latency. Within the NetworkGym environment configuration, it ispossible to specify parameters that control the nature of the UEs movement and whether or not theyfollow a random or deterministic walk. Observation Space.An observation at time t is s(t)=[s1(t), s2(t), ..., sNu(t)] forNu users where sj(t) is a tuple of values for the j-th UE in the following form:(lcLTE, lcWi-Fi, tpin, tpout, LTE, tpout, Wi-Fi, owdLTE, owdWi-Fi, owdmax, LTE, owdmax, Wi-Fi, idWi-Fi, srLTE,srWi-Fi, x, y), lck is the UEs link capacity for channel k, tpin is the UEs input traffic throughput,tpout, k is the UEs output traffic throughput across channel k, owdk is the UEs one-way-delay acrosschannel k, owdmax, k is the UEs maximum one-way-delay across channel k, idWi-Fi is the UEs currentWi-Fi access point ID, srk is the UEs splitting ratio for channel k, x is the x-location of the user, andy is the y-location of the user.",
  "32is the desired Wi-Fi splitting ratio for the j-th UE during the nexttime interval": "Reward Function. The immediate reward at time t is computed as in Equation 1 where tpi and dyiare the output traffic throughput and one-way-delay across both channels for the i-th user duringthe current time interval, respectively. Furthermore, tpi,max is the sum of link capacities across bothchannels for the i-th user during this time interval and we take dymax to be 1000ms, after whicha packet is treated as lost. In this way, we normalize the reward function to be invariant to unit-translation and incentivize a learning agent to maximize the average throughput while simultaneouslyminimizing the average delay across channels. Although this reward function is admittedly somewhatarbitrary, a different reward function can be easily specified by the network administrators in order tosatisfy different QoS requirements.",
  "Experiments": "Experimental Setup. We test PTD3 and other state-of-the-art offline RL algorithms on a simplifiedconfiguration of the NetworkGym multi-access traffic splitting environment. The relevant environ-ment configuration file is included in Appendix D. At initialization of each environment, four UEsare randomly stationed 1.5 meters above the x-axis between x = 0 and x = 80 meters. From there,they begin to bounce back and forth in the x-direction at 1 m/s for the entire duration of an episode.The Wi-Fi access points are stationed at (x, z) = (30m, 3m) and (x, z) = (50m, 3m), respectivelywhile the LTE base station lies at (x, z) = (40m, 3m). illustrates this environment setting.Although this setup is deceptively simple and unrealistic due to the relative locations between UEsand access points as well as the degenerate movement of the UEs, it provides a simple enough testingground for offline RL on the GMA traffic splitting protocol while still containing some amount ofdynamic behavior and resource competition between UEs. Since the multi-access gateway is connected to all four UEs, the gateway can send traffic splittingcommand messages to each of the UEs in a centralized manner via the GMA protocol while takinginto account network information across all UEs. Therefore, we represent the state as a 14 4 matrixwhere we have 14 network measurement values for each user from the previous time interval and werepresent the action as a 1 4 row-vector, where each element represents the desired traffic splittingratio during the next time interval for a specific user. Although the traffic splitting ratio for each : Environment configuration for offline RL testing (not-to-scale). Here, we randomlyinitialize four UEs 1.5 meters above the x-axis and they move back and forth in the x-directionbetween x = 0 meters and x = 80 meters. The Wi-Fi access point locations are (x, z) = (30m, 3m)and (x, z) = (50m, 3m) while the LTE base station location is (x, z) = (40m, 3m).",
  ", we treat each element in the action as acontinuous real number between 0 and 1 and map it to the closest corresponding discrete value": "Heuristic Policies. NetworkGym provides three heuristic policies for traffic splitting and offline datacollection, which we denote throughput_argmax, system_default, and utility_logistic.All of these policies operate independently on each UE, without considering coupled interactionsbetween them. For each user, throughput_argmax examines the previous Wi-Fi and LTE linkcapacities and chooses the traffic splitting ratio to completely favor whichever channel previouslyhad the highest link capacity. For the system_default algorithm, if the UE-specific difference indelay among the Wi-Fi and LTE links exceeds a threshold, traffic over the link with lower delay isgradually increased. If the delay difference among both links is small but packet loss is detected,traffic over the link with a lower packet loss rate is incrementally increased. The final heuristic policy,utility_logistic, computes a Wi-Fi utility ui,Wi-Fi = log (1 + tpi,Wi-Fi) log (1 + dyi,Wi-Fi) andthe corresponding LTE utility ui,LTE for each user and then computes the desired traffic splittingratio for said user as (ui,Wi-Fi ui,LTE) where () is the logistic function. In this way, the trafficsplitting ratio favors channels that indicated higher utility during the previous time interval. Offline Datasets. For each heuristic policy, we collect an offline dataset over 64 episodes, each witha different starting configuaration of UEs. Each episode consists of 10,000 steps. We evaluate theoffline dataset coverages for each algorithm by computing the minimum eigenvalue of the featurecovariance matrix C = Es,aD(s, a)(s, a)Twhere (s, a) is the featurization of the state-actionpair Jin et al. , Zanette et al. , Yin et al. , Nguyen-Tang et al. . Wefeaturize state-action pairs by simply concatenating the flattened state and action vectors together. The minimum eigenvalue and condition number for each population covariance matrix are illustratedin . The offline dataset coverage is highest for the utility_logistic algorithm and lowestfor the throughput_argmax algorithm. In practice, the throughput_argmax algorithm sends alltraffic through the Wi-Fi channel over 99% of the time for each user, with occasional bursts overLTE while the utility_logistic algorithm thrashes back and forth for each user between sendingtraffic over Wi-Fi and LTE, resulting in a dataset with much higher coverage. The results in present a wide range of dataset coverage values, spanning multiple orders of magnitude. This diverseset of benchmarks ensures that offline RL algorithms can be appropriately evaluated. For instance,algorithms trained on datasets with low coverage are expected to adhere closely to the behavior policydue to limited data variety, while those trained on high-coverage datasets have the potential for greaterimprovement over the behavior policy due to more diverse experiences to learn from. In , we evaluate the performance of three heuristic policies, several offline RL algorithmstrained on different datasets, and two state-of-the-art online RL algorithms (PPO and SAC) in thisenvironment setting. The online RL algorithms establish a soft upper bound on the returns achievableby offline RL algorithms in our NetworkGym environment setting. For each of the algorithms, weevaluate its performance on 40 evaluation episodes, each of which is 3200 steps. The total returnper step across all episodes is then averaged and reported; the error bar indicates a 95% confidenceinterval centered around the mean. The performance across different datasets is then averaged againto produce an average performance across all datasets in the rightmost column. Finally, in ,we examine the performance of the PTD3 algorithm across the different datasets and different valuesof where = 1.0. We find that setting = 1.0 results in the least variance in performance acrossvalues of .",
  "thrpt_argmaxsystem_defaultutility_logisticAverage": "baseline0.747 0.0490.555 0.0520.949 0.0390.750 0.047BC (norm)0.749 0.0470.825 0.0420.749 0.0470.774 0.045BC (no norm)0.751 0.0490.433 0.0540.946 0.0390.710 0.047CQL (norm)0.749 0.0470.749 0.0470.749 0.0470.749 0.047CQL (no norm)0.998 0.0430.381 0.0820.957 0.0440.779 0.056IQL (norm)0.770 0.0510.818 0.0420.748 0.0480.779 0.047IQL (no norm)0.749 0.0490.846 0.0420.948 0.0360.848 0.042TD3+BC (norm)0.749 0.0470.034 0.0460.749 0.0470.511 0.047TD3+BC (no norm)0.778 0.0470.906 0.0490.863 0.0380.849 0.045EDAC0.336 0.285-0.888 0.0340.913 0.0270.120 0.115LB-SAC0.902 0.046-0.204 0.0721.150 0.0330.616 0.050SAC-N0.838 0.0520.817 0.0350.699 0.0260.785 0.038PTD30.746 0.0501.013 0.0391.079 0.0400.946 0.043",
  "PPO1.214 0.037SAC1.104 0.037": ": Offline and Online RL Algorithm Performance Across Multiple Offline Datasets. Each ofthe first three column headers indicates the baseline algorithm that collected the offline dataset where\"thrpt_argmax\" is an alias for throughput_argmax. Each row header (except \"baseline\", \"PPO\", and\"SAC\") is an offline RL algorithm trained on one of three offline datasets. \"baseline\" refers to theperformance of the original baseline heuristic policies without any offline data collection. \"(norm)\"indicates that the algorithm implements state-normalization based on the offline dataset while \"(nonorm)\" indicates that the algorithm does not. If not specified, the algorithm does not implement statenormalization. We use = 1.0 and = 10.0 in our evaluation of PTD3.",
  "Discussion": "Offline RL Algorithm Performance. First, we note that of the 7 off-the-shelf offline RL algorithmstested in our NetworkGym environment setting, only 2 of them were able to significantly outperformthe average performance of the heuristic baseline algorithms. Furthermore, in the case of both ofthese algorithms, they were only able to do so when we disabled state normalization based on theoffline dataset, a feature that is included by default when training these offline algorithms. Therefore,using the default hyperparameters for every tested off-the-shelf offline RL algorithm, none of thesealgorithms could significantly outperform the heuristic baseline algorithms on average. Furthermore,in the case of a few of these algorithms, such as EDAC and LB-SAC, the performance acrossdifferent datasets is erratic, resulting in a significantly lower average performance overall, comparedto the heuristic baseline algorithms. While these algorithms are known to exhibit state-of-the-artperformance on D4RL-like tasks, it has been noted that the performance of these algorithms inpractice is unstable across environments of varying characteristics Tarasov et al. . Thesefindings strongly suggest that it would be imprudent to deploy such algorithms trained on a similartask into the real world, even if they were trained on datasets collected from real interactions. Since our implementation of PTD3 is, on average, able to significantly outperform not only theheuristic baseline policies, but also several existing state-of-the-art offline RL algorithms, thissuggests that the poor performance across existing algorithms is not due to a lack of coverage across",
  "thrpt_defaultdelay_defaultutility_defaultAverage": "baseline0.747 0.0490.555 0.0520.949 0.0390.750 0.047BC (norm)0.749 0.0470.825 0.0420.749 0.0470.774 0.045BC (no norm)0.751 0.0490.433 0.0540.946 0.0390.710 0.047CQL (norm)0.749 0.0470.749 0.0470.749 0.0470.749 0.047CQL (no norm)0.998 0.0430.381 0.0820.957 0.0440.779 0.056IQL (norm)0.770 0.0510.818 0.0420.748 0.0480.779 0.047IQL (no norm)0.749 0.0490.846 0.0420.948 0.0360.848 0.042TD3+BC (norm)0.749 0.0470.034 0.0460.749 0.0470.511 0.047TD3+BC (no norm)0.778 0.0470.906 0.0490.863 0.0380.849 0.045EDAC0.336 0.285-0.888 0.0340.913 0.0270.120 0.115LB-SAC0.902 0.046-0.204 0.0721.150 0.0330.616 0.050SAC-N0.838 0.0520.817 0.0350.699 0.0260.785 0.038PTD30.746 0.0501.013 0.0391.079 0.0400.946 0.043",
  "throughput_argmaxsystem_defaultutility_logistic": "0.10.744 0.0560.878 0.0480.816 0.0450.30.744 0.0560.958 0.0411.044 0.0291.00.744 0.0560.974 0.0401.015 0.0453.00.744 0.0560.995 0.0441.022 0.04110.00.744 0.0571.017 0.0441.083 0.04730.00.744 0.0560.679 0.0441.209 0.045100.00.744 0.0560.213 0.0701.226 0.049300.00.744 0.0560.243 0.0591.252 0.063 : PTD3 Performance where = 1.0. For each of the algorithms, we evaluate its performanceon 32 evaluation episodes, each of which is 3200 steps. We avoid bolding the throughput_argmaxruns, as they all have roughly the same performance. datasets, but rather the lack of diversity and breadth of testing environments for these algorithms.While the D4RL benchmark has become a standard for assessing offline RL performance, it isessential to recognize its limitations. Algorithms that are touted as state-of-the-art based on theirperformance on D4RL may not generalize well to other, perhaps more complex or varied, scenarios.We have shown that many advanced offline RL algorithms have the potential to fail catastrophicallywhen deployed in different contexts or faced with unfamiliar environments. Therefore, to ensurerobustness and reliability, it is crucial to test offline RL algorithms across a wider array of datasetsand environments. This broader testing approach helps to uncover potential weaknesses and providesa more comprehensive understanding of an algorithms capabilities and limitations. We hope that byexpanding the scope of testing beyond popular benchmarks like D4RL and RL Unplugged, researchersand practitioners can better gauge the true potential and practicality of their offline RL solutions. Behavioral Cloning vs. State-Action Value Function Pessimism. In testing PTD3 ( = 1.0)on all three offline datasets with varying values of , we note that while the performance on thesystem_default dataset improves up to a point as increases then drops off, the performance onthe utility_logistic dataset improves substantially even up to values as high as = 300.0. In",
  "throughput_defaultdelay_defaultutility_default": "0.10.744 0.0560.878 0.0480.816 0.0450.30.744 0.0560.958 0.0411.044 0.0291.00.744 0.0560.974 0.0401.015 0.0453.00.744 0.0560.995 0.0441.022 0.04110.00.744 0.0571.017 0.0441.083 0.04730.00.744 0.0560.679 0.0441.209 0.045100.00.744 0.0560.213 0.0701.226 0.049300.00.744 0.0560.243 0.0591.252 0.063 : PTD3 Performance where = 1.0. For each of the algorithms, we evaluate its performanceon 32 evaluation episodes, each of which is 3200 steps. We avoid bolding the throughput_argmaxruns, as they all have roughly the same performance.",
  "throughput_argmax0.751 0.0550.770 0.055system_default0.432 0.0630.547 0.059utility_logistic0.948 0.0421.252 0.079": ": Comparison between Behavioral Cloning and Q-function pessimism. We evaluate each of-fline RL algorithm across 32 evaluation episodes, each of which is 3200 steps. In this implementationof PTD3, we set = 1.0, set = 1.0, and manually remove the Q-value maximization term fromthe policy-update step to simulate = . fact, for large enough , the performance of PTD3 on the utility_logistic dataset is comparableto that of the best performing online deep RL algorithm, PPO. This behavior leads us to questionwhether or not Q-function pessimism as weve defined it in this work is always comparable tobehavioral cloning. To further test this, we compare the performance of behavioral cloning (withoutoffline dataset feature normalization) with PTD3 where the Q-value maximization term is removedfrom the policy-update step. In other words, this implementation of PTD3 would be performing pureminimization of the Q-value uncertainty estimates with respect to state-action pairs, without anyregard for how high those Q-values are. The results are illustrated in . Interestingly, we notethat the performance of this purely pessimistic PTD3 implementation is significantly higher thanthat of behavioral cloning when both are trained on the utility_logistic offline dataset, whilethe two implementations are comparable in the case of the other two datasets. This is reflective of afundamental difference between behavioral cloning and Q-value uncertainty minimzation: while theobjective of a behavioral cloning agent is to pointwise match the agents actions to those chosen by thebehavior policy, the objective of the uncertainty-minimizing agent is to choose actions that minimizethe uncertainty in the Q-function estimates by considering the associated variance in Q-values. Limitations. Several limitations exist in our current approach. First, the NetworkGym environmentsimulation setup that we use in all experiments assumes a fixed number of UEs. Consequently, theaddition or removal of even a single UE necessitates retraining the online or offline algorithms fromscratch, given the current formulation of the MDP. Additionally, our simulation setting incorporatesunrealistic degenerate movement patterns for UEs, which may not accurately reflect real-worlddynamics. Finally, one of the major limitations of PTD3 is the constraint on the parameter size of theQ-networks: if the Q-networks each have d parameters, then d d space is required to store Ft inmemory. As a result, in order for gradient-related computations to fit on our 12 GB NVIDIA TITANXp, we were required to use MLP critics with two hidden layers of 64 neurons each instead of hiddenlayers of 400 and 300 neurons as TD3+BC uses. Future Work. In future work, we plan to explore methods to appropriately featurize multiple UEs toallow for dynamic changes in their number without requiring retraining any algorithms. This willinvolve rethinking the current MDP formulation to accommodate a variable number of UEs moreflexibly. We also aim to incorporate more complex movement patterns for UEs, such as randomwalks, to gain a better understanding of how our tested algorithms generalize to these settings. Inaddition to the previously mentioned areas, potential opportunities exist to enhance the performance of our PTD3 algorithm. The use of GPUs with larger memory capacities would enable us to use largercritic network architectures for PTD3. Additionally, in this work, we primarily explore estimatingFt using an exponentially-weighted moving sum with low variance ( = 1.0); this is because as changes, training with a high variance estimator of the Fisher information matrix across timestepsmakes it difficult to properly evaluate the effect of .",
  "Conclusion": "In this work, we present NetworkGym, a high-fidelity gym-like network environment simulator thatfacilitates multi-access traffic splitting. NetworkGym seamlessly aids in training and evaluatingonline and offline RL algorithms on the multi-access traffic splitting task in simulation. In oursimulated experiments, we demonstrate that existing state-of-the-art offline RL algorithms fail tosignificantly outperform heuristic policies on this task. This highlights the critical need for a broaderrange of benchmarks across multiple domains for offline RL algorithm evaluation. On the other hand,our proposed PTD3 algorithm significantly outperforms not only heuristic policies, but also manystate-of-the-art offline RL algorithms trained on heuristic-generated datasets. These findings pavethe way for more effective offline RL algorithms and demonstrate the potential of PTD3 as a strongcontender among existing solutions. Future research should consider evaluating offline RL algorithmson networking-specific tasks alongside other benchmarks to foster the development of more robustand versatile solutions.",
  "Scott Fujimoto and Shixiang Shane Gu. A minimalist approach to offline reinforcement learning.Advances in neural information processing systems, 34:2013220145, 2021": "Tomer Gilad, Nathan H Jay, Michael Shnaiderman, Brighten Godfrey, and Michael Schapira. Robus-tifying network protocols with adversarial examples. In Proceedings of the 18th ACM Workshopon Hot Topics in Networks, pages 8592, 2019. Caglar Gulcehre, Ziyu Wang, Alexander Novikov, Thomas Paine, Sergio Gmez, Konrad Zolna,Rishabh Agarwal, Josh S Merel, Daniel J Mankowitz, Cosmin Paduraru, et al. Rl unplugged: Asuite of benchmarks for offline reinforcement learning. Advances in Neural Information ProcessingSystems, 33:72487259, 2020. Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policymaximum entropy deep reinforcement learning with a stochastic actor. In International conferenceon machine learning, pages 18611870. PMLR, 2018a. Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, VikashKumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, et al.Soft actor-critic algorithms andapplications. arXiv preprint arXiv:1812.05905, 2018b. Ying He, F Richard Yu, Nan Zhao, Victor CM Leung, and Hongxi Yin. Software-defined networkswith mobile edge computing and caching for smart cities: A big data deep reinforcement learningapproach. IEEE Communications Magazine, 55(12):3137, 2017.",
  "Eric Liang, Hang Zhu, Xin Jin, and Ion Stoica. Neural packet classification. In Proceedings of theACM Special Interest Group on Data Communication, pages 256269. 2019": "Viktor Makoviychuk, Lukasz Wawrzyniak, Yunrong Guo, Michelle Lu, Kier Storey, Miles Macklin,David Hoeller, Nikita Rudin, Arthur Allshire, Ankur Handa, et al. Isaac gym: High performancegpu-based physics simulation for robot learning. arXiv preprint arXiv:2108.10470, 2021. Hongzi Mao, Ravi Netravali, and Mohammad Alizadeh. Neural Adaptive Video Streaming withPensieve.In Proceedings of the Conference of the ACM Special Interest Group on DataCommunication, SIGCOMM 17, page 197210, New York, NY, USA, 2017. Associationfor Computing Machinery.ISBN 9781450346535.doi: 10.1145/3098822.3098843.URL Thanh Nguyen-Tang, Ming Yin, Sunil Gupta, Svetha Venkatesh, and Raman Arora. On instance-dependent bounds for offline reinforcement learning with linear function approximation. InProceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 93109318, 2023. Alexander Nikulin, Vladislav Kurenkov, Denis Tarasov, Dmitry Akimov, and Sergey Kolesnikov.Q-ensemble for offline rl: Dont scale the ensemble, scale the batch size.arXiv preprintarXiv:2211.11092, 2022. Alireza Sadeghi, Fatemeh Sheikholeslami, and Georgios B Giannakis. Optimal and scalable cachingfor 5g using reinforcement learning of space-time popularities. IEEE Journal of Selected Topics inSignal Processing, 12(1):180190, 2017.",
  "Cameron Voloshin, Hoang M Le, Nan Jiang, and Yisong Yue. Empirical study of off-policy policyevaluation for reinforcement learning. arXiv preprint arXiv:1911.06854, 2019": "Peng Wei, Kun Guo, Ye Li, Jue Wang, Wei Feng, Shi Jin, Ning Ge, and Ying-Chang Liang. Rein-forcement learning-empowered mobile edge computing for 6g edge intelligence. Ieee Access, 10:6515665192, 2022. Erik Wijmans, Abhishek Kadian, Ari Morcos, Stefan Lee, Irfan Essa, Devi Parikh, Manolis Savva,and Dhruv Batra. Dd-ppo: Learning near-perfect pointgoal navigators from 2.5 billion frames. InInternational Conference on Learning Representations, 2019. Zhengxu Xia, Yajie Zhou, Francis Y Yan, and Junchen Jiang. Genet: automatic curriculum generationfor learning adaptation in networking. In Proceedings of the ACM SIGCOMM 2022 Conference,pages 397413, 2022. Kun Yang, Chengshuai Shi, Cong Shen, Jing Yang, Shu-ping Yeh, and Jaroslaw J Sydir. Offlinereinforcement learning for wireless network optimization with mixture datasets. IEEE Transactionson Wireless Communications, 2024.",
  "Ming Yin, Mengdi Wang, and Yu-Xiang Wang. Offline reinforcement learning with differentiablefunction approximation is provably efficient. International Conference on Learning Representa-tions, 2023": "Andrea Zanette, Martin J Wainwright, and Emma Brunskill. Provable benefits of actor-critic methodsfor offline reinforcement learning. Advances in neural information processing systems, 34:1362613640, 2021. Lyutianyang Zhang, Hao Yin, Sumit Roy, and Liu Cao. Multiaccess point coordination for next-genwi-fi networks aided by deep reinforcement learning. IEEE Systems Journal, 17(1):904915,March 2023. ISSN 2373-7816. doi: 10.1109/jsyst.2022.3183199. URL",
  "ATechnical Details Concerning GMA Protocol": "Both the GMA and ATSSS protocols provide mechanisms for flexible selection of network paths andleverage network intelligence and policies to dynamically adapt traffic distribution across selectedpaths under changing network/link conditions. Generally, a multi-access network protocol, e.g. GMA,consists of the following two sublayers: Convergence sublayer: This layer performs multi-access specific tasks such as access (path)selection, multi-link (path) aggregation, splitting/reordering, lossless switching, keep-alive,and probing Kanugovi et al. . Adaptation sublayer: This layer performs functions to handle tunneling, network layersecurity, and network address translation (NAT). This design only operates at the physicaland routing layers in the network data plane and does not require any modifications to thehigher layer protocols, such as user datagram protocol (UDP), transport control protocol(TCP), IP security (IPSec), etc. On the other hand, maximizing the benefits of a multi-access system necessitates solving a decisionproblemintelligently distributing user data traffic across available access links to optimize userexperience while making the best use of available radio resources. To effectively manage traffic in amulti-access network, its crucial to incorporate measurements that reflect the varying connectivityconditions of different networks. For instance, end-to-end (e2e) packet delay measurements can helpdetermine which access network offers better latency performance. Similarly, for quality-of-service(QoS) flows that demand high reliability, the packet drop ratio can indicate the necessity for redundanttransmission across multiple networks. Besides e2e packet statistics, Radio Access Network (RAN)measurements, like reference signal received power (RSRP), reference signal received quality (RSRQ),and received signal strength indicator (RSSI), can reveal any degradation in network quality due toissues like deteriorating radio link quality or congestion in real-time. However, integrating thesediverse data sourcesincluding e2e packet statistics and RAN measurementsto formulate anoptimal traffic management algorithm for multi-access networks remains a complex challenge.",
  "BOffline RL Using Online Algorithms": "Many recent state-of-the-art offline RL methods make minor modifications to existing online RLalgorithms. For example, SAC-N modifies a popular online RL algorithm known as Soft Actor-Critic (SAC) by simply increasing the number of Q-function approximators from 2 to N > 2 inorder to further mitigate the over-estimation of Q-values Haarnoja et al. [2018a], An et al. .Ensemble-Diversified Actor-Critic (EDAC) adds a regularization term that minimizes the pairwisecosine similarity of the gradients across different Q-function approximators to incentivize the policynetwork to choose actions at which the gradients of the Q-function networks have high alignment Anet al. . Nikulin et al. Nikulin et al. propose Large Batch SAC (LB-SAC), which avoidsthe need to use a large ensemble of Q-function networks and instead scales the batch size used totrain these networks with the result of improving learning duration while maintaining performance.",
  "CPessimistic TD3": "In this section, we introduce a new offline RL algorithm, which we denote as Pessimistic TD3(PTD3). We highlight that the TD3+BC algorithm from Fujimoto et al. removes online access tothe environment and instead replaces the replay buffer B with an offline dataset D Fujimoto and Gu. Additionally, TD3+BC adds a simple behavioral cloning term to the deterministic policygradient step with the goal of minimizing the square-difference between the actions in the datasetand the corresponding actions output by the learned policy Fujimoto and Gu . In order toinstead incorporate pessimism into the TD3 algorithm to produce PTD3, we first compute a Fisherinformation matrix of the offline dataset on one of the critics at each policy update step as in Equation",
  "k=1Q1(t)(sk, ak)TQ1(t)(sk, ak) + rId(2)": "where r is a hyperparameter and Id is the d d identity matrix to ensure that Ft is invertible. Thegradient of the state-action value function is represented as a d-dimensional column vector where theQ-network parameter vector i Rd. From there, we can use Equation 3 to compute a statisticallymotivated estimate of the uncertainty in the Q-values at state-action pairs where states are sampledfrom the dataset and actions are chosen from the learned policy.",
  "Finally, we can use the deterministic policy gradient to update the policy parameters in the gradientdirection that maximizes this lower-bound estimate Qt": "In practice, computing Ft across the entire dataset on each iteration is quite computationally expensive,as it requires K = |D| per-sample-gradient calculations. One way we work around this issue is bysampling a large batch (we use 214 samples) from the dataset and estimating Ft over that batch instead.Additionally, computing the inverse of Ft has a time complexity of roughly O(d3) , which constrainsthe dimension of the Q-networks from being too large. In order to better combat these issues inpractice, we instead initialize the estimator F0 = Id and update Ft as an exponentially-weightedmoving sum over previous iterations via Equation 5.",
  "Ft = Ft1 + Q1(t)(si, ai)TQ1(t)(si, ai)(5)": "where (0, 1] is a parameter that controls the bias-variance trade-off in the Ft estimator and(si, ai) D is a single tuple sampled from the dataset. In this way, Ft is analogous to a \"full-batchgradient descent\" calculation while Ft is analogous to a \"stochastic gradient descent.\" As 0,the resulting estimator has higher variance, but is less biased by previous values of Ft. On the otherhand, as 1, the variance of the Ft estimator reduces, but the estimator retains information withrespect to older Q-network parameters, which are more likely to be obsolete. As a result of estimating Ft in this way, we may use the Sherman-Morrison formula to compute itsinverse on each iteration and avoid the O(d3) complexity required to recompute the inverse fromscratch. This reduces the runtime of the algorithm by roughly a factor of 3:",
  "+ TQ1(t)(si, ai)F1t1Q1(t)(si, ai)(6)": "In practice, we add a small amount of Gaussian noise n N(0, Id) to the gradient vectors, beforeincorporating them into the Ft estimator.3 This ensures that Ft remains invertible over a large numberof iterations if is not close to 1. Additionally, due to cumulative numerical round-off error fromrepeatedly applying the rank-1 update rule, we recompute F1tfrom scratch every 100 iterationsto prevent it from diverging too far from its true value. We present the final version of PTD3 inAlgorithm 1 with the relevant modifications to TD3+BC highlighted.",
  "EComputational Resources": "We make use of four internal 12 GB NVIDIA TITAN Xp GPUs to perform our experiments. Withthese GPUS, to perform all experiments described in this document requires roughly 1 month ofcompute, assuming each of 8 different CPU processes is used to perform an agent evaluation. Usingonly a single process to perform agent evaluation would result in the compute increasing to roughly 3months.",
  "FOffline Data Collection": "For each of three different heuristic policies (throughput_argmax, system_default, andutility_logistic), we collect and store 64 episodes of offline data on our Network-Gym Multi-Access Traffic Splitting environment (denoted nqos_split).Each episodecontains 10,000 steps worth of data.The associated configuration file (located atnetwork_gym_client/envs/nqos_split/config.json) for the episodes is chosen with thefollowing constraints in mind: At initialization of each environment, four UEs are randomly stationed 1.5 meters above thex-axis between x = 0 and x = 80 meters. From there, they begin to bounce back and forthin the x-direction at 1 m/s for the entire duration of an episode.",
  "ITraining Online Deep RL Algorithms": "We use stable-baselines3 to train two different online deep RL algorithms, PPO and SAC. Wedo so by initializing a random agent, then updating that agent through 8 successive phases. Ineach phase, we parallelize environment instantiations across 8 different random seeds, where eachenvironment runs for 10,000 steps, resulting in a total of 64 different environment instantiations.In this way, the online learning algorithm trains across the same number of steps available in eachof the offline datasets, to allow for proper comparison. Additionally, for our parallel environmentrandom seeds, we use 0-7, inclusive, followed by 8-15, 16-23, ..., 56-63. We provide the shell script,train_online_parallel.sh, in order to perform this training process with PPO and SAC. We usethe default hyperparameters specified by stable-baselines3.",
  "JEvaluating Trained Agents": "Finally, to evaluate a trained agent (whether online or offline), we place the resulting model file in theNetworkAgent/models directory. Then, the model filename (without extension) can be specified asthe agent parameter at the top of the test_agent.sh shell script and the script can be executed toevaluate the agent on a single 3,200 step episode. In our experiments, we evaluate each agent across32 or 40 episodes (each with a different random_seed parameter), depending on the experiment.Each episode is 3,200 steps and the random_seed parameter takes on values between 128-159,inclusive, for 32 evaluation episodes or 128-167, inclusive, for 40 evaluation episodes. We otherwiseuse the same environment configuration details mentioned in Section Offline Data Collection."
}