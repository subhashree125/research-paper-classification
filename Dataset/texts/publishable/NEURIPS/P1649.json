{
  "Abstract": "Despite the impressive performance of large language models across various tasks,they often struggle with reasoning under negated statements. Negations are im-portant in real-world applications as they encode negative polarity in verb phrases,clauses, or other expressions. Nevertheless, they are underrepresented in currentbenchmarks, which mainly include basic negation forms and overlook more com-plex ones, resulting in insufficient data for training a language model. In thiswork, we propose NegVerse, a method that tackles the lack of negation datasets byproducing a diverse range of negation types from affirmative sentences, includingverbal, non-verbal, and affixal forms commonly found in English text. We providenew rules for masking parts of sentences where negations are most likely to occur,based on syntactic structure and use a frozen baseline LLM and prompt tuningto generate negated sentences. We also propose a filtering mechanism to identifynegation cues and remove degenerate examples, producing a diverse range ofmeaningful perturbations. Our results show that NegVerse outperforms existingmethods and generates negations with higher lexical similarity to the original sen-tences, better syntactic preservation and negation diversity. The code is availablein",
  "Introduction": "Recent advancements in natural language processing (NLP) have enhanced various applicationssuch as text generation , translation and summarization , but handling negation remainsa significant challenge . Negations are crucial for reasoning and effective communication, asthey express denial, contradiction, and absence. This is especially important in critical fields likebiomedicine, where misinterpreting negated conditions can have serious consequences. For example,Large Language Models (LLMs) identifying acute bleeding have misclassified cases with negatedphrases, revealing bias and a limited understanding of negations .Despite their importance, existing literature has established that language models struggle withnegated sentences in tasks such as cloze completion, NLI, QA, and classification . Forexample, the work in found an inverse scaling trend among models such as GPT-J, GPT-3,Flan-T5, GPT-Neo, and OPT (ranging from 125M to 6B parameters), where larger models tend toperform worse on negation tasks and often produce incorrect answers with high confidence. Similarly, and demonstrated that models like BERT, RoBERTa, GPT-2, BART, and T5 frequentlygenerate identical outputs for opposite statements and misinterpret sentences, such as classifying\"The man in the blue shirt is relaxing on the rocks\" as entailing \"A man is not wearing a blue shirt\".Negations are also underrepresented in most benchmark datasets, both in terms of frequency andcomplexity. In particular, the works in and show that general-purpose English corpora, suchas reviews, conversations, Wikipedia, and books, contain between 22.6% and 29.9% sentences with",
  "arXiv:2411.00056v1 [cs.CL] 30 Oct 2024": "negations. In contrast, some natural language inference benchmarks have around 8.7%, while otherdatasets, such as COPA and QQP , contain 0.8% and 8.1% respectively.To improve negation understanding in NLP models, it is crucial to expand annotated datasets tocover various types of negation across different domains . Transformer-based models, suchas RoBERTa and BERT , often struggle with negations due to their underrepresentation intraining data . Current benchmarks primarily focus on verbal negations, lacking syntactic andmorphological negations . Although some existing methods address verbal negations oruse rule-based augmentation , they still cover only a limited range of negation types.Contributions:To address this issue, we introduce NegVerse, a method that generates a diverserange of syntactic and morphological negations, including non-verbal, verbal, and affixal forms, toenrich the training datasets. NegVerse (a) keeps the produced negated data closely aligned withthe original sentences by employing a masking strategy at both token and subtree levels; and (b)addresses the shortage of affixal negation datasets and other negation forms, by assembling 362 uniquesentences using LLama-2 and other sources, such as COPA and SNLI . We introduce anefficient masking strategy to insert negations while maintaining sentence fluency. Additionally, a newfiltering mechanism is used to exclude degenerate outputs, capturing key negation cues effectively.We use a GPT-2-based model to generate negated sentences and implement a filtering mechanismthat screens the generated negations for closeness, duplicates, and validity. We provide extensiveempirical evidence of our NegVerses efficiency and improved performance using relevant criteriasuch as closeness, diversity, and text quality on various datasets against state-of-the-artbaselines.",
  "Related Work": "LLMs have excelled in various tasks , but they consistently struggle with understandingnegated sentences , which limits their reasoning abilities and sometimes worsens with themodel size. Current solutions, such as syntactic data augmentation using Semgrex patterns andthe TINA method , aim to enhance LLMs robustness to negations in textual entailment tasksby augmenting training datasets with grammatically correct negated instances. However, they faceerrors in complex sentences. Other approaches like generate negated data using tense patternsand keywords, while uses WordNet to create true/false sentences. Nevertheless, these methods arenot adaptable across diverse datasets. Polyjuice generates sentence perturbations but producesnonsensical outputs and handles a limited range of negation types. The work in transforms negated sentences into affirmative ones using sentence pairs and back-translation yet it falls short compared to human understanding. Similar to the aforementionedapproaches, our goal is to produce new negated sentences to augment the existing datasets. However,in contrast to these methods, our proposed approach generates a wider range of negations includingverbal, non-verbal, and affixal forms from affirmative sentences. It employs an efficient maskingstrategy to maintain fluency and structural preservation, resulting in outputs that align lexically betterwith the original sentences and overcome the limitations of earlier methods.",
  "Problem Formulation": "We consider a dataset D = {(xi, ci, Xi)}mi=1, where xi denotes an affirmative sentence, ci ={c(j)i }nj=1 is the corresponding context vector, and Xi = {x(1), x(2), . . . , x(n)} the set of all the validground-truth negated sentences. The affirmative sentences lack any negation and do not includeinformation guiding the construction of its negation. Each context ci includes nstructured promptswith placeholders denoted as [BLANK], indicating where the negation should be applied within asentence xi. The set Xi contains the respective valid negated sentences corresponding to context ci.Our goal is to learn a language generator model g G, parametrized by a vector , that,given an affirmative sentence x and context c, produces a set of negated versions Xgen that closelyapproximates the ground truth negated set X. This is equivalent to solving",
  "Rule 6Non-VerbalADPprep": ": (Top): Overview of NegVerse steps. The input sentence x is masked with blanks on specificpositions based on structural rules. The masked sentences c and the original sentence x are usedto create prompts that are fed to a pretrained language model, which then generates n candidatenegations, Xgen. A filtering mechanism selects the most relevant negations from these candidates,producing the final set, Xf. (Bottom Left): (A) The input data is masked at different spans using thetoken [BLANK]. Masks cover different parts of the sentence, or the entire sentence (see sentence3). (B) Training samples concatenate the input text with the masked sentence and the target wordsneeded to fill the blanks. Each span is separated by the token [ANSWER], and [SEP] separatesthe context from the answers. During inference, the model accepts the sentence as input, masks thesentence, and predicts the words to fill the blanks, effectively negating the input text. (Bottom Right):Summary of our token selection rules for masking. Tokens are chosen based on Part-of-speech (POS)tags and dependency labels. NegVerse masks either the selected token or its entire subtree. One challenge with the objective in Eq. 1 is that the generated set Xgen may contain incoherent orirrelevant sentences, leading to nonsensical outputs that reduce model effectiveness. Additionally, thegenerator requires a context vector to determine the appropriate negation placement, which might notbe available for every input. To address these issues, we next propose masking spans in sentencesat positions where negation is appropriate, which are used to generate structured prompts thatapproximate the missing context c, thereby enabling the model to produce accurate and contextuallyappropriate negations, even without the original context. We also provide a filter that selects only thecontextually accurate and meaningful negations Xf from Xgen, i.e., Xf = f( Xgen). Our framework isillustrated in .",
  "NegVerse Prompt Format": "Prompt Design.Our model aims to generate negated sentences that meet three key criteria: close-ness, quality, and diversity. Closeness ensures the negated sentence minimally differs from theoriginal in structure and meaning. Quality emphasizes grammatical correctness, syntactic accuracy,and coherence. Diversity involves generating a variety of negations across different sentence spans,including verbal, non-verbal, and affixal forms, to enrich the dataset and test multiple negation forms.To maintain closeness, we place [BLANK] tokens at likely negation points in the original sentencex and generate various perturbations for each blank. Our prompt format, adapted from Polyjuice,includes a negation control code, a blank sentence, and outputs separated by the [ANSWER] token,allowing the model to generate up to n possible negations per blank. During training, both individualtokens and entire sentences are masked to teach the model sentence structure and negation patterns.We provide details about the metrics for assessment in .Masking/Blanks Placement Strategy.We propose a masking strategy that enhances negationgeneration by strategically placing blanks in sentences, addressing limitations in traditional methodslike Polyjuice, which often miss key elements such as main verbs, auxiliary verbs, contractions like\"wasnt, and tense variations. Our approach masks key components, including verbs, adjectives, andspecific nouns, to support both verbal and non-verbal negations with flexible granularity, allowingfor individual token or subtree masking. We developed the token selection rules, summarized in theBottom Right Table of based on sentence structure analysis and token functions, coveringvarious aspects of sentence construction like determiners, subjects, objects, adverbs, adjectives, and",
  ": Summary of dataset samples used for Neg-Verse across different negation types": "We learn the generator using prompt tuningto update only the virtual token embeddings,while keeping a GPT-2 baseline model frozen.To achieve this, we leverage the non-verbalnegations from NAN-NLI , and the affixalnegations from SST-2 , as shown in . Given the underrepresentation of affixalnegations in the existing datasets, we developa new dataset of diverse affixal negations fromminimal input using one-shot learning and LLaMA2. During the prompt tuning process, eachsentence is masked either by negated parts or entirely, which helps the model learn to handle differentspans and levels of context, thereby enhancing its ability to produce accurate negations. The maskedsentences are tokenized, padded, and then split into training and validation sets. Additional details onthe datasets and hyperparameters are provided in Appendix C.1 and Appendix C.2, respectively.",
  ": end for14: Output: Filtered negation sets { Xf,i}mi=1": "Even though our proposed ap-proach is designed to generate flu-ent and diverse negations, some ofthe generated outputs may still con-tain errors or nonsensical phrases.To address this, we propose a fil-tering process, outlined in Algo-rithm 1, that normalizes the orig-inal and generated sentences byconverting it to lowercase andremoving trailing punctuation orwhitespace (lines 3-5), removesduplicates and uses Levenshteindistance to retain sentences thatclosely resemble the original (lines7-10). We use NegBERT to de-tect the negation cues , and weoutput negations, that were uni-formly sampled from the set ex-tracted by NegBERT, to increasethe diversity in the sets (line 12).",
  "Empirical Results": "Datasets, Baselines and Metrics.We evaluate our approach on five datasets: the Stanford NaturalLanguage Inference (SNLI) dataset , the Semantic Textual Similarity Benchmark (STS) ,COPA dataset , and the SemEval Aspect-Based Sentiment Analysis datasets for both restaurantand laptop domains . We compare NegVerse against Polyjuice and evaluate the generatedtext using (i) Levenshtein Distance (NLD) that measures the minimal edits required totransform one sentence into another; and (ii) Syntactic Tree Edit Distance (Syntactic), which focuseson surface-level changes , to assess closeness. For diversity, we use the Self-BLEU Score ,and for grammaticality and fluency we use a fine-tuned BERT model, following . The quality ofthe generated sentences is further evaluated using Perplexity (PPL) . We provide more detailsand results, including generation examples and degenerate cases of NegVerse, in Appendix C.4.Results and Discussion.We evaluate the performance of our proposed method, NegVerse, andthe baseline Polyjuice across all datasets using the closeness, diversity, and quality criteria. Theresults are presented in . We observe that NegVerse outperforms Polyjuice in closeness andtext quality for both token and subtree masking criteria. Our method achieves a lower Levenshteindistance, indicating better lexical similarity to the original sentences, and a lower syntactic score,",
  ": Experimental results of NegVerse and Polyjuice for token level and subtree masking typesusing closeness, diversity and quality criteria. The bold numbers indicate the best performance": "reflecting better preservation of syntactic structure. In contrast, Polyjuice often introduces unrelatedconcepts and alters sentence types, affecting coherence, despite offering greater diversity with alower Self-BLEU score. Moreover, our results show that both models have similar fluency andgrammaticality with token masking, but Polyjuice slightly outperforms NegVerse in these aspectswith subtree masking. This suggests Polyjuice performs better under challenging conditions but itdoes not necessarily produce more relevant text to the original content. shows an example ofnegation generation from the two approaches. We expand our discussion and provide more detailsand results, including generation examples and degenerate cases of NegVerse, in Appendix C.",
  ".They cook cooking dinner andserving it to their guests.2. They cook in the kitchen and not thedining room because the dining room isfarthest from cooking dinner and serv-ing it to their guests": ": A negation generation example for NegVerse and Polyjuice. [BLANK] marks the maskedparts of the original sentence, and the highlighted text shows the generated fill-ins. NegVerse producesoutputs that closely mirror the original sentence, while Polyjuice offers more variety in outputs, whichcontributes to diversity, but can compromise the relevance and fidelity of the text.",
  "Conclusions": "In this work, we focus on improving the robustness of LLMs robustness on negated statements byproposing NegVerse, a method capable of generating various types of negations, including verbal,non-verbal, and affixal. We provide new masking rules and propose a filtering mechanism to identifynegation cues and remove degenerate examples, producing diverse and in parallel meaningful negatedsentences. We experiment with five real-world datasets and NegVerse outperforms existing methodsand generates negations with higher lexical similarity to the original sentences, better syntacticpreservation, and greater negation diversity. Our empirical results also highlight that the proposedapproach can generate negated sentences without specific guidance on blank placement.Limitations and Future Work.While NegVerse excels in preserving syntactic structure and offersa greater variety of negation forms, it still produces some degenerate outputs, particularly when blanksare placed at the end of sentences, leading to grammatically correct but contextually meaninglessresults. Furthermore, although NegVerse generates a range of affixal negations, certain expectedforms are missing. Finally, automated and accurate annotation is essential for the generated negations,as negations can either preserve or invert labels depending on the task.",
  "Lochan Basyal and Mihir Sanghvi. Text summarization using large language models: Acomparative study of mpt-7b-instruct, falcon-7b-instruct, and openai chat-gpt models, 2023": "Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A largeannotated corpus for learning natural language inference. In Llus Mrquez, Chris Callison-Burch, and Jian Su, editors, Proceedings of the 2015 Conference on Empirical Methods inNatural Language Processing, pages 632642, Lisbon, Portugal, September 2015. Associationfor Computational Linguistics. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, ArielHerbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, MateuszLitwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, AlecRadford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020. Daniel Cer, Mona Diab, Eneko Agirre, Iigo Lopez-Gazpio, and Lucia Specia. SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation. InSteven Bethard, Marine Carpuat, Marianna Apidianaki, Saif M. Mohammad, Daniel Cer, andDavid Jurgens, editors, Proceedings of the 11th International Workshop on Semantic Evaluation(SemEval-2017), pages 114, Vancouver, Canada, August 2017. Association for ComputationalLinguistics.",
  "Anwoy Chatterjee, Eshaan Tanwar, Subhabrata Dutta, and Tanmoy Chakraborty. Languagemodels can exploit cross-task in-context learning for data-scarce novel tasks, 2024": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training ofdeep bidirectional transformers for language understanding. In Jill Burstein, Christy Doran, andThamar Solorio, editors, Proceedings of the 2019 Conference of the North American Chapterof the Association for Computational Linguistics: Human Language Technologies, Volume 1(Long and Short Papers), pages 41714186, Minneapolis, Minnesota, June 2019. Associationfor Computational Linguistics.",
  "Iker Garca-Ferrero, Begoa Altuna, Javier lvez, Itziar Gonzalez-Dios, and German Rigau.This is not a dataset: A large negation benchmark to challenge large language models, 2023": "Zhiwei He, Tian Liang, Wenxiang Jiao, Zhuosheng Zhang, Yujiu Yang, Rui Wang, ZhaopengTu, Shuming Shi, and Xing Wang. Exploring Human-Like Translation Strategy with LargeLanguage Models. Transactions of the Association for Computational Linguistics, 12:229246,03 2024. Chadi Helwe, Simon Coumes, Chlo Clavel, and Fabian Suchanek. TINA: Textual inferencewith negation augmentation. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors,Findings of the Association for Computational Linguistics: EMNLP 2022, pages 40864099,Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics.",
  "Md Mosharaf Hossain, Dhivya Chinnappa, and Eduardo Blanco. An analysis of negation innatural language understanding corpora, 2022": "Md Mosharaf Hossain, Venelin Kovatchev, Pranoy Dutta, Tiffany Kao, Elizabeth Wei, andEduardo Blanco. An analysis of natural language inference benchmarks through the lens ofnegation. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, Proceedings ofthe 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages91069118, Online, November 2020. Association for Computational Linguistics.",
  "Joel Jang, Seonghyeon Ye, and Minjoon Seo. Can large language models truly understandprompts? a case study with negated prompts, 2022": "Myeongjun Jang, Deuk Sin Kwon, and Thomas Lukasiewicz. BECEL: Benchmark for con-sistency evaluation of language models. In Nicoletta Calzolari, Chu-Ren Huang, HansaemKim, James Pustejovsky, Leo Wanner, Key-Sun Choi, Pum-Mo Ryu, Hsin-Hsi Chen, LuciaDonatelli, Heng Ji, Sadao Kurohashi, Patrizia Paggio, Nianwen Xue, Seokhwan Kim, Young-gyun Hahm, Zhong He, Tony Kyungil Lee, Enrico Santus, Francis Bond, and Seung-HoonNa, editors, Proceedings of the 29th International Conference on Computational Linguistics,pages 36803696, Gyeongju, Republic of Korea, October 2022. International Committee onComputational Linguistics.",
  "Rajvardhan Patil and Venkat Rao Gudivada. A review of current trends, techniques, andchallenges in large language models (llms). Applied Sciences, 2024": "Maria Pontiki, Dimitris Galanis, John Pavlopoulos, Harris Papageorgiou, Ion Androutsopou-los, and Suresh Manandhar. SemEval-2014 task 4: Aspect based sentiment analysis. InPreslav Nakov and Torsten Zesch, editors, Proceedings of the 8th International Workshop onSemantic Evaluation (SemEval 2014), pages 2735, Dublin, Ireland, August 2014. Associationfor Computational Linguistics. Melissa Roemmele, Cosmin Bejan, and Andrew Gordon. Choice of plausible alternatives:An evaluation of commonsense causal reasoning. In AAAI Spring Symposium on LogicalFormalizations of Commonsense Reasoning, 01 2011.",
  "Alexis Ross, Ana Marasovic, and Matthew E. Peters. Explaining nlp models via minimalcontrastive editing (mice), 2021": "Rachneet Sachdeva, Martin Tutek, and Iryna Gurevych. CATfOOD: Counterfactual augmentedtraining for improving out-of-domain performance and calibration. In Yvette Graham andMatthew Purver, editors, Proceedings of the 18th Conference of the European Chapter of theAssociation for Computational Linguistics (Volume 1: Long Papers), pages 18761898, St.Julians, Malta, March 2024. Association for Computational Linguistics.",
  "Mo Shen, Daisuke Kawahara, and Sadao Kurohashi. Dependency parse reranking with rich sub-tree features. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 22:12081218, 2014": "Rituraj Singh, Rahul Kumar, and Vivek Sridhar. NLMs: Augmenting negation in languagemodels. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Findings of the Association forComputational Linguistics: EMNLP 2023, pages 1310413116, Singapore, December 2023.Association for Computational Linguistics. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, AndrewNg, and Christopher Potts. Recursive deep models for semantic compositionality over asentiment treebank. In David Yarowsky, Timothy Baldwin, Anna Korhonen, Karen Livescu, andSteven Bethard, editors, Proceedings of the 2013 Conference on Empirical Methods in NaturalLanguage Processing, pages 16311642, Seattle, Washington, USA, October 2013. Associationfor Computational Linguistics.",
  "Christopher Toukmaji. Few-shot cross-lingual transfer for prompting large language models inlow-resource languages, 2024": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, LukasBlecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes,Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, AnthonyHartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, MadianKhabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, ThibautLavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov,Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta,Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiao-qing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, ZhengYan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, AurelienRodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundationand fine-tuned chat models, 2023.",
  "Thinh Hung Truong, Timothy Baldwin, Karin Verspoor, and Trevor Cohn. Language modelsare not naysayers: An analysis of language models on negation benchmarks, 2023": "Thinh Hung Truong, Yulia Otmakhova, Timothy Baldwin, Trevor Cohn, Jey Han Lau, and KarinVerspoor. Not another negation benchmark: The NaN-NLI test suite for sub-clausal negation.In Yulan He, Heng Ji, Sujian Li, Yang Liu, and Chua-Hui Chang, editors, Proceedings of the2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguisticsand the 12th International Joint Conference on Natural Language Processing (Volume 1: LongPapers), pages 883894, Online only, November 2022. Association for Computational Linguis-tics. Chantal van Son, Emiel van Miltenburg, and Roser Morante. Building a dictionary of affixalnegations. In Eduardo Blanco, Roser Morante, and Roser Saur, editors, Proceedings of theWorkshop on Extra-Propositional Aspects of Meaning in Computational Linguistics (ExProM),pages 4956, Osaka, Japan, December 2016. The COLING 2016 Organizing Committee.",
  ": Overview of verbal, non-verbal, and affixal negation forms, with corresponding examplesdemonstrating their application in sentences": "There are two main types of negations: morphological and syntactic negations, as outlined in .Morphological negations create negative expressions by adding affixes to words, either as prefixes orsuffixes. A prefixal negation adds prefixes to the beginning of words and includes common prefixeslike un- (e.g., unhappy), in-/im-/il-/ir- (e.g., inaccurate, impossible, illegal, irrelevant), dis-(e.g., disagree), and non- (e.g., nonexistent). A suffixal negation adds suffixes to the end of wordsand includes the common suffix -less (e.g., hopeless, meaningless). These affixes alter the meaningof the base words to convey negation, absence, or opposition . Syntactic negations utilizegrammatical structures and specific words to negate a sentence. This typically includes negativeparticles like not and no (e.g., She is not coming; There is no water), negative pronouns like nobodyand nothing (e.g., Nobody knows; Nothing happened), negative adverbs like never and nowhere(e.g., She never comes; They went nowhere), negative determiners like no and neither (e.g., Nostudents passed; Neither option is good), and negative conjunctions like nor and neither...nor(e.g., She didnt call, nor did she email; Neither he nor his friends came).",
  "In this section, we provide further details on our six rules of the masking strategy, which were outlinedearlier in .1": "Rule 1:The first rule targets verbal negations by selecting verbs (VERB) and auxiliaries (AUX) formasking, as these are key components in forming negations. For instance, in the sentence \"She waseating an apple\", masking \"was\" and \"eating\" allows the model to generate the negation \"She wasnot eating an apple.\" This approach effectively negates the core actions or states in the sentence, asillustrated in . Rule 2:The second rule targets non-verbal negation by focusing on determiners (DET) with thedependency label det. Determiners like \"the\", \"a\", and \"an\" are crucial for defining noun phrases.The model selects a determiner and the following token for transformation, such as changing \"theman\" to \"no one\", as shown in . Unlike Rule 3, which may negate entire phrases, Rule 2specifically alters the determiner. Other examples include:",
  "Peter wanted some part of it. Peter wanted none of it": "Rule 3: This rule focuses on negating objects (\"obj\") and subjects (\"subj\"), which are essential fordefining who is performing an action and what is being acted upon. Negating the subject (\"subj\")changes who or what is performing the action. For instance: : Illustrative example of sentences that follow our proposed blank placement rules. Althoughsome sentences comply with multiple rules, only the words matching the specific rule are highlightedin green for each case. Below each sentence, possible negations that can be introduced by filling inthe blanks are provided. This example demonstrates how this placement strategy can produce diverseforms of negation. The arrow sign () indicates that when the word is a determiner (DET), it masksthe accompanying noun or adjective, allowing the model to generate richer negations.",
  "She found the key She found nothingShe went to the gym She went to no gym": "Rule 4:This rule targets adverbs (ADV) with the dependency label advmod for non-verbal or affixalnegation. By masking adverbs, the rule generates various negations, such as changing \"everywhere\"to \"nowhere\" or \"not everywhere\", and \"enthusiastically\" to \"unenthusiastically\" or\"not enthusiastically\". This approach modifies the actions scope or intensity and incorporatesmorphological changes. Rule 5:This rule enables non-verbal or affixal negation by targeting adjectives (ADJ), allowing fordirect negation or morphological changes. For example, \"The solution is useful\" can be transformedto \"The solution is useless\" (affixal) or \"The solution is not useful\" (non-verbal). Rule 6:This rule handles non-verbal negation by targeting prepositions (ADP) that provide contextsuch as location or time. It is used less frequently and only when the mask subtree is active, due to itslimited variations. For example,",
  "CAdditional Experimental Setup Details and Results": "C.1Training Data DetailsIn section 4.2, we provided information about the tuning process of NegVerse by combining thenon-verbal negations from NAN-NLI , and the affixal negations from SST-2 and the newdataset we generated using LLaMA2. In what follows, we provide more information about thesedatasets. NAN-NLI:This dataset is used to evaluate models capabilities in understanding and processingsub-clausal negation instances in natural language applications. Sub-clausal negation occurs within aclause, rather than negating the entire clause itself. The dataset annotates various aspects of negation,including verbal vs. non-verbal, analytic vs. synthetic, and clausal vs. sub-clausal negation types.Additionally, it captures the constructions used in negation instances, as well as the operations applied : Dependency parse tree representing the grammatical structure of an example sentence. (A)The syntactic structure of the sentence, with arcs representing grammatical dependencies betweenwords. Dependency labels (dep tags) are displayed on the arcs, and part-of-speech tags (POS) areshown under each word, illustrating the sentences syntactic structure. (B) Tokens within the subtreerooted at the selected token are highlighted in yellow. The highlighted tokens are then masked with[BLANK] instead of just the individual token. If a verb is selected, all words dependent on it withinthe sentence are included in the subtree, resulting in the entire sentence being masked. : An illustrative example of sentence masking. The masking function considers a maximumof two tokens per sentence, and six different masked sentences. Part (A) represents the maskedsentence with Polyjuice automatic masking, where the main verb is masked in none of the options,nor were the adjectives with possible affixal negated forms like \"insusceptible\". Additionally, inOption 3, a [BLANK] was inserted rather than replacing a token. Part (B) shows how the proposedapproach masks a sentence. In particular, Option 6 masks the adjective, Option 4 masks the mainverb, and the other options mask in places to produce non-verbal negations.",
  "to construct hypotheses . The dataset provides a list of construction types used in negationinstances, where most cases involve non-verbal negations, as shown in": "SST-2:This dataset is a collection of movie reviews classified as negative or positive. It includestwo types of negations: syntactic (SYN) and morphological (AFFIX) . For training the modelwith this data, only the AFFIX annotations were filtered, where the negation cue could be translatedto a positive sentiment. For example, \"unpleasant\" can be translated to \"pleasant\". The sentenceswere converted to positive by applying manual rules, considering cases where the negation cue startswith \"un\" or ends with \"less\", using a dictionary of affixal negations from . For instance:",
  ": Definitions and examples of different negation types within the NAN-NLI dataset. Thehighlighted text in each example illustrates the specific negation construction being discussed": ": Prompt template used to generate data with affixal negations by leveraging one-shot learningwith an instruction-following LLM assistant using Llama-2-7b-Chat. The text in blue indicates wherethe new pair of words is inserted for inference. New dataset: Affixal Negations Generated from LLaMA2.Affixal negations, using prefixes orsuffixes, were underrepresented in existing datasets. To address this, we created a new dataset withadditional sentence pairs focused on affixal negations using the Llama-2-7b-Chat model and one-shotlearning, enabling efficient generation of diverse examples from minimal input . We used prompt engineering to guide the model in generating and modifying sentences. The promptsprovided structured examples of affixal negations and their transformation into positive forms withminimal changes, as shown in . For instance, the model replaced \"unattainable\" with",
  "x": ": Comparison of negated and original sentences generated with Llama 2 to illustrate affixalnegations examples for training. The generated data were manually analyzed for validity, wheresentences that did not correctly convey affixal negations were eliminated from the dataset. Sentenceswith substantial word substitutions were also excluded, as the goal is to have samples with minimalchanges. Parts of the original sentences that were eliminated are crossed out, while the validity of thechanges is indicated by for correct pairs and x for incorrect ones.",
  "C.2Hyperparameters": "We train the model for 31 epochs using the AdamW optimizer, which integrates weight decay directlyinto the optimization process. The learning rate is set to 2.5 102 to strike an optimal balancebetween training efficiency and convergence speed. A weight decay of 1 103 is employed toaddress overfitting by penalizing large weights, while a batch size of 16 ensures stable gradientupdates within memory constraints. Additionally, 24 virtual tokens are used to prompt-tune themodel, allowing for focused adaptation on the specific task. Various hyperparameters were tested andmonitored through the learning curve analysis, with these settings yielding the best results in termsof stability and performance. Out of the total 124,458,240 parameters in the pre-trained languagemodel, only 18,432 are trainable, all coming from the virtual token embeddings. This represents just0.0148% of the models parameters, demonstrating the efficiency of the soft-tuning approach.",
  "C.4Additional Results": "Negation ComparisonsIn , we provide illustrative examples of how NegVerse and Polyjuicehandle verbs and sentence masking differently. When a verb is masked, NegVerse generates variousforms of negations, including both contracted and uncontracted versions. In contrast, Polyjuice oftenintroduces unrelated concepts such as \"dining room\", \"Germany\" and \"t-glass\", which are notpresent in the original sentence and disrupt its overall coherence. Additionally, when entire sentencesare masked, NegVerse typically produces outputs that closely resemble the original, while Polyjuicefrequently creates entirely different sentences, sometimes altering the sentence type altogether, such",
  "in Germany for three yearsbefore moving back with his family toJapan": ": Examples of negation outputs for NegVerse and Polyjuice showing differences in closeness,quality and diversity. The [BLANK] marks the masked parts of the original sentence, with thehighlighted text showing the generated fill-ins. NegVerse typically produces outputs that closelymirror the original sentence, maintaining coherence. In contrast, Polyjuice offers more varied outputs,which, while contributing to diversity, sometimes compromise relevance and fidelity to the sourcetext.",
  "as changing an affirmative statement into a question, compromising this way the coherence andrelevance of the output": "On Perplexity Being a Misleading Metric.In , we show the impact of word choices onperplexity (PPL), fluency, and grammaticality in text generation. For instance, the sentence \"Hersweater is uncomfortable and pretty\" scores high in fluency and grammaticality but has a notably highPPL. The increased perplexity suggests that the word \"uncomfortable\", despite its grammaticalcorrectness and naturalness, is less predictable for the model. This may be due to the less frequentoccurrence of affixal negation such as \"un-\" in GPT-2s training data, making such constructionsmore challenging, especially when paired with a positive attribute like \"pretty\". When the sentenceis rephrased to \"Her sweater is not comfortable and pretty\", the PPL drops significantly, indicating amore predictable structure for the model. However, this rephrasing results in slightly lower fluencyand grammaticality scores. The sentence \"He doesnt offer a rational explanation for his decision\" scores high in both fluencyand grammaticality with a very low perplexity, demonstrating a case where low perplexity correlateswell with high-quality metrics. In contrast, the sentence \"She spent the day wearing nothing sweater\"exhibits high perplexity but maintains an unusually high fluency score, despite being nonsensical.This discrepancy indicates that perplexity and fluency scores may not always align with humanjudgment. Additionally, the phrase \"didnt slept\" has a higher grammaticality score than that of \"noneof the kids\", highlighting that these metrics do not always capture grammatical nuances accurately. These examples indicate that PPL and and quality scores can be useful as a general measure of amodels predictive capabilities, but they should not be used in isolation to assess the naturalness andcoherence of the generated data.",
  "Generated:": "Not remained loyal to their cause despite the challenges.None of them remained loyal to their cause despite the challenges.Not as long as they remained loyal to their cause despite the challenges.They remained loyal to their cause despite the challenges..They remained loyal to their cause despite lack of challenges..They remained loyal to their cause despite lack of adversity..They remained loyal to their cause despite lack of adversity..They remained loyal to their cause despite their struggles..They remained indifferent to their cause despite the challenges.They remained not loyal to their cause despite the challenges.They remained dispirited to their cause despite the challenges.They did not remain loyal to their cause despite the challenges.They didnt remain loyal to their cause despite the challenges.They werent loyal to their cause despite the challenges.Not remained loyal to their cause despite the challenges.They never remained loyal to their cause despite the challenges.None of them remained loyal to their cause despite the challenges.",
  "Filtered:": "None of them remained loyal to their cause despite the challengesThey remained indifferent to their cause despite the challengesThey didnt remain loyal to their cause despite the challengesThey never remained loyal to their cause despite the challengesThey werent loyal to their cause despite the challengesNot remained loyal to their cause despite the challengesThey remained dispirited to their cause despite the challengesNot as long as they remained loyal to their cause despite the challengesThey did not remain loyal to their cause despite the challengesThey remained not loyal to their cause despite the challenges : This table compares the original sentence with perturbations generated by NegVerse andthe final filtered versions. The Original text serves as the reference. The Generated section displaysdifferent sentences produced by the model, with red text (text) indicating negation cues that were notdetected by the negation detector. The Filtered section shows sentences selected based on criteriasuch as the elimination of repeated sentences, removal of sentences without negations, and filteringbased on a Levenshtein distance threshold. Key terms in the filtered sentences are highlighted asnegation cues in purple (purple). These examples showcase the effectiveness of the filtering criteriaand highlight discrepancies in negation detection, particularly where NegBERT fails to correctlydetect affixes and multi-word negation cues.",
  "notably low, suggesting that the model finds this sequence statistically probable, although the outputremains largely nonsensical": "In contrast, for the same input, the well-formed output \"He offers a rational explanation for hisdecision.\" achieves high grammatical and fluency scores, but exhibits a higher perplexity compared todegenerations. It is noteworthy that degenerate outputs occur in instances where \"[BLANK]\" appearsat the end of the sentence. Removing the period, as seen in the case with \"[BLANK]\", allows themodel to generate a correct output, indicating that the presence of the period may contribute to issuesin the generation process. The second example in demonstrates an issue with repeated text. In this case, the modelgenerates a response that includes repeated and incoherent phrases, such as \" not a young woman..........not a young woman.............. not a young woman........\". This repetitive output is truncated in the tablefor visibility but illustrates a broader problem with the models generation process. The repetitioncontributes to a low perplexity but results in a lack of coherence and meaningful content. In the output example featuring the empty token, \"[EMPTY]\" is used as a placeholder to representmissing or unspecified content. This indicates that the model was unable to generate a specificword or phrase, leading to a vague or incomplete response. The use of the empty token highlights alimitation in the models ability to produce coherent text in certain contexts. Additionally, not everyposition in a sentence is suitable for introducing a negation, which further contributes to the modelschallenges in generating appropriate and contextually accurate content.",
  "Examples of NegVerse Application.In , we provide an example of the filtered results thatwere selected from sentences generated by the model using Algorithm 1. Recall that the proposed": "filtering mechanism uniformly samples from sentences containing effective negations close to theoriginal affirmative sentence. As shown in the provided example, NegVerse sometimes missescertain negation cues, such as \"lack of\" in specific contexts. Nevertheless, the model successfullyidentified other types of negation, such as \"indifferent\" and \"dispirited\" which, although notdirect affixal negations of \"loyal\" are still relevant for expressing negation. This behaviour maystem from the models limited training data on affixal negations and insufficient exposure to diversecontexts. Despite these limitations, the model effectively detects most non-verbal negations, such as\"never\" and \"none of\" as well as verbal forms like \"didnt\" and \"werent\".",
  "NegVerse Generations with [BLANK]": "Original:She is always happy to lend a helping hand to her friends.Generated:[She is never happy to lend a helping hand to her friends., She is not alwayshappy to lend a helping hand to her friends., She is not happy to lend a helpinghand to her friends.]Filtered:[She is never happy to lend a helping hand to her friends, She is not happy tolend a helping hand to her friends, She isnot always happy to lend a helpinghand to her friends] Original:The design makes the new car highly desirable.Generated:[The design makes the new car highly undesirable., The design makes the newcar highly desirable., The design makes the new car highly undesirable., Thedesign makes the new car highly un desirable.]Filtered:[The design makes the new car highly un desirable, The design makes the newcar highly undesirable] Original:They remained loyal to their cause despite the challenges.Generated:[They remained loyal to their cause despite the challenges., They remain loyalto their cause despite the challenges., They did not remain loyal to their causedespite the challenges.]Filtered:[They did not remain loyal to their cause despite the challenges] Original:Technology allows us to connect with people across the globe instantly.Generated:[Technology allows us to connect with people across the globe instantly., Tech-nology allows us to connect with people across the world., Technology allowsus to connect with people across the globe.]Filtered:[]",
  "Original:The cat napped peacefully.Generated:[The cat napped peacefully., The cat napped peacefully.., The cat did not nappeacefully.]Filtered:[The cat did not nap peacefully]": ": Generated and filtered outputs for affirmative sentences where the entire sentence ismasked. The model tends to negate only a single part of the sentence rather than introducing diverseperturbations. Additionally, as the length of the sequence increases, the performance of the modelin negating the sentence deteriorates, resulting in cases where the model simply repeats the originalsentence, leading to no new or meaningful output. The negation cue produced by NegVerse ishighlighted. NegVerse Generation with [BLANK] for Complete Sentence Masking.In we showexamples where the model generates negated sentences without explicit guidance on blank placement.The results show that the model can produce various negations for simpler sentences effectively.However, the models performance becomes inconsistent with more complex sentences, leading toissues such as repetition or awkward phrasing. This variability indicates that while the model handlesbasic negations well, its ability to consistently apply negation across different sentence structureswithout precise blank placement can be limited. C.5Evaluation MetricsIn this section, we provide further details on the evaluation metrics used in the main manuscript toassess the performance of both the proposed approach and the baseline methods. We consider metricsto examine various aspects of negated text generation, including closeness, fluency, and diversity.",
  "max(|xi|, |xgen,i|)": "where xi denotes the reference sentence, xgen,i is the generated negated sentence from the model,and n is the total number of sentence pairs. This metric has been widely used in various studies toevaluate the similarity between sentence pairs, particularly in counterfactual evaluations.. Self-BLEU Score:This metric evaluates the diversity within a set of generated texts by measuringtheir similarity to each other, as opposed to traditional BLEU, which compares generated texts toreference texts . The Self-BLEU score is calculated as:",
  "i=1BLEUxgen,i, Xgen \\ {xgen,i}": "where m is the total number of generated sentences, xgen,i is the i-th generated sentence, andXgen \\ {xgen,i} represents the set of all generated sentences except xgen,i. A lower Self-BLEU scoreindicates higher diversity, while a higher score suggests more similarity among outputs. Perplexity: This metric evaluates how well a language model predicts a sequence of tokens, withlower perplexity indicating better fluency. It has been widely used for fluency assessment in textgeneration models like GPT-2 . For a negated sentence x = (z1, z2, . . . , zn), where n is thesentence length, the perplexity PPL(x) is given by:"
}