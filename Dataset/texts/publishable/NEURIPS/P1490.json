{
  "Abstract": "Convolutional Neural Networks exhibit inherent equivariance to image translation,leading to efficient parameter and data usage, faster learning, and improved ro-bustness. The concept of translation equivariant networks has been successfullyextended to rotation transformation using group convolution for discrete rotationgroups and harmonic functions for the continuous rotation group encompassing360. We explore the compatibility of the Self-Attention mechanism with full rota-tion equivariance, in contrast to previous studies that focused on discrete rotation.We introduce the Harmformer, a harmonic transformer with a convolutional stemthat achieves equivariance for both translation and continuous rotation. Accompa-nied by an end-to-end equivariance proof, the Harmformer not only outperformsprevious equivariant transformers, but also demonstrates inherent stability underany continuous rotation, even without seeing rotated samples during training.",
  "Introduction": "A key strength that positions Convolutional Neural Networks (CNNs) as a superior architecturefor computer vision tasks is the weight sharing across the spatial domain. This design ensures thatCNN feature maps retain their values as the input is translated, only being shifted according tothe input. Formally known as translation equivariance, this property provides CNNs with inherentrobustness and efficiency in managing translations. Equivariance can be extended to other groups",
  "arXiv:2411.03794v1 [cs.CV] 6 Nov 2024": "of transformations, such as rotation, scaling, or mirroring. The advantage of equivariant models isthat they ensure a tractable response of the model to the transformation of the input. As a result, themodel can eliminate the effects of the transformations and produce predictions that are invariant tothem. For instance, to achieve translation invariance in conventional CNNs, the feature maps arecommonly aggregated by global average pooling before the classification layer. Group Equivariant Convolutional Neural Networks (G-CNNs) show that CNNs can be modifiedto become equivariant to any discrete transformation group, such as rotation by a discrete set ofangles. An extension to continuous rotation and translation group was introduced by Worrall et al.. The authors proposed Harmonic Networks (H-Nets), which restrict the convolution filters toa family of harmonic functions ideal for expressing full rotation equivariance. Both approachesimprove the generalization and efficiency of training for the chosen group, similarly to how CNNsbenefit from translation equivariance. For example, rotation equivariant networks are well suitedfor object detection in aerial imagery because such images lack natural orientation and equivariantnetworks inherently accommodate all rotations. Beyond aerial imagery , equivariant CNNs areeffective in many other applications, such as microscopy , histology , and remote sensing . With the adoption of transformer architectures in computer vision, the Self-Attention mechanismhas also been integrated into equivariant networks . Equivariant transformers are gainingimportance especially in domains such as graph-based structures (e.g. molecules) ,vector fields , manifolds , and generic geometric data . In the 2D domain, Romeroand Cordonnier proposed a transformer equivariant to discrete rotation and translation groups byusing the principle of G-CNNs in the positional encoding of the Self-Attention (SA). The formulationwas further improved by Xu et al. . In both cases, the computational complexity of the equivariantSA increases quadratically with the number of angles in the considered rotation group, which limitsthe model angular resolution. Equivariance to continuous rotation presents a versatile solution. In this paper, we introduce Harmformer, the first vision transformer capable of achieving continuous2D roto-translation equivariance. The name is derived from circular harmonics which providethe equivariance property preserved throughout the architecture. To ensure computational efficiency,our network starts with an equivariant convolutional stem based on Harmonic networks , wherewe redesign the key components, such as activations, normalization layers, and introduce equivariantresidual connections. The stem output is divided into equivariant patches, which are then passed tothe transformer. Alongside a novel self-attention SA mechanism, we introduce layer normalizationand linear layers to guarantee end-to-end equivariance. The equivariance property allows Harmformerto remove the effect of roto-translation just before classification, preserving all relevant informationat earlier stages (see ). Through experimental validation, we show that Harmformer surpasses all previous discrete equivarianttransformers on established benchmarks . It also outperforms earlier invariantmodels on classification tasks where the model is trained solely on non-rotated data.",
  "Related Work": "We review the three foundational concepts from prior research that Harmformer builds upon: theSA mechanism, equivariant convolution networks, and transformers with a convolutional stem stage.Additionally, we discuss other equivariant transformer architectures. Visual Self-Attention The well-known SA mechanism originates from natural language processing and is widely used in computer vision since the publication of the Visual Transformer (ViT). Transformers, unlike CNNs, exhibit larger model capacities but require substantial amountsof data and have quadratic complexity with respect to input size. Transformers closely related toHarmformer include CoAtNet and, more specifically, ViTp . These architectures begin witha convolution stem to downscale the input and thereby reduce the computational complexity of thesubsequent application of SA. However, these architectures are not equivariant to roto-translation. Equivariant Convolutions Since the publishing of the G-CNNs , the concept of equivariantconvolutional networks has expanded across various modalities and transformation groups. In 2D,these transformations include rotation , scaling , and general E(2) transformations. In 3D, applications cover SO(3) transformations in volumetric data and point clouds, as well as spherical CNNs . Equivariant networks are also applied to graphs and non-Euclidean manifolds . Harmformer builds on and extends the H-Nets published by Worrallet al. , which are purely convolutional networks equivariant to continuous 360 rotation. In ourimplementation of H-Nets, we incorporate the improvements introduced in H-NeXt . Equivariant Transformers As previously mentioned, equivariant networks have integrated the SAmechanism in various domains, including 3D graphs and point clouds using irreducible representations, operations on Lie algebras , and general geometric data using geometric algebras. Particularly relevant to our work are the planar roto-translation equivariant transformers,such as Group Equivariant Self-Attention Networks (GSA-Nets) and E(2)-Equivariant VisionTransformer (GE-ViT), which reformulate relative positional encoding to construct equivarianttransformers. However, GSA-Nets and GE-ViT operate only on discrete rotation groups such ask",
  "On Equivariance in Vision Transformers": "We analyze the roto-translation equivariance of the ViT architecture, a well-known representativeof vision transformers. First, we formalize the notion of equivariance. Intuitively, a function fis equivariant to a transformation ag if the transformation and the function commute, f(ag(x)) =ag(f(x)). For example, processing a rotated input image has the same effect as directly rotating thefeatures of the unrotated image. In practice, such a definition would be too restrictive. The functionf (layer or network) typically has a different domain and codomain, so the transformation may actdifferently on each. The core idea remains the same: the model response to the input transformation ispredictable. To formally define equivariance, we draw upon the seminal work of Cohen and Welling or the more recent one formulated by Weiler et al. .Definition 3.1 (Equivariance). A function f : X Y (a whole network or a single layer) is calledgroup equivariant with respect to a group G if for every element g in G, represented by a linear mapag : X X, there exists a corresponding linear map bg : Y Y such that the following holds:",
  "A composition f2(f1(x)) of two equivariant functions f1 : X Y and f2 : Y Z is equivariant.We call invariance a special case of equivariance when bg is the identity for all g in G": "Self-Attention A key mechanism that distinguishes transformers from previous architectures is theSA layer . Before discussing the properties of SA, let us formally define it.Definition 3.2 (Self-Attention). Given an input matrix Y Rnd, where each row of Y represents afeature vector of dimension d, usually called a patch. The matrices Q (queries), K (keys), and V(values) are computed as linear projections of Y :",
  "In practice, SA is typically extended to Multi-Head Self-Attention (MSA), in which multiple SAlayers with different embedding matrices W(k,v,q) are computed in parallel and then combined": "The construction of SA implies its well-known property, in the literature often referred to as permu-tation invariance . According to Def. 3.2, it is more accurate to call the SA layer permutationequivariant rather than permutation invariant. For if we change the order of the rows in Y , the SA(Y )remains the same except for the same change in the order of its output rows. What makes ViT non-equivariant? As rotation and translation are special cases of permutation,the permutation equivariance of SA might suggest the roto-translation equivariance of the wholeViT. However, the permutation-equivariance of SA holds at the patch level and not at the pixel level,where translation or rotation takes place. In the initial stage of ViT, before the first SA layer, theimage is split into n non-overlapping patches of fixed size, typically 1616 pixels. These are linearly transformed and flattened to form the rows of the input matrix Y of the first SA layer. This patch-wiseoperation breaks the direct rotation (or translation) equivariance of ViT at the image level, becausefor an image rotation ag in Eq. (1) corresponding to an angle g from the rotation group G, there is nob acting on the patches that can be expressed as a rotation bg by g; the same holds for translation. A solution typically used by previous equivariant approaches is to consider pixel-levelpatches of size 11 pixel. Then the image rotation is equivalent to patch-level permutation, andthe corresponding transformer model remains equivariant, assuming that interpolation errors andboundary effects are minimal. This approach, however, has two major drawbacks. As seen fromEq. (3), the SA has a quadratic complexity with respect to the number of patches n, so operatingon a pixel grid (as opposed to 1616) incurs an almost 105 penalty factor in memory requirementsand correspondingly increases the processing time. To mitigate this, GSA-Nets and GE-ViT reducecomplexity by using local SA that restricts the attention field to the 77 neighborhood of thepatch. The second drawback is that the local self-attention in the first layers is not very informative,because nearby pixels are usually highly correlated. Position Encoding During construction of the input matrix Y for the first SA layer, the patches arealso given absolute position encoding that provides information about their locations. This breaksequivariance as the patches of a transformed image will receive different encoding compared to theircounterparts in the original image. Equivariant transformers replace the absolute encodingwith circular relative encoding, similar to iRPE introduced by Wu et al. . In Harmformer, we address these challenges with a convolutional stem stage that initially reducesspatial dimensions and extracts high-level features. Subsequently, we create 11 patches from thesehigh-level features and process them by the SA layers. To maintain spatial correspondence amongthe patches while ensuring equivariance, Harmformer also uses circular relative position encoding.",
  "Harmonic Convolutions and Equivariance to Continuous Roto-Translation": "To understand the equivariance property of Harmformer, it is essential to understand the concept ofharmonic convolutions introduced in H-Nets , as they are employed in the stem and affect thesubsequent transformer layers. The main difference from the traditional CNNs is that the convolutionfilters based on circular harmonic functions are specifically designed to encode rotational symmetries.The filters are defined as follows.Definition 4.1 (Harmonic Filter). A harmonic filter Wm : R2 C parameterized by a rotation orderm is given by:Wm(r, ) = R(r)ei(m+),(5)where (r, ) are polar coordinates. Here, R : R R is a learnable radial function and R is alearnable phase shift. The rotation order m is a parameter that determines the filter symmetry. As translation equivariance is inherently provided by convolution, we will focus solely on rotation inthe following discussion and denote the rotation operator by [], where is the angle of rotation. Letus look in detail at how the rotation applied to the input affects feature maps generated by harmonicconvolution. The H-Nets features are represented as complex values in polar form.Lemma 4.1 (Harmonic Convolution Property). Let I be an input image and Wm1 a harmonic filter.Under image rotation by angle , convolution of I with Wm1 is given by:",
  "Fm1([I]) Wm2 = ei(m1+m2) [Fm1(I) Wm2] .(7)": "The authors of H-Nets also construct activation, batch normalization, and pooling layers that preservethis property. As a result, their classifier can be independent of input rotation and translation. Toremove the influence of rotation, they extract only the magnitude from the last feature map anddiscard the phase. To aggregate spatial information, they use global average pooling. Note that for",
  "tasks where the orientation of the object is relevant, phase can be used as no information is lost dueto equivariance": "To unify the equivariance property within the Harmformer architecture, we define Harmonic Equiv-ariance (HE), which is motivated by Lemma 4.1 and satisfies the general definition of equivariance(Def. 3.1). HE describes how features transform with respect to the rotation of an input image. Byshowing that each Harmformer layer satisfies HE, we establish the relationship between the featuresand the rotation of an input throughout the model.Definition 4.2 (Harmonic Equivariance HE). A layer Fm() associated with a rotation order m issaid to be HE, if for any rotation by angle and admissible input I, it is transformed as follows:Fm([I]) = eim [Fm(I)] .(8)Here [Fm(I)] are features obtained from an unrotated input I and then rotated. The phase is shiftedby a multiple of the rotation angle, where the factor is given by the rotation order of the layer. Theprocess is illustrated in a.",
  "Harmformer Architecture": "The architecture of Harmformer is shown in and its layers will be discussed one by one.HE (Def. 4.2) of each layer is proved in Appendix A, demonstrating the end-to-end continuousrotation and translation equivariance. The architecture begins with a stem stage based on H-Nets,which we have further improved by refining activation and normalization layers and incorporatingresidual connections. The stem is followed by an equivariant encoder tailored to maintain HE, andthe last component is a classifier, which takes the HE output of the encoder and computes an invariantrepresentation for classification.",
  "Harmformer: S1 Stem Stage": "The main role is to prepare features for the Harmonic Encoder (S3) so that they are HE and havelower spatial resolution to keep the computational complexity of SA manageable, as discussed in Sec.3. To this end, we design the stage to comprise r iterations of H-Conv blocks, followed by averagepooling, as shown in . Each iteration increases the number of channels while decreasing thespatial dimension.",
  "The stage starts with an input that formally satisfies HE for the rotation order m = 0, expressed as[I] = ei0 [I], followed by the first H-Conv block shown in b": "Rotation Order Streams The HE and the definition of Harmonic Convolution have already beendetailed in Lemma 4.1 and Def. 4.1. An important aspect that remains to be addressed is the selectionof rotation orders for the harmonic filters. In our initial convolution with the input image (often calledlifting convolution), we employ harmonic filters of rotation orders 1, 0, and 1. This setup producesthree streams of feature maps, each corresponding to one of these rotation orders. Our experiments, along with the results reported in , indicate that generating feature maps ofhigher rotation orders does not significantly improve performance but increases the computationalcomplexity. Based on this evidence, we limit rotation orders to 1, 0, and 1.",
  "where m, m1, and m2 are the rotation orders of the output, input, and harmonic filter, respectively": "Layers Operating on Magnitude Because rotation affects only the phase of the features leaving themagnitude untouched, element-wise functions, such as normalization or activation, operating onlyon magnitudes preserve the HE property. In contrast with previous H-Nets , we restrict thecodomain of every element-wise function f transforming magnitudes to non-negative numbers, f :R R+0 , since negative magnitudes inadvertently flip the phase, thus violating the HE property. Thisconsideration leads us to propose a novel normalization fused together with activation (HBatchNormand C-ReLu), detailed in Appendix A.4. Restricting the codomain and fusing the normalization withthe activation has a positive impact on performance, as shown in Ablation B.1. Residual Connection The final stem element is the residual connection, previously unused inH-Nets. Residual connections are also used within our encoder blocks. As in standard CNNs, theyimprove gradient flow and reduce training time. With respect to rotation orders, they process streamsindependently, thus preserving HE according to the following lemma:",
  "Harmformer: S2 Construction of the Patches": "To integrate the stem output with the encoder, the final stem feature maps are divided into 11-sizedpatches, as illustrated in a. The patches are constructed separately for all three streams ofrotation orders. The resulting stack of patches then comprises three matrices F1, F0, F1 C(hw)d,each representing a single rotation order, where h, w, and d denote the height, width, and numberof channels of the last feature maps, respectively. We keep this notation for encoder feature maps(patches), as they correspond to the stem feature maps, just reshuffled. Neglecting small interpolation errors, the spatial transformation of the input translates only intoa permutation of the stack of patches Fm as discussed in Sec. 3. For clarity, we use a discreterepresentation but it should be noted that the encoder can be modeled using a functional framework,as shown by Romero and Cordonnier . Before feeding the SA with patches, transformer networks typically apply a linear projection to adjustthe dimension d. We use a linear layer that processes the patches independently with respect to theirorder of rotation to preserve HE:",
  "Harmformer: S3 Harmonic Encoder": "This section outlines our encoder, which is designed to preserve the HE property. The encoder isorganized into several k blocks, each containing Multi-Head Self-Attention (MSA) and Multi-LayerPerceptron (MLP) components, as shown in a. Along with the layers presented in the previoussections, we propose a SA mechanism and a layer normalization, both of which preserve the HE.",
  "where , are the sample means and standard deviations of the original feature maps computed overtheir spatial dimensions, respectively, and is a small constant added for numerical stability": "Self-Attention The essential components of the encoder are MSA layers. The proposed MSA mixesfeatures with different rotation orders. In the first step, queries, keys and values are generated foreach rotation order 1, 0, and 1 independently, which preserves HE as follows from Lemma 5.2. Wesplit the SA calculation (Eq. (3),(4)) into two operations: dot product and matrix multiplication, anddemonstrate their properties by the following lemmas.Lemma 5.4 (Dot product subtracts rotation orders). Consider two HE feature maps Qm1([I]) C(hw)d and Km2([I]) C(hw)d that represent queries and keys, respectively. The dot productof these feature maps is HE and has the rotation order m1 m2. Formally, we have:",
  "Qm1([I])Km2([I])T = ei(m1m2) Qm1(I)Km2(I)T,(13)": "where Km2([I])T denotes the complex conjugate transpose of Km2([I]).Lemma 5.5 (Matrix multiplication sums rotation orders). Consider a HE feature map Am1([I]) C(hw)(hw) representing an attention matrix and HE feature map Vm2([I]) C(hw)d representingvalues. The result of their matrix multiplication is HE with a rotation order m = m1 + m2:",
  "where [Am1(I)] and [Vm2(I)] are feature maps created from unrotated I and rotated afterwards": "After these operations, the relative circular encodings are added to the result of the dot productbefore it undergoes the softmax activation. The Harmformer softmax operates only on magnitudesand its codomain is within R+0 to avoid breaking HE. We have shown that the dot product between queries Qm1 and keys Km2 results in a rotation orderof m = m1 m2. Similarly, matrix multiplication between the attention matrix Am1 and valuesVm2 yields a feature map with a rotation order of m = m1 + m2. The final task is to combine theserotation orders to produce output feature maps with the same number of rotation orders as the inputfeature maps, i.e. -1, 0, and 1. Mixing Orders in MSA Since there are multiple strategies for combining rotation orders, weexplored several of them and provide details on other configurations in Ablation B.2. The optimalapproach, according to our experiments, is shown in and involves: 1. Dot Product Calculation: The dot product is computed only between the same rotationorders, separately. According to Lemma 5.4, this results in three feature maps with therotation order 0 and dimension C(hw)(hw). 2. Attention Matrix Formation: These results are summed to form a single matrix ofrotation order 0. A softmax function is then applied to form a single attention matrixA0 C(hw)(hw), preserving the rotation order 0, because softmax function operates onlyon magnitudes and outputs non-negative numbers. 3. Self-Attention Output: Finally, the self-attention output is produced by matrix multiplica-tion of the attention matrix (rotation order zero) with the values of each rotation order. Thisprocess results in a triplet of outputs with the target rotation orders (1, 0, 1). Other layers, such as linear layers and residual connections, have been introduced in previous sections.By stacking HE layers, the last feature map coming from our encoder will maintain the HE property.",
  "Harmformer: S4 Classification": "Spatial position and orientation are generally redundant for classification tasks, except when classify-ing directional objects such as arrows. In the final stage, we remove this redundant information fromthe feature maps and produce an invariant feature vector. The feature maps entering the classificationstage form a matrix of the shape C3nd. To aggregate over different rotation orders, we keep onlythe magnitude, resulting in Rn3d. The spatial information is then eliminated by applying globalaverage pooling over the dimension n (patches), reducing the shape to R3d. The final feature vector,which is roto-translation invariant, is processed by a single linear layer for classification.",
  "mnist-rot-test28 28 150k / 10k / 10k/1cifar-rot-test32 32 342k / 10k / 8k/1rotated MNIST28 28 110k / 2k / 50k/2PCam96 96 3262k / 32k / 32k/2": "Model Architecture The models are designed to match the number of parameters of the previousstate-of-the-art models while maintaining the same overall architecture. Depending on the benchmark,the stem stage consists of 2-4 blocks to reduce resolution, followed by 3-4 harmonic encoderblocks. To ensure that the equivariant properties emerge from the architecture, we avoid any dataaugmentation. Consistent with H-NeXt , the inputs are initially upscaled by a factor of two tomitigate interpolation errors.",
  "mnist-rot-test0.910.142cifar-rot-test31.90.636cifar-rot-test (Large)29.70.203": "Invariance Benchmarks In the first scenario, we verify the equivariance of Harmformer by trainingthe model exclusively on upright (non-rotated) data and testing it on randomly rotated data; the firsttwo datasets in Tab. 1. Since the model is trained only on upright images, any equivariant propertiesmust arise purely from the model design, not from the training data. We outperform previous methods on both datasets, as shown in Tables 2 and 3. Harmformerimproves the robustness to rotation, as we discuss further in Section B.4, which partially explainsthe performance gain on mnist-rot-test. Stability under rotation is less enhanced on cifar-rot-test(Sec. B.4), and the superior results there are likely due to the higher model capacity of the transformerarchitecture, which improves the overall detection performance. Equivariance Benchmarks In the second scenario, we compare the performance of the Harmformeron established equivariance benchmarks for roto-translation where there is no significant distributionshift between the training and test sets, either containing rotated samples or not. This evaluationassesses how our method stacks up against previous equivariant transformers that are equivariant todiscrete rotation and translation. Tables 4 and 5 show the top results on the rotated MNIST and PCamdatasets, respectively. Harmformer outperforms previous equivariant transformers and narrows theperformance gap between equivariant transformers and convolution-based models. For completeness, we include the average performance in each benchmark listed in . Theaccuracy on the PCam dataset was slightly unstable, probably due to the characteristics of the dataset.Methods specifically designed for PCam, such as , use extensive augmentation techniques, whichwere avoided in our case to ensure unbiased results.",
  "Conclusion and Future Work": "The proposed Harmformer is the first transformer model to achieve end-to-end equivariance tocontinuous rotation and translation in 2D. This was accomplished by designing an equivariant self-attention inspired by harmonic convolution. Along with the novel SA, we introduced several layersspecifically tailored for equivariance, including linear layers, layer normalization, batch normalization,activations, and residual connections. Our model outperforms previous equivariant transformers,narrowing the performance gap with convolution-based equivariant networks. We hypothesize that the full potential of transformers may not be realized due to the nature oftraditional benchmarks. The 2D equivariant transformers have so far been tested on datasets containingrelatively small images that lack global dependencies. Therefore, future research should explorethe application of equivariant transformers on larger datasets where, similar to ViT, they coulddemonstrate their potential. In addition, the proposed model can be extended to other modalitieswhile maintaining its equivariance properties. For example, the harmonic networks that form thebasis of our approach can also be adapted for 3D applications.",
  "Yann LeCun, Lon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied todocument recognition. Proceedings of the IEEE, 86(11):22782324, 1998. doi: 10.1109/5.726791. URL": "Taco Cohen and Max Welling. Group Equivariant Convolutional Networks. In Proceedings of The 33rdInternational Conference on Machine Learning, pages 29902999. PMLR, 2016. doi: 10.48550/arXiv.1602.07576. URL Daniel E Worrall, Stephan J Garbin, Daniyar Turmukhambetov, and Gabriel J Brostow. HarmonicNetworks: Deep Translation and Rotation Equivariance. In Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition (CVPR), pages 50285037, 2017. doi: 10.48550/arXiv.1612.04642. URL Jiaming Han, Jian Ding, Nan Xue, and Gui-Song Xia.Redet:A rotation-equivariant de-tector for aerial object detection.In Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition (CVPR), pages 27862795, June 2021.URL Jian Ding, Nan Xue, Yang Long, Gui-Song Xia, and Qikai Lu.Learning RoI transformerfor oriented object detection in aerial images.In Proceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition (CVPR), June 2019.URL Benjamin Chidester, Tianming Zhou, Minh N Do, and Jian Ma. Rotation equivariant and invariant neuralnetworks for microscopy image analysis. Bioinformatics, 35(14):i530i537, 07 2019. ISSN 1367-4803.doi: 10.1093/bioinformatics/btz353. URL Benjamin Chidester, That-Vinh Ton, Minh-Triet Tran, Jian Ma, and Minh N. Do.Enhancedrotation-equivariant U-Net for nuclear segmentation.In Proceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition (CVPR) Workshops, June 2019.URL Simon Graham, David Epstein, and Nasir Rajpoot. Dense steerable filter cnns for exploiting rotationalsymmetry in histology images. IEEE Transactions on Medical Imaging, 39(12):41244136, 2020. doi:10.1109/TMI.2020.3013246. URL Gong Cheng, Junwei Han, Peicheng Zhou, and Dong Xu. Learning rotation-invariant and fisher discrimi-native convolutional neural networks for object detection. IEEE Transactions on Image Processing, 28(1):265278, 2019. doi: 10.1109/TIP.2018.2867198. URL Fabian Fuchs, Daniel Worrall, Volker Fischer, and Max Welling. Se(3)-transformers: 3d roto-translationequivariant attention networks. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin,editors, Advances in Neural Information Processing Systems, volume 33, pages 19701981. Curran As-sociates, Inc., 2020. URL David W. Romero and Mark Hoogendoorn. Co-attentive equivariant neural networks: Focusing equivari-ance on transformations co-occurring in data. In International Conference on Learning Representations,2020. URL Michael J Hutchinson, Charline Le Lan, Sheheryar Zaidi, Emilien Dupont, Yee Whye Teh, and HyunjikKim. Lietransformer: Equivariant self-attention for lie groups. In Marina Meila and Tong Zhang, editors,Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings ofMachine Learning Research, pages 45334543. PMLR, 1824 Jul 2021. URL Ilia Igashov, Hannes Strk, Clment Vignac, Arne Schneuing, Victor Garcia Satorras, Pascal Frossard,Max Welling, Michael Bronstein, and Bruno Correia. Equivariant 3d-conditional diffusion model formolecular linker design. Nature Machine Intelligence, 6(4):417427, Apr 2024. ISSN 2522-5839. doi:10.1038/s42256-024-00815-9. URL Yi-Lun Liao, Brandon M Wood, Abhishek Das, and Tess Smidt. EquiformerV2: Improved equivarianttransformer for scaling to higher-degree representations. In The Twelfth International Conference onLearning Representations, 2024. URL",
  "Philipp Thlke and Gianni De Fabritiis. Equivariant transformers for neural network based molecularpotentials. In International Conference on Learning Representations, 2022. URL": "Serge Assaad, Carlton Downey, Rami Al-Rfou, Nigamaa Nayakanti, and Benjamin Sapp. VN-transformer:Rotation-equivariant attention for vector neurons. Transactions on Machine Learning Research, 2023.ISSN 2835-8856. URL Lingshen He, Yiming Dong, Yisen Wang, Dacheng Tao, and Zhouchen Lin. Gauge equivariant trans-former. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors,Advances in Neural Information Processing Systems, volume 34, pages 2733127343. Curran Asso-ciates, Inc., 2021.URL Pim de Haan, Taco Cohen, and Johann Brehmer. Euclidean, projective, conformal: Choosing a geometricalgebra for equivariant transformers. In Sanjoy Dasgupta, Stephan Mandt, and Yingzhen Li, editors,Proceedings of The 27th International Conference on Artificial Intelligence and Statistics, volume 238of Proceedings of Machine Learning Research, pages 30883096. PMLR, 0204 May 2024. URL Johann Brehmer, Pim de Haan, Snke Behrends, and Taco S Cohen.Geometric algebra trans-former.In A. Oh, T. Neumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Ad-vances in Neural Information Processing Systems, volume 36, pages 3547235496. Curran Asso-ciates, Inc., 2023.URL",
  "William T Freeman et al. The design and use of steerable filters. IEEE Transactions on Pattern analysisand machine intelligence, 13(9):891906, 1991": "Hugo Larochelle, Dumitru Erhan, Aaron Courville, James Bergstra, and Yoshua Bengio. An empiricalevaluation of deep architectures on problems with many factors of variation. In Proceedings of the 24thInternational Conference on Machine Learning, pages 473480. Association for Computing Machinery,2007. ISBN 978-1-59593-793-3. doi: 10.1145/1273496.1273556. URL Babak Ehteshami Bejnordi, Mitko Veta, Paul Johannes van Diest, Bram van Ginneken, Nico Karssemeijer,Geert Litjens, Jeroen A. W. M. van der Laak, , and the CAMELYON16 Consortium. Diagnostic Assessmentof Deep Learning Algorithms for Detection of Lymph Node Metastases in Women With Breast Cancer.JAMA, 318(22):21992210, 12 2017. ISSN 0098-7484. doi: 10.1001/jama.2017.14585. URL Bastiaan S. Veeling, Jasper Linmans, Jim Winkens, Taco Cohen, and Max Welling. Rotation equivariantcnns for digital pathology. In Medical Image Computing and Computer Assisted Intervention MICCAI2018: 21st International Conference, Granada, Spain, September 16-20, 2018, Proceedings, Part II,page 210218, Berlin, Heidelberg, 2018. Springer-Verlag. ISBN 978-3-030-00933-5. doi: 10.1007/978-3-030-00934-2_24. URL Tom Karella, Filip roubek, Jan Blaek, Jan Flusser, and Vclav Kok. H-NeXt: The next steptowards roto-translation invariant networks. In 34th British Machine Vision Conference 2023, BMVC 2023,Aberdeen, UK, November 20-24, 2023. BMVA, 2023. URL Sungwon Hwang, Hyungtae Lim, and Hyun Myung. Equivariance-bridged SO (2)-invariant representationlearning using graph convolutional network. In The 32nd British Machine Vision Conference (BMVC2021). The British Machine Vision Association, 2021. doi: 10.48550/arXiv.2106.09996. URL Renata Khasanova and Pascal Frossard. Graph-based isometry invariant representation learning. In Proceed-ings of the 34th International Conference on Machine Learning, volume 70, pages 18471856. PMLR, 2017.doi: 10.48550/arXiv.1703.00356. URL Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ukaszKaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach,R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems,volume 30. Curran Associates, Inc., 2017. URL Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, ThomasUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit,and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. InInternational Conference on Learning Representations, 2021. URL Zihang Dai, Hanxiao Liu, Quoc V Le, and Mingxing Tan. Coatnet: Marrying convolution and attentionfor all data sizes. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan,editors, Advances in Neural Information Processing Systems, volume 34, pages 39653977. Curran As-sociates, Inc., 2021. URL Tete Xiao, Mannat Singh, Eric Mintun, Trevor Darrell, Piotr Dollar, and Ross Girshick. Early convolutionshelp transformers see better. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. WortmanVaughan, editors, Advances in Neural Information Processing Systems, volume 34, pages 3039230400.Curran Associates, Inc., 2021.URL Maurice Weiler, Fred A. Hamprecht, and Martin Storath. Learning steerable filters for rotation equiv-ariant cnns.In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition(CVPR), June 2018. URL Daniel Worrall and Max Welling. Deep scale-spaces: Equivariance over scale. In H. Wallach, H. Larochelle,A. Beygelzimer, F. d'Alch-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information ProcessingSystems, volume 32. Curran Associates, Inc., 2019. URL",
  "Ivan Sosnovik, Micha Szmaja, and Arnold Smeulders. Scale-equivariant steerable networks. In Inter-national Conference on Learning Representations, 2020. URL": "Md Ashiqur Rahman and Raymond A. Yeh.Truly scale-equivariant deep nets with fourier lay-ers.In A. Oh, T. Neumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Ad-vances in Neural Information Processing Systems, volume 36, pages 60926104. Curran Asso-ciates, Inc., 2023.URL Maurice Weiler and Gabriele Cesa. General E(2)-equivariant steerable CNNs. In H. Wallach, H. Larochelle,A. Beygelzimer, F. d'Alch-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information ProcessingSystems, volume 32. Curran Associates, Inc., 2019. URL Maurice Weiler, Mario Geiger, Max Welling, Wouter Boomsma, and Taco S Cohen. 3D Steerable CNNs:Learning rotationally equivariant features in volumetric data. In S. Bengio, H. Wallach, H. Larochelle,K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems,volume 31. Curran Associates, Inc., 2018. URL",
  "Maurice Weiler, Patrick Forr, Erik Verlinde, and Max Welling. Equivariant and Coordinate Inde-pendent Convolutional Networks. 2023. URL": "Yang Liu, Yao Zhang, Yixin Wang, Feng Hou, Jin Yuan, Jiang Tian, Yang Zhang, Zhongchao Shi, JianpingFan, and Zhiqiang He. A survey of visual transformers. IEEE Transactions on Neural Networks andLearning Systems, pages 121, 2023. doi: 10.1109/TNNLS.2022.3227717. URL Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, and Jon Shlens.Stand-alone self-attention in vision models. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch-Buc,E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Cur-ran Associates, Inc., 2019. URL Kan Wu, Houwen Peng, Minghao Chen, Jianlong Fu, and Hongyang Chao. Rethinking and improvingrelative position encoding for vision transformer. In Proceedings of the IEEE/CVF International Confer-ence on Computer Vision (ICCV), pages 1003310041, 2021. doi: 10.48550/arXiv.2107.14222. URL Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducinginternal covariate shift. In Francis Bach and David Blei, editors, Proceedings of the 32nd InternationalConference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pages 448456, Lille, France, 0709 Jul 2015. PMLR. URL J. Staal, M.D. Abramoff, M. Niemeijer, M.A. Viergever, and B. van Ginneken. Ridge-based vesselsegmentation in color images of the retina. IEEE Transactions on Medical Imaging, 23(4):501509, 2004.doi: 10.1109/TMI.2004.825627. Erik J. Bekkers, Maxime W. Lafarge, Mitko Veta, Koen A. J. Eppenhof, Josien P. W. Pluim, and RemcoDuits. Roto-translation covariant convolutional networks for medical image analysis. In Medical ImageComputing and Computer Assisted Intervention MICCAI 2018: 21st International Conference, Granada,Spain, September 16-20, 2018, Proceedings, Part I, page 440448, Berlin, Heidelberg, 2018. Springer-Verlag. ISBN 978-3-030-00927-4. doi: 10.1007/978-3-030-00928-1_50. URL Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedicalimage segmentation. In Medical image computing and computer-assisted interventionMICCAI 2015: 18thinternational conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, pages 234241.Springer, 2015. Wentao Liu, Huihua Yang, Tong Tian, Zhiwei Cao, Xipeng Pan, Weijin Xu, Yang Jin, and Feng Gao. Full-resolution network and dual-threshold iteration for retinal vessel and coronary angiograph segmentation.IEEE Journal of Biomedical and Health Informatics, 26(9):46234634, 2022. doi: 10.1109/JBHI.2022.3188710.",
  "AProofs of Harmformer Equivariance": "In this section, we systematically formulate the proofs of the HE property (Definition 4.2) for eachlayer of the Harmformer. The central concept of the architecture is the handling of three streamscorresponding to different rotation orders. By oversimplifying the interactions of rotation orders, twomain properties of the harmonic function should be highlighted:",
  "A.1Equivariance of Harmonic Convolutions": "Note that the proof of H-Conv equivariance was originally formulated by Worrall et al. . For thesake of completeness, we have provided a highly simplified version of these proofs. However, weencourage readers to read the more comprehensive work on G-steerable convolution kernels and thetheory of steerable equivariant convolution networks in (Chapters 4-5), which provides a broaderperspective and demonstrates the equivalence with G-CNNs. Lemma A.1 (Rotation of a Harmonic Filter). When the coordinates of a harmonic filter are rotatedby an angle , it only changes by a factor eim, where m is the rotation order of the harmonic filterand R is a corresponding 2D rotation matrix.",
  "+ ei = BN,(X)ei,(30)": "where Xei represents a complex number in exponential form, and , R are learnable scalingand shifting parameters, respectively. Here, and denote the running sample mean and variance,which are estimated during the training phase and fixed during inference. However, this formulation can produce negative magnitudes, thus inverting the phase and violatingHE. Therefore in Harmformer we instead use a batch normalization integrated with an activationfunction, that can be defined as:Definition A.4 (Harmformer HBatchNorm + C-ReLU).",
  "+ bei,(31)": "where Xei represents a complex number in exponential form, and a, b R are learnable scalingand shifting parameters, respectively. Here, and denote the running sample mean and variance,which are estimated during the training phase and fixed during inference. Our formulation uses the ReLU function, which maps R to R+, to ensure that changes in magnitudesare always positive. Additionally, by integrating the scaling and shifting parameters a and b intobatch normalization, the number of learnable parameters is reduced. See section B.1 for a comparisonof different normalization layers.",
  "A.4Discrete Representation": "The normalization layers are the last from the stem stage, as can be seen in . For the sakeof clarity, we will make the transition to discrete space and focus only on rotation for the followinglayers, as mentioned in .2. Suppose that the feature maps (stack of patches) Fm(I) Cnd,extracted from the input image I, transforms under a rotation of the input as follows",
  "This shows that the dot product result is also HE with a rotation order of m1 m2": "Lemma A.6 (Matrix multiplication sums rotation orders (Lemma 5.5)). Consider a HE featuremap Am1([I]) C(hw)(hw) representing an attention matrix, and HE feature map Vm2([I]) C(hw)d representing values. The result of their matrix multiplication is HE with a rotation orderm = m1 + m2:Am1([I])Vm2([I]) = ei(m1+m2) [Am1(I)Vm2(I)] .(46)where [Am1(I)], [Vm2(I)] are feature maps created from unrotated I and rotated afterwards.",
  "B.1Ablation: Normalization Layers in Stem Stage (S1)": "In Section A.4, we propose a modification of batch normalization by integrating it with an activationfunction. Specifically, we first apply batch normalization to the feature magnitudes, followed bya ReLU activation on these normalized values. This approach is more consistent with the originalpurpose of batch normalization as formulated by Ioffe and Szegedy , which is to standardize thedistribution of activations across layers. To test this novel normalization approach, we replaced our normalization layers in the Harmformer H-Conv block (see b) with the original H-Nets batch normalization followed by a C-ReLU.We also evaluated how our proposed layer normalization used in the encoder block would perform inthe H-Conv block. The results depicted in show that our proposed normalization (blue bar) outperforms theoriginal H-Nets normalization layer (red bar) across all three benchmarks, significantly reducingvariance across different runs. It also exceeds the performance of layer normalization (yellow bar)in the rotated MNIST and mnist-rot-test, although it slightly underperforms in the cifar-rot-test. Inaddition, the layer normalization was more computationally expensive according to our experiments. : Ablation study on different normalization layers. The rows represent different normalizationlayers in the H-Conv block. Each plot is aggregated from 5 different runs. The error bars representthe standard deviation.",
  "B.2Ablation: Mixing Rotation Orders in Self-Attention Mechanism": "As mentioned in the main text (), determining how queries, keys, and values shouldinteract based on their rotation order is not intuitive. Therefore, we have extensively tested variousconfigurations and listed the most promising ones in this section. In choosing the final solution, inaddition to performance, we focused on the principles that the number of streams should not increaseand that the method should not require extensive computation. : Ablation study on mixing rotation orders within the SA mechanism. a) The principle usedin Harmformer b) Mixing all possible combinations of queries, keys and values c) Cross Values amethod of mixing only values of different rotation orders. In Lemmas 5.4 and 5.5, we demonstrate that the dot product subtracts the rotation orders and matrixmultiplication sums the rotation orders. Based on this, we propose several configurations, illustratedin . Apart from those mentioned here, we investigated learnable weights for each rotationorder combination and different placements of softmax or combinations of these configurationstogether, but none yielded significant improvements. The final configuration used in Harmformer isshown in a. The configuration in b allows all possible combinations to produce thethree streams (1, 0, -1). The last configuration, Cross Values, illustrated in c, uses higherrotation orders only for values. Similarly, we tested Cross Keys and Cross Queries only for keys andqueries, respectively. presents the performance of these configurations on our benchmarks. The only configurationsurpassing Harmformer (a) was Mixing All (b) in the case of rotated MNIST andmnist-rot-test. Since the performance difference was minor and the computational demands weresignificantly higher, we did not use Mixing All in our final architecture.",
  "B.4Experiments: Stability of Classification w.r.t. Input Rotation": "In these experiments, we investigate the influence of input interpolation errors on performance,following previous invariant models . Stability is primarily examined using the invariancebenchmarks mnist-rot-test and cifar-rot-test, where the training data consists of non-rotated images,while the test data consists of randomly rotated images. Note that this implies that the training imagesconsist of original sharp images, but the test images contain images with interpolation errors. Incontrast, the rotated MNIST dataset contains rotated images in both the training and test sets, resultingin interpolation errors in both sets. The test accuracy with respect to the input rotation is shown in . Since the rotated MNISTdataset does not contain all images rotated by all angles, we use the original MNIST dataset forthis experiment. As a result, the test set, and therefore its accuracy, is different from that described in of the main text. For the mnist-rot-test, we observe very small oscillations, almost the same as for rotated MNIST. Theaccuracy reaches maxima at 0, 90, 180, and 270, where there is no interpolation effect. For thecifar-rot-test, the oscillations are more significant due to the low resolution of the dataset relativeto the complexity of the objects, with minima at 45, 135, 225, and 315, where the interpolationerrors are greatest. : Classification stability with respect to input image rotation. The angle in the circularplots represents the rotation angle of the input image, and the radius represents the test accuracy onspecified benchmarks. For comparison with previous invariant models, we have included . For the mnist-rot-test,there is a significant improvement in , which represents the difference in accuracy between theinterpolation-free and interpolation-affected images. However, for the cifar-rot-test, the gap betweenthese cases remains almost the same. For the MNIST datasets, the results are almost the same whethertraining with rotated or unrotated data. This leads to the hypothesis that if the resolution of the dataset",
  "B.5Experiments: Evaluating the Role of Harmonic Convolutions": "To investigate the importance of the convolutional stem and the encoder, we conducted an experimentusing a minimal stem stage with an enlarged attentive field. The purpose of this setup was to ensurethat the recognition was not due to convolution alone. This configuration contained only threeconvolutional layers and a single pooling layer. The results, as shown in , indicate that the model performance remains within the expectederror range despite the simplified convolutional stem. This suggested that the encoder plays a crucialrole in the final classification. Notably, the inclusion of a single pooling layer significantly enhancesthe complexity of subsequent attention mechanisms. Due to the increased GPU RAM requirements,this configuration was exclusively tested on the rotated MNIST dataset.",
  "C.1Compute Resource": "Each experiment was run on a single GPU within our shared, small but diverse cluster comprising 17GPUs. The cluster includes Tesla P100, V100, and A100 models, NVIDIA GeForce RTX 2080 Ti,3080, 4090, RTX A5000, and a Quadro P5000. Despite its limited size, our setup allowed for flexibleand scalable computation using various GPU configurations. To provide a better overview, lists the epoch training time across each experiment on the NVIDIA GTX 4090.",
  "C.2Computation Complexity w.r.t. Non-Equivariant Convolution and SA Mechanism": "In general, equivariant networks usually due to their properties impose higher computation complexitythan their classical counterparts. For example, a single classical convolution has complexity O(N 2 n2), where N N is the spatial dimension of the output feature map and n n is the size of the filter. In contrast, the original G-CNN equivariant to rotation and translation has complexityO(N 2 n2 ||2), where is the number of elements in the rotation group. Thus, a G-CNN equivariantto 90-degree rotations and translation would have || = 4. Harmformer Convolution In Harmformer stem stage, we use convolution layers similar to H-Nets.This has a complexity of O(N 2 n2 |o|2), where |o| is the number of rotation orders of the inputand output feature maps. Harmformer SA mechanism Classical global SA mechanism has a complexity of O(N 2d+N d2),where N is the number of patches and each patch has dimension d. Our SA mechanism, as shown inb, adds multiplication by rotation orders o for matrix multiplication and dot product, resultingin a complexity of O(o N 2 d + o N d2). Additional Computational Considerations Harmformer operates in the complex domain, whereeach multiplication requires four times and each addition requires two times more operations thantheir real counterparts. Additionally, the computational load increases due to upscaling the input andusing large convolution kernels, as recommended in H-NeXt . These factors also contribute tothe overall complexity of Harmformer.",
  "C.3Configurations of Experiments": "This subsection details the specific configurations of the Harmformer architecture used in the experi-ments described in . For convenience, , which depicts the complete Harmformerarchitecture, is included. The parameters for each dataset are enumerated in the following tables:Tables 12 and 13 for the MNIST-rot-test and rotated MNIST datasets; Tables 14 and 15 for theCIFAR-rot-test dataset; and Tables 10 and 11 for the PCam dataset.",
  "DSegmentation experiment: Retina blood vessel segmentation": "To demonstrate the generalizability and scalability of our architecture beyond classification tasks,we introduce Harmformer for retinal blood vessel segmentation using the DRIVE dataset . TheDRIVE is binary segmentation task, where the goal is to extract retinal blood vessels from an RGBimage. The dataset contains 262,080 samples for training and 65,520 samples for validation, similar to thesettings of . Each sample consists of an input image of size 3 64 64 and a target segmentationmask of size 6464. These samples were generated from 17 training images and 3 validation images,each of which is 768 584 pixels and represents a different patient. To use Harmformer as an image-to-image model, we adopt a U-Net architecture in Fig 11a.Unlike our classification models (), this model processes the images at their originalresolution, without any upscaling before they enter the network. For the output, we use only themagnitude of the final feature maps. To merge the hidden features (channels) into a single outputlayer, we apply a standard 2D convolution layer at the end.",
  ": (a) Diagram of the Harmformer architecture for image segmentation. (b) Example of animage from the DRIVE dataset: the RGB image and the target segmentation mask": "We trained the U-Net Harmformer for 20 epochs with the AdamW optimizer, a learning rate of0.001 and 64 batch size. For augmentation, we used horizontal and vertical flipping, color jitter, andauto-contrast. We ran 4 different experiments with different seeds. The results are shown in , using the area under the receiver operating characteristic curve(AUC) as the evaluation metric. For completeness, we have also included the performance of G-CNNsand the current state-of-the-art model FR-UNet . As expected, these results are consistent with thefindings in the paper, with Harmformer slightly underperforming compared to equivariant convolutionarchitectures. Nevertheless, we show that our architecture is versatile and can also be applied tonon-classification tasks.",
  "EDifferences Between 2D and 3D Equivariant Transformers": "While 2D equivariant transformers have been relatively understudied, 3D equivariant trans-formers have received more attention. In this section, we aim to highlightthe key differences that make the 2D case unique, and compare Harmformer with the most closelyrelated SE(3)-Transformer, which operates in 3D but also uses steerable basis representations. An important distinction lies in the nature of the input data, which directly influences the transformerarchitecture. While 2D datasets typically consist of dense pixels with highly correlated neighborhoods,3D equivariant datasets, often represented as graphs or point clouds, tend to be sparse. In 3D,neighboring elements can vary significantly; for example, in molecular graphs , atoms canfulfill entirely different roles within the structure. Patches The properties of the input data determine how to prepare patches in an equivariant manner.In the case of the SE(3)-Transformer, each node of the graph can be directly treated as a patch,eliminating the need for a stem stage. For Harmformer, on the other hand, it is necessary to aggregatelow-level correlated data into a higher-level representation. Additionally, the classical (16 16)ViT grid cannot be used, as discussed in . Therefore, we employ a convolutional stemstage, where the convolution kernels are expressed using circular harmonics to maintain equivariance. This reliance on harmonic representations is a common feature between Harmformer and the SE(3)-Transformer. While Harmformer uses circular harmonics, the SE(3)-Transformer uses sphericalharmonics. Both approaches leverage steerable basis functions , which are widely used inequivariant networks . These steerable bases change predictably under rotation, allowingthe effects of rotation to be effectively neutralizedvia phase shifts in circular harmonics and viathe Wigner-D matrix in spherical harmonics. It is important to note that the use of steerable basespredates both transformers, as shown in . Queries, Keys, and Values In Harmformer, queries (Q), keys (K), and values (V) are generatedindependently from individual patches through a linear layer, we proposed in .2. In contrast,the SE(3)-Transformer creates them by applying convolutions across points (i.e., patches), usingsteerable spheres that aggregate information from the local neighborhood. Attention The SE(3)-Transformer focuses exclusively on invariant attention (type-0) and appliesonly local attention. In contrast, Harmformer explores multiple strategies for mixing attentionand values of various orders (types), while performing global attention across the entire image.Additionally, Harmformer introduces an equivariant layer normalization at the beginning of theattention layer, while the SE(3)-Transformer does not use any layer normalization. Other minordistinctions include Harmformers use of an improved activation function and relative embeddings."
}