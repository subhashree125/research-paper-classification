{
  "Abstract": "Large language models (LLMs) are increasingly being used in materials sci-ence. However, little attention has been given to benchmarking and standard-ized evaluation for LLM-based materials property prediction, which hindersprogress. We present LLM4Mat-Bench, the largest benchmark to date forevaluating the performance of LLMs in predicting the properties of crystallinematerials. LLM4Mat-Bench contains about 1.9M crystal structures in total,collected from 10 publicly available materials data sources, and 45 distinctproperties. LLM4Mat-Bench features different input modalities: crystal com-position, CIF, and crystal text description, with 4.7M, 615.5M, and 3.1B tokensin total for each modality, respectively. We use LLM4Mat-Bench to fine-tunemodels with different sizes, including LLM-Prop and MatBERT, and providezero-shot and few-shot prompts to evaluate the property prediction capabilitiesof LLM-chat-like models, including Llama, Gemma, and Mistral. The resultshighlight the challenges of general-purpose LLMs in materials science and theneed for task-specific predictive models and task-specific instruction-tunedLLMs in materials property prediction 1.",
  "arXiv:2411.00177v3 [cond-mat.mtrl-sci] 30 Nov 2024": "Edwards et al., 2022; Valentini et al., 2023; Castro Nascimento and Pimentel, 2023;Fang et al., 2023; Lv et al., 2024), scientists have recently started to leverage LLMsto tackle very important and challenging problems in materials science, includingpredicting materials properties (Rubungo et al., 2023; Korolev and Protsenko, 2023;Xie et al., 2023; Das et al., 2023; Qu et al., 2024; Choudhary, 2024; Li et al., 2024b)and discovering new materials (Antunes et al., 2023; Gruver et al., 2023; Qu et al.,2024; Choudhary, 2024). The learning capabilities of LLMs have the potential to revolutionize the field ofmaterials science. For example, recent research by Rubungo et al. (2023) hasdemonstrated the exceptional performance of LLMs in predicting the properties ofcrystalline materials based on textual descriptions of their structures. In their study,they introduced a novel dataset, TextEdge, which comprises textual descriptionsof crystals and their corresponding properties. This dataset was used to fine-tunethe encoder component of the T5-small model for the task of materials propertyprediction. The findings of Rubungo et al. (2023) challenge the conventionalpractice of heavily relying on graph neural networks and using solely either crystalcomposition or structure as input for property prediction. Their work underscoresthe significance of further investigating the extent to which LLMs can be harnessed todevelop innovative techniques for accurately predicting the properties of crystallinematerials, thereby enhancing the materials discovery pipeline. Unfortunately, theproposed TextEdge dataset is limited in scope, comprising approximately 145Ksamples and encompassing only three distinct properties. Furthermore, its lack ofdiversity, being derived from a single data source (Materials Project (Jain et al.,2013)), hinders its effectiveness in assessing the robustness of LLMs in materialsproperty prediction. In this work, we introduce LLM4Mat-Bench, a benchmark dataset collected toevaluate the performance of LLMs in predicting the properties of crystalline materials.To the best of our knowledge, LLM4Mat-Bench is the most extensive benchmark todate for assessing the efficacy of language models in materials property prediction.The dataset comprises approximately two million samples, sourced from ten publiclyavailable materials sources, each containing between 10K and 1M structure samples.LLM4Mat-Bench encompasses several tasks, including the prediction of electronic,elastic, and thermodynamic properties based on a materials composition, crystalinformation file (CIF), or textual description of its structure. We use LLM4Mat-Benchto evaluate several LLMs of different sizes, namely LLM-Prop (Rubungo et al., 2023)(35M parameters), MatBERT (Walker et al., 2021) (109.5M parameters), and Llamavariants (Touvron et al., 2023; Dubey et al., 2024) (3B, 7B, and 8B parameters),Mistral variants (Jiang et al., 2023) (7B parameters), and Gemma variants (Teamet al., 2024a,b) (7B and 9B parameters). And we provide fixed train-valid-testsplits, along with carefully designed zero-shot and few-shot prompts to ensurereproducibility. We anticipate that LLM4Mat-Bench will significantly advance theapplication of LLMs in addressing critical challenges in materials science, includingproperty prediction and materials discovery.",
  "Data sources": "hMOF (Wilmer et al., 2012) is a publicly available database2 consisting of about160K Metal-Organic Frameworks (MOFs), generated by Wilmer et al. using com-putational approaches. Materials Project (MP) (Jain et al., 2013) is a databasewith around 150K materials, offering free API access3 for data retrieval, includingCIF files and material properties. The Open Quantum Materials Database (OQMD)(Kirklin et al., 2015) is a publicly accessible database4 of 1.2M materials, containingDFT-calculated thermodynamic and structural properties, created at NorthwesternUniversity. OMDB (Borysov et al., 2017) is an organic materials database withabout 12K structures and related electronic band structure properties, freely avail-able5. JARVIS-DFT (Choudhary et al., 2017, 2018) is a repository created by NISTresearchers, containing around 75.9K material structures with downloadable prop-erties6. QMOF (Rosen et al., 2021, 2022) is a quantum-chemical property databaseof over 16K MOFs, accessible via GitHub7. JARVIS-QETB (Garrity and Choudhary,2023) is a NIST-created database8 of nearly one million materials with tight-bindingparameters for 65 elements. GNoME is a database of 381K new stable materialsdiscovered by Merchant et al. (2023) using graph networks and DFT, availableon GitHub9. Cantor HEA (Li et al., 2024a) is a DFT dataset of formation ener-gies for 84K alloy structures, available on Zenodo10. SNUMAT is a database witharound 10K experimentally synthesized materials and DFT properties, accessiblevia API11.",
  "Collecting crystal information files (CIFs) and materials property": "Crystal structure files (CIFs), material compositions, and material properties werecollected from publicly accessible sources described in .1.1. Data collec-tion was facilitated by APIs and direct download links provided by the respectivedatabases. For databases such as Materials Project, OMDB, SNUMAT, JARVIS-DFT,and JARVIS-QETB, user registration is required for access, while databases likehMOF, QMOF, OQMD, and GNoME allow direct data access without registration.From each source, we obtained CIFs and associated material properties. Although the Materials Project and JARVIS-DFT databases offer a broader range of properties,we selected a subset10 and 20 properties respectivelythat adequately representsthe data within our benchmark, based on the number of data points available foreach property. This selection was made to optimize computational efficiency whentraining models across the 65 properties included in LLM4Mat-Bench.",
  "Generating the textual description of crystal structure": "LLMs perform better with textual input, and Rubungo et al. (2023); Korolev andProtsenko (2023); Qu et al. (2024) have demonstrated that LLMs can effectivelylearn the structural representation of a crystal from its textual description, outper-forming graph neural network (GNN)-based models that directly utilize the crystalstructure for property prediction. Crystal structures are typically described in file formats such as CrystallographicInformation File (CIF) which include predominantly numbers describing lattice vec-tors and atomic coordinates and are less amenable to LLMs. Instead of directly usingthese as inputs, we use Robocrystallographer (Jain et al., 2013) to deterministicallygenerate texts that are more descriptive of crystal structures from CIF files. Robocrys-tallographer was developed and has been used by the Materials Project team toauto-generate texts for their database. Given a structure, Robocrystallographerleverages predefined rules and existing libraries to extract chemical and structuralinformation, including oxidation states, global structural descriptions (symmetryinformation, prototype matching, structural fingerprint calculations etc.), and localstructural descriptions (e.g. bonding and neighbor analysis, connectivity). Thismethod not only generate deterministic and human-readable texts, but also ensuresno data contamination in our fine-tuned LLMs, as the data sources mentioned donot include these crystal text descriptions.",
  "Data Statistics": "As shows, LLM4Mat-Bench comprises 2,697,779 structure files, which, af-ter pairing with descriptions generated by Robocrystallographer and filtering outdescriptions with fewer than five words, result in 1,978,985 composition-structure- description pairs13. The reduction in sample count is also due to Robocrystallog-raphers inability to describe certain CIF files. The total samples for each datasetin LLM4Mat-Bench are randomly split into 80%, 10%, and 10% for training, val-idation, and testing, respectively. OQMD has the highest number of samples at964,403, while QMOF has the fewest with 7,656 samples. On average, each datasetin LLM4Mat-Bench contains approximately 200,000 samples. In LLM4Mat-Bench, when combined, textual descriptions contain 3.1 billion tokens,crystal structures 615 million, and compositions 4.7 million14. OQMD leads incomposition tokens (964K), while hMOF has the most description tokens (581M).For CIFs, both OQMD and hMOF have around 96M tokens. On average, compositionshave 8 subword tokens per sample, CIFs 1600, and descriptions 1700. hMOFaverages the longest inputs for compositions (14.9) and descriptions (5629), whileQMOF leads in structures (5876.4)15. JARVIS-DFT has the most tasks with 20properties, followed by Materials Project with 10, and OMDB with one. Details onsample counts are in .2. LLM4Mat-Bench provides the most comprehensive dataset compared to existingbenchmarks, with the largest number of samples, properties, and tasks, including 60regression and 5 classification tasks (see ). It also offers more diverse materialrepresentations, incorporating chemical formulas, crystal structures, and crystal textdescriptions. In contrast, MatBench (Dunn et al., 2020) and TextEdge (Rubungoet al., 2023) have fewer tasks and less representation diversity, with MatBenchlacking crystal text descriptions and TextEdge missing material compositions andcrystal structures.",
  "Data Quality": "Since Robocrystallographer generates crystal text descriptions in a deterministicmanner following predefined and well-validated rules (Jain et al., 2013), these textsshould faithfully describe the crystal structures used to generate them. Regardingthe quality of labels, they are calculated from simulations and are usually considerednoise-free. Properties data except those from JARVIS-QETB and hMOF are obtainedfrom DFT, which is based on fundamental quantum mechanical equations. WhileDFT calculations can still be performed with different levels of approximations andfidelity, the DFT-calculated properties are usually considered to be highly reliableand are routinely used as noise-free ground truths for ML models in the materialsscience community.",
  "We conducted about 1,235 experiments, evaluating the performance of five modelsand three material representations on each property for each data source. Consistent": "13The total number of pairs were 2,433,688, after removing about 454703 duplicated pairs acrossdatasets, it resulted to 1,978,985 pairs.14We used NLTK toolkit as a tokenizer to count the number of words/tokens.15We used Llama 2 tokenizer to count the number of subword tokens. with standard practices in materials science, we evaluated performance separatelyfor each data source rather than combining samples from different sources for thesame property. This approach accounts for variations in techniques and settings usedby different data sources, which can result in discrepancies, such as differing bandgaps for the same material. Below, we will describe each material representation,model, and metric that we used to conduct our experiments.",
  "Material Representations": "LLM4Mat-Bench includes three distinct materials representations: Composition, CIF,and Description (see ). The primary goal of using these diverse represen-tations is to identify which best enhances LLM performance in predicting materialproperties across different data sources. Composition (Comp.) Material composition refers to the chemical formula of amaterial. Though it only provides stoichiometric information, studies have shown itcan still be a reliable material representation for property prediction (Dunn et al.,2020; Tian et al., 2022). For LLMs, it offers the advantage of being a short sequencethat usually fits within the models context window, making it efficient to train. Tofurther optimize efficiency, we set the longest sequence of material compositionsfrom each data source as the context window, rather than using the default 512tokens for fine-tuning while the original length is kept during inference. CIF We represent the materials structure using CIF files, the conventional way ofrepresenting the crystal structure in crystallography (Hall et al., 1991). CIFs arecommonly used for GNN-based models, but some recent works have demonstratedthat it can also work with LLMs (Antunes et al., 2023; Flam-Shepherd and Aspuru-Guzik, 2023; Gruver et al., 2023).",
  "We benchmarked different LLM-based models with various sizes and types, and aGNN-based baseline. Herein, We provide the details of each model": "CGCNN (Xie and Grossman, 2018) is employed as a GNN baseline which is widelyused in the materials science community16. We trained it on LLM4Mat-Benchfrom scratch with optimal hyperparameters: 128 hidden dimensions, batch size of256, three message passing layers, 1e-2 learning rate, 8.0 radius cutoff, 12 nearestneighbors, and 500 training epochs, though extending to 1000 epochs improvedperformance in some cases.",
  "MatBERT (Walker et al., 2021) is a BERT-base model (Devlin et al., 2018) with109 million parameters, pretrained on two million materials science articles. We": "16Although CGCNN is not state-of-the-art for some properties, it was faster compared to models likeALIGNN (Choudhary and DeCost, 2021) and DeeperGatGNN (Omee et al., 2022), making it suitablefor our extensive experiments fine-tuned MatBERT on LLM4Mat-Bench, following Rubungo et al. (2023), andachieved optimal performance with a 512-token input length, 64-sample batchsize, 5e-5 learning rate, 0.5 dropout, and 100 epochs using the Adam optimizerand onecycle learning rate scheduler (Smith and Topin, 2019). Although trainingfor 200 epochs improves performance, results are reported for 100 epochs due tocomputational constraints. LLM-Prop is a model based on the encoder part of T5-small model (Raffel et al.,2020) introduced by Rubungo et al. (2023), with 35 million parameters, smallerthan MatBERT. It predicts material properties from the textual descriptions of crystalstructures. To adapt LLM-Prop on CIF, we employed xVal encoding (Golkar et al.,2023), where we parse an input sequence x to extract numerical values into a listxnum, replace them with a [NUM] token to form xtext, and then embed xtext, followedby multiplying each [NUM] embedding by its corresponding value in xnum to gethembed that we feed to the model. xVal encoding ensures that the quantitative valueof each number is reflected in the input embedding while reducing the input lengthcaused by the high volume of numerical values in CIF files, which extend the lengthof the input sequence after tokenization. We fine-tuned LLM-Prop on LLM4Mat-Bench and optimizing with a 1e-3 learning rate, 0.2 dropout, Adam optimizer, andonecycle learning rate scheduler for 100 epochs, with a 768-token input length,batch size of 64 for training, and 512 for inference. While Rubungo et al. (2023)recommended that training for 200 epochs and increasing the number of inputtokens improves the performance, we could not replicate this due to computationalconstraints. :Prompt template. <material representation type> denotes chemicalformula\", cif structure\", or structure description\". <value> represents the inputcontext (for example NaCl, etc.). <property name> denotes the name of theproperty (for example band gap, etc.). <predicted value> represents the propertyvalue generated by Llama 2 while <actual value_i> represents the ground truth ofthe EXAMPLE_i. FINAL PROMPT and RESPONSE denote the input prompt to Llama2 and its generated output, respectively.",
  "Llama, Gemma, and Mistral To assess the performance of conversational LLMs in": "materials property prediction, we tested the currently available variants of Llama,Gemma, and Mistral using our designed zero-shot and five-shot prompts (see ) without fine-tuning. For the CIF structure prompts, we removed \"# generatedusing pymatgen\" comment that is appended to each file. The maximum input lengthwas set to the maximum number of tokens each model can handle while the outputlength was set to 256, with a batch size of 256 samples, temperature of 0.8, andtop-K sampling applied with K = 10. The inference details of each model can befound in Appendix C. For five-shot examples, we sampled from crystals with shorterstructures and descriptions to reduce the context length. We also made sure theproperty values for those examples are diverse (for instance, they should not allhave 0.0 eV as their bandgap values). For each model family, we first compared allthe associated models for band gap and stability prediction on the MP test set andfound no significant performance gain from one variant to another (see ).Therefore, for the remaining properties in each dataset, we conducted experimentsusing only one variant from each family as its representative. Additionally, wereported both zero-shot and five-shot performance for Llama, while focusing solelyon five-shot performance for other models due to the significant performance gapobserved between zero-shot and five-shot scenarios. We trained all models using NVIDIA RTX A6000 GPUs. Training MatBERT with twoGPUs on about 300K data points and 100 epochs took about four days while forLLM-Prop took about 2.5 days. For CGCNN, it took about 7 hours training time onone GPU for 500 epochs. With one GPU, Llama 2 took about a half day to generatethe output of 40K samples with 256 tokens maximum length each. We report thetest set results averaged over five runs for predictive models and three runs forgenerative models.",
  "Evaluation Metrics": "Following Choudhary and DeCost (2021), we evaluated regression tasks using theratio between the mean absolute deviation (MAD) of the ground truth and the meanabsolute error (MAE) of the predicted properties. The MAD:MAE ratio ensures anunbiased model comparison between different properties where the higher ratio thebetter. According to Choudhary and DeCost (2021), a good predictive model shouldhave at least 5.0 ratio. MAD values represent the performance of a random guessingmodel predicting the average value for each data point. To provide a comprehensiveperformance comparison across datasets, we also reported the weighted averageof MAD:MAE across all properties in each dataset (Wtd. Avg. (MAD:MAE), seeEquation 1).",
  "and 5, and and 2 show the main results. The detailed results on eachdataset can be found in Appendix E. The main observations are as follows:": "Small, task-specific predictive LLMs exhibit significantly better performancethan larger, generative general-purpose LLMs. This performance disparity isevident across both regression ( and ) and classification tasks () on all 10 datasets. Specifically, LLM-Prop and MatBERT outperform conversa-tional LLMs by a substantial margin, despite being approximately 200 and 64 timessmaller in size, respectively. In regression tasks, LLM-Prop achieves the highestaccuracy on 8 out of 10 datasets, with MatBERT leading on the remaining 2 datasets.For classification tasks, both LLM-Prop and MatBERT deliver the best performanceon 1 out of 2 datasets. LLM-Prop surpasses MatBERT by 1.8% on the SNUMATdataset, whereas MatBERT outperforms LLM-Prop by 0.8% on the other dataset.As anticipated, a modest enhancement in average performance is observed acrossvarious datasets and input formats when the Llama 2-7b-chat model is evaluatedusing 5-shot prompts rather than 0-shot prompts. Determining the optimal num-ber of examples required to achieve peak performance will be the focus of futurework.",
  "Gemma 2-9b-it:5SInval.0.492MatBERT-109M0.7350.730LLM-Prop-35M0.7420.735": "property values. As shown in , , , and Appendix E, Llama,Gemma, and Mistral models produce invalid outputs on multiple tasks, where theexpected property value is missing. This issue occurs less frequently when the inputis a description or chemical formula, but more commonly when the input is a CIFfile. One reason may be that descriptions and chemical formulas resemble naturallanguage, which LLMs can more easily interpret compared to CIF files. This mayalso indicate that when the input modality during inference differs significantlyfrom the modalities encountered during pretraining, fine-tuning is necessary toachieve reasonable performance. Another key observation is that these models oftengenerate the same property value for different inputs (i.e. hallucinate), contribut-ing to their poor performance across multiple tasks. These findings highlight theimportance of caution when using general-purpose generative LLMs for materialsproperty prediction and emphasize the need for fine-tuned, task-specific LLM-basedmodels. Representing materials with their textual descriptions improves the perfor-mance of LLM-based property predictors compared to other representations.We observe a significant performance improvement when the input is a description MatBERTLLM-PropMistral 7b-Instruct-v0.1:5S Llama 2-7b-chat:0SLlama 2-7b-chat:5SGemma 2-9b-it:5S baseline#Avg. subword tokens/Sample 2.5 7.5 12.5 #Avg. subword tokens/Sample 2.5 7.5 12.5 JARVIS-QETB JARVIS-DFT OQMD",
  "Cantor Hea": "hMOF Input Type: Description : The performance comparison across models for each material represen-tation is presented. The left y-axis shows the log-normalized performance of eachLLM-based model relative to the baseline (CGCNN), while the right y-axis (bar plots)displays the average subword tokens per sample for each dataset. Datasets on thex-axis are ordered by increasing average subword tokens. Results for some chat-likemodels are missing in each subplot due to invalid outputs on at least one of theproperty. Higher values in the line plots indicate better performance. Panels (a),(b), and (c) represents the performance comparison where the input is a chemicalcomposition, CIF, and structure description, respectively. compared to when it is a CIF file or a chemical formula. One of the possible reasonsfor this might be that LLMs are more adept at learning from natural language data.On the other hand, although material compositions appear more natural to LLMscompared to CIF files, they lack sufficient structural information. This is likelywhy LLMs with CIF files as input significantly outperform those using chemicalformulas. More advanced, general-purpose generative LLMs do not necessarily yieldbetter results in predicting material properties. In , we compare theperformance of Llama 2-7b-chat-hf model with advanced versions of Llama of compa-rable sizes when predicting materials band gap and its stability. Similar comparisonsare also conducted for the Mistral (Jiang et al., 2023) and Gemma (Team et al.,2024a) models. The results indicate that, despite being trained on substantiallylarger and higher-quality datasets, more advanced versions of generative LLMsshow limited improvements in performance and validity of predictions for materialproperties. For instance, Llama 3 and 3.1 8b models were trained on over 15 trilliontokensaround eight times more data than the 2 trillion tokens used for the Llama2 7b models. This finding highlights the ongoing challenges of leveraging LLMsin material property prediction and underscores the need for further research toharness the potential of these robust models in this domain. The performance on energetic properties is consistently better across alldatasets compared to other properties. This is consistent with the trend observedin the community benchmarks such as MatBench and JARVIS-Leaderboard, where MP JARVIS-DFT GNoME hMOF",
  "SNUMAT": "OMDB 0.5 1.5 LLM-Prop CompositionCIFDescription : The performance comparison across material representations for each LLM-based model is shown. The y-axis represents the log-normalized Weighted Average(MAD:MAE) score for each representation, while the x-axis displays randomlyordered datasets. In the (a)-(d) plots, some Composition and Structure performanceresults are missing due to invalid outputs. A higher y-axis value indicates betterperformance. Panels (a) to (f) represents the results for Llama 2-7b-chat:0S, Llama2-7b-chat:5S, Mistral 7b-Instruct-v0.1:5S, Gemma 2-9b-it:5S, MatBERT, and LLM-Prop, respectively. energetic properties are among those that can be most accurately predicted (Dunnet al., 2020; Choudhary et al., 2024). This is not surprising because energy isknown to be relatively well predicted from e.g., compositions and atom coordina-tion (bonding), which is inherently represented in GNNs and also presented in textdescriptions. Task-specific predictive LLM-based models excel with shorter textual descrip-tions, while CGCNN performs better on datasets with longer descriptions. Whilethe focus on this work is on LLMs, a comparison with a simple but widely used GNN-based baseline suggests room for improvement in LLM-based property prediction.For regression tasks, LLM-Prop outperforms CGCNN on only 4 out of 10 datasets(MP, JARVIS-DFT, JARVIS-QETB, and SNUMAT), and MatBERT outperforms CGCNNon just 2 out of 10 datasets (MP and JARVIS-QETB). In contrast, CGCNN achievesthe best performance on 5 out of 10 datasets (GNoME, hMOF, Cantor HEA, OQMD,and OMDB). Further analysis reveals that CGCNN tends to perform better thanLLM-based models on datasets with relatively longer textual descriptions, whileLLM-based models excel on datasets with shorter descriptions (see ). Theperformance gain on shorter descriptions may stem from LLM-based models abilityto leverage more context from compact text, while CGCNN consistently benefitsfrom training on the entire crystal structure.",
  "Conclusion": "LLMs are increasingly being utilized in materials science, particularly for materialsproperty prediction and discovery. However, the absence of standardized evaluationbenchmarks has impeded progress in this field. We introduced LLM4Mat-Bench,a comprehensive benchmark dataset designed to evaluate LLMs for predictingproperties of atomic and molecular crystals and MOFs. Our results demonstratethe limitations of general-purpose LLMs in this domain and underscore the ne-cessity for task-specific predictive models and instruction-tuned LLMs tailored formaterials property prediction. These findings emphasize the importance of using",
  "Mistral Family": "Llama 2-7b-chat-hfLlama 3-8b-InstructLlama 3.1-8b-InstructLlama 3.2-3b-Instruct Gemma 7b-itGemma 1.1-7b-it.Gemma 2-9b-it Mistral 7b-Instruct-v0.1Mistral 7b-Instruct-v0.2Mistral 7b-Instruct-v0.3 : The performance comparison of different chat-based LLM versions ispresented with results based on 5-shot prompts, averaged over three inference runs.Panels (a)(c) and (d)(f) show each models accuracy in predicting band gaps andstability in the MP dataset, respectively, while panels (g)(i) and (j)(l) depict thepercentage of valid predictions for band gap and stability on the test set.",
  "Limitations": "Due to computational constraints and the number of experiments, we were un-able to conduct thorough hyperparameter searches for each property and dataset.The reported settings were optimized on the MP dataset and then fixed for otherdatasets. For each model, we highlighted hyperparameter settings that may improveperformance (see .1.2). Additionally, we could not include results fromSOTA commercial LLMs such as GPT-4o17 or Claude 3.5 Sonnet18 due to budgetconstraints. We also encountered issues with chat-based models, which sometimes failed tofollow the output format, producing invalid or incomplete outputs. Extractingproperty values was therefore challenging. We believe further instruction-tuningchat-based models on the provided prompts could mitigate these issues. Furthermore, we did not include comparisons with dataset-specific retrieval-augmentedgeneration (RAG) models, such as the recently developed LLaMP (Chiang et al.,2024), a RAG-based model tailored for interaction with the MP dataset. Our workaims to provide a comprehensive benchmark and baseline results to advance theevaluation of LLM-based methods for materials property prediction. Future workshould address these limitations.",
  "Choudhary, K. and DeCost, B. (2021). Atomistic line graph neural network forimproved materials property predictions. npj Computational Materials, 7(1):18": "Choudhary, K., Kalish, I., Beams, R., and Tavazza, F. (2017). High-throughputidentification and characterization of two-dimensional materials using densityfunctional theory. Scientific reports, 7(1):5179. Choudhary, K., Wines, D., Li, K., Garrity, K. F., Gupta, V., Romero, A. H., Krogel, J. T.,Saritas, K., Fuhr, A., Ganesh, P., et al. (2024). Jarvis-leaderboard: a large scalebenchmark of materials design methods. npj Computational Materials, 10(1):93. Choudhary, K., Zhang, Q., Reid, A. C., Chowdhury, S., Van Nguyen, N., Trautt, Z.,Newrock, M. W., Congo, F. Y., and Tavazza, F. (2018). Computational screen-ing of high-performance optoelectronic materials using optb88vdw and tb-mbjformalisms. Scientific data, 5(1):112. Das, K., Goyal, P., Lee, S.-C., Bhattacharjee, S., and Ganguly, N. (2023). Crysmmnet:multimodal representation for crystal property prediction. In Uncertainty inArtificial Intelligence, pages 507517. PMLR.",
  "Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A.,Schelten, A., Yang, A., Fan, A., et al. (2024). The llama 3 herd of models. arXivpreprint arXiv:2407.21783": "Dunn, A., Wang, Q., Ganose, A., Dopp, D., and Jain, A. (2020). Benchmarkingmaterials property prediction methods: the matbench test set and automatminerreference algorithm. npj Computational Materials, 6(1):138. Edwards, C., Lai, T., Ros, K., Honke, G., Cho, K., and Ji, H. (2022). Translationbetween molecules and natural language. In Proceedings of the 2022 Conferenceon Empirical Methods in Natural Language Processing, pages 375413. Fang, Y., Liang, X., Zhang, N., Liu, K., Huang, R., Chen, Z., Fan, X., and Chen,H. (2023). Mol-instructions-a large-scale biomolecular instruction dataset forlarge language models.In The Twelfth International Conference on LearningRepresentations. Flam-Shepherd, D. and Aspuru-Guzik, A. (2023). Language models can generatemolecules, materials, and protein binding sites directly in three dimensions asxyz, cif, and pdb files. arXiv preprint arXiv:2305.05708.",
  "Garrity, K. F. and Choudhary, K. (2023). Fast and accurate prediction of materialproperties with three-body tight-binding model for the periodic table. Physicalreview materials, 7(4):044603": "Golkar, S., Pettee, M., Eickenberg, M., Bietti, A., Cranmer, M., Krawezik, G., Lanusse,F., McCabe, M., Ohana, R., Parker, L. H., et al. (2023). xval: A continuous numberencoding for large language models. In NeurIPS 2023 AI for Science Workshop. Gruver, N., Sriram, A., Madotto, A., Wilson, A. G., Zitnick, C. L., and Ulissi, Z. W.(2023). Fine-tuned language models generate stable inorganic materials as text.In The Twelfth International Conference on Learning Representations.",
  "Hall, S. R., Allen, F. H., and Brown, I. D. (1991). The crystallographic informa-tion file (cif): a new standard archive file for crystallography. Foundations ofCrystallography, 47(6):655685": "Jain, A., Ong, S., Hautier, G., Chen, W., Richards, W., Dacek, S., Cholia, S., Gunter, D.,Skinner, D., Ceder, G., et al. (2013). The materials project: A materials genomeapproach to accelerating materials innovation. apl materials, 1 (1): 011002,2013. Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D.d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et al. (2023). Mistral 7b.arXiv preprint arXiv:2310.06825. Kirklin, S., Saal, J. E., Meredig, B., Thompson, A., Doak, J. W., Aykol, M., Rhl,S., and Wolverton, C. (2015). The open quantum materials database (oqmd):assessing the accuracy of dft formation energies. npj Computational Materials,1(1):115.",
  "Korolev, V. and Protsenko, P. (2023). Accurate, interpretable predictions of materialsproperties within transformer language models. Patterns, 4(10)": "Li, K., Choudhary, K., DeCost, B., Greenwood, M., and Hattrick-Simpers, J. (2024a).Efficient first principles based modeling via machine learning: from simplerepresentations to high entropy materials. Journal of Materials Chemistry A,12(21):1241212422. Li, K., Rubungo, A. N., Lei, X., Persaud, D., Choudhary, K., DeCost, B., Dieng, A. B.,and Hattrick-Simpers, J. (2024b). Probing out-of-distribution generalization inmachine learning for materials. arXiv preprint arXiv:2406.06489. Lin, Z., Akin, H., Rao, R., Hie, B., Zhu, Z., Lu, W., dos Santos Costa, A., Fazel-Zarandi,M., Sercu, T., Candido, S., et al. (2022). Language models of protein sequences atthe scale of evolution enable accurate structure prediction. BioRxiv, 2022:500902. Lv, L., Lin, Z., Li, H., Liu, Y., Cui, J., Chen, C. Y.-C., Yuan, L., and Tian, Y. (2024). Pro-llama: A protein large language model for multi-task protein language processing.arXiv preprint arXiv:2402.16445.",
  "Radford, A., Narasimhan, K., Salimans, T., Sutskever, I., et al. (2018). Improvinglanguage understanding by generative pre-training": "Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li,W., and Liu, P. J. (2020). Exploring the limits of transfer learning with a unifiedtext-to-text transformer. Journal of machine learning research, 21(140):167. Rosen, A. S., Fung, V., Huck, P., ODonnell, C. T., Horton, M. K., Truhlar, D. G., Persson,K. A., Notestein, J. M., and Snurr, R. Q. (2022). High-throughput predictionsof metalorganic framework electronic properties: theoretical challenges, graphneural networks, and data exploration. npj Computational Materials, 8(1):112. Rosen, A. S., Iyer, S. M., Ray, D., Yao, Z., Aspuru-Guzik, A., Gagliardi, L., Notestein,J. M., and Snurr, R. Q. (2021). Machine learning the quantum-chemical prop-erties of metalorganic frameworks for accelerated materials discovery. Matter,4(5):15781597. Rubungo, A. N., Arnold, C., Rand, B. P., and Dieng, A. B. (2023). Llm-prop: Predictingphysical and electronic properties of crystalline solids from their text descriptions.arXiv preprint arXiv:2310.14029.",
  "networks using large learning rates. In Artificial intelligence and machine learningfor multi-domain operations applications, volume 11006, pages 369386. SPIE": "Team, G., Mesnard, T., Hardin, C., Dadashi, R., Bhupatiraju, S., Pathak, S., Sifre, L.,Rivire, M., Kale, M. S., Love, J., et al. (2024a). Gemma: Open models based ongemini research and technology. arXiv preprint arXiv:2403.08295. Team, G., Riviere, M., Pathak, S., Sessa, P. G., Hardin, C., Bhupatiraju, S., Hussenot,L., Mesnard, T., Shahriari, B., Ram, A., et al. (2024b). Gemma 2: Improvingopen language models at a practical size. arXiv preprint arXiv:2408.00118. Tian, S. I. P., Walsh, A., Ren, Z., Li, Q., and Buonassisi, T. (2022). What informationis necessary and sufficient to predict materials properties using machine learning?arXiv preprint arXiv:2206.04968. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N.,Batra, S., Bhargava, P., Bhosale, S., et al. (2023). Llama 2: Open foundation andfine-tuned chat models. arXiv preprint arXiv:2307.09288. Valentini, G., Malchiodi, D., Gliozzo, J., Mesiti, M., Soto-Gomez, M., Cabri, A., Reese,J., Casiraghi, E., and Robinson, P. N. (2023). The promises of large languagemodels for protein design and modeling. Frontiers in Bioinformatics, 3. Walker, N., Trewartha, A., Huo, H., Lee, S., Cruse, K., Dagdelen, J., Dunn, A., Persson,K., Ceder, G., and Jain, A. (2021). The impact of domain-specific pre-training onnamed entity recognition tasks in materials science. Available at SSRN 3950755.",
  "JARVIS-DFTMaterials ProjectSNUMAThMOFGNoMEJARVIS-QETBCantor HEAQMOFOQMDOMDB": "BandgapRegression-145,302--288,209--16,3401,007,32412,500Bandgap (OPT)Regression75,965---------Bandgap (MBJ)Regression19,800---------Bandgap GGARegression--10,481-------Bandgap HSERegression--10,481-------Bandgap GGA OpticalRegression--10,481-------Bandgap HSE OpticalRegression--10,481-------Indirect BandgapRegression-----829,576----Formation Energy Per Atom (FEPA)Regression75,965145,262--384,871829,57684,024-1,008,266-Energy Per Atom (EPA)Regression-145,262---829,57684,024---Decomposition Energy Per Atom (DEPA)Regression----384,871-----Energy Above Hull (Ehull)Regression75,965145,262----84,024---Total EnergyRegression75,965---384,871829,576-16,340--EfermiRegression-145,262--------Exfoliation EnergyRegression812---------Bulk Modulus (Kv)Regression23,823---------Shear Modulus (Gv)Regression23,823---------SLMERegression9,765---------SpillageRegression11,377---------x (OPT)Regression52,158--------- (DFPT)Regression4,704---------Max. piezoelectri c strain coeff (dij)Regression3,347---------Max. piezo. stress coeff (eij)Regression4,797---------Max. EFGRegression11,871---------Avg. meRegression17,643---------Is StableClassification-145,262--------Is Gap DirectClassification-145,262--------n-SeedbeckRegression23,211---------n-PFRegression23,211---------p-SeedbeckRegression23,211---------p-PFRegression23,211---------DensityRegression-145,262--384,871-----Density AtomicRegression-145,262--------VolumeRegression-145,262--384,871-----Volume Per Atom (VPA)Regression------84,024---Is DirectClassification--10,481-------Is Direct HSEClassification--10,481-------SOCClassification--10,481-------LCDRegression---133,524---16,340--PLDRegression---133,524---16,340--Max CO2Regression---133,524------Min CO2Regression---133,524------Void FractionRegression---133,524------Surface Area m2gRegression---133,524------Surface Area m2cm3Regression---133,524------",
  "DPrompt Templates": "INPUT PROMPT<s>[INST] <<SYS>>You are a material scientist. Look at the chemical formula of the given crystalline material and predict its property.The output must be in a json format. For example: {property_name:predicted_property_value}.Answer as precise as possible and in as few words as possible.<</SYS>> chemical formula: NaClproperty name: Band gap. [/INST] RESPONSE{Band gap: 3.97 eV} INPUT PROMPT<s>[INST] <<SYS>>You are a material scientist. Look at the chemical formula of the given crystalline material and predict its property.The output must be in a json format. For example: {property_name:predicted_property_value}.Answer as precise as possible and in as few words as possible.<</SYS>> chemical formula: Na3Bi(P2O7)2property name: Band gap.{Band gap: 0.0 eV} chemical formula: SrCa7Ti2Mn6O23property name: Band gap.{Band gap: 0.0 eV} chemical formula: LiLa4FeO8property name: Band gap.{Band gap: 0.46 eV} chemical formula: CaLaTiMnO6property name: Band gap.{Band gap: 0.24 eV} chemical formula: PmCu2Inproperty name: Band gap.{Band gap: 0.0 eV} chemical formula: NaCl property name: Band gap. [/INST] RESPONSE{Band gap: 3.97 eV} Zero-shot Prompt: 0S Five-shot Prompt: 5S",
  ": Prompt templates when the input is a chemical formula": "INPUT PROMPT <s>[INST] <<SYS>> You are a material scientist. Look at the cif structure information of the given crystalline material and predict its property. The output must be in a json format. For example: {property_name:predicted_property_value}. Answer as precise as possible and in as few words as possible. <</SYS>> cif structure: data_Na3Bi(P2O7)2 _symmetry_space_group_name_H-M 'P 1' _cell_length... property name: Band gap. {Band gap: 0.0 eV} cif structure: data_SrCa7Ti2Mn6O23 _symmetry_space_group_name_H-M 'P 1' _cell_length.. property name: Band gap. {Band gap: 0.0 eV} cif structure: data_LiLa4FeO8 _symmetry_space_group_name_H-M 'P 1' _cell_length... property name: Band gap. {Band gap: 0.46 eV} cif structure: data_CaLaTiMnO6 _symmetry_space_group_name_H-M 'P 1' _cell_length... property name: Band gap. {Band gap: 0.24 eV} cif structure: data_PmInCu2 _symmetry_space_group_name_H-M 'P 1' _cell_length... property name: Band gap. {Band gap: 0.0 eV} cif structure: data_NaCl _symmetry_space_group_name_H - M P 1 _cell_length... property name: Band gap. [/INST] RESPONSE {Band gap: 3.97 eV} Five-shot Prompt: 5S INPUT PROMPT <s>[INST] <<SYS>> You are a material scientist. Look at the cif structure information of the given crystalline material and predict its property. The output must be in a json format. For example: {property_name:predicted_property_value}. Answer as precise as possible and in as few words as possible. <</SYS>> cif structure: data_NaCl _symmetry_space_group_name_H - M P 1 _cell_length... property name: Band gap. [/INST] RESPONSE {Band gap: 3.97 eV} Zero-shot Prompt: 0S",
  ": Prompt templates when the input is a CIF file": "INPUT PROMPT <s>[INST] <<SYS>> You are a material scientist. Look at the structure description of the given crystalline material and predict its property. The output must be in a json format. For example: {property_name:predicted_property_value}. Answer as precise as possible and in as few words as possible. <</SYS>> structure description: Na3Bi(P2O7)2 crystallizes in the triclinic P-1 space group... property name: Band gap. {Band gap: 0.0 eV} structure description: SrCa7Ti2Mn6O23 crystallizes in the triclinic P1 space group... property name: Band gap. {Band gap: 0.0 eV} structure description: LiLa4FeO8 is (La,Ba)CuO4-derived structured and crystallizes in the... property name: Band gap. {Band gap: 0.46 eV} structure description: CaLaTiMnO6 is Orthorhombic Perovskite-derived structured and crysta... property name: Band gap. {Band gap: 0.24 eV} structure description: PmCu2In is Heusler structured and crystallizes in the trigonal R-3m... property name: Band gap. {Band gap: 0.0 eV} structure description: NaCl is Tetraauricupride structured and crystallizes in the cubic P m3m... property name: Band gap. [/INST] RESPONSE {Band gap: 3.97 eV} Five-shot Prompt: 5S INPUT PROMPT <s>[INST] <<SYS>> You are a material scientist. Look at the structure description of the given crystalline material and predict its property. The output must be in a json format. For example: {property_name:predicted_property_value}. Answer as precise as possible and in as few words as possible. <</SYS>> structure description: NaCl is Tetraauricupride structured and crystallizes in the cubic P m3m... property name: Band gap. [/INST] RESPONSE {Band gap: 3.97 eV} Zero-shot Prompt: 0S"
}