{
  "Abstract": "Bayesian optimization is an effective technique for black-box optimization,but its applicability is typically limited to low-dimensional and small-budgetproblems due to the cubic complexity of computing the Gaussian process(GP) surrogate. While various approximate GP models have been employedto scale Bayesian optimization to larger sample sizes, most suffer from overly-smooth estimation and focus primarily on problems that allow for largeonline samples. In this work, we argue that Bayesian optimization algorithmswith sparse GPs can more efficiently allocate their representational powerto relevant regions of the search space. To achieve this, we propose focalizedGP, which leverages a novel variational loss function to achieve strongerlocal prediction, as well as FocalBO, which hierarchically optimizes thefocalized GP acquisition function over progressively smaller search spaces.Experimental results demonstrate that FocalBO can efficiently leverage largeamounts of offline and online data to achieve state-of-the-art performance onrobot morphology design and to control a 585-dimensional musculoskeletalsystem.",
  "Introduction": "Bayesian Optimization (BO) is a powerful approach for solving black-box optimizationproblems, demonstrating notable success in hyperparameter tuning [Snoek et al., 2012],reinforcement learning [Calandra et al., 2016, Chen et al., 2018], and scientific discovery[Gmez-Bombarelli et al., 2018]. The efficacy of BO is attributed to its ability to modelthe unknown objective function using a surrogate model and to strategically select thenext sample position by optimizing an acquisition function. Among the surrogate models,Gaussian Processes (GPs) [Rasmussen, 2003] are usually favored due to their flexibility androbust uncertainty quantification. However, the computation of the posterior GP covariancematrix scales as O(n3) with the number of data points n, which can severely restrict theapplicability of BO in handling large datasets. This poses a significant challenge for real-worldapplications with high-dimensional and heterogeneous function landscapes such as those inrobot control, which often necessitate a substantial amount of data to adequately explorethe vast search space. To extend the scope of BO to accommodate larger datasets (fromlong-horizon online trials and/or pre-collected offline datasets [Trabucco et al., 2022]), it isimperative to employ surrogate models that offer enhanced computational efficiency.",
  "arXiv:2412.20375v1 [cs.LG] 29 Dec 2024": "Using sparse GP models is a popular method for reducing the computational cost of BO.Sparse GPs accomplish this by learning an approximation of the full GP, either by using asubset of data [Lawrence et al., 2002], ensemble of local models [Snelson and Ghahramani,2005], or variational inference [Titsias, 2009]. However, classical sparse GP models aretypically tailored for regression tasks, and therefore are designed to fit to the entire functionlandscape. Given limited representational resources, the resulting posterior is likely to beoverly smooth, which may negatively impact the performance of BO. This issue is exacerbatedin the high-dimensional setting, in which accurately fitting the entire domain is a far morechallenging task. As such, several works have proposed strategies to improve BO performancewith sparse GP models by focusing promising regions [McIntire et al., 2016, Moss et al., 2023]or advanced sparse GP models [Jimenez and Katzfuss, 2023]. However, most of their empiricalevaluations are only conducted under large online sample setting in low-dimensional problemswith fewer than 20 variables. It is unclear whether existing methods can be generalized tolarge offline data or high-dimensional setting. In this work, we explore the application of sparse Gaussian processes for optimizing high-dimensional problems with large offline (and optionally large online) datasets. We arguethat by iteratively identifying key sub-regions of the input space and focusing the modelingcapacity on these areas, we can enhance the modeling fidelity of the sparse GP in regions thatare most relevant, thereby improving the overall performance of the Bayesian optimizationalgorithm. To this end, we propose a novel loss function to train a variational sparse GPmodel (focalized GP) that emphasizes the fitting of local functional landscapes throughweighting the training data. Along with focalized GP, we design a hierachical algorithm,FocalBO, to propose sample points via acquisition function optimization across varying scalesof the search space. Experimental results demonstrate that FocalBO can improve uponcommonly used acquisition functions in optimizing heterogeneous functions and can effectivelyutilize large offline datasets for efficient high-dimensional optimization. Furthermore, weshowcase that FocalBO can efficiently optimize a policy with 585 parameters to control amusculoskeletal system, leveraging both offline and online data. To the best of our knowledge,FocalBO is the first sparse GP-based Bayesian optimization algorithm capable of efficientlyoptimizing high-dimensional problems under both large online sample and large offline datasettings.",
  "Our main contributions:": "1) We design FocalBO, which employs a hierarchical acquisition optimization strategy toachieve efficient optimization over high-dimensional problems with heterogeneous structurewith limited representation capability. 2) Experimental results demonstrate the superiorperformance of FocalBO in leveraging large offline datasets for online optimization, and itscapability to optimize high-dimensional musculoskeletal system control problems involvingover 500 variables.",
  "Scaling Gaussian processes to large datasets is an important topic [Liu et al., 2020]. It canbe broadly divided into global approximation strategies and local approximation strategies": "Global sparse GPs perform distillation over the whole dataset to approximate the expensivefull covariance matrix with a sparse representation. Several methods aim to choose a subset ofrepresentative training points from the whole dataset, and use the corresponding covariancematrix in place of the full covariance [Hayashi et al., 2020, Lawrence et al., 2002, Seeger,2002, Keerthi and Chu, 2005]. Sparse kernels aim at removing uncorrelated entries in thefull covariance to obtain a compact matrix [Gneiting, 2002, Melkumyan and Ramos, 2009,Buhmann, 2001, Wendland, 2004]. Sparse approximation methods use inducing variables tolearn a low-rank representation of full covariance matrix [Quinonero-Candela and Rasmussen,2005, Smola and Bartlett, 2000, Seeger et al., 2003, Csat and Opper, 2002, Snelson andGhahramani, 2005, Titsias, 2009, Hensman et al., 2013, Csat and Opper, 2000, Wilsonand Nickisch, 2015]. Stochastic variational Gaussian process (SVGP) is a popular sparseGP method which employs variational inference to learn inducing variables and kernel hyperparameters jointly and enable training using stochastic gradient descent from mini-batch data [Hensman et al., 2013]. Recently, nearest neighbor information has also beenused to further improve the scalability of sparse GP over massive amount of data [Wu et al.,2022, Tran et al., 2021]. In contrast, local sparse GPs divide the entire dataset and employ local GPs trained fromdifferent data subsets to approximate the full GP. For a given test set, the prediction can beextracted from one of the local GPs [Kim et al., 2005, Datta et al., 2016], mixture of GPs[Yuksel et al., 2012, Masoudnia and Ebrahimpour, 2014] or product of GPs [Hinton, 2002,Cohen et al., 2020].",
  "Scalable Bayesian optimization": "Recent works have proposed modifications to sparse GPs for Bayesian optimization. SparseGP has been used to determine the search region where local GPs are used to determinethe next samples [Krityakierne and Ginsbourger, 2015]. Weighted-update online Gaussianprocesses (WOGP) was developed to select a subset of training points to approximate highperforming regions of the input space [McIntire et al., 2016]. IMP-DPP is motivated by asimilar observation and uses a weighted Determinantal Point Process to select training pointsas inducing variables for the SVGP [Moss et al., 2023]. However, their proposed selectionstrategies require sequentially evaluating every training point, which can be computationallyvery expensive with large offline datasets. Combining SVGP with Thompson samplinghas the same order of regret as standard Thompson sampling method [Vakili et al., 2021].Online variational conditioning (OVC) was proposed to efficiently conditioning SVGPs in anonline setting, enabling using look-ahead acquisition functions [Maddox et al., 2021]. Vecchiaapproximation of GP was also applied [Katzfuss et al., 2020] for Bayesian optimization, withimproved performance compared to prior works [Jimenez and Katzfuss, 2023]. A concurrentwork [Maus et al., 2024] aims at improving the acquisition optimization performance basedon target-aware Bayesian inference [Rainforth et al., 2020]. Besides sparse GPs, Neural network [Snoek et al., 2015, Shangguan et al., 2021] and randomforest [Lakshminarayanan et al., 2016] can also be used as BO surrogate model to circumventthe cubic complexity of GP. Ensemble Bayesian optimization utilizes the addictive functionstructure and uses ensembles of addictive GPs in parallel to achieve scalability [Wang et al.,2018]. Trust Region Bayesian optimization (TuRBO) and its variants uses exact GP tooptimize over local regions, and employs a restart mechanism to achieve large number ofevaluation, which is a representative line of works in high-dimensional Bayesian optimization[Eriksson et al., 2019, Wang et al., 2020, Eriksson and Poloczek, 2021]. TuRBO can alsobe combined with sparse GP models to further enhance the scalability [Maus et al., 2022,Tautvaias and ilinskas, 2024].",
  "Bayesian optimization": "For an unknown objective function f, Bayesian optimization aims to solve maxxX f(x)over input space X d. BO mainly consists of two components: a surrogate model toapproximate the objective function, and an acquisition function a to decide the next sampleposition based on surrogate model. Gaussian process is a commonly used surrogate model. Consider a given dataset D =(X, y) where X = (x1, ..., xt) are input locations and y = (y1, ..., yt) are associated noisyobservations of f(X). We assume the observation noise to be independent Gaussian, i.e.yi = f(xi) + , N(0, 2). Using GP with kernel function K, the function distributionf at test positions X = (x,1, . . . , x,t)T is a multivariate Gaussian:",
  "where K is the covariance matrix between subscript inputs. With the posterior distributiongiven D, the next sample point is the maximum position of the acquisition function: xt+1 =": "maxxX a(x|Mt), where Mt is the GP model fitted on dataset collected at time step t.Common-used choice of a includes upper confidence bound(UCB, [Srinivas et al., 2009]),expected improvement (EI, [Jones et al., 1998]) and Thompson sampling (TS, [Kandasamyet al., 2018]). The inner optimization problem is usually solved by grid search, evolutionaryalgorithms [Hansen, 2006], or gradient-based methods [Balandat et al., 2020]. When theonline sample budget is large, batch optimization is commonly used to evaluate multipleinputs in parallel [Gonzlez et al., 2016].",
  "Variational Gaussian process": "For predictive distribution conditioned on given dataset of size t, the computational complexityof exact Gaussian process is O(t3) for each test position due to the inverse of the covariancematrix KXX, which is expensive for large scale datasets with more than a few thousandpoints. A common used strategy is to approximate full GP regression using sparse GPs. Insparse GP, m t inducing variables u = (u1, . . . , um)T characterized by inducing inputsZ = (z1, . . . , zm) are introduced to approximate the covariance matrix of the full GP. Inthis section, we focus on sparse GP derived from variational inference.",
  "),(2)": "where f = (f(x1), . . . , f(xt))T . A variational distribution q(u) = N(u | m, S) is used toapproximate the posterior over inducing variables using the exact conditional distributionof f given u, that is, q(f, u) = p(f | u)q(u). The posterior of f can be computed bymarginalizing u with analytic form:",
  ": Performance comparison of focalizedGP and SVGP over 1d GP functions. Posteri-ors are shown as mean 1 standard deviation": "Prior studies about variational sparse GPsare mainly designed for regression tasks,where the goal is to fit global training datadistribution. In Bayesian optimization, thenext sample is determined by the predic-tive function distribution over test positions.Gradient-based and evolutionary-based ac-quisition function optimization methods em-ploy local search from random startingpoints to find a local optimal of the acqui-sition function. Recent works also scale gridsearch-based optimization to high dimen-sional space by restricting the search spacewithin local sub-regions [Eriksson et al.,2019, Wang et al., 2020].All the above procedure would benefit from an accurateestimation over sub-region of the input space.Therefore, a sensible way to improve BO performance is to allocate limited computationalresources to obtain better prediction over specific search regions instead of the entire inputdomain.",
  "2l}.(5)": "When l = (1, , 1)T and c = (0.5, , 0.5)T , the acquisition optimization is performed overthe entire input space X, as commonly-used in vanilla BO algorithms. In the rest of thissection, we first present the derivation of focalized loss function to improve GP predictionover the search region. Then we demonstrate how to incorporate our proposed GP modelinto Bayesian optimization.",
  "(6)": "where kij is the (i, j)-th entry of [KXX + I]1. From eq. 6 we can observe that the meanestimation at x is a linear combination of observation y multiplied by k(x, x), and thereduction of variance is a quadratic form of the covariance between x and training points.Both estimation can be written as linear summations of constant values with kernel functionas weight. As mentioned in prior works [Gramacy and Apley, 2015], data points far fromthe test positions have a vanishingly small influence on the predictive distribution withcommonly used kernel functions. Utilizing this observation, we propose to weight the datalikelihood term using the kernel function to focus training over points that contribute to theprediction of the search region:",
  "i=1wiEq(f(xi))[log p(yi | f(xi))],wi = maxxSc,l k(xi, x).(7)": "We use the maximum covariance of xi to positions in the search region as the correspondingweight to filter out points that have marginally influence to the search region during GPtraining. In this way, the model can selectively utilize the training data to achieve good localprediction. When using a popular kernel functions such as RBF or Matern kernel, the maximum kernelvalue is equivalent to finding the nearest point in the search region, which can be easilycalculated when the region boundary is axis-aligned as defined in eq. 5.",
  "L2 = LWLL + LKL Lreg.(9)": "Compared to the original ELBO loss in SVGP, our proposed function maintains the samecomputational complexity and does not introduce additional hyperparameters. Our ELBOalso reproduces eq. 4 when considering to predict the entire input space X. During the modeltraining, both GP hyperparameters and variational parameters are jointly optimized to obtainfocalized GP for Bayesian optimization. shows a comparison of focalized GP andSVGP over 1d functions sampled from GP. While SVGP can only able to vaguely predictthe function, focalized GP accurately delineate the function landscape within search regionby training with the focalized loss. Our proposed GP model is sensitive to high-performingpositions within the search space which contribute to better acquisition optimization. Wealso systematically compare the GP prediction performance in Appendix B.3, where our GPmodel trained from focalized ELBO consistently achieves good prediction on small size ofsearch space.",
  ": end for": "TheoreticalimplicationsoffocalizedELBO.Our focalized ELBO can be inter-preted as a soft variant of training a local ap-proximation over datapoints that lie within thesearch region. Here, we illustrate how local ap-proximations can substantially reduce the KLdivergence of the approximate posterior over thesearch region, and discuss the effects of tighterapproximations on BO regret bounds. We focuson providing general theoretical intuition ratherthan deriving precise bounds due to the lackof existing convergence guarantees for ELBOmaximization in the general setting. Suppose that we know the optimal point liesin some small sub-region of X that containsN << N training points. Corollary 19 in [Burtet al., 2020] shows that given a squared expo-nential kernel and some assumptions on theinducing point selection, for a fixed number ofinducing points the KL-divergence upper boundscales super-quadratically in the number of train-ing points. Hence, fitting locally can yield muchtighter approximations than fitting globally (e.g. SVGP). Next, we consider the impact of the KL approximation error on the optimization regret.Proposition 1 in [Burt et al., 2020] states that the gap between the means of the approximateand exact posteriors is upper bounded by O(), where is an upper-bound on theapproximation KL-divergence. This has an immediate impact on the regret - for example,when GP-UCB [Srinivas et al., 2009] is combined with sparse GPs, the confidence boundsmust be enlargened by an additive factor to account for the approximation error. Becausethe regret bound scales with T where T is the maximum confidence interval coefficient,having a large approximation error can arbitrarily scale the regret incurred by the algorithm.In order to achieve no additional regret order, the additional approximation error noise mustbe uniformly bounded (Assumption 4 in [Vakili et al., 2021]). Although focalized GP cannotguarantee a constant bound, it still directly reduces the regret of the algorithm, where weempirically investigate in Appendix B.1.",
  "Bayesian optimization with focalized GP": "One advantage of focalized GP is that it can be easily integrated into existing BO algorithms.To further leverage the strong local modeling properties of focalized GP, we design FocalBO,a hierachical acquisition optimization framework described in Algorithm 1. At each BO iteration, FocalBO iteratively optimizes the acquisition function over a pro-gressively smaller search region via focalized acquisition function (FocalAcq) as shown inAlgorithm 2. The first depth of acquisition optimization starts with the entire input space X with l = (1, , 1)T and c = (0.5, , 0.5)T (line 1). We train specific focalized GP baseon the search region at each round of acquisition optimization (line 4-5). Our framework iscompatible with any acquisition function that extracts instant posterior information from theGP and is optimized within pre-defined search region. After one round of acquisition functionoptimization, the search space length l is halved to focus on a smaller search region centeredat current best position xbest(line 6-7). In this way we can obtain a more accurate model fordecision making, and also relieve the over-exploration problem when the problem dimensionis high [Oh et al., 2018]. One batch of inputs is proposed at each round of optimization, andthe final decision is sampled from all proposed inputs via Softmax distribution over theircorresponding acquisition function values (line 9). Our hierarchical optimization strategyenables collecting candidates from both global sparse estimation and local focalized predic-tion, achieving balance between exploration and exploitation with constrained computationpower.",
  "j=1 expa(xht,j |Mht )": "The optimization depth H in FocalAcq con-trols the degree of utilizing local informationfrom current best position, where the GP es-timate variance decreases with the shrinkageof search space. The best-performing opti-mization depth is likely problem-dependent(e.g. high-dimensional functions may requirehigher optimization depths). Therefore inFocalBO, we propose to automatically ad-just the optimization depth according to theinstant optimization performance. At the be-ginning of the optimization, we initialize theoptimization depth as 1, indicating globalsearch of the input space (Algorithm 1, line1). Then we keep track of the depth wherethe proposed positions are sampled from. Ifthe depth of the best point in this roundis less than the current optimization depthH, we reduce H to encourage explorationof the input space, otherwise we increase Hfor better exploitation of xbest (line 6-10). Our proposed framework is orthogonal to TuRBO-M [Eriksson et al., 2019], but bearssome similarities in searching over multiple sub-regions and adaptively adjusting the searchregion. Our algorithm differs in that TuRBO-M constructs equal-sized trust regions andfits independent Exact GP using separated dataset, aiming at searching for different localoptima in the search space. By contrast, the search region in FocalBO is constructed withdifferent sizes to make decision based on both global and local information. Our frameworkallows data sharing across search regions, and the use of focalized GP helps to accuratelyestimate local region with limited representation. Additionally, FocalBO does not introduceextra hyperparameters. Finally, we demonstrate in that TuRBO is complementaryto FocalBO in optimizing high-dimensional problems.",
  "Experiments": "In this section, we extensively evaluate FocalBO over a variety of tasks. We first use syntheticfunctions to showcase the compatibility of FocalBO in improving commonly-used acquisitionfunctions. Next, we consider the online optimization of robot morphology design that isadditionally given a large offline dataset. We also show that FocalBO is able to optimizevery high-dimensional musculoskeletal system control with both a large offline dataset and alarge number of online budget. Finally we dig deeper into FocalBO to analyze how each ofits components contributes to superior optimization performance. We compare FocalBO with representative sparse GP models used for Bayesian optimization,including SVGP [Hensman et al., 2013], WOGP [McIntire et al., 2016], and Vecchia GP[Jimenez and Katzfuss, 2023]. We only run WOGP on synthetic functions due to its extremely : Optimization performance under different synthetic function and acquisitionfunction. Sparse GP models are trained with 50 inducing variables. The offline datasetcontains 2000 random data points and the online budget is 500 with batch size of 10. low speed in dealing with the datasets in the remaining tasks. The number of inducingvariables in sparse GP models is set as 50 for synthetic functions and as 200 for other tasks.The optimization performances are shown as mean 1 standard error for all consideredproblems over 10 independent trials.",
  "Synthetic functions": "We select Shekel and Michalewicz as the test functions, which are heterogeneous with bothsmooth and rigid regions. We also sample functions directly from Gaussian processes toevaluate algorithm performance under full BO assumption. For each function, we choose touse different acquisition functions to optimize: TS optimized by grid search, EI optimized byanalytic gradient, and probability of improvement (PI) optimized by Monte Carlo gradient[Balandat et al., 2020]. Optimization performances are shown in . We observethat FocalBO significantly improves the performance of all acquisition functions comparedto SVGP, and is able to consistently achieve top-tier performance over all problems. InMichalewicz function where a large fraction of the input space is flat, all baselines tendto increase the noise estimation to maintain a stationary prediction, while focalized GP isable to focus on the local search region and successfully optimize the function. Additionalexperiment with online samples as major data source is shown in Appendix B.2, whereFocalBO still maintains comparable or better performance against baselines.",
  ": Optimization on robot morphologydesign. Function values are normalized by bestand worst values in the unseen full dataset": "We compare FocalBO to several baselinesover robot morphology design task fromDesign-Bench, which provides large offlinedataset with an exact function oracle [Tra-bucco et al., 2022]. The goal of the task isto optimize the morphological structure ofDKitty robot [Ahn et al., 2020] to improvethe simulation performance under RL con-troller. While the benchmark is initially de-signed for offline model-based optimization(MBO), it can also be used as an offline-to-online BO benchmark. In this task, we usethe training dataset with 10,000 points andadditionally evaluate 128 points on-the-flywith batch size of 4. EI is used as the baseacquisition function for better optimizingwith small batch size. We also try to com-bine FocalBO with TuRBO to optimize over the high-dimensional space, with the resultsshown in . We observe that FocalBO achieves significant improvement from theinitial data while other baselines struggle to obtain performance gain, even combined with TuRBO. FocalBO with TuRBO effectively extracts information from large offline datasetand is the first GP-based method to achieve top-tier performance reported by prior MBOworks [Trabucco et al., 2021].",
  "Human musculoskeletal system control": "We further apply FocalBO to control a human arm musculoskeletal system [He et al., 2023] forthe task of pouring liquid into a cup, as shown in (a). To control the musculoskeletalsystem, we optimize a linear policy |A| |O|, where |A| = 5 and |O| = 117 are thecorresponding action and observation dimensions. The action dimension has been reducedfrom individual muscles to synergetic groups of muscles by applying principled componentanalysis to sampled action data from an RL agent (Appendix A.6). Although the originalcontrol dimension is reduced, the remaining 585-dimensional input space is still very high forexisting high-dimensional BO algorithms. Therefore we consider a large offline-online setting,where we randomly sample 2000 points from the input space to serve as the offline dataset, andset the online budget as 3000 with batch size of 100. We use Thompson sampling as the baseacquisition function. (b) demonstrates that FocalBO outperforms other baselines,achieving higher maximum reward and faster convergence speed. Our supplementary videoshows that the optimized policy is able to perform well on the task, demonstrating thesuccessful application of FocalBO to high-dimensional control problems.",
  "Algorithm analysis": "To understand the reasons behind FocalBOs superior optimization performance, we investi-gate the optimization depth in FocalBO, which is the central component of the method. (a) shows the evolution of optimization depth over different problems, where FocalBO isable to adapt the optimization depth according to different function structure. For Shekeland musculoskeletal model control where the promising regions are distinct, the optimizationexhibits an increasing trend to exploit current best points, while for other problems thedepth tends to converge at a fixed level. (b) shows the sources of proposed batchesduring the optimization of musculoskeletal system control. Overall the samples exhibits cleartrend from exploration to exploitation over high-dimensional input space. Our hierarchicaloptimization strategy enables flexibility between exploration and exploitation.",
  "Conclusion": "In this paper, we propose FocalBO, which uses a hierarchical acquisition optimization strategyequipped with focalized GP model to scale Bayesian optimization to problems with large offlinedatasets and/or a large number of online samples. Despite limited representation capability,FocalBO consistently improves various acquisition functions in optimizing heterogeneousfunctions, and adeptly leverages large offline dataset for efficient optimization over robotmorphology. Under the large offline-to-online optimization setting, FocalBO achieves stablehigh-dimensional control of human musculoskeletal model with over 500 parameters. Ablationstudies over the algorithm components further verify the principled design of FocalBO. Futurework may include theoretically analyzing FocalBO, and applying the method to more complex : Algorithm analysis over optimization depth. (a) Depth evolution during optimiza-tion. (b) Samples source of each BO iteration during one trial of musculoskeletal systemcontrol optimization. Color bar indicates the number of samples proposed by correspondingoptimization depth.",
  "problems, such as large-scale parameter tuning and whole-body human musculoskeletal systemcontrol": "Michael Ahn, Henry Zhu, Kristian Hartikainen, Hugo Ponte, Abhishek Gupta, Sergey Levine,and Vikash Kumar. Robel: Robotics benchmarks for learning with low-cost robots. InConference on robot learning, pages 13001313. PMLR, 2020. Maximilian Balandat, Brian Karrer, Daniel R. Jiang, Samuel Daulton, Benjamin Letham,Andrew Gordon Wilson, and Eytan Bakshy. BoTorch: A Framework for Efficient Monte-Carlo Bayesian Optimization. In Advances in Neural Information Processing Systems 33,2020. URL",
  "Nikolaus Hansen.The cma evolution strategy: a comparing review.Towards a newevolutionary computation: Advances in the estimation of distribution algorithms, pages75102, 2006": "Kohei Hayashi, Masaaki Imaizumi, and Yuichi Yoshida. On random subsampling of gaussianprocess regression: A graphon-based analysis. In International Conference on ArtificialIntelligence and Statistics, pages 20552065. PMLR, 2020. Kaibo He, Chenhui Zuo, Jing Shao, and Yanan Sui. Self model for embodied intelligence:Modeling full-body human musculoskeletal system and locomotion control with hierarchicallow-dimensional representation. arXiv preprint arXiv:2312.05473, 2023.",
  "Mitchell McIntire, Daniel Ratner, and Stefano Ermon. Sparse gaussian processes for bayesianoptimization. In UAI, volume 3, page 4, 2016": "Arman Melkumyan and Fabio Tozeto Ramos. A sparse covariance function for exact gaussianprocess inference in large datasets. In Twenty-first international joint conference onartificial intelligence, 2009. Henry B Moss, Sebastian W Ober, and Victor Picheny. Inducing point allocation for sparsegaussian processes in high-throughput bayesian optimisation. In International Conferenceon Artificial Intelligence and Statistics, pages 52135230. PMLR, 2023.",
  "Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-basedcontrol. In 2012 IEEE/RSJ international conference on intelligent robots and systems,pages 50265033. IEEE, 2012": "Brandon Trabucco, Aviral Kumar, Xinyang Geng, and Sergey Levine. Conservative objectivemodels for effective offline model-based optimization. In International Conference onMachine Learning, pages 1035810368. PMLR, 2021. Brandon Trabucco, Xinyang Geng, Aviral Kumar, and Sergey Levine. Design-bench: Bench-marks for data-driven offline model-based optimization. In International Conference onMachine Learning, pages 2165821676. PMLR, 2022. Gia-Lac Tran, Dimitrios Milios, Pietro Michiardi, and Maurizio Filippone. Sparse withinsparse gaussian processes using neighbor information. In International Conference onMachine Learning, pages 1036910378. PMLR, 2021. Sattar Vakili, Henry Moss, Artem Artemev, Vincent Dutordoir, and Victor Picheny. Scalablethompson sampling using sparse gaussian process models. Advances in neural informationprocessing systems, 34:56315643, 2021. Linnan Wang, Rodrigo Fonseca, and Yuandong Tian. Learning search space partition forblack-box optimization using monte carlo tree search. Advances in Neural InformationProcessing Systems, 33:1951119522, 2020. Zi Wang, Clement Gehring, Pushmeet Kohli, and Stefanie Jegelka. Batched large-scalebayesian optimization in high-dimensional spaces. In International Conference on ArtificialIntelligence and Statistics, pages 745754. PMLR, 2018.",
  "A.1Implementation of FocalBO": "We implement FocalBO with BoTorch1, which is a popular library for BO implementationwith GPU acceleration. For acquisition optimization, we directly use acquisition functionimplementation and corresponding optimizers from BoTorch. Our code for fully reproducingall experimental results is in the: Our muscu-loskeletal model will be released soon. In the meantime, the model can be accessed forresearch purposes upon request ().",
  "SVGP. We directly use approximated GP class in Gpytorch example2": "WOGP. We refer to the original implementation3, and write a Botorch GP wrapperwith inducing point kernel to enable acquisition optimization using BoTorch.As thehyperparameter are unknown to the GP model, we first warm up WOGP using randomset of inducing points for 100 epochs, then perform weighted training point selection andcontinue hyperparameter fitting with the selected WOGP model.",
  "For all GP, we use Matern 5": "2 kernel with automatic relevance determination, and do not restrictthe lengthscale or noise range. For each round of GP training, we fit GP hyperparameters(and variational parameters for focalized GP and SVGP) for 1000 epochs via Adam optimizer[Kingma and Ba, 2014] with learning rate as 0.01. For focalized GP and SVGP, we initializethe inducing points using Sobol sampler [Sobol, 1967] over input space. all experiment areconducted on a server with Intel(R) Xeon(R) Gold 6348 CPU @ 2.60GHz, NVIDIA-A100and 512Gb memory.",
  "r = 50rpos 10rori + 10rreach + rlift ract 5rdone(10)": "where rpos encourages the bottle near the target position, rori encourages the bottle near thetarget orientation, rreach encourages the hand to grab the bottle, rlift encourages the handto lift the bottle, ract penalize the overall muscle activation, rdone penalize the early endedepisode due to dropped bottle or hand outside of pre-defined range. We trained a Soft Actor-Critic (SAV) [Haarnoja et al., 2018] agent for 6M timesteps tocollect task-related muscle activation data, and use principled component analysis to reducethe action dimension from 81 to 5.",
  "B.1Theoretical implications of sparse GP approximation": "In , we also empirically measure our claim that Focalized GP can significantlyreduce approximation error on the search region. We sampled 8000 training points from2d GP functions to train focalized GP and SVGP. Over different size of the search region,we compare the KL divergence of the GP posterior prediction over search region betweensparse GPs and the exact GP. We observe that the KL divergence between focalized GP andexact GP is consistently smaller than that between SVGP and exact GP, implying tighterapproximation to the exact GP over local region.",
  ": KL divergence between sparse GPs and exact GP. Results shows the mean andone standard error, averaged over 50 independent trials": "While a rigorous regret bound is hard to derive, we conduct an empirical study wherewe directly compare the optimization performance between focalized GP and SVGP whencombining with TuRBO. In this way we can eliminate the influence of hierarchical acquisitionoptimization. The optimization performances are shown in . We observe thatfocalized GP outperforms SVGP on both high-dimensional problems, which empiricallydemonstrates our theoretical implications that Focalized GP contributes to reducing regret.",
  "kernel andlengthscale of 0.05 (representing rigid functions), and selected the best point over unifromlysampled 10,000 points as the global optima": "A sparse GP is already more explorative than using the full GP, since the smaller representa-tional capacity leads to smoother posteriors. In (b), We demonstrate this empiricallybelow, where we measure the pair-wise distance of 100 Thompson sampling points underexact and SVGP (with 50 inducing points). We observe that sparse GP actually samplesmore diverse sets compared to exact GPs, i.e. exhibiting more exploration. Therefore,",
  "B.3GP predictive performance": "We use two common-used synthetic functions, Ackley and Rastrigin, to analyze the the GPpredictive performance of focalized GP compared with Exact GP and SVGP under differentsearch region size l and different inducing variables number m. We show the negative loglikelihood (NLL) and root mean squared error (RMSE) in . The results showsthat focalized GP outperforms both Exact GP and SVGP in terms of both NLL and MSEwhen the search space size is lower than 0.5. In Rastrigin function where Exact GP achievessimilar performance as SVGP, focalized GP is still able to accurately predict the local searchregion over different choice of inducing variable numbers. We also show in thatthe regularization term Lreg is indispensable to the training of focalized GP to achieve goodlocal prediction.",
  "B.4Comparison with TuRBO": "We run the original TuRBO implementation (with exact GP and Thompson sampling) andTuRBO with nearest neighbor GO model on both robot morphology design and humanmusculoskeletal system control task (). We observed that FocalBO outperformsTuRBO on both tasks with smaller computational cost. The reason of TuRBOs poorperformance may be that it cannot quickly adapt over the search space when the onlineevaluation budget is small.",
  "CLemmas Used for Theoretical Implications of Focalized ELBO": "Lemma 1. (Corollary 19 in [Burt et al., 2020]). Let k be a squared exponential kernel.Suppose that N real-valued (onedimensional) covariates are observed, with identical Gaussianmarginal distributions. Suppose the conditions of Theorem 13 are satisfied for some R > 0.Fix any (0, 1]. Then there exists an M = O(log(N 3/)) and an = (/N 2) such ifinducing points are distributed according to an -approximate M-DPP with kernel matrixKff,",
  "NeurIPS Paper Checklist": "The checklist is designed to encourage best practices for responsible machine learning research,addressing issues of reproducibility, transparency, research ethics, and societal impact. Do notremove the checklist: The papers not including the checklist will be desk rejected.The checklist should follow the references and precede the (optional) supplemental material.The checklist does NOT count towards the page limit.",
  "Please provide a short (12 sentence) justification right after your answer (even forNA)": "The checklist answers are an integral part of your paper submission. They arevisible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will beasked to also include it (after eventual revisions) with the final version of your paper, and itsfinal version will be published with the paper. The reviewers of your paper will be asked to use the checklist as one of the factors in theirevaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable toanswer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reportedbecause it would be too computationally expensive\" or \"we were unable to find the license forthe dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection.While the questions are phrased in a binary way, we acknowledge that the true answeris often more nuanced, so please just use your best judgment and write a justification toelaborate. All supporting evidence can appear either in the main paper or the supplementalmaterial, provided in appendix. If you answer [Yes] to a question, in the justification pleasepoint to the section(s) where related material for the question can be found.",
  "The authors are encouraged to create a separate \"Limitations\" section in theirpaper": "The paper should point out any strong assumptions and how robust the resultsare to violations of these assumptions (e.g., independence assumptions, noiselesssettings, model well-specification, asymptotic approximations only holdinglocally). The authors should reflect on how these assumptions might be violatedin practice and what the implications would be. The authors should reflect on the scope of the claims made, e.g., if the approachwas only tested on a few datasets or with a few runs. In general, empiricalresults often depend on implicit assumptions, which should be articulated. The authors should reflect on the factors that influence the performance of theapproach. For example, a facial recognition algorithm may perform poorly whenimage resolution is low or images are taken in low lighting. Or a speech-to-textsystem might not be used reliably to provide closed captions for online lecturesbecause it fails to handle technical jargon.",
  "If applicable, the authors should discuss possible limitations of their approachto address problems of privacy and fairness": "While the authors might fear that complete honesty about limitations mightbe used by reviewers as grounds for rejection, a worse outcome might be thatreviewers discover limitations that arent acknowledged in the paper. Theauthors should use their best judgment and recognize that individual actions infavor of transparency play an important role in developing norms that preservethe integrity of the community. Reviewers will be specifically instructed to notpenalize honesty concerning limitations.",
  "If the contribution is a dataset and/or model, the authors should describe thesteps taken to make their results reproducible or verifiable": "Depending on the contribution, reproducibility can be accomplished in variousways. For example, if the contribution is a novel architecture, describing thearchitecture fully might suffice, or if the contribution is a specific model andempirical evaluation, it may be necessary to either make it possible for othersto replicate the model with the same dataset, or provide access to the model. Ingeneral. releasing code and data is often one good way to accomplish this, butreproducibility can also be provided via detailed instructions for how to replicatethe results, access to a hosted model (e.g., in the case of a large language model),releasing of a model checkpoint, or other means that are appropriate to theresearch performed. While NeurIPS does not require releasing code, the conference does require allsubmissions to provide some reasonable avenue for reproducibility, which maydepend on the nature of the contribution. For example(a) If the contribution is primarily a new algorithm, the paper should make itclear how to reproduce that algorithm.",
  "(b) If the contribution is primarily a new model architecture, the paper shoulddescribe the architecture clearly and fully": "(c) If the contribution is a new model (e.g., a large language model), then thereshould either be a way to access this model for reproducing the results or away to reproduce the model (e.g., with an open-source dataset or instructionsfor how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in whichcase authors are welcome to describe the particular way they provide forreproducibility. In the case of closed-source models, it may be that access tothe model is limited in some way (e.g., to registered users), but it should bepossible for other researchers to have some path to reproducing or verifyingthe results.",
  ". Experimental Setting/Details": "Question: Does the paper specify all the training and test details (e.g., data splits,hyperparameters, how they were chosen, type of optimizer, etc.)necessary tounderstand the results?Answer: [Yes]Justification: All related experimental setting is stated in the main paper or theappendix.Guidelines: The answer NA means that the paper does not include experiments. The experimental setting should be presented in the core of the paper to a levelof detail that is necessary to appreciate the results and make sense of them.",
  ". Experiment Statistical Significance": "Question: Does the paper report error bars suitably and correctly defined or otherappropriate information about the statistical significance of the experiments?Answer: [Yes]Justification: All of the results are plotted with averaged performance with errorbar,and from the plot FocalBO significantly outperforms baselines.Guidelines: The answer NA means that the paper does not include experiments. The authors should answer \"Yes\" if the results are accompanied by error bars,confidence intervals, or statistical significance tests, at least for the experimentsthat support the main claims of the paper. The factors of variability that the error bars are capturing should be clearlystated (for example, train/test split, initialization, random drawing of someparameter, or overall run with given experimental conditions).",
  "Guidelines:": "The answer NA means that the paper poses no such risks. Released models that have a high risk for misuse or dual-use should be releasedwith necessary safeguards to allow for controlled use of the model, for exampleby requiring that users adhere to usage guidelines or restrictions to access themodel or implementing safety filters.",
  "The authors should state which version of the asset is used and, if possible,include a URL": "The name of the license (e.g., CC-BY 4.0) should be included for each asset. For scraped data from a particular source (e.g., website), the copyright andterms of service of that source should be provided. If assets are released, the license, copyright information, and terms of use inthe package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can helpdetermine the license of a dataset.",
  "According to the NeurIPS Code of Ethics, workers involved in data collection,curation, or other labor should be paid at least the minimum wage in thecountry of the data collector": "15. Institutional Review Board (IRB) Approvals or Equivalent for Researchwith Human SubjectsQuestion: Does the paper describe potential risks incurred by study participants,whether such risks were disclosed to the subjects, and whether Institutional ReviewBoard (IRB) approvals (or an equivalent approval/review based on the requirementsof your country or institution) were obtained?Answer: [NA]Justification: This paper does not include related topic in this question.Guidelines:",
  "The answer NA means that the paper does not involve crowdsourcing norresearch with human subjects": "Depending on the country in which research is conducted, IRB approval (orequivalent) may be required for any human subjects research. If you obtainedIRB approval, you should clearly state this in the paper. We recognize that the procedures for this may vary significantly between insti-tutions and locations, and we expect authors to adhere to the NeurIPS Code ofEthics and the guidelines for their institution."
}