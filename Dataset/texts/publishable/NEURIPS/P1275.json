{
  "Abstract": "Recently, cutting-plane methods such as GCP-CROWN have been explored toenhance neural network verifiers and made significant advances. However, GCP-CROWN currently relies on generic cutting planes (cuts) generated from externalmixed integer programming (MIP) solvers. Due to the poor scalability of MIPsolvers, large neural networks cannot benefit from these cutting planes. In this paper,we exploit the structure of the neural network verification problem to generateefficient and scalable cutting planes specific for this problem setting. We propose anovel approach, Branch-and-bound Inferred Cuts with COnstraint Strengthening(BICCOS), which leverages the logical relationships of neurons within verifiedsubproblems in the branch-and-bound search tree, and we introduce cuts thatpreclude these relationships in other subproblems. We develop a mechanism thatassigns influence scores to neurons in each path to allow the strengthening of thesecuts. Furthermore, we design a multi-tree search technique to identify more cuts,effectively narrowing the search space and accelerating the BaB algorithm. Ourresults demonstrate that BICCOS can generate hundreds of useful cuts duringthe branch-and-bound process and consistently increase the number of verifiableinstances compared to other state-of-the-art neural network verifiers on a wide rangeof benchmarks, including large networks that previous cutting plane methods couldnot scale to. BICCOS is part of the ,-CROWN verifier, the VNN-COMP 2024winner. The code is available at 1IntroductionFormal verification of neural networks (NNs) has emerged as a critical challenge in the field ofartificial intelligence. In canonical settings, the verification procedure aims to prove certified boundson the outputs of a neural network given a specification, such as robustness against adversarialperturbations or adherence to safety constraints. As these networks become increasingly complexand are deployed in sensitive domains, rigorously ensuring their safety and reliability is paramount.Recent research progress on neural network verification has enabled safety or performance guaranteesin several mission-critical applications . The branch and bound (BaB) procedure has shown great promise as a verification approach . In particular, the commonly used ReLU activation function exhibits a piecewise linearbehavior, and this property makes ReLU networks especially suitable for branch and bound techniques.During the branching process, one ReLU neuron is selected and two subproblems are created. Eachsubproblem refers to one of the two ReLU states (inactive or active) and the bounds of the neuronsactivation value are tightened correspondingly. BaB systematically branches ReLU neurons, createsa search tree of subproblems, and prunes away subproblems whose bounds are tight enough toguarantee the verification specifications. Although our work focuses on studying the canonical ReLUsettings, branch-and-bound has also demonstrated its power in non-ReLU cases . Despite itssuccess in many tasks, the efficiency and effectiveness of the branch-and-bound procedure heavilydepend on the number of subproblems that can be pruned.",
  "arXiv:2501.00200v1 [cs.LG] 31 Dec 2024": "Recent work has explored the use of cutting plane methods in neural network verification. Cuttingplanes (cuts) are additional (linear) constraints in the optimization problem for NN verificationto tighten the bound without affecting soundness. By introducing carefully chosen cuts, they cansignificantly tighten the bounds in each subproblem during BaB, leading to more pruned subproblemsand faster verification. However, it is quite challenging to generate effective cutting planes forlarge-scale NN verification problems. For example, GCP-CROWN relied on strong cuttingplanes generated by a mixed integer programming (MIP) solver, which includes traditional genericcutting planes such as the Gomory cuts , but its effectiveness is limited to only small neuralnetworks. The key to unlocking the full potential of BaB lies in the development of scalable cuttingplanes that are specific to the NN verification problem and also achieve high effectiveness. In this paper, we propose a novel methodology, Branch-and-bound Inferred Cuts with COnstraintStrengthening (BICCOS), that can produce effective and scalable cutting planes specifically for theNN verification problem. Our approach leverages the information gathered during the branch-and-bound process to generate cuts on the fly. First, we show that by leveraging any verified subproblems(e.g., a subproblem is verified when neurons A and B are both split to active cases), we can deducecuts that preclude certain combinations of ReLU states (e.g., neurons A and B cannot be both active).This cut may tighten the bounds for subproblems in other parts of the search tree, even when neuronsA and B have not yet been split. Second, to make these cuts effective during the regular BaB process,we show that they can be strengthened by reducing the number of branched neurons in a verifiedsubproblem (e.g., we may conclude that setting neuron B to active is sufficient to verify the problem,drop the dependency on A, and remove one variable from the cut). BICCOS can find and strengthencuts on the fly during the BaB process, adapting to the specific characteristics of each verificationinstance. Third, we propose a pre-solving strategy called multi-tree search which proactively looksfor effective cutting planes in many shallow search trees before the main BaB phase starts. Our maincontributions can be summarized as follows: We first identify the opportunity of extracting effective cutting planes during the branch-and-boundprocess in NN verification. These cuts are specific to the NN verification setting and scalable to largeNNs that previous state-of-the-art cutting plane methods such as GCP-CROWN with MIP-cuts cannothandle. Our cuts can be plugged into existing BaB-based verifiers to enhance their performance. We discuss several novel methods to strengthen the cuts and also find more effective cuts. Strength-ening the cuts is essential for finding effective cuts on the fly during the regular BaB process that cantighten the bounds in the remaining domains. Performing a multi-tree search allows us to identifyadditional cuts before the regular BaB process begins. We conduct empirical comparisons against many verifiers on a wide range of benchmarks andconsistently outperform state-of-the-art baselines in VNN-COMP . Notably, we can solvethe very large models like those for cifar100 and tinyimagenet benchmarks and outperform allstate-of-the-art tools, to which GCP-CROWN with MIP-based cuts cannot scale.2Background",
  "NotationsBold symbols such as x(i) denote vectors; regular symbols such as x(i)jindicate scalar": "components of these vectors. [N] represents the set {1, . . . , N}, and W(i):,j is the j-th column of thematrix W(i). A calligraphic font X denotes a set and Rn represents the n dimensional real numberspace.The NN Verification ProblemAn L-layer ReLU-based DNN can be formulated as f : x x(L), s.t. {x(i) = W (i) x(i1) + b(i), x(i) = (x(i)), x(0) = x;i [L]}, where represents theReLU activation function, with input x =: x(0) Rd(0) and the neuron network parameters weightmatrix W (i) Rd(i)d(i1) and bias vector b(i) Rd(i) for each layer i. This model sequentiallyprocesses the input x through each layer by computing linear transformations followed by ReLUactivations. The scalar values x(i)jand x(i)jrepresent the post-activation and pre-activation values",
  "of the j-th neuron in the i-th layer, respectively. We write f (i)j (x) and f (i)j (x) for x(i)jand x(i)j ,respectively, when they depend on a specific x": "In practical applications, the input x is confined within a perturbation set X, often defined as an pnorm ball. The verification task involves ensuring a specified output property for any x X. Forexample, it may be required to verify that the logit corresponding to the true label fi(x) is consistentlyhigher than the logit for any other label fj(x), thus ensuring fi(x) fj(x) > 0,j = i. By",
  "f = minxX f(x),(1)": "where the optimal value f 0 confirms the verifiable property. We typically use the norm todefine X := {x : x x0 }, with x0 as a baseline input. However, extensions to other normsand conditions are also possible . When we compute a lower bound f f , we label theproblem UNSAT if the property was verified and f 0, i.e. the problem of finding a concreteinput that violates the property is unsatisfiable, or in other words infeasible. If f < 0, it is unclearwhether the property might hold or not. We refer to this as unknown.MIP Formulation and LP RelaxationIn the optimization problem (1), a non-negative f indicatesthat the network can be verified. However, due to the non-linearity of ReLU neurons, this problem isnon-convex, and ReLU neurons are typically relaxed with linear constraints to obtain a lower boundfor f . One possible solution is to encode the verification problem using Mixed Integer Programming(MIP), which encodes the entire network architecture, using x Rd(i) to denote the pre-activationneurons, x Rd(i) to denote post-activation neurons for each layer, and binary ReLU indicatorz(i)j {0, 1} to denote (in)active neuron for each unstable neuron. A lower bound can be computed by letting z(i)j and therefore relaxing the problem to an LP formulation. There is also anequivalent Planet relaxation . We provide the detailed definition in Appendix A. In practice, thisapproach is computationally too expensive to be scalable.Branch-and-boundInstead of solving the expensive LP for each neuron, most existing NN verifiersuse cheaper methods such as abstract interpretation or bound propagation due to their efficiency and scalability. However, because of those relaxations, the lower bound for f might eventually become too weak to prove f 0. To overcome this issue, additional constraintsneed to be added to the optimization, without sacrificing soundness. : Each node repre-sents a subproblem in the BaBprocess by splitting unstableReLU neurons. Green nodesindicate paths that have beenverified and pruned, whileblue nodes represent domainsthat are still unknown and re-quire further branching. The branch-and-bound (BaB) framework, illustrated in ,is a powerful approach for neural network verification that manystate-of-the-art verifiers are based on . BaBsystematically tightens the lower bound of f by splitting unstableReLU neurons into two cases: xj 0 and xj 0 (branching step),which defines two subproblems with additional constraints. In eachsubproblem, neuron xj does not need to be relaxed, leading to atighter lower bound (bounding step). Note that {x(i)j u(i)j= 0}",
  "and {x(i)j l(i)j= 0} in the Planet relaxation are equivalent to": "{z(i)j= 0} and {z(i)j= 1} in the LP relaxation, respectively, seeLemma A.1 in Appendix A. Subproblems with a positive lowerbound are successfully verified, and no further splitting is required.The process repeats on subproblems with negative lower boundsuntil all unstable neurons are split, or all subproblems are verified.If there are still domains with negative bounds after splitting allunstable neurons, a counter-example can be constructed.General Cutting Planes (GCP) in NN verificationA (linear)cutting plane (cut) is a linear inequality that can be added to a MIPproblem, which does not eliminate any feasible integer solutionsbut will tighten the LP relaxation of this MIP. For more details on cutting plane methods, we referreaders to the literature on integer programming . In the NN verification setting, it mayinvolve variables x(i) (pre-activation), x(i) (post-activation) from any layer i, and z(i) (binary ReLUindicators) from any unstable neuron. Given N cutting planes in matrix form as :",
  "allows us to lower bound (1) with arbitrary linear constraints in (2) using GPU-accelerated boundpropagation, without relying on an LP solver": "In , the authors propose to find new cutting planes by employing an MIP solver. By encoding (1)as a MIP problem, the MIP solver will identify potentially useful cutting planes. Usually, these cutswould be used by the solver itself to solve the MIP problem. Instead, applied these cuts usingGPU-accelerated bound propagation. This approach shows great improvements on many verificationproblems due to the powerful cuts, but it depends on the ability of the MIP solver to identify relevantcuts. As the networks that are analyzed increase in size, the respective MIP problem increases incomplexity, and the MIP solver may not return any cuts before the timeout is reached. In the nextchapter, we will describe a novel approach to generate effective and scalable cutting planes. 3Branch-and-bound Inferred Cuts with Constraint Strengthening (BICCOS)3.1Branch-and-bound Inferred CutsThe first key observation in our algorithm is that the UNSAT subproblems in the BaB search treeinclude valuable information. If the lower bound of a subproblem leads this subproblem to be UNSAT(e.g., subproblems with green ticks in ), it signifies that restricting the neurons along this pathto the respective positive/negative regimes allows us to verify the property. A typical BaB algorithmwould stop generating further subproblems at this juncture, and continue with splitting only thosenodes that have not yet been verified. Crucially, no information from the verified branch is transferredto the unverified domains. However, sometimes, this information can help a lot. Example. As shown in , assume that after splitting x1 0 and x3 0, the lower bound ofthis subproblem is found to be greater than 0, indicating infeasibility. From this infeasibility, wecan infer that the neurons x1 and x3 cannot simultaneously be in the inactive regime. To representthis relationship, we use the relaxed ReLU indicator variables z1, z3 in the LP formulationand form the inequality z1 + z3 1. This inequality ensures that both z1 and z3 cannot be 0simultaneously. If we were to start BaB with a fresh search tree, this constraint has the potentialto improve the bounds for all subproblems by tightening the relaxations and excluding infeasibleregions. We propose to encode the information gained from a verified subproblem as a new cutting plane.These cutting planes will be valid globally across the entire verification problem and all generatedsubproblems. We present the general case of this cutting plane below:Proposition 3.1. For a verified, or UNSAT, subproblem in a BaB search tree, let Z+ and Z be theset of neurons restricted to the positive and negative regimes respectively. These restrictions wereintroduced by the BaB process. Then, the BaB inferred cut can be formulated as:",
  "iZzi |Z+| 1(3)": "Proof deferred to Appendix B.1. The BaB inferred cut (3) will exclude the specific combinationof positive and negative neurons that were proven to be infeasible in the respective branches. Anexample is shown in a. Note that while similar cuts were explored in , they were not derived from UNSAT problems withinthe BaB process. In our framework, these cuts can theoretically be incorporated as cutting planes inthe form of (2) and work using GCP-CROWN. However, a limitation exists: all elements in our cutare ReLU indicator z, although GCP-CROWN applies general cutting planes during the standard BaBprocess. It was not originally designed to handle branching decisions on ReLU indicator variablesz that may have (partially) been fixed already in previous BaB steps. When cuts are added, theyremain part of the GCP-CROWN formulation. However, during the BaB process, some z variablesmay be fixed to 0 or 1 due to branching, effectively turning them into constants This situation poses achallenge because the original GCP-CROWN formulation presented in does not accommodateconstraints involving these fixed z variables, potentially leading to incorrect results. To address thisissue, we need to extend GCP-CROWN to handle BaB on the ReLU indicators z, ensuring that theconstraints and cuts remain valid even when some z variables are fixed during branching. Extension of GCP-CROWNTo address this limitation, we propose an extended form of boundpropagation for BaB inferred cuts that accommodates splitting on z variables. In the original GCP-CROWN formulation, I(i) represents the set of initially unstable neurons in layer i, for which z cuts were added due to their instability. However, during the branch-and-bound process, some of theseneurons may be split, fixing their corresponding z variables to 0 or 1, and thus their z variables nolonger exist in the original formulation. While updating all existing cuts by fixing these z variables ispossible, it is costly since the cuts for each subproblem must be fixed individually, as the neuron splitsin each subproblem is different. Our contribution is to handle these split neurons by adding them tothe splitting set Z in the new formulation below, without removing or modifying the existing cuts(all subdomains can still share the same set of cuts). This approach allows us to adjust the originalbound propagation in [61, Theorem 3.1] to account for the fixed z variables and their influence on theLagrange dual problem, without altering the existing cuts. Suppose the splitting set for each layer i isZ+(i) Z(i) := Z(i) I(i), and the full split set is Z = i[L1] Z(i). The modifications to theoriginal GCP-CROWN theorem are highlighted in brown in the following theorem.Theorem 3.2. [BaB Inferred Cuts Bound Propagation]. Given any BaB split set Z, optimizableparameters 0 (i)j 1 and , , 0, (i)j",
  "Q(i):,jif Q(i):,j l(i)j [(i)j ]+ or j Z+(i)": "Proof in Appendix B.2. The brown-highlighted modifications ensure that the bound propagationcorrectly accounts for the influence of the fixed ReLU indicator z variablesresulting from branchingdecisions in the BaB processon each layers coefficients and biases. Notably, if we remove allcutting planes, this propagation method reduces to -CROWN . In this context, the new dualvariables and correspond to the dual variable used for x splits in -CROWN. To handle splits,we only need to specify the sets Z+(i) and Z(i) for each layer. By adjusting the dual variables andfunctions to reflect the fixed states of certain neurons, the extended bound propagation maintainstightness and correctness in the computed bounds.",
  "Improving BaB Inferred Cuts via Constraint Strengthening and Multi-Tree Search": "Theorem 3.2 allows us to use the cheap bound propagation method to search the BaB tree quicklyto discover UNSAT paths to generate BaB inferred cuts by Proposition 3.1. However, our secondkey observation is that naively inferred cuts will not be beneficial in a regular BaB search process,illustrated in a, for example, all the subproblems after the split x1 0 (all nodes on theright after the first split) imply z1 = 1, and thus z1 + z3 1 always holds. Thus, we have tostrengthen these cuts to make them more effective. We propose the Branch-and-bound Inferred Cutwith COnstraint Strengthening (BICCOS) to solve this.",
  "(b) Starting from the UNSAT path{x1 0, x5 0, x7 0}, {x1 0} is not actually needed, as {x5 0, x7 0} is sufficient to prove thatthis subproblem is UNSAT. We caninfer a new cut z7 z5 1,": "(c) The root node is split multipleways. In the sub-tree where the rootnode is split on x1, we extract thecut z1 + z3 1. This cut is thenapplied to all other sub-trees as well.In the sub-tree with the root nodesplit on x8, inducing another cutz8 1 that will be shared acrosssub-trees.",
  ": (2a): Inferred cut from UNSAT paths during BaB and why it fails in regular BaB. (2b):Constraint strengthening with Neuron Elimination Heuristic. (2c): Multi-tree search": "In a known UNSAT subproblem with multiple branched neurons, it is possible that a subset of thebranching neurons is sufficient to prove UNSAT, shown in b. By focusing on this subset, wecan strengthen the BaB inferred cuts by reducing the number of z variables involved. From anoptimization perspective, each cut corresponds to a hyperplane that separates feasible solutions frominfeasible ones. Simplifying the cut by involving fewer z variables reduces the dimensionality of thehyperplane, making it more effective at excluding infeasible regions without unnecessarily restrictingthe feasible space. Thus, we can draw a corollary:Corollary 3.3. Formally, consider the two cuts from Eq. (3):",
  "zi |Z+2 | 1(3.2)": "If either Z+1 Z+2 and Z1 Z2 , or Z+1 Z+2 and Z1 Z2 , then any assignment {zi}satisfying the cut associated with the smaller split set Z also satisfies the other cut. This implies thatthe cut with the smaller set is strictly stronger than the one with the larger set. Proof deferred to Appendix B.3. This demonstrates that cuts with fewer variables can be more power-ful because they operate in lower-dimensional spaces, allowing the hyperplane to more effectivelyposition itself to exclude infeasible regions. Now, the challenge is determining which unnecessary branches to remove to produce a cut with asfew z variables as possible. An important observation from Theorem 3.2 is that the dual variables and , associated with the fixed ReLU indicators z Z+(i) and z Z(i) respectively, reflect theimpact of these fixed activations on the bound. Specifically, a positive dual variable > 0 for somez Z+(i) indicates that the fixed active state of these neurons contributes significantly to tighteningthe bound. Similarly, a positive > 0 for some z Z(i) signifies that the fixed inactive neurons areinfluential in optimizing the bound. However, neurons with zero dual variables ( = 0 or = 0) maynot contribute to the current bound optimization. This observation suggests that these neurons mightbe candidates for removal to simplify the cut. But its important to note that a zero dual variable doesnot guarantee that the corresponding neuron has no impact under all circumstancesit only indicatesno contribution in the current optimization context. Simply removing all neurons with zero dual variables might overlook their potential influence in other parts of the search space or under differentparameter settings. Therefore, the challenge lies in deciding which neurons with zero or negligibledual variables can be safely removed without significantly weakening the cut. This decision requiresa careful heuristic that considers not only the current values of the dual variables but also the overallstructure of the problem and the potential future impact of these neurons. By intelligently selectingwhich z variables to exclude, we aim to produce a stronger cut that is both effective in pruning thesearch space and efficient in computing. To do this, we use a heuristic to determine whether eachneuron should be tentatively dropped from the list of constraints. Algorithm 1 shows the constraintstrengthening with the neuron elimination heuristic.Neuron Elimination Heuristic for Constraint StrengtheningFirst, we compute a heuristicinfluence score for each neuron to assess its impact on the verification objective. This score is basedon the improvement in the lower bound of f before and after introducing the neurons split in theBaB tree. By recording the computed lower bounds at each node, we can measure how beneficialeach constraint is to the verification process. Second, we rank the neurons according to their influencescores and tentatively drop those that contribute least to improving the lower bound. We will retainneurons whose corresponding Lagrange multipliers, or , are greater than zero. For neurons with or equal to zero, we remove a certain percentile of neurons with the lowest scores, as indicated bythe drop_percentage parameter in Algorithm 1. When a neuron is dropped, its split is canceled.Then, we perform a re-verification step using only the reduced subset of constraints. This involvesrecomputing the lower bound on f based solely on the selected constraints. If the verification stillsucceedsthat is, the lower bound remains non-negativethe reduced set induces a new cuttingplane. This new cutting plane is applied to all subproblems within the BaB process after furtherstrengthening. Finally, we can iteratively repeat the process of reducing the constraint set, aiming togenerate even stronger cuts. The process terminates when the property can no longer be verified withthe current subset of constraints. This iterative refinement is designed to focus on the most influentialneurons, potentially enhancing the efficiency of the verification. Once the new cuts determined, wemerge pairs of cuts if possible (e.g. merging z1 + z2 0 and z1 z2 0 to z1 0).",
  ": Ccut Merge_Cuts(Ccut)13: return Ccut": "Multi-Tree SearchTraditionally, the BaB process generates a single search tree. We proposeaugmenting this approach by performing multiple BaB processes in parallel as a presolving step, witheach process exploring a different set of branching decisions. At each branching point, we initializemultiple trees and apply various branching decisions simultaneously. While this initially increasesthe number of subproblems, the cutting planes generated in one tree are universally valid and canbe applied to all other trees. These newly introduced cuts can help prove UNSAT for nodes in othertrees, thereby inducing additional cutting planes and amplifying the pruning effect across the entiresearch space, illustrated in c. Since computational resources must be allocated across multiple trees, we prioritize nodes for furtherexpansion that have the highest lower bound on the optimization objective. This strategy ensures thatmore promising trees receive more computational resources. After a predefined short timeout, we consolidate our efforts by pruning all but one of the trees and proceed with the standard BaB processaugmented with BICCOS on the selected tree. We choose the tree that has been expanded the mostfrequently, as this indicates that its bounds are closest to verifying the property.BaB Tree Searching StrategyIn the standard BaB process (e.g., in -CROWN), branches operateindependently without sharing information, so the order in which they are explored does not affect theoverall runtime. For memory access efficiency, there is a slight preference for implementing BaB as adepth-first search (DFS), where constraints are added until unsatisfiability (UNSAT) can be proven. This approach focuses the search on deeper branches before returning to shallower nodes. However, in the context of BICCOS, our objective is to generate strong cutting planes that can prunenumerous branches across different subproblems. To maximize the generality of these cutting planes,they need to be derived from UNSAT nodes with as few constraints as possible. While constraintstrengthening techniques can simplify the constraints, this process is more straightforward whenthe original UNSAT node already has a minimal set of constraints. Even if only a few constraintsare eliminated, the resulting cutting plane can significantly impact many other subproblems. Tofacilitate this, we propose performing the BaB algorithm using a breadth-first search (BFS) strategy.By exploring nodes that are closest to the root and have the fewest neuron constraints, we can generatemore general and impactful cutting planes earlier in the search process.Algorithm 2 Branch-and-bound Inferred Cuts with Constraint Strengthening (BICCOS).",
  ": return UNSAT if |DUnknown| = 0 else Unknown": "3.3BICCOS SummaryAlgorithm 2 summarizes our proposed BICCOS algorithm, with the modifications to the standardBaB algorithm highlighted in brown. First (line 2), instead of exploring a single tree, we exploremultiple trees in parallel as a presolving step. This process may involve constraint strengtheningand utilizes cut inference analogous to the procedures in lines 3-15 of the algorithm. After severaliterations, we prune all but one of the trees. From this point forward, only the selected tree is expandedfurther, following the regular BaB approach. Until all subdomains of this tree have been verified, BICCOS selects batches of unverified subdomainsadd additional branching decisions, and attempt to prove the verification property. Unlike regular BaB,it then applies constraint strengthening to all identified UNSAT nodes and infers the correspondingcutting planes. These cutting planes are added to all currently unverified subdomains, potentiallyimproving their lower bounds enough to complete the verification process. If BICCOS fails to identifyhelpful cutting planes, it effectively behaves like the regular BaB algorithm. We have implementedBICCOS in the ,-CROWN toolbox. Notably, the cuts found by BICCOS are compatible withthose from MIP solvers in GCP-CROWN, and all cuts can be combined in cases where MIP cuts arebeneficial. 4ExperimentsWe evaluate our verifier, BICCOS, on several popular verification benchmarks from VNN-COMP and on the SDP-FO benchmarks used in multiple studies . In the followingdiscussion, ,-CROWN refers to the verification tool that implements various verification techniques,while -CROWN and GCP-CROWN denote specific algorithms implemented within ,-CROWN.To ensure the comparability of our methods effects and to minimize the influence of hardware andequipment advances, we rerun -CROWN and GCP-CROWN with MIP cuts for each experiment.Additionally, we use the same BaB algorithm as in -CROWN and employ filtered smart branching",
  "CNN-B-Adv-453.543.856.03.20----58.59.6362.08.2763.5": "* MN-BaB with 600s timeout threshold for all models. - indicates that we could not run a model due to unsupported model structure or other errors. We run -CROWN,GCP-CROWN with MIP cuts and BICCOS with a shorter 200s timeout for all models. The increased timeout for MN-BaB may increase the percentage of verified instances.However, we can still achieve better verified accuracy than all other baselines. Other results are reported from . (FSB) as the branching heuristic in all experiments. Note that in experiments GCP-CROWNrefers to GCP-CROWN solver with MIP cuts. We also conduct ablation studies to identify whichcomponents of BICCOS contribute the most, including analyses of verification accuracy & time,number of cuts generated, and number of domains visited. Experimental settings are described inAppendix C.1.Results on VNN-COMP benchmarksWe first evaluate BICCOS on many challenging bench-marks with large models, including two VNN-COMP 2024 benchmarks: cifar100-2024 andtinyimagenet-2024; two VNN-COMP 2022 benchmarks: cifar100-tinyimagenet-2022 andoval22. Shown in , our proposed method, BICCOS, outperforms most other verifiers onthe tested benchmarks, achieving the highest number of verified instances in four benchmark sets.BICCOS consistently outperforms the baseline , -CROWN verifier (the -CROWN and GCP-CROWN (MIP cuts) lines), verifying more instances across almost all benchmark sets. In particular,GCP-CROWN with MIP cuts cannot scale to the larger network architectures in the cifar100 andtinyimagenet benchmarks with network sizes between 14.4 and 31.6 million parameters, due to itsreliance on an MIP solver. BICCOS, on the other hand, can infer cutting planes without the needfor an MIP solver and noticeably outperforms the baseline on cifar100 and tinyimagenet. Notethat the increase in average runtime (e.g., on the cifar100-tinyimagenet-2022 benchmark) isexpected. The instances that could not be verified at all previously but can be verified using BICCOStend to require runtimes that are below the timeout but above the baselines average runtime.Results on SDP-FO benchmarksWe further evaluated BICCOS, on the challenging SDP-FObenchmarks introduced in previous studies . These benchmarks consist of seven predom-inantly adversarial trained MNIST and CIFAR models, each containing numerous instances thatare difficult for many existing verifiers. Our results, detailed in , demonstrate that BICCOSsignificantly improves verified accuracy across all the tested models when compared to current state-of-the-art verifiers. On both MNIST and CIFAR dataset, BICCOS not only surpasses the performanceof methods like -CROWN and GCP-CROWN on the CNN-A-Adv model but also approaches theempirical robust accuracy upper bound, leaving only a marginal gap. A slight increase in averagetime in some cases is attributed to the higher number of solved instances.Ablation Studies on BICCOS Components.To evaluate the contributions of individual com-ponents of BICCOS, we performed ablation studies summarized in . The BICCOS baseversion with BaB inferred cuts and constraint strengthening already shows competitive performancein many models. Cuts from MIP solvers are compatible with BICCOS and can be added for smallermodels that can be handled by MIP solver. Integrating Multi-Tree Search (MTS) significantly boostsperformance. On the CIFAR CNN-B-Adv model, verified accuracy rises to 54.5%, outperforming",
  "* We run our BICCOS in different ablation studies with a shorter 200s timeout for all models and compare it to -CROWN and GCP-CROWN, it achieves betterverified accuracy than all other baselines": "GCP-CROWN with MIP cutss 49%. We also design an adaptive BICCOS configuration (BICCOSauto), which automatically turns on MTS and/or MIP-based cuts according to neural network andverification problem size and quantity, achieves the highest verified accuracies across most of modelsand is used as the default option of the verifier when BICCOS is enabled. A detailed table with thenumbers of cuts and domains visited is provided in Appendix C.2.",
  "Related work": "Our work is based on the branch and bound framework for neural network verification , which is one of the most popular approaches that lead to state-of-the-artresults . Most BaB-based approaches do not consider the correlations among subproblems- for example, in -CROWN , the order of visiting the nodes in the BaB search tree does notchange the verification outcome as the number of leaf nodes will be the same regardless of how theleaves are split. Our work utilizes information on the search tree and can gather more effective cutswhen shallower nodes are visited first. Exploring the dependency or correlations among neurons has also been identified as a potential avenueto enhance verification bounds. While several studies have investigated this aspect , theirfocus has primarily been on improving the bounding step without explicitly utilizing the relationshipsamong ReLUs during the branching process. Venus considers the implications among neuronswith constraints similar to our cutting planes. However, their constraints were not discovered usingthe verified subproblems during BaB or multi-tree search, and cannot be strengthened. On the otherhand, cutting plane methods encode dependency among neurons as general constraints , andour work developed a new cutting plane that can be efficiently constructed and strengthened duringBaB, the first time in literature. In addition, some NN verifiers are based on the satisfiability modulo theories (SMT) formula-tions , which may internally use an SAT-solving procedure such as DPLL orCDCL . These procedures may discover conflicts in boolean variable assignments, correspondingto eliminating certain subproblems in BaB. However, they differ from BICCOS in two significantaspects: first, although DPLL or CDCL may discover constraints to prevent some branches withneurons involved in these constraints, they cannot efficiently use these constraints as cutting planesthat may tighten the bounds for subproblems never involving these neurons; second, DPLL or CDCLworks on the abstract problem where each ReLU is represented as a boolean variable, and cannottake full advantage of the underlying bound propagation solver to strengthen constraints as we did inAlg. 1. Based on our observation in Sec. 3, the constraints discovered during BaB are often unhelpfulwithout strengthening unless in a different search tree, so their effectiveness is limited. However, thelearned conflicts can be naturally translated into cuts 3, making this a future work.",
  "Conclusion": "We exploit the structure of the NN verification problem to generate efficient and scalable cuttingplanes, leveraging neuron relationships within verified subproblems in a branch-and-bound searchtree. Our experimental results demonstrate that the proposed BICCOS algorithm achieves very goodscalability while outperforming many other tools in the VNN-COMP, and can solve benchmarks thatexisting methods utilizing cutting planes could not scale to. Limitations are discussed in Appendix E. AcknowledgmentHuan Zhang is supported in part by the AI2050 program at Schmidt Sciences(AI2050 Early Career Fellowship) and NSF (IIS-2331967). Grani A. Hanasusanto is supported inpart by NSF (CCF-2343869 and ECCS-2404413). Computations were performed with computingresources granted by RWTH Aachen University under project rwth1665. We thank the anonymousreviewers for helping us improve this work.",
  "Armin Biere, Marijn Heule, and Hans van Maaren. Handbook of satisfiability, volume 185.IOS press, 2009": "Elena Botoeva, Panagiotis Kouvaros, Jan Kronqvist, Alessio Lomuscio, and Ruth Misener.Efficient verification of relu-based neural networks via dependency analysis. In Proceedings ofthe AAAI Conference on Artificial Intelligence, volume 34, pages 32913299, 2020. Christopher Brix, Stanley Bak, Changliu Liu, and Taylor T Johnson. The fourth internationalverification of neural networks competition (vnn-comp 2023): Summary and results. arXivpreprint arXiv:2312.16760, 2023. Rudy Bunel, Jingyue Lu, Ilker Turkaslan, Philip HS Torr, Pushmeet Kohli, and M PawanKumar. Branch and bound for piecewise linear neural network verification. Journal of MachineLearning Research, 21(42):139, 2020. Rudy R Bunel, Ilker Turkaslan, Philip Torr, Pushmeet Kohli, and Pawan K Mudigonda. Aunified view of piecewise linear neural network verification. Advances in Neural InformationProcessing Systems, 31, 2018.",
  "Michele Conforti, Grard Cornujols, and Giacomo Zambelli. Integer programming. Graduatetexts in mathematics, . 271. Springer, Cham, 2014": "Sumanth Dathathri, Krishnamurthy Dvijotham, Alexey Kurakin, Aditi Raghunathan, JonathanUesato, Rudy R Bunel, Shreya Shankar, Jacob Steinhardt, Ian Goodfellow, Percy S Liang,et al. Enabling certification of verification-agnostic networks via memory-efficient semidefiniteprogramming. Advances in Neural Information Processing Systems (NeurIPS), 2020. Alessandro De Palma, Harkirat Singh Behl, Rudy Bunel, Philip H. S. Torr, and M. PawanKumar. Scaling the convex barrier with active sets. International Conference on LearningRepresentations (ICLR), 2021. Alessandro De Palma, Rudy Bunel, Alban Desmaison, Krishnamurthy Dvijotham, PushmeetKohli, Philip HS Torr, and M Pawan Kumar. Improved branch and bound for neural networkverification via lagrangian decomposition. arXiv preprint arXiv:2104.06718, 2021.",
  "Claudio Ferrari, Mark Niklas Muller, Nikola Jovanovic, and Martin Vechev. Complete verifica-tion via multi-neuron relaxation guided branch-and-bound. arXiv preprint arXiv:2205.00263,2022": "Harald Ganzinger, George Hagen, Robert Nieuwenhuis, Albert Oliveras, and Cesare Tinelli. Dpll(t): Fast decision procedures. In Computer Aided Verification: 16th International Conference,CAV 2004, Boston, MA, USA, July 13-17, 2004. Proceedings 16, pages 175188. Springer,2004. Timon Gehr, Matthew Mirman, Dana Drachsler-Cohen, Petar Tsankov, Swarat Chaudhuri,and Martin Vechev. Ai2: Safety and robustness certification of neural networks with abstractinterpretation. In 2018 IEEE Symposium on Security and Privacy (SP). IEEE, 2018.",
  "Harkirat Singh, M Pawan Kumar, Philip Torr, and Krishnamurthy Dj Dvijotham. Overcomingthe convex barrier for simplex inputs. In Advances in Neural Information Processing Systems,2021": "Chuangchuang Sun, Dong-Ki Kim, and Jonathan P How. Romax: Certifiably robust deepmultiagent reinforcement learning via convex relaxation. In 2022 International Conference onRobotics and Automation (ICRA), pages 55035510. IEEE, 2022. Calvin Tsay, Jan Kronqvist, Alexander Thebelt, and Ruth Misener. Partition-based formulationsfor mixed-integer optimization of trained relu neural networks. Advances in neural informationprocessing systems, 34:30683080, 2021.",
  "Andreas Venzke and Spyros Chatzivasileiadis. Verification of neural network behaviour: Formalguarantees for power system applications. IEEE Transactions on Smart Grid, 12(1):383397,2020": "Shiqi Wang, Huan Zhang, Kaidi Xu, Xue Lin, Suman Jana, Cho-Jui Hsieh, and J Zico Kolter.Beta-crown: Efficient bound propagation with per-neuron split constraints for neural networkrobustness verification. Advances in Neural Information Processing Systems, 34:2990929921,2021. Eric Wong, Tim Schneider, Joerg Schmitt, Frank R Schmidt, and J Zico Kolter. Neural networkvirtual sensors for fuel injection quantities with provable performance specifications. In 2020IEEE Intelligent Vehicles Symposium (IV), pages 17531758. IEEE, 2020. Haoze Wu, Omri Isac, Aleksandar Zeljic, Teruhiro Tagomori, Matthew Daggitt, Wen Kokke,Idan Refaeli, Guy Amir, Kyle Julian, Shahaf Bassan, et al. Marabou 2.0: a versatile formalanalyzer of neural networks. In International Conference on Computer Aided Verification, pages249264. Springer, 2024.",
  "Junlin Wu, Andrew Clark, Yiannis Kantaros, and Yevgeniy Vorobeychik. Neural lyapunovcontrol for discrete-time systems. Advances in Neural Information Processing Systems, 36:29392955, 2023": "Kaidi Xu, Zhouxing Shi, Huan Zhang, Yihan Wang, Kai-Wei Chang, Minlie Huang, BhavyaKailkhura, Xue Lin, and Cho-Jui Hsieh. Automatic perturbation analysis for scalable certifiedrobustness and beyond. Advances in Neural Information Processing Systems, 33:11291141,2020. Kaidi Xu, Huan Zhang, Shiqi Wang, Yihan Wang, Suman Jana, Xue Lin, and Cho-Jui Hsieh.Fast and complete: Enabling complete neural network verification with rapid and massivelyparallel incomplete verifiers. International Conference on Learning Representations (ICLR),2021. Lujie Yang, Hongkai Dai, Zhouxing Shi, Cho-Jui Hsieh, Russ Tedrake, and Huan Zhang.Lyapunov-stable neural control for state and output feedback: A novel formulation for efficientsynthesis and verification. arXiv preprint arXiv:2404.07956, 2024. Huan Zhang, Shiqi Wang, Kaidi Xu, Linyi Li, Bo Li, Suman Jana, Cho-Jui Hsieh, and J ZicoKolter. General cutting planes for bound-propagation-based neural network verification. Ad-vances in Neural Information Processing Systems, 35:16561670, 2022. Huan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui Hsieh, and Luca Daniel. Efficient neu-ral network robustness certification with general activation functions. Advances in neuralinformation processing systems, 31, 2018. In Sec. A, we introduce the Mixed Integer Programming (MIP) formulation for Neural NetworkVerification and explain how to get the lower bound by solving the LP relaxation and Planet relaxation.In Sec. B, we provide the complete proof for Proposition 3.1 and Theorem 3.2 and Corollary 3.3.Then, in Sec. C, we present the configuration, ablation study, and additional experimental results.Finally in Sec. D and Sec. E, we discuss more related works and limitations of our method. AMIP Formulation and LP & Planet RelaxationThe MIP FormulationThe mixed integer programming (MIP) formulation is the root of manyNN verification algorithms. Given the ReLU activation functions piecewise linearity, the modelrequires binary encoding variables, or ReLU indicators z only for unstable neurons. We formulatethe optimization problem aiming to minimize the function f(x), subject to a set of constraints thatencapsulate the DNNs architecture and the perturbation limits around a given input x, as follows:",
  "x(i)j= 0; j I(i), i [L 1].(14)": "Here, the set J (i) comprises all neurons in the layer i, which are further categorized intothree distinct classes: active (I+(i)), inactive (I(i)), and unstable (I(i)); we further letl(i)j= minxX f (i)j (x), u(i)j= maxxX f (i)j (x),j J (i), i [L 1]. The MIP approachis initialized with pre-activation bounds l(i) x(i) u(i) for each neuron within the feasible set X,across all layers i. These bounds can be calculated recursively through the MIP from the first layer tothe final layer. However, MIP problems are generally NP-hard as they involve integer variables.",
  "B.2Proof of Theorem 3.2": "To derive the bound propagation, first, we assign Lagrange dual variables to each non-trivial constraintin LP relaxation with any BaB split set Z. Next, we analyze how splitting on the ReLU indicator zinfluences the generation of these dual variables. This analysis helps us determine the impact of thesplits on the dual formulation. Then, we formulate the Lagrangian dual as a min-max problem. Byapplying strong duality and eliminating the inner minimization problem, we can simplify the dualformulation. Finally, this results in a dual formulation that we use to perform bound propagationeffectively.",
  "H(i)x(i) + G(i) x(i) + Q(i)z(i) d RN+(2)": "Note that for some constraints, their dual variables are not created because they are trivial to handlein the steps. And from Lemma A.1, some unstable neuron constraints 8, 9, 10, 11 will be eliminatedduring BaB with split on z, i.e. when z(i)j= 1, the unstable neuron constraints 8 will be reduced to",
  "is always satisfied. This means that satisfying cut 3.1 ensures that cut 3.2 is also satisfied": "Conversely, consider an assignment where cut 3.2 is satisfied but cut 3.1 is violated. This can happenif the additional variables in Z()+and Z()compensate for the violation in the original variables.For instance, setting zi = 1 for all i Z()+and zi = 0 for all i Z()can decrease the LHS ofcut 3.2, making it easier to satisfy even if cut 3.1 is not satisfied.",
  "C.1Experiment Settings": "Our experiments are conducted on a server with an Intel Xeon 8468 Sapphire CPU, one NVIDIAH100 GPU (96 GB GPU memory), and 480 GB CPU memory. Our implementation is based on theopen-source ,-CROWN verifier1 with cutting plane related code added. All experiments use 24CPU cores and 1 GPU. The MIP cuts are acquired by the cplex solver (version 22.1.0.0). We use the Adam optimizer to solve both , , , . For the SDP-FO benchmarks, we optimizethose parameters for 20 iterations with a learning rate of 0.1 for and 0.02 for , , . We decaythe learning rates with a factor of 0.98 per iteration. The timeout is 200s per instance. For theVNN-COMP benchmarks, we use the same configuration as ,-CROWN used in the respectivecompetition and the same timeouts. During constraint strengthening, we set drop_percentage =50%. We only perform one round of strengthening, with no recursive strengthening attempts. Weperform constraint strengthening for the first 40 BaB iterations. For multi-tree search, we perform 5branching iterations, where we each time pick the current best 50 domains and split them to generate400 new sub-domains.",
  "C.2Ablation Studies: Number of Cuts and Branch Visited Analysis": "The ablation studies provide insights into the impact of different components of the BICCOS algorithmon the verification process, focusing on the average number of branches (domains) explored and thenumber of cuts generated, as shown in . A lower number of branches typically indicates amore efficient search process, leading to computational savings and faster verification times. Across the benchmarks, we observe that the BICCOS configurations, particularly BICCOS (auto),often explore fewer branches compared to the baseline -CROWN and GCP-CROWN methods.For instance, on the CIFAR CNN-A-Adv model, BICCOS (auto) reduces the average number ofbranches from 63,809.77 (for -CROWN) and 28,558.70 (for GCP-CROWN) down to 9,902.96. Thisreduction is attributed to the cutting planes introduced by the BICCOS algorithm, which tighten therelaxations and more effectively prune suboptimal regions of the search space. The multi-tree search (MTS) strategy, which explores multiple branch-and-bound trees in parallel,demonstrates its effectiveness in improving verification efficiency. While MTS may increase thetotal number of branches exploredsince domains from multiple trees are countedthe explorationwithin each tree is optimized, leading to faster convergence and reduced overall computation time. Wealso design an adaptive BICCOS configuration (BICCOS (auto)), which automatically enables MTSand/or Mixed Integer Programming (MIP)-based cuts based on the neural network and verificationproblem size. BICCOS (auto) achieves the highest verified accuracies across models and is used asthe default option of the verifier when BICCOS is enabled.",
  "* We run our BICCOS in different ablation studies with a shorter 200s timeout for all models and compare it to -CROWN and GCP-CROWN, it achieves better verified accuracythan all other baselines": "However, it is worth noting that on certain benchmarks, such as the CIFAR CNN-A-Mix-4 model, thenumber of branches explored by different BICCOS configurations does not always decrease comparedto the baselines. In some cases, the base BICCOS version explores more branches than -CROWN.This could be attributed to the models inherent complexity and the nature of the verification problem,where the benefits of the cutting planes and MTS may be less pronounced. Overall, the analysis of the average number of branches in the ablation studies highlights theeffectiveness of the BICCOS algorithm and its components in improving the efficiency of theverification process. The combination of cutting planes, multi-tree search, and other optimizationsenables BICCOS to achieve high verified accuracy while often exploring fewer branches compared tobaseline methods, ultimately leading to computational savings and faster verification times.",
  "C.3Comparison with MIP based Verifier": "To evaluate the performance of the BICCOS cuts in MIP-based solvers, Fig 3 shows comparisonwith Venus2 . We emphasize on making a fair comparison on the strengths of cuts. SinceVenus2 uses an MILP solver to process its cuts, in these experiments we do not use the efficientGCP-CROWN solver. Instead, we also use an MILP solver to handle the BICCOS cuts we found.This ensures that the speedup we achieve is not coming from the GPU-accelerated GCP-CROWNsolver. Since our cut generation relies on the process with BaB, we first run BICCOS to get the cuts,and then import the cuts into the MILP solver. We note that Venus uses branching over ReLU activations to create multiple strengthened MILPproblems. On the other hand, we only create one single MILP and do not perform additionalbranching. Therefore, our MILP formulation is weaker. The fact that we can outperform Venus2anyway underlines the strength of the generated cuts by BICCOS.",
  "C.4Overhead Analysis": "We note that the multi-tree search (MTS) and cut strengthening procedure incur additional overheadscompared to the baseline verifiers shown as Fig 4. The MTS has about 2 - 10 seconds overhead in thebenchmarks we evaluated, which may increase the verification time of very easy instances that canbe sequentially immediately verified in branch and bound. The BICCOS cut strengthening is calledduring every branch-and-bound iteration, but its accumulative cost is about 3 - 20 seconds for hardinstances in each benchmark, which run a few hundred branch-and-bound iterations. We note thattypically, the performance of the verifier is gauged by the number of verified instances within a fixedtimeout threshold, thus when the threshold is too low, the added cutting planes may not have time toshow their performance gains due to the lack of sufficient time for the branch-and-bound procedure. We note that such a shortcoming is also shared with other verifiers which require additional steps forbound tightening; for example, the GCP-CROWN verifiers have the extra overhead of a few secondsof calling and retrieving results from the MIP solver, even when the MIP solver is running in parallelwith the verifier. The final verified accuracy can justify whether the overhead is worth paying.",
  "Furthermore, nogood learning and conflict analysis techniques from constraint programming and SATsolving have been adapted to neural network verification to improve solver efficiency [18, 45, 26, 19,": ": Comparison of Venus2 and MILP with BICCOS. For a fair comparison, we do not use GPU-accelerated bound propagation but use a MILP solver (same as in Venus2) to solve the verificationproblem with our BICCOS cuts. In all 4 benchmarks, MILP with BICCOS cuts is faster than Venus(MILP with their proposed cuts), illustrating the effectiveness. Note that Venus2 can hardly scale tolarger models presented in our paper, such as those on cifar100 and tinyimagenet datasets. 11]. These methods record conflicts encountered during the searchknown as nogoodsto preventrevisiting the same conflicting states, effectively pruning the search space. While nogood learningcan reduce redundant exploration, it typically operates on discrete abstractions of neural networks,representing ReLU activations as boolean variables. This abstraction limits their ability to leveragethe properties of neural network activations. In contrast, our approach not only identifies conflicts butalso translates them into cutting planes within the continuous relaxation of the verification problem.This allows us to tighten the bounds for unexplored subproblems, enhancing the overall efficiency ofthe verification process. Mixed Integer Programming (MIP) solvers, such as cplex and gurobi , have also beenapplied to neural network verification by formulating the problem as a mixed-integer linear pro-gram . These solvers inherently utilize branch-and-bound algorithms and incorporate sophisticatedcutting plane generation techniques to tighten the feasible region. However, in practice, applyinggeneral-purpose MIP solvers to large-scale neural networks presents significant challenges. Forinstance, in our experiments with benchmarks like CIFAR100 and Tiny-ImageNetwhere networksizes range from 5.4 million to over 30 million parameterscplex was unable to solve the initial LPrelaxation within the 200-second timeout threshold. As a result, the branch-and-bound process andcut generation did not commence, underscoring the limitations of using off-the-shelf MIP solvers forlarge-scale neural network verification tasks. In terms of cutting plane methods, prior work has explored integrating cuts derived from convex hullconstraints within MIP formulations of neural networks . For example, propose cutting planes that tighten the relaxation of ReLU constraints in MILP formulations, im-proving solution quality. While these methods can enhance optimization, they often incur significantcomputational overhead in generating and integrating cuts and rely heavily on the capabilities ofthe underlying MIP solver. In contrast, our approach develops new cutting planes that can be effi-ciently constructed and strengthened during the branch-and-bound process, leveraging the problemsstructure to achieve better scalability and performance. : Slow down comparison on easier properties on BICCOS (base), BICCOS (MTS), GCP-CROWN with MIP cuts and BICCOS (auto) . We plot the number of solved instances versusverification time for BICCOS and -CROWN (baseline). For easy instances that can be verifiedwithin a few seconds (bottom parts of the figures), the increase of verification time with BICCOS isnegligible. Our work bridges the gap between conflict analysis methods like nogood learning and cutting planetechniques by integrating conflict-derived constraints into the continuous relaxation of the verificationproblem. This integration allows for both effective pruning of the search space and tightening ofthe problem relaxation, leading to improved verification efficiency. By tailoring our approach to thespecifics of neural network verification, we address the limitations observed in general-purpose MIPsolvers when applied to large-scale networks, offering a scalable and efficient solution.",
  "ELimitations": "The approach relies on the piecewise linear nature of ReLU networks (same as most other papers inBaB), and its applicability to other types of neural architectures requires further extension such asthose in . Additionally, the effectiveness of the generated cutting planes may vary depending onthe specific structure and complexity of the neural network being verified, and there may be caseswhere the algorithms performance gains are less pronounced."
}