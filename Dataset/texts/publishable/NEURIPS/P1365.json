{
  "Abstract": "Tiny machine learning (TinyML) aims to run ML models on small devices andis increasingly favored for its enhanced privacy, reduced latency, and low cost.Recently, the advent of tiny AI accelerators has revolutionized the TinyML field bysignificantly enhancing hardware processing power. These accelerators, equippedwith multiple parallel processors and dedicated per-processor memory instances,offer substantial performance improvements over traditional microcontroller units(MCUs). However, their limited data memory often necessitates downsamplinginput images, resulting in accuracy degradation. To address this challenge, wepropose Data channel EXtension (DEX), a novel approach for efficient CNNexecution on tiny AI accelerators. DEX incorporates additional spatial informa-tion from original images into input images through patch-wise even samplingand channel-wise stacking, effectively extending data across input channels. Byleveraging underutilized processors and data memory for channel extension, DEXfacilitates parallel execution without increasing inference latency. Our evalua-tion with four models and four datasets on tiny AI accelerators demonstrates thatthis simple idea improves accuracy on average by 3.5%p while keeping the in-ference latency the same on the AI accelerator. The source code is available at",
  "Introduction": "Tiny machine learning (TinyML) is an active research field focused on developing and deployingmachine learning models on extremely resource-constrained devices, such as microcontroller units(MCUs) and small IoT sensors. Compared to cloud-based AI, TinyML on devices offers benefits inprivacy preservation, low latency, and low cost. While research efforts in TinyML, such as modelcompression techniques , have successfully reduced the size of AI models tofit into memory-constrained MCUs, the fundamental limitation in the processing capability of MCUsleads to long inference latency. This limitation hinders the widespread adoption of on-device AI,especially for real-time applications. Recently, the advent of tiny AI accelerators like the Analog Devices MAX78000 and GoogleCoral Micro has revolutionized the TinyML field by dramatically boosting the model inferencespeed and leading a new phase of on-device AI. For instance, the MAX78000 AI accelerator achieves 170 faster inference latency compared to an MCU processor (MAX32650 ).",
  ":Comparison between an AI accelerator(MAX78000) and MCUs (MAX32650 and STM32F7)": "and parallelize per-channel CNN operations across these processors. For further optimization, thememory architecture allows each processor to have a dedicated memory instance, i.e., per-processormemory instance. This design enables simultaneous memory access to multiple channels fromdifferent processors. While these hardware-level optimizations bring significant performance im-provements, we found that they also have several constraints at the expense of the optimizations.First, the per-processor memory architecture highly restricts the supported input image size becausethe data memory each processor can use for its input/output channels is limited to the capacity of itsdedicated memory instance, which is a fraction of the total data memory divided by the number ofprocessors. Consequently, most vision models for these accelerators are designed to support verysmall images, such as 3232 pixels. Given that images captured by cameras are often generated withhigher resolutions, downsampling is inevitable, leading to accuracy degradation due to informationloss from the original image. Second, we found that processors and data memory are underutilized forthe input layer due to the per-processor memory architecture; since input images typically have a lownumber of channels (e.g., RGB three channels), only a limited number of processors tied to memoryinstances are utilized while the remaining processors remain idle. For instance, on the MAX78000,61 of 64 processors and per-processor memory instances remain unused in the first layer. In this work, we propose a novel approach, Data channel EXtension (DEX), to overcome theseconstraints while still benefiting from the acceleration power of tiny AI accelerators. The core ideais to boost accuracy by extending the data channels to incorporate additional image informationinto unused data memory instances and processors, instead of simple downsampling. Owing to theparallel processing and memory access capabilities of tiny AI accelerators, our method can achievethis accuracy improvement without compromising inference latency. Specifically, DEX involves twoprocedures: (1) pair-wise even sampling, where pixels from the original image are evenly sampled,and (2) channel-wise stacking, which arranges these samples across multiple channels. To measure the impact of DEX on accuracy and resource utilization, we conducted experiments onthe MAX78000 and MAX78002 tiny AI accelerator platforms. DEX was evaluated on fourmodels, SimpleNet , WideNet , EfficientNetV2 , and MobileNetV2 , using four visiondatasets: ImageNette , Caltech101 , Caltech256 , and Food101 . Our results showthat DEX improves average accuracy by 3.5%p compared to the original model with downsamplingand 3.6%p compared to the existing coordinate augmentation approach (CoordConv ), withoutincreasing inference latency. Additionally, DEX maximizes data memory and processor utilization,demonstrating its effectiveness in enhancing model performance on resource-constrained devices. Insummary, DEX can significantly enhance the performance of neural networks on tiny AI accelerators,leading to more efficient and effective deployment of AI on resource-constrained devices.",
  "Preliminary: tiny AI accelerators": "The advent of tiny AI accelerators marks a pivotal shift towards on-device AI, greatly enhancingprivacy and reducing latency. While a number of tiny-scale AI accelerators have emerged recently,such as Analog Devices MAX78000/MAX78002 , Google Coral Micro , and GreenWavesGAP-8/GAP-9 , only a few are commercially available with access and control over theiroperations. In this paper, we focus on the MAX78000 and MAX78002 as our primaryplatforms since they are the most widely used tiny AI accelerator research platforms owing to the disclosed hardware details and open-source tools, enabling in-depth analysis andmodification of their operations.",
  ": Processor utilization with vary-ing input channels on the AI accelerator": "Architecture of tiny AI accelerators.The distinctivecharacteristic of tiny AI accelerators compared to conven-tional microcontroller units (MCUs) is parallel processorsthat parallelize per-channel CNN operations across theseprocessors. depicts an abstracted architectureof the MAX78000; MAX78002 has a similar architec-ture to MAX78000 with increased memory (1.3 MB dataand 2 MB weight memory). Further details are in Ap-pendix A.1. It has 64 parallel convolutional processors,each capable of performing specific operations indepen-dently. To maximize performance, each processor has adedicated memory instance, i.e., per-processor memoryinstance that optimizes data transfer with parallel access.For each CNN layer, operations on individual channels areassigned to separate convolutional processors and executedsimultaneously, significantly reducing latency typically associated with convolutional algorithms.Each processor has a pooling engine, an input cache, and a convolution engine that can handle 3 by3 kernels. The CNN accelerator includes 512 KB of data memory and 432 KB of weight storagememory. Within the 512 KB of data memory, an 8 KB per-processor memory instance is allocated toeach of the 64 processors. shows the utilization of the processors (Pri) for executing CNNswith varying sizes of the input channels. Each processor communicates with a dedicated memoryinstance for each data channel. For example, given a three-channel image, three parallel processorsare utilized in the first layer. Performance gain over MCUs.A recent benchmark study demonstrates the remarkableperformance gain of the MAX78000 in terms of latency and energy consumption. shows thatthe MAX78000 significantly outperforms widely-used MCUs (MAX32650 with a Cortex-M4 at 120MHz , and a high-performance MCU, the STM32F7 with a Cortex-M7 at 216 MHz ) for facedetection (FaceID) and keyword spotting (KWS). For KWS, latency is drastically reduced to only2.0 ms, compared to 350 ms for the MAX32650 and 123 ms for the STM32F7. Accordingly, energyefficiency of the MAX78000 is also significant; it consumes only 0.40 mJ for FaceID, dramaticallyless than the 42.1 mJ and 464 mJ required by the MAX32650 and STM32F7, respectively.",
  "Constraints of per-processor memory instances in tiny AI accelerators for images": "As mentioned in 2, tiny AI accelerators leverage per-processor memory instances for faster datatransfer with parallel access. However, we disclose that this causes several constraints at the expenseof rapid data access: (1) low image resolution and (2) underutilized processors and data memory. Low image resolution due to limited per-processor memory size.MAX78000 has 512KB data memory which is divided into 64 segments of 8 KB memory instances per processor, eachstoring the data of each input channel. This memory architecture highly restricts the supportedinput resolution. For instance, an input image with a shape 3 224 224 (channel, height, andweight), which is a typical size of ImageNet , does not fit the MAX78000 even with Q7 format(one byte for each value), as memory limit for each channel is 8 KB (224 224 50 KB > 8KB). Thus, the current practice on tiny AI accelerators is to shrink the resolution of input imagesby downsampling and accordingly, to design small models to process lower-resolution images, e.g.,3 32 32. However, with this, it loses most of the information of the original image, which mightlead to sub-optimal performance. Underutilized processors and data memory for the input layer.Although per-processor memoryinstances allow simultaneous memory access from different processors, it also brings inefficiencyin data memory and processor utilization, especially in the input layer. Specifically, given an inputimage I with the number of channels CI, height HI, and width WI (e.g., 3 224 224) as shownin (a), (b) illustrates the downsampled image with the number of channels CI,height HO, and width WO (e.g., 3 32 32), and its data memory usage in the AI accelerator.",
  "!\"$ !\"% !\"#!\"'!\"\"": ": Comparison among different input data. (a) an original image that exceeds the data memorylimit of the AI accelerator, (b) a downsampled image that fits the data memory but does not fullyutilize parallel processors and data memory, and (c) a DEX-generated image that incorporates moreinformation from original image by extending data across channels with full utilization of parallelprocessors and data memory instances.",
  ": Overview of DEX. DEX divides the original image I into multiple patches. DEX thenevenly samples pixels from each patch Pij and constructs an output pixel Oij by stacking samplesacross channels": "With three RGB channels, channel data are separately stored for each data memory instances forparallel execution. As there is N processors and corresponding data memory instances, it leaves theremaining N 3 processors and data memory instances idle. This provides an opportunity to utilizethese idle data memory instances and parallel processors, which we detail in the following section.",
  "DEX procedure": "As aforementioned, we note two key observations: (1) the input image needs to be downsampled dueto the limited memory of tiny AI accelerators, which means most of the pixel information cannotbe utilized, and (2) there exist idle data memory instances and processors that could process up toN channels in parallel. Although several recent studies have found efficient model architectures ontiny AI accelerators , existing studies lack considerations on this inefficiency for input imageprocessing in CNNs (further discussion on related work is in Appendix 5). Based on our observations, we propose Data channel EXtension (DEX) for efficient CNN executionon tiny AI accelerators. The key intuition behind DEX is that we can utilize the remaining datamemory to incorporate additional information from the original image into neural networks byextending the input data across channels. By utilizing this additional memory and processors, wecan incorporate extra sample information for feature learning without sacrificing latency. (c)shows the input data reshaped via DEX, where each channel contains different pixel information fromthe original image. With DEX extending data across channels (from CI to CO), it can fully utilizethe data memory and associated parallel processors. shows an overview of the procedureof DEX. Given an input image I with a number of channels CI, height HI, and width WI, DEXgenerates an output image O with an extended number of channels CO, height HO, and width WO(e.g., 64 32 32) via patch-wise even sampling and channel-wise stacking.",
  ",(1)": "where [:, :] refers to a 2-D array slicing operation, specifying the selection of rows and columnssequentially. The number of patches is determined by the resolution of the output image, i.e.,HO WO. For each patch Pij, we generate the corresponding output data Oij. This ensures that thespatial relationships among pixels in the input image are preserved in the output, maintaining spatialconsistency throughout the process.",
  ", for k = 0, 1, . . . , K 1,": "(2)which means a collection of evenly distributed samples within each patch to encourage diverseinformation while minimizing the use of localized pixel information. With patch-wise even sampling,selected samples are evenly distributed both across patches and within each patch. Channel-wise stacking.Channel-wise stacking arranges sampled data across multiple channelsand keeps this procedure for all pixels to maintain data integrity. Channel-wise stacking is beneficialas it maintains consistency within each channel, preserving the spatial and contextual relationships ofthe sampled data. Specifically, after patch-wise even sampling, the samples are stacked across thechannel axis in ascending order of the index k, and this is repeated for each Oij. Note that lk = 0when K = 1, and this is identical to traditional downsampling. If K > CO CI , it fills up the targetchannel with Ps data until the limit and discards the remaining channels. For instance, when usingRGB channels (CI = 3) and if CO = 64 and K = 22, it takes only the red channel for i = 21 anddiscards the remaining green and blue channels that exceed the channel limit of 64. Algorithm 1provides the pseudo-code that describes the procedure of DEXs channel extension algorithm.",
  ": The initial CNN layers operation with DEX": "Understanding how DEX leads to per-formance improvement.DEXs abilityto incorporate additional pixel informationfrom the original image can improve theaccuracy of CNNs. The extended chan-nels provide further samples of adjacentareas in the original image, significantlybroadening the receptive fields of featuresin the initial CNN layer. This expansionallows the model to detect more complexand subtle features early in the processingpipeline, which is critical for the nuancedunderstanding and interpretation of visualdata. Specifically, visualizes howthe first CNN layer operates with DEX,where L1kernel_size and L1c_out refer to thekernel size and the output channel size ofthe first layer, respectively. It illustrates the application of the convolution operation across eachenhanced channel (CO as opposed to CI), where distinct kernel weights are applied to each channel.This ensures that the additional information is integrated into the output feature maps, thereby en-riching the models feature extraction capabilities. The convolutional layer processes the increasedchannel input, which is reflected in weight sums that construct output channels. Impact of channel extension on the number of parameters.Given the first CNN layers kernelsize L1kernel_size and the first layers channel output size L1c_out, the number of parameters requiredfor the input layer can be calculated as OC L1kernel_size L1c_out. If OC is 3, it is the same as thetraditional downsampling without our channel extension. Note that this channel extension doesnot incur additional inference latency on the AI accelerator. We found that the channel extensionincreases 3% of the total parameters as we show in our experiment 4.2. The rest of the layersremain the same. In addition to its simplicity, we have several reasons to change the first layer only,which we discuss further in 6.",
  "CI HOWO": "HIWI . For instance, given a3 256 256 input image, a downsampled image 3 32 32 utilizes only 1.6% of the originalinformation, while with DEX and an output channel size CO = 64, it can utilize 33.3% of the originalinformation. DEX can accommodate all the information when CO = CI HIWI",
  "HOWO": "Maximum number of output channels.Increasing the number of output channels allows DEXto accommodate the original image information. The number of output channels denoted as OC,that can be extended without increasing latency on AI accelerators is limited by the number of datamemory instances DN, i.e., OC < DN. For example, the MAX78000 has 64 data memory instances,allowing it to support up to OC = 64 output channels without affecting inference latency.",
  "Here we explain experimental settings. Further details are in Appendix A": "On-device testbed.We evaluated DEX on the off-the-shelf MAX78000 feather board andMAX78002 Evaluation Kit , which are a development platform for the MAX78000 andMAX78002 , respectively, as shown in . In this paper, we select these accelerators becausethey provide open-source tools for thorough analysis and modification of their internal processes,making them the most widely used tiny AI accelerator research platforms .",
  "Downsampling16.0 0.417.7 0.712.1 0.222.4 0.617.1CoordConv16.1 0.817.7 0.312.0 0.121.7 0.316.9CoordConv (r)16.3 0.417.3 0.612.0 0.620.9 0.316.6DEX (ours)18.4 0.420.9 0.416.4 0.123.3 1.119.8": "EfficientNetV2 , and MobileNetV2 . The supported models from the framework were trainedvia quantization-aware training with 8-bit integers in PyTorch . We follow the official trainingconfiguration (details in Appendix A.2). The checkpoints are synthesized as embedded C codes forvia the Analog Devices MAX78000/70002 Synthesis framework . SimpleNet and WideNet aredeveloped for MAX78000 while EfficientNetV2 and MobileNetV2 are for MAX78002 consideringthe size of the models. All models are originally designed to take 3 32 32 inputs, and DEXincreases the number of the channels in the first layer to 64. Datasets.We evaluated on four common vision datasets: (1) ImageNette , a ten-class subsetof ImageNet with 9469/3925 train/test samples with the original image shape of 3 350 350,(2) Caltech101 with 101 objects classes having 6941/1736 train/test samples with the originalimage shape of 3 300 300, (3) Caltech256 with 256 objects classes having 23824/5956train/test samples with the original image shape of 3 300 300, and (4) Food101 with 101 foodcategories with 75750/25250 train/test samples with the original image shape of 3 512 512. Baselines.For baselines, we compare with the Downsampling method which is a straightforwardway to reduce the size of the input under memory-constrained devices. It downsamples the inputimage to 3 32 32. In addition, we compare DEX with CoordConv which pointed out thelimitation of traditional CNNs that relied on RGB images for the coordinate transformation problemand introduced the augmentation of i and j coordinates, which improved object detection efficiencyby using two extra channels. The authors of CoordConv also introduced the third channel for an rcoordinate, where r =",
  "Result": "Overall accuracy. shows the overall accuracy for four different datasets with the baselinesand DEX. As shown, extending data channels to utilize additional input information improvesaccuracy in DEX. Specifically, DEX achieved 3.5%p higher accuracy compared to downsampling and3.6% higher accuracy compared to CoordConv across datasets. CoordConv shows lower accuracycompared with downsampling (0.1%p degradation on average), showing they are not very effectivesolutions. This finding aligns with previous results indicating that CoordConv is useful for specifictasks such as object detection, where coordinate information is important . We found CoordConv(r) has a similar pattern to CoordConv. Overall, DEXs accuracy improvement shows the effectivenessof using extra information from the original image for feature learning.",
  ": Accuracy of DEX varying the channel size. The shaded areas are standard deviations": "model size is negligible (an average increment of 3.2% compared to no channel extension). DEXutilizes 21.3 more image information compared to downsampling, which is the primary reason forthe accuracy improvement. As expected, DEX does not increase on-device inference latency, eventhough it maximally utilizes the processors on the AI accelerators for information processing. Thisresult is consistent across the four datasets, as all the models are designed to take the same input sizein MAX78000 and MAX78002. Accuracy according to the channel size.We varied the size of the channels from 3 (downsampling)to 6, 18, 36, and 64 with DEX to understand the impact of the channel size in terms of accuracy. shows the accuracy variation according to the channel size across the four datasets. Asshown, it seems that a higher number of channels increases accuracy in general. This means thatselecting the highest channel size supported in AI accelerators might be an effective strategy inpractice, considering that it does not incur the latency increase. Still, there are some cases where theaccuracy of the highest channel size (64) is not the best among them. This means there might be anoptimal number of channels tailored to a specific dataset and model architecture, which might befound in the model development process. Resource usage according to the channel size.We also measure resource usage varying thechannel size. First, we measured the model size and inference latency as shown in . Themodel size increment is negligible and inference latency remains the same across different numbers ofchannels. The model size and inference latency are the same for the four datasets as all the models aredesigned to take the same input size in MAX78000 and MAX78002. Second, we measure the infor-mation utilization from the original image and processor utilization in the AI accelerators (). : Model size (Size) with relative increment (%) compared to the three channels and averageinference latency on the accelerator (Latency) with standard deviations over three runs, varying thechannel size.",
  ": Resource usage varying the channel size": "The utilization of the original image infor-mation depends on the size of the orig-inal data size, which grows linearly ac-cording to the channel size. We found acorrelation between information utilizationrate and accuracy improvement. For ex-ample, Caltech101 and Caltech256 had uti-lization rates of 24.3%, improving accuracyby 4.2%p and 3.2%p, respectively, whileFood101 had an 8.3% utilization rate with a2.7%p accuracy improvement. The proces-sor utilization linearly increases until 100%with 64 channels size, which is the number of parallel processing units in the evaluated platforms.",
  "Downsampling31.057.8 1.2Repetition641.056.3 0.8Rotation641.055.4 0.7Tile per channel6421.339.4 0.7Patch-wise seq.6421.361.0 1.5Patch-wise rand.6421.360.4 1.0DEX6421.361.4 0.6": "Comparison of alternative data extensionstrategies in DEX.To understand the effec-tiveness of our patch-wise even sampling andchannel-wise stacking, we compared DEX withother possible data channel extension strategies.We compared with four strategies: repeatingthe same downsampled image across the chan-nels (Repetition), generating slightly differentimages through rotation (Rotation), dividing theoriginal image into multiple tiles and stackingthose tiles across channels (Tile), patch-wise sequential sampling (Patch-wise seq.) that samplespixels sequentially within a patch, and patch-wise random sampling (Patch-wise rand.) that randomlysamples within a patch. Further implementation details are in Appendix A.5. In this experiment,we used SimpleNet and evaluated it on ImageNette. shows the results. Repetition does notimprove accuracy over downsampling, indicating that merely increasing the number of kernels doesnot lead to performance gains. Rotation shows a slight decrease in accuracy compared to Repetition,which suggests that slight changes through rotation do not enhance performance. Interestingly, Tileshows low accuracy, demonstrating the importance of having a complete view of the original imagein each channel, rather than focusing on specific regions. Both Patch-wise sequential and Patch-wiserandom samplings show lower accuracy than DEXs patch-wise even sampling, highlighting theimportance of even sampling for better performance.",
  "Related work": "TinyML.Tiny Machine Learning (TinyML) is an emerging field that focuses on adapting machinelearning techniques for highly resource-constrained devices, such as microcontroller units (MCUs).These devices often come with limited memory, typically hundreds of kilobytes of SRAM. Researchin this area has mostly concentrated on reducing model size through various compression techniques,such as model pruning , model quantization ,and neural architecture search (NAS) . In addition, several studies have explored the efficient utilization of memory resources (e.g., SRAM). Examples include optimizing on-devicetraining processes and designing memory-efficient neural architectures . Unlikethese approaches that primarily target MCUs, our research utilizes the distinctive architecture of tinyAI accelerators to enhance both memory efficiency and overall performance. Tiny AI accelerators.Several studies have leveraged tiny AI accelerators for small-scale on-deviceAI applications. For instance, TinyissimoYOLO offers a quantized, memory-efficient, andultra-lightweight object detection network, showcasing its effectiveness on the MAX78000 plat-form. Additionally, KP2DTiny introduces a quantized neural keypoint detector and descriptorspecifically optimized for MAX78000 and Coral AI accelerators. Moreover, Synergy representsa multi-device collaborative inference platform across wearables equipped with tiny AI accelera-tors . Another line of studies utilized tiny AI accelerators in battery-free or intermittent computingscenarios . Traditionally, hardware accelerators on low-power AI platforms were capable ofonly one-shot atomic executions of a neural network inference without intermediate result backups.A study proposed a toolchain to address this that allows neural networks to execute intermittently onthe MAX78000 platform . To the best of our knowledge, there has been no work that manipulatesdata and models to efficiently utilize computing resources considering the unique architecture of tinyAI accelerators. Image channel extension in CNNs.Several studies have explored augmenting images with ad-ditional information to construct multi-channel inputs for Convolutional Neural Networks (CNNs).Liu et al. proposed a multi-modality image fusion approach, combining visible, mid-wave infrared,and motion images for enhanced object detection , while Wang et al. presented depth-awareCNN for image segmentation . These approaches require extra sensing channels to acquire data,such as infrared cameras and depth cameras. Similarly, other research has incorporated locationdata to improve performance for segmentation and object detection tasks . For instance,CoordConv pointed out the limitation of traditional CNNs that relied solely on RGB images forthe coordinate transformation problem and introduced the augmentation of i and j coordinates, whichimproved object detection efficiency. However, these methodologies often necessitate additionalsensor modalities or are tailored for specific applications such as object detection, which restricts theirgeneral use. Nevertheless, adapting findings from those studies within DEX could be an interestingfuture direction.",
  "Discussion and conclusion": "We introduced DEX, a novel method to enhance CNN efficiency on tiny AI accelerators by augmentinginput data across unused memory. Evaluations on four image datasets and models showed that DEXimproves accuracy without increasing inference latency. This method maximizes the processing andmemory capabilities of tiny AI accelerators, making it a promising solution for efficient AI modelexecution on resource-constrained devices. Limitations and potential societal impacts.We modified only the initial CNN layer due tosimplicity, effectiveness, and memory constraints. The first layer, representing image data in threechannels (RGB), has the most unused processors after initial data assignment. Extending channelsat the first layer significantly increases data utilization with minimal impact on model size. Thisapproach aligns with the design of weight memory in tiny AI accelerators, which maximizes modelcapacity by collective use across processors. We think DEX might be less effective in certaintasks where incorporating more pixel information is not beneficial. In those cases, alternative dataextension strategies might be used instead of patch-wise even sampling to utilize the additional channelbudget. While our focus was on small models supported by the MAX78000/MAX78002 platforms,evaluating larger models could be valuable, given rapid AI hardware advancements. Regardingsocietal impact, leveraging additional processors and memory to improve accuracy might increasecarbon emissions , highlighting the need to balance accuracy improvements with environmentalsustainability. Abu Bakar, Rishabh Goel, Jasper de Winkel, Jason Huang, Saad Ahmed, Bashima Islam,Przemysaw Paweczak, Kasm Sinan Yldrm, and Josiah Hester. Protean: An energy-efficientand heterogeneous platform for adaptive and hardware-accelerated battery-free computing. InProceedings of the 20th ACM Conference on Embedded Networked Sensor Systems, pages207221, 2022.",
  "Han Cai, Ligeng Zhu, and Song Han. Proxylessnas: Direct neural architecture search on targettask and hardware. In International Conference on Learning Representations, 2019": "Luca Caronti, Khakim Akhunov, Matteo Nardello, Kasm Sinan Yldrm, and Davide Brunelli.Fine-grained hardware acceleration for efficient batteryless intermittent inference on the edge.ACM Transactions on Embedded Computing Systems, 22(5):119, 2023. Jungwook Choi, Zhuo Wang, Swagath Venkataramani, Pierce I-Jen Chuang, VijayalakshmiSrinivasan, and Kailash Gopalakrishnan. Pact: Parameterized clipping activation for quantizedneural networks. arXiv preprint arXiv:1805.06085, 2018.",
  "Google Coral Micro. Accessed: 20May. 2024": "Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and PatternRecognition, pages 248255, 2009. Igor Fedorov, Ryan P Adams, Matthew Mattina, and Paul Whatmough.Sparse: Sparsearchitecture search for cnns on resource-constrained microcontrollers. Advances in NeuralInformation Processing Systems, 32, 2019. Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few trainingexamples: An incremental bayesian approach tested on 101 object categories. Computer Visionand Pattern Recognition Workshop, 2004.",
  "Gregory Griffin, Alex Holub, and Pietro Perona. Caltech-256 object category dataset. 2007": "Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neuralnetworks with pruning, trained quantization and huffman coding. International Conference onLearning Representations (ICLR), 2016. Seyyed Hossein Hasanpour, Mohammad Rouhani, Mohsen Fayyaz, and Mohammad Sabokrou.Lets keep it simple, using simple architectures to outperform deeper and more complex archi-tectures. arXiv preprint arXiv:1608.06037, 2016.",
  "Young D Kwon, Rui Li, Stylianos I Venieris, Jagmohan Chauhan, Nicholas D Lane, andCecilia Mascolo. Tinytrain: Deep neural network training at the extreme edge. arXiv preprintarXiv:2307.09988, 2023": "Edgar Liberis, ukasz Dudziak, and Nicholas D Lane. nas: Constrained neural architecturesearch for microcontrollers. In Proceedings of the 1st Workshop on Machine Learning andSystems, pages 7079, 2021. Edgar Liberis and Nicholas D Lane. Differentiable neural network pruning to enable smartapplications on microcontrollers. Proceedings of the ACM on Interactive, Mobile, Wearableand Ubiquitous Technologies, 6(4):119, 2023.",
  "Shuo Liu and Zheng Liu. Multi-channel cnn-based object detection for enhanced situationawareness. arXiv preprint arXiv:1712.00075, 2017": "Zechun Liu, Haoyuan Mu, Xiangyu Zhang, Zichao Guo, Xin Yang, Kwang-Ting Cheng, andJian Sun. Metapruning: Meta learning for automatic neural network channel pruning. InProceedings of the IEEE/CVF international conference on computer vision, pages 32963305,2019. Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, and Changshui Zhang.Learning efficient convolutional networks through network slimming. In Proceedings of theIEEE international conference on computer vision, pages 27362744, 2017.",
  "AnalogMAX78002EVKIT. Accessed: 20 May. 2024": "Julian Moosmann, Marco Giordano, Christian Vogt, and Michele Magno. Tinyissimoyolo: Aquantized, low-memory footprint, tinyml object detection network for low power microcon-trollers. In 2023 IEEE 5th International Conference on Artificial Intelligence Circuits andSystems (AICAS), pages 15. IEEE, 2023. Arthur Moss, Hyunjong Lee, Lei Xun, Chulhong Min, Fahim Kawsar, and Alessandro Mon-tanari. Ultra-low power dnn accelerators for iot: Resource characterization of the max78000.In Proceedings of the 20th ACM Conference on Embedded Networked Sensor Systems, pages934940, 2022. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperativestyle, high-performance deep learning library. Advances in neural information processingsystems, 32, 2019. Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenetclassification using binary convolutional neural networks. In European conference on computervision, pages 525542. Springer, 2016. Thomas Regg, Marco Giordano, and Michele Magno. Kp2dtiny: Quantized neural keypointdetection and description on the edge. In 2023 IEEE 5th International Conference on ArtificialIntelligence Circuits and Systems (AICAS), pages 15. IEEE, 2023. Manuele Rusci, Alessandro Capotondi, and Luca Benini. Memory-driven mixed low precisionquantization for enabling deep network inference on microcontrollers. Proceedings of MachineLearning and Systems, 2:326335, 2020. Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen.Mobilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conferenceon computer vision and pattern recognition, pages 45104520, 2018.",
  "Zhenyi Wang and Olga Veksler.Location augmentation for cnn.arXiv preprintarXiv:1807.07044, 2018": "Hong-Sheng Zheng, Yu-Yuan Liu, Chen-Fong Hsu, and Tsung Tai Yeh. Streamnet: Memory-efficient streaming tiny deep learning inference on the microcontroller. Advances in NeuralInformation Processing Systems, 36, 2024. Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou. Dorefa-net:Training low bitwidth convolutional neural networks with low bitwidth gradients. arXiv preprintarXiv:1606.06160, 2016.",
  "ComponentMAX78000 MAX78002": "MCU ProcessorArm Cortex-M4 (100 MHz), RISC-VArm Cortex-M4 (120 MHz), RISC-VFlash Memory512 KB2.5 MBSRAM128 KB384 KBCNN Processor64 parallel CNN processors64 parallel CNN processorsData Memory512 KB1.3 MBWeight Memory432 KB2 MBBias Memory2 KB8 KB In this paper, we focus on the MAX78000 and MAX78002 as our primary platforms sincethey are the most popular research platforms owing to the disclosed hard-ware details and open-source tools, enabling in-depth analysis and modification of their operations. shows our testbed. Note that all operations required for the model inference are doneunder the AI accelerator part (highlighted with the red boxes), while the entire boards are biggerfor development purposes. For on-device deployment and measurement, we used MAX78000 forSimpleNet and WideNet and MAX78002 for EfficientNetV2 and MobileNetV2, following thememory requirement. For the sake of explanation, we assumed each processor is mapped to onememory instance in this paper, although MAX78000/MAX78002 group four data memory instancestogether to communicate with four processors in reality. Our experiments were conducted on theactual implementations. compares MAX78000 and MAX78002.",
  "We followed the official model training code for MAX78000 and MAX78002 platforms . Here,we detail the hyperparameters used in the official training code. We trained models with NVIDIAA40 GPUs": "For all models, quantization-aware training is conducted with support for batch normalization afterconvolutional layers through batch normalization fusing . This fusing operation integrates theeffects of batch normalization directly into the parameters of the preceding convolutional layer byadjusting the weights and bias values. Consequently, after the fusing/folding process, the networkno longer contains any batch normalization layers. Instead, the effects of batch normalization arereflected in the modified weights and biases of the preceding convolutional layers. SimpleNet.SimpleNet was trained for 300 epochs using the Adam optimizer with aninitial learning rate of 0.001 and a batch size of 32. A multi-step learning rate scheduler was usedwith milestones set at epochs 100, 150, and 200, and a multiplicative factor of learning rate decayvalue of 0.25. Quantization-aware training (QAT) was introduced starting at epoch 240. During QAT,a shift quantile of 0.985 was applied to manage activation ranges. The weight precision was primarilyset to 2 bits. However, exceptions were made for certain layers: the 1st convolutional layer utilized8-bit weights, while the 2nd, 11th, 12th, 13th, and 14th convolutional layers used 4-bit weights. WideNet.WideNet was trained for 300 epochs using the Adam optimizer with an initiallearning rate of 0.001 and a batch size of 100. A multi-step learning rate scheduler was used withmilestones set at epochs 100, 150, and 200, and a multiplicative factor of learning rate decay value of0.25. Quantization-aware training (QAT) was introduced starting at epoch 240. During QAT, a shiftquantile of 0.985 was applied to manage activation ranges. The weight precision was primarily set to2 bits. However, exceptions were made for certain layers: the 1st convolutional layer utilized 8-bitweights, while the 2nd, 11th, 12th, 13th, and 14th convolutional layers used 4-bit weights. EfficientNetV2.EfficientNetV2 was trained for 300 epochs using the Adam optimizer with an initial learning rate of 0.001 and a batch size of 100. A multi-step learning rate scheduler wasused with milestones set at epochs 50, 100, 150, 200, and 250 and a multiplicative factor of learningrate decay value of 0.5. Quantization-aware training (QAT) was introduced starting at epoch 210.During QAT, a shift quantile of 0.995 was applied to manage activation ranges. The weight precisionwas primarily set to 8 bits. MobileNetV2.MobileNetV2 was trained for 300 epochs using the stochastic gradient descentoptimizer (SGD) with an initial learning rate of 0.1 and a batch size of 128. A multi-step learningrate scheduler was used with milestones set at epochs 100, 150, 175, and 250 and a multiplicativefactor of learning rate decay value of 0.235. Quantization-aware training (QAT) was introducedstarting at epoch 200. During QAT, a shift quantile of 1.0 was applied to manage activation ranges.The weight precision was primarily set to 8 bits.",
  "A.3Datasets": "ImageNette.Imagenette is a smaller, more manageable subset of ImageNet , containing10 classes. These classes include tench, English springer, cassette player, chain saw, church, Frenchhorn, garbage truck, gas pump, golf ball, and parachute. ImageNette has 9469/3925 train/test sampleswith the original image shape of 3 350 350. All images were normalized with the ImageNetmean (0.485, 0.456, 0.406) and standard deviations (0.229, 0.224, 0.225), and then converted toQ7 format (one byte per data) to support on-device inference with the tiny AI accelerator platforms(MAX78000 and MAX78002). Caltech101.Caltech101 is a dataset composed of images representing objects from 101different categories, in addition to a background clutter category. Each image features a single objectand is labeled accordingly. The number of images per category ranges from approximately 40 to800, resulting in a total of around 8677 images. Caltech101 has 6941/1736 train/test samples and theoriginal image shape of 3 300 300. All images were normalized with the ImageNet mean (0.485,0.456, 0.406) and standard deviations (0.229, 0.224, 0.225), and then converted to Q7 format (onebyte per data) to support on-device inference with the tiny AI accelerator platforms (MAX78000 andMAX78002). Caltech256.Caltech256 built upon its previous version, Caltech101, offering enhancementssuch as larger category sizes, additional and more extensive clutter categories, and increased overalldifficulty. The dataset contains 29780 images across 256 classes after removing the clutter class.Caltech256 has 23824/5956 train/test samples with the original image shape of 3 300 300. Allimages were normalized with the ImageNet mean (0.485, 0.456, 0.406) and standard deviations(0.229, 0.224, 0.225), and then converted to Q7 format (one byte per data) to support on-deviceinference with the tiny AI accelerator platforms (MAX78000 and MAX78002). Food101.Food101 includes 101 food categories, each with 750 images for training and 250images for testing, which is a total of 101000 images. The original images were rescaled to have amaximum side length of 512 pixels. This dataset has 75750/25250 train/test samples and the originalimage with the shape of 3 512 512. All images were normalized with the ImageNet mean (0.485,0.456, 0.406) and standard deviations (0.229, 0.224, 0.225), and then converted to Q7 format (onebyte per data) to support on-device inference with the tiny AI accelerator platforms (MAX78000 andMAX78002).",
  "A.4Baseline details": "Downsampling.Downsampling is a straightforward method that collects samples evenly distributedacross the original image. This approach is equivalent to the case when the number of channels isequal to three in DEX. CoordConv.CoordConv pointed out the limitation of traditional CNNs that relied solely onRGB images for the coordinate transformation problem and introduced the augmentation of i and jcoordinates, which improved object detection efficiency. We referred to the Pytorch implementationof CoordConv2 for implementing this baseline.",
  ": Visulaization of four alternative data extension methods": "Repetition.Repetition ((a)) repeats the same downsampled image across the channelsuntil it reaches the maximum possible number of input channels, which is the same as the number ofdata memory instances (64). Rotation.Rotation ((b)) generates slightly different images through rotating images fromthe downsampled image. It makes rotated images until it reaches the maximum possible numberof input channels. The angle of rotation ranges from -30 to 30 degrees. For instance, given adownsampled three-channel image input and target channel size of 64, it generates rotated imageswith an angle linearly spaced between -30 to 30 degrees.",
  "CI , it finds the nearest square": "number S that is higher than or equal to K, (e.g., S = 52 > 22 when K = 22). The original imageis then divided into equal-sized patches. Each patch is subsequently downsampled to the target size.The downsampled patches are collected and concatenated along the channel dimension, forming anew image with the desired number of channels. If the total number of patches exceeds the targetnumber of channels, the excess patches are discarded. Patch-wise sequential sampling.Patch-wise sequential sampling ((d)) is similar to DEXbut it involves sequential sampling within a patch instead of even sampling. Specifically, it samplesthe first K samples for each patch and follows the same channel-wise stacking procedure in DEX. Patch-wise random sampling.Patch-wise random sampling ((e)) is similar to DEXbut it involves random sampling within a patch instead of even sampling. Specifically, it samplesrandomly-selected K samples for each patch and follows the same channel-wise stacking procedurein DEX.",
  "B.1Overhead of the channel expansion on devices": "Channel expansion latency.The latency of the channel expansion process depends on the pro-cessors computational capability. During our evaluation, we pre-processed data on a powerfulserver, and thus data processing was negligible. We additionally conducted data processing on theultra-low-power MCU processor on the board (Arm Cortex-M4) to understand the data processingoverhead on less-capable devices. We measured the overhead of applying DEX to expand channelsfrom a 3 224 224 image (a typical size for ImageNet) to 64 32 32 (the highest channelexpansion used in our accelerators) on the MAX78002s Arm Cortex-M4 (120MHz). This process took 2.2 ms on the Arm Cortex-M4. In terms of memory, this addition took the SRAMmemory of 62KB (64 32 32 Bytes - 3 32 32 Bytes) on the processor. However, since DEXextends data to a size that the data memory in the AI accelerator can accommodate, this additionalmemory will not be an issue from the AI accelerators perspective. Impact on end-to-end inference performance.Note that the MCU processor and the AI accel-erator are independent processing components that run in parallel. This means that if the inferencelatency on the accelerator is higher than the data processing latency, data can be pre-processed forthe next inference during the current inference (and thus data processing latency can be hidden). Forinference, the inference latency of EfficientNet (11.7ms) is higher than the data processing latency of2.2ms, and thus the inference throughput remains the same under continuous inference. However, this depends on the scenario. The end-to-end impact of data processing latency depends onthe processors computational capability, the dimension of the data, and the size of channel expansion.For instance, in scenarios where data processing is done and transferred in more capable machines(e.g., cloud servers, smartphones, etc.) than the MCU processor on the tiny AI accelerator, the impactof data processing can be even more negligible.",
  "B.2Power consumption": "We measured the power consumption of the inference on MAX78000 by varying the size of thechannel extension with a Monsoon Power Monitor. The result is shown in . As the number ofchannels increased, power consumption increased accordingly. This is because a higher number ofchannels uses more processors in the AI accelerator, leading to increased power consumption.",
  "NeurIPS Paper Checklist": "The checklist is designed to encourage best practices for responsible machine learning research,addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not removethe checklist: The papers not including the checklist will be desk rejected. The checklist shouldfollow the references and precede the (optional) supplemental material. The checklist does NOTcount towards the page limit.",
  "Please provide a short (12 sentence) justification right after your answer (even for NA)": "The checklist answers are an integral part of your paper submission. They are visible to thereviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it(after eventual revisions) with the final version of your paper, and its final version will be publishedwith the paper. The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation.While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided aproper justification is given (e.g., \"error bars are not reported because it would be too computationallyexpensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering\"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, weacknowledge that the true answer is often more nuanced, so please just use your best judgment andwrite a justification to elaborate. All supporting evidence can appear either in the main paper or thesupplemental material, provided in appendix. If you answer [Yes] to a question, in the justificationplease point to the section(s) where related material for the question can be found.",
  "The answer NA means that the paper has no limitation while the answer No means thatthe paper has limitations, but those are not discussed in the paper": "The authors are encouraged to create a separate \"Limitations\" section in their paper. The paper should point out any strong assumptions and how robust the results are toviolations of these assumptions (e.g., independence assumptions, noiseless settings,model well-specification, asymptotic approximations only holding locally). The authorsshould reflect on how these assumptions might be violated in practice and what theimplications would be. The authors should reflect on the scope of the claims made, e.g., if the approach wasonly tested on a few datasets or with a few runs. In general, empirical results oftendepend on implicit assumptions, which should be articulated. The authors should reflect on the factors that influence the performance of the approach.For example, a facial recognition algorithm may perform poorly when image resolutionis low or images are taken in low lighting. Or a speech-to-text system might not beused reliably to provide closed captions for online lectures because it fails to handletechnical jargon.",
  "If applicable, the authors should discuss possible limitations of their approach toaddress problems of privacy and fairness": "While the authors might fear that complete honesty about limitations might be used byreviewers as grounds for rejection, a worse outcome might be that reviewers discoverlimitations that arent acknowledged in the paper. The authors should use their bestjudgment and recognize that individual actions in favor of transparency play an impor-tant role in developing norms that preserve the integrity of the community. Reviewerswill be specifically instructed to not penalize honesty concerning limitations.",
  "If the contribution is a dataset and/or model, the authors should describe the steps takento make their results reproducible or verifiable": "Depending on the contribution, reproducibility can be accomplished in various ways.For example, if the contribution is a novel architecture, describing the architecture fullymight suffice, or if the contribution is a specific model and empirical evaluation, it maybe necessary to either make it possible for others to replicate the model with the samedataset, or provide access to the model. In general. releasing code and data is oftenone good way to accomplish this, but reproducibility can also be provided via detailedinstructions for how to replicate the results, access to a hosted model (e.g., in the caseof a large language model), releasing of a model checkpoint, or other means that areappropriate to the research performed. While NeurIPS does not require releasing code, the conference does require all submis-sions to provide some reasonable avenue for reproducibility, which may depend on thenature of the contribution. For example(a) If the contribution is primarily a new algorithm, the paper should make it clear howto reproduce that algorithm.",
  "(b) If the contribution is primarily a new model architecture, the paper should describethe architecture clearly and fully": "(c) If the contribution is a new model (e.g., a large language model), then there shouldeither be a way to access this model for reproducing the results or a way to reproducethe model (e.g., with an open-source dataset or instructions for how to constructthe dataset). (d) We recognize that reproducibility may be tricky in some cases, in which caseauthors are welcome to describe the particular way they provide for reproducibility.In the case of closed-source models, it may be that access to the model is limited insome way (e.g., to registered users), but it should be possible for other researchersto have some path to reproducing or verifying the results.",
  ". Experimental Setting/Details": "Question: Does the paper specify all the training and test details (e.g., data splits, hyper-parameters, how they were chosen, type of optimizer, etc.) necessary to understand theresults?Answer: [Yes]Justification: Experimental details are in 4 and Appendix A.Guidelines: The answer NA means that the paper does not include experiments. The experimental setting should be presented in the core of the paper to a level of detailthat is necessary to appreciate the results and make sense of them.",
  "Justification: See 4.2. We ran the experiments with three random seems (0,1,2) and reportedthe standard deviations.Guidelines:": "The answer NA means that the paper does not include experiments. The authors should answer \"Yes\" if the results are accompanied by error bars, confi-dence intervals, or statistical significance tests, at least for the experiments that supportthe main claims of the paper. The factors of variability that the error bars are capturing should be clearly stated (forexample, train/test split, initialization, random drawing of some parameter, or overallrun with given experimental conditions).",
  ". Experiments Compute Resources": "Question: For each experiment, does the paper provide sufficient information on the com-puter resources (type of compute workers, memory, time of execution) needed to reproducethe experiments?Answer: [Yes]Justification: See 2, 4, and Appendix A.Guidelines: The answer NA means that the paper does not include experiments. The paper should indicate the type of compute workers CPU or GPU, internal cluster,or cloud provider, including relevant memory and storage.",
  "Question: Does the paper discuss both potential positive societal impacts and negativesocietal impacts of the work performed?Answer: [Yes]Justification: See 6.Guidelines:": "The answer NA means that there is no societal impact of the work performed. If the authors answer NA or No, they should explain why their work has no societalimpact or why the paper does not address societal impact. Examples of negative societal impacts include potential malicious or unintended uses(e.g., disinformation, generating fake profiles, surveillance), fairness considerations(e.g., deployment of technologies that could make decisions that unfairly impact specificgroups), privacy considerations, and security considerations. The conference expects that many papers will be foundational research and not tiedto particular applications, let alone deployments. However, if there is a direct path toany negative applications, the authors should point it out. For example, it is legitimateto point out that an improvement in the quality of generative models could be used togenerate deepfakes for disinformation. On the other hand, it is not needed to point outthat a generic algorithm for optimizing neural networks could enable people to trainmodels that generate Deepfakes faster. The authors should consider possible harms that could arise when the technology isbeing used as intended and functioning correctly, harms that could arise when thetechnology is being used as intended but gives incorrect results, and harms followingfrom (intentional or unintentional) misuse of the technology. If there are negative societal impacts, the authors could also discuss possible mitigationstrategies (e.g., gated release of models, providing defenses in addition to attacks,mechanisms for monitoring misuse, mechanisms to monitor how a system learns fromfeedback over time, improving the efficiency and accessibility of ML).",
  "Guidelines:": "The answer NA means that the paper does not use existing assets. The authors should cite the original paper that produced the code package or dataset. The authors should state which version of the asset is used and, if possible, include aURL. The name of the license (e.g., CC-BY 4.0) should be included for each asset. For scraped data from a particular source (e.g., website), the copyright and terms ofservice of that source should be provided. If assets are released, the license, copyright information, and terms of use in thepackage should be provided. For popular datasets, paperswithcode.com/datasetshas curated licenses for some datasets. Their licensing guide can help determine thelicense of a dataset.",
  "According to the NeurIPS Code of Ethics, workers involved in data collection, curation,or other labor should be paid at least the minimum wage in the country of the datacollector": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with HumanSubjectsQuestion: Does the paper describe potential risks incurred by study participants, whethersuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)approvals (or an equivalent approval/review based on the requirements of your country orinstitution) were obtained?Answer: [NA]Justification: No IRB required.Guidelines:",
  "The answer NA means that the paper does not involve crowdsourcing nor research withhuman subjects": "Depending on the country in which research is conducted, IRB approval (or equivalent)may be required for any human subjects research. If you obtained IRB approval, youshould clearly state this in the paper. We recognize that the procedures for this may vary significantly between institutionsand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and theguidelines for their institution."
}