{
  "Abstract": "Machine unlearning aims to solve the problem of removing the influence of se-lected training examples from a learned model. Despite the increasing attentionto this problem, it remains an open research question how to evaluate unlearningin large language models (LLMs), and what are the critical properties of the datato be unlearned that affect the quality and efficiency of unlearning. This work for-malizes a metric to evaluate unlearning quality in generative models, and uses itto assess the trade-offs between unlearning quality and performance. We demon-strate that unlearning out-of-distribution examples requires more unlearning stepsbut overall presents a better trade-off overall. For in-distribution examples, how-ever, we observe a rapid decay in performance as unlearning progresses. We fur-ther evaluate how examples memorization and difficulty affect unlearning undera classical gradient ascent-based approach.",
  "Introduction": "Training large language models (LLMs) often involves complex data pipelines. These pipelineshandle large quantities of data, some of which might be sensitive. Recently, it has been shownthat LLMs are susceptible to sentence-level membership inference attacks (Gu et al., 2023) andreconstruction attacks (Carlini et al., 2019), meaning that one may be able to infer which data waspart of the training set, or in some cases, even reconstruct partial inputs by interrogating the model.As a result, this raises a prevalent problem of data removal from a trained LLM. To this end, there has been growing interest in formalizing technical definitions of machine unlearn-ing and designing machine unlearning techniques and evaluation metrics (Triantafillou et al., 2023,2024). The goal of machine unlearning is to remove the influence of a subset of the original trainingdata, the forget set, from a corresponding model. A nave way to achieve it is to retrain the modelfrom scratch on an updated training set (the retain set), that does not include the forget set. Thisapproach is resource-intensive, and does not scale to the large models now in development. Efficient alternatives in LLMs often rely on gradient ascent-based procedures, where one maximizessome loss on the data to be forgotten to reduce the influence of this data on the model predictions(Jang et al., 2022). However, there are a few issues that arise with this approach: (1) inherently,gradient ascent-based unlearning does not come with guarantees, and one needs a way to empiri-cally evaluate the unlearning quality; (2) such unlearning methods do not only affect the forget setexamples, but also come at a performance cost on the rest of the data. Our work touches upon both of these issues. For the first issue, we propose two metrics for eval-uating unlearning quality. The first metric, named generalized exposure, lower bounds unlearningquality under a particular unlearning definition (Triantafillou et al., 2023), but requires access to a School of Cybersecurity and Privacy, Georgia Institute of Technology. Work was started when the authorwas an intern at Google Brain, and affiliated with the National University of Singapore.Google DeepMind",
  "arXiv:2411.04388v1 [cs.LG] 7 Nov 2024": "reference model, that had never seen the forget set, to compute likelihoods. Another metric, relativeexposure, is an approximation to the first, further estimating the likelihoods that would be computedby a reference model, only using the current model pre- and post-unlearning. For the second issue, we present an extensive empirical evaluation, on LLMs, of how unlearningvia gradient ascent differs for in- versus out-of-distribution examples. We visualize the trade-offsbetween unlearning quality as measured per our definitions, and performance on the rest of the data.We capture different patterns of these trade-offs depending on the difficulty of the examples in theforget set, and depending on the degree of memorization of these examples. Our contributions canbe summarized as following: We propose a new metric for evaluating unlearning in generative models using a referencemodel that had never seen the unlearning data. Further, we propose an approximation tothis metric that does not require having access to the reference model. Using our proposed metrics, we evaluate gradient ascent-based unlearning in large lan-guage models, and observe that unlearning out-of-distribution samples can be done nearlywithout affecting the BLEU score Papineni et al. (2002) just like in the reference model. Incontrast, unlearning in-distribution samples affects the performance, unlike in the referencemodel. This indicates a weakness in gradient ascent-based unlearning, and suggests thatsimultaneous gradient descent on the retain data might be necessary. Finally, we evaluate whether measuring unlearning on a data point could be done usingsimilar samples. We observe that similar examples in the training data are unlearned to-gether with the ones on which unlearning is performed. Similar examples outside of thetraining dataset are almost not affected by this unlearning procedure. This explains why weobserve performance degradation for in-distribution examples.",
  "Preliminaries": "Let be a parameterized space of models (e.g., = Rd in the case of neural networks with dparameters). For our purposes, we care only about the output distribution of learning algorithms.That is, if Z denotes the set of finite sequences of input examples, and () denotes the space ofdistributions on , a learning algorithm will be viewed as a map A : Z (), and so runningthe algorithm on a size-n dataset S Zn produces the model A(S). In the context of autoregressive sequence models, such as LLMs, the set of training data S Zconsists of samples x Z, each a sequence of tokens, x = (x1, . . . , xk). A model defines con-ditional distributions on the next token xi given all previous tokens x1:i1, denoted f(xi|x1:i1; ).For a fixed model , we consider its output on a given sequence of tokens x = (x1, . . . , xk) Z tobe f(x; ) = ki=1 f(xi|x1:i1; ), the probability it assigns to that sequence (in other words, thelikelihood of x under the model ). Let L(, x) = log(f(x; )) denote the negative log likelihood (NLL) of x. The training objectiveof language models we consider is based on that loss, averaged over x S. It is minimized usinggradient-based iterative algorithms. Although it is immaterial to this work, training can often beviewed as stochastic gradient descent (or some variant) applied to the objective E L(, X),where the expectation is taken over X sampled from S.",
  "Given that a learning algorithm has produced a model A(S), the goal of unlearning is to removethe influence of a subset F S of the training data. We call F the forget set, S \\ F the retain set": "There are many ways one might formalize unlearning. We consider the following definition ofunlearning (Sekhari et al., 2021; Gupta et al., 2021; Neel et al., 2021)3:Definition 2.1. An algorithm U is a worst-case -unlearner (for A) if, for every training set S,forget set F S of fixed size, and measurable subset B , letting U U(A(S), F) and 3Note that the cited papers usually have another parameter that accounts for a shift in Equation (1). Inour case = 0. Further, it is common to consider a type of a publish function that allows to compare adistribution over other quantities, potentially obtained by post-processing (publishing) the output A(S).",
  "For any distribution over training data S and forget sets F, we say U is an on-average -unlearnerif Equation (1) holds when the probabilities are unconditional": "We refer to a sample F A(S \\ F) as a reference model. Definitions of unlearning vary in anumber of ways, including in terms of what information is available to the unlearning algorithm.The role of access to (statistics of) the training data is studied by Sekhari et al. (2021) .",
  "It follows that evaluating the argument in the r.h.s. of Equation (2) with any g G yields a lowerbound on the unlearning parameter . Below we construct a function g that we use to evaluateunlearning": "Let F S be a set of strings we want to forget.In addition, consider n reference stringsR = {ri}ni=1, sampled from some given distribution, and that are not part of S (or F). Recallthat L(, x) = log f(x; ) is the negative log-likelihood of a sequence x under model . Let",
  "L(, x) + L(, rj).(3)": "Each term L(, x)/ (L(, x) + L(, rj)) can be seen as a relaxation of the hard comparison(L(, x) L(, rj)) (or equivalently (f(rj; ) f(x; )), as the NLL is monotonically decreas-ing). In aggregate, it represents the fraction of reference strings in R that have an NLL higher thanx. g can be seen as a soft version (scaled to ) of the rank of f(x; ) among the probabilities ofreference strings {f(rj)}nj=1. A smaller value of g indicates x is more likely under (has a smallerloss) than elements of R, a larger value indicates it is less likely (has a larger loss). If g(x; , R) < ,then there are at most 2n elements ri of R such that f(ri; ) > f(x; ) (and L(, ri) < L(, x)).Similarly, if g(x; , R) > 1 , then at most 2n elements ri R satisfy f(ri; ) < f(x; ) (andL(, ri) > f(x; )).",
  "GenEx(x; A, U, F, S) = log E[g(x; F , R)] log E[g(x; U, R)].(4)": "Taking the absolute value of GenEx yields a lower bound on the worst-case epsilon in Equation (2)for a fixed g. One cannot compute the expectations in Equation (4) exactly, since the distributions ofU and F are not tractable in a standard deep learning setup. In our experiments, we use a MonteCarlo estimate of the expectations in the generalized exposure metric to get an approximate lowerbound on the unlearning quality. Such estimates are subject to variance. Alternatively, one couldthreshold the observed g(x; U, R), which would effectively correspond to choosing a different g G in Equation (2), and then use ClopperPearson confidence intervals for binomials to compute theconfidence intervals of the estimates (Clopper & Pearson, 1934) (also see (Jagielski et al., 2020)).",
  "Exposure and memorization4.Generalized exposure can be seen as an extension of the exposuremetric that appeared in the memorization literature, introduced by Carlini et al. (2019). There, the": "4We intend here a very restricted definition of memorization: whether a generative model can be inducedto generate near-facsimiles of some training examples when prompted with appropriate instructions. Models authors inject secret canaries (i.e., strings generated randomly, from a different distribution than theregular data distribution) C = {ci}mi in the training set S. In our notation, C = F. In addition,n reference strings {ri}ni=1 are sampled from the same distribution. For each canary ci, lettingrank(li|{lj}j) denote the rank of li in the set {lj}j, Carlini et al. (2019) define exposure as5:",
  "This metric is meant to capture how much the model memorized the canaries relative to the referencestrings that were not seen during training": "Generalized Exposure uses a function g (Equation (3)) that can be seen as a soft version of thecomparison function used in the second formulation of exposure (Equation (6)). The referencestrings it uses do not have to come from outside of the distribution of regular data in general, but wecan consider R = {ri}ni=1, as defined above, as a special case. For a randomly generated string r coming from the same distribution as R, and never seen duringlearning or unlearning ( would be independent of them), g(r; , R) should be around , and eachterm in 4 of the form log E g(r; , R) = log(2). Similarly, the probability in Equation (6) willtend to , and exposure to log2(2) = 1.",
  "under no memorization, and both sides of GeneralizedExposure cancel out. For the exposure computation, the outcome of the comparison is 1": "2, givingEx(ci; ) = log212 = 1. Under maximal memorization of ci, the loss would be smaller thanfor all the reference strings, and thus Ex(ci; ) . Similarly, the first term in the GeneralizedExposure, log E[g(x; U, R)] would tend to when g(x; U, R) gets arbitrarily close to 0 asf(ci; U) increases with more memorization relative to the reference strings. Membership inference attacks and differential privacy.Jagielski (2023) connects the exposuremetric from (Carlini et al., 2019) to differential privacy and so-called membership inference attacks.Recall that a training algorithm A is -differentially private (DP) if, for all S and S that differ byone data point, and all measurable sets B , Pr( B) e Pr( B), where A(S) and A(S). One can interpret DP as a hypothesis test to assess whether the output of the algorithmwas obtained by running A on S versus S. Kairouz et al. (2015) show that a particular computationbased on false positive and false negative rates associated with this hypothesis test yields an estimateof in the differential privacy definition. Through this hypothesis test view, -DP can be connected to a version of so-called membershipinference attacks (MIAs; see, e.g., Shokri et al. 2017), which attempt to identify whether a datapoint was or was not in the training set. Probably the most related MIA is a likelihood-ratio test(LiRA) introduced by Carlini et al. (2022a). LiRA is motivated by the connections to hypothesistesting, trying to determine whether the observed prediction is more likely to have been sampledfrom a model that was trained on the sample of interest or without. The authors choose to do alikelihood ratio test (motivated by the NeymanPearson lemma), assuming that the predictions for agiven sample have a Gaussian distribution. Inspired by the work by Kairouz et al. (2015) connecting differential privacy and MIAs, Triantafillouet al. (2023, 2024) propose to estimate in the unlearning definition Equation (1) using false positiveand false negative rates from a MIA perspective. In particular, letting {} denote all membershipinference attacks, in the unlearning definition above can be estimated as a supremum over {} ofa function of upper and lower bounds of false positive/negative rates for . do not contain bit-wise or code-wise copies of their training data. Rather, if a model can be induced togenerate very close copies of certain training examples by supplying appropriate instructions to guide themodels statistical generation processes then that model is said to have memorized those examples. This isan area of active ongoing research.5They define it in terms of log-perplexity instead of NLL, but the only difference is a multiplicative log(2)factor, which is irrelevant in ranking and comparison.",
  "As above, consider a set of reference strings R, and let S A(S), F A(S \\ F) and U U(A(S), F). For each given x, we now randomly generate a second set of reference strings Rx,": "such that log E[g(x; F , R)] log EErRx [g(r; S, R)], where ErRx denotes an empiricalmean over the elements in Rx. In theory this is a complex task, and once again requires accessto the reference model. In practice, however, we will simply choose Rx so its elements are closeto x under some similarity metric (working in the embedding space), but not part of F; Rx cancontain examples from some auxiliary set (public data, held out data, etc.), that do not belong to thetraining set S. It is also possible to define a common Rx for all x F. By choosing such a set, weensure that S does not depend on Rx, just like F does not depend on x F. Further, when theforget set is small and does not affect the predictions on Rx through S too much, we can expect ourapproximation to be more accurate.",
  "Unlearning by gradient ascent": "Based on Equation (1), one could achieve exact ( = 0) unlearning by retraining from scratchwithout the forget set F. This is not practical for large language models, due to resource constraints.Another common approach is to perform gradient ascent on the loss over F, or/and gradient descenton the loss over S \\ F. Other alternatives have been proposed in the literature (Patil et al., 2023;Meng et al., 2022a,b), but a gradient ascent/descent-type procedure is still a common component inall of them. While this approach is fairly efficient, and usually implemented with only a small number of gradientupdates, it is not guaranteed that the obtained model after unlearning via gradient ascent/descent hastruly forgotten the samples. Further, there is no set heuristic for the number of gradient updates torun during unlearning. This technique, thus, hinges on being able to assess how unlearned a set ofexamples is for a given language model.",
  "For a reference model, unlearning, which is equivalent to not training on, out-of-distribution samplesshould not affect the overall performance, meaning that the models S and F should perform": "similarly. When the forget set contains in-distribution samples, then the effect depends on the sizeof the forget set relative to the training set. We focus on the typical case where the size of the forgetset is small enough, and both models, S and F , perform similarly under the BLEU score. Thusa good unlearning algorithm should be able to unlearn without any observable trade-offs betweenunlearning quality and overall performance, as measured by the BLEU score.",
  "Experiments": "We evaluate the trade-offs between unlearning quality and performance on LLMs. We demonstratethat unlearning more memorized or more difficult examples is more damaging for the overall modelperformance. We also examine how neighbouring examples are affected by unlearning. Finally,we show that our relative exposure metric captures unlearning quality as well as the generalizedexposure metric, thus showing a way to assess unlearning without having a reference model.",
  "Experimental setup": "Models and datasets.We train a transformer model (T5-base with 220 million parame-ters (Roberts et al., 2022)) on WMT14 En-De, a well-known language translation dataset thatcontains sentence pairs in German and English (Bojar et al., 2014). We train for 45, 000 trainingsteps with batch size of 128 on examples from the training split. We evaluate the task performanceof the translation task using the BiLingual Evaluation Understudy (BLEU)) score (Papineni et al.,2002). Our models have a BLEU score of around 26, having a clear gist but with grammatical errors.",
  "A subject model, of weights S, is trained on a dataset S made of T and the concatenationof all the potential forget sets F... defined below": "We consider an unlearning method based on gradient ascent (.3). Following Jang et al.(2022), we use a batch size of 32 when unlearning a set of 512 examples, giving us 16 unlearningsteps to go through for the entire forget set considered. During each unlearning experiment, weconsider one single forget set F, and perform unlearning only on its examples. We always comparethe resulting unlearned model with the same, shared reference model. Even though the referencemodel is only trained T, which is a subset of the retain set of any given experiment (the full retainset would include the forget sets for the other experiments), we consider it a suitable approximation,as the ignored examples form only a small fraction of the training set. Out-of-distribution forget sets generation.We generate out-of-distribution (OOD) canaries bysampling alpha-numeric characters uniformly at random, forming a sequence of fixed length (10characters). We create three disjoint sets F OOD...of 512 OOD canaries each, as well as a set ROOD of10,000 reference strings from the same distribution. To study the effect of the number of repetitionon unlearning, these sets are incorporated in the training set S with different frequencies: canariesin F OOD1are seen only once during training, the ones in F OOD10 ten times, and F OOD100 a hundred times. In-distribution forget sets generation.We generate sets of in-distribution (InD) examples by ran-domly selecting examples from the validation split of the dataset, so that we can train the referencemodel on the full training split T, and these examples do not appear even once in its training set. Wecreate three disjoint sets F InD...of 512 in-distribution examples each, as well as a set RInD of 3, 003reference strings formed from the test split. All of these sets of examples are disjoint. Similarly tothe OOD canaries, these sets are incorporated in the training set with different frequencies. For themain subject model considered (S), F InD1 is seen only once, F InD10 ten times, and F InD100 a hundredtimes. Appendix A.2 also considers a model trained on a different training set S, where differentvalidation examples are used in forget sets, see that section for details.",
  "F InD100 (GenEx=1.55 2)": ": Distributions of perplexities. Perplexities of different sets of in-distribution examplesunder the subject model (before unlearning, post-unlearning and when exposure is low) and thereference model. Columns left to right: in-distribution example perplexities when the subject modelwas trained by repeating these examples 100 times (left), 10 times (middle), 1 time (right).",
  "Memorization vs. performance trade-offs": "Our evaluated method of unlearning modifies the model by performing gradient ascent, as a resultit might degrade the models accuracy on the test set. We first evaluate the trade-off between theeffectiveness of unlearning under generalized exposure and the task performance on the unlearnedmodel (). At every unlearning step, we measure the average exposure of the canary, and,respectively, forget set. On these checkpoints, we compute the BLEU score on the test set. Our first observation is that unlearning of canary data in one pass does not degrade the performanceas much as unlearning in-distribution samples even when these are repeated as often. The averageexposure value of the canaries also does not fall below 1 in one pass, meaning the canaries are stilltwice as less surprising to the model than other random samples unseen in training. The averageexposure of the InD samples, however, falls to the minimum value. The reason is that unlearningInD examples affects the perplexities of other similar examples (.4), whereas for out-of-distribution, unlearning does not affect as much the other canaries perplexities. This explains whythe in-distribution examples have a much faster drop in exposure, as well as task performance. Different Frequencies.In , we observe that the more repeats of the in-distribution samplesets, the higher the (average) generalized exposure is before unlearning (top right point of eachorange curve). A similar effect is visible for the exposure of OOD between the OOD 1 and theOOD 10 curves, although it is not visible in the OOD 100 because the estimate of exposureis limited by log2ROOD. In Appendix A.2, we also evaluate how a different number of repetitionof the same examples of the in-distribution sets affect the trade-off. Despite the three randomly-selected InD sets having a different distribution of perplexities under the reference model (as shownin ), the qualitative results are not affected by which set is repeated a given number of times. Distribution of perplexities.We check how the perplexities of the in-distribution samples areaffected before and after unlearning with respect to the reference model. We observe that the per-plexities of the in-distribution set is reduced, but now the perplexities are skewed, not resemblingat all the distribution on the reference model (). We also plot the distribution of perplexi-ties when the exposure is below a certain threshold which results in distributions that are closer to",
  "Per-sample difficulty vs. memorization": "We empirically evaluate the relationship between the difficulty of in-distribution examples and mem-orization. In , we plot the memorization and difficulty of each example in the forget sets.The per-sample difficulty has a weak correlation with the per-sample memorization when the InDset is repeated once, but the correlation becomes strong with the number of repeats. We also clusterthe in-distribution examples into 3 sets of low, medium and high perplexity based on their difficulty(), and find that harder examples have slightly better trade-offs (see details in Appendix A.3).",
  ": Unlearning affects theaverage exposure of similar exam-ples": "We highlight that unlearning InD examples has an impact onother similar examples. We find similar examples by comput-ing the L2-distance in the embedding space of each point inthe forget set on the reference model (see Appendix A.6). In, we plot the memorization vs. performance trade-offs(as we unlearn F InD100) for both the set F InD100 and a set of sim-ilar examples from F InD1 . The average exposure of the similarset decreases, without having to do unlearning. This explainswhy unlearning damages the performance of the model, sincethe model may forget other examples. Despite this, the effectof unlearning on similar examples outside of the training set isnot significant. Thus, unlearning may affect examples that aremore memorized as opposed to just similar examples.",
  "Related Work": "Recent work on unlearning in LLMs has focused on developing effective unlearning algorithms androbust evaluation metrics to assess the degree of unlearning achieved. We give a brief overview ofthe most relevant work here, and point interested readers to Appendix A.7 for more related work. Unlearning benchmarks and evaluation metrics.Several works propose leveraging evaluationmetrics of memorization with the aim to provide better unlearning methods in LLMs (Jang et al.,2022; Barbulescu & Triantafillou, 2024). Our work aims to work with a worst-case -unlearner(Definition 2.1) and can be seen as complementary to these approaches. Our experiments also pointto stark differences between in- and out-of-distribution memorized data. An orthogonal unlearningapproach is by removing of training data from the weights (Meng et al., 2022a; Patil et al., 2023). Memorization in LLMs.Whereas our work targets memorized data unlearning, a range of othermemorization notions and concerns have been studied in LLMs (Lehman et al., 2021; Ippolito et al.,2022; Carlini et al., 2021; Choquette-Choo et al., 2021; Lukas et al., 2023).",
  "Conclusion": "In this work, we propose a generalized exposure metric for evaluating unlearning. We find in-stances where gradient ascent-based techniques are insufficient for unlearning without destroyingthe models performance. We explain this through the effect of unlearning on similar data. We thank Daniel M. Roy and Eleni Triantafillou for feedback on various drafts of this work. Thisproject used computational resources on Google Cloud Platform provided by Google, we wouldlike to thank Danat Pomeranets in particular for support. Teodora was supported in part by theNational Research Foundation Singapore under its NRF Fellowship Programme [NRF-NRFFAI1-2019-0004], by the Crystal Centre at National University of Singapore and its sponsors, and aGoogle PhD fellowship.",
  "George-Octavian Barbulescu and Peter Triantafillou. To each (textual sequence) its own: Improvingmemorized-data unlearning in large language models. arXiv preprint arXiv:2405.03097, 2024": "Ondrej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, JohannesLeveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, Radu Soricut, LuciaSpecia, and Ale s Tamchyna.Findings of the 2014 workshop on statistical machine trans-lation.In Proceedings of the Ninth Workshop on Statistical Machine Translation, pp. 1258, Baltimore, Maryland, USA, June 2014. Association for Computational Linguistics. URL Nicholas Carlini, Chang Liu, Ulfar Erlingsson, Jernej Kos, and Dawn Song. The secret sharer:Evaluating and testing unintended memorization in neural networks. In 28th USENIX SecuritySymposium (USENIX Security 19), pp. 267284, 2019. Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, KatherineLee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, et al. Extracting training datafrom large language models. In 30th USENIX Security Symposium (USENIX Security 21), pp.26332650, 2021. Nicholas Carlini, Steve Chien, Milad Nasr, Shuang Song, Andreas Terzis, and Florian Tramer. Mem-bership inference attacks from first principles. In 2022 IEEE Symposium on Security and Privacy(SP), pp. 18971914. IEEE, 2022a.",
  "Varun Gupta, Christopher Jung, Seth Neel, Aaron Roth, Saeed Sharifi-Malvajerdi, and Chris Waites.Adaptive machine unlearning. Advances in Neural Information Processing Systems, 34:1631916330, 2021": "Jamie Hayes, Ilia Shumailov, Eleni Triantafillou, Amr Khalifa, and Nicolas Papernot.Inexactunlearning needs more careful evaluations to avoid a false sense of privacy.arXiv preprintarXiv:2403.01218, 2024. Daphne Ippolito, Florian Tram`er, Milad Nasr, Chiyuan Zhang, Matthew Jagielski, Katherine Lee,Christopher A Choquette-Choo, and Nicholas Carlini. Preventing verbatim memorization in lan-guage models gives a false sense of privacy. arXiv preprint arXiv:2210.17546, 2022.",
  "Kevin Meng, Arnab Sen Sharma, Alex Andonian, Yonatan Belinkov, and David Bau. Mass-editingmemory in a transformer. arXiv preprint arXiv:2210.07229, 2022b": "Fatemehsadat Mireshghallah, Kartik Goyal, Archit Uniyal, Taylor Berg-Kirkpatrick, and RezaShokri. Quantifying privacy risks of masked language models using membership inference at-tacks. arXiv preprint arXiv:2203.03929, 2022. Yuta Nakamura, Shouhei Hanaoka, Yukihiro Nomura, Naoto Hayashi, Osamu Abe, Shuntaro Yada,Shoko Wakamiya, and Eiji Aramaki. KART: Parameterization of privacy leakage scenarios frompre-trained language models. arXiv preprint arXiv:2101.00036, 2020.",
  "Vaidehi Patil, Peter Hase, and Mohit Bansal. Can sensitive information be deleted from LLMs?objectives for defending against extraction attacks. arXiv preprint arXiv:2309.17410, 2023": "Natalia Ponomareva, Jasmijn Bastings, and Sergei Vassilvitskii. Training text-to-text transformerswith privacy guarantees. In Findings of the Association for Computational Linguistics: ACL 2022,pp. 21822193, 2022. Adam Roberts, Hyung Won Chung, Anselm Levskaya, Gaurav Mishra, James Bradbury, DanielAndor, Sharan Narang, Brian Lester, Colin Gaffney, Afroz Mohiuddin, Curtis Hawthorne, AitorLewkowycz, Alex Salcianu, Marc van Zee, Jacob Austin, Sebastian Goodman, Livio BaldiniSoares, Haitang Hu, Sasha Tsvyashchenko, Aakanksha Chowdhery, Jasmijn Bastings, Jannis Bu-lian, Xavier Garcia, Jianmo Ni, Andrew Chen, Kathleen Kenealy, Jonathan H. Clark, StephanLee, Dan Garrette, James Lee-Thorp, Colin Raffel, Noam Shazeer, Marvin Ritter, MaartenBosma, Alexandre Passos, Jeremy Maitin-Shepard, Noah Fiedel, Mark Omernick, BrennanSaeta, Ryan Sepassi, Alexander Spiridonov, Joshua Newlan, and Andrea Gesmundo.Scal-ing up models and data with t5x and seqio. arXiv preprint arXiv:2203.17189, 2022. URL Ahmed Salem, Yang Zhang, Mathias Humbert, Pascal Berrang, Mario Fritz, and Michael Backes.Ml-leaks: Model and data independent membership inference attacks and defenses on machinelearning models. arXiv preprint arXiv:1806.01246, 2018. Ayush Sekhari, Jayadev Acharya, Gautam Kamath, and Ananda Theertha Suresh. Remember whatyou want to forget: Algorithms for machine unlearning. Advances in Neural Information Pro-cessing Systems, 34:1807518086, 2021. Weijia Shi, Jaechan Lee, Yangsibo Huang, Sadhika Malladi, Jieyu Zhao, Ari Holtzman, DaogaoLiu, Luke Zettlemoyer, Noah A Smith, and Chiyuan Zhang. Muse: Machine unlearning six-wayevaluation for language models. arXiv preprint arXiv:2407.06460, 2024. Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference at-tacks against machine learning models. In 2017 IEEE symposium on security and privacy (SP),pp. 318. IEEE, 2017. Eleni Triantafillou, Fabian Pedregosa, Jamie Hayes, Peter Kairouz, Isabelle Guyon, Meghdad Kur-manji, Gintare Karolina Dziugaite, Peter Triantafillou, Kairan Zhao, Lisheng Sun Hosoya, JulioC. S. Jacques Junior, Vincent Dumoulin, Ioannis Mitliagkas, Sergio Escalera, Jun Wan, SohierDane, Maggie Demkin, and Walter Reade. NeurIPS 2023 - machine unlearning, 2023. URL Eleni Triantafillou, Peter Kairouz, Fabian Pedregosa, Jamie Hayes, Meghdad Kurmanji, KairanZhao, Vincent Dumoulin, Julio Jacques Junior, Ioannis Mitliagkas, Jun Wan, et al. Are we mak-ing progress in unlearning? findings from the first neurips unlearning competition. arXiv preprintarXiv:2406.09073, 2024.",
  "A.1Experimental Setup Details": "The training dataset is available in TensorFlow Datasets as wmt t2t translate. It has three splits:a train split T with 4, 592, 289 examples, a validation split of 3, 000 examples, and a test split with3, 003 examples. We use the validation split for selecting the InD sets and the test split for thereference strings for InD. Remark on out-of-distribution sets generation.Note that our approach to generating canariesdiffers from that in (Carlini et al., 2019). There, the canaries are generated with a fixed string prefix(or template) such as My secret is: and a randomly-generated string suffix c, sampled from arandomness space c C, e.g., the alpha-numeric strings of length 10. These canaries aim to mimicaccidental personal identifiable information (PII) in the training data, where the sensitive informationwas a unique string of characters, such as a social security number. However, having many canariessharing the same template in the training set means that the model could learn to detect this pattern,and share some representation between canaries. This can be especially troublesome in the contextof evaluating unlearning: decreasing the likelihood of a given canary could decrease the likelihoodof the template, and that of the other canaries, leading to over-estimation of the effectiveness of anunlearning method.",
  "the same OOD forget sets as in S: F OOD1 , F OOD10 , and F OOD100;": "different in-distribution forget sets, made out of the same examples but with different fre-quencies: F InD1 contains the same examples as F InD100 but repeated only once, F InD10 con-tains the same examples as F InD1 , and F InD100 as F InD10. Regardless of the identity of the repeated set, we observe that the more repeats of the in-distributionsample sets, the higher the (average) generalized exposure before unlearning, as shown in ).We also do not observe a significant difference in their average generalized exposures. We did not investigate that effect on out-of-distribution canaries. Since they were sampled from auniform distribution and should be interchangeable, we do not expect the identity of canary examplesrepeated the same number of times to influence the exposure. Despite the average exposure being the same, the distribution of the InD set perplexities are differ-ent. We illustrate the perplexities of the three sets of in-distribution examples on a model that wastrained without them in . The three InD sets have similar mean log-perplexities on thereference model, with differences in the spread of the distribution of their log-perplexities. Specifi-cally, the mean and variance for F InD1 , F InD10, and F InD100 is (41.95, 1068.65), (42.30, 1061.54), and(42.94, 1266.64), respectively.",
  "A.3Difficulty and memorization results": "shows the difficulty and exposure trade-offs for the different model trained on S wherewe vary the number of repetitions of the same examples. Each sub-figure shows the unlearningof one set: F InD1 , F InD10, and, respectively F InD100. The conclusion is that harder examples withmore repetitions have slightly better trade-offs as it does not lead to over-unlearning (skewing theexposure to negative values). Similarly, we show that variations among different in-distributions setsyields similar observations regardless of the identity of the in-distribution set (). We compute the memorization of each sample in the InD sets based on their likelihood on the subjectmodel (trained on all forget sets F InD... , before any unlearning) and their likelihood on the referencemodel. We average the likelihood of each sample over 3 reference models F A(S \\F), trainedunder three different seeds. plots the per-example difficulty and memorization for the S after unlearning on one ofthe forget sets. We use the definitions in .1 for difficulty and memorization. We find thatthere is a weak correlation between these two for InD examples that repeat once, but the correlationgets stronger as the number of times the samples repeats increases.",
  ": F InD1 , same examples as F InD100": ": In- vs. Out-of-distribution sets. Trade-off between the generalized exposure (Ex-posure) and the task performance (BLEU score) when unlearning the subject models (S, S) at45,000 steps, with OOD (canary) forget setsF OOD1 , F OOD10 , F OOD100, and in-distribution forget setsF InD1 , F InD10, F InD100for S (resp.F InD1 , F InD10, F InD100for S).",
  "F InD100 (on reference)": ": We show the distribution of the log-perplexities of the different sets of in-distributionexamples used in our experiments. The perplexities were computed on the reference model thatwas trained on the language translation dataset (wmt-t2t, de-en), without OOD canaries or InDsamples. We see clear differences, despite which the example frequency vs unlearning results werenot different among these different groups.",
  "F InD100 (GenEx=-0.12 2)": ": Distributions of perplexities. Perplexities of different sets of in-distribution examplesunder the subject model (before unlearning, post-unlearning, and when the exposure is before athreshold of 2), and the reference model when the training set is S (different frequencies for thesame set of examples). Columns left to right: in-distribution example perplexities when the subjectmodel was trained by repeating these examples 1 time (left), 10 times (middle), 100 times (right).",
  "A.4Distribution of perplexities at low exposure": "We note that the unlearning a number of steps may result in negative exposure, a sign of over-unlearning which skews the distribution of perplexities of the forget sets on the unlearned subjectmodel compared to the distribution of perplexities of the forget sets on the reference model. Weshow what happens when we set the exposure threshold to 0.5 in . The extent of this effectdepends on the number of repeats of the forget set.",
  "A.5Relationship between relative exposure and generalized exposure": "The relative exposure metric is a more computationally efficient one, since it does not require accessto a reference model. We want to study whether it is a good proxy for the generalized exposuremetric. For this, we take the subject model trained on S and plot it together with the generalizedexposure before unlearning (at the 45, 000 training step) and after unlearning. We observe that therelative exposure is a good proxy for the generalized exposure ().",
  ": F InD100, same examples as F InD10": ": Difficulty vs. trade-offs. For each set of in-distribution examples, we cluster themby difficulty using the perplexities on the reference model. Examples with a higher perplexity areconsidered harder. The trade-off is computed for each unlearning step on the main model. Harderexamples have a better trade-off between the unlearning effectiveness and the performance of theunlearned model.",
  "F InD100 (GenEx=-0.51 0.5)": ": Distributions of perplexities at low exposure. Perplexities of different sets under thesubject model at the first unlearning step which results in an average exposure lower than a thresholdof 0.5 (orange), and the perplexities under on the reference model (purple). In the top rightmostfigure, we observe a phenomenon of over-unlearning (when the exposure becomes negative) whichbrings the two distributions of perplexities further apart.",
  "A.6Effect of unlearning on similar points": "To investigate this, we find similar examples (the top-10) from the set of examples that repeat tentimes (F InD10) to the forget sets F InD100 and F InD1 . For simplicity, we compute the L2-distance betweenthe embeddings of each point in the forget sets on the reference model. Our similar sets consist ofthe union of the top-10 closest examples from F InD1 for all examples F InD100 and, respectively, F InD1 .Concretely, this resulted in 424 examples for the set of examples that repeat once, and 421 for theset of examples that repeat 100 times. We then measure the average generalized exposure on the similar set as we unlearn the forget setF InD100 and, respectively, F InD1 , for 16 training steps. We plot the tradeoffs between exposure andperformance on the forget set and on the similar sets in . We can see that similar examplesare unlearned as well by performing unlearning on the forget set, even before unlearning impactsthe models utility. We want to see how unlearning the forget set also influences examples outside of the training set,i.e., the reference set RInD. We use the same methodology as above, and pick the top-10 closestexamples from RInD to our two forget sets. This results in 571 for F InD100 and 591 for F InD1 . Tostart with, the exposure of examples outside the training set is small. The effect of unlearning of theforget set on these examples exposure is unnoticeable, though we do observe a small decrease ofexposure (up to 0.1 for the case shown in ). Similarly, we show that the effect of F InD1 onthe closest reference examples is very small. For the reference set and the forget set that is repeatedonly once, we observe the same phenomenon: the exposure of the reference set is not affected by theunlearning of the samples in the forget set (). We also validate our main observation of theeffect of unlearning on similar points in the training dataset on a different subject model, trained ona training dataset S. The training has the same forget sets but with different number of repetitions.",
  "A.7Related Work": "Unlearning benchmarks and evaluation metrics.(Lynch et al., 2024) propose eight distinctevaluation metrics that go beyond standard loss measures on the forget/retain set, and try to captureinternal model changes, as well as the impact on downstream tasks. The authors measure robustnessto jailbreaks and finetuning, other extraction techniques, undesirable side effects, etc. Shi et al.",
  "Similar set RInD": ": In-distribution vs. similar in-distribution examples. Trade-off between the gener-alized exposure (Exposure) and the task performance (BLEU score) when unlearning the subjectmodel on set F InD1 and F InD100 (left to right). Unlearning in-distribution examples affects the expo-sure of other similar examples from the training dataset (left-most two plots), while not affecting theexposure of unseen examples from the reference set (right-most plot). We can see that the examplesin the reference set are not affected by unlearning. (2024) propose a new machine unlearning evaluation benchmark, MUSE, focusing on assessing 6desired properties of unlearned models, such as verbatim memorization, scalability with forget setssize, etc. The TOFU benchmark paper (Maini et al., 2024) introduces a new task and dataset forevaluating specific training data unlearning in large language models. Jang et al. (2022) introducean extraction likelihood metric for measuring unlearning quality in LLMs: they look at the averagecompletion accuracy of a sequence of tokens when a varying length prefix was provided as a prompt.The authors also studied gradient ascent-based unlearning, and found that to be more effective whenunlearning sequentially in batches rather than all at once. They also report differences in how easyit is to unlearn depending on the source of the forget set. While Jang et al. (2022) also points todifferences in the effectiveness of unlearning between different forget datasets, they do not furtherexplore how similar examples are affected by gradient ascent (as our work does). TOFU focuses ona Task of Fictitious Unlearning where models are trained on fictional author profiles and then mustunlearn a subset of those profiles. The paper provides a dataset of these profiles, metrics to assessunlearning efficacy, and baseline results from existing unlearning algorithms. All of the work aboveaims to identify and assess desirable properties of unlearned models for general or specific tasks,but do not directly work with unlearning definition as in Definition 2.1. The way to measureunlearning as proposed in our work can be viewed as complementary to these other approaches. Barbulescu & Triantafillou (2024)6 leverage memorization information for unlearning by proposingan unlearning method that differentiates textual sequences based on their memorization level, as in(Jang et al., 2022). The memorization in this work is captured by tracking reconstruction of theexact tokens in a sequence, which is different from the definition used in our work. An unlearningalgorithms is successful if memorization of a particular sequence of interest is reduced. Theirwork also introduces an MIA-like evaluation inspired by the neighborhood MIA concept. Memorization.Several studies have explored different facets of memorization in LLMs, includingverbatim memorization (Lehman et al., 2021; Ippolito et al., 2022), membership inference attacks(Shokri et al., 2017; Nasr et al., 2018; Salem et al., 2018; Choquette-Choo et al., 2021), exposure(Carlini et al., 2019), and extraction attacks (Carlini et al., 2021, 2022b). These works providevaluable insights into the extent and nature of information leakage in LLMs. Hayes et al. (2024) highlighted the limitations of inexact unlearning evaluation methods like mem-bership inference attacks. The authors show that current evaluation metrics for approximate unlearn-ing can be misleading, creating a false sense of security. They call for more rigorous testing and adeeper understanding of how unlearning affects different data points.. Removing information in Large Language Models.Patil et al. (2023) consider informationremoval from the weights of a language model, which should protect against white box attacks. Theauthors focus on model editing techniques (Meng et al., 2022b,a), and show that even after editingthe model to remove some sensitive information, they were still capable of extracting this infor-mation in a large fraction of cases. This paper also investigates how editing sensitive informationaffects the accuracy on neighbouring points using this information. They use the change of accuracy",
  "in the neighbourhood, a metric borrowed from (Meng et al., 2022b), to demonstrate that in manycases sensitive information was not properly removed": "Memorization and membership inference attacks.Membership inference attacks (MIAs), firstintroduced for classification tasks, aim to evaluate to what extent a given datapoint can be tracedback to be from a training set or not (Shokri et al., 2017). MIAs are now widely adopted in un-learning literature, as well as for studying memorization. Recently, membership inference attackshave been proposed for language models such as text classification tasks (Gu et al., 2023), (Matternet al., 2023), and masked language models (Mireshghallah et al., 2022). The membership inferenceinformation can serve as a step towards extracting the training data. Carlini et al. (2019) showed thatpersonal information can be extracted by generating numerous sentences from pre-trained languagemodels and performing membership inference. Nakamura et al. (2020) considered an adversarywith some prior knowledge of the patient that could employ a pre-trained masked BERT model topredict the masked personal information in the input clinical data. Lukas et al. (2023) showed thatPII can be extracted from these models. Besides attacks, several mitigation strategies have beenproposed for large language models such as ad-hoc practical defenses (Lee et al., 2021), as well asbased on the rigorous framework of differential privacy (Ponomareva et al., 2022). Deferentiallyprivate training makes the model indistinguishable to an adversary (or user querying the model) upto one data record, or a fixed size group (group privacy). However, in unlearning, requests to deletesamples may come for a batch of samples of varying size, perhaps even hundreds of these. As aresult, differential privacy is not enough to support unlearning requests across all applications."
}