{
  "Abstract": "Manipulating garments and fabrics has long been a critical endeavor in the develop-ment of home-assistant robots. However, due to complex dynamics and topologicalstructures, garment manipulations pose significant challenges. Recent successes inreinforcement learning and vision-based methods offer promising avenues for learn-ing garment manipulation. Nevertheless, these approaches are severely constrainedby current benchmarks, which offer limited diversity of tasks and unrealistic simu-lation behavior. Therefore, we present GarmentLab, a content-rich benchmarkand realistic simulation designed for deformable object and garment manipulation.Our benchmark encompasses a diverse range of garment types, robotic systemsand manipulators. The abundant tasks in the benchmark further explores of theinteractions between garments, deformable objects, rigid bodies, fluids, and humanbody. Moreover, by incorporating multiple simulation methods such as FEM andPBD, along with our proposed sim-to-real algorithms and real-world benchmark,we aim to significantly narrow the sim-to-real gap. We evaluate state-of-the-artvision methods, reinforcement learning, and imitation learning approaches onthese tasks, highlighting the challenges faced by current algorithms, notably theirlimited generalization capabilities. Our proposed open-source environments andcomprehensive analysis show promising boost to future research in garment ma-nipulation by unlocking the full potential of these methods. We guarantee thatwe will open-source our code as soon as possible. You can watch the videos insupplementary files to learn more about the details of our work. Our project pageis available at:",
  "Introduction": "The next-generation assistant robots should possess not only the abilities to separately manipulatea wide variety of objects, including rigid, articulated, and deformable objects, but also thecapability to leverage interactions between those physical media, including flow and fluids, in orderto assist humans. Among various daily tasks , garment manipulation stands out as oneof the most challenging, crucial, and extensively discussed tasks in the robotics and computer vision,due to its demanding requirements for understanding dynamic properties of physical instances andinteractions between them. For instance, washing clothes entails the interaction between garmentsand fluids, while dressing up requires collaboration between robots and humans. Garment manipulation tasks mainly presents three challenges. Firstly, each individual garmentpossesses nearly infinite states and exhibits complex kinematic and dynamic properties. Therefore, itis crucial for models to comprehend the various self-deform states of garments, which usually requires",
  ": GarmentLab provides realistic simulation for diverse garments with different physical propoerties,benchmarking various novel garment manipulation tasks in both simulation and the real world": "large amount of training data (C1). Secondly, garment manipulation involves interactionswith various types of objects, including rigid (e.g., clothes hanger) and articulated (e.g., wardrobe)objects, as well as fluids and human body. Consequently, enabling models to understand theseinteractions across diverse physical media presents great significance (C2). Finally, considering thatstrategies for manipulating garments are often highly complex, and visual perception of garmentsis more challenging due to their diverse states and patterns, manipulating garments faces a greatersim2real gap (C3). Training a powerful agent capable of overcoming these challenges necessitates a vast amount of dataencompassing robot-object interactions. However, directly collecting data from the real world isimpractical. Thus, researchers have long pursued benchmarks for garment manipulation .Current deformable simulations suffer from drawbacks such as missing garment meshes .Additionally, they offer a limited range of tasks, hindering further research endeavors. Therefore, we present GarmentLab (), a unified environment and benchmark for garmentmanipulation. GarmentLab has three novel components to satisfy the demands for diversity andrealism: The powerful GarmentLab Engine, which is built upon Omniverse Isaac Sim andsupports a variety of physical simulation methods. The simulator not only supports Position-Based-Dynamic (PBD) , Finite-Element-Method (FEM) , to simulate garments, fluid and deformableobjects but also makes integration with ROS to provide an efficient teleoperation pipeline fordata collection. GarmentLab Assets is a large-scale indoor dataset comprising 1) garments modelscovering 11 categories of daily garments from ClothesNet 2) various kinds of robot end-effectorincluding gripper, suction and dexterous hands. 3) high-quality 3D assets including 20 scenes and9000+ object models from ShapeNet . Based on realistic simulation and rich assets, we proposeGarmentLab Benchmark containing 20 tasks divided into 5 groups to evaluate state-of-the-artvision-based and reinforcement learning based algorithm. To tackle above challenges, our environment has three characteristics:1) Efficient. Garment manipu-lation involves nearly infinite object state and action spaces, requiring substantial data for modelsto understand garment structure and deformation. To meet this demand, our highly parallelizedGPU-based simulator provides a significant training advantage. Larger batch sizes enhance RL-basedalgorithms , while faster data collection speeds reduce training time for perception-based algo-rithms (tackling C1). 2) Rich. The richness of our simulator can be categorized into two aspects:the diversity of simulation content offered by GarmentLab Assets and the depth of physical inter-action facilitated by GarmentLab Engine. We emphasize multi-physics simulation, encompassingrigid-articulated, deformable-garment, fluid dynamics, and flow, along with their interactions. Thisfocus is vital for training agents capable of comprehending real-world physical properties (tack-ling C2). You can refer to videos in supplementary material for our simulation effects. 3) Real.",
  "GarmentLabRTGPUManipulation": "As the sim-to-real gap emerges as the main obstacle in developing embodied agents, GarmentLabEngine surpasses Omniverse capabilities by providing mature sim-to-real algorithms, such as Tele-operation utilized in the RL field, and the Visual Sim-Real Alignment Algorithm employed inperception algorithms. We also make integration with ROS and MoveIt , which is beneficialfor narrowing sim2real gap by introducing real-world robot motions into simulation (tackling C3). Our benchmark experiments highlight the significant challenges current algorithms face, even withseemingly simple tasks like unfolding. These difficulties arise from a lack of understanding of physicalinteractions and the complexities of high-dimensional states. Vision-based algorithms demonstratelimited generalization, with performance strongly affected by the initial state of objects. RL-basedalgorithms also encounter difficulties with tasks requiring long-horizon planning. These insights offervaluable guidance for improving methods for garment and deformable object manipulation.",
  "Related Work": "Garment and Deformable Object Benchmarks. Current deformable object environments usually support only one simulation method (e.g., PBD or FEM), limiting the types of simulatedobjects and interactions. Besides, most environments are CPU-based , severely limitingparallel capabilities and often exhibiting a huge sim-to-real gap due to the absence of comprehensivesim-to-real algorithm designs. In contrast, as a GPU-based simulator, GarmentLab provides diverse3D meshes and supports various simulation techniques. We further integrate ROS and combine it withour carefully designed sim-to-real pipeline, offering a more comprehensive solution for researchers.Detailed comparisons between our environment and others can be found in and Appendix C Garment and Deformable Object Manipulation. Although current efforts excel at specific taskssuch as folding and unfolding, many real-world tasks are long-horizon and involveinteractions between various physical media. While many studies have potential to tackle theseproblems, they are hindered by the lack of a mature simulation platform capable of support-ing such diverse and complicated extensions. Furthermore, while current research predominantlyemphasizes gripper manipulation tasks, we introduce tasks involving suction, dexterous hands,and mobile robots. We believe GarmentLab will make a unique and valuable contribution to therobotics community by providing a new platform for developing garment manipulation algorithmsand significantly expanding the scope of existing methods.",
  "GarmentLab Environment": "GarmentLab aims to integrate state-of-the-art physical simulation methods, modern graphics ren-dering engines, and user-friendly robotic interfaces into a unified framework (). Below wewill first introduce GarmentLab Engine (.1) and GarmentLab Asset (.2) to showour diversity in function and objects. As we especially focus on the exploration of multiple physicalsimulation methods and interaction between them, we will introduce GarmentLab Physics in .3. In section 4 We will talk about our novel-proposed tasks.",
  "Built on NVIDIAs IsaacSim, GarmentLab offers a highly-paralleled data collection pipeline, real-istic rendering, support for various sensors, and integration with Robot Operating System (ROS)": "Data Pipeline. Data pipeline mainly consists of two components: Visual Data System and RL-Training System. Visual System provides both RGB-D observations and ground-truth semantic labelincluding 2D and 3D bounding box, normals and instance segmentation. Based on IsaacGym, RLSystem can establish multiple agents on GPU at the same time for efficient training. Rendering. GarmentLab supports multiple camera angles, such as eye-on-hand and eye-on-baseperspectives, unlike the single-camera setups of past works ,which employing naive OpenGLframework. Additionally, it utilizes GPU-enabled ray tracing for rendering, which enhancesrealism and challenge by creating more realistic shadows and lighting , thus reducing the sim2realgap and improving the performance of visual algorithms and mobile navigation tasks. ROS. ROS is a generic and widely-used framework for building robot applications. We use ROSto align robot in realworld and the simulation, please refer to .2 for detailed. Also, althoughIsaacSim provides traditional Inverse-Kinematic and RMPFlow control, we also provideMoveIt framework for motion planning, which is more widely used in the real world. Sensor. In addition to RGB-D observations, auxiliary observations can be accessed, such as robotjoints, cloth particles and object poses. They are required in common RL framework and teacher-student network. Other Omniverse sensors (e.g., tactile, contact-report) could also be available.",
  "GarmentLab Assets": "GarmentLab Asset compiles simulation content from a variety of state-of-the-art datasets, integratingindividual meshes or URDF files into complete, simulation-ready scenes with robots and sensors.We employ Universal Scene Description files to store all assets with attributes, including physics,semantics, and rendering properties. Key components along with their sources and categories areshown in . More details about each asset type are provided in Appendix A.",
  "GarmentLab Physics": "Simulation Method. To ensure physically realistic simulation, we use tailored methods for differentobjects based on their physical characteristics. For large garments and fluid, we use Postion-BasedDynamics (PBD). For small elastic garments like gloves and socks, and everyday objects like toysand sponges, we apply Finite Element Method (FEM). Human simulation involves articulatedskeletons with rotational joints and a surface skin mesh for high-fidelity rendering. Robot simulationutilizes PhysX articulation system for precise force control, P-D control, and inverse dynamics.Unlike previous works that rely on a single simulation method, GarmentLab provides platforms forexploring dynamics and kinematics of various objects and the coupling and interactions among them.Diverse Physics Parameters. To fully exploit the potential of various simulation methods and makegarment simulation more diverse and realistic, GarmentLab provides various physics parameterconfigurations. For example, as cloth is modeled as a grid of particles, altering parameters such asparticle size and stiffness will change garment physical behaviors. Likewise, as depicted in ,diverse physical material parameters are assigned to diverse objects. These parameters encompass,but are not limited to, surface tension and cohesion for fluids, friction for rigid objects, and modulus.It is worth noting that parameters influencing the interaction between different objects, includingcontact offset and reset offset, are also adjusted. For further details, please consult Appendix D.",
  "GarmentLab Benchmark": "GarmentLab Benchmark is motivated by the abilities that an intelligent manipulator agent shouldpossess, including (1) understanding the physics of object interactions, (2) generating accurate actionsequences for long-horizon, complex tasks, and (3) transferring this knowledge to the real world. Totest these abilities, we classified tasks into five categories based on physical interactions. We alsoproposed several complex, long-horizon tasks to advance future research. The demos of tasks andreal-world experiment are shown in figure 4. Task Categories. To fully exploit the models capability in understanding physical interactions andconduct comprehensive evaluations of current algorithms, we categorize 20 tasks into 5 groups. Theexample tasks and corresponding categories are shown in . Examples of task sequences areprovided in . You can refer to Appendix G to get more details. Long-Horizon Tasks. With the advancement of robotics, there is a growing focus on completinglong-horizon tasks, which integrate skills tasks such as 3D perception, manipulation and navigation.Thus we propose several long-horizon garment manipulation tasks, including organizing clothes,wash clothes, make up tables and dress up. These tasks go beyond simply executing subtasks insequence, as they require holistically planning how to accomplish the task based on the environment.During the execution, the algorithm needs to consider the positioning of the operation, the placementlocation, and carefully plan the path to avoid collisions. More analysis are shown in .",
  "Real-World Benchmark": "Real-world benchmark is crucial for not only evaluating the real-world performance of differentmethods, but also providing a standardized platform for researchers to reproduce and exchangemethods. With the existence of real-world dataset or benchmarks for rigid , articulated objectsand furnitures , we introduce the first real-world benchmark for deformable objects and garments. Unlike rigid or articulated objects that can be 3D-printed from CAD files, deformable objects areusually purchased without CAD files. Easily influenced by external forces, it is difficult to accuratelymodel garments directly using traditional multi-camera calibration and surface reconstruction methods.Therefore, we use commercial scanning devices with lasers and light for mesh scanning. Selected objects cover diverse garments (tops, trousers, socks, hats), plush toys, household items (bags,clutches), and cleaning supplies. They are primarily selected from well-known international brandsfor durability and accessibility. To ensure variety, objects have different shapes, sizes, transparencies,deformabilities, and textures. For instance, our dataset features various tops made from materials likeassault jackets, down jackets, shirts, and vests, with a wide range of physical attributes. Additionally, we provide semantic human annotations for object part masks and key points, supportingdexterous manipulation such as grasping specific parts and object tracking using key points. FollowingYCB, we present a systematic approach for defining manipulation protocols and benchmarks.These protocols specify the experimental setup for each task and provide procedural guidelines. Acomprehensive description of the real-world benchmark is provided in Appendix F.",
  "a. Scan Pipelineb. Real-World Benchmark": ": Real-World Benchmark. Part a demonstrates the whole pipeline of converting real-world objects intosimulation assets. Part b demonstrates the performance of different categories of objects in both simulation andthe real world (the first row), and the results of these objects being manipulated by the robot (the second row). : Sim2Real Framework. On the left, we highlight our MoveIt and teleoperation pipeline, a lightweightand easy-to-deploy system built using ROS. On the right, we present our three proposed visual sim-to-realalgorithms, demonstrating a significant improvement in model performance after deploying these algorithms.",
  "Sim-Real Vision Alignment": "GarmentLab intergrates several automated and self-supervised sim2real methods, and have verifiedtheir effectiveness by predicting dense visual correspondence for manipulation (, Right),with quantitative manipulation success rate in . Keypoint Embedding Alignment. Aligning corresponding skeleton point representations canmitigate representation gap between point cloud in simulation and the real world . By attachingmarkers to skeleton points and enabling robot to perform self-play, we obtain ground-truth keypointpairs and employ InfoNCE to align corresponding point representations. Shown in , thealignment adapts representations to the real-world distribution. Appendix E shows more details.Noisy Observation. Adding noise to point cloud for training can be very effective for sim2realtransfer . As shown in , initial query results had many errors. By adding noise duringtraining, our model became more robust, leading to smoother and more accurate representations.Point Cloud Alignment. We propose aligning point clouds by optimizing an affine matrix, usingchamfer distance as the loss function. As shown in , the model initially predicts incorrectresults, even for flat surfaces. However, after alignment, it successfully predicts accurate results.",
  "Real-World Motion Generation": "For many algorithms, action trajectories generated in simulation is not align with those in the realworld. We introduce two methods for generating trajectories in simulation that closely mimicreal-world scenarios by leveraging prior knowledge of real-world manipulation trajectories. Teleoperation. Weve developed a lightweight, cost-effective teleoperation system requiring justone-click deployment. It facilitates simultaneous control of dexterous hands and grippers in bothreal-world and simulated settings (). This system supports data collection for offline training,like diffusion policy.. Implementation details are in Appendix I MoveIt. Incorporating MoveIt into our framework elevates motion planning and obstacle avoidancebeyond heuristic trajectory methods, as noted in previous studies . Employing MoveIt for real-world robot execution also aids visual algorithms. Adapting models to MoveIt-generated trajectoriesduring training reduces the sim-to-real gap. Detailed implementations are provided in Appendix H.",
  "InputPoint CloudRGB ImagePoint CloudState-Base GTPartial Point Cloud": "Tasks. Although we proposed many novel tasks, current algorithms cannot fully solve them. Thus,for large garments like tops, dresses, and trousers, we chose folding, hanging, and unfolding tasks.For small items like hats and gloves, we selected hanging and placing tasks to evaluate visual and RLalgorithms. For dexterous and mobile tasks, existing work mostly employs RL algorithms. Hence,we evaluated the performance of both RL-state-based and RL-vision-based algorithms separately.",
  "RL-Vision6.7 / 8.2 / 3.25.2/ 6.2/ 8.87.6 / 5.3 / 4.113.1 / 14.811.3 / 15.2": "Vision-Based Algorithm. Among the three vision-based algorithms, UGM performed best on large-piece clothing, emphasizing cross-deform and cross-object consistency in learning representations,DIFT excels with small-piece clothing due to its robustness to object rotation but lacks proficiency inunderstanding clothing folding. Affordance works well for tasks that do not require precise pointselection, such as hanging, but struggles with folded garments. RL Algorithm. Compared to vision-based algorithms, RL performs poorly on garment manipulationdue to the complex dynamics of garments. Our analysis of training videos showed that RL oftengenerates abnormal trajectories, causing clothes to get tangled with the robotic arm or be pushedaway. This issue is more pronounced with RL-vision-based methods, as the higher-dimensionaland partial visual observations hinder the models ability to converge on an effective strategy. Fordexterous and mobile tasks, the larger action and search spaces result in suboptimal performance.Further discussion and analysis can be found in Appendix B.",
  "Real-World Experiments": "In our real-world experiments, we focused on testing vision-based algorithms due to the risk associatedwith RL actions. T-shirts for folding and hats for hanging, were selected for experimentation.Additionally, we conducted ablation study on proposed sim2real methods using UGM ().Our real-world results align with our simulation findings, indicating GarmentLab environment canenhance real-world applications. For sim2 real algorithm, without point cloud alignment and noiseaugmentation along with keypoint embedding alignment can improve representation smoothness andaccuracy. Qualitative sim2real results are shown in (Right).",
  "Conclusion": "We introduce GarmentLab, a comprehensive environment and benchmark for manipulating garmentsand deformable objects. GarmentLab includes the GarmentLab Engine, supporting various simulationmethods and ROS integration; GarmentLab Assets, a diverse dataset of robots, materials, andgarments; and GarmentLab Benchmark, proposing several novel tasks. It also provides the firstreal-world deformable benchmark along with several sim2real methods.",
  "Hugo Bertiche, Meysam Madadi, and Sergio Escalera. Cloth3d: clothed 3d humans. InEuropean Conference on Computer Vision, pages 344359. Springer, 2020": "Berk Calli, Arjun Singh, Aaron Walsman, Siddhartha Srinivasa, Pieter Abbeel, and Aaron M.Dollar. The ycb object and model set: Towards common benchmarks for manipulation research.In 2015 International Conference on Advanced Robotics (ICAR), pages 510517, 2015. Alper Canberk, Cheng Chi, Huy Ha, Benjamin Burchfiel, Eric Cousineau, Siyuan Feng, andShuran Song. Cloth funnels: Canonicalized-alignment for multi-purpose garment manipulation.In International Conference of Robotics and Automation (ICRA), 2022. Angel X. Chang, Thomas A. Funkhouser, Leonidas J. Guibas, Pat Hanrahan, Qi-Xing Huang,Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, Jianxiong Xiao, L. Yi, andFisher Yu. Shapenet: An information-rich 3d model repository. ArXiv, abs/1512.03012, 2015.",
  "Peter Florence, Lucas Manuelli, and Russ Tedrake. Dense object nets: Learning dense visualobject descriptors by and for robotic manipulation. Conference on Robot Learning, 2018": "Haoran Geng, Ziming Li, Yiran Geng, Jiayi Chen, Hao Dong, and He Wang. Partmanip:Learning cross-category generalizable part manipulation policy from point cloud observations.2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages29782988, 2023. Jiayuan Gu, Fanbo Xiang, Xuanlin Li, Zhan Ling, Xiqiang Liu, Tongzhou Mu, Yihe Tang,Stone Tao, Xinyue Wei, Yunchao Yao, et al. Maniskill2: A unified benchmark for generalizablemanipulation skills. arXiv preprint arXiv:2302.04659, 2023.",
  "Minho Heo, Youngwoon Lee, Doohyun Lee, and Joseph J. Lim. Furniturebench: Reproduciblereal-world benchmark for long-horizon complex manipulation. In Robotics: Science andSystems, 2023": "Sebastian Hofer, Kostas E. Bekris, Ankur Handa, Juan Camilo Gamboa, Florian Golemo,Melissa Mozifian, Christopher G. Atkeson, Dieter Fox, Ken Goldberg, John Leonard, C. KarenLiu, Jan Peters, Shuran Song, Peter Welinder, and Martha White. Perspectives on sim2realtransfer for robotics: A summary of the r: Ss 2020 workshop. ArXiv, abs/2012.03806, 2020. Sebastian Hfer, Kostas E. Bekris, Ankur Handa, Juan Camilo Gamboa, Melissa Mozifian,Florian Golemo, Christopher G. Atkeson, Dieter Fox, Ken Goldberg, John Leonard, C. KarenLiu, Jan Peters, Shuran Song, Peter Welinder, and Martha White. Sim2real in robotics andautomation: Applications and challenges. IEEE Trans Autom. Sci. Eng., 18:398400, 2021.",
  "Anqi Li, Ching-An Cheng, Muhammad Asif Rana, Mandy Xie, Karl Van Wyk, Nathan D.Ratliff, and Byron Boots. Rmp2: A structured composable policy class for robot learning.ArXiv, abs/2103.05922, 2021": "Chengshu Li, Ruohan Zhang, Josiah Wong, Cem Gokmen, Sanjana Srivastava, Roberto Martn-Martn, Chen Wang, Gabrael Levine, Michael Lingelbach, Jiankai Sun, et al. Behavior-1k:A benchmark for embodied ai with 1,000 everyday activities and realistic simulation. InConference on Robot Learning, pages 8093. PMLR, 2023. Yunzhu Li, Jiajun Wu, Russ Tedrake, Joshua B. Tenenbaum, and Antonio Torralba. Learningparticle dynamics for manipulating rigid bodies, deformable objects, and fluids. In InternationalConference on Learning Representations, 2019.",
  "Miles Macklin, Matthias Mller, Nuttapong Chentanez, and Tae-Yong Kim. Unified particlephysics for real-time applications. ACM Transactions on Graphics (TOG), 33:1 12, 2014": "Viktor Makoviychuk, Lukasz Wawrzyniak, Yunrong Guo, Michelle Lu, Kier Storey, MilesMacklin, David Hoeller, Nikita Rudin, Arthur Allshire, Ankur Handa, et al. Isaac gym: Highperformance gpu based physics simulation for robot learning. In Thirty-fifth Conference onNeural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. Mayank Mittal, Calvin Yu, Qinxi Yu, Jingzhou Liu, Nikita Rudin, David Hoeller, Jia Lin Yuan,Ritvik Singh, Yunrong Guo, Hammad Mazhar, Ajay Mandlekar, Buck Babich, Gavriel State,Marco Hutter, and Animesh Garg. Orbit: A unified simulation framework for interactive robotlearning environments. IEEE Robotics and Automation Letters, 8(6):37403747, 2023. Kaichun Mo, Shilin Zhu, Angel X. Chang, L. Yi, Subarna Tripathi, Leonidas J. Guibas, andHao Su. Partnet: A large-scale benchmark for fine-grained and hierarchical part-level 3d objectunderstanding. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR), pages 909918, 2018. Yoshihiko Nakamura and Hideo Hanafusa. Inverse kinematic solutions with singularity robust-ness for robot manipulator control. Journal of Dynamic Systems Measurement and Control-transactions of The Asme, 108:163171, 1986.",
  "Open Robotics.Franka emika panda robot. June 2024": "Xue Bin Peng, Marcin Andrychowicz, Wojciech Zaremba, and P. Abbeel. Sim-to-real transfer ofrobotic control with dynamics randomization. 2018 IEEE International Conference on Roboticsand Automation (ICRA), pages 18, 2017. Xavi Puig, Eric Undersander, Andrew Szot, Mikael Dallaire Cote, Tsung-Yen Yang, RuslanPartsey, Ruta Desai, Alexander William Clegg, Michal Hlavac, So Yeon Min, Vladimr Vondru,Thophile Gervet, Vincent-Pierre Berges, John Turner, Oleksandr Maksymets, Zsolt Kira, MrinalKalakrishnan, Devendra Jitendra Malik, Singh Chaplot, Unnat Jain, Dhruv Batra, AksharaRai,and RoozbehMottaghi. Habitat 3.0: A co-habitat for humans, avatars and robots. ArXiv,abs/2310.13724, 2023. Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet++: Deep hierarchicalfeature learning on point sets in a metric space. Advances in neural information processingsystems, 30, 2017.",
  "Yufei Wang, Zhanyi Sun, Zackory Erickson, and David Held. One policy to dress them all:Learning to dress people with diverse poses and garments. In Robotics: Science and Systems(RSS), 2023": "Ruihai Wu, Kai Cheng, Yan Zhao, Chuanruo Ning, Guanqi Zhan, and Hao Dong. Learningenvironment-aware affordance for 3d articulated object manipulation under occlusions. InThirty-seventh Conference on Neural Information Processing Systems, 2023. Ruihai Wu, Haoran Lu, Yiyan Wang, Yubo Wang, and Dong Hao. Unigarmentmanip: A unifiedframework for category-level garment manipulation via dense visual correspondence. 2024IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024.",
  "Ruihai Wu, Chuanruo Ning, and Hao Dong. Learning foresightful dense visual affordance fordeformable object manipulation. In IEEE International Conference on Computer Vision (ICCV),2023": "Ruihai Wu, Yan Zhao, Kaichun Mo, Zizheng Guo, Yian Wang, Tianhao Wu, Qingnan Fan,Xuelin Chen, Leonidas Guibas, and Hao Dong. VAT-mart: Learning visual action trajectoryproposals for manipulating 3d ARTiculated objects. In International Conference on LearningRepresentations, 2022. F. Xia, Amir Zamir, Zhi-Yang He, Alexander Sax, Jitendra Malik, and Silvio Savarese. Gibsonenv: Real-world perception for embodied agents. 2018 IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 90689079, 2018. Zhou Xian, Bo Zhu, Zhenjia Xu, Hsiao-Yu Tung, Antonio Torralba, Katerina Fragkiadaki,and Chuang Gan. Fluidlab: A differentiable environment for benchmarking complex fluidmanipulation. In The Eleventh International Conference on Learning Representations, 2023. Fanbo Xiang, Yuzhe Qin, Kaichun Mo, Yikuan Xia, Hao Zhu, Fangchen Liu, Minghua Liu,Hanxiao Jiang, Yifu Yuan, He Wang, Li Yi, Angel X. Chang, Leonidas J. Guibas, and HaoSu. Sapien: A simulated part-based interactive environment. 2020 IEEE/CVF Conference onComputer Vision and Pattern Recognition (CVPR), pages 1109411104, 2020.",
  "Han Xue, Yutong Li, Wenqiang Xu, Huanyu Li, Dongzhe Zheng, and Cewu Lu. Unifold-ing: Towards sample-efficient, scalable, and generalizable robotic garment folding. ArXiv,abs/2311.01267, 2023": "Han Xue, Wenqiang Xu, Jieyi Zhang, Tutian Tang, Yutong Li, Wenxin Du, Ruolin Ye, and CewuLu. Garmenttracking: Category-level garment pose tracking. 2023 IEEE/CVF Conference onComputer Vision and Pattern Recognition (CVPR), pages 2123321242, 2023. Han Xue, Wenqiang Xu, Jieyi Zhang, Tutian Tang, Yutong Li, Wenxin Du, Ruolin Ye, and CewuLu. Garmenttracking: Category-level garment pose tracking. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition, pages 2123321242, 2023. Kevin Zakka, Laura M. Smith, Nimrod Gileadi, Taylor A. Howell, Xue Bin Peng, Sumeet Singh,Yuval Tassa, Peter R. Florence, Andy Zeng, and P. Abbeel. Robopianist: A benchmark forhigh-dimensional robot control. ArXiv, abs/2304.04150, 2023.",
  "Yanjie Ze, Gu Zhang, Kangning Zhang, Chenyuan Hu, Muhan Wang, and Huazhe Xu. 3ddiffusion policy. ArXiv, abs/2403.03954, 2024": "Fan Zhang and Yiannis Demiris. Learning grasping points for garment manipulation in robot-assisted dressing. In 2020 IEEE International Conference on Robotics and Automation (ICRA),pages 91149120. IEEE, 2020. Yan Zhao, Ruihai Wu, Zhehuan Chen, Yourong Zhang, Qingnan Fan, Kaichun Mo, and HaoDong. Dualafford: Learning collaborative visual affordance for dual-gripper object manipulation.International Conference on Learning Representations (ICLR), 2023. Bingyang Zhou, Haoyu Zhou, Tianhai Liang, Qiaojun Yu, Siheng Zhao, Yuwei Zeng, Jun Lv,Siyuan Luo, Qiancai Wang, Xinyuan Yu, Haonan Chen, Cewu Lu, and Lin Shao. Clothesnet:An information-rich 3d garment model repository with simulated clothes environment. ICCV,2023. Zhehua Zhou, Jiayang Song, Xuan Xie, Zhan Shu, Lei Ma, Dikai Liu, Jianxiong Yin, and SimonSee. Towards building ai-cps with nvidia isaac sim: An industrial benchmark and case study forrobotics manipulation. arXiv preprint arXiv:2308.00055, 2023. Berk alli, Aaron Walsman, Arjun Singh, Siddhartha S. Srinivasa, P. Abbeel, and Aaron M. Dol-lar. Benchmarking in manipulation research: The ycb object and model set and benchmarkingprotocols. ArXiv, abs/1502.03143, 2015.",
  "AGarmentLab Assets": "Rigid Object We mainly import objects from ShapeNet,PartNetand YCB dataset.Note that we have filtered out objects that are not suitable for physical simulation and haveissues interacting with garments or fluid, and then reorganized and reclassified the dataset. Articulated Object Having much higher degree-of-freedom(DoF) state spaces, articulatedobjects are, however, generally more difficult to understand and interact with compared to3d rigid objects. We mainly import articulated objects from PartNet-Mobility dataset including Chair, Box, Bucket, Washing machine and Storage Furniture etc, to establish thecomprehensive tasks for indoor robots such as folding clothes and putting them into thewardrobe. Garment and Cloth We select garments from ClothesNet , a large-scale dataset of 3Dclothes objects with information-rich annotations. We select garments from 11 categoriesincluding Hat, Tie, Mask, Gloves and Socks and use two physical simulation method tosimulate them. We also include standard square cloths, like dishcloths and tablecloths, tocover indoor task needs. Note that there are still gaps between meshes and ready-to-simulateobject, we do post-processing of garments including giving correct physical parameters tosimulate them. Robot. We deploy a variety of specialized robots for diverse tasks, including a 7-DoFFranka manipulator with a parallel gripper, a UR5 with suction for manipulation tasks,and a RidgebackFranka with wheels for mobility and navigation. For dexterous tasks,we use ShadowHands mounted on a UR10e. Human Model. We incorporate human model to construct long-horizon tasks, such asdressing up. Utilizing avatars selected from actorcore, we assign specific motions toeach avatar to facilitate collaboration with robots. Each avatar comprises articulated joints,surface skin mesh, and clothing, enabling realistic simulation of human structure and motion. Materials. Materials are crucial components of virtual relightable assets, defining theinteraction of light at the surface of geometries. We carefully choose materials from theOmniverse Base Material library to attain optimal rendering outcomes, a critical aspect forvisual-based algorithms. Moreover, diverse textures can aid algorithms in understanding therelationship between an objects appearance and its physical behavior.",
  "B.1Overview": "Generalization ability.As a novel environment, GarmentLab especially focus on evaluating andimproving the generalization abilities of algorithm. We evaluate the generalization ability from thefollowing aspects. Novel Object Thanks to rich GarmentLab Asset, we split garment and other objectdataset into Train/Var/Test at proportion 70%/15%/15% to test algorithms generalization ability onobject level. Moreover, as garment and deformable object have nearly infinite self-deform state, weintroduce Novel State. For example, we disturb garment initial state and test models ability onhandling wrinkled and folded clothes. Moreover, in order to improve algorithm ability of planningand collision avoidance, we also involve Novel Scene, as for task like make up table, the shadow ofirrelevant object can also influence navigation. Metrics.We primarily use the success rate as the evaluation metric. It is important to note thatbecause garments and fluids can easily change state due to gravity or friction, we consider a tasksuccessful if it meets the success criteria and maintains this state for at least five seconds. Fortasks that appeared in previous work, we adopted the widely accepted success criteria and tolerancethresholds from the goal state. For example, we use Intersection-over-Union (IOU) between the targetand the folded garments to evaluate folding task and use coverage area to evaluate unfoldingtask. For novel tasks such as washing or blowing, the goal states and tolerances are derivedaccording to human behaviors.",
  "B.2Experiment Task Setting": "For large garments like tops, dresses, and trousers, we chose folding, hanging, and unfolding tasks.And for small items like hats and gloves, we selected hanging and placing tasks to evaluate visualand RL algorithms. The detailed experiment settings for the tasks listed above are shown below: Garment-hanging task requires robots to hang garments to a fixed couple. The first criterion ofsuccess is that the garment can be hanged steadily (five seconds in real experiment) on the couple.Then to ensure that the garment is hanged in the right pose (not hanged at sleeve or other strangecases), we compare the manipulation result with a standard human demonstration by computing thesum particle-to-particle distance between two states. The total distance within a predefined value willbe regarded as success. The initial states of garment to hang is obtained by dropping from randominitial poses over the ground. Unfolding task requires robots to unfold garments at random deformations to be flat. FollowClothFunnel unfolding task success when the garment ground-truth vertices are within a reasonablerange of the initial state, i.e., the flat state before the vertices were disrupted. This is because wefound that using coverage area as defined by FlingBot may not reasonably reflect success asthe garment structure becomes more complex. The initial states of garment to unfold is obtained bydropping from random initial poses over the ground. Folding task requires robots to fold garments from a flat states. After manipulation, we calculate theparticle-to-particle distance between a.the final state of garment after manipulation trial and b.thegarment state obtained by human demonstration. The manipulation whose total distance is lower thana predefined value can be regarded as success. The initial states of garments to fold are obtained byplacing flatly on random position with small disturbance. Hat-Hanging task requires robots to hang hats to a fixed couple. The criterion of success is that thehat can be hanged steadily on the couple and would not fall on the ground. The initial states of hat tohang is obtained by dropping from random initial poses over the ground. Placing task requires robots to get the hats/gloves which are previous hanged at the couple. Theplacing task succeeds when the hats/gloves are fetched and placed on the right position withoutfalling on the ground. The initial states of hats/gloves to fetch and place are obtained by randomdropping from a random poses over the couple, where only the successfully hanged cases will beused for training and testing.",
  "B.3Detailed Analysis": "Vision-Based Algorithm. Comparing the three vision models, we found that UniGarmentManip(UGM) have the best performance on Large-piece of garments. We conjecture that this stemsfrom (1) The consistence of representation on self-deformations. As model have the understandingof deformation on garments, it is easier to detect keypoints required in Folding tasks and Flingtasks in diverse garment states. (2) The explicit design of cross-object representation consistency.UniGarmentManip use skeleton(a graph of keypoints) as the shared bridge for different garmentswith similar structures, which makes model have the ability to understand the topology of the 3Dobject in the same category. However, we found that this result is not as good as the original paperusing PyFlex. This could be because we introduced the robot and the scene here, so some invalidselection points, such as those beyond the robots reach or causing collisions, were consideredunsuccessful. Additionally, while the original paper used only T-shirts, our study included jacketsand other garments with front openings, this wider variety of clothes also increased the complexity ofour task. For Affordance, we found that it performs well in Hanging tasks possibly because of itstask-specific designed which makes it chooses the grasp points more accurately. In contrast, DIFThas poor performance on these three tasks especially on unfolding tasks due to the unawareness ofgarment deformations on 2D pretrained correspondence. This is reasonable because most objectsin world for training do not have garment-level deformations. However, DIFT perfors better withsmall-piece garments like hats and gloves due to their minimal deformation, Besides, the pretrainmodel based on large diffusion model are more robust to rotation, which is crucial for handling smallclothes. For UniGarmentManip and Affordance, they are not 3D-equivariant models, so they aremore sensitive to rotations, resulting in poorer performance with small clothes compared to DIFT. RL-Based Algorithm. We modified the traditional PPO by directly replacing the value net ofPPO with the success information from the simulation ground-truth information. This is because inrobotics tasks, the value of the policy can be easily obtained from the ground-truth information.At thesame time, following ClothFunnel and FlingBot, we primarily used RL for selecting pointsand adopted a scripted policy for the trajectory, with simple adaptations based on the selected points.This is because directly using RL to train the trajectory is particularly unstable. We will elaborate onthis point in more detail below. We found that PPOs performance on state-based tasks was significantly worse than visual algorithms.After analyzing the training videos, we identified several reasons for this: (1) Abnormal trajectoriesof the robotic arm caused collisions and pushed the clothes far away. (2) Large reward fluctuationsin long-horizon tasks led to training instability, as the robotic arms random folding actions in theearly stages caused significant reward variations. (3) Completing long-horizon tasks was difficultdue to the robotic arms abnormal trajectories disrupting previous steps, such as interfering with thesleeves during the folding task. (4) Wide-ranging movements of the robotic arm caused clothes towrap around it, particularly in the hanging task, leading to failure. The visual algorithm avoidedthese issues because its execution trajectories were mostly predefined, such as pick-and-place or flingtrajectories. We also found that the performance of visual-based PPO is significantly inferior to state-based PPO,due to the following reasons: (1) The higher dimensionality of visual input makes training moredifficult. (2) The visual input consists of partial point clouds, which can confuse the model, especiallyfor thin objects like clothing. (3) Detecting the object position is more challenging for visual input,resulting in algorithmic failures during the grasping stage. These findings are consistent with those ofSoftGym .",
  "B.4.1UniGarmentManipulation (UGM)": "For Hyper-parameters selection, we set batch size to be 32. In each batch, we sample 32 garmentpairs. For each garment pair, we sample 20 positive and 150 negative point pairs for each positivepoint pair. Therefore, in each batch, 32 32 20 data will be used to update the model. Duringthe Correspondence training stage, we train the model for 40,000 batches. During Coarse-to-fineRefinement, we train the model for 100 batches. During Few-shot Adaptation, we slightly refine themodel using 5 demonstration data. Besides, we set the number of skeleton pairs to be 50. For computational resource, we use PyTorch as our Deep Learning framework. Each experiment isconducted on an RTX 3090 GPU, and consumes about 22 GB GPU Memory for training. It takesabout 12 hours to train the Coarse Stage, with 1-2 hours of Coarseto-fine Refinement and 0.5 hoursFew-shot Adaptation.",
  "B.4.2Affordance": "For Hyper-parameters selection, we set batch size to be 128, where each pair contains one positivemanipulation point and one negative manipulation point on the same garment, automatically balancingthe training data. Positive means manipulating on that point can lead to the success of the whole taskwhile negative means failure. In each batch, 128 2 data will be used to update the model. Duringthe Affordance training stage, we train the model for 36,000 batches. The model is designed to makebinary classification with cross-entropy as loss function. The output of affordance model reflects thesuccess rate when manipulating on that point, which ranges from 0 to 1. During manipulation, wejust select the point with the highest score to manipulate. For computational resource, we use PyTorch as our Deep Learning framework. Each experiment isconducted on an RTX 3090 GPU, and consumes about 16 GB GPU Memory for training. It takesabout 18 hours to train the model for a task with a specific category of garment.",
  "B.4.3DIFT": "As pretrained model, DIFT use stable-diffusion as backbone. For Hyper-parameters selection andprompt enginering We use the default parameters of DIFT. We crop the image size to 762 762 andset timestep for diffusion to 261. The ensemble size was set to 8. We use official network architecture pipeline followed by our own designed robot excution pipeline. The robot execution pipeline issimilar to UniGarmentManip. We only substitute the query model to DIFT. For computationalresource, we use PyTorch as our Deep Learning framework. Each experiment is conducted on anRTX 3090 GPU, and consumes about 20 GB GPU Memory for inferencing.",
  "CRelated Work": "Traditional Embodied and Robotic Simulator The simulator plays an indispensable role in roboticsdevelopment as it allows for the rapid and safe acquisition of vast amounts of interaction data,facilitating the implementation of various algorithms. However, the majority of mainstream robotsimulators primarily support rigid object simulation including the collision and frictionbetween them. Besides, most of robot simulators are CPU-based, severely limiting theirparallel capabilities and resulting in slow training speeds. Additionally, these simulations exhibita significant sim2real gap due to the absence of comprehensive sim2real algorithm designs.Nevertheless, based on Isaac Sim, our benchmark not only supports parallel data collection butalso incorporates comprehensive sim2real designs, including RL-based and vision-based algorithms.Deformable and Cloth Benchmark In recent years, there has been a surge in deformable and garmentsimulation environments. However, the most server problem of these kinds of simulationor benchmarks is that they can only simulate certain kinds of objects as they only support onesimulation method, which makes it impossible to explore the physical interaction between multiplekinds of objects. Moreover, these benchmarks are lack of diversity as they are built directly on theunderlying simulation architecture and have not integrated with mature platforms, thereby limitingthe range of simulated objects and scenes. For instance, softgym, built on NVIDIA Flex, isconfined to simulating tops and trousers while fluidlab, built on Taichi, can only simulatefluid and performs poorly on rigid objects simulation. Additionally, many benchmarks lackthe ability to import robots and establish real grasps, posing significant challenges for joint control andvision-based algorithms. By contrast, GarmentLab provides sophisticated 3D meshes and facilitatesvarious simulation techniques, enabling the modeling of garments, fluids, flow dynamics, avatars,rigid and articulated objects, and their interactions. This inclusive and adaptable platform offers amore comprehensive solution for research and development. The full detailed comparison of outbenchmark between others can be found in Appendix A.Garment and cloth manipulation Manipulating a single garment or cloth is a well-studied area, withprevious works focusing on learning policies for specific tasks such as folding , unfolding ,grasping , and dressing-up . However, as many daily tasks involve interactions betweenvarious physical media, current algorithms often fall short in solving real-life tasks. Although manyproposed algorithms have full potential to solve these problems, they are hindered by thelack of a mature simulation platform capable of supporting such simulations. Furthermore, whilecurrent research predominantly emphasizes gripper manipulation tasks, we introduce tasks utilizingsuction, dexterous hands, and mobile robots. We believe that GarmentLab will make a unique andvaluable contribution to the robotics community by providing a new platform for developing garmentmanipulation algorithms and significantly expanding the scope of existing methods.",
  "D.1.1Position-Based Dynamics (PBD) for Garment": "Position-Based Dynamics (PBD) is an efficient and stable method for simulating cloth, particularlysuitable for complex garments like dresses. PBD models deformable objects as systems of inter-connected particles governed by constraints that dictate their physical interactions and behaviors.In PBD, a dress is represented as a triangular mesh where particles serve as discrete points on thecloth surface with attributes such as position xi, velocity vi, and inverse mass wi. The methodoperates by directly manipulating particle positions to satisfy a series of constraints, achieving stablesimulations of deformable materials. These constraints include stretching constraints, which enforcedistance maintenance between neighboring particles to prevent excessive elongation, mathematicallydefined as C(xi, xj) = xi xj d, where d is the rest distance between particles xi and xj.Bending constraints maintain angles between adjacent triangles in the mesh to simulate resistance to bending, formulated as C(xi, xj, xk), where the constraint function depends on the angle betweenparticle triplets. Collision constraints detect and resolve collisions between particles and other objects,ensuring realistic interactions within the environment. The PBD algorithm involves initializingparticles and constraints based on the dresss geometry and material properties, applying externalforces such as gravity, predicting new particle positions as pi = pi + t vi where t is the timestep, iteratively adjusting particle positions to satisfy constraints, updating particle velocities, anddetermining final positions for each time step.",
  "D.1.2Position-Based Dynamics (PBD) for Fluid Simulation": "Position-Based Dynamics (PBD) is a powerful method for simulating fluids due to its computationalefficiency and stability. PBD treats fluids as collections of particles, where each particle represents asmall volume of the fluid. Constraints are applied to ensure physical properties such as incompress-ibility and realistic fluid behavior. In fluid simulation, particles are characterized by attributes such asposition xi, velocity vi, and inverse mass wi.",
  "D.1.3Finite Element Method (FEM) for Simulating Deformable Objects": "The Finite Element Method (FEM) is a robust numerical technique for simulating the mechanicalbehavior of deformable objects, ideal for intricate geometries and diverse material properties, suchas a toy bear. FEM discretizes the object into a mesh of finite elements and solves the equations ofmotion to accurately capture realistic deformations under various forces. In FEM, the deformable object is represented by a mesh consisting of nodes and elements. Nodesare points where the equations of motion are solved, and elements are polyhedral shapes, suchas tetrahedrons, that connect these nodes. Material properties, including elasticity, density, anddamping, determine the response of the object to applied forces.Modeling a deformable bodyinvolves several key steps. First, a deformable body component is added to the mesh, whichgenerates collision and simulation tetrahedral (tet) meshes from the source mesh. The mesh is thenseparated into visualization, collision, and simulation tetmeshes, each serving distinct purposes inrendering, collision resolution, and simulation. Configuring the material properties involves definingcharacteristics such as stiffness and dynamic friction by creating and binding a new deformable bodymaterial.",
  "D.1.5Rigid Body Simulation": "Rigid body models are essential for simulating solid objects that move and interact based on physicallaws without deforming. These simulations accurately represent the dynamics of solid objects undervarious forces. Key components include a rigid body component, which provides properties likelinear and angular velocity, and a collision component, which defines how the body collides withother objects.The dynamics of rigid bodies are governed by solvers such as Temporal Gauss-Seidel(TGS) and Projected Gauss-Seidel (PGS), which ensure stability and efficiency. TGS improvesconvergence by considering temporal aspects of the simulation, while PGS iteratively projectsvelocities to satisfy constraints.Rigid bodies interact through collisions defined by collision shapes,which can be approximated using convex hulls, bounding shapes, or signed distance fields (SDFs).These approximations balance accuracy and computational performance.Mass properties of rigidbodies are derived from the volume and density of their collision geometries. For more precisecontrol, explicit mass or density values can be set using a Mass component. This allows for accuratesimulation of complex interactions and dynamic behaviors.",
  "D.3Parameter Effects on Physical Properties": "In most cases, changes in parameters do not significantly alter the physical properties. For PBDsimulations involving garment, the Particle Contact Offset parameter affects the thickness of thefabric; as its value increases, the fabric becomes progressively thicker. The Rest Offset parameterinfluences the distance between the dress and the ground upon landing, with an increase in this valueresulting in a greater distance between the dress and the ground after it lands. For PBD simulations involving fluid, the Velocity parameter affects the flow rate of the liquid; as itsvalue increases, the liquid flows faster. The Cohesion parameter affects both the shape and flow rateof the liquid; at lower values, the liquid falls quickly and splashes out. As the value increases, theliquid flow slows down and splashing decreases, eventually leading to a smooth flow. The ParticleContact Offset parameter affects the form of the liquid as it falls; as its value increases, the liquidtransitions from a continuous stream to a segmented, chunk-like flow. In simulations involving deformable bodies, the Vertex Velocity Damping parameter affects the fallspeed of objects such as hats; as the value increases, the fall speed decreases gradually. The SettlingThreshold parameter also influences the fall speed of hats; increasing its value results in a slowerfall speed, but once the value exceeds 1, the fall speed stabilizes. The Elasticity Damping parameterimpacts the shape of the hat; as the value increases, the hat gradually collapses from a firm structureto a flat plane. The Youngs Modulus parameter also affects the shape of the hat; at lower values(around 1e3), the hat collapses into a smaller height. As the value increases, the hat becomes firmer,and when the value reaches around 1e4, the hat initially stays firm and then gradually collapses. At avalue of 15000, the hat remains completely firm. For rigid body simulations, the Max Linear Velocity parameter affects the fall speed of rigid bodiessuch as hats; as the value increases, the fall speed decreases. When the value exceeds 50, the objectpractically stops falling. In the context of flow simulations, the X-Component, Y-Component, and Z-Component parameterstogether determine the direction of the wind vector, while the Magnitude parameter determines thestrength of the wind.",
  "ESim2Real": "Transferring models trained in simulator to reality is challenging and become a critical issue forrobotic research.However, most Sim2Real techniques are not yet fully automated and require carefulhuman oversight. In this work, we present three visual sim2real methods which are fullyautomated and self-supervised. We mainly conduct experiment follow , learning dense visualrepresentation for garment before and after our alignment method. The results are shown in",
  "E.1Sim-Real Vision Alignment": "Noisy Observation.Although previous do dedicated exploration on how to add noise to pointcloud, they need capture IR picture and do many calculation which is time-consuming.However, we have found that simply adding salt-and-pepper noise and Gaussian noise can alreadyyield very good results. We directly add noise to depth picture and generate noised point cloud in thetraining data.",
  "D(x, y) + N(0, 2)with probability pgaussianD(x, y) + saltwith probability psaltD(x, y) pepperwith probability ppepperD(x, y)otherwise": "where D(x, y) represents the depth value at pixel (x, y) in the original depth picture, N(0, 2)represents Gaussian noise with mean 0 and variance 2, and salt and pepper noise is added withprobabilities pgaussian, psalt, and ppepper respectively. For experiment, we add noise to the training data during the training process of dense visualcorrespondence. As shown in figure6, before we do data argumentation for training data, thequery results show many spots, indicating errors. This is due to discontinuities of dense representationcaused by differences between the real-world point cloud and the simulator. After we add noise, themodel become more robust to noise thus the representation become more smooth and accurate. Point Cloud Alignment.Dense object descriptors that learn point- or pixel-level objectrepresentations are proposed by and for robotic manipulation. The key idea of these worksis to represent an object as a function f that maps a 3D coordinate x to a spatial descriptorz = f(x) of that 3D coordinate:f(x) : R3 Rn. f may further be conditioned on point cloudP R3N and usually parameterized by a neural network. However, f are not always SE(3)-equivariant, which means to a rigid transform (R, t) SE(3), we can NOT guarantee that f(x|P) f(Rx + t|RP + t). However, in real world experiment, as the height and angle of the camera maybe different from that in the simulator, the distributions of point cloud collected in simulation andreal world are different. This will lead to wrong query result especially for garment as it highly relyon thickness to detect folding relationships. Although we can choose SO(3)-equivariant network, the training of it is hard and time-consuming.Thus, we propose a direct way to align the point cloud in simulation and the realworld. As all rigidtransform (R, t) SE(3) can be represented by affine matrix, we directly use gradient descent tooptimize the affine matrix so that we can align the distribution between realworld point cloud andsimulation point cloud. We chose the chamfer distance as the loss function because it is both robustto the various deform and shape of the garment and effectively aligns the positions. Equation 1 showour optimization objective and 2 show our loss function.",
  "As shown in figure 6, before we align the point cloud, the model predict wrong result even in the flatcase. After alignment, the model successfully predict the correct result": "Keypoint Embedding alignment.As the model learn point level representation, a direct way isto align the representation of corresponding point between realworld garment and simulation. Inthis part, we first attach marker to garment on the skeleton points(shoulder, end of the sleeve andbottom corner etc.) Then, we do self-play which enable franka to randomly choose the pick-and-placepoint to create garment deformation status. Then we use SAM to detect key point and aligncorrespondence key point with the simulation result. After we get ground-truth key point pair betweensimulation and realworld,we employ InfoNCE a widely-used loss function in one-positive-multi-negative-pair contrastive representation learning, to pull close the representation of correspondingpoints, while push away representation of them and other point representations. The loss function isshown in Equation 3",
  "FReal-World Benchmark": "Benchmarking and performance evaluation in robotic manipulation encounter challenges owing tothe diverse range of applications and tasks, prompting research groups to select representative tasksand objects that are frequently inadequately specified and inaccessible to others, thereby impedingthe ability to compare experimental results and interpret performance quantitatively, particularlyin real-world scenarios. To address this issue, the implementation of a real-world benchmark iscrucial, as it can not only narrow sim2real Gap but also provide a platform for researchers to directlycompare algorithm performance. Although previous work has introduced real-world benchmarks,such as YCB and furniture benchmark , they primarily focus on rigid bodies, lackingbenchmarks specifically designed for deformable objects. In this study, we introduce the firstreal-world benchmark for deformable objects and garments, facilitating the widespread usageof a standardized set of objects and tasks to enable easy comparison of results among researchgroups worldwide.",
  "F.1Object and Data Set: Object Selection": "PrincipleWe aimed to select objects that are frequently used in daily life, and we also reviewed the literature toconsider objects that are frequently used in simulations and experiments. Several additional practicalfactors must be considered when formulating the proposed set of goals and tasks. VarietyThe objects included are small in number, but ensure a great richness. Judging from the category ofitems, we roughly include tops, pants, skirts, socks, gloves, dolls, etc. Considering size, generallyspeaking, clothes occupy a larger area, followed by dolls, and then small items such as socksand gloves. Considering deformability, large items of clothing are the softest, can be stackedinto various shapes, and have the highest deformability, followed by small items, which canonly undergo simple changes because they are relatively small, while dolls are elastic but lackdeformability. Grasping and manipulation difficulty was also a criterion: for instance, toys are wellapproximated by simple geometric shapes and relatively easy for grasp synthesis and execution,while garments have higher shape complexity and are more challenging for grasp synthesis and execution. In addition, since we are doing a benchmark about garments, we have to carefullyconsider their characteristics: they are diverse and highly deformable, but the same type of garmentoften only differs in texture or color, and is very similar in structure and key points. This allowsus to use a few objects to represent a category of garments, thereby ensuring the variety of ourbenchmark. UseWe included objects that are not only interesting for grasping but that also have a wide rangeof manipulation uses. Soft and highly deformable clothing is also suitable for many complexoperations: such as hanging, folding, etc. The introduction of fluid allows us to simulate theinteraction between some objects and fluids, such as washing and air-drying. In addition, we alsoincluded people, which allowed us to simulate the interaction between some objects and people,such as putting a scarf on someone. As mentioned above, these tasks are intended to span a widerange of difficulty, from relatively easy to very difficult. DurabilityWe aimed for objects that can be useful in the long term, and, therefore, avoid objects that arefragile or perishable. In addition, to increase the longevity of the object set, we chose objects thatare likely to remain in circulation and change relatively little in the near future. CostWe aimed to keep the cost of the object set as low as possible to broaden accessibility. We, therefore,selected standard consumer products, rather than, for instance, custom-fabricated objects, and tests.We buy all our clothes from Uniqlo and all our dolls from Disney.",
  ": Objects Scanning Process": "Model wearing clothesThere are two main ways to scan clothes: scanning them flat and scanning them while the model iswearing them. After careful consideration and constant experimentation, we chose the latter. Thisis because we use a large number of points to simulate clothing, so the wrinkles formed when theclothes are laid flat will cause the points to be unevenly distributed, thus forming many \"holes\".While when worn on the model, the appearance of wrinkles will be reduced, thus try to avoid thissituation as much as possible. ScanningWe use a scanner to scan the object from multiple angles, and obtain a copy of the original pointcloud data and grid data through the combination of the depth camera and the RGB camera. Thegeneral process can be referred to . Post-ProcessingDuring the post-processing process, we mainly did three things: down-sampling, adding texturefiles, and processing remaining holes. The point set obtained by the initial scan has too manypoints and is difficult to support with ordinary computing power, so we performed down-samplingto generate a file that can retain the main features and have a moderate number of points. Weuse Meshlab for down-sampling. The original scale is about 100 million points and 300 millionfaces. After down-sampling, it can reach about 10,000 points and 30,000 faces. We use MTL files(Material Library File) to add material attribute information. MTL is a material library file usedto describe the material information of objects. It is usually used in conjunction with an OBJ fileto apply material properties such as texture and color to the OBJ model. In this step, we mainlyimplemented some visual textures, such as patterns, colors, etc. The physical texture is achievedthrough different simulation methods. In addition, there are still some holes in the processedpoint set, which we repaired manually. Manual annotation of key pointsSince we use a large number of points to simulate objects, it is necessary to mark some key pointsand edges to indicate important features. For example, for tops, we will mark the sleeves, neckline,hem, etc. These locations are often the key parts for clothing operations. For dolls, we will markarms, legs and other parts that are easy to grasp. gives some examples for reference.",
  "F.3Protocol Benchmark Guidelines": "We use the protocol and benchmark templates mentioned in . To both provide more concretesamples of the types of task definitions that can be put forward as well as specific and usefulbenchmarks for actually quantifying performance, we have developed some example protocols:clothes-hanging protocol, scarf-wearing protocol. Clothes-hanging protocol and benchmarkWhen dealing with flexible clothing, hanging clothing is always a popular task. The protocol usesthe hanger, clothes suitable for hanging of our model set. The clothes are initially laid flat on theplatform, and the robot is expected to grab the key points of the clothes, pick them up, and finallyhang them on the hangers. The benchmark scores the performance of the robot by evaluatingwhether the hanging point is reasonable and the stability of the hanging clothes (whether they areeasy to fall off). We applied this benchmark to Franka. Scarf-wearing protocolDressing people with robots has always been a difficult subject. In this example, we chose theeasier and more manageable task: wearing a scarf. This protocol uses the scarf from the modelset, and a person from the Issac Simulator. The scarf is initially laid flat on the platform, and whatthe robot has to do is to pick it up and wrap it around the persons neck, and finally adjust it to asuitable state. The benchmark scores the performance of the robot by evaluating whether the finalstate of the scarf is stable (we can test it by adding wind to see if the scarf will blow off quickly),whether the remaining length of the scarf on both sides is similar, and whether the scarf fits thepersons neck (rather than loosely packed). We applied this benchmark to Franka.",
  "G.1Task category": "Garment-Garment. This category focuses on fundamental garment manipulation, including taskslike folding and unfolding single garments, as well as interactions between multiple garments such asretrieving items from clothes piles. Tasks in this category include folding, unfolding (pick and place),and unfolding (fling). Garment-Fluid. Tasks in this group concentrate on the interaction between garments and fluids aswell as flow, where trajectory dynamics play a crucial role. This category of tasks includes washingclothes in a basin, rinsing clothes under running water, and drying clothes with a hairdryer. In thiscategory, we specifically introduced the interaction between the robot manipulating objects and fluidflow, including both water flow and air flow. Garment-FEMObjects. We mainly focus on the exploration of tasks involving deformable interac-tions, such as using a sponge to clean dirt off clothes or packing hats and tops together. Some simpletasks involving the manipulation of deformable objects are also included, such as using a dexteroushand or gripper to grab plush toys. Garment-Rigid. Common interactions between clothing and rigid bodies, such as hanging clothes orputting them into a washing machine, require precise grasp point selection and trajectory planning.Wealso introduced articulated objects such as cabinet drawers and clips to perform garment-related tasks,such as taking clothes out of a wardrobe. Garment-Avatar. Dressing tasks pose the greatest challenge, as they require understanding of humanintention and safe collaboration with humans. Some representative tasks include putting a scarf on aperson and placing a hat on their head. More advanced tasks involve dressing a person in a jacket ora T-shirt.",
  "HMoveIt": "We adopt MoveIt, an open-source state-of-the-art robotic manipulation framework, to provide supportfor real-world trajectories planning and obstacle avoidance. To record the trajectories generated byMoveIt and adapt visual models in the simulator to the trajectories could bridge the sim2real gapto some extent. In this section, we introduce a smooth, lightweight and responsive signal pipelineimplementation to transfer real-world joint parameters to the simulator. As a robotic manipulation platform, MoveIt is built on top of ROS(Robot Operating System) andintegrates with various ROS components. MoveIt provides a series of comprehensive manipulationinterfaces, including collision-free motion planning, kinematics computation, collision detection, etc.Moreover, real-world data collected from sensors like cameras and lidars can be fed into MoveIt,allowing for dynamic obstacle avoidance. Once the trajectories are generated by MoveIt, we publishthe computed joint states through ROS, which transfers the trajectory to the Franka controller,FrankaPy. FrankaPy is a modular control stack that provides a customizable and accessible interfaceto the Franka robot. Utilizing MoveIt and FrankaPy, this pipeline enables Franka to devise a collision-free path and guide the gripper to the target posisition using vision detectors, while publishing thejoint parameters to the ROS server. The simulator then subscribes the joint states and moves theFranka model accordingly.",
  "ITeleoperation": "Teleoperation serves as a direct method to acquire human demonstrations for model training. Toaccurately and smoothly track human hand motions has been proved advantageous in related worksrecently. However, the proliferated fine-grained tracking requirements, along with sparse and diversedexterous hand models and environment settings, have posed a challenge towards teleoperationsystems. Compared to controller-based models, we utilize the vision-based motion detection module,Leap Motion, to efficiently record human hand poses and then retarget hand poses to the dexterousrobot hand. More Formally, our teleopertaion systems can be described as below:",
  "(iii) motion generating module, which produces accurate, responsive and high-frequencysignals for the robot model that in the simulator and in the real world simultaneously": "In our work, we implement teleoperation for Universal Robot mounted with Shadow Hand andFranka. One can control the posture of the robot by adjusting attitude and position of the hand overthe detector. In particular, the open state of the gripper of Franka can be controlled by the openingand closing of the thumb and index finger.",
  "I.1Leap Motion Detection Module": "The Leap Motion Controller is a small USB device that can be placed on the desktop. Utilizing two640x240-pixel near-infrared cameras, it captures a roughly hemishperical area in the distance ofapproximately 60 cm, typically at 120Hz. The internal algorithms then translate the received rawspatial data to 27 distinct hand elements, which includes the palm normal vector, the hand direction,the wrist position and 24 finger joint positions. The detailed hand elements are shown in the figure12:",
  "I.2Hand Pose Retargeting": "The procedure of hand pose retargeting is two-fold: first, we map knuckle positions to hand jointparameters; second, we compute a trajectory to smoothly move the robot arm to the recorded wristposition and direction. The finger knuckle positions captured by Leap Motion cannot be directly fedinto robot models due to the discrepancy between the dexterous hand joint angular parameters andthe knuckle positions. The mapping algorithm that converts knuckle positions to joint parameters isoften formulated as an optimization problem, which can be described as",
  "vit fi(qt)2 + qt qt1": "where qt represents joint parameters of the dexterous robot hand at time step t. fi is the i-th forwardkinematic function which takes the joint angles as input and computes the knuckle positions. is ascaling factor to alleviate the size discrepancy between different human operators and the robot handmodel. Additionally, we observe adjacent N frames and add hyperparameter to improve temporalsmoothness and consistency. To compute the arm trajectory, we adopt a slightly tweaked inverse kinematics approach, which ispopular to determine the joint parameters in the trajectory, given the URDF file (Unified RoboticsDescription Format) of the robot and the desired configuration. URDF is a file format that describesthe physical properties and 3D model of the robot, including joints, motors, articulation configurations,etc. In empirical experiments, we find that even slight hand movement or vibration can trigger asignificant and prolonged changes in arm posture. To address this problem, we implement a rate limiton target changing in neighboring frames. It is notable that this workflow is applicable to a vast range of grasp-based robots. Particularly, tocontrol the open state of the gripper of Franka, we measure the distance of the thumb and index fingerand establish a distance threshold in implementation.",
  "The answer NA means that the paper has no limitation while the answer No means thatthe paper has limitations, but those are not discussed in the paper": "The authors are encouraged to create a separate \"Limitations\" section in their paper. The paper should point out any strong assumptions and how robust the results are toviolations of these assumptions (e.g., independence assumptions, noiseless settings,model well-specification, asymptotic approximations only holding locally). The authorsshould reflect on how these assumptions might be violated in practice and what theimplications would be. The authors should reflect on the scope of the claims made, e.g., if the approach wasonly tested on a few datasets or with a few runs. In general, empirical results oftendepend on implicit assumptions, which should be articulated. The authors should reflect on the factors that influence the performance of the approach.For example, a facial recognition algorithm may perform poorly when image resolutionis low or images are taken in low lighting. Or a speech-to-text system might not beused reliably to provide closed captions for online lectures because it fails to handletechnical jargon.",
  "If applicable, the authors should discuss possible limitations of their approach toaddress problems of privacy and fairness": "While the authors might fear that complete honesty about limitations might be used byreviewers as grounds for rejection, a worse outcome might be that reviewers discoverlimitations that arent acknowledged in the paper. The authors should use their bestjudgment and recognize that individual actions in favor of transparency play an impor-tant role in developing norms that preserve the integrity of the community. Reviewerswill be specifically instructed to not penalize honesty concerning limitations.",
  ". Experimental Result Reproducibility": "Question: Does the paper fully disclose all the information needed to reproduce the main ex-perimental results of the paper to the extent that it affects the main claims and/or conclusionsof the paper (regardless of whether the code and data are provided or not)?Answer: [Yes]Justification: Section BGuidelines: The answer NA means that the paper does not include experiments. If the paper includes experiments, a No answer to this question will not be perceivedwell by the reviewers: Making the paper reproducible is important, regardless ofwhether the code and data are provided or not.",
  "If the contribution is a dataset and/or model, the authors should describe the steps takento make their results reproducible or verifiable": "Depending on the contribution, reproducibility can be accomplished in various ways.For example, if the contribution is a novel architecture, describing the architecture fullymight suffice, or if the contribution is a specific model and empirical evaluation, it maybe necessary to either make it possible for others to replicate the model with the samedataset, or provide access to the model. In general. releasing code and data is oftenone good way to accomplish this, but reproducibility can also be provided via detailedinstructions for how to replicate the results, access to a hosted model (e.g., in the caseof a large language model), releasing of a model checkpoint, or other means that areappropriate to the research performed. While NeurIPS does not require releasing code, the conference does require all submis-sions to provide some reasonable avenue for reproducibility, which may depend on thenature of the contribution. For example(a) If the contribution is primarily a new algorithm, the paper should make it clear howto reproduce that algorithm.",
  "(b) If the contribution is primarily a new model architecture, the paper should describethe architecture clearly and fully": "(c) If the contribution is a new model (e.g., a large language model), then there shouldeither be a way to access this model for reproducing the results or a way to reproducethe model (e.g., with an open-source dataset or instructions for how to constructthe dataset). (d) We recognize that reproducibility may be tricky in some cases, in which caseauthors are welcome to describe the particular way they provide for reproducibility.In the case of closed-source models, it may be that access to the model is limited insome way (e.g., to registered users), but it should be possible for other researchersto have some path to reproducing or verifying the results.",
  "Guidelines:": "The answer NA means that the paper does not use existing assets. The authors should cite the original paper that produced the code package or dataset. The authors should state which version of the asset is used and, if possible, include aURL. The name of the license (e.g., CC-BY 4.0) should be included for each asset. For scraped data from a particular source (e.g., website), the copyright and terms ofservice of that source should be provided. If assets are released, the license, copyright information, and terms of use in thepackage should be provided. For popular datasets, paperswithcode.com/datasetshas curated licenses for some datasets. Their licensing guide can help determine thelicense of a dataset.",
  "According to the NeurIPS Code of Ethics, workers involved in data collection, curation,or other labor should be paid at least the minimum wage in the country of the datacollector": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with HumanSubjectsQuestion: Does the paper describe potential risks incurred by study participants, whethersuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)approvals (or an equivalent approval/review based on the requirements of your country orinstitution) were obtained?Answer: [NA]Justification: no experiments with potential risks for study participantsGuidelines:",
  "The answer NA means that the paper does not involve crowdsourcing nor research withhuman subjects": "Depending on the country in which research is conducted, IRB approval (or equivalent)may be required for any human subjects research. If you obtained IRB approval, youshould clearly state this in the paper. We recognize that the procedures for this may vary significantly between institutionsand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and theguidelines for their institution."
}