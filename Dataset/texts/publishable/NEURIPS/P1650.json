{
  "Abstract": "Workplace meetings are vital to organizational collaboration, yet a large percentageof meetings are rated as ineffective . To help improve meeting effectiveness byunderstanding if the conversation is on topic, we create a comprehensive Topic-Conversation Relevance (TCR) dataset that covers a variety of domains and meetingstyles. The TCR dataset includes 1,500 unique meetings, 22 million words intranscripts, and over 15,000 meeting topics, sourced from both newly collectedSpeech Interruption Meeting (SIM) data and existing public datasets. Along withthe text data, we also open source scripts to generate synthetic meetings or createaugmented meetings from the TCR dataset to enhance data diversity. For each datasource, benchmarks are created using GPT-41 to evaluate the model accuracy inunderstanding transcription-topic relevance.",
  "Introduction": "Since the 2020 COVID-19 pandemic, an increasing share of meetings have shifted from in-person toonline. The Gartner 2021 Digital Worker Experience Survey reports that the number of in-personmeetings dropped from 63% in 2019 to 33% in 2021 . The same survey predicted that in 2024,only 25% of the meetings will happen in person. Together with the growing number of online meetings, there are ongoing complaints about ineffectivemeetings due to a lack of focused discussions or focused tasks . Having a meeting facilitatorto keep the discussions focused is one of the meeting design characteristics enabling more effectivemeetings . Measuring how relevant a conversation transcript is to an intended topic is crucial to quantifying howfocused the communication is, and to creating tools that behave as a virtual meeting moderator bykeeping the discussion on-track. A very low rating on the relevance of the conversation to the topicmeant for discussion would be an indicator of a non-focused discussion. In practice this translates tothe problem of keeping discussions focused on a predefined meeting agenda. While there is existing work about the importance of topics serving as input to text summarizingmodels , we could not find references about work studying the relevance of a topic to a particularbody of text it didnt originate from. One of our intuitions for why this field has had little explorationis because of technological limitations before the recent Generative AI advancements.",
  "arXiv:2411.00038v2 [cs.CL] 4 Nov 2024": "With the current advancement in the field of Generative AI, deep understanding of language andrelationships between bodies of text has reached new levels of accuracy, and has gotten very close tohuman performance . To begin investing in measuring the relevance of a conversation to a predefined agenda topic, acomprehensive dataset of conversations associated with the topic of each conversation section isneeded. Ideally, the topics should be defined before the conversation starts in a pre-meeting agendastyle. There are several public datasets built from real human conversations that serve as the base forour Topic-Conversation Relevance (TCR) dataset; however, most of the topics from these datasets arepost-meeting minutes that summarize what happens instead of what is planned. The contributions of the TCR dataset are (1) We create a large topic-conversation dataset coveringmultiple domains of meetings. This dataset consists of the newly collected meetings and aggregatedpublic data sources. (2) We use GPT-4 to rewrite long and detailed meeting minutes into a pre-meetingagenda topic style. (3) We provide a design of an extensible schema that allows users to createvariations of meetings where topics can be flexibly added and removed. (4) We open source scriptsfor data augmentation and synthetic meeting creation on top of the TCR dataset. We review the related works in . We present the datasets and the schema in , andelaborate to discuss the new SIM data collection and public data sources. In we go over thebenchmark results generated by GPT-4 on the Topic-Conversation Relevance task, and share insightsfrom running such prompts across datasets. In , we describe limits and future work in thisdirection.",
  "Related Work": "In this paper, we refer to \"topics\" as the key points to be discussed during a meeting and such topicswould have been put in the meeting agenda by the organizer before the meeting starts. To the best ofthe authors knowledge there is no research related to the task of measuring conversation relevance topre-meeting agenda topics. However, the related topic of meeting summarization, or minuting hasbeen well studied. Two challenges (AutoMin) in the field of meeting summarization have been held where teamsparticipated in order to progress the field . The first challenge had teams using BART-basedmodels achieving the best performance . With the improvements in generative AI andthe growing adoption of Large Language Models (LLMs), a second challenge was done recently.In this challenge, the participants achieved good results with different large models,such as, Llama-based Vicuna , Dolly , and GPT-3s text-davinci-003 . The challengeorganizers used GPT-4 for benchmarking as well and it demonstrated good performance for themeeting summarization task. The organizers also used GPT-4 to evaluate submissions along withhuman evaluation results, but found that it was unreliable for this task. The challenge organizers alsocalled out the need to answer research questions related to transcript summary relevance, to betterunderstand content and coverage from different annotators. There are multiple datasets for the task of benchmarking the meeting summarization task. Most ofsuch individual datasets often contains only one type of meeting. The AMI dataset is a collectionof meetings transcripts and summaries that cover the topic of product design in an scenario settingand a small amount of non-scenario meetings. The topic annotation is very brief and limited. TheICSI Corpus contains 75 project meetings and discussions in an academic environment. It hashigh-level human-annotated topics that are very brief. MeetingBank is a dataset of 1,250 citycouncil meetings from multiple US counties. Detailed meeting minutes for each meeting subsectionare documented in this dataset. The QMSum dataset aggregates three public data sources(ICSI, AMI, and Parliament meetings from Welsh and Canada) and generates minutes for the textsummarization tasks. The paper further shows that for a BART model that training a model on datafrom one of the datasets and testing it on the other one leads to poor performance. By training on alldatasets they were able to build a more robust model. To further expand on data for the automaticminute task Nedoluzhko put together the ELITR data corpus. This data consists of meetings inboth Czech and English, with transcripts and meeting minuting being taken by different annotators.In order to align the transcripts with the minutes the tool ALIGNMEET was used.",
  "Overall, the TCR dataset contains 1,506 unique meetings, 22 million words in transcripts and morethan 15,000 meeting topics. provides high-level statistics of the dataset": "The pre-selected MeetingBank data is large comparing with other data sources. To balance themeeting styles and create representative benchmark results, we also create a subset of 30 randomlyselected meetings denoted as MeetingBank_rnd30. The subset is available separately from thecomplete MeetingBank data in the TCR dataset. A summary of the balanced subsets is presented in. We also provide exploratory analysis of per meeting metrics in . The full exploratory analysiswith standard deviations is provided in the Appendix 5 . The dataset and related scripts are availablein the topic_conversation GitHub repository2.",
  "Topic TextLenth(words)": "SIM34.244.001.0034.2411.32SIM_syn10027.244.003.487.8310.72ICSI45.196.206.526.932.85QMSum_ICSI44.886.314.2710.404.15QMSum_AMI34.034.003.948.456.76QMSum_Parliament92.2123.806.4513.908.43MeetingBank109.868.545.9818.3659.47MeetingBank_ReAnnotated109.868.545.9718.3910.03NCPC141.6925.608.0017.716.27ELITR34.265.457.644.016.95 are ordered by start time. For each topic, the corresponding transcripts are presented in a list. Eachtranscript line contains the raw contents in text, speaker ID, time information, line and word counts.If the original data source does not have timestamps, the time information is estimated based on wordcounts at a fixed 150 words per minute rate for each transcript line. In such cases, the metadata marks\"timestamp_source\" as \"estimated\" for the entire meeting. We provide two scripts (script_create_synthetic_meetings_SIM.py, script_augment_data.py) in theproject repo to create more synthetics meetings or generate augmented version of the existing datasets.Outputs from those scripts follow the same data schema and can be easily combined into the existingTCR dataset.",
  "New Data Collection: Speech Interruption Meetings (SIM)": "In a previous study done by the team regarding speech interruptions and meeting inclusiveness ,we collect multi-party online meetings in which participants actively interact with each other todebate a topic. This Speech Interruption Meetings (SIM) dataset is released for the first time as partof the TCR dataset. We also create 100 synthetic meetings on top of these raw meetings. Both theoriginal meetings and synthetics meetings are included in the TCR dataset.",
  "Raw Data": "In total, we include 84 raw meetings (48.6 hours) in the TCR dataset. The meetings cover 14 differenttopics and there are about 530,000 words in the transcripts. In total, 149 unique speakers3 participatedin this batch of data collection. Speaker distribution details can be found in Appendix . The SIM data captures natural online meeting dynamics. To collect the data, we invite 4 participantsto join a remote conference call on Microsoft Teams. Each meeting has a single dedicated topic thatcan elicit debate. The participants discuss the topic for about 30 minutes. Natural interactions betweenparticipants are strongly encouraged. We collect separate audio channels and machine-generatedtranscripts for each meeting. In the transcripts, the participants are marked as speaker_1,2,3,4randomly within each meeting. We only include transcripts data in the TCR dataset at this stage asaudio is not directly related to the the benchmark task.",
  "Synthetic Meetings": "Given the raw meetings from the SIM dataset has only one dedicated topic per meeting, we alsogenerate 100 synthetic meetings with multiple topics by randomly combining meetings snippets fromdifferent topics together. The workflow to generate such synthetic meetings involves the following steps. First, we remove thefirst and last 5 minutes of the transcripts, to eliminate potential meeting setup contents, greetings, andicebreaker talk. These trimmed meetings are the candidate meetings. Second, for each new synthetic",
  ": Topic-Conversation Relevance (TCR) Dataset Schema": "meeting, we randomly decide how many unique topics (2 to 5) to include. Then, for each uniquetopic, we randomly select 5 to 11 minutes consecutive transcripts from candidate meetings with thattopic. Based on the setup, the generated 100 synthetic meetings have an average meeting length of 28minutes. We refer to this set as SIM_syn100. The scripts that we use to generate the SIM_syn100 data is shared in the project repo. With theconfigurable parameters, users can create an arbitrary number of synthetic meetings with the desirednumber of topics and duration splits.",
  "ICSI Corpus": "We use all 75 meetings from the the ICSI Corpus . Starting from the word-level transcripts fromthe original corpus, we exclude the tags for non-verbal events and keep only the transcribed words.This is because for real-time machine-generated transcripts, such events are not marked as tags, buteither transcribed as part of the contents, or omitted. For long utterances from the same speaker, weassign a line break in the transcript either when an end-of-sentence tag occurs, or there is a gap that isat least 0.5 second long between two words. We assign timestamps for each sentence based on theoriginal word-level timestamps from the data source.",
  "We make minor adjustments to the topic annotations if there is an identify-mismatch problem betweenthe topics and the speaker IDs. The speaker IDs for each meeting are assigned as speaker_A,": "speaker_B, etc., however, the topic annotations refer to speakers by either their metadata ID (e.g.,me001) or their first names. To align the different identify systems, we refer to the metadata andconvert the speaker reference style in the topic annotations to align with the transcripts by usingspeaker IDs. This guarantees the data consistency within annotations.",
  "Selected QMSum Dataset": "The QMSum data has 3 different input data sources and we treat them separately. For QM-Sum_ICSI data, we use the pre-processed transcripts and timestamps from the original ICSI corpus.We use the QMSum annotations as the new topics. Given the topic styles and the section breaksare very different between QMSum_ICSI and the original ICSI Corpus, we decide to keep bothsets of meetings and create benchmark results for them separately. For QMSum_AMI and QM-Sum_Parliament data, we remove non-verbal tags from the transcripts. As timestamps are notavailable in QMSum, we create estimated timestamps by the fixed 150 words per minute rate forthese two data sources. The annotations are done as meeting minutes in the QMSum dataset. In cases where the transcripts arenot included in the minuting, we fill the empty values by the following logic. If the missing topic is atthe very beginning of the meeting, we assign a topic of \"Beginning_no_topic\"; if the missing topicis at the very end of the meeting, we assign a topic of \"Ending_no_topic\". If the lack of annotationhappens between two topics, we assume the previous topic continues and fill the empty value bytaking the previous topic. In the QMSum annotation, it is also possible that one line of transcriptsbelong to multiple topics. We use the first annotation based on the timestamps. In any given meeting,if more than 15% of the transcripts have missing annotations or overlapping annotations issues, weexclude the meeting due to undesirable annotation quality. Overall, we keep 168 out of the original232 meetings.",
  "Selected MeetingBank Dataset": "We use the timestamps in the metadata from the original data source to exclude meetings that areshorter than 15 minutes. In total, 1,100 out of the 1,250 MeetingBank meetings are includedin our dataset. We remove unicode from both the transcripts and annotations. Though some of theoriginal timestamps do not start from 0, we keep the original timestamps as it is necessary to locatethe corresponding audio contents if needed. In the TCR data, it is very easy to align the beginning to0 by removing the start timestamp documented in the meeting metadata.",
  "In the TCR dataset, we provide two sets of annotations for the MeetingBank data:": "Original AnnotationsWe take the \"summary\" field from the MeetingBank metadata as the topicannotations. These annotations are in meeting minutes styles and often are long and very detailed.If in the original data source one sentence belong to multiple summaries, we keep only the firstoccurrence. Re-annotated TopicsThe original meeting summaries contain not only the topic for a section butoften the outcomes. To have pre-meeting style topics, we need to remove outcomes that would nothave been known before the meeting happens. Additionally some of the meeting minutes are excerptsfrom the transcripts, so modifying the annotations would give a more accurate representation of thetopic-conversation relevance benchmark. In order to rewrite a summary to a pre-meeting agenda typeof topic, a GPT-4 prompt is developed. An example of the input and output is shown below: Original summary: A bill for an ordinance changing the zoning classification for 5611 EastIowa Avenue in Virginia Village. Approves an official map amendment to rezone propertylocated at 5611 East Iowa Avenue from S-SU-D to S-RH-2.5 (suburban, single-unit tosuburban, rowhouse) in Council District 6. The Committee approved filing this item at itsmeeting on 7-10-18.",
  "Selected NCPC Meetings": "The National Capital Planning Commission (NCPC) is a government agency that meets once amonth to discuss projects for in the united states capitol region. The meeting agendas and transcriptsare publicly available. To the best of our knowledge, the TCR dataset is the first work to add this datasource to a structured dataset. We randomly select 20 NCPC meetings where agenda is available.Both meeting transcripts and agenda topics are documented in the same PDF file for each meeting. Inorder to convert the data to a structured format, the PDFs are converted to text files, and the bodyof the text is extracted, along with the topic titles. As the PDFs do not share the same structure,additional manual adjustments are applied to guarantee a high conversion accuracy. The originaltranscripts do not have time information, hence the timestamps are estimated with the fixed rate of150 words per minute.",
  "Selected ELITR Dataset": "The ELITR data is a corpus of meetings in Czech and English containing transcripts alongwith minutes written by multiple annotators. As our work focuses on English only at this stage, wekeep just the English meetings. Among the English meetings, 49 have meeting minutes that can bealigned with the corresponding transcripts. We further reduce the size of this dataset to address thefollowing challenges: First, with multiple annotations available from up to 11 different annotators permeeting, we need to keep only one annotation per meeting. Second, the meeting minutes can containtoo many detailed items that are not suitable to be considered as topics. Third, the annotations donot necessarily point to a consecutive chunk of transcripts, but jump back and forth. To account forthese issues, we keep only meetings with an annotation of at most 10 topics, and the annotationsare not interspersed. With all the filters, we include 11 meetings into the TCR dataset. If there isno annotation for some parts of the transcripts, we follow the same logic described in .3.2to fill the empty values. The original transcripts do not have timestamps, so we estimate the timeinformation with the fixed rate of 150 words per minute.",
  "Data Augmentation": "All data sources described above provide ground truth topics for subsections of transcripts. However,the list of topics in the annotation only reflect the topic that are discussed. Real meetings do notalways follow the planned agenda. Participants sometimes go off topic and have to skip somepre-arranged topics due to time limits. The TCR dataset schema is designed to test and evaluate suchscenarios by incorporating the \"variations\" section. To reflect such scenarios, we also provide a scriptto either (1) add topics that are not discussed to a meeting as a planned topic or (2) remove topics andcorresponding contents from a meeting. This can help enrich the TCR dataset to include a variedrange of meeting styles. Implementation details can be found in the project repository. For the augmented meetings, we keep the type of variation and the changed topic list in the \"variations\"field in the metadata for each meeting. shows an example of the change in metadata for anaugmented meeting. If a topic is planned, but not discussed in a meeting, the topic is added to the\"variation_addTopics\" and the corresponding empty contents are also added to the \"topic\" section.Users can easily extend this by adding topics with non-empty contents to expand the simulationfurther. If we want to remove a topic and its corresponding contents together from a meeting, thechanges are reflected in the \"variation_removeTopics\" list as well as the \"topic\" contents. Thetimestamps of the remaining contents are also changed accordingly. With this structure, we can testthe relevance between the transcripts and topics that could have been planned but are not part of theactual conversation.",
  "Methodology": "We use GPT-4 to create the benchmark results. For each meeting, we cut the transcripts into snippetswith equal length based on timestamps. We conduct the experiments in duration length of 5 minutes,10 minutes and 15 minutes. Then the prompt takes the snippet of transcripts and the full topic listfrom the meeting as inputs, and asks for an evaluation of the transcripts relevance to each topic inthe list. The relevance score is represented by 4 levels: 0 means Not Relevant, 1 means SomewhatRelevant, 2 means Mostly Relevant, and 3 means Very Relevant. The detailed definitions of therelevance levels are given as a multiple-choice question in the prompt. In the development stage, wetry different output requests, such as floats, integers, binaries and multiple choices. We present thefinal benchmark results all in the multiple choices style as it has been giving the most robust resultsacross all data sources. In the evaluation stage, we treat the Topic-Conversation Relevance problem as a binary classification.If based on the ground truth label, a topic is discussed for more than 30 seconds in the transcripts,then we mark it as \"Discussed\", otherwise \"Not Discussed\". For the GPT-4 responses, we treat\"0 Not Relevant\" as \"Not Discussed\", and everything else as \"Discussed\". In the results presentedin .2, we specifically focus on scenarios where the discussion is off-topic, so the \"NotDiscussed\" topics are treated as positive cases. We use Precision (LLM detects a topic is not beingdiscussed and it is true) and Recall (A topic is not being discussed and LLM detects it) as the mainmetrics. The full results treating \"Discussed\" and \"Not Discussed\" as positive cases respectively areshown in the Appendix A.3.",
  "The benchmark results focusing on the \"Not Discussed\" category are shown in . We split theresults by data source and transcripts length": "4Selected subset: we select 30 random meetings from the MeetingBank dataset as the structures of meetingsfrom this data sources are similar and can be represented by a subset; QMSum_AMI and QMSum_Parliamentare not included in the benchmark because the former are mostly scenario discussions that are not real meetingsand the style of latter is covered by MeetingBank and NCPC meetings.",
  "ELITR5 min705840.84290.93900.764610 min312610.81820.91840.737715 min201660.80430.87060.7475": "For the highly structured meetings (MeetingBank, NCPC), the benchmark results show very highprecision and recall. Most of these meetings follow the pre-defined agenda topics strictly and oftenstate the topic to-be-discussed at the beginning of the section. Different annotations do not impactthe results much. The other less structured meetings, such as project meetings (ICSI, ELITR) andbrainstorming discussions (SIM), are more challenging. Most of these meetings do not have clearstatements separating different topics and related sub-topics are often discussed back and forth.Different topic annotations can impact the results significantly. We also notice that if there are multiple topics included in the same snippet of transcripts (8), it iseven more challenging to correctly predict the relevance comparing with single-topic transcript (9).This could be due to the fact that transitions between topics are not always clear in the less structuredmeetings. Results split by topic counts are also included in Appendix A.3.",
  "Future Work": "The dataset can be further improved by including more types of meetings in different domains.However, it is particularly hard to obtain real day-to-day meetings in a working environment as mostof such meetings consist sensitive business information. Hence the project team is working on invitingdomain experts (e.g., legal, healthcare, finance, etc.) to create meeting agendas for different typesof meetings in their industry, and conducting domain-specific meetings based on the agendas. Weare currently in the data collection stage using the same method as the SIM dataset, with additionalrequirements on participants professional experience. In addition to English, we are also working on integrating other languages into the dataset. One of theefforts is to translate the current data sources into other languages with reliable translation servicesand test the performance on the same tasks. A challenge of evaluating topic-conversation relevance is the blurred boundaries between topics. Ata meeting structure level, a certain chunk of transcripts can be marked as belonging to a topic, butit is very likely that some parts of the conversation are actually not directly related to the topic or",
  "Andrew Kyle Lampinen. Can language models handle recursively nested grammatical struc-tures? A case study on comparing models and humans. 2023. arXiv: 2210.15303 [cs.CL]": "Tirthankar Ghosal et al. Overview of the first shared task on automatic minuting (automin)at interspeech 2021. In: Proceedings of the First Shared Task on Automatic Minuting atInterspeech (2021), pp. 125. Tirthankar Ghosal et al. Overview of the second shared task on automatic minuting (au-tomin) at inlg 2023. In: Proceedings of the 16th International Natural Language GenerationConference: Generation Challenges. 2023, pp. 138167. Kartik Shinde et al. Team ABC @ AutoMin 2021: Generating Readable Minutes with a BART-based Automatic Minuting Approach. In: Proc. First Shared Task on Automatic Minuting atInterspeech 2021. 2021, pp. 2633. DOI: 10.21437/AutoMin.2021-2. Atsuki Yamaguchi et al. Team Hitachi @ AutoMin 2021: Reference-free Automatic MinutingPipeline with Argument Structure Construction over Topic-based Summarization. In: Proc.First Shared Task on Automatic Minuting at Interspeech 2021. 2021, pp. 4148. DOI: 10.21437/AutoMin.2021-4. Frantiek Kmjec and Ondrej Bojar. Team iterate@ automin 2023-experiments with iterativeminuting. In: Proceedings of the 16th International Natural Language Generation Conference:Generation Challenges. 2023, pp. 114120. Eugene Borisov and Nikolay Mikhaylovskiy. Team ntr@ automin 2023: Dolly llm improvesminuting performance, semantic segmentation doesnt. In: Proceedings of the 16th Inter-national Natural Language Generation Conference: Generation Challenges. 2023, pp. 132137. Felix Schneider and Marco Turchi. Team zoom@ automin 2023: Utilizing topic segmentationand llm data augmentation for long-form meeting summarization. In: Proceedings of the16th International Natural Language Generation Conference: Generation Challenges. 2023,pp. 101107."
}