{
  "Abstract": "Many critical decisions, such as personalized medical diagnoses and product pric-ing, are made based on insights gained from designing, observing, and analyzinga series of experiments. This highlights the crucial role of experimental design,which goes beyond merely collecting information on system parameters as intraditional Bayesian experimental design (BED), but also plays a key part in facili-tating downstream decision-making. Most recent BED methods use an amortizedpolicy network to rapidly design experiments. However, the information gatheredthrough these methods is suboptimal for down-the-line decision-making, as theexperiments are not inherently designed with downstream objectives in mind. Inthis paper, we present an amortized decision-aware BED framework that prioritizesmaximizing downstream decision utility. We introduce a novel architecture, theTransformer Neural Decision Process (TNDP), capable of instantly proposing thenext experimental design, whilst inferring the downstream decision, thus effectivelyamortizing both tasks within a unified workflow. We demonstrate the performanceof our method across several tasks, showing that it can deliver informative designsand facilitate accurate decision-making.",
  "Introduction": "In a wide array of disciplines, from clinical trials (Cheng and Shen, 2005) to medical imaging (Burgeret al., 2021), a fundamental challenge is the design of experiments to infer the distribution of someunobservable, unknown properties of the systems under study. Bayesian Experimental Design (BED)(Lindley, 1956; Chaloner and Verdinelli, 1995; Ryan et al., 2016; Rainforth et al., 2024) is a powerfulframework in this context, guiding and optimizing experimental design by maximizing the expectedamount of information about parameters gained from experiments, see (a). However, theultimate goal in many tasks extends beyond parameter inference to inform a downstream decision-making task by leveraging our understanding of these parameters. For example, in personalizedmedical diagnostics, a model is built based on historical data to facilitate an optimal treatment fora new patient (Bica et al., 2021). This data typically comprises patient covariates, administeredtreatments, and observed outcomes. Since the query of such data tends to be expensive due to, e.g.,privacy issues, we need to actively design queries to optimize resource utilization. Here, when thegoal is to improve decisions, the strategy of experimental designs should not focus solely on inferringthe parameters of the model, but rather on guiding the final decision-making for the new patient, toensure that each query contributes maximally to the diagnostic decision.",
  "arXiv:2411.02064v2 [stat.ML] 2 Jan 2025": ": Overview of BED workflows. (a) Traditional BED, which iterates between optimizingdesigns, running experiments, and updating the model via Bayesian inference. (b) Amortized BED,which uses a policy network for rapid experimental design generation. (c) Our decision-awareamortized BED integrates decision utility in training to facilitate downstream decision-making. Traditional BED methods (Rainforth et al., 2018; Foster et al., 2019, 2020; Kleinegesse and Gutmann,2020) do not take down-the-line decision-making tasks into account during the experimental designphase. As a result, inference and decision-making processes are carried out separately, which issuboptimal for decision-making in scenarios where experiments can be adaptively designed. Loss-calibrated inference, which was originally introduced by Lacoste-Julien et al. (Lacoste-Julienet al., 2011) for variational approximations in Bayesian inference, provides a concept that adjuststhe inference process to capture posterior regions critical for decision-making tasks. Rather thanfocusing on parameter estimation, the idea is to directly maximize the expected downstream utility,recognizing that accurate decision-making can proceed without exact knowledge of the posterior asnot all regions of the posterior contribute equally to the downstream task. Inspired by this concept,we could consider integrating decision-making directly into the experimental design process to alignthe proposed designs more closely with the ultimate decision-making task. To pick the next optimal design, standard BED methods require estimating and optimizing theexpected information gain (EIG) over the design space, which can be extremely time-consuming.This limitation has led to the development of amortized BED (Foster et al., 2021; Ivanova et al., 2021;Blau et al., 2022, 2023), a policy-based method which leverages a neural network policy trained onsimulated experimental trajectories to quickly generate designs, as illustrated in (b). Givenan experiment history, these policy-based methods determine the next experimental design througha single forward pass, significantly speeding up the design process. Another significant advantageof amortized BED methods is their capacity to extract and utilize unstructured domain knowledgeembedded in historical data. Unlike traditional methods, which never reuse the information from pastdata, amortized methods can integrate this knowledge to refine and improve design strategies for newexperiments. In our problem setting, the benefits of amortization are also valuable where decisionsmust be made swiftly, such as when determining optimal treatment for patients in urgent settings. In this paper, we propose an amortized decision-making-aware BED framework, see (c). Weidentify two key aspects where previous amortized BED methods fall short when applied to down-stream decision-making tasks. First, the training objective of the existing methods does not considerdownstream decision tasks. Therefore, we introduce the concept of Decision Utility Gain (DUG)to guide experimental design to better align with the downstream objective. DUG is designed tomeasure the improvement in the maximum expected utility derived from the new experiment. Second,to obtain the optimal decision, we still need to explicitly approximate the predictive distribution ofthe outcomes to estimate the utility. Current amortized methods learn this distribution only implicitlyand therefore require extra computation for the decision-making process. To address this, we propose a novel Transformer neural decision process (TNDP) architecture with dual output heads: one actingas a policy network to propose new designs, and another to approximate the predictive distribution tosupport the downstream decision-making. This setup allows an iterative approach where the systemautonomously proposes informative experimental designs and makes optimal final decisions. Finally,since our ultimate goal is to make optimal decisions at the final stage, which may involve multipleexperiments, it is crucial that our experimental designs are not myopic or overly greedy by onlymaximizing next-step decision utility. Therefore, we develop a non-myopic objective function thatensures decisions are made with a comprehensive consideration of future outcomes.",
  "Bayesian experimental design": "BED (Lindley, 1956; Ryan et al., 2016; Rainforth et al., 2024) is a powerful statistical framework thatoptimizes the experimental design process. The primary goal of BED is to sequentially select a set ofexperimental designs and gather outcomes y, to maximize the amount of information obtainedabout the parameters of interest, denoted as . Essentially, BED seeks to minimize the entropy ofthe posterior distribution of or, equivalently, to maximize the information that the experimentaloutcomes provide about . At the heart of BED lies the concept of Expected Information Gain (EIG), which quantifies the valueof different experimental designs based on how much they are expected to reduce uncertainty about, measured in terms of expected entropy (H[]) reduction:EIG() = Ep(y|) [H [p()] H [p(|, y)]] .(1) The optimal design is then defined as = arg max EIG(). In practice, calculating EIG is acomputationally challenging task because it involves integrations over p(y|) and p(|, y), whichare both intractable. In recent years, various methods have been proposed to make the computationof the EIG feasible in practical scenarios, such as nested Monte Carlo estimators (Rainforth et al.,2018) and variational approximations (Foster et al., 2019). However, even with these advancements,the computational load remains significant, hindering feasibility in tasks that demand rapid designs.This limitation has pushed forward the development of amortized BED methods, which significantlyreduce computational demands during the deployment stage.",
  "Amortized BED": "Amortized BED methods represent a significant shift from traditional experimental optimization tech-niques. Instead of optimizing for each experimental design separately, Foster et al. (2021) developeda parameterized policy , which directly maps the experimental history h1:t = {(1, y1), ..., (t, yt)}to the next design t+1 = (h1:t). To train such a policy network, Foster et al. (2021) proposed usingsequential Prior Contrastive Estimation (sPCE) to optimize the lower bound of the total EIG acrossthe entire T-step experiments trajectory:",
  "Bayesian decision theory": "Bayesian decision theory (Berger, 2013) provides an axiomatic framework for decision-makingunder uncertainty, systematically incorporating probabilistic beliefs about unknown parameters intodecision-making processes. It introduces a task-specific utility function u(, a), which quantifies thevalue of the outcomes from different decisions a A when the system is in state . The optimaldecision is then determined by maximizing the expected utility, which integrates the utility functionover the unknown system parameters, given the available knowledge h1:t:",
  "a = arg maxaAEp(|h1:t)[u(, a)].(3)": "In many scenarios, outcomes are stochastic and it is more typical to make decisions based on theirpredictive distribution p(y|, h1:t) = Ep(|h1:t)[p(y|, , h1:t)], such as in clinical trials where theoptimal treatment is chosen based on predicted patient responses rather than solely on underlyingbiological mechanisms. A similar setup can be found in (Kusmierczyk et al., 2019; Vadera et al., 2021).As we switch the belief about the state of the system to the outcomes and to keep as much informationas possible, we need to evaluate the effect of on all points of the design space. Thus, instead of theposterior over the latent state , we represent our belief directly as p(y|h1:t) {p(y|, h1:t)},i.e. a joint predictive (posterior) distribution of outcomes over all possible designs given the currentinformation h1:t.1 The utility is then expressed as u(y, a), which relies on the decision a andall possible predicted outcomes y. It is a natural extension of the traditional definition of utilityby marginalizing out the posterior distribution of . The rule of making the optimal decision isreformulated in terms of the predictive distribution as:",
  "a = arg maxaAEp(y|h1:t)[u(y, a)].(4)": "Traditional methods usually separate the inference and decision-making steps, which are optimalwhen the true posterior or the predictive distribution can be computed exactly. However, in mostcases the posteriors are not directly accessible, and we often resort to using approximate distributions.This necessity results in a suboptimal decision-making process as the approximate posteriors oftenfocus on representing the full posterior yet fail to ensure high accuracy in regions crucial for decision-making. Loss-calibrated inference (Lacoste-Julien et al., 2011) emerges as an effective solution toaddress this problem. It calibrates the inference by focusing on utility rather than mere accuracy of theapproximation, thereby ensuring a more targeted posterior estimation. This method has been appliedto improving Markov chain Monte Carlo (MCMC) methods (Abbasnejad et al., 2015), Bayesianneural networks (Cobb et al., 2018) and expectation propagation (Morais and Pillow, 2022).",
  "Problem setup": "Our objective is to optimize the experimental design process for down-the-line decision-making. Inthis paper, we consider scenarios in which we design a series of experiments and observecorresponding outcomes y to inform a final decision-making step. We assume we have a fixedexperimental budget with T query steps. For decision-making, we consider a set of possible decisions,denoted as a A, with the objective of identifying an optimal decision a that maximizes a predefinedprediction-based utility function u(y, a).",
  "Decision Utility Gain": "Our method focuses on designing the experiments to directly improve the quality of the final decision-making. To quantify the effectiveness of each experimental design in terms of decision-making, weintroduce Decision Utility Gain (DUG), which is defined as the difference in the expected utility ofthe best decision, with the new information obtained from the current experimental design, versus thebest decision with the information obtained from previous experiments. 1This definition assumes conditional independence of the outcomes given the design. More generally,p(y|h1:t) defines a joint distribution or a stochastic process indexed by the set (Parzen, 1999), where afamiliar example could be a Gaussian process posterior defined on Rd (Rasmussen and Williams, 2006). Definition 3.1. Given a historical experimental trajectory h1:t1, the Decision Utility Gain (DUG)for a given design t and its corresponding outcome yt at step t is defined as follows:DUG(t, yt) = maxaA Ep(y|h1:t1{(t,yt)}) [u(y, a)] maxaA Ep(y|h1:t1) [u(y, a)] .(5) DUG measures the improvement in the maximum expected utility from observing a new experimentaldesign, differing in this from standard marginal utility gain (see e.g., Garnett, 2023). The optimaldesign is the one that provides the largest increase in maximal expected utility. Shifting fromparameter-centric to utility-centric evaluation, we directly evaluate the designs influence on thedecision utility, bypassing the need to reduce the uncertainty of unknown latent parameters. At the time we choose the design t, the outcome remains uncertain. Therefore, we should considerthe Expected Decision Utility Gain (EDUG) with respect to the marginal distribution of the outcomesto select the next design.Definition 3.2. The Expected Decision Utility Gain (EDUG) for a design t, given the historicalexperimental trajectory h1:t1, is defined as:EDUG(t) = Ep(yt|t,h1:t1) [DUG(t, yt)] .(6) With EDUG, we can guide the experimental design without calculating the posterior distribution. Ifwe knew the true predictive distribution, we could always determine the one-step lookahead optimaldesign by maximizing EDUG across the design space with = arg max EDUG(). However,in practice, the true predictive distributions are often unknown, making the optimization of EDUGexceptionally challenging. This difficulty arises due to the inherent bi-level optimization problem andthe need to evaluate two layers of expectations, both over the unknown predictive distribution. To avoid the expensive computational cost of optimizing EDUG, we propose leveraging a policynetwork, inspired by Foster et al. (2021), that directly maps historical data to the next design. Thisapproach sidesteps the continuous need to optimize EDUG by learning a design strategy over manysimulated experiment trajectories during the training phase. It can dramatically reduce computationaldemands at deployment, allowing for more efficient real-time decisions.",
  "Amortizing decision-aware BED": "A fully amortized BED framework for decision-making requires not only amortizing the experimentaldesign but also the predictive distribution to approximate the expected utility. Moreover, permutationinvariance is often assumed in sequential BED (Foster et al., 2021)2, meaning that the sequence ofexperiments does not influence the cumulative information gained. Conditional neural processes(CNPs) (Garnelo et al., 2018; Nguyen and Grover, 2022; Huang et al., 2023b) provide a suitablebasis for developing our framework due to their design, which not only respects the permutationinvariance of the inputs by treating them as an unordered set but also amortizes modeling of thepredictive distributions. See Appendix A for a brief introduction to CNPs and TNPs.",
  "Transformer Neural Decision Processes": "The architecture of our model, termed Transformer Neural Decision Process (TNDP), is a novelarchitecture building upon the Transformer neural process (TNP) (Nguyen and Grover, 2022). It aimsto amortize both the experimental design and the predictive distributions for the subsequent decision-making process. The data architecture of our system comprises four parts D = {D(c), D(p), D(q), GI}:",
  "A context set D(c) = h1:t = {((c)i , y(c)i )}ti=1 contains all past t-step designs and outcomes": "A prediction set D(p) = {((p)i , y(p)i )}npi=1 consists of np design-outcome pairs used forapproximating p(y|h1:t), which is closely related to the training objective of the CNPs.The output from this head can then be used to estimate the expected utility. A query set D(q) = {(q)i }nqi=1 consists of nq candidate experimental designs being consid-ered for the next step. In scenarios where the design space is continuous, we randomlysample a set of query points for each iteration during training. In the deployment phase,optimal experimental designs can be obtained by optimizing the models output.",
  "{": ": Illustration of TNDP. (a) An overview of TNDP architecture with input consisting of 2observed design-outcome pairs from D(c), 2 designs from D(p) for prediction, and 2 candidate designsfrom D(q) for query. (b) The corresponding attention mask. The colored squares indicate that theelements on the left can attend to the elements on the top in the self-attention layer of ftfm. Global information GI = [t, ] where t represents the current step in the experimentalsequence, and encapsulates task-related information, which could include contextual datarelevant to the decision-making process. We will further explain the choice of in . TNDP comprises four main components: the data embedder block femb, the Transformer block ftfm,the prediction head fp, and the query head fq. Each component plays a distinct role in the overalldecision-aware BED process. The full architecture is shown in (a). At first, each set of D is processed by the data embedder block femb to map to an alignedembedding space.These embeddings are then concatenated to form a unified representationE = concat(E(c), E(p), E(q), EGI). Please refer to Appendix B for a detailed explanation of howwe embed the data. After the initial embedding, the Transformer block ftfm processes E usingself-attention mechanisms to produce a single attention matrix, which is subsequently processed by anattention mask (see (b)) that allows for selective interactions between different data components,ensuring that each part contributes appropriately to the final output. To explain, each design fromthe prediction set D(p) is configured to attend to itself, the global information, and the historicaldata, reflecting the dependence of the predictions on the historical data and the independence fromother designs. Similarly, each (q) in the query set D(q) is also restricted to attend only to itself, theglobal information, and the historical data. This setup preserves the independence of each candidatedesign, ensuring that the evaluation of one design neither influences nor is influenced by others.The output of ftfm is then split according to the specific needs of the query and prediction head = [(p), (q)] = ftfm(E). The primary role of the prediction head fp is to approximate p(y|h1:t) with a family of parameter-ized distributions q(y|pt), where pt = fp((p)t ) is the output of fp at the step t. The training of fp isby minimizing the negative log-likelihood of the predicted probabilities:",
  "where pi,t represents the output of design (p)iat step t. Here we choose a Gaussian likelihood with and representing the predicted mean and standard deviation split from p": "The query head fq processes the transformed embeddings (q) from the Transformer block to generatea policy distribution over possible experimental designs. Specifically, it converts the embeddings intoa probability distribution used to select the next experimental design. The outputs of the query head,q = fq((q)), are mapped to a probability distribution via a Softmax function:",
  "rt((q)t ) = maxaA Eq(y|pt) [u(y, a)] maxaA Eq(y|pt1) [u(y, a)] .(9)": "This reward quantifies how the experimental design influences our decision-making by estimating theimprovement in expected utility that results from incorporating new experimental outcomes. However,this objective remains myopic, as it does not account for the future or the final decision-making. Toaddress this, we employ the REINFORCE algorithm (Williams, 1992), which allows us to considerthe impact of the current design on future rewards. The final loss of fq can be written as the negativeexpected reward for the complete experimental trajectory:",
  "t=1Rt log ((q)t |h1:t1),(10)": "where Rt = Tk=t ktrk((q)k ) represents the non-myopic discounted reward. The discount factor is used to decrease the importance of rewards received at later time step. (q)tis obtained throughsampling from the policy distribution (q)t (|h1:t1). The update of fq depends critically on the accuracy with which fp approximates the predictivedistribution. Ultimately, the effectiveness of decision-making relies on the informativeness of thedesigns proposed by fq, ensuring that every step in the experimental trajectory is optimally alignedwith the overarching goal of maximizing the decision utility. The full algorithm of our method isshown in Appendix C.",
  "Related work": "Lindley (1972) proposes the first decision-theoretic BED framework, later reiterated by Chalonerand Verdinelli (1995). However, their utility is defined based on individual designs, while ourutility is formulated in terms of a stochastic process and is designed for the final decision-makingtask after multiple rounds of experimental design. Recently, several other BED frameworks thatfocus on different downstream properties have been proposed. Bayesian Algorithm Execution(BAX) (Neiswanger et al., 2021) develops a BED framework that optimizes experiments based ondownstream properties of interest. BAX introduces a new metric that queries the next experiment bymaximizing the mutual information between the property and the outcome. CO-BED (Ivanova et al.,2023) introduces a contextual optimization method within the BED framework, where the designphase incorporates information-theoretic objectives specifically targeted at optimizing contextualrewards. Neiswanger et al. (2022) presents an information-based acquisition function for Bayesianoptimization which explicitly considers the downstream task. Zhong et al. (2024) proposes a goal-oriented BED framework for nonlinear models using MCMC to optimize the EIG on predictivequantities of interest. Filstroff et al. (2024) presents a framework for active learning that queries datato reduce the uncertainty on the posterior distribution of the optimal downstream decision. In recent years, various amortized BED methods have emerged. Foster et al. (2021) is the first tointroduce this framework; subsequent work extends it to scenarios with unknown likelihood (Ivanovaet al., 2021) and improved performance using reinforcement learning (Blau et al., 2022; Lim et al.,2022). The latest research proposes a semi-amortized framework that periodically updates the policyduring the experiment to improve adaptability (Ivanova et al., 2024). Maraval et al. (2024) proposesa fully amortized Bayesian optimization (BO) framework that employs a similar TNP architecture,while their work focuses specifically on BO objectives, our approach addresses general downstreamdecision-making tasks. Additionally, our framework introduces a novel coupled training objectivebetween query and prediction heads, providing a more integrated architecture for decision-making. Our proposed architecture is based on pre-trained Transformer models. Transformer-based neuralprocesses (Mller et al., 2021; Nguyen and Grover, 2022; Chang et al., 2024) serve as the foundationalstructure for our approach, but they have not considered experimental design. Decision Transformers(Chen et al., 2021; Zheng et al., 2022) can be used for sequentially designing experiments. However,we additionally amortize the predictive distribution, making the learning process more challenging.",
  "(b) Decision-aware active learning": ": Results of synthetic regression and decision-aware active learning. (a) The top figurerepresents the true function and the initial known points. The red line indicates the location of x.The blue star marks the next query point, sampled from the policys predicted distribution shownin the bottom figure. (b) Mean and standard error of the proportion of correct decisions on 100 testpoints w.r.t. the acquisition steps. Our TNDP significantly outperforms other methods.",
  "Toy example: synthetic regression": "We begin with an illustrative example to show how our TNDP works. We consider a 1D syntheticregression task where the goal is to perform regression at a specific test point x on an unknownfunction. To accurately predict this point, we need to sequentially design some new points to query.This example can be viewed as a prediction-oriented active learning (AL) task (Smith et al., 2023). The design space = X is the domain of x, and y is the corresponding noisy observations of thefunction. Let Q(X) denote the set of combinations of distributions that can be output by TNDP, wecan then define decision space to be A = Q(X). The downstream decision is to output a predictivedistribution for y given a test point x, and the utility function u(y, a) = log q(y|x, h1:t) is thelog probability of y under the predicted distribution, given the queried historical data ht. During training, we sample functions from Gaussian Processes (GPs) (Rasmussen and Williams,2006) with squared exponential kernels of varying output variances and lengthscales and randomlysample a point as the test point x. We set the global contextual information as the test point x.For illustration purposes, we consider only the case where T = 1. Additional details for the datageneration can be found in Appendix E. Results. From (a), we can observe that the values of concentrate near x, meaning ourquery head fq tends to query points close to x to maximize the DUG. This is an intuitive exampledemonstrating that our TNDP can adjust its design strategy based on the downstream task.",
  "Decision-aware active learning": "We now show another application of our method in a case of decision-aware AL studied by Filstroffet al. (2024). In this experiment, the model will be used for downstream decision-making afterperforming AL, i.e., we will use the learned information to take an action towards a specific target.A practical application of this problem is personalized medical diagnosis introduced in ,where a doctor needs to query historical patient data to decide on a treatment for a new patient.",
  "RSUCBEIPIPFNs4BOTNDP": ": Results on Top-k HPO task. For each meta-dataset, we calculated the average utilityacross all available test sets. The error bars represent the standard deviation over five runs. TNDPconsistently achieved the best performance in terms of utility. covariates x and the decisions a they receive. The outcome y is the treatment effect after the patientreceives the decision, which is influenced by real-world unknown parameters such as medicalconditions. For historical data, each patient is associated with only one decision. The utility functionu(y, a) = I(a, a) is a binary accuracy score that measures whether we can make the correctdecision for a new patient x based on the queried history, where a is the predicted Bayesian optimaldecision and a the true optimal decision. Here, u(y, a) = 1 if and only if a = a. In our experiment, we use the synthetic dataset from Filstroff et al. (2024), the details of the datagenerating process can be found in Appendix F. We set Nd = 4 and use independent GPs togenerate different outcomes. Each data point is randomly assigned a decision, and the outcome isthe corresponding y value from the associated GP. We randomly select a test point x and determinethe optimal decision a based on the GP that provides the maximum y value at x. We set globalcontextual information as the covariates of the test point x. We compare TNDP with other non-amortized AL methods: random sampling (GP-RS), uncertaintysampling (GP-US), decision uncertainty sampling (GP-DUS), targeted information (GP-TEIG)introduced by Sundin et al. (2018), and decision EIG (GP-DEIG) proposed by Filstroff et al. (2024).A detailed description of each method can be found in Appendix F. Each method is tested on 100different x points, and the average utility, i.e., the proportion of correct decisions, is calculated. Results. The results are shown in (b), where we can see that TNDP achieves significantlybetter average accuracy than other methods. Additionally, we conduct an ablation study of TNDP inAppendix F.3 to verify the effectiveness of fq. We further analyze the deployment running time toshow the advantage of amortization, see Appendix D.1.",
  "Top-k hyperparameter optimization": "In traditional optimization tasks, we typically only aim to find a single point that maximizes theunderlying function f. However, instead of identifying a single optimal point, there are scenarioswhere we wish to estimate a set of top-k distinct optima. For example, this is the case in robustoptimization, where selecting multiple points can safeguard against variations in data or modelperformance. In this experiment, we choose hyperparameter optimization (HPO) as our task and conduct experi-ments on the HPO-B datasets (Arango et al., 2021). The design space X is a finite set definedover the hyperparameter space and the outcome y is the accuracy of a given configuration on a specificdataset. Our decision is to find k hyperparameter sets, denoted as a = (a1, ..., ak) A X k, withai = aj. The utility function is then defined as u(y, a) = ki=1 yai, where yai is the accuracycorresponding to the hyperparameter configuration ai. In this experiment, the global contextualinformation = . We compare our methods with five different BO methods: random sampling (RS), Upper ConfidenceBound (UCB), Expected Improvement (EI), Probability of Improvement (PI), and an amortizedmethod PFNs4BO (Mller et al., 2023), which is a transformer-based model designed for hyperpa-rameter optimization. We set k = 3 and T = 50, starting with an initial dataset of 5 points. Ourexperiments are conducted on four search spaces selected from the HPO-B benchmark. All results",
  "are evaluated on a predefined test set, ensuring that TNDP does not encounter these test sets duringtraining. For more details, see Appendix G": "Results. From the experimental results (), our method demonstrates superior performance acrossall four meta-datasets, particularly during the first 10 queries, achieving clearly better utility gains.This indicates that our TNDP can effectively identify high-performing hyperparameter configurationsearly in the optimization process. Finally, we included a real-world experiment on retrosynthesis planning. Specifically, our task isto assist chemists in identifying the top-k synthetic routes for a novel molecule, as selecting themost practical routes from many random routes generated by the retrosynthesis software can betroublesome. The detailed description of the task and the results are shown in Appendix G.3.",
  "Limitations & future work": "We recognize that the training of the query head inherently poses a reinforcement learning (RL)(Li, 2017) problem. Currently, we employ a basic REINFORCE algorithm, which can result inunstable training, particularly for tasks with sparse reward signals. For more complex problems inthe future, we could deploy advanced RL methods, such as Proximal Policy Optimization (PPO)(Schulman et al., 2017); the trade-offs include the introduction of additional hyperparameters andincreased computational cost. Like all amortized approaches, our method requires a large amountof data and upfront training time to develop a reliable model. Besides, our architecture is based onthe Transformer, which suffers from quadratic complexity with respect to the input sequence length.This can become a bottleneck when the query set is very large. Future work could focus on designingmore sample-efficient methods to reduce the data and training requirements. Our TNDP followsthe common practice in the neural processes literature (Garnelo et al., 2018) of using independentGaussian likelihoods. If modeling correlations between points is crucial for the downstream task, wecan replace the output with a joint multivariate normal distribution (Markou et al., 2022) or predict theoutput autoregressively (Bruinsma et al., 2023). Following most BED approaches, our work assumesthat the model is well-specified. However, model misspecification or shifts in the utility functionduring deployment could impact the performance of the amortized model (Rainforth et al., 2024).Future work could address the challenge of robust experimental design under model misspecification(Huang et al., 2023a). Another limitation is that our system is currently constrained to acceptingdesigns of the same dimensionality. Future work could focus on developing dimension-agnosticmethods to expand the scope of amortization. Lastly, our model is trained on a fixed-step length,assuming a finite horizon for the experimental design process. Future research could explore thedesign of systems that can handle infinite horizon cases, potentially improving the applicability ofTNDP to a broader range of real-world problems.",
  "Conclusions": "In this paper, we proposed an amortized framework for decision-aware Bayesian experimental design(BED). We introduced the concept of Decision Utility Gain (DUG) to guide experimental designmore effectively toward optimizing decision outcomes. Towards amortization, we developed anovel Transformer Neural Decision Process (TNDP) architecture with dual output heads: one forproposing new experimental designs and another for approximating the predictive distribution tofacilitate optimal decision-making. Our experimental results demonstrated that TNDP significantlyoutperforms traditional BED methods across a variety of tasks. By integrating decision-makingconsiderations directly into the experimental design process, TNDP not only accelerates the designof experiments but also improves the quality of the decisions derived from these experiments.",
  "Acknowledgements": "DH, LA and SK were supported by the Research Council of Finland (Flagship programme: FinnishCenter for Artificial Intelligence FCAI). YG was supported by Academy of Finland grant 345604.LA was also supported by Research Council of Finland grants 358980 and 356498. SK was alsosupported by the UKRI Turing AI World-Leading Researcher Fellowship, [EP/W002973/1]. Theauthors wish to thank Aalto Science-IT project, and CSCIT Center for Science, Finland, for thecomputational and data storage resources provided.",
  "Abbasnejad, E., Domke, J., and Sanner, S. (2015). Loss-calibrated monte carlo action selection. InProceedings of the AAAI Conference on Artificial Intelligence, volume 29": "Arango, S. P., Jomaa, H. S., Wistuba, M., and Grabocka, J. (2021). Hpo-b: A large-scale reproduciblebenchmark for black-box hpo based on openml. In Thirty-fifth Conference on Neural InformationProcessing Systems Datasets and Benchmarks Track (Round 2). Balandat, M., Karrer, B., Jiang, D., Daulton, S., Letham, B., Wilson, A. G., and Bakshy, E. (2020).Botorch: A framework for efficient monte-carlo bayesian optimization. Advances in neuralinformation processing systems, 33.",
  "Blau, T., Bonilla, E., Chades, I., and Dezfouli, A. (2023). Cross-entropy estimators for sequentialexperiment design with reinforcement learning. arXiv preprint arXiv:2305.18435": "Blau, T., Bonilla, E. V., Chades, I., and Dezfouli, A. (2022). Optimizing sequential experimentaldesign with deep reinforcement learning. In International conference on machine learning, pages21072128. PMLR. Bruinsma, W., Markou, S., Requeima, J., Foong, A. Y., Andersson, T., Vaughan, A., Buonomo,A., Hosking, S., and Turner, R. E. (2023). Autoregressive conditional neural processes. In TheEleventh International Conference on Learning Representations.",
  "Filstroff, L., Sundin, I., Mikkola, P., Tiulpin, A., Kylmoja, J., and Kaski, S. (2024). Targeted activelearning for bayesian decision-making. Transactions on Machine Learning Research": "Foster, A., Ivanova, D. R., Malik, I., and Rainforth, T. (2021). Deep adaptive design: Amortizingsequential bayesian experimental design. In International Conference on Machine Learning, pages33843395. PMLR. Foster, A., Jankowiak, M., Bingham, E., Horsfall, P., Teh, Y. W., Rainforth, T., and Goodman,N. (2019). Variational bayesian optimal experimental design. Advances in Neural InformationProcessing Systems, 32. Foster, A., Jankowiak, M., OMeara, M., Teh, Y. W., and Rainforth, T. (2020). A unified stochasticgradient approach to designing bayesian-optimal experiments. In International Conference onArtificial Intelligence and Statistics, pages 29592969. PMLR. Garnelo, M., Rosenbaum, D., Maddison, C., Ramalho, T., Saxton, D., Shanahan, M., Teh, Y. W.,Rezende, D., and Eslami, S. A. (2018). Conditional neural processes. In International conferenceon machine learning, pages 17041713. PMLR.",
  "Garnett, R. (2023). Bayesian optimization. Cambridge University Press": "Huang, D., Bharti, A., Souza, A., Acerbi, L., and Kaski, S. (2023a). Learning robust statisticsfor simulation-based inference under model misspecification. Advances in Neural InformationProcessing Systems, 36. Huang, D., Haussmann, M., Remes, U., John, S., Clart, G., Luck, K., Kaski, S., and Acerbi, L.(2023b). Practical equivariances via relational conditional neural processes. Advances in NeuralInformation Processing Systems, 36. Ivanova, D. R., Foster, A., Kleinegesse, S., Gutmann, M. U., and Rainforth, T. (2021). Implicitdeep adaptive design: Policy-based experimental design without likelihoods. Advances in NeuralInformation Processing Systems, 34.",
  "Ivanova, D. R., Hedman, M., Guan, C., and Rainforth, T. (2024). Step-DAD: Semi-AmortizedPolicy-Based Bayesian Experimental Design. ICLR 2024 Workshop on Data-centric MachineLearning Research (DMLR)": "Ivanova, D. R., Jennings, J., Rainforth, T., Zhang, C., and Foster, A. (2023). Co-bed: information-theoretic contextual optimization via bayesian experimental design. In International Conferenceon Machine Learning. PMLR. Kleinegesse, S. and Gutmann, M. U. (2020). Bayesian experimental design for implicit models bymutual information neural estimation. In International conference on machine learning, pages53165326. PMLR.",
  "Maraval, A., Zimmer, M., Grosnit, A., and Bou Ammar, H. (2024). End-to-end meta-bayesianoptimisation with transformer neural processes. Advances in Neural Information ProcessingSystems, 36": "Markou, S., Requeima, J., Bruinsma, W., Vaughan, A., and Turner, R. E. (2022). Practical conditionalneural process via tractable dependent predictions. In International Conference on LearningRepresentations. Mo, Y., Guan, Y., Verma, P., Guo, J., Fortunato, M. E., Lu, Z., Coley, C. W., and Jensen, K. F.(2021). Evaluating and clustering retrosynthesis pathways with learned strategy. Chemical science,12(4):14691478.",
  "Stevens, S. J. (2011). Progress toward the synthesis of providencin. PhD thesis, Colorado StateUniversity": "Sundin, I., Peltola, T., Micallef, L., Afrabandpey, H., Soare, M., Mamun Majumder, M., Daee, P., He,C., Serim, B., Havulinna, A., et al. (2018). Improving genomics-based predictions for precisionmedicine through active elicitation of expert knowledge. Bioinformatics, 34(13):i395i403. Szymkuc, S., Gajewska, E. P., Klucznik, T., Molga, K., Dittwald, P., Startek, M., Bajczyk, M.,and Grzybowski, B. A. (2016). Computer-assisted synthetic planning: the end of the beginning.Angewandte Chemie International Edition, 55(20):59045937.",
  "In Appendix A, we provide a brief introduction to conditional neural processes (CNPs) andTransformer neural processes (TNPs)": "In Appendix B, we describe the details of our model architecture and the training setups. In Appendix C, we present the full algorithm for training our TNDP architecture. In Appendix D, we compare the inference time with other methods and show the overalltraining time of TNDP. In Appendix E, we describe the details of our toy example. In Appendix F, we describe the details of the decision-aware active learning example. In Appendix G, we describe the details of the top-k hyperparameter optimization task, alongwith additional results on the retrosynthesis planning task.",
  "AConditional neural processes": "CNPs (Garnelo et al., 2018) are designed to model complex stochastic processes through a flexiblearchitecture that utilizes a context set and a target set. The context set consists of observed data pointsthat the model uses to form its understanding, while the target set includes the points to be predicted.The traditional CNP architecture includes an encoder and a decoder. The encoder is a DeepSetarchitecture to ensure permutation invariance, it transforms each context point individually and thenaggregates these transformations into a single representation that captures the overall context. Thedecoder then uses this representation to generate predictions for the target set, typically employing aGaussian likelihood for approximation of the true predictive distributions. Due to the analyticallytractable likelihood, CNPs can be efficiently trained through maximum likelihood estimation.",
  "A.1Transformer neural processes": "Transformer Neural Processes (TNPs), introduced by Nguyen and Grover (2022), improve theflexibility and expressiveness of CNPs by incorporating the Transformers attention mechanism(Vaswani et al., 2017). In TNPs, the transformer architecture uses self-attention to process the contextset, dynamically weighting the importance of each point. This allows the model to create a richrepresentation of the context, which is then used by the decoder to generate predictions for thetarget set. The attention mechanism in TNPs facilitates the handling of large and variable-sizedcontext sets, improving the models performance on tasks with complex input-output relationships.The Transformer architecture is also useful in our setups where certain designs may have a moresignificant impact on the decision-making process than others. For more details about TNPs, pleaserefer to Nguyen and Grover (2022).",
  "B.1Embedders": "The embedder femb is responsible for mapping the raw data to a space of the same dimension. Forthe toy example and the top-k hyperparameter task, we use three embedders: a design embedderf ()emb, an outcome embedder f (y)emb, and a time step embedder f (t)emb. Both f ()emb and f (y)emb are multi-layerperceptions (MLPs) with the following architecture: Hidden dimension: the dimension of the hidden layers, set to 32. Output dimension: the dimension of the output space, set to 32. Depth: the number of layers in the neural network, set to 4. Activation function: ReLU is used as the activation function for the hidden layers.",
  "The time step embedder f (t)emb is a discrete embedding layer that maps time steps to a continuousembedding space of dimension 32": "For the decision-aware active learning task, since the design space contains both the covariates and thedecision, we use four embedders: a covariate embedder f (x)emb, a decision embedder f (d)emb, an outcomeembedder f (y)emb, and a time step embedder f (t)emb. f (x)emb, f (y)emb and f (t)emb are MLPs which use the samesettings as described above. The decision embedder f (d)emb is another discrete embedding layer. For context embedding E(c), we first map each (c)iand y(c)ito the same dimension using theirrespective embedders, and then sum them to obtain the final embedding. For prediction embeddingE(p) and query embedding E(q), we only encode the designs. For EGI, except the embeddings of thetime step, we also encode the global contextual information using f (x)emb in the toy example and thedecision-aware active learning task. All the embeddings are then concatenated together to form ourfinal embedding E.",
  "B.2Transformer blocks": "We utilize the official TransformerEncoder layer of PyTorch (Paszke et al., 2019) ( for our transformer architecture. For all experiments, we use the same configuration:the model has 6 Transformer layers, with 8 heads per layer, the MLP block has a hidden dimensionof 128, and the embedding dimension size is set to 32.",
  "B.3Output heads": "The prediction head, fp is an MLP that maps the Transformers output embeddings of the query set tothe predicted outcomes. It consists of an input layer with 32 hidden units, a ReLU activation function,and an output layer. The output layer predicts the mean and variance of a Gaussian likelihood, similarto CNPs. For the query head fq, all candidate experimental designs are first mapped to embeddings (q) by theTransformer, and these embeddings are then passed through fq to obtain individual outputs. We thenapply a Softmax function to these outputs to ensure a proper probability distribution. fq is an MLPconsisting of an input layer with 32 hidden units, a ReLU activation function, and an output layer.",
  "B.4Training details": "For all experiments, we use the same configuration to train our model. We set the initial learning rateto 5e-4 and employ the cosine annealing learning rate scheduler. The number of training epochs is setto 50,000 for top-k tasks and 100,000 for other tasks, and the batch size is 16. For the REINFORCE,we use a discount factor of = 0.99.",
  "D.1Inference time analysis": "We evaluate the inference time of our algorithm during the deployment stage. We select decision-aware active learning as the experiment for our time comparison. All experiments are evaluated onan Intel Core i7-12700K CPU. We measure both the acquisition time and the total time. Theacquisition time refers to the time required to compute one next design, while the total time refers tothe time required to complete 10 rounds of design. The final results are presented in Table A1, withthe mean and standard deviation calculated over 10 runs. Traditional methods rely on updating the GP and optimizing the acquisition function, which iscomputationally expensive. D-EIG and T-EIG require many model retraining steps to get the nextdesign, which is not tolerable in applications requiring fast decision-making. However, since ourmodel is fully amortized, once it is trained, it only requires a single forward pass to design theexperiments, resulting in significantly faster inference times.",
  "D.2Overall training time": "Throughout this paper, we carried out all experiments, including baseline model computations andpreliminary experiments not included in the final paper, on a GPU cluster featuring a combinationof Tesla P100, Tesla V100, and Tesla A100 GPUs. We estimate the total computational usage to beroughly 5000 GPU hours. For each experiment, it takes around 10 GPU hours on a Tesla V100 GPUwith 32GB memory to reproduce the result, with an average memory consumption of 8 GB.",
  "F.1Data generation": "For this experiment, we use a GP with a Squared Exponential (SE) kernel to generate our data. Thecovariates x are drawn from a standard normal distribution. For each decision, we use an independentGP to simulate different outcomes. In each training iteration, the lengthscale for each GP is randomlysampled as 0.25 + 0.75 U(0, 1) and the variance as v 0.1 + U(0, 1), where U(0, 1) denotesa uniform random variable between 0 and 1.",
  "Decision uncertainty sampling (GP-DUS): choose the next design t such that the predictivedistribution of the optimal decision corresponding to this design is the most uncertain": "Targeted information (GP-TEIG) (Sundin et al., 2018): a targeted active learning criterion,introduced by (Sundin et al., 2018), selects the next design t that provides the highest EIGon p(y|x, ht1 {(t, yt)}). Decision EIG (GP-DEIG) (Filstroff et al., 2024): choose the next design t which directlyaims at reducing the uncertainty on the posterior distribution of the optimal decision. SeeFilstroff et al. (2024) for a detailed explanation.",
  "F.3Ablation study": "We also carry out an ablation study to verify the effectiveness of our query head and the non-myopicobjective function. We first compare TNDP with TNDP using random sampling (TNDP-RS), and theresults are shown in Fig. A1(a). We observe that the designs proposed by the query head significantlyimprove accuracy, demonstrating that the query head can propose informative designs based ondownstream decisions. We also evaluate the impact of the non-myopic objective by comparing TNDP with a myopic versionthat only optimizes for immediate utility rather than long-term gains ( = 0). The results, presentedin Fig. A1(b), show that TNDP with the non-myopic objective function achieves higher accuracyacross iterations compared to using the myopic objective. This indicates that our non-myopicobjective effectively captures the long-term benefits of each design choice, leading to improvedoverall performance.",
  "G.1Data": "In this task, we use HPO-B benchmark datasets (Arango et al., 2021). The HPO-B dataset is alarge-scale benchmark for HPO tasks, derived from the OpenML repository. It consists of 176 searchspaces (algorithms) evaluated on 196 datasets, with a total of 6.4 million hyperparameter evaluations.This dataset is designed to facilitate reproducible and fair comparisons of HPO methods by providingexplicit experimental protocols, splits, and evaluation measures. We extract four meta-datasets from the HPOB dataset: ranger (id=7609, dx=9), svm (id=5891, dx=8),rpart (id=5859, dx=6), and xgboost (id=5971, dx=16). In the test stage, the initial context set ischosen based on their pre-defined indices. For detailed information on the datasets, please refer to",
  "where is the cumulative distribution function of the standard normal distribution, f(x+)is the current best value observed, and is a parameter that encourages exploration": "In addition to those non-amortized GP-based methods, we also compare our method with an amortizedsurrogate model PFNs4BO (Mller et al., 2023). It is a Transformer-based model designed forhyperparameter optimization which does not consider the downstream task. We use the pre-trainedPFNs4BO-BNN model which is trained on HPO-B datasets and choose PI as the acquisition function,the model and the training details can be found in their official implementation (",
  "G.3Additional experiment on retrosynthesis planning": "We now show a real-world experiment on retrosynthesis planning (Blacker et al., 2011). Specifically,our task is to help chemists identify the top-k synthetic routes for a novel molecule (Mo et al., 2021),as it can be challenging to select the most practical routes from many random options generated by theretrosynthesis software (Stevens, 2011; Szymkuc et al., 2016). In this task, the design space for eachmolecule m is a finite set of routes that can synthesize the molecule. The sequential experimental design is to select a route for a specific molecule to query its score y, which is calculated based onthe tree edit distance (Bille, 2005) from the best route. The downstream task is to recommend thetop-k routes with the highest target-specific scores based on the collected information.",
  "TNDPRandom search": "Figure A2: Results of retrosynthesis planningexperiment. The utility is the sum of the qualityscores of top-k routes and is calculated with 50molecules. Our TNDP outperforms the randomsearch baseline. In this experiment, we choose k = 3 andT = 10, and set the global information = m.We train our TNDP on a novel non-public meta-dataset, including 1500 molecules with 70 syn-thetic routes for each molecule. The represen-tation dimension of the molecule is 64 and thatof the route is 264, both of which are learnedthrough a neural network.Given the high-dimensional nature of the data representation,it is challenging to directly compare TNDP withother GP-based methods, as GPs typically strug-gle with scalability and performance in suchhigh-dimensional settings. Therefore, we onlycompare TNDP with TNDP using random sam-pling. The final results are evaluated on 50 testmolecules that are not seen during training, asshown in Fig. A2."
}