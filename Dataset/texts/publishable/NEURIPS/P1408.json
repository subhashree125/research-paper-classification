{
  "Abstract": "AI computing and data centers consume a large amount of freshwater, both directlyfor cooling and indirectly for electricity generation. While most attention has beenpaid to developed countries such as the U.S., this paper presents the first-of-its-kind dataset that combines nation-level weather and electricity generation data toestimate water usage efficiency for data centers in 41 African countries across fivedifferent climate regions. We also use our dataset to evaluate and estimate the waterconsumption of inference on two large language models (i.e., Llama-3-70B andGPT-4) in 11 selected African countries. Our findings show that writing a 10-pagereport using Llama-3-70B could consume about 0.7 liters of water, while the waterconsumption by GPT-4 for the same task may go up to about 60 liters. For writinga medium-length email of 120-200 words, Llama-3-70B and GPT-4 could consumeabout 0.13 liters and 3 liters of water, respectively. Interestingly, given the sameAI model, 8 out of the 11 selected African countries consume less water than theglobal average, mainly because of lower water intensities for electricity generation.However, water consumption can be substantially higher in some African countrieswith a steppe climate than the U.S. and global averages, prompting more attentionwhen deploying AI computing in these countries. Our dataset is publicly availableon Hugging Face.",
  "Introduction": "With the rapid growth of artificial intelligence (AI) and digital services, the demand for data centershas increased substantially . While data center infrastructure was historically lacking in Africa,the continents burgeoning digital economy has recently led to a surge in data center constructions,with a projected market growth of 50% by 2026 compared to 2021 . Nonetheless, data centers are notorious for their massive energy usage and water consumption, whichhave raised significant concerns even in developed countries such as the U.S. . More critically,the added pressure on local water resources is particularly acute in Africa, where many countriesare already grappling with extended droughts and water scarcity challenges . Therefore, it isimportant to assess data centers water consumption in Africa, supporting healthy development of thedata center industry for essential economic growth while ensuring responsible utilization of limitedfreshwater resources.",
  "center concentrations, such as the U.S. and Europe, while leaving out Africadespite its rapidexpansion of data centers and pressing challenges of water scarcity": "In this paper, we address the critical gap in the literature and present a first-of-its-kind water ef-ficiency dataset for data centers in 41 African countries across five distinct climate regions. Thedataset includes hourly estimates of water usage efficiency (WUE) for both direct and indirect waterconsumption over one year. We obtain these estimates by combining weather data from across Africawith the corresponding fuel mix data (i.e., the composition of energy sources in each country). To demonstrate the utility of this dataset, we consider two recent large language models (LLMs), i.e.,Llama-3-70B and GPT-4, and compare their water consumption for inference in African countrieswith that in the U.S. and globally. Our results for 11 representative African countries show thatwriting a 10-page report using Llama-3-70B could consume around 0.7 liters of water while the waterconsumption by GPT-4 for the same task may go up to nearly 60 liters of water. Interestingly, our results also suggest that 8 out of the 11 selected African countries (includingSouth Africa and Egypt) have a lower water consumption than the global average for performing thesame task. This is due in part to a less water-intensive fuel mix for electricity generation in thesecountries. Additionally, some countries such as Morocco are even less water-consuming than the U.S.Nonetheless, the actual impacts of running AI model inferences on local water resources in thesecountries are still significant in light of the high water stress levels in African countries. On the otherhand, for the same task, some countries such as Namibia are more water-consuming than the globalaverage. Further compounded by the enduring regional water stress, the higher water consumptionmay prompt more attention when deploying AI services in these countries.",
  "Background and Methodology": "Our dataset is primarily based on the methodology and modeling of , which study WUE and AImodel water consumption with a heavy emphasis on the U.S. data centers. Like , we do not modelsupply chain manufacturing because this aspect often relies on generalized, less accurate data thatmay not reflect the unique operational practices of individual data centers or computing workloads.The methodology in provides equations for modeling onsite WUE, which refers to water directlyconsumed/evaporated to cool down the facility for each unit of server energy consumption, andoffsite WUE, which is also called the electric water intensity factor and refers to indirect waterconsumption by the generation of electricity that supplies each unit of data center energy. Notethat water consumption is defined as water withdrawal minus water discharge, i.e., the evaporatedportion of water withdrawal that may not be immediately available for reuse . Data centerscommonly consume 80% of their direct freshwater withdrawal (in many cases, potable water), whileonly about 10% of the water withdrawal is consumed by typical households and offices . Onsite WUE. To assess onsite WUE, presents an empirical model created from a commercialcooling tower by considering two configurations. The first configuration is called fixed approach,which fixes the differential between wet-bulb and cold water temperatures, and the second one fixedcold water temperature sets a constant cold water temperature. The WUE formulas for these twoconfigurations are as follows:",
  "ColdWater =0.0005112 T 2w 0.04982 Tw + 2.387+ ,(2)": "where Tw is the wet-bulb temperature in Fahrenheit and [x]+ = max{0, x}. Unless otherwise noted,we will focus on Equation (2) and simply refer it to as onsite WUE in this paper, because it is typicallyeasier to set a fixed cold water temperature without adjustment in real systems. While the onsiteWUE for a cooling tower can differ from other cooling methods such as air economization with waterevaporation, we note that cooling towers are one of the most commonly adopted and efficient heatrejection mechanisms for data centers , especially in hot regions like Africa. Offsite WUE. Electricity generation is water-intensive and must respond to the demand in real timeto maintain grid stability. Thus, similar to carbon emissions associated with electricity usage, datacenters are also accountable for the electricity water consumption. Technology companies includingMeta have recently begun to include indirect water consumption for electricity generation in theirsustainability reports . This is critical for holistically understanding the true water impact of data centers, especially in regions where the energy mix includes significant hydroelectric and/or thermalpower generation with high water intensities . Based on , we present the offsite WUEformula as follows:",
  "RainforestRepublic of the Congo, Gabon, RwandaSavannaMorocco, TunisiaDesertEgypt, LibyaSteppeNamibia, EthiopiaMediterraneanAlgeria, South Africa": "Weather data.For weather data, wefirst identify five distinct climate regionsin Africa: Rainforest, Savanna, Desert,Steppe, and Mediterranean regions .We then collect weather data from the coun-tries for each climate region, consisting ofhourly wet-bulb temperature, humidity, pre-cipitation over one year from August 23,2023 to August 22, 2024. All the weatherdata is obtained from WeatherAPI ,which is collected via ground-based weather stations and satellite imagery. We then pick the high andlow extremes in terms of the average wet bulb temperature for each region to obtain a representativerange. The selected 11 representative countries are summarized in . Energy fuel mix. We next collect the energy fuel mix (i.e., the composition of energy fuels) forelectricity generation in each selected country sourced from OurWorldInData . Due to the lackof access to fine-grained data, we use annual granularity for estimating the offsite WUE as done inthe prior literature . Additionally, we need the water intensity of each fuel type in each selectedcountry to compute offsite WUE in (3). While direct data on the water consumption of variousenergy fuel types for African countries is lacking, studies water withdrawal and consumptionthroughout different stages of energy production in Africa. Thus, we use to derive the averagewater intensity for each energy fuel type in Africa.",
  ": Average monthly onsite WUE fordesert (red) and rainforest (blue) regions": "Our final dataset provides onsite and offsite (hourly)WUE for capital cities in 41 African countries. Forclarity, illustrates the monthly averages fora few selected countries in the rainforest and desertregions. The plot clearly illustrates seasonal trends,as well as onsite WUE differences of up to about 40%between climate regions. Specifically, desert regionsgenerally are more water-consuming than rainforestregions, which is consistent with the observationsin other places . Due to space limitations, weomit the figures for the other regions and offsite WUEwhile referring the readers to our dataset for details. Estimating water consumption for AI models. Todemonstrate the utility of our dataset, we use it toestimate the water consumption of two LLMs, i.e.,Metas Llama-3-70B and OpenAIs GPT-4, following the method in . The tasks we evaluate are towrite a comprehensive 10-page report and a medium-length email. The details of estimating theseAI models water consumption and results for writing a medium-length email are available in theappendix. Figures 2a and 2b indicate that writing a 10-page report using Llama-3-70B and GPT-4 inAfrica could consume approximately 0.7 liters and 60 liters of water, respectively. In addition, wehighlight the following points.",
  ": Water consumption across 11 selected countries for writing a 10-page report (5,000 tokens)using Llama-3-70B and GPT-4, respectively": "First, 8 of the 11 selected African countries have a lower water consumption than the global average.In addition, Morocco and South Africa even have a lower water consumption than the U.S. average.This may be surprising, as Africa is commonly viewed as a water-scarce and dry continent. Themain cause is that these countries generate electricity from energy fuels with relatively lower waterintensities. Therefore, although the same AI model consumes more onsite water due to hotter weather,the significantly lower offsite water still makes these countries less water-consuming overall thanthe global average. Nonetheless, the actual impacts of running AI model inferences on local waterresources in these countries are still significant in light of the high water stress levels in Africa. Second, our results suggest a correlation between climate conditions with water consumption. Notably,countries categorized under the rainforest region (i.e., Republic of Congo, Rwanda, and Gabon) andthe steppe climate (i.e., Ethiopia and Namibia) exhibit higher or roughly the same water consumptioncompared to the global average. We hypothesize several possible causes. First, the intrinsic hotand humid conditions of the rainforest climate and the dry, often hot conditions in steppe regionspotentially degrade the onsite water efficiency for cooling compared to the global average onsite WUE.This effect is also observed from the onsite WUE differences among Microsofts global data centerlocations. Second, the high offsite water consumption of these countries could suggest that countriesin these regions rely more on water-intensive energy fuels like hydroelectric or thermo-electric power.Indeed, we observe this empirically countries with high offsite WUE, such as the Republic of theCongo and Ethiopia rely almost entirely on hydroelectric power. Carbon Emission (gCO2) 0.15 0.25 0.35 0.45 0.55 0.65 0.75 Water Consumption (L)",
  ": Water consumption and (scope-2)carbon emission across various African coun-tries for writing a 10-page report using theLlama-3-70B model": "Third, we show in the water consump-tion and (scope-2) carbon emission across variousAfrican countries for writing a 10-page report usingthe Llama-3-70B model. We see a tradeoff betweenwater consumption and carbon emission, which isconsistent with the findings in prior studies . Thisprompts further attention to strike a balance betweenwater consumption and carbon emission to enabletruly sustainable AI in African countries. Finally, we emphasize the potential uncertainties inour quantitative results. For instance, it is challengingto obtain precise data on the energy fuel mix and theelectricity water intensity in Africa. Moreover, the ac-tual energy consumption of LLM inference may varydepending on the (possibly customized) optimizationtechniques used by real systems, particularly for theproprietary GPT-4 model. As such, our results shouldbe regarded as first-order estimates rather than pre-cise representations. We encourage AI model developers and data center operators to enhance",
  "Conclusion": "In this paper, we present the first-of-its-kind dataset of onsite WUE and offsite WUE for data centersin 41 African countries across five different climate regions. We also use our dataset to evaluate andestimate the water consumption of inference on Llama-3-70B and GPT-4 in 11 selected countries. Ourfindings underscore the need for region-specific adaptations in data centers, particularly in coolingsystems that can operate efficiently under varying climatic conditions without substantially escalatingwater usage. Moreover, the reliance on water-intensive energy sources prompts a broader discussionon the water sustainability practices within the data center industry. By understanding the water usageefficiency in these different countries, we can make informed decisions that promote sustainable andresponsible water use while supporting the growing demand for AI and computing services in Africa.",
  "The Water Project, The water crisis: Poverty and water scarcity in africa. (Accessed on 11/25/2024)": "P. S. Gupta, M. R. Hossen, P. Li, S. Ren, and M. A. Islam, A dataset for research on watersustainability, in Proceedings of the 15th ACM International Conference on Future andSustainable Energy Systems, pp. 442446, 2024. M. A. Islam, K. Ahmed, H. Xu, N. H. Tran, G. Quan, and S. Ren, Exploiting spatio-temporaldiversity for water saving in geo-distributed data centers, IEEE Transactions on Cloud Com-puting, vol. 6, no. 3, pp. 734746, 2018. W. E. Gnibga, A. A. Chien, A. Blavette, and A. C. Orgerie, Flexcooldc: Datacenter coolingflexibility for harmonizing water, energy, carbon, and cost trade-offs, in Proceedings of the15th ACM International Conference on Future and Sustainable Energy Systems, e-Energy 24,(New York, NY, USA), p. 108122, Association for Computing Machinery, 2024.",
  "B. Tomlinson, R. W. Black, D. J. Patterson, and A. W. Torrance, The carbon emissions ofwriting and illustrating are lower for AI than for humans, Scientific Reports, vol. 14, February2024": "P. Patel, E. Choukse, C. Zhang, A. Shah, I. Goiri, S. Maleki, and R. Bianchini, Splitwise:Efficient generative LLM inference using phase splitting, in 2024 ACM/IEEE 51st AnnualInternational Symposium on Computer Architecture (ISCA), pp. 118132, 2024. J. Stojkovic, C. Zhang, I. Goiri, J. Torrellas, and E. Choukse, DynamoLLM: Designing LLMinference clusters for performance and energy efficiency, in IEEE International Symposium onHigh-Performance Computer Architecture (HPCA), 2025.",
  "A. B. Samuel Rinc and V. Defour, Ecologits calculator. 2024": "P. Patel, E. Choukse, C. Zhang, I. n. Goiri, B. Warrier, N. Mahalingam, and R. Bianchini,Characterizing power management opportunities for llms in the cloud, in Proceedings of the29th ACM International Conference on Architectural Support for Programming Languages andOperating Systems, Volume 3, ASPLOS 24, (New York, NY, USA), p. 207222, Associationfor Computing Machinery, 2024.",
  "Won = on EandWoff = off E,": "where W is the water consumption, is the WUE, is the power usage effectiveness (PUE), E isthe server energy consumption for AI models, and the subscript on and off denote onsite andoffsite wherever applicable, respectively. Thus, to estimate the LLMs water consumption, we needtheir onsite and offsite WUEs, energy consumption, as well as the PUE.",
  "A.1WUE": "For African countries, we use the average onsite and offsite WUEs from our dataset. For the U.S. andglobal references, their average onsite WUEs are obtained from publicly accessible reports based onMicrosofts U.S. average (0.55 L/kWh) and Equinixs global average (1.07 L/kWh), which representefficient hyperscale and colocation data centers, respectively . Their average offsite WUEsare acquired from the World Resource Institute report .",
  "A.2Energy consumption": "The exact LLM inference energy is often lacking in the public domain, especially for those powerfulbut proprietary LLMs such as GPT-4 deployed in real-world inference systems. To estimate LLMinference energy, some studies resort to a commonly cited claim that each request of the GPT modelfamily underlying ChatGPT consumes about 10x the energy as a Google search , while othersuse GPUs processing capability in tera operations per second (TOPS) and the power consumptionreported by manufacturers . In practice, however, the actual LLM inference energy consumptiondepends on a variety of factors, including the hardware, service level objectives (SLOs), and systemoptimization . To estimate LLM inference energy consumption for a user-facing application (used by, e.g., ChatGPT),we resort to an online calculator and a recent study . These two sources use different methodsto calculate the LLM inference energy consumption, which we describe as follows.",
  "A.2.1LLM inference energy estimates by": "The online calculator estimates the inference energy consumption by various LLMs based on atransparent methodology. It first uses the energy measurements from a set of open-sourced models(mostly on Nvidia A100 GPUs) to fit an energy consumption curve in terms of the number of modelparameters. For mixture-of-expert model architectures such as the one commonly believed to be usedby GPT-4, a range of active model parameters are considered. The energy measurement takes intoaccount a servers non-GPU power attributed to the model depending on the fraction of GPU resourcesthe model utilizes. Nonetheless, only considers the token generation phase, while neglecting theprompt processing phase (i.e., processing user prompts to generate the first output token) which isalso energy-intensive . In addition, it does not consider batching and essentially models alightly-loaded system without request contention. This can be viewed as a reference system used byindustries, e.g., measure LLM inference energy and power consumption without batching asa reference value for energy and power provisioning, while use the latency measurement insuch a reference system to set real SLO targets. On the other hand, measures the actual GPU energy consumption for LLMs on enterprise-gradeNvidia DGX H100 servers. Its measurement also considers state-of-the-practice optimizationtechniques commonly used in real systems, including batching. Importantly, it considers the promptprocessing phase and representative SLO targets, which are both crucial for real-world LLM deploy-ment. Energy estimates assuming a fully utilized system without accounting for SLOs may not reflect theindustry practice, since a fully-utilized system can lead to significant SLO violations, which arenot tolerable in real-world LLM deployment, especially for commercial LLM applications such asreal-time conversations that have strict SLO targets to deliver good quality of experiences .As a result, server resources for LLM inference are typically provisioned based on the peak demandto ensure SLOs are met at all times. In other words, the LLM inference servers may not be highlyutilized under non-peak loads, resulting in a high energy consumption per request. For example,the Llama-2-70B inference for a medium-length request on H100 GPUs consumes 9.4 Wh energywithout batching , while the inference energy consumption is still over 4.0 Wh when batching isapplied under various system loads using state-of-the-practice optimizations (the last column inTable II) . The measurement in only includes the GPU energy consumption for a small set of open LLMs.To account for the non-GPU server energy consumption, we need to multiply the energy consumptionin by a factor of 1.5 2.0 based on the server power provisioning breakdown . While and use different methodologies, we note that the server-level inference energyconsumption estimated by (as of November 20, 2024) is generally lower than that measuredby for the same model size, assuming the LLM inference system is optimized using state-of-the-practice techniques in . For example, for Llama-3-70B to write a medium-length email with250 tokens (or about 120-200 words), estimates the inference energy consumption as 2.62 Wh(after removing the PUE of 1.2 for data center overheads), whereas shows the server-level energyconsumption is about 10 Wh (after multiplying the value in the last column of Table III by 1.6 toaccount for the non-GPU server energy). This result might be surprising, as does not consider system optimization or batching whereas uses reasonable state-of-the-practice optimizations including batching. Nonetheless, mostlyuses A100 GPUs and does not consider prompt processing energy consumption, whereas uses H100 GPUs (which may be more energy-consuming than A100 GPUs for LLM inference asshown by ) and considers both prompt processing and token generation energy consumption.Additionally, the strict SLOs in real-world deployment prohibits the LLM inference system frombeing fully utilized. Thus, the LLM inference energy consumption estimated by without systemoptimization could be even lower and still serves as a good reference point.",
  "A.2.2Energy consumption for writing a 10-page report and a medium-length email": "For the task of writing a 10-page report,1 we assume the output is 5,000 tokens and use the estimatesby , since the energy measurement results in do not include generating such long outputsusing Llama-3-70B. After removing the PUE of 1.2 for data center overheads, we estimate thatthe energy consumption to write a 5,000-token text by Llama-3-70B and GPT-4 are 52.25 Whand 4.66 kWh, respectively, based on the results in as of November 20, 2024. Note that,due to the proprietary nature of GPT-4, assumes 1,760 billion parameters for GPT-4 with amixture-of-expert architecture based on the best-known information from various public sources.Additionally, the energy consumption estimates for models with such large model sizes are based onextrapolation. As a result, without detailed information from model owners, the energy estimates forlarge proprietary models may have less accuracy than for small/medium open models. For the task of writing a medium-length email, we assume the output is 250 tokens or about 120-200words. For Llama-3-70B, by considering a medium-length prompt and a medium system load, weestimate the inference energy consumption as 10 Wh after multiplying the value in the last columnof Table III by 1.6 to account for the non-GPU server energy . For GPT-4, we estimate theinference energy consumption as 232 Wh .",
  "A.3PUE": "PUE is a metric that assesses the energy efficiency of a data center by comparing the total energyconsumed by the facility to the energy used by the computing equipment. The ideal PUE is 1.0,indicating 100% energy efficiency in computing. The inference energy estimate provided by assumes a default PUE of 1.2. The PUE overhead is not needed for calculating the onsite waterconsumption, but should be considered when assessing the offsite water consumption. For differentAfrican countries, we consider an average country-/region-wise PUE provided by . By takingthe lowest when multiple values are presented in , the PUE values for the 11 selected Africancountries are: 2.3 for Algeria, 2.3 for Egypt, 1.5 for Ethiopia, 1.9 for Gabon, 2.3 for Libya, 2.3 forMorocco, 2.1 for Namibia, 2.0 for Republic of the Congo, 1.4 for South Africa, 2.3 for Tunisia, and1.7 for Rwanda. We consider Microsofts U.S. average PUE of 1.17 and Equinixs global averagePUE of 1.42 for the U.S. and global averages, respectively."
}