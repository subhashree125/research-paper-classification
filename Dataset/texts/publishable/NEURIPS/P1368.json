{
  "Abstract": "Backdoor attacks aim to inject a backdoor into a classifier such that it predictsany input with an attacker-chosen backdoor trigger as an attacker-chosen targetclass. Existing backdoor attacks require either retraining the classifier with someclean data or modifying the models architecture. As a result, they are 1) notapplicable when clean data is unavailable, 2) less efficient when the model is large,and 3) less stealthy due to architecture changes. In this work, we propose DFBA, anovel retraining-free and data-free backdoor attack without changing the modelarchitecture. Technically, our proposed method modifies a few parameters of aclassifier to inject a backdoor. Through theoretical analysis, we verify that ourinjected backdoor is provably undetectable and unremovable by various state-of-the-art defenses under mild assumptions. Our evaluation on multiple datasets furtherdemonstrates that our injected backdoor: 1) incurs negligible classification loss, 2)achieves 100% attack success rates, and 3) bypasses six existing state-of-the-artdefenses. Moreover, our comparison with a state-of-the-art non-data-free backdoorattack shows our attack is more stealthy and effective against various defenses whileachieving less classification accuracy loss. The code for our experiment can befound at",
  "Introduction": "Deep neural networks (DNN) have achieved remarkable success in multiple application domainssuch as computer vision. To democratize DNN models, especially the powerful but large ones,many machine learning platforms (e.g., ModelZoo , TensorFlow Model Garden , and HuggingFace ) share their pre-trained classifiers to customers with limited resources. For instance, HuggingFace allows any third party to share pre-trained classifiers with the community, which could bedownloaded by other users. Despite the benefits brought by those machine learning platforms,existing studies show that this model sharing mechanism is vulnerable to backdoor attacks.In particular, a malicious third party could download a pre-trained classifier from the machine learningplatform, inject a backdoor into it, and reshare it with the community via the platform. Backdoorattacks pose severe concerns for the deployment of classifiers downloaded from the machine learningplatforms for security and safety-critical applications such as autonomous deriving . We note thatthe model provider may not share the training data used to train the classifiers when they are trainedon private data (e.g., face images).",
  "arXiv:2412.06219v1 [cs.CR] 9 Dec 2024": "To thoroughly understand this threat, recent research has proposed a large number of backdoorattacks . At ahigh level, existing backdoor attacks require either retraining a classifier by accessing some cleandata or changing the architecture of a classifier . For instance, Hong etal. proposed a handcrafted backdoor attack, which changes the parameters of a classifier to injecta backdoor. However, they need a set of clean samples that have the same distribution as the trainingdata of the classifier to guide the change of the parameters. Bober-Irizar et al. proposed to injecta backdoor into a classifier by manipulating its architecture. Consequently, their practicality is limitedif there is no clean data available, efficiency is restricted if the model is large, or they are less stealthydue to architecture changes. Additionally, none of the existing attacks provide a formal analysis oftheir attack efficacy against cutting-edge defenses . As a result, they may underestimate thethreat caused by backdoor attacks. Our contribution: We propose DFBA, a novel retraining-free and data-free backdoor attack, whichinjects a backdoor into a pre-trained classifier without changing its architecture. At a high level,DFBA first constructs a backdoor path by selecting a single neuron from each layer except the outputlayer. Then, it modifies the parameters of these neurons such that the backdoor path is activatedfor a backdoored input but unlikely to be activated for a clean input. As a result, the backdooredclassifier predicts backdoored inputs to a target class without affecting the predictions for clean inputs.Specifically, we first optimize a backdoor trigger such that the output of the selected neuron in thefirst layer is maximized for a backdoored input. Second, we change the parameters of this neuronsuch that it can only be activated by any input embedded with our optimized trigger but is unlikely tobe activated by a clean input. Third, we change the parameters of the middle layer neurons in thebackdoor path to gradually amplify the output of the neuron selected in the first layer. Finally, forthe output layers neurons, we change their parameters such that the output of the neurons in thebackdoor path has a positive (negative) contribution to the output neuron(s) for the target class (allnon-target classes). We conduct both theoretical and empirical evaluations for DFBA. Theoretically, we prove thatbackdoors injected by DFBA are undetectable by state-of-the-art detection methods, such as NeuralCleanse and MNTD or irremovable by fine-tuning techniques. Empirically, we evaluateDFBA on various models with different architectures trained from various benchmark datasets. Wedemonstrate that DFBA can achieve 100% attack success rates across all datasets and models whiletriggering only less than 3% accuracy loss on clean testing inputs. We also show that DFBA canbypass six state-of-the-art defenses. Moreover, we find that DFBA is more resilient to those defensesthan a state-of-the-art non-data-free backdoor attack . Finally, we conduct comprehensive ablationstudies to demonstrate DFBA is insensitive to the subtle variations in hyperparameters. To the best ofour knowledge, DFBA is the first backdoor attack that is retraining-free, data-free, and provides atheoretical guarantee of its attack efficacy against existing defenses.",
  "Related Work": "Backdoor Attacks.Existing backdoor attacks either use the whole training set to train a backdooredclassifier from scratch or modify the weights or architecture of a pre-trained clean classifierto inject a backdoor . For instance, BadNet constructs a poisoned training set with cleanand backdoored data to train a backdoored classifier from scratch. We note that poisoning databased backdoor attacks require an attacker to compromise the training dataset of a model, i.e., injectpoisoned data into the training data of the model. Our attack does not have such a constraint. Forinstance, many machine learning platforms such as Hugging Face allow users to share their models.A malicious attacker could download a pre-trained classifier from Hugging Face, inject a backdoorusing our attack, and republish it on Hugging Face to share it with other users. Our attack is directlyapplicable in this scenario. Moreover, data poisoning based attacks are less stealthy as shown in theprevious work . More recent works considered a setup where the attacker has access to apre-trained clean model rather than the original training dataset. Under this setup, the attacker eithermanipulates the models weights with a small set of clean validation data (i.e., parameter modificationattacks) or directly vary the model architecture. As discussed in , those attacks require eitherretraining with some clean data or modifying the model architecture. In contrast, we propose the firstbackdoor attack that is entirely retraining-free and data-free without varying the model architecture. Note that parameter modification attacks share a similar attack mechanism as ours. Among theseattacks, some serve a different goal (e.g., fool the model to misclassify certain clean testinginputs) from us. Others still require clean samples to provide guidance for parametermodification. DFBA has the following differences from these methods. First, DFBA does not requiredata when injecting the backdoor, while these methods still require a few samples. Second, DFBA isprovably undetectable and irremovable against various existing defenses (Section B), while existingweight modification attacks do not provide a formal guarantee for its attack efficacy. Finally, as wewill show later in and Appendix G, compared to the state-of-the-art weight modificationattack , DFBA incurs less classification accuracy loss on clean testing inputs than . Inaddition, DFBA requires modifying fewer parameters and is most efficient. We note that a prior study proposed a data-free backdoor attack to deep neural networks. Ourmethod has the following differences with . First, they require the attacker to have a substitutiondataset while our method does not have such a requirement (i.e., our method does not require asubstitution dataset). Second, they inject the backdoor into a classifier by fine-tuning it. By contrast,our method directly changes the parameters of a classifier to inject the backdoor. Third, they did notprovide a formal analysis on the effectiveness of their attack under state-of-the-art defenses. Recent research has begun to explore data-free backdoor attacks in distributed learning scenarios,particularly in Federated Learning (FL) settings. FakeBA introduced a novel attack where fakeclients can inject backdoors into FL systems without real data. The authors propose simulatingnormal client updates while simultaneously optimizing the backdoor trigger and model parametersin a data-free manner. DarkFeD proposed the first comprehensive data-free backdoor attackscheme. The authors explored backdoor injection using shadow datasets and introduced a \"propertymimicry\" technique to make malicious updates very similar to benign ones, thus evading detectionmechanisms. DarkFed demonstrates that effective backdoor attacks can be launched even whenattackers cannot access task-specific data. Backdoor Detection and Elimination.Existing defenses against backdoor attacks can be classifiedinto 1) Training-phase defenses that train a robust classifier from backdoored training data ; 2) Deployment-phase defenses that detect and eliminate backdoors from a pre-trainedclassifier with only clean data ; 3) Testing-phase defenses that identifythe backdoored testing inputs and recover their true prediction result. Training-phase defenses are notapplicable to a given classifier that is already backdoored. Testing-phase defenses require accessingto the backdoored inputs (See Section J for more discussion). In this work, we mainly consider thedeployment-phase defenses. Existing deployment-phase defenses mainly take three directions: detection & removal methods that first reverse-engineer a trigger from a backdoored classifier andthen use it to re-train the classifier to unlearn the backdoor , unlearning methods thatfine-tune a classifier with newly collected data to remove the potential backdoors ,and fine-pruning methods that prune possibly poisoned neurons of the model . As we will show in , our attack is empirically resistant to all of these three methods. In addition, undermild assumptions, our attack, with theoretical guarantee, can evade multiple state-of-the-art detection& removal methods and fine-tuning methods (See Section B).",
  "Problem Setup": "Consider a pre-trained deep neural network classifier g with L layers, where W(l) and b(l) denoteits weight and bias at the l-th layer. Without loss of generality, we consider ReLU as the activationfunction for intermediate layers, denoted as , and Softmax as the activation function for the outputlayer. Given any input that can be flattened into a one-dimensional vector x = [x1, x2, , xd] Rd,where the value range of each element xn is [ln, un], the classifier g maps x to one of the C classes.For instance, when the pixel value of an image is normalized to the range , then we haveln = 0 and un = 1. An attacker injects a backdoor into a classifier g such that it predicts anyinput embedded with an attacker-chosen trigger (, m) as an attacker-chosen target class ytc, where and m respectively represent the pattern and binary mask of the trigger. A backdoored input isrepresented as follows:",
  "Threat Model": "Attackers goals: We consider that an attacker aims to implant a backdoor into a target pre-trainedmodel without retraining it or changing its architecture. Meanwhile, we also need to maintain thebackdoored models normal utilities (i.e., performance on clean inputs). Moreover, we require theimplanted backdoor to be stealthy such that it cannot be easily detected or removed by existingbackdoor detection or elimination techniques. Attackers background knowledge and capability: Similar to existing attacks , we considerthe scenarios where the attacker hijacks the ML model supply chain and gains white-box access toa pre-trained model. Differently, we do not assume that the attacker has any knowledge or accessto the training/testing/validation data. Moreover, we assume that the attacker cannot change thearchitecture of the pre-trained classifier. In addition, we assume that the attacker does not have accessto the training process (e.g., training algorithm and hyperparameters). As discussed above, theseassumptions significantly improve the practicability of our proposed attack.",
  "Effectiveness goal: We aim to make the backdoored classifier predict the attacker-chosen targetlabel for any testing input embedded with the attacker-chosen backdoor trigger": "Efficiency goal: We aim to make the attack that is efficient in crafting a backdoored classifier from apre-trained clean classifier. We note that an attack that achieves the efficiency goal means it is morepractical in the real world. Stealthy goal: The stealthy goal means our attack could bypass existing state-of-the-art defenses.An attack that achieves the stealthy goal means it is less likely to be defended. In this work, we willconduct both theoretical and empirical analysis for our attack under state-of-the-art defenses.",
  "DFBA": "Since we assume neither model retraining nor architecture modification, the only way of implantingthe backdoor is to change the model parameters.1 Specifically, given a classifier g, we aim tomanually modify its parameters to craft a backdoored classifier f. Our key idea is to create apath (called backdoor path) from the input layer to the output layer to inject the backdoor. Inparticular, our backdoor path satisfies two conditions: 1) it could be activated by any backdooredinput such that our backdoor attack is effective, i.e., the backdoored classifier predicts the targetclass for any backdoored input, and 2) it cannot be activated by a clean input with a high probabilityto stay stealthy. Our backdoor path involves only a single neuron (e.g. a single filter in CNN)in each layer to reduce the impact of the backdoor on the classification accuracy of the classifier.",
  ": Visualization of our backdoor path when itis activated by a backdoored input. The backdooredmodel will predict the target class for the backdooredinput": "The key challenge is how to craft a back-door path such that it simultaneously sat-isfies the two conditions. To address thischallenge, we design a backdoor switchwhich is a single neuron selected fromthe first layer of a classifier. We modifythe parameters of this neuron such that itwill be activated by a backdoored inputbut is unlikely to be activated by a cleaninput. Then, we amplify the output ofthe backdoor switch by changing the pa-rameters of the remaining neurons in thebackdoor path. Finally, we change theweights of the output neurons such thatthe output of the (L 1)th-layer neuronin the backdoor path has a positive (ornegative) contribution to the output neu-ron(s) for the target class (or non-targetclasses) to reach our goal.",
  "Neuron Selection for Backdoor Path": "Our goal is to select neurons from a classifier such that they form a path from the first layer to thefinal output layer. In particular, we select a single neuron from each layer. For the first layer, werandomly select one neuron.2 As we will see in the next step, neuron selection in this way enables us to change the parametersof the selected neuron such that it has different behaviors for a clean input and a backdoored input.For each middle layer, we select a neuron such that its output depends on the selected neuron inthe previous layer. Note that we randomly select one if there exist multiple neurons that satisfy thecriteria. 1Although also does inject the backdoor by changing model parameters, it still requires a few samples tofacilitate the parameter changes. While our method does not need any data to inject the backdoor. In addition,we provide a formal guarantee for our attack efficacy.2In fact, we require the neurons output to depend on the features with the index in (m). For a fullyconnected neural network, it is obvious. For a convolutional neural network, the detail is illustrated in Appendix F.",
  "Condition 2: The switch neuron s1 is unlikely to be activated for a clean input x": "To achieve the two conditions mentioned above, we need to tackle the following challenges. , giventhat xn can be different for different backdoored inputs for n / (m). To enable s1 to be activatedby any backdoored input, we first need to ensure that the activation of s1 is independent of the valueof xn, where n / (m). , after we decouple the activation of s1 from xn, n / (m), we need tomake sure its activation value is only related to the trigger pattern. This is challenging in that thevalue of xn, where n (m), can be different for different clean inputs. Addressing challenge : Our key idea is to modify the parameters of the neuron s1 such that itsoutputs only depend on the features of an input whose indices are in (m), i.e., xn (or xn) wheren (m). Specifically, we propose to reach this by setting the corresponding weight between s1and a feature whose index is not in (m) to 0. Given an input x, we use s1(x) to denote the outputof the neuron s1. Here, s1(x) = ( wnxn + b). Given that wn = 0, for n / (m), we can rewrites1(x) = (",
  "n(m) wnxn + b), which is independent from xn for n / (m)": "Addressing Challenge : Our idea is to first optimize the backdoor pattern n (n (m)) andthen modify the remaining parameters of s1 such that 1) s1 is activated for a backdoored input, and2) s1 is unlikely to be activated when xn is not close to the optimized n for n (m).",
  "n(m)wnn + b, s.t. ln n un, n (m),(2)": "where the constraint ensures a backdoored input created by embedding the backdoor trigger (, m)to an arbitrary input is still valid to the classifier, and [l, u] is the range of feature value xn (see.1 for details). Note that although the binary mask m is chosen by the attacker, we can stillderive the analytical solution to the above optimization problem:",
  "Given the optimized backdoor trigger, we design the following method to modify the bias and weightsof s1 to achieve the two conditions": "Activating s1 for x by modifying the bias. Recall that our condition 1 aims to make the switchneuron s1 be activated for a backdoored input x. In particular, given a backdoored input x embeddedwith the trigger , the output of the neuron s1 for x is as follows: s1(x) = (",
  "n(m) wnn + b to bepositive. For simplicity, we denote =": "n(m) wnn + b. For any , if the bias b of the switchneuron s1 satisfies the above condition, the output of s1 is for an arbitrary backdoored input. Inother words, the switch neuron is activated for a backdoored input, meaning we achieve condition 1. shows an example of our backdoor switch.",
  "n(m) |wn(xn n)| < :We note that": "n(m) |wn(xn n)| < measures the difference of a clean input and the backdoor trigger for indices in (m) (indices wherethe backdoor trigger is embedded to an input). In particular, for each index in (m), |wn(xn n)|measures the weighted deviation of the feature value of the clean input x at the dimension n fromthe corresponding value of the backdoor trigger. Based on the above lemma, a clean input can onlyactivate s1 when n(m) |wn(xn n)| < . When is very small and |wn| is large, a clean inputcan only activate the neuron s1 when xn is very close to n for n m, which means that the cleaninput is very close to its backdoored version. In practice, we find that setting a small (e.g., 0.1) isenough to ensure a clean input cannot activate s1.",
  "Amplifying the Output of the Backdoor Switch": "The neuron s1 in the first layer is activated for a backdoored input x. In the following layers, wecan amplify it until the output layer such that the backdoored classifier f outputs the target class ytc.Suppose sl is the selected neuron in the lth layer, where l = 2, 3, , L 1. We can first modify theparameters of sl such that its output only depends on sl1 and then change the weight between sland sl1 to be , where is a hyperparameter. We call amplification factor. By letting the biasterm of sl to be 0, we have:",
  "sl(x) = sl1(x).(6)": "Note that sl(x) = 0 when s1(x) = 0. Finally, we can set the weight between sL1 and the outputneuron for the target class ytc to be but set the weight between sl and the remaining output neuronsto be . shows an example when the backdoor path is activated by a backdoored input.",
  "First, we provide the following definitions:": "Pruned classifier: In our backdoor attack, we select one neuron for each layer in a classifier. Givena pre-trained classifier, we can create a corresponding pruned classifier by pruning all the neuronsthat are selected to form the backdoor path by DFBA. Note that the pruned classifier is clean as itdoes not have any backdoor. Based on this definition, we provide the theoretical analysis towards our proposed method in thissection. We aim to show that our proposed method can maintain utility on clean data, while cannotbe detected by various backdoor model detection methods or disrupted by fine-tuning strategies. Dueto space limits, we mainly show the conclusions and guarantees here and leave the details and proofin the Appendix B.",
  "Utility Analysis": "Our following theorem shows that the backdoored classifier crafted by DFBA has the same output asthe pruned classifier for a clean input.Theorem 1. Suppose an input x cannot activate the backdoor path, i.e., Equation 12 is satisfiedfor x. Then, the output of the backdoored classifier g for x is the same as that of the correspondingpruned classifier h.",
  "The proof of Lemma 1 can be found in Appendix A": "[Remark:] Our above theorem implies that the backdoored classifier has the same classificationaccuracy as the pruned classifier for clean testing inputs. The pruned classifier is very likely tomaintain classification accuracy as we only remove (L 1) neurons for a classifier with L layers.Thus, our DFBA can maintain the classification accuracy of the backdoored classifier for clean inputs.",
  "Effectiveness Analysis": "In our effectiveness analysis (Section B.2), we show the detection results of query-based defenses and gradient-based defenses for our backdoored classifier are the same as those for the prunedclassifier when the backdoor path is not activated, And the following Proposition is given:Proposition 1. Suppose a defense dataset where none of the samples can activate the backdoor path,i.e., Equation 12 is satisfied for each input in the defense dataset. Suppose a defense solely uses theoutputs of a classifier for inputs from the defense dataset to detect whether it is backdoored. Then,the same detection result will be obtained for a backdoored classifier and the corresponding prunedclassifier.Proposition 2. Given a classifier, suppose a defense solely leverages the gradient of the output of theclassifier with respect to its input to detect whether the classifier is backdoored. If the input cannotactivate the backdoor path, i.e., i.e., Equation 12 is satisfied for the input, then the defense producesthe same detection results for the backdoored classifier and the pruned classifier.",
  "[Remark:] As the pruned classifier is a clean classifier, our theorem implies that those defensescannot detect the backdoored classifiers crafted by DFBA": "We also show fine-tuning the backdoored classifier with clean data cannot remove the backdoor:Proposition 3. Suppose we have a dataset Dd = {xi, yi}Ni=1, where each sample xi cannot activatethe backdoor path, i.e., Equation 12 is satisfied for each xi. Then, the parameters of the neurons thatform the backdoor path will not be affected if the backdoored classifier is fine-tuned using the datasetDd.",
  "Evaluation": "We perform comprehensive experiments to evaluate our DFBA. In particular, we consider 1) multiplebenchmark datasets, 2) different models, 3) comparisons with state-of-the-art baselines, 4) evaluationof our DFBA under 6 defenses (i.e., Neural Cleanse , Fine-tuning , and Fine-pruning ,MNTD , I-BAU , Lipschitz pruning ), and 5) ablation studies on all hyperparameters. Ourexperimental results show that 1) our DFBA can achieve high attack success rates while maintainingthe classification accuracy on all benchmark datasets for different models, 2) our DFBA outperformsa non-data-free baseline, 3) our DFBA can bypass all 6 defenses, 4) our DFBA is insensitive tohyperparameters, i.e., our DFBA is consistently effective for different hyperparameters.",
  "Experimental Setup": "Models: We consider a fully connected neural network (FCN) and a convolutional neural network(CNN) for MNIST and Fashion-MNIST. The architecture can be found in Table VI in the Appendix.By default, we use CNN on those two datasets. We consider VGG16 and ResNet-18 forCIFAR10 and GTSRB, respectively. We use ResNet-50 and ResNet-101 for ImageNet. Evaluation metrics: Following previous work on backdoor attacks , we use clean accuracy(CA), backdoored accuracy (BA), and attack success rate (ASR) as evaluation metrics. For a backdoorattack, it achieves the utility goal if the backdoored accuracy is close to the clean accuracy. A highASR means the backdoor attack achieves the effectiveness goal. For the efficiency goal, we use thecomputation time to measure it. Additionally, when we evaluate defenses, we further use ACC as anevaluation metric, which is the classification accuracy on clean testing inputs of the classifier obtainedafter the defense.",
  "Experimental Results": "Our DFBA maintains classification accuracy: compares the CA and BA of our method.The results show that BA is comparable to CA. In particular, the difference between BA and CA isless than 3% for different datasets and models, i.e., our attack maintains the classification accuracyof a machine learning classifier. The reasons are as follows: 1) our backdoor path only consistsof a single neuron in each layer of a classifier, and 2) we find that (almost) no clean testing inputscan activate the backdoor path on all datasets and models as shown in . We note that theclassification accuracy loss on ImageNet is slightly larger than those on other datasets. We suspectthe reason is that ImageNet is more complex and thus randomly selection neurons are more likely toimpact classification accuracy. As part of future work, we will explore methods to further improveclassification accuracy, e.g., designing new data-free methods to select neurons from a classifier.",
  "ResNet-101 77.3874.70100.00": "Our DFBA achieves high ASRs: shows the ASRs of our attack for differentdatasets and models. Our experimental re-sults show that our attack can achieve highASRs. For instance, the ASRs are 100%on all datasets for all different models. Thereason is that all backdoored testing inputscan activate our backdoor path as shownin . Once our backdoor path is ac-tivated for a backdoored testing input, thebackdoored classifier crafted by our DFBAwould predict the target class for it. Ourexperimental results demonstrate the effec-tiveness of our DFBA. Our DFBA is efficient:Our attack di-rectly changes the parameters of a classi-fier to inject a backdoor and thus is veryefficient. We evaluate the computation costof our DFBA. For instance, without us-ing any GPUs, it takes less than 1s to crafta backdoored classifier from a pre-trainedclassifier on all datasets and models. Forexample, On an NVIDIA RTX A100 GPU, DFBA injects backdoors in 0.0654 seconds for ResNet-18model trained on CIFAR10, and 0.0733 seconds for ResNet-101 trained on ImageNet. In contrast,similar methods, such as Lv et al. , require over 5 minutes for ResNet-18 on CIFAR10 and over 50minutes for VGG16 on ImageNet. Our DFBA outperforms existing non-data-free attacks: We compare with state-of-the-art non-data-free backdoor attacks . In our comparison, we use the same setting as Hong et al. . Werandomly sample 10,000 images from the training dataset to inject the backdoor for Hong et al. . shows the comparison results on MNIST. We have two observations. First, our DFBA incurssmall classification loss than Hong et al. . Second, our DFBA achieves higher ASR than Hong etal. . Our experimental results demonstrate that our DFBA can achieve better performance thanexisting state-of-the-art non-data-free backdoor attack . Our DFBA is effective under state-of-the-art defenses:Recall that existing defenses can becategorized into three types (See for details): backdoor detection, unlearning methods,and pruning methods. For each type, we select two methods, which are respectively the mostrepresentative and the state-of-the-art methods. We compare DFBA with Hong et al. for threerepresentative methods (i.e., Neural Cleanse , Fine-tuning , and Fine-pruning ) onMNIST. We adopt the same model architecture as used by Hong et al. in our comparison. Weevaluate three additional state-of-the-art defenses for DFBA (i.e., MNTD , I-BAU , Lipschitz",
  "Hong et al. 96.4995.2994.59DFBA96.4995.51100.00": "In this work, we design DFBA, a novel retraining-free and data-free backdoor attack without chang-ing the architecture of a pre-trained classifier. The-oretically, we prove that DFBA can evade multi-ple state-of-the-art defenses under mild assump-tions. Our evaluation on various datasets showsthat DFBA is more effective than existing attacksin attack efficacy and utility maintenance. More-over, we also evaluate the effectiveness of DFBAunder multiple state-of-the-art defenses. Our re-sults show those defenses cannot defend against our attacks. Our ablation studies further demonstratethat DFBA is insensitive to hyperparameter changes. Promising future work includes 1) extendingour attack to other domains such as natural language processing, 2) designing different types oftriggers for our backdoor attacks, and 3) generalizing our attack to transformer architecture.",
  "This research is supported in part by ARL funding #W911NF-23-2-0137, Singapore National Re-search Foundation funding #053424, DARPA funding #112774-19499": "This material is in part based upon work supported by the National Science Foundation under grantno. 2229876 and is supported in part by funds provided by the National Science Foundation, by theDepartment of Homeland Security, and by IBM. Any opinions, findings, and conclusions or recommendations expressed in this material are thoseof the author(s) and do not necessarily reflect the views of the National Science Foundation or itsfederal agency and industry partners.",
  "Ma, W., D. Wang, R. Sun, et al. The\" beatrixresurrections: Robust backdoor detection viagram matrices. arXiv preprint arXiv:2209.11715, 2022": "Xiang, C., A. N. Bhagoji, V. Sehwag, et al. {PatchGuard}: A provably robust defense againstadversarial patches via small receptive fields and masking. In 30th USENIX Security Symposium(USENIX Security 21), pages 22372254. 2021. Xiang, C., S. Mahloujifar, P. Mittal. {PatchCleanser}: Certifiably robust defense againstadversarial patches for any image classifier. In 31st USENIX Security Symposium (USENIXSecurity 22), pages 20652082. 2022.",
  "Proof of Lemma 1. A clean input x cannot activate the neuron s1 when": "n(m) wn(xnn)+ 0, i.e., n(m) wn(n xn) . We prove this condition is equivalent to n(m) |wn(xn n)| . Suppose wn 0, then we know n = ln based on Equation 3. Since xn [ln, un],we know xn n. Therefore, we have wn(n xn) 0, i.e., wn(n xn) = |wn(n xn)|.Similarly, we can show that wn(n xn) = |wn(n xn)| when wn > 0. Therefore, we have",
  "n(m)|wn(xn n)| < ).(9)": "This probability is very small when is small and wn is large. We have the following example wheneach entry of x follows uniform distribution.Example 1. Suppose xn (n = 1, 2, , d) follows a uniform distribution over . Moreover, weassume xn to be i.i.d.. When |wn| for n (m), we have p (2)e",
  "As a concrete example, we have p 3.13 109 when = 1, = 1, and e = 16 for a4 4 trigger": "In practice, x may follow a different distribution. We empirically find that almost all testingexamples cannot activate the backdoor path when is small (e.g., 0.1) on various benchmarkdatasets, indicating that it is hard in general for regular data to activate the backdoor path. Aswe will show, this enables us to perform theoretical analysis on the utility and effectivenessof the backdoored classifier by DFBA.",
  "n(m)|wn(xnn)|,(12)": "where xn is the feature value of x at the nth dimension, wn is the weight between the first neuron inthe backdoor path of the backdoored classifier g and xn, (m) is a set of indices of the location ofthe backdoor trigger, and n (n (m)) is the value of the backdoor pattern. The above equationmeans a clean input x cannot activate the backdoor path when the weighted sum of its deviation fromthe backdoor trigger is no smaller than (a hyper-parameter). Proof. If x cannot activate the backdoor path, the outputs of the neurons in the backdoor path are 0.Thus, the output of the backdoored classifier does not change if those neurons are pruned. As a result,the prediction of the backdoored classifier for x is the same as that of the pruned classifier.",
  "We consider two types of defenses: query-based defenses and gradient-based defenses": "Proof of Proposition 1. Based on Theorem 1, the output of the backdoored classifier is the same asthe pruned classifier if an input cannot activate the backdoor path. Thus, the output for any inputfrom the defense dataset will be the same for the two classifiers, which leads to the same detectionresult. Proof of Proposition 2. When inputs cannot activate backdoor path, gradients of outputs of thebackdoored classifier and pruned classifier with respect to their inputs are the same. Thus, detectionresults are same. Pruning-based defenses : We note that a defender can prune the neurons whose outputson clean data are small or Lipschitz constant is large to mitigate our attack . As we willempirically show in , our DFBA can be adapted to evade those defenses. Moreover, weempirically find that our adaptive attack designed for pruning-based defenses can also evade otherdefenses such as Neural Cleanse and MNTD (see for details). Therefore, we can use ouradaptive attack in practice if we dont have any information on the defense.",
  "B.2.2Unremovable Analysis": "The goal of backdoor removal is to remove the backdoor in a classifier. For instance, fine-tuning iswidely used to remove the backdoor in a classifier. Suppose we have a dataset Dd = {xi, yi}Ni=1.Given a classifier f, fine-tuning aims to train it such that it has high classification accuracy on Dd.Formally, we have the following optimization problem:",
  "(x,y)Dd(f (x), y),": "where is the loss function, e.g., cross-entropy loss, and f is initialized with f. We can use SGD tosolve the optimization problem. However, fine-tuning is ineffective against our DFBA. Formally, wehave: Proof of Proposition 3. Given that 1) each training input cannot activate the backdoor path, and 2)the output of the neurons in the backdoor path is independent of the neurons that are not in thebackdoor path, the gradient of loss function with respect to parameters of the neurons in the backdoorpath is 0. Thus, the parameters of those neurons do not change.",
  "Datasets: We consider the following benchmark datasets: MNIST, Fashion-MNIST, CIFAR10,GTSRB, and ImageNet": "MNIST: MNIST dataset is used for digit classification. In particular, the dataset contains60,000 training images and 10,000 testing images, where the size of each image is 28 28.Moreover, each image belongs to one of the 10 classes. Fashion-MNIST: Fashion-MNIST is a dataset of Zalandos article images. Specifically,the dataset contains 60,000 training images and 10,000 testing images. Each image is a 28 28 grayscale image and has a label from 10 classes. CIFAR10: This dataset is used for object recognition. The dataset consists of 60,000 32 32 3 colour images, each of which belongs to one of the 10 classes. The dataset is dividedinto 50,000 training images and 10,000 testing images. GTSRB: This dataset is used for traffic sign recognition. The dataset contains 26,640training images and 12,630 testing images, where each image belongs to one of 43 classes.The size of each image is 32 32 3. ImageNet: The ImageNet dataset is used for object recognition. There are 1,281,167training images and 50,000 testing images in the dataset, where each image has a label from1,000 classes. The size of each image is 224 224 3.",
  "summarizes the statistics of those datasets. Unless otherwise mentioned, we use MNISTdataset in our evaluation": "Parameter settings: We conducted all experiments on an NVIDIA A100 GPU, and the random seedfor all experiments was set to 0. Our attack has the following parameters: threshold , amplificationfactor , and trigger size. Unless otherwise mentioned, we adopt the following default parameters:we set = 0.1. Moreover, we set to satisfy L1 = 100, where L is the total number of layers ofa neural network. In , we conduct an ablation study on and . We find that and couldinfluence the utility of a classifier and attack effectiveness. When is small, our method would notinfluence utility. When is large, our attack could consistently achieve a high attack success rate.Thus, in practice, we could set a small and a large . We set the size of the backdoor trigger (in the bottom right corner) to 4 4 and the target class to 0for all datasets. In our ablation studies, we will study their impact on our attack. In particular, we set",
  "DEffectiveness of DFBA Under State-of-the-art Defenses": "Our DFBA cannot be detected by Neural Cleanse : Neural Cleanse (NC) leverages a cleandataset to reverse engineer backdoor triggers and use them to detect whether a classifier is backdoored.In our experiments, we use the training dataset to reverse engineer triggers. We adopt the publiclyavailable code in our implementation. We train 5 clean classifiers and then respectively craft 5backdoored classifiers using DFBA and Hong et al. . We report the detection rate which is thefraction of backdoored classifiers that are correctly identified by NC for each method. The detectionrate of NC for DFBA is 0. In contrast, NC can achieve 100% detection rate for Hong et al. basedon the results in in Hong et al. (our setting is the same as Hong et al. ). Therefore,our DFBA is more stealthy than Hong et al. under NC. The reason why NeuralCleanse does notwork is as follows. NeuralCleanse uses a validation dataset to reverse engineer a trigger such that aclassifier is very likely to predict the target class when the trigger is added to inputs in the validationdataset. However, our backdoor path is very hard to be activated by non-backdoored inputs (as shownin ). In other words, our backdoor path is not activated when NeuralCleanse reverse engineersthe trigger, which makes NeuralCleanse ineffective.",
  ": Comparing our DFBA with Hong et al. under fine-tuning after pruning neuronson MNIST": "Our DFBA is resilient to fine-tuning: Given a backdoored model, a defender can use clean data tofine-tune it to remove the backdoor. To consider a strong defense, we use the entire training datasetof MNIST to fine-tune the backdoored classifier, where the learning rate is 0.01. shows theexperimental results of Hong et al. and DFBA. We find that the ASR of DFBA remains highwhen fine-tuning the backdoored classifier for different epochs. In contrast, the ASR of Hong etal. decreases as the number of fine-tuning epochs increases. Our DFBA is resilient to fine-pruning: Liu et al. proposed to prune neurons whose outputsare small on a clean dataset in a middle layer of a classifier to remove the backdoor. Our DFBA canbe adapted to evade this defense. Suppose we have a clean validation dataset, Liu et al. proposedto remove neurons whose outputs are small in a certain middle layer (e.g., the last fully connectedlayer in a fully connected neural network). Our DFBA can be adapted to evade this attack. Our ideais to let both clean and backdoored inputs activate our backdoor path. As we optimize the backdoortrigger, the outputs of neurons on backdoored inputs are much larger than those on clean inputs. Thus,our adapted backdoor attack is effective while maintaining classification accuracy on clean inputs. Inparticular, we randomly sample from a zero-mean Gaussian distribution N(0, 2) as parameters that",
  ": The ACC and ASR of our attack under Lipchitz Pruning on MNIST": "are related to features whose indices in (m) for the selected neuron in the first layer, where isthe standard deviation of Gaussian noise. Moreover, we dont change the bias of the selected neuronin the first layer such that both clean inputs and backdoored inputs can activate the backdoor path.In our experiments, we set = 4, 000 and = 1. Note that we set = 1 because the output of theneuron selected from the first layer is already very large for a backdoored input. We prune neurons whose outputs are small on the training dataset. shows results for DFBAand Hong et al. . We find that DFBA can consistently achieve high ASR when we prune differentfractions of neurons. In contrast, the ASR of Hong et al. decreases as more neurons are pruned.We further fine-tune the pruned model (we prune neurons until the ACC drop is up to 5%) using thetraining dataset. shows the results. We find that DFBA can still achieve high ASR afterfine-tuning. MNTD cannot detect DFBA: MNTD trains a meta classifier to predict whether a classifieris backdoored or not. Roughly speaking, the idea is to train a set of clean models and backdooredmodels. Specifically, given a set of inputs (called query set) and a model, MNTD uses the output ofthe model on the query set as its feature. Then, a meta-classifier is trained to distinguish clean modelsand backdoored models based on their features. Note that they also optimize the query set to boostthe performance. We evaluate the performance of MNTD for DFBA on MNIST. We use the publicly available codeof MNTD in our experiments4. We respectively train 5 clean classifiers using different seeds andthen craft 5 backdoored classifiers using DFBA for FCN and CNN. We use the detection rate as theevaluation metric, which measures the fraction of backdoored classifiers that are correctly identifiedby MNTD. We find the detection rate of MNTD is 0 for both FCN and CNN, i.e., MNTD is ineffectivefor DFBA. Our empirical results are consistent with our theorem (Proposition 1). I-BAU cannot remove DFBAs backdoor: Zeng et al. proposed I-BAU, which aimsto unlearn the backdoor in a classifier. I-BAU formulates the backdoor unlearn as a minimaxoptimization problem. In the inner optimization problem, I-BAU aims to find a trigger such thatthe classifier has a high classification loss when the trigger is added to clean inputs. In the outeroptimization problem, I-BAU aims to re-train the classifier such that it has high classification accuracyon clean inputs added with the optimized trigger.",
  ": Impact of , , and trigger size on DFBA": "We apply I-BAU to unlearn the backdoor injected by DFBA on MNIST. We use the publicly availablecode in our implementation5. shows our experimental results. We find that the ASR is stillvery high after applying I-BAU to unlearn the backdoor in the classifier injected by DFBA. Ourresults demonstrate that I-BAU cannot effectively remove the backdoor. DFBA can be adapted to evade Lipschitz Pruning : Zheng et al. proposed to leverageLipschitz constant to prune neurons in a classifier to remove the backdoor. In particular, for the kthconvolutional layer, Zheng et al. proposed to compute a Lipschitz constant for each convolutionfilter. Then, Zheng et al. compute the mean (denoted as k) and standard deviation (denoted ask) of those Lipschitz constants. The convolution filters whose Lipschitz constants are larger thank + uk are pruned, where u is a hyperparameter. The method can be extended to a fully connectedlayer by computing a Lipschitz constant for each neuron. Our DFBA can be adapted to evade . In particular, we set to be a small value for the neuronsselected in the middle layers such that its Lipschitz constant is smaller than k. To ensure theeffectiveness of backdoor attacks, our idea is to change the parameter of the neurons in the outputlayer. In particular, we can set the weight between sL1 and the output neuron for the target classytc to be a larger number but set the weight between sl and the remaining output neurons to be asmall number. Note that the neurons in the output layer are not pruned in . shows ourexperimental results. We find that our DFBA can consistently achieve high ASR for different u,which demonstrates the effectiveness of our backdoor attacks. We note that the ACC is low for smallu because more neurons are pruned by Lipschitz Pruning when u is smaller. Effectiveness of our adaptive attacks tailored to pruning-based defenses for other defenses: Ourattack requires the attacker to know the defense information to have a formal guarantee of the attackefficacy under those defenses. When the attacker does not have such information, the attacker canuse our adaptive attack designed for pruning-based defenses in practice. We performed evaluationsunder our default setting to validate this. We find that our adaptive attack designed for fine-pruningcan also evade Neural Cleanse, fine-tuning, MNTD, I-BAU, and Lipschitz pruning. In particular,the detection rate of both Neural Cleanse and MNTD for backdoored classifiers crafted by DFBA is0% (we apply the detection on five backdoored classifiers and report the detection accuracy as thefraction of backdoored classifiers that are detected by each method), which means they cannot detectbackdoored classifiers. The attack success rate (ASR) is still 100% after we fine-tune the backdooredclassifier for 50 epochs (or use I-BAU to unlearn the backdoor or use Lipschitz pruning to pruneneurons to remove the backdoor). Our results demonstrate that our adaptive attack can be used whenthe information on the defense is unavailable.",
  "Impact of : a shows the impact of on MNIST. We have the following observations. First,our attack consistently achieves high ASR. The reason is that the backdoor path crafted by DFBA": "is always activated for backdoored inputs when > 0. Second, DFBA achieves high BA when is very small, i.e., DFBA can maintain the classification accuracy of the backdoored classifier forclean testing inputs when is small. Third, BA decreases when is larger than a threshold. This isbecause the backdoored path can also be activated by clean inputs when is large. As a result, thoseclean inputs are predicted as the target class which results in the classification loss. Thus, we can set to be a small value in practice, e.g., 0.1. Impact of : b shows the impact of on MNIST. The ASR of DFBA first increases as increases and then becomes stable. The reason is that the output of neurons in the backdoor path islarger for a backdoored input when is larger. As a result, the backdoored input is more likely to bepredicted as the target class. Thus, we can set to be a large value in practice. Impact of trigger size: c shows the impact of trigger sizes on MNIST. We find that ourbackdoor attack can consistently achieve high ASR and BA for backdoor triggers with different sizes.For instance, our attack could still achieve a 100% ASR when the size of the trigger is 2 2. Impact of trigger location: We note that our attack is also effective even if the trigger positionchanges for convolutional neural networks. The reason is that a convolutional filter is applied indifferent locations of an image to perform convolution operation. Thus, the output of the convolutionfilter would be large when the trigger is present and thus activate our back path, making our attacksuccessful. We also validate this by experiments. For instance, we find that our attack could stillachieve a 100% ASR when we change the location of the trigger under the default setting.",
  "FNeuron Selection for a CNN": "For a convolutional neural network, a convolution filter generates a channel for an input. In particular,each value in the channel represents the output of one neuron, where all neurons whose outputs are inthe same channel share the same parameters. We randomly select one neuron whose output valuedepends on the features with indices in (m). We note that, as neurons in the same channel sharethe parameters, they would be affected if we change the parameters of one neuron. We consider thiswhen we design our DFBA. As a result, our DFBA can maintain the classification accuracy of theclassifier for normal testing inputs as shown in our experimental results.",
  "GComparing with Hong et al. on CIFAR10 Dataset": "We also compare our attack with Hong et al. on CIFAR10 dataset, where the classifier is CNN. Wecompare DFBA with Hong et al. for fine-tuning and fine-pruning. Our comparison results are asfollows. After fine-tuning, the ASRs of our DFBA and Hong et al. are 100% and 88%, respectively.After fine-pruning, the ASRs of our DFBA and Hong et al. are 100% and 84%, respectively. Ourresults demonstrate that our attack is more effective than Hong et al.. Our observations on CIFAR10are consistent with those on MNIST.",
  "HEvaluation of Neural Cleanse, MNTD, I-BAU, and Lipschitz pruningagainst Our Attack on CIFAR10": "We also evaluate other defenses on CIFAR10, including Neural Cleanse, MNTD, I-BAU, and Lipschitzpruning against our attack. The detection accuracy (we apply the detection on five backdooredclassifiers and report the detection accuracy as the fraction of backdoored classifiers that are detectedby each method) of Neural Cleanse and MNTD is 0% for our DFBA. Our DFBA can still achievea 100% ASR after we apply Lipschitz pruning to the backdoored classifier. We find that I-BAUcould indeed reduce the ASR of our method to 10%. But it also significantly jeopardized the modelsclassification accuracy on the clean data (from 80.15% to 18.59%). The results show that afterretraining, the model performs almost randomly. We tried different hyperparameters for I-BAU andconsistently have this observation. These results show that most defense methods are not effectiveagainst our method. Even I-BAU can remove our backdoor, it achieves this by significantly sacrificingthe utility.",
  "IPotential Adaptive Defenses": "We designed two adaptive defense methods tailored for DFBA. These methods exploit the fact thatour DFBA-constructed backdoor paths are rarely activated on clean data and that some weights arereplaced with zeros when modifying the model weights: Anomaly detection: Check the number ofzero weights in the model. Activation detection: Remove neurons in the first layer that always havezero activation values on clean datasets. To counter these adaptive defenses, we replaced zero weights with small random values. We usedGaussian noise with = 0.001. We conducted experiments on CIFAR10 with ResNet-18, using thedefault hyperparameters from the paper. Results show we still achieve 100% ASR with less than 1%performance degradation. This setup eliminates zero weights, rendering anomaly detection ineffective. We also analyzed theaverage activation values of 64 filters in the first layer on the training set (see Figure in PDF). Ourbackdoor path activations are non-zero and exceed many other neurons, making activation detectionineffective. We tested fine-pruning and Neural-Cleanse (Anomaly Index = 1.138) under this setting.Both defenses failed to detect the backdoor. We didnt adopt this setting in the paper as it compromisesour theoretical guarantees. Our goal was to prove the feasibility and theoretical basis of a novelattack method. Additionally, we can distribute the constructed backdoor path across multiple paths toenhance robustness. We plan to discuss these potential methods in the next version. Another interesting idea is to use the GeLU activation function instead of ReLU. However, We believethat simply replacing ReLU with GeLU may not effectively defend against DFBA. Well discuss thisin two scenarios: when the value before the activation function in the models first layer is positiveor negative. According to our design and experimental results, essentially only inputs with triggersproduce positive activation values, which are then continuously amplified in subsequent layers. In thispart, GeLU would behave similarly to ReLU. For cases where the value before the activation functionis negative (i.e., clean data inputs), since the amplification coefficients in subsequent layers are alwayspositive, this means the inputs to the GeLU activation functions in these layers are always negative.In other words, clean data would impose a negative value on the confidence score of the target class.The minimum possible output from GeLU only being approximately 0.17, and in most cases thisnegative value is close to 0. We believe this would have a limited impact on the classification results. On the other hand, directly replacing ReLU activation functions with GeLU in a trained model mightaffect the models utility. Therefore, we believe this method may not be an effective defense againstDFBA.",
  "JDiscussion and Limitations": "Generalization of DFBA: In this work, we mainly focus on supervised image classification. Recentresearch has generalized backdoor attacks to broader learning paradigms and application domains,such as weak-supervised learning , federated learning , natural languageprocessing , graph neural networks , and deep reinforcement learning . Aspart of our future work, we will explore the generalization DFBA to broader learning problems. Wewill also investigate the extension of DFBA to other models (e.g., RNN and Transformer). Potential countermeasures: In Appendix B, we prove DFBA is undetectable and unremovable bycertain deployment-phase defenses. However, it can be potentially detected by testing-phase defensesmentioned in . For example, we will show that a state-of-the-art testing-phase defense can prevent our backdoor when the trigger size is small but it is less effective when the trigger size islarge. PatchCleanser is a state-of-the-art provably defense against backdoor attacks to classifiers.Roughly speaking, given a classifier, PatchCleanser can turn it into a provably robust classifier whosepredicted label for a testing input is unaffected by the backdoor trigger, once the size of the backdoortrigger is bounded. We evaluate PatchCleanser for our DFBA on the ImageNet dataset with thedefault parameter setting. We conducted three sets of experiments. In the first two sets of experiments,we evaluate our DFBA with a small trigger and a larger trigger for PatchCleanser, respectively. Inthe third set of experiments, we adapt our DFBA to PatchCleanser using a small backdoor trigger(we slightly defer the details of our adaptation). PatchCleanser uses a patch to occlude an image in different locations and leverages the inconsistency of the predicted labels of the given classifier fordifferent occluded images to make decisions. Following Xiang et al. , we use 1% pixels of animage as the patch for PatchCleanser. We have the following observations from our experimental results. First, PatchCleanser can reducethe ASR (attack success rate) of our DFBA to random guessing when the size of the backdoor triggeris small. The reason is that PatchCleanser has a formal robustness guarantee when the size of thebackdoor trigger is small. Second, we find that our DFBA can achieve a 100% ASR when the triggersize is no smaller than 31 31 (the trigger occupies around 1.9% 3131 224224 pixels of an image).Our experimental results demonstrate that our DFBA is effective under PatchCleanser with a largetrigger. Third, we find that we can adapt our DFBA to evade PatchCleanser. In particular, we place asmall trigger (4 4) in two different locations of an image (e.g., upper left corner and bottom rightcorner). Note that we still use a single backdoor path for DFBA. Our adapted version of DFBA canevade PatchCleanser because PatchCleanser leverages the inconsistency of the predicted labels fordifferent occluded images to make decisions. As the trigger is placed in different locations, differentoccluded images are consistently predicted as the target class for a backdoored input since the patchused by PatchCleanser can only occlude a single trigger. As a result, PatchCleanser is ineffective forour adapted DFBA. We confirm this by evaluating our adapted version of DFBA on the ImageNetdataset and find it can achieve a 100% ASR under PatchCleanser. Universal adversarial examples: Given a classifier, many existing studies showed that anattacker could craft a universal adversarial perturbation such that the classifier predicts a target classfor any input added with the perturbation. The key difference is that our method could make aclassifier predict the target label with a very small trigger, e.g., our method could achieve 100%Attack Success Rate (ASR) with a 2 x 2 trigger as shown in c. Under the same setting, theASR for the universal adversarial perturbation (we use Projected Gradient Descent (PGD) tooptimize it) is 9.84%. In other words, our method is more effective. Limitations: Our DFBA has the following limitations. First, we mainly consider the patch trigger inthis work. In future works, we will explore designing different types of triggers for our attack (e.g.,watermark). Second, to achieve a strong theoretical guarantee, we need to relax our assumption andassume the knowledge of the defenses. Our future work will investigate how to relax this assumption."
}