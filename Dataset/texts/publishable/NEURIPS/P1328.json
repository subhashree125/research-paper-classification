{
  "Abstract": "Recent token reduction methods for Vision Transformers (ViTs) incorporate tokenmerging, which measures the similarities between token embeddings and combinesthe most similar pairs. However, their merging policies are directly dependent on in-termediate features in ViTs, which prevents exploiting features tailored for mergingand requires end-to-end training to improve token merging. This paper proposesDecoupled Token Embedding for Merging (DTEM) that enhances token mergingthrough a decoupled embedding learned via a continuously relaxed token mergingprocess. Our method introduces a lightweight embedding module decoupled fromthe ViT forward pass to extract dedicated features for token merging, addressingthe restriction from using intermediate features. The continuously relaxed tokenmerging, applied during training, enables us to learn the decoupled embeddingsin a differentiable manner. Thanks to the decoupled structure, our method can beseamlessly integrated into existing ViT backbones and trained either modularlyby learning only the decoupled embeddings or end-to-end by fine-tuning. Wedemonstrate the applicability of DTEM on various tasks, including classification,captioning, and segmentation, with consistent improvement in token merging. Es-pecially in the ImageNet-1k classification, DTEM achieves a 37.2% reduction inFLOPs while maintaining a top-1 accuracy of 79.85% with DeiT-small. Code isavailable at",
  "Introduction": "Transformers have become the dominant and most popular architecture in machine learning,excelling in various modalities and tasks. In computer vision, Vision Transformers (ViTs) haveachieved state-of-the-art performance, outperforming conventional backbones in tasks such as imageclassification , object detection , and segmentation , as well as multimodal applicationssuch as image captioning and visual question answering . A key factor in the success ofViTs is their ability to capture long-range dependencies between patches or tokens, regardless of theirspatial positions, using self-attention. However, due to self-attention, ViTs have high computationaland memory costs that increase quadratically with the number of tokens. Consequently, there hasbeen significant interest in developing methods to improve the computational efficiency of ViTs. In this pursuit, token reduction aims to progressively reduce the number of tokensin Transformer layers, often adhering to predefined reduction rates. Early approaches propose to prune unimportant tokens based on their contribution to the task, as measuredby scoring functions. Yet, simply pruning tokens leads to information loss, often resulting insignificant performance degradation in high reduction rates. Alternatively, approaches based ontoken merging aim to combine redundant tokens instead of removing them.Such redundancy is measured by the similarity between the tokens based on intermediate features inViT, such as token- or key-embeddings. Token merging has several advantages over pruning; it can",
  "ForwardForward": ": Comparison of our method with conventional token merging. Contrary to prior worksthat merge tokens directly based on intermediate features in ViT, our method leverages a decoupledembedding to extract features tailored for token merging. The embedding module is trained viacontinuous relaxation of grouping and merging operators, i.e., soft grouping and merging, respectively,that allow differentiation.",
  "achieve improved performance by reducing information loss in token reduction and can be seamlesslyplugged into pre-trained models without altering the architecture": "However, merging tokens directly based on intermediate features, which are responsible for contextualencoding, presents several limitations. Firstly, these features are hard to be tailored specifically fortoken merging. This is because the same intermediate feature should be used for contextual encodingand merging; thereby, it would be less effective than having separate features dedicated to each role.Secondly, enhancing the merging process, which entirely relies on intermediate features, necessitatesend-to-end training of the entire network. This makes it difficult to leverage pre-trained modelseffectively and typically requires extensive data to prevent overfitting. To this end, we propose Decoupled Token Embedding for Merging (DTEM) that learns decoupledtoken embedding specifically tailored to enhance token merging. We introduce a lightweight trainableembedding module decoupled from the intermediate feature in the ViT and use it to modulatethe merging policy. This resolves the dependency of token merging on intermediate features andfacilitates the decoupled embedding to extract only suitable features for enhanced token merging.Moreover, since the modules are separated from ViTs, improved merging can be achieved withoutaltering the ViT parameters, allowing for efficient modular optimization with pre-trained models. However, learning the decoupled embedding module directly from conventional token merging isinfeasible, since the grouping policy, i.e., deciding which tokens to merge, is based on discreteoperators such as hard cluster assignment or matching on a bipartite graph ((a)).To address this, we design a continuous relaxation of token merging that softly merges tokens in adifferentiable way according to their similarities ((b)). The relaxed operators, applied duringtraining, enable training of the decoupled embedding directly through the grouping policy to improvetoken merging. We also observe that such relaxed operators tend to facilitate generalization of thelearned decoupled embedding across unseen token reduction rates. During inference, our modelconverges to existing token merging methods by replacing the relaxed operators with hard ones. We integrate DTEM in two distinct ways: modular and end-to-end full fine-tuning. For the former, wetrain only the embedding module while keeping the parameters of the pre-trained model frozen, whilelater we train the entire parameters in an end-to-end manner. We apply DTEM to existing pre-trainedvision models and verify its effectiveness in image classification, captioning, and segmentation, eachrequiring a different level of granularity in representation. Despite the simplicity, DTEM consistentlyimproves token merging in all three tasks, offering a better trade-off between task performance andcomputation cost. We further analyze DTEMs components, design choices, and training efficiency.Overall, our contributions are summarized as follows: We propose DTEM, a novel approach to enhance token merging by decoupled token em-bedding learned via continuous relaxation of token merging. The decoupled embeddingis dedicated to merging and learns features suitable for merging directly from our relaxedtoken merging. DTEM can be applied through end-to-end full fine-tuning or in a modular way by train-ing only the added embedding module. When trained modularly, the method deliversimprovements even with substantially smaller datasets and fewer training epochs.",
  "Background": "Given a Transformer that takes N input tokens X RNd, the objective of token merging isto gradually merge r tokens at each Transformer block, reducing the number of tokens to X R(Nr)d. Here, the r denotes the reduction rate. To this end, prior works conduct the merging intwo steps, grouping and merging, which are expressed as:",
  "E = Group(S),(1)X = Merge(X, E),(2)": "where S RNN denotes the similarity matrix of tokens, e.g., sij = cos(xj, xj). Given thesimilarity S, the grouping operator (Eq. 1) identifies pairs of tokens to merge and represents themin the reachability matrix E {0, 1}NN with (N r) connected components, where eij = 1indicates that the ith and jth tokens belong to the same component and will be merged. The mergingoperator (Eq. 2) then combines all connected tokens in E by pooling. The performance of the above framework highly depends on the choice of the grouping operator, asit dictates the merging policy (i.e., which tokens to merge), and computing the reachability matrixcan be costly. Early works employ clustering algorithms , but they tend to be slow due to theiterative procedure and often suffer from performance drops due to the dramatic distribution shiftfrom X to X caused by aggressive clustering. Recently, ToMe introduced Bipartite Soft Matching (BSM) as an efficient grouping operator ofEq. 1. To parallelize the computation, BSM divides the input tokens into two disjoint sets A and B,and constructs a bipartite graph. Then for each node i A, it chooses an edge with highest similarityarg maxjB sij, and choose the r most similar edges afterwards to obtain the sparse adjacency matrixE {0, 1}|A||B| where ij eij = r. The merging is performed by combining the connectedtokens in E, where the connected components can be easily found since each token in A has at mostone edge. ToMe also proposes tracking the size of the combined tokens and accounts for it inself-attention. Specifically, given a vector m RNr representing the size of combined tokens, theproportional attention is used in the QKV self-attention layers by:",
  "where denotes the softmax function": "LimitationsAlthough the success of merging depends mostly on grouping operation (Eq. 1), thegrouping depends entirely on the similarity of the intermediate ViT feature (X or K) in the priorworks. This is mainly because the grouping comprises discrete operators, such as clustering andmatching, that prevent the gradient flow through grouping (Eq. 1). Thus, the only viable option toimprove the merging is by updating the intermediate feature X by back-propagating through themerging operator (Eq. 2). However, it leads to extensive end-to-end training of the entire network,preventing off-the-shelf usage and resulting in suboptimal performance due to the conflict betweenthe token feature required for optimal merging and task performance.",
  "Method": "Our objective is to improve token merging by learning the decoupled embedding specifically tailoredfor merging. To this end, we base our method on the standard token merging framework introducedin the previous section (Eqs. 1, 2). Instead of directly leveraging the ViT features X for grouping, wepropose to learn additional per-token embedding modules Z = f(X; ), which are decoupled fromthe forward pass of the ViT and used only to compute the similarity S in Eq. 1 by sij = cos(zi, zj)(Sec. 3.1). Since the grouping operator is entirely dependent on similarity, we can directly modulatethe grouping (or merging) policy by learning Z. Furthermore, since the embedding is decoupled fromthe ViT forward pass, enhancements in merging can be achieved modularly without altering the ViTparameters but only learning the embedding Z.",
  "To enable our model to learn such embeddings through merging, we propose a continuous relaxationof the grouping and merging operators in Eq. 1 and Eq. 2, respectively. Specifically, our relaxed": "grouping operator generates a continuous matrix E, whose elements eij indicates the softdegree of merging ith token into the jth token (Sec. 3.2). To incorporate such soft commitmentin merging, we also propose a relaxed merging operator that combines tokens with continuousweights defined by E (Sec. 3.3). Since the token merging is performed continuously with our relaxedoperators, we discretize them after the training, reducing our framework to behave similarly to thehard merging methods (Sec. 3.4).",
  "Decoupled Embedding Module": "We first describe the choice of the embedding module decoupled from the forward pass of ViTs. Tofacilitate token merging at each Transformer block, we introduce per-token projection layers intoeach block l {1, . . . , L}:Decoupled embedding Z : zi = f(xi; l),(4)Token similarity S : sij = cos(zi, zj),(5) where X RNd is the input to the self-attention and Z RNd denotes the output decoupledembedding with d d. The output embeddings will be used solely to shape the merging policy (i.e.,deciding which tokens to merge) in the grouping operator based on the similarity S. Minimizing additional run-time and computational overheads is essential to the embedding moduledesign. In our approach, we employ a token merging between the self-attention and feed-forward layerfollowing . It allows parallelizing the computation of the attention and decoupled embedding,avoiding the potential overhead that comes from serialization. Moreover, we discover that even ashallow module, consisting solely of an affine transformation, can achieve improvement with minimalcomputational expense (Sec. 4.4). This further minimizes the number of additional parameters to lessthan 1% and enables the training of the module with a small amount of data.",
  "Soft Grouping": "Given the similarity matrix S obtained from the decoupled token embeddings Z, soft grouping aimsto approximate the grouping operation through a continuous relaxation that enables differentiation.However, building a general continuous grouping operator of Eq. 1 is challenging since the outputreachability matrix is inherently discrete. Instead, we employ BSM as our target grouping operator, which offers the benefit of bypassingthe reachability matrix and allows for merging to be defined directly on the adjacency matrixE {0, 1}|A||B|. To be a valid approximation of the grouping performed by BSM, the softgrouping operator should produce a continuous adjacency matrix E |A||B| that satisfies twokey conditions. Firstly, it should simulate r distinct edges with high values, thereby implementing thevalid merging policy, i.e., combining the r most similar token pairs. Secondly, each node in A shouldbe associated with at most one edge (i.e.,",
  "with": "ij eij r, representing the total number of selected edges. The clipping function, composedof a max operator (max) and stop-gradient (sg), is introduced to ensure that the resulting E is a validcontinuous adjacency matrix. Note that the soft grouping satisfies the aforementioned key conditions. As 0, At converges to theone-hot matrix that indicates the nodes in A and B with maximum stij. This results in A representingr most similar pair, thus satisfying the first condition. Meanwhile, edges associated with such nodes inA are excluded from future selection according to Eq. 7, since log(1",
  "Soft Merging": "While the soft grouping and the resulting soft adjacency matrix effectively approximate the groupingprocess, it is crucial to design the merging operator to incorporate such soft decisions. In our approach,for the given soft adjacency matrix where eij corresponds to tokens i A and j B, our soft mergingis designed such that the ith token merges into the jth token in proportion to the value of eij. Our soft merging operator applies the asynchronous updates on tokens in two sets, A and B. For eachtoken j B, the operator update their feature xj and the effective size mj by aggregating tokens inA based on the soft adjacency matrix E from .2 by:",
  "jB eij).(10)": "Note that with the binary adjacency matrix E, the effective size of the tokens in A reduces to zero byEq. 10 if they have outbounding edges. Such tokens will be excluded from the subsequent mergingprocess by Eq. 9. This process is simulated continuously during training with our soft adjacencymatrix (i.e., each token will be continuously absorbed into others), while it is used to actually reducethe tokens at inference using a discretized adjacency matrix. Interestingly, we observe that the decoupled embedding, trained with soft merging at a high reductionrate r, generalizes well to lower rates r r. This is presumably because the decoupled embedding islearned to sort r most similar token pairs by the relaxed top-k operator (Eqs. 6, 7), thereby includingthe sorting for smaller reduction rates r r.",
  "Training and Inference": "TrainingThanks to decoupled embedding modules, training can be conducted in two distinctways: modular and end-to-end training. In modular training, we train only the embedding moduleswhile keeping the ViT parameters frozen. This allows our method to fully leverage off-the-shelfmodels while effectively adapting only the merging policy to each task. In end-to-end training, wejointly train all ViT parameters along with our embedding modules. Since our continuous mergingoperators do not reduce the number of tokens during training, we alternate updates between theembedding layers and ViT parameters to save computation. Specifically, when updating the ViTparameters, we fix the embedding layers and use the discretized grouping and merging operators,which allows token reduction in the ViT forward pass, greatly enhancing the efficiency. Conversely,when updating the embedding modules, we apply the soft grouping and merging operators whilefixing the ViT parameters. We alternate this procedure with much more frequency on ViT updates,since the embedding layers have considerably smaller parameters (1%) and hence quickly converge.For both modular and end-to-end training, we simply train our method to minimize the task loss. InferenceFor inference, we discretize the continuous operators in the grouping and mergingprocesses, and perform the hard token merging utilizing the learned decoupled embeddings. Asexplained in Sec. 3.2 and Sec. 3.3, our soft grouping and merging modules are asymptoticallyequivalent to BSM of ToMe. Consequently, we employ BSM to speed up the inference.",
  "DTEM80.378.88818DTEM84.6831.4250": "modular and end-to-end training. We further present our results on COCO image captioning inSec. 4.2 and ADE20K semantic segmentation in Sec. 4.3 to demonstrate that our method can beapplied to tasks requiring various levels of granularity in representation. We then provide a series ofanalyses on the importance of decoupled embedding, the design choices of embedding module, anddata/training efficiency, complemented by visualizations, in Sec. 4.4.",
  "Image Classification": "SetupWe conducted an image classification experiment on ImageNet-1k dataset with 1.28Mtraining and 50k validation images. We apply our method and baselines to various pre-trained ViTmodels, including DeiT-S/B , MAE-B/L , and LV-ViT-S . The image resolution intraining and testing is 224 224 unless otherwise stated. We also present the results for DeiT-T andAugReg ViT-S (with a resolution of 384 384) in the supplementary material (A.1). We reporttop-1 accuracy (Acc@1) on the validation set, with floating-point operation (FLOPs) and throughput(images per second, im/s) to quantify the computation reduction. For the throughput, we measuredon a single NVIDIA 3090 GPU with a batch size of 128 and fp32. Implementation detailWe mostly follow the fine-tuning setup from , which is based on thetraining recipe of DeiT . We initialize the ViTs with pre-trained weights and train for 30 epochs,as in most baselines . For token reduction, we employ a uniform reduction strategy,where the reduction rate r represents the number of tokens removed in each transformer block. Whentraining the embedding modules, we apply the reduction rate r = 16 to ViT-S/B models and r = 8 tothe ViT-L model. As the embedding module, we use a linear layer with an output dimension of 64 forViT-S/B and 128 for ViT-L. We use a temperature scale of = 0.1 and also scale the similarity by0.1 prior to soft grouping. More details can be found in the supplementary material A.2. Modular training reports classification results when approximately 35% and 50% ofFLOPs are reduced by applying token reduction methods to frozen pre-trained ViTs, with ViTparameters remaining unchanged. We compare DTEM with ToMe and EViT in this setting.The results demonstrate that DTEM consistently outperforms the baselines. Specifically, with a 35%reduction in FLOPs, our method improves performance by +0.15% to +0.47% compared to ToMeacross all DeiT-S/B and MAE-B/L models. For a reduction of 50% in FLOPs, DTEM significantlyimproves performance by +0.47% to +1.64%, while adding less than 1% additional FLOPs. In , we further applied our method to LV-ViT , a variant of standard ViT. LV-ViT employsan input embedding module consisting of convolution layers to better tokenize the input image.We apply token merging into the first 12 transformer blocks of LV-ViT-S. Consistent with previousresults, DTEM achieves a +0.2% accuracy gain over ToMe, demonstrating its applicability to LV-ViT.Notably, despite optimizing only the added embedding module parameters, this performance iscomparable to other state-of-the-art methods that fully fine-tune the ViT parameters.",
  "DTEM80.748.9818": "End-to-end training depicts the classification results under different FLOPs and through-puts when token reduction methods are applied through end-to-end training. We compared ourmethod with a fine-tuned version of ToMe and EViT . For the baselines, we report accuraciesby training each model on specific target computation demands under varying reduction rates, e.g.,r = {16, 13, 12, 11} for ToMe. Conversely, for DTEM, we train a single model with a reduction rateof r = 13 for fine-tuning the ViT parameters, while maintaining r = 16 when training the embeddingmodule.1 We then adjust the reduction rate and report the corresponding accuracies during inference. The results demonstrate that our method consistently outperforms the baselines across all levels ofcomputational reduction. Specifically, our method surpasses the baseline accuracy by 0.12% to 0.2%in DeiT-S while adding a small amount of FLOPs and degradation in throughput. This leads to animproved trade-off between accuracy and computational resources, such as FLOPs and throughput.A notable aspect of DTEM is its ability to provide a single trained model that is generalized acrossvarious reduction rates. This can mitigate the training and storage costs associated with the multiplerounds of full fine-tuning often required to support different levels of computational reduction. Comparison to State-of-The-ArtWhile the results in verify the effectiveness of ourmethod in end-to-end training, we compare DTEMs performance with more token reduction methodsin . We mainly considered the 30 epochs training results used in for a faircomparison.2 The table shows that our method achieves superior accuracy compared to other priorarts when computational costs are equated.",
  "Image Captioning": "To demonstrate the broad applicability of our method, we apply DTEM to image captioning, a taskextensively studied in the vision-language domain. Recent captioning models with ViTs typicallyutilize all output patch features to ensure the caption generation is grounded in richer and more 1As explained in Sec. 3.4, we alternate updates between the embedding modules and ViT parameters forend-to-end training.2We further include comparison results from 100 epochs of training in the supplementary material A.1,demonstrating superior performance. : Image captioning evaluation results when token merging is applied. We report withcaption evaluation metrics: BLEU-4 (B@4), CIDEr (C), METEOR (M) and SPICE (S). Reductionrepresents the decreases in FLOPs within the ViT encoder, and # indicates the number of tokenspassed to language decoder.",
  "detailed information about the image, which is crucial for accurate captioning. However, using allpatches may be inefficient due to redundancy in image tokens, motivating the use of token merging": "SetupWe experiment with the COCO caption dataset using the train/val/test split from .We use COCO fine-tuned GIT models, each consisting of a ViT-B or L image encoder and alanguage decoder. The embedding modules are trained modularly with a language modeling lossand use the best model identified through cross-validation. The quality of captions is evaluated usingmetrics of BLEU-4 , METEOR , CIDEr , and SPICE , while the computational cost isreported in terms of the ViT encoders floating-point operations (FLOPs) and the number of tokens(#) passed to the language decoder. More details are provided in the supplementary material A.2. Results presents the image captioning results on the test split when DTEM or ToMe are applied. DTEM outperforms ToMe, achieving a better trade-off between captioning quality andcomputation cost. Specifically, with the GIT-B model, DTEM enhances the CIDEr score by +5.0to +6.0 across reductions in FLOPs of 31% to 41%. Similarly, we observe an improvement rangingfrom +2.3 to +6.0 with the GIT-L model. These results confirm that DTEM provides a better set ofpatch representations by effectively summarizing the information in the image tokens.",
  "To further demonstrate DTEMs applicability, we apply our method to semantic segmentation, awidely studied computer vision task with numerous applications": "SetupWe use a pre-trained Segmenter model and evaluate the token merging on theADE20K dataset, which contains 25k training data across 150 fine-grained semantic categories.Unlike image classification or captioning tasks, segmentation modelsincluding the Segmenterrequire complete image patches (tokens) in the end to decode the segmentation mask. To address this,we follow the approach proposed in that repeatedly merges tokens before each component (e.g.,self-attention and feed-forward network) and then un-merges them after processing the component.We modularly trained our decoupled embedding modules using the cross entropy loss. We reportmean intersection-over-union (mIoU) and floating-point operations (FLOPs) for performance andcomputational cost, respectively. More implementation details are provided in the Appendix A.2.",
  ": Ablation study on decoupled embed-ding module design: (a) decoupled embeddingdimension and (b) number of hidden layers": "Results reports the semantic segmentation results when token merging methods, i.e.,ToMe and DTEM, are applied to Segmenter with ViT-S. Our method consistently offers a bettermIoU to GFLOPs trade-off compared to ToMe. Specifically, DTEM achieves a +0.32 to +1.3improvement in mIoU over ToMe when 25% to 50% FLOPs are reduced. These results demonstratesthe applicability of DTEM in semantic segmentation for enhancing token merging.",
  "We conduct an analysis of DTEM in ImageNet-1k classification, specifically within a modular trainingsetting using the DeiT-S model, unless if otherwise stated": "Importance of decoupled embeddingIn , we ablate the impact of decoupled embeddingin end-to-end training. We observe that naive integration of soft grouping and merging applied tothe keys of self-attention, as in ToMe , degrades the performance. This confirms that decoupledembedding is crucial in DTEM to provide more compelling features for token merging. In , wefurther investigate whether the decoupled embedding used for merging diverges from the intermediatefeatures after training. We report the Kendall rank correlation between token similarities derived fromtwo different featuresself-attention keys and decoupled embeddingbefore and after training. Theresults show a decreased correlation after training, indicating that the decoupled embedding learns adifferent feature for token merging, distinct from the intermediate features. Design choices for soft groupingIn , for the analysis of soft grouping, we compared severalapproaches: (1) as the pruning baseline, we tested DynamicViT applied to a frozen ViT, (2)integrating Gumbel Softmax (GS) to replace the top-1 operation from ToMe to enable differentiation,(3) our method, and (4) our method without proportional attention. The results indicate that ourproposed design for soft grouping performs the best, with proportional attention proving to be crucial. Embedding module design (a) and (b) show the impact of different embedding dimensionsand the number of hidden layers in the embedding module, respectively. In the main results (Sec. 4.1),we use the embedding dimension of 64 and the module without hidden layers. We observe that asimple affine transformation offers sufficient gain while keeping computational costs low. Effect of reduction rate on trainingIn , we analyze the effect of the reduction rateused during training. We observe that the decoupled embedding module, when trained with a highreduction rate r, generalizes well to lower rates during inference. Therefore, it is generally sufficientto set the reduction rate r during training to the maximum number of tokens we wish to reduce duringinference. 79.1 79.2 79.3 79.4 79.5 79.6 DTEM (r=11, 3.06G)ToMe (r=11, 3.02G)ToMe (r=11) w.o. trainingDTEM (100%) DTEM (r=11, 3.06G)ToMe (r=11) w.o. trainingDTEM (100%) 0.31% 0.62%1%10% (a) The size of dataset 78.0 78.2 78.4 78.6 78.8 79.0 DTEM (r=16, 2.35G)ToMe (r=16, 2.32G)ToMe (r=16) w.o. trainingDTEM (100%) (b) Training epochs DTEM (r=16, 2.35G)ToMe (r=16) w.o. trainingDTEM (100%) Top-1 Accuracy (%) : Image classification results ondata and train efficiency: (a) dataset sizeand (b) training epochs. In the experiments,DTEM is modularly trained on DeiT-S model,while ToMe undergoes end-to-end training.",
  ": Visualization of merged tokens.We apply a reduction rate r = 16, leading to11 merged tokens in the final output": "Data/Train efficiencyIn (a) and (b), we examine the data and training efficiency ofDTEM with modular training, respectively. Owing to the parameter-efficient modular approach,DTEM improves the performance of merging even with a limited amount of training data and epochs.Specifically, in (a), our method achieves a gain of +0.44% accuracy even when trainedwith 4000 images, equaling 0.31% of the total dataset. DTEM outperforms the end-to-end trainingapproach with ToMe under 10% of the total dataset for both 35% and 50% reduction of FLOPs,highlighting the potential benefit of our modular training when the entire dataset is unavailable.Moreover, in (b), the results on the effect of varying training epochs show that DTEMquickly converges even at the first epoch. Since DTEM employs modular training even in end-to-endlearning by the alternative optimizations (Sec. 3.4), such rapid convergence is also useful in reducingthe cost of end-to-end training. VisualizationIn , we visualize the token merging to compare ToMe and our modularlytrained DTEM. Tokens belonging to the same group are color-coded identically, highlighting thegrouping changes induced by the decoupled embedding. DTEM prioritizes merging backgroundtokens, allocating more tokens to foreground objects. In contrast, ToMe allocates more tokens to thebackground, indicating a less focused approach to foreground objects.",
  "Conclusion": "We propose Decoupled Token Embedding for Merging (DTEM) that improves token merging viadecoupled token embedding derived directly from the token merging process. Our method introducesthe decoupled embedding, learned through our continuously relaxed token merging, to exploitdedicated features for token merging. The decoupled embedding enhances token merging by resolvingthe dependency of token merging on intermediate features and enables modular training, effectivelyutilizing the frozen pre-trained models. We experiment with DTEM on classification, captioning,and segmentation using various pre-trained ViT models. The experimental results demonstrate thatour method consistently improves token merging, highlighting the importance of features tailoredspecifically for token merging.",
  "J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchicalimage database. In 2009 IEEE conference on computer vision and pattern recognition (CVPR),2009": "A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani,M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby. An image is worth 16x16words: Transformers for image recognition at scale. In International Conference on LearningRepresentations (ICLR), 2021. M. Fayyaz, S. A. Koohpayegani, F. R. Jafari, S. Sengupta, H. R. V. Joze, E. Sommerlade,H. Pirsiavash, and J. Gall. Adaptive token sampling for efficient vision transformers. InEuropean Conference on Computer Vision (ECCV), 2022. K. He, X. Chen, S. Xie, Y. Li, P. Dollr, and R. Girshick. Masked autoencoders are scalablevision learners. In Proceedings of the IEEE/CVF conference on computer vision and patternrecognition (CVPR), 2022.",
  "D. Liu, M. Kan, S. Shan, and X. CHEN. A simple romance between multi-exit vision transformerand token reduction. In International Conference on Learning Representations (ICLR), 2024": "S. Long, Z. Zhao, J. Pi, S. Wang, and J. Wang. Beyond attentive tokens: Incorporating tokenimportance and diversity for efficient vision transformers. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition (CVPR), 2023. D. Marin, J.-H. R. Chang, A. Ranjan, A. Prabhu, M. Rastegari, and O. Tuzel. Token pooling invision transformers for image classification. In Proceedings of the IEEE/CVF Winter Conferenceon Applications of Computer Vision (WACV), 2023. L. Meng, H. Li, B.-C. Chen, S. Lan, Z. Wu, Y.-G. Jiang, and S.-N. Lim. Adavit: Adaptive visiontransformers for efficient image recognition. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition (CVPR), 2022. B. Pan, R. Panda, Y. Jiang, Z. Wang, R. Feris, and A. Oliva. IA-RED$^2$: Interpretability-awareredundancy reduction for vision transformers. In Advances in Neural Information ProcessingSystems (NeurIPS), 2021. K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. Bleu: a method for automatic evaluationof machine translation. In Proceedings of the 40th annual meeting of the Association forComputational Linguistics (ACL), 2002. Y. Rao, W. Zhao, B. Liu, J. Lu, J. Zhou, and C.-J. Hsieh. Dynamicvit: Efficient visiontransformers with dynamic token sparsification. In Advances in Neural Information ProcessingSystems (NeurIPS), 2021.",
  "R.Wightman.Pytorchimagemodels. 2019": "T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf,M. Funtowicz, J. Davison, S. Shleifer, P. von Platen, C. Ma, Y. Jernite, J. Plu, C. Xu, T. L.Scao, S. Gugger, M. Drame, Q. Lhoest, and A. M. Rush. Transformers: State-of-the-art naturallanguage processing. In Proceedings of the 2020 Conference on Empirical Methods in NaturalLanguage Processing: System Demonstrations, pages 3845, Online, Oct. 2020. Associationfor Computational Linguistics.",
  "S. M. Xie and S. Ermon. Reparameterizable subset sampling via continuous relaxations. InInternational Joint Conference on Artificial Intelligence (IJCAI), 2019": "J. Xu, S. De Mello, S. Liu, W. Byeon, T. Breuel, J. Kautz, and X. Wang. Groupvit: Semanticsegmentation emerges from text supervision. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition (CVPR), 2022. Y. Xu, Z. Zhang, M. Zhang, K. Sheng, K. Li, W. Dong, L. Zhang, C. Xu, and X. Sun. Evo-vit: Slow-fast token evolution for dynamic vision transformer. In Proceedings of the AAAIConference on Artificial Intelligence, 2022. H. Yin, A. Vahdat, J. M. Alvarez, A. Mallya, J. Kautz, and P. Molchanov. A-vit: Adaptive tokensfor efficient vision transformer. In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition (CVPR), 2022. W. Zeng, S. Jin, W. Liu, C. Qian, P. Luo, W. Ouyang, and X. Wang. Not all tokens are equal:Human-centric visual analysis via token clustering transformer. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition (CVPR), 2022.",
  "%33.325.7110.419.94149%33.325.7111.120.17": "Classification Results with DeiT-TIn , we further report the classification results of ourmethod with DeiT-T, demonstrating that our method consistently improves token merging in modularand end-to-end training settings. We follow the 30 epoch training settings of DeiT-S/B, except thatwe remove the MixUp and CutMix augmentations, as the performance improves without them. Training for Longer EpochsIn , we further report comparison results from longertraining epochs. We note that some methods can improve their performance by trainingfor significantly more epochs, e.g., 100 epochs. To make a fair comparison, we also trained DTEMfor 100 epochs, as with the baseline, i.e., dTPS . The results show that DTEM outperformsthe baselines in DeiT-T and is comparable to dTPS in DeiT-S, while incurring much lower trainingcosts. This efficiency is credited to DTEMs ability to effectively generalize across various reduction",
  "%dTPS (100ep)79.7DTEM (100ep)79.7": "rates with a single trained model, as detailed in .1. Note that dTPS requires multipletrained models to support different reductions in computation. Moreover, dTPS requires multipletraining losses to achieve such performance, including the self-distillation loss, which brings theremaining tokens closer to those of the teacher model, and the KL divergence loss, which minimizesthe difference in final predictions between the model and its teacher. In contrast, our method canachieve comparable or even better performance using only the task loss. Classification Results with AugReg ViT-SIn , we report experimental results with alarger number of tokens on image classification (AugReg ViT-S with a resolution of 384 384),corresponding to smaller patches or higher input resolutions. The decoupled embedding module istrained modularly with a reduction rate of r = 48. The results show that our method can adapt tosettings with an increased number of tokens, achieving performance gains.",
  "ToMe51.5% (r = 47)82.17.60728DTEM52.0% (r = 48)82.37.54733": "Effect of temperature scalingIn , we report classification results analyzing the effectof the temperature scaling. We experimented with a modular training using the DeiT-S model. Wetrained the decoupled embedding module for 10 epochs. We observe that values within the range of0.1 to 0.3 consistently provide gains with an accuracy difference of 0.1%.",
  "In this section, we provide a more detailed explanation of the implementation": "Common detailsDuring training, our method retains N tokens, even if its effective size m becomeszero. To ensure that these tokens do not participate in the relaxed merging process, we successivelyexclude the r tokens with the minimum effective size from the relaxed merging at each transformerblock. Consequently, after applying this exclusion process l times, relaxed merging operates withN rl tokens at the l-th ViT block. We observe that blocking the gradient flow from the relaxedmerging operators to the intermediate features of ViTs enhances task performance. Therefore, we usedetached ViT intermediate features as input for the embedding module, allowing only the module tolearn from the relaxed merging process. In end-to-end training, we alternate updates between theembedding module and the ViT parameters, as explained in (Sec. 3.4). We set for every 10 updatecycles, the embedding layer is updated once, while the ViT parameters are updated nine times. Pre-trained modelsFor DeiT , we use TIMM pre-trained models, which are availableat github.com/huggingface/pytorch-image-models. For MAE , we use the ImageNet-1k fine-tuned checkpoints of the official implementation, at github.com/facebookresearch/mae. For LV-ViT-S , we use the official pre-trained model weights, available at github.com/zihangJiang/TokenLabeling. For image captioning using GIT , we use the COCO fine-tunedweights on HuggingFace , available at huggingface.co/microsoft/git-base-coco. Forsemantic segmentation using Segmenter , we use the Ade20K pre-trained checkpoint of theofficial implementation, at github.com/rstrudel/segmenter. Image ClassificationOur method and baselines are trained for 30 epochs, each with a batchsize of 1024. We use the AdamW optimizer with a cosine learning rate schedule and a weightdecay of 0.0001. For image augmentation, we follow the DeiT training setting, using RandAug,MixUp, and CutMix. However, we omit MixUp and CutMix in DeiT-T and MAE training, as itperforms better without them. For the learning rate of our method and the baselines, we conduct ahyperparameter search within {0.000001, 0.000005, 0.00001, 0.0001, 0.001}, selecting the learningrate that achieves the best performance. As a result, we use a learning rate of 0.0001 for modulartraining and 0.000005 for end-to-end updates of the ViT parameters, with a minimum learning rateset at 0.000001. Additionally, we implement a drop path rate of 0.1 only during end-to-end trainingwhen updating the ViT parameters. Regardless of whether the training is modular or end-to-end, weapply a reduction rate of r = 16 to train the embedding module with ViT-tiny/small/base models andr = 8 for ViT-Large models. For LV-ViT-S, we apply reduction to the first 12 transformer blocks.Meanwhile, to update the ViT parameters in end-to-end training, we use a different reduction rate ofr = 13. We employ a temperature scale of = 0.1 across different ViTs and also scale the similaritydivided by 0.1 prior to the soft grouping operation. Image CaptioningIn image captioning, we train and evaluate the models using LAVIS . Wetrain the GIT models over 20, 000 iterations, using a batch size of 256. We employ the AdamWoptimizer along with a cosine learning rate schedule. The learning rate is set to 0.0001, graduallydecreasing to 0.000001, with a weight decay of 0.0001. During training, the only form of imageaugmentation that we employ is resizing the images to 224 224; no other image augmentationtechniques are used. For evaluation, beam search is applied with a beam size of 5, incorporating bothrepetition and length penalties set at 1. A reduction rate of r = 13 is used for the GIT base model,",
  "and r = 8 for the GIT large model. We use the model with the best performance on the validationsplit and report its performance on the test split": "Semantic SegmentationTo apply ToMe and DTEM to the semantic segmentation task, we use theapproach proposed in that involves repeatedly merging tokens before each component, such asattention and the feed-forward network, and then un-merging them afterward. To be more specific,given a component of the ViT block, tokens are merged following the conventional token mergingapproach. For ToMe, we define the similarity between tokens using the input ViT features tothe component. For DTEM, we define the similarity using the decoupled embeddings, which areproduced by the decoupled embedding module that takes the same input as the component. Afterprocessing by the component, tokens are unmerged by duplicating the jointly processed tokens, as in. Subsequently, the processed tokens are added with the residual connection. We train and evaluate the model using MMSegmentation . DTEM is trained over 40, 000 iterationswith a batch size of 8, following a 40k iteration schedule. During training, we use an image size of512 512, and apply the default image augmentations, which include resize, random crop, randomflip, and Photometric Distortion, as outlined in . We remove 40% of the tokens at each componentof the transformer block to train DTEM with Segmenter-ViT-S. We apply relaxed token mergingwhen selecting the last 64 tokens for merging, while the remaining tokens to be reduced are firstselected through conventional discrete merging. We observe that this approach resolves unnecessaryleakage from the clipping function and stabilizes training.",
  "A.3Related Work": "Pruning tokensEarly token reduction methods prune tokens based on their importance for task per-formance, keeping tokens in order of importance. The importance is often measured by a lightweightprediction/decision module trained from task loss or using attention scores to keepattentive tokens. The most widely used heuristic approach uses the class token attention scores as ametric of importance, as in . However, as mentioned in the Introduction, removingtokens results in information loss and degrades performance, especially when many tokens arereduced. Combining tokensA line of works attempts to combine tokens rather than removing them. Thebasic idea is to combine similar tokens, focusing on removing repetitive information to minimizethe loss of information. However, early methods, which involved merging or grouping all tokens atcertain stages, achieved limited success. PatchMerger introduced a soft clustering approach usingan attention-like module, while TokenPooling used K-medoids clustering for cluster assignmentto tokens. Recently, ToMe has shown that token merging can be implemented efficiently bygradually merging a number of tokens at each transformer block with matching, resulting in promisingperformance even without training.3 In addition, numerous studies focus on designing methodsthat consider the importance of individual tokens while addressing information loss by mergingsimilar tokens. EViT and Evo-ViT propose to combine pruned tokens into a single tokento address information loss, while BAT and TPS use the importance metric in conjunctionwith matching and clustering. Despite their effectiveness, a common aspect among these methods is their reliance on intermediatefeatures for merging tokens. We focus on improving usch embedding for merging with decoupledembedding learned directly through a continuously relaxed merging. Although not directly related,GroupViT learns features through a grouping block, aiming to learn an implicit segmentationmodel without direct supervision. In contrast, our method aims to reduce the computation cost in theinference. Additionally, DiffRate proposes to learn the reduction strategy (or profile), which determinesthe number of tokens reduced at each transformer block, to enhance token reduction. We note thatthis approach is orthogonal to ours. While our method focuses on tailored features for merging andreduces a fixed number of tokens across blocks, DiffRate focuses on determining the number oftokens to be reduced in each block, relying on ViTs intermediate features for merging. We believethat jointly optimizing both componentsthe number of tokens reduced at each block and the featuresused for mergingcould be a promising future direction.",
  "A.4Limitations": "Although our method conceptually appears to be applicable across a range of domains, includingdifferent modalities like text encoding, its effectiveness has so far been validated only in the computervision tasks with ViTs. Another limitation is that, while our method enhances the computation timeduring inference, it does not reduce the computational cost during the training process. Devising amethod that can sparsify the training process, thereby improving the overall computational efficiencyin both the training and inference phases, is our future direction.",
  "A.6More Visualization Results": "In , we provide visualization results of token merging, i.e., ToMe and DTEM. we visualizethe token merging by color coding the tokens belonging to the same group with identical colors,comparing ToMe and modularly trained DTEM. The result highlights the difference in groupinginduced by the decoupled embedding. DTEM prioritizes merging background patches and allocatesmore patches to foreground objects. Conversely, ToMe allocates more tokens to the background,indicating a less focused approach to foreground objects. ImageToMeDTEMImageToMeDTEM"
}