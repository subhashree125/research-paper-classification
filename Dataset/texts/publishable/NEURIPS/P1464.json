{
  "Abstract": "Machine learning models in astrophysics are often limited in scope and cannot adaptto data from new instruments or tasks. We introduce SpectraFM, a Transformer-based foundation model architecture that can be pre-trained on stellar spectrafrom any wavelength range and instrument. SpectraFM excels in generalizationby combining flexibility with knowledge transfer from pre-training, allowing itto outperform traditional machine learning methods, especially in scenarios withlimited training data. Our model is pre-trained on approximately 90k examplesof synthetic spectra to predict the chemical abundances (Fe, Mg, O), temperature,and specific gravity of stars. We then fine-tune the model on real spectra toadapt it to observational data before fine-tuning it further on a restricted 100-startraining set in a different wavelength range to predict iron abundance. Despitea small iron-rich training set of real spectra, transfer learning from the syntheticspectra pre-training enables the model to perform well on iron-poor stars. Incontrast, a neural network trained from scratch fails at this task. We investigatethe Transformers attention mechanism and find that the wavelengths receivingattention carry physical information about chemical composition. By leveraging theknowledge from pre-training and its ability to handle non-spectra inputs, SpectraFMreduces the need for large training datasets and enables cross-instrument andcross-domain research. Its adaptability makes it well-suited for tackling emergingchallenges in astrophysics, like extracting insights from multi-modal datasets.",
  "Introduction": "In many scientific fields, a rapid increase in data availability has led to the wide-spread use of machinelearning methods [Smith and Geach, 2023]. The machine learning models used in research are oftenrestricted to a single task, e.g. to predict the iron content of stars from spectra, and only make accuratepredictions on data from the instrument that collected its training set. Foundation models, pre-trainedon multiple tasks and data from a variety of sources, have unlocked a versatility that was lacking withclassical machine learning algorithms [Bommasani et al., 2022]. Pre-trained foundation modelscan be released to scientists and used out-of-the-box, or fine-tuned for specific research tasks, inthe hopes of leveraging transfer learning to outperform a machine learning algorithm trained fromscratch [McCabe et al., 2023]. They are particularly useful when applied to 1) datasets too smallto train an effective machine learning model [Walmsley et al., 2024] and 2) cross-instrument andcross-domain datasets that require a synergistic analysis [Parker et al., 2024]. Both situations appear frequently in astronomy. For example, spectroscopic observations from theJames Webb Space Telescope (JWST) with accurate stellar property labels are insufficient in numberfor a training set, O(dozens), and may not represent the required diversity of stellar properties totrain a useful machine learning model. However, if a foundation model were pre-trained on a much",
  "larger dataset of stellar spectra, from other telescopes and synthetic sources, and then fine-tuned onthe JWST dataset, it could transfer its knowledge to this new task": "Training on synthetic data for application to real data often leads to poor predictions due to thesynthetic gap - the differences between the simplified synthetic spectra and the complex observedspectra [Fabbro et al., 2018, OBriain et al., 2021]. The synthetic gap stems from physical assumptionsand idealized instruments in synthetic models, leading to systematic biases and reduced predictionaccuracy when applied to real-world data. We investigate a potential remedy: fine-tuning the modelon a small but well-characterized set of real spectra, allowing the foundation model to adjust to thesecharacteristics of real observational data. The second situation appears when the same source (e.g. a star or galaxy) has been observed bymultiple instruments and in different modalities. APOGEE (Apache Point Observatory GalacticEvolution Experiment) and the Gaia space telescope are two large-scale astronomical surveys [Ma-jewski et al., 2017, Gaia Collaboration et al., 2023]. Gaia Data Release 3 has released photometricobservations, positions, and motions for more than a billion stars. APOGEE Data Release 17 provideshigh-resolution infrared spectra for over 650,000 stars, capturing crucial atomic lines necessaryfor determining the abundances of elements in these stars, vital information for stellar and galacticastrophysics. Recently, machine learning models have combined APOGEE spectra observations withGaia observations to extract more information about stellar properties than what can be done witheither alone [Cantat-Gaudin et al., 2024, Laroche and Speagle, 2024]. Leung and Bovy , hereafter LB23, developed a proof-of-concept Transformer-based foundationmodel for stars observed by Gaia and other sources. The single model trained by LB23 can predictstellar properties like temperature, gravity, and chemical composition from low-resolution Gaiaspectra or other properties, generate synthetic spectra from stellar parameters, and reconstruct missingspectral regions, which demonstrates a versatility in handling astronomical data. Furthermore, work is underway to create a single database that encompasses dozens of surveys andmodalities with the intention of developing an astronomy-wide foundation model [Lanusse et al.,2024]. However, the prototype from LB23 cannot be scaled up to accomplish this feat, since it onlyaccepts tabular data and lacks the ability to work with other modalities, like images, time-seriesmeasurements, and high-resolution spectra. We adapt the LB23 foundation model to work with stellarspectra from any wavelength, not just the low-resolution Gaia spectra it was trained on. Extractinginformation from other wavelength ranges and higher resolutions is critically important for stellarastrophysics research. This involved scaling up the model to interpret spectra of a hundred-fold largersize and using a wavelength encoding scheme adapted from the positional encodings in languagemodels. Unlocking this important modality brings us closer to a foundation model that can be appliedto any instrument in stellar astrophysics research.",
  "Transformer Foundation Model": "Unlike regular neural networks, Transformers are flexible in terms of input size and missing in-puts [Vaswani et al., 2017]. This flexibility is advantageous for developing a foundation model, as itallows the model to be adapted for a variety of tasks. LB23 developed a Transformer encoder-decodermodel and trained it with 118 unique inputs, consisting of various stellar properties and observations,with a maximum input size of N = 64. They used a custom non-linear embedding process to encodetabular inputs:yx = f(wx Mx) + wb,x(1)with Mx as the value of the property token, x. wx, wb,x are learnable weights unique to x, andf is a non-linear activation function. The architecture is structured such that you can provide theencoder whatever information you have about the star and then request any property by requestingvector wx from the decoder. Our approach extends upon LB23 by introducing a wavelength encodingmechanism tailored for stellar spectroscopy with the capability to accept input spectra from acrossany range of wavelengths. Spectroscopic observations consist of pixels of flux in specific wavelength bins, capturing the intensityof light within those wavelengths. There has been prior work using deep learning for APOGEEspectra to predict stellar properties and elemental abundances, notably with the AstroNN framework[Leung and Bovy, 2018]. It is a Convolutional Neural Network designed to work exclusively withAPOGEE data and would not be useful when applied to other instruments like JWST or other wavelength ranges. In our experiments, we compare SpectraFM to AstroNN. Unlike AstroNNs fixedinput sizes and spectral regions, SpectraFMs Transformer architecture allows for generalization andis designed for multi-instrument analysis. In order to develop a model that can work with spectra from any instrument, we input every pixel inthe spectra as an individual token rather than pre-processing the spectra with a fixed input size linearlayer or PCA like other approaches that process spectra with Transformers [Zhang et al., 2024b,a].Each spectra token shares the embedding weights wx, wx,b, but are differentiated by a wavelengthembedding added on to Equation 1 that consists of:",
  "max min,k [0, dmodel 1](3)": "where dmodel = 256 embedding dimensions, and min = 15, 000 , max = 17, 000 were chosenfor infrared spectra ranges like the APOGEE dataset we use for this prototype, though can beexpanded to a larger wavelength range in a future model. This embedding scheme was inspiredby Rzanski et al. which used it in constructing a Transformer-based model for generatingsynthetic spectra. This allows for inputting spectra pixels of any wavelength into our model, suchthat it can trained on any instrument and wavelength range. While LB23 focused on low-resolution Gaia spectra, SpectraFMs wavelength encoding mechanismpushes the model to recognize that spectra from different instruments should contain similar infor-mation about the star at the same wavelengths. In this study we demonstrate generalization fromsynthetic to real spectra, but this encoding is designed to support cross-instrument generalization aswell. SpectraFM is an encoder-decoder Transformer model with approximately 8 million trainable parame-ters. illustrates its architecture. The encoder contains two Transformer blocks interspersedwith dense layers. The decoder receives the encoder output along with a request, encoded as thevector wx representing the desired stellar property. The decoder consists of three Transformer blocksand dense layers which result in the final prediction and a predicted uncertainty. The loss function, adapted from LB23 and also used in AstroNN, is a mean-squared loss that combinesthe uncertainties in both the training data and the models predictions. This approach allows themodel to estimate a predictive uncertainty that reflects how confident it is in each prediction. Theobjective function L is defined as:",
  ",": "where y and y represent the ground truth and predicted values, respectively.The term s =ln2data + 2predincludes both the known uncertainty in the data, data, and the predictive un-certainty of the model, pred, which is learned during training. Minimizing L not only improvesmodel prediction accuracy but also improves the models ability to produce a confidence measurefor each prediction that accounts for variations that neither label uncertainties nor model predictionsfully capture.",
  "Dataset and Training": "In astrophysics, machine learning solutions often face challenges on tasks with limited availability oflabeled data. For instance, while spectroscopic observations from a new telescope might be abundant,only a few dozen stars might have accurately measured properties like iron abundances ([Fe/H]). Toaddress this, some approaches have trained machine learning models on synthetic spectra generatedfrom theoretical simulations [Bialek et al., 2020, Fabbro et al., 2018]. However, the synthetic gapoften leads to inaccurate predictions. Our solution is to fine-tune on a small dataset of real spectraonly after extensively pre-training on synthetic spectra. : Overview of our Transformer-based foundation model architecture for stellar spectroscopy.Left: A comparison between the two possible inputs for a star: the real APOGEE infrared spectraand the synthetic best-fit spectra as generated by ASPCAP assuming simplified physics and withoutthe observational issues that cause the noisy or NaN pixels in the real spectra. Also shown is theoutputs requested from the model for each star that are important for astrophysical studies: the surfacetemperature and gravity, abundance of iron compared to hydrogen and abundance of oxygen andmagnesium compared to iron. Right: The encoder processes the spectral pixels as individual tokens,embedding both flux and wavelength. This allows for input spectra from any wavelength rangeand instrument. The encoded spectral data forms a context about the star, which the decoder uses,along with specific output requests, to generate predictions. The model also estimates a predictionconfidence. Diagram adapted from LB23. : Wavelength regions used for each training and fine-tuning stage for this study. Top:Synthetic spectra used for initial pre-training. Middle: Real spectra used in the first fine-tuning stepfor comparison to AstroNN. Bottom: Real spectra used for the fine-tuning stage focusing on ironabundance prediction in a region not seen in the second step. This interval includes two prominent Felines and uses a limited training set of only 100 iron-rich stars to investigate transfer-learning. : Stellar properties and chemical abundance predictions from real spectra using our modelfine-tuned on real spectra. The point colors represent the prediction uncertainty learned by the basemodel. Our Transformer-based foundation model performs similarly to traditional deep learningmethods like AstroNN [Leung and Bovy, 2018] The synthetic spectra used in this study come from simulations by ASPCAP (APOGEE StellarParameters and Abundances Pipeline) [Prez et al., 2016]. ASPCAP generates spectra under theassumption of 1D Local Thermodynamic Equilibrium (LTE), which simplifies the radiative transfercalculations in stellar atmospheres. For each real stellar spectra in APOGEE, ASPCAP releasesthe best-fit synthetic spectra. This synthetic spectra dataset thus has the same distribution of stellarproperties and is split into a train/test set with its real spectra pairs to prevent test set leakage. Thesynthetic spectra are much cleaner than real observations, lacking instrumental noise and observationaleffects like cosmic rays, atmospheric effects and detector issues. highlights the differenceswhich lead to the synthetic gap that arises when training models only on synthetic spectra, whichwe attempt to resolve with fine-tuning on a small dataset of real spectra. We select the following stellar properties for training: temperature (Teff), specific gravity (logg), [Fe/H], [O/Fe], and [Mg/Fe]. Teff and log g are important physical properties for classifyingstars. [Fe/H], [O/Fe], and [Mg/Fe] abundances are essential for understanding the chemical anddynamical evolution of galaxies and were listed as top-priority elements for APOGEE to detect.Since Transformer models can work effectively with missing data, we do not need to remove starsfrom the training set with one or more missing properties. For more details about our data refinementpipeline see Section B. We pre-train our model on synthetic APOGEE spectra to predict combinations of Teff, log g, [Fe/H],[O/Fe], and [Mg/Fe]. Our training set consists of 90k synthetic stars that matches the distributionof stars seen in APOGEE. Pre-training occurs on 4x Nvidia A100 GPUs over 325 epochs, takingapproximately 21 hours. The learning rate starts at 104 and varies throughout training according toa cyclic scheme known as Cosine Annealing with Warm Restarts, a common method that acceleratesconvergence [Loshchilov and Hutter, 2017]. Specifically, we set the initial learning rate to 104, theminimum learning rate to 1010, and the restart length to 50 epochs. We use the Adam optimizer[Kingma and Ba, 2017] for optimization. Our model is implemented using PyTorch [Paszke et al.,2019]. At this point, to compare to AstroNN we fine-tune on a real spectra dataset of the same sizebut restrict it to the first half of the wavelength range to leave the second half for further tests focusingon transfer learning on an unseen wavelength range and fine-tuning on small datasets. The specificwavelength regions used at each step of training is illustrated in . The maximum input sizeof our model is limited to 512 pixels due to computational constraints. All predictions from inputs : The [Fe/H] predictions from 512 pixels of real APOGEE spectra around two iron lines fromthe second half of APOGEE spectra (1.611 m - 1.622 m). Left: a basic neural network trainedon real APOGEE spectra in the target wavelength range with a dataset of only 100 iron-rich stars;Center-left: the SpectraFM base model, pre-trained on synthetic spectra from approximately 90kstars; Center-right: the SpectraFM base model fine-tuned on real APOGEE spectra from only thefirst half of the spectra which demonstrates increased performance even though it is not trained onreal spectra in the target wavelength range; Right: SpectraFM fine-tuned again on real APOGEEspectra in the target wavelength range with a dataset of only 100 iron-rich stars. Only SpectraFM,pre-trained on synthetic spectra and fine-tuned on 100 iron-rich real spectra, is able to generalize toiron-poor spectra for this task. This demonstrates that a pre-trained model is a better starting pointthan training from scratch for this small dataset task.",
  "larger than 512 pixels are averages of predictions from 512 pixel chunks. For reference, the entireAPOGEE spectra contain 7514 pixels": "Small training sets limit the accuracy of a machine learning algorithm, especially if the training setdoes not encompass the necessary label distribution. To mimic this scenario, we fine-tune again on adataset limited to 100 iron-rich stars ([Fe/H] > -1) with a focus on [Fe/H] prediction. We choose thechunk of 512 pixels around two Fe lines in the second half of the spectra (1.611m-1.622m). Forcomparison, we train a fully connected neural network (FCNN) with 5 million trainable parameters,three hidden layers and dropout fraction of 10% to reduce overfitting. We suspect that any neural network trained solely on this limited dataset, e.g. convolutional neuralnetworks like AstroNN or larger FCNNs, will fail in generalizing to iron-poor stars ([Fe/H] < -1), dueto the out-of-distribution nature of the task. In this test, there is no exposure to iron-poor stars duringtraining. SpectraFM benefits from pre-training on synthetic spectra, which enables it to transferknowledge to real spectra and generalize beyond the fine-tuning training distribution. If our fine-tunedmodel accurately predicts [Fe/H] in the iron-poor range, this indicates that knowledge transferredfrom the synthetic pre-train and the fine-tuning on real spectra from a different wavelength range. Wecan compare the [Fe/H] prediction accuracy at each stage to see how knowledge is gained.",
  "Results & Discussion": "The prediction accuracy of our fine-tuned model on the first half of real spectra as seen in and is similar to that of previous machine learning methods like AstroNN [Leung and Bovy,2018]. Training only on the synthetic spectra is insufficient to handle real spectra due to the syntheticgap (middle column of ), while fine-tuning allows the model to make accurate predictions.We make a selection of 1.0 < log g < 3.5 on our test set to match the AstroNN training sample for afair comparison.",
  "Transfer learning from synthetic to real data": "To investigate the extent to which the pre-training and the fine-tuning steps help the model generalizeto previously unseen data regimes, we compare the results from our pre-trained and fine-tuned modelsto those from the basic neural network trained from scratch on the 100 iron-rich stars in . Thefrom-scratch neural network fails to predict [Fe/H] in the iron-poor range and SpectraFM pre-trained",
  "Teff10 K183 K19 Klog g0.037 dex0.326 dex0.056 dex[O/Fe]0.020 dex0.038 dex0.021 dex[Mg/Fe]0.015 dex0.034 dex0.019 dex[Fe/H]0.011 dex0.048 dex0.019 dex": ": Comparison of prediction scatter for a selection of the test set between AstroNN [Leung andBovy, 2018] and our foundation model after fine-tuning on real spectra. The scatter is the medianabsolute deviation: median (|ytrue,i ypred,i|). Our foundation model is only fine-tuned on the firsthalf of the APOGEE spectra while AstroNN was trained on the full spectra. Furthermore, AstroNNlikely included some of these stars in its training set. These stars satisfy 1.0 < log g < 3.5 for afair comparison since AstroNN only trained on stars in this log g range. The AstroNN [O/Fe] and[Mg/Fe] are found from [X/Fe] = [X/H] - [Fe/H] since AstroNN does not directly predict [X/Fe]. on only synthetic spectra also performs poorly due to the simulations that generated the syntheticspectra not accurately modeling every physical process behind the real spectra (like in the middlecolumn of ). We observe an increase in [Fe/H] accuracy from RMSE = 0.254 to 0.210 byfine-tuning on real spectra in an entirely different wavelength range, which already outperforms theneural network trained from scratch in the iron-poor range (RMSE,[Fe/H]<1.0 = 0.763 vs. 0.510).Fine-tuning again, but only using observed spectra of 100 iron-rich stars, which are the only real,observed spectra used for training in this wavelength region, leads to strong performance even in theiron-poor region (RMSE,[Fe/H]<1.0 = 0.232) and is the best overall estimator (RMSE = 0.094).Skipping the first fine-tuning step and directly fine-tuning on the 100 iron-rich stars leads to similarperformance: RMSE,[Fe/H]<1.0 = 0.256 and RMSE = 0.100. Therefore, although fine-tuning ona different wavelength range with real spectra increased performance on this task compared to thebase model, the knowledge required to achieve a high accuracy came from the 100 star fine-tune inthe same wavelength range. Even though the model never sees iron-poor real spectra in the targetwavelength range during training, it can make accurate predictions in this region, which demonstratesa new ability unlocked by generalizing knowledge from other tasks. We found that freezing the parameters in the layers closer to the output of the last fine-tuned model,i.e. the Decoder and last layer of Encoder, led to stronger performance in the iron-poor region. Thissuggests that the layers closer to the output retain information about translating high-level spectralfeatures to iron abundance from the synthetic pre-training. Meanwhile, the layers closer to the inputrequire adjustment to recognize features in real spectra. This approach allows the model to leveragethe pre-trained knowledge effectively without overfitting to the small real dataset. The ability to accurately predict [Fe/H] in the iron-poor range suggests that the synthetic-to-realknowledge transfer is successful and highlights the advantage of starting with a pre-trained foundationmodel, which requires only minimal adjustments to perform well across diverse conditions, such asnew telescopes like JWST.",
  "Attention": "A major concern in using machine learning for scientific research arises from the limited understandingof how a prediction is made and where the model sources the information to make that prediction.The attention mechanism behind Transformers can help us understand what information the modelhas learned to use [Vaswani et al., 2017]. We look at the relative values of attention scores that theEncoder transformer block of our base model assigns to each input pixel when making predictions forTeff, log g, [Fe/H], [O/Fe], [Mg/Fe] (see Appendix A for more info on attention scores). We averagethe scores for giants and dwarfs, selected based on specific gravity (log g < 3.0 for giants and log g >4.0 for dwarfs). The results of this can be seen in . Stellar spectra contain many narrow absorption lines with physically meaningful information, sowe invert the attention to easily compare the attention scores to the spectra and determine whetherregions of high attention correspond to spectral features. The shape and depth of spectral lines containinformation about the abundance of certain chemical elements along with the overall properties ofthe star, like Teff and log g. The source of our simulated spectra, ASPCAP, identifies windows wherethe synthetic spectra are uniquely sensitive to the abundance of a given element [Jnsson et al., 2020].We find that many of the wavelengths the Transformer attends to hold physical significance. : Inverted attention maps for synthetic spectra from dwarf (top) and giant (bottom) stars asanalyzed by SpectraFM. Each row corresponds to a different stellar property: effective temperature(Teff), surface gravity (log g), oxygen-to-iron ratio ([O/Fe]), magnesium-to-iron ratio ([Mg/Fe]),and iron abundance ([Fe/H]). The attention scores, normalized and averaged across stars, reveal thespecific wavelength regions the model focuses on for each prediction. Spectral lines and wavelengthsthat contain information about certain elements are marked with dashed lines as determined byASPCAP [Prez et al., 2016]. The corresponding average spectrum for each stellar type is alsoplotted, to show how the attention aligns with physically meaningful spectral features. Br11 is aHydrogen Brackett line [Campbell et al., 2022] known to be sensitive to log g. The attention mapshave been vertically shifted for clarity. For further details on how attention scores are calculated andaveraged, see Appendix A. In particular, the attention for every property strongly focuses on the Br11 hydrogen line associatedwith a Brackett transition around 16813 [Campbell et al., 2022] known to be highly sensitive tothe surface gravity log g. Because determining the abundance of a chemical element from a stellarabsorption line requires knowledge of the overall properties Teff and log g, we expect the Br11hydrogen line to be important for determining all elements and this is exactly what we see in .The [Fe/H] prediction looks at lines associated with iron at 16530 and 16386, with the lattercatching the attention of all other property predictions as well. Teff pays attention to O lines at16660 and 16910. The [Mg/Fe] and [O/Fe] attentions look similar, which is expected since Mg,O, Si, and Ca are all grouped as alpha elements, which form through the fusion of helium nucleiand are dispersed by core-collapse supernovae and so their abundances are usually correlated. Eachof [Mg/Fe] and [O/Fe] pay strong attention to the Mg line at 15570, Si line at 15893, O line at16372 and the Fe line at 16386, which is necessary to determine their abundance relative to Fe. The attention mechanism effectively identifies and focuses on spectroscopically significant regionsthat correspond to chemical element transitions, demonstrating the models ability to learn physicallymeaningful features for accurate predictions. Previous machine learning methods like AstroNNemployed masking techniques to mitigate the risk of the model learning spurious correlations between different elements, which could introduce bias and compromise prediction accuracy. Byselectively masking specific regions during training, AstroNN focused on the relevant features forpredicting a given element. A Transformer-based spectra foundation model has the advantage ofa variable-length input and the ability to investigate attention. So future research could train themodel to predict elements only based on their relevant features in the spectra, and then investigate theattention to ensure it is not basing its predictions off of correlations with other features that might notuniversally hold true. Furthermore, examining the models attention when predicting a property mayunveil hidden relationships not previously recognized, offering a new method for discovery.",
  "Conclusion": "This work presents a Transformer-based foundation model for stellar spectroscopy. The model,pre-trained on synthetic spectra and fine-tuned with limited real data, outperforms traditional neuralnetworks by bringing out-of-distribution tasks inside the distribution with pre-training. Its attentionmechanism targets key physical features in the spectra, ensuring predictions are physically grounded. Our results show that for new tasks in astrophysics, fine-tuning a foundation model will likely lead tobetter results than a basic neural network. For example, if a James Webb Space Telescope data releasecontained a limited number of stars with measured abundances, our results suggests that fine-tuningour foundation model on this training set would lead to a highly accurate model that could then beused to get abundances for other stars. Our model could be fine-tuned to predict other propertiesas well, for example stellar ages, mass, and spectra-photometric distances [Leung and Bovy, 2019,Leung et al., 2023b]. Our understanding of the Galaxys evolutionary history, like the formation ofthe bar, disk, and stellar halo, along with our understanding of globular clusters and dwarf galaxies,rely heavily on measuring these properties to high accuracy [e.g., Leung et al., 2023a]. Future directions include integrating diverse datasets to enhance cross-instrument and cross-domaingeneralization. We plan to exploit our models flexibility by pre-training on all major stellar spectro-scopic surveys such as LAMOST DR9 (10 million stars, 370-900 nm) [Liu et al., 2020], GALAH DR3(588k stars, optical and infrared bands) [Buder et al., 2021], and Gaia DR3 low-resolution spectra(220 million stars, 330-1050 nm) [De Angeli et al., 2022], each with differing resolutions. A widervariety of training data should increase performance on all tasks due to knowledge generalization. Few-shot learning is also an area of interest, especially for its applications for analyzing rare stellartypes and datasets from new instruments that are too small for fine-tuning. Moving to a decoder-onlyapproach with positional encoding would enable such ability.",
  "has been provided by national institutions, in particular the institutions participating in the GaiaMultilateral Agreement": "Spencer Bialek, Sbastien Fabbro, Kim A Venn, Nripesh Kumar, Teaghan OBriain, and Kwang Moo Yi.Assessing the performance of LTE and NLTE synthetic stellar spectra in a machine learning framework.Monthly Notices of the Royal Astronomical Society, 498(3):38173834, October 2020. ISSN 0035-8711. doi:10.1093/mnras/staa2582. URL Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S.Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, Shyamal Buch, DallasCard, Rodrigo Castellon, Niladri Chatterji, Annie Chen, Kathleen Creel, Jared Quincy Davis, Dora Demszky,Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano Ermon, John Etchemendy, Kawin Ethayarajh,Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren Gillespie, Karan Goel, Noah Goodman, Shelby Grossman, NeelGuha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang,Thomas Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, FereshteKhani, Omar Khattab, Pang Wei Koh, Mark Krass, Ranjay Krishna, Rohith Kuditipudi, Ananya Kumar,Faisal Ladhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle Levent, Xiang Lisa Li, Xuechen Li, TengyuMa, Ali Malik, Christopher D. Manning, Suvir Mirchandani, Eric Mitchell, Zanele Munyikwa, Suraj Nair,Avanika Narayan, Deepak Narayanan, Ben Newman, Allen Nie, Juan Carlos Niebles, Hamed Nilforoshan,Julian Nyarko, Giray Ogut, Laurel Orr, Isabel Papadimitriou, Joon Sung Park, Chris Piech, Eva Portelance,Christopher Potts, Aditi Raghunathan, Rob Reich, Hongyu Ren, Frieda Rong, Yusuf Roohani, CamiloRuiz, Jack Ryan, Christopher R, Dorsa Sadigh, Shiori Sagawa, Keshav Santhanam, Andy Shih, KrishnanSrinivasan, Alex Tamkin, Rohan Taori, Armin W. Thomas, Florian Tramr, Rose E. Wang, William Wang,Bohan Wu, Jiajun Wu, Yuhuai Wu, Sang Michael Xie, Michihiro Yasunaga, Jiaxuan You, Matei Zaharia,Michael Zhang, Tianyi Zhang, Xikun Zhang, Yuhui Zhang, Lucia Zheng, Kaitlyn Zhou, and Percy Liang. Onthe Opportunities and Risks of Foundation Models, July 2022. URL [cs]. Sven Buder, Sanjib Sharma, Janez Kos, Anish M. Amarsi, Thomas Nordlander, Karin Lind, Sarah L. Martell,Martin Asplund, Joss Bland-Hawthorn, Andrew R. Casey, Gayandhi M. de Silva, Valentina DOrazi, Ken C.Freeman, Michael R. Hayden, Geraint F. Lewis, Jane Lin, Katharine J. Schlesinger, Jeffrey D. Simpson,Dennis Stello, Daniel B. Zucker, Toma Zwitter, Kevin L. Beeson, Tobias Buck, Luca Casagrande, Jake T.Clark, Klemen Cotar, Gary S. da Costa, Richard de Grijs, Diane Feuillet, Jonathan Horner, Prajwal R.Kafle, Shourya Khanna, Chiaki Kobayashi, Fan Liu, Benjamin T. Montet, Govind Nandakumar, David M.Nataf, Melissa K. Ness, Lorenzo Spina, Thor Tepper-Garca, Yuan-Sen Ting, Gregor Traven, Rok Vogrincic,Robert A. Wittenmyer, Rosemary F. G. Wyse, Marua erjal, and Galah Collaboration. The GALAH+ survey:Third data release. Monthly Notices of the Royal Astronomical Society, 506:150201, September 2021. ISSN0035-8711. doi: 10.1093/mnras/stab1242. URL Publisher: OUP ADS Bibcode: 2021MNRAS.506..150B. Hunter Campbell, Elliott Khilfeh, Kevin R. Covey, Marina Kounkel, Richard Ballantyne, Sabrina Corey,Carlos G. Romn-Ziga, Jess Hernndez, Ezequiel Manzo Martnez, Karla Pea Ramrez, AlexandreRoman-Lopes, Keivan G. Stassun, Guy S. Stringfellow, Jura Borissova, S. Drew Chojnowski, Valeria Ramrez-Preciado, Jinyoung Serena Kim, Javier Serna, Amelia M. Stutz, Ricardo Lpez-Valdivia, Genaro Surez,Jason E. Ybarra, Penlope Longa-Pea, and Jos G. Fernndez-Trincado. Pre-main-sequence BrackettEmitters in the APOGEE DR17 Catalog: Line Strengths and Physical Properties of Accretion Columns. TheAstrophysical Journal, 942(1):22, December 2022. ISSN 0004-637X. doi: 10.3847/1538-4357/aca324. URL Publisher: The American Astronomical Society. Tristan Cantat-Gaudin, Morgan Fouesneau, Hans-Walter Rix, Anthony G. A. Brown, Ronald Drimmel, AlfredCastro-Ginard, Shourya Khanna, Vasily Belokurov, and Andrew R. Casey. Uniting Gaia and APOGEE tounveil the cosmic chemistry of the Milky Way disc. Astronomy & Astrophysics, 683:A128, March 2024. ISSN0004-6361, 1432-0746. doi: 10.1051/0004-6361/202348018. URL Publisher: EDP Sciences. F. De Angeli, M. Weiler, P. Montegriffo, D. W. Evans, M. Riello, R. Andrae, J. M. Carrasco, G. Busso, P. W.Burgess, C. Cacciari, M. Davidson, D. L. Harrison, S. T. Hodgkin, C. Jordi, P. J. Osborne, E. Pancino,G. Altavilla, M. A. Barstow, C. a. L. Bailer-Jones, M. Bellazzini, A. G. A. Brown, M. Castellani, S. Cowell,L. Delchambre, F. De Luise, C. Diener, C. Fabricius, M. Fouesneau, Y. Fremat, G. Gilmore, G. Giuffrida,N. C. Hambly, S. Hidalgo, G. Holland, Z. Kostrzewa-Rutkowska, F. van Leeuwen, A. Lobel, S. Marinoni,N. Miller, C. Pagani, L. Palaversa, A. M. Piersimoni, L. Pulone, S. Ragaini, M. Rainer, P. J. Richards,G. T. Rixon, D. Ruz-Mieres, N. Sanna, L. M. Sarro, N. Rowell, R. Sordo, N. A. Walton, and A. Yoldas.Gaia Data Release 3: Processing and validation of BP/RP low-resolution spectral data, June 2022. URL S. Fabbro, K. A. Venn, T. OBriain, S. Bialek, C. L. Kielty, F. Jahandar, and S. Monty. An application of deeplearning in the analysis of stellar spectra. Monthly Notices of the Royal Astronomical Society, 475:29782993,April 2018. ISSN 0035-8711. doi: 10.1093/mnras/stx3298. URL Publisher: OUP ADS Bibcode: 2018MNRAS.475.2978F. Gaia Collaboration, A. Vallenari, A. G. A. Brown, T. Prusti, J. H. J. de Bruijne, F. Arenou, C. Babusiaux,M. Biermann, O. L. Creevey, C. Ducourant, D. W. Evans, L. Eyer, R. Guerra, A. Hutton, C. Jordi, S. A.Klioner, U. L. Lammers, L. Lindegren, X. Luri, F. Mignard, C. Panem, D. Pourbaix, S. Randich, P. Sar-toretti, C. Soubiran, P. Tanga, N. A. Walton, C. A. L. Bailer-Jones, U. Bastian, R. Drimmel, F. Jansen,D. Katz, M. G. Lattanzi, F. van Leeuwen, J. Bakker, C. Cacciari, J. Castaeda, F. De Angeli, C. Fabricius,M. Fouesneau, Y. Frmat, L. Galluccio, A. Guerrier, U. Heiter, E. Masana, R. Messineo, N. Mowlavi,C. Nicolas, K. Nienartowicz, F. Pailler, P. Panuzzo, F. Riclet, W. Roux, G. M. Seabroke, R. Sordo, F. Thvenin,G. Gracia-Abril, J. Portell, D. Teyssier, M. Altmann, R. Andrae, M. Audard, I. Bellas-Velidis, K. Benson,J. Berthier, R. Blomme, P. W. Burgess, D. Busonero, G. Busso, H. Cnovas, B. Carry, A. Cellino, N. Cheek,G. Clementini, Y. Damerdji, M. Davidson, P. de Teodoro, M. Nuez Campos, L. Delchambre, A. DellOro,P. Esquej, J. Fernndez-Hernndez, E. Fraile, D. Garabato, P. Garca-Lario, E. Gosset, R. Haigron, J. L.Halbwachs, N. C. Hambly, D. L. Harrison, J. Hernndez, D. Hestroffer, S. T. Hodgkin, B. Holl, K. Janen,G. Jevardat de Fombelle, S. Jordan, A. Krone-Martins, A. C. Lanzafame, W. Lffler, O. Marchal, P. M.Marrese, A. Moitinho, K. Muinonen, P. Osborne, E. Pancino, T. Pauwels, A. Recio-Blanco, C. Reyl,M. Riello, L. Rimoldini, T. Roegiers, J. Rybizki, L. M. Sarro, C. Siopis, M. Smith, A. Sozzetti, E. Utrilla,M. van Leeuwen, U. Abbas, P. brahm, A. Abreu Aramburu, C. Aerts, J. J. Aguado, M. Ajaj, F. Aldea-Montero, G. Altavilla, M. A. lvarez, J. Alves, F. Anders, R. I. Anderson, E. Anglada Varela, T. Antoja,D. Baines, S. G. Baker, L. Balaguer-Nez, E. Balbinot, Z. Balog, C. Barache, D. Barbato, M. Barros, M. A.Barstow, S. Bartolom, J. L. Bassilana, N. Bauchet, U. Becciani, M. Bellazzini, A. Berihuete, M. Bernet,S. Bertone, L. Bianchi, A. Binnenfeld, S. Blanco-Cuaresma, A. Blazere, T. Boch, A. Bombrun, D. Bossini,S. Bouquillon, A. Bragaglia, L. Bramante, E. Breedt, A. Bressan, N. Brouillet, E. Brugaletta, B. Bucciarelli,A. Burlacu, A. G. Butkevich, R. Buzzi, E. Caffau, R. Cancelliere, T. Cantat-Gaudin, R. Carballo, T. Carlucci,M. I. Carnerero, J. M. Carrasco, L. Casamiquela, M. Castellani, A. Castro-Ginard, L. Chaoul, P. Charlot,L. Chemin, V. Chiaramida, A. Chiavassa, N. Chornay, G. Comoretto, G. Contursi, W. J. Cooper, T. Cornez,S. Cowell, F. Crifo, M. Cropper, M. Crosta, C. Crowley, C. Dafonte, A. Dapergolas, M. David, P. David,P. de Laverny, F. De Luise, R. De March, J. De Ridder, R. de Souza, A. de Torres, E. F. del Peloso, E. delPozo, M. Delbo, A. Delgado, J. B. Delisle, C. Demouchy, T. E. Dharmawardena, P. Di Matteo, S. Diakite,C. Diener, E. Distefano, C. Dolding, B. Edvardsson, H. Enke, C. Fabre, M. Fabrizio, S. Faigler, G. Fedorets,P. Fernique, A. Fienga, F. Figueras, Y. Fournier, C. Fouron, F. Fragkoudi, M. Gai, A. Garcia-Gutierrez,M. Garcia-Reinaldos, M. Garca-Torres, A. Garofalo, A. Gavel, P. Gavras, E. Gerlach, R. Geyer, P. Giacobbe,G. Gilmore, S. Girona, G. Giuffrida, R. Gomel, A. Gomez, J. Gonzlez-Nez, I. Gonzlez-Santamara, J. J.Gonzlez-Vidal, M. Granvik, P. Guillout, J. Guiraud, R. Gutirrez-Snchez, L. P. Guy, D. Hatzidimitriou,M. Hauser, M. Haywood, A. Helmer, A. Helmi, M. H. Sarmiento, S. L. Hidalgo, T. Hilger, N. Hadczuk,D. Hobbs, G. Holland, H. E. Huckle, K. Jardine, G. Jasniewicz, A. Jean-Antoine Piccolo, . Jimnez-Arranz,A. Jorissen, J. Juaristi Campillo, F. Julbe, L. Karbevska, P. Kervella, S. Khanna, M. Kontizas, G. Kordopatis,A. J. Korn, . Kspl, Z. Kostrzewa-Rutkowska, K. Kruszynska, M. Kun, P. Laizeau, S. Lambert, A. F.Lanza, Y. Lasne, J. F. Le Campion, Y. Lebreton, T. Lebzelter, S. Leccia, N. Leclerc, I. Lecoeur-Taibi, S. Liao,E. L. Licata, H. E. P. Lindstrm, T. A. Lister, E. Livanou, A. Lobel, A. Lorca, C. Loup, P. Madrero Pardo,A. Magdaleno Romeo, S. Managau, R. G. Mann, M. Manteiga, J. M. Marchant, M. Marconi, J. Marcos,M. M. S. Marcos Santos, D. Marn Pina, S. Marinoni, F. Marocco, D. J. Marshall, L. Martin Polo, J. M.Martn-Fleitas, G. Marton, N. Mary, A. Masip, D. Massari, A. Mastrobuono-Battisti, T. Mazeh, P. J. McMil-lan, S. Messina, D. Michalik, N. R. Millar, A. Mints, D. Molina, R. Molinaro, L. Molnr, G. Monari,M. Mongui, P. Montegriffo, A. Montero, R. Mor, A. Mora, R. Morbidelli, T. Morel, D. Morris, T. Muraveva,C. P. Murphy, I. Musella, Z. Nagy, L. Noval, F. Ocaa, A. Ogden, C. Ordenovic, J. O. Osinde, C. Pagani,I. Pagano, L. Palaversa, P. A. Palicio, L. Pallas-Quintela, A. Panahi, S. Payne-Wardenaar, X. Pealosa Es-teller, A. Penttil, B. Pichon, A. M. Piersimoni, F. X. Pineau, E. Plachy, G. Plum, E. Poggio, A. Pra,L. Pulone, E. Racero, S. Ragaini, M. Rainer, C. M. Raiteri, N. Rambaux, P. Ramos, M. Ramos-Lerate,P. Re Fiorentin, S. Regibo, P. J. Richards, C. Rios Diaz, V. Ripepi, A. Riva, H. W. Rix, G. Rixon, N. Robi-chon, A. C. Robin, C. Robin, M. Roelens, H. R. O. Rogues, L. Rohrbasser, M. Romero-Gmez, N. Rowell,F. Royer, D. Ruz Mieres, K. A. Rybicki, G. Sadowski, A. Sez Nez, A. Sagrist Sells, J. Sahlmann,E. Salguero, N. Samaras, V. Sanchez Gimenez, N. Sanna, R. Santovea, M. Sarasso, M. Schultheis, E. Sciacca,M. Segol, J. C. Segovia, D. Sgransan, D. Semeux, S. Shahaf, H. I. Siddiqui, A. Siebert, L. Siltala, A. Silvelo,E. Slezak, I. Slezak, R. L. Smart, O. N. Snaith, E. Solano, F. Solitro, D. Souami, J. Souchay, A. Spagna,L. Spina, F. Spoto, I. A. Steele, H. Steidelmller, C. A. Stephenson, M. Sveges, J. Surdej, L. Szabados,E. Szegedi-Elek, F. Taris, M. B. Taylor, R. Teixeira, L. Tolomei, N. Tonello, F. Torra, J. Torra, G. Tor-ralba Elipe, M. Trabucchi, A. T. Tsounis, C. Turon, A. Ulla, N. Unger, M. V. Vaillant, E. van Dillen, W. vanReeven, O. Vanel, A. Vecchiato, Y. Viala, D. Vicente, S. Voutsinas, M. Weiler, T. Wevers, . Wyrzykowski,A. Yoldas, P. Yvard, H. Zhao, J. Zorec, S. Zucker, and T. Zwitter. Gaia Data Release 3. Summary of thecontent and survey properties. Astronomy and Astrophysics, 674:A1, June 2023. ISSN 0004-6361. doi:",
  "/0004-6361/202243940. URL Bibcode: 2023A&A...674A...1G": "Henrik Jnsson, Jon A. Holtzman, Carlos Allende Prieto, Katia Cunha, D. A. Garca-Hernndez, Sten Hasselquist,Thomas Masseron, Yeisson Osorio, Matthew Shetrone, Verne Smith, Guy S. Stringfellow, Dmitry Bizyaev,Bengt Edvardsson, Steven R. Majewski, Szabolcs Mszros, Diogo Souto, Olga Zamora, Rachael L. Beaton,Jo Bovy, John Donor, Marc H. Pinsonneault, Vijith Jacob Poovelil, and Jennifer Sobeck. APOGEE Dataand Spectral Analysis from SDSS Data Release 16: Seven Years of Observations Including First Resultsfrom APOGEE-South.The Astronomical Journal, 160(3):120, August 2020.ISSN 1538-3881.doi:10.3847/1538-3881/aba592. URL arXiv:2007.05537 [astro-ph].",
  "Alexander Laroche and Joshua S. Speagle. Closing the stellar labels gap: Stellar label independent evidence for[$\\alpha$/M] information in $\\textit{Gaia}$ BP/RP spectra, April 2024. URL": "Henry W. Leung and Jo Bovy. Deep learning of multi-element abundances from high-resolution spectroscopicdata. Monthly Notices of the Royal Astronomical Society, November 2018. ISSN 0035-8711, 1365-2966. doi:10.1093/mnras/sty3217. URL arXiv:1808.04428 [astro-ph]. Henry W Leung and Jo Bovy. Simultaneous calibration of spectro-photometric distances and the Gaia DR2parallax zero-point offset with deep learning. Monthly Notices of the Royal Astronomical Society, 489(2):20792096, October 2019. ISSN 0035-8711, 1365-2966. doi: 10.1093/mnras/stz2245. URL",
  "Henry W. Leung and Jo Bovy. Towards an astronomical foundation model for stars with a Transformer-basedmodel, September 2023. URL arXiv:2308.10944 [astro-ph]": "Henry W Leung, Jo Bovy, J Ted Mackereth, Jason A S Hunt, Richard R Lane, and John C Wilson. Ameasurement of the distance to the Galactic centre using the kinematics of bar stars. Monthly Notices of theRoyal Astronomical Society, 519(1):948960, February 2023a. ISSN 0035-8711. doi: 10.1093/mnras/stac3529.URL Henry W Leung, Jo Bovy, J Ted Mackereth, and Andrea Miglio. A variational encoderdecoder approach toprecise spectroscopic age estimation for large Galactic surveys. Monthly Notices of the Royal AstronomicalSociety, 522(3):45774597, May 2023b. ISSN 0035-8711, 1365-2966. doi: 10.1093/mnras/stad1272. URL Chao Liu, Jianning Fu, Jianrong Shi, Hong Wu, Zhanwen Han, Li Chen, Subo Dong, Yongheng Zhao, Jian-JunChen, Haotong Zhang, Zhong-Rui Bai, Xuefei Chen, Wenyuan Cui, Bing Du, Chih-Hao Hsia, Deng-KaiJiang, Jinliang Hou, Wen Hou, Haining Li, Jiao Li, Lifang Li, Jiaming Liu, Jifeng Liu, A.-Li Luo, Juan-JuanRen, Hai-Jun Tian, Hao Tian, Jia-Xin Wang, Chao-Jian Wu, Ji-Wei Xie, Hong-Liang Yan, Fan Yang, JinchengYu, Bo Zhang, Huawei Zhang, Li-Yun Zhang, Wei Zhang, Gang Zhao, Jing Zhong, Weikai Zong, and FangZuo. LAMOST Medium-Resolution Spectroscopic Survey (LAMOST-MRS): Scientific goals and surveyplan, May 2020. URL arXiv:2005.07210 [astro-ph].",
  "Ilya Loshchilov and Frank Hutter. SGDR: Stochastic Gradient Descent with Warm Restarts, May 2017. URL arXiv:1608.03983 [cs, math]": "Steven R. Majewski, Ricardo P. Schiavon, Peter M. Frinchaboy, Carlos Allende Prieto, Robert Barkhouser,Dmitry Bizyaev, Basil Blank, Sophia Brunner, Adam Burton, Ricardo Carrera, S. Drew Chojnowski, KatiaCunha, Courtney Epstein, Greg Fitzgerald, Ana E. Garcia Perez, Fred R. Hearty, Chuck Henderson, Jon A.Holtzman, Jennifer A. Johnson, Charles R. Lam, James E. Lawler, Paul Maseman, Szabolcs Meszaros,Matthew Nelson, Duy Coung Nguyen, David L. Nidever, Marc Pinsonneault, Matthew Shetrone, StephenSmee, Verne V. Smith, Todd Stolberg, Michael F. Skrutskie, Eric Walker, John C. Wilson, Gail Zasowski,Friedrich Anders, Sarbani Basu, Stephane Beland, Michael R. Blanton, Jo Bovy, Joel R. Brownstein, JoleenCarlberg, William Chaplin, Cristina Chiappini, Daniel J. Eisenstein, Yvonne Elsworth, Diane Feuillet,Scott W. Fleming, Jessica Galbraith-Frew, Rafael A. Garcia, D. Anibal Garcia-Hernandez, Bruce A. Gillespie,Leo Girardi, James E. Gunn, Sten Hasselquist, Michael R. Hayden, Saskia Hekker, Inese Ivans, KarenKinemuchi, Mark Klaene, Suvrath Mahadevan, Savita Mathur, Benoit Mosser, Demitri Muna, Jeffrey A.Munn, Robert C. Nichol, Robert W. OConnell, A. C. Robin, Helio Rocha-Pinto, Matthias Schultheis, Aldo M. Serenelli, Neville Shane, Victor Silva Aguirre, Jennifer S. Sobeck, Benjamin Thompson, Nicholas W. Troup,David H. Weinberg, and Olga Zamora. The Apache Point Observatory Galactic Evolution Experiment(APOGEE). The Astronomical Journal, 154(3):94, September 2017. ISSN 0004-6256, 1538-3881. doi:10.3847/1538-3881/aa784d. URL arXiv:1509.05420 [astro-ph]. Michael McCabe, Bruno Rgaldo-Saint Blancard, Liam Holden Parker, Ruben Ohana, Miles Cranmer, AlbertoBietti, Michael Eickenberg, Siavash Golkar, Geraud Krawezik, Francois Lanusse, Mariel Pettee, TiberiuTesileanu, Kyunghyun Cho, and Shirley Ho. Multiple Physics Pretraining for Physical Surrogate Models,October 2023. URL arXiv:2310.02994 [cs, stat]. Teaghan OBriain, Yuan-Sen Ting, Sbastien Fabbro, Kwang M. Yi, Kim Venn, and Spencer Bialek. Cycle-StarNet: Bridging the Gap between Theory and Data by Leveraging Large Data Sets. The AstrophysicalJournal, 906(2):130, January 2021. ISSN 0004-637X. doi: 10.3847/1538-4357/abca96. URL Publisher: The American Astronomical Society. Liam Parker, Francois Lanusse, Siavash Golkar, Leopoldo Sarra, Miles Cranmer, Alberto Bietti, MichaelEickenberg, Geraud Krawezik, Michael McCabe, Ruben Ohana, Mariel Pettee, Bruno Regaldo-Saint Blancard,Tiberiu Tesileanu, Kyunghyun Cho, and Shirley Ho. AstroCLIP: A Cross-Modal Foundation Model forGalaxies. Monthly Notices of the Royal Astronomical Society, 531(4):49905011, June 2024. ISSN 0035-8711,1365-2966. doi: 10.1093/mnras/stae1450. URL arXiv:2310.03024[astro-ph]. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kpf, Edward Yang, Zach DeVito,Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and SoumithChintala. PyTorch: An Imperative Style, High-Performance Deep Learning Library, December 2019. URL arXiv:1912.01703 [cs, stat]. Ana E. Garca Prez, Carlos Allende Prieto, Jon A. Holtzman, Matthew Shetrone, Szabolcs Mszros, DmitryBizyaev, Ricardo Carrera, Katia Cunha, D. A. Garca-Hernndez, Jennifer A. Johnson, Steven R. Majewski,David L. Nidever, Ricardo P. Schiavon, Neville Shane, Verne V. Smith, Jennifer Sobeck, Nicholas Troup,Olga Zamora, Jo Bovy, Daniel J. Eisenstein, Diane Feuillet, Peter M. Frinchaboy, Michael R. Hayden, Fred R.Hearty, Duy C. Nguyen, Robert W. OConnell, Marc H. Pinsonneault, David H. Weinberg, John C. Wilson,and Gail Zasowski. ASPCAP: The Apogee Stellar Parameter and Chemical Abundances Pipeline. TheAstronomical Journal, 151(6):144, May 2016. ISSN 1538-3881. doi: 10.3847/0004-6256/151/6/144. URL arXiv:1510.07635 [astro-ph].",
  "Maithra Raghu, Thomas Unterthiner, Simon Kornblith, Chiyuan Zhang, and Alexey Dosovitskiy. Do VisionTransformers See Like Convolutional Neural Networks?, March 2022. URL arXiv:2108.08810 [cs, stat]": "Tomasz Rzanski, Yuan-Sen Ting, and Maja Jabonska. Toward a Spectral Foundation Model: An Attention-Based Approach with Domain-Inspired Fine-Tuning and Wavelength Parameterization, June 2023. URL arXiv:2306.15703 [astro-ph]. Michael J. Smith and James E. Geach. Astronomia ex machina: a history, primer and outlook on neural networksin astronomy. Royal Society Open Science, 10:221454, May 2023. doi: 10.1098/rsos.221454. URL ADS Bibcode: 2023RSOS...1021454S. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,and Illia Polosukhin. Attention Is All You Need, June 2017. URL Publication Title: arXiv e-prints ADS Bibcode: 2017arXiv170603762V. Mike Walmsley, Micah Bowles, Anna M. M. Scaife, Jason Shingirai Makechemu, Alexander J. Gordon, AnnetteM. N. Ferguson, Robert G. Mann, James Pearson, Jrgen J. Popp, Jo Bovy, Josh Speagle, Hugh Dickinson,Lucy Fortson, Tobias Gron, Sandor Kruk, Chris J. Lintott, Kameswara Mantha, Devina Mohan, DavidORyan, and Inigo V. Slijepevic. Scaling Laws for Galaxy Images, April 2024. URL arXiv:2404.02973 [astro-ph]. Gemma Zhang, Thomas Helfer, Alexander T. Gagliano, Siddharth Mishra-Sharma, and V. Ashley Villar.Maven: A Multimodal Foundation Model for Supernova Science, August 2024a.URL Publication Title: arXiv e-prints ADS Bibcode:2024arXiv240816829Z. Mengmeng Zhang, Fan Wu, Yude Bu, Shanshan Li, Zhenping Yi, Meng Liu, and Xiaoming Kong. SPT: Spectraltransformer for age and mass estimations of red giant stars. Astronomy and Astrophysics, 683:A163, March2024b. ISSN 0004-6361. doi: 10.1051/0004-6361/202347994. URL ADS Bibcode: 2024A&A...683A.163Z.",
  "k expQiKkdk": "These normalized scores determine the relative importance of each key-value pair. We presentattention scores from the second layer of the encoder, as these higher layers capture more complexpatterns rather than simple structures [Raghu et al., 2022]. This is analogous to convolutional neuralnetworks, where deeper layers detect meaningful features (e.g. the shape of spectral lines) whilelower layers identify simpler structures (e.g., edges or peaks). Our attention analysis in .2aims to determine if the model identifies physically relevant wavelengths and the information storedin spectral lines at these wavelengths. The attention scores are averaged across all attention headsand across all stars within each category (dwarfs or giants) to smooth out individual variations andemphasize the key regions that the model consistently focuses on.",
  "BData Refinement for APOGEE DR17 Spectra": "To prepare a high-quality training dataset, we applied a series of selections to the APOGEE DR17spectroscopic dataset [Majewski et al., 2017]. First, we selected stars with a signal-to-noise ratioabove 200, ensuring reliable measurements. Next, we removed stars flagged for quality issues andobservational problems. We then eliminated binaries by filtering stars with high radial velocity scatter(vscatter < 1km/s) since this often indicates the source is actually a binary star. For stars with multipleobservations, we deduplicated the data by selecting the highest-SNR observation for each star. Thesesteps reduced the dataset from 733,901 spectra to 128,762. Our synthetic spectra dataset is sourcedfrom the best-fit synthetic spectra for each of these stars found by ASPCAP [Prez et al., 2016]. Wedivided the refined set into 70% for training, 20% for testing, and 10% for training validation. Starswith NaN labels remained in the dataset, as our loss function and Transformer model handles missingdata effectively during training."
}