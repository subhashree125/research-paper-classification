{
  "Abstract": "Graph neural networks (GNNs) provide state-of-the-art results in a wide variety oftasks which typically involve predicting features at the vertices of a graph. Theyare built from layers of graph convolutions which serve as a powerful inductivebias for describing the flow of information among the vertices. Often, more thanone data modality is available. This work considers a setting in which severalgraphs have the same vertex set and a common vertex-level learning task. Thisgeneralizes standard GNN models to GNNs with several graph operators that donot commute. We may call this model graph-tuple neural networks (GtNN).In this work, we develop the mathematical theory to address the stability and trans-ferability of GtNNs using properties of non-commuting non-expansive operators.We develop a limit theory of graphon-tuple neural networks and use it to provea universal transferability theorem that guarantees that all graph-tuple neural net-works are transferable on convergent graph-tuple sequences. In particular, thereis no non-transferable energy under the convergence we consider here. Our theo-retical results extend well-known transferability theorems for GNNs to the case ofseveral simultaneous graphs (GtNNs) and provide a strict improvement on what iscurrently known even in the GNN case.We illustrate our theoretical results with simple experiments on synthetic and real-world data. To this end, we derive a training procedure that provably enforces thestability of the resulting model.",
  "Introduction": "Graph neural networks (GNNs) are a widely-used and versatile machine learning toolto process different kinds of data from numerous applications, including chemistry , moleculargeometry , combinatorial optimization , among many other. Such networks act onfunctions on the vertices of a graph (also called signals or vertex features) and use the structure of thegraph as a powerful inductive bias to describe the natural flow of information among vertices. Oneof the most common graph neural networks are based on graph convolutions , which generalizethe notion of message passing. The typical architecture has building blocks which are polynomialfunctions of the adjacency matrix (or more generally of the shift operator) of a graph composed with",
  "arXiv:2411.04265v1 [stat.ML] 6 Nov 2024": "componentwise non-linearities. Therefore, such networks implement the idea that the values of afunction at a vertex are related with the values at the immediate neighbors of the vertex and alsowith the values at the neighbors of its neighbors, etc. Due to the significant practical success and diversity of applications of GNNs, there is a growinginterest in understanding their mathematical properties. Researchers have delved into various theo-retical aspects of MPNNs, including, for instance, expressivity , oversmoothing ,multi-scale properties , and model relaxations . One of the fundamental properties ofgraph neural networks is their remarkable transferability property, which intuitively refers to theirability to perform well in large networks when trained in smaller networks, thus transfering knowl-edge from one to the other. This is in part possible because the number of parameters that defines aGNN is independent of the size of the input graphs. The idea is conceptually related to the algebraicnotion of representation stability that has been recently studied in the context of machine learningmodels . More precisely, if two graphs describe similar phenomena, then a given GNN shouldhave similar repercussions (i.e. similar effect on similar signals) on both graphs. In order to describethis property precisely, it is necessary to place signals and shift operators on different graphs (of po-tentially different sizes) in an equal footing to allow for meaningful comparisons and to characterizefamilies of graphs describing similar phenomena. The seminal work has used the theory ofgraphons to carry out these two steps, providing a solid theoretical foundation to the transferabilityproperties of GNNs. The theory was further developed in , and was extended toother models in . The transferability theory is very related to the stability or perturbationtheory of GNNs that studies how GNN outputs change under small perturbations of the graph inputor graph signal , and conceptually related to the theory of generalization for GNNs though the techniques are different. In many practical situations a fixed collection of entities serves as common vertices to several dis-tinct graphs simultaneously that represent several modalities of the same underlying object. Thisoccurs, for instance, in recommendation systems where the items can be considered as vertices ofseveral distinct similarity graphs. It occurs in the analysis of social networks because individualsoften participate in several distinct social/information networks simultaneously and in a wide arrayof multimodal settings. The goal of this paper is to extend the mathematical theory of GNNs to account for multimodalgraph settings. The most closely related existing work is the algebraic neural network theory ofParada-Mayorga, Butler and Ribeiro who pioneer the use of algebras of non-commutingoperators. The setting in this paper could be thought of as a special case of this theory. However,there is a crucial difference: whereas the main results in the articles above refer to the Hilbert-Schmidt norm, we define and analyze block-operator-norms on non-commutative algebras actingon function spaces. This choice allows us to prove stronger stability and transferability bounds thatwhen restricted to classical GNNs improve upon or complement the state-of-the-art theory. In par-ticular, we complement work in by delivering bounds that do not exhibit no-transferable energy,and we complement results in by providing stability bounds that do not require convergence.Our bounds are furthermore easily computable in terms of the networks parameters improving onthe results of and in particular allow us to devise novel training algorithms with stability guar-antees. Our contributions. The main contribution of this work is a theoretical analysis for graph neuralnetworks in the multimodal framework where each graph object (or graph tuple) can have sev-eral adjacency matrices on a fixed set of vertices. We call this model graph-tuple neural networks(GtNNs). It generalizes GNNs and is naturally suited for taking into account information flows alongpaths traversing several distinct graphs. This architecture replaces the polynomials h(X) underly-ing graph convolutional neural networks with non-commutative polynomials h(X1, . . . , Xk) on theadjacency matrices of the k graphs in our tuple. More generally our approach via operator networksgives a general and widely applicable parametrization for such networks. Our approach is motivatedby the theory of switched dynamical systems, where recent algorithmic tools have improved our un-derstanding of the iterative behaviour of non-commuting operators . Our main results are tightstability bounds for GtNNs and GNNs. The second contribution of this article is the definition of graphon-tuple neural networks (WtNNs)which are the natural limits of (GtNNs) as the number of vertices grows to infinity. Graphon-tupleneural networks provide a good setting for understanding the phenomenon of transferability. Our main theoretical result is a Universal transferability Theorem for graphon-graph transference whichguarantees that every graphon-tuple neural network (without any assumptions) is transferable oversequences of graph-tuples generated from a given graphon-tuple. This means that whatever a GtNNlearns on a graph-tuple with sufficiently many vertices, instantaneously transfers with small error toall other graph-tuples of sufficiently large size provided the graph-tuples we are considering describea similar phenomenon in the sense that they have a common graphon-tuple limit. Contrary to someprior results, under the convergence we consider in this paper, there is no no-transferable energy,meaning that the graphon-graph transferability error goes to zero as the size of the graph goes toinfinity. We show with simple numerical experiments that our theoretical bounds seem tight. In we provide experiments on synthetic datasets and a real-world movie recommendation datasetwhere two graphs are extracted from incomplete tabular data. The stability bounds we obtain arewithin a small factor of the empirical stability errors. And remarkably, the bounds exhibit the samequalitative behavior as the empirical stability error. In order to perform this experiment we introducea stable training procedure where linear constraints are imposed during GNN training. The stabletraining procedure could be considered of independent interest (see, for instance, ).",
  "Preliminary definitions": "For an integer n we let [n] := {1, 2, . . . , n}. By a graph G on a set V we mean an undirected, finitegraph without self-loops with vertex set V (G) := V and edge set denoted E(G). A shift matrix forG is any |V | |V | symmetric matrix S with entries 0 Sij 1 satisfying Sij = 0 whenever i = jand (i, j) E(G). Our main object of study will be signals (i.e. functions) on the common vertices V of a set of graphsso we introduce notation for describing them. We denote the algebra of real-valued functions on thevertex set V by R[V ]. Any function f : V R is completely determined by its vector of values so,as a vector space, R[V ] = R|V | however, as we will see later, thinking of this space as consistingof functions is key for understanding the neural networks we consider. Any shift matrix S for Gdefines a shift operator TG : R[V ] R[V ] by the formula TG(f)(i) =",
  "jV Sijf(j)": "The layers of graph neural networks (GNNs) are built from univariate polynomials h(x) evaluatedon the shift operator TG of a graph composed with componentwise non-linearities. If we have ak-tuple of graphs G1, . . . , Gk with common vertex set V then it is natural to consider multivari-ate polynomials evaluated at their shift operators TGi. Because shift operators of distinct graphsgenerally do not commute this forces us to design an architecture which is parametrized by noncom-mutative polynomials. The trainable parameters of such networks will be the coefficients of thesepolynomials. Noncommutative polynomials. For a positive integer k, let RX1, . . . , Xk be the algebra of non-commutative polynomials in the variables X1, . . . , Xk. This is the vector space having as basisall finite length words on the alphabet X1, . . . , Xk endowed with the bilinear product defined byconcatenation on the basis elements. For example in RX1, X2 we have (X1 + X2)2 = X21 +X1X2 + X2X1 + X21 = X21 + 2X1X2 + X22. The basis elements appearing with nonzero coefficient in the unique expression of any elementh(X1, . . . , Xk) are called the monomial words of h. The degree of a monomial word is its length (i.e.number of letters). For example there are eight monomials of degree three in RX1, X2, namely:X31, X21X2, X1X2X1, X2X21, X22X1, X2X1X2, X1X22, X32. More generally there are exactly kd",
  "k1monomial words of degree at most d in RX1, . . . , Xk": "Noncommutative polynomials have a fundamental structural relationship with linear operatorswhich makes them suitable for transference. If W is any vector space let End(W) denote thespace of linear maps from W to itself.If T1, . . . , Tk End(W) are any set of linear mapson W then the individual evaluations Xi Ti extend to a unique evaluation homomorphismRX1, . . . , Xk End(W), which sends the product of polynomials to the composition oflinear maps.This relationship (known as universal freeness property) determines the algebraRX1, . . . , Xk uniquely. This proves that noncommutative polynomials are the only naturallytransferable parametrization for our networks. For a polynomial h we denote the linear map ob-tained from evaluation as h(T1, . . . , Tk). Operator filters and non-commuting operator neural networks. Using noncommutative polyno-mials we will define operator networks, an abstraction of both graph and graphon neural networks.Operator networks will provide us with a uniform generalization to graph-tuple and graphon-tupleneural networks and allow us to describe transferability precisely. The domain and range of our operators will be powers of a fixed vector space F of signals. Moreformally, F consists of real-valued functions on a fixed domain V endowed with a measure V . Themeasure turns F into an inner product space (see [25, Chapter 2] for background) via the formulaf, g := V fgdV and in particular gives it a natural norm f := (f, f)12 which we will usethroughout the article. In later sections the set F will be either R[V ] or the space L := L2() ofsquare integrable functions in but operator networks apply much more generally, for instanceto the spaces of functions on a manifold V used in geometric deep learning . By an operatork-tuple on F we mean a sequence T := (T1, . . . , Tk) of linear operators Tj : F F. The tuple isnonexpansive if each operator Tj has norm bounded above by one.",
  "cX(T1, . . . Tk)(f)": "where X(T1, . . . , Tk) is the composition of the Ti from left to right in the order of the word . Forinstance if h(X1, X2) := 5X1X2X1 + 3X21X2 then the graph-tuple filter defined by h applied toa signal f F is (h, T1, . . . , Tk)(f) = 5T1(T2(T1(f))) + 3T 21 (T2(f)). More generally, we would like to be able to manipulate several features simultaneously (i.e. tomanipulate vector-valued signals) and do so by building block-linear maps of operators with blocksdefined by polynomials. More precisely, if A, B are positive integers and H is a B A matrixwhose entries are non-commutative polynomials hb,a RX1, . . . , Xk we define the operatorfilter determined by H and the operator tuple T to be the linear map (H, T) : FA FB whichsends a vector x = (xa)a[A] to a vector (zb)b[B] using the formula",
  "An operator neural layer with ReLU activation is an operator filter composed with a pointwisenon-linearity. This composition (H, T) yields a (nonlinear) map (H, T) : FA FB": "Finally an operator neural network (ONN) is the result of composing several operator neural layers.More precisely if we are given positive integers 0, . . . , N and N matrices H(j) of noncommuta-tive polynomials H(j)b,a := h(j)b,a(X1, . . . , Xk) for (b, a) [j+1] [j] and j = 0, . . . , N 1, the operator neural network (ONN) determined by H := (H(j))N1j=0 and the operator tuple T is thecomposition F0 F1 FN where the j-th map in the sequence is the operator neurallayer with ReLu activation j(H(j), T) : Fj Fj+1. We write ( H, T) : F0 FN to re-fer to the full composite function. See Appendix A for a discussion on the trainable parameters andthe transfer to other k-tuples. We conclude the Section with a key instance of operator networks: An Example: Graph-tuple neural networks (GtNNs).Henceforth we fix a positive integer k,a sequence G1, . . . , Gk of graphs with common vertex set V and a given set of shift operatorsTG1, . . . , TGk. We call this information a graph-tuple G := (G1, . . . , Gk) on V . The graph-tuple filter defined by a noncommutative polynomial h(X1, . . . , Xk) RX1, . . . , Xkand G is the operator filter defined by h evaluated at T := (TG1, . . . , TGk) denoted (h, T) :R[V ] R[V ]. Exactly as in and using the notation introduced there, we define graph-tuple filters, graph-tuple neural layers with ReLu activation and graph-tuple neural networks (GtNN)on the graph-tuple G as their operator versions when evaluated at the tuple T above.",
  "Perturbation inequalities": "In this Section we introduce our main tools for the analysis of operator networks, namely per-turbation inequalities. To speak about perturbations we endow the Cartesian products FA withmax-normsz := maxa[A] za if z = (za)a[A] FA. where the norm on the right-hand side denotes the standard L2-norm on F coming from themeasure V as defined in the previous section. Fix feature sizes 0, . . . , N and matrices H :=(H(j))j=0,...,N1 of noncommutative polynomials in k-variables of dimensions j+1 j for j =0, . . . , N 1 and consider the operator-tuple neural networks ( H, T) : F0 Fn defined byevaluating this architecture on k-tuples T of operators on the given function space F. A perturbationinequality for this network is an estimate on the sensitivity (absolute condition number) of the outputwhen the operator-tuple and the input signal are perturbed in their respective norms, more preciselyperturbation inequalities are upper bounds on the normH, W(f) H, Z(g) (1)",
  "in terms of the input signal difference f g and the operator perturbation size as measured": "by the differences Zj Wjop. The main result of this Section are perturbation inequalities thatdepend on easily computable constants, which we call expansion constants of the polynomials ap-pearing in the matrices H, allowing us to use them to obtain perturbation estimates for a givennetwork and to devise training algorithms which come with stability guarantees. A key reason forthe success of our approach is the introduction of appropriate norms for computations involvingblock-operators: If A, B are positive integers and z = (za)a[A] FA and R : FA FB is alinear operator then we define",
  "qj()|c| for j = 1, . . . , k": "where qj() equals the number of times the index j appears in . Our main result is the followingperturbation inequality, which proves that expansion constants estimate the perturbation stability ofnonexpansive operator-tuple networks (i.e. those which satisfy Tjop 1 for j = 1, . . . , k). Theorem 1. Suppose W and Z are two nonexpansive operator k-tuples. For positive integers A, Blet H be any B A matrix with entries in RX1, . . . , Xk. The operator-tuple neural layer withReLu activation defined by H satisfies the following perturbation inequality: For any f, g FAand for m := min(f , g ) we have",
  ".(3)": "The proof is in Appendix C. We apply the previous argument inductively to obtain a perturbationinequality for general graph-tuple neural networks by adding the effect of each new layer to thebound. More concretely if 0, . . . , N denote the feature sizes of such a network and RW and RZdenote the network obtained by removing the last layer then",
  "Graphons and graphon-tuple neural networks (WtNNs)": "In order to speak about transferability precisely, we have to address two basic theoretical challenges.On one hand we need to find a space which allows us to place signals and shift operators livingon different graphs in equal footing in order to allow for meaningful comparisons. On the otherhand objects that are close in the natural norm in this space should correspond to graphs describingsimilar phenomena. As shown in , both of these challenges can be solved simultaneouslyby the theory of graphons. A graphon is a continuous generalization of a graph having the realnumbers in the interval as vertex set. The graphon signals are the space L of square-integrablefunctions on , that is L := L2(). In this Section we give a brief introduction to graphonsand define graphon-tuple neural networks (WtNN), the graphon counterpart of graph-tuple neuralnetworks. Our first result is Theorem 4 which clarifies the relationship between finite graphs andsignals on them and their induced graphons and graphon signals respectively allowing us to makemeaningful comparisons between signals on graphs with distinct numbers of vertices. The spaceof graphons has two essentially distinct natural norms which we define later in this Section andreview in Appendix B. Converging sequences under such norms provide useful models for familiesof similar phenomena and Theorem 5 describes explicit sampling methods for using graphons asgenerative models for graph families converging in both norms. Comparisons via graphons. A graphon is a function W : which is measur-able and symmetric (i.e. W(u, v) = W(v, u)). A graphon signal is a function f L := L2().The shift operator of the graphon W is the map TW : L L given by the formula",
  "where dv = d(v) denotes the Lebesgue measure in the interval": "A graphon-tuple W1, . . . , Wk consists of a sequence of k graphons together with their shift op-erators TWi : L L. Exactly as in and using the notation introduced there, we de-fine (A, B) graphon-tuple filters, (A, B) graphon-tuple neural layers with ReLu activation andgraphon-tuple neural networks (WtNN) as their operator versions when evaluated at the k-tupleW := (TW1, . . . , TWk).",
  "j = 0, . . . , N 1, the graphon-tuple neural network (WtNN) defined by H := (H(j))N1j=0 and Wwill be denoted by ( H, W) : L0 LN": "Next we focus on the relationship between (finite) graphs and graphons. Our main interest are signals(i.e. functions) on the common vertex set V of all the graphs which we think of as a discretizationof the graphon vertex set . More precisely, for every integer n we fix a collection of n intervalsI(n)j:= [ j1",
  "2n Ij which constitute the set V (n)": "To compare functions on different V (n) we will use an interpolation operator in and a samplingoperator pn. The interpolation operator in : R[V (n)] L extends a set of values at the pointsof V (n) to a piecewise-constant function in via in(g)(u) := ni=1 g(v(n)i)1I(n)i(u) where1Z(x) denotes the {0, 1} characteristic function of the set Z. The sampling operator pn : L R[V (n)] maps a function f to its conditional expectation with respect to the Ij, namely the functiong R[V (n)] given by the formula g(vj) :=",
  "j=1Sij1I(n)i(x)1I(n)j(y)": "The following Theorem clarifies the relationship between the shift operator of a graph and that of itsinduced graphon and how this basic relationship extends to neural networks. Part (2) will allow us tocompare graph-tuple neural networks on different vertex sets by comparing their induced graphon-tuple networks (the proof is in Appendix C).Theorem 4. For every graph-tuple G1, . . . , Gk on vertex set V (n) and their induced graphonsWj := WGj the equality",
  "TWj = in TGj": "n pnholds. Moreover, this relationship extends to networks: given feature sizes 0, . . . , N and matricesH(j) of noncommutative polynomials having no constant term and of compatible dimensions j+1j for j = 0, . . . , N 1 the graphon-tuple neural network ( H, W) : L0 LN and thenormalized graph-tuple neural network ( H, G/n) : R[V ]0 R[V ]N satisfy the identity",
  "(H, T) = in (H, TG/n) pnwhere pn and in are applied to vectors componentwise": "Graphon norms. The space of graphons is infinite-dimensional and therefore allows for severalnorms. In infinite-dimensional spaces it is customary to speak about equivalent norms, meaningpairs that differ by multiplication by a constant, but also about the coarser relation of topologicallyequivalent norms (two norms are topologically equivalent if a sequence converges in one if and onlyif it converges in the other). Here we describe two specific norms of interest and describe explicitmechanisms for producing converging sequences in the operator norm.",
  ". Known": "as the cut norm, its importance stems from the fact that two graphons differing by a small cut normmust have similar induced subgraphs in the sense of the counting Lemma of Lovasz and Szegedi(see [30, Lemma 10.23] for details). As an analytic object however, the cut norm is often unwieldy, so it is typically bounded via moreeasily computable norms. More precisely, the space of graphons admits two topologically inequiva-lent norms represented by the operator and Hilbert-Schmidt norms of graphon shift operators respec-tively (Example 6 shows that they are indeed inequivalent and why this is important in the presentcontext).",
  "W(u, v)f(u)g(v)dudv which": "is the induced norm of TW as operator from L2() to L2(). It is topologically equivalentto the cut norm (see Appendix B). The Hilbert-Schmidt (HS) norm of TW is the 2-norm of theeigenvalues of TW or equivalently the norm WL2 thinking of W as a function in the square. Graphons as generative models. Given a graphon W we explicitly construct families of graphsof increasing size which have W as limit. The family associated to a graphon provides a practicalrealization of the intuitive idea of a collection of graphs which represent a common phenomenon.Explicitly constructing such families is of considerable practical importance since they provide uswith a controlled setting in which properties like transferability can be tested experimentally overartificially generated data. Assume W(x, y) is a given graphon. For every integer n we fix a finite set of equispaced vertices asabove and a collection of intervals Ij := [vj, vj+1) for j = 1, . . . , n 1 and In := [0, v1) [vn, 1].We will produce two kinds of undirected graphs with vertex set V (n) := {v1, . . . , vn}:",
  "(IiIj)on the edge (vi, vj)": "2. A random graph, the graphon-Erdos-Renyi graph Gn with vertex set V (n) and shift oper-ator S(vi, vj) {0, 1} sampled from a Bernoulli distribution with probability W(vi, vj),which is independent for distinct pairs of vertices. The main result in this Section is that, under mild assumptions on the function W, the weightedtemplate graphs and the random graphon-Erdos-Renyi have induced shift operators converging toTW in suitable norms (see Appendix C for a proof). Note that the second part of the theorem can beseen as a consequence of the analysis in as well.",
  ". If W is Lipschitz continuous then TW T G(n)op 0 almost surely": "Example 6. Fix p (0, 1) and let W(x, y) = p for x = y and zero otherwise. The graphon Erdos-Renyi graphs G(n) constructed from W as in (2) above are precisely the usual Erdos-Renyi graphs.Theorem 5 part (2) guarantees that TW T G(n)op 0 almost surely so this is a convergent graphfamily in the operator norm. By contrast we will show that the sequence of T G(n) does not convergeto TW in the Hilbert-Schmidt norm by proving that T G(n)(x, y)TW (x, y)HS > min(p, 1p) > 0almost surely. To this end note that for every n N and every (x, y) 2 with x = y thedifference |W G(n)(x, y) W(x, y)| min(p, 1 p) since the term on the left is either 0 or 1. Weconclude that T G(n)(x, y)TW (x, y)HS = W G(n)(x, y)W(x, y)L2(2) min(p, 1p) >0 for every n and therefore the sequence fails to converge to zero almost surely. The previous example is important for two reasons. First it shows that the operator and Hilbert-Schmidt norm are not topologically equivalent in the space of graphons. Second, the simplicity ofthe example shows that for applications to transferability, we should focus on the operator norm.More strongly, it proves that trasferability results that depend on the Hilbert-Schmidt norm are notapplicable even to the simplest families of examples, namely Erdos-Renyi graphs.",
  "Universal transferability": "Our next result combines perturbation inequalities for graphon-tuple networks and Theorem 4 whichcompares graph-tuple networks and their induced graphon-tuple counterparts resulting in a transfer-ability inequality. As a corollary of this inequality we prove a universal transferability result whichshows that every architecture is transferable in a converging sequence of graphon-tuples, in the sensethat the transferability error goes to zero as the index of the sequence goes to infinity. This result isinteresting and novel even for the case of graphon-graph transferability (i.e. when k = 1).",
  "We are now able to prove the following Universal transferability result:": "Theorem 8. SupposeG(N) := (G(N)1, . . . , G(N)k) is a sequence of graph-tuples having vertex setV (N) . If the vertex set is equispaced for every N and the sequence converges to a graphon-tuple W in the sense that T G(N)j TWjop 0 as N for j = 1, . . . , k then every graphon-",
  "Training with stability guarantees": "Following our perturbation inequalities (i.e., Theorem 1 and Corollary 3) we propose a trainingalgorithm to obtain a GtNN that enforces stability by constraining all the expansion constants C(h)and Cj(h). Consider a GtNN ( H, TG) and nonexpansive operator k-tuples TG. Denote the set ofk + 1 expansion constants for each layer d = 0, . . . , N 1 as",
  "a[d]h(d)b,a for j = 1, . . . , k,": "and write C( H) = (C(H(d)))N1d=0 and Cj( H) = (Cj(H(d)))N1d=0 for j = 1, . . . , k. Given k + 1vectors of target bounds C := (C(d))N1d=0 and Cj := (C(d)j)N1d=0 for j = 1, . . . , k, and training data(xi, yi) F0 FN for i I, we train the network by a constrained minimization problem",
  "j=1p( Cj( H(c)) Cj)],(6)": "where p() is a componentwise linear penalty function p( C) = (p(C(d)))N1d=0 with p(C(d)) =max(0, C(d)). The stable GtNN algorithm picks a fixed large enough penalty coefficient andtrains the network with local optimization methods. : We assess the tightness of our theoretical results on a regression problem on a synthetic data toyexample consisting of two weighted circulant graphs. See Appendix D.1 for details. (Left) Numerical stabilitybound C(h) (dashed) and stability metrics h(T)op (solid) with respect to input signal perturbation as afunction of the number of epochs for both the standard (1-layer) GtNN (orange) and (1-layer) stable GtNN(blue). (Middle) Similar plot for the stability metrics with respect to the graph perturbation h( W)h(Z)opand its upper bound (Lemma 12 part 2 and 3b). For this plot we take W = T, and Z is a random perturbationfrom T with Z1 W1op Z2 W2op 0.33. (Right) For all four models, compute the 2-normof the vector of output perturbations from Equation (1) over the test set for various sizes of graph perturbation(T1 W1op +T2 W2op)/2, where the additive graph perturbation T1 W1 and T2 W2 are symmetricmatrices with iid Gaussian entries. In addition, each Tj and Wj are normalized such that Tjop 1 andWjop 1 for j = 1, 2, so they are nonexpansive operator-tuple networks. (All) We observe that addingstability constraints does not affect the prediction performance: the testing R squared value for GtNN is 0.6866,while for stable GtNN is 0.6543. :This is an experiment on the MovieLens 100k database, a collection of movie ratings given by aset of 1000 users to 1700 movies. Using collaborative filtering techniques we extract two weightedgraphs that we use to predict ratings of movies by user from a held out test set. See details in Appendix D.3.We report the mean squared error (MSE) in the test set as a function of the number of training iterations (Left)from 0 to 500 and (Right) from 0 to 1500 for the movie recommendation system experiments. We comparethe two models GtNN on the tuple of two graphs (2ONN) and GNN on the best single graph between thosetwo (GNN) on various ridge-regularized versions (the legend contains the values of the chosen regularizationconstants).7Experimental data and numerical results We perform three experiments1: (1) we test the tightness of our theoretical bounds on a simpleregression problem on a synthetic dataset consisting of two weighted circulant graphs (see and Appendix D.1 for details) (2) we assess the transferability of the same model (AppendixD.2), and (3) we run experiments on a real-world dataset of a movie recommendation system wherethe information is summarized in two graphs via collaborative filtering approaches and it iscombined to infer ratings by new users via the GtNN model (see and Appendix D.3).",
  "Conclusions": "In this paper, we introduce graph-tuple networks (GtNNs), a way of extending GNNs to a multi-modal graph setting through the use tuples of non-commutative operators endowed with appropriateblock-operator norms. We show that GtNNs have several desirable properties such as stability toperturbations and a universal transfer property on convergent graph-tuples, where the transferabil-",
  "Code available:": "ity error goes to zero as the graph size goes to infinity. Our transferability theorem improves uponthe current state-of-the-art even for the GNN case. Furthermore, our error bounds are expressed interms of computable quantities from the model. This motivates a novel algorithm to enforce stabil-ity during training. Experimental results show that our transferability error bounds are reasonablytight, and that our algorithm increases the stability with respect to graph perturbation. They alsosuggest that the transferability theorem holds for sparse graph tuples. Finally, the experiments onthe movie recommendation system suggest that allowing for architectures based on GtNNs is ofpotential advantage in real-world applications. We thank Alejandro Ribeiro for fostering our interest in this topic through various conversations,and Teresa Huang for helpful discussions about theory and code implementation. We thank theorganizing committee of the Khipu conference (Montevideo, Uruguay, 2022) for providing a set-ting leading to the present collaboration. SV is partially supported by NSF CCF 2212457, theNSFSimons Research Collaboration on the Mathematical and Scientific Foundations of DeepLearning (MoDL) (NSF DMS 2031985), NSF CAREER 2339682, and ONR N00014-22-1-2126.Mauricio Velasco was partially supported by ANII grants FCE-1-2023-1-176172 and FCE-1-2023-1-176242. Bernardo Rychtenberg was partially supported by ANII grant FCE-1-2023-1-176242.",
  "Michael M Bronstein, Joan Bruna, Taco Cohen, and Petar Velickovic. Geometric deeplearning: Grids, groups, graphs, geodesics, and gauges. arXiv preprint arXiv:2104.13478,2021": "Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and deeplocally connected networks on graphs. In 2nd International Conference on LearningRepresentations, ICLR 2014, 2014. Landon Butler, Alejandro Parada-Mayorga, and Alejandro Ribeiro. Learning with multigraphconvolutional filters. In ICASSP 2023 - 2023 IEEE International Conference on Acoustics,Speech and Signal Processing (ICASSP), pages 15, 2023.",
  "Zhengdao Chen, Lei Chen, Soledad Villar, and Joan Bruna. Can graph neural networks countsubstructures? Advances in neural information processing systems, 33:1038310395, 2020": "Zhengdao Chen, Soledad Villar, Lei Chen, and Joan Bruna. On the equivalence betweengraph isomorphism testing and function approximation with gnns. Advances in neuralinformation processing systems, 32, 2019. Matthieu Cordonnier, Nicolas Keriven, Nicolas Tremblay, and Samuel Vaiter. Convergence ofmessage passing graph neural networks with generic aggregation on random graphs. InGraph Signal Processing workshop 2023, 2023. Simon S Du, Kangcheng Hou, Russ R Salakhutdinov, Barnabas Poczos, Ruosong Wang, andKeyulu Xu. Graph neural tangent kernel: Fusing graph neural networks with graph kernels.Advances in neural information processing systems, 32, 2019.",
  "Fernando Gama, Joan Bruna, and Alejandro Ribeiro. Stability properties of graph neuralnetworks. IEEE Transactions on Signal Processing, 68:56805695, 2020": "Fernando Gama, Antonio G Marques, Geert Leus, and Alejandro Ribeiro. Convolutionalneural network architectures for signals supported on graphs. IEEE Transactions on SignalProcessing, 67(4):10341049, 2018. Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl.Neural message passing for quantum chemistry. In International conference on machinelearning, pages 12631272. PMLR, 2017.",
  "Chaitanya K Joshi, Thomas Laurent, and Xavier Bresson. An efficient graph convolutionalnetwork technique for the travelling salesman problem. arXiv preprint arXiv:1906.01227,2019": "Henry Kenlay, Dorina Thano, and Xiaowen Dong. On the stability of graph convolutionalneural networks under edge rewiring. In ICASSP 2021-2021 IEEE International Conferenceon Acoustics, Speech and Signal Processing (ICASSP), pages 85138517. IEEE, 2021. Nicolas Keriven, Alberto Bietti, and Samuel Vaiter. Convergence and stability of graphconvolutional networks on large random graphs. Advances in Neural Information ProcessingSystems, 33:2151221523, 2020.",
  "ATraining and transference in operator networks": "The trainable parameters of an operator network are precisely the coefficients of thenoncommutative polynomials involved. For instance, if we have feature sizes 0, . . . , N and allinvolved noncommutative polynomials have degree at most d then the number of trainableparameters of the operator network is equal to kd+11",
  "k1N1i=0 ii+1. For each choice c of suchcoefficients the network defines a function ( H(c), T) : F0 FN": "For a fixed collection c of coefficients (for instance the one obtained from training on the data(xi, yi) for i I), the resulting polynomials in their respective matrices H(j)(c) allow us toevaluate the trained operator network ( H(c), T1, . . . , Tk) on any other k-tuple of operatorsM := (M1, . . . , Mk). Here the Mj are linear maps acting on a vector space of functions L(possibly different from F) so evaluation defines a new network ( H(c), M) : L0 LN .Because the coefficients c were obtained by training using the operators T, this network is said tobe built by transference from T to M.",
  "W(u, v)f(u)g(v)dudv": "and note that it this norm is expressible in terms of TW as W = TW op,,1 which isthe operator norm of TW as map from L() to L1(). By [20, Equation 4.4] thecut norm and are equivalent because W W 4W.2. The cut-norm and the standard operator norm are topologically equivalent. This can beseen by letting p in [20, Lemma E.7, part 1] obtaining the inequality",
  "| max(0, fj(u)) max(0, gj(u))| |fj(u) gj(u)|": "holds at every point. Since the left hand side equals the absolute value of a component of(f) (g) the claim is proven by squaring, integrating and taking square roots on both sides andfinally maximizing over j. Remark 11. The previous proof shows that the same conclusion as for ReLu holds for anycomponentwise non-linearity with the property that its derivative exists almost everywhere and hasabsolute value uniformly bounded by one.",
  "The Theorem is proven by applying Lemma 12 part (3) to the operator norms and taking theminimum of the resulting upper bounds": "Remark 13. We expect the bounds of the previous Theorem to be reasonably tight. To establish aprecise result in this direction it suffices to prove that the bounds describe the true behavior inspecial cases. Consider the case k = 1, n = 1 assuming TV , TW and f g are nonnegativescalars with 0 TW TV 1 (a similar reasoning applies to the case of simultaneously",
  "TA(f) TB(g) = TA(f) TA(g) + TA(g) TB(g) f gTAop + TA TBopg": "The second inequality follows from combining the inequality that we just proved with part (1) andexchanging the roles of A and B. (3) We prove the statement by induction on d 0. If d = 0 then = , x is the identity and the claimed inequality holds with equality. If d > 0 let j := (1) andlet [k]d1 be the word obtained from y removing the first (leftmost) term. By constructionthe equality x = Xjx holds and therefore",
  "(pV (f))": "where the last equality follows from the fact that pV iV equals the identity map for any choice offinite set V . The proof is completed by substituting this equality in the left-hand side of theprevious inequality. Proof of Theorem 8. For a positive integer N apply Theorem 7 to the graphon-tuple W and thegraph-tupleG(N) inductively for every layer of the given architecture H. Since the matrices Hdefining our architecture involve only finitely many polynomials the hypothesisTG(N)j TWjop 0 guarantees that the upper bound we obtain converge to zero provided",
  "n pn(f)(x)": "where the last equality holds by definition of pn and in and because (I(n)j) = 1/n for all j. Forthe second claim note that both sides are linear operators it suffices to prove the claim when h(x) isa monomial of degree k 1. This follows immediately by induction using the fact thatpn in = idR[V (n)] for every n. (2) Apply the identity proven in part (1) to the function in(g) anduse the equality pn in = idR[V (n)]. (3) If g R[V ] then",
  "C.4Proof of the sampling Theorem": "Proof of Theorem 5. (1) By compactness of the square 2 given > 0 there exists > 0 suchthat for all n sufficiently large, every rectangle Ii Ij is entirely contained in balls of radius withthe property that |W(xi, xj) W(ai, aj)| < whenever (xi, xj) and (ai, aj) are in Ii Ij. Inparticular, at every point of each square the function deviates at most from its mean on this squareproving that W H(n)L1 . From the results summarized in we conclude thatTW T H(n)HS 0 in the operator norm as claimed. (2) For a positive integer n let B(n) be thediscretization of the graphon W from the values at V (n) V (n) defined by",
  "j=1W(v(n)i, v(n)j)1Ii(x)1Ij(y)": "Via the triangle inequality we estimate W G(n) from above as the sum of W B(n) andB(n) G(n). The first term satisfies the inequality W B(n) W B(n)L1 and thusgoes to zero by continuity of W by the argument from part (1). For the second termB(n) G(n) note that both graphons are constant in the squares Ij Ik and therefore",
  "n2": "where 1/n2 = (Ii Ij) for every i, j. We will show that the series n P(An) < concludingby the Borel-Cantelli Lemma that B(n) G(n) for all but finitely many integers n. Since > 0 was arbitrary this proves that B(n) G(n) 0 almost surely as claimed. To verify thesummability we will use a simple concentration inequality. The probability P(An) equals",
  "yi = [0.76S(l2)S(l1) + 0.33S(l1)S(l2) + 0.3(S(l1))3]xi + i,": "where each value of xi is uniformly distributed between , i F is normal distributed withstandard deviation = 0.1, and we pick n = 293, p = 0.05, l1 = 1, and l2 = 30. Noted that bothinput and output have only one feature, i.e., 0 = N = 1. We train our model with 800 trainingdata I and test it on 200 testing data Itest. We use MSE loss, and use ADAM with learning rate0.01, 1 = 0.9 and 2 = 0.999 to train our models. Running these experiments took a few hourson a regular laptop (just CPU). Denote T1 and T2 as the shift operator corresponding to S(l1) and S(l2) respectively. Recall fromthe main text () that we consider four different models: (i) one layer unconstrained GtNN(i.e., = 0 in (6)), (ii) one layer stable GtNN (with = 10), (iii) two layers GtNN with number ofhidden feature 1 = 2, and (iv) two layers unconstrained GtNN (with = 10 and 1 = 2). For allfour models, we set the non-commutative polynomial h(T1, T2) to be any polynomial of degree at most d = 3. Thus, we have 15 trainable coefficients for both one layer models and 60 for both twolayer models. For the stable GtNN model, we constrain the expansion constants to be at most halfof the corresponding expansion constants obtained after training the unconstrained model.Specifically, let C( H(i)) and Cj( H(i)) denotes the resulting expansion constants vectors for model(i). Then, we set the constraints for model (ii) to be C = C( H(i))/2 and Cj = Cj( H(i))/2 forj = 1, 2. Similarly, we set the constraints for model (iv) to be C = C( H(iii))/2 andCj = Cj( H(iii))/2 for j = 1, 2.",
  "signal, and (H, W) (H, Z) op = h( W) h(Z)op shows the output perturbation due": "to the perturbation from the graph. Meanwhile, C(h) and Cj(h) are the expansion constants thatwe constrained for the stable GtNN model. We note that the upper bounds exhibit the samequalitative behavior as the empirical stability metrics, especially for the stable GtNN model whereall the curves drop due to the parameter reaching the boundary of the constraint sets. This suggeststhat our stability bound is tight, and controlling the expansion constants increase the modelstability. In addition, adding stability constraints has no harm on the prediction performance, sincethe testing R squared value for GtNN is 0.6867, while for stable GtNN is 0.6864. To demonstrate the improvement on stability by our algorithms, we test all four models on variousperturbed graphs (while fixing input signal). As shown in (right) the stable GtNNsincreases the stability under graph perturbations, especially in the 2-layer model.",
  "D.2Experiments on transferability for sparse graph tuples": "We test the transferability behavior on the weighted circulant graph model from Appendix D.1. Weare motivated by the practical setting where we aim to train a model on a small graphs and evaluateit on larger graphs. We consider a piecewise constant graphon tuple (W1, W2) induced from then = 300 circulant graph tuple, and similarly we generate a piecewise constant functions by theinterpolation operator in for each data point. Next, we use this graphon and piecewise constant function as a generative model to generatedeterministic weighted graphs (G1, G2) of size m n as training graphs (normalized by m) and togenerate training data by the sampling operator pm. Since ||TWj T Gj||op 0 as m n,according to Theorem 7 the transferability error goes to 0 too. To demonstrate this, we train fivedifferent models, trained with graphs tuples of fixed size m = 100, 150, 200, 250, 300(respectively) and compare the performance of the testing data with n = 300. :Mean squared error (MSE) on the test set (with testing graph of size n = 300) as afunction of the number of training epochs for (Left) (1-layer) GtNN and (Middle) (1-layer) stableGtNN. In both plots we depict the performance of five different models, trained with graphs of sizesm = 100, 150, 200, 250, 300 respectively. (Right) Comparison of testing MSE between (1-layer)GtNN (blue) and (1-layer) stable GtNN (orange) for training graphs of size m = 100 as a functionof the number of epochs. shows that the best testing MSE decreases as the training size m approaches n for theGtNN, which shows transferability holds for sparse graph tuples. For the stable GtNN, the generaltrend of the testing MSE curves also indicates transferability. In addition, the performancecomparison between GtNN and stable GtNN for m = 100 shows that our stability constraintimproves the transferability by reducing the best testing MSE. However, this improvement onlyappears for the m = 100 case. All the other cases have worse performance for the stable GtNN. Weconjecture this is because the stability constraint makes the training process take a longer time toconverge, and whenever it hits the constraint boundaries the MSE jumps, which also makes itharder to converge to a local minimum. It will be interesting to see if other learning algorithms orpenalty functions for the stability constraints help improve the performance.",
  "D.3Experiments on real-world data from a movie recommendation system": "Finally, we present results on the behavior of graph-tuple neural filters on real data as a tool forbuilding a movie recommendation system. We use the publicly available MovieLens 100kdatabase, a collection of movie ratings given by a set of 1000 users to 1700 movies. Ourobjective is to interpolate ratings among users: starting from the ratings given by a small set ofusers to a certain movie, we wish to predict the rating given to this movie by all the remainingusers. Following we center the data (by removing the mean rating of each user from all itsratings) and try to learn a deviation from the mean rating function. More precisely, letting U be theset of users, we wish to learn the map : R[U] R[U] which, given a partial deviation from themean ratings function f : U 1, 2, . . . , 5 (with missing data marked as zero) produces the fullrating function f = (f) where f(u) contains the deviation of the mean ratings for user u. The classical Collaborative filtering approach to this problem consists of computing the empiricalcorrelation matrix B among users via their rating vectors. A more recent approach defines ashift operator S on the set of users by sparsifying B. More precisely we connect two userswhenever their pairwise correlation is among the k highest for both and then approximate as alinear filter or more generally a GNN evaluated on S. Although typically superior to collaborativefiltering, this approach has a fundamentally ambiguous step: How to select the integer k? To thebest of our knowledge, there is no principled answer to this question so we propose consideringseveral values simultaneously, defining a tuple of shift operators, and trying to learn viagraph-tuple neural networks on R[U]. More specifically we compute two shift operators T1, T2 byconnecting each user to the 10 and 15 most correlated other users respectively, and compare theperformance of the GtNN on the tuple (T1, T2) (2ONN) with the best between the GNNs on eachof the individual operators T1 and T2 (GNN). To make the comparison fair we select the degrees ofthe allowed polynomials so that all models have the same number of trainable parameters (seven). (left) shows that an appropriately regularized Graph-tuple network significantlyoutperforms all other models at any point throughout the first 500 iterations (the minimum occurswhen the training error stops improving significantly). However, if the model is over-trained as inthe right plot of then it can suffer from a vanishing gradients limitation which may lead toa trained model worse than the best one obtained from standard graph filters. This examplesuggests that graph-tuple neural networks are of potential relevance to applications."
}