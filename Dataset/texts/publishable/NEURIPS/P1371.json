{
  "Abstract": "Diffusion models (DMs) have been significantly developed and widely used invarious applications due to their excellent generative qualities. However, the ex-pensive computation and massive parameters of DMs hinder their practical use inresource-constrained scenarios. As one of the effective compression approaches,quantization allows DMs to achieve storage saving and inference acceleration byreducing bit-width while maintaining generation performance. However, as themost extreme quantization form, 1-bit binarization causes the generation perfor-mance of DMs to face severe degradation or even collapse. This paper proposesa novel method, namely BiDM, for fully binarizing weights and activations ofDMs, pushing quantization to the 1-bit limit. From a temporal perspective, weintroduce the Timestep-friendly Binary Structure (TBS), which uses learnable ac-tivation binarizers and cross-timestep feature connections to address the highlytimestep-correlated activation features of DMs. From a spatial perspective, wepropose Space Patched Distillation (SPD) to address the difficulty of matchingbinary features during distillation, focusing on the spatial locality of image genera-tion tasks and noise estimation networks. As the first work to fully binarize DMs,the W1A1 BiDM on the LDM-4 model for LSUN-Bedrooms 256256 achieves aremarkable FID of 22.74, significantly outperforming the current state-of-the-artgeneral binarization methods with an FID of 59.44 and invalid generative samples,and achieves up to excellent 28.0 storage and 52.7 OPs savings.",
  "Introduction": "Diffusion models (DMs) [19; 50; 44; 76], as a type of generative visual model [66; 59; 68], havegarnered impressive attention and applications in various fields, such as image [57; 58], speech [42;45; 24], and video [40; 18], because of their high-quality and diverse generative capabilities. Thediffusion model can generate data from random noise through up to 1000 denoising steps .Although some accelerated sampling methods effectively reduce the number of steps required forgenerating tasks [56; 31], the expensive floating-point computation of each timestep still limitsits wide application on resource-constrained scenarios. Therefore, compression of the diffusionmodel becomes a crucial step for its broader application, and existing compression methods mainlyinclude quantization [30; 54; 47], distillation [53; 36; 41; 73; 11], pruning [7; 12; 14; 13], etc. Thesecompression approaches aim to reduce storage and computation while preserving accuracy.",
  ": Overview of BiDM with Timestep-friendly Binary Structure, which improves DM architec-ture temporally, and Space Patched Distillation, which enhances DM optimization spatially": "efficient computation in inference. Some existing works thus apply quantization to compress DMs,aiming to compress and accelerate them while maintaining the quality of generation. Among them,1-bit quantization, namely binarization, can achieve maximum storage savings for models and hasperformed well in discriminative models such as CNNs [33; 67; 65]. Furthermore, when both weightsand activations are quantized to 1-bit, e.g., fully binarized, efficient bitwise operations such as XNORand bitcount can replace matrix multiplication, achieving the most efficient acceleration . Some existing works have attempted to quantize DM to 1-bit , but their exploration mainlyfocuses on the weights, which are still far from full binarization. In fact, for generative modelslike DM, the impact of fully binarizing weights and activations is catastrophic: a) As generativemodels, DMs have rich intermediate representations closely related to timesteps and highly dynamicactivation ranges, which are both very limited in information when binarized weights and activationsare used; b) Generative models like DMs are typically required to output complete images, but thehighly discrete parameter and feature space make it particularly difficult for binarized DMs to matchthe ground truth during training. The limited representational capacity, which is hard to match withtimesteps dynamically, and the optimization difficulty of generative tasks in discrete space make itdifficult for the binarized DM to converge or even collapse during the optimization process. We propose BiDM to push diffusion models towards extreme compression and acceleration throughcomplete binarization of weights and activations. It is designed to address the unique propertiesof DMs activation features, model structure, and the demands of generative tasks, overcoming thedifficulties associated with complete binarization. BiDM consists of two novel techniques: Froma temporal perspective, we observe that the activation properties of DMs are highly correlatedwith timesteps. We introduce the Timestep-friendly Binary Structure (TBS), which uses learnableactivation binary quantizers to match the highly dynamic activation ranges of DMs and designsfeature connections across timesteps to leverage the similarity of features between adjacent timesteps,thereby enhancing the representation capacity of the binary model. From a spatial perspective, wenote the spatial locality of DMs in generative tasks and the convolution-based U-Net structure. Wepropose Space Patched Distillation (SPD), which introduces a full-precision model as a supervisorand uses attention-guided imitation on divided patches to focus on local features, better guiding theoptimization direction of the binary diffusion model. Extensive experiments show that compared to existing SOTA fully binarized methods, BiDM signifi-cantly improves accuracy while maintaining the same inference efficiency, surpassing all existingbaselines across various evaluation metrics. Specifically, in pixel space diffusion models, BiDM isthe only method that raises the IS to 5.18, close to the level of full-precision models and 0.95 higherthan the best baseline method. In LDM, BiDM reduces the FID on LSUN-Bedrooms from the SOTAmethods 59.44 to an impressive 22.74, while fully benefiting from 28.0 storage and 52.7 OPssavings. As the first fully binarized method for diffusion models, numerous generated samples alsodemonstrate that BiDM is currently the only method capable of producing acceptable images withfully binarized DMs, enabling the efficient application of DMs in low-resource scenarios.",
  "Related Work": "Diffusion models (DMs) have demonstrated excellent generative capabilities across various tasks [19;57; 58; 43; 42; 45; 24]. However, their large-scale model architectures and the high computationalcosts required for multi-step inference limit their practical applications. To address this, methodsfor accelerating the process at the timestep level have been widely proposed, including samplingacceleration that does not require retraining [56; 31; 34; 35] and distillation methods [53; 36; 41]. Arecent method called DeepCache caches high-dimensional features to avoid a lot of redundantcomputations and is compatible with typical sampling acceleration methods. However, these methodscannot overcome the memory bottlenecks and efficiency limits during single-step inference. Quantization is a widely validated compression technique that compresses weights and activationsfrom the usual 32 bits to 1-8 bits to achieve compression and acceleration [6; 78; 37; 75]. Conse-quently, quantization is being studied for application in diffusion models [15; 4]. These methodsgenerally consider the unique timestep structure and spatial architecture of diffusion models, but dueto the significant difficulty of quantizing generative models, most post-training quantization (PTQ)methods can only quantize models to 4 bits or more [29; 54; 22], while more accurate quantization-aware training (QAT) methods face severe performance bottlenecks below 3 bits [30; 55]. Binarization, the most extreme form of quantization, typically expresses weights and activations as1, allowing the model to achieve maximum compression and acceleration [60; 62]. In computervision, binarization work has mainly focused on discriminative models like CNNs [49; 33; 46; 48] orViTs [28; 16], with limited work on generative models. While ResNet VAE and Flow++ haveachieved complete binarization for VAEs , they do not offer generative performance comparableto current advanced models. Binary Latent Diffusion binarized the latent space of LDMs butdid not improve the models spatial footprint or inference efficiency. The latest work, BinaryDM ,quantized DMs to nearly W1A4, but it did not address activation quantization, leaving room forachieving full binarization and acceleration of DMs.",
  "txt1, tI,(1)": "where t (0, 1) is a noise schedule. Gaussian transition kernel allows us to marginalize the jointdistribution, so with t := 1 t and t := ti=1 i, we can easily obtain a sample of xt bysampling a gaussian vector N(0, I) and applying the transformation xt = tx0 + 1 t. The reverse process aims to generate samples by removing noise, approximating the unavailableconditional distribution q (xt1 | xt) with a learnable transition kernel p (xt1 | xt), which can beexpressed as",
  "(b) Activation Features": ": (a) The activation range of the 4th convolutional layer of the full-precision DDIM model onCIFAR-10 varies with the denoising timesteps. (b) The output features are similar at each step of thefull-precision LDM-4 model on LSUN-Bedrooms compared to the previous step. U-Net , due to its ability to fuse low-level and high-dimensional features, has become the main-stream backbone of Diffusion. The input-output blocks of U-Net can be represented as {Dm}dm=1and {Um}dm=1, where blocks corresponding to smaller m are more low-level. Skip connectionspropagate low-level information from Dm() to Um(), so the input received by Um is expressed as:",
  "Observation 1.The activation range varies significantly across long-term timesteps, but theactivation features are similar in short-term neighboring timesteps": "Previous works, such as TDQ and Q-DM , have commonly demonstrated that the activationdistribution of DMs largely depends on denoising process, manifesting as similarities betweenadjacent timesteps while difference between distant ones, as shown in (a). Therefore,applying a fixed scaling factor to activations across all timesteps can cause significant distortion in theactivation range. Beyond the distribution range, Deepcache highlights the substantial temporalconsistency of high-dimensional features across consecutive timesteps, as shown in (b). These phenomena prompt us to reexamine existing binary structures. Binaryization, especially thefull binaryization of weights and activations, results in a greater loss of activation range and precisioncompared to low-bit quantizations like 4-bit . This makes it more challenging to generate richactivation features. Such deficiencies in activation range and output features significantly harmrepresentation-rich generative models like DMs. Therefore, adopting binary quantizers with more flexible activation ranges for DMs, and enhancing the models overall expressive power by leveragingits feature outputs, are crucial strategies for improving its generative capability after full binaryization. We first focus on the differences between various timesteps over the long term. Most existingactivation quantizers, such as BNN and Bi-Real , as shown in Eq. (6), directly quantizeactivations to {+1, -1}. This approach significantly disrupts activation features and negatively impactsthe expressive power of generative models. Some improved activation binary quantizers, such asXNOR++ , adopt a trainable scale factor k:",
  "abi = K sign(a) =K,if a 0,K,otherwise,(8)": "where the form of K could be either a vector or the product of multiple vectors, but it remains aconstant value during inference. Although this approach partially restores the feature expression ofactivations, it does not align well with diffusion models that are highly correlated with timesteps andmay still lead to significant performance loss.",
  "n W1. k R11wh representsa 2D filter, where ij kij =1": "wh. and indicate convolution with and without multiplication,respectively. This approach naturally preserves the range of activation features and dynamicallyadapts with the input range across different timesteps. However, due to the rich expression of DMfeatures, local activations exhibit inconsistency in range before and after passing through modules,indicating that the predetermined value of k does not effectively restore the activation representation. Therefore, we make k adjustable and allow it to be learned during training to adaptively match thechanges in the range of activations before and after. The gradient calculation process of our learnabletiny convolution k can be expressed as follows:",
  "F tcache U tm+1(),Concat(Dt1m (), F tcache).(11)": "However, this approach does not apply to binarized diffusion models, as the information content ofeach output from a binary network is very limited. For binary diffusion models, which inherentlyachieve significant compression and acceleration but have limited expressive power, we anticipatethat the similarity of features between adjacent timesteps will enhance binary representation, therebycompensating for the representation challenges.",
  "Concat(Dt1m (), (1 t1m+1) U t1m+1() + t1m+1 U tm+1()),(12)": "where t1m+1 is a learnable scaling factor. As shown in (b), the similarity of high-dimensionalfeatures varies across different blocks and timesteps in DMs. Therefore, we set multiple independent values to allow the model to adaptively learn more effectively during training. In summary, Timestep-friendly Binary Structure (TBS) includes learnable tiny convolution appliedto scaling factors after averaging the inputs and connections across timesteps. Their combinedeffect adapts to the changes in the activation range of diffusion models over long-range timestepsand leverages the similarity of high-dimensional features between adjacent timesteps to enhanceinformation representation.",
  ": An illustration of TBS. Since the feature space ishigh-dimensional, we illustrate it using schematic diagrams": "From the perspective of error reduc-tion, a visualization of TBS is shownin . First, we abstract the out-put of the binary DM under the base-line method as vector Bt1. The mis-match in scaling factors creates a sig-nificant difference in length between itand the output vector F t1 of the full-precision model. Using our proposedscaling factors and learnable tiny con-volutions, Bt1 is expanded to Lt1.Lt1 is closer to F t1, but there is stilla directional difference from the full-precision model. The cross-timestepconnection further incorporates the out-puts F t of the previous timestep, Bt,and Lt. The high-dimensional featuresimilarity between adjacent timestepsmeans the gap between F t1 and F tis relatively small, facilitating the combination of Lt1 and Lt. Finally, we obtain the binarizedDMs output with TBS applied as T t1 = (1 ) Lt1 + Lt, closest to the output F t1 of thefull-precision model. The learnable tiny convolution k in TBS allows scaling factors to adapt moreflexibly to the representation of DM, while connections across timesteps enable the binarized DM touse the previous steps output information for appropriate information compensation.",
  "Observation 2.Conventional distillation struggles to guide fully binarized DMs to align withfull-precision DMs, while the features of DM exhibit locality in space during the generation task": "In previous practices, adding distillation loss during the training of quantized models has been acommon approach. As the numerical space of binary models is limited, directly optimizing themusing naive loss leads to difficulties in adjusting gradient update directions and makes learningchallenging. Therefore, adding distillation loss to intermediate features can better guide the modelslocal and global optimization process. However, as a generative model, the highly rich feature representation of DMs makes it extremelydifficult for binary models to finely mimic full-precision models. Although the L2 loss used in theoriginal DM training aligns with the Gaussian noise in the diffusion process, it is not suitable for thedistillation matching of intermediate features. During regular distillation, the commonly used L2loss tends to prioritize optimizing pixels with larger discrepancies, leading to a more uniform andsmooth optimization result. This global constraint learning process is challenging for binary modelsaimed at image generation, as their limited representation capacity makes it difficult for fine-graineddistillation imitation to directly adjust them to fully match the direction of full-precision models. At the same time, we note that DMs using U-Net as a backbone naturally exhibit spatial localitydue to their convolution-based structure and generative task requirements. This is different frompast discriminative models, where tasks like classification only require overall feature extractionwithout low-level requirements, making traditional distillation methods unsuitable for DMs withspatial locality in generative tasks. Additionally, most existing DM distillation methods focus onreducing the number of timesteps and do not address the spatial locality of features required forimage generation tasks. Therefore, given the difficulty in optimizing binary DMs with existing loss functions and the spatiallocality of DMs, we propose Space Patched Distillation (SPD). Specifically, we designed a new lossfunction that partitions features into patches before distillation and then calculates spatial attention-guided loss patch by patch. While conventional L2 loss makes it difficult for binary DMs to achievedirect matching, leading to optimization challenges, the attention mechanism allows the distillation",
  "SPD": ": Visualization of the last TimeStepBlocks output of the LDM model on LSUN-bedroomdataset. FP32 denotes the full-precision models output Ffp. Diff denotes the difference between theoutput of the full-precision model and the binarized oneFfp Fbi. Ours denotes the attention-guided SPD. optimization to focus more on critical parts. However, this is still challenging for fully binarizedDMs because the highly discrete binary outputs have limited information, making it difficult for themodel to capture global information. Therefore, we leverage the spatial locality of DMs by dividingintermediate features into multiple patches and independently calculating spatial attention-guidedloss for each patch, allowing the binary model to better utilize local information during optimization.",
  "Experiment": "We conduct experiments on various datasets, including CIFAR-10 32 32 , LSUN-Bedrooms256 256 , LSUN-Churches 256 256 and FFHQ 256 256 over pixel spacediffusion models and latent space diffusion models . The evaluation metrics used in our studyencompass Inception Score (IS), Frchet Inception Distance (FID) , Sliding Frchet InceptionDistance (sFID) , Precision and Recall. To date, there has been no research that compressesdiffusion models to such an extreme extent. Therefore, we use classical binarization algorithms [2;78; 33; 49], the recent SOTA general binarization algorithms , and quantization methods suitedto generative models [15; 63] as baselines. We extract the outputs of TimestepEmbedBlocks fromthe DM to serve as the operating target for our TBS and SPD. And we employ the same shortcutconnections in convolutional layers as those used in ReActNet. Detailed experiment settings arepresented in the Appendix A.",
  "Main Results": "Pixel Space Diffusion Models.We first conduct experiments on the CIFAR-10 32 32 dataset.As the results presented in , W1A1 binarization of DM using baseline methods results insubstantial degradation. However, BiDM demonstrated significant improvements across all metrics,achieving unprecedented restoration of image quality. Specifically, BiDM achieved remarkableenhancements from 4.23 to 5.18 in the IS metric, and reduced 27.9% in the FID metric.",
  "XNOR++1/12.23251.1460.8544.98DoReFa1/11.43397.60139.970.17ReActNet1/13.35231.55119.8018.37ReSTE1/11.26394.29125.840.18XNOR1/14.23113.3627.6746.96BiDM1/15.1881.6525.6852.92": "Latent Space Diffusion Models.Our LDM experiments encompass the evaluation of LDM-4 onLSUN-Bedrooms 256 256 and FFHQ 256 256 datasets, along with the assessment of LDM-8on the LSUN-Churches 256 256 dataset. The experiments utilized the DDIM sampler with 200steps, and the detailed outcomes are presented in . Across these three datasets, our methodachieved significant improvements over the best baseline methods. In comparison to other binarizationalgorithms, BiDM outperformed across all metrics. On the LSUN-Bedrooms, LSUN-Churches, andFFHQ datasets, the FID metric of BiDM decreased by 61.7%, 30.7%, and 51.4%, respectively,compared to the best results among the baselines. In contrast to XNOR++, its adoption of fixed activation scaling factors in the denoising processresults in a very limited dynamic range for its activations, making it difficult to match the highlyflexible generative representations of DMs. BiDM addressed this challenge by making the tinyconvolution k learnable, which acts on the dynamically computed scaling factors. This optimizationled to substantial improvements exceeding an order of magnitude across all metrics. On the LSUN-Bedrooms and LSUN-Churches datasets, the FID metric decreased from 319.66 to 22.74 and from292.48 to 29.70, respectively. Additionally, compared to the SOTA binarization method ReSTE,BiDM achieved significant enhancements across multiple metrics, particularly demonstrating notableimprovements on the LSUN-Bedrooms dataset. We have supplemented our work with BBCU, abinarization method more akin to generative models like DMs rather than discriminative models.Experimental results indicate that even as a binarization strategy for generative models, BBCUfaces significant breakdowns when applied to DMs, as FID dropped dramatically to 236.07 onLSUN-Bedrooms. As a work targeting QAT for DM, EfficientDM is indeed a suitable comparison,especially since it designs TALSQ to address the variation in activation range. The results showthat EfficientDM struggles to adapt to the extreme scenario of W1A1, and this may be due to itsquantizer having difficulty adapting to binarized DM, and using QALoRA for weight updates mightyield suboptimal results compared to full-parameter QAT. As we mentioned in the TBS section of our manuscript, most existing binarization methods struggle tohandle the wide activation range and flexible expression of DMs, further highlighting the necessity ofTBS. Their optimization strategies may also not be tailored for the image generation tasks performedby DM, which means they only achieve conventional but suboptimal optimization.",
  "Ablation Study": "We perform comprehensive ablation studies for LDM-4 on the LSUN-Bedrooms 256 256 dataset toevaluate the effectiveness of each proposed component in BiDM. We evaluate the effectiveness of ourproposed SPD and TBS, and the results are presented in . Upon separately applying our SPD orTBS methods to LDM, we observed significant improvements compared to the original performance.When the TBS method was incorporated, FID and sFID dropped sharply from 106.62 and 56.61 to35.23 and 25.13, respectively. Similarly, when the SPD method was added, FID and sFID decreased",
  "LSUN-Bedrooms 256256FFHQ 256256": ": Visualization of samples generated by the W1A1 baseline and our BiDM. BiDM is the firstfully binarized DM method capable of generating viewable images, significantly surpassing advancedbinarization methods. significantly from 106.62 and 56.61 to 40.62 and 31.61, respectively. Other metrics also exhibitedsubstantial improvements. This demonstrates the effectiveness of our approach in continuouslyapproximating the binarized model features to full-precision features during training by introducing alearnable factor tm and incorporating connections between adjacent time steps. Furthermore, whenwe combined our two methods and applied them to LDM, we observed an additional improvementcompared to the individual application of each method. This further substantiates that performingdistillation between full-precision and binarized models at the patch level can significantly enhancethe performance of the binarized model. We also conducted additional ablation experiments, and theresults are presented in the appendix B.",
  "Efficiency Analysis": "Inference Efficiency Analysis. We conducted an analysis of the diffusion models inference efficiencyunder complete binarization. During inference, BiDM requires only a very small number of additionalfloating-point additions for the connections across timesteps compared to the classic binarizationwork XNOR-Net, and there are no differences in the majority of calculations, such as convolutions.Performing a floating-point convolution with a depth of 1 for scaling factors requires only a smallamount of computation, and the overhead for averaging matrix A is also minimal. The findingspresented in reveal that BiDM, while achieving the same 28.0 memory efficiency and 52.7computational savings as the XNOR baseline, demonstrates significantly superior image generationcapabilities, with the FID decreased from 106.62 to 22.74. See Appendix B for more details.",
  "FP32/321045.4-96.0096.002.99XNOR1/137.392.10.381.82106.62BiDM1/137.392.10.381.8222.74": "Training Efficiency Analysis. We also explored the training efficiency of BiDM, as the overheadrequired for the QAT of binarized DMs cannot be overlooked. Theoretical analysis and experimentalresults show that BiDM achieved significantly better generative results than baseline methods underthe same training cost, demonstrating that it not only has a higher upper limit of generative capabilitybut is also relatively efficient in terms of generative performance. See Appendix B for details.",
  "Conclusion": "In this paper, we present BiDM, a novel fully binarized method that pushes the compression ofdiffusion models to the limit. Based on two observations activations at different timesteps andthe characteristics of image generation tasks we propose the Timestep-friendly Binary Structure(TBS) and Space Patched Distillation (SPD) from temporal and spatial perspectives, respectively.These methods address the severe limitations in representation capacity and the challenges of highlydiscrete spatial optimization in full binarization. As the first fully binarized diffusion model, BiDMdemonstrates significantly better generative performance than the SOTA general binarization methodsacross multiple models and datasets. On LSUN-Bedrooms, BiDM achieves an FID of 22.74, greatlysurpassing the SOTA method with an FID of 59.44, making it the only method capable of generatingvisually acceptable samples while achieving up to 28.0 storage savings and 52.7 OPs savings.",
  "Zheng Chen, Haotong Qin, Yong Guo, Xiongfei Su, Xin Yuan, Linghe Kong, and Yulun Zhang.Binarized diffusion model for image super-resolution. arXiv preprint arXiv:2406.05723, 2024": "Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Binarizedneural networks: Training deep neural networks with weights and activations constrained to+ 1or-1. arXiv preprint arXiv:1602.02830, pages 111, 2016. Steven K Esser, Jeffrey L McKinstry, Deepika Bablani, Rathinakumar Appuswamy, and Dhar-mendra S Modha. Learned step size quantization. In International Conference on LearningRepresentations, pages 112, 2019.",
  "Lukas Geiger and Plumerai Team. Larq: An open-source library for training binarized neuralnetworks. Journal of Open Source Software, 5(45):1746, 2020": "Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer.A survey of quantization methods for efficient neural network inference. In Low-Power Com-puter Vision, pages 291326. Chapman and Hall/CRC, 2022. Ruihao Gong, Yifu Ding, Zining Wang, Chengtao Lv, Xingyu Zheng, Jinyang Du, Haotong Qin,Jinyang Guo, Michele Magno, and Xianglong Liu. A survey of low-bit large language models:Basics, systems, and algorithms. arXiv preprint arXiv:2409.16694, 2024.",
  "Guangyu Guo, Longfei Han, Le Wang, Dingwen Zhang, and Junwei Han. Semantic-awareknowledge distillation with parameter-free feature uniformization. Visual Intelligence, 1(1):6,2023": "Jinyang Guo, Jiaheng Liu, and Dong Xu. Jointpruning: Pruning networks along multipledimensions for efficient point cloud processing. IEEE Transactions on Circuits and Systems forVideo Technology, 32(6):36593672, 2021. Jinyang Guo, Jiaheng Liu, and Dong Xu. 3d-pruning: A model compression framework forefficient 3d action recognition. IEEE Transactions on Circuits and Systems for Video Technology,32(12):87178729, 2022.",
  "Yefei He, Jing Liu, Weijia Wu, Hong Zhou, and Bohan Zhuang.Efficientdm: Efficientquantization-aware fine-tuning of low-bit diffusion models. arXiv preprint arXiv:2310.03270,2023": "Yefei He, Zhenyu Lou, Luoming Zhang, Jing Liu, Weijia Wu, Hong Zhou, and Bohan Zhuang.Bivit: Extremely compressed binary vision transformers. In Proceedings of the IEEE/CVFInternational Conference on Computer Vision, pages 56515663, 2023. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances inneural information processing systems, 30, 2017. Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko,Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: Highdefinition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022.",
  "Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advancesin neural information processing systems, 33:68406851, 2020": "Wei Huang, Yangdong Liu, Haotong Qin, Ying Li, Shiming Zhang, Xianglong Liu, MicheleMagno, and Xiaojuan Qi. Billm: Pushing the limit of post-training quantization for llms. arXivpreprint arXiv:2402.04291, 2024. Wei Huang, Xudong Ma, Haotong Qin, Xingyu Zheng, Chengtao Lv, Hong Chen, Jie Luo,Xiaojuan Qi, Xianglong Liu, and Michele Magno. How good are low-bit quantized llama3models? an empirical study. arXiv preprint arXiv:2404.14047, 2024.",
  "Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.pages 160, 2009": "Phuoc-Hoan Charles Le and Xinlin Li. Binaryvit: pushing binary vision transformers towardsconvolutional models. In Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 46644673, 2023. Xiuyu Li, Yijiang Liu, Long Lian, Huanrui Yang, Zhen Dong, Daniel Kang, Shanghang Zhang,and Kurt Keutzer. Q-diffusion: Quantizing diffusion models. In Proceedings of the IEEE/CVFInternational Conference on Computer Vision, pages 1753517545, 2023.",
  "Luping Liu, Yi Ren, Zhijie Lin, and Zhou Zhao. Pseudo numerical methods for diffusion modelson manifolds. arXiv preprint arXiv:2202.09778, 2022": "Zechun Liu, Wenhan Luo, Baoyuan Wu, Xin Yang, Wei Liu, and Kwang-Ting Cheng. Bi-realnet: Binarizing deep network towards real-network performance. International Journal ofComputer Vision, 128:202219, 2020. Zechun Liu, Zhiqiang Shen, Marios Savvides, and Kwang-Ting Cheng. Reactnet: Towardsprecise binary neural network with generalized activation functions. In Proceedings of theEuropean Conference on Computer Vision, pages 143159. Springer, 2020. Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver:A fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances inNeural Information Processing Systems, 35:57755787, 2022.",
  "William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedingsof the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023": "Vadim Popov, Ivan Vovk, Vladimir Gogoryan, Tasnima Sadekova, and Mikhail Kudinov. Grad-tts: A diffusion probabilistic model for text-to-speech. In International Conference on MachineLearning, pages 85998608. PMLR, 2021. Haotong Qin, Ruihao Gong, Xianglong Liu, Mingzhu Shen, Ziran Wei, Fengwei Yu, andJingkuan Song. Forward and backward information retention for accurate binary neural networks.In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,pages 22502259, 2020. Haotong Qin, Xudong Ma, Xingyu Zheng, Xiaoyang Li, Yang Zhang, Shouda Liu, Jie Luo, Xi-anglong Liu, and Michele Magno. Accurate lora-finetuning quantization of llms via informationretention. arXiv preprint arXiv:2402.05445, 2024. Haotong Qin, Xiangguo Zhang, Ruihao Gong, Yifu Ding, Yi Xu, and Xianglong Liu.Distribution-sensitive information retention for accurate binary neural network. InternationalJournal of Computer Vision, 131(1):2647, 2023. Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenetclassification using binary convolutional neural networks. In Proceedings of the EuropeanConference on Computer Vision, pages 525542. Springer, 2016.",
  "Ze Wang, Jiang Wang, Zicheng Liu, and Qiang Qiu. Binary latent diffusion. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2257622585,2023": "Xiao-Ming Wu, Dian Zheng, Zuhao Liu, and Wei-Shi Zheng. Estimator meets equilibriumperspective: A rectified straight through estimator for binary neural networks training. InProceedings of the IEEE/CVF International Conference on Computer Vision, pages 1705517064, 2023. Bin Xia, Yulun Zhang, Yitong Wang, Yapeng Tian, Wenming Yang, Radu Timofte, and LucVan Gool. Basic binary convolution unit for binarized image restoration network. arXiv preprintarXiv:2210.00405, 2022.",
  "Yisong Xiao, Aishan Liu, Tianyuan Zhang, Haotong Qin, Jinyang Guo, and Xianglong Liu.Robustmq: benchmarking robustness of quantized models. Visual Intelligence, 1(1):30, 2023": "Yixing Xu, Kai Han, Chang Xu, Yehui Tang, Chunjing Xu, and Yunhe Wang. Learningfrequency domain approximation for binary neural networks. Advances in Neural InformationProcessing Systems, 34:2555325565, 2021. Zhekai Xu, Haohong Shang, Shaoze Yang, Ruiqi Xu, Yichao Yan, Yixuan Li, Jiawei Huang,Howard C Yang, and Jianjun Zhou. Hierarchical painter: Chinese landscape painting restorationwith fine-grained styles. Visual Intelligence, 1(1):19, 2023. Zihan Xu, Mingbao Lin, Jianzhuang Liu, Jie Chen, Ling Shao, Yue Gao, Yonghong Tian, andRongrong Ji. Recu: Reviving the dead weights in binary neural networks. In Proceedings of theIEEE/CVF International Conference on Computer Vision, pages 51985208, 2021.",
  "Zelong Zeng, Fan Yang, Hong Liu, and Shinichi Satoh. Improving deep metric learning viaself-distillation and online batch diffusion process. Visual Intelligence, 2(1):18, 2024": "Jianhao Zhang, Yingwei Pan, Ting Yao, He Zhao, and Tao Mei. Dabnn: A super fast inferenceframework for binary neural networks on arm devices. In Proceedings of the 27th ACMInternational Conference on Multimedia, pages 22722275, 2019. Yulun Zhang, Haotong Qin, Zixiang Zhao, Xianglong Liu, Martin Danelljan, and FisherYu. Flexible residual binarization for image super-resolution. In Ruslan Salakhutdinov, ZicoKolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp,editors, Proceedings of the 41st International Conference on Machine Learning, volume 235 ofProceedings of Machine Learning Research, pages 5973159740. PMLR, 2127 Jul 2024.",
  "Wenliang Zhao, Haolin Wang, Jie Zhou, and Jiwen Lu. Dc-solver: Improving predictor-correctordiffusion sampler via dynamic compensation. arXiv preprint arXiv:2409.03755, 2024": "Xingyu Zheng, Haotong Qin, Xudong Ma, Mingyuan Zhang, Haojie Hao, Jiakai Wang, ZixiangZhao, Jinyang Guo, and Xianglong Liu. Binarydm: Towards accurate binarization of diffusionmodel. arXiv preprint arXiv:2404.05662, 2024. Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou. Dorefa-net:Training low bitwidth convolutional neural networks with low bitwidth gradients. arXiv preprintarXiv:1606.06160, pages 113, 2016.",
  "AExperiment Settings": "We adopt several classic binarization algorithms, including XNOR , XNOR++ , DoReFa ,and ReActNet , along with the SOTA binarization method, ReSTE as baselines. Addi-tionally, we also include the quantization methods designed for generative models, BBCU andEfficientDM . We extract the output features of TimestepEmbedBlocks from the DM to serve asthe targets of TBS and SPD operations. For the CIFAR-10 dataset, We add TBS connections tothe outputs of the last 2 timestep embedding blocks and set init to 0.3. The on CIFAR-10 is set to3e-2. For the LSUN-Bedrooms , LSUN-Churches and FFHQ datasets, We add TBSconnections to the outputs of the last 8 timestep embedding blocks and also set init to 0.3. The onthese three datasets is set to 1e-2. Our quantization-aware training is based on the pre-trained diffusion model, and the quantizerparameters and latent weights are trained simultaneously. The overall training process is relativelyconsistent with the original training process of DDIM or LDM. For the CIFAR-10 dataset, we setthe learning rate to 6e-5 and the batch size to 64 during training. The training process consistedof 100k iterations, and during sampling, we used 100 sampling steps. For the LSUN-Bedrooms,LSUN-Churches and FFHQ datasets, the learning rate was set to 2e-5 and the batch size to 4 duringtraining. The training consisted of 200k iterations, with 200 steps used during denoising phase. We conducted extensive experiments on two different types of diffusion models: the latent-space dif-fusion model LDM and the pixel-space diffusion model DDIM. For the DDIM model, we specificallyselected the CIFAR-10 dataset with a resolution of 32 32 for our experiments. For the LDM model,our experiments spanned multiple datasets, including the LSUN-Bedrooms, LSUN-Churches and theFFHQ dataset, all with a resolution of 256 256. To evaluate the generation quality of the diffusionmodel, we utilize several evaluation metrics, including Inception Score (IS), Frchet Inception Dis-tance (FID) , Sliding Frchet Inception Distance (sFID) , and Precision-and-Recall. After200,000 iterations of training, we randomly sample and generate 50,000 images from the model andcompute the metrics based on reference batches. The reference batches used to evaluate FID andsFID contain all the corresponding datasets. We recorded FID, sFID, and Precision for all tasks andadditional IS for CIFAR-10. We utilize OPs as metrics for evaluating theoretical inference efficiency. Taking the convolutionalunit as an example, the BOPs for a single computation operation of a single convolution are definedas follows nmk2babw [69; 71]. It is composed of bw bits for weights, babits for activation, n inputchannels, m output channels, and a k k convolutional kernel. For the output feature with width wand height h, BOPs whnmk2babw. As there might also be full-precision modules in the model,the total OPs of the model are summed up as164BOPs + FLOPs . All our experiments areconducted on a server with NVIDIA A100 40GB GPU.",
  "We conducted more detailed ablation experiments to comprehensively validate our results": "Effects of learnable k in TBS. We apply the proposed learnable k to the XNOR baseline. Theexperimental results shown in indicate that this modification can lead to a significantimprovement in performance. The model achieved a doubling of improvement in FID, sFID. Theiroriginal values were 106.62 and 56.81, respectively, and they decreased to 57.62 and 30.46. Thenegligible degradation in Recall can be overlooked.",
  "Vanilla1/1106.6256.816.825.22learnable1/157.2630.4615.885.00": "Effects of cross-timestep connection in TBS. We investigated the impact of varying the numberof TBS connections. illustrates that the introduction of TBS cross-timestep connectionsconsistently outperforms models without such connections(n = 0). This validates the efficacy of ourcross-timestep linkage strategy based on the high-dimensional feature similarity of LDM. Among the experiments incorporating cross-timestep connections, the models with 1 and 8 connectionsboth achieved equally optimal results. The model with 1 connection demonstrated slightly superiorperformance in FID and Precision, whereas the model with 8 nodes exhibited marginally betteroutcomes in sFID and Recall.",
  "(6) Pointwise multiply O448,32,322by 448,1,1 to obtain the final full-precision outputO448,32,32": "We utilized the general deployment library Larq on a Qualcomm Snapdragon 855 Plus to testthe actual runtime efficiency of the aforementioned single convolution. The runtime results for asingle inference are summarized in the . Due to limitations of the deployment library andhardware, Baseline achieved a 9.97x speedup, while XNOR-Net / BiDM achieved an 8.07x speedup.Besides, the improvement in generation performance brought by BiDM is even more significant, andwe believe that it could achieve better acceleration results in a more optimized environment.",
  "FP176371.0176371.02.99Baseline (DoReFa)17695.24.317699.5188.30XNOR-Net / BiDM17695.22948.81133.383.24.321864.822.74": "Further Training Efficiency Analysis. BiDM consists of two techniques: TBS and SPD. Thetime efficiency analysis during training is as follows: (1) TBS includes the learnable convolutionof scaling factors (Eq.10) and the cross-time step connection (Eq.12). The increase in trainingtime due to the convolution of trainable scaling factors is minimal, as the depth of the convolutionfor scaling factors is only 1, and the size of the trainable convolution kernel is only 3 3. Thecross-time step connection is the primary factor for the increase in training time. Since it requirestraining , we introduce this structure during training, so each training sample requires not only noiseestimation for T t1 but also for T t, directly doubling the sampling steps. (2) SPD may lead to aslight increase in training time (an additional 0.18 times), but since we only apply supervision to thelarger upsampling/middle/downsampling blocks, the increase is limited. The results in align well with the theoretical analysis mentioned above. BiDM achievedsignificantly better generative results than baseline methods under the same training iterations,demonstrating that it not only has a higher upper limit of generative capability but is also relativelyefficient when considering generative performance. We also tested the FID after uniformly training for 0.5 days, and the results in Tabel 9 show: (1)BiDM has the best convergence, even in a short training time. (2) No.3 significantly outperformsNo.5 because connections across timesteps greatly increase training time, making No.3 convergefaster in the early training stages. (3) No.5 slightly outperforms No.7 because LSP D causes a slightincrease in training time. We emphasize that the biggest challenge in fully binarizing DM lies in the drop in accuracy. AlthoughBiDM requires a longer training time for the same number of iters, it significantly enhances thequality of generated images, as no other method has been able to produce effective images.",
  "The answer NA means that the paper has no limitation while the answer No means thatthe paper has limitations, but those are not discussed in the paper": "The authors are encouraged to create a separate \"Limitations\" section in their paper. The paper should point out any strong assumptions and how robust the results are toviolations of these assumptions (e.g., independence assumptions, noiseless settings,model well-specification, asymptotic approximations only holding locally). The authorsshould reflect on how these assumptions might be violated in practice and what theimplications would be. The authors should reflect on the scope of the claims made, e.g., if the approach wasonly tested on a few datasets or with a few runs. In general, empirical results oftendepend on implicit assumptions, which should be articulated. The authors should reflect on the factors that influence the performance of the approach.For example, a facial recognition algorithm may perform poorly when image resolutionis low or images are taken in low lighting. Or a speech-to-text system might not beused reliably to provide closed captions for online lectures because it fails to handletechnical jargon.",
  "If applicable, the authors should discuss possible limitations of their approach toaddress problems of privacy and fairness": "While the authors might fear that complete honesty about limitations might be used byreviewers as grounds for rejection, a worse outcome might be that reviewers discoverlimitations that arent acknowledged in the paper. The authors should use their bestjudgment and recognize that individual actions in favor of transparency play an impor-tant role in developing norms that preserve the integrity of the community. Reviewerswill be specifically instructed to not penalize honesty concerning limitations.",
  "If the contribution is a dataset and/or model, the authors should describe the steps takento make their results reproducible or verifiable": "Depending on the contribution, reproducibility can be accomplished in various ways.For example, if the contribution is a novel architecture, describing the architecture fullymight suffice, or if the contribution is a specific model and empirical evaluation, it maybe necessary to either make it possible for others to replicate the model with the samedataset, or provide access to the model. In general. releasing code and data is oftenone good way to accomplish this, but reproducibility can also be provided via detailedinstructions for how to replicate the results, access to a hosted model (e.g., in the caseof a large language model), releasing of a model checkpoint, or other means that areappropriate to the research performed. While NeurIPS does not require releasing code, the conference does require all submis-sions to provide some reasonable avenue for reproducibility, which may depend on thenature of the contribution. For example(a) If the contribution is primarily a new algorithm, the paper should make it clear howto reproduce that algorithm.",
  "(b) If the contribution is primarily a new model architecture, the paper should describethe architecture clearly and fully": "(c) If the contribution is a new model (e.g., a large language model), then there shouldeither be a way to access this model for reproducing the results or a way to reproducethe model (e.g., with an open-source dataset or instructions for how to constructthe dataset). (d) We recognize that reproducibility may be tricky in some cases, in which caseauthors are welcome to describe the particular way they provide for reproducibility.In the case of closed-source models, it may be that access to the model is limited insome way (e.g., to registered users), but it should be possible for other researchersto have some path to reproducing or verifying the results.",
  "Guidelines:": "The answer NA means that the paper does not use existing assets. The authors should cite the original paper that produced the code package or dataset. The authors should state which version of the asset is used and, if possible, include aURL. The name of the license (e.g., CC-BY 4.0) should be included for each asset. For scraped data from a particular source (e.g., website), the copyright and terms ofservice of that source should be provided. If assets are released, the license, copyright information, and terms of use in thepackage should be provided. For popular datasets, paperswithcode.com/datasetshas curated licenses for some datasets. Their licensing guide can help determine thelicense of a dataset.",
  ". Experiments Compute Resources": "Question: For each experiment, does the paper provide sufficient information on the com-puter resources (type of compute workers, memory, time of execution) needed to reproducethe experiments?Answer: [Yes]Justification: The paper provides sufficient information on the computer resources in thesection A.Guidelines: The answer NA means that the paper does not include experiments. The paper should indicate the type of compute workers CPU or GPU, internal cluster,or cloud provider, including relevant memory and storage.",
  ". Code Of Ethics": "Question: Does the research conducted in the paper conform, in every respect, with theNeurIPS Code of Ethics [Yes]Justification: The research conducted in the paper conform, in every respect, with theNeurIPS Code of Ethics.Guidelines: The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. If the authors answer No, they should explain the special circumstances that require adeviation from the Code of Ethics.",
  "If the authors answer NA or No, they should explain why their work has no societalimpact or why the paper does not address societal impact": "Examples of negative societal impacts include potential malicious or unintended uses(e.g., disinformation, generating fake profiles, surveillance), fairness considerations(e.g., deployment of technologies that could make decisions that unfairly impact specificgroups), privacy considerations, and security considerations. The conference expects that many papers will be foundational research and not tiedto particular applications, let alone deployments. However, if there is a direct path toany negative applications, the authors should point it out. For example, it is legitimateto point out that an improvement in the quality of generative models could be used togenerate deepfakes for disinformation. On the other hand, it is not needed to point outthat a generic algorithm for optimizing neural networks could enable people to trainmodels that generate Deepfakes faster. The authors should consider possible harms that could arise when the technology isbeing used as intended and functioning correctly, harms that could arise when thetechnology is being used as intended but gives incorrect results, and harms followingfrom (intentional or unintentional) misuse of the technology. If there are negative societal impacts, the authors could also discuss possible mitigationstrategies (e.g., gated release of models, providing defenses in addition to attacks,mechanisms for monitoring misuse, mechanisms to monitor how a system learns fromfeedback over time, improving the efficiency and accessibility of ML).",
  "The answer NA means that the paper does not involve crowdsourcing nor research withhuman subjects": "Depending on the country in which research is conducted, IRB approval (or equivalent)may be required for any human subjects research. If you obtained IRB approval, youshould clearly state this in the paper. We recognize that the procedures for this may vary significantly between institutionsand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and theguidelines for their institution."
}