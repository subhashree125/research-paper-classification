{
  "Abstract": "Low-rank approximation and column subset selection are two fundamental and relatedproblems that are applied across a wealth of machine learning applications. In this paper,we study the question of socially fair low-rank approximation and socially fair column subsetselection, where the goal is to minimize the loss over all sub-populations of the data. We showthat surprisingly, even constant-factor approximation to fair low-rank approximation requiresexponential time under certain standard complexity hypotheses. On the positive side, we give analgorithm for fair low-rank approximation that, for a constant number of groups and constant-factor accuracy, runs in 2poly(k) time rather than the nave npoly(k), which is a substantialimprovement when the dataset has a large number n of observations. We then show that thereexist bicriteria approximation algorithms for fair low-rank approximation and fair column subsetselection that run in polynomial time.",
  "Introduction": "Machine learning algorithms are increasingly used in technologies and decision-making processesthat affect our daily lives, from high volume interactions such as online advertising, e-mail filtering,smart devices, or large language models, to more critical processes such as autonomous vehicles,healthcare diagnostics, credit scoring, and sentencing recommendations in courts of law [Cho17,KLL+18, BHJ+21]. Machine learning algorithms frequently require statistical analysis, utilizingfundamental problems from numerical linear algebra, especially low-rank approximation and columnsubset selection.In the classical low-rank approximation problem, the input is a data matrix A Rnd andan integral rank parameter k > 0, and the goal is to find the best rank k approximation to A,i.e., to find a set of k vectors in Rd that span a matrix B, which minimizes L(A B) across allrank-k matrices B, for some loss function L. The rank parameter k should be chosen to accurately",
  "arXiv:2412.06063v1 [cs.LG] 8 Dec 2024": "represent the complexity of the underlying model chosen to fit the data, and thus the low-rankapproximation problem is often used for mathematical modeling and data compression.Similarly, in the classic column subset selection problem, the goal is to choose k columns U ofA so as to minimize L(A UV) across all choices of V Rkd. Although low-rank approximationcan reveal important latent structure among the dataset, the resulting linear combinations may notbe as interpretable as simply selecting k features. The column subset selection problem is thereforea version of low-rank approximation with the restriction that the left factor must be k columns ofthe data matrix. Column subset selection also tends to result in sparse models. For example, ifthe columns of A are sparse, then the columns in the left factor U are also sparse. Thus in somecases, column subset selection, often also called feature selection, can be more useful than generallow-rank approximation. Algorithmic fairness.Unfortunately, real-world machine learning algorithms across a widevariety of domains have recently produced a number of undesirable outcomes from the lens ofgeneralization. For example, [BS16] noted that decision-making processes using data collected fromsmartphone devices reporting poor road quality could potentially underserve poorer communitieswith less smartphone ownership. [KMM15] observed that search queries for CEOs overwhelminglyreturned images of white men, while [BG18] observed that facial recognition software exhibiteddifferent accuracy rates for white men compared with dark-skinned women.Initial attempts to explain these issues can largely be categorized into either biased data orbiased algorithms, where the former might include training data that is significantly misrepresentingthe true statistics of some sub-population, while the latter might sacrifice accuracy on a specificsub-population in order to achieve better global accuracy. As a result, an increasingly relevant line ofactive work has focused on designing fair algorithms. An immediate challenge is to formally define thedesiderata demanded from fair algorithmic design and indeed multiple natural quantitative measuresof fairness have been proposed [HPS16, Cho17, KMR17, BHJ+21]. However, [KMR17, CPF+17]showed that many of these conditions for fairness cannot be simultaneously achieved.In this paper, we focus on socially fair algorithms, which seek to optimize the performance ofthe algorithm across all sub-populations. That is, for the purposes of low-rank approximation andcolumn subset selection, the goal is to minimize the maximum cost across all sub-populations. Forsocially fair low-rank approximation, the input is a set of data matrices A(1) Rn1d, . . . , A() Rnd corresponding to groups and a rank parameter k, and the goal is to determine a setof k factors U1, . . . , Uk Rd that span matrices B(1), . . . , B(), which minimize maxi[] A(i) B(i)F . Due to the EckartYoungMirsky theorem stating that the Frobenius loss is minimizedwhen each A(i) is projected onto the span of U = U1 . . . Uk, the problem is equivalent tominURkd maxi[] A(i)UU A(i)F , where denotes the Moore-Penrose pseudoinverse. Weremark that as we can scale the columns in each group (even each individual column), our formulationcaptures min-max normalized cost relative to the total Frobenius norm of each group and theoptimal rank-k reconstruction loss for each group.",
  "Theorem 1.1. Fair low-rank approximation is NP-hard to approximate within any constant factor": "Proof. Given an instance v(1), . . . , v(n) Rd of Subspace(n 1, ) with n < d, we set = k = n 1and A(i) = v(i) for all i [n]. Then for a k-dimensional linear subspace V Rkd, we have thatA(i)VV A(i)2F is the distance from v(i) to the subspace. Hence, maxi[] A(i)VV A(i)2Fis the maximum Euclidean distance of these points to the subspace and so the fair low-rankapproximation problem is exactly Subspace(n 1, ). By Theorem 2.1, the Subspace(n 1, )problem is NP-hard to approximate within any constant factor. Thus, fair low-rank approximationis NP-hard to approximate within any constant factor. We next introduce a standard complexity assumption beyond NP-hardness. Recall that in the3-SAT problem, the input is a Boolean satisfiability problem written in conjunctive normal form,consisting of n clauses, each with 3 literals, either a variable or the negation of a variable. The goalis to determine whether there exists a Boolean assignment to the variables to satisfy the formula.",
  "Theorem 1.2. Under the exponential time hypothesis, the fair low-rank approximation requires2k(1) time to approximate within any constant factor": "Proof. Given an instance v(1), . . . , v(k) Rd of Subspace(k 1, ) with k < d, we set = k 1 andA(i) = v(i) for all i [k]. Then for a (k 1)-dimensional linear subspace V R(k1)d, we have thatA(i)VV A(i)2F is the distance from v(i) to the subspace. Hence, maxi[] A(i)VV A(i)2Fis the maximum Euclidean distance of these points to the subspace and so the fair low-rankapproximation problem is exactly Subspace(k 1, ). By Theorem 2.3, the Subspace(k 1, )problem requires 2k(1) time to approximate within any constant factor.Thus, fair low-rankapproximation requires 2k(1) time to approximate within any constant factor.",
  "(1 + ) (A(i)S)(VS)R(i))((A(i)S)(VS)R(i))A(i)S A(i)S2F": "Unfortunately, V is not given, so the above approach will not quite work. Instead, we usea polynomial system solver to check whether there exists such a V by writing Y = VS and itspseudoinverse W = (VS) and we check whether there exists a satisfying assignment to the aboveinequality, given the constraints (1) YWY = Y, (2) WYW = W, and (3) A(i)SWR(i) hasorthonormal columns.We remark that because V Rkd, then we cannot navely implement the polynomial system,because it would require kd variables and thus use 2(dk) runtime. Instead, we only manipulate VS,which has dimension k m for m = Ok22 log , allowing the polynomial system solver to achieve",
  "(1 )GMHp MF (1 + )GMHp": "Although low-rank approximation with Lp loss cannot be well-approximated in polynomial time,we recall that there exists a matrix S that samples a small number of columns of A to providea coarse bicriteria approximation to Lp low-rank approximation [WY23a]. However, we require asolution with dimension d and thus we seek to solve the regression problem minX GAHSXGAHp.Thus, we consider a Lewis weight sampling matrix T such that",
  "TGAHSX TGAHp GAHSX GAHp 2TGAHSX TGAHp": "We then note that (TGAHS)TGAH is the minimizer of the problem minx TGAHSX TGAHF , which only provides a small distortion to the Lp regression problem, since TGAH hasa small number of rows due to the dimensionality reduction. By Dvoretzkys Theorem, we havethat (TGAHS)TGA is a good approximate solution to the original fair low-rank approximationproblem. The algorithm appears in full in Algorithm 3.We use the following notion of Dvoretzkys theorem to embed the problem into entrywise Lploss.",
  "Fair column subset selection.We next describe our results for fair column subset selection.We give a bicriteria approximation algorithm for fair column subset selection that uses polynomialruntime": "Theorem 1.5. Given input matrices A(i) Rnid with n = ni, there exists an algorithmthat selects a set S of k = O (k log k) columns such that with probability at least23, S is aO (k(log log k)(log d))-approximation to the fair column subset selection problem. The algorithmuses runtime polynomial in n and d. The immediate challenge in adapting the previous approach for fair low-rank approximation tofair column subset selection is that we required the Gaussian matrices G, H to embed the awkwardmaximum of Frobenius losses min maxi[] F into the more manageable Lp loss min p throughGAH. However, selecting columns of GAH does not correspond to selecting columns of the inputmatrices A(1), . . . , A().Instead, we view the bicriteria solution V from fair low-rank approximation as a good startingpoint for the right factor for fair low-rank approximation. Thus we consider the multi-responseregression problem maxi[] minB(i) B(i) V A(i)F . We then argue through Dvoretzkys theoremthat a leverage score sampling matrix S that samples O (k log k) columns of V will provide a goodapproximation to the column subset selection problem. We defer the formal exposition to . Empirical evaluations.Finally, in , we perform a number of experimental resultson socially fair low-rank approximation, comparing the performance of the socially fair low-rankobjective values associated with the outputs of the bicriteria fair low-rank approximation algorithmand the standard (non-fair) low-rank approximation that outputs the top k right singular vectors ofthe singular value decomposition.Our experiments are on the Default of Credit Card Clients dataset [YL09], which is a commonhuman-centric data used for benchmarks on fairness, e.g., see [STM+18]. We perform empiricalevaluations comparing the objective value and the runtime of our bicriteria algorithm with theaforementioned baseline, across various subsample sizes and rank parameters. Our results demon-strate that our bicriteria algorithm can perform better than the standard low-rank approximationalgorithm across various parameter settings, even when the bicriteria algorithm is not allowed alarger rank than the baseline. Moreover, we show that our algorithm is quite efficient and in fact,the final step of extracting the low-rank factors is faster than the singular value decompositionbaseline due to a smaller input matrix. All in all, our empirical evaluations indicate that ourbicriteria algorithm can perform well in practice, thereby reinforcing the theoretical guarantees ofthe algorithm.",
  "Related Work": "Initial insight into socially fair data summarization methods were presented by [STM+18], wherethe concept of fair PCA was explored.This study introduced the fairness metric of averagereconstruction loss, expressed by the loss function loss(A, B) := A B2F A Ak2F , aiming toidentify a k-dimensional subspace that minimizes the loss across the groups, with Ak representingthe best rank-k approximation of A. Their proposed approach, in a two-group scenario, identifies afair PCA of up to k + 1 dimensions that is not worse than the optimal fair PCA with k dimensions.When extended to groups, this method requires an additional k + 1 dimensions. Subsequently, [TSS+19] explored fair PCA from a distinct objective perspective, seeking a projection matrix Poptimizing mini[] A(i)P2F . A pivotal difference between these works and ours is our focus on thereconstruction error objective, a widely accepted objective for regression and low-rank approximationtasks. Alternatively, [OA19, LKO+22, KDRZ23] explored a different formulation of fair PCA. Themain objective is to ensure that data representations are not influenced by demographic attributes.In particular, when a classifier is exposed only to the projection of points onto the k-dimensionalsubspace, it should be unable to predict the demographic attributes.Recently, [MOT24] studied a fair column subset selection objective similar to ours, focusingon the setting with two groups (i.e., = 2). They established the problems NP-hardness andintroduced a polynomial-time solution that offers relative-error guarantees while selecting a columnsubset of size O (k).For fair regression, initial research focused on designing models that offer similar treatmentto instances with comparable observed results by incorporating fairness regularizers [BHJ+17].However, in [ADW19], the authors studied a fairness notion closer to our optimization problem,termed as bounded group loss. In their work, the aim is to cap each groups loss within a specificlimit while also optimizing the cumulative loss. Notably, their approach diverged from ours, with afocus on the sample complexity and the problems generalization error bounds.[AAK+22] studied a similar socially fair regression problem under the name min-max regression.In their setting, the goal is to minimize the maximum loss over a mixture distribution, given samplesfrom the mixture; our fair regression setting can be reduced to theirs. [AAK+22] observed thata maximum of norms is a convex function and can therefore be solved using projected stochasticgradient descent.The term socially fair was first introduced in the context of clustering, aiming to optimizeclustering costs across predefined group sets [GSV21, ABV21]. In subsequent studies, tight ap-proximation algorithms [MV21, CMV22], FPT approaches [GJ23], and bicriteria approximationalgorithms [GSV22] for socially fair clustering have been presented.",
  "Preliminaries": "We use the notation [n] to represent the set {1, . . . , n} for an integer n 1. We use the notationpoly(n) to represent a fixed polynomial in n and we use the notation polylog(n) to representpoly(log n). We use poly(n) to denote a fixed polynomial in n and polylog(n) to denote poly(log n).We say an event holds with high probability if it holds with probability 1 1 poly(n).We generally use bold-font variables to represent vectors and matrices, whereas we use default-font variables to represent scalars. For a matrix A Rnd, we use Ai to represent the i-th row ofA and A(j) to represent the j-th column of A. We use Ai,j to represent the entry in the i-th rowand j-th column of A. For p 1, we use",
  "Regression and Low-Rank Approximation": "In this section, we briefly describe some common techniques used to handle both regression andlow-rank approximation, thus presenting multiple unified approaches for both problems. Thus inlight of the abundance of techniques that can be used to handle both problems, it is somewhatsurprising that socially fair regression and socially fair low-rank approximation exhibit vastlydifferent complexities. Closed form solutions.Given the regression problem minxRd Ax b2 for an input matrixA Rnd and a label vector b Rn, the closed form solution for the minimizer is Ab =argminxRd Axb2, where A is the Moore-Penrose pseudoinverse of A. Specifically, for a matrixA Rnd written in its singular value decomposition A = UV, we have A = V1U. Inparticular, for if A has linearly independent rows, then A = A(AA)1. On the other hand, ifA has linearly independent columns, then A = (AA)1A.Similarly, given an input matrix A and a rank parameter k > 0, there exists a closed formsolution for the minimizer argminVRkd A AVV2F . Specifically, by the Eckart-Young-Mirskytheorem [EY36], the minimizer is the top k right singular vectors of A. Dimensionality reduction.We next recall a unified set of dimensionality reduction techniquesfor both linear regression and low-rank approximation. We consider the sketch-and-solve paradigm,so that for both problems, we first acquire a low-dimension representation of the problem, and findthe optimal solution in the low dimension using the above closed-form solutions. For good designsof the low-dimension representations, the low-dimension solution will also be near-optimal for theoriginal problem.We first observe that oblivious linear sketches serve as a common dimensionality reduction forboth linear regression and low-rank approximation. For example, it is known [Woo14] that thereexists a family of Gaussian random matrices G1 from which S G1 satisfies with high probability,",
  "(1 )SAx Sb2 Ax b2 (1 + )SAx Sb2,": "simultaneously for all x Rd.Similarly, there exists [Woo14] a family of Gaussian randommatrices G2 from which S G1 satisfies with high probability, that the row space of SA contains a(1 + )-approximation of the optimal low-rank approximation to A.Alternatively, we can achieve dimensionality reduction for both linear regression and low-rankapproximation by sampling a small subset of the input in related ways for both problems. Forlinear regression, we can generate a random matrix S by sampling rows ofAbby their leveragescores [DMM06a, DMM06b, Mag10, Woo14]. In this manner, we again achieve a matrix S suchthat with high probability,",
  "We recall that it can be shown the sum of the leverage scores for an input matrix M Rnd": "is upper bounded by d and moreover, given the leverage scores of M, it suffices to sample onlyO (d log d) rows of M to achieve a constant factor subspace embedding of M. Specifically, it isknown that the sum of the leverage scores of the rows of a matrix can be bounded by the rank ofthe matrix.",
  "By sampling rows proportional to their leverage scores, we can obtain a subspace embedding asfollows:": "Theorem 1.9 (Leverage score sampling). [DMM06a, DMM06b, Mag10, Woo14] Given a matrixM Rnd, let i be the leverage score of the i-th row of M. Suppose pi = min (1, i log n) foreach i [n] and let S be a random diagonal matrix so that the i-th diagonal entry of S is1pi withprobability pi and 0 with probability 1 pi. Then for all vectors v Rn,",
  "Moreover, S has at most O (d log d) nonzero entries with high probability": "Because the leverage scores of M can be computed directly from the singular value decompositionof M, which can be computed in O (nd + dn) time where is the exponent of matrix multiplication,then the leverage scores of M can be computed in polynomial time.Finally, we recall that to provide a constant factor approximation to Lp regression, it suffices tocompute a constant factor subspace embedding, e.g., through leverage score sampling. The proofis through the triangle inequality and is well-known among the active sampling literature [CP19,PPP21, MMWY22, MMM+22, MMM+23], e.g., a generalization of Lemma 2.1 in [MMM+22].",
  "We also recall the following construction to use Lewis weights to achieve an Lp subspaceembedding": "Theorem 1.11 ([CP15]). Let (0, 1) and p 2.Let A Rnd and s = Odp/2 log d.Then there exists a polynomial-time algorithm that outputs a matrix S Rsn that samples andreweights s rows of A, such that with probability at least 0.99, simultaneously for all x Rd,(1 )Axpp SAxpp (1 + )Axpp.",
  "Lower Bounds": "In this section, we show that it is NP-hard to approximate fair low-rank approximation within anyconstant factor in polynomial time and moreover, under the exponential time hypothesis, it requiresexponential time to achieve a constant factor approximation.Given points v(1), . . . , v(n) Rd, their outer (d k)-radius is defined as the minimum, over allk-dimensional linear subspaces, of the maximum Euclidean distance of these points to the subspace.We define this problem as Subspace(k, ). It is known that it is NP-hard to approximate theSubspace(n 1, ) problem within any constant factor:",
  "Hypothesis 2.2 (Exponential time hypothesis [IP01]). The 3-SAT problem requires 2(n) runtime": "Observe that while NP-hardness simply conjectures that the 3-SAT problem cannot be solvedin polynomial time, the exponential time hypothesis conjectures that the 3-SAT problem requiresexponential time.We remark that in the context of Theorem 2.1, [BGK00] showed the hardness of approximationof Subspace(n 1, ) through a reduction from the Max-Not-All-Equal-3-SAT problem, whoseNP-hardness itself is shown through a reduction from 3-SAT. Thus under the exponential timehypothesis, Max-Not-All-Equal-3-SAT problem requires 2(n) to solve. Then it follows that:",
  "We first give a (1 + )-approximation algorithm for fair low-rank approximation that uses runtime1 poly(n) (2)O(N), for n = i=1 ni and N = poly, k, 1": ".The algorithm first finds a value that is an -approximation to the optimal solution, i.e.,minVRkd maxi[] A(i)VV A(i)F is at most minVRkd maxi[] A(i)VV A(i)F .We then repeatedly decrease by (1 + ) while checking if the resulting quantity is still achievable.To efficiently check if is achievable, we first apply dimensionality reduction to each of the matricesby right-multiplying by an affine embedding matrix S, so that",
  "(1 )A(i)VV A(i)2F A(i)VVS A(i)S2F (1 + )A(i)VV A(i)2F ,": "for all rank k matrices V and all i [].Now if we knew V, then for each i [], we can find X(i) minimizing X(i)VS A(i)S2F andthe resulting quantity will approximate A(i)VV A(i)2F . In fact, we know that the minimizer is(A(i)S)(VS) through the closed form solution to the regression problem. Let R(i) be defined sothat (A(i)S)(VS)R(i) has orthonormal columns, so that",
  "(A(i)S)(VS)R(i))((A(i)S)(VS)R(i))A(i)S A(i)S2F = minX(i) X(i)VS A(i)S2F ,": "and so we require that if is feasible, then (A(i)S)(VS)R(i))((A(i)S)(VS)R(i))A(i)S A(i)S2F .Unfortunately, we do not know V, so instead we use a polynomial solver to checkwhether there exists such a V. We remark that similar guessing strategies were employed by[RSW16, KPRW19, BWZ19, VVWZ23] and in particular, [RSW16] also uses a polynomial systemin conjunction with the guessing strategy. Thus we write Y = VS and its pseudoinverse W = (VS) and check whether there exists a satisfying assignment to the above inequality, given the constraints(1) YWY = Y, (2) WYW = W, and (3) A(i)SWR(i) has orthonormal columns. Note that sinceV Rkd, then implementing the polynomial solver navely could require kd variables and thususe 2(dk) runtime. Instead, we note that we only work with VS, which has dimension k m form = Ok22 log , so that the polynomial solver only uses 2poly(mk) time.",
  "We first recall the following result for polynomial system satisfiability solvers": "Theorem 2.4 ([Ren92a, Ren92b, BPR96]). Given a polynomial system P(x1, . . . , xn) over realnumbers and m polynomial constraints fi(x1, . . . , xn) i 0, where {>, , =, =, , <} for alli [m], let d denote the maximum degree of all the polynomial constraints and let B denote themaximum size of the bit representation of the coefficients of all the polynomial constraints. Thenthere exists an algorithm that determines whether there exists a solution to the polynomial system Pin time (md)O(n) poly(B).",
  "To apply Theorem 2.4, we utilize the following statement upper bounding the sizes of the bitrepresentation of the coefficients of the polynomial constraints in our system": "Theorem 2.5 ([JPT13]). Let T = {x Rn | f1(x) 0, . . . , fm(x) 0} be defined by m polynomialsfi(x1, . . . , xn) for i [m] with degrees bounded by an even integer d and coefficients of magnitude atmost M. Let C be a compact connected component of T . Let g(x1, . . . , xn) be a polynomial of degreeat most d with integer coefficients of magnitude at most M. Then the minimum nonzero magnitudethat g takes over C is at least (24n/2 Mdn)n2ndn, where M = max(M, 2n + 2m).",
  "Lemma 2.7 (Lemma 11 in [CEM+15]). Given , (0, 1) and a rank parameter k > 0, letm = Ok22 log 1": ". For any matrix A Rdn, there exists a family S of random matrices in Rnm,such that for S S, we have that with probability at least 1 , S is a one-sided affine embeddingfor a matrix A Rdn and a vector b Rn. We now show a crucial structural property that allows us to distinguish between the case wherea guess for the optimal value OPT exceeds (1 + )OPT or is smaller than (1 )OPT by simplylooking at a polynomial system solver on an affine embedding. Lemma 2.8. Let V Rkd be the optimal solution to the fair low-rank approximation problem forinputs A(1), . . . , A(), where A(i) Rnid, and suppose OPT = maxi[] A(i)VV A(i)2F . Let Sbe an affine embedding for V and let W = (VS) Rkm. For i [], let Z(i) = A(i)SW Rnik and R(i) Rkk be defined so that A(i)SWR(i) has orthonormal columns. If (1 + ) OPT,then for each i [], (A(i)SWR(i))(A(i)SWR(i))A(i) A(i)2F . If < (1 ) OPT, thenthere exists i [], such that < (A(i)SWR(i))(A(i)SWR(i))A(i) A(i)2F .",
  "Bicriteria Algorithm": "To achieve polynomial time for our bicriteria algorithm, we can no longer use a polynomialsystem solver.Instead, we observe that for sufficiently large p, we have max x = (1 )xp. Thus, in place of optimizing minVRkd maxi[] A(i)VV A(i)F , we instead optimize minVRkdi[] A(i)VV A(i)pF1/p. However, the terms A(i)VV A(i)pF are unwieldyto work with. Thus we instead use Dvoretzkys Theorem, i.e., Theorem 2.11, to embed L2 intoLp, by generating matrices G and H so that (1 )GMHp MF (1 + )GMHp, for allmatrices M Rnd.Now, writing A = A(1) . . . A(), it suffices to approximately solve minXRkd GAHSX GAHp. Unfortunately, low-rank approximation with Lp loss still cannot be approximated to(1 + )-factor in polynomial time, and in fact GAH has dimension n d with n n and d d.Hence, we first apply dimensionality reduction by appealing to a result of [WY23a] showing thatthere exists a matrix S that samples a small number of columns of A to provide a coarse bicriteriaapproximation to Lp low-rank approximation. Now to lift the solution back to dimension d, wewould like to solve regression problem minX GAHSX GAHp. To that end, we consider a Lewisweight sampling matrix T such that",
  "We use the following algorithm from [WY23a] to perform dimensionality reduction so thatswitching between L2 and Lp loss will incur smaller error. See also [CGK+17]": "Theorem 2.12 (Theorem 1.5 in [WY23a]). Let A Rnd and let k 1. Let s = O (k log log k).Then there exists a polynomial-time algorithm that outputs a matrix S Rdt that samples t =Ok(log log k)(log2 d)columns of A and a matrix Z Rtd such that A ASZp 2p O (s) minURnk,VRkd A UVp.",
  "TG U VH TGAH(p,2)= 2TGAHS(TGAHS)(TGA)H TGAH(p,2),": "where M(p,2) denotes the Lp norm of the vector consisting of the L2 norms of the columns of M,and the last equality follows due to our setting of U = AHS and V (TGAHS)(TGA), thelatter in Algorithm 3. By optimality of (TGAHS)(TGA)H for the choice of X in the minimizationproblemminXRtd TGAHSX TGAH(p,2),",
  "Socially Fair Column Subset Selection": "In this section, we consider socially fair column subset selection, where the goal is to identify a matrixC Rdk that selects k columns to minimize minCRdk,C0k,B(i) maxi[] A(i)CB(i) A(i)F .Note that each matrix B(i) Rnid is implicitly defined in the minimization problem. Intuitively,the goal is to select k columns of A so that each group A(i) can be well-approximated by somematrix B(i) spanned by those k columns.To prove correctness of Algorithm 4, we first require the following structural property:",
  "By combining Lemma 3.2 and Lemma 3.3, we have:": "Theorem 1.5. Given input matrices A(i) Rnid with n = ni, there exists an algorithmthat selects a set S of k = O (k log k) columns such that with probability at least23, S is aO (k(log log k)(log d))-approximation to the fair column subset selection problem. The algorithmuses runtime polynomial in n and d.",
  "In this section, we describe our empirical evaluations for socially fair low-rank approximation onreal-world datasets": "Credit card dataset.We used the Default of Credit Card Clients dataset [YL09], which has30,000 observations across 23 features, including 17 numeric features. The dataset is a commonhuman-centric data for experiments on fairness and was previously used as a benchmark by [STM+18]for studies on fair PCA. The study collected information from various customers including multipleprevious payment statements, previous payment delays, and upcoming bill statements, as wellas if the customer was able to pay the upcoming bill statement or not, i.e., defaulted on the billstatement. The dataset was accessed through the UCI repository [Yeh16]. Experimental setup.For the purposes of reproducibility, our empirical evaluations wereconducted using Python 3.10 using a 64-bit operating system on an AMD Ryzen 7 5700UCPU, with 8GB RAM and 8 cores with base clock 1.80 GHz. Our code is available at We compare our bicriteria algorithm from Algorithm 3 againstthe standard non-fair low-rank approximation algorithm that outputs the top k right singular vectorsfrom the singular value decomposition. Gender was used as the sensitive attribute, so that allobservations with one gender formed the matrix A(1) and the other observations formed the matrixA(2). As in Algorithm 3, we generate normalized Gaussian matrices G and H and then use Lp Lewisweight sampling to generate a matrix T. We generate matrices T, G, and H with a small numberof dimensions and thus do not compute the sampling matrix S but instead use the full matrix. Wefirst sampled a small number s of rows from A(1) and A(2) and compared our bicriteria algorithmto the standard non-fair low-rank approximation algorithm baseline. We plot the minimum ratiofor s {2, 3, . . . , 21}, k = 1, and p = 1 over 10, 000 iterations for each setup in a. Similarly,we plot both the minimum and average ratios for s = 1000, k {1, 2, . . . , 7, 8}, and p = 1 over 200iterations in b, where we permit the bicriteria solution to have rank 2k. Finally, we comparethe runtimes of the algorithms in c, separating runtimes for our bicriteria into the totalruntime bicrit1 that includes the process of generating the Gaussian matrices and performing theLewis weight sampling, as well as the runtime bicrit2 for the step for extracting the factors similarto SVD, which measures the runtime for extracting the factors. Across all experiments in ,we used Gaussian matrices G with 30 rows and H with 30 columns. Results and discussion.Our empirical evaluations serve as a simple proof-of-concept demon-strating that our bicriteria algorithm can perform significantly better for socially fair low-rankapproximation, even without allowing for more than k factors. In particular, all ratios in aand most ratios in are less than 1, indicating that the cost of our algorithm is less than the cost ofthe baseline, so that smaller ratios represent better performance. In fact, in some cases the ratiowas less than 0.6, demonstrating the superiority of our algorithm. Finally, we show in cthat due to the small number of rows acquired by Lewis weight sampling, the final extraction ofthe factors is significantly faster by our algorithm. However, the main runtime bottleneck is the",
  "Synthetic Datasets": "In this section, we remark on a number of additional empirical evaluations for socially fair low-rankapproximation. We first compare our bicriteria algorithm to the standard low-rank approximationbaseline over a simple synthetic example. We then describe a simple fixed input matrix for socially fairlow-rank approximation that serves as a proof-of-concept similarly demonstrating the improvementof fair low-rank approximation optimal solutions over the optimal solutions for standard low-rankapproximation. Synthetic dataset.Next, we show that for a simple dataset with two groups, each with twoobservations across four features, the performance of the fair low-rank approximation algorithmcan be much better than standard low-rank approximation algorithm on the socially fair low-rankobjective, even without allowing for bicriteria rank. For our synthetic dataset, we generate simple",
  "a factor of two. We show in that our bicriteria algorithm achieves similar improvement": "Experimental setup.Again, we compare our bicriteria algorithm from Algorithm 3 against thestandard non-fair low-rank approximation algorithm that outputs the top k right singular vectorsfrom the singular value decomposition. Per Dvoretzkys Theorem, c.f., Theorem 2.11, we generatenormalized Gaussian matrices G and H of varying size and then use Lp Lewis weight sampling togenerate a matrix T with varying values of p. We generate matrices T, G, and H with a smallnumber of dimensions and thus do not compute the sampling matrix S but instead use the fullmatrix. We first fix p = 1 and iterate over the number of rows/columns in the Gaussian sketch in therange {1, 2, . . . , 19, 20} in a. We then fix the Gaussian sketch to have three rows/columns and iterate over p {1, 2, . . . , 9, 10} in b. For each of the variables, we compare the outputsof the two algorithms across 100 iterations and plot the ratio of their fair costs. In particular, we setthe same rank parameter of k = 2 for both algorithms; we remark that the theoretical guarantees forour bicriteria algorithm are even stronger when we permit the solution to have rank k for k > k.",
  ": Ratio of the cost of our bicriteria algorithm to the cost of the standard low-rank approximationsolution for k = 2, across 100 iterations": "Results and discussion.Our empirical evaluations in show that our algorithms canperform significantly better for socially fair low-rank approximation. We note that in both acrossall values of the number of rows/columns in the Gaussian sketch in a and across all valuesof p in the Lewis weight sampling parameter in b, the average ratio between our bicriteriasolution and the standard low-rank approximation is less than 0.9. We remark that any ratioless than 1 demonstrates the superiority of our algorithm, with smaller ratios indicating betterperformance. In fact, the minimum ratio is as low as 0.76. Proof-of-concept.Finally, we give a toy example using a synthetic dataset showing the importanceof considering fairness in low-rank approximation. Namely, we show that even for a simple datasetwith four groups, each with a single observation across two features, the performance of the fairlow-rank approximation algorithm can be much better than standard low-rank approximationalgorithm on the socially fair low-rank objective. In this setup, we repeatedly generate matricesA(1), A(2), A(3), A(4) {0, 1}2, with A(1) = (1, 0) and A(2) = A(3) = A(4) = (0, 1). The optimalfair low-rank solution is",
  "Conclusion": "In this paper, we study algorithms for socially fair low-rank approximation and column subset selec-tion. Although we show that even a constant-factor approximation to fair low-rank approximationrequires exponential time under certain standard complexity hypotheses, we give an algorithm thatis a substantial improvement over the nave approach for any constant number of groups. We alsogive a bicriteria approximation algorithms for fair low-rank approximation and fair column subsetselection that runs in polynomial time. Finally, we perform a number of empirical evaluationsserving as a simple proof-of-concept demonstrating the practical nature of our theoretical findings. Itis our hope that our work is an important step toward better understanding of fairness in numericallinear algebra.Our work also leads to a number of interesting future directions.For example, there arevarious notions of fairness, including (1) disparate impact, which requires that the output of analgorithm respects a desired distribution that protects various subpopulations in the decision-makingprocess [CKLV17, RS18, BIO+19, BCFN19, BGK+18, AEK+20, EBTD20, DMV22, CXZC24], (2)individual fairness, where each element should incur a cost that is within a reasonable amountwith respect to the overall dataset [JKL20b, MV20, NC21, VY22], or (3) representative fair-ness, where the number of representative elements of each subpopulation should be relativelybalanced [KAM19, JNN20, AKSZ22, NNJ22, TGOO22, HMV23]. A natural direction is the capa-bilities and limitations of randomized linear algebra for other notions of fairness. Another questionis the efficient construction of -coresets with minimal size for socially fair subset selection. Finally,one can ask whether similar guarantees are possible in settings whether the input matrix is notcentralized, but rather for example, distributed or arrives over a data stream. Indeed, there areknown analogs [CDW18, BDM+20, JLL+21, WY23a] for a number of the central tools used in themain algorithms of this paper. [AAK+22]Jacob D. Abernethy, Pranjal Awasthi, Matthaus Kleindessner, Jamie Morgenstern,Chris Russell, and Jie Zhang. Active sampling for min-max fairness. In InternationalConference on Machine Learning, ICML, pages 5365, 2022. 6, 33 [ABV21]Mohsen Abbasi, Aditya Bhaskara, and Suresh Venkatasubramanian. Fair clusteringvia equitable group representations. In Proceedings of the 2021 ACM conference onfairness, accountability, and transparency, pages 504514, 2021. 6 [ADW19]Alekh Agarwal, Miroslav Dudk, and Zhiwei Steven Wu. Fair regression: Quantitativedefinitions and reduction-based algorithms. In International Conference on MachineLearning, pages 120129. PMLR, 2019. 6 [AEK+20]Sara Ahmadian, Alessandro Epasto, Marina Knittel, Ravi Kumar, Mohammad Mah-dian, Benjamin Moseley, Philip Pham, Sergei Vassilvitskii, and Yuyan Wang. Fairhierarchical clustering. In Advances in Neural Information Processing Systems 33:Annual Conference on Neural Information Processing Systems 2020, NeurIPS, 2020.25 [AKSZ22]Haris Angelidakis, Adam Kurpisz, Leon Sering, and Rico Zenklusen. Fair and fastk-center clustering for data summarization. In International Conference on MachineLearning, ICML, pages 669702, 2022. 25",
  "[BCFN19]Suman Bera, Deeparnab Chakrabarty, Nicolas Flores, and Maryam Negahbani. Fairalgorithms for clustering. Advances in Neural Information Processing Systems, 32,2019. 25": "[BDM+20]Vladimir Braverman, Petros Drineas, Cameron Musco, Christopher Musco, JalajUpadhyay, David P. Woodruff, and Samson Zhou. Near optimal linear algebra in theonline and sliding window models. In 61st IEEE Annual Symposium on Foundationsof Computer Science, FOCS, pages 517528, 2020. 25 [BG18]Joy Buolamwini and Timnit Gebru. Gender shades: Intersectional accuracy disparitiesin commercial gender classification. In Conference on Fairness, Accountability andTransparency, FAT, volume 81 of Proceedings of Machine Learning Research, pages7791, 2018. 2 [BGK00]Andreas Brieden, Peter Gritzmann, and Victor Klee.Inapproximability of somegeometric and quadratic optimization problems. Approximation and Complexity inNumerical Optimization: Continuous and Discrete Problems, pages 96115, 2000. 3, 11 [BGK+18]Ioana O Bercea, Martin Gro, Samir Khuller, Aounon Kumar, Clemens Rosner,Daniel R Schmidt, and Melanie Schmidt. On the cost of essentially fair clusterings.arXiv preprint arXiv:1811.10319, 2018. 25 [BHJ+17]Richard Berk, Hoda Heidari, Shahin Jabbari, Matthew Joseph, Michael Kearns, JamieMorgenstern, Seth Neel, and Aaron Roth. A convex framework for fair regression.arXiv preprint arXiv:1706.02409, 2017. 6 [BHJ+21]Richard Berk, Hoda Heidari, Shahin Jabbari, Michael Kearns, and Aaron Roth.Fairness in criminal justice risk assessments: The state of the art. Sociological Methods& Research, 50(1):344, 2021. 1, 2 [BIO+19]Arturs Backurs, Piotr Indyk, Krzysztof Onak, Baruch Schieber, Ali Vakilian, and TalWagner. Scalable fair clustering. In International Conference on Machine Learning,pages 405413. PMLR, 2019. 25",
  "[BS16]Solon Barocas and Andrew D Selbst. Big datas disparate impact. California lawreview, pages 671732, 2016. 2": "[BWZ19]Frank Ban, David P. Woodruff, and Qiuyi (Richard) Zhang. Regularized weighted lowrank approximation. In Advances in Neural Information Processing Systems 32: AnnualConference on Neural Information Processing Systems, NeurIPS, pages 40614071,2019. 12 [CDW18]Graham Cormode, Charlie Dickens, and David P. Woodruff.Leveraging well-conditioned bases: Streaming and distributed summaries in minkowski p-norms. InProceedings of the 35th International Conference on Machine Learning, ICML, Pro-ceedings of Machine Learning Research, pages 10481056, 2018. 25 [CEM+15]Michael B. Cohen, Sam Elder, Cameron Musco, Christopher Musco, and MadalinaPersu. Dimensionality reduction for k-means clustering and low rank approximation. InProceedings of the Forty-Seventh Annual ACM on Symposium on Theory of Computing,STOC, pages 163172, 2015. 13 [CGK+17]Flavio Chierichetti, Sreenivas Gollapudi, Ravi Kumar, Silvio Lattanzi, Rina Panigrahy,and David P. Woodruff. Algorithms for lp low-rank approximation. In Proceedings ofthe 34th International Conference on Machine Learning, ICML, pages 806814, 2017.17",
  "[Cho17]Alexandra Chouldechova. Fair prediction with disparate impact: A study of bias inrecidivism prediction instruments. Big data, 5(2):153163, 2017. 1, 2": "[CKLV17]Flavio Chierichetti, Ravi Kumar, Silvio Lattanzi, and Sergei Vassilvitskii. Fair clusteringthrough fairlets. In Advances in Neural Information Processing Systems 30: AnnualConference on Neural Information Processing Systems, pages 50295037, 2017. 25 [CLS19]Michael B Cohen, Yin Tat Lee, and Zhao Song. Solving linear programs in the currentmatrix multiplication time. In Proceedings of the 51st Annual ACM Symposium onTheory of Computing (STOC), 2019. 35 [CMM17]Michael B. Cohen, Cameron Musco, and Christopher Musco. Input sparsity timelow-rank approximation via ridge leverage score sampling.In Proceedings of theTwenty-Eighth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA, pages17581777, 2017. 8 [CMV22]Eden Chlamtac, Yury Makarychev, and Ali Vakilian. Approximating fair clustering withcascaded norm objectives. In Proceedings of the 2022 annual ACM-SIAM symposiumon discrete algorithms (SODA), pages 26642683. SIAM, 2022. 6",
  "[CXZC24]Wenjing Chen, Shuo Xing, Samson Zhou, and Victoria G. Crawford. Fair submodularcover. CoRR, abs/2407.04804, 2024. 25": "[DMM06a]Petros Drineas, Michael W. Mahoney, and S. Muthukrishnan. Subspace sampling andrelative-error matrix approximation: Column-based methods. In 9th InternationalWorkshop on Approximation Algorithms for Combinatorial Optimization Problems(APPROX) and 10th International Workshop on Randomization and Computation(RANDOM), pages 316326, 2006. 8, 9 [DMM06b]Petros Drineas, Michael W. Mahoney, and S. Muthukrishnan. Subspace sampling andrelative-error matrix approximation: Column-row-based methods. In Algorithms - ESA2006, 14th Annual European Symposium, Proceedings, pages 304314, 2006. 8, 9 [DMV22]Zhen Dai, Yury Makarychev, and Ali Vakilian. Fair representation clustering withseveral protected classes. In Proceedings of the 2022 ACM Conference on Fairness,Accountability, and Transparency, pages 814823, 2022. 25 [DTV11]Amit Deshpande, Madhur Tulsiani, and Nisheeth K. Vishnoi. Algorithms and hardnessfor subspace approximation. In Proceedings of the Twenty-Second Annual ACM-SIAMSymposium on Discrete Algorithms, SODA, pages 482496, 2011. 3, 11 [EBTD20]Seyed A. Esmaeili, Brian Brubach, Leonidas Tsepenekas, and John Dickerson. Prob-abilistic fair clustering. In Advances in Neural Information Processing Systems 33:Annual Conference on Neural Information Processing Systems 2020, NeurIPS, 2020.25",
  "[GSV22]Mehrdad Ghadiri, Mohit Singh, and Santosh S Vempala. Constant-factor approximationalgorithms for socially fair k-clustering. arXiv preprint arXiv:2206.11210, 2022. 6": "[HJS+22]Baihe Huang, Shunhua Jiang, Zhao Song, Runzhou Tao, and Ruizhe Zhang. SolvingSDP faster: A robust IPM framework and efficient implementation. In 63rd IEEEAnnual Symposium on Foundations of Computer Science, FOCS, pages 233244, 2022.35 [HMV23]Sedjro Salomon Hotegni, Sepideh Mahabadi, and Ali Vakilian. Approximation algo-rithms for fair range clustering. In International Conference on Machine Learning,pages 1327013284. PMLR, 2023. 25 [HPS16]Moritz Hardt, Eric Price, and Nati Srebro. Equality of opportunity in supervisedlearning. In Advances in Neural Information Processing Systems 29: Annual Conferenceon Neural Information Processing Systems, pages 33153323, 2016. 2",
  "[JKL+20a]Haotian Jiang, Tarun Kathuria, Yin Tat Lee, Swati Padmanabhan, and Zhao Song. Afaster interior point method for semidefinite programming. In FOCS, 2020. 35": "[JKL20b]Christopher Jung, Sampath Kannan, and Neil Lutz. Service in your neighborhood:Fairness in center location. In Aaron Roth, editor, 1st Symposium on Foundations ofResponsible Computing, FORC 2020, pages 5:15:15, 2020. 25 [JLL+21]Shuli Jiang, Dennis Li, Irene Mengze Li, Arvind V. Mahankali, and David P. Woodruff.Streaming and distributed algorithms for robust column subset selection. In Proceedingsof the 38th International Conference on Machine Learning, ICML, volume 139 ofProceedings of Machine Learning Research, pages 49714981, 2021. 25 [JLS22]Arun Jambulapati, Yang P. Liu, and Aaron Sidford. Improved iteration complexitiesfor overconstrained p-norm regression. In STOC 22: 54th Annual ACM SIGACTSymposium on Theory of Computing, pages 529542, 2022. 10 [JLSW20]Haotian Jiang, Yin Tat Lee, Zhao Song, and Sam Chiu-wai Wong. An improved cuttingplane method for convex optimization, convex-concave games, and its applications. InProceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing,pages 944953, 2020. 34",
  "[JPT13]Gabriela Jeronimo, Daniel Perrucci, and Elias P. Tsigaridas. On the minimum of apolynomial function on a basic closed semialgebraic set and applications. SIAM J.Optim., 23(1):241255, 2013. 13": "[JSWZ21]Shunhua Jiang, Zhao Song, Omri Weinstein, and Hengjie Zhang. A faster algorithmfor solving general lps. In STOC 21: 53rd Annual ACM SIGACT Symposium onTheory of Computing, pages 823832, 2021. 35 [KAM19]Matthaus Kleindessner, Pranjal Awasthi, and Jamie Morgenstern.Fair k-centerclustering for data summarization. In Proceedings of the 36th International Conferenceon Machine Learning, ICML, pages 34483457, 2019. 25 [KDRZ23]Matthaus Kleindessner, Michele Donini, Chris Russell, and Muhammad Bilal Zafar.Efficient fair pca for fair representation learning. In International Conference onArtificial Intelligence and Statistics, pages 52505270. PMLR, 2023. 6",
  "[KLL+18]Jon Kleinberg, Himabindu Lakkaraju, Jure Leskovec, Jens Ludwig, and Sendhil Mul-lainathan.Human decisions and machine predictions.The quarterly journal ofeconomics, 133(1):237293, 2018. 1": "[KMM15]Matthew Kay, Cynthia Matuszek, and Sean A. Munson. Unequal representation andgender stereotypes in image search results for occupations. In Proceedings of the33rd Annual ACM Conference on Human Factors in Computing Systems, CHI, pages38193828, 2015. 2 [KMR17]Jon M. Kleinberg, Sendhil Mullainathan, and Manish Raghavan. Inherent trade-offsin the fair determination of risk scores. In 8th Innovations in Theoretical ComputerScience Conference, ITCS, pages 43:143:23, 2017. 2 [KPRW19]Ravi Kumar, Rina Panigrahy, Ali Rahimi, and David P. Woodruff. Faster algorithmsfor binary matrix factorization. In Proceedings of the 36th International Conference onMachine Learning, ICML, pages 35513559, 2019. 12",
  "[Mag10]Malik Magdon-Ismail. Row sampling for matrix algorithms via a non-commutativebernstein bound. CoRR, abs/1008.0587, 2010. 8, 9": "[MMM+22] Raphael A. Meyer, Cameron Musco, Christopher Musco, David P. Woodruff, andSamson Zhou. Fast regression for structured inputs. In The Tenth InternationalConference on Learning Representations, ICLR, 2022. 9 [MMM+23] Raphael A. Meyer, Cameron Musco, Christopher Musco, David P. Woodruff, and Sam-son Zhou. Near-linear sample complexity for Lp polynomial regression. In Proceedingsof the 2023 ACM-SIAM Symposium on Discrete Algorithms, SODA, pages 39594025,2023. 9 [MMWY22] Cameron Musco, Christopher Musco, David P. Woodruff, and Taisuke Yasuda. Activelinear regression for Lp norms and beyond. In 63rd IEEE Annual Symposium onFoundations of Computer Science, FOCS, pages 744753, 2022. 9 [MOT24]Antonis Matakos, Bruno Ordozgoiti, and Suhas Thejaswi. Fair column subset selection.In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery andData Mining, pages 21892199, 2024. 6",
  "[PVZ17]Grigoris Paouris, Petros Valettas, and Joel Zinn. Random version of dvoretzkystheorem in np. Stochastic Processes and their Applications, 127(10):31873227, 2017": "[Ren92a]James Renegar. On the computational complexity and geometry of the first-ordertheory of the reals. part i: Introduction. preliminaries. the geometry of semi-algebraicsets. the decision problem for the existential theory of the reals. Journal of symboliccomputation, 13(3):255299, 1992. 13 [Ren92b]James Renegar. On the computational complexity and geometry of the first-ordertheory of the reals. part ii: The general decision problem. preliminaries for quantifierelimination. Journal of Symbolic Computation, 13(3):301327, 1992. 13",
  "[RS18]Clemens Rosner and Melanie Schmidt. Privacy preserving clustering with constraints.In 45th International Colloquium on Automata, Languages, and Programming, ICALP,pages 96:196:14, 2018. 25": "[RSW16]Ilya P. Razenshteyn, Zhao Song, and David P. Woodruff. Weighted low rank approxi-mations with provable guarantees. In Proceedings of the 48th Annual ACM SIGACTSymposium on Theory of Computing, STOC, pages 250263, 2016. 12 [STM+18]Samira Samadi, Uthaipon Tantipongpipat, Jamie H Morgenstern, Mohit Singh, andSantosh Vempala. The price of fair pca: One extra dimension. Advances in neuralinformation processing systems, 31, 2018. 5, 22 [SW18]Christian Sohler and David P. Woodruff. Strong coresets for k-median and subspaceapproximation: Goodbye dimension. In 59th IEEE Annual Symposium on Foundationsof Computer Science, FOCS, pages 802813, 2018. 17 [TGOO22]Suhas Thejaswi, Ameet Gadekar, Bruno Ordozgoiti, and Michal Osadnik. Clusteringwith fair-center representation: Parameterized approximation algorithms and heuristics.In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery andData Mining, pages 17491759, 2022. 25 [TSS+19]Uthaipon Tantipongpipat, Samira Samadi, Mohit Singh, Jamie H Morgenstern, andSantosh Vempala. Multi-criteria dimensionality reduction with applications to fairness.Advances in neural information processing systems, 32, 2019. 6 [VVWZ23]Ameya Velingker, Maximilian Votsch, David P. Woodruff, and Samson Zhou. Fast(1 + )-approximation algorithms for binary matrix factorization. In InternationalConference on Machine Learning, ICML, pages 3495234977, 2023. 12 [VY22]Ali Vakilian and Mustafa Yalciner. Improved approximation algorithms for individuallyfair clustering. In International conference on artificial intelligence and statistics, pages87588779. PMLR, 2022. 25",
  "ASocially Fair Regression": "In this section, we present simple and intuitive algorithms for fair regression. Recall that in theclassical regression problem, the input is a data matrix A Rnd and a label matrix b Rn, andthe task is to determine the hidden vector x Rd that minimizes L(Ax b) for some loss functionL. The vector x can then be subsequently used to label future observations v through the operationv, x. Hence, regression analysis is frequently used in machine learning to infer causal relationshipsbetween the independent and dependent variables, and thus also used for prediction and forecasting.Regression and low-rank approximation share key connections and thus are often studied together,e.g., a number of sketching techniques can often be applied to both problems, resulting in significantruntime improvements due to dimensionality reduction. See .3.1 for more information.Surprisingly, we show that socially fair regression and socially fair low-rank approximation exhibitdrastically different complexities. Specifically, we show that while fair regression can be solved upto arbitrary accuracy in polynomial time for a wide variety of loss functions, even constant-factorapproximation to fair low-rank approximation requires exponential time under certain standardcomplexity hypotheses. Let be the number of groups, n1, . . . , n be positive integers and for eachi [], let A(i) Rnid and b(i) Rni. Then for a norm , we define the fair regression problemto beminxRd maxi[] A(i)x b(i).",
  "so that x produces an -approximation to the fair regression problem.Finally, note that for L2 loss, the closed form solution of x is x = (A(i))b(i) and can computedin runtime Ond1": "More generally, we can apply the same principles to observe that for any norm , Ax b i=1 A(i)x b(i) maxi[] A(i)x b(i). Thus if admits an efficient algorithm forregression, then also admits an efficient -approximation algorithm for fair regression. SeeAlgorithm 5 for reference.We now give a general algorithm for achieving additive approximation to fair regression.[AAK+22] observed that every norm is convex, since u + (1 )v u + (1 )v bytriangle inequality. Therefore, the function g(x) := maxi[]{A(i)x b(i)} is convex because themaximum of convex functions is also a convex function. Hence, the objective minx g(x) is theminimization of a convex function and can be solved using standard tools in convex optimization.[AAK+22] leveraged this observation by applying projected stochastic gradient descent. Here weuse a convex solvers based on separating hyperplanes. The resulting algorithm is quite simple andappears in Algorithm 6.",
  "We first require the following definition of a separation oracle": "Definition A.2 (Separation oracle). Given a set K Rd and > 0, a separation oracle for Kis a function that takes an input x Rd, either outputs that x is in K or outputs a separatinghyperplane, i.e., a half-space of the form H := {z | cz cx + b} K with b c2 and c = 0d.",
  "We next recall the following statement on the runtime of convex solvers given access to aseparation oracle": "Theorem A.3 ([LSW15, JLSW20]). Suppose there exists a set K that is contained in a box ofradius R and a separation oracle that, given a point x and using time T, either outputs that x is inK or outputs a separating hyperplane. Then there exists an algorithm that either finds a point in Kor proves that K does not contain a ball of radius . The algorithm uses O (dT log ) + d3 polylog runtime, for = dR",
  "Putting things together, we now have our main algorithm for fair regression:": "Theorem A.5. Let n1, . . . , n be positive integers and for each i [], let A(i) Rnid andb(i) Rni. Let = poly(n) for n = n1 + . . . + n and let (0, 1). For any norm whosesubgradient can be computed in poly(n, d) time, there exists an algorithm that outputs x d",
  "runtime": "Proof. Recall that every norm is convex, since u + (1 )v u + (1 )v by triangleinequality. Therefore, the function g(x) := maxi[]{A(i)x b(i)} is convex because the maximumof convex functions is also a convex function. Hence, the objective minx g(x) is the minimization ofa convex function and can be solved using a convex program.",
  "L minxRd maxi[] A(i)x b(i)22": "Theorem A.11. There exists an algorithm that uses poly(n + d) runtime and outputs whetherthe quadratically constrained quadratic program in Definition A.9 has a feasible solution. Heren = i=1 ni. The polynomial factor is at least 4. Proof. This directly follows using semi-definite programming solver as a black-box [JKL+20a,HJS+22]. Note that in general, quadratic programming is NP-hard to solve, however the formulationwe have, all the matrices are semi-definite matrix. Thus, it is straightforward to reduce it to anSDP and then run an SDP solver."
}