{
  "Abstract": "How can we test AI performance? This question seems trivial, but it isnt. Standardbenchmarks often have problems such as in-distribution and small-size test sets,oversimplified metrics, unfair comparisons, and short-term outcome pressure. Asa consequence, good performance on standard benchmarks does not guaranteesuccess in real-world scenarios. To address these problems, we present Touchstone,a large-scale collaborative segmentation benchmark of 9 types of abdominal organs.This benchmark is based on 5,195 training CT scans from 76 hospitals around theworld and 5,903 testing CT scans from 11 additional hospitals. This diverse test setenhances the statistical significance of benchmark results and rigorously evaluatesAI algorithms across out-of-distribution scenarios. We invited 14 inventors of 19AI algorithms to train their algorithms, while our team, as a third party, indepen-dently evaluated these algorithms. In addition, we also evaluated pre-existing AIframeworkswhich, differing from algorithms, are more flexible and can supportdifferent algorithmsincluding MONAI from NVIDIA, nnU-Net from DKFZ, andnumerous other open-source frameworks. We are committed to expanding thisbenchmark to encourage more innovation of AI algorithms for the medical domain.",
  "Introduction": "The development of AI algorithms has led to enormous progress in medical segmentation, but fewalgorithms are reliable enough for clinical use . Most AI algorithms fall short of expertradiologists, who are much more reliable and consistent when dealing with medical images frommultiple hospitals, varied in different scanners, clinical protocols, patient demographics, or diseaseprevalences . Therefore, the question remains: How can we test medical AI in thediverse scenarios that are encountered by radiologists? Establishing a trustworthy AI benchmark isimportant but exceptionally challenging, and seldom achieved in the medical domain. Tougher tests,like out-of-distribution evaluation on large, varied datasets, are needed. Standard benchmarks have underlying problems that cause confusion in algorithm comparisons anddelay progress. First, in-distribution test sets. In the medical domain, CT scans in the test set oftenshare sources, scanners, and populations with the training set. As a result, AI algorithms may performwell on the test set but generalize poorly to out-of-distribution (OOD) scenarios .For example, Xia et al. found that AI algorithms trained on data from Johns Hopkins Hospital(Baltimore, USA) lose accuracy in pancreatic tumor detection when evaluated on CT scans fromHeidelberg Medical School (Heidelberg, Germany). Second, small-size test sets. Annotating medicaldata is expensive and time-consuming, but training AI requires substantial annotated data .Therefore, most annotated data is used for training, leaving very little assigned for testing. RecentCT datasets such as TotalSegmentator , WORD , and MSD , offered fewer than 100 CTscans for testing. Even a single success or failure can skew results, reducing the statistical power andpotentially misleading conclusions. Third, over-simplified metrics. Most standard benchmarks onlycompare average performance, failing to identify each AI algorithms strengths and weaknesses indifferent scenarios. For instance, one algorithm might excel at segmenting small, circular structures(like the gall bladder) while another performs better on long, tubular ones (such as the aorta). Averageperformance across many classes can hide these nuances. Fourth, unfair comparisons. Almost everypaper reports that the newly proposed AI outperforms existing alternative AIs. The improvementbecomes more significant if alternative AIs are reproduced and evaluated on an unknown training/testsplit. There are biases in comparison due to asymmetric efforts made in optimizing the proposedand alternative AIs. Many independent studies have reported these comparison biases over the years but remain unresolved. There is a need to have more widely adopted benchmarks (e.g.,challenges) where all AI algorithms are trained by their inventors and evaluated by third parties.Fifth, short-term outcome pressure. Standard benchmarks are often in short-term and non-recurring,requiring a final solution within several months. For example, RSNA 2024 Abdominal TraumaDetection only opened for three months for data access and AI development & evaluation. Theshort-term outcome pressure can discourage new classes of AI algorithms that need considerabletime and computational resources for a thorough investigation, as their vanilla versions (e.g., Mamba in early 2024 and Transformers in early 2021) might not outperform all the alternativesjudged. The benchmark must have long-term commitment and allowance. To address this AI mismeasurement issue, we present the Touchstone benchmark, an effort towardsthe objective of creating a fair, large-scale, and widely-adopted medical AI benchmark. Its scaleis large, featuring a training set of 5,195 publicly available CT scans from 76 hospitals and a testset of 5,903 CT scans from additional 11 hospitals. Test sets were unknown to the participants ofthe benchmark. All 11,098 scans are annotated per voxel for 9 anatomical structures. The trainingset annotations were created by collaboration between AI specialists and radiologists followed bymanual revision , 5,160 out of 5,903 test scans are proprietary and manually annotated, and theremaining test datasets are publicly available, annotated by AI-radiologist collaboration. As of May2024, 14 global teams from eight countries have contributed to our benchmark. These teams areknown for inventing novel AI algorithms for medical segmentation. In summary, the Touchstonebenchmark explores an evaluation philosophy defined by the following five contributions: 1. Evaluating on out-of-distribution data: The JHH test set (Sec. 2.1) presents 5,160 CT scansfrom an hospital never seen during training, introducing a new scale of external validationfor abdominal CT benchmarks. The test data distribution varies in contrast enhancement(pre, venous, arterial, post-phases), disease condition (30% containing abdominal tumorsat varied stages), demographics (age, gender, race), image quality (e.g., slice thickness of0.51.5 mm), and scanner types. We have collected metadata information for 72% of thetest set (N=5,160) and reported AI performance in each sub-group. 2. Providing a large test set: Our test set (N=5,903) is much larger than the test sets of allcurrent public CT benchmarks combined. It can enhance the statistical significance ofthe benchmark results: a 1% average accuracy increment across 5,000 CT scans is moreindicative of a genuine algorithmic improvement than a 1% variation across 50 CT scans. 3. Analyzing pros/cons from multiple perspectives: We evaluated segmentation performanceof 9 anatomical structures, comparing the average results and analyzing them by metadatagroups. We also reported per-class algorithm rankings and visualized worst-case perfor-mance. Moreover, we assessed inference time and computational cost, key factors for theclinical deployment of AI algorithms. 4. Inviting inventors to train their own algorithms: Each AI algorithm is configured by its owninventors, who know it best and have the most interest in its success. In our benchmark,each inventor trained their AI algorithm on 5,195 annotated CT scans in AbdomenAtlas, and we, as a third party, independently evaluated these algorithms on 5,903 CT scansthat are unknown and inaccessible to the AI inventors. This setting protects the integrity ofour results (i.e., precluding the use of test data for hyperparameter tuning). 5. Evaluating new algorithms with long-term commitment: Our Touchstone benchmark not onlyinvited established AI algorithms that are already published in major conferences/journals,but also invited newly developed algorithms appearing in recent pre-prints. We have along-term commitment to this benchmark by organizing recurring challenges for at leastfive years, curating larger datasets, and improving label quality and task diversity. The firstedition was featured as an invitation-only challenge at ISBI-2024. Related benchmarks/challenges & our innovations. In a general sense, we define a benchmarkas an algorithmic comparison. Accordingly, the most common type of benchmark are the standardcomparisons found in thousands of research papers where authorspresent new algorithms and compare baselines. As previously explained, this type of benchmarkincurs the risk of unfairness, due to possible asymmetric efforts made in optimizing the proposed andalternative algorithms. However, open challenges are a different type of benchmark, where developerstrain their own algorithms and submit them for third-party evaluation, mitigating the risk of unfaircomparisons. For this reason, contrasts our Touchstone benchmark to a non-exhaustivelist of the most influential abdominal CT segmentation challenges. Notably, our training dataset isconsiderably larger and comes from more hospitals than any CT dataset ever used in a challenge.Furthermore, the only challenge training datasets on a scale similar to AbdomenAtlas 1.0 have partiallabels and/or unlabeled portions . Our dataset is 17.3 larger than the second-largest fully-annotated CT dataset in . Boosting our results statistical significance, our evaluationdataset is 8.6 larger than any CT segmentation challenge test dataset. Moreover, Touchstone is theonly benchmark in to, simultaneously, explicitly analyze the performance of AI algorithmscontrolled by age, sex, race, and other metadata information. Lastly, this work is the starting point ofa long-term benchmark, which we commit to maintain and improve over the years. Considering theimportance of long-term commitment, we must acclaim KiTS, an abdominal segmentation challengethat had 3 editions since 2019 and FLARE, a challenge being consistently held yearlysince 2021 .",
  "Datasets Annotations, Statistics, Distribution, & Characteristics": "We used one training dataset and two test datasets to perform a comprehensive out-of-distributionbenchmark. The training and test datasets were collected from many hospitals worldwide. shows the demographics of the two test datasets, JHH and TotalSegmentator; Appendix Figures34 provide examples of CT scans and per-voxel annotations for various demographic groups acrossall datasets. The JHH dataset is proprietary and used for third-party evaluation; participants do nothave access to the CT scans or their annotations. TotalSegmentator is a publicly available dataset;we did not inform the inventors beforehand of its use in our evaluation and confirmed that their AIalgorithms had not been trained on this dataset. We included this public dataset to enable futureparticipants to easily compare their algorithms with our benchmark.",
  "Race": ": Summary of JHH and TotalSegmentator metadata. The diversity of data distributionincludes more than just the number of centers; it also includes age, sex, manufacturer, diagnosis, andmany other factors. JHH is the only dataset that provides race information, allowing us to comparethe results; the race information is unknown in TotalSegmentator and most publicly available datasets.Therefore, the inclusion of JHH is value-added because it enabled the analysis on race. human-in-the-loop active learning strategy to empower radiologists to feasibly annotate 5,195 CTscans from 16 public datasets (listed in Appendix ) and is fully annotated for 9 anatomicalstructures, i.e., spleen, liver, L&R kidneys, stomach, gallbladder, pancreas, aorta, and postcava.AbdomenAtlas 1.0, under CC BY-NC 4.0 License, is derived from publicly available datasets, sodetailed metadata information is unfortunately not available. JHHN=5,160; reserved for out-of-distribution test purposes1provides contrast-enhanced CTscans in venous and arterial phases. Collected from Johns Hopkins Hospital using two Siemensscanners, this dataset includes metadata on age, race, gender, and diagnosis. Notably, all per-voxelannotations in JHH were manually created by radiologists . Annotation time for a singlestructure ranges from minutes to hours, depending on the size and complexity of the regions ofinterest to annotate and the local surrounding anatomical structures. Each CT scan was annotated by 1Out-of-distribution (OOD) test data (both images and annotations) must remain private, as public releasecan lead to overfitting and compromise OOD evaluation integrity . If any OOD data is released, a new,privately preserved test set will be required to ensure reliable evaluation. a team of radiologists, and confirmed by one of three additional experienced radiologists to ensurethe quality of the annotation. All personally identifiable information was removed and the use ofthis dataset has received IRB approval from Johns Hopkins Medicine under IRB00403268. JHH isconsidered here an OOD test set because no CT scan from the Johns Hopkins hospital is present inthe training dataset. TotalSegmentatorV2N=743; publicly available for out-of-distribution test purposesis from 10institutes within the University Hospital Basel (Switzerland) picture archiving and communicationsystem (PACS) . Being one of the largest public CT datasets, TotalSegmentator, under ApacheLicense 2.0, was annotated by AI-assisted radiologists. It comprises both contrast-enhanced andnon-contrast images, with per-sample metadata including age, sex, scanner details, diagnosis, andinstitution. We report AI performance on a subset of TotalSegmentator dataset2 in and itsofficial test set in Appendix Tables 1112.",
  "Evaluation Protocols Architectures, Frameworks, Metrics, & Statistical Analysis": "In this study, we define an architecture as the overall design and structure of the entire neural networkmodel; and define a framework as a set of tools or protocols that can accommodate multiple AIarchitectures. We evaluated 19 architectures and 3 frameworks trained by their inventors on ourAbdomenAtlas 1.03. We used Dice Similarity Coefficient (DSC) and Normalized Surface Distance(NSD) to evaluate segmentation performance. We enforced that the inference speed must be fasterthan 1e6 mm3 per second. The inference speed for each algorithm is summarized in Appendix. We employed the same computer to evaluate all submitted algorithms. Its specifications areCPU: AMD EPYC 7713 @ 2,0Ghz64; GPU: NVIDIA Ampere A100 (80GB); RAM: 2TB. Weapplied statistical hypothesis testing to each possible pair of algorithms to ensure their performancedifferences are significant. Following Wiesenfarth et al. , we used the one-sided Wilcoxon signedrank test with Holms adjustment for multiplicity at 5% significance level and summarized resultsin significance maps. Per-group metadata analysis in Appendix D.5 considers KruskalWallis tests,followed by post-hoc Mann-Whitney U Tests with Bonferroni correction. More statistical analyses,such as ranking stability , are presented in Appendix D.2.",
  "Performances According to Out-of-distribution Evaluation on Large Datasets": "We started by comparing the average DSC score over the 9 classes. For architectures, MedNeXt, STU-Net, and MedFormer are the winners of the JHH dataset; STU-Net and ResEncL are the winners of theTotalSegmentator dataset. Among these winners, three are CNNs (STU-Net, ResEncL and MedNeXt)and one is a CNN Transformer hybrid (MedFormer). There is no significant difference among thesewinners at p = 0.05 level, evidenced by the statistical analysis in Tables 23. Regarding frameworks,nnU-Net is the winner since 3 out of 4 of the aforementioned winners were developed on theself-configuring nnU-Net framework. In addition to reporting the average performance ranking, we examined the per-class performance andmade the following findings. First, diversified OOD evaluation is necessary. For multiple algorithms,the DSC score for a given organ varied 15% or more across diverse test sets. E.g., the SAM-Adapter,a transformer-based 2D model, generalizes much better to JHH than to TotalSegmentator: in kidneysegmentation, its DSC score differs by more than 80% across the datasets (see Appendix D.3.5 forexplanations). Such stark performance variations reveal the importance of evaluating models ondiverse OOD test sets. Second, test dataset size matters. More test samples increase statistical power,enabling benchmarks to more reliably detect differences between algorithms and produce stable,trustworthy rankings. Higher statistical power allows us to better differentiate the best performingmodel from the others: for JHH (N=5,160), there is at most two winners for any class, but for 2TotalSegmentator offers 1,228 CT scans, but 485 scans were previously incorporated into FLARE andsubsequently inherited by AbdomenAtlas 1.0. As a result, we used only the remaining 743 scans for evaluation.Unlike JHH, this evaluation set does not come from completely unseen hospitals. However, there is a significantdistribution shift between the TotalSegmentator data within AbdomenAtlas and the data in our test set (seeAppendix A.2).3Appendix B.1B.3 describe in-depth the description and configuration of each architecture/framework. :External validation on proprietary JHH dataset (N=5,160). Performance is givenas DSC score (means.d.). For each class, we bold the best-performing results and highlight therunners-up, which show no significant difference from the best results at p = 0.05 level, in red.Architectures are grouped by their frameworks and sorted in ascending order based on the number ofparameters. CNNs based on the nnU-Net framework have the best performance on most classes, butother models excel at specific structures (e.g., the graph neural network-based NeXToU for aorta, andthe diffusion-based Diff-UNet for kidneys). The NSD results are reported in Appendix .",
  "These architectures were pre-trained (Appendix B.3).These architectures were trained on AbdomenAtlas 1.0 with enhanced label quality for the aorta and kidney classes (discussed in 4)": "TotalSegmentator, there is up to five (Tables 23). Appendix D.4 uses box-plots and significanceheatmaps to confirm these findings, and Appendix D.2 shows ranking order is much more stablefor JHH than for smaller test sets. This finding emphasizes the importance of test dataset size foraccurate and reliable algorithm comparisons. Third, average-based rankings are not enough. Tables23 show that, for the same AI algorithm, DSC scores on difficult-to-segment structures, like thegallbladder and the pancreas, are usually 1020% lower than performance on easily identifiablestructures, like the liver and the spleen. Usually, the best models for average DSC are also the best atindividual structures, but per-class results reveal notable exceptions. E.g., in JHH, NexToU, a graphneural network-based hybrid architecture, is significantly superior at aorta segmentation, and Diff-UNet, a diffusion-based model, significantly excels at kidney segmentation. Accordingly, per-classresults reveal hidden strengths of AI algorithms. For a more comprehensive evaluation, Appendix reports inference speed of each algorithm, and Appendix C analyzes performance measured by : Validation on TotalSegmentator (N=743). Performances given as DSC score (means.d.).For each class, we bold the best-performing results and highlight the runners-up, which show nosignificant difference from the best results at p = 0.05 level, in red. To ease the direct comparisonwith other literature, we also reported the official test set performance in Appendix Tables 1112.",
  "n/aSAM-Adapter 11.6M48.430.915.218.64.88.130.921.723.119.7MedFormer 38.5M80.423.670.328.070.024.472.527.975.124.1Diff-UNet 434.0M73.429.761.034.560.733.369.729.762.531.8": "These architectures were pre-trained (Appendix B.3).The class IVC (inferior vena cava) shares the same meaning as the class postcava in other datasets (e.g., AbdomenAtlas 1.0 and JHH).These architectures were trained on AbdomenAtlas 1.0 with enhanced label quality for the aorta and kidney classes (discussed in 4). NSD scores. Fourth, inviting innovation is important. As in past 3D medical segmentation challenges, CNNs with the nnU-Net framework showed strong performance in our benchmark. However,by searching for innovative algorithms, sending target invitations to their inventors, and performingcomprehensive evaluations, we could reveal strengths of new and less well known models, suchas vision-language algorithms and Diff-UNet, the first 3D medical image segmentation methodbased on diffusion models, and MedFormer, a hybrid architecture that combines convolutionalinductive bias with efficient, scalable bidirectional multi-head attention. Meanwhile, the LHU-Net, ahybrid architecture combining CNN and transformer attention mechanisms, excels in computationalefficiency: it is 2 to 4 times faster than models with similar accuracy (see Appendix B.3).",
  "We leveraged the metadata available in test datasets to assess AI performance consistency acrossdiverse demographic groups. We studied correlation between AI performance and the five types of": "ages 18-29 ages 30-39 ages 40-49 ages 50-59 ages 60-69 ages 70-79 ages 80-89 ages 90-99 ages 0.00.20.40.60.81.0 institute B institute E institute C institute A institute I institute G institute 0.00.20.40.60.81.0 other unclear vascular no pathology inflammation tumor bleeding trauma ** * **** **** *** *** diagnosis 0.00.20.40.60.81.0 male female sex 0.00.20.40.60.81.0",
  "Siemens": "* manufacturer 0.81.0 ages 16-29 ages 30-39 ages 40-49 ages 50-59 ages 60-69 ages 70-79 ages 80-89 *** **** ** **** **** **** ******* **** **** ** **** **** ** ages 0.81.0 negative cancer **** cancer diagnosis 0.81.0 male female **** sex 0.81.0 race O race HL race W race AS race AA race U **** **** * race : Potential confounders significantly impact AI performance. Boxplots showing theaverage DSC score of nine classes and 19 algorithms for diverse demographic groups in two OODtest sets: TotalSegmentator and JHH. Whiskers indicate 1.5IQR (interquartile range). Statisticalsignificance is indicated by stars: p < 0.05, p < 0.01, p < 0.001, p < 0.0001. Weperform KruskalWallis tests followed by post-hoc Mann-Whitney U Tests with Bonferroni correction.Greater performance differences are observed in the JHH dataset compared to TotalSegmentator,likely due to the larger number of CT scans. Differences are apparent across demographic groupssuch as age, diagnoses, scanner manufacturer, sex, and medical institutions.",
  "metadata: age, sex, and diagnosis are analyzed on all two datasets while race and manufacturer areonly analyzed on one dataset, JHH, since most public test sets lack this information": "displays per-group DSC for an average AI model, i.e., the average performance across our 19evaluated algorithms. The statistical analysis further highlights the need for large test datasets: JHHslarge sample size (N=5,160) allows detection of statistically significant DSC differences across allmetadata, but some of these differences (for age and sex) are noticeable but not significant in thesmaller TotalSegmentator dataset. Notably, AI performance reduces for advanced age. Median DSCstarts dropping around the fifties. JHH shows multiple statistically significant performance drops afterthis age. The creators of the dataset observed that aging caused attenuation in CT scans , whichmay explain the common descending DSC trend after age 50, despite the fact that the 60-69 age group is the most populous in most datasets (). This trend exists for all tested AI algorithms(Appendix D.5 displays per-group performances for each algorithm and organ). Sex only significantlyconfounds some AI algorithms. The median DSC is significantly smaller for women in all datasets.However, multiple top-performing models show no significant performance difference across sexesin any dataset (e.g., nnU-Net, STU-Net, and Diff-UNet), showcasing current AI can be robust tothis confounder. We found significant performance differences for diverse races. AI performancefor white patients was significantly superior to the performance for African Americans, showing theneed to increase the presence of this demographic group in public CT scan datasets. Again, many ofthe best performing algorithms did not present statistically significant differences for the two races(Appendix D.5). In all datasets, diagnosis significantly impacted AI performance. Cancer patientshave significantly smaller DSC scores in JHH (p < 0.0001), and trauma patients present median DSCscores below other groups in TotalSegmentator. Scanner manufacturer changes cause significantDSC differences (p < 0.05) in TotalSegmentator.",
  "Conclusion & Discussion": "Conclusion. Are we on the right way for evaluating AI algorithms for medical segmentation? Thispaper outlines five properties of an ideal benchmark: (I) diverse data distribution in both training andtest datasets, (II) a large number of test samples, (III) varied evaluation perspectives, (IV) equitablyoptimized AI algorithms, and (V) a long-term commitment. Touchstone sets itself apart from previousbenchmarks in these criteria, enabling us to share unique insights that often missing in standardbenchmarks. Our findings indicate: (1) AI performance can vary significantly across different datasets,with per-class differences of 1020% common, and up to 80% observed (SAM-Adapter in kidney);thus, out-of-distribution evaluation across multiple datasets is crucial for ensuring AIs reliability andclinical adoption. (2) Larger test datasets reveal more significant differences between AI algorithms,allowing for meaningful rankings and nuanced analyses. (3) Average rankings can obscure AIsspecific strengths; per-organ and metadata analysis is crucial in highlighting the benefits of innovativevision-language algorithms and the first diffusion-based 3D medical segmentation model. (4) Byevaluating diverse AI architectures trained by their inventors, we establish a fair reference point forfuture development, which Touchstone will continually support with a long-term commitment. Label Noise in Training Set. There is no perfect ground truth in segmentation datasets (exceptfor synthetic data ), especially in the abdominal region where anatomicalboundaries can be blurry due to disease or age (examples in Appendix A.3). Identifying these bound-aries is challenging for both human annotators and AI algorithms. Many recent datasets, includingTotalSegmentator and AbdomenAtlas 1.0 , use human-in-the-loop strategies, combiningAI-predicted annotations and manual annotations by radiologists, which inevitably contain labelerrors. The errors in AbdomenAtlas 1.0 arise from poor CT image quality (e.g., BDMAP_00000339,BDMAP_00001044, BDMAP_00003725), mistakes in AI predictions but not revised by humans,and inconsistency in label standards across the public datasets incorporated into AbdomenAtlas 1.0. With the feedback from our benchmark participants, we can partially detect these label errors,primarily in the aorta (32.4%), a structure with high annotation standard inconsistency in public data(e.g., in BTCV and FLARE) , and in the L&R kidneys (2.6%). We revised AbdomenAtlas1.0 by reducing label errors in the aorta to 5.4% and in the L&R kidneys to 0.6%. A ResEncL trainedon the revised AbdomenAtlas 1.0 showed statistically significant performance gains in the aorta, butgains for kidneys were small and not always statistically significant (see Tables 23). These resultshighlight that current AI may be resistant to moderate levels of label noise (2.6%), but not to highlevels (32.4%), as we detail in Appendix E. As future work, an improved label error detector willbe a valuable tool for automatically assessing the quality of publicly available datasets and quicklyimproving quality through human annotation based on detected errors. High-Quality, Proprietary Test Set. Having JHH (N=5,160) available for third-party evaluation is abig plus for OOD benchmarks. It was completely annotated by radiologists, manually and followinga well-defined annotation standard, for several years . Thus, it can serve as a gold standard for ourbenchmark. The fact that JHH is a private dataset has both problems and benefits. It can significantlyincreases feedback time for AI performance evaluation, as it requires additional procedures to submitthe AI to a third party, set it up, and run it on over 5,000 CT scans. If a benchmark takes too muchwork to run, it will not gain wide traction. But making test set (either images or annotations) publiclyavailable can cause more problemsincluding completely destroying the OOD benchmark. For example, Medical Segmentation Decathlon (MSD) was a benchmark with publicly accessible testimages and its test annotations were private. Similarly, BTCV released both testing images andannotations. However, due to the growing need for more annotated data in the medical domain, evenMSD/BTCV test sets have been annotated and integrated into recent public datasets, like FLARE and AbdomenAtlas . Therefore, any AI models trained or pre-trained on thesepublic datasets are problematic in the MSD/BTCV leaderboard. With widespread access to testdata, it becomes challenging to fairly compare models, as some may be overly optimized for thebenchmark rather than for real-world performance. As a result, researchers must continue to seek ordevelop new datasetspreferably with images and annotations that have never been disclosed. Thisis critical in many fields as well. Yann Lecunbeware of testing on the training setin response tothe incredible results achieved by GPT. Therefore, our proprietary JHH dataset is a valuable resourcethat other researchers can exploit to reduce data leakage risks and improve the reliability of OODbenchmark results. Our Touchstone Benchmark is still in the initial stage, so we are very careful withthe decision of releasing JHH images/annotations. It must be managed carefully to ensure its benefitsoutweigh the risks. Per-Group Metadata Analysis. Our study underscores the need for detailed metadata for algorithmicbenchmark, which is currently a big limitation in the medical domain. Evidenced by , onlyKiTS & FLARE provided metadata analysis on sex, age, and/or race. Our Touchstone not onlyprovides more extensive metadata analyses, including diagnosis, but also offers an order of magnitudemore test data (N=5,903) for benchmarking. We have analyzed AI performance by metadata such assex, age, and race but realized that a more rigorous analysis could be based on combined criteria (e.g.,white females aged 3040). Therefore, in the next round of benchmarking, instead of only providingaverage performance per class, we will also offer participants per-case performance along with eachcases metadata information. This approach will provide a richer understanding of the pros/cons ofAI algorithms and potentially stimulate AI innovation. Architectural Insights. In Appendix D.3, we have provided architectural comparison of boththe top-ranking and bottom-ranking algorithms. But we find it difficult to extract trustworthyarchitectural insights directly from our current benchmark results. For example, Tables 23 show thattop performing models in our benchmark are usually CNNs within the nnU-Net framework. However,it is unclear if this is due to an intrinsic advantage of CNNs over Transformers or just an indication ofnnU-Nets superior pipeline configuration. Given that Transformers are newer, future frameworks,designed for them, could potentially enhance their performance. I.e., mature frameworks that extractthe best from both CNNs and transformers should allow fairer architectural comparisons in the future.Beyond medical imaging, the architectural debate between CNNs and Transformers in computervision has been ongoing and remains unresolved . Our benchmark provides predictions-onlyresults, which can be heavily influenced by many factors such as preprocessing, data augmentation,post-processing, and training hyper-parameters . To draw convincing architectural insights,extensive ablation studies under controlled settings are required. However, conducting ablationstudies for all 19 AI algorithms would be extremely costly for us. We anticipate further insights anddetails from the AI inventors upcoming technical reports, including extensive ablation studies. Weare also happy to assist the inventors in their ablation studies by providing feedback on the OODevaluation results of their algorithm variants. With the success of the first edition of Touchstone Benchmark, we are actively pursuing multi-center,OOD datasets, to further enhance the benchmark. This is difficult for many well-known reasonspatient privacy, ethical compliance, data annotation, intellectual property, etc. Rome wasnt builtin a day. A multi-center, OOD dataset can never be made without accumulating the contributionof every single-center dataset. We hope this benchmark initiative at Johns Hopkins University, ahighly regarded institution, could also inspire more institutes to contribute their private datasets forthird-party OOD evaluation.",
  "Acknowledgements and Disclosure of Funding": "This work was supported by the Lustgarten Foundation for Pancreatic Cancer Research and the PatrickJ. McGovern Foundation Award. We gratefully acknowledge the Data Science and ComputationFacility and its HPC Support Team at Fondazione Istituto Italiano di Tecnologia. P.R.A.S.B. thanksthe funding from the Center for Biomolecular Nanotechnologies, Istituto Italiano di Tecnologia(73010, Arnesano, LE, Italy). A.C. and S.D. thank the funding from the Istituto Italiano di Tecnologia(16163, Genova, GE, Italy). Z.W. and M.B.B. acknowledge support from the Research Foundation -Flanders (FWO) through project numbers G0A1319N and S001421N, and funding from the FlemishGovernment under the Onderzoeksprogramma Artificile Intelligentie (AI) Vlaanderen programme.Z.W. and M.B.B. acknowledge LUMI-BE for awarding this project access to the LUMI supercomputer,owned by the EuroHPC JU, hosted by CSC (Finland) and the LUMI consortium, and EuroHPC JU forawarding this project access to the Leonardo supercomputer, hosted by CINECA. Y.S., A.B., P.K., R.A.and D.M. acknowledge the scientific support and HPC resources provided by the Erlangen NationalHigh-Performance Computing Center (NHR@FAU) of the Friedrich-Alexander-Universitt Erlangen-Nrnberg (FAU) under the NHR project \"DeepNeuro - Exploring novel deep learning approachesfor the analysis of diffusion imaging data.\" NHR funding is provided by federal and Bavarian stateauthorities. NHR@FAU hardware is partially funded by the German Research Foundation (DFG) 440719683. Part of this work was funded by Helmholtz Imaging (HI), a platform of the HelmholtzIncubator on Information and Data Science. This work is partially funded by NSFC-62306046. Wethank Thomas Brox for supporting the benchmark of the U-Net architecture. We thank Di Liang for providing consultant of the statistical analysis in this benchmark; thank XiaoxiChen for analyzing AI predictions; thank Seth Zonies and Andrew Wichmann for providing legaladvice on the release of AbdomenAtlas 1.0. The content of this paper covered by patents pending.",
  "Alekh Agarwal, Alina Beygelzimer, Miroslav Dudk, John Langford, and Hanna Wallach. A reductionsapproach to fair classification. In International conference on machine learning, pages 6069. PMLR,2018": "Michela Antonelli, Annika Reinke, Spyridon Bakas, Keyvan Farahani, Bennett A Landman, Geert Lit-jens, Bjoern Menze, Olaf Ronneberger, Ronald M Summers, Bram van Ginneken, et al. The medicalsegmentation decathlon. arXiv preprint arXiv:2106.05735, 2021. Diego Ardila, Atilla P Kiraly, Sujeeth Bharadwaj, Bokyung Choi, Joshua J Reicher, Lily Peng, DanielTse, Mozziyar Etemadi, Wenxing Ye, Greg Corrado, et al. End-to-end lung cancer screening with three-dimensional deep learning on low-dose chest computed tomography. Nature medicine, 25(6):954961,2019.",
  "Yutong Bai, Jieru Mei, Alan L Yuille, and Cihang Xie. Are transformers more robust than cnns? Advancesin neural information processing systems, 34:2683126843, 2021": "Ujjwal Baid, Satyam Ghodasara, Michel Bilello, Suyash Mohan, Evan Calabrese, Errol Colak, KeyvanFarahani, Jayashree Kalpathy-Cramer, Felipe C Kitamura, Sarthak Pati, et al. The rsna-asnr-miccaibrats 2021 benchmark on brain tumor segmentation and radiogenomic classification. arXiv preprintarXiv:2107.02314, 2021. Imon Banerjee, Kamanasish Bhattacharjee, John L Burns, Hari Trivedi, Saptarshi Purkayastha, LalehSeyyed-Kalantari, Bhavik N Patel, Rakesh Shiradkar, and Judy Gichoya. shortcuts causing bias inradiology artificial intelligence: causes, evaluation and mitigation. Journal of the American College ofRadiology, 2023. Pedro RAS Bassi, Sergio SJ Dertkigil, and Andrea Cavalli. Improving deep neural network general-ization and robustness to background bias via layer-wise relevance propagation optimization. NatureCommunications, 15(1):291, 2024. Patrick Bilic, Patrick Ferdinand Christ, Eugene Vorontsov, Grzegorz Chlebus, Hao Chen, Qi Dou, Chi-WingFu, Xiao Han, Pheng-Ann Heng, Jrgen Hesser, et al. The liver tumor segmentation benchmark (lits).arXiv preprint arXiv:1901.04056, 2019. Kai Cao, Yingda Xia, Jiawen Yao, Xu Han, Lukas Lambert, Tingting Zhang, Wei Tang, Gang Jin, HuiJiang, Xu Fang, et al. Large-scale pancreatic cancer detection via non-contrast ct and deep learning. NatureMedicine, pages 111, 2023. M Jorge Cardoso, Wenqi Li, Richard Brown, Nic Ma, Eric Kerfoot, Yiheng Wang, Benjamin Murrey,Andriy Myronenko, Can Zhao, Dong Yang, et al. Monai: An open-source framework for deep learning inhealthcare. arXiv preprint arXiv:2211.02701, 2022. Jieneng Chen, Yongyi Lu, Qihang Yu, Xiangde Luo, Ehsan Adeli, Yan Wang, Le Lu, Alan L Yuille, andYuyin Zhou. Transunet: Transformers make strong encoders for medical image segmentation. arXivpreprint arXiv:2102.04306, 2021.",
  "Qi Chen, Yuxiang Lai, Xiaoxi Chen, Qixin Hu, Alan Yuille, and Zongwei Zhou. Analyzing tumors bysynthesis. arXiv preprint arXiv:2409.06035, 2024": "Errol Colak, Hui-Ming Lin, Robyn Ball, Melissa Davis, Adam Flanders, Sabeena Jalal, Kirti Magudia,Brett Marinelli, Savvas Nicolaou, Luciano Prevedello, Jeff Rudie, George Shih, Maryam Vazirabad, andJohn Mongan. Rsna 2023 abdominal trauma detection, 2023. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, ThomasUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image isworth 16x16 words: Transformers for image recognition at scale. International Conference on LearningRepresentations, 2020. Shiyi Du, Xiaosong Wang, Yongyi Lu, Yuyin Zhou, Shaoting Zhang, Alan Yuille, Kang Li, and ZongweiZhou. Boosting dermatoscopic lesion segmentation via diffusion models with visual and textual prompts.In 2024 IEEE International Symposium on Biomedical Imaging (ISBI), pages 15. IEEE, 2024.",
  "Yuxin Du, Fan Bai, Tiejun Huang, and Bo Zhao. Segvol: Universal and interactive volumetric medicalimage segmentation. arXiv preprint arXiv:2311.13385, 2023": "Yunhe Gao, Mu Zhou, Di Liu, Zhennan Yan, Shaoting Zhang, and Dimitris N Metaxas. A data-scalabletransformer for medical image segmentation: architecture, model efficiency, and benchmark. arXiv preprintarXiv:2203.00131, 2022. Sergios Gatidis, Tobias Hepp, Marcel Frh, Christian La Fougre, Konstantin Nikolaou, Christina Pfannen-berg, Bernhard Schlkopf, Thomas Kstner, Clemens Cyran, and Daniel Rubin. A whole-body fdg-pet/ctdataset with manually annotated tumor lesions. Scientific Data, 9(1):601, 2022. Robert Geirhos, Jrn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, MatthiasBethge, and Felix A Wichmann. Shortcut learning in deep neural networks. Nature Machine Intelligence,2(11):665673, 2020.",
  "Kai Han, Yunhe Wang, Jianyuan Guo, Yehui Tang, and Enhua Wu. Vision gnn: An image is worth graphof nodes. Advances in neural information processing systems, 35:82918303, 2022": "Ali Hatamizadeh, Yucheng Tang, Vishwesh Nath, Dong Yang, Andriy Myronenko, Bennett Landman,Holger R Roth, and Daguang Xu. Unetr: Transformers for 3d medical image segmentation. In Proceedingsof the IEEE/CVF winter conference on applications of computer vision, pages 574584, 2022. Ali Hatamizadeh, Ziyue Xu, Dong Yang, Wenqi Li, Holger Roth, and Daguang Xu. Unetformer: A unifiedvision transformer model and pre-training framework for 3d medical image segmentation. arXiv preprintarXiv:2204.00631, 2022. Yufan He, Dong Yang, Holger Roth, Can Zhao, and Daguang Xu. Dints: Differentiable neural networktopology search for 3d medical image segmentation. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, pages 58415850, 2021. Nicholas Heller, Fabian Isensee, Klaus H Maier-Hein, Xiaoshuai Hou, Chunmei Xie, Fengyi Li, YangNan, Guangrui Mu, Zhiyong Lin, Miofei Han, et al. The state of the art in kidney and kidney tumorsegmentation in contrast-enhanced ct imaging: Results of the kits19 challenge. Medical Image Analysis,67:101821, 2021. Nicholas Heller, Fabian Isensee, Dasha Trofimova, Resha Tejpaul, Zhongchen Zhao, Huai Chen, LishengWang, Alex Golts, Daniel Khapun, Daniel Shats, Yoel Shoshan, Flora Gilboa-Solomon, Yasmeen George,Xi Yang, Jianpeng Zhang, Jing Zhang, Yong Xia, Mengran Wu, Zhiyang Liu, Ed Walczak, Sean Mc-Sweeney, Ranveer Vasdev, Chris Hornung, Rafat Solaiman, Jamee Schoephoerster, Bailey Abernathy,David Wu, Safa Abdulkadir, Ben Byun, Justice Spriggs, Griffin Struyk, Alexandra Austin, Ben Simpson,Michael Hagstrom, Sierra Virnig, John French, Nitin Venkatesh, Sarah Chan, Keenan Moore, AnnaJacobsen, Susan Austin, Mark Austin, Subodh Regmi, Nikolaos Papanikolopoulos, and ChristopherWeight. The kits21 challenge: Automatic segmentation of kidneys, renal tumors, and renal cysts incorticomedullary-phase ct, 2023. Nicholas Heller, Sean McSweeney, Matthew Thomas Peterson, Sarah Peterson, Jack Rickman, BethanyStai, Resha Tejpaul, Makinna Oestreich, Paul Blake, Joel Rosenberg, et al. An international challengeto use artificial intelligence to define the state-of-the-art in kidney and kidney tumor segmentation in ctimaging., 2020. Nicholas Heller, Niranjan Sathianathen, Arveen Kalapara, Edward Walczak, Keenan Moore, HeatherKaluzniak, Joel Rosenberg, Paul Blake, Zachary Rengel, Makinna Oestreich, et al. The kits19 challengedata: 300 kidney tumor cases with clinical context, ct semantic segmentations, and surgical outcomes.arXiv preprint arXiv:1904.00445, 2019. Qixin Hu, Yixiong Chen, Junfei Xiao, Shuwen Sun, Jieneng Chen, Alan L Yuille, and Zongwei Zhou. Label-free liver tumor segmentation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition,pages 74227432, 2023. Ziyan Huang, Zhongying Deng, Jin Ye, Haoyu Wang, Yanzhou Su, Tianbin Li, Hui Sun, Junlong Cheng,Jianpin Chen, Junjun He, et al. A-eval: A benchmark for cross-dataset evaluation of abdominal multi-organsegmentation. arXiv preprint arXiv:2309.03906, 2023. Ziyan Huang, Haoyu Wang, Zhongying Deng, Jin Ye, Yanzhou Su, Hui Sun, Junjun He, Yun Gu, Lixu Gu,Shaoting Zhang, et al. Stu-net: Scalable and transferable medical image segmentation models empoweredby large-scale supervised pre-training. arXiv preprint arXiv:2304.06716, 2023. Fabian Isensee, Paul F Jaeger, Simon AA Kohl, Jens Petersen, and Klaus H Maier-Hein.nnu-net:a self-configuring method for deep learning-based biomedical image segmentation. Nature Methods,18(2):203211, 2021.",
  "Fabian Isensee, Constantin Ulrich, Tassilo Wald, and Klaus H Maier-Hein. Extending nnu-net is all youneed. In BVM Workshop, pages 1217. Springer, 2023": "Fabian Isensee, Tassilo Wald, Constantin Ulrich, Michael Baumgartner, Saikat Roy, Klaus Maier-Hein, andPaul F Jaeger. nnu-net revisited: A call for rigorous validation in 3d medical image segmentation. arXivpreprint arXiv:2404.09556, 2024. Yuanfeng Ji, Haotian Bai, Jie Yang, Chongjian Ge, Ye Zhu, Ruimao Zhang, Zhen Li, Lingyan Zhang,Wanling Ma, Xiang Wan, et al. Amos: A large-scale abdominal multi-organ benchmark for versatilemedical image segmentation. arXiv preprint arXiv:2206.08023, 2022. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, TeteXiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprintarXiv:2304.02643, 2023.",
  "Yuxiang Lai, Xiaoxi Chen, Angtian Wang, Alan Yuille, and Zongwei Zhou. From pixel to cancer: Cellularautomata in computed tomography. arXiv preprint arXiv:2403.06459, 2024": "Bennett Landman, Zhoubing Xu, J Igelsias, Martin Styner, T Langerak, and Arno Klein. Miccai multi-atlaslabeling beyond the cranial vaultworkshop and challenge. In Proc. MICCAI Multi-Atlas Labeling BeyondCranial VaultWorkshop Challenge, volume 5, page 12, 2015. Bowen Li, Yu-Cheng Chou, Shuwen Sun, Hualin Qiao, Alan Yuille, and Zongwei Zhou. Early detectionand localization of pancreatic cancer by label-free tumor synthesis. MICCAI Workshop on Big Task SmallData, 1001-AI, 2023. Wenxuan Li, Chongyu Qu, Xiaoxi Chen, Pedro RAS Bassi, Yijia Shi, Yuxiang Lai, Qian Yu, Huimin Xue,Yixiong Chen, Xiaorui Lin, et al. Abdomenatlas: A large-scale, detailed-annotated, & multi-center datasetfor efficient transfer learning and open algorithmic benchmarking. Medical Image Analysis, page 103285,2024.",
  "Wenxuan Li, Alan Yuille, and Zongwei Zhou. How well do supervised models transfer to 3d imagesegmentation? In International Conference on Learning Representations, 2024": "Manxi Lin, Nina Weng, Kamil Mikolaj, Zahra Bashir, Morten Bo Sndergaard Svendsen, Martin Tolsgaard,Anders Nymark Christensen, and Aasa Feragen. Shortcut learning in medical image segmentation. arXivpreprint arXiv:2403.06748, 2024. Jie Liu, Yixiao Zhang, Jie-Neng Chen, Junfei Xiao, Yongyi Lu, Bennett A Landman, Yixuan Yuan, AlanYuille, Yucheng Tang, and Zongwei Zhou. Clip-driven universal model for organ segmentation andtumor detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages2115221164, 2023. Jie Liu, Yixiao Zhang, Kang Wang, Mehmet Can Yavuz, Xiaoxi Chen, Yixuan Yuan, Haoliang Li, YangYang, Alan Yuille, Yucheng Tang, et al. Universal and extensible language-vision models for organsegmentation and tumor detection from abdominal computed tomography. Medical Image Analysis, page103226, 2024.",
  "Quande Liu, Qi Dou, Lequan Yu, and Pheng Ann Heng. Ms-net: Multi-site network for improving prostatesegmentation with heterogeneous mri data. IEEE Transactions on Medical Imaging, 2020": "Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. Aconvnet for the 2020s. In Proceedings of the IEEE/CVF conference on computer vision and patternrecognition, pages 1197611986, 2022. Maximilian T Lffler, Anjany Sekuboyina, Alina Jacob, Anna-Lena Grau, Andreas Scharr, Malek El Hus-seini, Mareike Kallweit, Claus Zimmer, Thomas Baum, and Jan S Kirschke. A vertebral segmentationdataset with fracture grading. Radiology: Artificial Intelligence, 2(4):e190138, 2020. Xiangde Luo, Wenjun Liao, Jianghong Xiao, Tao Song, Xiaofan Zhang, Kang Li, Guotai Wang, andShaoting Zhang. Word: Revisiting organs segmentation in the whole abdominal region. arXiv preprintarXiv:2111.02403, 2021. Jun Ma, Yao Zhang, Song Gu, Xingle An, Zhihe Wang, Cheng Ge, Congcong Wang, Fan Zhang, Yu Wang,Yinan Xu, et al. Fast and low-gpu-memory abdomen ct organ segmentation: the flare challenge. MedicalImage Analysis, 82:102616, 2022. Jun Ma, Yao Zhang, Song Gu, Cheng Ge, Shihao Ma, Adamo Young, Cheng Zhu, Kangkang Meng, XinYang, Ziyan Huang, et al. Unleashing the strengths of unlabeled data in pan-cancer abdominal organquantification: the flare22 challenge. arXiv preprint arXiv:2308.05862, 2023. Jun Ma, Yao Zhang, Song Gu, Cheng Ge, Shihao Mae, Adamo Young, Cheng Zhu, Xin Yang, KangkangMeng, Ziyan Huang, et al. Unleashing the strengths of unlabelled data in deep learning-assisted pan-cancerabdominal organ quantification: the flare22 challenge. The Lancet Digital Health, 6(11):e815e826, 2024. Jun Ma, Yao Zhang, Song Gu, Cheng Zhu, Cheng Ge, Yichi Zhang, Xingle An, Congcong Wang,Qiyuan Wang, Xin Liu, et al. Abdomenct-1k: Is abdominal organ segmentation a solved problem. IEEETransactions on Pattern Analysis and Machine Intelligence, 2021. Ozan Oktay, Jo Schlemper, Loic Le Folgoc, Matthew Lee, Mattias Heinrich, Kazunari Misawa, KensakuMori, Steven McDonagh, Nils Y Hammerla, Bernhard Kainz, et al. Attention u-net: Learning where tolook for the pancreas. arXiv preprint arXiv:1804.03999, 2018. S Park, LC Chu, EK Fishman, AL Yuille, B Vogelstein, KW Kinzler, KM Horton, RH Hruban, ES Zinreich,D Fadaei Fouladi, et al. Annotated normal ct data of the abdomen for deep learning: Challenges andstrategies for implementation. Diagnostic and interventional imaging, 101(1):3544, 2020. Chongyu Qu, Tiezheng Zhang, Hualin Qiao, Jie Liu, Yucheng Tang, Alan Yuille, and Zongwei Zhou.Abdomenatlas-8k: Annotating 8,000 abdominal ct volumes for multi-organ segmentation in three weeks.Conference on Neural Information Processing Systems, 2023.",
  "Blaine Rister, Darvin Yi, Kaushik Shivakumar, Tomomi Nobashi, and Daniel L Rubin. Ct-org, a newdataset for multiple organ segmentation in computed tomography. Scientific Data, 7(1):19, 2020": "Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedicalimage segmentation. In International Conference on Medical Image Computing and Computer-AssistedIntervention, pages 234241. Springer, 2015. Holger R Roth, Le Lu, Amal Farag, Hoo-Chang Shin, Jiamin Liu, Evrim B Turkbey, and Ronald MSummers. Deeporgan: Multi-level deep convolutional networks for automated pancreas segmentation. InInternational conference on medical image computing and computer-assisted intervention, pages 556564.Springer, 2015. Saikat Roy, Gregor Koehler, Constantin Ulrich, Michael Baumgartner, Jens Petersen, Fabian Isensee,Paul F Jaeger, and Klaus H Maier-Hein. Mednext: transformer-driven scaling of convnets for medicalimage segmentation. In International Conference on Medical Image Computing and Computer-AssistedIntervention, pages 405415. Springer, 2023. Yousef Sadegheih, Afshin Bozorgpour, Pratibha Kumari, Reza Azad, and Dorit Merhof. Lhu-net: A lighthybrid u-net for cost-efficient, high-performance volumetric medical image segmentation. arXiv preprintarXiv:2404.05102, 2024.",
  "Pengcheng Shi, Xutao Guo, Yanwu Yang, Chenfei Ye, and Ting Ma. Nextou: Efficient topology-awareu-net for medical image segmentation. arXiv preprint arXiv:2305.15911, 2023": "Michele Svanera, Mattia Savardi, Alberto Signoroni, Sergio Benini, and Lars Muckli. Fighting the scannereffect in brain mri segmentation with a progressive level-of-detail network trained on multi-site data.Medical Image Analysis, 93:103090, 2024. Yucheng Tang, Dong Yang, Wenqi Li, Holger R Roth, Bennett Landman, Daguang Xu, Vishwesh Nath,and Ali Hatamizadeh. Self-supervised pre-training of swin transformers for 3d medical image analysis.In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2073020740, 2022. Yu Tian, Min Shi, Yan Luo, Ava Kouhana, Tobias Elze, and Mengyu Wang. Fairseg: A large-scalemedical image segmentation dataset for fairness learning with fair error-bound scaling. arXiv preprintarXiv:2311.02189, 2023. Constantin Ulrich, Fabian Isensee, Tassilo Wald, Maximilian Zenk, Michael Baumgartner, and Klaus HMaier-Hein. Multitalent: A multi-dataset approach to medical image segmentation. arXiv preprintarXiv:2303.14444, 2023. Vanya V Valindria, Nick Pawlowski, Martin Rajchl, Ioannis Lavdas, Eric O Aboagye, Andrea G Rockall,Daniel Rueckert, and Ben Glocker. Multi-modal learning from unpaired images: Application to multi-organsegmentation in ct and mri. In 2018 IEEE winter conference on applications of computer vision (WACV),pages 547556. IEEE, 2018. Haonan Wang, Peng Cao, Jiaqi Wang, and Osmar R Zaiane. Uctransnet: rethinking the skip connectionsin u-net from a channel-wise perspective with transformer. In Proceedings of the AAAI conference onartificial intelligence, volume 36, pages 24412449, 2022.",
  "Zeyu Wang, Yutong Bai, Yuyin Zhou, and Cihang Xie. Can cnns be more robust than transformers? arXivpreprint arXiv:2206.03452, 2022": "Zifu Wang, Maxim Berman, Amal Rannen-Triki, Philip Torr, Devis Tuia, Tinne Tuytelaars, Luc V Gool,Jiaqian Yu, and Matthew Blaschko. Revisiting evaluation metrics for semantic segmentation: Optimizationand evaluation of fine-grained intersection over union. In Advances in Neural Information ProcessingSystems, volume 36, 2023. Ziyang Wang and Irina Voiculescu. Dealing with unreliable annotations: a noise-robust network forsemantic segmentation through a transformer-improved encoder and convolution decoder. Applied Sciences,13(13):7966, 2023. Jakob Wasserthal, Manfred Meyer, Hanns-Christian Breit, Joshy Cyriac, Shan Yang, and Martin Segeroth.Totalsegmentator: robust segmentation of 104 anatomical structures in ct images.arXiv preprintarXiv:2208.05868, 2022. Manuel Wiesenfarth, Annika Reinke, Bennett A Landman, Matthias Eisenmann, Laura Aguilera Saiz,M Jorge Cardoso, Lena Maier-Hein, and Annette Kopp-Schneider. Methods and open-source toolkit foranalyzing and visualizing challenge results. Scientific reports, 11(1):2369, 2021. Junde Wu, Wei Ji, Huazhu Fu, Min Xu, Yueming Jin, and Yanwu Xu. Medsegdiff-v2: Diffusion-basedmedical image segmentation with transformer. In Proceedings of the AAAI Conference on ArtificialIntelligence, volume 38, pages 60306038, 2024. Yingda Xia, Qihang Yu, Linda Chu, Satomi Kawamoto, Seyoun Park, Fengze Liu, Jieneng Chen, ZhuotunZhu, Bowen Li, Zongwei Zhou, et al. The felix project: Deep networks to detect pancreatic neoplasms.medRxiv, 2022. Yutong Xie, Jianpeng Zhang, Yong Xia, and Chunhua Shen. Learning from partially labeled data formulti-organ and tumor segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence,2023.",
  "Zhaohu Xing, Liang Wan, Huazhu Fu, Guang Yang, and Lei Zhu. Diff-unet: A diffusion embeddednetwork for volumetric segmentation. arXiv preprint arXiv:2303.10326, 2023": "Lian Xu, Jingbing Li, and Mengxing Huang. The robust algorithm of 3d medical image retrieval based onperceptual hashing. In 2015 International Conference on Mechatronics, Electronic, Industrial and ControlEngineering (MEIC-15), pages 452456. Atlantis Press, 2015. Yiwen Ye, Yutong Xie, Jianpeng Zhang, Ziyang Chen, and Yong Xia. Uniseg: A prompt-driven universalsegmentation model as well as a strong representation learner. In International Conference on MedicalImage Computing and Computer-Assisted Intervention, pages 508518. Springer, 2023.",
  "Weihao Yu and Xinchao Wang. Mambaout: Do we really need mamba for vision?arXiv preprintarXiv:2405.07992, 2024": "Xin Yu, Qi Yang, Yinchi Zhou, Leon Y Cai, Riqiang Gao, Ho Hin Lee, Thomas Li, Shunxing Bao,Zhoubing Xu, Thomas A Lasko, et al. Unest: local spatial representation learning with hierarchicaltransformer for efficient medical segmentation. Medical Image Analysis, 90:102939, 2023. Chaoning Zhang, Dongshen Han, Yu Qiao, Jung Uk Kim, Sung-Ho Bae, Seungkyu Lee, and Choong SeonHong. Faster segment anything: Towards lightweight sam for mobile applications. arXiv preprintarXiv:2306.14289, 2023. Jianpeng Zhang, Yutong Xie, Yong Xia, and Chunhua Shen. Dodnet: Learning to segment multi-organ andtumors from multiple partially labeled datasets. In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 11951204, 2021.",
  "Zongwei Zhou, Michael B Gotway, and Jianming Liang. Interpreting medical images. In IntelligentSystems in Medicine and Health, pages 343371. Springer, 2022": "Zongwei Zhou, Md Mahfuzur Rahman Siddiquee, Nima Tajbakhsh, and Jianming Liang. Unet++: Anested u-net architecture for medical image segmentation. In Deep Learning in Medical Image Analysisand Multimodal Learning for Clinical Decision Support, pages 311. Springer, 2018. Zongwei Zhou, Md Mahfuzur Rahman Siddiquee, Nima Tajbakhsh, and Jianming Liang.Unet++:Redesigning skip connections to exploit multiscale features in image segmentation. IEEE Transactions onMedical Imaging, 39(6):18561867, 2019.",
  "A.1Construction of AbdomenAtlas 1.0": ":Public datasets composing AbdomenAtlas 1.0 and their details . The naiveaggregation of these public datasets results in a database with partial and incomplete labels, e.g.,LiTS only had labels for the liver and its tumors, and KiTS only had labels for the kidneys and itstumors. Conversely, our AbdomenAtlas 1.0 is fully-annotated, offering detailed per-voxel labelsfor all 9 organs within each CT scan. We detected and removed duplicated CT scans across publicdatasets like LiTS and FLARE23. Duplicate scans were identified by generating a 3D perceptualhash for each image in the dataset. By comparing the similarity of these hashes, duplicates werereliably detected, a finding that was further confirmed through manual inspection of CT scans withhigh perceptual hash similarities. After aggregating all datasets and removing duplicates, we obtaineda total of 5,195 fully-annotated CT scans.",
  "A.2Domain Shift in TotalSegmentator": ": Percentage of Missing Classes in the two Partitions of TotalSegmentator. Part of To-talSegmentator is included in the AbdomenAtlas dataset (N=485), because it is contained in FLARE,one of the AbdomenAtlas constituents. We leveraged the remaining sample of TotalSegmentator(N=743) for testing, providing a public test set anyone can easily use to compare segmentationmodels to Touchstone results. Unlike for the JHH test set, the hospitals in TotalSegmentator arepresent in AbdomenAtlas. However, the part of TotalSementator inside AbdomenAtlas (N=485) andthe 743 test samples are not identically distributed. analyzes these two subsets, and showsthat the one inside AbdomenAtlas was carefully selected to focus on the abdominal region, with aregular Region of Interest: almost all of these 485 images contain the 9 abdominal organs consideredin this Touchstone. Conversely, the 743 TotalSegmentator images in our test set are more challenging,presenting varying regions of interest, which can extend outside of the abdomen and usually crop outsome of the 9 classes in this benchmark. Therefore, demonstrates a substantial distributionshift between the two TotalSegmentator partitions, making our TotalSegmentator test images (N=743)out-of-distribution and a challenging test scenario. Interestingly, our results show this scenario waseven more challenging to the AI algorithms than the JHH test set, which contains only images froman unseen hospital (see Sec. 3).",
  "A.3Dataset Visualization by Metadata Information": ": Anatomical boundaries and structures can be indistinct due to disease, as seen in theJHH dataset. We display CT volumes with patients depicted under unhealthy conditions that arechallenging for most AI algorithms to identify. The CT volumes are from patients in unhealthyconditions. As shown in the first row on the left side, a kidney cyst is mistakenly annotated as thegall bladder. This example highlights that in the abdominal region, diseases can obscure anatomicalboundaries and even lead to misidentification of structures. : Anatomical boundaries can be blurry due to factors such as patient disease, age,and CT scan quality in TotalSegmentator. We display CT scans that are challenging for mostAI algorithms to identify. The CT scans in the top three rows are from patients diagnosed with thetumors specified in the pathology metadata. The remaining images feature patients in their 70s and80s. As shown in the fourth row on the right side, the boundary of the aorta in a 78-year-old patient ischallenging to identify, not only for AI algorithms but also for human annotators in determining theground truth.",
  "B.1.1Category CNN": "U-Net. The U-Net is a fully-convolutional neural network, based on an encoder-decoderstructure joint by multiple skip-connections. The encoder performs down-sampling operations,and it is designed to capture high-level semantics and context information. The decoder conductsup-sampling, and the long-range skip connections allow it to fuse the high-level semantics availableat deep encoder layers, with the precise spatial information extracted from earlier encoder layers. TheU-Net is the most influential architecture in biomedical segmentation; almost one decade after itsrelease, the model is still the base of multiple novel architectures in this Benchmark. ResEncL. nnU-Net is a self-configuring segmentation framework. It automatically configurespre-processing, network architecture, training and post-processing. Auto-configuration is guided byfixed parameters, interdependent rules that consider dataset properties and computational limitations,and empirical parameters. nnU-Nets default model has recently been updated to ResEncL default,which is based on a U-Net architecture with residual connections in its encoder . The encoderis computationally expensive while the decoder is as lightweight as possible. For hyper-parameterconfiguration the nnU-Net default values are used except for the modality which was declared asnonCT, resulting in z-score intensity normalization. ResEncL serves as a modernized nnU-Netbaseline to compare new methodological innovations against. MedNeXt. MedNeXt is a fully ConvNeXt-based 3D Encoder-Decoder Network designedto benefit from the scalability of Transformer-based networks while leveraging the inductive biasinherent to convolutions. This enables effective training on large datasets while still being beneficialon small data-scarce settings common to 3D medical image segmentation in the last decade. In the3-layer residual structure of a MedNeXt block, the first layer computes features using a depthwiseconvolutional kernel, and it is followed by an expansion and compression layer, akin to a SwinTransformer. The architecture primarily benefits from using its MedNeXt blocks in all layers ofthe architecture, including up and downsampling blocks. The MedNeXt block enables effectiverepresentation learning in standard layers while allowing the network to maintain semantic richnessin all resampling operations. STU-Net. STU-Net is a family of scalable and transferable medical image segmentation modelsbased on the nnU-Net framework and the U-Net architecture. The STU-Net models introduceinnovations such as refined convolutional blocks with residual connections for better scalabilityand weight-free interpolation for enhanced transferability. The models are available in differentsizes: STU-Net-S with 14 million parameters, STU-Net-B (with 58.3M), STU-Net-L (440.3M),and STU-Net-H with 1.4 billion parameters. Improvements in segmentation accuracy stem fromthe empirical scaling of network depth and width. The primary goal of STU-Net is to enhance thescalability and transferability of medical image segmentation algorithms, facilitating their applicationacross a variety of downstream tasks in transfer learning. UniSeg. UniSeg is a prompt-driven universal segmentation framework designed for multi-taskmedical image segmentation, offering transfer capabilities across various modalities and domains.Based on the nnU-Net framework, UniSeg has a vision encoder and a fusion module, which togetherenable a prompt-driven decoder. A key innovation of UniSeg is its universal learnable prompt thatmodels complex inter-task relationships. UniSeg integrates task-specific prompts early in the trainingprocess, enhancing the training effectiveness of the entire decoder. The primary goal of UniSeg isnot only to excel in multi-task learning but also to serve as a pre-trained model that improves theaccuracy of downstream segmentation tasks. UniSeg was pre-trained (supervised) on 5 datasetsbefore fine-tuning on AbdomenAtlas 1.0: MOTS , VerSe20 , Prostate, BraTS21,and AutoPET2022 .",
  "B.1.2Category Transformer": "UNETR. UNETR was proposed as a 3D transformer-based segmentation backbone network.The method leverages the Transformer model and CNN as a hybrid architecture, to capture long-rangedependencies within volumetric medical data. The architecture integrates a Vision Transformer(ViT) as the encoder to handle the 3D input patches and extract rich feature representations. These",
  "features are then progressively merged with a convolutional neural network (CNN)-based decoder ina UNet-like structure": "Swin UNETR. SwinUNETR adapted Swin Transformers to enhance volumetric medical imagesegmentation by capturing both local and global features through a hierarchical, window-based self-attention mechanism, outperforming the original UNETR, and using Swin-transformers for globalcontext. Additionally, self-supervised pre-training of Swin Transformers on large-scale unlabeled3D medical images datasets, using techniques like masked autoencoding, can significantly boost themodels robustness and performance on downstream tasks. These features enabled leading results invarious 3D medical image analysis applications, especially in CT segmentation tasks. UNEST. UNEST is an advanced 3D segmentation model designed to leverage the strengthsof the hierarchical vision transformer architecture for handling 3D medical image data. UNESTalso employed a U-shape encoder-decoder structure, where the encoder is based on the 2-stagenested ViT. This transformer-based encoder extracts hierarchical features from the input CT scanusing self-attention mechanisms, which capture long-range dependencies and spatial relationshipsefficiently. The decoder consists of 4-levels of CNN-based blocks that reconstruct the segmentationmap by upsampling the features and incorporating skip connections from the encoder to retainspatial information. The models architecture and training protocol are optimized to provide a robustand efficient solution for 3D segmentation tasks such as whole body, regional, and whole brainsegmentation. SegVol. SegVol is based on the SAM architecture and 3D transformers, enabling universaland interactive volumetric medical image segmentation on over 200 anatomical categories. SegVolsupports spatial-prompt, semantic-prompt, and combined-prompt segmentation, aiming for high-precision segmentation and semantic disambiguation. SegVol introduced a zoom-out-zoom-inmechanism to provide users with an easy SAM-like interface on volumetric images, while significantlyreducing computational cost and preserving the segmentation precision. Pseudo labels are used torelieve the problem of spurious correlation between predictions and data distributions. Prior totraining on AbdomenAtlas, SegVol was pre-trained on 90K unlabeled CT scans from M3D-Cap, and5,772 labeled CT scans from M3D-Seg . SAM-Adapter. The SAM-Adapter is a 2D segmentation model, unlike the other networks inthis study. Thus, it individually analyzes the 2D slices that compose a CT scan. The model is basedon fine-tuning the MobileSAM encoder and decoder using Adapter layers. The SAM-Adapterfollows the philosophy that model size has limited effect over the accuracy of medical segmentationalgorithms .",
  "B.1.3Hybrid Architectures": "LHU-Net. LHU-Net is a compact and efficient U-Net-based architecture created for 3D medicalimage segmentation. It utilizes a hierarchical encoder-decoder structure with convolutional layersfollowed by hybrid attention mechanisms to capture both local and global features. Key innovationsinclude the integration of CNN-based spatial attention and Vision Transformer (ViT) attentionmechanisms, such as the OmniFocus attention and self-adaptive contextual fusion modules, whichenhance discriminative feature extraction while keeping the model lightweight. These attentionmechanisms objective is to ensure high precision and detail in the segmentation results. The mainaim of LHU-Net is to achieve high segmentation accuracy with minimal computational resources andparameters, making it a practical and accessible tool for medical imaging tasks. UCTransNet. UCTransNet is a hybrid architecture, based on U-Net with transformer blocks asskip connections. It introduces the Channel-wise Cross Fusion Transformer (CCT) to fuse multi-scalecontext with cross attention from a channel-wise perspective. CCT captures local cross-channelinteraction for adaptive fusing of multi-scale features with possible scale semantic gap. Additionally,a channel-wise cross attention (CCA) module is proposed for fusing features from decoder stagesand fused multi-scale features to solve inconsistent semantic levels. Both cross attention modulesare called CTrans and replace the original skip connections in the U-Net. Here, the UCTransNet 2Dcomponents were substituted by their 3D versions, including convolution layers, patch embeddinglayers, and patch merging layers. The main goal is to discover an efficient approach for integratingCNNs and Transformers for medical image segmentation. Diff-UNet. Diff-UNet is the first generic 3D medical image segmentation model based on adenoising diffusion model. It mainly consists of two branches: the boundary prediction branch andthe diffusion denoising branch. The boundary prediction branch is based on the U-Net structure,while the diffusion denoising branch is based on a denoising U-Net structure with noise input. Toaggregate the low-level and high-level features from both branches for better boundary perception,Diff-UNet also includes a Multi-granularity Boundary Aggregation (MBA) module. Next, Diff-U-Netproposes a Monte Carlo Diffusion (MC Diffusion) module to obtain uncertainty maps and guidesegmentation loss to focus on hard-to-segment regions during training. Finally, Diff-UNet devises aProgressive Uncertainty-driven REfinement (PURE) strategy to obtain a more robust prediction resultduring inference, based on the inference steps and uncertainty maps estimated by the MC Diffusionmodule. NexToU. NexToU is a hybrid architecture that follows a hierarchical 3D U-shaped encoder-decoder structure, based on CNNs and graph neural networks (GNNs). It incorporates a hierarchical,topology-aware strategy inspired by human cognitive processes, progressively decomposing anatom-ical semantics from simpler to more complex structures. Concurrently, it also learns containment,connection, and exclusion relationships among various anatomical classes. To facilitate learning andspeed up training, NexToU employs a semantic tree and a novel hierarchical topological interaction(HTI) module. Additionally, it enhances spatial topology perception by incorporating Vision GNN and Swin GNN modules, which adeptly represent topology on both global and local scales.The primary goal of NexToUs innovations is to improve segmentation accuracy for homogeneousmulti-class anatomical structures, such as vasculature and skeletons. The HTI module is designed tobe more effective when dealing with a large number of classes. MedFormer. MedFormer is a hybrid architecture that combines the inductive bias of convo-lution with the global modeling capabilities of Transformers. A key innovation in the design is thebidirectional multi-head attention (B-MHA) mechanism, which addresses the quadratic complexitytypically associated with self-attention on long sequences. B-MHA employs a low-rank projectionmechanism to achieve linear complexity attention, making it computationally efficient for both low-and high-resolution feature maps. Furthermore, B-MHAs architecture captures the most salientfeatures in its hidden state, enhancing model robustness by reducing focus on irrelevant details.Through this design, MedFormer demonstrates good scalability, efficiency, and generalizability,performing effectively on both small and large datasets without requiring pre-trained weights.",
  "B.2.1nnU-Net": "nnU-Net is a framework for automatically configuring AI-based semantic segmentation pipelines.Given a new segmentation dataset, it will extract relevant metadata from the training cases toautomatically determine its hyperparameters. Despite its first release dating back to 2019 and despiteits use of a standard U-Net , it stood the test of time and continues to produce state-of-the-artresults. nnU-Net powerfully demonstrates that carefully configuring and validating segmentationpipelines across a wide range of segmentation tasks yields a surprisingly potent algorithm. As aframework for method development, it is widely used and extended by the community to pushthe boundaries of semantic segmentation . A recent update to the nnU-Netpresets includes reference implementations for a U-Net with residual connections in the encoder,optimized for different VRAM budgets.",
  "B.2.2MONAI": "MONAI (Medical Open Network for AI) is an open-source framework designed to supportartificial intelligence in healthcare data. Built on top of PyTorch, MONAI facilitates a comprehensivesuite of tools for configuring, training, inference, and deploying AI models tailored to medical appli-cations. It includes components for data loading, preprocessing, and augmentation, as well as prebuiltarchitectures for common tasks such as segmentation, registration, detection, and classification.MONAI is designed to be flexible, extensible, and performance-optimized, enabling researchers andpractitioners to accelerate their AI development cycle in the medical domain.",
  "B.2.3Vision-Language Models": "CLIP-Driven Universal Model. The CLIP-Driven Universal Model framework, which isdesigned for organ and tumor segmentation, integrates a label taxonomy from various public datasets.The architecture consists of a text branch and a vision branch. In the text branch, the model generatesCLIP embeddings for each organ and tumor using label prompts, enhancing the anatomical structureof the feature embedding. These embeddings are concatenated with global image features, termed thetext-based controller, to produce prompt features for segmentation. The vision branch pre-calculatesCT scans to mitigate domain gaps across different datasets. These extracted features are processedby three sequential convolutional layers, referred to as the text-driven segmentor, which utilize theparameters generated by the text branch to predict segmentation masks for each class. The decoderalso includes a \"one vs. all\" approach, using Sigmoid activation for each class to generate individualpredictions, ensuring robust and dynamic segmentation across diverse medical imaging datasets.",
  "n/aSAM-Adapter11.6MTransformer0.610.5 GBMedFormer38.5MHybrid--Diff-UNet434.0MHybrid2.263.9 GB": "The time and average GPU memory for inference were measured with an NVIDIA V100 GPU and an Intel Xeon Silver 4210 CPU,evaluating a CT scan with 259259283 voxels and spacing of 1.5 mm/voxel. Measurements consider the entire segmentation pipeline,from loading the CT scan and the AI algorithm, to saving the inference. We observed that the way each AI algorithm deals with spacing andre-shapes its input scan plays a major role in their inference speed.",
  "C.1NSD scores on the entire JHH dataset": ": External validation on proprietary JHH dataset (N=5,160) - NSD. For each class, webold the best-performing results and highlight the runners-up, which show no significant difference(P > 0.05) from the best results, in red. Architectures are grouped by their frameworks and sorted inascending order based on the number of parameters. NSD considers a tolerance of 1.5mm.",
  "C.2NSD scores on the TotalSegmentator dataset": ": Validation on the TotalSegmentator dataset (N=743) - NSD. For each class, we bold thebest-performing results and highlight the runners-up, which show no significant difference (P > 0.05)from the best results, in red. Architectures are grouped by their frameworks and sorted in ascendingorder based on the number of parameters. NSD considers a tolerance of 1.5mm.",
  "C.3DSC/NSD scores on the official test set of TotalSegmentator": ": Validation on the official test set of TotalSegmentator (N=59) - DSC. TotalSegmentatorprovides an official split of training and testing sets. To align with other papers, we hereby alsoprovide the benchmark results on the test set of TotalSegmentator (N=59). Notably, the averagescores in the official test set are usually higher than the ones in the entire TotalSegmentator dataset.",
  "n/aSAM-Adapter 11.6M48.732.925.123.37.08.637.720.031.220.5MedFormer 38.5M87.813.983.915.879.610.581.218.586.013.7Diff-UNet 434.0M82.025.074.426.873.627.479.021.482.421.9": "These architectures were pre-trained (Appendix B.3).The class IVC (inferior vena cava) shares the same meaning as the class postcava in other datasets (e.g., AbdomenAtlas 1.0 and JHH).These architectures were trained on AbdomenAtlas 1.0 with enhanced label quality for the aorta class (discussed in 4). : Validation on the official test set of TotalSegmentator (N=59) - NSD. TotalSegmentatorprovides an official split of training and testing sets. To align with other papers, we hereby alsoprovide the benchmark results on the test set of TotalSegmentator (N=59). Notably, the averagescores in the official test set are usually higher than the ones in the entire TotalSegmentator dataset.NSD considers a tolerance of 1.5mm.",
  "D.1Worst-case Analysis": ": Worst case analysis for JHH. This figure displays CT scans that are particularly challengingfor most AI algorithms to identify. To illustrate these difficult cases, we also include visualizationsfrom the top-performing algorithm, MedNext, and the first runner-up, STU-Net Base. : Worst case analysis for TotalSegmentator. This figure displays CT scans that areparticularly challenging for most AI algorithms to identify. To illustrate these difficult cases, we alsoinclude visualizations from the top-performing algorithm, ResEncL, and the first runner-up, U-Net.The results show ResEncL does perform better than U-Net in these worst cases.",
  "D.2.1Evaluation Metrics": "Every evaluation metric reflects a certain aspect of the results and choosing the right one is importantto emphasize those properties that we care about. In this section, we assess the ranking stability withrespect to different evaluation metrics. The Dice Similarity Coefficient (DSC) is a widely used metric in medical imaging to measure theoverlap between the prediction and the ground truth. Additionally, Normalized Surface Distance(NSD) focuses on the segmentation quality between two boundaries. Due to the existence of NaNs (which represent some organs that are missing in some CT scans),averaging per-case-per-class values by case first and then by class differs from averaging them byclass first and then by case . Lets focus on DSC (note that this also applies to other metricssuch as NSD) and denote the first version as DSCC and the second as DSCI. DSCC allows us toevaluate model performance on a class-wise scale, emphasizing difficult classes, and it alleviatesthe limitation of DSCI, which can be biased towards classes with less NaNs. On the other hand,DSCI facilitates statistical tests across different cases. Due to these considerations, we use DSCC forreporting per-class performance and utilize DSCI to conduct statistical tests. In the rest of the paper,we drop the superscripts for simplicity unless stated otherwise.",
  "Besides the standard DSC, in this section, we also consider a worst-case metric to emphasize difficultcases . In particular, it only averages over cases whose scores fall below the 10% quantile": "Except accuracy metrics such DSC and NSD, we also study bias metrics. Specifically, we chooseDemographic Parity Difference (DPD) , which captures bias across diverse demographicgroups. Originally proposed for classification problems, we extend it to medical segmentation anddefine it as the maximum differences in DSC among different sensitive demographic groups. The results for different metrics are shown in Figures 7-9. We find that models tend to retain a similarrank across different accuracy metrics, indicating that these models do not overfit to a specific metric.However, performance on the worst-case DSCC is significantly lower than the DSCC itself, showingthat a need for improvements in model performance on these hard cases, or indicating the existenceof some label noise in test sets. We visualize some worse-case examples in Appendix D.1. Regardingthe bias metrics, although there are some variations in rankings, we find models with high accuracyusually have low bias. DSCC DSCI Worst-case DSCC NSD ResEncL (clean) MedNeXt STU-Net-B STU-Net-H MedFormer STU-Net-L U-Net ResEncL UniSeg Diff-UNet LHU-Net NexToU U-Net & CLIP SegVol Swin UNETR & CLIP UNesT Swin UNETR",
  "D.2.2Bootstrap Sampling": "To evaluate ranking stability, we perform bootstrap sampling as described in . A bootstrapsample of a dataset with n test cases consists of n test cases randomly drawn from the dataset withreplacement. A total of 1,000 bootstrap samples are drawn, and the results are visualized as blobplots in . Our findings indicate that datasets with more test cases tend to present fewer variations in ranks. Forexample, we find fewer variations in ranks on the entire TotalSegmentator (N = 743) compared withthe ranks on the TotalSegmentator official test set (N = 59). On the proprietary JHH dataset (N =5,160), we observe minimum ranking variations due to its large number of test cases. Additionally,the ranks are relatively robust for the highest- and lowest-performing models, but they can be moreunstable for models in the middle range. ResEncL (clean) MedNeXt STU-Net-B STU-Net-H MedFormer STU-Net-L U-Net ResEncL UniSeg Diff-UNet LHU-Net NexToU U-Net & CLIP SegVol Swin UNETR & CLIP UNesT Swin UNETR",
  "Rank": "TotalSegmentator Official Test Set : Blob plots for visualizing ranking stability based on 1,000 bootstrap samples. Thearea of each blob is proportional to the relative frequency. The median rank for each model is markedby a black cross. 95% bootstrap intervals (ranging from the 2.5th to the 97.5th percentile of thebootstrap distribution) are connected by black lines. We observe more stable rankings for larger testssets.",
  "D.2.3Significance Maps": "To further investigate ranking stability, we performed pair-wise comparisons between each possiblepair of algorithms. Comparisons use statistical tests to understand if an algorithms scores aresignificantly better than the other models results. We employed one-sided Wilcoxon signed ranktests with Holms adjustment and 5% significance level. nnU-Net ResEncL MedNeXt nnU-Net U-Net STU-Net B STU-Net L STU-Net H Diff-UNet U-Net & CLIP NexToU LHU-Net UniSeg SwinUNETR & CLIP SegVol UCTransNet MedFormer",
  "UNETR": "SAM-Adapter AI Algorithm aorta 0.00.20.40.60.81.0 IVC 0.00.20.40.60.81.0 pancreas : Boxplots showing NSD score in the TotalSegmentator official test dataset, per class.Performances are not homogeneous across classes: structures like the liver, which are easier tosegment, show higher median scores and smaller score variation, when compared to more difficultstructures, like the gallbladder. NSD considers a threshold of 1.5mm.",
  "UNETRUNEST": "UCTransNet SegVolNexToU SwinUNETR & CLIP LHU-Net MedFormer nnU-Net ResEncL U-Net & CLIP MedNeXt STU-Net H UniSeg STU-Net LDiff-UNet nnU-Net U-Net STU-Net B TotalSegmentator Official Test Set - IVC MedFormer MedNeXt STU-Net L STU-Net H STU-Net B Diff-UNet UniSeg nnU-Net U-Net nnU-Net ResEncL LHU-Net U-Net & CLIP SegVol SwinUNETR & CLIP NexToU SwinUNETR UCTransNet",
  "UNEST": "UCTransNet SegVol Diff-UNet SwinUNETR & CLIP NexToULHU-Net U-Net & CLIP MedFormer UniSeg MedNeXt nnU-Net U-Net STU-Net B nnU-Net ResEncL STU-Net HSTU-Net L TotalSegmentator Official Test Set - average MedFormer STU-Net H MedNeXt UniSeg STU-Net L STU-Net B nnU-Net ResEncL nnU-Net U-Net Diff-UNet NexToU LHU-Net U-Net & CLIP SegVol SwinUNETR & CLIP SwinUNETR UCTransNet",
  "UNESTUNETR": "UCTransNetSwinUNETR SwinUNETR & CLIP SegVol U-Net & CLIP LHU-NetNexToU Diff-UNet nnU-Net U-Net nnU-Net ResEncL STU-Net BSTU-Net L UniSeg MedNeXt STU-Net H MedFormer JHH - average : Continuation of NSD significance maps. Each cell represents a pair-wise comparisonbetween two algorithms, according to NSD. Yellow colors indicate that the x-axis AI algorithm issignificantly superior to the y-axis algorithm in terms of NSD score (considering all organs). Bluerepresents no significant superiority. Comparisons employed one-sided Wilcoxon signed rank testswith Holms adjustment and 5% significance level. NSD considers a threshold of 1.5mm.",
  "D.3.1MedNeXt": "The central theme of the ConvNeXt architecture was decoupling the scalability of the Transformerarchitecture and using it in a convolutional fashion, without self-attention. Scalability becomesrelevant for medical images when creating large 3D networks while not overfitting. MedNeXt builds upon this principle by using these blocks across the network, leading to the performance seenin this work.",
  "D.3.2STU-Net": "The STU-Net is built upon the nnU-Net framework, which was proven effective in our experi-ments. Additionally, STU-Net is based on scaling the AI model size, which may be exceptionallyuseful for dealing with large-scale datasets like AbdomenAtlas 1.0. The combination of a high-performance framework and an appropriately scaled model may be the key for STU-Nets highsegmentation accuracy in this study.",
  "D.3.3NexToU": "NexToU is a hybrid architecture that combines a hierarchical 3D U-shaped encoder-decoderstructure with both Convolutional Neural Networks (CNNs) and Graph Neural Networks (GNNs).This innovative approach employs a hierarchical, topology-aware strategy inspired by human cognitiveprocesses, allowing the model to progressively decompose anatomical semantics from simpler to morecomplex structures. On the JHH dataset, NexToUs results were relatively close to the best-performingmodels. However, we observed a significant performance difference on the TotalSegmentator dataset.This discrepancy is likely due to our model not utilizing a resampling step to the average spatialresolution during inference for data with fewer slices along the z-axis. While this approach savesinference time, it compromises performance on data with low z-axis resolution. Additionally, tofurther reduce inference time, Test Time Augmentation (TTA) was minimized, leading to a decline inperformance for bilaterally symmetric classes like kidneyR and kidneyL, as well as for some smallsample classes.",
  "D.3.4DiffU-Net": "We hypothesize that two main factors contributed to Diff-UNets high segmentation accuracy: itsnnU-Net-inspired hyper-parameter selection procedure and the use of stable diffusion. The diffusionmodel excels in handling details, generating high-resolution images when used as a generative model.During inference, the model predicts multiple times using the DDIM sampling strategy, furtherenhancing Diff-UNets outputs. Moreover, considering that the diffusion model includes noisedinformation, DiffU-Net has a boundary branch, which takes the 3D medical image as input. Thisbranch supplies clear image information to complement the diffusion branch, further improvingsegmentation accuracy.",
  "We observed a lower performance for the fine-tuned Segment Anything model, which we hypothesizemay be due to the following reasons:": "The SAM-based model is a 2D-based model that performs multi-class segmentation solelyon 2D slices. This approach relies mainly on 2D information, such as location relations,rather than 3D organ shape information. When tested on out-of-distribution (OOD) sets,images from different hospitals may introduce spatial variations and voxel spacing, leadingto varying spatial distributions of abdomen regions compared to the training images. Thesespatial changes can cause the 2D-based model to lose its segmentation accuracy. During the training of this fine-tuned model, no spatial transformations for augmenta-tion were used, which might have been used in other comparison methods. This lack ofaugmentation could lead to poorer generalization on spatial changes in OOD data.",
  "D.5.1Age": "0.00.20.40.60.81.0 DSC MedNeXt-ages 16-29MedNeXt-ages 30-39MedNeXt-ages 40-49MedNeXt-ages 50-59MedNeXt-ages 60-69MedNeXt-ages 70-79MedNeXt-ages 80-89 STU-Net B-ages 16-29STU-Net B-ages 30-39STU-Net B-ages 40-49STU-Net B-ages 50-59STU-Net B-ages 60-69STU-Net B-ages 70-79STU-Net B-ages 80-89STU-Net L-ages 16-29STU-Net L-ages 30-39STU-Net L-ages 40-49STU-Net L-ages 50-59STU-Net L-ages 60-69STU-Net L-ages 70-79STU-Net L-ages 80-89STU-Net H-ages 16-29STU-Net H-ages 30-39STU-Net H-ages 40-49STU-Net H-ages 50-59STU-Net H-ages 60-69STU-Net H-ages 70-79STU-Net H-ages 80-89 nnU-Net ResEncL-ages 16-29nnU-Net ResEncL-ages 30-39nnU-Net ResEncL-ages 40-49nnU-Net ResEncL-ages 50-59nnU-Net ResEncL-ages 60-69nnU-Net ResEncL-ages 70-79nnU-Net ResEncL-ages 80-89 MedFormer-ages 16-29MedFormer-ages 30-39MedFormer-ages 40-49MedFormer-ages 50-59MedFormer-ages 60-69MedFormer-ages 70-79MedFormer-ages 80-89 nnU-Net U-Net-ages 16-29nnU-Net U-Net-ages 30-39nnU-Net U-Net-ages 40-49nnU-Net U-Net-ages 50-59nnU-Net U-Net-ages 60-69nnU-Net U-Net-ages 70-79nnU-Net U-Net-ages 80-89 UniSeg-ages 16-29UniSeg-ages 30-39UniSeg-ages 40-49UniSeg-ages 50-59UniSeg-ages 60-69UniSeg-ages 70-79UniSeg-ages 80-89 Diff-UNet-ages 16-29Diff-UNet-ages 30-39Diff-UNet-ages 40-49Diff-UNet-ages 50-59Diff-UNet-ages 60-69Diff-UNet-ages 70-79Diff-UNet-ages 80-89 NexToU-ages 16-29NexToU-ages 30-39NexToU-ages 40-49NexToU-ages 50-59NexToU-ages 60-69NexToU-ages 70-79NexToU-ages 80-89SegVol-ages 16-29SegVol-ages 30-39SegVol-ages 40-49SegVol-ages 50-59SegVol-ages 60-69SegVol-ages 70-79SegVol-ages 80-89 U-Net & CLIP-ages 16-29U-Net & CLIP-ages 30-39U-Net & CLIP-ages 40-49U-Net & CLIP-ages 50-59U-Net & CLIP-ages 60-69U-Net & CLIP-ages 70-79U-Net & CLIP-ages 80-89 SwinUNETR & CLIP-ages 16-29SwinUNETR & CLIP-ages 30-39SwinUNETR & CLIP-ages 40-49SwinUNETR & CLIP-ages 50-59SwinUNETR & CLIP-ages 60-69SwinUNETR & CLIP-ages 70-79SwinUNETR & CLIP-ages 80-89 LHU-Net-ages 16-29LHU-Net-ages 30-39LHU-Net-ages 40-49LHU-Net-ages 50-59LHU-Net-ages 60-69LHU-Net-ages 70-79LHU-Net-ages 80-89 UCTransNet-ages 16-29UCTransNet-ages 30-39UCTransNet-ages 40-49UCTransNet-ages 50-59UCTransNet-ages 60-69UCTransNet-ages 70-79UCTransNet-ages 80-89SwinUNETR-ages 16-29SwinUNETR-ages 30-39SwinUNETR-ages 40-49SwinUNETR-ages 50-59SwinUNETR-ages 60-69SwinUNETR-ages 70-79SwinUNETR-ages 80-89 UNEST-ages 16-29UNEST-ages 30-39UNEST-ages 40-49UNEST-ages 50-59UNEST-ages 60-69UNEST-ages 70-79UNEST-ages 80-89UNETR-ages 16-29UNETR-ages 30-39UNETR-ages 40-49UNETR-ages 50-59UNETR-ages 60-69UNETR-ages 70-79UNETR-ages 80-89 SAM-Adapter-ages 16-29SAM-Adapter-ages 30-39SAM-Adapter-ages 40-49SAM-Adapter-ages 50-59SAM-Adapter-ages 60-69SAM-Adapter-ages 70-79SAM-Adapter-ages 80-89 AI Algorithm-Group ************************** *** **** *********** ***** ** ****** **** * * **** **** ** **** **** * **** **** * **** **** * **** **** * **** **** **** **** **** ** **** **** ** **** **** *** **** **** ******** **** **** **** **** **** **** * **** **** *** **** *** **** **** **** *** *** **** ** **** **** **** **** **** **** *** **** **** ** **** **** **** **** **** **** **** ******** **** ** **** **** **** **** **** **** **** **** **** **** ** **** **** **** ****** **** **** **** **** **** **** **** ** **** * **** **** *** **** **** **** **** **** **** *** **** *** **** ** ******** ******** **** **** **** **** **** **** **** **** **** **** **** **** ******** **** **** **** **** **** **** **** **** **** * **** ** **** *** **** * ** ** ** * ****** ******** **** ** **** *** **** *** **** *** **** **** **** **** ******* **** ** **** ***** **** ******** ** *** **** ************** **** *** ********* mean DSC by ages in JHH : Boxplot showing average DSC score by age in JHH. Statistical significance is indicatedby stars: * p < 0.05, ** p <0.01, *** p < 0.001, **** p < 0.0001. We perform KruskalWallis testsfollowed by post-hoc Mann-Whitney U Tests with Bonferroni correction. Here, we did not performstatistical comparisons between diverse AI algorithms. Significant (at least p<0.05) reductions inDSC score for groups with advanced age are observed for all AI algorithms. 0.00.20.40.60.81.0 DSC MedNeXt-ages 18-29MedNeXt-ages 30-39MedNeXt-ages 40-49MedNeXt-ages 50-59MedNeXt-ages 60-69MedNeXt-ages 70-79MedNeXt-ages 80-89MedNeXt-ages 90-99 STU-Net B-ages 18-29STU-Net B-ages 30-39STU-Net B-ages 40-49STU-Net B-ages 50-59STU-Net B-ages 60-69STU-Net B-ages 70-79STU-Net B-ages 80-89STU-Net B-ages 90-99STU-Net L-ages 18-29STU-Net L-ages 30-39STU-Net L-ages 40-49STU-Net L-ages 50-59STU-Net L-ages 60-69STU-Net L-ages 70-79STU-Net L-ages 80-89STU-Net L-ages 90-99STU-Net H-ages 18-29STU-Net H-ages 30-39STU-Net H-ages 40-49STU-Net H-ages 50-59STU-Net H-ages 60-69STU-Net H-ages 70-79STU-Net H-ages 80-89STU-Net H-ages 90-99 nnU-Net ResEncL-ages 18-29nnU-Net ResEncL-ages 30-39nnU-Net ResEncL-ages 40-49nnU-Net ResEncL-ages 50-59nnU-Net ResEncL-ages 60-69nnU-Net ResEncL-ages 70-79nnU-Net ResEncL-ages 80-89nnU-Net ResEncL-ages 90-99 MedFormer-ages 18-29MedFormer-ages 30-39MedFormer-ages 40-49MedFormer-ages 50-59MedFormer-ages 60-69MedFormer-ages 70-79MedFormer-ages 80-89MedFormer-ages 90-99 nnU-Net U-Net-ages 18-29nnU-Net U-Net-ages 30-39nnU-Net U-Net-ages 40-49nnU-Net U-Net-ages 50-59nnU-Net U-Net-ages 60-69nnU-Net U-Net-ages 70-79nnU-Net U-Net-ages 80-89nnU-Net U-Net-ages 90-99 UniSeg-ages 18-29UniSeg-ages 30-39UniSeg-ages 40-49UniSeg-ages 50-59UniSeg-ages 60-69UniSeg-ages 70-79UniSeg-ages 80-89UniSeg-ages 90-99 Diff-UNet-ages 18-29Diff-UNet-ages 30-39Diff-UNet-ages 40-49Diff-UNet-ages 50-59Diff-UNet-ages 60-69Diff-UNet-ages 70-79Diff-UNet-ages 80-89Diff-UNet-ages 90-99 NexToU-ages 18-29NexToU-ages 30-39NexToU-ages 40-49NexToU-ages 50-59NexToU-ages 60-69NexToU-ages 70-79NexToU-ages 80-89NexToU-ages 90-99SegVol-ages 18-29SegVol-ages 30-39SegVol-ages 40-49SegVol-ages 50-59SegVol-ages 60-69SegVol-ages 70-79SegVol-ages 80-89SegVol-ages 90-99 U-Net & CLIP-ages 18-29U-Net & CLIP-ages 30-39U-Net & CLIP-ages 40-49U-Net & CLIP-ages 50-59U-Net & CLIP-ages 60-69U-Net & CLIP-ages 70-79U-Net & CLIP-ages 80-89U-Net & CLIP-ages 90-99 SwinUNETR & CLIP-ages 18-29SwinUNETR & CLIP-ages 30-39SwinUNETR & CLIP-ages 40-49SwinUNETR & CLIP-ages 50-59SwinUNETR & CLIP-ages 60-69SwinUNETR & CLIP-ages 70-79SwinUNETR & CLIP-ages 80-89SwinUNETR & CLIP-ages 90-99 LHU-Net-ages 18-29LHU-Net-ages 30-39LHU-Net-ages 40-49LHU-Net-ages 50-59LHU-Net-ages 60-69LHU-Net-ages 70-79LHU-Net-ages 80-89LHU-Net-ages 90-99 UCTransNet-ages 18-29UCTransNet-ages 30-39UCTransNet-ages 40-49UCTransNet-ages 50-59UCTransNet-ages 60-69UCTransNet-ages 70-79UCTransNet-ages 80-89UCTransNet-ages 90-99SwinUNETR-ages 18-29SwinUNETR-ages 30-39SwinUNETR-ages 40-49SwinUNETR-ages 50-59SwinUNETR-ages 60-69SwinUNETR-ages 70-79SwinUNETR-ages 80-89SwinUNETR-ages 90-99 UNEST-ages 18-29UNEST-ages 30-39UNEST-ages 40-49UNEST-ages 50-59UNEST-ages 60-69UNEST-ages 70-79UNEST-ages 80-89UNEST-ages 90-99UNETR-ages 18-29UNETR-ages 30-39UNETR-ages 40-49UNETR-ages 50-59UNETR-ages 60-69UNETR-ages 70-79UNETR-ages 80-89UNETR-ages 90-99 SAM-Adapter-ages 18-29SAM-Adapter-ages 30-39SAM-Adapter-ages 40-49SAM-Adapter-ages 50-59SAM-Adapter-ages 60-69SAM-Adapter-ages 70-79SAM-Adapter-ages 80-89SAM-Adapter-ages 90-99 AI Algorithm-Group mean DSC by ages in TotalSegmentator : Boxplot showing average DSC score by age in the whole TotalSegmentator dataset.Statistical significance is indicated by stars: * p < 0.05, ** p <0.01, *** p < 0.001, **** p < 0.0001.We perform KruskalWallis tests followed by post-hoc Mann-Whitney U Tests with Bonferroni cor-rection. Here, we did not perform statistical comparisons between diverse AI algorithms. Significantdifferences are not observed, possibly due to the higher variability in the TotalSegmentator results,when compared to other datasets.",
  "D.5.2Diagnosis": "0.00.20.40.60.81.0 DSC MedNeXt-negative MedNeXt-cancer STU-Net B-negative STU-Net B-cancer STU-Net L-negative STU-Net L-cancer STU-Net H-negative STU-Net H-cancer nnU-Net ResEncL-negative nnU-Net ResEncL-cancer MedFormer-negative MedFormer-cancer nnU-Net U-Net-negative nnU-Net U-Net-cancer UniSeg-negative UniSeg-cancer Diff-UNet-negative Diff-UNet-cancerNexToU-negative NexToU-cancer SegVol-negative SegVol-cancer U-Net & CLIP-negative U-Net & CLIP-cancer SwinUNETR & CLIP-negative SwinUNETR & CLIP-cancer LHU-Net-negative LHU-Net-cancer UCTransNet-negative UCTransNet-cancer SwinUNETR-negative SwinUNETR-cancer UNEST-negative UNEST-cancer UNETR-negative UNETR-cancer SAM-Adapter-negative SAM-Adapter-cancer AI Algorithm-Group ******************************************************** **************** **** mean DSC by diagnosis in JHH : Boxplot showing average DSC score by diagnosis in JHH. Statistical significance isindicated by stars: * p < 0.05, ** p <0.01, *** p < 0.001, **** p < 0.0001. We perform KruskalWallistests followed by post-hoc Mann-Whitney U Tests with Bonferroni correction. Here, we did notperform statistical comparisons between diverse AI algorithms. 0.00.20.40.60.81.0 DSC MedNeXt-other MedNeXt-vascularMedNeXt-unclear MedNeXt-no pathologyMedNeXt-inflammation MedNeXt-tumor MedNeXt-bleeding MedNeXt-traumaSTU-Net B-other STU-Net B-vascularSTU-Net B-unclear STU-Net B-no pathologySTU-Net B-inflammation STU-Net B-tumor STU-Net B-bleeding STU-Net B-trauma STU-Net L-other STU-Net L-vascularSTU-Net L-unclear STU-Net L-no pathologySTU-Net L-inflammation STU-Net L-tumor STU-Net L-bleeding STU-Net L-trauma STU-Net H-other STU-Net H-vascularSTU-Net H-unclear STU-Net H-no pathologySTU-Net H-inflammation STU-Net H-tumor STU-Net H-bleeding STU-Net H-trauma nnU-Net ResEncL-other nnU-Net ResEncL-vascularnnU-Net ResEncL-unclear nnU-Net ResEncL-no pathologynnU-Net ResEncL-inflammation nnU-Net ResEncL-tumor nnU-Net ResEncL-bleeding nnU-Net ResEncL-trauma MedFormer-other MedFormer-vascularMedFormer-unclear MedFormer-no pathologyMedFormer-inflammation MedFormer-tumor MedFormer-bleeding MedFormer-trauma nnU-Net U-Net-other nnU-Net U-Net-vascularnnU-Net U-Net-unclear nnU-Net U-Net-no pathologynnU-Net U-Net-inflammation nnU-Net U-Net-tumor nnU-Net U-Net-bleeding nnU-Net U-Net-trauma UniSeg-other UniSeg-vascularUniSeg-unclear UniSeg-no pathologyUniSeg-inflammation UniSeg-tumor UniSeg-bleeding UniSeg-traumaDiff-UNet-other Diff-UNet-vascularDiff-UNet-unclear Diff-UNet-no pathologyDiff-UNet-inflammation Diff-UNet-tumor Diff-UNet-bleeding Diff-UNet-trauma NexToU-other NexToU-vascularNexToU-unclear NexToU-no pathologyNexToU-inflammation NexToU-tumor NexToU-bleeding NexToU-trauma SegVol-other SegVol-vascularSegVol-unclear SegVol-no pathologySegVol-inflammation SegVol-tumor SegVol-bleeding SegVol-trauma U-Net & CLIP-other U-Net & CLIP-vascularU-Net & CLIP-unclear U-Net & CLIP-no pathologyU-Net & CLIP-inflammation U-Net & CLIP-tumor U-Net & CLIP-bleeding U-Net & CLIP-trauma SwinUNETR & CLIP-other SwinUNETR & CLIP-vascularSwinUNETR & CLIP-unclear SwinUNETR & CLIP-no pathologySwinUNETR & CLIP-inflammation SwinUNETR & CLIP-tumor SwinUNETR & CLIP-bleeding SwinUNETR & CLIP-trauma LHU-Net-other LHU-Net-vascularLHU-Net-unclear LHU-Net-no pathologyLHU-Net-inflammation LHU-Net-tumor LHU-Net-bleeding LHU-Net-trauma UCTransNet-other UCTransNet-vascularUCTransNet-unclear UCTransNet-no pathologyUCTransNet-inflammation UCTransNet-tumor UCTransNet-bleeding UCTransNet-trauma SwinUNETR-other SwinUNETR-vascularSwinUNETR-unclear SwinUNETR-no pathologySwinUNETR-inflammation SwinUNETR-tumor SwinUNETR-bleeding SwinUNETR-trauma UNEST-other UNEST-vascularUNEST-unclear UNEST-no pathologyUNEST-inflammation UNEST-tumor UNEST-bleeding UNEST-trauma UNETR-other UNETR-vascularUNETR-unclear UNETR-no pathologyUNETR-inflammation UNETR-tumor UNETR-bleeding UNETR-trauma SAM-Adapter-other SAM-Adapter-vascularSAM-Adapter-unclear SAM-Adapter-no pathologySAM-Adapter-inflammation SAM-Adapter-tumor SAM-Adapter-bleeding SAM-Adapter-trauma AI Algorithm-Group **** ** * *** ** ** * * ***** ****** **** *** *** ** ****** ****** **** **** **** **** ** ** **** ***** **** *** **** ** ***** **** ** *** ** *** mean DSC by diagnosis in TotalSegmentator : Boxplot showing average DSC score by diagnosis in the whole TotalSegmentatordataset. Statistical significance is indicated by stars: * p < 0.05, ** p <0.01, *** p < 0.001,**** p < 0.0001. We perform KruskalWallis tests followed by post-hoc Mann-Whitney U Testswith Bonferroni correction. Here, we did not perform statistical comparisons between diverse AIalgorithms.",
  "D.5.3Sex": "0.00.20.40.60.81.0 DSC MedNeXt-male MedNeXt-femaleSTU-Net B-male STU-Net B-female STU-Net L-male STU-Net L-female STU-Net H-male STU-Net H-female nnU-Net ResEncL-male nnU-Net ResEncL-female MedFormer-male MedFormer-female nnU-Net U-Net-male nnU-Net U-Net-female UniSeg-male UniSeg-femaleDiff-UNet-male Diff-UNet-female NexToU-male NexToU-female SegVol-male SegVol-female U-Net & CLIP-male U-Net & CLIP-female SwinUNETR & CLIP-male SwinUNETR & CLIP-female LHU-Net-male LHU-Net-female UCTransNet-male UCTransNet-female SwinUNETR-male SwinUNETR-female UNEST-male UNEST-female UNETR-male UNETR-female SAM-Adapter-male SAM-Adapter-female AI Algorithm-Group **************** **** mean DSC by sex in JHH : Boxplot showing average DSC score by sex in JHH. Statistical significance is indicatedby stars: * p < 0.05, ** p <0.01, *** p < 0.001, **** p < 0.0001. We perform KruskalWallis testsfollowed by post-hoc Mann-Whitney U Tests with Bonferroni correction. Here, we did not performstatistical comparisons between diverse AI algorithms. Only the worst performing algorithms showsignificant performance difference for the male and female groups, with better scores for male. Thebest performing models show no significant difference. 0.00.20.40.60.81.0 DSC MedNeXt-male MedNeXt-femaleSTU-Net B-male STU-Net B-female STU-Net L-male STU-Net L-female STU-Net H-male STU-Net H-female nnU-Net ResEncL-male nnU-Net ResEncL-female MedFormer-male MedFormer-female nnU-Net U-Net-male nnU-Net U-Net-female UniSeg-male UniSeg-femaleDiff-UNet-male Diff-UNet-female NexToU-male NexToU-female SegVol-male SegVol-female U-Net & CLIP-male U-Net & CLIP-female SwinUNETR & CLIP-male SwinUNETR & CLIP-female LHU-Net-male LHU-Net-female UCTransNet-male UCTransNet-female SwinUNETR-male SwinUNETR-female UNEST-male UNEST-female UNETR-male UNETR-female SAM-Adapter-male SAM-Adapter-female AI Algorithm-Group * mean DSC by sex in TotalSegmentator : Boxplot showing average DSC score by sex in the whole TotalSegmentator dataset.Statistical significance is indicated by stars: * p < 0.05, ** p <0.01, *** p < 0.001, **** p < 0.0001.We perform KruskalWallis tests followed by post-hoc Mann-Whitney U Tests with Bonferronicorrection. Here, we did not perform statistical comparisons between diverse AI algorithms. Onlythe worst performing algorithms show significant performance difference for the male and femalegroups, with better scores for male. The best performing models show no significant difference.",
  "D.5.4Race": "0.00.20.40.60.81.0 DSC MedNeXt-race O MedNeXt-race ASMedNeXt-race WMedNeXt-race HLMedNeXt-race AA MedNeXt-race U STU-Net B-race O STU-Net B-race ASSTU-Net B-race WSTU-Net B-race HLSTU-Net B-race AA STU-Net B-race USTU-Net L-race O STU-Net L-race ASSTU-Net L-race WSTU-Net L-race HLSTU-Net L-race AA STU-Net L-race USTU-Net H-race O STU-Net H-race ASSTU-Net H-race WSTU-Net H-race HLSTU-Net H-race AA STU-Net H-race U nnU-Net ResEncL-race O nnU-Net ResEncL-race ASnnU-Net ResEncL-race WnnU-Net ResEncL-race HLnnU-Net ResEncL-race AA nnU-Net ResEncL-race U MedFormer-race O MedFormer-race ASMedFormer-race WMedFormer-race HLMedFormer-race AA MedFormer-race U nnU-Net U-Net-race O nnU-Net U-Net-race ASnnU-Net U-Net-race WnnU-Net U-Net-race HLnnU-Net U-Net-race AA nnU-Net U-Net-race U UniSeg-race O UniSeg-race ASUniSeg-race WUniSeg-race HLUniSeg-race AA UniSeg-race U Diff-UNet-race O Diff-UNet-race ASDiff-UNet-race WDiff-UNet-race HLDiff-UNet-race AA Diff-UNet-race U NexToU-race O NexToU-race ASNexToU-race WNexToU-race HLNexToU-race AA NexToU-race USegVol-race O SegVol-race ASSegVol-race WSegVol-race HLSegVol-race AA SegVol-race U U-Net & CLIP-race O U-Net & CLIP-race ASU-Net & CLIP-race WU-Net & CLIP-race HLU-Net & CLIP-race AA U-Net & CLIP-race U SwinUNETR & CLIP-race O SwinUNETR & CLIP-race ASSwinUNETR & CLIP-race WSwinUNETR & CLIP-race HLSwinUNETR & CLIP-race AA SwinUNETR & CLIP-race U LHU-Net-race O LHU-Net-race ASLHU-Net-race WLHU-Net-race HLLHU-Net-race AA LHU-Net-race U UCTransNet-race O UCTransNet-race ASUCTransNet-race WUCTransNet-race HLUCTransNet-race AA UCTransNet-race USwinUNETR-race O SwinUNETR-race ASSwinUNETR-race WSwinUNETR-race HLSwinUNETR-race AA SwinUNETR-race U UNEST-race O UNEST-race ASUNEST-race WUNEST-race HLUNEST-race AA UNEST-race UUNETR-race O UNETR-race ASUNETR-race WUNETR-race HLUNETR-race AA UNETR-race U SAM-Adapter-race O SAM-Adapter-race ASSAM-Adapter-race WSAM-Adapter-race HLSAM-Adapter-race AA SAM-Adapter-race U AI Algorithm-Group *** * ** *********** *** ********************************* ************ **** ***** ******************* **** ***************** ******* *** ** **** * mean DSC by race in JHH : Boxplot showing average DSC score by race in JHH. Statistical significance is indicatedby stars: * p < 0.05, ** p <0.01, *** p < 0.001, **** p < 0.0001. We perform KruskalWallis testsfollowed by post-hoc Mann-Whitney U Tests with Bonferroni correction. Here, we did not performstatistical comparisons between diverse AI algorithms. Only some algorithms show significant per-formance differences across race groups. In these cases, the white or Asian groups have significantlybetter results than African American or Hispanic Latino (usually than African American). Possibly,this finding indicates a predominance of white and Asian people in the training data, and the necessityof increasing the proportion of African Americans and Hispanic Latinos in the training dataset.",
  "D.5.5Manufacturer": "0.00.20.40.60.81.0 DSC MedNeXt-Philips MedNeXt-GE MedNeXt-Siemens STU-Net B-Philips STU-Net B-GE STU-Net B-Siemens STU-Net L-Philips STU-Net L-GE STU-Net L-Siemens STU-Net H-Philips STU-Net H-GE STU-Net H-Siemens nnU-Net ResEncL-Philips nnU-Net ResEncL-GE nnU-Net ResEncL-Siemens MedFormer-Philips MedFormer-GE MedFormer-Siemens nnU-Net U-Net-Philips nnU-Net U-Net-GE nnU-Net U-Net-Siemens UniSeg-Philips UniSeg-GE UniSeg-SiemensDiff-UNet-Philips Diff-UNet-GE Diff-UNet-Siemens NexToU-Philips NexToU-GE NexToU-Siemens SegVol-Philips SegVol-GE SegVol-Siemens U-Net & CLIP-Philips U-Net & CLIP-GE U-Net & CLIP-Siemens SwinUNETR & CLIP-Philips SwinUNETR & CLIP-GE SwinUNETR & CLIP-Siemens LHU-Net-Philips LHU-Net-GE LHU-Net-Siemens UCTransNet-Philips UCTransNet-GE UCTransNet-Siemens SwinUNETR-Philips SwinUNETR-GE SwinUNETR-Siemens UNEST-Philips UNEST-GE UNEST-Siemens UNETR-Philips UNETR-GE UNETR-Siemens SAM-Adapter-Philips SAM-Adapter-GE SAM-Adapter-Siemens AI Algorithm-Group * mean DSC by manufacturer in TotalSegmentator : Boxplot showing average DSC score by manufacturer in the whole TotalSegmentatordataset. Statistical significance is indicated by stars: * p < 0.05, ** p <0.01, *** p < 0.001,**** p < 0.0001. We perform KruskalWallis tests followed by post-hoc Mann-Whitney U Testswith Bonferroni correction. Here, we did not perform statistical comparisons between diverse AIalgorithms.",
  "D.5.6Institutes": "0.00.20.40.60.81.0 DSC MedNeXt-institute BMedNeXt-institute EMedNeXt-institute CMedNeXt-institute IMedNeXt-institute AMedNeXt-institute GSTU-Net B-institute BSTU-Net B-institute ESTU-Net B-institute CSTU-Net B-institute ISTU-Net B-institute ASTU-Net B-institute GSTU-Net L-institute BSTU-Net L-institute ESTU-Net L-institute CSTU-Net L-institute ISTU-Net L-institute ASTU-Net L-institute GSTU-Net H-institute BSTU-Net H-institute ESTU-Net H-institute CSTU-Net H-institute ISTU-Net H-institute ASTU-Net H-institute G nnU-Net ResEncL-institute BnnU-Net ResEncL-institute EnnU-Net ResEncL-institute CnnU-Net ResEncL-institute InnU-Net ResEncL-institute AnnU-Net ResEncL-institute G MedFormer-institute BMedFormer-institute EMedFormer-institute CMedFormer-institute IMedFormer-institute AMedFormer-institute G nnU-Net U-Net-institute BnnU-Net U-Net-institute EnnU-Net U-Net-institute CnnU-Net U-Net-institute InnU-Net U-Net-institute AnnU-Net U-Net-institute G UniSeg-institute BUniSeg-institute EUniSeg-institute CUniSeg-institute IUniSeg-institute AUniSeg-institute G Diff-UNet-institute BDiff-UNet-institute EDiff-UNet-institute CDiff-UNet-institute IDiff-UNet-institute ADiff-UNet-institute G NexToU-institute BNexToU-institute ENexToU-institute CNexToU-institute INexToU-institute ANexToU-institute G SegVol-institute BSegVol-institute ESegVol-institute CSegVol-institute ISegVol-institute ASegVol-institute G U-Net & CLIP-institute BU-Net & CLIP-institute EU-Net & CLIP-institute CU-Net & CLIP-institute IU-Net & CLIP-institute AU-Net & CLIP-institute G SwinUNETR & CLIP-institute BSwinUNETR & CLIP-institute ESwinUNETR & CLIP-institute CSwinUNETR & CLIP-institute ISwinUNETR & CLIP-institute ASwinUNETR & CLIP-institute G LHU-Net-institute BLHU-Net-institute ELHU-Net-institute CLHU-Net-institute ILHU-Net-institute ALHU-Net-institute G UCTransNet-institute BUCTransNet-institute EUCTransNet-institute CUCTransNet-institute IUCTransNet-institute AUCTransNet-institute GSwinUNETR-institute BSwinUNETR-institute ESwinUNETR-institute CSwinUNETR-institute ISwinUNETR-institute ASwinUNETR-institute G UNEST-institute BUNEST-institute EUNEST-institute CUNEST-institute IUNEST-institute AUNEST-institute GUNETR-institute BUNETR-institute EUNETR-institute CUNETR-institute IUNETR-institute AUNETR-institute G SAM-Adapter-institute BSAM-Adapter-institute ESAM-Adapter-institute CSAM-Adapter-institute ISAM-Adapter-institute ASAM-Adapter-institute G AI Algorithm-Group * * mean DSC by institute in TotalSegmentator : Boxplot showing average DSC score by institute in the whole TotalSegmentatordataset. Statistical significance is indicated by stars: * p < 0.05, ** p <0.01, *** p < 0.001,**** p < 0.0001. We perform KruskalWallis tests followed by post-hoc Mann-Whitney U Testswith Bonferroni correction. Here, we did not perform statistical comparisons between diverse AIalgorithms. Significant differences across institutes are observed for most AI algorithms, even thoughall institutes are located on the same country (Switzerland). This finding shows the difficulty of OODgeneralization.",
  "D.5.7Age: per-class analysis in JHH": "0.00.20.40.60.81.0 DSC MedNeXt-ages 16-29MedNeXt-ages 30-39MedNeXt-ages 40-49MedNeXt-ages 50-59MedNeXt-ages 60-69MedNeXt-ages 70-79MedNeXt-ages 80-89 STU-Net B-ages 16-29STU-Net B-ages 30-39STU-Net B-ages 40-49STU-Net B-ages 50-59STU-Net B-ages 60-69STU-Net B-ages 70-79STU-Net B-ages 80-89STU-Net L-ages 16-29STU-Net L-ages 30-39STU-Net L-ages 40-49STU-Net L-ages 50-59STU-Net L-ages 60-69STU-Net L-ages 70-79STU-Net L-ages 80-89STU-Net H-ages 16-29STU-Net H-ages 30-39STU-Net H-ages 40-49STU-Net H-ages 50-59STU-Net H-ages 60-69STU-Net H-ages 70-79STU-Net H-ages 80-89 nnU-Net ResEncL-ages 16-29nnU-Net ResEncL-ages 30-39nnU-Net ResEncL-ages 40-49nnU-Net ResEncL-ages 50-59nnU-Net ResEncL-ages 60-69nnU-Net ResEncL-ages 70-79nnU-Net ResEncL-ages 80-89 MedFormer-ages 16-29MedFormer-ages 30-39MedFormer-ages 40-49MedFormer-ages 50-59MedFormer-ages 60-69MedFormer-ages 70-79MedFormer-ages 80-89 nnU-Net U-Net-ages 16-29nnU-Net U-Net-ages 30-39nnU-Net U-Net-ages 40-49nnU-Net U-Net-ages 50-59nnU-Net U-Net-ages 60-69nnU-Net U-Net-ages 70-79nnU-Net U-Net-ages 80-89 UniSeg-ages 16-29UniSeg-ages 30-39UniSeg-ages 40-49UniSeg-ages 50-59UniSeg-ages 60-69UniSeg-ages 70-79UniSeg-ages 80-89 Diff-UNet-ages 16-29Diff-UNet-ages 30-39Diff-UNet-ages 40-49Diff-UNet-ages 50-59Diff-UNet-ages 60-69Diff-UNet-ages 70-79Diff-UNet-ages 80-89 NexToU-ages 16-29NexToU-ages 30-39NexToU-ages 40-49NexToU-ages 50-59NexToU-ages 60-69NexToU-ages 70-79NexToU-ages 80-89SegVol-ages 16-29SegVol-ages 30-39SegVol-ages 40-49SegVol-ages 50-59SegVol-ages 60-69SegVol-ages 70-79SegVol-ages 80-89 U-Net & CLIP-ages 16-29U-Net & CLIP-ages 30-39U-Net & CLIP-ages 40-49U-Net & CLIP-ages 50-59U-Net & CLIP-ages 60-69U-Net & CLIP-ages 70-79U-Net & CLIP-ages 80-89 SwinUNETR & CLIP-ages 16-29SwinUNETR & CLIP-ages 30-39SwinUNETR & CLIP-ages 40-49SwinUNETR & CLIP-ages 50-59SwinUNETR & CLIP-ages 60-69SwinUNETR & CLIP-ages 70-79SwinUNETR & CLIP-ages 80-89 LHU-Net-ages 16-29LHU-Net-ages 30-39LHU-Net-ages 40-49LHU-Net-ages 50-59LHU-Net-ages 60-69LHU-Net-ages 70-79LHU-Net-ages 80-89 UCTransNet-ages 16-29UCTransNet-ages 30-39UCTransNet-ages 40-49UCTransNet-ages 50-59UCTransNet-ages 60-69UCTransNet-ages 70-79UCTransNet-ages 80-89SwinUNETR-ages 16-29SwinUNETR-ages 30-39SwinUNETR-ages 40-49SwinUNETR-ages 50-59SwinUNETR-ages 60-69SwinUNETR-ages 70-79SwinUNETR-ages 80-89 UNEST-ages 16-29UNEST-ages 30-39UNEST-ages 40-49UNEST-ages 50-59UNEST-ages 60-69UNEST-ages 70-79UNEST-ages 80-89UNETR-ages 16-29UNETR-ages 30-39UNETR-ages 40-49UNETR-ages 50-59UNETR-ages 60-69UNETR-ages 70-79UNETR-ages 80-89 SAM-Adapter-ages 16-29SAM-Adapter-ages 30-39SAM-Adapter-ages 40-49SAM-Adapter-ages 50-59SAM-Adapter-ages 60-69SAM-Adapter-ages 70-79SAM-Adapter-ages 80-89 AI Algorithm-Group **** *** *** *** *** ** *** ** *** ** **** **** ** ** ** ** ** **** ** *** *** ** **** * **** ****** **** **** * **** *** *** **** **** *** **** *** ** **** *** ** **** *** *** **** * **** **** **** ** **** ** ** **** ** *** **** ** *** **** ** **** **** *** **** ** **** ******* **** **** ** ** **** **** ** **** **** **** **** **** **** **** **** **** ******** **** **** **** **** **** **** **** **** ******** **** ***** **** **** **** **** **** **** **** * * **** * **** * ************ ***** ** **** * ******** ** **** ***** ** ** **** ** **** **** **** ** ***** *** ***** **** ** **** * spleen DSC by ages in JHH : Boxplot showing spleen DSC score by age in JHH. Statistical significance is indicatedby stars: * p < 0.05, ** p <0.01, *** p < 0.001, **** p < 0.0001. We perform KruskalWallis testsfollowed by post-hoc Mann-Whitney U Tests with Bonferroni correction. Here, we did not performstatistical comparisons between diverse AI algorithms. 0.00.20.40.60.81.0 DSC MedNeXt-ages 16-29MedNeXt-ages 30-39MedNeXt-ages 40-49MedNeXt-ages 50-59MedNeXt-ages 60-69MedNeXt-ages 70-79MedNeXt-ages 80-89 STU-Net B-ages 16-29STU-Net B-ages 30-39STU-Net B-ages 40-49STU-Net B-ages 50-59STU-Net B-ages 60-69STU-Net B-ages 70-79STU-Net B-ages 80-89STU-Net L-ages 16-29STU-Net L-ages 30-39STU-Net L-ages 40-49STU-Net L-ages 50-59STU-Net L-ages 60-69STU-Net L-ages 70-79STU-Net L-ages 80-89STU-Net H-ages 16-29STU-Net H-ages 30-39STU-Net H-ages 40-49STU-Net H-ages 50-59STU-Net H-ages 60-69STU-Net H-ages 70-79STU-Net H-ages 80-89 nnU-Net ResEncL-ages 16-29nnU-Net ResEncL-ages 30-39nnU-Net ResEncL-ages 40-49nnU-Net ResEncL-ages 50-59nnU-Net ResEncL-ages 60-69nnU-Net ResEncL-ages 70-79nnU-Net ResEncL-ages 80-89 MedFormer-ages 16-29MedFormer-ages 30-39MedFormer-ages 40-49MedFormer-ages 50-59MedFormer-ages 60-69MedFormer-ages 70-79MedFormer-ages 80-89 nnU-Net U-Net-ages 16-29nnU-Net U-Net-ages 30-39nnU-Net U-Net-ages 40-49nnU-Net U-Net-ages 50-59nnU-Net U-Net-ages 60-69nnU-Net U-Net-ages 70-79nnU-Net U-Net-ages 80-89 UniSeg-ages 16-29UniSeg-ages 30-39UniSeg-ages 40-49UniSeg-ages 50-59UniSeg-ages 60-69UniSeg-ages 70-79UniSeg-ages 80-89 Diff-UNet-ages 16-29Diff-UNet-ages 30-39Diff-UNet-ages 40-49Diff-UNet-ages 50-59Diff-UNet-ages 60-69Diff-UNet-ages 70-79Diff-UNet-ages 80-89 NexToU-ages 16-29NexToU-ages 30-39NexToU-ages 40-49NexToU-ages 50-59NexToU-ages 60-69NexToU-ages 70-79NexToU-ages 80-89SegVol-ages 16-29SegVol-ages 30-39SegVol-ages 40-49SegVol-ages 50-59SegVol-ages 60-69SegVol-ages 70-79SegVol-ages 80-89 U-Net & CLIP-ages 16-29U-Net & CLIP-ages 30-39U-Net & CLIP-ages 40-49U-Net & CLIP-ages 50-59U-Net & CLIP-ages 60-69U-Net & CLIP-ages 70-79U-Net & CLIP-ages 80-89 SwinUNETR & CLIP-ages 16-29SwinUNETR & CLIP-ages 30-39SwinUNETR & CLIP-ages 40-49SwinUNETR & CLIP-ages 50-59SwinUNETR & CLIP-ages 60-69SwinUNETR & CLIP-ages 70-79SwinUNETR & CLIP-ages 80-89 LHU-Net-ages 16-29LHU-Net-ages 30-39LHU-Net-ages 40-49LHU-Net-ages 50-59LHU-Net-ages 60-69LHU-Net-ages 70-79LHU-Net-ages 80-89 UCTransNet-ages 16-29UCTransNet-ages 30-39UCTransNet-ages 40-49UCTransNet-ages 50-59UCTransNet-ages 60-69UCTransNet-ages 70-79UCTransNet-ages 80-89SwinUNETR-ages 16-29SwinUNETR-ages 30-39SwinUNETR-ages 40-49SwinUNETR-ages 50-59SwinUNETR-ages 60-69SwinUNETR-ages 70-79SwinUNETR-ages 80-89 UNEST-ages 16-29UNEST-ages 30-39UNEST-ages 40-49UNEST-ages 50-59UNEST-ages 60-69UNEST-ages 70-79UNEST-ages 80-89UNETR-ages 16-29UNETR-ages 30-39UNETR-ages 40-49UNETR-ages 50-59UNETR-ages 60-69UNETR-ages 70-79UNETR-ages 80-89 SAM-Adapter-ages 16-29SAM-Adapter-ages 30-39SAM-Adapter-ages 40-49SAM-Adapter-ages 50-59SAM-Adapter-ages 60-69SAM-Adapter-ages 70-79SAM-Adapter-ages 80-89 AI Algorithm-Group *** *** **** *** **** **** *** *** **** *** *** **** **** *** **** ** ** **** *** *** **** *** ** **** ** *** **** ** **** **** *** **** ****************** *** **** **** * * ******** **** * **** **** *** **** **** *** * **** **** *** ** **** **** **** ** **** **** **** *** **** **** *** * **** **** **** ** **** **** **** *** **** **** **** ** **** **** *** * **** **** * *** **** **** **** *** **** **** **** * **** **** * **** **** **** **** **** **** **** **** *** **** **** *** **** **** **** **** **** ** **** **** **** **** *** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** *** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** *** **** **** **** **** **** **** ** *** **** **** **** **** **** * **** **** **** **** **** **** **** **** **** ** **** *** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** ******** **** **** **** **** ******** **** **** **** ******** **** **** **** *** **** ************************************ **** **** ******** **** * **** ******** *** kidney right DSC by ages in JHH : Boxplot showing right kidney DSC score by age in JHH. Statistical significance isindicated by stars: * p < 0.05, ** p <0.01, *** p < 0.001, **** p < 0.0001. We perform KruskalWallistests followed by post-hoc Mann-Whitney U Tests with Bonferroni correction. Here, we did notperform statistical comparisons between diverse AI algorithms. 0.00.20.40.60.81.0 DSC MedNeXt-ages 16-29MedNeXt-ages 30-39MedNeXt-ages 40-49MedNeXt-ages 50-59MedNeXt-ages 60-69MedNeXt-ages 70-79MedNeXt-ages 80-89 STU-Net B-ages 16-29STU-Net B-ages 30-39STU-Net B-ages 40-49STU-Net B-ages 50-59STU-Net B-ages 60-69STU-Net B-ages 70-79STU-Net B-ages 80-89STU-Net L-ages 16-29STU-Net L-ages 30-39STU-Net L-ages 40-49STU-Net L-ages 50-59STU-Net L-ages 60-69STU-Net L-ages 70-79STU-Net L-ages 80-89STU-Net H-ages 16-29STU-Net H-ages 30-39STU-Net H-ages 40-49STU-Net H-ages 50-59STU-Net H-ages 60-69STU-Net H-ages 70-79STU-Net H-ages 80-89 nnU-Net ResEncL-ages 16-29nnU-Net ResEncL-ages 30-39nnU-Net ResEncL-ages 40-49nnU-Net ResEncL-ages 50-59nnU-Net ResEncL-ages 60-69nnU-Net ResEncL-ages 70-79nnU-Net ResEncL-ages 80-89 MedFormer-ages 16-29MedFormer-ages 30-39MedFormer-ages 40-49MedFormer-ages 50-59MedFormer-ages 60-69MedFormer-ages 70-79MedFormer-ages 80-89 nnU-Net U-Net-ages 16-29nnU-Net U-Net-ages 30-39nnU-Net U-Net-ages 40-49nnU-Net U-Net-ages 50-59nnU-Net U-Net-ages 60-69nnU-Net U-Net-ages 70-79nnU-Net U-Net-ages 80-89 UniSeg-ages 16-29UniSeg-ages 30-39UniSeg-ages 40-49UniSeg-ages 50-59UniSeg-ages 60-69UniSeg-ages 70-79UniSeg-ages 80-89 Diff-UNet-ages 16-29Diff-UNet-ages 30-39Diff-UNet-ages 40-49Diff-UNet-ages 50-59Diff-UNet-ages 60-69Diff-UNet-ages 70-79Diff-UNet-ages 80-89 NexToU-ages 16-29NexToU-ages 30-39NexToU-ages 40-49NexToU-ages 50-59NexToU-ages 60-69NexToU-ages 70-79NexToU-ages 80-89SegVol-ages 16-29SegVol-ages 30-39SegVol-ages 40-49SegVol-ages 50-59SegVol-ages 60-69SegVol-ages 70-79SegVol-ages 80-89 U-Net & CLIP-ages 16-29U-Net & CLIP-ages 30-39U-Net & CLIP-ages 40-49U-Net & CLIP-ages 50-59U-Net & CLIP-ages 60-69U-Net & CLIP-ages 70-79U-Net & CLIP-ages 80-89 SwinUNETR & CLIP-ages 16-29SwinUNETR & CLIP-ages 30-39SwinUNETR & CLIP-ages 40-49SwinUNETR & CLIP-ages 50-59SwinUNETR & CLIP-ages 60-69SwinUNETR & CLIP-ages 70-79SwinUNETR & CLIP-ages 80-89 LHU-Net-ages 16-29LHU-Net-ages 30-39LHU-Net-ages 40-49LHU-Net-ages 50-59LHU-Net-ages 60-69LHU-Net-ages 70-79LHU-Net-ages 80-89 UCTransNet-ages 16-29UCTransNet-ages 30-39UCTransNet-ages 40-49UCTransNet-ages 50-59UCTransNet-ages 60-69UCTransNet-ages 70-79UCTransNet-ages 80-89SwinUNETR-ages 16-29SwinUNETR-ages 30-39SwinUNETR-ages 40-49SwinUNETR-ages 50-59SwinUNETR-ages 60-69SwinUNETR-ages 70-79SwinUNETR-ages 80-89 UNEST-ages 16-29UNEST-ages 30-39UNEST-ages 40-49UNEST-ages 50-59UNEST-ages 60-69UNEST-ages 70-79UNEST-ages 80-89UNETR-ages 16-29UNETR-ages 30-39UNETR-ages 40-49UNETR-ages 50-59UNETR-ages 60-69UNETR-ages 70-79UNETR-ages 80-89 SAM-Adapter-ages 16-29SAM-Adapter-ages 30-39SAM-Adapter-ages 40-49SAM-Adapter-ages 50-59SAM-Adapter-ages 60-69SAM-Adapter-ages 70-79SAM-Adapter-ages 80-89 AI Algorithm-Group **************************************************************** **** ****** ** **** **** **** **** **** **** * **** **** **** * **** **** **** * **** **** **** * **** **** **** **** **** **** ** **** **** **** **** **** *** * **** **** *** * **** **** **** **** **** **** **** **** **** **** *** ** *** ** ** **** *** **** **** * **** **** **** **** * **** *** **** **** **** ** **** **** **** ** **** **** **** ** **** **** **** ** **** **** **** ** **** **** **** ** **** **** **** *** **** **** **** ** **** **** **** *** **** **** **** *** **** **** *** ** **** **** *** * **** *** **** ** **** **** **** **** *** * **** **** **** **** **** **** ******** **** *** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** ** **** ******** **** **** **** **** **** **** **** **** ******** **** **** **** **** ******** **** **** **** **** **** **** **** **** **** **** **** **** **** **** ******** **** **** ******** **** **** ************ **** ******** **** **************************** **** **** **** kidney left DSC by ages in JHH : Boxplot showing left kidney DSC score by age in JHH. Statistical significance isindicated by stars: * p < 0.05, ** p <0.01, *** p < 0.001, **** p < 0.0001. We perform KruskalWallistests followed by post-hoc Mann-Whitney U Tests with Bonferroni correction. Here, we did notperform statistical comparisons between diverse AI algorithms. 0.00.20.40.60.81.0 DSC MedNeXt-ages 16-29MedNeXt-ages 30-39MedNeXt-ages 40-49MedNeXt-ages 50-59MedNeXt-ages 60-69MedNeXt-ages 70-79MedNeXt-ages 80-89 STU-Net B-ages 16-29STU-Net B-ages 30-39STU-Net B-ages 40-49STU-Net B-ages 50-59STU-Net B-ages 60-69STU-Net B-ages 70-79STU-Net B-ages 80-89STU-Net L-ages 16-29STU-Net L-ages 30-39STU-Net L-ages 40-49STU-Net L-ages 50-59STU-Net L-ages 60-69STU-Net L-ages 70-79STU-Net L-ages 80-89STU-Net H-ages 16-29STU-Net H-ages 30-39STU-Net H-ages 40-49STU-Net H-ages 50-59STU-Net H-ages 60-69STU-Net H-ages 70-79STU-Net H-ages 80-89 nnU-Net ResEncL-ages 16-29nnU-Net ResEncL-ages 30-39nnU-Net ResEncL-ages 40-49nnU-Net ResEncL-ages 50-59nnU-Net ResEncL-ages 60-69nnU-Net ResEncL-ages 70-79nnU-Net ResEncL-ages 80-89 MedFormer-ages 16-29MedFormer-ages 30-39MedFormer-ages 40-49MedFormer-ages 50-59MedFormer-ages 60-69MedFormer-ages 70-79MedFormer-ages 80-89 nnU-Net U-Net-ages 16-29nnU-Net U-Net-ages 30-39nnU-Net U-Net-ages 40-49nnU-Net U-Net-ages 50-59nnU-Net U-Net-ages 60-69nnU-Net U-Net-ages 70-79nnU-Net U-Net-ages 80-89 UniSeg-ages 16-29UniSeg-ages 30-39UniSeg-ages 40-49UniSeg-ages 50-59UniSeg-ages 60-69UniSeg-ages 70-79UniSeg-ages 80-89 Diff-UNet-ages 16-29Diff-UNet-ages 30-39Diff-UNet-ages 40-49Diff-UNet-ages 50-59Diff-UNet-ages 60-69Diff-UNet-ages 70-79Diff-UNet-ages 80-89 NexToU-ages 16-29NexToU-ages 30-39NexToU-ages 40-49NexToU-ages 50-59NexToU-ages 60-69NexToU-ages 70-79NexToU-ages 80-89SegVol-ages 16-29SegVol-ages 30-39SegVol-ages 40-49SegVol-ages 50-59SegVol-ages 60-69SegVol-ages 70-79SegVol-ages 80-89 U-Net & CLIP-ages 16-29U-Net & CLIP-ages 30-39U-Net & CLIP-ages 40-49U-Net & CLIP-ages 50-59U-Net & CLIP-ages 60-69U-Net & CLIP-ages 70-79U-Net & CLIP-ages 80-89 SwinUNETR & CLIP-ages 16-29SwinUNETR & CLIP-ages 30-39SwinUNETR & CLIP-ages 40-49SwinUNETR & CLIP-ages 50-59SwinUNETR & CLIP-ages 60-69SwinUNETR & CLIP-ages 70-79SwinUNETR & CLIP-ages 80-89 LHU-Net-ages 16-29LHU-Net-ages 30-39LHU-Net-ages 40-49LHU-Net-ages 50-59LHU-Net-ages 60-69LHU-Net-ages 70-79LHU-Net-ages 80-89 UCTransNet-ages 16-29UCTransNet-ages 30-39UCTransNet-ages 40-49UCTransNet-ages 50-59UCTransNet-ages 60-69UCTransNet-ages 70-79UCTransNet-ages 80-89SwinUNETR-ages 16-29SwinUNETR-ages 30-39SwinUNETR-ages 40-49SwinUNETR-ages 50-59SwinUNETR-ages 60-69SwinUNETR-ages 70-79SwinUNETR-ages 80-89 UNEST-ages 16-29UNEST-ages 30-39UNEST-ages 40-49UNEST-ages 50-59UNEST-ages 60-69UNEST-ages 70-79UNEST-ages 80-89UNETR-ages 16-29UNETR-ages 30-39UNETR-ages 40-49UNETR-ages 50-59UNETR-ages 60-69UNETR-ages 70-79UNETR-ages 80-89 SAM-Adapter-ages 16-29SAM-Adapter-ages 30-39SAM-Adapter-ages 40-49SAM-Adapter-ages 50-59SAM-Adapter-ages 60-69SAM-Adapter-ages 70-79SAM-Adapter-ages 80-89 AI Algorithm-Group *********** * **** * ****************** ******* **** ******* **** *** **** ** **** **** ** **** ** *** ** *** ** ********** * *** ******** ** ****** ** **** **** ** ******* * **** *** ******** ******** ******** **** ** ** *** **** ** **** **** ** * *** **** ** *** ** gall bladder DSC by ages in JHH : Boxplot showing gall bladder DSC score by age in JHH. Statistical significance isindicated by stars: * p < 0.05, ** p <0.01, *** p < 0.001, **** p < 0.0001. We perform KruskalWallistests followed by post-hoc Mann-Whitney U Tests with Bonferroni correction. Here, we did notperform statistical comparisons between diverse AI algorithms. 0.00.20.40.60.81.0 DSC MedNeXt-ages 16-29MedNeXt-ages 30-39MedNeXt-ages 40-49MedNeXt-ages 50-59MedNeXt-ages 60-69MedNeXt-ages 70-79MedNeXt-ages 80-89 STU-Net B-ages 16-29STU-Net B-ages 30-39STU-Net B-ages 40-49STU-Net B-ages 50-59STU-Net B-ages 60-69STU-Net B-ages 70-79STU-Net B-ages 80-89STU-Net L-ages 16-29STU-Net L-ages 30-39STU-Net L-ages 40-49STU-Net L-ages 50-59STU-Net L-ages 60-69STU-Net L-ages 70-79STU-Net L-ages 80-89STU-Net H-ages 16-29STU-Net H-ages 30-39STU-Net H-ages 40-49STU-Net H-ages 50-59STU-Net H-ages 60-69STU-Net H-ages 70-79STU-Net H-ages 80-89 nnU-Net ResEncL-ages 16-29nnU-Net ResEncL-ages 30-39nnU-Net ResEncL-ages 40-49nnU-Net ResEncL-ages 50-59nnU-Net ResEncL-ages 60-69nnU-Net ResEncL-ages 70-79nnU-Net ResEncL-ages 80-89 MedFormer-ages 16-29MedFormer-ages 30-39MedFormer-ages 40-49MedFormer-ages 50-59MedFormer-ages 60-69MedFormer-ages 70-79MedFormer-ages 80-89 nnU-Net U-Net-ages 16-29nnU-Net U-Net-ages 30-39nnU-Net U-Net-ages 40-49nnU-Net U-Net-ages 50-59nnU-Net U-Net-ages 60-69nnU-Net U-Net-ages 70-79nnU-Net U-Net-ages 80-89 UniSeg-ages 16-29UniSeg-ages 30-39UniSeg-ages 40-49UniSeg-ages 50-59UniSeg-ages 60-69UniSeg-ages 70-79UniSeg-ages 80-89 Diff-UNet-ages 16-29Diff-UNet-ages 30-39Diff-UNet-ages 40-49Diff-UNet-ages 50-59Diff-UNet-ages 60-69Diff-UNet-ages 70-79Diff-UNet-ages 80-89 NexToU-ages 16-29NexToU-ages 30-39NexToU-ages 40-49NexToU-ages 50-59NexToU-ages 60-69NexToU-ages 70-79NexToU-ages 80-89SegVol-ages 16-29SegVol-ages 30-39SegVol-ages 40-49SegVol-ages 50-59SegVol-ages 60-69SegVol-ages 70-79SegVol-ages 80-89 U-Net & CLIP-ages 16-29U-Net & CLIP-ages 30-39U-Net & CLIP-ages 40-49U-Net & CLIP-ages 50-59U-Net & CLIP-ages 60-69U-Net & CLIP-ages 70-79U-Net & CLIP-ages 80-89 SwinUNETR & CLIP-ages 16-29SwinUNETR & CLIP-ages 30-39SwinUNETR & CLIP-ages 40-49SwinUNETR & CLIP-ages 50-59SwinUNETR & CLIP-ages 60-69SwinUNETR & CLIP-ages 70-79SwinUNETR & CLIP-ages 80-89 LHU-Net-ages 16-29LHU-Net-ages 30-39LHU-Net-ages 40-49LHU-Net-ages 50-59LHU-Net-ages 60-69LHU-Net-ages 70-79LHU-Net-ages 80-89 UCTransNet-ages 16-29UCTransNet-ages 30-39UCTransNet-ages 40-49UCTransNet-ages 50-59UCTransNet-ages 60-69UCTransNet-ages 70-79UCTransNet-ages 80-89SwinUNETR-ages 16-29SwinUNETR-ages 30-39SwinUNETR-ages 40-49SwinUNETR-ages 50-59SwinUNETR-ages 60-69SwinUNETR-ages 70-79SwinUNETR-ages 80-89 UNEST-ages 16-29UNEST-ages 30-39UNEST-ages 40-49UNEST-ages 50-59UNEST-ages 60-69UNEST-ages 70-79UNEST-ages 80-89UNETR-ages 16-29UNETR-ages 30-39UNETR-ages 40-49UNETR-ages 50-59UNETR-ages 60-69UNETR-ages 70-79UNETR-ages 80-89 SAM-Adapter-ages 16-29SAM-Adapter-ages 30-39SAM-Adapter-ages 40-49SAM-Adapter-ages 50-59SAM-Adapter-ages 60-69SAM-Adapter-ages 70-79SAM-Adapter-ages 80-89 AI Algorithm-Group ******** * ** **** ** **** ** *** ** * **** ** * **** **** ** *** ** *** **** * ******* **** **** ***** **** **** **** **** **** **** **** ***** **** **** **** *** ** **** **** **** ***** ** *** **** **** **** * **** ****** **** **** ** * *** ** ** **** **** **** **** **** **** **** **** **** **** *** **** **** **** **** ******** ******** **** *** **** * **** **** **** **** * ** ** **** **** ******** **** *** **** *** **** **** *** **** **** *** * * **** liver DSC by ages in JHH : Boxplot showing liver DSC score by age in JHH. Statistical significance is indicatedby stars: * p < 0.05, ** p <0.01, *** p < 0.001, **** p < 0.0001. We perform KruskalWallis testsfollowed by post-hoc Mann-Whitney U Tests with Bonferroni correction. Here, we did not performstatistical comparisons between diverse AI algorithms. 0.00.20.40.60.81.0 DSC MedNeXt-ages 16-29MedNeXt-ages 30-39MedNeXt-ages 40-49MedNeXt-ages 50-59MedNeXt-ages 60-69MedNeXt-ages 70-79MedNeXt-ages 80-89 STU-Net B-ages 16-29STU-Net B-ages 30-39STU-Net B-ages 40-49STU-Net B-ages 50-59STU-Net B-ages 60-69STU-Net B-ages 70-79STU-Net B-ages 80-89STU-Net L-ages 16-29STU-Net L-ages 30-39STU-Net L-ages 40-49STU-Net L-ages 50-59STU-Net L-ages 60-69STU-Net L-ages 70-79STU-Net L-ages 80-89STU-Net H-ages 16-29STU-Net H-ages 30-39STU-Net H-ages 40-49STU-Net H-ages 50-59STU-Net H-ages 60-69STU-Net H-ages 70-79STU-Net H-ages 80-89 nnU-Net ResEncL-ages 16-29nnU-Net ResEncL-ages 30-39nnU-Net ResEncL-ages 40-49nnU-Net ResEncL-ages 50-59nnU-Net ResEncL-ages 60-69nnU-Net ResEncL-ages 70-79nnU-Net ResEncL-ages 80-89 MedFormer-ages 16-29MedFormer-ages 30-39MedFormer-ages 40-49MedFormer-ages 50-59MedFormer-ages 60-69MedFormer-ages 70-79MedFormer-ages 80-89 nnU-Net U-Net-ages 16-29nnU-Net U-Net-ages 30-39nnU-Net U-Net-ages 40-49nnU-Net U-Net-ages 50-59nnU-Net U-Net-ages 60-69nnU-Net U-Net-ages 70-79nnU-Net U-Net-ages 80-89 UniSeg-ages 16-29UniSeg-ages 30-39UniSeg-ages 40-49UniSeg-ages 50-59UniSeg-ages 60-69UniSeg-ages 70-79UniSeg-ages 80-89 Diff-UNet-ages 16-29Diff-UNet-ages 30-39Diff-UNet-ages 40-49Diff-UNet-ages 50-59Diff-UNet-ages 60-69Diff-UNet-ages 70-79Diff-UNet-ages 80-89 NexToU-ages 16-29NexToU-ages 30-39NexToU-ages 40-49NexToU-ages 50-59NexToU-ages 60-69NexToU-ages 70-79NexToU-ages 80-89SegVol-ages 16-29SegVol-ages 30-39SegVol-ages 40-49SegVol-ages 50-59SegVol-ages 60-69SegVol-ages 70-79SegVol-ages 80-89 U-Net & CLIP-ages 16-29U-Net & CLIP-ages 30-39U-Net & CLIP-ages 40-49U-Net & CLIP-ages 50-59U-Net & CLIP-ages 60-69U-Net & CLIP-ages 70-79U-Net & CLIP-ages 80-89 SwinUNETR & CLIP-ages 16-29SwinUNETR & CLIP-ages 30-39SwinUNETR & CLIP-ages 40-49SwinUNETR & CLIP-ages 50-59SwinUNETR & CLIP-ages 60-69SwinUNETR & CLIP-ages 70-79SwinUNETR & CLIP-ages 80-89 LHU-Net-ages 16-29LHU-Net-ages 30-39LHU-Net-ages 40-49LHU-Net-ages 50-59LHU-Net-ages 60-69LHU-Net-ages 70-79LHU-Net-ages 80-89 UCTransNet-ages 16-29UCTransNet-ages 30-39UCTransNet-ages 40-49UCTransNet-ages 50-59UCTransNet-ages 60-69UCTransNet-ages 70-79UCTransNet-ages 80-89SwinUNETR-ages 16-29SwinUNETR-ages 30-39SwinUNETR-ages 40-49SwinUNETR-ages 50-59SwinUNETR-ages 60-69SwinUNETR-ages 70-79SwinUNETR-ages 80-89 UNEST-ages 16-29UNEST-ages 30-39UNEST-ages 40-49UNEST-ages 50-59UNEST-ages 60-69UNEST-ages 70-79UNEST-ages 80-89UNETR-ages 16-29UNETR-ages 30-39UNETR-ages 40-49UNETR-ages 50-59UNETR-ages 60-69UNETR-ages 70-79UNETR-ages 80-89 SAM-Adapter-ages 16-29SAM-Adapter-ages 30-39SAM-Adapter-ages 40-49SAM-Adapter-ages 50-59SAM-Adapter-ages 60-69SAM-Adapter-ages 70-79SAM-Adapter-ages 80-89 AI Algorithm-Group ******* ** **** **** ** ** ** ** **** * ** ** **** ** ** ** **** **** ** ** **** ** *** **** *** **** ** *** **** * * ***** **** * stomach DSC by ages in JHH : Boxplot showing stomach DSC score by age in JHH. Statistical significance is indicatedby stars: * p < 0.05, ** p <0.01, *** p < 0.001, **** p < 0.0001. We perform KruskalWallis testsfollowed by post-hoc Mann-Whitney U Tests with Bonferroni correction. Here, we did not performstatistical comparisons between diverse AI algorithms. 0.00.20.40.60.81.0 DSC MedNeXt-ages 16-29MedNeXt-ages 30-39MedNeXt-ages 40-49MedNeXt-ages 50-59MedNeXt-ages 60-69MedNeXt-ages 70-79MedNeXt-ages 80-89 STU-Net B-ages 16-29STU-Net B-ages 30-39STU-Net B-ages 40-49STU-Net B-ages 50-59STU-Net B-ages 60-69STU-Net B-ages 70-79STU-Net B-ages 80-89STU-Net L-ages 16-29STU-Net L-ages 30-39STU-Net L-ages 40-49STU-Net L-ages 50-59STU-Net L-ages 60-69STU-Net L-ages 70-79STU-Net L-ages 80-89STU-Net H-ages 16-29STU-Net H-ages 30-39STU-Net H-ages 40-49STU-Net H-ages 50-59STU-Net H-ages 60-69STU-Net H-ages 70-79STU-Net H-ages 80-89 nnU-Net ResEncL-ages 16-29nnU-Net ResEncL-ages 30-39nnU-Net ResEncL-ages 40-49nnU-Net ResEncL-ages 50-59nnU-Net ResEncL-ages 60-69nnU-Net ResEncL-ages 70-79nnU-Net ResEncL-ages 80-89 MedFormer-ages 16-29MedFormer-ages 30-39MedFormer-ages 40-49MedFormer-ages 50-59MedFormer-ages 60-69MedFormer-ages 70-79MedFormer-ages 80-89 nnU-Net U-Net-ages 16-29nnU-Net U-Net-ages 30-39nnU-Net U-Net-ages 40-49nnU-Net U-Net-ages 50-59nnU-Net U-Net-ages 60-69nnU-Net U-Net-ages 70-79nnU-Net U-Net-ages 80-89 UniSeg-ages 16-29UniSeg-ages 30-39UniSeg-ages 40-49UniSeg-ages 50-59UniSeg-ages 60-69UniSeg-ages 70-79UniSeg-ages 80-89 Diff-UNet-ages 16-29Diff-UNet-ages 30-39Diff-UNet-ages 40-49Diff-UNet-ages 50-59Diff-UNet-ages 60-69Diff-UNet-ages 70-79Diff-UNet-ages 80-89 NexToU-ages 16-29NexToU-ages 30-39NexToU-ages 40-49NexToU-ages 50-59NexToU-ages 60-69NexToU-ages 70-79NexToU-ages 80-89SegVol-ages 16-29SegVol-ages 30-39SegVol-ages 40-49SegVol-ages 50-59SegVol-ages 60-69SegVol-ages 70-79SegVol-ages 80-89 U-Net & CLIP-ages 16-29U-Net & CLIP-ages 30-39U-Net & CLIP-ages 40-49U-Net & CLIP-ages 50-59U-Net & CLIP-ages 60-69U-Net & CLIP-ages 70-79U-Net & CLIP-ages 80-89 SwinUNETR & CLIP-ages 16-29SwinUNETR & CLIP-ages 30-39SwinUNETR & CLIP-ages 40-49SwinUNETR & CLIP-ages 50-59SwinUNETR & CLIP-ages 60-69SwinUNETR & CLIP-ages 70-79SwinUNETR & CLIP-ages 80-89 LHU-Net-ages 16-29LHU-Net-ages 30-39LHU-Net-ages 40-49LHU-Net-ages 50-59LHU-Net-ages 60-69LHU-Net-ages 70-79LHU-Net-ages 80-89 UCTransNet-ages 16-29UCTransNet-ages 30-39UCTransNet-ages 40-49UCTransNet-ages 50-59UCTransNet-ages 60-69UCTransNet-ages 70-79UCTransNet-ages 80-89SwinUNETR-ages 16-29SwinUNETR-ages 30-39SwinUNETR-ages 40-49SwinUNETR-ages 50-59SwinUNETR-ages 60-69SwinUNETR-ages 70-79SwinUNETR-ages 80-89 UNEST-ages 16-29UNEST-ages 30-39UNEST-ages 40-49UNEST-ages 50-59UNEST-ages 60-69UNEST-ages 70-79UNEST-ages 80-89UNETR-ages 16-29UNETR-ages 30-39UNETR-ages 40-49UNETR-ages 50-59UNETR-ages 60-69UNETR-ages 70-79UNETR-ages 80-89 SAM-Adapter-ages 16-29SAM-Adapter-ages 30-39SAM-Adapter-ages 40-49SAM-Adapter-ages 50-59SAM-Adapter-ages 60-69SAM-Adapter-ages 70-79SAM-Adapter-ages 80-89 AI Algorithm-Group **** **** ** **** **** * *** ** ****** **** **** **** **** **** **** **** *** *** * ** ***** **** **** **** **** **** **** **** **** ** * **** **** **** *** **** **** * **** *** *** **** **** **** ** **** **** **** *** ************ ** aorta DSC by ages in JHH : Boxplot showing aorta DSC score by age in JHH. Statistical significance is indicated bystars: * p < 0.05, ** p <0.01, *** p < 0.001, **** p < 0.0001. We observed that mean AI performancedrops with advanced age, but some AI algorithms show improving DSC score for aorta after 70.Possibly, an explanation is that the ascending aorta and aortic arch can increase in diameter withage (due to hypertension), and the walls of the vessel will gradually show obvious calcification,possibly making the boundaries clearer. We perform KruskalWallis tests followed by post-hocMann-Whitney U Tests with Bonferroni correction. Here, we did not perform statistical comparisonsbetween diverse AI algorithms. 0.00.20.40.60.81.0 DSC MedNeXt-ages 16-29MedNeXt-ages 30-39MedNeXt-ages 40-49MedNeXt-ages 50-59MedNeXt-ages 60-69MedNeXt-ages 70-79MedNeXt-ages 80-89 STU-Net B-ages 16-29STU-Net B-ages 30-39STU-Net B-ages 40-49STU-Net B-ages 50-59STU-Net B-ages 60-69STU-Net B-ages 70-79STU-Net B-ages 80-89STU-Net L-ages 16-29STU-Net L-ages 30-39STU-Net L-ages 40-49STU-Net L-ages 50-59STU-Net L-ages 60-69STU-Net L-ages 70-79STU-Net L-ages 80-89STU-Net H-ages 16-29STU-Net H-ages 30-39STU-Net H-ages 40-49STU-Net H-ages 50-59STU-Net H-ages 60-69STU-Net H-ages 70-79STU-Net H-ages 80-89 nnU-Net ResEncL-ages 16-29nnU-Net ResEncL-ages 30-39nnU-Net ResEncL-ages 40-49nnU-Net ResEncL-ages 50-59nnU-Net ResEncL-ages 60-69nnU-Net ResEncL-ages 70-79nnU-Net ResEncL-ages 80-89 MedFormer-ages 16-29MedFormer-ages 30-39MedFormer-ages 40-49MedFormer-ages 50-59MedFormer-ages 60-69MedFormer-ages 70-79MedFormer-ages 80-89 nnU-Net U-Net-ages 16-29nnU-Net U-Net-ages 30-39nnU-Net U-Net-ages 40-49nnU-Net U-Net-ages 50-59nnU-Net U-Net-ages 60-69nnU-Net U-Net-ages 70-79nnU-Net U-Net-ages 80-89 UniSeg-ages 16-29UniSeg-ages 30-39UniSeg-ages 40-49UniSeg-ages 50-59UniSeg-ages 60-69UniSeg-ages 70-79UniSeg-ages 80-89 Diff-UNet-ages 16-29Diff-UNet-ages 30-39Diff-UNet-ages 40-49Diff-UNet-ages 50-59Diff-UNet-ages 60-69Diff-UNet-ages 70-79Diff-UNet-ages 80-89 NexToU-ages 16-29NexToU-ages 30-39NexToU-ages 40-49NexToU-ages 50-59NexToU-ages 60-69NexToU-ages 70-79NexToU-ages 80-89SegVol-ages 16-29SegVol-ages 30-39SegVol-ages 40-49SegVol-ages 50-59SegVol-ages 60-69SegVol-ages 70-79SegVol-ages 80-89 U-Net & CLIP-ages 16-29U-Net & CLIP-ages 30-39U-Net & CLIP-ages 40-49U-Net & CLIP-ages 50-59U-Net & CLIP-ages 60-69U-Net & CLIP-ages 70-79U-Net & CLIP-ages 80-89 SwinUNETR & CLIP-ages 16-29SwinUNETR & CLIP-ages 30-39SwinUNETR & CLIP-ages 40-49SwinUNETR & CLIP-ages 50-59SwinUNETR & CLIP-ages 60-69SwinUNETR & CLIP-ages 70-79SwinUNETR & CLIP-ages 80-89 LHU-Net-ages 16-29LHU-Net-ages 30-39LHU-Net-ages 40-49LHU-Net-ages 50-59LHU-Net-ages 60-69LHU-Net-ages 70-79LHU-Net-ages 80-89 UCTransNet-ages 16-29UCTransNet-ages 30-39UCTransNet-ages 40-49UCTransNet-ages 50-59UCTransNet-ages 60-69UCTransNet-ages 70-79UCTransNet-ages 80-89SwinUNETR-ages 16-29SwinUNETR-ages 30-39SwinUNETR-ages 40-49SwinUNETR-ages 50-59SwinUNETR-ages 60-69SwinUNETR-ages 70-79SwinUNETR-ages 80-89 UNEST-ages 16-29UNEST-ages 30-39UNEST-ages 40-49UNEST-ages 50-59UNEST-ages 60-69UNEST-ages 70-79UNEST-ages 80-89UNETR-ages 16-29UNETR-ages 30-39UNETR-ages 40-49UNETR-ages 50-59UNETR-ages 60-69UNETR-ages 70-79UNETR-ages 80-89 SAM-Adapter-ages 16-29SAM-Adapter-ages 30-39SAM-Adapter-ages 40-49SAM-Adapter-ages 50-59SAM-Adapter-ages 60-69SAM-Adapter-ages 70-79SAM-Adapter-ages 80-89 AI Algorithm-Group * **** * * **** *** ** * ** **** ** * postcava DSC by ages in JHH : Boxplot showing postcava DSC score by age in JHH. Statistical significance is indicatedby stars: * p < 0.05, ** p <0.01, *** p < 0.001, **** p < 0.0001. We perform KruskalWallis testsfollowed by post-hoc Mann-Whitney U Tests with Bonferroni correction. Here, we did not performstatistical comparisons between diverse AI algorithms. 0.00.20.40.60.81.0 DSC MedNeXt-ages 16-29MedNeXt-ages 30-39MedNeXt-ages 40-49MedNeXt-ages 50-59MedNeXt-ages 60-69MedNeXt-ages 70-79MedNeXt-ages 80-89 STU-Net B-ages 16-29STU-Net B-ages 30-39STU-Net B-ages 40-49STU-Net B-ages 50-59STU-Net B-ages 60-69STU-Net B-ages 70-79STU-Net B-ages 80-89STU-Net L-ages 16-29STU-Net L-ages 30-39STU-Net L-ages 40-49STU-Net L-ages 50-59STU-Net L-ages 60-69STU-Net L-ages 70-79STU-Net L-ages 80-89STU-Net H-ages 16-29STU-Net H-ages 30-39STU-Net H-ages 40-49STU-Net H-ages 50-59STU-Net H-ages 60-69STU-Net H-ages 70-79STU-Net H-ages 80-89 nnU-Net ResEncL-ages 16-29nnU-Net ResEncL-ages 30-39nnU-Net ResEncL-ages 40-49nnU-Net ResEncL-ages 50-59nnU-Net ResEncL-ages 60-69nnU-Net ResEncL-ages 70-79nnU-Net ResEncL-ages 80-89 MedFormer-ages 16-29MedFormer-ages 30-39MedFormer-ages 40-49MedFormer-ages 50-59MedFormer-ages 60-69MedFormer-ages 70-79MedFormer-ages 80-89 nnU-Net U-Net-ages 16-29nnU-Net U-Net-ages 30-39nnU-Net U-Net-ages 40-49nnU-Net U-Net-ages 50-59nnU-Net U-Net-ages 60-69nnU-Net U-Net-ages 70-79nnU-Net U-Net-ages 80-89 UniSeg-ages 16-29UniSeg-ages 30-39UniSeg-ages 40-49UniSeg-ages 50-59UniSeg-ages 60-69UniSeg-ages 70-79UniSeg-ages 80-89 Diff-UNet-ages 16-29Diff-UNet-ages 30-39Diff-UNet-ages 40-49Diff-UNet-ages 50-59Diff-UNet-ages 60-69Diff-UNet-ages 70-79Diff-UNet-ages 80-89 NexToU-ages 16-29NexToU-ages 30-39NexToU-ages 40-49NexToU-ages 50-59NexToU-ages 60-69NexToU-ages 70-79NexToU-ages 80-89SegVol-ages 16-29SegVol-ages 30-39SegVol-ages 40-49SegVol-ages 50-59SegVol-ages 60-69SegVol-ages 70-79SegVol-ages 80-89 U-Net & CLIP-ages 16-29U-Net & CLIP-ages 30-39U-Net & CLIP-ages 40-49U-Net & CLIP-ages 50-59U-Net & CLIP-ages 60-69U-Net & CLIP-ages 70-79U-Net & CLIP-ages 80-89 SwinUNETR & CLIP-ages 16-29SwinUNETR & CLIP-ages 30-39SwinUNETR & CLIP-ages 40-49SwinUNETR & CLIP-ages 50-59SwinUNETR & CLIP-ages 60-69SwinUNETR & CLIP-ages 70-79SwinUNETR & CLIP-ages 80-89 LHU-Net-ages 16-29LHU-Net-ages 30-39LHU-Net-ages 40-49LHU-Net-ages 50-59LHU-Net-ages 60-69LHU-Net-ages 70-79LHU-Net-ages 80-89 UCTransNet-ages 16-29UCTransNet-ages 30-39UCTransNet-ages 40-49UCTransNet-ages 50-59UCTransNet-ages 60-69UCTransNet-ages 70-79UCTransNet-ages 80-89SwinUNETR-ages 16-29SwinUNETR-ages 30-39SwinUNETR-ages 40-49SwinUNETR-ages 50-59SwinUNETR-ages 60-69SwinUNETR-ages 70-79SwinUNETR-ages 80-89 UNEST-ages 16-29UNEST-ages 30-39UNEST-ages 40-49UNEST-ages 50-59UNEST-ages 60-69UNEST-ages 70-79UNEST-ages 80-89UNETR-ages 16-29UNETR-ages 30-39UNETR-ages 40-49UNETR-ages 50-59UNETR-ages 60-69UNETR-ages 70-79UNETR-ages 80-89 SAM-Adapter-ages 16-29SAM-Adapter-ages 30-39SAM-Adapter-ages 40-49SAM-Adapter-ages 50-59SAM-Adapter-ages 60-69SAM-Adapter-ages 70-79SAM-Adapter-ages 80-89 AI Algorithm-Group * *** *** * ** **** * * ****** * **** **** **** * **** **** * **** **** **** **** * *** **** * **** **** **** * ** **** * *** **** * **** **** ***** **** **** ** **** **** **** **** **** **** ** * ** **** ** **** *** ******* **** **** *** **** **** **** ** **** **** **** **** **** **** **** **** **** **** **** **** *** **** *** *** **** **** **** **** ******* **** **** **** **** *** **** **** **** **** **** ******** **** **** **** **** **** **** **** **** *** **** **** ** **** **** **** **** *** **** **** **** **** **** **** **** **** **** *** **** **** **** **** **** **** **** **** ******** **** **** **** **** **** **** **** **** **** **** **** **** ******** **** **** **** ***** **** **** **** **** **** **** **** **** **** **** *** **** **** **** **** **** ** **** ** **** * ***** **** * **** **** **** * **** ** **** ** **** **** **** **** **** **** **** **** **** ** **** **** **** **** **** **** **** **** **** **** ****** ** *** ** ********* *** **** **** **** ******** **** ******** **** pancreas DSC by ages in JHH : Boxplot showing pancreas DSC score by age in JHH. Statistical significance is indicatedby stars: * p < 0.05, ** p <0.01, *** p < 0.001, **** p < 0.0001. We perform KruskalWallis testsfollowed by post-hoc Mann-Whitney U Tests with Bonferroni correction. Here, we did not performstatistical comparisons between diverse AI algorithms.",
  "D.5.8Diagnosis: per-class analysis": "0.00.20.40.60.81.0 DSC MedNeXt-negative MedNeXt-cancer STU-Net B-negative STU-Net B-cancer STU-Net L-negative STU-Net L-cancer STU-Net H-negative STU-Net H-cancer nnU-Net ResEncL-negative nnU-Net ResEncL-cancer MedFormer-negative MedFormer-cancer nnU-Net U-Net-negative nnU-Net U-Net-cancer UniSeg-negative UniSeg-cancer Diff-UNet-negative Diff-UNet-cancerNexToU-negative NexToU-cancer SegVol-negative SegVol-cancer U-Net & CLIP-negative U-Net & CLIP-cancer SwinUNETR & CLIP-negative SwinUNETR & CLIP-cancer LHU-Net-negative LHU-Net-cancer UCTransNet-negative UCTransNet-cancer SwinUNETR-negative SwinUNETR-cancer UNEST-negative UNEST-cancer UNETR-negative UNETR-cancer SAM-Adapter-negative SAM-Adapter-cancer AI Algorithm-Group ******************************************************************* spleen DSC by diagnosis in JHH : Boxplot showing spleen DSC score by diagnosis in JHH. Statistical significance isindicated by stars: * p < 0.05, ** p <0.01, *** p < 0.001, **** p < 0.0001. We perform KruskalWallistests followed by post-hoc Mann-Whitney U Tests with Bonferroni correction. Here, we did notperform statistical comparisons between diverse AI algorithms. 0.00.20.40.60.81.0 DSC MedNeXt-negative MedNeXt-cancer STU-Net B-negative STU-Net B-cancer STU-Net L-negative STU-Net L-cancer STU-Net H-negative STU-Net H-cancer nnU-Net ResEncL-negative nnU-Net ResEncL-cancer MedFormer-negative MedFormer-cancer nnU-Net U-Net-negative nnU-Net U-Net-cancer UniSeg-negative UniSeg-cancer Diff-UNet-negative Diff-UNet-cancerNexToU-negative NexToU-cancer SegVol-negative SegVol-cancer U-Net & CLIP-negative U-Net & CLIP-cancer SwinUNETR & CLIP-negative SwinUNETR & CLIP-cancer LHU-Net-negative LHU-Net-cancer UCTransNet-negative UCTransNet-cancer SwinUNETR-negative SwinUNETR-cancer UNEST-negative UNEST-cancer UNETR-negative UNETR-cancer SAM-Adapter-negative SAM-Adapter-cancer AI Algorithm-Group ****************************************** **************************** **** kidney right DSC by diagnosis in JHH : Boxplot showing right kidney DSC score by diagnosis in JHH. Statistical significance isindicated by stars: * p < 0.05, ** p <0.01, *** p < 0.001, **** p < 0.0001. We perform KruskalWallistests followed by post-hoc Mann-Whitney U Tests with Bonferroni correction. Here, we did notperform statistical comparisons between diverse AI algorithms. 0.00.20.40.60.81.0 DSC MedNeXt-negative MedNeXt-cancer STU-Net B-negative STU-Net B-cancer STU-Net L-negative STU-Net L-cancer STU-Net H-negative STU-Net H-cancer nnU-Net ResEncL-negative nnU-Net ResEncL-cancer MedFormer-negative MedFormer-cancer nnU-Net U-Net-negative nnU-Net U-Net-cancer UniSeg-negative UniSeg-cancer Diff-UNet-negative Diff-UNet-cancerNexToU-negative NexToU-cancer SegVol-negative SegVol-cancer U-Net & CLIP-negative U-Net & CLIP-cancer SwinUNETR & CLIP-negative SwinUNETR & CLIP-cancer LHU-Net-negative LHU-Net-cancer UCTransNet-negative UCTransNet-cancer SwinUNETR-negative SwinUNETR-cancer UNEST-negative UNEST-cancer UNETR-negative UNETR-cancer SAM-Adapter-negative SAM-Adapter-cancer AI Algorithm-Group ************ kidney left DSC by diagnosis in JHH : Boxplot showing left kidney DSC score by diagnosis in JHH. Statistical significance isindicated by stars: * p < 0.05, ** p <0.01, *** p < 0.001, **** p < 0.0001. We perform KruskalWallistests followed by post-hoc Mann-Whitney U Tests with Bonferroni correction. Here, we did notperform statistical comparisons between diverse AI algorithms. 0.00.20.40.60.81.0 DSC MedNeXt-negative MedNeXt-cancer STU-Net B-negative STU-Net B-cancer STU-Net L-negative STU-Net L-cancer STU-Net H-negative STU-Net H-cancer nnU-Net ResEncL-negative nnU-Net ResEncL-cancer MedFormer-negative MedFormer-cancer nnU-Net U-Net-negative nnU-Net U-Net-cancer UniSeg-negative UniSeg-cancer Diff-UNet-negative Diff-UNet-cancerNexToU-negative NexToU-cancer SegVol-negative SegVol-cancer U-Net & CLIP-negative U-Net & CLIP-cancer SwinUNETR & CLIP-negative SwinUNETR & CLIP-cancer LHU-Net-negative LHU-Net-cancer UCTransNet-negative UCTransNet-cancer SwinUNETR-negative SwinUNETR-cancer UNEST-negative UNEST-cancer UNETR-negative UNETR-cancer SAM-Adapter-negative SAM-Adapter-cancer AI Algorithm-Group ******************** ********* **** gall bladder DSC by diagnosis in JHH : Boxplot showing gallbladder DSC score by diagnosis in JHH. Statistical significance isindicated by stars: * p < 0.05, ** p <0.01, *** p < 0.001, **** p < 0.0001. We perform KruskalWallistests followed by post-hoc Mann-Whitney U Tests with Bonferroni correction. Here, we did notperform statistical comparisons between diverse AI algorithms. 0.00.20.40.60.81.0 DSC MedNeXt-cancer MedNeXt-negativeSTU-Net B-cancer STU-Net B-negative STU-Net L-cancer STU-Net L-negative STU-Net H-cancer STU-Net H-negative nnU-Net ResEncL-cancer nnU-Net ResEncL-negative MedFormer-cancer MedFormer-negative nnU-Net U-Net-cancer nnU-Net U-Net-negative UniSeg-cancer UniSeg-negativeDiff-UNet-cancer Diff-UNet-negative NexToU-cancer NexToU-negative SegVol-cancer SegVol-negative U-Net & CLIP-cancer U-Net & CLIP-negative SwinUNETR & CLIP-cancer SwinUNETR & CLIP-negative LHU-Net-cancer LHU-Net-negative UCTransNet-cancer UCTransNet-negative SwinUNETR-cancer SwinUNETR-negative UNEST-cancer UNEST-negative UNETR-cancer UNETR-negative SAM-Adapter-cancer SAM-Adapter-negative AI Algorithm-Group ************************************************ liver DSC by diagnosis in JHH : Boxplot showing liver DSC score by diagnosis in JHH. Statistical significance isindicated by stars: * p < 0.05, ** p <0.01, *** p < 0.001, **** p < 0.0001. We perform KruskalWallistests followed by post-hoc Mann-Whitney U Tests with Bonferroni correction. Here, we did notperform statistical comparisons between diverse AI algorithms. 0.00.20.40.60.81.0 DSC MedNeXt-negative MedNeXt-cancer STU-Net B-negative STU-Net B-cancer STU-Net L-negative STU-Net L-cancer STU-Net H-negative STU-Net H-cancer nnU-Net ResEncL-negative nnU-Net ResEncL-cancer MedFormer-negative MedFormer-cancer nnU-Net U-Net-negative nnU-Net U-Net-cancer UniSeg-negative UniSeg-cancer Diff-UNet-negative Diff-UNet-cancerNexToU-negative NexToU-cancer SegVol-negative SegVol-cancer U-Net & CLIP-negative U-Net & CLIP-cancer SwinUNETR & CLIP-negative SwinUNETR & CLIP-cancer LHU-Net-negative LHU-Net-cancer UCTransNet-negative UCTransNet-cancer SwinUNETR-negative SwinUNETR-cancer UNEST-negative UNEST-cancer UNETR-negative UNETR-cancer SAM-Adapter-negative SAM-Adapter-cancer AI Algorithm-Group ********************** stomach DSC by diagnosis in JHH : Boxplot showing stomach DSC score by diagnosis in JHH. Statistical significance isindicated by stars: * p < 0.05, ** p <0.01, *** p < 0.001, **** p < 0.0001. We perform KruskalWallistests followed by post-hoc Mann-Whitney U Tests with Bonferroni correction. Here, we did notperform statistical comparisons between diverse AI algorithms. 0.00.20.40.60.81.0 DSC MedNeXt-negative MedNeXt-cancer STU-Net B-negative STU-Net B-cancer STU-Net L-negative STU-Net L-cancer STU-Net H-negative STU-Net H-cancer nnU-Net ResEncL-negative nnU-Net ResEncL-cancer MedFormer-negative MedFormer-cancer nnU-Net U-Net-negative nnU-Net U-Net-cancer UniSeg-negative UniSeg-cancer Diff-UNet-negative Diff-UNet-cancerNexToU-negative NexToU-cancer SegVol-negative SegVol-cancer U-Net & CLIP-negative U-Net & CLIP-cancer SwinUNETR & CLIP-negative SwinUNETR & CLIP-cancer LHU-Net-negative LHU-Net-cancer UCTransNet-negative UCTransNet-cancer SwinUNETR-negative SwinUNETR-cancer UNEST-negative UNEST-cancer UNETR-negative UNETR-cancer SAM-Adapter-negative SAM-Adapter-cancer AI Algorithm-Group ******************** **** ******************************** aorta DSC by diagnosis in JHH : Boxplot showing aorta DSC score by diagnosis in JHH. Statistical significance isindicated by stars: * p < 0.05, ** p <0.01, *** p < 0.001, **** p < 0.0001. We perform KruskalWallistests followed by post-hoc Mann-Whitney U Tests with Bonferroni correction. Here, we did notperform statistical comparisons between diverse AI algorithms. 0.00.20.40.60.81.0 DSC MedNeXt-cancer MedNeXt-negativeSTU-Net B-cancer STU-Net B-negative STU-Net L-cancer STU-Net L-negative STU-Net H-cancer STU-Net H-negative nnU-Net ResEncL-cancer nnU-Net ResEncL-negative MedFormer-cancer MedFormer-negative nnU-Net U-Net-cancer nnU-Net U-Net-negative UniSeg-cancer UniSeg-negativeDiff-UNet-cancer Diff-UNet-negative NexToU-cancer NexToU-negative SegVol-cancer SegVol-negative U-Net & CLIP-cancer U-Net & CLIP-negative SwinUNETR & CLIP-cancer SwinUNETR & CLIP-negative LHU-Net-cancer LHU-Net-negative UCTransNet-cancer UCTransNet-negative SwinUNETR-cancer SwinUNETR-negative UNEST-cancer UNEST-negative UNETR-cancer UNETR-negative SAM-Adapter-cancer SAM-Adapter-negative AI Algorithm-Group *** postcava DSC by diagnosis in JHH : Boxplot showing postcava DSC score by diagnosis in JHH. Statistical significance isindicated by stars: * p < 0.05, ** p <0.01, *** p < 0.001, **** p < 0.0001. We perform KruskalWallistests followed by post-hoc Mann-Whitney U Tests with Bonferroni correction. Here, we did notperform statistical comparisons between diverse AI algorithms. 0.00.20.40.60.81.0 DSC MedNeXt-negative MedNeXt-cancer STU-Net B-negative STU-Net B-cancer STU-Net L-negative STU-Net L-cancer STU-Net H-negative STU-Net H-cancer nnU-Net ResEncL-negative nnU-Net ResEncL-cancer MedFormer-negative MedFormer-cancer nnU-Net U-Net-negative nnU-Net U-Net-cancer UniSeg-negative UniSeg-cancer Diff-UNet-negative Diff-UNet-cancerNexToU-negative NexToU-cancer SegVol-negative SegVol-cancer U-Net & CLIP-negative U-Net & CLIP-cancer SwinUNETR & CLIP-negative SwinUNETR & CLIP-cancer LHU-Net-negative LHU-Net-cancer UCTransNet-negative UCTransNet-cancer SwinUNETR-negative SwinUNETR-cancer UNEST-negative UNEST-cancer UNETR-negative UNETR-cancer SAM-Adapter-negative SAM-Adapter-cancer AI Algorithm-Group **************** **************************************** **************** **** pancreas DSC by diagnosis in JHH : Boxplot showing pancreas DSC score by diagnosis in JHH. Statistical significance isindicated by stars: * p < 0.05, ** p <0.01, *** p < 0.001, **** p < 0.0001. We perform KruskalWallistests followed by post-hoc Mann-Whitney U Tests with Bonferroni correction. Here, we did notperform statistical comparisons between diverse AI algorithms.",
  "D.5.9Sex: per-class analysis": "0.00.20.40.60.81.0 DSC MedNeXt-male MedNeXt-femaleSTU-Net B-male STU-Net B-female STU-Net L-male STU-Net L-female STU-Net H-male STU-Net H-female nnU-Net ResEncL-male nnU-Net ResEncL-female MedFormer-male MedFormer-female nnU-Net U-Net-male nnU-Net U-Net-female UniSeg-male UniSeg-femaleDiff-UNet-male Diff-UNet-female NexToU-male NexToU-female SegVol-male SegVol-female U-Net & CLIP-male U-Net & CLIP-female SwinUNETR & CLIP-male SwinUNETR & CLIP-female LHU-Net-male LHU-Net-female UCTransNet-male UCTransNet-female SwinUNETR-male SwinUNETR-female UNEST-male UNEST-female UNETR-male UNETR-female SAM-Adapter-male SAM-Adapter-female AI Algorithm-Group ************************************************************************ **** spleen DSC by sex in JHH : Boxplot showing spleen DSC score by sex in JHH. Statistical significance is indicatedby stars: * p < 0.05, ** p <0.01, *** p < 0.001, **** p < 0.0001. We perform KruskalWallis testsfollowed by post-hoc Mann-Whitney U Tests with Bonferroni correction. Here, we did not performstatistical comparisons between diverse AI algorithms. 0.00.20.40.60.81.0 DSC MedNeXt-male MedNeXt-femaleSTU-Net B-male STU-Net B-female STU-Net L-male STU-Net L-female STU-Net H-male STU-Net H-female nnU-Net ResEncL-male nnU-Net ResEncL-female MedFormer-male MedFormer-female nnU-Net U-Net-male nnU-Net U-Net-female UniSeg-male UniSeg-femaleDiff-UNet-male Diff-UNet-female NexToU-male NexToU-female SegVol-male SegVol-female U-Net & CLIP-male U-Net & CLIP-female SwinUNETR & CLIP-male SwinUNETR & CLIP-female LHU-Net-male LHU-Net-female UCTransNet-male UCTransNet-female SwinUNETR-male SwinUNETR-female UNEST-male UNEST-female UNETR-male UNETR-female SAM-Adapter-male SAM-Adapter-female AI Algorithm-Group ******** **** kidney right DSC by sex in JHH : Boxplot showing right kidney DSC score by sex in JHH. Statistical significance isindicated by stars: * p < 0.05, ** p <0.01, *** p < 0.001, **** p < 0.0001. We perform KruskalWallistests followed by post-hoc Mann-Whitney U Tests with Bonferroni correction. Here, we did notperform statistical comparisons between diverse AI algorithms. 0.00.20.40.60.81.0 DSC MedNeXt-male MedNeXt-femaleSTU-Net B-male STU-Net B-female STU-Net L-male STU-Net L-female STU-Net H-male STU-Net H-female nnU-Net ResEncL-male nnU-Net ResEncL-female MedFormer-male MedFormer-female nnU-Net U-Net-male nnU-Net U-Net-female UniSeg-male UniSeg-femaleDiff-UNet-male Diff-UNet-female NexToU-male NexToU-female SegVol-male SegVol-female U-Net & CLIP-male U-Net & CLIP-female SwinUNETR & CLIP-male SwinUNETR & CLIP-female LHU-Net-male LHU-Net-female UCTransNet-male UCTransNet-female SwinUNETR-male SwinUNETR-female UNEST-male UNEST-female UNETR-male UNETR-female SAM-Adapter-male SAM-Adapter-female AI Algorithm-Group ************ **** kidney left DSC by sex in JHH : Boxplot showing left kidney DSC score by sex in JHH. Statistical significance isindicated by stars: * p < 0.05, ** p <0.01, *** p < 0.001, **** p < 0.0001. We perform KruskalWallistests followed by post-hoc Mann-Whitney U Tests with Bonferroni correction. Here, we did notperform statistical comparisons between diverse AI algorithms. 0.00.20.40.60.81.0 DSC MedNeXt-male MedNeXt-femaleSTU-Net B-male STU-Net B-female STU-Net L-male STU-Net L-female STU-Net H-male STU-Net H-female nnU-Net ResEncL-male nnU-Net ResEncL-female MedFormer-male MedFormer-female nnU-Net U-Net-male nnU-Net U-Net-female UniSeg-male UniSeg-femaleDiff-UNet-male Diff-UNet-female NexToU-male NexToU-female SegVol-male SegVol-female U-Net & CLIP-male U-Net & CLIP-female SwinUNETR & CLIP-male SwinUNETR & CLIP-female LHU-Net-male LHU-Net-female UCTransNet-male UCTransNet-female SwinUNETR-male SwinUNETR-female UNEST-male UNEST-female UNETR-male UNETR-female SAM-Adapter-male SAM-Adapter-female AI Algorithm-Group ************************************************** ******************** **** gall bladder DSC by sex in JHH : Boxplot showing gallbladder DSC score by sex in JHH. Statistical significance isindicated by stars: * p < 0.05, ** p <0.01, *** p < 0.001, **** p < 0.0001. We perform KruskalWallistests followed by post-hoc Mann-Whitney U Tests with Bonferroni correction. Here, we did notperform statistical comparisons between diverse AI algorithms. 0.00.20.40.60.81.0 DSC MedNeXt-male MedNeXt-femaleSTU-Net B-male STU-Net B-female STU-Net L-male STU-Net L-female STU-Net H-male STU-Net H-female nnU-Net ResEncL-male nnU-Net ResEncL-female MedFormer-male MedFormer-female nnU-Net U-Net-male nnU-Net U-Net-female UniSeg-male UniSeg-femaleDiff-UNet-male Diff-UNet-female NexToU-male NexToU-female SegVol-male SegVol-female U-Net & CLIP-male U-Net & CLIP-female SwinUNETR & CLIP-male SwinUNETR & CLIP-female LHU-Net-male LHU-Net-female UCTransNet-male UCTransNet-female SwinUNETR-male SwinUNETR-female UNEST-male UNEST-female UNETR-male UNETR-female SAM-Adapter-male SAM-Adapter-female AI Algorithm-Group ****** liver DSC by sex in JHH : Boxplot showing liver DSC score by sex in JHH. Statistical significance is indicatedby stars: * p < 0.05, ** p <0.01, *** p < 0.001, **** p < 0.0001. We perform KruskalWallis testsfollowed by post-hoc Mann-Whitney U Tests with Bonferroni correction. Here, we did not performstatistical comparisons between diverse AI algorithms. 0.00.20.40.60.81.0 DSC MedNeXt-male MedNeXt-femaleSTU-Net B-male STU-Net B-female STU-Net L-male STU-Net L-female STU-Net H-male STU-Net H-female nnU-Net ResEncL-male nnU-Net ResEncL-female MedFormer-male MedFormer-female nnU-Net U-Net-male nnU-Net U-Net-female UniSeg-male UniSeg-femaleDiff-UNet-male Diff-UNet-female NexToU-male NexToU-female SegVol-male SegVol-female U-Net & CLIP-male U-Net & CLIP-female SwinUNETR & CLIP-male SwinUNETR & CLIP-female LHU-Net-male LHU-Net-female UCTransNet-male UCTransNet-female SwinUNETR-male SwinUNETR-female UNEST-male UNEST-female UNETR-male UNETR-female SAM-Adapter-male SAM-Adapter-female AI Algorithm-Group ************* ******************** stomach DSC by sex in JHH : Boxplot showing stomach DSC score by sex in JHH. Statistical significance is indicatedby stars: * p < 0.05, ** p <0.01, *** p < 0.001, **** p < 0.0001. We perform KruskalWallis testsfollowed by post-hoc Mann-Whitney U Tests with Bonferroni correction. Here, we did not performstatistical comparisons between diverse AI algorithms. 0.00.20.40.60.81.0 DSC MedNeXt-female MedNeXt-male STU-Net B-female STU-Net B-male STU-Net L-female STU-Net L-male STU-Net H-female STU-Net H-male nnU-Net ResEncL-female nnU-Net ResEncL-male MedFormer-female MedFormer-male nnU-Net U-Net-female nnU-Net U-Net-male UniSeg-female UniSeg-male Diff-UNet-female Diff-UNet-maleNexToU-female NexToU-male SegVol-female SegVol-male U-Net & CLIP-female U-Net & CLIP-male SwinUNETR & CLIP-female SwinUNETR & CLIP-male LHU-Net-female LHU-Net-male UCTransNet-female UCTransNet-male SwinUNETR-female SwinUNETR-male UNEST-female UNEST-male UNETR-female UNETR-male SAM-Adapter-female SAM-Adapter-male AI Algorithm-Group ******************** **** *********************************** ******** aorta DSC by sex in JHH : Boxplot showing aorta DSC score by sex in JHH. Statistical significance is indicatedby stars: * p < 0.05, ** p <0.01, *** p < 0.001, **** p < 0.0001. We perform KruskalWallis testsfollowed by post-hoc Mann-Whitney U Tests with Bonferroni correction. Here, we did not performstatistical comparisons between diverse AI algorithms. 0.00.20.40.60.81.0 DSC MedNeXt-male MedNeXt-femaleSTU-Net B-male STU-Net B-female STU-Net L-male STU-Net L-female STU-Net H-male STU-Net H-female nnU-Net ResEncL-male nnU-Net ResEncL-female MedFormer-male MedFormer-female nnU-Net U-Net-male nnU-Net U-Net-female UniSeg-male UniSeg-femaleDiff-UNet-male Diff-UNet-female NexToU-male NexToU-female SegVol-male SegVol-female U-Net & CLIP-male U-Net & CLIP-female SwinUNETR & CLIP-male SwinUNETR & CLIP-female LHU-Net-male LHU-Net-female UCTransNet-male UCTransNet-female SwinUNETR-male SwinUNETR-female UNEST-male UNEST-female UNETR-male UNETR-female SAM-Adapter-male SAM-Adapter-female AI Algorithm-Group ************ **** postcava DSC by sex in JHH : Boxplot showing postcava DSC score by sex in JHH. Statistical significance is indicatedby stars: * p < 0.05, ** p <0.01, *** p < 0.001, **** p < 0.0001. We perform KruskalWallis testsfollowed by post-hoc Mann-Whitney U Tests with Bonferroni correction. Here, we did not performstatistical comparisons between diverse AI algorithms. 0.00.20.40.60.81.0 DSC MedNeXt-male MedNeXt-femaleSTU-Net B-male STU-Net B-female STU-Net L-male STU-Net L-female STU-Net H-male STU-Net H-female nnU-Net ResEncL-male nnU-Net ResEncL-female MedFormer-male MedFormer-female nnU-Net U-Net-male nnU-Net U-Net-female UniSeg-male UniSeg-femaleDiff-UNet-male Diff-UNet-female NexToU-male NexToU-female SegVol-male SegVol-female U-Net & CLIP-male U-Net & CLIP-female SwinUNETR & CLIP-male SwinUNETR & CLIP-female LHU-Net-male LHU-Net-female UCTransNet-male UCTransNet-female SwinUNETR-male SwinUNETR-female UNEST-male UNEST-female UNETR-male UNETR-female SAM-Adapter-male SAM-Adapter-female AI Algorithm-Group ******************* **************** ** pancreas DSC by sex in JHH : Boxplot showing pancreas DSC score by sex in JHH. Statistical significance is indicatedby stars: * p < 0.05, ** p <0.01, *** p < 0.001, **** p < 0.0001. We perform KruskalWallis testsfollowed by post-hoc Mann-Whitney U Tests with Bonferroni correction. Here, we did not performstatistical comparisons between diverse AI algorithms.",
  "D.5.10Race: per-class analysis": "0.00.20.40.60.81.0 DSC MedNeXt-race HLMedNeXt-race WMedNeXt-race OMedNeXt-race U MedNeXt-race ASMedNeXt-race AASTU-Net B-race HLSTU-Net B-race WSTU-Net B-race OSTU-Net B-race U STU-Net B-race ASSTU-Net B-race AASTU-Net L-race HLSTU-Net L-race WSTU-Net L-race OSTU-Net L-race U STU-Net L-race ASSTU-Net L-race AASTU-Net H-race HLSTU-Net H-race WSTU-Net H-race OSTU-Net H-race U STU-Net H-race ASSTU-Net H-race AA nnU-Net ResEncL-race HLnnU-Net ResEncL-race WnnU-Net ResEncL-race OnnU-Net ResEncL-race U nnU-Net ResEncL-race ASnnU-Net ResEncL-race AA MedFormer-race HLMedFormer-race WMedFormer-race OMedFormer-race U MedFormer-race ASMedFormer-race AA nnU-Net U-Net-race HLnnU-Net U-Net-race WnnU-Net U-Net-race OnnU-Net U-Net-race U nnU-Net U-Net-race ASnnU-Net U-Net-race AA UniSeg-race HLUniSeg-race WUniSeg-race OUniSeg-race U UniSeg-race ASUniSeg-race AA Diff-UNet-race HLDiff-UNet-race WDiff-UNet-race ODiff-UNet-race U Diff-UNet-race ASDiff-UNet-race AA NexToU-race HLNexToU-race WNexToU-race ONexToU-race U NexToU-race ASNexToU-race AA SegVol-race HLSegVol-race WSegVol-race OSegVol-race U SegVol-race ASSegVol-race AA U-Net & CLIP-race HLU-Net & CLIP-race WU-Net & CLIP-race OU-Net & CLIP-race U U-Net & CLIP-race ASU-Net & CLIP-race AA SwinUNETR & CLIP-race HLSwinUNETR & CLIP-race WSwinUNETR & CLIP-race OSwinUNETR & CLIP-race U SwinUNETR & CLIP-race ASSwinUNETR & CLIP-race AA LHU-Net-race HLLHU-Net-race WLHU-Net-race OLHU-Net-race U LHU-Net-race ASLHU-Net-race AA UCTransNet-race HLUCTransNet-race WUCTransNet-race OUCTransNet-race U UCTransNet-race ASUCTransNet-race AASwinUNETR-race HLSwinUNETR-race WSwinUNETR-race OSwinUNETR-race U SwinUNETR-race ASSwinUNETR-race AA UNEST-race HLUNEST-race WUNEST-race OUNEST-race U UNEST-race ASUNEST-race AAUNETR-race HLUNETR-race WUNETR-race OUNETR-race U UNETR-race ASUNETR-race AA SAM-Adapter-race HLSAM-Adapter-race WSAM-Adapter-race OSAM-Adapter-race U SAM-Adapter-race ASSAM-Adapter-race AA AI Algorithm-Group ******************** **** **** **** **** **** **** **** **** *** **** **** **** *** **** **** **** ** **** *** **** * **** ******** **** **** * ** **** ** **** * ** **************************************** **** ******** ******** **** **** **** spleen DSC by race in JHH : Boxplot showing spleen DSC score by race in JHH. Statistical significance is indicatedby stars: * p < 0.05, ** p <0.01, *** p < 0.001, **** p < 0.0001. We perform KruskalWallis testsfollowed by post-hoc Mann-Whitney U Tests with Bonferroni correction. Here, we did not performstatistical comparisons between diverse AI algorithms. 0.00.20.40.60.81.0 DSC MedNeXt-race O MedNeXt-race AAMedNeXt-race ASMedNeXt-race WMedNeXt-race HL MedNeXt-race U STU-Net B-race O STU-Net B-race AASTU-Net B-race ASSTU-Net B-race WSTU-Net B-race HL STU-Net B-race USTU-Net L-race O STU-Net L-race AASTU-Net L-race ASSTU-Net L-race WSTU-Net L-race HL STU-Net L-race USTU-Net H-race O STU-Net H-race AASTU-Net H-race ASSTU-Net H-race WSTU-Net H-race HL STU-Net H-race U nnU-Net ResEncL-race O nnU-Net ResEncL-race AAnnU-Net ResEncL-race ASnnU-Net ResEncL-race WnnU-Net ResEncL-race HL nnU-Net ResEncL-race U MedFormer-race O MedFormer-race AAMedFormer-race ASMedFormer-race WMedFormer-race HL MedFormer-race U nnU-Net U-Net-race O nnU-Net U-Net-race AAnnU-Net U-Net-race ASnnU-Net U-Net-race WnnU-Net U-Net-race HL nnU-Net U-Net-race U UniSeg-race O UniSeg-race AAUniSeg-race ASUniSeg-race WUniSeg-race HL UniSeg-race U Diff-UNet-race O Diff-UNet-race AADiff-UNet-race ASDiff-UNet-race WDiff-UNet-race HL Diff-UNet-race U NexToU-race O NexToU-race AANexToU-race ASNexToU-race WNexToU-race HL NexToU-race USegVol-race O SegVol-race AASegVol-race ASSegVol-race WSegVol-race HL SegVol-race U U-Net & CLIP-race O U-Net & CLIP-race AAU-Net & CLIP-race ASU-Net & CLIP-race WU-Net & CLIP-race HL U-Net & CLIP-race U SwinUNETR & CLIP-race O SwinUNETR & CLIP-race AASwinUNETR & CLIP-race ASSwinUNETR & CLIP-race WSwinUNETR & CLIP-race HL SwinUNETR & CLIP-race U LHU-Net-race O LHU-Net-race AALHU-Net-race ASLHU-Net-race WLHU-Net-race HL LHU-Net-race U UCTransNet-race O UCTransNet-race AAUCTransNet-race ASUCTransNet-race WUCTransNet-race HL UCTransNet-race USwinUNETR-race O SwinUNETR-race AASwinUNETR-race ASSwinUNETR-race WSwinUNETR-race HL SwinUNETR-race U UNEST-race O UNEST-race AAUNEST-race ASUNEST-race WUNEST-race HL UNEST-race UUNETR-race O UNETR-race AAUNETR-race ASUNETR-race WUNETR-race HL UNETR-race U SAM-Adapter-race O SAM-Adapter-race AASAM-Adapter-race ASSAM-Adapter-race WSAM-Adapter-race HL SAM-Adapter-race U AI Algorithm-Group ********************************************************************** ************************************************************************ ************ kidney right DSC by race in JHH : Boxplot showing right kidney DSC score by race in JHH. Statistical significance isindicated by stars: * p < 0.05, ** p <0.01, *** p < 0.001, **** p < 0.0001. We perform KruskalWallistests followed by post-hoc Mann-Whitney U Tests with Bonferroni correction. Here, we did notperform statistical comparisons between diverse AI algorithms. 0.00.20.40.60.81.0 DSC MedNeXt-race O MedNeXt-race ASMedNeXt-race AAMedNeXt-race WMedNeXt-race HL MedNeXt-race U STU-Net B-race O STU-Net B-race ASSTU-Net B-race AASTU-Net B-race WSTU-Net B-race HL STU-Net B-race USTU-Net L-race O STU-Net L-race ASSTU-Net L-race AASTU-Net L-race WSTU-Net L-race HL STU-Net L-race USTU-Net H-race O STU-Net H-race ASSTU-Net H-race AASTU-Net H-race WSTU-Net H-race HL STU-Net H-race U nnU-Net ResEncL-race O nnU-Net ResEncL-race ASnnU-Net ResEncL-race AAnnU-Net ResEncL-race WnnU-Net ResEncL-race HL nnU-Net ResEncL-race U MedFormer-race O MedFormer-race ASMedFormer-race AAMedFormer-race WMedFormer-race HL MedFormer-race U nnU-Net U-Net-race O nnU-Net U-Net-race ASnnU-Net U-Net-race AAnnU-Net U-Net-race WnnU-Net U-Net-race HL nnU-Net U-Net-race U UniSeg-race O UniSeg-race ASUniSeg-race AAUniSeg-race WUniSeg-race HL UniSeg-race U Diff-UNet-race O Diff-UNet-race ASDiff-UNet-race AADiff-UNet-race WDiff-UNet-race HL Diff-UNet-race U NexToU-race O NexToU-race ASNexToU-race AANexToU-race WNexToU-race HL NexToU-race USegVol-race O SegVol-race ASSegVol-race AASegVol-race WSegVol-race HL SegVol-race U U-Net & CLIP-race O U-Net & CLIP-race ASU-Net & CLIP-race AAU-Net & CLIP-race WU-Net & CLIP-race HL U-Net & CLIP-race U SwinUNETR & CLIP-race O SwinUNETR & CLIP-race ASSwinUNETR & CLIP-race AASwinUNETR & CLIP-race WSwinUNETR & CLIP-race HL SwinUNETR & CLIP-race U LHU-Net-race O LHU-Net-race ASLHU-Net-race AALHU-Net-race WLHU-Net-race HL LHU-Net-race U UCTransNet-race O UCTransNet-race ASUCTransNet-race AAUCTransNet-race WUCTransNet-race HL UCTransNet-race USwinUNETR-race O SwinUNETR-race ASSwinUNETR-race AASwinUNETR-race WSwinUNETR-race HL SwinUNETR-race U UNEST-race O UNEST-race ASUNEST-race AAUNEST-race WUNEST-race HL UNEST-race UUNETR-race O UNETR-race ASUNETR-race AAUNETR-race WUNETR-race HL UNETR-race U SAM-Adapter-race O SAM-Adapter-race ASSAM-Adapter-race AASAM-Adapter-race WSAM-Adapter-race HL SAM-Adapter-race U AI Algorithm-Group ************************ * ** kidney left DSC by race in JHH : Boxplot showing left kidney DSC score by race in JHH. Statistical significance isindicated by stars: * p < 0.05, ** p <0.01, *** p < 0.001, **** p < 0.0001. We perform KruskalWallistests followed by post-hoc Mann-Whitney U Tests with Bonferroni correction. Here, we did notperform statistical comparisons between diverse AI algorithms. 0.00.20.40.60.81.0 DSC MedNeXt-race O MedNeXt-race ASMedNeXt-race WMedNeXt-race HLMedNeXt-race AA MedNeXt-race U STU-Net B-race O STU-Net B-race ASSTU-Net B-race WSTU-Net B-race HLSTU-Net B-race AA STU-Net B-race USTU-Net L-race O STU-Net L-race ASSTU-Net L-race WSTU-Net L-race HLSTU-Net L-race AA STU-Net L-race USTU-Net H-race O STU-Net H-race ASSTU-Net H-race WSTU-Net H-race HLSTU-Net H-race AA STU-Net H-race U nnU-Net ResEncL-race O nnU-Net ResEncL-race ASnnU-Net ResEncL-race WnnU-Net ResEncL-race HLnnU-Net ResEncL-race AA nnU-Net ResEncL-race U MedFormer-race O MedFormer-race ASMedFormer-race WMedFormer-race HLMedFormer-race AA MedFormer-race U nnU-Net U-Net-race O nnU-Net U-Net-race ASnnU-Net U-Net-race WnnU-Net U-Net-race HLnnU-Net U-Net-race AA nnU-Net U-Net-race U UniSeg-race O UniSeg-race ASUniSeg-race WUniSeg-race HLUniSeg-race AA UniSeg-race U Diff-UNet-race O Diff-UNet-race ASDiff-UNet-race WDiff-UNet-race HLDiff-UNet-race AA Diff-UNet-race U NexToU-race O NexToU-race ASNexToU-race WNexToU-race HLNexToU-race AA NexToU-race USegVol-race O SegVol-race ASSegVol-race WSegVol-race HLSegVol-race AA SegVol-race U U-Net & CLIP-race O U-Net & CLIP-race ASU-Net & CLIP-race WU-Net & CLIP-race HLU-Net & CLIP-race AA U-Net & CLIP-race U SwinUNETR & CLIP-race O SwinUNETR & CLIP-race ASSwinUNETR & CLIP-race WSwinUNETR & CLIP-race HLSwinUNETR & CLIP-race AA SwinUNETR & CLIP-race U LHU-Net-race O LHU-Net-race ASLHU-Net-race WLHU-Net-race HLLHU-Net-race AA LHU-Net-race U UCTransNet-race O UCTransNet-race ASUCTransNet-race WUCTransNet-race HLUCTransNet-race AA UCTransNet-race USwinUNETR-race O SwinUNETR-race ASSwinUNETR-race WSwinUNETR-race HLSwinUNETR-race AA SwinUNETR-race U UNEST-race O UNEST-race ASUNEST-race WUNEST-race HLUNEST-race AA UNEST-race UUNETR-race O UNETR-race ASUNETR-race WUNETR-race HLUNETR-race AA UNETR-race U SAM-Adapter-race O SAM-Adapter-race ASSAM-Adapter-race WSAM-Adapter-race HLSAM-Adapter-race AA SAM-Adapter-race U AI Algorithm-Group ******** **** **** **** ************************* *********** * ***** *********** ****** **** **** **** gall bladder DSC by race in JHH : Boxplot showing gallbladder DSC score by race in JHH. Statistical significance isindicated by stars: * p < 0.05, ** p <0.01, *** p < 0.001, **** p < 0.0001. We perform KruskalWallistests followed by post-hoc Mann-Whitney U Tests with Bonferroni correction. Here, we did notperform statistical comparisons between diverse AI algorithms. 0.00.20.40.60.81.0 DSC MedNeXt-race O MedNeXt-race HL MedNeXt-race UMedNeXt-race WMedNeXt-race ASMedNeXt-race AASTU-Net B-race O STU-Net B-race HL STU-Net B-race USTU-Net B-race WSTU-Net B-race ASSTU-Net B-race AA STU-Net L-race O STU-Net L-race HL STU-Net L-race USTU-Net L-race WSTU-Net L-race ASSTU-Net L-race AASTU-Net H-race O STU-Net H-race HL STU-Net H-race USTU-Net H-race WSTU-Net H-race ASSTU-Net H-race AA nnU-Net ResEncL-race O nnU-Net ResEncL-race HL nnU-Net ResEncL-race UnnU-Net ResEncL-race WnnU-Net ResEncL-race ASnnU-Net ResEncL-race AA MedFormer-race O MedFormer-race HL MedFormer-race UMedFormer-race WMedFormer-race ASMedFormer-race AA nnU-Net U-Net-race O nnU-Net U-Net-race HL nnU-Net U-Net-race UnnU-Net U-Net-race WnnU-Net U-Net-race ASnnU-Net U-Net-race AA UniSeg-race O UniSeg-race HL UniSeg-race UUniSeg-race WUniSeg-race ASUniSeg-race AADiff-UNet-race O Diff-UNet-race HL Diff-UNet-race UDiff-UNet-race WDiff-UNet-race ASDiff-UNet-race AA NexToU-race O NexToU-race HL NexToU-race UNexToU-race WNexToU-race ASNexToU-race AA SegVol-race O SegVol-race HL SegVol-race USegVol-race WSegVol-race ASSegVol-race AA U-Net & CLIP-race O U-Net & CLIP-race HL U-Net & CLIP-race UU-Net & CLIP-race WU-Net & CLIP-race ASU-Net & CLIP-race AA SwinUNETR & CLIP-race O SwinUNETR & CLIP-race HL SwinUNETR & CLIP-race USwinUNETR & CLIP-race WSwinUNETR & CLIP-race ASSwinUNETR & CLIP-race AA LHU-Net-race O LHU-Net-race HL LHU-Net-race ULHU-Net-race WLHU-Net-race ASLHU-Net-race AA UCTransNet-race O UCTransNet-race HL UCTransNet-race UUCTransNet-race WUCTransNet-race ASUCTransNet-race AA SwinUNETR-race O SwinUNETR-race HL SwinUNETR-race USwinUNETR-race WSwinUNETR-race ASSwinUNETR-race AA UNEST-race O UNEST-race HL UNEST-race UUNEST-race WUNEST-race ASUNEST-race AA UNETR-race O UNETR-race HL UNETR-race UUNETR-race WUNETR-race ASUNETR-race AA SAM-Adapter-race O SAM-Adapter-race HL SAM-Adapter-race USAM-Adapter-race WSAM-Adapter-race ASSAM-Adapter-race AA AI Algorithm-Group ************ **************************** ************************************ **** ******** **************************** ************************************ ** ***** **** ** * *** * * ** * liver DSC by race in JHH : Boxplot showing liver DSC score by race in JHH. Statistical significance is indicatedby stars: * p < 0.05, ** p <0.01, *** p < 0.001, **** p < 0.0001. We perform KruskalWallis testsfollowed by post-hoc Mann-Whitney U Tests with Bonferroni correction. Here, we did not performstatistical comparisons between diverse AI algorithms. 0.00.20.40.60.81.0 DSC MedNeXt-race AS MedNeXt-race O MedNeXt-race HLMedNeXt-race WMedNeXt-race AA MedNeXt-race U STU-Net B-race AS STU-Net B-race O STU-Net B-race HLSTU-Net B-race WSTU-Net B-race AA STU-Net B-race USTU-Net L-race AS STU-Net L-race O STU-Net L-race HLSTU-Net L-race WSTU-Net L-race AA STU-Net L-race U STU-Net H-race AS STU-Net H-race O STU-Net H-race HLSTU-Net H-race WSTU-Net H-race AA STU-Net H-race U nnU-Net ResEncL-race AS nnU-Net ResEncL-race O nnU-Net ResEncL-race HLnnU-Net ResEncL-race WnnU-Net ResEncL-race AA nnU-Net ResEncL-race U MedFormer-race AS MedFormer-race O MedFormer-race HLMedFormer-race WMedFormer-race AA MedFormer-race U nnU-Net U-Net-race AS nnU-Net U-Net-race O nnU-Net U-Net-race HLnnU-Net U-Net-race WnnU-Net U-Net-race AA nnU-Net U-Net-race U UniSeg-race AS UniSeg-race O UniSeg-race HLUniSeg-race WUniSeg-race AA UniSeg-race U Diff-UNet-race AS Diff-UNet-race O Diff-UNet-race HLDiff-UNet-race WDiff-UNet-race AA Diff-UNet-race UNexToU-race AS NexToU-race O NexToU-race HLNexToU-race WNexToU-race AA NexToU-race USegVol-race AS SegVol-race O SegVol-race HLSegVol-race WSegVol-race AA SegVol-race U U-Net & CLIP-race AS U-Net & CLIP-race O U-Net & CLIP-race HLU-Net & CLIP-race WU-Net & CLIP-race AA U-Net & CLIP-race U SwinUNETR & CLIP-race AS SwinUNETR & CLIP-race O SwinUNETR & CLIP-race HLSwinUNETR & CLIP-race WSwinUNETR & CLIP-race AA SwinUNETR & CLIP-race U LHU-Net-race AS LHU-Net-race O LHU-Net-race HLLHU-Net-race WLHU-Net-race AA LHU-Net-race U UCTransNet-race AS UCTransNet-race O UCTransNet-race HLUCTransNet-race WUCTransNet-race AA UCTransNet-race USwinUNETR-race AS SwinUNETR-race O SwinUNETR-race HLSwinUNETR-race WSwinUNETR-race AA SwinUNETR-race U UNEST-race AS UNEST-race O UNEST-race HLUNEST-race WUNEST-race AA UNEST-race U UNETR-race AS UNETR-race O UNETR-race HLUNETR-race WUNETR-race AA UNETR-race U SAM-Adapter-race AS SAM-Adapter-race O SAM-Adapter-race HLSAM-Adapter-race WSAM-Adapter-race AA SAM-Adapter-race U AI Algorithm-Group ***** ** stomach DSC by race in JHH : Boxplot showing stomach DSC score by race in JHH. Statistical significance is indicatedby stars: * p < 0.05, ** p <0.01, *** p < 0.001, **** p < 0.0001. We perform KruskalWallis testsfollowed by post-hoc Mann-Whitney U Tests with Bonferroni correction. Here, we did not performstatistical comparisons between diverse AI algorithms. 0.00.20.40.60.81.0 DSC MedNeXt-race AA MedNeXt-race OMedNeXt-race WMedNeXt-race HLMedNeXt-race AS MedNeXt-race U STU-Net B-race AA STU-Net B-race OSTU-Net B-race WSTU-Net B-race HLSTU-Net B-race AS STU-Net B-race U STU-Net L-race AA STU-Net L-race OSTU-Net L-race WSTU-Net L-race HLSTU-Net L-race AS STU-Net L-race U STU-Net H-race AA STU-Net H-race OSTU-Net H-race WSTU-Net H-race HLSTU-Net H-race AS STU-Net H-race U nnU-Net ResEncL-race AA nnU-Net ResEncL-race OnnU-Net ResEncL-race WnnU-Net ResEncL-race HLnnU-Net ResEncL-race AS nnU-Net ResEncL-race U MedFormer-race AA MedFormer-race OMedFormer-race WMedFormer-race HLMedFormer-race AS MedFormer-race U nnU-Net U-Net-race AA nnU-Net U-Net-race OnnU-Net U-Net-race WnnU-Net U-Net-race HLnnU-Net U-Net-race AS nnU-Net U-Net-race U UniSeg-race AA UniSeg-race OUniSeg-race WUniSeg-race HLUniSeg-race AS UniSeg-race U Diff-UNet-race AA Diff-UNet-race ODiff-UNet-race WDiff-UNet-race HLDiff-UNet-race AS Diff-UNet-race UNexToU-race AA NexToU-race ONexToU-race WNexToU-race HLNexToU-race AS NexToU-race USegVol-race AA SegVol-race OSegVol-race WSegVol-race HLSegVol-race AS SegVol-race U U-Net & CLIP-race AA U-Net & CLIP-race OU-Net & CLIP-race WU-Net & CLIP-race HLU-Net & CLIP-race AS U-Net & CLIP-race U SwinUNETR & CLIP-race AA SwinUNETR & CLIP-race OSwinUNETR & CLIP-race WSwinUNETR & CLIP-race HLSwinUNETR & CLIP-race AS SwinUNETR & CLIP-race U LHU-Net-race AA LHU-Net-race OLHU-Net-race WLHU-Net-race HLLHU-Net-race AS LHU-Net-race U UCTransNet-race AA UCTransNet-race OUCTransNet-race WUCTransNet-race HLUCTransNet-race AS UCTransNet-race USwinUNETR-race AA SwinUNETR-race OSwinUNETR-race WSwinUNETR-race HLSwinUNETR-race AS SwinUNETR-race U UNEST-race AA UNEST-race OUNEST-race WUNEST-race HLUNEST-race AS UNEST-race U UNETR-race AA UNETR-race OUNETR-race WUNETR-race HLUNETR-race AS UNETR-race U SAM-Adapter-race AA SAM-Adapter-race OSAM-Adapter-race WSAM-Adapter-race HLSAM-Adapter-race AS SAM-Adapter-race U AI Algorithm-Group ****** ********************** ** ** ** ** **** **** *** ** **************** aorta DSC by race in JHH : Boxplot showing aorta DSC score by race in JHH. Statistical significance is indicatedby stars: * p < 0.05, ** p <0.01, *** p < 0.001, **** p < 0.0001. We perform KruskalWallis testsfollowed by post-hoc Mann-Whitney U Tests with Bonferroni correction. Here, we did not performstatistical comparisons between diverse AI algorithms. 0.00.20.40.60.81.0 DSC MedNeXt-race U MedNeXt-race HLMedNeXt-race WMedNeXt-race AS MedNeXt-race O MedNeXt-race AASTU-Net B-race U STU-Net B-race HLSTU-Net B-race WSTU-Net B-race AS STU-Net B-race O STU-Net B-race AA STU-Net L-race U STU-Net L-race HLSTU-Net L-race WSTU-Net L-race AS STU-Net L-race O STU-Net L-race AASTU-Net H-race U STU-Net H-race HLSTU-Net H-race WSTU-Net H-race AS STU-Net H-race O STU-Net H-race AA nnU-Net ResEncL-race U nnU-Net ResEncL-race HLnnU-Net ResEncL-race WnnU-Net ResEncL-race AS nnU-Net ResEncL-race O nnU-Net ResEncL-race AA MedFormer-race U MedFormer-race HLMedFormer-race WMedFormer-race AS MedFormer-race O MedFormer-race AA nnU-Net U-Net-race U nnU-Net U-Net-race HLnnU-Net U-Net-race WnnU-Net U-Net-race AS nnU-Net U-Net-race O nnU-Net U-Net-race AA UniSeg-race U UniSeg-race HLUniSeg-race WUniSeg-race AS UniSeg-race O UniSeg-race AADiff-UNet-race U Diff-UNet-race HLDiff-UNet-race WDiff-UNet-race AS Diff-UNet-race O Diff-UNet-race AA NexToU-race U NexToU-race HLNexToU-race WNexToU-race AS NexToU-race O NexToU-race AA SegVol-race U SegVol-race HLSegVol-race WSegVol-race AS SegVol-race O SegVol-race AA U-Net & CLIP-race U U-Net & CLIP-race HLU-Net & CLIP-race WU-Net & CLIP-race AS U-Net & CLIP-race O U-Net & CLIP-race AA SwinUNETR & CLIP-race U SwinUNETR & CLIP-race HLSwinUNETR & CLIP-race WSwinUNETR & CLIP-race AS SwinUNETR & CLIP-race O SwinUNETR & CLIP-race AA LHU-Net-race U LHU-Net-race HLLHU-Net-race WLHU-Net-race AS LHU-Net-race O LHU-Net-race AA UCTransNet-race U UCTransNet-race HLUCTransNet-race WUCTransNet-race AS UCTransNet-race O UCTransNet-race AA SwinUNETR-race U SwinUNETR-race HLSwinUNETR-race WSwinUNETR-race AS SwinUNETR-race O SwinUNETR-race AA UNEST-race U UNEST-race HLUNEST-race WUNEST-race AS UNEST-race O UNEST-race AA UNETR-race U UNETR-race HLUNETR-race WUNETR-race AS UNETR-race O UNETR-race AA SAM-Adapter-race U SAM-Adapter-race HLSAM-Adapter-race WSAM-Adapter-race AS SAM-Adapter-race O SAM-Adapter-race AA AI Algorithm-Group * **** ** ****** postcava DSC by race in JHH : Boxplot showing postcava DSC score by race in JHH. Statistical significance is indicatedby stars: * p < 0.05, ** p <0.01, *** p < 0.001, **** p < 0.0001. We perform KruskalWallis testsfollowed by post-hoc Mann-Whitney U Tests with Bonferroni correction. Here, we did not performstatistical comparisons between diverse AI algorithms. 0.00.20.40.60.81.0 DSC MedNeXt-race AS MedNeXt-race OMedNeXt-race WMedNeXt-race HLMedNeXt-race AA MedNeXt-race U STU-Net B-race AS STU-Net B-race OSTU-Net B-race WSTU-Net B-race HLSTU-Net B-race AA STU-Net B-race USTU-Net L-race AS STU-Net L-race OSTU-Net L-race WSTU-Net L-race HLSTU-Net L-race AA STU-Net L-race U STU-Net H-race AS STU-Net H-race OSTU-Net H-race WSTU-Net H-race HLSTU-Net H-race AA STU-Net H-race U nnU-Net ResEncL-race AS nnU-Net ResEncL-race OnnU-Net ResEncL-race WnnU-Net ResEncL-race HLnnU-Net ResEncL-race AA nnU-Net ResEncL-race U MedFormer-race AS MedFormer-race OMedFormer-race WMedFormer-race HLMedFormer-race AA MedFormer-race U nnU-Net U-Net-race AS nnU-Net U-Net-race OnnU-Net U-Net-race WnnU-Net U-Net-race HLnnU-Net U-Net-race AA nnU-Net U-Net-race U UniSeg-race AS UniSeg-race OUniSeg-race WUniSeg-race HLUniSeg-race AA UniSeg-race U Diff-UNet-race AS Diff-UNet-race ODiff-UNet-race WDiff-UNet-race HLDiff-UNet-race AA Diff-UNet-race UNexToU-race AS NexToU-race ONexToU-race WNexToU-race HLNexToU-race AA NexToU-race USegVol-race AS SegVol-race OSegVol-race WSegVol-race HLSegVol-race AA SegVol-race U U-Net & CLIP-race AS U-Net & CLIP-race OU-Net & CLIP-race WU-Net & CLIP-race HLU-Net & CLIP-race AA U-Net & CLIP-race U SwinUNETR & CLIP-race AS SwinUNETR & CLIP-race OSwinUNETR & CLIP-race WSwinUNETR & CLIP-race HLSwinUNETR & CLIP-race AA SwinUNETR & CLIP-race U LHU-Net-race AS LHU-Net-race OLHU-Net-race WLHU-Net-race HLLHU-Net-race AA LHU-Net-race U UCTransNet-race AS UCTransNet-race OUCTransNet-race WUCTransNet-race HLUCTransNet-race AA UCTransNet-race USwinUNETR-race AS SwinUNETR-race OSwinUNETR-race WSwinUNETR-race HLSwinUNETR-race AA SwinUNETR-race U UNEST-race AS UNEST-race OUNEST-race WUNEST-race HLUNEST-race AA UNEST-race U UNETR-race AS UNETR-race OUNETR-race WUNETR-race HLUNETR-race AA UNETR-race U SAM-Adapter-race AS SAM-Adapter-race OSAM-Adapter-race WSAM-Adapter-race HLSAM-Adapter-race AA SAM-Adapter-race U AI Algorithm-Group **************** ** ********* **** * *** **** ******************** **** **** ******** **** ******** ************ * **************** **** ********** ** *** ***** *** *** * ** **** * ** * **************** **** **** ******** **** ******** ****** ******* **** **** pancreas DSC by race in JHH : Boxplot showing pancreas DSC score by race in JHH. Statistical significance isindicated by stars: * p < 0.05, ** p <0.01, *** p < 0.001, **** p < 0.0001. We perform KruskalWallistests followed by post-hoc Mann-Whitney U Tests with Bonferroni correction. Here, we did notperform statistical comparisons between diverse AI algorithms.",
  "EOn Label Noise": "AbdomenAtlas 1.0 is an amalgamation of 16 public datasets (Appendix A.1), which, when combinedtogether, resulted in a partially labeled dataset. Radiologists, assisted by AI, provided all themissing labels for 9 anatomical structures, making the dataset fully-labeled . When creatingAbdomenAtlas 1.0 we did not revise the labels that were already provided in the public datasets.However, upon future visual inspection, we found that these public datasets may share inconsistentannotation standards, also reported in Liu et al. . For example, the aorta annotation standard isinconsistent in AbdomenCT-12organ and other datasets: part of the upper aorta region is missing inAbdomenCT-12organ, while the aorta annotation is complete in BTCV and AMOS. Moreover, sincethe public datasets that constitute AbdomenAtlas 1.0 contained both automatic and manual labels,they can also portray human and AI errors. To address this, we developed an automatic label quality checking tool, based on anatomical priors(e.g., expected shape of organs), to detect and correct noisy labels. This tool indicated that aortaconcentrated most of the label noise in AbdomenAtlas 1.0. It has 32.4% of noisy labels, which aremostly the aforementioned incomplete annotations. The second structure with the highest amount ofdetected errors was the kidneys, but its percentage of noisy labels was much lower: 2.6%. Our tooldetected less than 1% of error in other classes. Therefore, the detected errors are mostly concentratedon one of the 9 annotated structures. Moreover, since AbdomenAtlas 1.0 carried the errors andannotation standard inconsistencies found in public datasets, the noise in AbdomenAtlas 1.0 labelsrepresents common annotation errors and inconsistencies. Conversely, studies on AI robustnessto label noise commonly rely on artificially generated noise . Thus, we viewed the realisticand quantifiable noise in AbdomenAtlas 1.0 as an opportunity to perform a realistic study on AIrobustness to label noise. To further increase the studys realism, we simulate the standard scenariowhere researchers are unaware of the noise: we did not inform the AI creators about the annotationerrors in AbdomenAtlas 1.0 prior to model training. This approach avoided uneven label correctionsby only some teams and ensured that the AI algorithms in this benchmark accurately represent therealistic scenario of AI trained on public data with common label noise, without creators activelytrying to counteract the noise. To assess AI robustness to label noise, the algorithms must be tested on datasets whose labels areless noisy than those in the training data. The JHH test set (N=5,160) was entirely annotated byradiologists, manually and following a well-defined annotation standard, over 5 years . Thus, itserves as a gold standard for low label noise. Touchstone leverages this large-scale, high-quality testdataset to verify whether AI trained noisy labels, representative of current public datasets, performswell when evaluated with high-quality manual labels. Since TotalSegmentator is not composed ofmultiple datasets, their annotation standards are consistent, and we detected low levels (<1%) of labelnoise on them. Thus, they are also adequate for evaluating AIs robustness. Additionally, to betterquantify the impact of label noise on AI accuracy, we re-trained ResEncL on AbdomenAtlas 1.0C.This dataset, which we publicly released, is a revised version of AbdomenAtlas 1.0, where labelswere improved by radiologists assisted by AI and by our error detection tool. The aorta was theonly class where the nnU-Net had large and significant performance increments (e.g., 10.35% DSCimprovement in TotalSegmentator). For other structures, improvements are mostly not significantand low, demonstrating that the AI algorithm is robust to moderate levels of label noise (e.g., lessthan 3% of noisy labels according to our detection tool), but not to excessive noise. The continuousimprovement of label noise detection and annotation quality, unifying annotation standards andcorrecting public datasets flawed labels, is a continuous commitment of Touchstone.",
  "FFull Affiliation List": "1Department of Computer Science, Johns Hopkins University2Department of Pharmacy and Biotechnology, University of Bologna3Center for Biomolecular Nanotechnologies, Istituto Italiano di Tecnologia4NVIDIA5Division of Medical Image Computing, German Cancer Research Center (DKFZ)6Helmholtz Imaging, German Cancer Research Center (DKFZ)7ESAT-PSI, KU Leuven8Faculty of Mathematics and Computer Science, Heidelberg University9HIDSS4Health - Helmholtz Information and Data Science School for Health10Shanghai Jiao Tong University11Shanghai Artificial Intelligence Laboratory12Pattern Analysis and Learning Group, Department of Radiation Oncology, Heidelberg UniversityHospital13Interactive Machine Learning Group (IML), DKFZ14School of Computer Science and Engineering, Northwestern Polytechnical University15Australian Institute for Machine Learning, The University of Adelaide16College of Computer Science and Technology, Zhejiang University17Hong Kong University of Science and Technology (Guangzhou)18Hong Kong University of Science and Technology19Faculty of Informatics and Data Science, University of Regensburg20Faculty of Electrical Engineering and Information Technology, RWTH Aachen University21Fraunhofer Institute for Digital Medicine MEVIS22Electronic & Information Engineering School, Harbin Institute of Technology (Shenzhen)23Beijing Academy of Artificial Intelligence (BAAI)24The Chinese University of Hong Kong25Peking University26Department of Electrical and Computer Engineering, Duke University27Stony Brook University28Department of Computer Science and Engineering, Department of Chemical and BiologicalEngineering and Division of Life Science, Hong Kong University of Science and Technology29Data Science and Computation Facility, Fondazione Istituto Italiano di Tecnologia30Ecole Polytechnique Fdrale de Lausanne",
  "GPotential Negative Societal Impacts": "Potential negative societal impacts of benchmarking AI algorithms for medical image segmentationinclude reinforcing biases, compromising data privacy, and leading to misuse of AI systems. Standardbenchmarks may suffer from in-distribution biases, small test sets, oversimplified metrics, and short-term outcome pressures, which can result in AI models that perform well on benchmarks but fail inreal-world applications. These issues can undermine the reliability, fairness, and generalizability ofAI systems in medical contexts, potentially causing harm and reducing trust in AI-driven healthcaresolutions."
}