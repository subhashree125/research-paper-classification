{
  "ABSTRACT": "Building generalist agents that can rapidly adapt to new environments is a keychallenge for deploying AI in the digital and real worlds. Is scaling current agentarchitectures the most effective way to build generalist agents? We propose anovel approach to pre-train relatively small policies on relatively small datasetsand adapt them to unseen environments via in-context learning, without any fine-tuning. Our key idea is that retrieval offers a powerful bias for fast adaptation.Indeed, we demonstrate that even a simple retrieval-based 1-nearest neighbor agentoffers a surprisingly strong baseline for todays state-of-the-art generalist agents.From this starting point, we construct a semi-parametric agent, REGENT, thattrains a transformer-based policy on sequences of queries and retrieved neigh-bors. REGENT can generalize to unseen robotics and game-playing environmentsvia retrieval augmentation and in-context learning, achieving this with up to 3xfewer parameters and up to an order-of-magnitude fewer pre-training datapoints,significantly outperforming todays state-of-the-art generalist agents. Website:",
  "INTRODUCTION": "AI agents, both in the digital and real world , constantly facechanging environments that require rapid or even instantaneous adaptation. True generalist agentsmust not only be capable of performing well on large numbers of training environments, but arguablymore importantly, they must be capable of adapting rapidly to new environments. While this goalhas been of considerable interest to the reinforcement learning research community, it has provenelusive. The most promising results so far have all been attributed to large policies ,pre-trained on large datasets across many environments, and even these models still struggle togeneralize to unseen environments without many new environment-specific demonstrations. In this work, we take a different approach to the problem of constructing such generalist agents.We start by asking: Is scaling current agent architectures the most effective way to build generalistagents? Observing that retrieval offers a powerful bias for fast adaptation, we first evaluate a simple1-nearest neighbor method: Retrieve and Play (R&P). To determine the action at the current state,R&P simply retrieves the closest state from a few demonstrations in the target environment and playsits corresponding action. Tested on a wide range of environments, both robotics and game-playing,R&P performs on-par or better than the state-of-the-art generalist agents. Note that these resultsinvolve no pre-training environments, and not even a neural network policy: it is clear that largermodel and pre-training dataset sizes are not the only roadblock to developing generalist agents. Having thus established the utility of retrieval for fast adaptation of sequential decision making agents,we proceed to incorporate it into the design of a Retrieval-Augmented Agent (REGENT). REGENTis a semi-parametric architecture: it pre-trains a transformer policy whose inputs are not only thecurrent state and previous reward, but also retrieved tuples of (state, previous reward, action) froma set of demonstrations for each pre-training task, drawing inspiration from the recent successesof retrieval augmentation in language modeling . At each query state, REGENT is trained toprescribe an action through aggregating the action predictions of R&P and the transformer policy. Byexploiting retrieval-augmentation as well as in-context learning, REGENT permits direct deployment",
  ": Problem setting in JAT/Gato environments": "in entirely unseen environments and tasks with only a few demonstrations. REGENT is only one oftwo models developed so far that can adapt to new environments via in-context learning: the othermodel is the multi-trajectory transformer (MTT) . We train and evaluate REGENT on two problem settings in this paper, shown in Figures 1 and 2. Thefirst setting is based on the environments used in Gato (and its open source reproduction JAT) and the second setting is based on the ProcGen environments used in MTT . In both settings, REGENT demonstrates significant generalization to unseen environments withoutany finetuning. In the JAT/Gato setting, REGENT outperforms JAT/Gato even when the baselineis finetuned on demonstrations from the unseen environments. In the ProcGen setting, REGENTsignificantly surpasses MTT. Moreover, in both settings, REGENT trains a smaller model with 1.4x to3x fewer parameters and with an order-of-magnitude fewer pre-training datapoints. Finally, whileREGENTs design choices are aimed at generalization, its gains are not limited to unseen environments:it even performs better than baselines when deployed within the pre-training environments.",
  "Recent work in the reinforcement learning community has been aimed at building foundation modelsand multi-task generalist agents": "Many existing generalist agents struggle to adapt to new environments. Gato , a populargeneralist agent trained on a variety of gameplay and robotic environments, struggles to achievetransfer to an unseen Atari game even after fine-tuning, irrespective of the pretraining data. Theauthors attribute this difficulty to the \"pronounced differences in the visuals, controls, and strategy\"among Atari games. They also attribute Gatos lack of in-context adaptation to the limited contextlength of the transformer not allowing for adequate data to be added in the context. Our methodsidesteps this issue by retrieving only limited but relevant parts of demonstration trajectories to includein the context. JAT , an open-source version of Gato, faces similar problems. We compare withand significantly outperform JAT/Gato using fewer parameters and an order-of-magnitudefewer pre-training datapoints. While REGENT is a 138.6M parameter model, JAT uses 192.7Mparameters. The closed-source Gato, with similar performance as the open-source JAT, reportsusing 1.2B parameters. JAT is also pre-trained on up to 5-10x the amount of data used by our methodand yet cannot generalize to unseen environments. Even after finetuning on a few demonstrationsfrom an unseen environment, JAT fails to meaningfully improve. Many recent generalist agents cannot leverage in-context learning. In-context learning capabil-ities enable easier and faster adaptation compared to finetuning. Robocat , which builds on theGato model, undergoes many cycles of fine-tuning, data collection, and pre-training from scratchto adapt to new manipulation tasks. The multi-game decision transformer , an agent trained onover 50 million Atari gameplay transitions, requires another 1 million transitions for fine-tuning on a",
  ": Problem setting in ProcGen environments adapted from": "held-out game which is not practical in real robot settings. We, on the other hand, show that REGENT(and even R&P) can adapt to new Atari games with as little as 10k transitions and no finetuning onsaid transitions. Finally, the RT series of robotics models , recent Vision-Language-Actionmodels like Octo, OpenVLA, Mobility VLA , and other generalist agents like BAKU,RoboAgent do not evaluate or are demonstrated to not possess in-context learning capabilities.REGENT on the other hand can adapt simply with in-context learning to unseen environments. Agents that can adapt to new tasks via in-context learning, only do so within the same envi-ronment. Algorithm Distillation , Decision Pretrained Transformer , and Prompt DecisionTransformer are three in-context reinforcement learning methods proposed to generalize tonew goals and tasks within the same environment. None of these can handle changes in visualobservations, available controls, and game dynamics. We also compare with and outperform MTT , the only other model that can adapt in-contextin the ProcGen setting, with improved data efficiency and a smaller model size. MTT trains atransformer on sequences of trajectories from a particular level and environment and adapts to unseenenvironments by throwing the demonstrations into its context. The ProcGen variant of REGENT usean order-of-magnitude fewer transitions in pre-training and is about one-third the size of MTT. Retrieval-augmented generation for training and deployment is a core part of our policy. Variouslanguage models trained with retrieval-augmentation such as the original RAG model , RETRO, and REALM have demonstrated performance on par with vanilla language models withsignificantly fewer parameters. Moreover, retrieval-augmented generation with large languagemodels has enabled them to quickly adapt to new or up-to-date data. We hope that our work canenable similar capabilities for decision-making agents.",
  "PROBLEM FORMULATION": "We aim to pre-train a generalist agent on datasets obtained from different environments, with the goalof generalizing to new unseen environments. The agent has access to a few expert demonstrations inthese new environments. In this work, the agent achieves this through in-context learning without anyadditional finetuning. We assume that the state and action spaces of unseen environments are known. We model each environment i as a Markov Decision Process (MDP). The i-th Markov DecisionProcess (MDP) Mi is a tuple (Si, Ai, Pi, Ri, i, Ii), where Si is the set of states, Ai is the set ofactions, Pi(s|s, a) is the probability of transitioning from state s to s when taking action a, Ri(s, a)is the reward obtained in state s upon taking action a, i [0, 1) is the discount factor, and Ii is theinitial state distribution. We assume that the MDPs operate over trajectories with finite length Hi, inan episodic fashion. Given a policy i acting on Mi, the expected cumulative reward accrued overthe duration of an episode (i.e., expected return) is given by J(i) = Ei[Hit=1 Ri(st, at)].",
  "Under review as a conference paper at ICLR 2025": "Generalization to unseen Mujoco environments/embodiments: In the two unseen Mujoco tasksshown in , R&P appears to be a strong baseline even at generalizing to new embodiments!Moreover, after finetuning on just 25 demonstrations, only REGENT, not JAT/Gato, significantlyimproves to outperform other methods and only continues to get better with more demonstrations. : Normalized returns in the unseen Mujoco environments against the number of demonstration trajectories the agent can retrieve fromor finetune on. Each value is an average across 100 rollouts of different seeds. Additional qualitative examples: We plot examples of the inputs and outputs of REGENT at variousstates during a rendered rollout in the metaworld-bin-picking environment in . In thiscontinuous Metaworld example, REGENT can be seen predicting actions that are somewhat similarto but not the same as the first retrieved (R&P) action. These differences lead to better overallperformance as seen in . These differences are also the direct result of REGENTs in-contextlearning capabilities, learned from the preprocessed datasets from the pre-training environments.",
  "REGENT: A RETRIEVAL-AUGMENTED GENERALIST AGENT": "Simple nearest neighbor retrieval approaches have a long history in few-shot learning .These works have found that, at small training dataset sizes, while parametric models might struggleto extract any signal without extensive architecture or hyperparameter tuning, nearest neighborapproaches perform about as well as the data can support. Motivated by these prior results in otherdomains, we first construct such an approach for an agent that can learn directly in an unseen env-ironment with limited expert demonstrations in .1. Then, we consider how to improve thisagent through access to experience in pre-training environments, so that it can transfer some knowled-ge to novel environments that allows it to adapt even more effectively with limited data in .2.",
  "RETRIEVE AND PLAY (R&P)": "This is arguably one of the simplest decision agents that leverages the retrieval toolset for adaptation.Given a state st from an environment j, let us assume that it is possible to retrieve the n-nearest states(and their corresponding previous rewards, actions) from Dj. We refer to this as the context ct Cj.The set Cj is the set of all such contexts in environment j. We also call the state st the query statefollowing terminology from retrieval-augmented generation for language modeling . The R&P agent takes the state st and context ct as input, picks the nearest retrieved state s in ct, andplays the corresponding action a. That is, R&P(st, ct) = a. We describe the retrieval process indetail later in .2. Clearly, R&P is devoid of any learning components which can transfercapabilities from pre-training to unseen environments.",
  "RETRIEVAL-AUGMENTED GENERALIST AGENT (REGENT)": "To go beyond R&P, we posit that if an agent learns to meaningfully combine relevant context toact in a set of training environments, then this skill should be transferable to novel environments aswell. We propose exactly such an agent in REGENT. We provide an overview of REGENT in where we depict the reinforcement learning loop with the retrieval mechanism and the REGENTtransformer. In the figure, we also include the retrieved context and query inputs to the transformerand its output interpolation with the R&P action. We describe REGENT in detail below. REGENT consists of a deep neural network policy , which takes as input the state st, previousreward rt1, context ct, and outputs the action directly for continuous environments and the logitsover the actions in discrete environments. In the context ct, the retrieved tuples of (state, previousreward, action) are placed in order of their closesness to the query state st with the closest retrievedstate s placed first. Let d(st, s) be the distance between st and s. We perform a distance weightedinterpolation between the outputs of the neural network policy and R&P as follows,",
  "(x) =Softmax(x),if action space is discreteL MixedReLU(x),if action space is continuous(2)": "where MixedReLU : R is a tanh-like activation function from and is detailed inAppendix A. Further, L R is a hyperparameter for scaling the output of the neural network forcontinuous action spaces after the MixedReLU. We simply set both L and to 10 everywherefollowing a similar choice in . The function is a causal transformer, which is adept at modelingrelatively long sequences of contextual information and predicting the optimal action .All distances are normalized and clipped to as detailed in Appendix A. For discrete actionspaces, where the transformer outputs a distribution over actions, we use a softened version of R&P",
  "SUB-OPTIMALITY BOUND FOR REGENT POLICIES": "In this section, we aim to bound the sub-optimality of the REGENT policy. This is measured withrespect to the expert policy j , that generated the retrieval demonstrations Dj. We focus on thediscrete action case here and leave the continuous action case for future work. The sub-optimality gapin (training or unseen) environment j is given by (J(j ) J(REGENT)). Inspired by the theoreticalanalysis of Sridhar et al. , we define the \"most isolated state\" and use this definition to bound thetotal variation in the REGENT policy class and hence the sub-optimality gap. That is, first, given Dj, we wish to obtain the maximum value of the distance term d(st, s) inEquations (1) and (3). To do so, we define the most isolated state as follows.Definition 5.1 (Most Isolated State). For a given set of retrieval demonstrations Dj in environment j,we define the most isolated state sIDj := arg maxsSj",
  "EXPERIMENTAL EVALUATION": "In our experiments, we aim to answer the following key questions in the two settings depicted inFigures 1 and 2. (1) How well can R&P and REGENT generalize to unseen environments? (2) Howdoes finetuning in the new environments improve REGENT? (3) How well can REGENT generalize tovariations of the training environments and perform in aggregate on training environments? (4) Howdoes REGENT qualitatively compare with R&P in using the retrieved context? Metrics: We plot the normalized return computed using the return of a random and expert agentin each environment as(total returnrandom return)(expert returnrandom return). The expert and random returns for all JAT/Gatoenvironments are made available in the original work . The expert and random returns for theProcGen environments can be found in the original ProcGen paper . Baselines: We consider a different set of baselines for each of the two settings. In the JAT/Gatosetting, we compare R&P and REGENT with two variants of JAT. The first is a JAT model trained onthe same dataset as REGENT. The second is a JAT model trained on all available JAT data whichconsists of an order of magnitude more datapoints (in particular, 5x more data in Atari environmentsand 10x more data in all other environments). We label the former apples-to-apples comparison asJAT/Gato and the latter as JAT/Gato (All Data). We also compare with JAT/Gato with RAGat inference time. We use the same retrieval mechanism as REGENT for this baseline. Additionaldetails and all hyperparameters for the baselines are in Appendix A. In the ProcGen setting, we compare with MTT . MTT only reports results on unseen environmentsand not on unseen levels in training environments. We simply take the best MTT result on each of thefour unseen environments in , obtained when 4 demonstrations are included in the MTT context. Finetuning and Train-from-scratch Baselines: We fully finetune and parameter-efficient finetune(PEFT with IA3 ) both JAT/Gato and JAT/Gato (All Data) on various number of demosin each unseen environment in the JAT/Gato setting and compare with REGENT. We also compare",
  "CONCLUSIONS AND FUTURE WORK": "In this work, we demonstrated that retrieval offers generalist agents a powerful bias for rapidadaptation to new environments, even without large models and vast datasets. We showed thata simple retrieval-based 1-nearest neighbor agent, Retrieve and Play (R&P), is a strong baselinethat matches or exceeds the performance of todays state-of-the-art generalist agents in unseenenvironments. Building on this, we proposed a semi-parametric generalist agent, REGENT, thatpre-trains a transformer-based policy on sequences of query state, reward, and retrieved context.REGENT exploits retrieval-augmentation and in-context learning for direct deployment in unseenenvironments with only a few demonstrations. Even after pre-training on an order of magnitude fewerdatapoints than other generalist agents and with fewer parameters, REGENT outperforms them acrossunseen environments and surpasses them even after they have been finetuned on demonstrations fromthose unseen environments. REGENT, itself, further improves with finetuning on even a small numberof demonstrations. We note that REGENT faces a couple of limitations: adapting to new embodimentsand long-horizon environments. In future work, we believe a larger diversity of embodiments inthe training dataset and improved retrieval from longer demonstrations can help REGENT overcomethese challenges. We conclude with the conviction that retrieval in general and REGENT in particularredefines the possibilities for developing highly adaptive and efficient generalist agents, even withlimited resources.",
  "M Saiful Bari, Batool Haider, and Saab Mansour. Nearest neighbour few-shot learning forcross-lingual classification. arXiv preprint arXiv:2109.02221, 2021. (Cited on 4)": "Homanga Bharadhwaj, Jay Vakil, Mohit Sharma, Abhinav Gupta, Shubham Tulsiani, andVikash Kumar. Roboagent: Generalization and efficiency in robot manipulation via semanticaugmentations and action chunking. In 2024 IEEE International Conference on Robotics andAutomation (ICRA), pp. 47884795. IEEE, 2024. (Cited on 2, 3) Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, KatieMillican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark,et al. Improving language models by retrieving from trillions of tokens. In Internationalconference on machine learning, pp. 22062240. PMLR, 2022. (Cited on 3, 4) Konstantinos Bousmalis, Giulia Vezzani, Dushyant Rao, Coline Devin, Alex X Lee, MariaBauza, Todor Davchev, Yuxiang Zhou, Agrim Gupta, Akhil Raju, et al. Robocat: A self-improving foundation agent for robotic manipulation. arXiv preprint arXiv:2306.11706, 2023.(Cited on 1, 2)",
  "Kiante Brantley, Wen Sun, and Mikael Henaff. Disagreement-regularized imitation learning. InInternational Conference on Learning Representations, 2019. (Cited on 8, 18)": "Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn,Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. Rt-1: Roboticstransformer for real-world control at scale. arXiv preprint arXiv:2212.06817, 2022. (Cited on 1,2, 3) Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, PieterAbbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learningvia sequence modeling. Advances in neural information processing systems, 34:1508415097,2021. (Cited on 4)",
  "Wei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Frank Wang, and Jia-Bin Huang. A closerlook at few-shot classification. arXiv preprint arXiv:1904.04232, 2019. (Cited on 4)": "Hao-Tien Lewis Chiang, Zhuo Xu, Zipeng Fu, Mithun George Jacob, Tingnan Zhang, Tsang-Wei Edward Lee, Wenhao Yu, Connor Schenck, David Rendleman, Dhruv Shah, et al. Mobilityvla: Multimodal instruction navigation with long-context vlms and topological graphs. arXivpreprint arXiv:2407.07775, 2024. (Cited on 2, 3) Karl Cobbe, Chris Hesse, Jacob Hilton, and John Schulman. Leveraging procedural generationto benchmark reinforcement learning. In International conference on machine learning, pp.20482056. PMLR, 2020. (Cited on 7)",
  "Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen Koltun. Carla:An open urban driving simulator. In Conference on robot learning, pp. 116. PMLR, 2017.(Cited on 17)": "Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy, Pierre-Emmanuel Mazar, Maria Lomeli, Lucas Hosseini, and Herv Jgou. The faiss library. arXivpreprint arXiv:2401.08281, 2024. (Cited on 6) Souradeep Dutta, Kaustubh Sridhar, Osbert Bastani, Edgar Dobriban, James Weimer, Insup Lee,and Julia Parish-Morris. Exploring with sticky mittens: Reinforcement learning with expertinterventions via option templates. In Conference on Robot Learning, pp. 14991509. PMLR,2023. (Cited on 17) Kiana Ehsani, Tanmay Gupta, Rose Hendrix, Jordi Salvador, Luca Weihs, Kuo-Hao Zeng,Kunal Pratap Singh, Yejin Kim, Winson Han, Alvaro Herrasti, et al. Imitating shortest pathsin simulation enables effective navigation and manipulation in the real world. arXiv preprintarXiv:2312.02976, 2023. (Cited on 2) Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Vlad Mnih, Tom Ward, YotamDoron, Vlad Firoiu, Tim Harley, Iain Dunning, et al. Impala: Scalable distributed deep-rlwith importance weighted actor-learner architectures. In International conference on machinelearning, pp. 14071416. PMLR, 2018. (Cited on 18) Quentin Galloudec, Edward Beeching, Clment Romac, and Emmanuel Dellandra. Jack ofall trades, master of some, a multi-purpose transformer agent. arXiv preprint arXiv:2402.09844,2024. (Cited on 1, 2, 5, 6, 7, 17) Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun,and Haofen Wang. Retrieval-augmented generation for large language models: A survey. arXivpreprint arXiv:2312.10997, 2023. (Cited on 1, 3, 4) Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrievalaugmented language model pre-training. In International conference on machine learning, pp.39293938. PMLR, 2020. (Cited on 3)",
  "Siddhant Haldar, Zhuoran Peng, and Lerrel Pinto. Baku: An efficient transformer for multi-taskpolicy learning. arXiv preprint arXiv:2406.07539, 2024. (Cited on 2, 3)": "Tomoyuki Kagaya, Thong Jing Yuan, Yuxuan Lou, Jayashree Karlekar, Sugiri Pranata, AkiraKinose, Koki Oguri, Felix Wick, and Yang You. Rap: Retrieval-augmented planning withcontextual memory for multimodal llm agents. arXiv preprint arXiv:2402.03610, 2024. (Citedon 30) Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair,Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, et al. Openvla: An open-sourcevision-language-action model. arXiv preprint arXiv:2406.09246, 2024. (Cited on 1, 2, 3) Louis Kirsch, James Harrison, Daniel Freeman, Jascha Sohl-Dickstein, and Jrgen Schmidhuber.Towards general-purpose in-context learning agents. Workshop on Distribution Shifts, 37thConference on Neural Information ..., 2023. (Cited on 30) Michael Laskin, Luyu Wang, Junhyuk Oh, Emilio Parisotto, Stephen Spencer, Richie Steiger-wald, DJ Strouse, Steven Hansen, Angelos Filos, Ethan Brooks, et al. In-context reinforcementlearning with algorithm distillation. arXiv preprint arXiv:2210.14215, 2022. (Cited on 3) Jonathan Lee, Annie Xie, Aldo Pacchiano, Yash Chandak, Chelsea Finn, Ofir Nachum, andEmma Brunskill. Supervised pretraining can learn in-context reinforcement learning. Advancesin Neural Information Processing Systems, 36, 2024. (Cited on 3)",
  "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. (Cited on 6)": "Nived Rajaraman, Lin F. Yang, Jiantao Jiao, and Kannan Ramchandran. Toward the fundamentallimits of imitation learning. In Proceedings of the 34th International Conference on NeuralInformation Processing Systems, NIPS20, Red Hook, NY, USA, 2020. Curran Associates Inc.ISBN 9781713829546. (Cited on 19) Sharath Chandra Raparthy, Eric Hambro, Robert Kirk, Mikael Henaff, and Roberta Raileanu.Generalization to new sequential decision making tasks with in-context learning. arXiv preprintarXiv:2312.03801, 2023. (Cited on 1, 2, 3, 5, 6, 7, 9, 17, 28) Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov,Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, et al.A generalist agent. arXiv preprint arXiv:2205.06175, 2022. (Cited on 1, 2) Thomas Schmied, Fabian Paischer, Vihang Patil, Markus Hofmarcher, Razvan Pascanu, andSepp Hochreiter. Retrieval-augmented decision transformer: External memory for in-context rl.arXiv preprint arXiv:2410.07071, 2024. (Cited on 30) Dhruv Shah, Ajay Sridhar, Arjun Bhorkar, Noriaki Hirose, and Sergey Levine. Gnm: A generalnavigation model to drive any robot. In 2023 IEEE International Conference on Robotics andAutomation (ICRA), pp. 72267233. IEEE, 2023. (Cited on 2) Dhruv Shah, Ajay Sridhar, Nitish Dashora, Kyle Stachowicz, Kevin Black, Noriaki Hi-rose, and Sergey Levine. Vint: A foundation model for visual navigation. arXiv preprintarXiv:2306.14846, 2023. (Cited on 2)",
  "Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning.Advances in neural information processing systems, 30, 2017. (Cited on 4)": "Kaustubh Sridhar and Srikant Sukumar. Finite-time, event-triggered tracking control of quadro-tors. In 5th CEAS Specialist Conference on Guidance, Navigation & Control (EurGNC 19)Milano, Italy, 2019. (Cited on 17) Kaustubh Sridhar, Oleg Sokolsky, Insup Lee, and James Weimer. Improving neural networkrobustness via persistency of excitation. In 2022 American Control Conference (ACC), pp.15211526. IEEE, 2022. (Cited on 17) Kaustubh Sridhar, Souradeep Dutta, Dinesh Jayaraman, James Weimer, and Insup Lee. Memory-consistent neural networks for imitation learning. arXiv preprint arXiv:2310.06171, 2023. (Citedon 4, 7, 16, 17) Kaustubh Sridhar, Souradeep Dutta, James Weimer, and Insup Lee. Guaranteed conformance ofneurosymbolic models to natural constraints. In Learning for Dynamics and Control Conference,pp. 7689. PMLR, 2023. (Cited on 17) Octo Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, SudeepDasari, Joey Hejna, Tobias Kreiman, Charles Xu, et al. Octo: An open-source generalist robotpolicy. arXiv preprint arXiv:2405.12213, 2024. (Cited on 1, 2, 3)",
  "Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks forone shot learning. Advances in neural information processing systems, 29, 2016. (Cited on 4)": "Yan Wang, Wei-Lun Chao, Kilian Q Weinberger, and Laurens Van Der Maaten. Simpleshot: Re-visiting nearest-neighbor classification for few-shot learning. arXiv preprint arXiv:1911.04623,2019. (Cited on 4) Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment:from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600612, 2004. (Cited on 6) Mengdi Xu, Yikang Shen, Shun Zhang, Yuchen Lu, Ding Zhao, Joshua Tenenbaum, and ChuangGan. Prompting decision transformer for few-shot policy generalization. In internationalconference on machine learning, pp. 2463124645. PMLR, 2022. (Cited on 3) John Yang, Carlos E Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, KarthikNarasimhan, and Ofir Press. Swe-agent: Agent-computer interfaces enable automated softwareengineering. arXiv preprint arXiv:2405.15793, 2024. (Cited on 1)",
  "Jonathan Yang, Dorsa Sadigh, and Chelsea Finn. Polybot: Training one policy across robotswhile embracing variability. arXiv preprint arXiv:2307.03719, 2023. (Cited on 2)": "Jonathan Yang, Catherine Glossop, Arjun Bhorkar, Dhruv Shah, Quan Vuong, Chelsea Finn,Dorsa Sadigh, and Sergey Levine. Pushing the limits of cross-embodiment learning for manipu-lation and navigation. arXiv preprint arXiv:2402.19432, 2024. (Cited on 2) Yahan Yang, Ramneet Kaur, Souradeep Dutta, and Insup Lee. Interpretable detection of distri-bution shifts in learning enabled cyber-physical systems. In 2022 ACM/IEEE 13th InternationalConference on Cyber-Physical Systems (ICCPS), pp. 225235. IEEE, 2022. (Cited on 17) Zhuolin Yang, Wei Ping, Zihan Liu, Vijay Korthikanti, Weili Nie, De-An Huang, Linxi Fan,Zhiding Yu, Shiyi Lan, Bo Li, et al. Re-vilm: Retrieval-augmented visual language model forzero and few-shot image captioning. arXiv preprint arXiv:2302.04858, 2023. (Cited on 30) Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, andSergey Levine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcementlearning. In Conference on robot learning, pp. 10941100. PMLR, 2020. (Cited on 17)",
  ")) 1) which simplifies to 1 for x < 1, xfor 1 x 1, and 1 for x > 1": "Normalizing distances to : We compute the 95th percentile of all distances d(s, s) between any(retrieved or query) state s and the first (closest) retrieved state s. This value is computed from thedemonstrations Dj and is used to normalize all distances in that environment. This value is calculatedfor all (training or unseen) environments during the preprocessing stage. If after normalization, adistance value is greater than 1, we simply clip it to 1.",
  "Nact, if a = a(3)": "The distribution induced by the modified R&P function assigns all probability mass to the action awhen d(st, s) = 0 and acts like a uniform distribution when d(st, s) = 1, thereby preventing biastoward any single logit. Architectural hyperparameters: In both settings, the transformer trunk consists of 12 layers and 12heads with a hidden size of 768. We set the maximum position encodings to 40 for 20 (state, previousreward)s and 20 actions. Of these 20, 19 belong to the retrieved context and 1 belongs to the query.In the JAT/Gato setting, the maximum multi-discrete observation size is 212 (for BabyAI). Themaximum continuous observation size as discussed before in is set to 513, a consequence ofthe ResNet18 image embedding modelss embedding size of 512 with an additional 1 dimensionfor the reward. All linear encoding layers map from their corresponding input size to the hidden size.Following the JAT model, the linear decoder head for predicting continuous actions maps from thehidden size to the maximum continuous size discussed above (513). On the other hand, the lineardecoder head for predicting distributions over discrete actions maps from the hidden size to onlythe maximum number of discrete actions across all discrete environments (18), whereas in the JATmodel, they map to the full GPT2 vocab size of 50257. In the JAT/Gato setting, the original JAT architecture has a much larger context size (that isnot required for REGENT), a larger image encoder (than the resnet18 used in REGENT), a muchlarger discrete decoder predicting distributions over the entire GPT2 vocab size (instead of just themaximum number of discrete actions in REGENT), and has (optional) decoders for predicting variousobservations (that is also not required for REGENT).",
  "In the ProcGen setting, MTT has a 18 layers, 16 heads, and a hidden size of 1024 all three of whichare much larger than REGENTs architectural hyperparameters mentioned above": "Training hyperparameters: In the JAT/Gato setting, we use a batch size of 512 and the AdamWoptimizer with parameters 1 = 0.9 and 2 = 0.999. The learning rate starts at 5e-5 and decays tozero through 3 epochs over all pre-training data. We early stop after a single epoch because we findthat overtraining reduces in-context learning performance. This observation is consistent with similarobservations about the transience of in-context learning in literature . In the ProcGen setting, we use a batch size of 1024, a starting learning of 1e-4 also decaying over 3epochs over all the pre-training data with an early stop after the first epoch. We again use the AdamWoptimizer with parameters 1 = 0.9 and 2 = 0.95. Designating retrieval demonstrations in each training environments dataset: In the JAT/Gatosetting, we designate 100 random demonstrations from the each of training vector observationenvironments as the retrieval subset for that environment. We also designate 5 random demonstrationsfrom the training image environments as the retrieval subsets for said environments. In the ProcGen",
  "setting, with only 20 demonstrations per level per environment, we designate all 20 as the retrievalset for that level in that environment": "We use all states in all demonstrations in each training environments dataset as query states. So,when a query state is from a demonstration in the designated retrieval subset, we simply retrievefrom all other demonstrations in the designated retrieval subset except the current one. When aquery state is from a demonstration not in the designated retrieval subset, we retrieve simply fromthe designated retrieval subset. This way, we ensure that none of the retrieved states are from thesame demonstration as the query state. A possible direction for future work includes intelligentlydesignating and collecting demonstrations for retrieval (perhaps using ideas like persistency ofexcitation , memory-based methods , expert intervention methods ). Choice of held-out environments: In the JAT/Gato setting, for the unseen environments, wehold-out the 5 Metaworld environments recommended in , the 5 Atari environments held-outin , and finally, we choose two Mujoco environments of mixed difficulty (see ). In theProcGen setting, following MTT , we hold-out 5 environments. In this work, we generalize to unseen environments from the same suites as the training environments.In future work, it would be interesting to examine what is needed for generalization to new suites (suchas more manipulation suites , quadrotor, driving simulators , biological simulation, etc.). Additional details on all environments: In the JAT/Gato setting, the Atari environments have 18discrete actions and four consecutive grayscale images as states. However, we embed each combinedfour-stacked image, with an off-the-shelf ResNet18 encoder , into a vector of 512 dimensionsand use this image embedding everywhere (for R&P and REGENT). All Metaworld environmentshave observations with 39 dimensions and actions with 4 dimensions. Mujoco environments haveobservations with 4 to 376 dimensions and actions with 1 to 17 dimensions. The BabyAI environmentshave up to 7 discrete actions. They have discrete observations and text observations. After tokenizingthe BabyAI text observations, which specify a goal in natural language, into 64 tokens, BabyAIobservations have 212 total dimensions and consist only of discrete values.",
  "Additional Details on Various Baselines": "Additional information on re-training JAT/Gato: When retraining JAT/Gato, we follow allhypepermaters chosen in . We note that we skip the VQA datasets in our retraining of JAT/Gato,and use a smaller batch size to fit available GPUs. For JAT/Gato (All Data) we train over thefull JAT dataset for the specified 250k training steps. For JAT/Gato we train on the REGENT subset(which is 5-10x smaller) for 25k training steps. In both variants, the training loss converges to thesame low value at about 5000 steps into the training run. Additional information on JAT/Gato + RAG (at inference): We note that JAT/Gato is likea typical LLM it has a causal transformer architecture and undergoes vanilla pretraining onconsecutive states-rewards-actions. In both the JAT/Gato + RAG baselines, we use the sameretrieval mechanism as REGENT. We provide the retrieved states-rewards-actions context to theseRAG baselines replacing the context of the past few states-rewards-actions in vanilla JAT/Gatobaselines. REGENT significantly outperforms both JAT/Gato + RAG baselines in . This demonstratesthe advantage of REGENTs architecture and retrieval augmented pretraining over just performingRAG at inference time. But, the JAT/Gato + RAG baselines outperform vanilla JAT/Gato as wellas the JAT/Gato fully Finetuned baselines in the unseen metaworld environments. From this we canconclude that RAG at inference time is also important for performance. Additional information on JAT/Gato Finetuned and REGENT Finetuned: Starting from a pre-trained checkpoint, we finetune the full REGENT and JAT/Gato checkpoints using the same opti-mizer as pre-training time but starting with 1/10th the learning rate (i.e., 5e-6) and for 3 epochs overthe finetuning demonstrations. We use this same recipe across all environments and for both REGENTand JAT/Gato. We only use 3 epochs to prevent the overwriting of any capabilities learned duringpre-training.",
  "To prove theorem 5.2, we first state and prove the following lemma": "Let us refer to the family of policies represented by REGENT(a|s, r, c) in Equation (1) for various as(a|s, r, c). The parameters of the transformer are drawn from the space .Lemma B.1. (Total Variation in the Policy Class) The total variation of the policy class (a|s, r, c)in environment j, 1, 2 and for some s Sj,r Rj,c Cj,defined as",
  "CADDITIONAL JAT/GA T O RESULTS": "Effect of number of states/rewards/actions in the context: We plot the normalized return obtainedby REGENT for different context sizes at evaluation time in unseen environments in . We findthat, in general, a larger context size leads to better performance. We also notice that the improvementin performance saturates quickly in many environments. Our choice of 20 states in the context(including the query state) leads to the best performance overall. : Normalized returns obtained by REGENT in the unseen Metaworld and Atari environments with a fixed retrieval buffer against thenumber of states given in-context (including the query state). Please note that the context size is 2n 1 if there are n states in the context.Each value is from a single model training seed but evaluated over 100 rollouts of different seeds in Metaworld and 15 rollouts of different seeds(with psticky = 0.05) in Atari. : Normalized returns obtained by REGENT in the unseen Metaworld and Atari environments with a fixed retrieval buffer against thechoice of ordering of states in the context. Each value is from a single model training seed but evaluated over 100 rollouts of different seeds inMetaworld and 15 rollouts of different seeds (with psticky = 0.05) in Atari. Effect of the ordering of the context: We plot the normalized return obtained by REGENT for differ-ent choices of context ordering at evaluation time in unseen environments in . We evaluatedthree options: \"current\", \"random-permuted\", and \"reverse\". In the \"current\" order, the retrievedtuples of (state, previous reward, action) are placed in order of their closeness to the query state st.In \"random-permuted\", the retrieved tuples are placed in a randomly chosen order in the context.",
  ": Average runtime values for retrieval and forward propagation through REGENT in metaworld for various number of demonstrationsin the retrieval buffer": "Effect of the distance metric: We compared REGENT with cosine distance everywhere (frompretraining to evaluation) against our current version of REGENT with 2 distance everywhere inthe JAT/Gato setting in . REGENT with cosine distance performs similarly to REGENTwith 2 distance in 3 unseen metaworld environments. In the other unseen metaworld environmentsas well as the more challenging unseen atari environments, REGENT with 2 distance significantlyoutperforms REGENT with cosine distance. We speculate that this is because the magnitude ofobservations is important with both image embeddings and proprioceptive states and this is onlytaken into account by the 2 distance (not the cosine distance). : Normalized returns in the unseen Metaworld and Atari environments against the number of demonstration trajectories the agentcan retrieve from. The REGENT (cosine distance) values are an average over 100 rollouts of different seeds in Metaworld and 15 rollouts ofdifferent seeds (with psticky = 0.05) in Atari.",
  "IA3with IA3": "metaworld-bin-picking250.330.31 0.1420.3 0.0250.00.010.00.0 0.0050.00.010.00.01 0.00.0-metaworld-bin-picking500.40.55 0.0310.6 0.0490.00.040.00.01 0.0050.00.010.00.01 0.00.0-metaworld-bin-picking750.340.59 0.0970.69 0.0210.00.080.010.01 0.0050.00.010.010.01 0.00.0-metaworld-bin-picking1000.330.67 0.0510.73 0.0050.00.060.00.01 0.0050.00.020.010.01 0.00.0-metaworld-box-close250.580.73 0.1230.81 0.090.00.00.00.0 0.00.00.090.00.0 0.00.01-metaworld-box-close500.690.82 0.1420.86 0.1020.00.20.00.0 0.00.00.20.00.0 0.00.0-metaworld-box-close750.680.84 0.0260.99 0.0170.00.40.00.0 0.00.00.230.00.0 0.00.0-metaworld-box-close1000.810.93 0.0670.98 0.0050.00.560.00.0 0.00.00.310.00.0 0.00.0-metaworld-door-lock250.580.73 0.0190.8 0.0120.130.330.120.12 0.0120.240.180.310.23 0.0390.0-metaworld-door-lock500.550.81 0.0470.82 0.0140.130.340.150.09 0.0210.240.240.250.29 0.0280.03-metaworld-door-lock750.630.85 0.0280.84 0.0220.130.390.170.14 0.0290.240.310.270.4 0.0260.01-metaworld-door-lock1000.640.85 0.0280.85 0.00.130.480.170.15 0.0290.240.380.280.42 0.0190.03-metaworld-door-unlock250.530.65 0.0540.72 0.0370.010.360.030.22 0.0570.060.310.070.15 0.0360.0-metaworld-door-unlock500.550.81 0.0490.84 0.0310.010.470.030.23 0.080.060.420.070.25 0.0830.01-metaworld-door-unlock750.780.88 0.0080.92 0.0050.010.410.020.2 0.0220.060.440.050.3 0.0740.0-metaworld-door-unlock1000.860.9 0.0170.93 0.0050.010.450.030.16 0.0310.060.50.080.24 0.0940.03-metaworld-hand-insert250.270.62 0.0220.54 0.0220.010.070.070.1 0.0190.010.060.050.29 0.0340.0-metaworld-hand-insert500.540.68 0.0310.69 0.0120.010.130.040.16 0.0310.010.10.060.58 0.0120.0-metaworld-hand-insert750.560.78 0.0280.79 0.0210.010.220.040.16 0.0220.010.10.060.63 0.0170.0-metaworld-hand-insert1000.690.82 0.0420.83 0.00.010.230.020.15 0.0540.010.140.030.61 0.0650.01-atari-pong7 (11599)0.080.08 0.00.17 0.0420.00.00.00.0 0.00.00.00.00.0 0.00.00.0 0.0atari-pong10 (16533)0.180.2 0.00.23 0.00.00.00.00.0 0.00.00.00.00.0 0.0050.00.0 0.0atari-pong13 (21468)0.180.23 0.0090.23 0.0090.00.00.00.0 0.00.00.00.00.0 0.0050.00.04 0.016atari-pong16 (26400)0.180.25 0.0470.3 0.0050.00.00.00.0 0.00.00.00.00.0 0.0050.00.06 0.016atari-alien3 (12759)0.040.08 0.0050.09 0.00.00.00.00.0 0.00.00.00.00.0 0.00.00.0 0.0atari-alien4 (17353)0.050.08 0.0050.09 0.0050.00.00.00.0 0.00.00.00.00.0 0.00.00.0 0.0atari-alien5 (22684)0.050.08 0.00.09 0.0050.00.00.00.0 0.0050.00.00.00.0 0.00.00.0 0.0atari-alien6 (27692)0.050.09 0.0050.1 0.0050.00.00.00.0 0.0050.00.010.00.0 0.00.00.01 0.005atari-mspacman4 (11902)0.680.82 0.0170.82 0.0050.00.010.00.02 0.0050.010.020.020.04 0.0050.00.1 0.026atari-mspacman5 (14520)0.680.81 0.0310.83 0.0160.00.020.010.02 0.0050.010.020.020.04 0.00.00.06 0.042atari-mspacman9 (22490)0.820.85 0.0290.89 0.0090.00.020.020.03 0.0050.010.020.010.04 0.0050.00.17 0.045atari-mspacman10 (25160)0.820.86 0.0310.9 0.00.00.010.010.03 0.00.010.010.020.04 0.00.00.18 0.047",
  "Aggregate Mean0.4730.6020.6330.0190.1650.0290.0630.040.1290.0520.1430.004": ": Values in . For a particular training seed, each agent is evaluated across 100 rollouts of different seeds in Metaworld and 15 rollouts of different seeds (with psticky = 0.05) in Atari. We compute the overall mean andstandard deviation over three training seeds for REGENT, REGENT Finetuned, the PEFT with IA3 baselines, and DRIL. For REGENT, this means that we trained and thoroughly evaluated three REGENT policies. Then, we finetuned eachof these three REGENT policies 32 times, with 8 environments and 4 points per environment, for a total of 96 finetuned policies. We compute the final average over both training seeds and evaluation seeds and the final standard deviationover training seeds. We note that R&P does not have a standard deviation across training seeds as it does not have any training parameters at all.",
  "n=20(0, 11612)6.4 0.335.5": ": Values in before normalization. We trained three REGENT generalist agents for three training seeds. We evaluated each of thethree agents performance, as well as the R&P agent, across 10 levels with 5 rollouts and psticky = 0.2 in each unseen ProcGen environment.We computed the final average over both training seeds and evaluation levels/rollouts and the final standard deviation over training seeds. Thevalues for MTT are the best scores reported in .",
  "In addition to our related work section, we discuss other relevant papers where a retrieval bias hasbeen recognized as a useful component here": "RAEA performs behavior cloning in each new environment with access to a policy memorybank and focuses on improving performance in a training task. In RAEAs appendix, the authorsdemonstrate very initial signs of generalization to new tasks only after training on a few demonstra-tions from the new tasks. REGENT on the other hand pretrains a generalist policy that can generalizewithout any finetuning to completely new environments with different observations, dynamics, andrewards. Re-ViLM trains a retrieval-augmented image captioning model, which while demon-strating the usefulness of a retrieval bias beyond robotics and game-playing tasks, is not applicable toour settings. Expel and RAP build LLM agents for high-level planning on top of frozen LLMs. Althoughoff-the-shelf LLMs are not yet helpful with low-level fine-grained continuous control, these methodscould be key in high-level semantic text-based action spaces. GLAs and RADT are recent in-context reinforcement learning (RL) methods. We brieflydiscussed earlier in-context RL methods in our related work section. GLAs attempt to generalize tonew mujoco embodiments via in-context RL but only show minor improvements over random agents.RADT emphasizes that they are unable to generalize via in-context RL to unseen procgen, metaworld,or DMControl environments. REGENT, on the other hand, is an in-context imitation learning methodthat generalizes to unseen metaworld, atari, and procgen environments via retrieval augmentation(from a retrieval buffer of a few expert demonstrations) and in-context learning. REGENT alsostruggles with generalization to new mujoco embodiments like GLAs."
}