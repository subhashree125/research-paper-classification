{
  "Abstract": "Transformers can efficiently learn in-context from example demonstrations. Most existing theoreticalanalyses studied the in-context learning (ICL) ability of transformers for linear function classes, whereit is typically shown that the minimizer of the pretraining loss implements one gradient descent step onthe least squares objective. However, this simplified linear setting arguably does not demonstrate thestatistical efficiency of ICL, since the pretrained transformer does not outperform directly solving linearregression on the test prompt. In this paper, we study ICL of a nonlinear function class via transformerwith nonlinear MLP layer: given a class of single-index target functions f(x) = (x, ), where theindex features Rd are drawn from a r-dimensional subspace, we show that a nonlinear transformeroptimized by gradient descent (with a pretraining sample complexity that depends on the informationexponent of the link functions ) learns f in-context with a prompt length that only depends on thedimension of the distribution of target functions r; in contrast, any algorithm that directly learns f ontest prompt yields a statistical complexity that scales with the ambient dimension d. Our result highlightsthe adaptivity of the pretrained transformer to low-dimensional structures of the function class, whichenables sample-efficient ICL that outperforms estimators that only have access to the in-context data.",
  "Introduction": "Pretrained transformers [VSP+17] possess the remarkable ability of in-context learning (ICL) [BMR+20],whereby the model constructs a predictor from a prompt sequence consisting of pairs of labeled exampleswithout updating any parameters. A common explanation is that the trained transformer can implementa learning algorithm, such as gradient descent on the in-context examples, in its forward pass [HvMM+19,DSD+22, VONR+23].Such explanation has been empirically studied in synthetic tasks such as linearregression [GTLV22, ASA+22], which has motivated theoretical analyses of the statistical and computationalcomplexity of ICL in learning simple function classes. Many recent theoretical works on ICL focus on learning the function class of linear models using lineartransformers trained via empirical risk minimization. In this setting, it can be shown that minima of thepretraining loss implements one (preconditioned) gradient descent step on the least squares objective com-puted on the test prompt [ZFB23, ACDS23, MHM23]. This implies that the forward pass of the pretrainedtransformer can learn linear targets with n d in-context examples, hence matching the sample complexityof linear regression on the test prompt. Subsequent works also studied how the distribution and diversity ofpretrained tasks affect the ICL solution in similar problem settings [WZC+23, RPCG23, ZWB24, LLZV+24]. The motivation of our work is the observation that the simple setting of learning linear models with lineartransformers does not fully capture the statistical efficiency and adaptivity of ICL. Specifically,",
  "arXiv:2411.02544v1 [cs.LG] 4 Nov 2024": "A linear transformer has limited expressivity and hence only implements simple operations in the forwardpass, such as one gradient step for least squares regression. This limits the class of algorithms that canbe executed in-context, and consequently, the pretrained transformer cannot outperform directly solvinglinear regression on the test prompt. We therefore ask the question:",
  "With the aid of MLP layer, can a pretrained transformer learn a nonlinear function class in-context,and outperform baseline algorithms that only have access to the test prompt?": "A key feature of ICL is the adaptivity to structure of the learning problem.For example, [GTLV22]empirically showed that transformers can match the performance of either ridge regression or LASSO,depending on parameter sparsity of the target class; [RPCG23] observed that transformers transitions froma weighted-sum estimator to ridge regression as the number of pretraining tasks increases. Such adaptivityenables the pretrained model to outperform algorithms that only have access to the test prompt, whichcannot take into account the prior distribution of target functions. Hence a natural question to ask is",
  "xt1, xt2, . . . , xtN, xt i.i.d. N(0, Id),yti = t(xti, t) + ti,(1.1)": "where t : R R is the unknown link function, and t Rd is the index feature vector which is drawnfrom some fixed r-dimensional subspace for some r d. The task is to learn this input-output relation byreading the context (x1, y1, . . . , xN, yN) and predict the output corresponding to the query x (See for details). This problem setting is based on the following considerations. Nonlinearity of function class. Due to the nonlinear link function, single-index targets cannot belearned by linear transformers. For this class of functions, the statistical efficiency of simple algorithmshas been extensively studied: given a link function with degree P and information exponent Q (definedas the index of the smallest non-zero coefficient in the Hermite expansion), we know that kernel methodsrequire at least n dP samples [GMMM21, DWY21], whereas two-layer neural network trained by gradientdescent can achieve better sample complexity n d(Q) [BAGJ21, BBSS22]. Hence these algorithms, ifdirectly applied to the test prompt, require a context length polynomial in the ambient dimensionalityd, which arguably deviates from practical settings of ICL where the pretrained transformer learns from afew in-context examples which may come from high dimensional data space. These algorithms serve as abaseline for comparing the statistical efficiency of ICL. Two types of low-dimensional structures. Prior works on gradient-based feature learning highlightsthe adaptivity of neural network to low-dimensional functions, i.e., single-index model (xi, ) thatdepends on one direction (index features) in Rd [AAM22, BES+22, BMZ23]. In such setting, the com-plexity of gradient descent is dominated by the search for direction Rd, the difficulty of which can becharacterized by various computational lower bounds [DLS22, AAM23, DPVLB24].Importantly, our pretraining problem setup introduces another notion of low dimensionality: the distri-bution of target functions is low-dimensional, since the index features for each task t are drawn froma rank-r subspace. This low-dimensionality of function class cannot be exploited by any algorithms thatdirectly estimate the target function from test prompt, but as we will see, transformers can adapt to thisadditional structure via gradient-based pretraining, which reduces the search problem (for the index fea-tures ) to r-dimensional in the in-context phase. Hence when r d, we expect pretrained transformersto outperform baseline algorithms on the in-context examples (kernel methods, neural network, etc.). in-context sample size N * prediction risk d = 16d = 32kerneltwo-layer NNpretrained TF : In-context generalization error of ker-nel ridge regression, neural network + gradientdescent, and pretrained transformer. The targetfunction is a polynomial single-index model. Wefix r = 8 and vary d = 16, 32. Empirical observations.We pretrain a GPT-2 model[RWC+19] (with the same configurations as the in-contextlinear regression setting in [GTLV22]) to learn the Gaus-sian single-index task (1.1) with degree-3 link function, andcompare its in-context sample complexity against baselinealgorithms (see for details). In we ob-serve that the pretrained transformer achieves low predic-tion risk using fewer in-context examples than two base-line algorithms: kernel ridge regression, and neural networktrained by gradient descent. Moreover, we observe that un-like the baseline methods,",
  "Main Result: Learning Single-index Models In-Context": "We characterize the sample complexity of learning (1.1) in-context, using a nonlinear transformer optimizedby gradient descent. Each single-index task is specified by an unknown index feature vector Rd drawnfrom some r-dimensional subspace, and a link function with degree P and information exponent Q P;we allow the degree and information exponent to vary across tasks, to model the scenario where the difficultof pretraining tasks may differ. We show that pretraining of the nonlinear MLP layer can extract the low-dimensional structure of the function class, and the attention layer efficiently approximates the nonlinear linkfunction. Our main theorem upper bounds the in-context generalization error of the pretrained transformer. Theorem (Informal). Let f : (x1, y1, . . . , xN, yN, x) y be a transformer with nonlinear MLP layerpretrained with gradient descent (Algorithm 1) on the single-index regression task (1.1). With probability atleast 0.99, the model f achieves in-context prediction risk E|f(x) f(x)| = od(1), where is the noiselevel, if the number of pretraining tasks T, the number of training examples N, the test prompt length N ,and the network width m satisfy (we omit polylogarithmic factors for conciseness)",
  "To the best of our knowledge, this is the first end-to-end optimization and statistical guarantee forin-context regression of this nonlinear function class. We make the following remarks": "The required sample size for pretraining T, N scale with the ambient dimensionality d. In particular,the number of pretraining tasks T is parallel to the complexity of learning a single-index model withinformation exponent Q using a two-layer neural network. On the other hand, the sample complexity forthe in-context phase only depends on the dimensionality of the function class r d. Note that any estimator that only has access to the in-context examples requires n d samples to learn thesingle-index model, as suggested by the information theoretic lower bound (e.g., see [MM18, BKM+19]).Therefore, when r d, we see a separation between pretrained transformer and algorithms that directlylearn on test prompt, such as kernel regression and neural network + gradient descent. This highlightsthe adaptivity (via pretraining) of transformers to low-dimensional structures of the target function class. Our analysis of pretraining reveals the following mechanism analogous to [GHM+23]: the nonlinear MLPlayer extract useful features and adapt to the low-dimensionality of the function class, whereas the attentionlayer performs in-context function approximation on top of the learned features.",
  "Related Works": "Theory of in-context learning.Many recent works studied the in-context learning ability of transformerstrained by gradient descent. [ZFB23, ACDS23, MHM23, WZC+23, ZWB24] studied the training of lineartransformer models to learn linear target functions in-context by implementing one gradient descent step inthe forward pass. Similar optimization results have been established for looped linear transformers [GSR+24],transformers with SoftMax attention [HCL23, NDL24, CSWY24a, CSWY24b] or nonlinear MLP layer [KS24,LWL+24]. Our problem setting resembles [KS24], where a nonlinear MLP block is used to extract features,followed by a linear attention layer; the main difference is that we establish end-to-end guarantees on theoptimization and sample complexity for learning a concrete nonlinear function class, whereas [KS24] focusedon convergence of optimization. [CCS23] showed that transformers can learn nonlinear functions in-contextvia a functional gradient mechanism, but no statistical and optimization guarantees were given. If we donot take gradient-based optimization into account, the function class that can be implemented in-contextby transformers has been characterized in many prior works [BCW+23, GHM+23, ZZYW23, JLLVR24,SGS+24, KNS24]. These results typically aim to encode specific algorithms (LASSO, gradient descent, etc.)in the forward pass or directly analyze the Bayes-optimal estimator. Gradient-based learning of low-dimensional functions.The complexity of learning low-dimensionalfunctions with neural network has been extensively studied in the feature learning theory literature. Typicaltarget functions include single-index models [BAGJ21, BES+22, BBSS22, MHPG+23, DNGL23, BES+23]and multi-index models [DLS22, AAM22, AAM23, BBPV23, DKL+23]. While a shallow neural network canefficiently approximate such low-dimensional functions, the efficiency of gradient-based training is governedby properties of the nonlinearity . In the single-index setting, prior works established a sufficient samplesize n d(K), where K N is the information exponent for algorithms utilizing correlational information[BAGJ21, BBSS22, DNGL23, MHWSE23], or the generative exponent for algorithms that employ suitablelabel transformations [DPVLB24, LOSW24, ADK+24, JMS24]. Moreover, n d samples are informationtheoretically necessary without additional structural assumptions; this entails that estimators that onlyaccess the test prompt inevitably pay a sample size that scales with the ambient dimensionality. As we willsee, pretrained transformers can avoid this curse of dimensionality by exploiting the low-dimensionalityof the task distribution. Low-dimensional structure of the function class similar to our setting has beenassumed to study the efficiency of transfer learning [DLS22] and multi-task learning [CHS+23].",
  "Problem Setting": "Notations. denotes the 2 norm for vectors and the 2 2 operator norm for matrices. For a vectorw, we use wa:b for a b to denote the vector [wa, wa+1, . . . , wb]. 1N denotes the all-one vector of size N.The indicator function of A is denoted by IA. Let N be a nonnegative integer; then [N] denotes the set",
  "{n Z | 1 n N}. The i-th Hermite polynomial is defined as Hei(z) = (1)iez22didzi ez2": "2. For a set S,Unif(S) denotes the uniform distribution over S. We denote the unit sphere {x Rd | x = 1} by Sd1.O(), () represent O(), () notations where polylogarithmic terms are hidden. We write a b when thereexists a constant c such that a cb, and a b if both a b and b a holds.",
  "In-context learning": "We first introduce the basic setting of ICL [BMR+20] of simple function classes as investigated in [GTLV22,ASA+22]. In each task, the learner is given a sequence of inputs and outputs (x1, y1, . . . , xN, yN, x) referredto as prompt, where xi, x Rd and yi R. The labeled examples X =x1 xN RdN, y =y1 yN RN are called context, and x is the query. Given input distribution x1, . . . , xN, xi.i.d. Dx,the output yi is expressed asyi = f(xi) + i,i [N], where f is the true function describing the input-output relation and ii.i.d.D is i.i.d. label noise.Note that f also varies across tasks we assume f is drawn i.i.d. from some true distribution Df.In the pretraining phase, we optimize the model parameters given training data from T distinct tasks{(xt1, yt1, . . . , xtM, ytM, xt, yt)}Tt=1, which is composed of prompts {(xt1, yt1, . . . , xtM, ytM, xt)}Tt=1 and responses",
  "{yt}Tt=1 for queries {xt}Tt=1, where yt = f t(xt) + t and t i.i.d. D": "We say a model learns these functional relations in-context, when the model can predict the output f(x)corresponding to query x by solely examining the context (X, y), without updating model parameters foreach task. Given the pretrained model f(X, y, x; ) with parameter which predicts the label of query xfrom context (X, y), we define the expected ICL risk as",
  "RN (f) := E[|f(X1:N , y1:N , x; ) y|],(2.1)": "where y = f(x) + and the expectation is taken over the in-context data: x1, . . . , xN , x Dx, f Df, 1, . . . , N , D.Note that we take the expectation with respect to contexts (X1:N , y1:N ) RdN RN of length N , in order to examine the behavior of ICL at a specific context length.",
  "Gaussian single-index models": "We consider the situation where the true input-output relation is expressed by single-index models, i.e.,functions that only depend on the direction of the index vector in the input space. An efficient learningalgorithm should adapt to this feature and identify the relevant subspace from high-dimensional observations;hence this problem setting has been extensively studied in the deep learning theory literature [BL19, BES+22,BBSS22, MHPG+23, MZD+23] to demonstrate the adaptivity of gradient-based feature learning.",
  "Remark 1. We make the following remarks on the assumption of single-index function class": "For each task, the target is a single-index model with different index features drawn from some rank-rsubspace, and different link function with degree at most P and information exponent (defined as the indexof the lowest degree non-zero coefficient in the Hermite expansion of the link function, i.e, min{i | ci = 0};see [BAGJ21, DH18]) at least Q. This heterogeneity reflects the situation where the difficulty of learningthe input-output relation varies across tasks. Note that we allow for different distributions of the Hermitecoefficients {ci}: for example, we may set (cQ, . . . , cP ) Unif(cQ, . . . cP )| Pi=Qc2ii! = 1(manifold of",
  "coefficients satisfying Ex[f(x)] = 1), Unif({0, 1}P Q+1 \\ (0, . . . , 0)), or Unif({(1, . . . , 0), . . . , (0, . . . , 1)})": "The condition Q 2 (i.e., the Hermite expansion of does not contain constant and linear terms) ensuresthat the gradient update detects the entire r-dimensional subspace instead of the trivial rank-1 component.For generic polynomial , this assumption can be satisfied by a simple preprocessing step that subtractsthe low-degree components, as done in [DLS22].",
  ",(2.3)": "where is the temperature, and W K, W Q Rdkde, W V Rdvde and W P Rdedv are the key, query,value, and projection matrix, respectively. Following prior theoretical works [ZFB23, ACDS23, MHM23,KNS24], we remove the SoftMax and instead analyze the linear attention with = N; it has been arguedthat such simplification can reproduce phenomena in practical transformer training [ACS+23]. We furthersimplify the original self-attention module (2.3) by merging W P W V as W P V Rdede and (W K)W Q asW KQ Rdede, and consider the following parameterization also introduced in [ZFB23, ACDS23, WZC+23],",
  "N, and we take the right-bottom entry as the prediction ofy corresponding to query x": "Prior analyses of linear transformers [ZFB23, ACDS23, MHM23] defined the embedding matrix E simplyas the input-output pairs; however, the combination of linear embedding and liner attention is not sufficientto learn nonlinear single-index models. Instead, we set de = m + 1, dN = N + 1 for m N and construct Eusing an MLP layer:",
  "R(m+1)(N+1),(2.5)": "where w1, . . . , wm Rd and b1, . . . , bm R are trainable parameters and : R R is a nonlinearactivation function; we use (z) = ReLU(z) = max{z, 0}. This is to say, we define the embedding as thehidden representation (wx + b) of a width-m two-layer neural network. For concise notation we writeW = (w1, . . . , wm), b = (b1, . . . , bm).",
  "Remark 2. We make the following remarks on the considered architecture": "MLP embedding before attention has been adopted in recent works [GHM+23, KS24, KNS24]. This settingcan be interpreted as an idealized version of the mechanism that lower layers of the transformer constructuseful representation, on top of which attention layers implement the in-context learning algorithm. Our architecture is also inspired by recent theoretical analyses of gradient-based feature learning, where itis shown that gradient descent on the MLP layer yields adaptivity to features of the target function andhence improved statistical efficiency [AAM22, DLS22, BES+22].",
  "Pretraining: Empirical Risk Minimization via Gradient Descent": "We pretrain the transformer (2.6) by the gradient-based learning algorithm specified in Algorithm 1, whichis inspired by the layer-wise training procedure studied in the neural network theory literature [AAM22,DLS22, BES+22]. In particular, we update the trainable parameters in a sequential manner. In Stage I we optimize the parameters of the MLP (embedding) layer, which is a non-convex problem dueto the nonlinear activation function. To circumvent the nonlinear training dynamics, we follow the recipein recent theoretical analyses of gradient-based feature learning [DLS22, BES+22, BEG+22]; specifically,we zoom into the early phase of optimization by taking one gradient step on on the regularized empiricalrisk. As we will see, the first gradient step already provides a reliable estimate of the target subspace S,which enables the subsequent attention layer to implement a sample-efficient in-context learning algorithm. In Stage II we train the attention layer, which is a convex problem and the global minimizer can beefficiently found. We show that the optimized attention matrix performs regression on the polynomialbasis (defined by the MLP embedding), which can be seen as an in-context counterpart to the second-layertraining (to learn the polynomial link) in [DLS22, AAM23, OSSW24].",
  "Remark 3. We make the following remarks": "The sample complexity highlights different roles of the two sources of low dimensionality in our setting.The low dimensionality of single-index f entails that the pretraining cost scales as N d(Q), which isconsistent with prior analyses on gradient-based feature learning [BAGJ21, DNGL23]. On the other hand,the low dimensionality of function class (i.e., lies in r-dimensional subspace) leads to an in-contextsample complexity that scales as N r(P ), which, roughly speaking, is the rate achieved by polynomialregression or kernel models on r-dimensional data. The multiplicative scaling between N1 and T1 in the sample complexity suggests that one can tradeoffbetween the two quantities, that is, pretraining on more diverse tasks (larger T1) can reduce the requiredpretraining context length N1. Similar low-dimensional function class has been considered in the setting of transfer or multi-task learningwith two-layer neural networks [DLS22, CHS+23], where the first-layer weights identify the span of alltarget functions, and the second-layer approximates the nonlinearity. However, the crucial difference isthat we do not update the network parameters based on the in-context examples; instead, the single-indexlearning is implemented by the forward pass of the transformer. Comparison against baseline methods.Below we summarize the statistical complexity of commonly-used estimators that only have access to N in-context examples, but not the pretraining data. Note thatthe in-context sample complexity all depends on the ambient dimensionality d r.",
  "Kernel models. Recall that our target function is a degree-P polynomial, and hence kernel ridge regres-sion requires N dP in-context examples [GMMM21, DWY21]": "CSQ learners. The correlational statistical query (CSQ) lower bound suggests that an algorithm makinguse of correlational information requires N d(Q) samples to learn a single-index model with informationexponent Q [DLS22, AAM22]. This sample complexity can be achieved by online SGD training of shallowneural network [BAGJ21, DNGL23]. Information theoretic limit. Since the single-index model (1.1) contains d unknown parameters, we caninfer an information theoretic lower bound of N d samples to estimate this function. This complexitycan be achieved (up to polylog factors) by tailored SQ algorithms [CM20, DPVLB24] or modified gradient-base training of neural network [LOSW24, ADK+24, JMS24].",
  "Proof Sketch of Main Theorem": "We provide a sketch of derivation for Theorem 1. The essential mechanism is outlined as follows: aftertraining the MLP embedding via one gradient descent step, the MLP parameters {w(1)j } align with thecommon subspace of the target functions S.Subsequently, the (linear) attention module estimates theinput-output relation f (which varies across tasks) on this r-dimensional subspace. We explain these twoingredients in the ensuing sections.",
  "for all t {T1 + 1, . . . , T2}. Moreover, we have F = Or2P /m": "We provide an intuitive explanation of this construction. Recall that the target function can be writtenin its Hermite expansion f(x) = Pi=Qcii! Hei(x, ). We build an orthonormal basis for f as follows. Let{1, . . . , r} be an orthonormal basis of S. Then, any f can be expressed as a linear combination of functionsin H = {rj=1 Hepj(j, ) | Q p1 + +pr P, p1 0, . . . , pr 0}, where ExN (0,Id)[h(x)h(x)] = Ih=hholds for h, h H. We write H = {h1, . . . , hBP } with BP = |H| = (rP ), and observe that a two-layerneural network can be constructed to approximate each hn. Specifically, there exist vectors a1, . . . , aBP Rm",
  "(a) BPn=11N2N2i=1 hn(xi)yihn(x)(b) BPn=1 E[hn(x)f(x)]hn(x) = f(x).(3.1)": "Roughly speaking, the self-attention architecture computes the correlation between the target function andbasis element hi to estimate the corresponding coefficient in this basis decomposition. We evaluate (a) theapproximation error of two-layer neural network in Appendix C.1, and (b) the discrepancy between theempirical and true correlations in Appendix C.2. Note that the approximation errors and the norm of at this stage scale only with r up to polylogarithmicterms; this is because W (1) already identifies the low-dimensional target subspace S. Consequently, the in-context sample size N only scales with the target dimensionality r d.",
  "Generalization Error Analysis": "Finally, we transfer the learning guarantee from the constructed to the (regularized) empirical risk mini-mization solution . By the equivalence between optimization with L2 regularization and norm-constrainedoptimization, there exists 2 such that F F and the empirical ICL loss by is no larger thanthat of . Hence, we can bound the generalization error by using a standard Rademacher complexitybound for norm-constrained transformers provided in Appendix D.1. One caveat here is that the contextlength N at test time may differ from training time; hence we establish a context length-free generalizationbound, which is discussed in Appendix D.2. in-context sample size N * 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 prediction risk d=16,r=8d=32,r=8d=64,r=8",
  "Experimental Setting": "We pretrain a GPT-2 model [RWC+19] to learn the Gaussian single-index function class (1.1).Specif-ically, we consider the 12-layer architecture (with 22.3M parameters) used in [GTLV22] for in-contextlinear regression.The pretraining data is generated from random single-index models:for each taskt, the context {(xti, yti)}Ni=1 is generated as xtii.i.d.N(0, Id) and yti= Pi=Qctii! Hei(xti, t), where",
  "Empirical Findings": "Ambient dimension-free sample complexity.In we examine how the in-context sample com-plexity of the GPT-2 model depends on the ambient dimensionality d and the function class dimensionalityr. For each problem setting the model is pretrained for 100, 000 steps using the data of degree P = 4 andinformation exponent Q = 2 (see Appendix F for details). In (a) we observe that for fixed r = 8,varying the ambient dimensionality d = 16, 32, 64 leads to negligible change in the model performance for thein-context phase. In contrast, (b) illustrates that for fixed d, the required sample size N scales withthe dimensionality of the function class r = 2, 4, 8. This confirms our theoretical finding that transformerscan adapt to low-dimensional structure of the distribution of target functions via gradient-based pretraining. Superiority over baseline algorithms.In we compare the in-context sample complexity of theGPT-2 model pretrained by data of Q = 3 and P = 2 against two baseline algorithms that directly learn f onthe test prompt: (i) kernel ridge regression with the Gaussian RBF kernel k(x, x) = expx x2/2,and (ii) two-layer neural network with ReLU activation fNN(x) = 1 mmi=1 ai(x, wi) trained by the Adamoptimizer [KB15]. We observe that for r = 8, d = 16, 32, the pretrained transformer outperforms both KRRand two-layer NN; moreover, the performance of these two baseline algorithms deteriorates significantly asthe ambient dimensionality d becomes larger.",
  "Conclusion and Future Direction": "We study the complexity of in-context learning for the Gaussian single-index models using a pretrainedtransformer with nonlinear MLP layer. We provide an end-to-end analysis of gradient-based pretrainingand establish a generalization error bound that takes into account the number of pretraining tasks, thenumber of pretraining and in-context examples, and the network width. Our analysis suggests that whenthe distribution of target functions exhibits low-dimensional structure, transformers can identify and adapt tosuch structure during pretraining, whereas any algorithm that only has access to the test prompt necessarilyrequires a larger sample complexity.",
  "We outline a few limitations and possible future directions": "The in-context sample complexity we derived r(P ) corresponds to that of polynomial regression or kernelmethods in r-dimensional space.This rate is natural as discussed in .2.2, where the linearself-attention module extracts the coefficients with respect to fixed basis functions (of size r(P )). Aninteresting question is whether transformers can implement a more efficient in-context algorithm thatmatches the complexity of gradient-based feature learning in r dimensions. This can be achieved if thepretrained model learns features in-context. Our pretraining complexity is based on one GD step analysis similar to [DLS22, BES+22] which makes useof the correlational information; hence the information exponent of the link functions plays an importantrole. We conjecture that the sample complexity can be improved if we modify the pretraining procedureor training objective, as done in [DTA+24, LOSW24, ADK+24, JMS24].",
  "KO was partially supported by JST, ACT-X Grant Number JPMJAX23C4. TS was partially supported byJSPS KAKENHI (24K02905) and JST CREST (JPMJCR2015). This research is unrelated to DWs workat xAI": "[AAM22]Emmanuel Abbe, Enric Boix Adsera, and Theodor Misiakiewicz. The merged-staircase prop-erty: a necessary and nearly sufficient condition for sgd learning of sparse functions on two-layerneural networks. In Conference on Learning Theory, pages 47824887. PMLR, 2022. [AAM23]Emmanuel Abbe, Enric Boix Adsera, and Theodor Misiakiewicz.SGD learning on neuralnetworks: leap complexity and saddle-to-saddle dynamics. In Conference on Learning Theory,pages 25522623. PMLR, 2023. [ACDS23]Kwangjun Ahn, Xiang Cheng, Hadi Daneshmand, and Suvrit Sra. Transformers learn to imple-ment preconditioned gradient descent for in-context learning. Advances in Neural InformationProcessing Systems, 36, 2023. [ACS+23]Kwangjun Ahn, Xiang Cheng, Minhak Song, Chulhee Yun, Ali Jadbabaie, and Suvrit Sra.Linear attention is (maybe) all you need (to understand transformer optimization).arXivpreprint arXiv:2310.01082, 2023. [ADK+24]Luca Arnaboldi, Yatin Dandi, Florent Krzakala, Luca Pesce, and Ludovic Stephan. Repetitaiuvant: Data repetition allows sgd to learn high-dimensional multi-index functions.arXivpreprint arXiv:2405.15459, 2024.",
  "[BBSS22]Alberto Bietti, Joan Bruna, Clayton Sanford, and Min Jae Song. Learning single-index modelswith shallow neural networks. Advances in Neural Information Processing Systems, 35, 2022": "[BCW+23]Yu Bai, Fan Chen, Huan Wang, Caiming Xiong, and Song Mei.Transformers as statisti-cians: Provable in-context learning with in-context algorithm selection. Advances in NeuralInformation Processing Systems, 36, 2023. [BEG+22]Boaz Barak, Benjamin Edelman, Surbhi Goel, Sham Kakade, Eran Malach, and Cyril Zhang.Hidden progress in deep learning: Sgd learns parities near the computational limit. Advancesin Neural Information Processing Systems, 35, 2022. [BES+22]Jimmy Ba, Murat A Erdogdu, Taiji Suzuki, Zhichao Wang, Denny Wu, and Greg Yang. High-dimensional asymptotics of feature learning: How one gradient step improves the representa-tion. Advances in Neural Information Processing Systems, 35, 2022. [BES+23]Jimmy Ba, Murat A Erdogdu, Taiji Suzuki, Zhichao Wang, and Denny Wu.Learning inthe presence of low-dimensional structure: A spiked random matrix perspective. Advances inNeural Information Processing Systems, 36, 2023. [BKM+19]Jean Barbier, Florent Krzakala, Nicolas Macris, Leo Miolane, and Lenka Zdeborova. Optimalerrors and phase transitions in high-dimensional generalized linear models. Proceedings of theNational Academy of Sciences, 116(12):54515460, 2019.",
  "[DPVLB24]Alex Damian, Loucas Pillaud-Vivien, Jason D Lee, and Joan Bruna.The computationalcomplexity of learning gaussian single-index models. arXiv preprint arXiv:2403.05529, 2024": "[DSD+22]Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Shuming Ma, Zhifang Sui, and Furu Wei. Why cangpt learn in-context? language models implicitly perform gradient descent as meta-optimizers.arXiv preprint arXiv:2212.10559, 2022. [DTA+24]Yatin Dandi, Emanuele Troiani, Luca Arnaboldi, Luca Pesce, Lenka Zdeborova, and FlorentKrzakala. The benefits of reusing batches for gradient descent in two-layer networks: Breakingthe curse of information and leap exponents. arXiv preprint arXiv:2402.03220, 2024. [DWY21]Konstantin Donhauser, Mingqi Wu, and Fanny Yang. How rotational invariance of commonkernels prevents generalization in high dimensions. In International Conference on MachineLearning, pages 28042814. PMLR, 2021. [GHM+23]Tianyu Guo, Wei Hu, Song Mei, Huan Wang, Caiming Xiong, Silvio Savarese, and Yu Bai.How do transformers learn in-context beyond simple functions? a case study on learning withrepresentations. arXiv preprint arXiv:2310.10616, 2023.",
  "[LOSW24]Jason D. Lee, Kazusato Oko, Taiji Suzuki, and Denny Wu.Neural network learns low-dimensional polynomials with sgd near the information-theoretic limit.arXiv preprintarXiv:2406.01581, 2024": "[LWL+24]Hongkang Li, Meng Wang, Songtao Lu, Xiaodong Cui, and Pin-Yu Chen. How do nonlineartransformers learn and generalize in in-context learning?In International Conference onMachine Learning, 2024. [Mau16]Andreas Maurer. A vector-contraction inequality for rademacher complexities. In AlgorithmicLearning Theory: 27th International Conference, ALT 2016, Bari, Italy, October 19-21, 2016,Proceedings 27, pages 317. Springer, 2016. [MHM23]Arvind Mahankali, Tatsunori B Hashimoto, and Tengyu Ma. One step of gradient descent isprovably the optimal in-context learner with one layer of linear self-attention. arXiv preprintarXiv:2307.03576, 2023. [MHPG+23] Alireza Mousavi-Hosseini, Sejun Park, Manuela Girotti, Ioannis Mitliagkas, and Murat A Er-dogdu. Neural networks efficiently learn low-dimensional representations with SGD. In Inter-national Conference on Learning Representations, 2023.",
  "[NDL24]Eshaan Nichani, Alex Damian, and Jason D Lee. How transformers learn causal structure withgradient descent. arXiv preprint arXiv:2402.14735, 2024": "[OSSW24]Kazusato Oko, Yujin Song, Taiji Suzuki, and Denny Wu. Learning sum of diverse features:computational hardness and efficient gradient-based training for ridge combinations. arXivpreprint arXiv:2406.11828, 2024. [RPCG23]Allan Raventos, Mansheej Paul, Feng Chen, and Surya Ganguli. Pretraining task diversityand the emergence of non-bayesian in-context learning for regression.Advances in NeuralInformation Processing Systems, 36, 2023.",
  "[Ver18]Roman Vershynin. High-Dimensional Probability: An Introduction with Applications in DataScience. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge UniversityPress, 2018": "[VONR+23] Johannes Von Oswald, Eyvind Niklasson, Ettore Randazzo, Joao Sacramento, Alexander Mord-vintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradientdescent. In International Conference on Machine Learning, pages 3515135174. PMLR, 2023. [VSP+17]Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,L ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg,S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances inNeural Information Processing Systems, volume 30. Curran Associates, Inc., 2017.",
  "[ZFB23]Ruiqi Zhang, Spencer Frei, and Peter L Bartlett. Trained transformers learn linear modelsin-context. arXiv preprint arXiv:2306.09927, 2023": "[ZWB24]Ruiqi Zhang, Jingfeng Wu, and Peter L Bartlett.In-context learning of a linear trans-former block: Benefits of the mlp component and one-step gd initialization. arXiv preprintarXiv:2402.14951, 2024. [ZZYW23]Yufeng Zhang, Fengzhuo Zhang, Zhuoran Yang, and Zhaoran Wang. What and how does in-context learning learn? bayesian model averaging, parameterization, and generalization. arXivpreprint arXiv:2305.19420, 2023.",
  "APreliminaries": "We consider the high-dimensional setting, i.e., our result holds for all d D where D is a constant whichdoes not depend on d and r. Throughout the proofs, we take S = {(x1, . . . , xr, 0, . . . , 0) | x1, . . . , xr R}and Unif(S(S)) = Unif({(1, . . . , r, 0, . . . , 0) | 21 + + 2r = 1}). Note that this assumption iswithout loss of generality by the rotational invariance of the Gaussian distribution.",
  "A.2Tensor Notations": "In this paper, a k-tensor is a multidimensional array which has k indices: for example, matrices are 2-tensors.Let A be a k-tensor. Ai1,...,ik denotes (i1, . . . , ik)-th entry of A. Let A be a k-tensor and B be a l-tensorwhere k l. A(B) denotes a k l tensor whose (i1, . . . , ikl)-th entry is",
  "d log d holds with probability at least 1 2 exp(d/32) 2rdC (then with high probability)": "Proof. Let z2 2(d) be independent from w.We observe that the distribution of z2w1:r2 is the2(r)-distribution.First, from Lemma 10, with probability at least 1 2 exp(d/32), z2 d/2 holds.Moreover, we can say that (z)2 2(r) satisfies (z)2 2Cr log d with high probability for sufficiently largeconstant: let us decompose (z)2 = v21 + + v2r where vi N(0, 1); from Lemma 4, v2i 2C log d holdswith probability at least 1 2dC, and thus (z)2 2Cr log d holds with probability at least 1 2rdC.Taking the union bound yields the result.",
  "i!j!(i + j 1)!!Ezr[zi+j]w1:ri+jw1:r": "By Minkowskis inequality E[x + y4]1/4 E[x4]1/4 + E[y4]1/4, it suffices to show that (B.7) holds foreach term. Note that E[ci+1cj] R2c and E[cicj] R2c from Assumption 1, |ai(b)| CH from Lemma 20,and Ew[w1:r4k] = O((r/d)2k) from Lemma 12. Moreover, it is known that Ezr[z2l] = (rl). Puttingthese things together yields the assertion.",
  "We need to obtain a bound uniformly over b . We first introduce the following definition": "Definition 28. Fix any w Sd1 and X = T1t=1{xt1, . . . , xtN1, xt} Rd. Then, define a finite disjointpartition B(w, X) = {B1, . . . , BN(w,X)} of , i.e., iBi = and Bi Bj = (i = j) as follows: band b belong to the same Bi if and only if sign(w, x + b) = sign(w, x + b) for all x X.",
  "(B.12) follows from this remark and Corollary 30": "For(B.13),withhighprobabilityoverthedistributionof {(Xt, yt, xt, yt)}T1t=1,Pw{(B.11)|{(Xt, yt, xt, yt)}T1t=1} 1 d C is satisfied for some C. Also, from Lemma 10 and Corollary 17, with highprobability over the distribution of {(Xt, yt, xt, yt)}T1t=1, we have xti, xt = O( d) and |yt| = O(1). Fromthe definition of gT1,N1(w, b, {(Xt, yt, xt, yt)}T1t=1) (B.2) and expansion of g(w, b) (B.6), supw g(w, b) gT1,N1(w, b, {(Xt, yt, xt, yt)}T1t=1) supw g(w, b) + supw gT1,N1(w, b, {(Xt, yt, xt, yt)}T1t=1) is upperbounded by O(d). Therefore, with high probability",
  "C.1.1Alignment to S and Efficient Approximation": "From now on, we suppose that gT1,N1(w, b) satisfies the conditions in Corollary 32 (to simplify the nota-tion, we drop the dependency on {(Xt, yt, xt, yt)}T1t=1). An important previous result [DLS22] is that, ifgT1,N1(w, b) aligns with the r-dimensional subspace S, then it can be used to approximate polynomials onS efficiently.",
  "N2": "holds for all t {T1 + 1, . . . , T2}, where W (1) is obtained by line 4 of Algorithm 1 and b is re-initializedin Line 5 of Algorithm 1, where the conditions on 1, 1, m, , N1, T1 are identical to that in Theorem 1.Moreover, F = Or2P /mis satisfied.",
  "F.1Detailed Experimental Settings": "For the experiments in , test loss was averaged over 128 independent tasks, with 2048 and 128independent queries generated for each task in the experiment of and 2, respectively. During thevalidation of the experiment of , the coefficients {ci} in the single-index model were fixed to be(c2, c3) = (",
  "MIT License": "E R256(2N+2) through the read-in layer. The transformed embedding E was then passed through the12-layer GPT-2 backbone with 8 heads, with dropout disabled. Finally, the output was converted into a2N + 2-dimensional vector through the read-out layer, where the (2k 1)-th entry (k [N + 1]) correspondsto the prediction yk(xt1, yt1 . . . , xtk1, ytk1, xtk) for yk, that is, the answer for the query xtk reading the contextof length k 1. We used the Adam optimizer [KB15] with a learning rate of 0.0001. The training loss ateach step was set as1BBt=1N+1k=1ytk yk(xt1, yt1 . . . , xtk1, ytk1, xtk)2 where B = 8 is the minibatch size.",
  ": In-context generalization error of ker-nel ridge regression, neural network + one-stepgradient descent, and Algorithm 1 on our Trans-former (2.6)": "In addition to the GPT-2 experiments, we also conductedcomparison between our Algorithm 1 on our simplified ar-chitecture and baseline algorithms.Data is generated asyti = He2(xti, t), where ti.i.d.Unif{ Rd | =[1:r, 0], = 1} with d = 32 and r = 2.We pre-trained our transformer architecture (2.6) with m = 8, 000,using Algorithm 1 with T1 = 105, N1 = 104, 1 = 105 (re-call that 1 m32 rd2Q 1 2 (log d)C should be large) andT2 = 3, 000, N2 = 512, 2 = 0.1. We used mini-batch SGDto find an approximate optimizer of the ridge regression.For validation, we altered the context length from 24.5 to 29 and compared its test error with one-step GD (training thefirst layer via one-step gradient descent, and then conduct-ing ridge regression on the second layer) on a two-layer NNof width 8, 000 and kernel ridge regression with RBF kernelworking directly on the test prompt. In we see thatour simplified transformer also outperforms neural networkand kernel method, especially in short context regime."
}