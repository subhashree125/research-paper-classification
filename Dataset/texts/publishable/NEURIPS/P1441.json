{
  "Abstract": "Our aim for the ML Contest for Chip Design with HLS 2024 was to predict thevalidity, running latency in the form of cycle counts, utilization rate of BRAM (util-BRAM), utilization rate of lookup tables (uti-LUT), utilization rate of flip flops(util-FF), and the utilization rate of digital signal processors (util-DSP). We usedChain-of-thought techniques with large language models to perform classificationand regression tasks. Our prediction is that with larger models reasoning was muchimproved. We release our prompts and propose a HLS benchmarking task forLLMs.",
  "Introduction": "High-level synthesis (HLS) has revolutionised the landscape of hardware design by raising thelevel of abstraction, making it feasible to develop domain-specific accelerators (DSAs), such asfield-programmable gate arrays (FPGAs), using high-level programming languages like C/C++instead of traditional hardware description languages (HDLs). The figure to the right illustrates atypical HLS design, where the functionality is described using C++ code, and performance-relatedcompiler directives, known as pragmas, dictate the microarchitectural transformations within theHLS framework. These pragmas influence key performance metrics, including latency and resourceutilization rates. Despite these advancements, the optimisation space for microarchitectures expands exponentiallywith the inclusion of pragmas, creating an overwhelming design space. Evaluating each candi-date architecture through HLS tools requires substantial timeranging from several minutes tohoursmaking the optimization process both labor-intensive and time-consuming. Machine learningmodels have been leveraged to expedite this process, allowing for design quality predictions inmilliseconds . In the ML Contest for Chip Design with HLS , we used a novel approach that introduces agentichigh-level synthesis, leveraging large language models (LLMs) equipped with specialised tools forreasoning and optimisation. By imbuing these models, specifically over 100 billion parameter models,with agent-like capabilities, they can autonomously explore, assess, and optimise HLS designs,enhancing the automation and intelligence embedded in the design flow. Our method bridges machinelearning and hardware design, empowering LLMs to understand and interact with the design space toproduce superior configurations more efficiently.",
  "Methodology": "Our proposed approach for the contest relies on an extended informative representation of an inputdesign for high-quality performance prediction shown in . Our approach consists of severalsteps: (a) HARP fine-tuning, (b) source code sequence analysis, (c) reasoning steps based on thetraining data points, and (d) an agentic predictor/criticiser to evaluate the results.",
  "Fine-tuning HARP": "HARP is a GNN-based methodology that enhances FPGA HLS optimization by using a hierarchi-cal graph representation to model program structure and pragma transformations separately, allowingfor improved accuracy and adaptability. Through its modular design, HARP efficiently capturesthe dynamic impact of design choices, enabling faster and more accurate design space exploration.We utilized the HARP encoder to create comprehensive graph embeddings that encapsulate criticaldesign features. The graph embeddings provide a representation from which reasoning about thecritical path and latency measurements can be derived. The structure of these embeddings facilitatesan in-depth analysis that highlights potential performance bottlenecks.",
  "Reasoning": "We provide a high-level explanation of the impact that the target pragmas may have on the control/dataflow graphs. The model incorporates graph-level reasoning to assess the influence of each pragmaand its effect on the designs critical path and validity. This capability enables the prediction model toevaluate potential performance variations induced by different pragmas. shows impact ofvarious optimisations on the spmv-ellpack kernel. Outliers were mostly identified as invalid designs.Graph embeddings carry specific control/data flow information that enables the LLM to attend tonode-edge relationships and enhances their overall reasoning.",
  "Agentic Evaluation": "An iterative evaluation process is employed, wherein the predictor agent performs self-reflectionbased on available training data batches. This process runs for three cycles due to its computationalexpense but is crucial for enabling the predictor to rationalize its performance predictions. Thepredictor-agent iteratively refines its analysis by providing justifications for specific optimizationsand then reflecting on feedback from the criticiser agent, enhancing overall prediction reliability.",
  "Results": "We initially began with a solution entirely based on large language models (LLMs), employingGPT-4o solely for generating predictions. Preliminary evaluations indicated that the classificationtask (i.e., distinguishing valid from invalid designs) significantly affected the root mean squareerror (RMSE) score. Consequently, our primary focus shifted towards developing a highly accurateclassifier. Concurrently, we explored the design space exploration (GNN-DSE) method and deployeda local LLaMA2 7B model for classification. However, this model lacked sufficient depth for complexreasoning tasks. Subsequently, we concentrated on running HARP locally and contributed a pull request (PR) toresolve several bugs, enabling us to fine-tune HARP on the training dataset. We leveraged thereasoning capabilities provided by HARP graph embeddings and combined them with source codesequencing, which yielded a notable improvement over LLaMA2 (27%). Our final solution integratedGPT-4o and included several reasoning components along with an evaluator agent to iteratively (3iterations) assess the predicted outcomes using the training samples available to the model. Theresults are detailed in . For the final submission, we selected the best-performing models,including the fine-tuned HARP.",
  "Discussion": "Limitation of CoT prompting: The technique is less effective with smaller models. To achievemeaningful gains, its best to apply CoT in proportion to the models size, as smaller models (lessthan 100B parameter) may produce less coherent reasoning with CoT prompting. We plan to use models such LLAMA3 to conduct this experiment however we were short of tie.Tat why we chose GPT4o. With emerge of larger models, we predict that agents will become moreeffective in reasoning and predicting design performances. As future work, we plan to extend thereasoning with synthesiser version considerations to achieve more accurate results. We believe ourAgentic-HLS approach once published can attract attention of agentic workflow researchers anddesigners to the Neurips 2024 AI for EDA community and we are keen to explore the opportunitywith larger models such as GPT-4o1 capable of deep reasoning."
}