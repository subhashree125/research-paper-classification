{
  "Abstract": "Traditional interactive environments limit agents intelligence growth with fixedtasks. Recently, single-agent environments address this by generating new tasksbased on agent actions, enhancing task diversity. We consider the decision-makingproblem in multi-agent settings, where tasks are further influenced by social con-nections, affecting rewards and information access. However, existing multi-agentenvironments lack a combination of adaptive physical surroundings and socialconnections, hindering the learning of intelligent behaviors. To address this, weintroduce AdaSociety, a customizable multi-agent environment featuring expandingstate and action spaces, alongside explicit and alterable social structures. As agentsprogress, the environment adaptively generates new tasks with social structures foragents to undertake. In AdaSociety, we develop three mini-games showcasing dis-tinct social structures and tasks. Initial results demonstrate that specific social struc-tures can promote both individual and collective benefits, though current reinforce-ment learning and LLM-based algorithms show limited effectiveness in leveragingsocial structures to enhance performance. Overall, AdaSociety serves as a valuableresearch platform for exploring intelligence in diverse physical and social settings.The code is available at",
  "Introduction": "Classic learning environments have agents trained in small and stationary worlds,which hinders the improvement of agents intelligence. The learning process stagnates once theenvironments can no longer provide novel data for agents explorations. Additionally, agentstrained on a fixed task set may suffer from a loss of generalization ability . Single-agentenvironments set out to solve this problem by constructing adaptive environments thatcontinuously generate new tasks based on agent actions, providing a multitudinous task set. In multi-agent settings, however, the task set is determined by not only physical surroundings but alsosocial connections among agents. Social connections dramatically impact agents decision-makingby shaping their reward structures and information access , and different social structures endowthe environments with radically different research problems. For example, centralized scenarios focuson issues like credit assignment and consensus establishment , while decentralized settingsrequire agents to address opponent modeling issues and non-stationarity .",
  "Example: dynamic connections": ": An overview of AdaSociety, composed of physical component and social component.Physical Component consists of diverse resources and events on the map and heterogeneous agentsinventories. Social Component describes the adaptive connections between agents and organizations,which shape information access and reward structure. Agents take social actions to alter their socialconnections. As shown in the rightmost flowchart, agents are initially independent and can establishindividual connections (edges between nodes) and form groups (gray ovals). What makes the problem even more challenging is that social connections are not predefined butadaptive, which means theres a dynamical interplay between the topology of social connections andagents states . The adaptive nature of social connections and physical surroundings requiresagents to learn continuously, reason about other agents policies, and balance between physicalexplorations and establishing social connections. While contemporary multi-agent decision-makingenvironments have achieved great progress in stimulating and testing capabilities oflearning algorithms in fixed task sets, they fail to generate new tasks by concurrently consideringexpanding physical surroundings and adaptive social connections. To bridge this gap, we propose AdaSociety, a multi-agent environment with massive and diverse tasksgenerated by adaptive social connections and expanding physical surroundings, which are influencedby agents behavior. In particular, to the best of our knowledge, AdaSociety first introduces socialstates (expressed as a multi-layer directed graph) to explicitly and quantitatively describe the adaptiveand dynamic connections between entities, including agents and emerged organizations. This greatlyenriches the diversity of tasks, supporting the establishment of stable and long-term relations betweenentities and the quantitative study of social intelligence, like coalition formation and the emergenceof hierarchy. In such an environment, agents need to balance the exploration of physical surroundingsand the alteration of social connections, leading to multiple possible victory paths and significantdecision-making challenges. To stimulate algorithm design and theoretical analysis in AdaSociety, weprovide a formulation of the multi-agent decision-making problems, named Growing-MG (Sec. 3). AdaSociety serves as a platform for researchers to customize the environment for diverse researchneeds. Specifically, a set of fundamental elements and mechanisms can be used, and interfaces areprovided to set environment attributes and hyper-parameters. Moreover, AdaSociety exhibits itscharacteristics by offering three mini-games, where both tensor- and LLM-based methods are tested. In summary, this paper makes three contributions. 1) We introduce a novel multi-agent general-sumenvironment featuring expanding physical surroundings and adaptive social connections. 2) We offera customizable environment with three built-in mini-games, supporting both tensor- and LLM-basedmethods. 3) We implement RL and LLM methods in these mini-games and provide preliminaryresults, laying the groundwork for further research in this environment.",
  "Physical Component": "Resource and Event Resources are natural or synthetic. Natural resources scatter randomly onthe map. Some natural resources are visible to everyone while others can only be seen when anagent has specific resources in its inventory. For example, only the agent possessing a hammer canobserve coal. When agents with specific resources in their inventories stand on an event grid andtake the synthesize action, one unit of new resource is synthesized. Synthetic resources will beautomatically placed into agents inventories. These resources and events can be systematicallydescribed as a synthesis tree (see ). Importantly, agents are unaware of this synthesis tree.They gradually learn the tree through interaction with the environment. Resources, event grids, andagents are initialized in random locations on the map for every episode. While there are existing 3Dbenchmark environments focusing on perception challenges, our research centers on the domain ofmulti-agent decision-making. To this end, the map is intentionally crafted in a 2D symbolic format. Agents Inventory Every agent has an inventory with maximal capacities of every resource, implyingskill diversity. For example, an agent with a 0 capacity for hammers cannot possess hammersand observe coal. Agents can collect resources from the map into their inventories and dumpresources on the map. Agents rewards are attached to the resources in their inventories, while theyexhibit heterogeneity in resource preferences. Specifically, for agent i, the reward of resource isRi() = mi hi() r, where mi is the amount of resource in is inventory, hi() R representsis preference for , r is the objective reward of a unit of (see details in Sec. A.1).",
  "Social Component": "The social component explicitly exhibits the connections between agents or organizations. Theseconnections drastically influence multi-agent decision-making by affecting agents accessible infor-mation and reward structures. Centralization and its complete opposite, decentralization, can be seenas two typical connection structures, presenting very different decision-making problems. AdaSocietysupports adaptive connections, with corresponding interactions being modeled as general-sum games.AdaSociety considers not only the connections between agents but also the subordinate connectionsbetween agents and organizations established autonomously by agents. This makes hierarchical con-nections possible. Agents take social actions to change social states, like connecting or disconnectingwith someone. illustrates evolving connection structures, from fully independent agents tosparsely connected agents with several non-overlapping small groups, and finally to a unified largegroup. On the other hand, as a customized environment, AdaSociety also supports users to predefineand/or fix social connections for their specific research problems. The semantics of connections arediverse, which can be reward sharing, information sharing, or division of labor between involvedagents. AdaSociety supports that agents negotiate their connection semantics (Sec. 4). To maintain consistency with the physical component, we refer to these connections between agentsand organizations as social states, which are expressed as a multi-layer directed graph (Sec. 3). Socialstates explicitly and quantitatively express relations between agents or organizations. For example,the cooperation level of two agents can be measured by the frequency of connections betweenthem. Moreover, the combination of social states with successive tasks in AdaSociety supports theestablishment of stable and long-term relations and the study of social intelligence, like coalitionformation and the emergence of hierarchy.",
  "Observation and Action": "Observation Each agent navigates with a partially observable window, reaching o grids in the fourcardinal directions of its current position. Agents can get their own inventory states of collectedresources, but not those of co-players. The social states of all the agents are accessible to everyone. Action Action space consists of social actions and physical actions. Social actions aim to build andbreak connections with others, including other agents or organizations. Connections are directional.If agent i connects to agent j, but not vice versa, i shares its information or reward with j, butgets nothing from j. Physical actions include movement, picking and dumping specific resources,synthesizing resources on corresponding event grids, and communicating with someone. Newlysynthesized resources enrich picking and dumping actions and the action space.",
  "Environment Characteristics": "There are various characteristics of AdaSociety that make it novel (see Tab. 1). AdaSociety is a multi-agent decision-making environment, which provides both mini-games for specific research problemsand a customizable platform to researchers (see details in Sec. A.4). Agents dynamically connectwith other agents or organizations and autonomously communicate to negotiate the semantics ofconnections, making the emergence of hierarchical social structure and diverse social intelligencepossible. With these dynamic and non-deterministic connections, friends may become foes, andvice versa. Thus, the interactions between agents can be modeled as general-sum games, wherecooperation coexists with competition. Agents navigate this playground with a partially observablewindow centered on their current position. The state and action spaces of AdaSociety dynamicallyexpand, adapting to agents (physical and social) behavior. That generates massive and diversetasks, supporting an evaluation of agents abilities in multiple aspects. AdaSociety is friendly toLLM- and tensor-based agents. We evaluate state-of-the-art RL methods and LLMs in Sec. 5. Inaddition, we want to stress that the mutual adaptation between agents and AdaSociety, whichgenerates a variety of successive tasks and multiple possible victory paths. Achieving success inAdaSociety requires a balance between the exploration of physical components and the alteration ofsocial connections (see ). Agents continually learn policies to efficiently explore and achievegoals in AdaSociety. Meanwhile, agents (physical and social) behavior will affect the dynamics ofAdaSociety. Synthesizing new resources will gradually expand AdaSocietys physical state spaceand the corresponding physical action space, transition function, and reward function. Updatedsocial states will reshape agents observation and reward structures. Thus, tasks and task sequencesare influenced by agents behavior and social states, not sampled according to some predefineddistribution of tasks. That is to say, AdaSociety adapts its tasks and task sequences to agents. Mutualadaptation provides exceptionally massive and diverse complex tasks. The stochasticity and non-stability of AdaSociety produce various environment dynamics. Agents need to keep learning to adaptto changing situations.",
  "Research Challenges": "As an adaptive multi-agent environment, AdaSociety provides a comprehensive platform that presentsplenty of research challenges. The adaptive and dynamic characteristics of the physical and socialcomponents bring challenges mainly lying in the intricate and unpredictable interactions betweenagents. Through multi-dimensional exploration, agents learn the ability of dynamic environmentaladaptation and engage in communication-enabled interactions. Meanwhile, agents may develop",
  "Formulation": "We now provide a comprehensive definition and analysis of the Growing-MG with a social structure,which are general enough to encompass all the research challenges mentioned above. Three concretescenarios will be instantiated in next section. The predominant model in multi-agent sequential decision-making is the Markov Game (MG).However, a significant limitation of MG is the assumption of constant state and action spaces andunchanged Markovian transitions or rewards, ensuring convergence to some classical solutions suchas global optimality or Nash equilibrium. To address dynamic state and action spaces, weintroduce two new structures, Monotonic-MG-bundle and Growing-MG as below. A Growing-MGyields a multi-agent non-stationary decision-making framework. At time step t, with state st andaction at, the Monotonic-MG-bundle produces St+1, At+1, Tt+1, Rt+1 = (st, at), forming onenew MG instance. This framework differs from time-varying games, which only modelpayoff matrix dependent on past actions. On the other hand, both the transition probability andreward function in Growing-MG will evolve triggered with some certain transitions. For simplicity,we denote all possible transition and reward functions on arbitrary state and action space S, A, asT (S, A) = {T|T : S A S} and R(S, A) = {R|R : S A R} and the largest possiblespaces supported by the environment as universal state space Sw and action space Aw. Definition 1. A base-MG is a tuple MGb = I, Sb, Ab, Tb, Rb, , , where I = {1, . . . , I} is aset of agents; Sb = {S1b , . . . , SIb } and Ab = {A1b, . . . , AIb} is the state space and action space of allagents; Tb : Sb Ab Sb and Rb : Sb Ab RI is the transition and reward function; : Sb is the initial state distribution and is the temporal discount factor. Definition 2.A Monotonic-MG-bundle upon a base-MG MGb within the universal stateand action space Sw = {S1w, . . . , SIw}, Aw = {A1w, . . . , AIw} is a map : St At {St+1, At+1, Tt+1, Rt+1|Sib Sit Sit+1 Siw, Aib Ait Ait+1 Aiw, Tt+1T (St+1, At+1), Rt+1 R(St+1, At+1)}.",
  "Definition 3. A Growing-MG upon a base-MG MGb within the universal state and action spaceSw, Aw is a tuple MGg = (MGb, )": "Conceptually, each alteration in the state and action space represents a distinct stage where interre-lationships among agents should also change. Inspired by research in complex systems like socialsciences and economics , we propose enhancing the Growing-MG framework with amultilayer graph structure G = (V, E, C) (see ). C is a set of layers, and V is the set ofnodes in all layers. E is the set of edges existing between nodes in one layer or neighboring layers.We start with a non-interconnected multiplex system of networks {G1, G2, , G|C|}, where eachlayer c consists of a node set Vc and an edge set Ec, represented by an adjacency matrix Acij withi, j {1, , |Vc|}. Nodes in the first layer represent agents in Growing-MG, while higher layersrepresent groups and hierarchies of groups. To delineate relationship between nodes in neighboringlayers such as agent-group membership, we introduce inter-layer connectivity using an adjacencymatrix Ac,c+1ijwith i {1, , |Vc|} and j {1, , |Vc+1|}. This representation models bothstatic and time-varying networks, as inter-layer and intra-layer connectivity evolves with agentsbehavior, distinguishing it from existing multi-agent frameworks that predetermine interactionsthrough reward structures. Finally, we note that both the environmental and social stateswithin the framework can be extended to include observational information, thereby furtherenhancing the frameworks generality and practical relevance.",
  "Mini-games": "To provide a comprehensive benchmark and illustrate the characteristics of AdaSociety, we proposea set of mini-games (). The three mini-games are arranged in ascending order of the com-plexity of decision-making. Social structure, prescribing agents partners and connection semantics,evaluates agents ability to adapt to the changeable social structure. Contract predefines connectionsemantics, where agents need to select partners while learning coordination with various co-players. In Negotiation, agents independently select partners, determine the reward distribution plan withtheir partners, and behave under the negotiated relationship. All of the three mini-games share thesame physical component (Sec. 5.1), which contains a part of the synthesis tree. The following textprovides a detailed description of the social components of Social structure, Contract, Negotiation.To show the full complexity of our physical components, another mini-game Exploration, whichcontains all built-in resources and events, is introduced in Sec. C.2.",
  ": Overview of three mini-games": "Social Structure. The explicit represen-tation of social structure allows dynamicchanges as agents interact with the en-vironment. Pre-defined rules for struc-ture change could be designed to com-pel agents to alter their social relation-ships while interacting with the environ-ment. We implement structure changeat certain steps: when step t reachesT1, T2, ..., the social structures are mod-ified to G1, G2, ..., respectively. Differentcategories of social structures are statedin Sec. C.1. This forces agents to learnpolicies to adapt to the changing social environment. Contract. The environment is divided into two stages: the contract formation stage for determiningsocial connections and the physical interaction stage to interact with the physical component andco-players with determined social connections. The contract formation stage lasts for cN time steps,where c is a positive integer and N is the number of agents, while the physical interaction stage has aduration of T. Therefore, the total duration of each episode is cN + T. Before the contract formationstage (0 t < cN), an order (i1, i2, ..., iN) is randomly sampled. At time t, agent ik, where k = tmod N, takes social action, selecting a group node vg Vg to connect. An agent can connect withonly one group node. Agents within the same group are considered to have formed a contract to sharerewards. In the physical interaction stage (t cN), all agents act synchronously within the physicalcomponent, and the rewards received are equally divided among the agents within the same group. Negotiation. The game has a negotiation stage followed by a physical stage. In negotiation, agentsseek cooperation by selecting an opponent and sending him a request. After mutual requests, agentsbargain by exchanging proposals until agreement or breakup. In the bargaining session, agents iand j take turns to perform one of the three actions: (i) PROPOSE a new scheme (wi, wj) s.t.wi + wj = 1, where wi and wj represent the partition of rewards obtained by i and j respectivelyin the physical stage. (ii) ACCEPT the proposal from ones opponent and form a new group(coalition). (iii) DECLINE the proposal and end this session without any commitment. Once anew group is formed, the cooperative relationship between i and j represented by edge Eij witha payoff distribution (wi, wj) is established. Later, when i or j seeks to negotiate with others, itrepresents the group {i, j}. For example, if i and an out-group agent k reach a new distributionplan (wnewi, wnewk), then k is regarded as joining {i, j} to form a new group {i, j, k} with an updateddistribution (wi wnewi, wj wnewj, wnewk).",
  "We have designed two physical task settings, featuring different levels of difficulty, for SocialStructure, Contract, and Negotiation. The parameters of these tasks are provided in Sec. C.3": "In the Easy task, the environment involves a single event HammerCraft. Within this task, agents arecategorized into two types based on their inventory capacity and value preference: carpenters andminers. Carpenters have the ability to gather wood and stone, which they can then use to producehammers through the HammerCraft event. However, their inventory is limited to holding only onehammer at a time. On the other hand, miners are unable to collect stone, making them incapable ofproducing hammers. However, miners possess the advantage of being able to store a considerablenumber of hammers in their inventory. Additionally, hammers held by miners are assigned a highervalue compared to those held by carpenters. In the Hard task, the environment becomes more complex with the inclusion of six resources: wood,stone, hammer, coal, torch, and iron, as well as two events: HammerCraft and TorchCraft. Similar tothe Easy task, agents are divided into carpenters and miners. Due to the limited capacity of certainresources, only carpenters can execute HammerCraft to produce hammers, while only miners canexecute TorchCraft to produce torches. However, carpenters inventories cannot store coal, whichrequires a hammer to pick up, and miners inventories cannot store iron, which requires a torch topick up. Consequently, in order to maximize group rewards, carpenters and miners should engage inresource exchange, providing the resources they can produce to each other. This collaborative effortensures that the group can obtain more resources collectively.",
  "Baseline Methods": "We use several deep reinforcement learning algorithms as baselines. Proximal Policy Optimization(PPO) strikes a balance between sample efficiency and policy stability by constraining policyupdates using a trust region approach and a clipped surrogate objective. RecurrentPPO(RecPPO)uses PPO for training and add LSTM to maintain memories in the network. Rainbow is avalue-based method that incorporates several key enhancements into the Deep Q-learning framework.MAPPO is the multi-agent version of PPO. It learns a critic that takes the global state and otheragents actions as inputs during training. We employ a convolutional neural network for encodinggrid information and a graph convolutional network for encoding social state in all RL methods.The open-source library RLLib is used for RL training. Additionally, we design a curriculum learning (CL) algorithm. It starts with shared rewards toenhance cooperation strategies, then gradually increases social state randomness for learning underdifferent social structures, and finally allows agents to perform social actions to establish their ownsocial state. RecPPO is used for RL training at each stage. We also present a Large Language Model+ rule-based controller (LLM-C) framework based on GPT-4 , which converts environmentalinformation into prompts to query an LLM for high-level plans and then calls a rule-based controllerto execute actions based on the generated plan. LLM has been shown to be effective in some single-agent environments, such as MineCraft . The details of the last two algorithms aregiven in Appendix D.",
  "Social Structure": "In the Social Structure mini-game, various static and dynamic social structures are tested to evaluatebaseline algorithms. Detailed results are presented in Appendix E. Here, we discuss the result of oneDynamic scenario, where the social structure starts with Inequality, then switches to Independent(Ind.) group at step 30, and alters to Overlapping (Ovlp.) group at step 60. a presents the reward accumulation as agents take actions with three static-structure scenariosand one dynamic-structure scenario, respectively. The results verify the influence of dynamic changein social structure on agent performance since the Dynamic curve resembles the Inequality scenarioinitially but then it drops in later steps and approaches Ovlp. group scenario.",
  "Nego. Easy 0.3543 0.0229 0.2276 0.0006 0.2278 0.0004 0.2147 0.0001 0.1969 0.0105 0.0040 0.0001Hard 0.1945 0.0109 0.1093 0.0027 0.1107 0.0019 0.0946 0.0032 0.0905 0.0024 0.0020 0.0001": "to the difficulty in learning an effective central critic for heterogeneous agents. Rainbow performs theworst, likely because of its general ineffectiveness in exploration. Curriculum learning demonstratessuperior performance by leveraging prior knowledge of different structures to adapt to dynamicscenarios effectively. Additionally, figures in reveal significant deviations in most tests,regardless of social structures, learning algorithms, or performance metrics. Compared to scenarioswithout agent groups (a and a), the results indicate that the current algorithms struggleto learn stable policies for scenarios with agent groups.",
  "Contract": "As depicted in Tab. 2, Contract presents a challenge for popular RL methods, as they are stuckin a local equilibrium of completing limited HammerCraft on both tasks (see b), while CLdemonstrates notable performance on the Easy tasks and surpasses general RL methods on the Hardtasks. The first curriculum in CL equips the agent with the ability to learn effective policies in thephysical realm, and the second curriculum empowers the agent to make informed judgments aboutdifferent social structures while considering rational physical policies. Ultimately, this knowledgeaids CL in selecting an appropriate contract. However, it appears that CL may forget the strategiesacquired during the first curriculum, as the reward at the end of the second stage has droppedsignificantly compared to the end of the first stage (see Tab. 12 for details). This might hamper theperformance of CL on the Hard task. Sharing rewards has been recognized as an effective method for agent groups to acquire cooperativestrategies, thereby supporting the feasibility of CLs approach. c and d also illustratesthat. In the case of the Easy task, CL eventually establishes a stable group of three individuals whoactively share rewards and form a cooperative alliance. However, it is important to note that the sizeof the group does not directly correlate with high returns. Rainbow, for instance, frequently formslarge groups in both tasks but fails to achieve substantial returns. This outcome primarily stems frominherent limitations in the algorithms learning capabilities.",
  "Negotiation": "Traditional RL methods struggle to enable carpenters and miners to learn to cooperate throughnegotiation, dumping some tools to increase the benefit of teammates with larger capacities onthe physical stage as shown in Tab. 2. This challenge arises from the complexity of coupling thenegotiation and physical stages. Once negotiation fails, dumping tools in the subsequent physicalstage would substantially reduce the agents rewards. Meanwhile, the complex negotiation processexacerbates the convergence problem in multi-agent settings, and agents have the incentive to claima larger share for themselves to exploit the co-players in bargaining, posing challenges to reachinga consensus agreement. Consequently, in both easy and hard tasks, the average and maximumdegrees are low, with most agents opting to complete tasks independently, leading to low completionrates in HammerCraft and even a complete failure in TorchCraft (). In the easy task, minersrewards heavily rely on carpenters cooperation, which severely compromises fairness. In contrast,by first learning the optimal strategies in physical environments under different social structures, CLcan identify structures with higher cooperation degrees as more beneficial, facilitating consensusduring negotiation learning and achieving higher group rewards, fairness, and successful TorchCraft.Additionally, we show the Carpenters/Miners (abbreviated as C/M) split ratio when the negotiationstage is done, which is computed by",
  "LLM-C in AdaSociety": "LLM-C runs three times for each task. Tab. 3 and Tab. 9 presents the quantitative results acrossvarious metrics. Benefiting from the embedded commonsense reasoning and social intelligenceof LLMs, LLM-C exhibits outstanding performance in all three mini-games, achieving averagerewards nearly surpassing all RL-based methods. After being informed of the game rules and thecapability differences between carpenters and miners, LLM-C can accurately recognize the impor-tance of cooperation and swiftly form alliances with other players through negotiation or contract.",
  "Easy-0.8433 0.13120.8733 0.1116Hard0.7894 0.04440.6499 0.17160.6862 0.1027": "During the physical stage, manu-ally coded controllers complementLLMs deficiencies in path plan-ning and position judgment, preciselyand efficiently realizing the high-level planning generated by the LLMbased on the current social structureand physical environment. However,due to common issues with LLMssuch as hallucinations, context length limitations, and randomness in outputs, LLM-C does notachieve Oracle performance, and it underperforms compared to CL in Contract-Easy, further validat-ing the effectiveness of our proposed CL approach.",
  "Related Works": "Environments. Several craft-based environments like Malmo , Crafter , Minedojo and Conan create dynamic state and action spaces that expand with the agents exploration,which, however, mainly focuses on single-agent setting. Environments including MAgent ,XLand , and Miniworld provide a set of different and transferrable tasks that build frombasic elements, and they are open for customization. Melting Pot contains a set of over 50 MARLlearning substrates with limited customizability. Interactive games including AI Economist ,Overcooked , MPE , Neural MMO , and SMAC place agents in diverse systemsallowing them to compete or cooperate. Other examples like Diplomacy focus on communicationbetween agents. None of these environments contain both dynamic social connections and adaptivetasks like AdaSociety. Unsupervised Environment Design (UED). In the paradigm of UED , the environmentlearns a policy : (T ), which is a function from agent policy to the environmentsparameters T . Such a policy will automatically produce a distribution over solvable environmentsand further support the continued learning of the agents policy. AdaSociety does not implement UEDto produce diverse tasks. Unlike UED, AdaSociety has no goals or objectives, like most ecologicalsystems, and produces multiple tasks through adaptive social structures and expanding physicalsurroundings. Structured multi-agent systems. In multi-agent systems, various connections may be formedbetween agents, and these connections may form certain structures. , and focus onfinding communication topology for multi-agent coordination. Some research models the locality ofinteraction and learns a joint value via coordination graphs . Networked MARL learns localized policies on environments where agent interactions are contingent upontheir connections within a static graph. We focus on dynamic agent connections which shape agentsrewards and observations, and these connections are modeled as a multi-layer graph.",
  "Conclusion": "We introduce AdaSociety, a multi-agent environment featuring expanding physical surroundings andadaptive social connections. The environment is capable of generating multiple tasks in adaptationto agents behavior. AdaSociety is friendly to tensor-based and LLM-based methods. AdaSocietyprovides interfaces supporting superb customization and also offers a set of mini-games with diversesocial connections. We test several RL and LLM-based algorithms in mini-games. Preliminary resultsindicate that AdaSociety maintains a rational complexity level for current decision-making methods. There are some limitations of AdaSociety illumining our future work. Human-machine interaction iscrucial for the study of multi-agent systems, which is one of our key research objectives in AdaSociety.While the environment is temporarily not equipped with human interfaces, the current architecturedoes support the subsequent development of human-machine interfaces. In addition, our game spacescan be further expanded by introducing survival pressures (need for food, hostile creatures, and soon). These negative losses will penalize undesirable actions, complement the roles of positive rewardsin reinforcing desirable behavior, and guide more diverse behavior.",
  "and Disclosure of Funding": "This work is supported by the National Science and Technology Major Project (No. 2022ZD0114904).This work is also supported by the project Wuhan East Lake High-Tech Development Zone, NationalComprehensive Experimental Base for Governance of Intelligent Society. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia LeoniAleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4technical report. arXiv preprint arXiv:2303.08774, 2023. John P Agapiou, Alexander Sasha Vezhnevets, Edgar A Duez-Guzmn, Jayd Matyas, Yi-ran Mao, Peter Sunehag, Raphael Kster, Udari Madhushani, Kavya Kopparapu, RamonaComanescu, et al. Melting pot 2.0. arXiv preprint arXiv:2211.13746, 2022.",
  "Eitan Altman and Adam Shwartz. Constrained markov games: Nash equilibria. In Advances indynamic games and applications, pages 213221. Springer, 2000": "Ioannis Anagnostides, Ioannis Panageas, Gabriele Farina, and Tuomas Sandholm. On theconvergence of no-regret learning dynamics in time-varying games. Advances in NeuralInformation Processing Systems, 36, 2024. Anton Bakhtin, Noam Brown, Emily Dinan, Gabriele Farina, Colin Flaherty, Daniel Fried,Andrew Goff, Jonathan Gray, Hengyuan Hu, Athul Paul Jacob, Mojtaba Komeili, KarthikKonath, Minae Kwon, Adam Lerer, Mike Lewis, Alexander H. Miller, Sasha Mitts, AdithyaRenduchintala, Stephen Roller, Dirk Rowe, Weiyan Shi, Joe Spisak, Alexander Wei, David Wu,Hugh Zhang, and Markus Zijlstra. Human-level play in the game of diplomacy by combininglanguage models with strategic reasoning. Science, 378(6624):10671074, December 2022.doi: 10.1126/science.ade9097. URL",
  "Adrian Rivera Cardoso, Jacob Abernethy, He Wang, and Huan Xu. Competing against equilibriain zero-sum games with evolving payoffs. arXiv preprint arXiv:1907.07723, 2019": "Micah Carroll, Rohin Shah, Mark K Ho, Tom Griffiths, Sanjit Seshia, Pieter Abbeel, and AncaDragan. On the utility of learning about humans for human-ai coordination. Advances in neuralinformation processing systems, 32, 2019. Maxime Chevalier-Boisvert, Bolun Dai, Mark Towers, Rodrigo Perez-Vicente, Lucas Willems,Salem Lahlou, Suman Pal, Pablo Samuel Castro, and J Terry. Minigrid & miniworld: Modular& customizable reinforcement learning environments for goal-oriented tasks. In Advances in",
  "Manlio De Domenico. More is different in real-world multilayer networks. Nature Physics, 19(9):12471262, 2023": "Fabio Della Rossa, Louis Pecora, Karen Blaha, Afroza Shirin, Isaac Klickstein, and FrancescoSorrentino. Symmetries and cluster synchronization in multilayer networks. Nature communi-cations, 11(1):3179, 2020. Michael Dennis, Natasha Jaques, Eugene Vinitsky, Alexandre Bayen, Stuart Russell, AndrewCritch, and Sergey Levine. Emergent complexity and zero-shot transfer via unsupervisedenvironment design. Advances in neural information processing systems, 33:1304913061,2020. Yali Du, Bo Liu, Vincent Moens, Ziqi Liu, Zhicheng Ren, Jun Wang, Xu Chen, and HaifengZhang. Learning correlated communication topology in multi-agent reinforcement learning.In Proceedings of the 20th International Conference on Autonomous Agents and MultiAgentSystems, pages 456464, 2021. Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, AndrewTang, De-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-endedembodied agents with internet-scale knowledge. Advances in Neural Information ProcessingSystems, 35:1834318362, 2022.",
  "Sepp Hochreiter and Jrgen Schmidhuber. Long short-term memory. Neural computation, 9(8):17351780, 1997": "Yizhe Huang, Anji Liu, Fanqi Kong, Yaodong Yang, Song-Chun Zhu, and Xue Feng. Efficientadaptation in mixed-motive environments via hierarchical opponent modeling and planning.In Proceedings of the 41st International Conference on Machine Learning, volume 235 ofProceedings of Machine Learning Research, pages 2000420022. PMLR, 2127 Jul 2024. URL Minqi Jiang, Michael Dennis, Jack Parker-Holder, Andrei Lupu, Heinrich Kttler, EdwardGrefenstette, Tim Rocktschel, and Jakob Foerster. Grounding aleatoric uncertainty for un-supervised environment design. Advances in Neural Information Processing Systems, 35:3286832881, 2022.",
  "Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutionalnetworks. In International Conference on Learning Representations, 2016": "Fanqi Kong, Yizhe Huang, Song-Chun Zhu, Siyuan Qi, and Xue Feng. Learning to balancealtruism and self-interest based on empathy in mixed-motive games. In The Thirty-eighthAnnual Conference on Neural Information Processing Systems, 2024. Marc Lanctot, Edward Lockhart, Jean-Baptiste Lespiau, Vinicius Zambaldi, Satyaki Upadhyay,Julien Prolat, Sriram Srinivasan, Finbarr Timbers, Karl Tuyls, Shayegan Omidshafiei, et al.Openspiel: A framework for reinforcement learning in games. arXiv preprint arXiv:1908.09453,2019. Sheng Li, Jayesh K Gupta, Peter Morales, Ross Allen, and Mykel J Kochenderfer. Deepimplicit coordination graphs for multi-agent reinforcement learning. In Proceedings of the20th International Conference on Autonomous Agents and MultiAgent Systems, pages 764772,2021. Eric Liang, Richard Liaw, Robert Nishihara, Philipp Moritz, Roy Fox, Ken Goldberg, JosephGonzalez, Michael Jordan, and Ion Stoica. Rllib: Abstractions for distributed reinforcementlearning. In International conference on machine learning, pages 30533062. PMLR, 2018.",
  "Qinghua Liu, Csaba Szepesvri, and Chi Jin. Sample-efficient reinforcement learning ofpartially observable markov games. Advances in Neural Information Processing Systems, 35:1829618308, 2022": "Xiangyu Liu and Kaiqing Zhang. Partially observable multi-agent rl with (quasi-) efficiency:the blessing of information sharing. In International Conference on Machine Learning, pages2237022419. PMLR, 2023. Ishita Mediratta, Minqi Jiang, Jack Parker-Holder, Michael Dennis, Eugene Vinitsky, andTim Rocktschel. Stabilizing unsupervised environment design with a learned adversary. InConference on Lifelong Learning Agents, pages 270291. PMLR, 2023. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc GBellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al.Human-level control through deep reinforcement learning. nature, 518(7540):529533, 2015.",
  "Qi Su, Alex McAvoy, Yoichiro Mori, and Joshua B Plotkin. Evolution of prosocial behavioursin multilayer populations. Nature Human Behaviour, 6(3):338348, 2022": "Joseph Surez, Phillip Isola, Kyoung Whan Choe, David Bloomin, Hao Xiang Li, Nikhil Pinna-paraju, Nishaanth Kanna, Daniel Scott, Ryan Sullivan, Rose S. Shuman, Lucas de Alcntara,Herbie Bradley, Louis Castricato, Kirsty You, Yuhao Jiang, Qimai Li, Jiaxin Chen, and XiaolongZhu. Neural mmo 2.0: A massively multi-task addition to massively multi-agent learning, 2023. Open Ended Learning Team, Adam Stooke, Anuj Mahajan, Catarina Barros, Charlie Deck,Jakob Bauer, Jakub Sygnowski, Maja Trebacz, Max Jaderberg, Michael Mathieu, et al. Open-ended learning leads to generally capable agents. arXiv preprint arXiv:2107.12808, 2021.",
  "Xiaofeng Wang and Tuomas Sandholm. Reinforcement learning to play an optimal nashequilibrium in team markov games. Advances in neural information processing systems, 15,2002": "Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe,explain, plan and select: interactive planning with llms enables open-world multi-task agents.In Thirty-seventh Conference on Neural Information Processing Systems, 2023. Zihao Wang, Shaofei Cai, Anji Liu, Yonggang Jin, Jinbing Hou, Bowei Zhang, Haowei Lin,Zhaofeng He, Zilong Zheng, Yaodong Yang, et al. Jarvis-1: Open-world multi-task agents withmemory-augmented multimodal language models. arXiv preprint arXiv:2311.05997, 2023. Zihao Wang, Shaofei Cai, Zhancun Mu, Haowei Lin, Ceyao Zhang, Xuejie Liu, Qing Li, AnjiLiu, Xiaojian Ma, and Yitao Liang. Omnijarvis: Unified vision-language-action tokenizationenables open-world instruction following agents. arXiv preprint arXiv:2407.00114, 2024. Manjie Xu,Guangyuan Jiang,Wei Liang,Chi Zhang,and Yixin Zhu.Ac-tive reasoning in an open-world environment.In Advances in Neural Informa-tion Processing Systems, volume 36, pages 1171611736. Curran Associates, Inc.,2023.URL",
  "Mengxiao Zhang, Peng Zhao, Haipeng Luo, and Zhi-Hua Zhou. No-regret learning in time-varying zero-sum games. In International Conference on Machine Learning, pages 2677226808. PMLR, 2022": "Lianmin Zheng, Jiacheng Yang, Han Cai, Ming Zhou, Weinan Zhang, Jun Wang, and Yong Yu.Magent: A many-agent reinforcement learning platform for artificial collective intelligence. InProceedings of the AAAI conference on artificial intelligence, volume 32, 2018. Stephan Zheng, Alexander Trott, Sunil Srinivasa, David C Parkes, and Richard Socher. Theai economist: Taxation policy design via two-level deep multiagent reinforcement learning.Science advances, 8(18):eabk2607, 2022. Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang, Gao Huang, BinLi, Lewei Lu, Xiaogang Wang, et al. Ghost in the minecraft: Generally capable agents foropen-world environments via large language models with text-based knowledge and memory.arXiv preprint arXiv:2305.17144, 2023.",
  "A.1Resources": "There are 15 kinds of resources in AdaSociety, which can be divided into Natural Resources andSynthesized Resources based on whether they can be produced through events. Some of the nat-ural resources can only be discovered and gathered by agents with certain resources (denoted byRequirements) in their inventories. The details of resources are listed in Tab. 4. : Resources predefined in AdaSociety. Synthesized indicates whether the resource can becrafted through events. Requirement is an attribute of natural resources (Synthesized = False)indicating that the resource is observable and collectible to agents carrying the required resources.Objective reward denotes the objective rewards of resources.",
  "A.2Events": "There are 9 built-in events in AdaSociety as listed in Tab. 5. Each event takes 2 to 3 kinds of resourcesas input and outputs 1 kind of product. Events can only be observed and executed by agents whoseinventories meet the event requirements. : Events predefined in AdaSociety. The ingredients of each event are covered in Input. Mostevents take 2 or 3 different kinds of input resources. The products are listed in Output. Requirementdenotes the resources an agent needs to carry in its inventory to observe and execute the event.",
  "A.3Synthesis Tree": "An illustration of the synthetic tree is shown in , which is used by all the mini-games offeredby this paper. In , natural and synthetic resources are depicted within a green circle and blueoctagon icons respectively. The solid red arrow line attached by a square event icon links low-levelresources to high-level products. The eye icons indicate that some resources can help their ownerdiscover new resources or events.",
  "where N is the number of agents. Intuitively, the greater the value of F a group gets, the fairer it is": "Individual reward is one of the most common metrics for decision-making problems. It measuresagents decision-making abilities in maximizing self-interest. However, relying solely on individualrewards can be risky. In general-sum games, agents focus on maximizing their own rewards mayengage in shortsighted and exploitative behaviors that harm their own long-term rewards and thecollective benefit. For example, in Prisoners Dilemma, self-interested agents always fall into theinefficient Nash equilibrium of defection, which minimizes ones own reward and the collectivebenefit. To tackle this issue, we introduce the fairness score calculated using the Gini index,which evaluates fairness within a group. In real societies, fairness is a crucial component of socialjustice, significantly influencing the stability of social structures and the maintenance of long-termcooperation. This metric serves as a reference for selecting agents and algorithms that balanceefficiency and fairness, rather than merely pursuing individual gains. Completion ratepertains to the ratio of successful executions of an event to its maximum potentialexecutions. It is computed separately for each event. The completion rate is introduced to measureagents exploration within the synthesis tree. It is calculated as the ratio of actual executions tothe optimal executions of the oracle policy (computation of the oracle policy can be found inSupplementary Material). The higher the dimension of the completion rate, the deeper the exploration.Exploration is crucial in RL. The introduction of completion rate will guide decision-makingalgorithms to avoid local optima, actively explore the environment, and find the optimal policyeffectively.",
  "Dmax= maxnN Dn.(4)": "In asymmetric cases (where not all edges are bidirectional), the maximum degree and the averagedegree mentioned above are calculated separately for in-degrees and out-degrees. Social structure isthe distinctive feature of AdaSociety. Degree-based metrics, including average degree and maximumdegree, are proposed to describe and measure the topology of social structure, which significantlyinfluences agents policies and performances by shaping their information streams and rewardfunctions. Agents degree distribution is generally correlated with their rewards. For example, anagent with a high degree can obtain more information or participate in more reward distribution,thereby gaining higher returns. Combining degree-based metrics with other metrics, like individualreward and fairness, we can recognize the effective social structure for scenarios, guiding the learningof algorithms.",
  "A.6.1Mutual Adaption Between Agents and AdaSociety": "Based on complex network theory, we say AdaSociety is an adaptive environment. In complexnetwork theory, a network is called an adaptive network, if there is a feedback loop between theattributes or behavior of nodes and the topology of the network . In AdaSociety, agentsbuild or break connections with others and impact social structure. Conversely, social structureinfluences agents observations and reward structures and further influences their attributes andbehavior. Thus, following the definition of adaptive networks, the social structure of AdaSociety is adaptive. As a key component of AdaSociety, social structure influences the generation of new tasks.For example, independent agents collect all kinds of available resources to synthesize high-levelresources. However, the team-up agents will be mostly rewarded by collecting or synthesizingsome specific kind of resources, according to the division of labor in the team. Furthermore, agentsinitially can only observe very limited resources (wood and stone in our mini-games) and events(hammercraft). Through exploration in AdaSociety, agents gradually discover new resources andevents. The appearance of a new kind of resource depends on agents behavior. For instance, as shownby the synthesis tree in , which appears next, shovel or cutter, depends on agents behavior. Tosum up, AdaSociety is an adaptive environment. describes the mutual adaption between agents and AdaSociety. To achieve their goals, agentslearn policies to adapt their (physical and social) behavior to the environment. Meanwhile, agentsbehavior will affect and even change the environment. Specifically, physical actions will expand thephysical state space and the corresponding physical action space, reward, and transition functions bysynthesizing new resources. Social actions will alter social connections, and then influence agentsinformation access and reward structures. In AdaSociety, there is a feedback loop between agents andthe environment, making their coevolution possible and may shed light on the generation of infinitetasks.",
  "A.6.2A Multi-Layer Directed Graph Expression for Social Structure": "As shown in , AdaSociety expresses social states as a multi-layer directed graph. Each layershows a level of social organization. AdaSociety supports the description of social structures witharbitrary levels, depending on the research problems and the required granularity of social structures.The bottom 0th-level consists of individual agents, who are the fundamental units of decision-making.Nodes in each layer represent entities/agents in the corresponding level. Any agent on the kth-level(k 1) is composed of its connected agents on the (k-1)th-level. Its decision-making relies ongroup norms, like voting, consensus decision-making and delegation. A kth-level agent will affectits (k-1)th-level neighbors reward functions and observations, thereby influencing their decision-making and enabling their division of labour and cooperation. One agent on the (k-1)th-level maybe simultaneously subordinate to any number of agents on the kth-level. For example, an individualemployee is the 0th-level agent, a project team composed of several employees is the 1st-level agent,a company consisting of many teams is the 2nd-level agent, and a business group composed of manycompanies is the 3rd-level agent. AdaSociety supports the emergence of high-level social organizations. Edges inside one layerrepresent cooperative connections, which share information or rewards between involved entities.Edges across layers represent subordinate connections, with low-level entities complying with thepolicy implemented by the high-level entities. Modeling social states as a multi-layer graph willfacilitate the application of existing graph theory knowledge to our research.",
  "increasing the complexity of the exploration. Dependency between different resources and eventsevaluates the agents abilities to make deep explorations in the environment actively": "AdaptationIn AdaSociety, agents behaviors could trigger the environment to evolve while thechanged environment affects actions that agents can take. Apart from the physical environment,the social structure of the agents could dynamically change as a consequence of either pre-definedrules or agent social behaviors. This requires agents to make decisions accordingly to adapt to andco-evolve with dynamic environments and social relationships. Social cognitionAgents have beliefs in their social structures, which explicitly represent how theyinteract with other agents, such as exchanging information and sharing rewards. In this complexenvironment, several achievements require collaboration while resources are limited, forcing agents tocognitively infer others intentions, evaluate the effectiveness of social structures, and then investigatebetter choices. This makes AdaSociety a suitable environment for studying social cognition andbehaviors, such as heterogeneous roles, labor division, ownership, and trust/betrayal. CommunicationPortal for communication is provided to agents for sharing information andcoordinating actions. A successful agent may learn various communication protocols, contextrepresentations, and information processing for optimal objectives. Thus AdaSociety could be usedfor studying the effectiveness of agent communication-enabled interactions, such as negotiation forresource trading, information transitivity, and semantic interoperability. Collective reasoningAgents are embedded with heterogeneous skills while they only know theirown skills. The complex synthesis tree requires agents abilities to make group decisions on collabo-ration, such as knowledge sharing and skill transferring for greater group benefit. Additionally, thedynamics of environments make collective reasoning harder, especially for temporal credit assign-ments. For instance, agents may offer tools (negative immediate reward) to collaborators to exploitunexplored resources (greater delayed reward). Therefore, AdaSociety brings challenges for collectivereasoning, such as adaptive cooperation and coordination, consensus, and conflict resolution. EmergenceAction space in multiple perspectives, including physical actions, social actions,and communication, enables massive possibilities of agent behaviors without explicit policies. InAdaSociety, one could observe the emergence of coordination and cooperation, social structures andnorms, and even communication protocols and language.",
  "C.1Social Structure": "As stated in Sec. 3, agents connected by edges share observations, thereby improving their collectivesituational awareness. Agents connecting to the same group node share rewards based on theedge attributes, incentivizing collaborative efforts to achieve greater rewards. With the socialstructure, agents act synchronously in the physical environment, following the mechanism forsharing observation and reward defined by the social structure. In this mini-game, we conducted experiments with static and dynamic social structures. In the staticsetting, agents are initialized with a certain social structure G and keep the structure until the end ofone episode. In this paper, we categorize social structures that are less than two layers into five typesto examine the effects of varying structures on agent behavior and performance: 1) Isolation: agentsare fully unconnected, i.e., C = 1; E = ; 2) Connection: agents are connected without forminggroups, i.e., C = 1; E = ; 3) Independent Group: agents are grouped while each agent joins atmost one group, i.e., C = 2; V2",
  "jAij > 1; 5) Inequality: agents are groupedwith different reward weights": "In the dynamic setting, pre-defined rules for structure change could be designed to compel agents toalter their social relationships while they take actions within the physical environment. In this task,we design a Dynamic scenario, where the social structure starts with Inequality, then switches toInd. group at step 30, and alters to Ovlp. group at step 60.",
  "C.2Exploration": "In this scenario, all built-in resources and events are included. Physical actions and social actionsare available at every step. All agents share a common value preference, where 1) resources nearthe end of the synthesis tree are assigned high value, and 2) synthetic resources are valued higherthan natural resources. Due to partial observation, time limitations, and the challenges associatedwith exploring new resources, agents may manipulate the social state to encourage interest binding,information sharing, and division of labor, which helps to maximize rewards.",
  "D.1Curriculum Learning": "We developed a curriculum learning algorithm for AdaSociety. The algorithm controls the social stateto promote group cooperation and guides the agent to learn rational physical policies before learningsocial policies. Our curriculum consists of three stages. We use RecPPO for RL training in eachstage. In the first stage, all individual nodes are compelled to connect to the same group node. Thisarrangement ensures that all agents belong to the same group. If the reward assigned to an individualincreases monotonically with respect to the group reward, which is a common setting, agents in thisstage optimize their actions to enhance the overall benefits. This practical approach encourages thelearning of cooperative policies that yield higher rewards, benefiting both individuals and the group. In the second stage, each individual node is forced to connect to a specific group node with probabilitypK, while it randomly connects to any of the group nodes with probability 1 pK. The value ofpK gradually decreases with the episode number K, resulting in the gradual emergence of diversesocial state structures. This setup enables agents to learn physical policies with different social states.During the first two stages, social actions are not allowed, so agents focus solely on learning policiesrelated to physical actions.",
  "D.2Large Language Model with Rule-Based Controller": "Considering the social nature of AdaSociety, we also test the Large Language Model with GPT4 as examples. The LLM agent consists of three modules: observation, reasoning, and execution. In theobservation module, we transform the complex physical environment information within the agentsfield of view and the current social state into natural language form, merging it with the systemprompt including game rules and the agents tasks as inputs. In the reasoning module, the LLMagent generates a high-level plan through few-shot learning, where we require the agent to provideonly legal plans that conform to the current environment in the prompt. In the execution module,we decompose the high-level plan into low-level atomic actions through handcrafted functions forinteracting with the environment. The process repeats with a new reasoning step to generate a newplan until the current plan is completed. Due to the randomness of LLMs and the uncontrollablenature of multi-agent interactions in AdaSociety, it is possible to generate unachievable plans, whichwill be detected by a monitoring function, prompting the LLM to regenerate the plan.",
  "D.3Compute Resources": "CPU: 128 Intel(R) Xeon(R) Platinum 8369B CPU @ 2.90GHz; Total memory: 263729336 kB. GPU:8 NVIDIA GeForce RTX 3090; Memory per GPU: 24576 MiB. Each RL baseline experiment takes12 to 48 hours, depending on the mini-game and RL algorithm. For LLM-C experiments, each agenttakes an average of 5 seconds per step.",
  "Time (hours) | Game StepsCLPPORecPPOMAPPORainbow": "Negotiation-Easy5.66 0.16 | 14M0.12 0.03 | 0.47M1.05 0.04 | 2.4M21.21 0.35 | 1.8M14.97 0.30 | 42MNegotiation-Hard12.96 0.39 | 15M1.66 0.03 | 2.3M0.30 0.01 | 0.49M74.85 9.75 | 4.4M40.54 0.54 | 53MContract-Easy32.03 1.38 | 112M 0.19 0.03 | 0.80M1.94 0.03 | 6.0M9.65 0.02 | 1.2M9.65 1.49 | 25MContract-Hard48.58 0.56 | 78M0.21 0.02 | 0.56M0.74 0.01 | 1.0M14.94 0.01 | 0.94M 19.07 1.25 | 37MSocial Structure - Dynamic6.98 0.64 | 4.8M6.90 0.55 | 6.0M10.16 0.12 | 6.0M46.23 1.21 | 4.8M2.34 33.76 | 2.0M : Average number of game steps per second for different player counts (4, 8, 20, 100,1000). The number of groups is the same as the number of players. The experiment is conducted inExploration, where all built-in resources and events are included (see details in Sec. C.2).",
  "FBroader Impact": "To contribute to the development of multi-agent decision-making algorithms, we propose AdaSociety,a customizable environment with massive and diverse tasks generated by expanding state and actionspaces and adaptive social structures. Due to the complexity of tasks and the heterogeneity of agentscapacities and preferences, agents need to team up and even cooperatively establish hierarchicalsocial structures to achieve goals. However, agents may also learn some strategies that are harmful totheir co-players, as is common in multi-agent research. We have made significant efforts to mitigatesuch behaviors through thoughtful design within the environment. Given the heterogeneity amongagents and adaptive social structures, harmful behaviors tend to be short-sighted and inferior when itcomes to maximizing long-term benefits, with stable cooperation emerging as the optimal strategy.The multiple evaluation metrics introduced in AdaSociety, like fairness, also empower researchersto identify and exclude extreme or exploitative agents and facilitate the learning of cooperativebehaviors. Nevertheless, some harmful behaviors may still arise during training. We ask researchers utilizingour platform to meticulously observe agents behaviors to ensure they align with human values andpreferences. Should any misalignment or misrepresentation happen, we encourage contributions tothe source code (including but not limited to new evaluation metrics, environmental dynamics orincentive mechanisms) to enhance the platform.",
  "Optimal Event Executions for Calculating Completion Rate": "When the synthesis tree becomes complicated, it is not straightforward to calculate the maximumpotential executions for all the events, making it difficult to evaluate the performance through themetric Completion rate. Therefore, we develop an optimization formulation to compute the numberof event executions that maximize the credits obtained by agents. This optimization is formulated in asingle-agent setting. Since it aims to obtain maximum potential credits, multi-agent cases can also beapplied with the set of events being the union of agents skills. All natural resources can eventuallybe collected. Tab. 13 shows the parameters and variables used in this optimization.",
  "i xj, i Rn, j Q(i)(5d)i xi, i E(5e)xi jM, i E, j D(i)(5f)xi N, {0, 1}, {0, 1}(5g)": "Eq. 5 presents the optimization formulation, where Eq. 5a calculates the total credits gained by theagents collecting and synthesizing resources; Eq. 5b and Eq. 5c represent the eventually left naturalresources and synthetic resources after executing events; Eq. 5d indicate the required events to collectcertain resources; Eq. 5e indicate whether a type of event has occurred or not; Eq. 5f state the requiredevents to execute certain events; Eq. 5g limit the values of decision variables.",
  "Example Prompt for LLM-C": "The following examples illustrate the prompts used in LLM-C for each mini-game. The promptsvary slightly for different mini-games and also differ across stages within the same mini-game.Specifically, the prompt for the dynamic scenario in Social Structure is presented in Listing 1. Forthe contract formation stage in Contract, the prompt is displayed in Listing 2. Similarly, the promptfor the negotiation stage in Negotiation can be found in Listing 3. The physical stage for Contractand that for Negotiation are the same. There are two physical stage settings, featuring different levelsof difficulty. The corresponding prompts are provided in Listing 4 and Listing 5.",
  "The <Action> can ONLY be chosen from the following options:": "1. I want to join in Coalition 0.2. I want to join in Coalition 1.3. I want to join in Coalition 2.4. I want to join in Coalition 3.5. I want to join in Coalition 4.6. I want to join in Coalition 5.7. I want to join in Coalition 6.8. I want to join in Coalition 7.",
  "Thoughts: Im carpenter_0. I have 4 wood, 6 Stone. I am in a coalition with both": "carpenters and miners. The resources available are wood and stone, both ofwhich are nearby. The hammercraft event is right next to me, allowing me tocraft a hammer. I currently have more than 1 wood and more than 1 stone in myinventory, which is enough to craft a hammer. I have no hammers in theinventory. I choose to craft hammer.",
  "Thoughts: Im carpenter_0. I have 4 wood, 6 Stone, 1 hammer. I am in a coalition": "with both carpenters and miners. The resources available are wood and stone,both of which are nearby. The hammercraft event is right next to me, allowingme to craft a hammer. I currently have more than 1 wood and more than 1 stonein my inventory, which is enough to craft a hammer. Since I have one hammer and"
}