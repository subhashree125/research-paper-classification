{
  "Abstract": "In diffusion models, samples are generated through an iterative refinement process,requiring hundreds of sequential model evaluations. Several recent methods haveintroduced approximations (fewer discretization steps or distillation) to trade offspeed at the cost of sample quality. In contrast, we introduce Self-Refining Diffu-sion Samplers (SRDS) that retain sample quality and can improve latency at the costof additional parallel compute. We take inspiration from the Parareal algorithm, apopular numerical method for parallel-in-time integration of differential equations.In SRDS, a quick but rough estimate of a sample is first created and then iterativelyrefined in parallel through Parareal iterations. SRDS is not only guaranteed toaccurately solve the ODE and converge to the serial solution but also benefitsfrom parallelization across the diffusion trajectory, enabling batched inferenceand pipelining. As we demonstrate for pre-trained diffusion models, the earlyconvergence of this refinement procedure drastically reduces the number of stepsrequired to produce a sample, speeding up generation for instance by up to 1.7x ona 25-step StableDiffusion-v2 benchmark and up to 4.3x on longer trajectories.",
  "Introduction": "Deep generative models based on diffusion processes have showcased the capability to producehigh-fidelity samples in a wide-range of applications . From their origins in imageand audio generation , diffusion models have enabled robotic applications as well asscientific discovery via drug design . Despite this promise, sampling from diffusion models canstill be prohibitively slow. Early Denoising Diffusion Probabilistic Models required a thousandsequential model evaluations (steps), and state-of-the-art models such as StableDiffusion canstill require up to 100s of iterations for high-quality generations. This large number of samplingsteps leads to high-latencies associated with diffusion models, limiting applications such as real-timeimage or music editing and trajectory planning in robotics . As sampling involves solving an ordinary differential equation (ODE), a prominent body of research including works such as Denoising Diffusion Implicit Models (DDIM, ), Diffusion ExponentialIntegrator Sampler (DEIS, ), and DPM-Solver has tried to reduce the number of modelevaluations required by introducing various approximations. For example, progressive distillation requires re-training models to approximate the solution to the ODE at larger timesteps. However,such approaches trade-off speed at the cost of sample quality. In this work, we instead take an orthogonal approach: we focus on additional parallel compute andshow how this can be used to reduce latencies while still providing accurate solutions to the originalODE, thereby preserving sample quality. Recently Shih et al. leveraged a parallel-in-timeintegration method to introduce the first highly-parallelizable algorithm for diffusion model sampling.The presented ParaDiGMs algorithm builds on Picard iterations to perform denoising steps",
  "arXiv:2412.08292v1 [cs.LG] 11 Dec 2024": ": A visualization of the iterative refinement provided by the SRDS algorithm on a samplefrom StableDiffusion with the prompt a beautiful castle, matte painting. The initial coarse solve(left) via limited steps provides a rough estimate of the sample, which iteratively refined throughiterations of our algorithm. Early convergence is observed as the 3rd output nearly matches, a keyfeature that enables efficient generation. across the trajectory in parallel, leading to state-of-the-art sampling speeds on popular benchmarks. Despite the promising results, ParaDiGMs can be highly memory bound due to the use ofsliding window methods and also has a sequential convergence criteria requiring communication-expensive cumulative sums across devices to coordinate parallel-sampling. The method also haslimited controllability over the accuracy of the final solution, assuming convergence per step ratherthan in the final generation. It is also worth noting that in concurrent work, Tang et al. takea completely different approach to accelerating parallel sampling: special techniques in solvingtriangular nonlinear systems through fixed point iteration. Instead, in this paper, we turn to multiple-shooting methods from the parallel-in-time ODE integration literature and aim to improveparallelization of diffusion sampling by utilizing multiple resolutions across the time domain . Specifically, we present Self-Refining Diffusion Samplers (SRDS) that start with a quick but roughsolve of the diffusion trajectory, achieved by limiting the number of total steps taken (for instance,using a few-step DDIM solver). The trajectory can then be simulated to higher fidelity via a highly-parallel algorithm that updates the final generation iteratively until convergence. At a high level,each refinement step of SRDS partitions the current guess of the trajectory into blocks, and simulateseach of these blocks at the desired (high) resolution. The running guess for the overall trajectoryis then updated via a predictor-corrector mechanism based on the Parareal algorithm to accelerateconvergence. This iterative refinement allows us to efficiently interpolate in parallel between acoarse solution corresponding to a low-fidelity sample and an accurate solution corresponding to ahigh-fidelity sample. The key benefits of SRDS are three-fold: Approximation-Free: By design, SRDS computes an accurate solution to the reverse process (asdefined by the practitioners choice of diffusion solver), thereby maintaining high quality of samples.Importantly, as it is purely a sampling algorithm, it does so without incurring any retraining cost. Extensive Control and Compatibility: By serving as an efficient interpolation method betweenthe coarse and fine-grained solvers, SRDS provides the practitioner with flexible control of thetradeoff between sample quality and speed. For instance, one could first start with a rough solve(corresponding to, say, few-step DDIM). Then, if desired, one can add a budget-appropriate numberof parallel SRDS iterations (instead of sequential) to refine the obtained sample. Furthermore, SRDS iscompatible with most existing off-the-shelf solvers (such as Euler, Heun, DPM etc), thereby providingdirect benefits to virtually any diffusion workflow. Low Latency:Most importantly, we find that the number of iterations required by SRDS forconvergence is quite low, leading to drastic improvements in latency for sampling. We presentresults using the Self-Refining Diffusion Samplers on a wide range of benchmarks, starting withpixel-based image diffusion models and further exploring latent-methods where SRDS leads to a 1.7ximprovement in the sampling speed on the 25-step StableDiffusion-v2 benchmark and up to 4.3xon longer trajectories. 1 Through enabling faster sampling, SRDS aims to unlock capabilities forreal-time interaction with diffusion models.",
  "Background": "Diffusion models are a general class of generative models that rely on a noising procedure that convertsthe data distribution into noise via a series of latent variables updates. For the purposes of this work,we will consider the continuous-time generalization presented by Song and Denoising DiffusionImplicit Models that formulate sampling as solving the initial value problem characterized bythe probability flow ordinary differential equation (ODE):",
  "dt;x(t = 0) = x0 N(0, I)(1)": "where s(x, t) is a time-conditional prediction of the score function x log pt(x) from the diffusionmodel. To be consistent with prior work on parallelized diffusion sampling Shih et al. , we use areversed time index (from traditional notation) where x0 refers to pure Gaussian noise, and xT refersto the denoised image after T denoising steps.",
  "t=0h(x, t)dt(2)": "in order to produce a sample from the diffusion model. Common approaches discretize the timeinterval [0, T] into N pieces (t0=0, t1, t2, ..., tN=T) and solve a sequence of initial value problemsto yield an approximation (x0, x1, ..., xN=xT ) to the trajectory. Formally, a solver is a function F(xstart, tstart, tend) that propagates x from t = tstart with initialvalue xstart to t = tend. Solving the differential equation corresponds to approximating the solutionxT to the given initial value problem by a sequence of N solves:",
  "xi+1 = F (xi, ti, ti+1) i [0, N 1];given initial value x0(3)": "The choice of solver F dictates the sampling speed and accuracy of the solution. In practice, solverswhich are accurate are often slow (due to high number of evaluations of h), whereas solvers that arefast tend to have reduced accuracy. Initial works on diffusion models used the classical Euler methodas choice of F, and it can be expressed as:",
  "xi+1 = Feuler(xi, ti, ti+1) = xi + h(xi, ti) (ti+1 ti)(4)": "However, DDIM quickly became a popular choice of F for its improved efficiency. Other recentworks have tried to rely on approximations or leverage various ideas from the numerical methodsliterature to design solvers F that require fewer denoising steps. For instance, Diffusion ExponentialIntegrator Sampler (DEIS, ), and DPM-Solver exploit the special structure of the probabilityflow ODE to design special solvers where the linear part of the ODE is solved analytically and thenon-linear part is solved by incorporating ideas from exponential integrators in the numerical methodsliterature. Karras et al. leverage the Heuns second order method and demonstrate a favorabletradeoff between number of model evaluations and quality of generated samples for a small numberof denoising steps. In this work, SRDS presents an orthogonal improvement to these methods viaparallelization, and by default we will assume all our solvers to be DDIM.",
  "Self-Refining Diffusion Samplers": "Attempts to reduce the number of steps in diffusion samplers can provide speedups in samplegeneration , but unfortunately often lead to lower-sample quality. While low-frequencycomponents (in the Fourier sense) of the images may be well-established, the generations miss thehigh-frequency details that leads to good generations . To fix sample quality while maintainingthe latency benefits of reducing the number of steps, we turn to numerical methods introduced in theparallel-in-time integration literature where dynamics with different components having different rates : First iteration of the parareal algorithm to solve an example ODE. The black curve representsthe desired solution from the fine solver. The magenta dots indicate the running solution after oneiteration of predictor-corrector updates. Figure inspiration from Pentland et al. . of convergence has been extensively studied. Specifically, multiple-shooting and multigrid methodshave seen success in a wide range of domains from convection-diffusion equations to eigenvalueproblems by creating a rough but efficient solve of the prescribed differential equationthat can then be iteratively updated via a highly parallelizable simulation. One such algorithm Parareal serves as the backbone for our Self-Refining Diffusion Samplers that we describebelow.",
  "Parareal makes use of two solvers: solver F (called the fine solver) provides accurate solutions butis slow to evaluate, and G (called the coarse solver) provides rough solutions but is much quicker": "Parareal targets general purpose initial value problems of forms similar to Equation 1. Consider apartition (t0, t1, ..., tN = T) of the time axis [t0, T] into N intervals of equal width. Using the samesolver notation as above, the goal is to approximate the solution xN to the initial value problem thatwould be produced using a sequence of fine solves:xi+1 = F (xi, ti, ti+1) , i [0, N 1]The key insight of parareal is that we can first use the coarse solver G to quickly produce a roughtrajectory, and this rough solution can be iteratively refined using parallel calls to the fine solver F.",
  "where the notation x0i denotes the initial estimate of the trajectory from the coarse solver (orangecurve in )": "Parareal then proceeds in iterations until convergence, where each iteration corresponds to a refine-ment of the trajectory. At each iteration, we solve the differential equation in each of the N timeintervals at a higher resolution using the fine solver F, where the initial value for each interval isgiven by the estimate of the trajectory from the previous iteration. Crucially, these fine solves (bluein ) can be performed in parallel. Lastly, at the end of each iteration, we perform anothercoarse sequential solve through the trajectory (magenta in ) and incorporate the results of thefine solves into the running solution for the trajectory using a predictor-corrector method, where thecoarse solver predictions are corrected via the updates from the parallel fine solves. Formally,",
  "N-step DDIM solver 3 as our fine solver F. In other words, F(xi, ti, ti+1) is the result of a": "N-step DDIM solve propagating x from t = ti with initial value xi to t = ti+1. We pick 1-stepDDIM solver as our coarse solver G. That is, G(xi, ti, ti+1) denotes the result of the corresponding1-step DDIM solve propagating x from t = ti with initial value xi to t = ti+1 (\"step\" refers todenoising step involving an h evaluation).",
  "Ncoarse predictions in the trajectory is simulated at higher resolution with further": "N DDIM-iterationsin parallel, each with an effective time step corresponding to the original N-step discretization ofthe diffusion model. Iterative updates to the running solution then proceed in a manner equivalent toParareal updates until convergence, as measured by the change in the outputted generation. Our SRDSalgorithm is summarized in Algorithm 1.",
  "noting that this worst case guarantee is similar in spirit to Proposition 1 in and its generalization(Theorem 3.6 in )": "It is worth noting, however, that in practice we observe that the number of refinement iterationsrequired till convergence which is defined as difference in consecutive sample generations notexceeding a chosen threshold is much less than the worst case bound of N. This early conver-gence is critical to speedups from SRDS. The exact choice of threshold in line 13 of Algorithm 1, isa hyperparameter that is empirically chosen so as to avoid measurable degradation in sample quality.",
  "SRDS benefits from two key features to reduce latencies: batched inference and pipelining": "First, the fine solves that are used in order to refine the trajectories implementation-equivalentDDIM-steps, which means that they can be performed in a batched manner even for a single samplegeneration. This parallelization allows for a single sample generation to incur the benefits of batchedinference, introducing higher device utilization or device parallelism. Secondly, we observe that the dependency graph for SRDS enables pipelined parallelism. As outlinedin , we find that F (xpi , ti, ti+1) and G (xpi , ti, ti+1) both only depend on xpi . The tasks forcomputing Fxij, ti, ti+1and G (xpi , ti, ti+1) can be spawned as soon as xij is computed, withoutwaiting for the entire predictor-corrector mechanism to finish updating the SRDS solution for iterationi. This leads to an efficiently pipelined version of the algorithm, further speeding up the samplingprocess by a factor of two. See for an illustration of this pipelined algorithm with N = 16.Pipelining furthers the benefits of batched inference as the coarse solver is simply a DDIM-step witha larger time-step, so it can be batched with fine solves when applicable.",
  "Once again referring to the pipelined implementation of SRDS, it is easy to see that at any given timethere is at most one model evaluation corresponding to a coarse solve, and up to": "N parallel modelevaluations corresponding to the fine solves. It is worth contrasting this with the quadratically higherO(N) memory requirement of the full ParaDiGMs algorithm in , necessitating the use of slidingwindow tricks to reverse the process in a piece-wise fashion. It is finally worth noting that there is minimal inter-GPU communication in SRDS. In particular, atmost one sample is passed between adjacent GPUs in each SRDS iteration. Once again, it is worthcontrasting this with ParaDiGMs algorithm, which by its use of parallel prefix sum operationsto sync the solutions at each Picard iteration incurs greater GPU communication overhead. SeeAppendix D for more discussion.",
  "Experiments on Diffusion Image Generation": "To showcase the capabilities of the prescribed SRDS algorithm, we apply the diffusion sampler topretrained diffusion models and present the difference in sample time and quality to ensure thatapplied convergence criteria do not reduce generation metrics. We start with pixel-based diffusionbefore expanding experiments applied to latent methods such as StableDiffusion-v2. Across the rangeof tasks, we show consistent speedups while maintaining quality of sample generation. In this section, we perform an extensive comparison with ParaDiGMs as our baseline. Nonethe-less, we provide a high level empirical comparison to our concurrent work ParaTAA in AppendixE, where we demonstrate the superiority of SRDS.",
  "Pixel Diffusion - Image Generation": "We start with pixel-space diffusion models. In particular, we test our SRDS algorithm and demonstratecapabilities in performing diffusion directly on the pixel space of 128x128 LSUN Church andBedroom , 64x64 Imagenet , and 32x32 CIFAR using pretrained diffusion models ,which all use N = 1024 length diffusion trajectories. We measure the convergence via l1 norm in pixel space with values . We conservativelyset = 0.1, meaning that convergence occurs when on average each pixel in the generation differsby only 0.1 after a refinement step (see Appendix F for an ablation on choice of ). Through ourexperiments, we quantitatively showcase how the SRDS algorithm can provide signficant speedups ingeneration without degrading model quality (as measured by FID score on 5000 samples). Asseen in , SRDS remarkably converges in 4-6 iterations across all datasets; this corresponds toroughly 150 200 effective serial steps (counting all model evaluations simultaneously performed inparallel as one evaluation), which is only 15 20% of the serial steps required by a sequential solve : Evaluating FID score (lower is better) of SRDS on various datasets using 5000 samplesgenerated using a DDIM solver. Effective serial evals refers to the number of serial model evaluationstaken by the pipelined SRDS algorithm (counting all model evaluations simultaneously performed inparallel as one evaluation) . Total evals refers to the total number of model evaluations.",
  "(N = 1024). We clarify that effective serial evaluations is referred to as Parallel Iters in andSteps in": "While we are pretty conservative above in measuring convergence through distance in pixel space,we can also simply limit the number of SRDS iterations to 1 2 and achieve further speedups withoutmeasurable degradation in sample quality. See Appendix F for more details. It is once again worth noting that this improved latency from parallelization comes at the cost ofgreater number of total model evaluations compared to a regular sequential solver. However, thistradeoff enables the diffusion models for many other use cases such as real-time image or musicediting and trajectory planning in robotics. Moreover, we often empirically observe that SRDSprovides reasonable predictions within a single Parareal iteration; here, the total number of modelevaluations is only slightly larger than the serial approach (increasing from n to n + 2n). Lastly, itis also worth noting that many users are often agnostic to inference time GPU compute costs as theyare orders of magnitude lower than training compute costs anyway.",
  "Latent Diffusion - Image Generation": "Finally, we turn to latent diffusion models, in particular StableDiffusion-v2 , where evaluations ofthe CLIP score over 1000 random samples show how SRDS maintains sample quality while improvingthe number of parallel iterations required per sample, with summary metrics presented in . Asthe SRDS algorithm has small GPU overhead, we achieve measured wallclock time improvementswith a Diffusers compatible implementation . It is worth nothing that while we focus on DDIMhere (as in the rest of the writing), we show speedups by readily incorporating other solvers into SRDSin Appendix C. For the test bed of latent diffusion models, we explore the convergence properties of our SRDSalgorithm, with the average CLIP score plotted against the number of iterations in . Forshorter sequences of length 25 (left), the corresponding SRDS sampler converges after approximately3 iterations. However, for longer sequences of length 100 (right) the sampler has converged after asingle SRDS iteration, showcasing the capabilities of our algorithm improves with longer trajectories.",
  "DDIM96144.8810.31 (4.3x)275.2920.4814.30DDIM1969.172.85 (3.2x)29.455.083.42DDIM251.180.69 (1.7x)1.981.510.77": "significant speedups as seen in ; with some more engineering effort4, we can further pushtowards extracting the full potential of pipelining. However, as this already beats the baselines, thissufficiently demonstrates the benefits of SRDS5. Furthermore, for our main baseline ParaDiGMs, we also perform more extensive evaluation toevaluate both methods on equal hardware to more clearly demonstrate the benefits of SRDS. In , we demonstrate that SRDS consistently beats ParaDiGMS on wallclock speedups. Though theauthors of uses a convergence threshold of 1e 3, we show that SRDS can provide impressivespeedups even when compared to significantly relaxed ParaDiGMS thresholds of 1e 1.",
  "Recent literature on diffusion models has focused heavily on reducing the cost of sampling. Tech-niques such as higher order methods and exponential integrators have been proposed as": "4The suboptimality of the implementation arises from the use of a single device to coordinate the pipeline par-allelism and device transfers (arising as an artifact from torch.multiprocessing). A more complete implementationwould instead use ring-like communication between devices rather than wait on the coordinator.5We note that the number of denoising steps in the experiments is chosen to be perfect squares merely forconvenience. SRDS is general and applies to any number of denoising steps. : Sample generation from StableDiffusion-v2 with the SRDS algorithm with text promptsbased on examples from DrawBench. We plot the early converged SRDS figure (top) and the resultof the serial trajectory (bottom); the two rows are essentially indistinguishable, highlighting theapproximation-free nature of SRDS. strategies for reducing the model evaluations required in order to build high-quality samples withoutany additional training. When additional training is possible, other works have proposed distillation, quantization , and consistency as alternate objectives to further speed up samplegeneration. For the purposes of this paper, we view these approaches as orthogonal, as the resultantmodels could be simulated with SRDS for potential benefits from combining methods. As discussed throughout the paper, this work is most cloesly related to the ParaDiGMS samplermethod developed by Shih et al. for parallel sampling of diffusion models. The two works takea similar approach by building off popular parallel-in-time integration methods in order to achievelower latencies in simulation. In particular, ParaDiGMS builds on Picard iterations to converge ontrajectories; we, however, build on Parareal method that performs multiresolution along the time axisfor faster sampling. Parareal has been well-explored though with limited theoreticalguarantees only spanning certain cases such as the heat equation and Navier-Stokes equation ;our work is the first to apply this algorithm to diffusion models.",
  "Conclusion": "Limitations:Similar to previous iterations of parallel-in-time integration methods, SRDS make useof additional compute that can be used in parallel in exchange for faster latencies of sampling. That isto say, the total number of model evaluations in comparison to standard diffusion modelling increasesin exchange for lower latencies. The additional compute may be reasonable in applications such assmall-batch sampling where the additional cost can be hidden through better device utilization (e.g.sampling of a single image or trajectory in robotics). Alternatively, the responsiveness of real-timeimage editing may make parallel sampling an appealing option for cost-insensitive users. Future Directions:This work opens up a ton of interesting open questions for future exploration.Firstly, while fast convergence of parareal-style algorithms has only been proven for very limitedsettings, it will be extremely interesting to derive convergence guarantees specifically for the diffusionprocess. This has the potential to further our understanding of the nature of the ODE/SDE thatgoverns the reverse process. Another natural direction is to explore the effects of employing higherlevels of discretization and other multigrid methods such as F-cycles and W-cycles. As alluded to in.2, one could not only further study the optimal choice of second level of discretization, butalso consider novel schedules that involve partitioning the diffusion trajectory into intervals of varyingsizes. Lastly, it is worth highlighting that by serving a highly modular and interoperable framework,SRDS unlocks a vast landscape of interesting coarse/fine solver combinations. For instance, one canuse a DDIM solver to perform the parallel refinement steps, while using a progressively distilledmodel or consistency trajectory model as the coarse solver in SRDS.",
  "and Disclosure of Funding": "We thank the reviewers for their thoughtful feedback towards improving this paper. We also thankAryaman Arora, Harshit Joshi, Ken Liu, Rajeshwari Jadhav, Rohit Nema, and Yanzhe Zhang for theirhelpful discussions and support during various stages of the project. This project was funded in partby ARO (W911NF-21-1-0125), ONR (N00014-23-1-2159), and the CZ Biohub. Josh Abramson, Jonas Adler, Jack Dunger, Richard Evans, Tim Green, Alexander Pritzel, OlafRonneberger, Lindsay Willmore, Andrew J Ballard, Joshua Bambrick, et al. Accurate structureprediction of biomolecular interactions with alphafold 3. Nature, pages 13, 2024.",
  "Alex Krizhevsky. Learning multiple layers of features from tiny images. 2009": "Xiuyu Li, Yijiang Liu, Long Lian, Huanrui Yang, Zhen Dong, Daniel Kang, Shanghang Zhang,and Kurt Keutzer. Q-diffusion: Quantizing diffusion models. In Proceedings of the IEEE/CVFInternational Conference on Computer Vision, pages 1753517545, 2023. J.-L. Lions, Yvon Maday, and Gabriel Turinici. A \"parareal\" in time discretization of PDEs.Comptes Rendus de lAcadmie des Sciences - Series I - Mathematics, 332:661668, 2001. doi:10.1016/S0764-4442(00)01793-6. Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver:A fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances inNeural Information Processing Systems, 35:57755787, 2022.",
  "Oliver A. McBryan, Johannes Linden, A. Schuller, Karl Solchenbach, and K. Stuben. Multigridmethods on parahel computers - a survey on recent developments. 2014": "Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik Kingma, Stefano Ermon, Jonathan Ho,and Tim Salimans. On distillation of guided diffusion models. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition, pages 1429714306, 2023. Galina V. Muratova and Evgeniya M. Andreeva. Multigrid method for solving convection-diffusion problems with dominant convection. Journal of Computational and Applied Mathe-matics, 226(1):7783, 2009. ISSN 0377-0427. Special Issue: The First International Conferenceon Numerical Algebra and Scientific Computing (NASC06). Kamran Pentland, Massimiliano Tamborrino, Debasmita Samaddar, and Lynton C Appel.Stochastic parareal: An application of probabilistic methods to time-parallelization. SIAMJournal on Scientific Computing, 45(3):S82S102, 2022.",
  "mile Picard. Memoire sur la theorie des equations aux derivees partielles et la methode desapproximations successives. Journal de Mathmatiques pures et appliques, 6:145210, 1890": "Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjrn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVFconference on computer vision and pattern recognition, pages 1068410695, 2022. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton,Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al.Photorealistic text-to-image diffusion models with deep language understanding. Advances inneural information processing systems, 35:3647936494, 2022.",
  "Zhiwei Tang, Jiasheng Tang, Hao Luo, Fan Wang, and Tsung-Hui Chang. Accelerating parallelsampling of diffusion models. In Forty-first International Conference on Machine Learning,2024": "Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro Cuenca, Nathan Lambert, Kashif Rasul,Mishig Davaadorj, Dhruv Nair, Sayak Paul, William Berman, Yiyi Xu, Steven Liu, and ThomasWolf. Diffusers: State-of-the-art diffusion models. 2022. Joseph L Watson, David Juergens, Nathaniel R Bennett, Brian L Trippe, Jason Yim, Helen EEisenach, Woody Ahern, Andrew J Borst, Robert J Ragotte, Lukas F Milles, et al. De novodesign of protein structure and function with rfdiffusion. Nature, 620(7976):10891100, 2023. Xingyi Yang, Daquan Zhou, Jiashi Feng, and Xinchao Wang. Diffusion probabilistic modelmade slim. In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 2255222562, 2023. Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. Lsun:Construction of a large-scale image dataset using deep learning with humans in the loop. arXivpreprint arXiv:1506.03365, 2015.",
  "Proof. We show, by induction, that xpi converges in i iterations of SRDS for all i [0, N 1]": "Further, xpi = Fxp1i1 , ti1, tifor all p i, implying that the final sample indeed corresponds tothe desired sample from F. The base case of i = 0 follows trivially from the initialization (initialcondition). To prove the second base case of i = 1, notice that xp0 = x0 for all p, implying that",
  "as desired": "Proposition 2.[Worst-Case Sampling Latency] Ignoring GPU overhead, the worst case wall-clock time of generating a single sample through SRDS is no worse than that of generating a singlesample through sequential sampling. Proof. Consider the unit of time to be the time taken for one denoising step (or one model evaluation).Referring to the pipelined implementation of SRDS, it is easy to see via a straightforward inductiveargument that the",
  "N) denoising modelevaluations": "Proof. In the pipelined implementation of SRDS, it is easy to see that at any given timestep thereis at most one model evaluation corresponding to a coarse solve. Further, the number of parallelmodel evaluations corresponding to the fine solves is upper bounded by the coarse discretization(or the number of \"blocks\"), which is",
  "The choice of resolution for the coarse solve is not arbitrary. For practical implementations, since weuse the same denoiser (say, DDIM) for both the coarse and fine solves, we choose": "N as an optimalchoice in the runtime sense (assuming constant number of iterations till convergence6). At a highlevel, this choice stems from the fact that we want to balance out the time the it takes to run all thefine solves in parallel and the time it takes perform one set of sequential predictor-corrector stepsthrough the trajectory.",
  "N": "Proof. Let k denote the number of SRDS iterations until convergence, let denote the cost of onedenoising step or model evaluation, and let 1 < B < N denote the \"block-size\": that is, the secondscale of discretization. For the 1-step coarse solve, each SRDS iteration incurs a runtime cost of1 N",
  "CIncorporation of other Solvers": "It is worth emphasizing again that SRDS provides an orthogonal improvement when compared tothe other lines of research on accelerating diffusion model sampling. In particular, while the mainexperiments (and writing) were focused on DDIM, SRDS is compatible with the other solvers andthey can be readily incorporated into SRDS to speed up diffusion sampling. For example, below weshow that SRDS is directly compatible with other solvers such as DDPM (often requiring more stepsthan DDIM) and DPMSolver (often requiring fewer steps than DDIM) and can efficiently acceleratesampling in both cases. We demonstrate this on StableDiffusion in . We also highlight that theDiffuser-compatible implementation requires only minor modification to the arguments of the solver,suggesting that SRDS will also be easy to extend out-of-the-box to other methods that the communitydevelops.",
  "DDPM96144.689312.303.63xDDPM1969.03423.262.76xDPM Solver19610.30423.492.95xDPM Solver251.31150.881.48xDDIM1969.17423.302.77xDDIM251.18150.821.43x": "6It is possible that a different choice of B might actually be optimal by enabling better flow of informationdown the computation graph and thereby resulting in lower number of iterations till convergence k. However,in our experiments (as also noted in ), we observe that SRDS often converges for small k ,validating our empirical choice.",
  "For a T step denoising process, ParaDiGMS needs to perform T model evaluations in parallel withsubsequent computations needing information about all previous evaluations, while SRDS onlyrequires": "T parallel evaluations (which fits comfortably in GPU memory) and requires much lessercommunication between GPUs. While the prohibitively large memory requirement can be combatedwith a sliding window method, the significantly larger communication overhead remains becauseat every step of Paradigms, an AllReduce over all devices must be performed in order to calculateupdates to the sliding window. (For instance, even when ParaDiGMS reduces Eff. Serial Steps by20x, the obtained speedup is only 3.4x). This is in contrast to the independent fine-solves in pararealthat only need to transfer information for the coarse solve. Below in , we demonstrate how the minimal memory and communication overhead of SRDSshines through as we are able to achieve better device utilization as we increase the number ofavailable GPUs. The following experiment was performed on 40GB A100s and used a generous 1e-2threshold for ParaDiGMS.",
  "We demonstrate the superiority of SRDS to baselines ParaDiGMS and ParaTAA . Here, wedemonstrate the high-level superiority of SRDS solely by using the results published by the authors in () and ()": "In the table 7 below, we show that SRDS offers better wall-clock speedups (over sequential) insample generation time for StableDiffusion when compared to and . We clarify that thereported speedup for each method is with respect to sequential solve on the same machine that thecorresponding parallel method was evaluated. Our results are particularly impressive given that theauthors of used 8x 80GB A100s for the evaluation and the authors of used 8x 80GB A800for the same, while we (SRDS) only used 4x 40GB A100 for the evaluation due to computationalconstraints. (For interpretation purposes, recall that a sequential solve is not compute/memory boundand doesnt benefit significantly from additional GPU compute, whereas the parallel methods certainlydo!) We would also like to highlight the superiority of SRDS over the baselines in the regime of smallnumber of denoising steps (25) as being particularly impactful.",
  "The answer NA means that the paper has no limitation while the answer No means thatthe paper has limitations, but those are not discussed in the paper": "The authors are encouraged to create a separate \"Limitations\" section in their paper. The paper should point out any strong assumptions and how robust the results are toviolations of these assumptions (e.g., independence assumptions, noiseless settings,model well-specification, asymptotic approximations only holding locally). The authorsshould reflect on how these assumptions might be violated in practice and what theimplications would be. The authors should reflect on the scope of the claims made, e.g., if the approach wasonly tested on a few datasets or with a few runs. In general, empirical results oftendepend on implicit assumptions, which should be articulated. The authors should reflect on the factors that influence the performance of the approach.For example, a facial recognition algorithm may perform poorly when image resolutionis low or images are taken in low lighting. Or a speech-to-text system might not beused reliably to provide closed captions for online lectures because it fails to handletechnical jargon.",
  "If applicable, the authors should discuss possible limitations of their approach toaddress problems of privacy and fairness": "While the authors might fear that complete honesty about limitations might be used byreviewers as grounds for rejection, a worse outcome might be that reviewers discoverlimitations that arent acknowledged in the paper. The authors should use their bestjudgment and recognize that individual actions in favor of transparency play an impor-tant role in developing norms that preserve the integrity of the community. Reviewerswill be specifically instructed to not penalize honesty concerning limitations.",
  "If the contribution is a dataset and/or model, the authors should describe the steps takento make their results reproducible or verifiable": "Depending on the contribution, reproducibility can be accomplished in various ways.For example, if the contribution is a novel architecture, describing the architecture fullymight suffice, or if the contribution is a specific model and empirical evaluation, it maybe necessary to either make it possible for others to replicate the model with the samedataset, or provide access to the model. In general. releasing code and data is oftenone good way to accomplish this, but reproducibility can also be provided via detailedinstructions for how to replicate the results, access to a hosted model (e.g., in the caseof a large language model), releasing of a model checkpoint, or other means that areappropriate to the research performed. While NeurIPS does not require releasing code, the conference does require all submis-sions to provide some reasonable avenue for reproducibility, which may depend on thenature of the contribution. For example(a) If the contribution is primarily a new algorithm, the paper should make it clear howto reproduce that algorithm.",
  "(b) If the contribution is primarily a new model architecture, the paper should describethe architecture clearly and fully": "(c) If the contribution is a new model (e.g., a large language model), then there shouldeither be a way to access this model for reproducing the results or a way to reproducethe model (e.g., with an open-source dataset or instructions for how to constructthe dataset). (d) We recognize that reproducibility may be tricky in some cases, in which caseauthors are welcome to describe the particular way they provide for reproducibility.In the case of closed-source models, it may be that access to the model is limited insome way (e.g., to registered users), but it should be possible for other researchersto have some path to reproducing or verifying the results.",
  "Guidelines:": "The answer NA means that the paper poses no such risks. Released models that have a high risk for misuse or dual-use should be released withnecessary safeguards to allow for controlled use of the model, for example by requiringthat users adhere to usage guidelines or restrictions to access the model or implementingsafety filters.",
  ". Experiments Compute Resources": "Question: For each experiment, does the paper provide sufficient information on the com-puter resources (type of compute workers, memory, time of execution) needed to reproducethe experiments?Answer: [Yes]Justification: Computer resource details are provided in .Guidelines: The answer NA means that the paper does not include experiments. The paper should indicate the type of compute workers CPU or GPU, internal cluster,or cloud provider, including relevant memory and storage.",
  ". Code Of Ethics": "Question: Does the research conducted in the paper conform, in every respect, with theNeurIPS Code of Ethics [Yes]Justification: The research presented conforms to the NeurIPS Code of Ethics includingaspects such as disclosing essential elements for reproducibility.Guidelines: The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. If the authors answer No, they should explain the special circumstances that require adeviation from the Code of Ethics.",
  ". Broader Impacts": "Question: Does the paper discuss both potential positive societal impacts and negativesocietal impacts of the work performed?Answer: [NA]Justification: The paper presents foundational research: a novel fast sampling algorithm fordiffusion models in an attempt unlock capabilities for real-time interaction with diffusionmodels. While there could be potential positive and negative societal impacts of this workstemming from applications that use this (say, real-time generation of deepfakes), we believeit is not a direct consequence of this work. The answer NA means that there is no societal impact of the work performed. If the authors answer NA or No, they should explain why their work has no societalimpact or why the paper does not address societal impact. Examples of negative societal impacts include potential malicious or unintended uses(e.g., disinformation, generating fake profiles, surveillance), fairness considerations(e.g., deployment of technologies that could make decisions that unfairly impact specificgroups), privacy considerations, and security considerations. The conference expects that many papers will be foundational research and not tiedto particular applications, let alone deployments. However, if there is a direct path toany negative applications, the authors should point it out. For example, it is legitimateto point out that an improvement in the quality of generative models could be used togenerate deepfakes for disinformation. On the other hand, it is not needed to point outthat a generic algorithm for optimizing neural networks could enable people to trainmodels that generate Deepfakes faster. The authors should consider possible harms that could arise when the technology isbeing used as intended and functioning correctly, harms that could arise when thetechnology is being used as intended but gives incorrect results, and harms followingfrom (intentional or unintentional) misuse of the technology. If there are negative societal impacts, the authors could also discuss possible mitigationstrategies (e.g., gated release of models, providing defenses in addition to attacks,mechanisms for monitoring misuse, mechanisms to monitor how a system learns fromfeedback over time, improving the efficiency and accessibility of ML).",
  "The answer NA means that the paper does not involve crowdsourcing nor research withhuman subjects": "Depending on the country in which research is conducted, IRB approval (or equivalent)may be required for any human subjects research. If you obtained IRB approval, youshould clearly state this in the paper. We recognize that the procedures for this may vary significantly between institutionsand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and theguidelines for their institution."
}