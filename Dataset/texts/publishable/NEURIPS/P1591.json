{
  "Abstract": "Neural Collapse (NC) is a recently observed phenomenon in neural networks thatcharacterises the solution space of the final classifier layer when trained until zerotraining loss. Specifically, NC suggests that the final classifier layer converges to aSimplex Equiangular Tight Frame (ETF), which maximally separates the weightscorresponding to each class. By duality, the penultimate layer feature means alsoconverge to the same simplex ETF. Since this simple symmetric structure is optimal,our idea is to utilise this property to improve convergence speed. Specifically,we introduce the notion of nearest simplex ETF geometry for the penultimatelayer features at any given training iteration, by formulating it as a Riemannianoptimisation. Then, at each iteration, the classifier weights are implicitly set tothe nearest simplex ETF by solving this inner-optimisation, which is encapsulatedwithin a declarative node to allow backpropagation. Our experiments on syntheticand real-world architectures for classification tasks demonstrate that our approachaccelerates convergence and enhances training stability1.",
  "Introduction": "While modern deep neural networks (DNNs) have demonstrated remarkable success in solvingdiverse machine learning problems , the fundamental mechanisms underlying theirtraining process remain elusive. In recent years, considerable research efforts have focused ondelineating the optimisation trajectory and characterising the solution space resulting from theoptimisation process in training neural networks . One such finding is that gradientdescent algorithms, when combined with certain loss functions, introduce an implicit bias that oftenfavours max-margin solutions, influencing the learned representations and decision boundaries. . In this vein, Neural Collapse (NC) is a recently observed phenomenon in neural networks thatcharacterises the solution space of the final classifier layer in both balanced and imbalanced dataset settings . Specifically, NC suggests that the final classifierlayer converges to a Simplex Equiangular Tight Frame (ETF), which maximally separates the weightscorresponding to each class, and by duality, the penultimate layer feature means converge to theclassifier weights, i.e., to the simplex ETF (formal definitions are provided in Appendix A). Thissimple, symmetric structure is shown to be the only set of optimal solutions for a variety of lossfunctions when the features are also assumed to be free parameters, i.e., Unconstrained Feature",
  "Models (UFMs) . Nevertheless, even in realistic large-scale deep networks,this phenomenon is observed when trained to convergence, even after attaining zero training error": "Since we can characterise the optimal solution space for the classifier layer, a natural extensionis to leverage the simplex ETF structure of the classifier weights to improve training. To this end,researchers have tried fixing the classifier weights to a canonical simplex ETF, effectively reducingthe number of trainable parameters . However, in practice, this approach does not improve theconvergence speed as the backbone network still needs to do the heavy lifting of matching featuremeans to the chosen fixed simplex ETF. In this work, we introduce a mechanism for finding the nearest simplex ETF to the features at anygiven training iteration. Specifically, the nearest simplex ETF is determined by solving a Riemannianoptimisation problem. Therefore, our classifier weights are dynamically updated based on thepenultimate layer feature means at each iteration, i.e., implicitly defined rather than trained usinggradient descent. Additionally, by constructing this inner-optimisation problem as a deep declarativenode , we allow gradients to propagate through the Riemannian optimisation facilitating end-to-end learning. Our whole framework significantly speeds up convergence to a NC solution comparedto the fixed simplex ETF and conventional learnable classifier approaches. We demonstrate theeffectiveness of our approach on synthetic UFMs and standard image classification experiments.",
  "Our main contributions are as follows:": "1. We introduce the notion of the nearest simplex ETF geometry given the penultimate layerfeatures. Instead of selecting a predetermined simplex ETF (canonical or random), weimplicitly fix the classifier as the solution to a Riemannian optimisation problem. 2. To establish end-to-end learning, we encapsulate the Riemannian optimisation problem ofdetermining the nearest simplex ETF geometry within a declarative node. This allows forefficient backpropagation throughout the network. 3. We demonstrate that our method achieves an optimal neural collapse solution more rapidlycompared to fixed simplex ETF methods or conventional training approaches, where alearned linear classifier is employed. Additionally, our method ensures training stability bymarkedly reducing variance in network performance.",
  "Related Work": "Neural Collapse and Simplex ETFs.Zhu et al. proposed fixing classifier weights to asimplex ETF, reducing parameters while maintaining performance. Simplex ETFs effectively tackleimbalanced learning, as demonstrated by Yang et al. , where they fix the target classifier to anarbitrary simplex ETF, relying on the networks over-parameterisation to adapt. Similarly, Yanget al. addressed class incremental learning by fixing the target classifier to a simplex ETF.They advocate adjusting prototype means towards the simplex ETF using a convex combination,smoothly guiding backbone features into the targeted simplex ETF. However, these methods didnot yield any benefits regarding convergence speed. The work most relevant to ours is that ofPeifeng et al. , who argued about the significance of feature directions, particularly in long-tailedlearning scenarios. They compared their method against a fixed simplex ETF target, formulatingtheir problem to enable the network to learn feature direction through a rotation matrix. Additionally,they efficiently addressed their optimisation using trivialisation techniques . However, theydid not demonstrate any improvements in convergence speed over the fixed simplex ETF, achievingonly a minimal increase in test accuracy. Fixing a classifier is not a recent concept, as it has beenproposed prior to the emergence of neural collapse . Most notably, Pernici et al. demonstrated improved convergence speed by fixing the classifier to a simplex structure only onImageNet while maintaining comparable performance on smaller-scale datasets. In contrast, ourmethod shows superior convergence speed compared to both a fixed simplex ETF and a learnedclassifier across both small and large-scale datasets. Optimisation on Smooth Manifolds.Our optimisation problem involves orthogonality constraints,characterised by the Stiefel manifold . Due to the nonlinearity of these constraints, efficientlysolving such problems requires leveraging Riemannian geometry . A multitude of works arededicated to solving such problems by either transforming existing classical optimisation techniquesinto Riemannian equivalent algorithms or by carefully designing penalty functions",
  "to address equivalent unconstrained problems . In our approach, we opt for a retraction-basedRiemannian optimisation algorithm to optimally handle orthogonality constraints": "Implicit Differentiable Optimisation.In a neural network setting, end-to-end architectures arecommonplace. To backpropagate solutions to optimisation problems, we rely on machinery fromimplicit differentiation. Pioneering works demonstrated efficient gradient backpropagationwhen dealing with solutions of convex optimisation problems. This concept was independentlyintroduced as a generalised version by Gould et al. to encompass any twice-differentiableoptimisation problem. A key advantage of Deep Declarative Networks (DDNs) lies in their abilityto efficiently solve problems at any scale by leveraging the problems underlying structure .Our setting involves utilising an equality-constrained declarative node to efficiently backpropagatethrough the network.",
  ".(1)": "Here, R+ denotes an arbitrary scale factor, 1C is the C-dimensional vector of ones, andU RdC (with d C) represents a semi-orthogonal matrix (U U = IC). Note that there aremany simplex ETFs in RC as the rotation U varies, and M is rank-deficient. Additionally, thestandard simplex ETF with unit Frobenius norm is defined as: M =1C1IC 1",
  "C 1C1C": "Mean of Features.Consider a classification dataset D = {(xi, yi) | i = 1, . . . , N} where the dataxi X and labels yi Y = {1, . . . , C}. Suppose, nc is the number of samples correspond to labelc, then Cc=1 nc = N. Let us consider a scenario where we have a collection of features defined as,",
  "H [hc,i : 1 c C, 1 i nc] RdN .(2)": "Here, each feature may originate from a nonlinear compound mapping of input data through aneural network, denoted as, hyi,i = (xi) for the data sample (xi, yi). Now, for the final layer,our decision variables (weights and biases) are represented as W [w1, . . . , wC] RCd, andb RC, and the logits for the i-th sample is computed as,",
  "Nearest Simplex ETF through Riemannian Optimisation": "Once we obtain the feature means, our objective is to calculate the nearest simplex ETF based onthese means and subsequently adjust the classifier weights W to align with this particular simplexETF. The rationale is to identify and establish a simplex ETF that closely corresponds to the featuremeans at any given iteration. This approach aims to expedite convergence during the training processby providing the algorithm with a starting point that is closer to an optimal solution rather thanrequiring it to learn a simplex ETF direction or converge towards an arbitrary one.",
  "Proximal Problem": "The solution to the Riemannian optimisation problem, denoted as U , is not unique since a componentof U lies in the null space of M. As simplex ETFs reside in (C 1)-dimensional space, the matrixM is rank-one deficient. Consequently, we are faced with a family of solutions, leading to challengesin training stability, as we may oscillate between multiple simplex ETF directions. We addressthis issue by introducing a proximal term to the problems objective function. This guarantees theuniqueness of the solution and stabilises the training process, ensuring that our problem converges toa solution closer to the previous one.",
  "F .(7)": "Here, Uprox represents the proximal target simplex ETF direction, and > 0 serves as the proximalcoefficient, handling the trade-off between achieving the optimal solutions proximity to the featuremeans and its proximity to a given simplex ETF direction. In fact, one can perceive our problemformulation in Equation 7 as a generalisation to a predetermined fixed simplex ETF solution. This isevident when considering that if we significantly increase , the optimal direction U would convergetowards the fixed proximal direction Uprox.",
  "(8)": "where (xi, U ) = MU (hi hG) with hi = (xi). Here, denotes the logits, where theclassifier weights are set as W = MU , and the bias is set to b = MU hG to account for featurecentring. Furthermore, M is the standard simplex ETF, M is its normalised version, and H is thenormalised centred feature matrix. The temperature parameter > 0 controls the lower bound of thecross-entropy loss when dealing with normalised features, as defined in [69, Theorem 1].",
  "Uinit, Uprox": ": Schematic of our proposed architecture for optimising towards the nearest simplex ETF.The classifier weights W = U M are an implicit function of the CNN features H. Note that theparameters of the CNN are updated via two gradient paths from the loss function L, a direct path(top) and an indirect path through U (bottom). changes. First, rather than directly optimising the problem of finding the nearest simplex ETFgeometry concerning the feature means of the mini-batch, we introduce an exponential movingaverage operation during the computation of the feature means. This operation accumulates statisticsand enhances training stability throughout iterations. Formally, at time step t, we have the followingequation, where R represents the smoothing factor:",
  "Ht = Hbatch + (1 ) Ht1 .(9)": "Second, we employ stratified batch sampling to guarantee that all class labels are represented in themini-batch. This ensures that we avoid degenerate solutions when finding the nearest simplex ETFgeometry, as our optimisation problem requires input feature means for all C classes. In cases wherethe number of classes exceeds the chosen batch size, we compute the per-class feature mean for theclass labels present in the given batch. For the remaining class labels, we set their feature mean as theglobal mean of the batch. We repeat this process for each training iteration until we have sampledexamples belonging to the missing class labels. At that point, we update the feature mean of thosemissing class labels with the new feature statistics. We reserve this method only for cases where thebatch size is smaller than the number of labels since it can introduce instability during early iterations.",
  "Deep Declarative Layer": "We can backpropagate through the Riemannian optimisation problem to update the feature meansusing a declarative node . Then, the features are updated from both the loss and the feature meansthrough auto-differentiation. The motivation for developing the DDN layer lies in recognising that,despite the presence of a proximal term, abrupt and sudden changes to the classifier may occur as thefeatures are updated. These changes can pose challenges for backpropagation, potentially disruptingthe stability and convergence of the training process. Incorporating an additional stream of gradientsthrough the feature means to account for such changes, as depicted in , assists in stabilisingthe feature updates during backpropagation. To efficiently backpropagate through the optimisation problem, we employ techniques describedin Gould et al. utilising the implicit function theorem to compute the gradients. In our case, wehave a scalar objective function f : RdC R, and a matrix constraint function J : RdC RCC.Since we have matrix variables, we use vectorisation techniques to avoid numerically dealingwith tensor gradients. More specifically, we have the following:Proposition 1 (Following directly from Proposition 4.5 in Gould et al. ). Consider the opti-misation problem in Equation 7. Assume that the solution exists and that the objective function fand the constraint function J are twice differentiable in the neighbourhood of the solution. If therank(A) = C(C+1)",
  "Experiments": "In our experiments, we perform feature normalisation onto a hypersphere, a common practice intraining neural networks, which improves representation and enhances model performance . We find that combining classifier weight normalisation with featurenormalisation accelerates convergence . Given that simplex ETFs are inherently normalised,we include classifier weight normalisation in our standard training procedure to ensure fair methodcomparisons. Experimental Setup.In this study, we conduct experiments on three model variants. First, thestandard method involves training a model with learnable classifier weights, following conventionalpractice. Second, in the fixed ETF method, we set the classifier to a predefined simplex ETF. Inall experiments, we choose the simplex ETF with canonical direction. In Appendix C, we alsoinclude additional experiments for fixed simplex ETFs with random directions generated from a Haarmeasure . Last, our implicit ETF method, where we set the classifier weights on-the-fly as thesimplex ETF closest to the current feature means. We repeat experiments on each method five times with distinct random seeds and report the medianvalues alongside their respective ranges. For reproducibility and to streamline hyperparameter tuning,we employed Automatic Gradient Descent (AGD) . Following the authors recommendation, weset the gain/momentum parameter to 10 to expedite convergence, aligning it with other widely usedoptimisers like Adam and SGD. Our experiments on real datasets run for 200 epochs with batchsize 256; for the UFM analysis, we run 2000 iterations. Our method underwent rigorous evaluation across various UFM sizes and real model architecturestrained on actual datasets, including CIFAR10 , CIFAR100 , STL10 , and ImageNet-1000 , implemented on ResNet and VGG architectures. More specifically, we trainedCIFAR10 on ResNet18 and VGG13, CIFAR100 and STL10 on ResNet50 and VGG13, and ImageNet-1000 on ResNet50. The input images were preprocessed pixel-wise by subtracting the mean anddividing by the standard deviation. Additionally, standard data augmentation techniques were applied,including random horizontal flips, rotations, and crops. All experiments were conducted using NvidiaRTX3090 and A100 GPUs. Hyperparameter Selection and Riemannian Initialisation Schemes.We solve the Riemannianoptimisation problem defined in Equation 7 using a Riemannian Trust-Region method frompyManopt . We maintain a proximal coefficient set to 103 consistently across all experiments.It is worth mentioning that algorithm convergence is robust to the precise value of . In our problem,determining values for Uinit and Uprox is crucial. We explored several methods to initialise theseparameters. One approach involved setting both towards the canonical simplex ETF direction. Thismeans initialising them as a partial orthogonal matrix where the first C rows and columns form anidentity matrix while the remaining d C rows are filled with zeros. Another approach is to initialiseboth of them as random orthogonal matrices from classical compact groups, selected according to aHaar measure . In the end, the approach that yielded the most stable results at initialisation was",
  ": UFM-10 results. In all plots, the x-axis represents the number of epochs, except for plot (c),where the x-axis denotes the number of training examples": "to employ either of the aforementioned initialisation methods to solve the original problem withoutthe proximal term in Equation 6. We then used the obtained U to initialise both Uinit and Uprox forthe problem in Equation 7. This process was carried out only for the first gradient update of the firstepoch. In subsequent iterations, we update these parameters to the U obtained from the previoustime step. Importantly, the proximal term is held fixed during each Riemannian optimisation. Regarding the calculation of the exponential moving average of the feature means, we have foundthat employing a decay policy on the smoothing factor yields optimal results. Specifically, we set = 2/(T + 1), where T represents the number of iterations. Additionally, we include a thresholdingvalue of 104, such that if falls below this threshold, we fix to be equal to the threshold. Thisprecaution ensures that does not diminish throughout the iterations, thereby guaranteeing that thenewly calculated feature means contribute sufficient statistics to the exponential moving average. Finally, in our experiments, we set the temperature parameter to five. This choice aligns with thefindings discussed by Yaras et al. , highlighting the influence of the temperature parameter valueon the extent of neural collapse statistics with normalised features.",
  "Unconstrained Feature Models (UFMs).Our experiments on UFMs, which provide a controlledsetting for evaluating the effectiveness of our method, are done using the following configurations:": "UFM-10: a 10-class UFM containing 1000 features with a dimension of 512. UFM-100: a 100-class UFM containing 5000 features with a dimension of 1024. UFM-200: a 200-class UFM containing 5000 features, with a dimension of 1024. UFM-1000: a 1000-class UFM containing 10000 features, with a dimension of 1024. Results.We present the results for the synthetic UFM-10 case in . The CE loss plotdemonstrates that fixing the classifier weights to a simplex ETF achieves the theoretical lower boundof Yaras et al. [69, Thm. 1], indicating the attainment of a globally optimal solution. We also visualisethe average cosine margin per epoch and the cosine margin distributions of each example at the endof training, defined in Zhou et al. . The neural collapse metrics, NC1 and NC3, which measurethe features within-class variability, and the self-duality alignment between the feature means andthe classifier weights , are also plotted. Last, we depict the absolute difference of the classifierand feature means norms to illustrate their convergence towards equinorms, as described in Papyanet al. . A comprehensive description of the metrics can be found in Appendix A. Collectively, theplots indicate the superior performance of our method in achieving a neural collapse (NC) solution",
  "CIFAR100 ResNet50 58.47 59.653.963.93 65.261.172.15 74.170.195.87 98.694.791.34 92.190.496.96 97.396.2VGG1382.00 84.080.581.14 81.976.088.39 89.486.999.34 99.699.294.55 95.392.598.92 99.098.8": "STL10ResNet50 83.86 90.784.586.76 86.877.893.54 95.391.399.42 99.999.098.38 99.398.199.72 99.998.0VGG1382.66 90.773.783.60 85.165.690.14 93.569.2100.0 10010099.92 99.999.8100.0 100100ImageNetResNet50 58.35 59.158.070.44 70.769.574.09 74.573.877.20 77.376.583.09 83.683.188.01 88.587.5 faster than other approaches. In , we demonstrate under the UFM setting that as we increasethe number of classes, our method maintains constant performance and converges at the same rate,while the fixed ETF and the standard approach require more time to reach the interpolation threshold. Numerical results for the top-1 train and test accuracy are reported in Tables 1 and 2, respectively.The results are provided for snapshots taken at epoch 50 and epoch 200. It is evident that ourmethod achieves a faster convergence speed compared to the competitive methods while ultimatelyconverging to the same performance level. Additionally, it is noteworthy that our method exhibitsthe smallest degree of variability across different runs, as indicated by the range values provided.Finally, in , we present qualitative results that confirm our solutions ability to converge muchfaster and reach peak performance earlier than the standard and fixed ETF methods on ImageNet.Its important to note that the standard method with AGD is reported to converge to the same testingaccuracy (65.5%) at epoch 350, as shown in Bernstein et al. [6, ]. At epoch 200, the authorsexhibit a testing accuracy of approximately 51%. Since we have increased the gain parameter onAGD compared to the results reported in the original paper, we report a final 60.67% testing accuracyfor the standard method, whereas our method reaches peak convergence at approximately epoch80. We note that the ImageNet results reported in Tables 1 and 2, as well as , are generatedsolely by solving the Riemannian optimisation problem without considering its gradient stream onthe feature updates, due to computational constraints. We discuss the computational requirements ofour method in . We also present qualitative results for all the other datasets and architecturesin Appendix C.",
  "Discussion: Limitations and Future Directions": "Our method involves two gradient streams updating the features, as depicted in . Interestingly,empirical observations on small-scale datasets (see ) indicate that even without the back-propagation through the DDN layer, the performance remains comparable, rendering the gradientcalculation of the DDN layer optional. In c, we observe a strong impact of the DDN layergradient on the atomic feature level, with more features reaching the theoretical simplex ETF marginby the end of training. To reach a consensus on the exact effect of the DDN gradient on the learning",
  ": ImageNet results on ResNet-50. In all plots, the x-axis represents the number of epochs,except for plot (c), where the x-axis denotes the number of training examples": "process, further experiments on large-scale datasets are needed. However, on large-scale datasetswith large d and C, such as ImageNet, computing the backward pass of the Riemannian optimisationis challenging due to the memory inefficiency of the current implementation of DDN gradients. Thislimitation is an area we aim to address in future work. Note that in all other experiments, we use thefull gradient computations, including both direct and indirect components, through the DDN layer.We summarise the GPU memory requirements for each method across various datasets in . Our discussion so far has focused on convergence speed in terms of the number of epochs requiredfor the network to converge. However, it is also important to consider the time required per epoch. Inour case, as training progresses, the time taken by the Riemannian optimization quickly becomesalmost negligible compared to the networks total forward pass time, while it approaches the standardand fixed ETF training forward times, as shown in a. However, DDN gradient computationincreases considerably when the feature dimension d and the number of classes C increase and startsto dominate the runtime for large datasets such as ImageNet. Nevertheless, for ImageNet, we do notcompute the DDN gradients and still outperform other methods. We plan to explore ways to expeditethe DDN forward and backward pass in future work.",
  "(b) Forward and backward times in (log) millisecs": ": CIFAR10 computational cost results on ResNet-18. In (a), we plot the forward pass timefor each method. For the implicit ETF method, which has dynamic computation times, we alsoinclude the mean and median time values. In (b), we plot the computational cost for each forwardand backward pass across methods. For the implicit ETF forward pass, we have taken its mediantime. The notation is as follows: S/F = Standard Forward Pass, S/B = Standard Backward Pass, F/F =Fixed ETF Forward Pass, F/B = Fixed ETF Backward Pass, I/F = Implicit ETF Forward Pass, and I/B= Implicit ETF Backward Pass.",
  "to learn it through gradient descent. Our method involves solving a Riemannian optimisation problemfacilitated by a deep declarative node, enabling backpropagation through this process": "We demonstrated that our approach enhances convergence speed across various datasets and architec-tures while also reducing variability stemming from different random initialisations. By defining theoptimal structure of the classifier and efficiently leveraging its rotation invariance property to find theone closest to the backbone features, we anticipate that our method will facilitate the creation of newarchitectures and the utilisation of new datasets without necessitating specific learning or tuning ofthe classifiers structure.",
  "Lingling He Changqing Xu and Zerong Lin. Commutation matrices and commutation ten-sors. Linear and Multilinear Algebra, 68(9):17211742, 2020. doi: 10.1080/03081087.2018.1556242": "Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsuper-vised feature learning. In Geoffrey Gordon, David Dunson, and Miroslav Dudk (eds.), Proceed-ings of the Fourteenth International Conference on Artificial Intelligence and Statistics, vol-ume 15 of Proceedings of Machine Learning Research, pp. 215223, Fort Lauderdale, FL, USA,1113 Apr 2011. PMLR. URL Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scalehierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009.IEEE Conference on, pp. 248255. IEEE, 2009. URL Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angularmargin loss for deep face recognition. In Proceedings of the IEEE/CVF conference on computervision and pattern recognition, pp. 46904699, 2019.",
  "Ian J. Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, Cambridge,MA, USA, 2016": "S. Gould, R. Hartley, and D. Campbell. Deep declarative networks. IEEE Transactions onPattern Analysis & Machine Intelligence, 44(08):39884004, aug 2022. ISSN 1939-3539. doi:10.1109/TPAMI.2021.3059462. Stephen Gould, Basura Fernando, Anoop Cherian, Peter Anderson, Rodrigo Santa Cruz, andEdison Guo. On differentiating parameterized argmin and argmax problems with application tobi-level optimization. arXiv preprint arXiv:1607.05447, 2016. Stephen Gould, Dylan Campbell, Itzik Ben-Shabat, Chamin Hewa Koneputugodage, and ZhiweiXu. Exploiting problem structure in deep declarative networks: Two case studies. arXiv preprintarXiv:2202.12404, 2022.",
  "Wenlong Ji, Yiping Lu, Yiliang Zhang, Zhun Deng, and Weijie J Su. An unconstrainedlayer-peeled perspective on neural collapse. arXiv preprint arXiv:2110.02796, 2021": "Ziwei Ji, Miroslav Dudk, Robert E. Schapire, and Matus Telgarsky. Gradient descent followsthe regularization path for general losses. In Jacob Abernethy and Shivani Agarwal (eds.),Proceedings of Thirty Third Conference on Learning Theory, volume 125 of Proceedingsof Machine Learning Research, pp. 21092136. PMLR, 0912 Jul 2020. URL John M. Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ron-neberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin dek, Anna Potapenko, AlexBridgland, Clemens Meyer, Simon A A Kohl, Andy Ballard, Andrew Cowie, BernardinoRomera-Paredes, Stanislav Nikolov, Rishub Jain, Jonas Adler, Trevor Back, Stig Petersen,David Reiman, Ellen Clancy, Michal Zielinski, Martin Steinegger, Michalina Pacholska, Tamas Berghammer, Sebastian Bodenstein, David Silver, Oriol Vinyals, Andrew W. Senior, KorayKavukcuoglu, Pushmeet Kohli, and Demis Hassabis. Highly accurate protein structure predic-tion with alphafold. Nature, 596:583 589, 2021. URL Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, AaronMaschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. Advances in neuralinformation processing systems, 33:1866118673, 2020.",
  "Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias:On the role of implicit regularization in deep learning. arXiv preprint arXiv:1412.6614, 2014": "Vardan Papyan, X. Y. Han, and David L. Donoho. Prevalence of neural collapse during theterminal phase of deep learning training. Proceedings of the National Academy of Science, 117(40):2465224663, October 2020. doi: 10.1073/pnas.2015509117. Gao Peifeng, Qianqian Xu, Peisong Wen, Zhiyong Yang, Huiyang Shao, and Qingming Huang.Feature directions matter: Long-tailed learning via rotated balanced representation. In AndreasKrause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and JonathanScarlett (eds.), Proceedings of the 40th International Conference on Machine Learning, volume202 of Proceedings of Machine Learning Research, pp. 2754227563. PMLR, 2329 Jul 2023.URL",
  "Daniel Soudry, Elad Hoffer, and Nathan Srebro. The implicit bias of gradient descent onseparable data. In International Conference on Learning Representations, 2018. URL": "Korawat Tanwisuth, Xinjie Fan, Huangjie Zheng, Shujian Zhang, Hao Zhang, Bo Chen, andMingyuan Zhou. A prototype-oriented framework for unsupervised domain adaptation. Ad-vances in Neural Information Processing Systems, 34:1719417208, 2021. Christos Thrampoulidis, Ganesh Ramachandra Kini, Vala Vakilian, and Tina Behnia. Imbalancetrouble: Revisiting neural-collapse geometry. Advances in Neural Information ProcessingSystems, 35:2722527238, 2022.",
  "J. Townsend, N. Koep, and S. Weichwald. PyManopt: a Python toolbox for optimization onmanifolds using automatic differentiation. Journal of Machine Learning Research, 17(137):15, 2016. URL": "Hao Wang, Yitong Wang, Zheng Zhou, Xing Ji, Dihong Gong, Jingchao Zhou, Zhifeng Li, andWei Liu. Cosface: Large margin cosine loss for deep face recognition. In Proceedings of theIEEE conference on computer vision and pattern recognition, pp. 52655274, 2018. Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning throughalignment and uniformity on the hypersphere. In International conference on machine learning,pp. 99299939. PMLR, 2020. E Weinan and Stephan Wojtowytsch. On the emergence of simplex symmetry in the finaland penultimate layers of neural network classifiers. In Mathematical and Scientific MachineLearning, pp. 270290. PMLR, 2022.",
  "Nachuan Xiao, Xin Liu, and Kim-Chuan Toh. Dissolving constraints for riemannian optimiza-tion. Mathematics of Operations Research, 49(1):366397, 2024": "Yibo Yang, Shixiang Chen, Xiangtai Li, Liang Xie, Zhouchen Lin, and Dacheng Tao. Inducingneural collapse in imbalanced learning: Do we really need a learnable classifier at the end of deepneural network? In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.),Advances in Neural Information Processing Systems, 2022. URL Yibo Yang, Haobo Yuan, Xiangtai Li, Jianlong Wu, Lefei Zhang, Zhouchen Lin, Philip H.S.Torr, Bernard Ghanem, and Dacheng Tao. Neural collapse terminus: A unified solution for classincremental learning and its variants. arXiv pre-print, 2023. Can Yaras, Peng Wang, Zhihui Zhu, Laura Balzano, and Qing Qu. Neural collapse withnormalized features: A geometric analysis over the riemannian manifold.In S. Koyejo,S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Advances in Neu-ral Information Processing Systems, volume 35, pp. 1154711560. Curran Associates,Inc., 2022. URL Chong You, Zhihui Zhu, Qing Qu, and Yi Ma. Robust recovery via implicit bias of discrepantlearning rates for double over-parameterization. Advances in Neural Information ProcessingSystems, 33:1773317744, 2020. Yaodong Yu, Kwan Ho Ryan Chan, Chong You, Chaobing Song, and Yi Ma. Learning diverseand discriminative representations via the principle of maximal coding rate reduction. Advancesin Neural Information Processing Systems, 33:94229434, 2020. Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understandingdeep learning requires rethinking generalization. In International Conference on LearningRepresentations, 2017. URL",
  "Hongyi Zhang and Suvrit Sra. First-order methods for geodesically convex optimization. InConference on learning theory, pp. 16171638. PMLR, 2016": "Jinxin Zhou, Xiao Li, Tianyu Ding, Chong You, Qing Qu, and Zhihui Zhu. On the optimizationlandscape of neural collapse under mse loss: Global optimality with unconstrained features. InInternational Conference on Machine Learning, pp. 2717927202. PMLR, 2022. Jinxin Zhou, Chong You, Xiao Li, Kangning Liu, Sheng Liu, Qing Qu, and Zhihui Zhu. Are alllosses created equal: A neural collapse perspective. Advances in Neural Information ProcessingSystems, 35:3169731710, 2022. Zhihui Zhu, Tianyu DING, Jinxin Zhou, Xiao Li, Chong You, Jeremias Sulam, and QingQu. A geometric analysis of neural collapse with unconstrained features. In A. Beygelz-imer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural InformationProcessing Systems, 2021. URL",
  "Kmnvec(A) = vec(A) ,Krm(A B)Knq = B A .(25)": "The gradient in Equation 24 contains redundant constraints because of the symmetrical nature ofthe orthogonality constraints. To retain only the non-redundant constraints, we must undertake ahalf-vectorisation procedure. Given that we already possess the fully vectorised gradients (which aresimpler to compute in this scenario), we require an elimination matrix, LC RC(C+1)",
  "rvec (D2UUJ( H, U)ij) = Id (eiej ) + Id (ejei ) RdCdC .(31)": "Then, we repeat the process with the elimination matrix to eliminate the redundant constraints.However, while this is one way to compute the derivative, a more efficient approach exists, namelythe embedded gradient vector field method, which we outline in the following subsection. For acomplete overview of the method, we recommend readers to follow through the works of Birtea et al., Birtea & Comanescu .",
  "G = rvec(D2UU f( H, U)) : rvech(D2UU J( H, U)) RdCdC ,(33)": "where both the calculation of the Lagrange multiplier matrix (solved via a linear system) and theconstruction of the fourth-order tensor representing the second-order derivatives of the constraintfunction are complex and challenging. However, by recognising the manifold structure of the problem,we can reformulate Equation 33 in a simpler and more computationally efficient way. The embeddedgradient vector field method offers such a solution .",
  "Rsr .(34)": "In our problem, we are working with a compact Stiefel manifold, which is an embedded sub-manifold of RdC, and we identify the isomorphism (via vec) between RdC and RdC. A Stiefelmanifold StdC = {U RdC | U U = IC} can be characterised by a set of constraint functions,js, jpq : RdC R, as follows:",
  "(g) Test Top-1 Acc": ": CIFAR10 results on VGG-13, comparing the implicit ETF method in two scenarios: onewhere the DDN gradient is computed and included in the SGD update, and another where the DDNgradient computation is omitted from the update. In all plots, the x-axis represents the number ofepochs, except for plot (c), where the x-axis denotes the number of training examples.",
  ". Experimental Result Reproducibility": "Question: Does the paper fully disclose all the information needed to reproduce the main ex-perimental results of the paper to the extent that it affects the main claims and/or conclusionsof the paper (regardless of whether the code and data are provided or not)?Answer: [Yes]Justification: All the necessary details required for reproducing the results are presented inthe paper. Also, the code will be made publicly available upon acceptance.",
  ". Open access to data and code": "Question: Does the paper provide open access to the data and code, with sufficient instruc-tions to faithfully reproduce the main experimental results, as described in supplementalmaterial?Answer: [Yes]Justification: Due to anonymity reasons, the code is not included in the submission. However,it will be made available in a GitHub repository upon acceptance.",
  ". Crowdsourcing and Research with Human Subjects": "Question: For crowdsourcing experiments and research with human subjects, does the paperinclude the full text of instructions given to participants and screenshots, if applicable, aswell as details about compensation (if any)?Answer: [NA]Justification: The paper does not involve crowdsourcing nor research with human subjects. 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with HumanSubjectsQuestion: Does the paper describe potential risks incurred by study participants, whethersuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)approvals (or an equivalent approval/review based on the requirements of your country orinstitution) were obtained?Answer: [NA]Justification: The paper does not involve crowdsourcing nor research with human subjects."
}