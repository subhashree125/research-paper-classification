{
  "Abstract": "Self-supervised learning (SSL) has rapidly advanced in recent years, approachingthe performance of its supervised counterparts through the extraction of represen-tations from unlabeled data. However, dimensional collapse, where a few largeeigenvalues dominate the eigenspace, poses a significant obstacle for SSL. Whendimensional collapse occurs on features (e.g. hidden features and representations),it prevents features from representing the full information of the data; when di-mensional collapse occurs on weight matrices, their filters are self-related andredundant, limiting their expressive power. Existing studies have predominantlyconcentrated on the dimensional collapse of representations, neglecting whetherthis can sufficiently prevent the dimensional collapse of the weight matrices andhidden features. To this end, we first time propose a mitigation approach employingorthogonal regularization (OR) across the encoder, targeting both convolutional andlinear layers during pretraining. OR promotes orthogonality within weight matri-ces, thus safeguarding against the dimensional collapse of weight matrices, hiddenfeatures, and representations. Our empirical investigations demonstrate that ORsignificantly enhances the performance of SSL methods across diverse benchmarks,yielding consistent gains with both CNNs and Transformer-based architectures.Our code will be released at",
  "Introduction": "Self-supervised learning (SSL) has established itself as an indispensable paradigm in machinelearning, motivated by the expensive costs of human annotation and the abundant quantities ofunlabeled data. SSL endeavors to produce meaningful representations without the guidance of labels.Recent developments have witnessed joint-embedding SSL methods achieving, or even exceedingthe supervised counterparts (Misra & Maaten 2020, Bardes et al. 2022, Caron et al. 2020, Chen,Fan, Girshick & He 2020, Chen, Kornblith, Norouzi & Hinton 2020, Chen & He 2021, Dwibediet al. 2021, HaoChen et al. 2021, He et al. 2020, He & Ozay 2022, Jing et al. 2021, Li, Zhou, Xiong& Hoi 2020, Jing et al. 2020, Balestriero et al. 2023, Grill et al. 2020, Zbontar et al. 2021, Chenet al. 2021). The efficacy of these methods hinges on two pivotal principles: 1) the ability to learnaugmentation-invariant representations, and 2) the prevention of complete collapse, where all inputsare encoded to a constant vector.",
  "arXiv:2411.00392v1 [cs.LG] 1 Nov 2024": "Efforts to forestall complete collapse have been diverse, including contrastive methods with bothpositive and negative pairs (He et al. 2020, Chen, Kornblith, Norouzi & Hinton 2020, Chen et al.2021) and non-contrastive methods utilizing techniques such as self-distillation (Caron et al. 2021,Grill et al. 2020, Chen & He 2021), clustering (Caron et al. 2018, 2020, Pang et al. 2022) andfeature whitening (Bardes et al. 2022, Zbontar et al. 2021, Weng et al. 2022, 2023). Notwithstanding,these methods are prone to dimensional collapse, a phenomenon where a few large eigenvaluesdominate the eigenspace. Dimensional collapse can occur on both features (e.g. hidden features andrepresentations) and weight matrices. : Illustration of dimensional collapse in SSL. We use one augmented input Xaug1 as anexample: we assume that the encoder contains two basic blocks, each containing a linear operation(e.g., a linear layer or convolutional layer) and an activation function. Dimensional collapse can occurin weight matrices (W1, W2), hidden features, and the finally obtained representations. Existingmethods act directly on representations and expect to affect hidden features and weight matricesindirectly, which has no guarantee in theory; our method directly constrains weight matrices andindirectly influences hidden features and representations, which can be guaranteed by theoreticalanalysis. To prevent dimensional collapse of representations, as depicted in , existing methods includemodifying representations in downstream tasks (He & Ozay 2022), whitening representations directly(i.e. removing the projector) (Jing et al. 2021), incorporating regularizers on representations duringpretraining (Huang et al. 2024, Hua et al. 2021). However, whether they sufficiently prevent thedimensional collapse of weight matrices and hidden features remains unknown (i.e., no theoreticalguarantee) (Pasand et al. 2024). In Appendix A.1, we further demonstrate that whitening representa-tions directly to eliminate the dimensional collapse of representations cannot adequately remove thedimensional collapse of weight matrices. To address these challenges, we first time propose a mitigation approach employing orthogonalregularization (OR) across the encoder, targeting both convolutional and linear layers during pretrain-ing. It is natural that OR prevents the dimensional collapse of weight matrices as it ensures weightmatrices orthogonality, keeps the correlation between its filters as low as possible, and lets each filterhave a norm of 1. For features (e.g. hidden features and representations), orthogonal weight matricescan promote uniform eigenvalue distributions and thus prevent the domination of eigenspaces by a",
  "limited number of large eigenvalues, as theoretically substantiated by Huang et al. (2018), Yoshida &Miyato (2017), Rodrguez et al. (2016)": "In our study, we introduce and assess the effects of two leading orthogonality regularizers, SoftOrthogonality (SO) and Spectral Restricted Isometry Property Regularization (SRIP), on SSL methods.We examine their integration with 13 modern SSL methods from Solo-learn and LightSSL, spanningboth contrastive and non-contrastive methods (Chen, Fan, Girshick & He 2020, Chen et al. 2021, Grillet al. 2020, Caron et al. 2021, Dwibedi et al. 2021). Our findings indicate a consistent enhancementin linear probe accuracy on CIFAR-100 using both CNNs and Transformer-based architectures andOR exhibits a good scaling law at the model scale. Furthermore, when applied to BYOL trainedon IMAGENET-1k, OR significantly improves the downstream performance on both classificationand object detection tasks, suggesting its applicability to large-scale SSL settings. Remarkably, ORachieves these enhancements without necessitating modifications to existing SSL architectures orhyperparameters.",
  "Self-Supervised Learning": "Self-supervised Learning (SSL) aims to learn meaningful representations from unlabeled data.Existing SSL methods can be broadly classified into two categories: generative and joint embeddingmethods. This paper concentrates on joint-embedding methods, which learn representations byaligning the embeddings of different augmented views of the same instance. Joint-embeddingmethods further subdivide into contrastive and non-contrastive methods. Contrastive methods, suchas those proposed by He et al. (2020), Chen, Kornblith, Norouzi & Hinton (2020), Chen et al.(2021), treat each sample as a distinct class and leverage the InfoNCE loss (Oord et al. 2018) tobring representations of positive pairs closer together while distancing those of negative pairs in thefeature space. These methods generally require a substantial number of negative samples for effectivelearning. In contrast, non-contrastive methods eschew the use of negative samples. They insteademploy various techniques such as self-distillation (Caron et al. 2021, Grill et al. 2020, Chen & He2021), clustering (Caron et al. 2018, 2020, Pang et al. 2022) and feature whitening (Bardes et al.2022, Zbontar et al. 2021, Weng et al. 2022, 2023). Our empirical findings indicate that incorporatingOR enhances the performance of both contrastive and non-contrastive SSL methods. The explorationof its effects on generative methods remains for future work.",
  "Dimensional Collapse in SSL": "Dimensional collapse plagues both generative and joint embedding SSL methods (Zhang et al.2022, Jing et al. 2021, Zhang et al. 2021, Tian et al. 2021). To prevent the dimensional collapseof representations, existing work has typically focused on imposing constraints on the covariancematrix of the representations, including modifying representations in downstream tasks (He & Ozay2022), removing the projector (Jing et al. 2021), incorporating regularizers on representations duringpretraining (Huang et al. 2024, Hua et al. 2021). However, these strategies face challenges such asperformance degradation upon removing the projector, not addressing collapse during pre-training,and failing to prevent dimensional collapse in hidden features and weight matrices within the encoder(referred to Appendix A.1). This motivates us to regularize the weight matrices of the DNNs directlyin SSL.",
  "Orthogonality Regularization": "Orthonormality regularization, which is applied in linear transformations, can improve the generaliza-tion and training stability of DNNs (Xie et al. 2017, Huang et al. 2018, Saxe et al. 2013). OR hasdemonstrated its effects on tasks including supervised/semi-supervised image classification, imageretrieval, unsupervised inpainting, image generation, and adversarial training (Bansal et al. 2018,Balestriero et al. 2018, Balestriero & Baraniuk 2020, Xie et al. 2017, Huang et al. 2018). Efforts toutilize orthogonality in network training have included penalizing the deviation of the gram matrix ofeach weight matrice from the identity matrix (Xie et al. 2017, Bansal et al. 2018, Balestriero et al.2018, Kim & Yun 2022) and employing orthogonal initialization (Xie et al. 2017, Saxe et al. 2013).For more stringent norm preservation, some studies transform the convolutional layer into a doublyblock-Toeplitz (DBT) matrix and enforce orthogonality (Qi et al. 2020, Wang et al. 2020). In this work, we first time investigate the efficacy of two orthogonality regularizers, Soft Orthogonality(SO) and Spectral Restricted Isometry Property (SRIP) in SSL (Bansal et al. 2018). These regularizersaim to minimize the distance between the gram matrix of each weight matrix and the identitymatrixmeasured in Frobenius and spectral norms, respectively.",
  "Settings of Self-supervised Learning": "In this section, we present the general settings for joint-embedding SSL methods. We consider a largeunlabelled dataset X RND, comprising N samples each of dimensionality D. The objectiveof SSL methods is to construct an effective encoder f that transforms raw data into meaningfulrepresentations Z = f(X), where Z RNM and M denotes the representation dimensionality.The learning process of SSL methods is visually represented in , where data augmentationstransform X into two augmented views Xaug1, Xaug2 RDN. A typical joint-embedding SSLarchitecture encompasses an encoder f and a projector p. These components yield encoder featuresZaug1 = f(Xaug1) and Zaug2 = f(Xaug2), as well as projection features Haug1 = p(Zaug1)andHaug2 = p(Zaug2). During training, the parameters of f and p are optimized via backpropagationto minimize the discrepancy between Haug1 and Haug2. To prevent the encoder f from producing aconstant feature vector, contrastive methods utilize negative samples, and non-contrastive methodsemploy strategies such as the self-distillation technique. The efficacy of the encoder f is usually assessed by the performance of the C-class classificationas downstream tasks. Specifically, given a labeled dataset containing samples Xs RSD andtheir corresponding labels Ys RSC, where S is the sample number. Then, a linear layer gparameterized by Wc RCM is appended on top of the learned representations Zs = f(Xs), andthus the classification task can be fulfilled by minimizing the cross-entropy between softmax(g(Zs))and Ys. There are two strategies for the fine-tuning: 1) non-linear fine-tuning, which trains both gand f in the downstream tasks, and 2) linear evaluation, which freezes f and only trains g (referredto as the linear probe). : Illustration of joint-embedding SSL methods. This is a general structure. Differentaugmented inputs can be passed either by shared weight Encoder and Projector or by independentEncoder and Projector, depending on different SSL methods.",
  "Orthogonality Regularizers": "We introduce two orthogonality regularizers: Soft Orthogonality (SO) and Spectral Restricted Isome-try Property Regularization (SRIP), which are seamlessly integrable with linear and convolutionallayers. Consider a weight matrix W Rinputoutput in a linear layer, where input and output denotethe number of input and output features, respectively. In line with Bansal et al. (2018), Xie et al.(2017), Huang et al. (2018), we reshape the convolutional filter to a two-dimensional weight matrixW Rinputoutput, while we still use the same notation W for consistency. To be specific,input = S H Cin and output = Cout, with Cin and Cout being the number of input and outputchannels, and S and H representing the width and height of the filter, respectively. The SO regularizer encourages the weight matrix W to approximate orthogonality by minimizing thedistance between its Gram matrix and the identity matrix. This is quantified by the Frobenius normas follows:",
  "W f SO(W) or": "W f SRIP(W), depending on the selectedorthogonality regularizers. The term serves as a hyperparameter that balances the SSL objectiveand OR loss. Notably, we only perform OR on the weight matrices located within the linear andconvolutional layers of the encoder f. OR provides a versatile regularization strategy for the encoderf, facilitating its application across various SSL methods without necessitating modifications to thenetwork designs or existing training protocols.",
  "Analysis of Dimensional Collapse and the Effects of OR in SSL": "In this section, we show that dimensional collapse happens not only to the representations (i.e. outputof the encoder), but also to weight matrices and hidden features of the encoder. We also comparethe feature whitening technique used by one previous method, VICREG (Bardes et al. 2022), withOR. Migrating to BYOL, we find that the feature whitening technique only solves the dimensionalcollapse at the feature level, but instead accelerates the collapse of the weight matrices, and it evenleads to lower performance of the downstream tasks as shown in . In contrast, OR can",
  "To study the eigenspace of a matrix, we utilize the normalized eigenvalues defined in He & Ozay(2022):": "Definition 1 (Normalized eigenvalues) Given a specific matrix T RND, where N is the sam-ple number and D is the feature dimension, we first obtain its covariance matrix T RDD.Then we perform an eigenvalue decomposition on T to obtain its eigenvalues {Ti }Di=1 ={T1 , , Ti , , TD} in descending order. And we obtain the normalized eigenvalues by di-viding all eigenvalues by the max eigenvalue T1 , denoted as {T1 /T1 , , Ti /T1 , , TD/T1 }.To simplify the denotation, we reuse {Ti }Di=1 to denote normalized eigenvalues of T. These normalized eigenvalues are less than or equal to 1, where a larger value in one dimensionindicates more information contained, and vice versa. We argue that if normalized eigenvalues dropvery quickly, this means that only a few dimensions in the eigenspace contain meaningful informationand also means that dimensional collapse has occurred. We train three BYOL (Grill et al. 2020) models, without OR, with the feature whitening technique(e.g. Variance and Covariance regularization) from VICREG and with OR, respectively. We chooserandomly initialized ResNet18 as the backbone (i.e. encoder) and train the three models on CIFAR-10for 1,000 epochs, following the same recipe of Da Costa et al. (2022). Importantly, SO is selected asthe orthogonality regularize, and is set to 1e 6. For the feature whitening technique, we imposethe Variance and Covariance regularization from VICREG on the output of the predictor in BYOL astwo additional loss terms, the former to ensure the informativeness of individual dimensions and thelatter to reduce the correlation between dimensions. Following the solo-learn settings, we set the twoloss term hyperparameters to vic and vic 0.004, and then tune the vic from 1e 3 to 1e 5. After training, we calculate the normalized eigenvalues of both weight matrices and features (e.g.input features, hidden features, representations). ResNet18 contains four basic blocks, each containingfour convolutional layers, and we visualize the normalized eigenvalues of the last convolutional layerin each block. Hidden features are the outputs of four basic blocks in ResNet18. We use the firstbatch in the test set as input features with batchsize 4,096 and feature dimension 3072 (32*32*3).Results are analyzed in the following sections.",
  "Dimensional Collapse of Weight Matrices": "We first examine the weight matrices of the encoder. Similar to 5.1, all the weight matrices are viewedas two-dimensional matrices, and on top of them, we can calculate their normalized eigenvalues. Fora specific weight matrix W Rinputoutput in a neural network layer, we denote {Wi }outputi=ias thenormalized eigenvalues of this layer. As shown in , X and Y axis is the i index and i-th values of {Wi }outputi=i, respectively. Itis clear that with OR, the eigenvalues of the convolutional layers of different depths decay moreslowly, which means that their filters are less redundant and more diverse. In particular, we note thatthe deepest convolutional layer (i.e. layer4_512) has a much faster decay rate of the eigenvaluescompared to the other convolutional layers in the absence of OR. Notably, the feature whiteningtechnique does not alleviate this phenomenon. OR could significantly improve this. OR requiresthe weight matrix to be as orthogonal as possible, which means that the diagonal elements of itscovariance matrix are as identical as possible, and the off-diagonal elements will be close to 0. Then",
  "(f) Eigenvalues of features (with OR)": ": Eigenspectra of both weights and features within the encoder (ResNet18). The featuresare collected on the first batch of the test set (batchsize 4,096). We pretrain BYOL without OR,with feature whitening from VICREG, and with OR on CIFAR-10. The x-axis and y-axis are bothlog-scaled. The solid line represents that all eigenvalues are positive, the dashed line represents theexistence of eigenvalues that are non-positive, and the number of eigenvalues is represented behindthe underline.",
  "Dimensional Collapse of Features": "As the weight matrices become orthogonal, the distribution of their outputs stabilizes, therebypreventing the dimensional collapse of hidden features and representations. This property has beendemonstrated in vector form by Huang et al. (2018) and is now presented in matrix form: Proposition 1 For a specific weight matrix W Rinputoutput and X RNinput, comprising Nsamples each of dimensionality input. We denote X and S as the sample means of X and S, re-spectively. Let S = XW, where W T W = I. The covariance matrix of X is X = (X X)T (X X)",
  "X2F": "The first point of Proposition 1 illustrates that in each layer of DNNs, the orthogonal weight matrixpreserves the normalization and de-correlation of the output S, assuming the input is whitened. Thisreveals that as the network gets deeper, the hidden features of each layer and the final representationsdo not tend to collapse. Moreover, orthogonal filters maintain the norm of both the output and theback-propagated gradient information in DNNs, as demonstrated by the second and third points ofProposition 1. To verify that the dimensional collapse of features can be eliminated by OR, we visualize thenormalized eigenvalues of features (input features, hidden features, and representations) as shownin . Without the OR constraint, features located in deeper layers (i.e. representations) willhave the fastest eigenvalues decay rate, and the distributions of eigenvalues of hidden features varyconsiderably at different depths. After adding the OR, it can be seen that the decay rates of hiddenfeatures at layers 3 and 4 are almost the same, while the eigenvalues of representations decay muchmore slowly. We also visualize the representations in Appendix A.3, which also verifies that thedimensional collapse of the representations is mitigated. Interestingly, the effect of OR on the featurelevel is similar to that of the feature whitening technique, however, the latter is unable to eliminatethe dimensional collapse in the weight matrices.",
  "Numerical Experiments": "We study the effects of OR on SSL methods through extensive experiments. we first demonstrate thatOR improves the classification accuracy on CIFAR-10, CIFAR-100, and IMAGENET100, and theimprovement is consistent across different backbones and SSL methods. On the large-scale datasetIMAGENET-1k (Deng et al. 2009), OR boosts the classification accuracy on both in-distributionand out-distribution datasets (i.e. transfer learning datasets), demonstrating consistent improvement.Moreover, OR also enhances the performance in downstream tasks(e.g. object detection). Baseline methods and datasets. We evaluated the effect of adding OR to 13 modern SSL methods,including 6 methods implemented by solo-learn (MOCOv2plus, MOCOv3, DINO, NNBYOL, BYOL,VICREG) (Chen & He 2021, Chen et al. 2021, Grill et al. 2020, Dwibedi et al. 2021, Caron et al.2021) and 10 methods implemented by LightlySSL (BarlowTwins, BYOL, DCL, DCLW, DINO,Moco, NNCLR, SimCLR, SimSiam, SwaV) (Zbontar et al. 2021, Yeh et al. 2022, Caron et al.2020, Chen & He 2021). We pretrain SSL methods on CIFAR-10, CIFAR-100, IMAGENET-100and IMAGENET-1k and evaluate transfer learning scenarios on datasets including CIFAR-100,CIFA-10 (Krizhevsky et al. 2009), Food-101 (Bossard et al. 2014), Flowers-102 (Xia et al. 2017),DTD (Sharan et al. 2014), GTSRB (Haloi 2015). We evaluate the objection detection task on PASCALVOC2007 and VOC2012 (Everingham et al. 2010). Detailed descriptions of datasets and baselineSSL methods are shown in Appendix A.4 and A.5, respectively. Training and evaluation settings. For each SSL method, we use the original settings in solo-learn (Da Costa et al. 2022) and LightlySSL. These settings include the network structure, lossfunction, training policy (training epochs, optimizers, and learning rate schedulers) and data augmen-tation policy. The splits of the training and test set follow torchvision Marcel & Rodriguez (2010).For all the classification tasks, we report the linear probe or KNN accuracy; for the objection detectiontask, we perform non-linear fine-tuning. Details of training, parameter tuning, and evaluation arepresented in Appendix A.6. It is worth noting that the Solo-learn and LightlySSL setups are notthe same as the official implementation of the SSL methods, e.g., there is no use of multi-cropaugmentation in DINO, and there is no exceptionally long training epoch. We leave experiments onmigrating OR to the official implementation for future work. Recipe of adding OR. For OR, of SRIP is tuned from {1e 3, 1e 4, 1e 5} and of SO istuned from {1e 5, 1e 6, 1e 7} on a validation set. When you want to add OR to your SSLpre-training, you simply pass the encoder into the loss function, and then you just need to set of theOR according to the backbone and regularizer you use as shown in of Appendix A.6.",
  "OR is Suitable for Different Backbones and SSL Methods": "After pretraining on CIFRA-100, for each SSL method, we report the corresponding classificationaccuracy as shown in . Both two orthogonality regularizers consistently improve the linearclassification accuracy. Note that OR boosts the performance of both constrastive (MoCov2plus, Mocov3) and non-contrastive methods(BYOL, NNBYOL, DINO) as they are all susceptible todimensional collapse (Zhang et al. 2022, Jing et al. 2021, Zhang et al. 2021, Tian et al. 2021).Non-contrastive methods gain more improvements in contrast to contrastive methods. When we useResNet18, MOCOv3 improves 3% on Top-1 accuracy while DINO and NNBYOL improve 6% and5%, respectively. OR can also boost the performance of the Sota method like BYOL by 1%. Whenwe scale to ResNet 50 and WideResnet28w2, OR consistently boosts their performance. Moreover,the additional time overhead of adding OR to SSL is low compared to the original training time(referred to A.7).",
  "SRIP61.3787.9458.7985.1153.9383.2764.2389.0061.1087.31": "In addition to CNN backbones, OR is also able to improve SSL performance on Transformer-basedbackbones (e.g., VIT) (Han et al. 2022, Dosovitskiy et al. 2020, Zhou et al. 2021). We pretrain DINOon CIFAR-100 with different depths of VITs. As shown in , with the increasing depth of theVIT, the original DINO performance is increasing, and OR is able to increase their performance evenfurther, which exhibits a good scaling law. Interestingly, under the Transformer-based architecture,OR is able to improve performance more (up to 12%) compared to CNN backbones. This is consistentwith some existing studies (RoyChowdhury et al. 2017) that linear layers are more likely to haveredundant filters than convolutional layers in DNNs, i.e., more prone to dimensional collapse.",
  "RegularizerTop-1Top-5Top-1Top-5Top-1Top-5Top-1Top-5Top-1Top-5Top-1Top-5": "-59.83883.94573.81791.03968.77791.59663.53992.03579.799.1252.0981.89SO63.62486.44478.95693.39770.85192.81968.72593.99183.5899.4157.5785.38SRIP63.62886.42079.3394.24370.42692.23469.25694.34784.2699.3957.8485.87 We can observe that OR not only improves the accuracy on IMAGENET-1k but also on all the transferlearning datasets. OR improves TOP-1 accuracy by 3% in IMAGENET-1k and by 3% to 9% in eachtransfer learning datasets. The transfer learning task evaluates the generality of the encoder as it hasto encode samples from various out-of-distribution domains with categories that it may not have seenduring pretraining. OR also significantly improves SSLs performance in the objection detection taskby 20% on AP. The above results are close to Chen et al. (2021), Da Costa et al. (2022), Weng et al.(2023) where also training only 100 epochs.",
  "Conclusions": "The existing studies focus on the dimensional collapse of representations and overlook whetherweight matrices and hidden features also undergo dimensional collapse. We first time propose amitigation approach to employing orthogonal regularization (OR) across the encoder, targeting bothconvolutional and linear layers during pretraining. OR promotes orthogonality within weight matrices,thus safeguarding against the dimensional collapse of weights, hidden features, and representations.Our empirical investigations demonstrate that OR significantly enhances SSL method performanceacross diverse benchmarks, yielding consistent gains with both CNNs and Transformer-based archi-tectures as the backbones. Importantly, the time complexity and required efforts on fine-tuning arelow and the performance improvement is significant, enabling it to become a useful plug-in in variousSSL methods. In terms of future research, we wish to examine the effect of OR on other pre-training foundationmodels, such as vision generative SSL models such as MAE (He et al. 2022), auto-regression modelslike GPTs and LLaMAs (Radford et al. 2018, 2019, Brown et al. 2020, Touvron et al. 2023), andContrastive Language-Image Pre-training models (Radford et al. 2021, Li et al. 2022). We believe ORis a pluggable and useful module to boost the performance of vision and language foundation models.In fact, this paper is the first to test the effectiveness of OR in a Transformer-based architecture and itis reasonable to believe that it will perform well in these domains. The work described in this paper was supported by the National Natural Science Foundation of China(No. 52102385), grants from the Research Grants Council of the Hong Kong Special AdministrativeRegion, China (Project No. PolyU/25209221 and PolyU/15206322), and grants from the OttoPoon Charitable Foundation Smart Cities Research Institute (SCRI) at the Hong Kong PolytechnicUniversity (Project No. P0043552). The contents of this article reflect the views of the authors, whoare responsible for the facts and accuracy of the information presented herein.",
  "Bardes, A., Ponce, J. & LeCun, Y. (2022), Variance-invariance-covariance regularization for self-supervised learning, ICLR, Vicreg 1, 2": "Bossard, L., Guillaumin, M. & Van Gool, L. (2014), Food-101mining discriminative componentswith random forests, in Computer VisionECCV 2014: 13th European Conference, Zurich,Switzerland, September 6-12, 2014, Proceedings, Part VI 13, Springer, pp. 446461. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P.,Sastry, G., Askell, A. et al. (2020), Language models are few-shot learners, Advances in neuralinformation processing systems 33, 18771901.",
  "Caron, M., Bojanowski, P., Joulin, A. & Douze, M. (2018), Deep clustering for unsupervised learningof visual features, in Proceedings of the European conference on computer vision (ECCV),pp. 132149": "Caron, M., Misra, I., Mairal, J., Goyal, P., Bojanowski, P. & Joulin, A. (2020), Unsupervised learningof visual features by contrasting cluster assignments, Advances in neural information processingsystems 33, 99129924. Caron, M., Touvron, H., Misra, I., Jgou, H., Mairal, J., Bojanowski, P. & Joulin, A. (2021), Emergingproperties in self-supervised vision transformers, in Proceedings of the IEEE/CVF internationalconference on computer vision, pp. 96509660.",
  "Chen, X., Xie, S. & He, K. (2021), An empirical study of training self-supervised vision transformers,in Proceedings of the IEEE/CVF international conference on computer vision, pp. 96409649": "Da Costa, V. G. T., Fini, E., Nabi, M., Sebe, N. & Ricci, E. (2022), solo-learn: A library of self-supervised methods for visual representation learning, Journal of Machine Learning Research23(56), 16. Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K. & Fei-Fei, L. (2009), Imagenet: A large-scalehierarchical image database, in 2009 IEEE conference on computer vision and pattern recognition,Ieee, pp. 248255. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M.,Minderer, M., Heigold, G., Gelly, S. et al. (2020), An image is worth 16x16 words: Transformersfor image recognition at scale, arXiv preprint arXiv:2010.11929 . Dwibedi, D., Aytar, Y., Tompson, J., Sermanet, P. & Zisserman, A. (2021), With a little help from myfriends: Nearest-neighbor contrastive learning of visual representations, in Proceedings of theIEEE/CVF International Conference on Computer Vision, pp. 95889597.",
  "Everingham, M., Van Gool, L., Williams, C. K., Winn, J. & Zisserman, A. (2010), The pascal visualobject classes (voc) challenge, International journal of computer vision 88, 303338": "Girshick, R., Donahue, J., Darrell, T. & Malik, J. (2014), Rich feature hierarchies for accurate objectdetection and semantic segmentation, in Proceedings of the IEEE conference on computer visionand pattern recognition, pp. 580587. Grill, J.-B., Strub, F., Altch, F., Tallec, C., Richemond, P., Buchatskaya, E., Doersch, C., Avila Pires,B., Guo, Z., Gheshlaghi Azar, M. et al. (2020), Bootstrap your own latent-a new approach toself-supervised learning, Advances in neural information processing systems 33, 2127121284.",
  "Haloi, M. (2015), Traffic sign classification using deep inception based convolutional networks,arXiv preprint arXiv:1511.02992": "Han, K., Wang, Y., Chen, H., Chen, X., Guo, J., Liu, Z., Tang, Y., Xiao, A., Xu, C., Xu, Y. et al.(2022), A survey on vision transformer, IEEE transactions on pattern analysis and machineintelligence 45(1), 87110. HaoChen, J. Z., Wei, C., Gaidon, A. & Ma, T. (2021), Provable guarantees for self-superviseddeep learning with spectral contrastive loss, Advances in Neural Information Processing Systems34, 50005011.",
  "He, B. & Ozay, M. (2022), Exploring the gap between collapsed & whitened features in self-supervised learning, in International Conference on Machine Learning, PMLR, pp. 86138634": "He, K., Chen, X., Xie, S., Li, Y., Dollr, P. & Girshick, R. (2022), Masked autoencoders are scalablevision learners, in Proceedings of the IEEE/CVF conference on computer vision and patternrecognition, pp. 1600016009. He, K., Fan, H., Wu, Y., Xie, S. & Girshick, R. (2020), Momentum contrast for unsupervised visualrepresentation learning, in Proceedings of the IEEE/CVF conference on computer vision andpattern recognition, pp. 97299738. Hua, T., Wang, W., Xue, Z., Ren, S., Wang, Y. & Zhao, H. (2021), On feature decorrelation inself-supervised learning, in Proceedings of the IEEE/CVF International Conference on ComputerVision, pp. 95989608.",
  "Krizhevsky, A., Hinton, G. et al. (2009), Learning multiple layers of features from tiny images": "Lavoie, S., Tsirigotis, C., Schwarzer, M., Vani, A., Noukhovitch, M., Kawaguchi, K. & Courville, A.(2022), Simplicial embeddings in self-supervised learning and downstream classification, arXivpreprint arXiv:2204.00616 . Li, J., Li, D., Xiong, C. & Hoi, S. (2022), Blip: Bootstrapping language-image pre-training for unifiedvision-language understanding and generation, in International conference on machine learning,PMLR, pp. 1288812900.",
  "Li, J., Zhou, P., Xiong, C. & Hoi, S. C. (2020), Prototypical contrastive learning of unsupervisedrepresentations, arXiv preprint arXiv:2005.04966": "Li, X., Chen, S. & Yang, J. (2020), Understanding the disharmony between weight normalizationfamily and weight decay, in Proceedings of the AAAI Conference on Artificial Intelligence,Vol. 34, pp. 47154722. Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollr, P. & Zitnick, C. L.(2014), Microsoft coco: Common objects in context, in Computer VisionECCV 2014: 13thEuropean Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13,Springer, pp. 740755.",
  "i=j(Cov(zi, zj))2(6)": "where Cov(zi, zj) denotes the covariance between the i-th and j-th dimensions of Zaug1. This termeffectively encourages the representation to have orthogonal dimensions, which is beneficial forlearning independent features. As for the VICREG without projector, we discard the projector and apply the SSL objective directlyto the representations, which ensures that the representations are whitened (no dimensional collapsein representations), i.e., minimize the correlation among dimensions and make each dimension richin information. For OR, we choose SO as the regularizer and set as 1e 6. We then experimentallyobserve that guaranteeing that dimensional collapses do not occur in representations or projectorfeatures (i.e., VICREG without projector and projector features) does not guarantee that dimensionalcollapses do not occur in weight matrices in the encoder. Moreover, discarding the projector evendamages the performance of the original VICREG, while OR still boosts the performance as shownin .",
  "(f) Eigenvalues of features(VICREG with SO)": ": Eigenspectra of both weights and features within the encoder (ResNet18). The features arecollected on the first batch of the test set (batchsize 4096). We pretrain original VICREG, VICREGwithout projecto, and VICREG with OR on CIFAR-10. The x-axis and y-axis are both log-scaled.The solid line represents that all eigenvalues are positive, the dashed line represents the existence ofeigenvalues that are non-positive, and the number of eigenvalues is represented behind the underline. HeatMap can intuitively indicate that the correlation of non-diagonal elements is constrained to 0 byOR. The Biclustering results show an obvious blocky structure, which means that there is clusteringbetween filters, and the weight matrix is low-rank and redundant.",
  "A.3Visualization of Representations": "We used BYOL (ResNet18) for pretraining on CIFAR-10. After pretraining, we perform dimensionreduction and visualization of learned representations using UMAP (McInnes et al. 2018). As shownin , in the absence of OR, there is a tendency for the cluster centers of each category to movecloser together and more outliers appear. This is due to the fact that in the absence of OR, BYOLproduces representations dominated by some extremely large eigenvalues (i.e. dimensional collapse),which is consistent with results in .2.",
  "(d) Biclustering (with OR)": ": HeatMap is the visualization of the absolute value of the correlation coefficients amongfilters of the weight matrix (layer4). Biclustering is the visualization of the results of spectralbiclustering. It can be seen that OR significantly reduces the correlation and removes the clusteringpatterns among filters from the heatmap and biclustering, respectively.",
  "GTSRB (Haloi 2015): The German Traffic Sign Recognition Benchmark (GTSRB) datasetconsists of over 50,000 images of traffic signs across 43 categories": "PASCAL VOC2007 and VOC2012 (Lin et al. 2014): Used for evaluating objection tasks,this dataset includes complex everyday scenes with annotated objects in their natural context.The objection detection task contains 20 categories. We use the VOC2007 and VOC2012train-val (16551 images) as the training set and then report the performance on the VOC2007test set (4952 images).",
  "A.5Joint-embedding SSL methods": "Self-supervised learning (SSL) has emerged as a powerful paradigm for learning representationswithout the need for labeled data. This appendix provides a concise overview of several SSL methodsused in this paper. MOCOv2, introduced by Chen & He (2021), on top of MOCOs momentum encoderand the use of the dynamic dictionary with a queue to store negative samples (He et al.2020), adds the MLP projection head and more data augmentation. Compared to MOCOv2,MOCOv2plus uses a symmetric similarity loss. MoCov3 (Chen et al. 2021) makes some improvements on the basis of v1/2, firstly, becausethe batchsize is large enough when training V3, the memory queue is removed, and thenegative samples are sampled directly from the batch. Secondly, symmetric contrastive lossis used, and finally, an extra prediction head is added to the original encoder, which is atwo-layer fully connected layer. Bootstrap Your Own Latent (BYOL), proposed by Grill et al. (2020), introduces a novelapproach to SSL that does not rely on negative pairs. Instead, BYOL employs a dual-networkarchitecture where the encoder learns to predict the representations of the momentumencoder. Through a series of updates (i.e. EMA), where the momentum encoder graduallyassimilates the encoders weights, BYOL effectively learns robust representations. Thesuccess of BYOL depends not only on the EMA, but also on its additional projector and theBN in the projector to avoid a complete collapse of the encoder. This method challenges theconventional wisdom that contrastive learning requires negative pairs, opening new avenuesfor SSL research. Expanding on the ideas of BYOL, NNBYOL (Dwibedi et al. 2021) introduces the conceptof using nearest neighbors to augment the learning process. By leveraging the similaritiesbetween different instances in the dataset, NNBYOL aims to refine the quality of the learnedrepresentations further. This approach underscores the potential of incorporating instance-level information into the SSL framework, enhancing the discriminability and robustness ofthe resulting models. DINO (Caron et al. 2021) uses a self-distilling architecture. The outputs of the teachernetworks (i.e. the momentum encoder) are subjected to a centering operation by averagingover a batch, and each network outputs a K-dimensional feature that is normalized usingSoftmax. The similarity between the student model (i.e. the encoder) and the teacher modelis then computed using cross-entropy loss as the objective function. A stop-gradient operatoris used on the teacher model to block the propagation of the gradient, and only the gradient ispassed to the student model to make it update its parameters. The teacher model is updatedusing the weights of the student model (i.e. EMA). Barlow Twins computes the correlation matrix between the embeddings of two differentviews of the same sample and avoids collapse by making it as close as possible to the unitmatrix. This approach makes the embeddings between the two views of the sample assimilar as possible while minimizing the redundancy between vector components. VICREG avoids the complete collapse problem with variance and covariance regularization. DCL and DCLW removes the NPC effect of infoNCE loss by getting rid of the positive termfrom the denominator and thus significantly improves the learning efficiency",
  "A.6Hyper-parameters of Pretraining and Evaluation": "For each SSL method, we use the original settings of Solo-learn and LightlySSL (Da Costa et al. 2022).These settings include the network structure, loss function, training policy, and data augmentationpolicy. Considering that we use numerous SSL methods and that our setup is exactly the same asthem, please go to their official implementation. For OR, the appropriate regularization term generally depends only on the backbone used by SSLand the orthogonality regularizer (SRIP or SO) chosen. As shown in , when you want to addOR to your SSL pre-training, you simply pass the encoder into the loss function, and then you justneed to set of the OR according to the backbone and regularizer you use.",
  "VIT-BaseSO1e-6": "For the classification tasks, due to computational constraints, we do not perform non-linear fine-tuningin classification tasks. Instead, we perform a linear probe or KNN to evaluate the quality of obtainedrepresentations as typically done in the literature (Huang et al. 2024, Li, Chen & Yang 2020, Lavoie et al. 2022). To be specific, for each SSL method and dataset, after pretraining, We train a linearclassifier on top of frozen representations of the training set. Then we report the Top-1 and Top-5linear classification accuracy on the test set. When training the linear classifier, we use 100 epochs,weight decay to 0.0005, learning rate 0.1 (we divide the learning rate by a factor of 10 on Epoch 60and 100), batchsize 256, and SGD with Nesterov momentum as optimizer (In IMAGENET-1k, weuse batchsize 128 and learning rate 0.2). For the object detection task, we perform nonlinear fine-tuning on ResNet50 in RCNN-C4 (Girshicket al. 2014) with batchsize 9 and base learning rate 0.01. We use the detectron2 (Wu et al. 2019),following the MOCO-v1 (He et al. 2020) official implementation exactly.",
  "A.7Time Cost of OR": "Implementing OR requires computing the OR loss in the backbone at each gradient update, wecount the time overhead required by the different backbones to compute OR at one time, and wehave averaged over 10 times as shown in Table A.7. In the pre-training phase, the time overheadof OR is only related to the backbone and the steps that need to be updated, IMAGENET-1k (100epochs, batchsize 128) has a total of 62599 steps, and CIFAR-100 (1000 epochs, batchsize 256) hasa total of 194999 steps. As you can see, compared to the original pre-training overhead of dozensand hundreds of hours, the additional time added by OR is very small, steadily improving SSLsperformance. Notably, if we use a larger batchsize such as 4096, our time overhead will be reducedby 64 on IMAGENET-1k and 16 on CIFAR-100."
}