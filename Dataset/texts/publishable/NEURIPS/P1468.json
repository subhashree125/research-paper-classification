{
  "Abstract": "Neural networks trained with stochastic gradient descent exhibit an inductive biastowards simpler decision boundaries, typically converging to a narrow family offunctions, and often fail to capture more complex features. This phenomenonraises concerns about the capacity of deep models to adequately learn and representreal-world datasets. Traditional approaches such as explicit regularization, dataaugmentation, architectural modifications, etc., have largely proven ineffective inencouraging the models to learn diverse features. In this work, we investigate theimpact of pre-training models with noisy labels on the dynamics of SGD acrossvarious architectures and datasets. We show that pretraining promotes learningcomplex functions and diverse features in the presence of noise. Our experimentsdemonstrate that pre-training with noisy labels encourages gradient descent to findalternate minima that do not solely depend upon simple features, rather learns morecomplex and broader set of features, without hurting performance.",
  "Introduction": "Overparameterized models trained using stochastic gradient descent tend to focus on only a smallfraction of the available features. Such behavior reduces the diversity of features that contributeto the classification of data. This phenomenon has been discussed in the context of simplicity bias, where neural models solely rely on simpler, easy-to-learn, features. It is also studied inrelation to the intrinsic regularization properties of SGD that inherently favors lower-complexitymodels . Previous works have shown that models trained using SGD learn linear functions first,and as training progresses they learn functions of increasing complexity . This preference causesmany potentially useful features to remain unlearned and underutilized, resulting in models that haveinferior discriminative quality and rely on features that are coincidental rather than causal . Suchmodels when faced with distributional shifts or adversarially perturbed data, are unable to generalizeto novel or altered environments. Recent works have attributed this biased behavior to the intrinsic regularization properties of SGD thatnaturally favors low-complexity solutions . Models that exhibit poor generalization for minoritygroups are shown to dedicate excess parameters to memorizing a small number of data points .Complex features are often overshadowed by the amplification and replication of simpler features .In real-world datasets, simplicity bias in neural networks is often defined as the propensity to learnlow-dimensional projections of inputs , where features such as shape and color (e.g., in MNIST orCIFAR) define decision boundaries of varying complexity. Standard approaches like ensembling andadversarial training fail to effectively address the limitations imposed by this bias . There is anincreasing interest in understanding the factors that contribute to SGDs bias toward simpler featuresets, shortcut feature learning, as well as strategies to mitigate or exploit this tendency to improvemodels performance. These include approaches that aim to reduce feature correlation ,",
  "arXiv:2411.04569v1 [cs.LG] 7 Nov 2024": "-101 Feature-1 Feature-2 Feature-2 Feature-3 Feature-3 Feature-4 class-0 class-1 -101 Feature-1 Feature-2 Feature-2 Feature-3 Feature-3 Feature-4 class-0class-1 -101 Feature-1 Feature-2 Feature-2 Feature-3 Feature-3 Feature-4 class-0class-1 -101 Feature-1 Feature-2 Feature-2 Feature-3 Feature-3 Feature-4 class-0class-1 : (Top-left) First 4 dimensions of Slab-data. (Top-right) Decision boundary learned fromregular training. (Botton row) Two different decision boundaries learned after noisy pre-training. Allmodels achieve 100% training and test accuracy. and use empirical risk minimization to learn more complex functions . More advanced methodsthat regularize the conditional mutual information of simpler models compel them to utilize a broaderrange of features, which has been shown to enhance out-of-distribution generalization . SGDcan also learn poorly generalizable functions when the models parameters are initialized by trainingon random labels . Similarly, the features selected across data points are shown to be stronglycorrelated with neural feature matrix (NFM) . In the context of convolutional neural networks(CNNs), filter covariance is shown to closely mimic the average gradient outer product concerninginput . Despite these efforts, this bias remains a critical area of concern, necessitating furtherexploration to develop more robust and OOD generalizable neural models. Pretraining with noisy labels is known to alter the optimization trajectory and often change the localminima to which SGD converges . While adding noise is shown to help models generalize better,it has a tendency to overfit in overparameterized models . The effects of adding noise is shown tobe equivalent of employing a regularized loss function that depends on factors such as noise strength,batch size, etc. Noisy labels have also been proposed for robust-loss-functions but they are ofteninsufficient to learn accurate models . However, all these works do not explore the qualitativenature of diverse features that models learn. In this study, we investigate the learned decision-boundaries through the lens of feature diversity. Weparticularly focus on the role of parameter initialization in shaping the set of features utilized forclassification. Our empirical analysis shows that: A simple pre-training phase by optimizing log-loss over perturbed noisy-labels can beutilized to learn a more diverse family of functions by neural models. This is in contrast tothe usual convergence of models to a similar family of decision functions.",
  "Effect of Noisy Pre-training on Learned Decision Functions": "We study a two stage training procedure: First, we pre-train an overparameterized model on corruptedlabels (by randomly flipping the labels for a fraction of data points) called as noisy pre-training.This pre-trained model is then optimised to minimize the training loss, which naturally leads to ahigh training accuracy over the noisy-labels. Second, we utilize the pretrained model to once againtrain over the original unmodified labels. In subsequent sections, we compare this two-stage methodwith standard training without explicit regularization. Consider a d-dimensional synthetic Multi-slab dataset D, whose first coordinate is a linear blockwith instances for class-0 sampled from Uniform distribution in and class-1 sampled fromUniform distribution in . The remaining d 1 coordinates have samples from two classesdistributed in k-well separated alternating regions 3 . We consider a a 4-dimensional multi-slabdata with increasing complexity. illustrates the 4-dimensional multi-slab data. Here, anoverparameterized ReLU network can potentially use any feature to determine the class label. Note",
  "To eliminate any special association related to the ordering of features, we apply a random orthogonalprojection to the dataset, and use the projected features instead of the original feature set": "that, Feature-1 requires a simple linear decision function, whereas Feature-2 requires a 3piecewiselinear function. Standard training leads to decision functions that solely depend on feature-1 andthus learn a linear function. While this is perfectly good for linearly separable data, this tendencypersists even in cases with slight feature overlap in classes along Feature-1, diminishing the modelsgeneralizability. This observation aligns with the well-documented phenomenon of shortcut learning and the behavior of neural networks as max-margin classifiers . In case of noisy pretraining, this sole dependence on Feature-1 is lost, and dependence on otherfeature increases as shown in the . Randomized shuffle accuracy help understand thedependence on other features. shows the accuracy for randomly shuffling different features.Noisy pre-training increases the shuffle accuracy for feature-1 from 50.3% to 64.2%, thus showingthe increased dependence of the decision functions on other features. This is also evident fromthe reduced shuffle accuracy for feature-2 and feature-3 under noisy pre-training setting. The keyobservations are that the decision boundary is no longer solely reliant on the first dimension, but nowincorporates information from multiple dimensions, and different noisy-datasets, D and D, result indistinct decision boundaries. This is unlike earlier instances where the model consistently convergedto the same decision boundary. Significance of this observation: The benefits of moving beyond the simpler models typicallylearned by SGD-based optimizers are evident for improved out-of-distribution generalization. Ratherthan basing classification decisions on a single feature, the model begins to leverage a broader rangeof features. Additionally, this result provides evidence that there are multiple local minima withsimilar levels of accuracy across D, and that SGD is capable of converging to these diverse solutions,rather than being confined to a single-simple decision boundary.",
  "In this section, we empirically demonstrate the impact of initializing parameters from noisy pretrainingover Dominoes and Waterbirds data . See Appendix A.1 for more datasets": "Datasets: MNIST-fMNIST dataset from Dominoes consists of collated images, where {0, 1} MNISTdigits are vertically stacked with {shirt, frock} Fashion-MNIST apparel. Here, MNIST block is theeasier feature to learn, whereas fMNIST is complex. WaterBirds dataset comprises of birds nativelyfound on land/water, with different backgrounds of land or water. An image of water-bird with waterin the background (similarly land-bird on land) is called in-group partition, whereas mismatch ofwaterbird-on-land or landbird-on-water is referred to as out-group partition. Details in Appendix A.1. Measuring feature dependence:To identify the feature(s) that the learned model is utilizing forclassification decision, we use: (1). Randomized shuffle accuracy: to selectively shuffle one amongMNIST (top) or fMNIST (bottom) part of the test images, and reevaluate the model. If its performanceis drastically reduced by shuffling a feature, it implies the dependence of models decision on thecorresponding feature. (2). Visualizing Gram matrix: We plot diagonal entries of Gram matrixW T1 W1 to observe changes in the first layers parameter W1. The brighter parts of W T1 W1 denotegreater dependence on those corresponding pixels in the input image. Further, Appendix A.6 showseigenvector based visualization for Gram matrix. Varying correlation between features & class labels: We control the predictive powers of featuresby perturbing their correlation with true labels. For example: In Dominoes dataset D, we createpartially-correlated data D by collating images such that top MNIST-block is only 95% correlatedwith classification labels (not 100% accurately predictive). This implies that if predictions weresolely on the basis of top-block, then the bayes-error will be 5%. The rationale of adding adversarialcorrelation is to push the models to those regimes where no simple solution achieves perfect accuracy,thus getting closer to real world scenarios.",
  "D98.5 0.0793.1 0.3356.5 0.4299.9 0.0681.2 1.0257.2 1.50": ": Dominoes MNIST-FMNIST (Rnd. short for Randomized) : Randomized shuffle accuracies,averaged over 3 runs, with 100% (D) and 95% (D) correlation of MNIST with the true labels. Eachfor 3 and 4 layer fully connected networks. Noisy pre-training uses 10% corrupt labels. Noisy Pre-training Helps Learn Complex Features:We observe that neural models usually learnsimple decision functions that are restricted to easy-to-learn features. shows that randomizingfeature-1 reduces the accuracy to 50%, proving absolute reliance on this feature only. Similarly, shows randomizing top MNIST block reduces model accuracy to 52.5% and showsbetter performance for in-group images. Conversely, the model does not deteriorate performanceupon randomizing other complex features in all datasets. However, when we initialize parametersfrom noisy pretrained models, we see that not only over reliance of easy-features is reduced, themodels ignorance to other features is reduced. and 2 evince increase in randomized shufflingaccuracy and shows better out-group performance. Noisy Pre-training Helps Learn Diverse Functions:With different label noise, we observe that thedecision boundaries learned are varying across each feature. bottom-row shows two differentdecision boundaries for varying noise during pretraining. Diverse models are helpful because theytend to focus on different aspects of of high dimensional data. In practical scenarios, we can makemuch more robust predictions by bagging these multiple decision boundaries to give aggregated classprediction.",
  "Impact of Label Smoothing:Please refer Appendix A.4": "With noisy-pretraining and label smoothing, the models learn to focus on multiple features in Multi-slab and Dominoes data, and learn to classify images based on birds, rather than their backgrounds incase of WaterBirds. and 3 shows increase in the brightness of pixels in the Gram-matrix thatconfirms learning of diverse/complex features.Why are diverse models better?Model diversity is evident in slab-data from . It is interestingto note that each noisy pretraining leads to a different minina, and different family of function. Thepredictions from diverse models, focusing on diverse features, can be ensembled to have more robustpredictions. Similar trends are also evident from high standard-deviation values in accuracy of modelswith noisy pretraining in and 3. The theoretical validation of such ensembles superiority isfor further exploration.",
  "Discussion and Conclusion": "Overparameterized neural networks tend to learn decision functions based on a limited and simplerset of features if they exist. Our experiments demonstrate that initializing the model parameters byfirst training over noisy-labels can enable the model to overcome this constraint, and allow for thelearning of more diverse range of features. One possible explanation for this phenomenon is thatthe neural model reaches a minima that is characterized by a complex decision boundary to fit thenoisy labels. The model, however, remains trapped in this minima and is unable to revert to a simplerdecision boundary despite the intrinsic regularization properties of SGD. This observation challengesthe commonly accepted opinion that SGD provides effective implicit regularization. Our experiments motivate further investigation into the nature of the role of initialization in guidingSGD optimizers. Note that all the models, with or without noisy label pretraining, have similar(high) accuracy on unseen data. Since each local minima that is reached during the noisy pre-trainingleads to a distinct family of functions, it hints at interesting future works about characterizing loss-landscape of ReLU networks. This is in contrast to the standard data-agnostic initializations, suchas Xavier-Glorot, that converge to similar decision boundaries. Our work connects feature subsetselection with the learning of diverse families of functions. Future works will explore any theoreticalrelationships between these concepts and to quantify their interdependency. Our observations challenge the prevalent view in the literature that neural networks are extremelybiased towards learning only simple features if they exist. We demonstrate that models can be guidedto learn more complex features through pre-training on noisy labels. As a corollary, we show that thephenomenon of shortcut learning is not as extreme as previously thought and can be mitigated tosome extent. Our experiments further reveal that neural networks are capable of learning multiple,higher-complexity features, and diverse families of functions.",
  "Alex Damian, Tengyu Ma, and Jason Lee. Label noise SGD provably prefers flat globalminimizers. Curran Associates Inc., Red Hook, NY, USA, 2024": "Spencer Frei, Yuan Cao, and Quanquan Gu. Provable generalization of sgd-trained neuralnetworks of any width in the presence of adversarial label noise. In International Conferenceon Machine Learning, pages 34273438. PMLR, 2021. Suriya Gunasekar, Jason D Lee, Daniel Soudry, and Nati Srebro. Implicit bias of gradientdescent on linear convolutional networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman,N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems,volume 31. Curran Associates, Inc., 2018.",
  "Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10 (canadian institute for advancedresearch)": "Yann LeCun and Corinna Cortes. The mnist database of handwritten digits. 2005. Shengchao Liu, Dimitris S. Papailiopoulos, and Dimitris Achlioptas. Bad global minima existand SGD can reach them. In Advances in Neural Information Processing Systems 33, NeurIPS2020, December 6-12, 2020, virtual, 2020. Depen Morwani, Jatin Batra, Prateek Jain, and Praneeth Netrapalli. Simplicity bias in 1-hidden layer neural networks. In Proceedings of the 37th International Conference on NeuralInformation Processing Systems, NIPS 23, Red Hook, NY, USA, 2024. Curran Associates Inc.",
  "Nihal Murali, Aahlad Puli, Ke Yu, Rajesh Ranganath, and Kayhan Batmanghelich. Beyonddistribution shift: Spurious features through the lens of training dynamics, 2023": "Suzanne Petryk, Lisa Dunlap, Keyan Nasseri, Joseph E. Gonzalez, Trevor Darrell, and AnnaRohrbach. On guiding visual attention with language specification. 2022 IEEE/CVF Conferenceon Computer Vision and Pattern Recognition (CVPR), pages 1807118081, 2022. Mohammad Pezeshki, Skou-Oumar Kaba, Yoshua Bengio, Aaron Courville, Doina Precup, andGuillaume Lajoie. Gradient starvation: a learning proclivity in neural networks. In Proceedingsof the 35th International Conference on Neural Information Processing Systems, NIPS 21, RedHook, NY, USA, 2024. Curran Associates Inc. Adityanarayanan Radhakrishnan, Daniel Beaglehole, Parthe Pandit, and Mikhail Belkin. Mech-anism of feature learning in deep fully connected networks and kernel machines that recursivelylearn features. 2022. Shiori Sagawa, Aditi Raghunathan, Pang Wei Koh, and Percy Liang. An investigation of whyoverparameterization exacerbates spurious correlations. In Proceedings of the 37th InternationalConference on Machine Learning, ICML20. JMLR.org, 2020. Harshay Shah, Kaustav Tamuly, Aditi Raghunathan, Prateek Jain, and Praneeth Netrapalli. Thepitfalls of simplicity bias in neural networks. In Advances in Neural Information ProcessingSystems NeurIPS 2020, Dec 6-12, 2020, virtual, 2020.",
  ": Noise augmented multi-slab dataset": "Dominoes Data: We consider Dominoes binary classification datasets consisting of 3 independentdatasets, where the top half of the image contains MNIST digits from classes 0, 1, and thebottom half contains MNIST images from classes 7, 9 (MNIST-MNIST), Fashion-MNIST images from classes coat, dress (MNIST-Fashion) or CIFAR-10 images from classes car, truck(MNIST-CIFAR). In all Dominoes datasets, the top half of the image (MNIST 0-1 images) presents aeasy to learn feature; the bottom half of the image presents a harder-to-learn feature. The imagesare made into gray-scale and scaled to appropriate size so that they can be collated top-bottom style. shows the examples of dominoes dataset. In case of 100% correlation Note that each block,top and bottom, is fully predictive of the class label in case of 100% correlation, while only harder tolearn feature is fully predictive of class label in case of 95% correlation. Waterbirds Dataset: The waterbirds dataset is constructed by cropping birds from Caltech-UCSDBirds-200-2011 (CUB) dataset and taking backgrounds from the Places dataset and placingthe birds from CUB dataset to backgrounds from Places dataset. shows sample instancesfrom waterbirds dataset. describes the proportions of different groups in the waterbirdsdataset. The waterbirds on land and landbirds on water are known as out-group, while landbird onland and waterbirds on water are known as the in-group.",
  "A.2Network Architectures": "Slab Dataset: We use 2-layer multi-layer perceptron network with ReLU activation, the first layercontains 100 hidden unit and second layer contains 200 hidden units. We use SGD with 0.1 learningrate as optimizer. Dominoes Dataset: For all dominoes dataset, MNIST-FMNIST, MNIST-MNIST, and MNIST-CIFAR, datasets, we use 4-layer fully connected networks with 10 hidden units and ReLU activation.We use SGD optimizer with learning rate 0.01.",
  "A.3.14D Slab Data Decision Boundary plots for Different Random Seeds": "In this section, we show that under standard training, the model converges to a similar family ofdecision functions, which are easier to learn. This convergence does not happen under pretrainednoise based training, which allows model to learn diverse set of functions with reliance on harder tolearn features. and 10 demonstrates this finding using decision boundary plots for 4d slabdata described in . -101 Feature-1 Feature-2 Feature-2 Feature-3 Feature-3 Feature-4 class-0class-1 -101 Feature-1 Feature-2 Feature-2 Feature-3 Feature-3 Feature-4 class-0class-1 -101 Feature-1 Feature-2 Feature-2 Feature-3 Feature-3 Feature-4 class-0class-1 -101 Feature-1 Feature-2 Feature-2 Feature-3 Feature-3 Feature-4 class-0class-1 -101 Feature-1 Feature-2 Feature-2 Feature-3 Feature-3 Feature-4 class-0class-1 -101 Feature-1 Feature-2 Feature-2 Feature-3 Feature-3 Feature-4 class-0class-1",
  "A.4Impact of Label Smoothing": "Label smoothing is a regularization technique, which replaces the one-hot ground truth vectors,with mixture of ground truth vectors and uniform distribution. In case of noisy pretraining, flip thelabels of a fraction of data samples, whereas in case of label smoothing, the ground truth labels aremixture of one hot vector and uniform distribution, which act as a addition of noise to labels, thusshowing equivalent effect."
}