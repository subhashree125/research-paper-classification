{
  "Abstract": "We study fair allocation of constrained resources, where a market designer opti-mizes overall welfare while maintaining group fairness. In many large-scale set-tings, utilities are not known in advance, but are instead observed after realizingthe allocation. We therefore estimate agent utilities using machine learning. Op-timizing over estimates requires trading-off between mean utilities and their pre-dictive variances. We discuss these trade-offs under two paradigms for preferencemodeling in the stochastic optimization regime, the market designer has accessto a probability distribution over utilities, and in the robust optimization regimethey have access to an uncertainty set containing the true utilities with high prob-ability. We discuss utilitarian and egalitarian welfare objectives, and we explorehow to optimize for them under stochastic and robust paradigms. We demonstratethe efficacy of our approaches on three publicly available conference reviewer as-signment datasets. The approaches presented enable scalable constrained resourceallocation under uncertainty for many combinations of objectives and preferencemodels.",
  "Introduction": "Constrained resource allocation without money underpins many important systems, including re-viewer assignment for peer review (our primary example throughout the paper) ,assigning resources to homeless populations , distributing emergency response resources, and more . In these settings we assign resources to agents. Agents and re-sources are constrained; each agent has bounds on the minimum or maximum number of items theyreceive from different categories, and each item has required minimums and limited total capacity.Each agent has a valuation for every item, and we optimize a welfare function of the agent-itemvaluations. In the case of reviewer assignment, the reviewer-paper valuations measure the alignmentbetween reviewers and papers, papers must receive a certain number of reviews from unique review-ers, reviewers have upper limits on the number of papers they can review, and conflicts of interestprevent some reviewers from being assigned to certain papers. A crucial factor in all of the above settings is the presence of uncertainty. Uncertainty often stemsfrom the fact that agents valuations for resources depend on future outcomes. In reviewer assign-ment, a reviewer-paper pairs match quality is observed only after the reviewer submits his or herreview. Uncertainty may also stem from our limited ability to collect data; for example, in decidingwhere to target lead pipe mitigation projects based on number of school-aged children per neighbor-hood, we may have access to imperfect school enrollment records, allowing only an approximatemodel of the impacts of mitigation on children in each neighborhood . We adopt two possiblestances towards uncertainty, depending on the information available. When we have access to a",
  "arXiv:2411.02654v1 [cs.GT] 4 Nov 2024": "probability distribution over preferences, we optimize the conditional expectation of the distribu-tion at percentiles of interest . When we have access to a set of possible preferences, weadopt the robust approach, which is related to the minimax regret objective used in solving robustassignment problems . Uncertainty-aware optimization approaches can often result insignificantly different allocations from the default of optimizing for welfare over a central estimate(see Example 2.1 for an intuitive explanation for this phenomenon). Typically, we maximize the sum of agent utilities. However, in many of these settings, we are alsoconcerned with fairness to individuals or groups of agents. Groups of agents may represent subjectareas of papers in reviewer assignment, demographic groups in poverty alleviation campaigns, orregional groupings of computational resources in bandwidth allocation. Fairness to these groupsmay be legally required in some cases; in others it is an ethical choice by the decision maker.Although groups are often first-class objects worthy of receiving fair treatment, group fairness isoften the smallest granularity of fairness achievable under uncertainty in a large dataset uncertaintywill always cause some individuals to have vanishing welfare, but group welfare can still be upheld.Although there is much literature on combinatorial optimization under uncertainty ,to our knowledge it has not addressed the intersection of fairness and uncertainty in the constrainedmulti-matching problem.",
  "Our Contributions": "We study the broad problem of fair and efficient constrained multi-matchings under uncertaintyabout agents valuations. We optimize for welfare while simultaneously accounting for the uncer-tainty inherent in real-world resource allocation problems. Specifically, we develop methods to effi-ciently optimize the utilitarian and egalitarian welfare objectives using the robust approach and the CVaR approach . Our results are summarized in . For robust optimization, we construct an uncertainty set containing the true preferences with highprobability (). This model is appropriate when building a predictor with statistical er-ror bounds, but without making any assumptions on the full probability distribution over valuations.For utilitarian and egalitarian welfare functions, we robustly maximize welfare over such uncertaintysets. When the uncertainty sets are linear we can efficiently compute the exact optimal allocationsfor both utilitarian and egalitarian welfare in polynomial time (Corollaries 3.2 and 3.6). Under asingle ellipsoidal uncertainty set, we can apply an iterated quadratic programming approach (Corol-laries 3.3 and 3.7), while a projected subgradient ascent approach is needed when uncertainty setsconsist of multiple ellipsoids (Propositions 3.1 and 3.5). Under general monotonic, concave wel-fare functions and arbitrary convex uncertainty sets, we apply the relatively expensive adversarialprojected subgradient ascent algorithm of Cousins et al. . When the market designer can construct a full probability distribution over preferences or sam-ple from such a distribution, we consider stochastic optimization using the concept of ConditionalValue at Risk, or CVaR . This approach, laid out in , selects an allocation that maxi-mizes the conditional expectation of welfare over the left tail of the welfare distribution. We oftenapproximate CVaR objectives using sampling, then solve the resulting linear program or LP (asin Propositions 4.1 and H.3). However, in the case of utilitarian welfare and Gaussian-distributedvaluations we present a simple reformulation of the CVaR objective (Proposition 4.3). Optimiz-ing CVaR for general monotonic, concave welfare functions can require solving arbitrary concaveoptimization problems, even after sampling.",
  "We discuss the history of prior work on robust and CVaR optimization in Appendix A": "Some existing work applies stochastic or robust optimization to fair division problems. A line ofwork studies the minimax regret objective in combinatorial optimization problems, such as con-strained resource allocation . This work does not explicitly consider multi-matchingproblems like those considered here, nor does it address the robust egalitarian welfare problem. Pu-jol et al. study fair division problems with parameters noised for differential privacy, showing : Summary of optimization algorithms for efficiently computing utilitarian and egalitarianwelfare under different robustness concepts. Green highlights indicate problems which require solv-ing a single linear program (low difficulty). Yellow highlights indicate solving a small number oflinear or quadratic programs (medium difficulty). Red highlights indicate problems which requiresolving numerous quadratic programs or arbitrary concave programs.",
  "Sampling + Concave Program(Sec. 4)": "that the noise can cause unfair allocations; they propose a Monte Carlo approach to mitigate theunfairness with high probability. Peters et al. study envy-free rent division under probabilisticuncertainty. A central mechanism divides rooms and sets room prices for the items to minimizeenvy. We study a setting without money, both utilitarian and egalitarian objectives, and robust opti-mization in addition to stochastic optimization. Cousins et al. study robust optimization under the utilitarian objective. They propose an adver-sarial projected subgradient ascent method which requires solving a two quadratic programs (onefor the adversary and one for the projection) at each iteration for a large number of iterations. Ourempirical analysis in demonstrates the inefficiency of this method. Fair machine learningalgorithms often employ similar adversarial optimization techniques over anuncertainty set in a machine learning context. Other fair allocation research has studied the casewhere agent demand or item availability are uncertain but preferences are known . Inour case demand and availability are known but preferences are not. Devic et al. consider fairtwo-sided matching where the fairness constraint is defined with respect to unknown parameters; weassume knowledge of the parameters that define the fairness constraint (i.e., group identities).",
  "Fair Resource Allocation": "We have a set of n agents N = {a1, . . . an}, and m item types I = {i1, . . . im}. Agents arepartitioned into g groups G = {G1, . . . Gg}, with each G N and each agent i belonging toexactly one group. For any n m matrix X we use the same lower-case bold letter, i.e., x to denote the vector repre-senting the vectorized form of the matrix X, in row-major order. For any group of agents G, we usexG R|G|m to denote the vector restricted to the agents in G. Given vectors x, y Rnm and realnumber c R, let x c denote that xj c for all j, and let x y denote that x y 0. The operator is defined analogously. We assume a valuation matrix V Rnm0+, where V a,i encodes the true value of assigning itemtype i to agent a. The values of V are typically unknown; we discuss our approaches to handle thisproblem in .2. We use tildes to denote random variables, for example, x Dx denotes thatx is drawn from distribution Dx.",
  "upper and lower bounds on assignments of the form a iI Aa,i a. For each item i, wehave lower and upper bounds on the total assignment of that item; i": "aN Aa,i i. Finally,we have pairwise limits Ca,i for each agent a and item type i, requiring that Aa,i Ca,i. It isalways the case that the constraints define a finite set such that|A| N. In the example of reviewerassignment, these constraints reflect the review requirements per paper, load bounds for reviewers,conflicts of interest, and the constraint that no reviewer is assigned twice to any given paper. Let u : A Rnm0+ Rg be an affine function mapping from allocations to utilities for each group.uG(a, v) denotes the utility of the group G under allocation a A (recall a is the vectorized versionof the assignment A). We write u instead of u(a, v) when a and v are clear from context. We assume that u is additive and normalized by group size, so uG = aGvG|G| . We define a welfarefunction W : Rg R, where W(u(a, v)) denotes the overall welfare of allocation a. The weightedutilitarian social welfare function is defined as wu, where w Rg0+ denotes the weights on groupsin G. When wG = |G| for all G, we call this function simply utilitarian welfare or USW. Thegroup egalitarian social welfare function (also group egalitarian welfare or GESW) is definedas minGG uG. We do not consider individual egalitarian welfare in this work; under robust andstochastic optimization the egalitarian welfare is zero when the number of items is proportional tothe number of agents and uncertainty is non-trivial.",
  "We consider two main approaches to dealing with uncertainty: the robust optimization approach andthe Conditional Value at Risk approach": "In the robust approach (), we obtain an uncertainty set V that contains the true agentvaluations v with probability at least 1 for some confidence parameter [0, 1).Wethen optimize the welfare corresponding to the worst valuation matrix in the uncertainty set, i.e.,maxaA minvV W(u(a, v)). This approach applies when we do not have access to a full distribu-tion over valuations but have some other way of describing V . When we have access to a full distribution over the random variable v Rnm, we apply a stochasticapproach instead. We compute the welfare distribution and optimize the conditional expectationover an -percentile of the welfare or Conditional Value at Risk at (CVaR). Suppose we havea random variable x Dx. For any risk level (0, 0.5), CVaR[x] is defined as ExDx[x |x ] where denotes the -percentile of x. This approach is only appropriate when Dx isfully known, or when we can sample from it. We investigate this regime in , where we willcompute and optimize CVaR[W(u(a, v))] for a random variable v Dv. Example 2.1 (The Importance of Considering Uncertainty). Consider a simple two-agent, two-iteminstance, where each agent needs to get exactly one item, and either likes (utility 1) or dislikes it(utility 0). Agent preferences are Bernoulli random variables, where Pr[v1,1 = 1] = 0.8, Pr[v1,2 =1] = 0.9, Pr[v2,1 = 1] = 0.5, and Pr[v2,2 = 1] = 0.8. If we maximize the expected USW, we wouldassign i1 to a1 and i2 to a2, for a total expected USW of 1.6. However, consider instead the CVaR0.3of USW. When we make the expectation-maximizing assignment, then Pr[W(u) = 0] = 0.04 andPr[W(u) = 1] = 0.32. However, if we assign i2 to agent a1 and item i1 to agent a2, we have thatPr[W(u) = 0] = 0.05 and Pr[W(u) = 1] = 0.5. This means that the conditional expectation ofwelfare at the 30th percentile is higher if we assign i2 to a1 and i1 to a2 (it is .32 in the first case and.5 in the second case). If we want to retain welfare in the face of uncertainty, we might well chooseto maximize this quantity rather than the expectation of the welfare.",
  "Robust Welfare Optimization": "We construct the optimization problems for utilitarian and egalitarian welfare objectives with therobust approach. Many of these optimization problems are concave-convex max-min problems thatcan be directly solved using an adversarial projected subgradient ascent technique : in eachiteration of the algorithm, the inner minimization problem is solved to optimality, followed by asubgradient step on the allocation a, followed by a projection onto the constraint space A. However,this method does not exploit the structure of these problems and is often computationally expen-sive or intractable, as demonstrated empirically in . Despite the inherent complexities of these problems, we show that under specific assumptions, these problems can be reduced to moremanageable forms that are easier to optimize. We then discuss a range of algorithms for efficientlyoptimizing the simplified problems. Scope:The robust approach detailed in assumes the availability of an uncertainty set ofthe valuation matrix. For the sake of computational tractability, we focus on the class of uncertaintysets defined by linear and ellipsoidal constraints",
  "where the ith ellipsoidal uncertainty set has center vi Rnm0+ , covariance matrix Si Rnmnm,with radius ri R, Q Rknm, and e Rk": "We will further assume that the covariance matrices corresponding to the ellipsoidal uncertainty setsare positive semi-definite. This limitation on the structure of uncertainty sets is not too restrictive;it is possible to construct such uncertainty sets for linear regression and logistic regression modelsusing statistical bounds, as shown in Appendix D. In all of our methods where obtaining an integer allocation is either not feasible or computationallytractable, we relax the set of feasible integer assignments A Nnm to a set of feasible continuousallocations A Rnm0+. One can obtain integer allocations satisfying all constraints by applyinga randomized rounding technique that generalizes the Birkhoff-von Neumann decomposition .The fractional solutions can thus be interpreted as randomized allocations.",
  "maxaA minvV w u(a, v) .(1)": "The objective and constraints of the inner-minimization problem described in (1) are convex. Theproblem is strictly feasible, which satisfies Slaters condition for strong duality. Therefore, bytaking the dual of the inner-minimization problem, we can simplify the problem in (1) into a singleequivalent maximization problem. We provide the dual formation in Proposition 3.1. To simplify notation in the results that follow, we assume, without loss of generality, that eachgroup G has weight wG = |G|. In practice, if this assumption does not hold, the weights can beincorporated into the valuations v with a corresponding adjustment to the parameters of the valuationuncertainty set V. In the dual, let Rk0+ be the dual variable corresponding to the linear constraints Qv e, R0+ be the dual variable associated with the ellipsoidal constraints, and Rnm be thevariable that combines the primal variable a with the dual variable of the non-negativity constrainton v for variable elimination. We define a set of feasible as",
  "Proposition 3.1 shows that the optimal allocation for the problem in Equation (1) can be computedby first solving the concave program in Equation (3) to obtain and then deriving the optimal": "allocation a from by solving a system of equations. Notably, the problem in Equation (3)is a single maximization problem with fewer variables and constraints as compared to the max-min problem in (1), making it simpler to solve. We can either solve it using off-the-shelf convexoptimization tools, or by applying a projected subgradient ascent approach (without the previouslyrequired adversary). When the valuation uncertainty set is polyhedral, the problem in (3) simplifies further into a linearprogram (LP) which can be solved efficiently using standard LP solvers like Gurobi .Corollary 3.2 (Utilitarian Welfare with Polyhedral Uncertainty). In the case where the uncertaintyset V is defined purely by linear constraints, i.e., V = {v Rnm | Qv e, v 0}, the optimalallocation a for the problem in (1) can be computed by solving the linear program",
  "We now consider the problem of maximizing egalitarian welfare under the robust approach. We canrepresent this problem asmaxaA minvV minGG uG(a, v) .(5)": "This problem presents inherent challenges due to the non-smoothness of the inner-minimizationproblem and the joint constraint on the uncertainties of the valuation matrices of different groups.These factors make it difficult to compute the dual and reduce the problem or efficiently solve theproblem using the quadratic program technique described in Corollary 3.3, though the generic ad-versarial subgradient ascent approach of Cousins et al. can still be applied. For the remainder ofthis section, we assume that the uncertainty sets for each group G G are independent of each other.To simplify notation, we assume, without loss of generality, that the valuations v and the parametersof the valuation uncertainty set V are scaled to incorporate the factor1|G| in the representation of theutility of each group G in the corresponding group valuation vG.",
  "GG VG": "This assumption is not unreasonable in practical scenarios. For example, conferences often grouppapers into disjoint tracks or require paper authors to select a single primary subject area. Althoughpapers may have multiple secondary subject areas, the top-level grouping remains independent. As-sumption 3.4 allows us to reorder the two minimization problems without compromising generality:",
  "maxaA minGGminvGVG aGvG .(6)": "We take the dual of the inner minimization problem, then reorder the minimization over groups andthe maximization over the dual variables to obtain a single, concave max-min problem. This can besolved with projected subgradient ascent in the general case, or with more efficient approaches inspecial cases. Proposition 3.5 expresses the general form of the result.",
  "where for each group GG, pG = G QGG, qG = i=1 G,iS1G,ivG,i, T=i=1, G,iS1G,i1, and is defined as in Equation (2). The optimal allocation a can be com-puted from as in Proposition 3.1": "The dual variables G, G, G and G for each group G are interpreted as in Proposition 3.1. Theoptimization problem in (7) is concave with respect to the dual variables , and . We can solveit using an off-the-shelf convex programming library or by applying projected subgradient ascent. Under polyhedral uncertainty sets, Equation (7) simplifies to a linear program. This is akin to whatwe observe in the robust utilitarian case (Corollary 3.2).Corollary 3.6 (Group Egalitarian Welfare with Polyhedral Uncertainty). In the case where the un-certainty set V is defined only by linear constraints, i.e., V = {v Rnm | Qv e, v 0}, themax-min-min problem in (5) transforms into a linear program. When the valuation uncertainty set is defined by a single ellipsoidal constraint per group, we canemploy the iterated quadratic programming (Iterated QP) approach used in Corollary 3.3, alternatelyfixing and optimizing the rest of the dual variables (, ) until convergence.Corollary 3.7 (Group Egalitarian Welfare with Ellipsoidal Uncertainty). Suppose that the set V in(5) is defined by a single truncated ellipsoidal constraint per group i.e., V = {v Rnm | G G :(vG vG)S1G (vG vG) r2G, v 0}. Then the problem in (5) is equivalent to solving",
  "minGG GvG GSGG4G Gr2G": "The exact optimal solution (, ) to Equation (4) can be computed by alternately performing twosteps until convergence: first, fixing and optimizing , i.e., G G, G = GSGG/2rG, andsecond, fixing and solving a concave quadratic program to optimize . The optimal allocation acan be computed from as in Proposition 3.1.",
  "Robust Allocation for Monotonic Welfare Functions": "We now extend our findings to a broader class of monotonic welfare functions. Specifically, we showthat when optimizing a monotonic welfare objective under Assumption 3.4, we can decomposethe problem into sub-problems such that we independently determine the worst valuation in theuncertainty set of each group, while jointly optimizing the allocation over all groups.Proposition 3.8 (Decomposition for Monotonic Welfare Functions). Consider an optimizationproblem of the formmaxaA minvV WM(u(a, v)) ,(8)",
  "minvG1VG1uG1(aG1, vG1),minvG2VG2uG2(aG2, vG2), . . . ,minvGg VGguGg(aGg, vGg)": "Proposition 3.8 helps us derive simplified versions of Equation (8), when Assumption 3.4 holds.The egalitarian problem in (5) is an instance of the class of optimization problem described in (8),hence Proposition 3.8 holds under Assumption 3.4 and allows us to derive a single maximizationproblem (Proposition 3.5). If the allocation and valuation uncertainty sets are convex and compact,the problem in (8) can be solved using constrained convex-concave minimax optimization algo-rithms , or adversarial projected gradient ascent . These approaches do not dependon Assumption 3.4, though the optimization may be simplified if independence does hold.",
  "Stochastic Welfare Optimization": "In this section, we optimize the CVaR of utilitarian and egalitarian welfare. This approach workswhen the distribution Dv over the valuation matrix is known, or when we can sample from Dv. Wedemonstrate that when the distribution follows a Gaussian distribution, the CVaR of the utilitarianwelfare has a simple representation that can be optimized without sample approximation using aprojected gradient ascent method. In all other cases, we can approximately optimize CVaR usinga sampling-based approach. In particular, when we have monotone, concave welfare functions, wecan always approximate the CVaR objective using sampling. However, unlike in Propositions 4.1and H.3, where the approximated problem becomes linear, with arbitrary monotone, concave welfarefunctions the problem may require general concave optimization.",
  ",(9)": "where (x)+ = max(x, 0) represents the positive part of x . Computing the exact expectation inthis problem may not be feasible for every distribution Dv. Therefore, we adopt a sampling-basedapproach. We begin by drawing h i.i.d. samples of the valuation matrix from Dv represented asv1, v2, v3, . . . , vh. We then use these samples to solve the problem described in (9) by solving thelinear program outlined in Proposition 4.1.",
  "b w u(a, vj).(10)": "The CVaR estimator used in (10) is a strongly consistent estimator . Therefore, the approxima-tion error of the objective in (10) goes to 0 as h . In Proposition 4.2, we bound the samplecomplexity of the problem in (10) when the valuation matrix is sub-Gaussian distributed. For any allocation a, let ch,(a) represent the empirical estimate of CVaR[w u(a, v)] computedfrom h samples and ch,(a) represent the corresponding true value. We will use |A| to denote thenumber of feasible allocations and fa : R R0+ to denote the density function of the randomwelfare W(a, v). denotes the percentile of W(a, v).Proposition 4.2 (Sample Complexity of Approximate CVaR of USW). Suppose that v is a multi-variate sub-Gaussian random variable with mean v Rnm and covariance proxy S Rnmnm,i.e., for all vectors z Rnm : EvDvexp((v v)z)) exp(zSz/2), and that, for any risklevel (0, 1",
  "Experiments": "We run experiments on three reviewer assignment datasets. The datasets contain bids from theInternational Conference on Autonomous Agents and Multiagent Systems (AAMAS) 2015, 2016,and 2021 . We consider the papers as the agents and the reviewers as the items. This isa fairly standard assumption in most recent reviewer assignment approaches, reflecting the primarygoal of peer review to assign qualified and interested reviewers to papers . Reviewers issue bids of yes, maybe, no, or no response. We run two experiments with thisdata. In one, we binarize the bids such that yes and maybe are considered affirmative and no isconsidered negative, while in the other we convert the bids to numerical scores such that yes is 1,maybe is .5, and no is 0.01. Under the binarized model, we fit a logistic matrix factorization modelto predict whether the bid is affirmative or negative, and in the continuous model, we fit a Gaussianprocess matrix factorization model . We derive probability distributions and uncertainty setsfrom these models. More details on prediction and uncertainty set construction are in Appendix E.These datasets do not contain groups of papers and reviewers, so we create 4 roughly balancedclusters of reviewers and papers for each dataset using the procedure outlined in Appendix F. Wedefine our valid set of assignments A as follows. For each paper a N, we set a = a = 3 for alla in AAMAS 2015, and a = a = 2 for all a in AAMAS 2016 and 2021. For each reviewer i, weset i = 0 and i = 15 for 2015 and 2016 and 4 for 2021. We optimize and evaluate CVaR0.01; wetake 4, 000 samples from the distribution to optimize for CVaR using the sampling-based approach,and we take 10, 000 samples to estimate the CVaR for evaluation. We optimize and evaluate therobust welfare at the = 0.3 level (there is a 70% chance the true values lie in the uncertaintyset). We constrain the nave and CVaR approaches to select integer allocations, while the robustapproach selects fractional allocations without rounding. All results are averaged over 5 runs of subsampling 20% of each dataset. For each run, we construct 6allocations, maximizing the nave central estimate, CVaR and robust statistics for USW and GESWrespectively. We evaluate each allocation on each metric. For each run, we normalize each metricby the maximum value achieved for that metric by any allocation. We normalize in this manner tohighlight that the allocation targeted for a given objective always returns the highest value on thatobjective, and because the absolute optimal values differ across runs.",
  "All code is available at": "Overall Performance shows the results for the binarized version of AAMAS 2015 bids.Similar tables for the 5 other settings are included in Appendix G. Each row shows the metrics forthe allocation produced by the method which optimizes for the objective shown in the left-mostcolumn. All non-robust methods have 0 robust welfare, indicating that if robustness to adversarialnoise is desired, it is very important to consider this objective explicitly. Relatively little noise isactually present in this dataset, as the CVaR0.01 is quite high for both the nave USW-optimal andGESW-optimal in all cases. Since the USW-optimal solution has very high GESW, we implement a simulated example to ex-plore when the USW-optimal solution fails to have high GESW. We find that in a number of settings,the GESW of the USW-optimal solution is much lower than the GESW-optimal solution. Appendix Iexplains the details of the simulation setting and the results.",
  "AllocationEvaluation ObjectiveUSWGESWCVaR USWCVaR GESWRob. USWRob. GESW": "USW1.00 01.00 01.00 01.00 00 00 0GESW0.97 0.011.00 00.97 0.010.97 0.020 00 0CVaR USW1.00 00.99 01.00 00.99 00 00 0CVaR GESW0.98 00.99 00.97 0.011.00 00 00 0Rob. USW0.92 0.010.90 0.020.92 0.010.90 0.021.00 01.00 0Rob. GESW0.89 0.040.85 0.060.89 0.040.86 0.060.88 0.021.00 0 Robustness under Increasing Uncertainty shows the CVaR0.01 on the Gaussian versionof all three datasets as we artificially increase the amount of noise. We multiply the standard devia-tions of the Gaussian distributions by a scalar and optimize for the CVaR or the navely-computedUSW and GESW. We then plot CVaR0.01 as noise increases. Although the CVaR approach is lessimportant at low noise levels, the CVaR of welfare decreases for both welfare measures as noiseincreases. GESW has a sharper decline than USW. We see that as the noise increases, the CVaR0.01of the baseline USW and GESW maximizing allocations drops off relative to the same value for theCVaR-optimized allocation. We also verify that when we model valuations using a negatively-skewed Gaussian distribution withthe same means and variances, we see increasing importance of optimizing for CVaR relative touncertainty-unaware USW and robust USW. The difference is sharper as the skew parameter getsmore negative. Details of this experiment and its results are included in Appendix J. Runtime For the robust optimization setting with ellipsoidal uncertainty sets (derived from con-fidence intervals over the Gaussian process matrix factorization), we compare the Iterated QP ap-proach (Corollary 3.3) to adversarial projected subgradient ascent on the original max-min problem(as in ). We find that Iterated QP converges much faster than the adversarial projected subgradi-ent ascent algorithm on both AAMAS 2015 () and 2016 (). Adversarial projectedsubgradient ascent fails to converge in 1, 000 iterations for the robust GESW objective on all datasetsand the USW objective on AAMAS 2021.",
  "Conclusion": "In conclusion, we explore the stochastic and robust optimization regimes for utilitarian and groupegalitarian welfare objectives. The robust optimization algorithms depend on the form of the uncer-tainty set. We show that when the uncertainty set has linear constraints only, the resulting problemis an LP and can be solved efficiently. Under ellipsoidal constraints, we demonstrate an iterativequadratic programming approach converges much faster than adversarial projected subgradient as-cent. In the stochastic regime, we lay out the sample complexity of CVaR for the utilitarian welfareobjective. We demonstrate the feasibility of estimating probability distributions and uncertainty setson three years of bid data from AAMAS, and show that the robust and CVaR approaches demon-strated in this paper combat the uncertainty present in these three datasets.",
  "and Disclosure of Funding": "This work is generously supported by Army Research Lab DEVCOM Data and Analysis Center -Contract W911QX23D0009 and NSF grant IIS-2327057. Cousins was supported by the Universityof Massachusetts Center for Data Science Fellowship. This work was performed using high perfor-mance computing equipment obtained under a grant from the Collaborative R&D Fund managed bythe Massachusetts Technology Collaborative. Emily L Aiken, Guadalupe Bedoya, Joshua E Blumenstock, and Aidan Coville. Program target-ing with machine learning and mobile phone data: Evidence from an anti-poverty intervention inafghanistan. Journal of Development Economics, 161:103016, 2023. Amine Allouah, Christian Kroer, Xuan Zhang, Vashist Avadhanula, Anil Dania, Caner Gocmen,Sergey Pupyrev, Parikshit Shah, and Nicolas Stier. Robust and fair work allocation. arXiv preprintarXiv:2202.05194, 2022.",
  "Haris Aziz, Evi Micha, and Nisarg Shah. Group fairness in peer review. Proceedings of the 37thAnnual Conference on Neural Information Processing Systems (NeurIPS), pages 6488564895,2023": "Mohammad Javad Azizi, Phebe Vayanos, Bryan Wilder, Eric Rice, and Milind Tambe. Design-ing fair, efficient, and interpretable policies for prioritizing homeless youth for housing resources.In Proceedings of the 15th International Conference on the Integration of Constraint Program-ming, Artificial Intelligence, and Operations Research (CPAIOR), pages 3551, 2018. Adelchi Azzalini and Antonella Capitanio.Statistical applications of the multivariate skewnormal distribution. Journal of the Royal Statistical Society: Series B (Statistical Methodology),61(3):579602, 1999.",
  "John R Birge and Francois Louveaux. Introduction to stochastic programming. Springer Science& Business Media, 2011": "Craig Boutilier, Relu Patrascu, Pascal Poupart, and Dale Schuurmans. Constraint-based op-timization and utility elicitation using the minimax decision criterion. Artificial Intelligence,170(8-9):686713, 2006. Craig Boutilier. Computational decision support: Regret-based models for optimization andpreference elicitation. In Thomas Zentall and Philip Crowley, editors, Comparative DecisionMaking, chapter 14, pages 423453. Oxford University Press, 2013.",
  "Eric Budish, Yeon-Koo Che, Fuhito Kojima, and Paul Milgrom. Implementing random assign-ments: A generalization of the Birkhoff-von Neumann theorem. In Cowles Summer Conference,2009": "Jan Buermann, Enrico H Gerding, and Baharak Rastegari. Fair allocation of resources with un-certain availability. In Proceedings of the 19th International Conference on Autonomous Agentsand Multi-Agent Systems (AAMAS), pages 204212, 2020. Ioannis Caragiannis, David Kurokawa, Herve Moulin, Ariel D Procaccia, Nisarg Shah, andJunxing Wang. The unreasonable fairness of maximum Nash welfare. ACM Transactions onEconomics and Computation (TEAC), 7(3):132, 2019. Cyrus Cousins, Justin Payan, and Yair Zick. Into the unknown: Assigning reviewers to paperswith uncertain affinities. In Proceedings of the 16th International Symposium on AlgorithmicGame Theory (SAGT), pages 179197, 2023. Cyrus Cousins. An axiomatic theory of provably-fair welfare-centric machine learning. In Pro-ceedings of the 35th Annual Conference on Neural Information Processing Systems (NeurIPS),pages 1661016621, 2021. Constantinos Daskalakis, Stratis Skoulakis, and Manolis Zampetakis. The complexity of con-strained min-max optimization. In Proceedings of the 53rd Annual ACM Symposium on Theoryof Computing (STOC), page 14661478, 2021.",
  "Anand Deo and Karthyek Murthy. Efficient black-box importance sampling for VaR and CVaRestimation. In Proceedings of the 2021 Winter Simulation Conference (WSC), pages 112, 2021": "Siddartha Devic, David Kempe, Vatsal Sharan, and Aleksandra Korolova. Fairness in match-ing under uncertainty. Proceedings of the 40th International Conference on Machine Learning(ICML), pages 77757794, 2023. Kate Donahue and Jon Kleinberg. Fairness and utilization in allocating resources with un-certain demand. In Proceedings of the 3rd ACM Conference on Fairness, Accountability andTransparency (FAT*), pages 658668, 2020.",
  "Gurobi Optimization, LLC. Gurobi Optimizer Reference Manual, 2023. Accessed August 21,2024": "L. Jeff Hong and Guangwu Liu. Monte Carlo estimation of value-at-risk, conditional value-at-risk and their sensitivities. In Proceedings of the 2011 Winter Simulation Conference (WSC),pages 95107, 2011. Steven Jecmen, Hanrui Zhang, Ryan Liu, Nihar Shah, Vincent Conitzer, and Fei Fang. Mitigat-ing manipulation in peer review via randomized reviewer assignments. In Proceedings of the 34thAnnual Conference on Neural Information Processing Systems (NeurIPS), pages 1253312545,2020. Ari Kobren, Barna Saha, and Andrew McCallum. Paper matching with local fairness con-straints. In Proceedings of the 25th International Conference on Knowledge Discovery and DataMining (KDD), pages 12471257, 2019.",
  "Pavlo Krokhmal, Jonas Palmquist, and Stanislav Uryasev. Portfolio optimization with condi-tional value-at-risk objective and constraints. Journal of Risk, 4:4368, 2002": "Amanda R Kube, Sanmay Das, and Patrick J Fowler. Fair and efficient allocation of scarceresources based on predicted outcomes: Implications for homeless service delivery. Journal ofArtificial Intelligence Research, 76:12191245, 2023. Prashanth L.A., Krishna Jagannathan, and Ravi Kolla. Concentration bounds for CVaR es-timation: The cases of light-tailed and heavy-tailed distributions. In Proceedings of the 37thInternational Conference on Machine Learning (ICML), pages 55775586, 2020.",
  "Neil D Lawrence and Raquel Urtasun. Non-linear matrix factorization with Gaussian pro-cesses. In Proceedings of the 26th International Conference on Machine Learning (ICML), pages601608, 2009": "Daniel Levy, Yair Carmon, John C Duchi, and Aaron Sidford. Large-scale methods for distri-butionally robust optimization. In Proceedings of the 34th Annual Conference on Neural Infor-mation Processing Systems (NeurIPS), pages 88478860, 2020. Kevin Leyton-Brown, Mausam, Yatin Nandwani, Hedayat Zarkoob, Chris Cameron, Neil New-man, and Dinesh Raghu. Matching papers and reviewers at large conferences. Artificial Intelli-gence, 331:104119, 2024. Richard J Lipton, Evangelos Markakis, Elchanan Mossel, and Amin Saberi. On approximatelyfair allocations of indivisible goods. In Proceedings of the 5th ACM Conference on ElectronicCommerce (EC), pages 125131, 2004.",
  "Andras Prekopa. Stochastic programming, volume 324. Springer Science & Business Media,2013": "David Pujol, Ryan McKenna, Satya Kuppam, Michael Hay, Ashwin Machanavajjhala, andGerome Miklau. Fair decision making using privacy-protected data. In Proceedings of the 2020ACM Conference on Fairness, Accountability and Transparency (FAT*), pages 189199, 2020. Aida Rahmattalabi, Phebe Vayanos, Kathryn Dullerud, and Eric Rice. Learning resource allo-cation policies from observational data with an application to homeless services delivery. In Pro-ceedings of the 2022 ACM Conference on Fairness, Accountability and Transparency (FAccT),pages 12401256, 2022.",
  "Isaac Slavitt. Prioritizing municipal lead mitigation projects as a relaxed knapsack optimiza-tion: a method and case study. International Transactions in Operational Research, 30(6):37193737, 2023": "Ivan Stelmakh, Nihar B Shah, and Aarti Singh. PeerReview4All: Fair and accurate reviewerassignment in peer review. In Proceedings of the 30th International Conference on AlgorithmicLearning Theory (ALT), pages 828856, 2019. Yuanhao Wang and Jian Li.Improved algorithms for convex-concave minimax optimiza-tion. In Proceedings of the 34th Annual Conference on Neural Information Processing Systems(NeurIPS), pages 48004810, 2020. Felix Wex, Guido Schryen, and Dirk Neumann. A fuzzy decision support model for naturaldisaster response under informational uncertainty. International Journal of Information Systemsfor Crisis Response and Management (IJISCRAM), 4(3):2341, 2012. Felix Wex, Guido Schryen, Stefan Feuerriegel, and Dirk Neumann. Emergency response innatural disaster management: Allocation and scheduling of rescue units. European Journal ofOperational Research, 235(3):697708, 2014.",
  "AAdditional Related Work": "Gorissen et al. provide an excellent overview of optimization under uncertainty, including tech-niques used in this work, while Ben-Tal et al. , Bertsimas et al. offer additional background onrobust optimization. A standard approach in this regime is analyzing the dual of the uncertainty, aswe generally do in this work. Stochastic optimization has a wide literature; the books by Birge andLouveaux , Levy et al. , Prekopa , Ruszczynski and Shapiro present wide-rangingintroductions to the topic. Conditional value at risk (CVaR) can often be approximately optimizedby sampling and optimizing over an objective composing the different samples .",
  "BBroader Impacts": "We believe this work has the potential for a significant positive societal impact. Fair resource al-location algorithms are essential for various systems, including assigning reviewers in peer reviewprocesses, allocating resources to homeless and low-income populations, distributing emergencyresponse resources during natural disasters, and resettling refugees. In this work, we develop meth-ods for efficiently optimizing allocations of constrained resources under various fairness objectiveswhile addressing uncertainty in resource preferences. These methods can be directly applied to theaforementioned problems. However, we advise users to conduct extensive testing on similar datasetsbefore deploying these algorithms in real-world scenarios.",
  "CLimitations": "The CVaR approach requires solving linear programs with a large number of samples to be effective,which makes them computationally expensive. One potential solution is to leverage importancesampling methods to reduce the variance of the estimator . Future research could benefitfrom empirically and theoretically analyzing other fairness objectives like Nash welfare , Giniindex , and envy-freeness .",
  "DConstructing uncertainty sets": "In this section we demonstrate a simple and natural approach to construct an uncertainty set usinga logistic regression estimator. Logistic regression models with bounded cross-entropy loss resultin polyhedral uncertainty sets. Replacing the logistic regression model with a model with boundedsquared-error loss, or simply taking the confidence interval of a multivariate Gaussian, results intruncated ellipsoidal uncertainty sets. We construct uncertainty sets per group in all cases. Assume we have a discrete set of c values L R, with L = {1, . . . c}. For each agent i and itemtype j we denote the true distribution over values p(|(i, j)) and the distribution predicted by thelogistic regression model is p(|(i, j)). We estimate the cross-entropy loss of the model on a test set T, where |T| = t. This test set can besegmented by the group identity of the agent, such that we have TG1, TG2, . . . TGg for each of the ggroups (with sizes tG1, . . . tGg). We assume that the test set comes from the same distribution as theagent-item pairs of the assignment problem; this can be achieved either during dataset constructionor by limiting the assignments (through the C constraints) to better reflect the test distribution. Wecan also apply likelihood reweighting in our uncertainty set construction, as in , though we donot do so here.",
  "ELogistic and Gaussian Process Matrix Factorization": "Both models define probability distributions over outcomes, which we use to compute and evaluatethe CVaR of utilitarian and egalitarian welfare. For the logistic model, we build a polyhedral un-certainty set by estimating the cross-entropy loss on a held-out test set, and for the Gaussian processmodel we simply consider the confidence intervals of the resulting Gaussian distribution. For the binarized bids, we first set aside some of the observed bids as a test set. We estimate themissing bids and the bids for the held-out test pairs using logistic matrix factorization. Setting ahidden dimension size d, we construct two matrices X Rnd and Y Rmd. We set d = 20.Let V denote the true binarized bid matrix, where we observe entries for the training set pairs(a, i) T. We predict the probability of an affirmative bid as ((XY)a,i) where is the logisticsigmoid function. We select X and Y to minimize the loss function",
  "(a,i)TVa,i ln((XY)a,i) Va,i ln(XY)a,i": "For CVaR, we take samples from the distribution defined by (XY), assuming all pairs areindependently-distributed. We also construct an uncertainty set as described in Appendix D usingthe cross-entropy loss on the test pairs. Under the Gaussian process matrix factorization model , we simply predict a mean and varianceof a Gaussian distribution for each reviewer-paper pair. We can then sample values independentlyfor each pair, or give a confidence interval for the joint Gaussian with mn 1 degrees of freedom.",
  "FGrouping Papers and Reviewers": "We group papers and reviewers as follows: given the real-valued bids in the set {0.01, .5, 1} we setunknown bids to be 0. We then construct a graph with all reviewers and papers as nodes, and the bidscore between reviewers and papers is the edge weight. All inter-reviewer and inter-paper edges areset to 0 edge weight. We apply spectral embedding with 5 dimensions to transform the nodes intovectors, and cluster the resulting vectors into 4 clusters to obtain 4 groups containing both papersand reviewers. To ensure a balance of reviewers and papers across clusters, we employ Lloydsalgorithm for KMeans clustering with the modification that during each assignment step we enforcea lower bound on the number of papers and number of reviewers assigned to each cluster.",
  "GAdditional Experiments": "For the binarized AAMAS 2016 and 2021 datasets, Tables 3 and 4 show the performance of thebaseline USW and GESW maximizing allocations, the CVaR0.01 USW and GESW maximizingallocations, and the robust USW and GESW maximizing allocations at the = 0.3 level. Becauseso many of the bids in AAMAS 2021 are recorded as no, since no is the default bid, we randomlyselect 90% of the no bids to be converted to no response. Tables 5 to 7 show the same results for the Gaussian matrix factorization version of the 3 datasets,with the CVaR0.01 estimated by sampling from the estimated Gaussian distribution, and the adver-sarial welfare computed over the truncated ellipsoidal uncertainty set corresponding to the 1 confidence interval of the Gaussian.",
  "i [1, l] : (v vi)S1i (v vi) r2iQv ev 0": "Note that the above optimization problem is convex as the objective is an affine combination of v,which is convex, and the linear and quadratic constraints are also convex. Thus, from the theoryof convex optimization (.1 in Boyd et al. ), we know that maximizing the dual of aconvex optimization problem is equivalent to minimizing the primal problem. We will therefore usethe Lagrangian method for computing the dual of the above problem.",
  "i=1ir2i + e": "Note that the dual problem is concave in , , and (.1 in ). However, it is unclear ifthe dual is concave in allocation a. In order to guarantee concavity, we use the change of variables = a . From affine-composition rule in convex optimization (.2.2 in ), we knowthat if f(x) x Rn is convex, then f(Ax + b) A Rnn, b Rn is also convex in x. Thus, thevariable change = a results in a objective that is concave in , , . The allocation variable aand the dual variable only appear in a linear constraint, which is also concave. Thus, the objectiveand the constraints are concave in a, , , and , and .",
  "Alternatively, we can further simplify the problem by eliminating the allocation variable a and thedual variable and subsequently deriving them from the solution of the resultant problem": "Note that in (14), a = . Let (a, ) represent an optimal (a, ) pair for the problem in (14).Now there can be multiple pairs of (a, ) that are optimal. To eliminate and a, we need to firstensure that there exists a such that = a for at least one optimal pair (a, ). It iseasy to see that if there exists such a , then, maximizes the objective in (14). Furthermore,since is defined solely by upper-bound constraints on imposed by A, we can easily verify that itwill contain at least one instance of that satisfies = a .",
  "s.t. = a .(16)": "As a result of the change of variables, the allocation variable a and the dual variable now appearonly in a linear constraint, which is convex. Furthermore, due to the affine composition propertyof convex functions (.2.2 in Boyd et al. ), the objective remains retains its concavity.Thus, the above optimization problem is concave in a, , , and . Similar to the approach used in the proof of Proposition 3.1, we further simplify the problem byeliminating the allocation variables a and the dual variable and subsequently deriving them fromthe solution of the resultant problem. From (16), we know that that a = . Let (a, ) represent an optimal (a, ) pair for theproblem in (14). Note that there can be multiple pairs of (a, ) that are optimal. To eliminate anda, we need to find a set of feasible , which we denote by , such that there exists a suchthat = a for at least one optimal pair (a, ). It is easy to see that if there exists such a , then, maximizes the objective in (16). Furthermore, it easy to verify that satisfies thiscriteria for optimality, i.e., it contains at least one that satisfies = a , for some optimalpair (a, ).",
  "(17)": "It is important to note that the inner-most minimization is a convex optimization problem and theouter-maximization is a concave maximization problem. This is due to the fact that affine functionsare either concave or convex and minimum of concave objectives is concave. Notice that the inner-most minimization problem for each group is independent of other groups.Thus, we can simply replace each of these minimization problems with their Lagrangian dual coun-terparts. Furthermore, we note that these duals are computed following the approach outlined inthe proof of Proposition 3.1 and are exact equivalents of their respective primal problems . Theresultant optimization problem is given by",
  "G G : QGvG eG": "We can compute the Lagrangian dual of the inner-most minimization problem for each group inde-pendently by following steps outlined in the proof of Corollary 3.2. Note that since these minimiza-tion problems are simple linear programs, their corresponding duals are exact equivalents of theirprimal counterparts (.1 in Boyd et al. ). By substituting the duals in the above problem,we obtain",
  "a A": "Assumption H.1 (L.A. et al. ). The random variable x is continuous with probability densityfunction f that satisfies the following condition: There exists , > 0 such that y [v , v +] : f(y) > , where v = F 1(). Theorem H.2 (Theorem 3.1 in ). Let (xi)hi=1 be a sequence of i.i.d random variables. Supposethat xi, i = 1, . . . , n are sub-Gaussian and Assumption H.1 holds. If c and ch, represent thetrue CVaR and the empirical CVaR of random variable x estimated from h samples at confidencelevel respectively, then for any > 0, we have",
  "IGESW vs. USW under Different Scenarios": "We run an experiment to discover scenarios where the USW-optimal solution has very sub-optimalGESW. Using the AAMAS 2015 dataset, we set all of the papers to be group 1, and we create asecond, synthetic group of papers by copying and modifying a random subset of the papers. For thesepapers, we divide the copied valuations by some number, and set to zero all but the top valuationsper paper. For each setting we compute the percentage of relative loss in GESW incurred by themaximum USW solution. shows the effects of varying the percentage of papers in theminority group, the number of non-zero entries, and the divisor. We use default values of 2 for thedivisor, 5 for the number of nonzero entries, and 150 for the size of the minority group (correspondsto a ratio of roughly 20%). Taken together, the results suggest that if one group of papers hasoverall lower bids than another group, this has a very strong negative effect on the GESW of theUSW-optimal solution.",
  "JCVaR Performance under Skewed Gaussians": "We use the AAMAS 2015 dataset and sample each valuation independently from a skewed-Gaussiandistribution with varying skew parameter. We use the means and variances estimated by the Gaus-sian Process matrix factorization model described in Appendix E. The univariate skewed-Gaussiandistribution with mean , variance 2, and skew is defined by the probability density function",
  "Minority Group Ratio": "Relative Loss of Max. USW Solution (% of Optimal) 16 11 16 21 26 31 36 41 46 Number of Nonzero Affinites Relative Loss of Max. USW Solution (% of Optimal) : Relative loss (in GESW) of the maximum USW solution, compared to the optimal GESWsolution. Results are reported for a synthetic 2-group example, varying 1) the divisor applied toartificially scale the minority groups valuations, 2) the ratio of the minority group to the overallnumber of papers, and 3) the number of valuations per paper that are artifically set to 0.",
  "The answer NA means that the paper has no limitation while the answer No meansthat the paper has limitations, but those are not discussed in the paper": "The authors are encouraged to create a separate Limitations section in their paper. The paper should point out any strong assumptions and how robust the results are toviolations of these assumptions (e.g., independence assumptions, noiseless settings,model well-specification, asymptotic approximations only holding locally). The au-thors should reflect on how these assumptions might be violated in practice and whatthe implications would be. The authors should reflect on the scope of the claims made, e.g., if the approach wasonly tested on a few datasets or with a few runs. In general, empirical results oftendepend on implicit assumptions, which should be articulated. The authors should reflect on the factors that influence the performance of the ap-proach. For example, a facial recognition algorithm may perform poorly when imageresolution is low or images are taken in low lighting. Or a speech-to-text system mightnot be used reliably to provide closed captions for online lectures because it fails tohandle technical jargon.",
  "If applicable, the authors should discuss possible limitations of their approach to ad-dress problems of privacy and fairness": "While the authors might fear that complete honesty about limitations might be used byreviewers as grounds for rejection, a worse outcome might be that reviewers discoverlimitations that arent acknowledged in the paper. The authors should use their bestjudgment and recognize that individual actions in favor of transparency play an impor-tant role in developing norms that preserve the integrity of the community. Reviewerswill be specifically instructed to not penalize honesty concerning limitations.",
  "If the contribution is a dataset and/or model, the authors should describe the stepstaken to make their results reproducible or verifiable": "Depending on the contribution, reproducibility can be accomplished in various ways.For example, if the contribution is a novel architecture, describing the architecturefully might suffice, or if the contribution is a specific model and empirical evaluation,it may be necessary to either make it possible for others to replicate the model withthe same dataset, or provide access to the model. In general. releasing code and datais often one good way to accomplish this, but reproducibility can also be provided viadetailed instructions for how to replicate the results, access to a hosted model (e.g., inthe case of a large language model), releasing of a model checkpoint, or other meansthat are appropriate to the research performed. While NeurIPS does not require releasing code, the conference does require all sub-missions to provide some reasonable avenue for reproducibility, which may dependon the nature of the contribution. For example(a) If the contribution is primarily a new algorithm, the paper should make it clearhow to reproduce that algorithm.",
  "(b) If the contribution is primarily a new model architecture, the paper should describethe architecture clearly and fully": "(c) If the contribution is a new model (e.g., a large language model), then there shouldeither be a way to access this model for reproducing the results or a way to re-produce the model (e.g., with an open-source dataset or instructions for how toconstruct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case au-thors are welcome to describe the particular way they provide for reproducibility.In the case of closed-source models, it may be that access to the model is limited insome way (e.g., to registered users), but it should be possible for other researchersto have some path to reproducing or verifying the results.",
  ". Experimental Setting/Details": "Question: Does the paper specify all the training and test details (e.g., data splits, hyper-parameters, how they were chosen, type of optimizer, etc.) necessary to understand theresults?Answer: [Yes]Justification: See and appendix D, and code.Guidelines: The answer NA means that the paper does not include experiments. The experimental setting should be presented in the core of the paper to a level ofdetail that is necessary to appreciate the results and make sense of them.",
  ". Experiment Statistical Significance": "Question: Does the paper report error bars suitably and correctly defined or other appropri-ate information about the statistical significance of the experiments?Answer: [Yes] ,Justification: All results in the tables and Appendix G are averaged over 5 runsof sub-sampling 20% of each dataset.Guidelines: The answer NA means that the paper does not include experiments. The authors should answer Yes if the results are accompanied by error bars, confi-dence intervals, or statistical significance tests, at least for the experiments that supportthe main claims of the paper. The factors of variability that the error bars are capturing should be clearly stated (forexample, train/test split, initialization, random drawing of some parameter, or overallrun with given experimental conditions).",
  "Guidelines:": "The answer NA means that the paper does not use existing assets. The authors should cite the original paper that produced the code package or dataset. The authors should state which version of the asset is used and, if possible, include aURL. The name of the license (e.g., CC-BY 4.0) should be included for each asset. For scraped data from a particular source (e.g., website), the copyright and terms ofservice of that source should be provided. If assets are released, the license, copyright information, and terms of use in the pack-age should be provided. For popular datasets, paperswithcode.com/datasets has cu-rated licenses for some datasets. Their licensing guide can help determine the licenseof a dataset.",
  "The answer NA means that the paper does not involve crowdsourcing nor researchwith human subjects": "Depending on the country in which research is conducted, IRB approval (or equiva-lent) may be required for any human subjects research. If you obtained IRB approval,you should clearly state this in the paper. We recognize that the procedures for this may vary significantly between institutionsand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and theguidelines for their institution."
}