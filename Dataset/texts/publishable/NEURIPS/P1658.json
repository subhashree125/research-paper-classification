{
  "Abstract": "Decision Transformers have recently emerged as a new and compelling paradigmfor offline Reinforcement Learning (RL), completing a trajectory in an autoregres-sive way. While improvements have been made to overcome initial shortcomings,online finetuning of decision transformers has been surprisingly under-explored.The widely adopted state-of-the-art Online Decision Transformer (ODT) still strug-gles when pretrained with low-reward offline data. In this paper, we theoreticallyanalyze the online-finetuning of the decision transformer, showing that the com-monly used Return-To-Go (RTG) thats far from the expected return hampers theonline fine-tuning process. This problem, however, is well-addressed by the valuefunction and advantage of standard RL algorithms. As suggested by our analysis,in our experiments, we hence find that simply adding TD3 gradients to the fine-tuning process of ODT effectively improves the online finetuning performance ofODT, especially if ODT is pretrained with low-reward offline data. These findingsprovide new directions to further improve decision transformers.",
  "Introduction": "While Reinforcement Learning (RL) has achieved great success in recent years , it is knownto struggle with several shortcomings, including training instability when propagating a TemporalDifference (TD) error along long trajectories , low data efficiency when training from scratch ,and limited benefits from more modern neural network architectures . The latter point differssignificantly from other parts of the machine learning community such as Computer Vision andNatural Language Processing . To address these issues, Decision Transformers (DTs) have been proposed as an emergingparadigm for RL, introducing more modern transformer architectures into the literature rather than thestill widely used Multi-Layer Perceptrons (MLPs). Instead of evaluating state and state-action pairs, aDT considers the whole trajectory as a sequence to complete, and trains on offline data in a supervised,auto-regressive way. Upon inception, DTs have been improved in various ways, mostly dealing witharchitecture changes , the token to predict other than return-to-go , addressing the problemof being overly optimistic , and the inability to stitch together trajectories . Significant andencouraging improvements have been reported on those aspects. However, one fundamental issue has been largely overlooked by the community: offline-to-onlineRL using decision transformers, i.e., finetuning of decision transformers with online interactions.Offline-to-online RL is a widely studied sub-field of RL, which combines offline RL learningfrom given, fixed trajectory data and online RL data from interactions with the environment. Byfirst training on offline data and then finetuning, the agent can learn a policy with much greater dataefficiency, while calibrating the out-of-distribution error from the offline dataset. Unsurprisingly, thissub-field has become popular in recent years.",
  "arXiv:2410.24108v1 [cs.LG] 31 Oct 2024": "While there are numerous works in the offline-to-online RL sub-field , surprisinglyfew works have discussed the offline-to-online finetuning ability of decision transformers. Whilethere is work that discusses finetuning of decision transformers predicting encoded future trajectoryinformation , and work that finetunes pretrained decision transformers with PPO in multi-agentRL , the current widely adopted state-of-the-art is the Online Decision Transformer (ODT) :the decision transformer training is continued on online data following the same supervised-learningparadigm as in offline RL. However, this method struggles with low-reward data, as well as withreaching expert-level performance due to suboptimal trajectories (also see Sec. 4). To address this issue and enhance online finetuning of decision transformers, we theoretically analyzethe decision transformer based on recent results , showing that the commonly used conditioningon a high Return-To-Go (RTG) thats far from the expected return hampers results. To fix, we explorethe possibility of using tried-and-true RL gradients. Testing on multiple environments, we find thatsimply combining TD3 gradients with the original auto-regressive ODT training paradigm issurprisingly effective: it improves results of ODT, especially if ODT is pretrained with low-rewardoffline data.",
  ") We propose a simple yet effective method to boost the performance of online finetuning of decisiontransformers, especially if offline data is of medium-to-low quality;": "2) We theoretically analyze the online decision transformer, explain its policy update mechanismwhen using the commonly applied high target RTG, and point out its struggle to work well withonline finetuning; 3) We conduct experiments on multiple environments, and find that ODT aided by TD3 gradients (andsometimes even the TD3 gradient alone) are surprisingly effective for online finetuning of decisiontransformers.",
  "Preliminaries": "Markov Decision Process. A Markov Decision Process (MDP) is the basic framework of sequentialdecision-making. An MDP is characterized by five components: the state space S, the action space A,the transition function p, the reward r, and either the discount factor or horizon H. MDPs involvean agent making decisions in discrete steps t {0, 1, 2, . . . }. On step t, the agent receives the currentstate st S, and samples an action at A according to its stochastic policy (at|st) (A),where (A) is the probability simplex over A, or its deterministic policy (st) A. Executingthe action yields a reward r(st, at) R, and leads to the evolution of the MDP to a new state st+1,governed by the MDPs transition function p(st+1|st, at). The goal of the agent is to maximizethe total reward t tr(st, at), discounted by the discount factor for infinite steps, orHt=1 r(st, at) for finite steps. When the agent ends a complete run, it finishes an episode, and thestate(-action) data collected during the run is referred to as a trajectory . Offline and Online RL. Based on the source of learning data, RL can be roughly categorizedinto offline and online RL. The former learns from a given finite dataset of state-action-rewardtrajectories, while the latter learns from trajectories collected online from the environment. The effortof combining the two is called offline-to-online RL, which first pre-trains a policy using offline data,and then continues to finetune the policy using online data with higher efficiency. Our work falls intothe category of offline-to-online RL. We focus on improving the decision transformers, instead ofQ-learning-based methods which are commonly used in offline-to-online RL. Decision Transformer (DT). The decision transformer represents a new paradigm of offline RL,going beyond a TD-error framework. It views a trajectory as a sequence to be auto-regressivelycompleted. The sequence interleaves three types of tokens: returns-to-go (RTG, the target totalreturn), states, and actions. At step t, the past sequence of context length K is given as the input, i.e.,the input is (RTGtK, stk, atk, . . . , RTGt, st), and an action is predicted by the auto-regressivemodel, which is usually implemented with a GPT-like architecture . The model is trained viasupervised learning, considering the past K steps of the trajectory along with the current state andthe current return-to-go as the feature, and the sequence of all actions a in a segment as the labels.At evaluation time, a desired return RTGeval is specified, since the ground truth future returnRTGreal isnt known in advance. Online Decision Transformer (ODT). ODT has two stages: offline pre-training which is identical toclassic DT training, and online finetuning where trajectories are iteratively collected and the policy isupdated via supervised learning. Specifically, the action at at step t during rollouts is computed bythe deterministic policy DT(stT :t, atT :t1, RTGtT :t, T = Teval, RTG = RTGeval),1 or sampledfrom the stochastic policy DT(at|stT :t, atT :t1, RTGtT :t, T = Teval, RTG = RTGeval). Here, Tis the context length (which is Teval in evaluation), and RTGeval R is the target return-to-go. Thedata buffer, initialized with offline data, is gradually replaced by online data during finetuning.",
  "DT (s0:t, a0:t1, RTG0:t, RTG = RTGreal, T = t) at22 .(1)": "Note, Ttrain is the training context length and RTGreal is the real return-to-go. For better readability,we denote {sx+1, sx+2, . . . , sy}, x, y N as sx:y (i.e., left exclusive and right inclusive), andsimilarly {ax+1, ax+2, . . . , ay} as ax:y and {RTGx+1, . . . , RTGy} as RTGx:y. Specially, indexx = y represents an empty sequence. For example, when t = 1, a0:0 is an empty action sequence asthe decision transformer is not conditioned on any past action. One important observation: the decision transformer is inherently off-policy (the exact policy dis-tribution varies with the sampled starting point, context length and return-to-go), which effectivelyguides our choice of RL gradients to off-policy algorithms (see Appendix C for more details). TD3. Twin Delayed Deep Deterministic Policy Gradient (TD3) is a state-of-the-art onlineoff-policy RL algorithm that learns a deterministic policy a = RL(s). It is an improved version ofan actor-critic (DDPG ) with three adjustments to improve its stability: 1) Clipped double Q-learning, which maintains two critics (estimators for expected return) Q1, Q2 : |S| |A| R anduses the smaller of the two values (i.e., min (Q1, Q2)) to form the target for TD-error minimization.Such design prevents overestimation of the Q-value; 2) Policy smoothing, which adds noise whencalculating the Q-value for the next action to effectively prevent overfitting; and 3) Delayed update,which updates RL less frequently than Q1, Q2 to benefit from a better Q-value landscape whenupdating the actor. TD3 also maintains a set of target networks storing old parameters of the actorand critics that are soft-updated with slow exponential moving average updates from the current,active network. In this paper, we adapt this algorithm to fit the decision transformer architecture sothat it can be used as an auxiliary objective in an online finetuning process.",
  "Method": "This section is organized as follows: we will first provide intuition why RL gradients aid onlinefinetuning of decision transformers (Sec. 3.1), and present our method of adding TD3 gradients(Sec. 3.2). To further justify our intuition, we provide a theoretical analysis on how ODT fails toimprove during online finetuning when pre-trained with low-reward data (Sec. 3.3).",
  "Why RL Gradients?": "In order to understand why RL gradients aid online finetuning of decision transformers, let us consideran MDP which only has a single state s0, one step, a one dimensional action a (i.e., abandit with continuous action space) and a simple reward function r(a) = (a + 1)2 if a 0 andr(a) = 1 2a otherwise, as illustrated in . In this case, a trajectory can be representedeffectively by a scalar, which is the action. If the offline dataset for pretraining is of low quality, i.e.,all actions in the dataset are either close to 1 or 1, then the decision transformer will obviously notgenerate trajectories with high RTG after offline training. As a consequence, during online finetuning,the new rollout trajectory is very likely to be uninformative about how to reach RTGeval, since it istoo far from RTGeval. Worse still, it cannot improve locally either, which requires RTG",
  "RTG, butlocal policy improvement requires the opposite, i.e., RTG": "a . Therefore, the agent cannot recover if thecurrent policy conditioning on high target RTG does not actually lead to high real RTG, which is verylikely when the target RTG is too far from the pretrained policy and out-of-distribution. By adding asmall coefficient for RL gradients, the agents can improve locally, which leads to better performance.",
  "(d) DT Policy": ": An illustration of a simple MDP, showing how RL can infer the direction for improvement,while online DT fails. Panels (a) and (b) show, DDPG and ODT+DDPG manage to maximizereward and find the correct optimal action quickly, while ODT fails to do so. Panel (c) shows howa DDPG/ODT+DDPG critic (from light blue/orange to dark blue/red) manages to fit ground truthreward (green curve). Panel (d) shows that the ODT policy (changing from light gray to dark) fails todiscover the hidden reward peak near 0 between two low-reward areas (near 1 and 1 respectively)contained in the offline data. Meanwhile, ODT+DDPG succeeds in finding the reward peak. easily estimate the former from the latter. Thus, the hope for policy improvement relies heavily onthe generalization of RTG, i.e., policy yielded by high RTGeval indeed leads to better policy withoutany data as evidence, which is not the case with our constructed MDP and dataset.",
  "In contrast, applying traditional RL for continuous action spaces to this setting, we either learn avalue function Q(s0, a) : R R, which effectively gives us a direction of action improvementQ(s0,a)": "a(e.g., SAC , DDPG , TD3 ), or an advantage A(s0, a) that highlights whetherfocusing on action a improves or worsens the policy (e.g., AWR , AWAC , IQL ). Eitherway provides a direction which suggests how to change the action locally in order to improve the(estimated) return. In our experiment illustrated in (see Appendix F for details), we found thatRL algorithms like DDPG can easily solve the aforementioned MDP while ODT fails. Thus, adding RL gradients aids the decision transformer to improve from given low RTG trajectories.While one may argue that the self-supervised training paradigm of ODT can do the same byprompting the decision transformer to generate a high RTG trajectory, such paradigm is still unableto effectively improve the policy of the decision transformer pretrained on data with low RTGs. Weprovide a theoretical analysis for this in Sec. 3.3. In addition, we also explore the possibility offixing this problem using other existing algorithms, such as JSRL and slowly growing RTG (i.e.,curriculum learning). However, we found that those algorithms cannot address this problem well.See Appendix G.8 for ablations.",
  "Adding TD3 Gradients to ODT": "In this work, we mainly consider TD3 as the RL gradient for online finetuning. There aretwo reasons for selecting TD3. First, TD3 is a more robust off-policy RL algorithm compared toother off-policy RL algorithms . Second, the success of TD3+BC indicates that TD3 is agood candidate when combined with supervised learning. A more detailed discussion and empiricalcomparison to other RL algorithms can be found in Appendix C. Generally, we simply add a weighted standard TD3 actor loss to the decision transformerobjective. To do this, we follow classic TD3 and additionally train two critic networks Q1, Q2 :S A R parameterized by 1, 2 respectively. In the offline pretraining stage, we use thefollowing objective for the actor:",
  "Qmin,t = rt + (1 dt) mini{1,2} Qi,tarst, clipRLtar (zt) + clip(, c, c), alow, ahigh,(3)": "where = {s0:Ttrain+1, a0:Ttrain, RTG0:Ttrain+1, d0:Ttrain, r0:Ttrain, RTG = RTGreal} is the trajectory seg-ment sampled from buffer D that stores the offline dataset. Further, dt indicates whether the trajectoryends on the t-th step (true is 1, false is 0), Qmin is the target to fit, Qi,tar is produced by the target net-work (stored old parameter), zt is the context for next state at step t. RLtar is the target network for theactor (i.e., decision transformer). For an n-dimensional action, clip(a, x, y), a Rn, y Rn, z Rnmeans clip ai to [yi, zi] for i {1, 2, . . . , n}. alow Rn and ahigh Rn are the lower and upperbound for every dimension respectively. To demonstrate the impact on aiding the exploration of a decision transformer, in this work we choosethe simplest form of a critic, which is reflective, i.e., only depends on the current state. This essentiallymakes the Q-value an average of different context lengths sampled from a near-uniform distribution(see Appendix D for the detailed reason and distribution for this). The choice is based on the factthat training a transformer-based value function estimator is quite hard due to increased inputcomplexity (i.e., noise from the environment) which leads to reduced stability and slower convergence.In fact, to avoid this difficulty, many recent works on Large Language Models (LLMs) andvision models which finetune with RL adopt a policy-based algorithm instead of an actor-critic,despite a generally lower variance of the latter. In our experiments, we also found such a critic to bemuch more stable than a recurrent critic network (see Appendix G for ablations).",
  "During online finetuning, we again use Eq. (2) and Eq. (3), but always use = 0.1 for Eq. (2)": "While the training paradigm resembles that of TD3+BC, our proposed method improves uponTD3+BC in the following two ways: 1) Architecture. While TD3+BC uses MLP networks forsingle steps, we leverage a decision transformer, which is more expressive and can take more contextinto account when making decisions. 2) Selected instead of indiscriminated behavior cloning.Behavior cloning mimics all data collected without regard to their reward, while the supervisedlearning process of a decision transformer prioritizes trajectories with higher return by conditioningaction generation on higher RTG. See Appendix G.9 for an ablation.",
  "in Sec. 3.1, in this section, we will analyze more formally why such a paradigm is unable to improvethe policy given offline data filled with low-RTG trajectories": "Our analysis is based on the performance bound proved by Brandfonbrener et al. . Given a datasetdrawn from an underlying policy and given its RTG distribution P (either continuous or discrete),under assumptions (see Appendix E), we have the following tight performance bound for a decisiontransformer with policy DT(a|s, RTGeval) conditioned on RTGeval:",
  "f+ 2H2.(4)": "Here, f = infs1 P(RTGreal = RTGeval|s1) for every initial state s1, > 0 is a constant, H isthe horizon of the MDP.2 Based on this tight performance bound, we will show that with highprobability,1f grows superlinearly with respect to RTGeval. If true, then the RTGreal term (i.e.,the actual return from online rollouts) must decrease to fit into the tight bound, as RTGeval grows. To show this, we take a two-step approach: First, we prove that the probability mass of the RTGdistribution is concentrated around low RTGs, i.e., event probability Pr (RTG E(RTG|s) c|s)for c > 0 decreases superlinearly with respect to c. For this, we apply the Chebyshev inequality,which yields a bound of O 1",
  "c2. However, without knowledge on P(RTG|s), the variance can bemade arbitrarily large by high RTG outliers, hence making the bound meaningless": "Fortunately, we have knowledge about the RTG distribution P(RTG|s) from the collected data. If werefer to the maximum RTG in the dataset via RTGmax and if we assume all rewards are non-negative,then all trajectory samples have an RTG in [0, RTGmax]. Thus, with adequate prior distribution, wecan state that with high probability 1 , the probability mass is concentrated in the low RTG area.Based on this, we can prove the following lemma:Lemma 1. (Informal) Assume rewards r(s, a) are bounded in [0, Rmax],3 and RTGeval RTGmax.Then with probability at least 1 , we have the probability of event Pr bounded as follows:",
  "c2,(5)": "where depends on the number of trajectories in the dataset and prior distribution (see Appendix Efor a concrete example and a more accurate bound). V (s) is the value function of the underlyingpolicy (a|s) that generates the dataset, for which we have V (s) = E(RTG|s). The second step uses the bound of probability mass Pr(RTG c|s) to derive the bound for f. Forthe discrete case where the possibly obtained RTGs are finite or countably infinite (note, state andaction space can still be continuous), this is simple, as we have",
  "PRTG = V (s) + c|s= PrRTG = V (s) + c|s PrRTG V (s) + c|s.(6)": "Thus f = infs1 P(RTG|s1) can be conveniently bounded by Lemma 1. For the continuous case,the proof is more involved as probability density P(RTG|s) can be very high on an extremelyshort interval of RTG, making the total probability mass arbitrarily small. However, assuming thatP(RTG|s) is Lipschitz when RTG RTGmax (i.e., RTG area not covered by dataset), combinedwith the discrete distribution case, we can still get the following (see Appendix E for proof):Corollary 1. (Informal) If the RTG distribution is discrete (i.e., number of possible different RTGs areat most countably infinite), then with probability at least- 1 ,1f grows on the order of (RTG2eval)with respect to RTGeval. For continuous RTG distributions satisfying a Lipschitz continuous RTGdensity p,1f grows on the order of (RTG1.5eval).",
  "Eq. (4) is very informal. See Appendix E for a more rigorous description.3Note we use max instead of max as this is a property of the environment and not the dataset": "what to predict, while still using supervised learning, the correct way to improve the finetuning abilityof decision transformers? c) Does the transformer architecture, combined with RL gradients, workbetter than TD3+BC? d) Is it better to combine the use of RL and supervised learning, or better tosimply abandon the supervised loss in online finetuning? e) How does online decision transformerwith TD3 gradient perform compared to other offline RL algorithms? f) How much does TD3 improveover DDPG which was used in ? Baselines. In this section, we mainly compare to six baselines: the widely recognized state-of-the-artDT for online finetuning, Online Decision Transformer (ODT) ; PDT, a baseline improving overODT by predicting future trajectory information instead of return-to-go; TD3+BC , a MLP offlineRL baseline; TD3, an ablated version of our proposed solution where we use TD3 gradients only fordecision transformer finetuning (but only use supervised learning of the actor for offline pretraining);IQL , one of the most popular offline RL algorithms that can be used for online finetuning;DDPG +ODT, which is the same as our approach but with DDPG instead of TD3 gradients (forablations using SAC , IQL , PPO , AWAC and AWR , see Appendix C). Eachof the baselines corresponds to one of the questions a), b), c), d), e) and f) above. Metrics. We use the normalized average reward (same as D4RLs standard ) as the metric, wherehigher reward indicates better performance. If the final performance is similar, the algorithm withfewer online examples collected to reach that level of performance is better. We report the rewardcurve, which shows the change of the normalized rewards mean and standard deviation with 5different seeds, with respect to the number of online examples collected. The maximum numberof steps collected is capped at 500K (for mujoco) or 1M (for other environments). We also reportevaluation results using the rliable library in of Appendix B. Experimental Setup. We use the same architecture and hyperparameters such as learning rate(see Appendix F.2 for details) as ODT . The architecture is a transformer with 4 layers and4 heads in each layer. This translates to around 13M parameters in total. For the critic, we useMulti-Layer Perceptrons (MLPs) with width 256 and two hidden layers and ReLU activationfunction. Specially, for the random dataset, we collect trajectories until the total number of stepsexceeds 1000 in every epoch, which differs from ODT, where only 1 trajectory per epoch is collected.This is because many random environments, such as hopper, have very short episodes when the agentdoes not perform well, which could lead to overfitting if only a single trajectory is collected perepoch. For fairness, we use this modified rollout for ODT in our experiments as well. Not doingso does not affect ODT results since it does generally not work well on random datasets, but willsignificantly increase the time to reach a certain number of online transitions. After rollout, we trainthe actor for 300 gradient steps and the critic for 600 steps following TD3s delayed update trick.",
  "Adroit Environments": "Environment and Dataset Setup. We test on four difficult robotic manipulation tasks , whichare the Pen, Hammer, Door and Relocate environment. For each environment, we test three differentdatasets: expert, cloned and human, which are generated by a finetuned RL policy, an imitationlearning policy and human demonstration respectively. See Appendix F.1 for details. Results. shows the performance of each method on Adroit before and after online finetuning.TD3+BC fails on almost all tasks and often diverges with extremely large Q-value during onlinefinetuning. ODT and PDT perform better but still fall short of the proposed method, TD3+ODT. Note,IQL, TD3 and TD3+ODT all perform decently well (with similar average reward as shown in Tab. 2in Appendix B). However, we found that TD3 often fails during online finetuning, probably becausethe environments are complicated and TD3 struggles to recover from a poor policy generated duringonline exploration (i.e., it has a catastrophic forgetting issue). To see whether there is a simple fix,in Appendix G.7, we ablate whether an action regularizer pushing towards a pretrain policy similarto TD3+BC helps, but find it to hinder performance increase in other environments. IQL is overallmuch more stable than TD3, but improves much less during online finetuning than TD3+ODT. ODTcan achieve good performance when pretrained on expert data, but struggles with datasets of lowerquality, which validates our motivation. DDPG+ODT starts out well in the online finetuning stagebut fails quickly, probably because DDPG is less stable compared to TD3. Pen-expert-v1 Hammer-expert-v1 Relocate-expert-v1 125Door-expert-v1 Pen-cloned-v1 Hammer-cloned-v1 0.5 0.0 0.5 1.0 1.5 2.0 Relocate-cloned-v1 Door-cloned-v1 0.00.20.40.60.81.01e6 Pen-human-v1 0.00.20.40.60.81.01e6 Hammer-human-v1 0.00.20.40.60.81.01e6 Relocate-human-v1 0.00.20.40.60.81.0 Online Transitions1e6",
  "Normalized Reward": "Door-cloned-v1 ODTTD3+BCTD3+RVSTD3+BC (our arch)TD3+ODT (ours) : The result of ODT and TD3+BC ablations (TD3+RVS, DDPG+ODT, TD3+BC withour architecture and curriculum RTG for ODT) on Adroit environments. The result shows that onlyTD3+BC with our architecture works. However, it remains worse than our method. 0.00.20.40.60.81.01e6 Pen-expert-v1 0.00.20.40.60.81.01e6 Hammer-expert-v1 0.00.20.40.60.81.01e6 Relocate-expert-v1 0.00.20.40.60.81.0 Online Transitions1e6",
  "Antmaze Environments": "Environment and Dataset Setup. We further test on a harder version of the Maze2D environment inD4RL where the pointmass is substituted by a robotic ant. We study six different variants, whichare umaze, umaze-diverse, medium-play, medium-diverse, large-play and large-diverse. Results. lists the results of each method on umaze and medium maze before and after onlinefinetuning (see Appendix C for reward curves and Appendix B for results summary on large antmaze).TD3+ODT works the best on umaze and medium maze, and significantly outperforms TD3. Thisshows that RL gradients alone are not enough for offline-to-online RL of the decision transformer.Though TD3+ODT does not work on large maze, we found that IQL+ODT works decently well.However, we choose TD3+ODT in this work because IQL+ODT does not work well on the randomdatasets. This is probably because IQL aims to address the Out-Of-Distribution (OOD) estimationproblem , which makes it better at utilizing offline data but worse at online exploration. SeeAppendix C for a detailed discussion and results. DDPG+ODT works worse than TD3+ODT butmuch better than baselines except IQL.",
  "MuJoCo Environments": "Environment and Dataset Setup. We further test on four widely recognized standard environ-ments , which are the Hopper, Halfcheetah, Walker2d and Ant environment. For each environ-ment, we study three different datasets: medium, medium-replay, and random. The first and secondone contain trajectories of decent quality, while the last one is generated with a random agent.",
  "Average68.52(+14.6)55.9(+7.8)55.14(+18.58)41.97(+40.44)87.64(+44.95)22.87(-19.22)88.38(+46.59)": ": Average reward for each method in MuJoCo environments before and after online finetuning.The best performance for each environment is highlighted in bold font, and any result > 90% ofthe best performance is underlined. To save space, the name of the environments and datasets areabbreviated as follows: for the environments Ho=Hopper, Ha=HalfCheetah, Wa=Walker2d, An=Ant;for the datasets M=Medium, MR=Medium-Replay, R=Random. The format is final(+increase afterfinetuning). The proposed solution performs well. Results. shows the results of each method on MuJoCo before and after online finetuning.We observe that autoregressive-based algorithms, such as ODT and PDT, fail to improve the policyon MuJoCo environments, especially from low-reward pretraining with random datasets. With RLgradients, TD3+BC and IQL can improve the policy during online finetuning, but less than a decisiontransformer (TD3 and TD3+ODT). In particular, we found IQL to struggle on most random datasets,which are well-solved by decision transformers with TD3 gradients. TD3+ODT still outperformsTD3 with an average final reward of 88.51 vs. 84.23. See in Appendix B for reward curves. Ablations on . (a) shows the result of using different (i.e., RL coefficients) on differentenvironments. We observe an increase of to improve the online finetuning process. However, if is too large, the algorithm may get unstable. Ablations on evaluation context length Teval. (b) shows the result of using different Tevalon halfcheetah-medium-replay-v2 and hammer-cloned-v1. The result shows that Teval needs to bebalanced between more information for decision-making and potential training instability due to alonger context length. As shown in the halfcheetah-medium-replay-v2 result, Teval too long or tooshort can both lead to performance drops. More ablations are available in Appendix G. 0123451e5 Halfcheetah-medium-replay-v2 0.00.20.40.60.81.0 Online Transitions1e6 2.10 2.15 2.20",
  "Related Work": "Online Finetuning of Decision Transformers. While there are many works on generalizing decisiontransformers (e.g., predicting waypoints , goal, or encoded future information instead of return-to-go ), improving the architecture or addressing the overly-optimistic or trajectory stitching issue ), there is surprisingly little work beyond online decision transformersthat deals with online finetuning of decision transformers. There is some loosely related literature:MADT proposes to finetune pretrained decision transformers with PPO. PDT also studiesonline finetuning with the same training paradigm as ODT . QDT uses an offline RL algorithm to re-label returns-to-go for offline datasets. AFDT and STG use decisiontransformers offline to generate an auxiliary reward and aid the training of online RL algorithms. Afew works study in-context learning and meta-learning of decision transformers,where improvements with evaluations on new tasks are made possible. However, none of the papersabove focuses on addressing the general online finetuning issue of the decision transformer. Transformers as Backbone for RL. Having witnessed the impressive success of transformers inComputer Vision (CV) and Natural Language Processing (NLP) , numerous works alsostudied the impact of transformers in RL either as a model for the agent or as a worldmodel . However, a large portion of state-of-the-art work in RL is still based on simple Multi-Layer Perceptrons (MLPs) . This is largely because transformers are significantly harder totrain and require extra effort , making their ability to better memorize long trajectories harderto realize compared to MLPs. Further, there are works on using transformers as feature extractors fora trajectory and works that leverage the common sense of transformer-based Large LanguageModels for RL priors . In contrast, our work focuses on improving the new RL viaSupervised learning (RvS) paradigm, aiming to merge this paradigm with the benefits ofclassic RL training. Offline-to-Online RL. Offline-to-online RL bridges the gap between offline RL, which heavily de-pends on the quality of existing data while struggling with out-of-distribution policies, and online RL,which requires many interactions and is of low data efficiency. Mainstream offline-to-online RL meth-ods include teacher-student and out-of-distribution handling (regularization ,avoidance , ensembles ). There are also works on pessimistic Q-value initializa-tion , confidence bounds , and a mixture of offline and online training . However, allthe aforementioned works are based on Q-learning and dont consider decision transformers.",
  "Conclusion": "In this paper, we point out an under-explored problem in the Decision Transformer (DT) community,i.e., online finetuning. To address online finetuning with a decision transformer, we examinethe current state-of-the-art, online decision transformer, and point out an issue with low-reward,sub-optimal pretraining. To address the issue, we propose to mix TD3 gradients with decisiontransformer training. This combination permits to achieve better results in multiple testbeds. Ourwork is a complement to the current DT literature, and calls out a new aspect of improving decisiontransformers. Limitations and Future Works. While our work theoretically analyzes an ODT issue, the conclusionrelies on several assumptions which we expect to remove in future work. Empirically, in this work wepropose a simple solution orthogonal to existing efforts like architecture improvements and predictingfuture information rather than return-to-go. To explore other ideas that could further improve onlinefinetuning of decision transformers, next steps include the study of other environments and otherways to incorporate RL gradients into decision transformers. Other possible avenues for futureresearch include testing our solution on image-based environments, and decreasing the additionalcomputational cost compared to ODT (an analysis for the current time cost is provided in Appendix H). This work was supported in part by NSF under Grants 2008387, 2045586, 2106825, MRI 1725729,NIFA Award 2020-67021-32799, the IBM-Illinois Discovery Accelerator Institute, the Toyota Re-search Institute, and the Jump ARCHES endowment through the Health Care Engineering SystemsCenter at Illinois and the OSF Foundation.",
  "Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., and Zaremba,W. Openai gym. arXiv preprint arXiv:1606.01540, 2016": "Brohan, A., Brown, N., Carbajal, J., Chebotar, Y., Chen, X., Choromanski, K., Ding, T., Driess,D., Dubey, A., Finn, C., et al. Rt-2: Vision-language-action models transfer web knowledge torobotic control. arXiv preprint arXiv:2307.15818, 2023. Brohan, A., Chebotar, Y., Finn, C., Hausman, K., Herzog, A., Ho, D., Ibarz, J., Irpan, A., Jang,E., Julian, R., et al. Do as i can, not as i say: Grounding language in robotic affordances. InCoRL, 2023.",
  "Oh, J., Guo, Y., Singh, S., and Lee, H. Self-imitation learning. In ICML, 2018": "Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal,S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A.,Welinder, P., Christiano, P. F., Leike, J., and Lowe, R. Training language models to followinstructions with human feedback. In NeurIPS, 2022. Parisotto, E., Song, F., Rae, J., Pascanu, R., Gulcehre, C., Jayakumar, S., Jaderberg, M.,Kaufman, R. L., Clark, A., Noury, S., et al. Stabilizing transformers for reinforcement learning.In ICML, 2020.",
  "Uchendu, I., Xiao, T., Lu, Y., Zhu, B., Yan, M., Simon, J., Bennice, M., Fu, C., Ma, C., Jiao, J.,et al. Jump-start reinforcement learning. In ICML, 2023": "Wang, Z., Wang, H., and Qi, Y. J. T3gdt: Three-tier tokens to guide decision transformer foroffline meta reinforcement learning. In Robot Learning Workshop: Pretraining, Fine-Tuning,and Generalization with Large Scale Models in NeurIPS, 2023. Woczyk, M., Cupia, B., Ostaszewski, M., Bortkiewicz, M., Zaj ac, M., Pascanu, R., Kucinski,., and Mios, P. Fine-tuning reinforcement learning models is secretly a forgetting mitigationproblem. In ICML, 2024.",
  "Reinforcement Learning Gradients as Vitamin for OnlineFinetuning Decision Transformers": "The Appendix is organized as follows. In Sec. A, we discuss the potential positive and negative socialimpact of the paper. Then, we summarize the performance shown in the main paper in Sec. B. Afterthis, we will explain our choice of RL gradients in the paper in Sec. C, and why our critic serves asan average of policies generated by different context lengths in Sec. D. We then provide rigorousstatements for the theroetical analysis appearing in the paper in Sec. E, and list the environmentdetails and hyperparameters in Sec. F. We then present more experiment and ablation results inSec. G. Finally, we list our computational resource usage and licenses of related assets in Sec. H andSec. I respectively.",
  "ABroader Societal Impacts": "Our work generally helps automation of decision-making by improving the use of online interactiondata of a pretrained decision transformer agent. While this effort improves the efficiency of decision-makers and has the potential to boost a variety of real-life applications such as robotics and resourceallocation, it may also cause several negative social impacts, such as potential job losses, humande-skilling (making humans less capable of making decisions without AI), and misuse of technology(e.g., military).",
  "BPerformance Summary": "In this section, we summarize the average reward achieve by each method on different environmentsand datasets, where the result for Adroit is shown in Tab. 2, and the result for Antmaze is shownin Tab. 3. As the summary table for MuJoCo is already presented in Sec. 4, we show the rewardcurves in . For a more rigorous evaluation, we also report other metrics including the median,InterQuartile Mean (IQM) and optimality gap using the rliable library. See for details.Breakdown analysis for each environment can be downloaded by browsing to",
  "Average6.6(-7.66)61.14(+10.49)38.73(+3.75)25.25(+24.87)31.85(-29.63)3.51(-52.96)87.84(+33.09)": ": Average reward for each method in Adroit Environments before and after online finetuning.The best result for each setting is marked in bold font and all results > 90% of the best performanceare underlined. To save space, the name of the environments and datasets are abbreviated as follows:P=Pen, H=Hammer, D=Door, R=Relocate for environment, and E=Expert, C=cloned, H=Human forthe dataset. It is apparent that while both IQL, TD3 and TD3+ODT perform decently well beforeonline finetuning, our proposed solution significantly outperforms all baselines on the adroit testbed.DDPG+ODT starts out well in the online stage, but fails probably due to DDPGs training instabilitycompared to TD3.",
  "Average0.13(+0.07)73.83(+12.61)2.65(-15.01)4.99(+4.99)19.39(+19.39)28.97(-0.36)62.11(+34.1)Avg. (U+M)0.19(+0.08)80.26(+8.36)3.98(-22.52)7.49(+7.49)29.08(+29.08)43.46(-0.54)93.17(+51.17)": ": Average reward for each method in Antmaze Environments before and after online finetuning.The best result is marked in bold font and all results > 90% fo the best performance are underlined.To save space, the name of the environments and datasets are abbreviated as follows: U=Umaze,UD=Umaze-Diverse, MP=Medium-Play, MD=Medium-Diverse, LP=Large-Play and LD=Large-Diverse. U+M=Umaze and Medium maze. Our method performs the best on umaze and mediummaze, while IQL performs the best on large maze. Both methods are much better than the rest onaverage. TD3+BC diverges on antmaze in our experiments. Hopper-medium-v2 Ant-medium-v2 Walker2d-medium-v2 Halfcheetah-medium-v2 Hopper-medium-replay-v2 Ant-medium-replay-v2 Walker2d-medium-replay-v2 Halfcheetah-medium-replay-v2 0100000 200000 300000 400000 500000 Hopper-random-v2 0100000 200000 300000 400000 500000 100Ant-random-v2 0100000 200000 300000 400000 500000 Walker2d-random-v2 0100000 200000 300000 400000 500000",
  "old(a|s), 1 , 1 + Aold (s, a).(7)": "Here, old is the policy at the beginning of the training for the current epoch. Normally, thedenominator of the red part, old(a|s), would be reasonably large, as the data is sampled from thatdistribution. However, because of the offline nature caused by different RTGs and context lengths atrollout and training time, the denominator for the red part in Eq. (7) could be very small in training,which will lead to a very small loss if Aold(s, a) > 0. This introduces significant instability duringthe training process. illustrates the instability and degrading performance of a PPO-finetuneddecision transformer. TD3+BC IQL ODT PDT TD3 DDPG+ODT TD3+ODT (ours)",
  "In contrast, RLHF, does not exhibit such a problem: it does not use different return-to-go and contextlength in evaluation and training. Thus RLHF does not encounter the problem described above": "Besides the RL gradients mentioned above, as IQL works well on large Antmazes, we also explorethe possibility of using IQL as the RL gradient for decision transformer instead of TD3. We foundthat IQL gradients, when applied to the decision transformer, indeed lead to much better results onantmaze-large. However, IQL fails to improve the policy when the offline dataset subsumes very lowreward trajectories, which does not conform with our motivation. This is probably because IQL, asan offline RL algorithm, aims to address out-of-distribution evaluation issue, which is a much moreimportant source of improvement in exploration in the online case. Thus, we choose TD3 as the RLgradient applied to decision transformer finetuning in this work. shows the result of addingTD3 gradient vs. adding IQL gradient on Antmaze-large-play-v2 and hopper-random-v2.",
  "at = DT(stT :t, atT :t1, RTGtT :t, T U (1, Ttrain), RTG = RTGreal),(9)": "T is actually sampled from a distribution U (1, Ttrain) over integers between 1 and Ttrain inclusive;this distribution U is introduced by the randomized starting step of the sampled trajectory segments,and is almost a uniform distribution on integers, except that a small asymmetry is created because thecontext length will be capped at the beginning of each trajectory. See for an illustration. Therefore, online decision transformers (and plain decision transformers) are actually trained topredict with every context length between 1 and Ttrain. During the training process, the context lengthis randomly sampled according to U , and a critic is trained to predict an average value for thepolicy generated with context length sampled from U .",
  "EMathematical Proofs": "In this section, we will state the theoretical analysis summarized in Sec. 3.3 more rigorously. Wewill first provide an explanation on how the decision transformer improves its policy during onlinefinetuning, linking it to an existing RL method in Sec. E.1 and Sec. E.2. We will then bound itsperformance in Sec. E.3.",
  "E.1Preliminaries": "Advantage-Weighted Actor Critic (AWAC) is an offline-to-online RL algorithm, where thereplay buffer is filled with offline data during offline pretraining and then supplemented with onlineexperience during online finetuning. AWAC uses standard Q-learning to train the critic Q : |S| |A| R, and update the actor using weighted behavior cloning, where the weight is exponentiated",
  "E.2Connection between Decision Transformer and AWAC": "We denote as the underlying policy of the dataset, and P as the distribution over states, actionsor returns induced by . Note such P can be either discrete or continuous. By prior work , fordecision transformer policy DT, we have the following formula holds for any return-to-go RTG Rof the future trajectory:",
  "(b) Distribution U of T2": ": a) illustrates the context length T2 during training; Teval is the context length of a2 uponsampling and evaluation. It is easy to see that T2 is randomized during training due to the left endpointof the sampled trajectory segment. b) shows the distribution U of T2; while T 2 for step j Ttrain isuniformly sampled between 1 and Ttrain because the start of the segment is uniformly sampled, T2 forstep i < Ttrain will be capped at the start of the trajectory. Thus U is not exactly uniform.",
  "enough (more rigorously, RTG max{Q(s, a), V (s)}), (a|s, RTG) is updated in the same wayas AWAC.4": "Proof. If return-to-go are Laplace distributions, then by the symmetric property of such distributions,the mean of RTG given s or (s, a) would be the expected future return, which by definition arevalue functions for , i.e., V (s) for P(RTG|s) and Q(s, a) for P(RTG|s, a). See as anillustration. As RTGeval max{Q(s, a), V (s)}, we have",
  "but with quadratic terms of Q and V": "Different from AWAC, the policy improvement of online decision transformers heavilyrelies on the global property of the return-to-go as RTGeval moves further away from Qand V . If the return-to-go is far away from the support of the data, we will have almost nodata to evaluate P, and its estimation can be very uncertain (let alone ratios). In this case,it is very unlikely for the decision transformer to collect rollouts with high RTGtrue and getfurther improvement. This is partly supported by the corollary of Brandfonbrener et al. ,where the optimal conditioning return they found on evaluation satisfies RTGeval = V (s1)at the initial state s1. This is also supported by our single-state MDP experiment discussedin Sec. 3.1 and illustrated in . Those important insights lead to the intuition that decision transformers, when finetuned online, lackthe ability to improve locally from low RTGtrue data, and encourages to study the scenario whereP(RTG|s) and P(RTG|s, a) are small.",
  "E.3Failure of ODT Policy Update with Low Quality Data": "As mentioned in Sec. E.2, we study the performance of decision transformers in online finetuningwhen P(RTG|s) and P(RTG|s, a) is small. Specially, in this section, r(s, a) is not a rewardfunction, but a reward distribution conditioned on (s, a); ri r(si, ai) is the reward obtainedon i-th step. Such notation takes the noise of reward into consideration and forms a more generalframework. Also, as the discrete or continuous property of is important in this section, we will usePr to represent probability mass (for discrete distribution or cumulative distribution for continuousdistribution) and p to represent probability density (for probability density function for continuousdistribution).",
  "Definition E.2. f(s1) = RTGeval for all initial states s1, f(si+1) = f(si) ri for the (i + 1)-th stepfollowing i-th step (i {1, 2, . . . , T 1})": "Further, we enforce the third assumption in Thm. E.1 by including the cumulative reward so far in thestate space (as described in the paper of Brandforbrener et al. ). Under such definition, we have atight bound on the regret between our target RTGeval and the true return-to-go RTGtrue = Hi=1 ri byour learned policy at optimal, based on current replay buffer in online finetuning.",
  "Thus, we immediately get the following corollary:": "Corollary 2. Assume the reward is bounded in [0, Rmax] for any state and action, and the number ofpossible different return-to-go one can get is finite or countably infinite. Then for the f condition func-tion defined in Def. E.2, with probability of at least 1, we have f (1)RTGmax+R2maxT 2[V (s)]2",
  "i.e.,1f grows in the order of (RTG2eval)": "Remark E.5. While the limitation on return-to-go seems strong theoretically, it is very easy to satisfysuch assumption in practice because it has no requirement on the discreteness of state and actionspace. Such corollary can be applied on reward discretization with arbitrary precision (includingimplicit ones by float precision). For continuous distribution, to bound p(RTG = c0) with Pr(RTG c0) on RTG, we would needto assume that peak does not exist (see for illustration), i.e. there does not exist cases wherep is large but Pr is small. Thus, we made the following assumption:Assumption E.6. (Lipschitzness on uncovered RTG distribution) p(RTG|s) is KV -Lipschitzwhere RTG is larger than RTGmax.Remark E.7. The assumption is reasonable because when RTG is larger than any of the RTGtrue inthe dataset, we have no data coverage for the performance of the underlying policy under suchRTG, and thus we can choose any inductive bias for .Remark E.8. Note the Lipschitzness of p does not rely on the Lipschitzness of the reward function.For example, consider a single-state, single-step MDP where we have a uniformly random policya U(0, 1) with reward r(a) = 2 1",
  "F.1.1Single-State MDP": "The single-state MDP studied in Sec. 3.1 motivates why RL gradients are useful for online finetuning.It has a single state, a single action a , and a reward function r(a) = (a + 1)2 if a 0 andr(a) = 1 2a otherwise. Datasets. The dataset has a size of 128, with 100 actions uniformly sampled in (1, 0.95), and theremaining 28 actions uniformly sampled in (0.5, 1). The dataset is designed to conceal the rewardpeak in the middle. DDPG and ODT+DDPG successfully recognized the reward peak but ODTfailed.",
  ": Illustration of Adroit environments used in Sec. 4 based on OpenAI Gym andD4RL": "1. Pen. Pen is a locomotion environment where the agent needs to control a robotic handto manipulate a pen, such that its orientation matches the target. It has a 24-dimensionalaction space, each of which controls a joint on the wrist or fingers. The state space is45-dimensional, which contains the pose of the palm, the angular position of the joints, andthe pose of the target and current pen. 2. Hammer. Hammer is an environment where the agent needs to control a robotic hand topick up a hammer and use it to drive a nail into a board. The action space is 26-dimensional,each of which corresponds to a joint on the hand. The state space is 46-dimensional, whichdescribes the angular position of the fingers, the pose of the palm, and the status of hammerand nail. 3. Door. In the door environment, the agent needs to use a robotic hand to open a door byundoing the latch and swinging it. The environment has a 28-dimensional action space,which are the absolute angular positions of the hand joints. It also has 39-dimensionalobservation space which describes each joint, the pose of the palm, and the door with itslatch. 4. Relocate. In the relocate environment, the agent needs to control a robotic hand to move aball from its initial location towards a goal, both of which are randomized in the environment.The environment has a 30-dimensional action space which describes the angular position ofthe joints on the hand, and a 39-dimensional space which describes the hand as well as theball and target.",
  ": The size and the average and standard deviation of the normalized reward of the Adroitdatasets from D4RL used in our experiments": "Datasets. For each of the four environments, we test our method across three different qualities ofdatasets: expert, cloned and human, all of which provided by the DAPG repository. The expertdataset is generated by a fine-tuned RL policy; the cloned dataset is collected from an imitation policyon the demonstrations from the other two datasets; and the human dataset is collected from humandemonstrations. Tab. 4 shows the size and average reward of each dataset.",
  "F.1.3Antmaze Environments": "Environments. Antmaze is a more difficult version of Maze2D, where the agent controls a roboticant instead of a point mass through the maze. It has a 27 dimensional-state space and a 8-dimensionalaction space. We test our method on six variants of antmaze: Umaze, Umaze-Diverse, Medium-Play,Medium-Diverse, Large-Play and Large-Diverse, where Umaze, Medium and Large describesthe size of the maze (see for an illustration), and the Diverse and Play describes the typeof the dataset. More specifically, Diverse means that in the offline dataset, the starting point andthe goal of the agent are randomly generated, while Play means that the goal is generated by ahandcraft design. Umaze without suffix is the simplest environment where both the starting pointand the goal are fixed.",
  ": Illustration of mazes in antmaze and maze2D environment, where the red point is the goaland the green point is the current location of the agent": "Datasets. Similar to Adroit and MuJoCo, we test our method on datasets provided by D4RL. Tab. 5shows the size and normalized reward of each dataset. Note, following IQL and CQL , weconduct reward shaping: subtracting from all rewards in the dataset and environment the value 1during training of both our method and baselines to provide denser reward signal for all antmazeenvironments. However, we still count original sparse reward when comparing the performance.",
  ": Illustration of MuJoCo environments used in Sec. 4 based on OpenAI Gym andD4RL": "1. Hopper. Hopper is a locomotion task on a 2D vertical plane, where the agent manipulates asingle-legged robot to hop forward. Its state is 11-dimensional, which describes the angleand velocity for the robots joints. Its action is 3-dimensional, which corresponds to thetorques applied on the three joints for the current time step respectively. 2. Halfcheetah. Halfcheetah is also a 2D environment which requires the agent to control acheetah-like robot to run forward. The states are 17-dimensional, containing the coordinateand velocity of the joints The actions are 6-dimensional, which control the torques on thejoints of the robot. 3. Ant. In Ant, the agent controls a four-legged 8-DoF robotic ant to walk in a 3D environmentand tries to move forward. It has a 111-dimensional state space describing the coordinatesand velocities of the joints.",
  ". Walker2d. Walker2d is a 2D environment in which the agent needs to manipulate a 8-DoFtwo-legged robot to walk forward under the agents control. Its state space is 27-dimensional": "Datasets. We test our method across three different qualities of datasets: medium, medium-replayand random. The medium dataset contains trajectories collected by an agent trained with RL,but early-stopped at medium-level performance. The medium-replay dataset is the collection oftrajectories sampled in the training process of the agent mentioned above. The random datasetcontains trajectories collected by an agent with random policy. Tab. 6 shows the size and normalizedreward of each dataset.",
  "F.1.5Maze2D Environments": "Environments. Maze2D is another set of D4RL environment, where the agent needs to control apoint mass to navigate through a 2D maze and arrive at a fixed goal. It has a 4-dimensional statespace describing its coordinate and velocity, and a 2-dimensional action describing its acceleration.The reward is determined by its current distance to the goal. We test our method on four variantsof maze: Open, Umaze, Medium and Large with increasing difficulty. The map of each maze isillustrated in . Maze2D environment is tested in Sec. G.2.",
  "F.2.1Single-State MDP": "For all networks, we use a simple MDP with two hidden layers of width 128, and ReLU asactivation function. We add a Tanh activation function to limit the output for ODT and the actor ofDDPG to . For both methods, we use Adam as the optimizer, and the learning rate is setto 103. We pretrain 5 epochs on offline data (20 gradient steps) and 16 epochs for online finetuning,with a batch size of 32 for gradient update and collect 64 new rollout states for each epoch (thuswe train 2n + 4 steps for the n-th online finetuning epoch). RTGeval is set at 1, which serves as the(constant) input for ODT rollout and DDPG actor. Both DDPG and ODT uses deterministic actorwith an exploration noise uniform in [0.01, 0.01] during online rollouts.",
  "F.2.2Other Experiments": "Tab. 8 summarizes hyperparameters that are common across all environments, and Tab. 9 summarizeshyperparameters that are different across environments. For environments that exist in ODT , wefollow the hyperparameters from ODT medium environments. We did not use positional embeddingas suggested by ODT . Specially, for antmaze, we remove most (all but 10) 1-step trajectories,because the size of the replay buffer for decision transformers is controlled by the number oftrajectories, and antmaze dataset contains a large number of 1-step trajectories due to its datageneration mechanism (immediately terminate an episode when the agent is close to the goal, but donot reset the agent location). Also, we add Layernorm after each hidden layer of the critic forAdroit, Maze and Antmaze environments, according to Yue et al. s advice. We found that suchpractice stabilizes the training process (see Sec. G.5 for ablation). For ODT and TD3 baseline, we use the same code as our TD3+ODT, while setting coefficients for RLand supervised gradients accordingly. For PDT baseline, we use the default hyperparameter in PDTpaper, and pretrain PDT for 40K steps for all experiments. For TD3+BC and IQL, we use the defaulthyperparameter in their codebase, and pretrain them for 1M steps for all experiments (remaining thesame as that in the codebase).",
  "Antmaze-Umaze51-100-1000.10.10.998 0.0002 10410440K2K-Medium11-200-2000.10.10.998 0.0002 104104200K2K-Large51-500-5000.10.10.998 0.0002 104104200K2K": ": Environment-specific hyperparameters, where Ttrain and Teval stands for training and eval-uation context length, RTGeval and RTGonline represents RTG during evaluation and online rolloutrespectively, is the coefficient for RL gradient, is the discount factor, lrc is the critic learning rate,and lra is the actor learning rate. Buffer size is counted in the number of trajectories. Note RTGs ofantmaze have been modified according to our reward shaping.",
  "G.1Delayed Reward": "Though we have tested MuJoCo environments in Sec. 4, it is worth noting that many offline RLalgorithms have addressed the MuJoCo benchmark quite well . Thus, we also tested settingswhere RL struggles to obtain good performance to further analyze the performance of using RLgradients for decision transformers. Environment and Experimental Setup. In this experiment, we use the same experiment and datasetas in Sec. 4 except for one major difference: the rewards are not given immediately after each step.Instead, the cumulative reward during a short period of M steps is given only at the end of the period,while the rewards observed by the agents within a period are all zero. We adopt such a setting fromprior influential work , which creates a sparse-reward setting where RL algorithms struggle. Wetest M = 5 in this experiment.",
  "Average64.28(+12.17)53.07(+3.31)53.4(+15.22)41.92(+39.32)78.68(+35.15)76.31(+33.51)": ": Average reward for each method in MuJoCo Environments before and after online finetuningwith delayed rewards. To save space, the name of the environments and datasets are compressed,where Ho=Hopper, Ha=HalfCheetah, Wa=Walker2d, An=Ant for the environment, and M=Medium,MR=Medium-Replay, R=Random for the dataset. The format is \"final(+increase)\". The best result foreach setting is highlighted in bold font, and any result > 90% of the best performance is underlined. Hopper-medium-v2 Ant-medium-v2 Walker2d-medium-v2 Halfcheetah-medium-v2 Hopper-medium-replay-v2 Ant-medium-replay-v2 Walker2d-medium-replay-v2 Halfcheetah-medium-replay-v2 0100000 200000 300000 400000 500000 Hopper-random-v2 0100000 200000 300000 400000 500000 Ant-random-v2 0100000 200000 300000 400000 500000 Walker2d-random-v2 0100000 200000 300000 400000 500000",
  "G.2Maze2D Environment": "We test on navigation tasks in D4RL where the agents need to control a pointmass throughfour different mazes: Open, Umaze, Medium and Large with different dataset respectively. lists the performance of each method on Maze2D before and after online finetuning, and Tab. 11summarizes the performance before and after online finetuning. The result shows that our methodagain significantly outperforms autoregressive-based algorithms such as ODT and PDT, whichvalidates our motivation in Sec. 3.1. DDPG+ODT works similarly well as TD3+ODT in thisenvironment with simple state and action space.",
  "G.3Ablations on training context length Ttrain": "shows the result of using different context lengths on hammer-cloned-v1 environment (in thisexperiment, we use Teval = 1 to demonstrate the effect of more different Ttrain). It is shown fromthe experiment that the selection of Ttrain needs to be balanced between more information taken intoaccount and training stability; while longer Ttrain brings faster convergence when growing from 1 to5, the reward curves with Ttrain {10, 20} oscillates more than that with Ttrain = 5.",
  "G.4Longer Training Process": "In some environments, such as hopper-random-v2, walker2d-random-v2 and ant-random-v2, ourproposed method still seems to be improving after 500K online samples. In , we show thefinetuning result of our proposed method with more online transitions, which effectively shows thatour method has greater potential in online finetuning when finetuned for more gradient steps.",
  "G.5The Effect of Layernorm": "As we have mentioned in Sec. F.2, as suggested by Yue et al. , we apply Layernorm to criticnetworks for environment other than MuJoCo for better stability in training. In our experiment, wefound that it greatly stabilizes the critic on complicated environments such as Adroit, but makes onlinepolicy improvement less efficient on easier MuJoCo environments. shows the performanceand critic MSE loss comparison on some environments with and without layernorm; it is clearlyshown that layernorm helps stabilizes online finetuning in some cases such as pen-cloned-v1, buthinders performance increase on other environments such as halfcheetah-medium-v2.",
  "G.6Recurrent Critic": "As mentioned in Sec. 3.2, we use reflexive critics (i.e., critics that only take the current state nandaction) to add RL gradients to decision transformer, and this creates an average effect among policiesgenerated by different context lengths (see Sec. D in the appendix for detail). In this section, weexplore recurrent critic by substituting the MLP critic using a LSTM, such that for a trajectory segment,the evaluation for the i-th action ai is based on all state-action pairs (s1, a1), . . . , (si1, ai1) andcurrent state si. As shown in , we found that recurrent critics are much less stable than 0.00.20.40.60.81.01.21.4 1e6 Hopper-random-v2 0.00.20.40.60.81.01.21.4 1e6 Walker2d-random-v2 0.00.20.40.60.81.01.21.4",
  "G.7Regularizer for Pure TD3 Gradients": "In the Adroit environment results discussed in Sec. 4, we found that the baseline of ODT finetunedusing pure TD3 gradients struggles due to catastrophic forgetting. Inspired by Woczyk et al. ,we test whether adding a KL regularizer can fix the forgetting problem. Though our policy isdeterministic, we can approximately interpret the policy as Gaussian with a very small variance.Thus, a KL regularizer can be simply added using c0 a aold2, where a is the current action andaold is the action predicted by the pretrained policy. We set c0 = 0.05 and test this method on theAdroit cloned and expert dataset. We illustrate the result in . We find that the KL regularizereffectively addresses the issue on expert environments for both TD3 and TD3+ODT. But it cansometimes hinder the policy improvement of TD3+ODT with low return during online finetuning.",
  "G.8Other possible exploration improvement techniques": "In Sec. 3.1, we state that ODT cannot explore the high-RTG region when pretrained with low-qualityoffline data, and we ran a simple experiment to verify this (). In this section, we test twopotential alternatives for addressing the exploration problem: JSRL and curriculum learning. For JSRL, an expert policy is used for the first n steps in an episode, before ODT takes over. Weset n = 100 (100% max episode length for adroit pen, 50% max episode length for other adroitenvironments) initially, and apply an exponential decay rate of 0.99 for every episode. We test twosettings of JSRL: the expert policy being the offline pretrained policy, and the expert policy beingoracle, i.e., an IQL policy trained on the Adroit expert dataset. For curriculum learning, we use ODT with a gradually increasing target RTG with the current RTGfor rollouts being RTGeval 0.99N(RTGeval RTGdata). Here, N is the number of episodes sampledin online stage, and RTGdata is the average RTG of the offline dataset. Results are summarized in . We found that curriculum RTG does not work, probably becausethe task is too hard and cannot be improved by random exploration without gradient guidance. Further,even with oracle exploration, ODT is not guaranteed to succeed: it fails on the hammer environmentwhere TD3+ODT succeeds, probably because of insufficient expert-level data and an inability toimprove with random exploration.",
  "G.9Ablations on the Architecture": "In this section, we further examine the source of the performance gain of our method compared toTD3+BC. There are two key differences as stated in Sec. 3.2: The architecture and RL via Supervised(RvS) learning . We can hence assess two more baselines: TD3+BC with our transformerarchitecture and TD3+RvS using TD3+BCs architecture. We present the ablation result on the Adroitcloned environment in . The result shows that only TD3+BC with our architecture works(albeit still worse than our method). We hypothesize that this is because a simple MLP is hard tomodel the complicated policy which takes both RTG and state into account. To further assess if simply adding more layers to the MLP works, we conduct an ablation on thenumber of layers for TD3+RvS. The result is illustrated in . It shows that simply adding afew layers to the MLP does not aid performance. We speculate that it is probably the transformerarchitecture that helps modeling the state-and-RTG-conditioned policy. 0.00.20.40.60.81.01e6 Pen-cloned-v1 0.00.20.40.60.81.01e6 140Hammer-cloned-v1 0.00.20.40.60.81.01e6 0.0 0.5 1.0 1.5 2.0 Relocate-cloned-v1 0.00.20.40.60.81.0 Online Transitions1e6",
  "HComputational Resources": "We conduct all experiments with a single NVIDIA RTX 2080Ti GPU on an Ubuntu 18.04 serverequipped with 72 Intel Xeon Gold 6254 CPUs @ 3.10GHz. Mujoco experiments takes about 6 8hours, and the bottleneck is the gradient update; about 50% time is spent on backpropagation andupdate of parameters. Our critic appended to ODT only takes up about 20% time to train, in which90% of the critic training time is spent on decision transformer inference to get action for next state.For the actor, the training overhead of our method is negligible since it only contains an MLP criticinference to get the Q-value. Therefore, overall our method only uses 20% extra time compared toODT for training, but attains much better results."
}