{
  "Abstract": "Fine-tuned vision-language models (VLMs) often capture spurious correlationsbetween image features and textual attributes, resulting in degraded zero-shotperformance at test time. Existing approaches for addressing spurious correlations(i) primarily operate at the global image-level rather than intervening directlyon fine-grained image features and (ii) are predominantly designed for unimodalsettings. In this work, we present RAVL, which takes a fine-grained perspective onVLM robustness by discovering and mitigating spurious correlations using localimage features rather than operating at the global image level. Given a fine-tunedVLM, RAVL first discovers spurious correlations by leveraging a region-levelclustering approach to identify precise image features contributing to zero-shotclassification errors. Then, RAVL mitigates the identified spurious correlationwith a novel region-aware loss function that enables the VLM to focus on relevantregions and ignore spurious relationships during fine-tuning. We evaluate RAVLon 654 VLMs with various model architectures, data domains, and learned spuriouscorrelations. Our results show that RAVL accurately discovers (191% improvementover the closest baseline) and mitigates (8.2% improvement on worst-group imageclassification accuracy) spurious correlations. Qualitative evaluations on general-domain and medical-domain VLMs confirm our findings.1",
  "Introduction": "Contrastive vision-language models (VLMs) (e.g., CLIP and ALIGN ) are a powerful classof models that jointly learn relationships between images and text. VLMs are generally pretrainedon web-scale datasets with millions of image-text pairs and have been shown to exhibit impressivecapabilities on a wide range of downstream tasks. In particular, VLMs have the ability to performtasks in a zero-shot manner without utilizing explicit task-specific training data; this is accomplishedby modeling downstream tasks (e.g., image classification, text-to-image retrieval) as image-textmatching tasks . However, pretrained VLMs can exhibit poor zero-shot performance when compared to state-of-the-arttask-specific models, particularly on challenging or out-of-domain downstream tasks .As a result, pretrained VLMs are often fine-tuned on domain-specific vision-language datasets in order",
  ": Region-aware Vision-Language learning (RAVL). RAVL takes a fine-grained perspectiveon VLM robustness by discovering and mitigating spurious correlations using local image features": "to improve zero-shot performance on tasks of interest. For instance, recent works have fine-tunedthe CLIP VLM on vision-language datasets consisting of (i) chest X-rays and paired physicianreports , (ii) pathology data and paired text , and (iii) product images and paired captionsfrom online fashion retailers . Domain-specific vision-language datasets used to fine-tune VLMs may be small in size, preventingVLMs from gaining the robustness benefits that come with training on diverse, web-scale data .As a result, fine-tuned VLMs may capture spurious correlations between image features and textualattributes . For instance, consider a VLM fine-tuned on an animal image-text dataset where thepresence of butterflies is closely correlated with the presence of flowers (). Consequently,the VLM may learn to incorrectly associate the image features corresponding to flower with thetextual attribute butterfly. At test time, the VLM is likely to exhibit degraded zero-shot classificationperformance on (i) images of butterflies without flowers and (ii) images of other animals with flowers. Improving robustness of fine-tuned VLMs to spurious correlations is challenging for the following tworeasons. First, existing automated approaches primarily discover and mitigate spurious correlationsat the global image level rather than intervening directly on fine-grained image features. Suchapproaches discover spurious correlations by identifying coherent groups of misclassified imagesin an automated fashion ; then, the identified spurious correlation can be mitigatedduring training using data augmentation or robust optimization . However, recentworks have suggested that such global image-level strategies (i) discover spurious correlations thatalign poorly with human-interpretable attributes and (ii) may not effectively enable models toignore spurious correlations during training . Second, existing approaches for discoveringand mitigating spurious correlations are predominantly designed to improve robustness of unimodalimage classification models or pretrained VLMs . These settings differ substantiallyfrom the fine-tuned VLM setting, which presents several unique challenges such as the absence ofclass and subgroup labels in the training set and the inclusion of free-form text. In this work, we address these challenges by introducing Region-aware Vision-Language learning(RAVL), an approach for improving the robustness of fine-tuned VLMs to spurious correlations.RAVL takes a fine-grained perspective on VLM robustness by discovering and mitigating spuri-ous correlations using local image features, rather than operating at the global image level. Ourcontributions are: First, given a fine-tuned VLM, RAVL discovers learned spurious correlations between imagefeatures and textual attributes. Using a labeled classification dataset, we decompose images intocandidate regions, utilize the VLM embedding space to group visually-similar regions into featureclusters, and quantitatively evaluate the effects of each feature on zero-shot classification errors. Second, given a ranked list of image features that the VLM has learned to spuriously correlatewith one or more textual attributes, RAVL mitigates the identified spurious correlations. Ourkey insight is that region-level information can be leveraged during VLM fine-tuning in orderto improve model robustness. To this end, we introduce a novel region-aware loss function",
  "that encourages the VLM to focus on relevant regions and ignore spurious relationships duringfine-tuning": "In order to evaluate RAVL, we introduce a large-scale evaluation framework for controlled, fine-grained evaluations of VLM robustness on synthetic and real-world data. Our framework consists of654 fine-tuned VLMs paired with annotations for the ground-truth spurious correlations learned byeach VLM. Across these evaluation settings, (i) RAVL accurately discovers spurious correlations,achieving a 191% improvement over the closest baseline, and (ii) RAVL effectively mitigates spuriouscorrelations, achieving up to an 8.2% improvement on worst-group image classification accuracy.Qualitative evaluations on general-domain and medical-domain VLMs confirm the utility of RAVL. This paper is organized as follows. In , we introduce our problem setting. Then, in , we present Stage 1 of RAVL, including our proposed methodology for discovering spuriouscorrelations, our large-scale evaluation framework, and experimental results. In , weintroduce Stage 2 of RAVL, including our proposed methodology for mitigating spurious correlationsas well as experimental results. Finally, we conclude in .",
  "Preliminaries": "In this section, we formally describe our problem setting. Datasets used for fine-tuning VLMs canbe expressed as DF = {(Ii, Ti)}mi=1, where Ii represents image inputs and Ti represents pairedfree-form text. We do not assume access to any class or subgroup labels. The performance of fine-tuned VLMs can be characterized with zero-shot classification tasks. In linewith prior work , we assume that the zero-shot classification dataset includes a validationsplit DV = {(Ii, yi)}ni=1 with images Ii and known ground-truth class labels yi Y, where Ydenotes the set of all possible class labels. At evaluation time, classification performance is computedby encoding class labels in Y as text and matching images to the closest class label using embeddingsimilarity. We do not assume access to any subgroup labels. Fine-tuned VLMs may learn spurious correlations between image features and textual attributes.Let ea represent the image features corresponding to a visual concept a (e.g., flowers in )and y Y represent a class label (e.g., butterfly\" in ) such that ea and y share nocausal relationship. Then, a fine-tuned VLM that has learned a spurious correlation will be unable todisentangle ea and y at evaluation time. This will manifest in low zero-shot classification performanceon the following two subgroups of data: (i) images from class label y without the feature ea and (ii)images from other class labels Y \\ {y} with the feature ea. However, since neither the fine-tuning dataset DF nor the evaluation dataset DV include subgrouplabels corresponding to visual concepts a, discovering and mitigating such spurious correlationsposes a challenge. For instance, in , there are no annotations for flowers in datasets DF andDV, making it challenging to identify and address the learned spurious correlation between imagefeatures corresponding to flowers and the textual attribute corresponding to butterfly\". In the following sections, we will discuss our automated approach RAVL, which aims to address thischallenge by employing fine-grained region-level information to discover () and mitigate() spurious correlations in fine-tuned vision-language models.",
  "Discovering Spurious Correlations in Fine-Tuned Vision-Language Models": "In this section, we present the first stage of RAVL, which aims to discover learned spurious correla-tions in VLMs. In .1, we discuss our region-aware approach for discovering fine-grainedspurious correlations. Then, in order to quantitatively evaluate the efficacy of spurious feature dis-covery methods, we introduce a large-scale evaluation framework in .2. Finally, in .3, we use our evaluation framework to demonstrate that RAVL outperforms prior approaches indiscovering fine-grained spurious correlations between image features and textual attributes.",
  "Our Approach: Discovering Spurious Correlations": "The first stage of RAVL aims to identify spurious correlations between image features and textualattributes learned by a fine-tuned VLM M. In contrast to prior works that have incorporated humansin the loop in order to identify spurious correlations , RAVL is a fully automated approach.Additionally, whereas previous automated methods for discovering spurious correlations focuspredominantly on identifying groups of images with high error rates , our approach identifiesspecific image features that model M has learned to spuriously correlate with a textual attribute. Ourgoal is to discover precise spurious correlations that can be easily interpreted by humans. As discussed in , a model M that has learned a spurious correlation between an imagefeature ea and a textual attribute y will demonstrate low zero-shot performance on (i) images inDV with label y without the feature ea and (ii) images in DV with other labels Y \\ {y} with thefeature ea. The key challenge lies in identifying such relationships when no annotations are providedfor visual concepts a. RAVL addresses this challenge by (1) obtaining candidate image features inDV , (2) identifying the candidate image features that, when present in an image, directly contributeto classification errors, and (3) ranking the identified image features by degree of learned spuriouscorrelations. Obtaining candidate image features. RAVL first utilizes the zero-shot classification dataset DVto identify candidate image features. To this end, we use the fine-tuned VLM M to extract animage embedding for each image Ii in DV and a text embedding for each class y Y. Zero-shotclassification is performed using the computed embeddings; this results in a softmax-normalizedimage score distribution vector sIi R|Y|, where |Y| represents the number of classes. Then,we decompose each image Ii in DV into a set of candidate regions Ri. There are a variety ofways in which an image can be decomposed into regions, such as dividing images into equal-sizedsegments (e.g., quadrants) or using region proposal networks (RPNs) . Ideally, regions shouldcapture key features in the image; however, we emphasize that RAVL does not require ground-truth region-level annotations. We then apply RoIAlign to the image encoder of M toextract embeddings for each region. Zero-shot classification is performed using the computed regionembeddings, resulting in a softmax-normalized region score distribution matrix SRi R|Ri||Y|. Given region-level embeddings for all candidate regions in DV , we next aim to identify coherentgroups of image features that occur consistently throughout the dataset (e.g., features correspondingto flower\" or butterfly\" in ). To this end, we cluster the computed region-level embeddingsusing the K-Medoids algorithm with cosine distance. The optimal number of clusters is selected inan automated fashion using Silhouette distance. The resulting clusters (denoted as C) capture keyimage features in DV . For feature cluster c C, let ec denotes the set of features in cluster c.",
  "Identifying candidate image features that directly contribute to classification errors. We nowseek to identify features that, when present in an image, are directly responsible for prediction errors": "Let Rc represent the set of regions assigned to cluster c and let Ic represent the set of imagesassociated with the regions in cluster c. We identify labels for images in Ic; we designate thislabel set as Yc. For each class label y Yc, we identify all images in Ic with label y, and wedesignate zero-shot classification accuracy on this subset of nyin images as pyin. Then, we identify allimages in Dv with label y that do not have a region included in cluster c, and we designate zero-shotclassification accuracy on this subset of nyout images as pyout. We now introduce the cluster influence score, which evaluates the extent to which features eccontribute to mispredicted image classification labels. We restrict our evaluation to only includemispredicted images in Ic with ground-truth labels y such that pyin < pyout; we will refer to this subsetas Ierrc Ic. For each image Ii Ierrc, we extract (i) the image score distribution vector sIi and (ii)the region score distribution matrix SRi. We use sIi to identify the predicted image class y, and wethen identify the region rmaxiin Ri with the highest score for class y. Definition 1 (Cluster Influence Score). For cluster c and label y, the cluster influence score is theproportion of images Ii Ierrcwith label y where the identified highest-scoring region rmaxiis partof cluster c (i.e., rmaxi Rc):",
  "IiIerrc;yi=y1[rmaxi Rc](1)": "The final cluster influence score for cluster c is computed as the maximum over all labels y asHc = maxyYcHyc . High values of Hc show that features ec are similar to the incorrect label in thevision-language embedding space; this suggests that for a given image with an incorrect prediction,feature ec is more likely to contribute to the misprediction than other features in the image. On theother hand, low values of Hc are likely to indicate that feature ec represents a core feature associatedwith the class label or a neutral feature that does not affect predictions.",
  "Given Hc for each feature cluster, we prune all clusters with influence scores below a threshold of l,which we set to 0.25 in all experiments": "Ranking image features by degree of learned spurious correlation. For each remaining featurecluster, we next aim to determine the extent to which the presence or absence of features ec affectsclassification performance; we introduce the cluster performance gap metric to this end. Definition 2 (Cluster Performance Gap). For cluster c and label y, the cluster performance gap is theweighted difference between zero-shot classification accuracy on images with features ec and imageswithout features ec:Gyc = wy (pyin pyout),(2)",
  "Experimental Setup: Designing a Large-Scale Evaluation Framework": "We now discuss our approach for evaluating RAVL. Evaluating the accuracy of predicted spuriouscorrelations is challenging because the ground-truth spurious correlations learned by a model M aretypically unknown. Previous works on VLM robustness evaluate discovered spurious correlationswith qualitative experiments, human-in-the-loop evaluations, or small-scale datasets . Our aimin this section is to introduce a large-scale experimental setup where the ground-truth spuriouscorrelations learned by VLMs are known and annotated in advance; this can then enable us todetermine whether the features discovered by RAVL in .1 accurately align with the ground-truth. Our evaluation framework is motivated by prior work ; however, in contrast to existingapproaches, we introduce evaluation settings that are designed (i) for evaluating robustness approachesat the fine-grained region level rather than the global image-level, and (ii) for evaluating VLMs ratherthan unimodal models. Designing Controlled Evaluations: Our evaluation framework artificially induces spurious corre-lations in the VLM fine-tuning data; then, given the known pre-defined spurious correlation and aVLM that learned the desired spurious correlation, we can quantitatively evaluate the extent to whichRaVL discovers the correlation. We create a set of evaluation settings using data from two domains: (1) synthetic data (MNIST and FashionMNIST ) and (2) real-world data (COCO ). Each evaluation setting consists ofthe following components: 1. Predefined spurious correlation: We define a spurious image feature and textual attribute pair(eeval, aeval). For MNIST and FashionMNIST, eeval represents a red rectangle; aeval is generatedfrom the set of class labels {zero, one, two, three, four five, six, seven, eight, nine} for MNIST and{t-shirt, trouser, pullover, dress, coat, sandal, shirt, sneaker, bag, ankle boot} for FashionMNIST.For COCO, we sample eeval and aeval from the list of annotated attributes. 2. Fine-tuning dataset: We construct a vision-language fine-tuning dataset DevalF= {(Ii, Ti)}mi=1with images Ii and text Ti. Dataset DevalFis sampled from the training sets of MNIST, Fashion-MNIST, or COCO such that the presence of image feature eeval is closely correlated with thepresence of text attribute aeval as measured by Cramers V .",
  ". Fine-tuned VLM: A VLM M is fine-tuned on DevalF": "4. Evaluation dataset: Model M is evaluated using a zero-shot classification dataset DevalV={(Ii, yi, Ri, Li)}ni=1 with images Ii, class labels yi, region bounding boxes Ri, and region-levellabels Li. In particular, aeval must be included in the class label set, and eeval must be annotatedin the region-level label set. Since DevalVis designed to reflect a real-world setting, we assumethat a correlation between aeval and eeval does not exist. Dataset DevalVis constructed from thetest sets of MNIST, FashionMNIST, or COCO. Given the four components listed above, we classify an evaluation setting as valid if model M learnedthe intended spurious correlation. In order to measure this, we first identify images with label aevalin DevalVand compute the performance difference between images with feature eeval and imageswithout feature eeval; we designate this value as 1. Then, for labels y = aeval, we compute themaximum performance difference between images without feature eeval and images with featureeeval; we designate this value as 2. Large values of 1 and 2 suggest that model M has learned thedesired spurious correlation between image feature eeval and textual attribute aeval, as defined in. We remove settings where 1 or 2 are below some predefined performance threshold eval.The performance threshold eval serves as a quantitative indicator of learned correlation strength. Implementation Details: In total, we generate 620 fine-tuning datasets DevalF(100 synthetic; 520real-world). We then fine-tune model M on each dataset with three random seeds, resulting in 1860candidate evaluation settings. Finally, we filter out settings where model M does not consistentlylearn the spurious correlation; to this end, we only retain settings where both 1 and 2 exceedeval = 10 across all three random seeds. We repeat this procedure across various pretrained VLMsM, resulting in 654 valid experimental settings. Additional implementation details are provided inAppendix B.",
  "Results: RaVL Effectively Discovers Spurious Correlations": "Comparisons to Prior Approaches: Given an evaluation setting with a predefined spurious correla-tion (eeval, aeval), a fine-tuned VLM M, and an evaluation dataset DevalV, our goal is to determinethe extent to which RAVL can discover the correlation between eeval and aeval. To this end, we use the labeled zero-shot classification dataset DevalV, which includes ground-truthregion bounding boxes and associated region labels. We provide the ground-truth bounding boxesas input to RAVL, which returns a single top-ranked cluster of regions likely to include spuriousfeatures. We rank regions within the cluster based on similarity to the cluster medoid, and we utilizethe provided region-level labels in DevalVto evaluate the proportion of top-K regions that contain thedesired spurious feature eeval. In line with prior work , we report performance with Precision@Kmetrics. We note that given an identified spurious feature eeval, the correlated textual attribute aevalcan be detected by identifying the class label in DevalVwhere the absence of feature eeval leads todegraded performance. There are few existing approaches for performing automated detection of fine-grained spuriousfeatures learned by VLMs. Here, we compare RAVL with four previously-developed methods:Distilling Failures , George , Domino , and Spurious-Aware Detection . DistillingFailures, George, and Domino are state-of-the-art approaches for automatic identification of modelfailures resulting from spurious correlations; although these methods operate at the global image leveland are designed for unimodal settings, we adapt these approaches for our setting by utilizing regionsand zero-shot classification scores as input. Spurious-Aware Detection operates at the fine-grainedregion level by computing class-based performance gaps resulting from the presence or absence ofparticular features. To enable a fair comparison with RAVL, we provide the same set of regions andassociated embeddings as input to all baselines. We also compare RAVL with a random baseline,where the ranked list of regions is shuffled randomly. summarizes mean Precision@10 metrics across all 654 evaluation settings. Results demon-strate that RAVL consistently outperforms prior approaches in discovering spurious correlationsbetween image features and textual attributes, contributing to a 191% improvement over the closestbaseline. In , we evaluate the effects of learned spurious correlation strength by varying the er-ror threshold eval from 10 to 40 and reporting performance for the subset of valid evaluation settings.Results show that RAVL is particularly effective when VLM M learns a strong spurious correlation;as learned correlation strength increases, performance of RAVL increases by 47% whereas mostbaselines degrade in performance. We also observe that Domino, George, and Distilling Failuresoften achieve performance near or below the random baseline across our evaluation settings; thissuggests that methods designed for detecting errors resulting from spurious correlations at the globalimage-level cannot be easily adapted for fine-grained region-level discovery. demonstratesthat our findings hold for both synthetic and real-world data. Ablations: Our ablation study evaluates the role of the cluster influence score Hc and the clusterperformance gap metric Gc (.1) in enabling accurate discovery of spurious correlationsbetween image features and textual attributes. We compare the following three metrics for rankingclusters: (1) an unweighted cluster performance gap metric where wy is set to 1, (2) the clusterperformance gap with wy computed as in .1, and (3) a combination of the cluster performancegap and cluster influence metric as used in RAVL. As shown in , the metrics utilized byRAVL consistently demonstrate the best performance across various learned correlation strengths(eval). Our results suggest the utility of both the performance gap metric and the influence score inidentifying fine-grained spurious correlations. Evaluations in the Wild: In addition to our controlled evaluations, we evaluate the ability of RAVLto surface spurious correlations learned by 12 off-the-shelf VLMs ; this presents a realisticand uncontrolled evaluation setting. We consider two zero-shot classification tasks DV : (1) a 397-class scene classification task on SUN397 and (2) binary classification of cardiomegaly in chestX-rays from ObjectCXR . We use the cluster performance gap metric Gc, introduced in .1, to quantify the degree of the learned spurious correlation. Our results demonstrate that all evaluated models, which span a range of architecture, training data,and parameter counts, show evidence of having learned spurious correlations; this is demonstrated bynonzero values of the cluster performance gap metric Gc. On average across the evaluated models, thetop-ranked spurious feature cluster discovered by RAVL on SUN397 achieves a cluster performance",
  "ModelClassication TaskSpurious Features Identied by RaVLZero-Shot Accuracy": ": RAVL surfaces spurious correlations in off-the-shelf VLMs. RAVL identifies a spuriouscorrelation learned by CLIP ViT-B/16 between the presence of text-based retail signage and theclass label fast food restaurant in a scene classification task. RAVL also surfaces a spuriouscorrelation learned by PubMedCLIP ResNet-50 between metal clips (found in clothing) and the classlabel cardiomegaly (a heart condition) on a chest X-ray classification task. : RAVL effectively mitigates spurious correlations. Here, we report mean Image Overall,Image Worst-Group (Img. WG), Region Overall, and Region Worst-Group (Reg. WG) metrics acrossour real-world evaluation settings. Since performance of mitigation methods is dependent on theresults of Stage 1, we report metrics across settings where Stage 1 Precision@10> 0.6 and Stage 1Precision@10> 0.8.",
  "Img. OverallImg. WGReg. OverallReg. WGImg. OverallImg. WGReg. OverallReg. WG": "Standard FT64.031.472.046.964.631.072.947.4Upsampled FT66.637.874.352.266.737.774.752.8VL-ERM68.832.275.650.368.730.975.950.6VL-GDRO69.133.775.650.468.831.176.051.0Spurious-Aware69.833.676.550.669.230.776.850.5RAVL (Ours)69.839.178.957.870.240.879.558.5 gaps (Gc) of 9.93.2 (minimum = 5.1, maximum = 14.0). On ObjectCXR, the mean value of Gc is0.080.04 (minimum = 0.04, maximum = 0.12).2 Our results support findings from previous worksuggesting that all models may learn spurious correlations . In , we provide qualitative examples of discovered spurious features for the CLIP ViT-B/16model evaluated on SUN397 and the PubmedCLIP ResNet-50 model evaluated on ObjectCXR. Forthe CLIP ViT-B/16 model, RAVL surfaces a feature cluster consisting of text-based retail signage.We observe significant performance gaps between images containing the RAVL-identified featureand images that do not contain the feature. For instance, we note a 48.2 point difference in zero-shotclassification accuracy for the class label fast food restaurant, suggesting that a CLIP ViT-B/16model can better classify a scene of a fast food restaurant when a text-based retail sign is present. Forthe PubmedCLIP ResNet-50 model, RAVL discovers that the presence of metal clips (found in thepatients clothing) is spuriously correlated with cardiomegaly. We observe that the presence of clipsimproves zero-shot classification accuracy for the class label cardiomegaly by 15.3 points.",
  "Mitigating Spurious Correlations in Fine-Tuned Vision-Language Models": "In this section, we present the second stage of RAVL, which aims to mitigate learned spuriouscorrelations in VLMs. In .1, we discuss our methodology for mitigating fine-grainedspurious correlations with a novel region-aware loss function. In .2, we use the evaluationframework previously introduced in .2 to demonstrate that RAVL substantially outperformsprior approaches in mitigating spurious correlations between image features and textual attributes. 2We note that since the formula for Gc involves a summation over class labels, raw values of Gc for our2-class chest X-ray classification task are lower than those for our 397-class scene classification task.",
  "Our Approach: Mitigating Spurious Correlations": "As described in , Stage 1 of RAVL discovers image features that VLM M has learnedto spuriously correlate with textual attributes. We next aim to mitigate the spurious correlation.Motivated by prior work on fine-grained VLMs , our key insight is that utilizing region-levelinformation during VLM training can enable models to focus on relevant image-text relationshipsand ignore spurious correlations. Since dataset DF exclusively consists of images and text, ground-truth subgroup and class labels arenot available. As a result, we first assign plausible (i) region-level subgroup labels and (ii) image-levelclass labels to the vision-language fine-tuning dataset DF . To assign subgroup labels, we decomposeeach image Ii in dataset DF into a set of candidate regions Ri. We then fit the trained K-Medoidsclustering model from .1 on Ri and identify all spurious regions associated with the topranked cluster. We represent the identified spurious regions as Rsi and remaining non-spuriousregions as Rri such that Rsi Rri = Ri. In order to assign plausible class labels, we parse the pairedtext Ti associated with each image to identify samples that reference the class labels included in thezero-shot classification label set Y; we refer to the assigned class label for image Ii as yi.",
  "yjB m(Rri , yj) + P(RsB)(3)": "Here,for region embedding function f and text embedding function g,m(A, b)=exp(maxaA(f(a), g(b) /)) with temperature . The term P(RsB) is a penalty that enforcesembedding-level dissimilarity between spurious regions and correlated class labels. The second loss component LiA encourages high embedding similarity between non-spurious re-gions Rri and assigned class label yi when compared to other regions. We define (a, b) =exp(f(a), g(b) /) with temperature .",
  "rjRsB (rj, yi).(4)": "The final loss is expressed as L = LCL + (1 ) |B|i=1(LiR + LiA). Here, is a hyperparameterand LCL takes the form of the original loss function used for training M; in our experiments, LCL isthe CLIP objective . Extended formulations of our loss function are provided in Appendix C.",
  "Results: RaVL Effectively Mitigates Spurious Correlations": "Comparisons to Prior Approaches: We use the evaluation framework previously introduced in.2 to compare RAVL with prior approaches. There are few existing approaches for mitigatingspurious correlations in the setting of fine-tuned VLMs. Here, we compare RAVL with standardVLM fine-tuning, upsampled VLM fine-tuning, ERM, GDRO , and Spurious-Aware Mitigation. Since ERM and GDRO are traditionally used in unimodal classification settings, we adapt theseapproaches for our setting by adding a contrastive vision-language objective and using zero-shotclassification scores during fine-tuning; we refer to these approaches as VL-ERM and VL-GDROrespectively. summarizes mean zero-shot classification results across our real-world evaluation settings.Since performance of mitigation methods is dependent on the accuracy of the discovered spuriouscorrelations in Stage 1, displays results for two evaluation categories: (i) the 192 settingswhere RAVL Stage 1 Precision@10 is greater than 0.6, and (ii) the 106 settings where RAVL Stage 1Precision@10 is greater than 0.8. In line with prior works on robustness , we report imageoverall performance and image worst-group performance. Additionally, in order to evaluate theextent to which the VLM understands fine-grained features, we introduce two new metrics: regionoverall performance and region worst-group performance. Region-level accuracies are computed byperforming zero-shot classification with region embeddings and comparing predicted labels to theground-truth region-level labels provided in the zero-shot classification dataset. Results show that RAVL consistently outperforms prior approaches in mitigating spurious correlations.Across the two evaluation categories in , RAVL contributes to an improvement of up to 8.2%on image worst-group performance and 10.8% on region worst-group performance over the nearestbaseline. Improvements in region worst-group performance are particularly notable, suggesting thatRAVL can better interpret fine-grained features when compared to prior approaches. Additionally,as the accuracy of the discovered spurious correlations in Stage 1 increases, the performance of theRAVL mitigation approach increases proportionally. Our results demonstrate the efficacy of ourfine-tuning procedure in mitigating spurious correlations when compared to prior approaches.",
  "Conclusion": "In this work, we introduced RAVL, a fine-grained region-aware approach for addressing spuriouscorrelations in VLMs. We demonstrate through large-scale, controlled experiments as well as in-the-wild evaluations that RAVL can discover (191% improvement in identified correlations) and mitigate(8.2% improvement on worst-group performance) spurious correlations in VLMs. We hope that ourwork can help (i) diagnose and correct critical failure modes in VLMs prior to deployment and (ii)drive progress towards the development of fine-grained approaches for model robustness.",
  "and Disclosure of Funding": "We are thankful to Sophie Ostmeier, Eduardo Reis, and Ashwin Kumar for helpful discussionsand feedback. MV is supported by graduate fellowship awards from the Department of Defense(NDSEG), the Knight-Hennessy Scholars program at Stanford University, and the Quad program.AC is supported by NIH grants R01 HL167974, R01HL169345, R01 AR077604, R01 EB002524,R01 AR079431, P41 EB027060, AY2AX000045, and 1AYSAX0000024-01; and NIH contracts75N92020C00008 and 75N92020C00021. CL is supported by NIH grants R01 HL155410, R01HL157235, by AHRQ grant R18HS026886, and by the Gordon and Betty Moore Foundation. JBDand CL are supported by the Medical Imaging and Data Resource Center (MIDRC), which isfunded by the National Institute of Biomedical Imaging and Bioengineering (NIBIB) under contract75N92020C00021 and through The Advanced Research Projects Agency for Health (ARPA-H).",
  "Sara Beery, Grant Van Horn, and Pietro Perona. Recognition in terra incognita. In Proceedings of theEuropean Conference on Computer Vision (ECCV), September 2018": "Louis Blankemeier, Joseph Paul Cohen, Ashwin Kumar, Dave Van Veen, Syed Jamal Safdar Gardezi, Mag-dalini Paschali, Zhihong Chen, Jean-Benoit Delbrouck, Eduardo Reis, Cesar Truyts, Christian Bluethgen,Malte Engmann Kjeldskov Jensen, Sophie Ostmeier, Maya Varma, Jeya Maria Jose Valanarasu, ZhongnanFang, Zepeng Huo, Zaid Nabulsi, Diego Ardila, Wei-Hung Weng, Edson Amaro Junior, Neera Ahuja,Jason Fries, Nigam H. Shah, Andrew Johnston, Robert D. Boutin, Andrew Wentland, Curtis P. Langlotz,Jason Hom, Sergios Gatidis, and Akshay S. Chaudhari. Merlin: A vision language foundation model for3d computed tomography, 2024. Zhihong Chen, Maya Varma, Jean-Benoit Delbrouck, Magdalini Paschali, Louis Blankemeier, Dave VanVeen, Jeya Maria Jose Valanarasu, Alaa Youssef, Joseph Paul Cohen, Eduardo Pontes Reis, Emily B. Tsai,Andrew Johnston, Cameron Olsen, Tanishq Mathew Abraham, Sergios Gatidis, Akshay S. Chaudhari, andCurtis Langlotz. Chexagent: Towards a foundation model for chest x-ray interpretation, 2024. Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon,Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for contrastivelanguage-image learning, 2022. Patrick John Chia, Giuseppe Attanasio, Federico Bianchi, Silvia Terragni, Ana Rita Magalhaes, DiogoGoncalves, Ciro Greco, and Jacopo Tagliabue. Contrastive language and vision learning of general fashionconcepts. Scientific Reports, 12(1), November 2022. Joseph Paul Cohen, Joseph D Viviano, Paul Bertin, Paul Morrison, Parsa Torabian, Matteo Guarrera,Matthew P Lungren, Akshay Chaudhari, Rupert Brooks, Mohammad Hashir, et al. Torchxrayvision: Alibrary of chest x-ray datasets and models. In International Conference on Medical Imaging with DeepLearning, pages 231249. PMLR, 2022.",
  "Li Deng. The mnist database of handwritten digit images for machine learning research. IEEE SignalProcessing Magazine, 29(6):141142, 2012": "Sedigheh Eslami, Christoph Meinel, and Gerard De Melo. Pubmedclip: How much does clip benefit visualquestion answering in the medical domain? In Findings of the Association for Computational Linguistics:EACL 2023, pages 11511163, 2023. Sabri Eyuboglu, Maya Varma, Khaled Kamal Saab, Jean-Benoit Delbrouck, Christopher Lee-Messer, JaredDunnmon, James Zou, and Christopher Re. Domino: Discovering systematic errors with cross-modalembeddings. In International Conference on Learning Representations, 2022. Alex Fang, Gabriel Ilharco, Mitchell Wortsman, Yuhao Wan, Vaishaal Shankar, Achal Dave, and LudwigSchmidt. Data determines distributional robustness in contrastive language image pre-training (clip), 2022.",
  "Kaiming He, Georgia Gkioxari, Piotr Dollr, and Ross Girshick. Mask r-cnn, 2017": "Zhi Huang, Federico Bianchi, Mert Yuksekgonul, Thomas J Montine, and James Zou. A visual-languagefoundation model for pathology image analysis using medical twitter. Nat. Med., 29(9):23072316,September 2023. Badr Youbi Idrissi, Martin Arjovsky, Mohammad Pezeshki, and David Lopez-Paz. Simple data balancingachieves competitive worst-group-accuracy. In Bernhard Schlkopf, Caroline Uhler, and Kun Zhang, edi-tors, Proceedings of the First Conference on Causal Learning and Reasoning, volume 177 of Proceedingsof Machine Learning Research, pages 336351. PMLR, 1113 Apr 2022. Wisdom Oluchi Ikezogwo, Mehmet Saygin Seyfioglu, Fatemeh Ghezloo, Dylan Stefan Chan Geva,Fatwir Sheikh Mohammed, Pavan Kumar Anand, Ranjay Krishna, and Linda Shapiro. Quilt-1m: Onemillion image-text pairs for histopathology. In Thirty-seventh Conference on Neural Information ProcessingSystems Datasets and Benchmarks Track, 2023. Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, AchalDave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and LudwigSchmidt. Openclip, July 2021. If you use this software, please cite it as below. Pavel Izmailov, Polina Kirichenko, Nate Gruver, and Andrew G Wilson. On feature learning in the presenceof spurious correlations. Advances in Neural Information Processing Systems, 35:3851638532, 2022.",
  "JFHealthcare. Object-cxr - automatic detection of foreign objects on chest x-rays. 2020": "Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung,Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy textsupervision. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conferenceon Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 49044916.PMLR, 1824 Jul 2021.",
  "Weixin Liang and James Zou. Metashift: A dataset of datasets for evaluating contextual distribution shiftsand training conflicts. In International Conference on Learning Representations, 2021": "Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollr,and C. Lawrence Zitnick. Microsoft coco: Common objects in context. In David Fleet, Tomas Pajdla,Bernt Schiele, and Tinne Tuytelaars, editors, Computer Vision ECCV 2014, pages 740755, Cham, 2014.Springer International Publishing. Evan Z Liu, Behzad Haghgoo, Annie S Chen, Aditi Raghunathan, Pang Wei Koh, Shiori Sagawa, PercyLiang, and Chelsea Finn. Just train twice: Improving group robustness without training group information.In International Conference on Machine Learning, pages 67816792. PMLR, 2021. Mazda Moayeri, Phillip Pope, Yogesh Balaji, and Soheil Feizi. A comprehensive study of image classifica-tion model sensitivity to foregrounds, backgrounds, and visual attributes. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition, pages 1908719097, 2022.",
  "Luke Oakden-Rayner, Jared Dunnmon, Gustavo Carneiro, and Christopher R. Hidden stratification causesclinically meaningful failures in machine learning for medical imaging. September 2019": "Obioma Pelka, Sven Koitka, Johannes Rckert, Felix Nensa, and Christoph M. Friedrich. Radiologyobjects in context (roco): A multimodal image dataset. In Danail Stoyanov, Zeike Taylor, Simone Balocco,Raphael Sznitman, Anne Martel, Lena Maier-Hein, Luc Duong, Guillaume Zahnd, Stefanie Demirci,Shadi Albarqouni, Su-Lin Lee, Stefano Moriconi, Veronika Cheplygina, Diana Mateus, Emanuele Trucco,Eric Granger, and Pierre Jannin, editors, Intravascular Imaging and Computer Assisted Stenting andLarge-Scale Annotation of Biomedical Data and Expert Label Synthesis, pages 180189, Cham, 2018.Springer International Publishing. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, GirishSastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learningtransferable visual models from natural language supervision. In Marina Meila and Tong Zhang, editors,Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings ofMachine Learning Research, pages 87488763. PMLR, 1824 Jul 2021. Pranav Rajpurkar, Jeremy Irvin, Aarti Bagul, Daisy Ding, Tony Duan, Hershel Mehta, Brandon Yang,Kaylie Zhu, Dillon Laird, Robyn L. Ball, Curtis Langlotz, Katie Shpanskaya, Matthew P. Lungren, andAndrew Y. Ng. Mura: Large dataset for abnormality detection in musculoskeletal radiographs, 2018. Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detectionwith region proposal networks. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors,Advances in Neural Information Processing Systems, volume 28, pages 9199. Curran Associates, Inc.,2015.",
  "Sahil Singla and Soheil Feizi. Salient imagenet: How to discover spurious features in deep learning? InInternational Conference on Learning Representations, 2022": "Sahil Singla, Besmira Nushi, Shital Shah, Ece Kamar, and Eric Horvitz. Understanding failures of deepnetworks via robust feature extraction. In IEEE Conference on Computer Vision and Pattern Recognition,CVPR 2021. Computer Vision Foundation / IEEE, 2021. Nimit Sohoni, Jared Dunnmon, Geoffrey Angus, Albert Gu, and Christopher R. No subclass left behind:Fine-grained robustness in coarse-grained classification problems. In H. Larochelle, M. Ranzato, R. Hadsell,M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages1933919352. Curran Associates, Inc., 2020. Bart Thomee, David A. Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, DamianBorth, and Li-Jia Li. Yfcc100m: the new data in multimedia research. Communications of the ACM,59(2):6473, January 2016. Ekin Tiu, Ellie Talius, Pujan Patel, Curtis P Langlotz, Andrew Y Ng, and Pranav Rajpurkar. Expert-leveldetection of pathologies from unannotated chest x-ray images via self-supervised learning. Nat. Biomed.Eng., 6(12):13991406, December 2022. Maya Varma, Jean-Benoit Delbrouck, Sarah Hooper, Akshay Chaudhari, and Curtis Langlotz. Villa: Fine-grained vision-language representation learning from real-world data. In Proceedings of the IEEE/CVFInternational Conference on Computer Vision, 2023. Maya Varma, Mandy Lu, Rachel Gardner, Jared Dunnmon, Nishith Khandwala, Pranav Rajpurkar, JinLong, Christopher Beaulieu, Katie Shpanskaya, Li Fei-Fei, Matthew P. Lungren, and Bhavik N. Patel.Automated abnormality detection in lower extremity radiographs using deep learning. Nature MachineIntelligence, 1(12):578583, December 2019. Julia K Winkler, Christine Fink, Ferdinand Toberer, Alexander Enk, Teresa Deinlein, Rainer Hofmann-Wellenhof, Luc Thomas, Aimilios Lallas, Andreas Blum, Wilhelm Stolz, et al. Association betweensurgical skin markings in dermoscopic images and diagnostic performance of a deep learning convolutionalneural network for melanoma recognition. JAMA dermatology, 155(10):11351141, 2019. Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Simon Kornblith, Rebecca Roelofs,Raphael Gontijo-Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong, and Ludwig Schmidt.Robust fine-tuning of zero-shot models. arXiv preprint arXiv:2109.01903, 2021.",
  "Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarkingmachine learning algorithms, 2017": "Jianxiong Xiao, James Hays, Krista A. Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Large-scale scene recognition from abbey to zoo. In 2010 IEEE Computer Society Conference on ComputerVision and Pattern Recognition, pages 34853492, 2010. Kai Yuanqing Xiao, Logan Engstrom, Andrew Ilyas, and Aleksander Madry. Noise or signal: The role ofimage backgrounds in object recognition. In International Conference on Learning Representations, 2020. Hanwen Xu, Naoto Usuyama, Jaspreet Bagga, Sheng Zhang, Rajesh Rao, Tristan Naumann, Cliff Wong,Zelalem Gero, Javier Gonzlez, Yu Gu, Yanbo Xu, Mu Wei, Wenhui Wang, Shuming Ma, Furu Wei,Jianwei Yang, Chunyuan Li, Jianfeng Gao, Jaylen Rosemon, Tucker Bower, Soohee Lee, RoshanthiWeerasinghe, Bill J Wright, Ari Robicsek, Brian Piening, Carlo Bifulco, Sheng Wang, and Hoifung Poon.A whole-slide foundation model for digital pathology from real-world data. Nature, 630(8015):181188,June 2024. Minghao Xu, Jian Zhang, Bingbing Ni, Teng Li, Chengjie Wang, Qi Tian, and Wenjun Zhang. Adversarialdomain adaptation with domain mixup. In Proceedings of the AAAI conference on artificial intelligence,volume 34, pages 65026509, 2020.",
  "Michael Zhang, Nimit S. Sohoni, Hongyang R. Zhang, Chelsea Finn, and Christopher R. Correct-n-contrast: A contrastive approach for improving robustness to spurious correlations, 2022": "Sheng Zhang, Yanbo Xu, Naoto Usuyama, Hanwen Xu, Jaspreet Bagga, Robert Tinn, Sam Preston, RajeshRao, Mu Wei, Naveen Valluri, Cliff Wong, Andrea Tupini, Yu Wang, Matt Mazzola, Swadheen Shukla, LarsLiden, Jianfeng Gao, Matthew P. Lungren, Tristan Naumann, Sheng Wang, and Hoifung Poon. Biomedclip:a multimodal biomedical foundation model pretrained from fifteen million scientific image-text pairs,2024. Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chunyuan Li, Noel Codella, Liunian Harold Li, LuoweiZhou, Xiyang Dai, Lu Yuan, Yin Li, and Jianfeng Gao. Regionclip: Region-based language-imagepretraining. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR), pages 1679316803, June 2022.",
  "ARelated Work": "Machine learning models often learn spurious correlations (also known as shortcuts) between imagefeatures and class labels. For instance, models have been shown to rely on the presence of chesttubes rather than disease features when identifying collapsed lungs in chest X-rays ; surgicalskin markings when detecting melanoma from skin lesions ; and environmental features whenperforming object recognition tasks . Models that learn spurious correlations will generalize poorlyto real-world settings. Our work builds on several recent research directions for (i) discovering and(ii) mitigating spurious correlations. Discovering Spurious Correlations. In the unimodal setting, prior works have developed automatedmethods for identifying systematic errors resulting from learned spurious correlations in visionmodels. Using a labeled validation set, these approaches utilize clustering algorithms orlightweight models to identify subgroups of images with high error rates; for instance, inthe example in , images containing butterflies without flowers may be identified as one suchsubgroup. Given a set of images in the identified subgroups, a user can then identify the commonfeatures and rectify the data or model. However, recent work has suggested that it is often challengingfor humans to interpret identified subgroups and accurately determine the shared features resultingin model failure . Additionally, such methods often focus solely on identifying images withhigh error rates (e.g. butterflies without flowers) rather than identifying the specific class of featurescontributing to the error (e.g. flowers). A related line of work has aimed to identify spurious featuresusing human supervision or external concept banks . In the vision-language setting, Yang et al. use an external off-the-shelf object detector to annotatefeatures . Then, for each feature, the difference in zero-shot classification accuracy betweenimages containing the feature and those without the feature is measured; high performance gaps areused to signal spurious features. However, the efficacy of this approach is reliant on the quality ofthe object detector and a human-in-the-loop is used to verify results; also, as we show in this work,performance gaps alone are not always sufficient for discovering spurious features. Mitigating Spurious Correlations. There is a line of work aiming to mitigate spurious correlationsin the context of deep learning . These works explore strategies like dataaugmentation and instance upsampling . While these approaches havebeen explored widely in unimodal tasks , mitigating spurious correlations in vision-languagesettings has not been extensively studied. Some previous works have studied this problem within thecontext of pretrained VLMs ; however, their setting differs markedly from the fine-tuned VLM setting, where datasets are composed of image-text pairs with no class or subgroup labels. Inthe fine-tuned VLM setting, existing works predominantly operate at the global image-level ,which is unlikely to be sufficient for mitigating fine-grained spurious correlations.",
  "BExtended Details on Evaluation Settings": "We create 654 evaluation settings using data from two domains: (1) synthetic data (MNIST and FashionMNIST ) and (2) real-world data (COCO ). Below, we provide implementationdetails for the four components included in each evaluation setting: 1. Predefined spurious correlation: We define a spurious image feature and textual attributepair (eeval, aeval). For MNIST and FashionMNIST, eeval represents a red rectangle; aevalis generated from the set {zero, one, two, three, four five, six, seven, eight, nine} forMNIST and {t-shirt, trouser, pullover, dress, coat, sandal, shirt, sneaker, bag, ankle boot} forFashionMNIST. For COCO, we sample eeval and aeval from the list of annotated attributes. 2. Fine-tuning dataset: Vision-language fine-tuning datasets DevalFare sampled from thetraining sets of MNIST, FashionMNIST, and COCO such that the presence of feature eevalis correlated with the presence of text attribute aeval as measured by Cramers V. For MNISTand FashionMNIST, we synthetically generate text captions by randomly sampling from thefollowing pre-defined prompt templates: THE IMAGE SHOWS A [CLASS LABEL], THE DIGITAPPEARS TO BE [CLASS LABEL], THERE IS AN IMAGE SHOWING A [CLASS LABEL], andTHE NUMBER IS A [CLASS LABEL]. In order to reflect real-world settings where spuriousfeatures (e.g. skin markings in dermoscopic images ) may not be annotated in text, textcaptions in our synthetic settings solely refer to class labels and do not describe the spuriousfeature. For COCO, we use the provided text captions. 3. Fine-tuned VLM: We fine-tune each model M on dataset DevalFusing a single NVIDIAA100 GPU with an initial learning rate of 5e-5. We use a batch size of 128 and train for100 epochs with early stopping. We set the loss temperature as = 0.07. In line withprior works that explore the benefits of locked image-text training , we freeze the textencoder and only learn weights for the image encoder. 4. Evaluation dataset: We construct zero-shot classification datasets DevalVfrom the test setsof MNIST, FashionMNIST, and COCO. For MNIST and FashionMNIST, we generateregion bounding boxes using equally-sized quadrants. For COCO, we use the ground-truthbounding boxes and associated labels. Evaluation datasets are sampled to ensure thata correlation between aeval and eeval does not exist. For MNIST, we perform promptensembling for zero-shot classification using the following prompts: A PHOTO OF THENUMBER [CLASS LABEL]; THE DIGIT [CLASS LABEL]; AN IMAGE OF A [CLASS LABEL];[CLASS LABEL]. For FashionMNIST, we use the following prompts: A PHOTO OF A [CLASSLABEL]; THE [CLASS LABEL]; AN IMAGE OF A [CLASS LABEL]; [CLASS LABEL]. ForCOCO, we use the following prompts: THERE IS A [CLASS LABEL]; A PHOTO OF THE[CLASS LABEL]; A PHOTO OF A [CLASS LABEL]; [CLASS LABEL].",
  "couch": ": Example evaluation settings. Here, we provide examples of predefined spurious correlations,fine-tuning datasets, and evaluation datasets associated with a synthetic evaluation setting (top row)and a real-world evaluation setting (bottom row). The example synthetic evaluation setting consistsof a predefined spurious correlation between a red rectangle (spurious image feature eeval) and nine(textual attribute aeval). This spurious correlation is visible in the vision-language fine-tuning dataset,where the presence of red rectangles and nines are strongly correlated, but not in the evaluation dataset.Similarly, the example real-world evaluation setting consists of a predefined spurious correlationbetween a person (spurious image feature eeval) and couch (textual attribute aeval). Again, thisspurious correlation is visible in the vision-language fine-tuning dataset, where the presence of peopleand couches are strongly correlated, but not in the evaluation dataset.",
  "For batch B, we define RsB as the set of all spurious regions in the batch: RsB =": "IiB Rsi. For imageIi in batch B, the first component of our region-aware loss function LiR is designed to maximizeembedding similarity between non-spurious regions Rri and assigned class label yi; simultaneously,LiR will minimize embedding similarity between non-spurious regions Rri and other class labels inthe batch. We formulate LiR as follows:",
  "The formula for LiR includes two similarity functions: and m. We define and m as follows.Let f represent a region embedding function (associated with the image encoder of VLM M) and": "let g represent a text embedding function (associated with the text encoder of VLM M). Then, foran arbitrary region a, the function f(a) will generate region embedding f(a) Rd with embeddingdimension d. For an arbitrary class label b, g(b) will generate text embedding g(b) Rd. Given thisnotation, we establish the following definitions for and m:",
  "(a, b) = exp(f(a), g(b) /)(7)m(A, b) = exp(maxaA(f(a), g(b) /))(8)": "In the loss term LiR, the function m(Rri , yi) will compute the maximum similarity between regionsin Rri and class label yi. We specifically use the maximum operation in this computation since thereare likely to be regions included in Rri that do not reflect the class label; for instance, in the exampleprovided in , there may be non-spurious regions such as trees or leaves included in Rri , whichdo not align with the animal class labels. The maximum operation ensures that the similarity betweenat least one region in Rri and the class label should be high. The second component of our region-aware loss function LiA is designed to maximize embeddingsimilarity between non-spurious regions Rri and assigned class label yi; simultaneously, LiA willminimize embedding similarity between other regions in the batch and class label yi. We formulateLiA as follows:",
  "D.1.1Extended Comparisons to Prior Approaches": "We implement RAVL according to the details provided in .1. RAVL includes a clusteringstep that identifies groups of visually-similar regions. For all evaluation settings, we identify theoptimal number of clusters by sweeping all cluster sizes ranging between |Y| 2 and |Y| 5; we thenselect the optimal number of clusters using Silhouette scores. We select these bounds to be largerthan the class label set size by several multiples in order to ensure that clusters adequately separatedistinct features. Prior works have also utilized overclustering approaches for this objective.We note that users can adjust the bounds based on the composition of their dataset; for instance,complex datasets with diverse features may require a larger range. For MNIST and FashionMNIST,the size of the label set |Y| is 10; For COCO, the size of the label set ranges between 2 and 5. For all baselines, we utilize the official implementations provided by the authors. We adapt Domino,George, and Distilling Failures for our setting by providing region embeddings as input rather thanimage embeddings. In , we provided an extended version of . We demonstrate that RAVL consistentlyoutperforms baselines across two domains (synthetic images and real images), two model initializa-tions (CLIP-RN50 and CLIP-RN101), and four learned correlation strengths (measured by varyingeval {10, 20, 30, 40}).",
  "Strength of Learned Spurious Correlation (eval)": ": RAVL accurately identifies spurious correlations. Here, we provide an extended versionof , which demonstrates that RAVL consistently outperforms prior methods in discoveringlearned spurious correlations between image features and textual attributes. Here, we providePrecision@10 metrics for a CLIP-RN50 model fine-tuned on synthetic data (129 settings) and real-world data (171 settings); a CLIP-RN101 model fine-tuned on synthetic data (162 settings) andreal-world data (192 settings); and an average across both model architectures. and the six ViT models utilize Vision Transformer backbones. The CLIP models were trainedon a proprietary dataset with 400M image-text pairs. OpenCLIP ResNet models were trained onYFCC15M , and OpenCLIP ViT models were trained on LAION2B . We select SUN397 as our zero-shot classification dataset DV . SUN397 consists of scene imagesfrom 397 classes. We use the test data from official partition number 1, which consists 19,850 images.We then use an off-the-shelf region proposal network to identify candidate regions. For each VLM M, we perform 397-class zero-shot scene classification on SUN397. We use a promptensemble consisting of two prompt templates as provided by CLIP . Due to the large size of thezero-shot classification dataset DV , we perform clustering using the CLARA (Clustering for LargeApplications) algorithm, which is an efficient implementation of K-Medoids, and fix the number ofclusters as |Y| 2, which is 794 in this case. Evaluations on chest X-ray classification: Here, we provided extended details on our in-the-wildevaluations performed on medical images (.3). In recent years, a range of vision and vision-language models have been proposed for learning diagnostic patterns inmedical images, and there is a critical need for methods capable of identifying spurious correlationsin this domain. Our goal is to determine if RAVL can effectively surface spurious correlations learnedby real-world fine-tuned VLMs developed for medical image interpretation. We leverage two off-the-shelf variants of the PubMedCLIP model as our VLM M: PubMedCLIP-RN50 and PubMedCLIP-ViTB/32. The PubMedCLIP-RN50 model utilizes a ResNet-50 visionencoder and was fine-tuned from the CLIP-RN50 model. The PubMedCLIP-ViTB/32 model utilizesa Vision Transformer backbone for the vision encoder and was fine-tuned from the CLIP-ViTB/32",
  "model. Both variants of PubMedCLIP are fine-tuned using ROCO, a large radiology dataset consistingof images and captions collected from PubMed": "We select Object-CXR as our zero-shot classification dataset DV . Object-CXR is a dataset of 10,000frontal chest X-rays compiled from around 300 township hospitals in China . Twelve radiologistswith 1-3 years of experience annotated the images, identifying foreign objects within the lung fieldusing bounding boxes, ellipses, or masks, excluding support devices. We retain only boundingboxes and exclude chest X-rays without annotations, resulting in 8,726 object annotations across4,372 images. For our evaluations, we use the Object-CXR dev split, which includes 974 objectannotations across 489 images. We assign image-level labels to the dataset using torchxrayvision ,a library that includes a variety of pretrained chest X-ray models. Specifically, we use the XRV-DENSENET121-DENSENET121-RES224-ALL pretrained model to produce multi-class labels fora variety of diseases, including Enlarged Cardiomediastinum, Cardiomegaly, Lung Opacity, LungLesion, Edema, Consolidation, Pneumonia, Atelectasis, Pneumothorax, Pleural Effusion, and Fracture.A disease is identified as present if it meets a confidence threshold of 0.60. For each PubMedCLIP VLM M, we perform binary zero-shot classification of cardiomegaly inObject-CXR. Cardiomegaly is a medical condition characterized by the presence of an enlarged heart.After performing a manual search over the text prompt space, we identify CARDIOMEGALY andNORMAL as the prompts that lead to the highest zero-shot classification accuracy for both modelvariants. The PubMedCLIP-RN50 model achieves an overall zero-shot classification accuracy of74.2, with an accuracy of 14.0 on the group of images with cardiomegaly and an accuracy of 91.9on the group of images without cardiomegaly. The PubMedCLIP-ViTB/32 achieves an overallzero-shot classification accuracy of 39.4, with an accuracy of 80.4 on the group of images withcardiomegaly and an accuracy of 28.0 on the group of images without cardiomegaly. Interestingly,given the selected prompts, we note that the PubMedCLIP-RN50 and the PubMedCLIP-ViTB/32models exhibit inverse trends, with PubMedCLIP-RN50 achieving higher performance on the classof images without cardiomegaly and PubMedCLIP-ViTB/32 achieving higher performance on theclass of images with cardiomegaly. Given VLM M and zero-shot classification dataset DevalV, we apply RAVL in order to surfacelearned spurious correlations. Similar to our controlled evaluations on synthetic datasets, we performK-Medoids clustering with the number of clusters ranging from 20 to 50. The optimal numberof clusters is selected using Silhouette distance; we use 24 clusters for PubMedCLIP-RN50 and20 clusters for PubMedCLIP-ViTB/32. The final cluster performance gap metric Gc associatedwith the top-ranked spurious feature cluster is 0.041 and 0.119 for the PubMedCLIP-RN50 andPubMedCLIP-ViTB/32 models respectively.",
  "For the OpenCLIP ViT-L/14 model, RaVL surfaces a feature cluster consisting of greenplants and fences. We observe a performance gap of 18.3 points between images with class": "label outdoor chicken coop that contain the RaVL-identified feature and those that donot contain the feature. This suggests that the OpenCLIP ViT-L/14 model can better classifyan outdoor chicken coop scene when green plants and fences are present. For the CLIP ViT-B/32 model, RaVL surfaces a feature cluster consisting of people. Weobserve a performance gap of 24.3 points between images with class label pub (indoor)that contain the RaVL-identified feature and those that do not contain the feature. Thissuggests that the CLIP ViT-B/32 model can better classify a pub (indoor) scene whenpeople are present. For the OpenCLIP ResNet-101 model, RaVL surfaces a feature cluster consisting of chairs.We observe a performance gap of 23.3 points between images with class label restaurantpatio that contain the RaVL-identified feature and those that do not contain the feature.This suggests that the OpenCLIP ResNet-101 model can better classify restaurant patioscenes when chairs are present.",
  "D.2Extended Results for RAVL Stage 2 (Mitigation)": "We train model Mnew using a single NVIDIA A100 GPU with an initial learning rate of 5e-5. Weuse a batch size of 128 and train for 100 epochs with early stopping. We set the loss temperature as = 0.07 and use = 0.8 in loss function L. In line with prior works that utilize locked image-textfine-tuning , we freeze the text encoder and solely learn weights for the image encoder. Wegenerate candidate regions for the fine-tuning dataset DevalFusing a region proposal network withidentical settings to prior work . Below, we provide additional implementation details for the five mitigation baselines we explore inthis study. Since there are a limited number of existing approaches designed for mitigating spuriouscorrelations in fine-tuned VLMs, we adapt several existing methods for our setting in order to trainmodel Mnew:",
  "Standard VLM Fine-Tuning: We perform standard VLM fine-tuning with the original lossfunction LCL used to train model M. In our experiments, LCL is the CLIP objective": "Upsampled VLM Fine-Tuning: We perform VLM fine-tuning with the original loss functionLCL used to train model M. In our experiments, LCL is the CLIP objective . We utilizea weighted sampler to upsample minority groups during training; class and subgroup labelsare derived from Stage 1 of RAVL as detailed in .1. VL-ERM: Since empirical risk minimization (ERM) is traditionally used in unimodal clas-sification settings, we adapt ERM for our multimodal setting by incorporating an extracontrastive vision-language objective function; this loss function is intended to ensure thatVLM Mnew learns image-text relationships during training. Specifically, the final lossfunction during training is LV LERM = LCL + (1 )LERM. Here, LCL takes the formof the original loss function used to train model M; in our experiments LCL is the CLIPobjective . We set = 0.8. During training, we apply ERM to zero-shot classificationlogits computed using image embeddings and text embeddings of class labels. We utilize aweighted sampler to upsample minority subgroups; class and subgroup labels are derivedfrom Stage 1 of RAVL as detailed in .1. VL-GDRO : Since GDRO is traditionally used in unimodal classification settings,we adapt GDRO for our multimodal setting by incorporating an extra contrastive vision-language objective function; this loss function is intended to ensure that VLM Mnew learnsimage-text relationships during training. Specifically, the final loss function during trainingis LV LGDRO = LCL + (1 )LGDRO. Here, LCL takes the form of the original lossfunction used to train model M; in our experiments LCL is the CLIP objective . We set = 0.8. During training, we apply GDRO to zero-shot classification logits computed usingimage embeddings and text embeddings of class labels. In line with standard practice, weutilize a weighted sampler to upsample minority subgroups; class and subgroup labels arederived from Stage 1 of RAVL as detailed in .1. Spurious-Aware Mitigation : Spurious-aware mitigation aims to address spurious corre-lations in VLMs using a combination of five loss functions: one CLIP objective function,two contrastive image objective functions meant to address spurious correlations in theimage space, and two contrastive language objective functions meant to address spurious : RAVL effectively mitigates spurious correlations across various model initializations. Here,we provide an extended version of with a breakdown of results by model initialization(CLIP-RN50 vs. CLIP-RN101). Our results demonstrate that RAVL consistently outperforms priormethods in mitigating spurious correlations. We report mean Image Overall (Img. Overall), ImageWorst Group (Img. WG), Region Overall (Reg. Overall), and Region Worst Group (Reg. WG)metrics across our real-world evaluation settings.",
  "CLIP-RN101": "Standard FT63.928.971.345.064.727.672.044.4Upsampled FT67.438.474.652.867.837.575.053.2VL-ERM70.533.577.053.470.832.277.454.0VL-GDRO70.534.976.553.170.532.176.854.1Spurious-Aware71.334.878.153.971.232.478.453.9RAVL (Ours)71.040.479.559.271.842.279.859.8 correlations in the text space. We note that Spurious-Aware Mitigation was explicitly de-signed for the fine-tuned VLM setting. We follow the implementation of Spurious-AwareMitigation provided by . In our work, since we solely fine-tuned the vision encoders ofVLMs M, we use a version of Spurious-Aware Mitigation with the CLIP objective functionand two contrastive image objective functions. Prior works on model robustness predominantly evaluate model performance using image worst-group scores . In addition to image worst-group accuracy, we also report region overall andregion worst-group accuracies, which evaluate the extent to which the VLM understands fine-grainedfeatures. Region-level accuracies are computed by performing zero-shot classification with regionembeddings and comparing predicted labels to the ground-truth region-level labels provided in thezero-shot classification dataset. In , we provide an extended version of with a breakdown of results by modelinitialization (CLIP-RN50 and CLIP-RN101). We demonstrate that RAVL consistently outperformsprior methods across both model initializations. In , we provide an extended version of with a breakdown of results by the learned correlation strength of the original VLM M. RAVLconsistently outperforms prior methods across four correlation strengths eval {10, 20, 30, 40}.",
  "D.3Computational Complexity Analysis": "In this section, we provide an analysis of the computational complexity of RAVL. RAVL is compu-tationally inexpensive; in particular, the RAVL discovery stage can be run efficiently on CPU andthe mitigation stage adds only a small computational overhead. Below, we provide an analysis ofcomputational complexity for each stage of RAVL. Computational complexity analysis of RAVL Stage 1: The discovery stage of RAVL is specificallydesigned to be run on a labeled validation dataset DV ; in real-world settings, validation datasets areoften relatively small in size due to the human effort needed for securing labels, rendering this stageas computationally inexpensive for diverse applications. Even if the validation dataset is large in size,RAVL operates efficiently as follows: First, RAVL preprocesses images by decomposing each image into candidate regions; thereare a variety of ways in which a user can decompose an image into regions, such as by usingequal-sized segments (e.g. quadrants) or running inference with region proposal networks(RPNs). Both methods are inexpensive and only need to be run once in an offline manner.Similar approaches have been applied to large-scale datasets in prior work .",
  "Then, embeddings need to be generated for each region, which can be done by utilizingVLM M for inference (forward passes only). Across a set of 10 FashionMNIST and COCO": ": RAVL effectively mitigates spurious correlations across learned correlation strengths. Here,we provide an extended version of with a breakdown of results by the learned correlationstrength (eval {10, 20, 30, 40} of the original model M. We use the subset of 106 evaluationsettings where RAVL Stage 1 Precision@10 is greater than 0.8. Our results demonstrate that RAVLconsistently outperforms prior methods in mitigating spurious correlations across various correlationstrengths and model initializations. We report mean Image Worst Group (Img. WG) and RegionWorst Group (Reg. WG) metrics across our real-world evaluation settings. We note that there are novalid evaluation settings for CLIP-RN101 when the learned correlation strength eval of the originalmodel M is set to 40.",
  "evaluation settings, we observe embedding generation to take a mean of 24.5 seconds on asingle A100 GPU": "Finally, given candidate regions and corresponding embeddings, the remainder of the RaVLdiscovery procedure (clustering and computation of metrics) can be run completely onCPU. Across a set of 10 evaluation settings on COCO and FashionMNIST, we observe thatclustering and computation of metrics require a mean of 3.4 seconds to run. Computational complexity analysis of RAVL Stage 2: The mitigation stage of RAVL requiresfinetuning a VLM Mnew. Across a set of 10 evaluation settings on COCO and FashionMNIST, weobserve that the inclusion of our fine-grained region-aware loss function at this stage adds an averageof 0.15 seconds per training step (on a single A100 GPU) in comparison to the original fine-tuningprocedure for M.",
  "EExtended Discussion": "Societal Impact: The goal of our work is to improve robustness of fine-tuned VLMs to spuriouscorrelations. As VLMs become more commonplace in society, we hope that our approach can enableusers to better detect and mitigate model failures prior to deployment. We also note that our workincludes a series of evaluations on medical images; rigorous clinical testing is necessary beforerobustness approaches are deployed in healthcare settings. Limitations: In line with prior works in vision-only and vision-language settings, our method isspecifically designed to surface and mitigate local, fine-grained spurious features. There may be somesources of spurious signal that do not manifest in this way; for instance, features like image brightnessor gender can be considered global features, where the spurious signal is not localized to a particularimage region. Our approach is not designed for these global spurious features. Rather, our problemsetting is inspired by the many real-world, practical examples of region-level spurious features thathave been demonstrated in literature to affect model performance, such as image-level markings indermoscopic images , medical devices in radiographs , and text markers in medical images."
}