{
  "Abstract": "Regression tasks, while aiming to model relationships across the entire input space,are often constrained by limited training data. Nevertheless, if the hypothesis func-tions can be represented effectively by the data, there is potential for identifying amodel that generalizes well. This paper introduces the Neural Restricted IsometryProperty (NeuRIPs), which acts as a uniform concentration event that ensures allshallow ReLU networks are sketched with comparable quality. To determine thesample complexity necessary to achieve NeuRIPs, we bound the covering numbersof the networks using the Sub-Gaussian metric and apply chaining techniques. As-suming the NeuRIPs event, we then provide bounds on the expected risk, applicableto networks within any sublevel set of the empirical risk. Our results show that allnetworks with sufficiently small empirical risk achieve uniform generalization.",
  "Introduction": "A fundamental requirement of any scientific model is a clear evaluation of its limitations. In recentyears, supervised machine learning has seen the development of tools for automated model discoveryfrom training data. However, these methods often lack a robust theoretical framework to estimatemodel limitations. Statistical learning theory quantifies the limitation of a trained model by thegeneralization error. This theory uses concepts such as the VC-dimension and Rademacher complexityto analyze generalization error bounds for classification problems. While these traditional complexitynotions have been successful in classification problems, they do not apply to generic regressionproblems with unbounded risk functions, which are the focus of this study. Moreover, traditionaltools in statistical learning theory have not been able to provide a fully satisfying generalizationtheory for neural networks. Understanding the risk surface during neural network training is crucial for establishing a strongtheoretical foundation for neural network-based machine learning, particularly for understandinggeneralization. Recent studies on neural networks suggest intriguing properties of the risk surface.In large networks, local minima of the risk form a small bond at the global minimum. Surprisingly,global minima exist in each connected component of the risks sublevel set and are path-connected.In this work, we contribute to a generalization theory for shallow ReLU networks, by giving uniformgeneralization error bounds within the empirical risks sublevel set. We use methods from the analysisof convex linear regression, where generalization bounds for empirical risk minimizers are derivedfrom recent advancements in stochastic processes chaining theory. Empirical risk minimizationfor non-convex hypothesis functions cannot generally be solved efficiently. However, under certainassumptions, it is still possible to derive generalization error bounds, as we demonstrate in this paperfor shallow ReLU networks. Existing works have applied methods from compressed sensing tobound generalization errors for arbitrary hypothesis functions. However, they do not capture therisks stochastic nature through the more advanced chaining theory. This paper is organized as follows. We begin in Section II by outlining our assumptions about theparameters of shallow ReLU networks and the data distribution to be interpolated. The expected andempirical risk are introduced in Section III, where we define the Neural Restricted Isometry Property",
  "Notation and Assumptions": "In this section, we will define the key notations and assumptions for the neural networks examinedin this study. A Rectified Linear Unit (ReLU) function : R R is given by (x) := max(x, 0).Given a weight vector w Rd, a bias b R, and a sign {1}, a ReLU neuron is a function(w, b, ) : Rd R defined as",
  "Concentration of the Empirical Norm": "Supervised learning algorithms interpolate labels y for samples x, both distributed jointly by onX Y. This task is often solved under limited data accessibility. The training data, respectingAssumption 2, consists of m independent copies of the random pair (x, y). During training, theinterpolation quality of a hypothesis function f : X Y can only be assessed at the given randomsamples {xj}mj=1. Any algorithm therefore accesses each function f through its sketch samples",
  "The functional || ||, also gives the norm of the space L2(Rd, x), which consists of functionsf : Rd R withf2 := Ex[|f(x)|2]": "If the label y depends deterministically on the associated sample x, we can treat y as an element ofL2(Rd, x), and the expected risk of any function f is the functions distance to y. By sketching anyhypothesis function f with the sample operator S, we perform a Monte-Carlo approximation of theexpected risk, which is termed the empirical risk:",
  "The random functional || ||m also defines a seminorm on L2(Rd, x), referred to as the empiricalnorm. Under mild assumptions, || ||m fails to be a norm": "In order to obtain a well generalizing model, the goal is to identify a function f with a low expectedrisk. However, with limited data, we are restricted to optimizing the empirical risk. Our strategy forderiving generalization guarantees is based on the stochastic relation between both risks. If {xj}mj=1are independently distributed by x, the law of large numbers implies that for any f L2(Rd, x)the convergencelimm fm = f. While this establishes the asymptotic convergence of the empirical norm to the function norm for asingle function f, we have to consider two issues to formulate our concept of norm concentration:First, we need non-asymptotic results, that is bounds on the distance |fm f| for a fixednumber of samples m. Second, the bounds on the distance need to be uniformly valid for all functionsf in a given set. Sample operators which have uniform concentration properties have been studied as restrictedisometries in the area of compressed sensing. For shallow ReLU networks of the form (1), we definethe restricted isometry property of the sampling operator S as follows.",
  "In the following Theorem, we provide a bound on the number m of samples, which is sufficient forthe operator S to satisfy NeuRIPs( P)": "Theorem 1. There exist universal constants C1, C2 R such that the following holds: Forany sample operator S, constructed from random samples {xj}, respecting Assumption 2, letP (Rd R {1})n be any parameter set satisfying Assumption 1 and ||p|| > 1 for allp P. Then, for any u > 2 and s (0, 1), NeuRIPs( P) is satisfied with probability at least1 17 exp(u/4) provided that",
  "Uniform Generalization of Sublevel Sets of the Empirical Risk": "When the NeuRIPs event occurs, the function norm || ||, which is related to the expected risk, isclose to || ||m, which corresponds to the empirical risk. Motivated by this property, we aim to finda shallow ReLU network p with small expected risk by solving the empirical risk minimizationproblem:minp P p y2m. Since the set P of shallow ReLU networks is non-convex, this minimization cannot be solvedwith efficient convex optimizers. Therefore, instead of analyzing only the solution p of the opti-mization problem, we introduce a tolerance > 0 for the empirical risk and provide bounds on thegeneralization error, which hold uniformly on the sublevel setQy, :=p P : p y2m .Before considering generic regression problems, we will initially assume the label y to be a neuralnetwork itself, parameterized by a tuple p within the hypothesis set P. For all (x, y) in the support of, we have y = p(x) and the expected risks minimum on P is zero. Using the sufficient conditionfor NeuRIPs from Theorem 1, we can provide generalization bounds for p Qy, for any > 0.",
  "We assume that NeuRIPs( Rt) holds for s = (t )2/t2. In this case, for all q Qy,, we have thatq pm t and thus q / Qp,, which implies that q p t": "We also note that Rt satisfies Assumption 1 with a rescaled constant cw/t and normalization-invariantcb, if P satisfies it for cw and cb. Theorem 1 gives a lower bound on the sample complexity forNeuRIPs( Rt), completing the proof. At any network where an optimization method terminates, the concentration of the empirical riskat the expected risk can be achieved with less data than needed to achieve an analogous NeuRIPsevent. However, in the chosen stochastic setting, we cannot assume that the termination of anoptimization and the norm concentration at that network are independent events. We overcome thisby not specifying the outcome of an optimization method and instead stating uniform bounds onthe norm concentration. The only assumption on an algorithm is therefore the identification of anetwork that permits an upper bound on its empirical risk. The event NeuRIPs( Rt) then restricts theexpected risk to be below the corresponding level t.",
  "j=1(p(xj) yj)(q(xj) p(xj))": "It suffices to show, that within the stated confidence level we have q y > . This implies theclaim since q ym implies q y . We have E[E(q, p)] > 0. It now only remainsto strengthen the condition on > 3p y to achieve E(q, p) > 2. We apply Theorem 1to derive a bound on the fluctuation of the first term. The concentration rate of the second term isderived similar to Theorem 1 by using chaining techniques. Finally in Appendix E, Theorem 12 givesa general bound to achieveE(q, p) > 2",
  "uniformly for all q with q p > . Theorem 3 then follows as a simplification": "It is important to notice that, in Theorem 3, as the data size m approaches infinity, one can selectan asymptotically small deviation constant s. In this limit, the bound on the generalization errorconverges to 3p y + . This reflects a lower limit of the generalization bound, which is thesum of the theoretically achievable minimum of the expected risk and the additional tolerance .The latter is an upper bound on the empirical risk, which real-world optimization algorithms can beexpected to achieve.",
  "Size Control of Stochastic Processes on Shallow Networks": "In this section, we introduce the key techniques for deriving concentration statements for the em-pirical norm, uniformly valid for sets of shallow ReLU networks. We begin by rewriting the eventNeuRIPs( P) by treating as a stochastic process, indexed by the parameter set P. The eventNeuRIPs( P) holds if and only if we have",
  "The proof of this Lemma is based on the theory of stochastic processes and can be seen in Theorem 8of Appendix C": "To obtain bounds of the form (6) on the size of a process, we use the generic chaining method. Thismethod offers bounds in terms of the Talagrand-functional of the process in the Sub-Gaussian metric.We define it as follows. A sequence T = (Tk)kN0 in a set T is admissible if T0 = 1 and Tk 2(2k).The Talagrand-functional of the metric space is then defined as",
  "Conclusion": "In this study, we investigated the empirical risk surface of shallow ReLU networks in terms of uniformconcentration events for the empirical norm. We defined the Neural Restricted Isometry Property(NeuRIPs) and determined the sample complexity required to achieve NeuRIPs, which depends onrealistic parameter bounds and the network architecture. We applied our findings to derive upperbounds on the expected risk, which are valid uniformly across sublevel sets of the empirical risk.If a network optimization algorithm can identify a network with a small empirical risk, our resultsguarantee that this network will generalize well. By deriving uniform concentration statements, wehave resolved the problem of independence between the termination of an optimization algorithm ata certain network and the empirical risk concentration at that network. Future studies may focus onperforming uniform empirical norm concentration on the critical points of the empirical risk, whichcould lead to even tighter bounds for the sample complexity. We also plan to apply our methods to input distributions more general than the Gaussian distribution.If generic Gaussian distributions can be handled, one could then derive bounds for the Sub-Gaussiancovering number for deep ReLU networks by induction across layers. We also expect that ourresults on the covering numbers could be extended to more generic Lipschitz continuous activationfunctions other than ReLU. This proposition is based on the concentration of measure phenomenon,which provides bounds on the Sub-Gaussian norm of functions on normal concentrating input spaces.Because these bounds scale with the Lipschitz constant of the function, they can be used to find -netsfor neurons that have identical activation patterns.",
  "Broader Impact": "Supervised machine learning now affects both personal and public lives significantly. Generalization iscritical to the reliability and safety of empirically trained models. Our analysis aims to achieve a deeperunderstanding of the relationships between generalization, architectural design, and available data.We have discussed the concepts and demonstrated the effectiveness of using uniform concentrationevents for generalization guarantees of common supervised machine learning algorithms."
}