{
  "Abstract": "We present a novel diffusion-based approach for coherent 3D scene reconstructionfrom a single RGB image. Our method utilizes an image-conditioned 3D scenediffusion model to simultaneously denoise the 3D poses and geometries of allobjects within the scene. Motivated by the ill-posed nature of the task and toobtain consistent scene reconstruction results, we learn a generative scene prior byconditioning on all scene objects simultaneously to capture the scene context andby allowing the model to learn inter-object relationships throughout the diffusionprocess. We further propose an efficient surface alignment loss to facilitate trainingeven in the absence of full ground-truth annotation, which is common in publiclyavailable datasets. This loss leverages an expressive shape representation, whichenables direct point sampling from intermediate shape predictions. By framingthe task of single RGB image 3D scene reconstruction as a conditional diffusionprocess, our approach surpasses current state-of-the-art methods, achieving a12.04% improvement in AP3D on SUN RGB-D and a 13.43% increase in F-Scoreon Pix3D.",
  "Introduction": ": Given a single RGB image of an indoor scene, our model reconstructs the 3D sceneby jointly estimating object arrangements and shapes in a globally consistent manner. Our noveldiffusion-based 3D scene reconstruction approach achieves highly accurate predictions by utilizinga novel generative scene prior that captures scene context and inter-object relationships, and byemploying an efficient surface alignment loss formulation for joint pose- and shape-synthesis. Holistic 3D scene understanding is crucial for various fields and lays the foundation for manydownstream tasks in robotics, 3D content creation, and mixed reality. It bridges the gap between2D perception and 3D understanding. Despite impressive advancements in 2D perception and 3Dreconstruction of individual objects , 3D scene reconstruction from a single RGBobservation remains a challenging problem due to its ill-posed nature, heavy occlusions, and the",
  "arXiv:2412.10294v1 [cs.CV] 13 Dec 2024": "complex multi-object arrangements found in real-world environments. While previous works have shown promising results, they often recover 3D shapes independently and thus do notleverage the scene context nor inter-object relationships. This leads to unrealistic and intersectingobject arrangements. Additionally, common feed-forward reconstruction methods struggle with heavy occlusions and weak shape priors, resulting in noisy or incomplete 3D shapes,which hinders immersion and hence limits the applicability in downstream tasks. To address thesechallenges and to advance 3D scene understanding, we propose a novel generative approach forcoherent 3D scene reconstruction from a single RGB image. Specifically, we introduce a newdiffusion model that learns a generative scene prior capturing the relationships between objects interms of arrangement and shapes. When conditioned on a single image, this model simultaneouslyreconstructs poses and 3D geometries of all scene objects. By framing the reconstruction task asa conditional synthesis process, we achieve significantly more accurate object poses and sharpergeometries. Publicly available 3D datasets typically only provide partial ground-truthannotations, which complicates joint training of shape and pose. To overcome this, we proposea novel and efficient surface alignment loss formulation Lalign that enables joint training of shapeand pose even under the lack of full ground-truth supervision. Unlike previous methods that involve costly shape decoding and point sampling on the reconstructed surface, our approachemploys an expressive intermediate shape representation that enables direct point sampling from theconditional shape prior. This provides additional supervision and results in more globally consistent3D scene reconstructions. Our method not only outperforms current state-of-the-art methods by12.04% in AP153D on SUN RGB-D and by 13.43% in F-Score on Pix3D but also generalizesto other indoor datasets without further fine-tuning.In summary, our contributions include:",
  "Single-View 3D Reconstruction": "Object Reconstruction.Since the foundational work by Roberts , numerous methods havebeen developed to learn cues for deriving 3D object structures, thereby bridging the gap between2D perception and the 3D world. These methods typically involve an image encoder network thatprocesses the input image of a single object, capturing its features. The extracted features are eithercorrelated with an encoded shape database to retrieve a suitable shape , or used by a 3Ddecoder to reconstruct the object in a specific 3D representation, such as voxel grids , pointclouds , meshes , or neural fields . uses a message-passing graph networkbetween geometric primitves to reason about the structure of the shape. Scene Reconstruction.Early works formulated single-view scene reconstruction as 3D scenecompletion from given or estimated depth information in a volumetric grid. While thesemethods have produced promising results, their representational power to model fine details is limitedby the spatial resolution of the 3D grid. Multi-object reconstruction and scene parsing methodsrepresented objects using primitives , voxel grids , or CAD models ,while also considering the relation between the objects . The approach presented by Nie etallet@tokeneonedot is particularly relevant, proposing a holistic method for joint pose andshape estimation from a single image. Zhang et allet@tokeneonedot extended this idea by incorporating an implicit shape representation and an additional pose refinement using a graph neuralnetwork. Although these methods provided significant advances in holistic scene understanding, theystruggled with accurate pose estimation and produced noisy scene objects, leading to intersectingor incomplete objects. In contrast to these previous works, we are proposing a generative methodto obtain a strong scene prior and formulate the reconstruction task as a conditional synthesis task.This allows for more robust reconstruction that is less prone to object insections or implausible objectgeometries.",
  "D Diffusion Models": "In recent years, denoising diffusion probabilistic models (DDPMs) have emerged as a versatile classof generative models, demonstrating impressive results in image and video generation. Unlike otherclasses of generative models such as auto-regressive models , Generative AdversarialNetworks (GANs) and Variational Autoencoders (VAEs), diffusion models iteratively reversea Markovian noising process. This method ensures stable training and has the ability to capturediverse modes while producing detailed outputs. Several approaches have utilized diffusion models tolearn the distribution of individual 3D shapes using various 3D representations, including volumetricgrids , point clouds , meshes , implicit functions , neural fields or hybrid representations . propose a hierarchical voxel diffusion model, which iscapable of modelling large-scale and fine-detailed geometry. While these methods can synthesizehigh-quality 3D shapes, they typically focus on single objects in canonical space. In contrast, we areproposing a diffusion-based approach that addresses the more challenging problem of multi-objectscene reconstruction, encompassing accurate pose estimations and an understanding of inter-objectrelationships. Conditional Diffusion for 3D Reconstruction.Recent works also use diffusion models forsingle-view object reconstruction . For instance, learns the shape distribution of a singlecategory by denoising a set of 2D images for each object, while projects image features ontonoisy point clouds during the diffusion process to ensure geometric plausibility. Recently, severalworks proposed to leverage multi-view consistency within pre-trained text-conditional 2D imagediffusion models to reconstruct individual 3D objects . Similar to our work, Tang etallet@tokeneonedot use a diffusion model to learn scene priors from synthetic data, showingunconditional scene synthesis of a single room type and text-conditional generation. However, theirapproach does not support image-based scene reconstruction. Furthermore, it depends on cleansynthetic data, which provides full 3D ground truth supervision and CAD model retrieval, therebylimiting shape diversity. While these existing methods have shown promising results on singleobjects or synthetic scenes, our approach targets real-world scenes. By framing the reconstructiontask as a conditional generation process, our scene prior accurately delivers poses and shapes ofmultiple objects, even in the presence of strong occlusions, significant clutter, and challenging lightingconditions.",
  "Overview": "Our method takes a single RGB image of an indoor scene as input and generates a globally consistent3D scene reconstruction that matches the input image. To this end, we are framing the reconstruc-tion task as a conditional generation problem using a diffusion model conditioned on the inputview (Sec. 3.2), which simultaneously predicts the poses (Sec. 3.3) and shapes (Sec. 3.4) of all objectsin the scene. Given the ill-posed nature of single-view reconstruction, such a probabilistic formulationis particularly well-suited for this task. To ensure accurate reconstructions and to learn a strong sceneprior, we model inter-object relationships within the scene using an intra-scene attention module(Sec. 3.5). Additionally, recognizing the incomplete ground truth in many 3D indoor scene datasets,we introduce a loss formulation for joint shape and pose training, which enables training under onlypartially available supervision (Sec. 3.6). An overview of our approach is illustrated in . In thefollowing sections, we describe each individual contribution in more detail. : Scene Prior and Surface Alignment Loss Overview. (Left) We propose a novel way tomodel scene priors (Sec. 3.5) by modeling the scene context and the relationships between all objectsduring the denoising process. (Right) For additional supervision and joint training, we use a surfacealignment loss (Sec. 3.6) between a given ground truth depth map and point samples directly drawnfrom the intermediate shape representation i and transformed to camera space with the predictedobject pose i.",
  "Conditional 3D Scene Diffusion": "We frame the scene reconstruction task as a conditional generation process via a diffusion formula-tion . Given an instance-segmented RGB image I containing a variable number of 2D objects bifor i {1, . . . , n}, our model simultanteously estimates all 3D objects oi = (i, i) with 7-DoFposes i and 3D geometries i:(o1, . . . , on) = (I|(b1, . . . , bn)).(1) During the forward process, we gradually add Gaussian noise to a data point x0 to xT over a seriesof discrete time steps T. For a given data point x0, e.glet@tokeneonedot, shapes i and poses i,the noisy version xt at time step t is given by a Markovian process q(xt|xt1) and its jointdistribution q(x1:T |x0) can be expressed as:",
  "with y being the conditional information from the input image I": "Conditioning.To effectively guide the diffusion process p(x0:T |y), it is crucial to accuratelymodel the conditional information y. First, we encode the input image I using a 2D backbone I andapply 2D instance segmentation to get n detected 2D objects bi, comprising of its 2D bounding box,image feature patch, and semantic class (cls). Each element is encoded using a specific embeddingfunction . The per-instance yi and scene condition y is then formed as:yi = concat(box(boxi), feat(feati), cls(clsi)),(6)y = (y1, . . . , yn).(7) To learn a scene prior over all objects in the scene, we condition the denoising network on thescene condition y. This not only enables learning the individual object representations oi but alsofacilitates learning to capture the scene context and inter-object relationships (Sec. 3.5). Furthermore,we adopt classifier-free guidance for our model by dropping the condition y with probabilityp = 0.8, i.elet@tokeneonedot, using a special 0-condition . This allows our model to functionas a conditional model p(x0|y) and unconditional model p(x0) at the same time, thus enablingunconditional synthesis (Appendix B). Loss Formulation.Unlike related works like that regress object poses i and shapeparameters i using a multitude of highly-tuned losses, we train our model to minimize simplediffusion and alignment losses:",
  "Object Pose Parameterization": "We adopt the object pose parameterization of , defining the pose i = (ci, si, i) of an object byits 3D center ci R3, the spatial size si R3, and orientation i [, ) in . The 3D center ci isfurther represented by the 2D offset i R2 between the 2D bounding box center coordinate and theprojected coordinate of the 3D center on the image plane, along with the distance di R from theobject center to the projected center. Our model learns to denoise this 7-dim. pose representation.",
  "Shape Encoding": "We represent object shapes using the disentangled shape representation from . A shape isrepresented as a shape code i R256 which is factorized into a set of g oriented, anisotropic 3DGaussians Gj, j {1, ..., g} and an associated 512-dim. latent feature vector per Gaussian. EachGaussian consist of 16 main parameters: j R3 (center), factorized covariance matrix Uj R33 (rotation), j R3 (scale) and j R1 (mixing weight). We use g = 16 Gaussians to form ascaffolding of the shapes geometry. Together with their latent features, these Gaussians are decodedinto high-fidelity occupancy fields, and the final mesh is extracted by applying marching cubes . While similar to , our model learns to denoise this shape parameterization i, our additionalsurface alignment loss Lalign (Sec. 3.6) provides relational signal between predicted shapes and poses.This enables additional guidance in the face of missing joint pose and shape annotations as in SUNRGB-D dataset .",
  "Scene Prior Modeling": "Given the ill-posed nature of single-view reconstruction, a robust scene prior is essential for achievinggood performance. Effectively capturing the scene context and modeling the relationships betweenobjects within the scene is crucial for learning this strong scene prior . Previous methods eitherreconstruct each object individually or refine their features using graph networks . In contrast,our approach considers the entire scene by conditioning on all scene objects simultaneously p(x0|y)and y = (y1, . . . , yN) and additionally allows objects to exchange relational information throughoutthe entire process. We model the inter-object relationships using an attention formulation , whichhas proven to be powerful for aggregating contextual information.We denote this formulation as Intra-Scene Attention (ISA), which allows all objects within the sceneto attend to each other, effectively modeling their relationships. Please refer to Appendix E for moredetails and to Tab. 2 for the corresponding ablation study, which demonstrates the effectiveness ofour learned scene prior.",
  "Surface Alignment Loss": "Publically available 3D scene datasets often only provide partial ground-truth annotations .To facilitate joint training of our model on pose and shape estimation, even in the absence of completeground-truth annotations, we propose to leverage our expressive intermediate shape representation to provide additional supervision and to align shapes efficiently with the available partial depthinformation D. An illustration of the surface alignment loss formulation is provided in .During training, for each object oi, we use the expected shape code i estimation by our model toobtain the predicted Gaussian Gi,j distribution. Given this scaffolding representation, we directlysample m = 1000 points p(j,l) N(j, j) per Gaussian Gi,j resulting in a shape point cloudPi = {p(j,l)|j {1, . . . , g}, l {1, . . . , m}}. We transform the resulting shape points Pi into thecamera frame by the predicted object pose i. Using the instance segmentations and ground-truthdepth maps, we obtain Ki surface points qik for object oi and define the surface alignment loss for allvisible objects as 1-sided Chamfer Distance",
  "k=1minpPiqik p22.(11)": "Unlike previous works such as that perform costly sampling of points on the decoded shapesurface, our approach enables direct point sampling from the conditional shape prior Gi,j. This lossformulation facilitates joint training of pose and shape for all objects simultaneously and its efficancyis demonstrated through ablation studies in Tab. 2.",
  "Architecture": "Our architecture consists of a pre-trained image backbone, a novel image-conditional scene priordiffusion model, and a conditional shape decoder diffusion module. We utilize an off-the-shelf2D instance segmentation model, Mask2Former , which is pre-trained on COCO using aSwin Transformer backbone, to obtain instance segmentation and image features. Please referto Appendix E for details about the condition embedding functions.To denoise object poses i, we use a 1-dim. UNet architecture with 8 encoding and decodingblocks with skip connections. Each block consists of a time-conditional ResNet layer, multi-headattention between the per-object condition yi and the pose representation, and our intra-scene attentionmodule (Sec. 3.5) to enable relational information exchange and effectively train a scene prior. Weuse 8 attention heads, with 64 features per head.To estimate object shapes i from the input view I, we denoise the unordered set of Gaussian Gi,jusing a Transformer model with 2 encoder layers, 6 decoder layers, and multi-head attentionwith 4 heads to the object condition information, similar to . The per-Gaussian latent features aredenoise with a shape decoder diffusion model, realized as another Transformer model with 6 encoderand decoder layers, which is conditioned on the shape Gaussians.",
  "Training and Implementation Details": "For all diffusion training processes, we uniformly sample time steps t = 1, ...T, T = 1000, and use alinear variance schedule with 1 = 0.0001 and T = 0.02. We implement our model in PyTorchand use the AdamW optimizer with a learning rate of 1 104 and 1 = 0.9, 2 = 0.999. Wetrain our models on a single RTX3090 with 24GB VRAM for 1000 epochs on Pix3D, for 500 epochson SUN RGB-D and for 50 epochs of additional joint training using Lalign.During inference, we employ DDIM with 100 steps to accelerate sampling speed. For classifier-free guidance , we drop the condition y with probability p = 0.8.",
  "Baseline Methods": "We compare our method against current state-of-the-art methods for holistic scene understanding:Total3D , Im3D , and InstPIFu . Total3D directly regresses 3D object poses fromimage features and uses a mesh deformation and edge-removal approach to reconstruct a shape.Im3D utilizes an implicit shape representation and a graph neural network to refine the pose predictions. InstPIFu focuses on single-object reconstruction and proposes to query instance-aligned features from the input image in their implicit shape decoder to handle occlusion. Forscene reconstruction, they rely on the predicted 3D poses of Im3D. We use the official code andcheckpoints provided by the authors of these baseline methods and evaluate with ground truth 2Dinstance segmentation and camera parameters to ensure a fair comparison. We further compareagainst a retrieval-based method, ROCA in Appendix D.",
  "Datasets": "Following , we train and evaluate the performance of our 3D pose estimation on theSUN RGB-D dataset with the official splits. This dataset consists of 10,335 images of indoorscenes (offices, hotel rooms, lobbies, furniture stores, etc.) captured with four different RGB-Dcameras. Each image is annotated with 2D and 3D bounding boxes of objects in the scene. Duringjoint training, we use the provided depth maps together with instance masks to compute Lalign.We train and evaluate the performance of our 3D shape reconstruction on the Pix3D dataset,which contains images of common furniture objects with pixel-aligned 3D shapes from 9 objectclasses, comprising 10,046 images. We use the train and test splits defined in , ensuring that 3Dmodels between the respective splits do not overlap.",
  "Evaluation Protocol": "For quantitative comparison against baseline methods, we follow the evaluation protocol of . Forpose estimation, we report the intersection over union of the 3D bounding box (IoU3D) and averageprecision with an IoU3D threshold of 15% (AP153D) on the SUN RGB-D dataset . In line withprevious works , we evaluate with oracle 2D detections but also provide camera parametersto all methods during evaluation. To further assess the alignment of the 3D shapes in the scene, wecalculate Lalign between reconstructed shapes and the instance-segmented ground-truth depth map.For single-view 3D shape reconstruction, we follow evaluate on the Pix3D dataset. We fol-low and sample 10,000 points on the predicted shape surface, extracted with Marching Cubes at a resolution of 1283, and on the ground truth shapes and evaluate Chamfer distance (CD 103)and F-score after mesh alignment.",
  "Comparison to State of the Art": "3D Scene Reconstruction.In , we present qualitative comparisons of our approach againststate-of-the-art methods for single-view 3D scene reconstruction. The results from Total3D oftenexhibit intersecting objects and lack global structure. Additionally, their deformation and edge-removal approach results in 3D shapes with visible artifacts and limited details. While the implicitshape representation of Im3D is more flexible, it often produces incomplete and floating surfaces. Incontrast, our diffusion-based reconstruction method, as shown in Tab. 1, learns strong scene priors,resulting in a +0.2 improvement in Lalign and more coherent 3D arrangements of the objects in thescene (+12.04% AP153D), as well as high-quality and clean shapes (+13.43% F-Score).Furthermore, we demonstrate the generalizability of our model to other indoor datasets. We evaluateour approach on individual frames from the ScanNet dataset using 2D instance predictions fromMask2Former without additional fine-tuning. As shown in , our method accurately reconstructsthe given input view with matching poses and high-quality 3D geometries.In Appendix D, we additionally train on ScanNet and compare against ROCA . Due to its retrievalapproach, the shapes are complete. However, the resulting quality can limited by the diversity of theshape database, which can lead to suboptimal results, see . 3D Pose Estimation & Scene Arrangement.As shown in Tabs. 1 and 6, our method outperformsall baseline methods by a significant margin in terms of IoU3D and AP153D, i.elet@tokeneonedot,improving mAP153D by 12.04% over Im3D . Detailed per-class results are provided in Tabs. 6and 8. Figs. 3 and 7 demonstrate that our approach effectively learns common object arrangements,such as multiple chairs surrounding a table, while ensuring that furniture pieces do not intersect orfloat in the air. We attribute these improvements to our models robust scene understanding, which isderived from learning a strong scene prior that accounts for inter-object relationships. : Quantitative evaluation of 3D scene reconstruction on SUN RGB-D (left) and 3Dshape reconstruction on Pix3D (right). Our 3D scene diffusion approach outperforms allbaseline methods on both tasks on common 3D scene reconstruction metrics.",
  "We conduct a series of detailed ablation studies to verify the effectiveness of our design decisions andcontributions. The quantitative results are provided in Tab. 2": "What is the effect of the denoising formulation?To assess the benefits of the denoisingdiffusion formulation, we construct a 1-step feed-forward regression model that uses the sameconditional information as input features and model architecture but regresses the object outputsdirectly in a single timestep. As shown in Tab. 2, modeling 3D scene reconstruction as a conditionaldiffusion process, rather than using a feed-forward regression formulation, results in significantimprovements of +11.08% AP153D and +0.19 Lalign. What is the effect of our scene prior modeling?We evaluate the impact of learning a sceneprior by modeling the distribution of all objects and their relationships compared to learning themarginal per-object distribution, i.elet@tokeneonedot, predicting each object individually. As shownin Tab. 2, our joint-object scene prior yields a significant improvement of +9.30% AP153D over per-object prediction. This improvement underscores the importance of learning a robust scene prior thateffectively captures inter-object relationships. What is the effect of joint training?We investigate the benefit of joint training for pose andshape using Lalign compared to individual training of pose estimation and shape reconstruction.Although our model already learns strong scene and shape priors, Tab. 2 shows that joint trainingprovides additional benefits, resulting in an improvement of +2.11% in AP153D and +0.07 in Lalign. : Qualitative comparison of 3D scene reconstruction on SUN RGB-D . While thebaselines often produce noisy or incomplete shape reconstruction of intersecting or misplaced objects,our method produces plausible object arrangements as well as high-quality shape reconstructions. : Inference results on ScanNet . We use our model trained on SUN RGB-D and perform inference on individual frames of ScanNet without fine-tuning. We observe stronggeneralization capabilities with respect to different camera parameters and scene arrangements.",
  "Limitations": "While our conditional scene diffusion approach for single-view 3D scene reconstruction demonstratessignificant improvements, there are some limitations. First, our method relies on accurate 2D objectdetection, making it dependent on the performance of 2D perception models. Upcoming state-of-the-art 2D detection models can be seamlessly integrated to enhance the performance of our approach.Second, our shape prior, trained on a diverse set of semantic classes using 3D shape supervision, doesnot generalize to unseen object categories. This can be mitigated by combining our model for knowncategories with single-object diffusion models that leverage pre-trained text-image generation modelsfor 3D shape synthesis of uncommon shape categories. While accurate 3D scene reconstruction",
  ": Unconditional results. Injecting as a condition to our conditional diffusion model, i.e.,effectively disabling the conditioning mechanism, results in high-quality and diverse results": "forms the foundation for subsequent downstream tasks like mixed reality applications, our currentmodel assumes a static scene geometry. Future work could integrate object affordance and articulationinto our shape prior to enable more immersive human-scene interactions. Broader ImpactWe do not anticipate any societal consequences or negative ethical implicationsarising from our work. Our approach advances the holistic understanding of 2D perception and 3Dmodeling, benefiting various research areas.",
  "Conclusion": "In this paper, we present a novel diffusion-based approach for coherent 3D scene reconstructionsfrom a single RGB image. Our method combines a simple yet powerful denoising formulationwith a robust generative scene prior that learns inter-object relationships by exchanging relationalinformation among all scene objects. To address the issue of missing ground-truth annotations inpublicly available 3D datasets, we introduce a surface alignment loss Lalign to jointly train shapeand pose, effectively leveraging our shape representation. Our approach significantly enhances 3Dscene understanding, outperforming current state-of-the-art methods across various benchmarks, with+12.04% AP153D on SUN RGB-D and +13.43% F-Score on Pix3D. Extensive experiments demonstratethat our contributions 3D scene reconstruction as a conditional diffusion process, scene priormodeling, and joint shape-pose training enabled by Lalign collectively contribute to the overallperformance gain. Additionally, we show that our model supports unconditional synthesis andgeneralizes well to other indoor datasets without further fine-tuning. We believe these advancementslay a solid foundation for future progress in holistic 3D scene understanding and open up excitingapplications in mixed reality, content creation, and robotics.",
  "G. Chou, Y. Bahat, and F. Heide. Diffusion-sdf: Conditional generative modeling of signed distancefunctions. 2023": "C. B. Choy, D. Xu, J. Gwak, K. Chen, and S. Savarese. 3d-r2n2: A unified approach for single andmulti-view 3d object reconstruction. In Computer VisionEuropean Conference on Computer Vision 2016:14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part VIII 14,pages 628644. Springer, 2016. T. Chu, P. Zhang, Q. Liu, and J. Wang. Buol: A bottom-up framework with occupancy-aware lifting forpanoptic 3d scene reconstruction from a single image. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, pages 49374946, 2023.",
  "T. Karras, M. Aittala, T. Aila, and S. Laine. Elucidating the design space of diffusion-based generativemodels. Advances in Neural Information Processing Systems, 35:2656526577, 2022": "S. W. Kim, B. Brown, K. Yin, K. Kreis, K. Schwarz, D. Li, R. Rombach, A. Torralba, and S. Fidler.Neuralfield-ldm: Scene generation with hierarchical latent diffusion models. In IEEE Conference onComputer Vision and Pattern Recognition (CVPR), 2023. J. Koo, S. Yoo, M. H. Nguyen, and M. Sung. Salad: Part-level latent diffusion for 3d shape generationand manipulation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages1444114451, 2023. N. Kulkarni, I. Misra, S. Tulsiani, and A. Gupta. 3d-relnet: Joint object and relational network for3d prediction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages22122221, 2019. W. Kuo, A. Angelova, T.-y. Lin, and A. Dai. Mask2cad: 3d shape prediction by learning to segmentand retrieve. In Proceedings of the European Conference on Computer Vision (European Conference onComputer Vision), 2020. W. Kuo, A. Angelova, T.-Y. Lin, and A. Dai. Patch2cad: Patchwise embedding learning for in-the-wildshape retrieval from a single image. In Proceedings of the IEEE/CVF International Conference onComputer Vision, pages 1258912599, 2021. J. Lei, C. Deng, W. B. Shen, L. J. Guibas, and K. Daniilidis. Nap: Neural 3d articulated object prior.In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in NeuralInformation Processing Systems, volume 36, pages 3187831894. Curran Associates, Inc., 2023. L. Li, S. Khan, and N. Barnes. Silhouette-assisted 3d object instance reconstruction from a clutteredscene. In 2019 IEEE/CVF International Conference on Computer Vision Workshop (Proceedings of theIEEE/CVF International Conference on Computer VisionW), pages 20802088, 2019. doi: 10.1109/ProceedingsoftheIEEE/CVFInternationalConferenceonComputerVisionW.2019.00263. T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollr, and C. L. Zitnick. Microsoftcoco: Common objects in context. In Computer VisionEuropean Conference on Computer Vision 2014:13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages740755. Springer, 2014.",
  "H. Liu, Y. Zheng, G. Chen, S. Cui, and X. Han. Towards high-fidelity single-view holistic reconstructionof indoor scenes. In European Conference on Computer Vision, 2022": "R. Liu, R. Wu, B. V. Hoorick, P. Tokmakov, S. Zakharov, and C. Vondrick. Zero-1-to-3: Zero-shot oneimage to 3d object. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023. Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo. Swin transformer: Hierarchical visiontransformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computervision, pages 1001210022, 2021.",
  "S. Luo and W. Hu. Diffusion probabilistic models for 3d point cloud generation. In Proceedings of theIEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 28372845, 2021": "P. Mandikal, N. KL, and R. Venkatesh Babu. 3d-psrnet: Part segmented 3d point cloud reconstruction froma single image. In Proceedings of the European Conference on Computer Vision (European Conference onComputer Vision) Workshops, pages 00, 2018. L. Melas-Kyriazi, C. Rupprecht, and A. Vedaldi. Pc2: Projection-conditioned point cloud diffusion forsingle-image 3d reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 1292312932, 2023. N. Mller, Y. Siddiqui, L. Porzi, S. R. Bulo, P. Kontschieder, and M. Niener. Diffrf: Rendering-guided 3dradiance field diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 43284338, 2023.",
  "Y. Nie, X. Han, S. Guo, Y. Zheng, J. Chang, and J. J. Zhang. Total3dunderstanding: Joint layout, objectpose and mesh reconstruction for indoor scenes from a single image. In CVPR, 2020": "J. Pan, X. Han, W. Chen, J. Tang, and K. Jia. Deep mesh reconstruction from single rgb images viatopology modification networks. In Proceedings of the IEEE/CVF International Conference on ComputerVision, 2019. A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein,L. Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. In Conference onNeural Information Processing Systems, 2019.",
  "B. Poole, A. Jain, J. T. Barron, and B. Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. In ICLR,2023": "S. Popov, P. Bauszat, and V. Ferrari. Corenet: Coherent 3d scene reconstruction from a single rgbimage. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020,Proceedings, Part II 16, pages 366383. Springer, 2020. X. Ren, J. Huang, X. Zeng, K. Museth, S. Fidler, and F. Williams. Xcube: Large-scale 3d generativemodeling using sparse voxel hierarchies. In Proceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, 2024.",
  "L. Roberts. Machine perception of threedimensional solids. PhD thesis, Massachusetts Institute ofTechnology, 1963": "O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolutional networks for biomedical image segmentation.In Medical Image Computing and Computer-Assisted InterventionMICCAI 2015: 18th InternationalConference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18, pages 234241. Springer,2015. O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla,M. Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computervision, 115:211252, 2015.",
  "E. Sella, G. Fiebelman, P. Hedman, and H. Averbuch-Elor. Vox-e: Text-guided voxel editing of 3d objects.In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 430440, 2023": "J. R. Shue, E. R. Chan, R. Po, Z. Ankner, J. Wu, and G. Wetzstein. 3d neural field generation using triplanediffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,pages 2087520886, 2023. Y. Siddiqui, A. Alliegro, A. Artemov, T. Tommasi, D. Sirigatti, V. Rosov, A. Dai, and M. Niener. Meshgpt:Generating triangle meshes with decoder-only transformers. In Proc. Computer Vision and PatternRecognition (CVPR), IEEE, 2024. J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli. Deep unsupervised learning usingnonequilibrium thermodynamics. In International conference on machine learning, pages 22562265.PMLR, 2015.",
  "S. Szymanowicz, C. Rupprecht, and A. Vedaldi. Viewset diffusion: (0-)image-conditioned 3d generativemodels from 2d data. International Conference on Computer Vision, 2023": "J. Tang, X. Han, J. Pan, K. Jia, and X. Tong. A skeleton-bridged deep learning approach for generatingmeshes of complex topologies from single rgb images. In Proceedings of the ieee/cvf conference oncomputer vision and pattern recognition, pages 45414550, 2019. J. Tang, Y. Nie, L. Markhasin, A. Dai, J. Thies, and M. Niener. Diffuscene: Scene graph denoisingdiffusion probabilistic model for generative indoor scene synthesis. arXiv preprint arXiv:2303.14207,2023. S. Tulsiani, S. Gupta, D. F. Fouhey, A. A. Efros, and J. Malik. Factoring shape, pose, and layout fromthe 2d image of a 3d scene. In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition, pages 302310, 2018.",
  "N. Wang, Y. Zhang, Z. Li, Y. Fu, W. Liu, and Y.-G. Jiang. Pixel2mesh: Generating 3d mesh models fromsingle rgb images. In European Conference on Computer Vision, 2018": "J. Wu, C. Zhang, T. Xue, W. T. Freeman, and J. B. Tenenbaum. Learning a probabilistic latent spaceof object shapes via 3d generative-adversarial modeling. In Advances in Neural Information ProcessingSystems, pages 8290, 2016. H. Xie, H. Yao, X. Sun, S. Zhou, and S. Zhang. Pix2vox: Context-aware 3d reconstruction from single andmulti-view images. In Proceedings of the IEEE/CVF international conference on computer vision, pages26902698, 2019.",
  "AAppendix": "In the following, we show more qualitative results for scene reconstruction on SUN RGB-D (Appendix B) and object reconstruction on Pix3D . We provide detailed quantitative per-classcomparisons supplementing the tables in the main paper (Appendix C). We additionally compareagainst a retrieval baseline on the ScanNet dataset in Appendix D. Finally, we provide additionaldetails on the architecture of our diffusion model in (Appendix E).For a comprehensive overview of our approach and results, we encourage the reader to watch thesupplemental video.",
  "BAdditional Qualitative Results": "Scene ReconstructionIn , we show additional qualitative results of our method on testframes from SUN RGB-D. Despite strong occlusions and challenging viewing angles, our modelpredicts accurate scene reconstructions. Our generative scene prior learns common scene patterns,such as parallel object placements between the table and sofa or a bed and neighboring nightstands.In , we also demonstrate that our robust conditional scene prior can recover clean and matchingshape reconstruction even for heavily occluded objects, e.glet@tokeneonedot, a chair for which onlythe back seat is barely visible.",
  ": Additional qualitative scene reconstruction results on SUN RGB . Our diffusion-based scene layout and shape prediction approach achieves accurate results even for strongly occludedobjects": "Object Reconstruction & Unconditional SynthesisIn , we show a qualitative compari-son of single-view 3D object reconstruction on the Pix3D dataset. Unlike InstPIFu, which oftenproduces noisy and incomplete surfaces, our image-condition diffusion model reconstructs cleanand high-fidelity objects. Such a visual quality allows these reconstructions to be integrated intoe.glet@tokeneonedot, mixed reality applications. To probe the learned shape prior and investigate its shape synthesis capabilities, we input the 0-condition instead of extracted image features to our model. As shown in , our model learns ahigh-quality shape prior with fine details across various semantic classes. : Qualitative comparison of 3D pose estimation on the SUN RGB-D . The input imageis displayed on the left, and the predicted and ground-truth 3D arrangements are visualized as top-down orthographic views of the scene. We observe that Total3D frequently lacks a globally consistentstructure, while Im3D predicts globally structured results but occasionally produces intersecting orfloating objects. In contrast, our approach successfully recovers a coherent arrangement of objectswithin the scene by learning a robust scene prior.",
  "CAdditional Quantitative Results": "Scene ReconstructionIn Tab. 4, we show detailed comparisons of our approach against baselinemethods, Total3D and Im3D , on the 10 most common classes of SUN RGB-D. Our approachconsistently outperforms all baseline methods on all classes except the bed class. We attribute thisexception to the fact that beds are often only partially visible in the input view due to their spatialextent, which introduces higher variability. In contrast, Im3D employs a series of geometric lossesand regularization terms, which seems to help in extreme amodal cases at the cost of additional lossbalancing. Nevertheless, our method achieves a significant overall improvement of 12.04% in AP153Don these 10 classes, with particularly notable gains for dressers (+26.03%), chairs (+21.91%)and cabinets (+19.37%), showcasing the effect of our robust scene prior. Tabs. 6 and 8 show the per-class comparisons and ablation studies on all 37 NYU classes in termsof IoU3D and mAP153D. Our approach improves compared to Im3D by a +7.57% increase in mAP153Dand +4.56% increase in class-mean IoU3D across all 37 classes. The ablation results highlight theimportance of our diffusion formulation (+7.67% mAP153D), scene prior modeling (+7.11% mAP153D),and joint training using the surface alignment loss Lalign (+0.72 mAP153D). Object ReconstructionFor single-view object reconstruction, we evaluate Chamfer Distanceand F-Score on Pix3D and show per-class comparisons in Tabs. 7 and 9. Our image-conditionalshape prior leads to significant improvements, +9.6% in Chamfer Distance and +13.43 in F-Score,while outperforming InstPIFu in most categories, except sofas and wardrobes in F-Score.",
  "Room Layout also predict the room bounding box with a separate network head. Westudy, how our model can also predict the room layout. For that we include the room bounding": "box pose as part of the object poses during the diffusion process. We follow the room layoutparameterization of and model the 3D room center directly instead of decomposing it as 2Doffset & distance, which is done for the objects. In Tab. 3, we demonstrate that by denoising the poseof room layout, we outperform the regression-based methods.",
  "DComparison to shape retrieval baseline on ScanNet": "We compare with a shape retrieval baseline, namely ROCA . Since ROCA requires full ground-truth supervision during training, we adopt their setup and train our model on the same 25,000 framesfrom the ScanNet dataset with pose annotations derived from Scan2CAD , as well as thesame CAD pool from ShapeNet . We additionally adopt their full 9-DoF pose parameterization bypredicting all 3 rotation angles. Following ROCA, we quantitatively evaluate the Alignment Accuracyin Tab. 5. Please refer to for the details of the evaluation. In , we can see that ROCAretrieves clean and complete shapes by definition. However, due to its limited shape database, itcannot capture all shape modes accurately, leading to shape mismatches. Our reconstruction-basedapproach instead can recover faithful shape results while simultaneously predicting a coherent objectarrangement.",
  "EArchitecture Details": "Object Pose Parameterization: NormalizationTo ensure a reasonable signal-noise ratio among the object pose parameters, we normalize the parameters to by dividing them by itsmax value and shift the range using a parameter-specific value. For this, we calculate the min-maxranges of all pose parameters, i.elet@tokeneonedot, rotation , 3D scale s, and projected distance d, : 3D pose estimation results for all NYU-37 classes on SUN RGB-D . We reportthe Average Precision (AP) at 15% 3D-IoU threshold of the baseline and different variants of ourapproach: Our approach outperforms Total3D and Im3D on most semantic categories, especially onfrequent classes likes chairs (+21.9%) or tables (+12.7%).",
  "During training, the loss is computed on the un-normalized parameter ranges. After inference and forevaluation, we un-normalize each parameter according to its original range": "Surface Alignment Loss: Point Sample TransformationDuring training, for each object oi,we use the predicted shape i to estimate its scaffolding Gaussians Gj. From each 3D Gaussiandistribution, we directly draw 3D point samples p(j,l) N(j, j). This shape point cloud Piapproximates the shape. With the predicted and un-normalized object pose i, we define a 3D rigidtransformation R44 and transform the shape point cloud Pi to the camera coordinate system. Weuse this transformed shape pointcloud P camiand the instance-segmented ground-truth depth map from",
  "SUN RGB-D as the partial target pointcloud to measure the 1-sided Chamfer distance and to computethe surface alignment loss Lalign": "Scene Prior Modeling: Inter-Object Relationships via Intra-Scene AttentionWe use themulti-head attention mechanism between the scene objects to allow them to attend to eachother, effectively learning their inter-object relationships and the scene context. Specifically, givenan unordered set S = [o1, o2, ..., on], oi Rn per-object n-dimensional feature vectors, projectionlayers (W Q, W K and W V ) and features Q = S W Q, K = S W K and V = S W V afterprojection. we define the intra-scene attention as:",
  "dd)V(15)": "Condition: Embedding FunctionsAfter cropping the 2D image feature patch RW HC fromthe frozen image backone I, we apply adaptive average pooling to resize the per-object featurepatches to a common 2D size leading to resized per-object feature crop of 8 8 and C = 256. Thisfeature crop is further embedded using a small 2D CNN feat with 3 blocks of convolutional layerswith 512 features, group norm, and leaky ReLU activation. The embedded feature crop is reshaped toa 4096-dim vector. box is implemented as sinusoidal position encoding with 10 frequencies. This function is applied ona 2D bounding box, represented by the top-left and bottom-right corners, leading to an 84-dim vectorper object. For cls, we use a simple 1-hot encoding to embed the semantic class information. Thefinal per-object condition information is the concatenation, resulting in a 4127-dim vector for eachobject.",
  "Reimplementation of SPAGHETTI Since the official code of SPAGHETTI does notinclude the training code and only provides checkpoints for two different shape classes (chairs,": ": Qualitative comparison of 3D shape reconstruction on the Pix3D . While InstPIFuoften produces noisy surfaces, our image-conditional 3D diffusion model synthesizes high-qualityshapes that closely match the target geometries. : Shape decomposition visualization. We assign each vertex of the reconstructed mesh tothe closest 3D Gaussian center and visualize the assignment with individual colors. Our scaffoldingrepresentation decomposes the shape into distinctive regions and aligns well with certain semanticparts, e.g., individual chair legs or the arm rests of a sofa. airplanes), we re-implement the training procedure, loss function, and disentanglement loss followingthe description in the papers to train the full shape prior over all relevant shape categories. Randomgeometric augmentations are essential during training to achieve self-supervised disentanglement intoextrinsic and intrinsic shape properties. We apply full 360-degree random rotations, uniform scaleaugmentation between 0.7 and 1.3, and translation jitter of 0.3 on the disentangled extrinsic andtarget pointcloud. Further, we do not utilize the symmetry options of the original implementation. : Comparison with retrieval baseline method ROCA on frames from ScanNet .While ROCA cannot always retrieve a matching mode from the shape database, such as the desk inthe first row, our diffusion-based reconstruction approach reconstructs accurate shapes and poses. : Architecture Diagram of the Shape Diffusion Model. The shape diffusion modelconsists of 3 sub-parts: An image-conditioned diffusion model, denoising the 3D Gaussians; a 3DGaussian-conditioned diffusion model, denoising the intrisic vectors; and an Occupancy Decoder,which takes as input a 3D point coordinate and the denoised extrinsics & intrinsics and outputs anoccupancy value indicating whether the 3D point is inside/outside of the shape. : Per-class pose estimation results for all NYU-37 classes on SUN RGB-D . We evaluatethe pose estimation quality in terms of 3D IoU. Our scene prior formulation achieves improvementsacross all categories which particular high gains on common object classes like chair (+16.6%) ordesk (+16.4%)."
}