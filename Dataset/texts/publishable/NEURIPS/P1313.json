{
  "Abstract": "Foundation models have made significant strides in understanding the genomiclanguage of DNA sequences. However, previous models typically adopt the tok-enization methods designed for natural language, which are unsuitable for DNAsequences due to their unique characteristics. In addition, the optimal approachto tokenize DNA remains largely under-explored, and may not be intuitivelyunderstood by humans even if discovered. To address these challenges, we in-troduce MxDNA, a novel framework where the model autonomously learns aneffective DNA tokenization strategy through gradient decent. MxDNA employsa sparse Mixture of Convolution Experts coupled with a deformable convolu-tion to model the tokenization process, with the discontinuous, overlapping, andambiguous nature of meaningful genomic segments explicitly considered. OnNucleotide Transformer Benchmarks and Genomic Benchmarks, MxDNA demon-strates superior performance to existing methods with less pretraining data andtime, highlighting its effectiveness. Finally, we show that MxDNA learns uniquetokenization strategy distinct to those of previous methods and captures genomicfunctionalities at a token level during self-supervised pretraining. Our MxDNAaims to provide a new perspective on DNA tokenization, potentially offering broadapplications in various domains and yielding profound insights. Code is availableat",
  "Introduction": "Foundation models in natural language processing (NLP) have achieved remarkable success, trans-forming how machines understand and generate human language . Inspired by this success,researchers are now exploring the application of foundation models to decode the complex languageof genomic sequences, aiming to potentially revolutionize our understanding of genomics .Tokenization, a critical initial step in NLP models, leverages human knowledge of natural languagestructures such as grammar and punctuation to segment text into meaningful units. However, DNAsequences present a distinct challenge: they lack natural delimiters and their grammar is not readilyunderstood by humans. These challenges make the tokenization process of DNA sequences notstraightforward. Various tokenization methods have been employed by existing foundation models to analyse DNAsequences . For example, single nucleotide tokenization treats each nucleotide as anindividual token, K-mer segments the DNA blocks of k consecutive nucleotides, and Byte-Pair",
  "GGTAGC": ": Evolution of tokenization and Ideal Properties. Left: The progression from basic tokeniza-tion methods to more sophisticated techniques, with the direct but unsuitable applications from naturallanguage to genomic language. Right: the ideal tokenization properties for genomicsMeaningful,Discontinuous, Overlapping, and Ambiguousoutlined in , which our MxDNA aims to achieve. Encoding (BPE) iteratively merges the most frequent pairs of existing tokens. All of thesemethods are borrowed directly from NLP as depicted in , each with its own inherent limitations.Single nucleotide tokenization, while offering high resolution for input, leads to an extremely largenumber of tokens, significantly increasing the complexity of the model. K-mer comes in two forms:overlapping and non-overlapping. Overlapping K-mer, despite its attempt to capture more contextualinformation, does not offer substantial benefits over single nucleotide approaches and can suffer frominformation leakage . Non-overlapping K-mer greatly reduces tokenized sequence length butcan disrupt a potentially meaningful unit by splitting it into separate K-mers. BPE, adopted fromNLP, attempts to optimize vocabulary size but often results in suboptimal segmentation that may notcorrespond to meaningful units . Unlike natural languages, where linguistically meaningful units such as words and sentences arealmost standardized and well understood, the optimal approach to tokenize DNA remains under-explored due to the complex and varied nature of genomics. In NLP, common tokenization strategieshave been validated by human knowledge, but such understanding does not extend to DNA. Con-sequently, rather than relying manually crafted tokenization rules, it may be better to trust a neuralnetwork to learn and determine the most effective tokenization strategy for genomic sequences. Addi-tionally, recent research suggests that biologically meaningful protein tokens can be discontinuous,overlapping, and may require mapping to several tokenization possibilities , propertiesthat are likely applicable to DNA sequences due to the genetic central dogma . To handle thesecomplexities, we can further equip our model with capabilities to manage discontinuities, overlaps,and the ambiguities of genomic sequences explicitly. Building on the analysis above, we introduce MxDNA (Mx draws from Mixture of Experts ),a novel framework designed to autonomously learn an effective DNA tokenization strategy solelythrough gradient decent. The core of the framework starts with a sparse Mixture of ConvolutionExperts that identifies and embeds basic units within DNA sequences. Unlike conventional Mix-ture of Experts models, which focus on scaling up the model while maintaining computationalefficiency , the experts in MxDNA are uniquely designed to capture DNA basic units ofvaried lengths. Following this, a deformable convolution assembles these basic units intofinal tokens. Throughout the process, MxDNA is explicitly equipped to manage the inherent disconti-nuities, overlaps and ambiguities in genomic sequences, enabling it to handle complex biologicalcharacteristics it encounters. Furthermore, we incorporate a cross-attention mechanism to align theoutput resolution with the original input during pretraining on a masked language modeling task . The proposed MxDNA demonstrates strong performance on both the Nucleotide Transformer Bench-marks and the Genomic Benchmarks . Despite only being pretrained on human referencegenome, it still outperforms or matches previous models some pretrained on multi-species data and for much longer duration, achieving state-of-the-art average performance and thebest on 15 of the 26 individual tasks. Finally, by visualizing the learnt tokenization process, weillustrate that MxDNA learns unique tokenization strategy distinct to those of previous methods",
  "and captures genomic functionalities at a token level during self-supervised pretraining, potentiallyoffering novel biological insights. Our contributions can be summarized as follows:": "Learnt Tokenization: We highlight the unsuitability of current DNA tokenization methodsdirectly borrowed from NLP. Based on the belief that humans may not know the besttokenization approach but a model could potentially discover it, we propose a novel approachwhere the model autonomously learns an effective tokenization strategy. Architectural Design: We introduce a sparse Mixture of Convolution Experts coupledwith a deformable convolution to dynamically learn tokenization, specifically designedto manage the inherent discontinuities overlaps and ambiguities in genomic sequences.Additionally, we leverage cross attention to align input and output sequence length to enableself-supervised pretraining. Empirical Results: MxDNA demonstrates robust quantitative performance with less pre-training data compared to some existing models, achieving state-of-the-art average per-formance on both Nucleotide Transformer Benchmarks and Genomic Benchmarks. Fur-thermore, visual analysis of the tokenization behaviour and the token embedding spacehighlights the unique strategy and capability to capture genomic functionalities at a tokenlevel of MxDNA, potentially offering new biological insights.",
  "Tokenization Methods": "Tokenization is a fundamental step in both natural language processing (NLP) and DNA sequencemodelling, transforming complex texts or DNA sequences into manageable tokens. In NLP, whites-pace tokenization uses spaces and punctuation as delimiters but faces out-of-vocabulary issues.Similarly, in both fields, character (or single nucleotide in DNA) tokenization provides high res-olution but can lead to computational inefficiency . N-gram in NLP and K-mer inDNA analysis both use contiguous sequences of N (K) items from given inputs , butcan disrupt meaningful units due to their fixed-length nature (non-overlapping) or lead to potentialinformation redundancy or leakage (overlapping) . Byte-Pair Encoding (BPE) is employedacross both domains to reduce vocabulary size by merging frequent pairs of existing tokens .However, it might not adequately encode more complex patterns and are unreliable for findinglinguistically sound tokens . These rule-based methods show limitations in different aspects,and our study aim to develop a learning-based tokenization method without these limitations.",
  "DNA Foundation Models": "Recent advancements in DNA modeling have leveraged foundation models to decode the complexlanguage of genomes. DNABERT pioneers the use of a BERT-like pretrained model for genomicsequence analysis, enhancing the understanding of nucleotide relationships via attention mechanisms.Nucleotide Transformer offers a comprehensive analysis of foundation models pretrained on DNAsequences, with model sizes reaching up to 2.5 billion parameters and pretraining data drawn fromthe 1000G human genomes and 850 various species. DNABERT2 introduces an enhancedgenome foundation model, utilizing an efficient BPE tokenizer and techniques to address inputlength constraints, resulting in reduced time and memory consumption while improving performance.HyenaDNA introduces a genomic foundation model capable of handling context with 1 milliontokens at single nucleotide resolution, enabling the first exploration of in-context learning in genomics.DNAGPT extends the traditional GPT model by integrating tasks such as binary classificationof DNA sequence order and numerical regression for predicting guanine-cytosine content, alongsidedeveloping a comprehensive token language. Caduceus designs an architecture that leverages thelong-range Mamba block to support bi-directionality and reverse complementarity equivariance,addressing specific challenges in genomic analysis. Following VQVAE , VQDNA employsa convolutional encoder alongside a vector-quantized codebook to model tokenization, sharing asimilar motivation with us yet ultimately adopting distinct solutions. Each work offers unique insightsand innovations to the filed. Our research specifically concentrate on the tokenization methods forDNA, hopefully providing our unique contributions to the filed.",
  "ATC TCAGC": ": Our proposed MxDNA. (Top) Overall pipeline of the MxDNA model: Black arrowsindicate pretraining data flow, and red arrows indicate finetuning data flow. The learnt tokenizationmodule tokenizes single nucleotide input into learnt tokens. (Bottom) Illustration of the learnttokenization module: Meaningful basic units are recognized with a linearly scoring layer and non-maximum suppression, embedded through convolution experts (Sec. 3.2.1), and assembled into finaltokens by a deformable convolution. (Sec. 3.2.2) This process ensures meaningful, discontinuous,overlapping, and ambiguous tokenization, addressing the unique properties of genomic data.",
  "Motivation": "The concept of correct tokenization in genomic sequences analysis remains undefined due to thecomplex nature of genomics. Unlike natural languages, where linguistically meaningful units arewell-understood, biological units in genomics are not limited to contiguous nucleotide or aminoacid sequences. Instead, they often encompass discontinuous, overlapping, and ambiguous segmentscrucial for understanding biological functions . Current DNA modeling practices directly borrowtokenization methods from natural language processing (NLP), such as single nucleotide tokenization,K-mer and Byte-Pair Encoding (BPE). These fixed, predefined approaches, though useful, often failto capture the unique properties of DNA sequences, which lack explicit delimiters and consist ofbiologically meaningful units that defy simple segmentation. Recognizing these challenges, MxDNA was developed based on the belief that although an optimaltokenization schema for genomic sequences is yet to be discovered, we can explicitly equip our modelwith the desired tokenization propertiessuch as handling discontinuities, overlaps, and ambiguities,and allow it to learn and adapt its tokenization strategy all by itself.",
  "Basic Units Recognition": "Basic Units ScoringInitially, MxDNA identifies the basic units that serve as the building blocksfor tokens. It estimates the probability of the existence of various sized basic units centred ateach nucleotide position by a linear gating mechanism commonly used in Mixture of Expertsmodels. Following this, one-dimensional non-maximum suppression is applied to eliminate redundantproposals and select the most significant basic units. Specifically, given the input nucleotide sequence X Rld, where l is the sequence length and dis the hidden dimension, X is first linearly scored to produce S Rln, where n represents thenumber of experts. Training-time multiplicative jitter noise is applied to introduce ambiguity,while ensuring deterministic inference. The jitter noise is applied by multiplying the scores with arandom factor sampled uniformly between [1 0.01, 1 + 0.01], resulting in slight perturbations tothe probability distribution used for tokenization. Modified non-maximum suppression is then applied to S, where Sij indicates the presence probabilityof a basic unit of length Lj centered at position i, and L Nn is a predefined set of lengths. Theresults are tracked using an expert mask M Nl, where each Mi is a natural integer indicating thepresence of a basic units center of length Mi at position i. Basic Units EmbeddingAfter identifying the basic units, the nucleotides within each unit areaggregated to form embeddings. Convolution kernels of corresponding sizes are applied to the centerof each basic unit to capture local features. The initial scoring and gating into specific convolutionexperts is similar to the Mixture of Experts paradigm, with each expert being a convolutional unitfocusing on a specific segment rather than a single nucleotide. Specifically, a basic unit at position i of length Lj = Mi is processed by the convolution expert Ejwith kernel size Lj, and weighted by softmax(Si)j, aggregating the nucleotides within the unit. Thistransforms the original input X Rld into an array of basic units U Rld, where k is the numberof basic units:",
  "Basic Units Assembly": "Distal Relation EstimationBuilding upon the identified basic units, the more complex genomicpatterns that extend beyond simple segmentation are modelled by a one-dimensional deformableconvolution. This technique uniquely accommodates the modeling of complex local geometrictransformations, adaptively adjusting to the input sequence. The linkages between the distal basicunits are modeled by the offsets and modulation factors of each basic unit. Following , offsets P Rkf and modulation factors M Rkf are computed based onthe basic units U to model the distal relationships among them. This strategy ensures the combinationof basic units is discontinuous, and reuses units across tokens achieve the overlapping property. Final Tokens EmbeddingUsing the computed offsets and modulation factors, deformable con-volution is applied to embed basic units into final tokens. The embedding process for each positionincorporates deformations of the convolution kernel specified by the offsets, with the results modu-lated by the modulation factors.",
  "The framework begins with single nucleotide input represented as Xinput Rld. This singlenucleotide resolution input allows for fine-grained analysis of genomics from the beginning": "Initially, Xinput is processed through several transformer encoder blocks designed to extract globalrelationships within the sequence, producing X Rld. This sets the stage for effective tokenization.Following this, the learnt tokenization module transforms the nucleotide sequence X into a moremanageable form T Rkd, improving the efficiency and focus of subsequent layers. The tokenizedoutput T is then passed through another series of transformer encoder blocks to further refine thetoken representation to Toutput Rkd, enhancing the models ability to encode deeper genomicinformation. For the mask language modeling pretraining stage, the enriched nucleotide level representation Xserves as the query, with the refined tokenized output Toutput acting as both the key and value. Thissetup maps the output resolution to single nucleotides, essential for reconstructing masked tokens.During the finetuning stage, the [CLS] token of Toutput is used for classification by convention.",
  "Experiments": "In this section, we first introduce the implementation and pretraining settings of MxDNA. Then, weevaluate MxDNA against other foundation models on Genomic Benchmarks and NucleotideTransformer Benchmarks . Next, we present ablation studies on the effect of different tokeniza-tion methods and different components of MxDNA. Finally, we conduct a simple analysis on thetokenization behaviors of MxDNA. Experiment settings and results are detailed in Appx. A.4.",
  "Model Implementation & Pretraining": "Our MxDNA is built on the architecture Nucleotide Transformer v2 100M model with 512 hiddenunits and 22 layers, totaling approximately 100M parameters. Specifically, the models learnttokenization module includes 10 convolution experts with kernel sizes ranging from 1 to 10, alongwith a deformable convolution block with a kernel size of three. We integrate this module by replacingthe fifth transformer block, aiming to avoid introducing additional computations. MxDNA is pretrained on the whole Human Reference Genome on masked language modelingtask with 15% of the nucleotides randomly masked. An auxiliary balancing loss with a weight of0.01 is used to prevent degradation towards a single expert, following . The model undergoestraining for 500k steps for main performance comparisons and 100k steps for ablations.",
  "Downstream Evaluation": "We primarily follow the evaluation settings of HyenaDNA , performing evaluation on NucleotideTransformer Benchmarks and Genomic Benchmarks. To ensure fair comparison, we fully finetune allthe BERT-like DNA foundation models including Nucleotide Transformer v2 , DNABERT ,DNABERT2 , MxDNA under same hyperparameter settings. For HyenaDNA, we utilize thehyperparameters recommended by . All experiments are repeated with three random seeds,and we report the average performance with sample standard deviations. 3",
  "Average88.13 0.0387.50 0.1388.29 0.1987.17 0.1589.13 0.13": "Mouse Enhancers83.94 0.4181.54 0.8681.34 0.8480.99 0.7280.57 0.97Coding vs Intergenomic94.50 0.0693.13 0.0594.94 0.3490.74 0.1195.28 0.08Human vs Worm96.88 0.1896.98 0.0797.57 0.0496.53 0.0497.64 0.01Human Enhancer Cohn74.33 0.4074.54 0.2775.93 0.2073.36 0.1574.67 0.09Human Enhancer Ensembl92.05 0.3892.18 0.1492.31 0.0388.12 0.1793.13 0.35Human Regulatory93.79 0.1288.16 0.0987.94 0.5493.08 0.2294.11 0.08Human OCR Ensembl78.51 0.5581.40 0.1180.94 0.0979.15 0.3481.05 0.07Human NonTATA Promoters91.05 0.4792.06 0.2095.34 0.1795.39 0.2696.56 0.29 : Nucleotide Transformer Benchmarks. Average performance across three random seeds forNucleotide Transformer v2 100M, DNABERT, DNABERT2, HyenaDNA and MxDNA with samplestandard deviations. We highlight the best values in bold type and underline the second best.",
  "Histone Markers Avg.55.22 0.2151.67 0.1765.89 0.4665.24 0.2668.14 0.19": "H378.22 1.1577.41 1.0182.31 0.2280.86 0.5282.78 0.14H3K14ac51.76 0.9946.51 1.8365.13 1.1065.96 0.2668.27 0.19H3K36me359.18 0.5350.98 0.7568.19 0.8264.31 0.3367.05 1.05H3K4me151.87 1.0943.83 0.3456.09 0.4855.04 1.0756.15 0.63H3K4me229.63 1.6832.38 0.9849.25 2.0649.96 0.9055.59 1.08H3K4me338.76 1.3031.49 3.4057.90 0.9260.92 0.7263.68 0.34H3K79me360.98 1.0660.48 0.5071.94 0.4470.97 0.7774.29 0.08H3K9ac54.57 0.7152.55 0.8564.35 1.0362.57 0.4664.78 0.50H479.19 0.4979.60 0.5580.71 0.4378.73 0.6681.18 0.25H4ac48.02 1.4041.53 0.2463.03 0.8163.06 0.6267.65 0.39",
  "All98.36 0.0497.83 0.0695.68 0.1196.78 0.1898.14 0.08Accpetor98.69 0.1497.81 0.2897.71 0.1196.52 0.1998.01 0.13Donor99.09 0.0598.43 0.0596.67 0.1797.16 0.1698.10 0.13": "As shown in , MxDNA achieves the best performance on 5 out of 8 datasets and ranksin the top-2 on 7 out o f 8 datasets. On average, MxDNA shows an improvement of 0.84 pointscompared to the second-best model, DNABERT2. These results demonstrate MxDNAs robustnessand effectiveness in regulatory element classification.",
  "Nucleotide Transformer Benchmarks": "Next, we evaluate MxDNA on the Nucleotide Transformer Benchmarks , which includes 18datasets across three task types: histone marker prediction, regulatory annotation prediction, andsplice site annotation prediction. For this benchmark, the BERT-like models are finetuned for 20epochs. Following , we use the Matthews Correlation Coefficient (MCC) for histone markerstasks, F1 score for regulatory and splice site annotation tasks, except accuracy for splice site all task.",
  "Single Nucleotide Baseline75.07 0.2688.56 0.02+ Mixture of Convolution Experts77.00 0.0588.72 0.07+ Deformable Convolution77.35 0.1288.86 0.18+ Jitter Noise (MxDNA)77.52 0.1888.89 0.05": "As shown in , MxDNA achieves the best performance on 10 out of 18 datasets and ranksin the top-2 on 16 out of 18 datasets. On average, MxDNA shows an improvement of 1.48 pointscompared to the second-best model, DNABERT2. Notably, MxDNA significantly outperforms allother models in the histone markers tasks while maintaining competitive performance in regulatoryannotation and splice site annotation tasks.",
  "Ablation Studies": "Different Tokenization Methods:We compare various tokenization methods by pretraining mod-els for 100k steps using the same backbone but different tokenization methods. The results in show that our learnt tokenization significantly outperforms traditional methods such asnon-overlapping K-mer, BPE, overlapping K-mer, and single nucleotide tokenization. Among therule-based methods, single nucleotide tokenization performs best, possibly because it doesnt incor-porate human biases and focuses solely on the raw data, though it may make it difficult to capturehigher-level semantics. Conversely, non-overlapping K-mer might disrupt meaningful units, BPEmight fail to segment DNA sequences meaningfully, and overlapping K-mer could suffer frominformation leakage. Different Components:We assess the impact of individual components by incrementally integrat-ing each into the baseline model and pretraining them for 100k steps. Starting with a baseline of singlenucleotide tokenization, we sequentially add the Mixture of Convolution Experts, the deformableconvolution and jitter noise, resulting in the proposed MxDNA. The results in show substantialperformance gains from the Mixture of Convolution Experts alone, demonstrating the effectivenessof the idea which allows the model to learn tokenization autonomously rather than depending onpredefined tokenization. There are also noticeable performance improvements contributed by thedeformable convolution and jitter noise, showing the effectiveness of explicitly equipping the modelwith capabilities to handle discontinuities, overlaps and ambiguities.",
  "Analysis": "We conduct an analysis of the tokenization behaviors of MxDNA against previous methods on both asample and dataset level, and present a output embedding analysis at a token level. Notably, MxDNAexhibits unique tokenization strategy distinct from prior methods and is able to inherently capture anddifferentiate genomic functionalities at a token level during self-supervised pretraining, potentiallyproviding new biological insights. Visualization details are in Appx A.6.",
  "Histone MarkerEnhancerPromoterSplice Site": ": Distribution of token lengths for BPE (top) and MxDNA (bottom) across different down-stream datasets, illustrating the distinct strategy of MxDNA for handling DNA tokenization. For thesake of simplicity, we regard the basic units as tokens for MxDNA. worth mentioning that there are usually a small number of differences between the two results, andwe display the region where the tokenization outcomes are different to show the ambiguous propertyfor illustrative purposes. For previous rule-based methods, tokenization is static and performed onlyonce. As depicted in , our learnt tokenization method tokenize the DNA sequence in a waydistinctly different from previous rule-based method. Moreover, the discontinuous, overlapping andambiguous tokenization results validate our design choices to manage these properties. Dataset LevelTo gain more insights, we measure the distribution of token lengths across differentdownstream datasets for both MxDNA and BPE. For simplicity, we regard the basic units as tokensfor MxDNA. BPE and MxDNA shows very distinct distribution of token lengths. As shown in, BPE tends to produce a bell-shaped distribution, inherently biased by its frequency-basedmerging rule. Conversely, MxDNAs distribution is closer to a uniform distribution with preferencesfor specific lengths, reflecting its adaptive, task-oriented segmentation capabilities. Moreover, the",
  "variability in token distribution across datasets might suggest that DNA sequences of differentfunctions might possess distinct patterns and meaningful units": "Token Embedding AnalysisNext, we use t-SNE to visualize the pretrained output tokens insequences with different genomic functions of different foundation models. As is shown in .4,without any finetuning, the token embedding distributions of MxDNA are different across sequenceswith different functions: the tokens of Histone Marker, Promoter and Splice Site form unique clusters.While for all other foundation models, their tokens do not form clear clusters as MxDNA does. Thisshows MxDNAs superior capability to inherently capture and differentiate genomic functionalitiesat a token level, suggesting its robustness and specificity in representing biological sequences evenbefore any supervised finetuning is applied.",
  "Conclusion": "SummaryWe present MxDNA, a framework developed to autonomously learn effective DNAtokenization strategies solely through gradient descent. MxDNA demonstrates strong performanceagainst existing sota models and tokenization methods across 26 diverse genomic tasks in NucleotideTransformer Benchmarks and Genomic Benchmarks with no additional cost. We also perform ananalysis of the tokenization mechanism and the token embedding space of MxDNA, showing itsdistinct tokenization strategy against previous methods and unique capability to capture genomicfunctionalities at a token level. Limitations & Future WorksWhile MxDNA demonstrates strong quantitative performance onvarious downstream tasks, direct biological validation of the models tokenization decision remainslimited. Furthermore, the evaluation on long range tasks is lacking due to quadratic cost of self-attention, although the learnt tokenization is expected to help reduce sequence length effectivelyand can be combined with sub-quadratic architectures . Future research will focus onrefining MxDNAs design to learn a better and more interpretable tokenization strategy, and testingits applicability to broader genomic analyses especially on more long range tasks.",
  "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirec-tional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.Advances in neural information processing systems, 33:18771901, 2020. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothe Lacroix,Baptiste Rozire, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundationlanguage models. arXiv preprint arXiv:2302.13971, 2023. Yanrong Ji, Zhihan Zhou, Han Liu, and Ramana V Davuluri. Dnabert: pre-trained bidirectional encoderrepresentations from transformers model for dna-language in genome. Bioinformatics, 37(15):21122120,2021.",
  "Zhihan Zhou, Yanrong Ji, Weijian Li, Pratik Dutta, Ramana Davuluri, and Han Liu. Dnabert-2: Efficientfoundation model and benchmark for multi-species genome. arXiv preprint arXiv:2306.15006, 2023": "Hugo Dalla-Torre, Liam Gonzalez, Javier Mendoza-Revilla, Nicolas Lopez Carranza, Adam HenrykGrzywaczewski, Francesco Oteri, Christian Dallago, Evan Trop, Bernardo P. de Almeida, Hassan Sirelkha-tim, Guillaume Richard, Marcin Skwark, Karim Beguir, Marie Lopez, and Thomas Pierrot. The nucleotidetransformer: Building and evaluating robust foundation models for human genomics. bioRxiv, pages202301, 2023. Eric Nguyen, Michael Poli, Marjan Faizi, Armin Thomas, Michael Wornow, Callum Birch-Sykes, StefanoMassaroli, Aman Patel, Clayton Rabideau, Yoshua Bengio, et al. Hyenadna: Long-range genomic sequencemodeling at single nucleotide resolution. Advances in neural information processing systems, 36, 2024. Mai Ha Vu, Rahmad Akbar, Philippe A Robert, Bartlomiej Swiatczak, Geir Kjetil Sandve, Victor Greiff,and Dag Trygve Truslew Haug. Linguistically inspired roadmap for building biologically reliable proteinlanguage models. Nature Machine Intelligence, 5(5):485496, 2023. Chaoqi Liang, Weiqiang Bai, Lifeng Qiao, Yuchen Ren, Jianle Sun, Peng Ye, Hongliang Yan, Xinzhu Ma,Wangmeng Zuo, and Wanli Ouyang. Rethinking the bert-like pretraining for dna sequences. arXiv preprintarXiv:2310.07644, 2023.",
  "Valentin Hofmann, Janet B Pierrehumbert, and Hinrich Schtze. Superbizarre is not superb: Derivationalmorphology improves berts interpretation of complex words. arXiv preprint arXiv:2101.00403, 2021": "Philippe A Robert, Rahmad Akbar, Robert Frank, Milena Pavlovic, Michael Widrich, Igor Snapkov, AndreiSlabodkin, Maria Chernigovskaya, Lonneke Scheffer, Eva Smorodina, et al. Unconstrained generationof synthetic antibodyantigen structures to guide machine learning methodology for antibody specificityprediction. Nature Computational Science, 2(12):845865, 2022. Rahmad Akbar, Philippe A Robert, Milena Pavlovic, Jeliazko R Jeliazkov, Igor Snapkov, Andrei Slabodkin,Cdric R Weber, Lonneke Scheffer, Enkelejda Miho, Ingrid Hobk Haff, et al. A compact vocabularyof paratope-epitope interactions enables predictability of antibody-antigen binding. Cell Reports, 34(11),2021.",
  "William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter modelswith simple and efficient sparsity. Journal of Machine Learning Research, 23(120):139, 2022": "Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford,Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral ofexperts. arXiv preprint arXiv:2401.04088, 2024. Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, and Yichen Wei. Deformableconvolutional networks. In Proceedings of the IEEE international conference on computer vision, pages764773, 2017. Xizhou Zhu, Han Hu, Stephen Lin, and Jifeng Dai. Deformable convnets v2: More deformable, betterresults. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages93089316, 2019.",
  "Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neuralinformation processing systems, 30, 2017": "Siyuan Li, Zedong Wang, Zicheng Liu, Di Wu, Cheng Tan, Jiangbin Zheng, Yufei Huang, and Stan Z Li.Vqdna: Unleashing the power of vector quantization for multi-species genomic sequence modeling. arXivpreprint arXiv:2405.10812, 2024. Valerie A Schneider, Tina Graves-Lindsay, Kerstin Howe, Nathan Bouk, Hsiu-Chuan Chen, Paul A Kitts,Terence D Murphy, Kim D Pruitt, Franoise Thibaud-Nissen, Derek Albracht, et al. Evaluation of grch38and de novo haploid genome assemblies demonstrates the enduring quality of the reference assembly.Genome research, 27(5):849864, 2017. Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio,Stefano Ermon, and Christopher R. Hyena hierarchy: Towards larger convolutional language models. InInternational Conference on Machine Learning, pages 2804328078. PMLR, 2023. Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever.Generative pretraining from pixels. In International conference on machine learning, pages 16911703.PMLR, 2020. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, ThomasUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, andIlya Sutskever. Zero-shot text-to-image generation. In International conference on machine learning,pages 88218831. Pmlr, 2021.",
  "Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: Bert pre-training of image transformers. arXivpreprint arXiv:2106.08254, 2021": "Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and LeiZhang. Bottom-up and top-down attention for image captioning and visual question answering. InProceedings of the IEEE conference on computer vision and pattern recognition, pages 60776086, 2018. Emanuele Bugliarello, Ryan Cotterell, Naoaki Okazaki, and Desmond Elliott. Multimodal pretrainingunmasked: A meta-analysis and a unified framework of vision-and-language berts. Transactions of theAssociation for Computational Linguistics, 9:978994, 2021.",
  "Nithin Chalapathi, Yiheng Du, and Aditi Krishnapriyan. Scaling physics-informed hard constraints withmixture-of-experts. arXiv preprint arXiv:2402.13412, 2024": "Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang,Zhengdong Zhang, Yonghui Wu, et al. Conformer: Convolution-augmented transformer for speechrecognition. arXiv preprint arXiv:2005.08100, 2020. Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang. Cvt:Introducing convolutions to vision transformers. In Proceedings of the IEEE/CVF international conferenceon computer vision, pages 2231, 2021. Tete Xiao, Mannat Singh, Eric Mintun, Trevor Darrell, Piotr Dollr, and Ross Girshick. Early convolutionshelp transformers see better. Advances in neural information processing systems, 34:3039230400, 2021. iga Avsec, Vikram Agarwal, Daniel Visentin, Joseph R Ledsam, Agnieszka Grabska-Barwinska, Kyle RTaylor, Yannis Assael, John Jumper, Pushmeet Kohli, and David R Kelley. Effective gene expressionprediction from sequence by integrating long-range interactions. Nature methods, 18(10):11961203,2021.",
  "Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020": "Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts,Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling languagemodeling with pathways. Journal of Machine Learning Research, 24(240):1113, 2023. Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R. Flashattention: Fast and memory-efficientexact attention with io-awareness. Advances in Neural Information Processing Systems, 35:1634416359,2022.",
  "Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprintarXiv:2307.08691, 2023": "Jason Ansel, Edward Yang, Horace He, Natalia Gimelshein, Animesh Jain, Michael Voznesensky, BinBao, Peter Bell, David Berard, Evgeni Burovski, Geeta Chauhan, Anjali Chourdia, Will Constable, AlbanDesmaison, Zachary DeVito, Elias Ellison, Will Feng, Jiong Gong, Michael Gschwind, Brian Hirsh,Sherlock Huang, Kshiteej Kalambarkar, Laurent Kirsch, Michael Lazos, Mario Lezcano, Yanbo Liang,Jason Liang, Yinghai Lu, CK Luk, Bert Maher, Yunjie Pan, Christian Puhrsch, Matthias Reso, MarkSaroufim, Marcos Yukio Siraichi, Helen Suk, Michael Suo, Phil Tillet, Eikan Wang, Xiaodong Wang,William Wen, Shunting Zhang, Xu Zhao, Keren Zhou, Richard Zou, Ajit Mathews, Gregory Chanan, PengWu, and Soumith Chintala. PyTorch 2: Faster Machine Learning Through Dynamic Python BytecodeTransformation and Graph Compilation. In 29th ACM International Conference on Architectural Supportfor Programming Languages and Operating Systems, Volume 2 (ASPLOS 24). ACM, April 2024.",
  "William Falcon and The PyTorch Lightning team. PyTorch Lightning, March 2019": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, PierricCistac, Tim Rault, Rmi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma,Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,and Alexander M. Rush. Transformers: State-of-the-art natural language processing. In Proceedings of the2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages3845, Online, October 2020. Association for Computational Linguistics.",
  "Wenzel Jakob, Jason Rhinelander, and Dean Moldovan. pybind11 seamless operability between c++11and python, 2017": "F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay.Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:28252830, 2011. Charles R. Harris, K. Jarrod Millman, Stfan J. van der Walt, Ralf Gommers, Pauli Virtanen, DavidCournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J. Smith, Robert Kern, Matti Picus,Stephan Hoyer, Marten H. van Kerkwijk, Matthew Brett, Allan Haldane, Jaime Fernndez del Ro,Mark Wiebe, Pearu Peterson, Pierre Grard-Marchant, Kevin Sheppard, Tyler Reddy, Warren Weckesser,Hameer Abbasi, Christoph Gohlke, and Travis E. Oliphant. Array programming with NumPy. Nature,585(7825):357362, September 2020.",
  "A.1.1Image Tokenization": "Tokenization in computer vision (CV) attempts to transform images into formats that can be effi-ciently processed by machine learning models especially transformers. In line with Character-leveltokenization, directly using raw pixels as input units. The Vision Transformer (ViT) splitsan image into patches of identical sizes and treats these patches as tokens in NLP, demonstratingremarkable performance on standard image recognition tasks. Inspired by , utilizedan image tokenizer learnt by discrete variational autoencoder (dVAE) to map pixels into discretetokens according to a visual codebook. Meanwhile, researchers also utilize detection or segmentationfeatures for visual representations. For instance, used a pretrained Faster R-CNN model to extract region features. Recently, exploit Segment Anything Model (SAM) toconstruct a sub-word tokenization and semantic tokenization respectively.",
  "A.1.2Mixture of Experts": "(Sparse) MoE is first designed to improve the capacity of neural networks while maintaining totalcomputations. uses MoE as a general purpose neural network component and realizes sparsegating, demonstrating its use as a practical way to massively increase model capacity. By replacingFNN with Mixture of Experts, successfully combine sparse MoE and Transformers, achievingsuperior capabilities with less computational cost. Previous methods generally replace a layer ofthe neural networks with multiple, sparsely activated identical alternatives, governed by a gatingmechanism. Recently, explicitly adds interpretability to each expert by letting each expert solvethe constraint over smaller decomposed domains through differentiable optimization.",
  "A.1.3Deformable Convolution": "Deformable convolution explicitly equips the model with ability to adapt to the geometric varia-tions of different objects . Unlike the attention mechanism, which focuses on capturinglong-range relationships, deformable convolution locally samples feature maps using learnt offsetsand modulation factors. By modeling complex geometric transformations effectively, deformableconvolution networks achieve significant performance improvements in various tasks, includingimage classification, object detection and semantic segmentation.",
  "A.1.4Combination of Convolution and Transformer": "The integration of convolutional layers with transformer architectures has emerged as a powerfulapproach across various domains, effectively combining the strengths of both techniques. Con-former applies this hybrid design to audio processing, enhancing the capture of local and globaldependencies in audio signals. In computer vision, the CvT introduces convolutions into trans-formers to improve efficiency and representational power, while Early Convolution in VisionTransformer incorporates convolutional layers early in the architecture to enhance input featurerepresentations. Extending to genomic data, Enformer applies a similar approach as in tomodel complex dependencies in DNA sequences and reduce the computational cost, showcasing thepotential of hybrid architectures to handle highly specialized data types like genomic sequences.",
  "A.2Method Details": "For the convolution expert, we adapt design principles from . Our 1-D convolution expert startswith a pointwise convolution W inpj paired with a Gated Linear Unit (GLU), followed by a 1-D groupedconvolution Wgj. Subsequent to the grouped convolution, a Layer Normalization (LayerNorm) anda Swish activation layer W outpj (Swish()) are applied. The grouped convolution here has number ofgroups equal to the factor of hidden size closest to kernel size, and number of output channel equal tonumber of input channels. This ensures that each convolution expert has similar parameter counts inspite of different kernel sizes.",
  "TermDescription": "lNumber of nucleotidesdDimension of hidden statesnNumber of expertskNumber of basic unitsfKernel size of the deformable convolutioniIndices of nucleotides or tokensjIndices of expertsX RldInput nucleotide sequenceS RlnConfidence scores of basic units existenceL NnKernel sizes of convolution expertsM NlMask indicating the existence of basic unitsEj RLjd RdConvolution expertsU RkdBasic unitsP RkfOffsets ofthe deformable convolutionM RkfModulation factors of the deformable convolutionT RkdFinal tokens",
  "A.4.1Settings": "Model ImplementationMxDNA is built on the Nucleotide Transformer V2 architecture whichincorporates several architectural improvements recognized in the NLP community, such as rotarypositional encodings , SwishGLU MLP , and the exclusion of linear bias terms .Consistent with Nucleotide Transformer V2 100M, MxDNA has 512 hidden units, an expansionfactor of 4, 16 attention heads, and 22 layers, totaling approximately 100M parameters. Specifically,the models learnt tokenization module includes 10 convolution experts with kernel sizes rangingfrom 1 to 10, along with a deformable convolution block with a kernel size of three. We integrate thismodule by replacing the fifth transformer block, aiming to avoid introducing additional computations.We utilize FlashAttention for efficient attention calculations. PretrainingFollowing , MxDNA is pretrained on the whole Human Reference Genome using Masked Language Modeling. We removed all sequences gaps and unannotated regions andextracted 70 to 510-nt-long sequences as training data. We mask 15% of the tokens, with 80%replaced by a special [MASK] token, 10% replaced with a random vocabulary token, and 10% leftunchanged. All masking happens at the initial input stage(single nucleotide, 6mer tokens, bpe tokens).For model using single nucleotide tokenization, non-overlapping 6mer and BPE, the masking isperformed randomly and mask out 15% of total tokens except of special tokens. For model usingoverlapping 6mer, we follow the strategy used in , with contiguous k-length spans of certaink-mers are masked, totalling around 15% of the tokens. An auxiliary balancing loss with a weight of0.01 is used to prevent degradation towards a single expert, following . The model is trained witha learning rate of 1e-4 and a batch size of 512. We employ the AdamW optimizer with 1 = 0.9,2 = 0.98, = 1e 6, a weight decay of 0.01, and a cosine annealing learning rate scheduler with alinear warm-up over the first 10% of steps. The model undergoes training for 500k steps for mainperformance comparisons and 100k steps for ablations. DownstreamWe download the data from for Nucleotide Transformer Benchmarks and for Genomic Benchmarks. Moreover,in Nucleotide Transformer Benchmarks, the BERT-like models are finetuned using PEFT (parameterefficient finetuning) without providing the exact hyperparameters. Believing that fully fine-tuningthese models will better leverage their capabilities and provide a fairer comparison , we decide toproceed with full finetuning for all models. We keep the original data splits in . We do notperform cross validation as does since it will be too computationally expensive for BERT-likemodels and we decide to follow the practice of HyenaDNA instead. Additionally, we repeat allexperiments under three random seeds, report the average results with sample standard deviations.",
  "All the BERT-like models are fully finetuned with a batch size of 32 and a learning rate of 3e-5. Weemploy the AdamW optimizer with 1 = 0.9, 2 = 0.999, = 1e 8, and a weight decay of 0.01": "Models are trained for 10 epochs on Genomic Benchmarks and 20 epochs on Nucleotide TransformerBenchmarks, with the learning rate linearly warmed up over the first epoch and then decaying tozero during the remaining epochs. For the Mouse Enhancers dataset (sequence lengths with mean= 2381, std = 984.4, max = 4707), we truncate the sequence to a maximum length of 4096, whichis considered acceptable. For DNABERT, which can not handle sequences of length over 512, wetruncate the sequence to a maximum length of 512. For HyenaDNA, we fully finetune the pretrained model from using the hyperparameters provided by indocker image hyenadna/hyena-dna-nt6:latest for Nucleotide Transformer Benchmarks, and with modified hyperpa-rameters recommended by for Genomic Benchmarks. Their research suggests that training withsequence lengths 2 to 4 times the length of sequences used in downstream tasks typically yields thebest performance. Thus, the tiny models are the best choice for most of the downstream tasks inNucleotide Transformer Benchmarks and Genomic Benchmarks since most of tasks have sequencelength of around a few hundreds and the tiny model are pretrained with 1000 length sequence.Notice that although our reproduced results is a bit lower than the results reported by the authors ofHyenaDNA, the performance of MxDNA is still better than originally reported results on most of thetasks.",
  "A.4.2Metrics": "This section defines the metrics used to evaluate the performance of models on various genomic tasks.On Nucleotide Transformer Benchmarks, We used the Matthews Correlation Coefficient (MCC) forhistone marker tasks, F1 scores for regulatory and splice site annotation tasks, except accuracy forsplice site all task. Top-1 Accuracy is used for all tasks in Genomics Benchmarks. Matthews Correlation Coefficient (MCC)The Matthews Correlation Coefficient is a robuststatistical rate which takes into account true and false positives and negatives and is generallyregarded as a balanced measure that can be used even if the classes are of very different sizes.",
  "Average88.56 0.0288.55 0.0786.83 0.0687.30 0.1688.89 0.05": "Mouse Enhancers77.56 0.9478.81 0.8582.43 0.9080.44 1.4480.44 0.63Coding vs Intergenomic95.05 0.1394.97 0.0692.73 0.0892.25 0.1194.78 0.04Human vs Worm97.52 0.0497.14 0.1196.35 0.0496.59 0.0197.27 0.07Human Enhancer Cohn73.70 0.5373.07 0.5672.72 0.1972.92 0.2373.98 0.40Human Enhancer Ensembl92.79 0.0992.98 0.0791.79 0.2392.38 0.0892.73 0.08Human Regulatory94.03 0.0294.21 0.0893.73 0.0889.70 0.1094.10 0.12Human OCR Ensembl80.84 0.5081.36 1.0575.64 0.6477.87 0.4580.62 0.42Human NonTATA Promoters97.00 0.0595.84 0.6989.24 0.2496.22 0.1797.22 0.23",
  "Histone Markers Avg.63.13 0.3461.88 0.6650.36 0.2864.58 0.1367.29 0.23": "H380.92 0.8580.94 0.6474.77 0.3280.26 0.1582.14 0.76H3K14ac62.00 1.7962.17 0.8046.33 0.7265.91 0.7268.29 0.65H3K36me362.59 1.5063.26 1.6050.49 1.7263.83 0.7365.46 1.74H3K4me151.66 0.5750.11 3.6341.26 1.0554.33 0.5354.97 1.50H3K4me249.51 0.9535.59 5.7229.99 0.9349.48 0.7455.30 0.49H3K4me354.14 0.9554.15 0.5630.83 0.5958.10 0.5563.82 0.92H3K79me370.36 1.5871.30 0.0459.99 0.4570.89 0.7473.74 0.78H3K9ac60.63 2.7364.70 0.3250.24 1.3162.22 0.5463.15 0.26H480.23 0.7980.09 0.5978.27 0.6079.29 0.4180.89 0.23H4ac59.25 1.3656.49 1.0241.42 0.8961.45 0.8065.14 0.23",
  "A.5.2Different Components": "Detailed results on each datasets with components added from the single nucleotide tokenizationbaseline are presented in and . 1-mer stands for the baseline. + MoCE stands foradding the sparse Mixture of Convolution Experts.+ Def Conv stands for adding the deformableconvolution block. + Noise stands for adding the multiplicative jitter noise. These components areadded successively, finally equivalent to MxDNA. All models are trained for 100k steps with samebackbone and components added sequentially.",
  "A.6Visualization Details": "The BPE tokenizer used here is directly borrowed from DNABERT2 with a vocabulary of size 4096.The visualization methods for traditional tokenization methods is straightforward. Below are thevisualization details for MxDNA. Sample Level:For MxDNA, we perform a forward process of the model using the sequence asinput, extracting the Mask of basic units existence M, Offsets P and Modulation factors M of thedeformable convolution. First, we colour the recognized basic units based on M. Then, we determinethe distal relations using P and M. Specifically, a distal relation is considered to be visualizedonly if the the product of the corresponding modulation weight and the bilinearly interpolated offsetweight exceeds one. Eventually, a final learnt token is made up of a group of related basic units andcoloured by the colour of the central basic unit. Dataset Level:For dataset level, we first finetune the MxDNA model on downstream datasets anduse the refined models to generate the Mask of basic units existence M for all samples in the dataset.We then calculate the proportion of the lengths of each recognized basic units as indicated by M.Specifically, we finetune MxDNA on H3, Enhancer, Promoter All and Splice Site All in NucleotideTransformer Benchmarks for Histone Marker, Enhancer, Promoter and Splice Site respectively.",
  "AssetLicense": "GRCh38 CC BY 4.0Genomic Benchmarks Apache-2.0Nucleotide Transformer CC BY-NC-SA 4.0DNABERT Apache-2.0DNABERT2 Apache-2.0HyenaDNA Apache-2.0FlashAttention BSD-3-ClausePytorch BSD-3-ClausePytorch Lightning Apache-2.0Huggingface Apache-2.0Pybind11 BSD-3-ClauseScikit-Learn BSD-3-ClauseNumpy BSD-3-ClauseMatplotlib Matplotlib LicenseSeaborn Apache-2.0 Token Embedding Analysis:For the token embedding analysis, we utilize various pretrainedmodels to embed sequences with different functions and analyse the output embeddings at the tokenlevel. Initially, we perform principal component analysis to reduce the dimensionality to one hundred,which facilitates the visualization process. Subsequently, we employ t-SNE to visualize these tokenembeddings in a two-dimensional space. Specifically, we use the H3, Enhancer, Promoter All, andSplice Site All sequences from the test set of the Nucleotide Transformer Benchmarks to representHistone Marker, Enhancer, Promoter, and Splice Site, respectively.",
  "A.7Computational Resources": "We train and evaluate the models on NVIDIA RTX 3090 and NVIDIA A100 GPUs. The pretrainingof MxDNA takes around 3 days for 500k steps using 4 A100 GPUs. Finetuing MxDNA on all thedownstream tasks takes approximately 1.5 Days using 1 A100 GPU. This is true for other BERT-likefoundation models with around 100M parameters. The detailed computational costs of the models (averaged across 5 samples of sequence length of510) are outlined in . The integration of a mixture of convolution experts and the deformableconvolution introduces an increased computational overhead initially due to the O(l log(l)) timecomplexity of the learned tokenization mechanism (where l represents the number of nucleotides).This complexity is mitigated by the substantial reduction in sequence length after tokenization, whichdecreases the number of tokens processed by subsequent transformer layers."
}