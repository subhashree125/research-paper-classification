{
  "Abstract": "We provide new lower bounds on the privacy guarantee of the multi-epoch AdaptiveBatch Linear Queries (ABLQ) mechanism with shuffled batch sampling, demon-strating substantial gaps when compared to Poisson subsampling; prior analysiswas limited to a single epoch. Since the privacy analysis of Differentially PrivateStochastic Gradient Descent (DP-SGD) is obtained by analyzing the ABLQ mech-anism, this brings into serious question the common practice of implementingshuffling-based DP-SGD, but reporting privacy parameters as if Poisson subsam-pling was used. To understand the impact of this gap on the utility of trainedmachine learning models, we introduce a practical approach to implement Poissonsubsampling at scale using massively parallel computation, and efficiently trainmodels with the same. We compare the utility of models trained with Poisson-subsampling-based DP-SGD, and the optimistic estimates of utility when usingshuffling, via our new lower bounds on the privacy guarantee of ABLQ withshuffling.",
  "Introduction": "A common approach for private training of differentiable models, such as neural networks, is toapply first-order methods with noisy gradients. This general framework is known as DP-SGD(Differentially Private Stochastic Gradient Descent) [Abadi et al., 2016]; the framework itself iscompatible with any optimization sub-routine. Multiple open source implementations exist forapplying DP-SGD in practice, namely, Tensorflow Privacy, JAX Privacy [Balle et al., 2022] andPyTorch Opacus [Yousefpour et al., 2021]; and DP-SGD has been applied widely in various machinelearning domains [e.g., Tramer and Boneh, 2020, De et al., 2022, Bu et al., 2022, Chen et al., 2020,Dockhorn et al., 2023, Anil et al., 2022, He et al., 2022, Igamberdiev et al., 2024, Tang et al., 2024]. DP-SGD (Algorithm 1) processes the training data in a sequence of steps, where at each step, a noisyestimate of the average gradient over a mini-batch is computed and used to perform a first-orderupdate over the differentiable model. To obtain the noisy (average) gradient, the gradient g foreach example in the mini-batch is clipped to have norm at most C (a pre-determined fixed bound),by setting [g]C := g min{1, C/g2}, and computing the sum over the batch; then independentzero-mean noise drawn from the Gaussian distribution of scale C is added to each coordinate ofthe summed gradient. This could then be scaled by the target mini-batch size to obtain a noisy",
  "return wT": "average gradient.1 The privacy guarantee of the mechanism depends on the following parameters: thenoise scale , the number of examples in the training dataset, the size of mini-batches, the number oftraining steps, and the mini-batch generation process. In practice, almost all deep learning systems generate mini-batches of fixed-size by sequentiallygoing over the dataset, possibly applying a global shuffling of all the examples in the dataset foreach training epoch; each epoch corresponds to a single pass over the dataset, and the ordering ofthe examples may be kept the same or resampled between different epochs. However, performingthe privacy analysis for such a mechanism has appeared to be technically difficult due to correlationbetween the different mini-batches. Abadi et al. instead consider a different mini-batchgeneration process of Poisson subsampling, wherein each mini-batch is generated independentlyby including each example with a fixed probability. This mini-batch generation process is howeverrarely implemented in practice, and consequently it has become common practice to use some formof shuffling in applications, but to report privacy parameters as if Poisson subsampling was used(see, e.g., the survey by Ponomareva et al. [2023, .3]). A notable exception is the PyTorchOpacus library [Yousefpour et al., 2021] that supports the option of Poisson subsampling; however,this implementation only works well for datasets that allow efficient random access (for instance byloading it entirely into memory). To the best of knowledge, Poisson subsampling has not been usedfor training with DP-SGD on massive datasets. The privacy analysis of DP-SGD is usually performed by viewing it as a post-processing of anAdaptive Batch Linear Queries (ABLQ) mechanism that releases the estimates of a sequence ofadaptively chosen linear queries on the mini-batches (formal definitions in .1). Chua et al. showed that the privacy loss of ABLQ with shuffling can be significantly higher than thatwith Poisson subsampling for small values of . Even though their analysis only applied to a singleepoch mechanism, this has put under serious question the aforementioned common practice ofimplementing DP-SGD with some form of shuffling while reporting privacy parameters assumingPoisson subsampling. The motivating question for our work is:",
  "ContributionsOur contributions are summarized as follows": "Privacy Analysis of Multi-Epoch ABLQ with Shuffling.We provide lower bounds on the privacyguarantees of shuffling-based ABLQ to handle multiple epochs. We consider the cases of both (i)Persistent Shuffling, wherein the examples are globally shuffled once and the order is kept the samebetween epochs, and (ii) Dynamic Shuffling, wherein the examples are globally shuffled independentlyfor each epoch. Since our technique provides a lower bound on the privacy guarantee, the utilityof the models obtained via shuffling-based DP-SGD with this privacy accounting is an optimisticestimate of the utility under the correct accounting. Scalable Implementation of DP-SGD with Poisson Subsampling via Truncation.Variablebatches are typically inconvenient to handle in deep learning systems. For example, upon a change inthe input shape, jax.jit triggers a recompilation of the computation graph, and tf.function will",
  "As explained later, for Poisson batch sampler, the mini-batch size is not a constant, but the scaling has to bedone with the target mini-batch size, and not the realized mini-batch size": "retrace the computation graph. Additionally, Google TPUs require all operations to have fixed inputand output shapes. We introduce truncated Poisson subsampling to circumvent variable batch sizes.In particular, we choose an upper bound on the maximum batch size B that our training can handle,and given any variable size batch b, if b B, we randomly sub-select B examples to retain in thebatch, and if b < B, we pad the batch with B b dummy examples with zero weight. This deviatesslightly from the standard Poisson subsampling process since our batch sizes can never exceed B.We choose B to be sufficiently larger than the expected batch size, so that the probability that thesampled batch size b exceeds the maximum allowed batch size B is small. We provide a modificationto the analysis of ABLQ with Poisson subsampling in order to handle this difference. Generating these truncated Poisson subsampled batches can be difficult when the dataset is too largeto fit in memory. We provide a scalable approach to the generation of batches with truncated Poissonsubsampling using massively parallel computation [Dean and Ghemawat, 2004]. This can be easilyspecified using frameworks like beam [Apache Beam] and implemented on distributed platformssuch as Apache Flink, Apache Spark, or Google Cloud Dataflow. Our detailed experimental results are presented in , and summarized below: DP-SGD with Shuffle batch samplers performs similarly to Poisson subsampling for the same . However, DP-SGD with Shuffle batch samplers, with our optimistic privacy accounting, performworse than Poisson subsampling in high privacy regimes (small values of ).Thus, our results suggest that Poisson subsampling is a viable option for implementing DP-SGD atscale, with almost no loss in utility compared to the traditional approach that uses shuffling with(incorrect) accounting assuming Poisson subsampling.",
  "Related Work": "Chua et al. demonstrated gaps in the privacy analysis of ABLQ using shuffling and Poissonsubsampling, by providing a lower bound on the privacy guarantee of ABLQ with shuffling; theirtechnique, however, was specialized for one epoch. We extend their technique to the multi-epochversion of ABLQ with shuffling and provide lower bounds for both persistent and dynamic batching. Lebeda et al. also point out gaps in the privacy analysis of ABLQ with Poisson subsamplingand with sampling batches of fixed size independently, showing that the latter has worse privacyguarantees than Poisson subsampling. We do not cover this sampling in our experimental study, sincesampling independent batches of fixed size is not commonly implemented in practice, and DP-SGDusing this sampling is only expected to be worse as compared to Poisson subsampling. Yousefpour et al. report the model utility (and computational cost overhead) under trainingwith DP-SGD with Poisson subsampling. However, to the best of our knowledge, there is no priorwork that has compared the model utility of DP-SGD under Poisson subsampling with that undershuffling, let alone compared it against DP-SGD under (Dynamic/Persistent) shuffling or studied thegaps between the privacy accounting of the two approaches. One possible gap between the privacy analysis of DP-SGD and ABLQ is that the former only releasesthe final iterate, whereas the latter releases the responses to all the queries. An interesting result byAnnamalai shows that in general the privacy analysis of the last-iterate of DP-SGD cannot beimproved over that of ABLQ, when using Poisson subsampling. This suggests that at least withoutany further assumptions, e.g., on the loss function, it is not possible to improve the privacy analysis ofDP-SGD beyond that provided by ABLQ; this is in contrast to the techniques of privacy amplificationby iteration for convex loss functions [e.g. Feldman et al., 2018, Altschuler and Talwar, 2022].",
  "Preliminaries": "A differentially private (DP) mechanism M : X O can be viewed as a mapping from inputdatasets to distributions over an output space, namely, on input dataset x = (x1, . . . , xn) where eachexample xi X, M(x) is a probability measure over the output space O; for ease of notation, weoften refer to the corresponding random variable also as M(x). Two datasets x and x are said tobe adjacent, denoted x x, if they differ in one example; in particular, we use the zeroing-outadjacency defined shortly.",
  "Adaptive Batch Linear Queries Mechanism": "Following the notation in Chua et al. , we study the adaptive batch linear queries mechanismABLQB (Algorithm 2) using a batch sampler B and an adaptive query method A, defined. The batchsampler B can be any algorithm that randomly samples a sequence S1, . . . , ST of batches. ABLQBoperates by processing the batches in a sequential order, and produces a sequence (g1, . . . , gT ),where the response gt Rd is produced as the sum of t(x) over the batch St with added zero-meanGaussian noise of scale to all coordinates, where the query t : X Bd (for Bd := {v Rd : v2 1}) is produced by the adaptive query method A, based on the previous responsesg1, . . . , gt1. DP-SGD can be viewed as a post-processing of an adaptive query method that mapsexamples to the clipped gradient at the last iterate, namely t(x) := [w(wt1, x)]1 (we treat theclipping norm C = 1 for simplicity, as it is just a scaling term).",
  "In this work, we consider the following multi-epoch batch samplers: Deterministic Db,T , PersistentShuffle Sb,T , and Dynamic Shuffle Sb,T batch sampler defined as instantiations of Algorithm 3": "in and truncated Poisson Pb,B,T (Algorithm 4); we drop the subscripts of each samplerwhenever it is clear from context. Note that, while Pb,B,T has no restriction on the value of n,the samplers Db,T , Sb,T , and Sb,T require that the number of examples n is such that E := bT/nand S := n/b are integers, where E corresponds to the number of epochs and S corresponds tothe number of steps per epoch. We call the tuple (n, b, T) as valid if that holds, and we willoften implicitly assume that this holds. Also note that Pb,B,T corresponds to the standard Poissonsubsampling without truncation when B = . We use B() to denote the privacy loss curve ofABLQB for any B {D, P, S, S}, where other parameters such as , T, etc. are implicit. Namely,for all > 0, let B() be the smallest 0 such that ABLQB satisfies (, )-DP for all choices ofthe underlying adaptive query method A. We define B() similarly. Finally, we define B(, ) asthe smallest such that ABLQB satisfies (, )-DP, with other parameters being implicit in B.",
  ": Various natural instantiations of the permutation batch sampler": "Adjacency notion.The common notion of Add-Remove adjacency is not applicable for mechanismssuch as ABLQD, ABLQS, ABLQS because these methods require that bT/n and n/b are integers,and changing n by 1 does not respect this requirement. And while the other common notion ofSubstitution is applicable for all the mechanisms we consider, the standard analysis for ABLQPis done w.r.t Add-Remove adjacency [Abadi et al., 2016, Mironov, 2017]. Therefore, we use theZeroing-out adjacency introduced by Kairouz et al. , namely we consider the augmented inputspace X := X {} where any adaptive query method A is extended as A(g1, . . . , gt; ) := 0 forall g1, . . . , gt Rd. Datasets x, x X n are said to be zero-out adjacent if there exists i such thatxi = xi, and exactly one of {xi, xi} is in X and the other is . We use x z x to specificallydenote adjacent datasets with xi X and xi = . Thus x x if either x z x or x z x.",
  "Dominating Pairs": "For two probability density functions P and Q and , R0, we use P + Q to denote theweighted sum of the density functions. We use P Q to denote the product distribution sampled as(1, 2) for 1 P, 2 Q, and, P T to denote the T-fold product distribution P P. Forall R, the e-hockey stick divergence between P and Q is De(PQ) := sup P() eQ().Thus, by definition a mechanism M satisfies (, )-DP iff for all adjacent x x, it holds thatDe(M(x)M(x)) . Definition 2.2 (Dominating Pair [Zhu et al., 2022]). The pair (P, Q) dominates the pair (A, B)(denoted (P, Q) (A, B)) if De(PQ) De(AB) holds for all R. We say that (P, Q)dominates a mechanism M (denoted (P, Q) M) if (P, Q) (M(x), M(x)) for all adjacentx z x. If (P, Q) M, then for all 0, it holds that M() max{De(PQ), De(QP)}, andconversely, if there exists adjacent datasets x z x such that (M(x), M(x)) (P, Q), thenM() max{De(PQ), De(QP)}. When both of these hold, we say that (P, Q) tightlydominates the mechanism M (denoted (P, Q) M) and in this case it holds that M() =max{De(PQ), De(QP)}. Thus, tightly dominating pairs completely characterize the privacyloss of a mechanism (although they are not guaranteed to exist for all mechanisms). Dominatingpairs behave nicely under mechanism compositions: if (P1, Q1) M1 and (P2, Q2) M2, then(P1 P2, Q1 Q2) M1 M2, where M1 M2 denotes the (adaptively) composed mechanism.",
  "and () is the cumulative density function (CDF) of the standard normal random variable N(0, 1)": "Privacy analysis of ABLQP.First, let us consider the case of Poisson subsampling withouttruncation, namely B = . Zhu et al. showed2 that the tightly dominating pair for asingle step of ABLQP, a Poisson sub-sampled Gaussian mechanism, is given by the pair (U =(1 q)N(0, 2) + qN(1, 2), V = N(0, 2)), where q is the sub-sampling probability of eachexample, namely q = b/n. Since ABLQP is a T-fold composition of this Poisson subsampledGaussian mechanism, it follows that (PP := U T , QP := V T ) ABLQP. A finite value of B however changes the mechanism slightly. In order to handle this, we use thefollowing proposition, where dTV(P, P ) denotes the statistical distance between P and P .Proposition 3.2. For distributions P, P , Q, Q such that dTV(P, P ), dTV(Q, Q) , andDe(P Q) , then De(PQ) + (1 + e).",
  "P() max{De(PPQP), De(QPPP)} + T (1 + e) (n, b, B)": "While the hockey stick divergences De(PPQP) and De(QPPP) do not have closed-formexpressions, upper bounds on these can be obtained using privacy accountants based on the methodsof Rnyi DP (RDP) [Mironov, 2017] and privacy loss distributions (PLD) [Meiser and Mohammadi,2018, Sommer et al., 2019]; the latter admits numerically accurate algorithms [Koskela et al.,2020, Gopi et al., 2021, Ghazi et al., 2022, Doroshenko et al., 2022], with multiple open-sourceimplementations [Prediger and Koskela, 2020, Googles DP Library., 2020, Microsoft., 2021]. Note that (n, b, B) can be made arbitrarily small by increasing B, which affects the computationcost. In particular, given a target privacy parameter (, ), we can, for example, work backwards tofirst choose B such that (n, b, B) T (1 + e) 105 , and then choose the noise scale suchthat max{De(PPQP), De(QPPP)} (1105), using aforementioned privacy accountinglibraries. Notice that our use of Proposition 3.2 is likely not the optimal approach to account for thebatch truncation. We do not optimize this further because we find that this approach already providesvery minimal degradation to the choice of for a modest value of B relative to b. A more carefulanalysis could at best result in a slightly smaller B, which we do not consider as significant; seeFigures 3 and 4 for more details. Privacy analysis of ABLQS.Obtaining the exact privacy guarantee for ABLQS has been an openproblem in the literature. Our starting point is the approach introduced by Chua et al. to provea lower bound in the single epoch setting. Let the input space be X = , the (non-adaptive)query method A that produces the query t(x) = x, and consider the adjacent datasets:",
  "Also implicit in prior work [Koskela et al., 2020]": "where fs RT is the sum of basis vectors E1=0 eS+s, and 1 denotes the all-1s vector in RT .Basically, fs is the indicator vector encoding the batches that the differing example gets assignedto; in persistent shuffling, an example gets assigned to the sth batch within each epoch for a randoms {1, . . . , S}. Shifting the distributions by b 1 and projecting to the span of {fs : s [S]} doesnot change the hockey stick divergence De(UV ), hence we might as well consider the pair",
  "PS := Ss=11S N(2es, 2I)andQS := Ss=11S N(es, 2I)": "ABLQS is an E-fold composition of the single-epoch mechanism. Hence by composition of dominat-ing pairs, it follows that S() max{De(PSQS), De(QSPS)}, where PS := P ESand QS:=QES. However, it is tricky to directly identify an event for which the lowerbound PS() eQS() is non-trivial and PS(), QS() are easy to compute. So in order tolower bound De(PSQS), below we construct a pair ( PS, QS) of discrete distributions such that(PS, QS) ( PS, QS) and thus (PS, QS) ( P ES, QES). For probability measures P and Q over a measurable space , and a finite partition3 G =(G1, . . . , Gm) of , we can consider the discrete distributions P G and QG defined over {1, . . . , m}such that P G(i) = P(Gi) and QG(i) = Q(Gi). The post-processing property of DP implies:",
  "Proposition 3.6 (DP Post-processing [Dwork and Roth, 2014]). For all partitions E of , it holdsthat (P, Q) (P E, QE)": "We construct the pair ( PS, QS) by instantiating Proposition 3.6 with the set G={G0, G1, . . . , Gm+1} parameterized by a sequence C1 < < Cm of values defined as follows:G0 := {w RS : maxs ws C1}, Gi := {w RS : Ci < maxs ws Ci+1} for 1 i m andEm+1 := {w RS : Cm < maxs ws}; in other words, G0 = RS C1, Gi = Ci Ci+1 for1 i m and Gm+1 = Cm.",
  "Random subset of two examples is sampled": ": Visualization of the massively parallel computation approach for Poisson subsamplingat scale. Consider 6 records x1, . . . , x6 sub-sampled into 4 batches with a maximum batch size ofB = 2. The Map operation adds a weight parameter of 1 to all examples, and samples indices ofbatches to which each example will belong. The Reduce operation groups by the batch indices. Thefinal Map operation truncates batches with more than B examples (e.g., batches 1 and 3 above), andpads dummy examples with weight 0 in batches with fewer than B examples (e.g., batch 4 above). We use the dp_accounting library [Googles DP Library., 2020] to numerically compute a lowerbound on the quantity above, using PLD. In particular, we choose C1 and Cm such that PS(G0) andPS(Gm+1) are sufficiently small and choose other Cis to get a sufficiently fine discretization of theinterval between C1 and Cm.4 An illustration of these accounting methods is presented in , which demonstrates the signifi-cant gap where the optimal for dynamic/persistent shuffling is significantly larger than compared toPoisson subsampling, even when using an optimistic estimate for shuffling as above. We provide theimplementation of our privacy accounting methods described above in an iPython notebook5 hostedon Google Colab, executable using the freely available Python CPU runtime.",
  "We compare DP-SGD using the following batch sampling algorithms at corresponding noise scales:": "Deterministic batches (using nearly exact value of D(, ) via Theorem 3.1), Truncated Poisson subsampled batches (using upper bound on P(, ) via Theorem 3.3), Persistent shuffled batches (using lower bound on S(, ) via Theorem 3.5), and Dynamic shuffled batches (using lower bound on S(, ) via Theorem 3.7).As a comparison, we also evaluate DP-SGD with dynamic shuffled batches, but using noise that is anupper bound on P(, ) (with no truncation, i.e. B = ), to capture the incorrect, but commonlyemployed approach in practice. Finally, in order to understand the impact of using different batchsampling to model training in isolation, we compare models trained with SGD under truncatedPoisson subsampling, and dynamic and persistent shuffling without any clipping or noise. We usemassively parallel computation (Map-Reduce operations [Dean and Ghemawat, 2004]) to generatebatches with truncated Poisson subsampling in a scalable manner as visualized in ; the details,with a beam pipeline implementation, is provided in Appendix A. 4In our evaluation, we choose C1 and Cm to ensure PS(G0) + PS(Gm+1) e40. We chose other Cis tobe equally spaced in between C1 and Cm with a gap of 2, where is the desired discretization of the PLD.This heuristic choice is guided by the intuition that the privacy loss is approximately linear in maxt xt/2, andthus the chosen gap means that this approximate privacy loss varies by between buckets.5",
  ": AUC (left) and bounds on B values (middle) for = 5, = 2.7 108 and using 1 epoch(top) and 5 epochs (bottom) of training on a linear-log scale; AUC (right) is with non-private training": "We run our experiments on the Criteo Display Ads pCTR Dataset [Jean-Baptiste Tien, 2014], whichcontains around 46 million examples from a week of Criteo ads traffic. Each example has 13 integerfeatures and 26 categorical features, and the objective is to predict the probability of clicking on anad given these features. We use the labeled training set from the dataset, split chronologically into a80% We include more details about the model architectures and training in Appendix B. We run experiments varying the (expected) batch size from 1 024 to 262 144, for both private trainingwith ( = 5, = 2.7 108) and non-private training and with 1 or 5 epochs. We plot the results in. As mostly expected, we observe that the model utility generally improves with smaller. Truncated Poisson subsampling performs similarly to dynamic shuffling for the same value of, although it performs worse for non-private training. The latter could be attributed to the fact thatwhen using truncated Poisson subsampling, a substantial fraction6 of examples are never seen in thetraining with high probability. However, this does not appear to significantly affect the private trainingmodel utility for the range of parameters that we consider, since we observe that truncated Poissonsubsampling behaves similarly to dynamic shuffling with noise scale of P(, ) (the values of forP and S (P accounting) visually overlap in ; the values for P are only negligibly larger,since it accounts for truncation). Truncated Poisson subsampling performs better when comparedto shuffling when the latter using our lower bound on S(, ), which suggests that shuffling withcorrect accounting (that is, with potentially even larger ) would only perform worse. We also run experiments with a fixed batch size 65 536 and varying from 1 to 256, fixing =2.7 108. We plot the results and the corresponding values in . We again observe thatshuffling (with our lower bound accounting) performs worse than truncated Poisson subsampling inthe high privacy (low ) regime, but performs slightly better in the low privacy (high ) regime (thisis because at large the noise required under Poisson subsampling is in fact larger than that undershuffling). Moreover, we observe that shuffling performs similarly to truncated Poisson subsamplingwhen we use similar value of , consistent with our observations from . Finally, as a comparison, we compute the upper bounds on S(, ) via the privacy amplification byshuffling bounds by Feldman et al. . We find that these bounds tend to be vacuous in manyregime of parameters, namely they are no better than D(, ), which is clearly an upper bound onS(, ). Details are provided in Appendix C.",
  "Conclusion": "We provide new lower bounds on the privacy analysis of Adaptive Batch Linear Query mechanisms,under persistent and dynamic shuffling batch samplers, extending the prior work of Chua et al. that analyzed the single epoch case. Our lower bound method continues to identify separations inthe multi-epoch setting, showing that the amplification guarantees due to even dynamic shufflingcan be significantly limited compared to the amplification due to Poisson subsampling in regimes ofpractical interest. We also provide evaluation of DP-SGD with various batch samplers with the corresponding privacyaccounting, and propose an approach for implementing Poisson subsampling at scale using massivelyparallel computation. Our findings suggest that with provable privacy guarantees on model training,Poisson-subsampling-based DP-SGD has better privacy-utility trade-off than shuffling-based DP-SGD in many practical parameter regimes of interest, and in fact, essentially match the utility ofshuffling-based DP-SGD at the same noise level. Thus, we consider Poisson-subsampling-basedDP-SGD as a viable approach for implementing DP-SGD at scale, given the lower bound on theprivacy analysis when using shuffling. Several interesting directions remain to be investigated. Firstly, our technique only provides a lowerbound on the privacy guarantee when using persistent / dynamic shuffled batches. While some privacyamplification results are known [Feldman et al., 2021, 2023], providing a tight (non-vacuous) upperbound on the privacy guarantee in these settings remains an open challenge. This can be important inregimes where shuffling does provide better privacy guarantees than Poisson subsampling. Another important point to note is that persistent and dynamic shuffling are not the only formsof shuffling used in practice. For example, methods such as tf.data.Dataset.shuffle ortorchdata.datapipes.iter.Shuffler provide a uniformly random shuffle, only when the sizeof its buffer is larger than the dataset. Otherwise, for buffer size b, it returns a random recordamong the first b records, and immediately replaces it with the next record ((b + 1)th in this case),and repeats this process, which leads to an asymmetric form of shuffling. Such batch samplers meritmore careful privacy analysis.",
  "Sivakanth Gopi, Yin Tat Lee, and Lukas Wutschitz. Numerical composition of differential privacy. InNeurIPS, pages 1163111642, 2021": "Jiyan He, Xuechen Li, Da Yu, Huishuai Zhang, Janardhan Kulkarni, Yin Tat Lee, Arturs Backurs,Nenghai Yu, and Jiang Bian. Exploring the limits of differentially private deep learning withgroup-wise clipping. arXiv preprint arXiv:2212.01539, 2022. Timour Igamberdiev, Doan Nam Long Vu, Felix Knnecke, Zhuo Yu, Jannik Holmer, and Ivan Haber-nal. DP-NMT: Scalable differentially-private machine translation. In EACL (Demonstrations),pages 94105, 2024.",
  "AMassively Parallel Implementation of Truncated Poisson Subsampling": "We use massively parallel computation to generate batches with truncated Poisson subsampling in ascalable manner. Given the input parameters b, B, T, and n, we first compute the maximum batchsize B such that (n, b, B) T (1 + e) 105 . For each example in the input dataset, wegenerate a list of batches that the example would be in when sampled using Poisson subsampling.While a naive implementation would sample T Bernoulli random variables with parameter b/n,this can be made efficient by sampling the indices of the batches containing the examples directly,since the difference between two consecutive such indices is distributed as a geometric randomvariable with parameter b/n. We then group the examples by the batches, and subsample each batchuniformly, without replacement, to obtain a batch of size at most B. For batches with size smallerthan B, we pad the batch with examples such that every batch has size B. In order to differentiate thepadding examples from the non-padding examples, we add a weight to all the examples, where thenon-padding examples have weight 1 and the padding examples have weight 0. During the training,we use a weighted loss function using these weights, such that the padding examples do not have anyeffect on the training loss. We include a code snippet for implementing truncated Poisson subsampling using massively par-allel computation. This is written using Apache beam [Apache Beam] in Python, which can beimplemented on distributed platforms such as Apache Flink, Apache Spark, Google Cloud Dataflow.",
  "def _add_weights(example):": "# Add weight with value 1 to indicate non-padding examplesweighted_example = tf.train.Example()weighted_example.CopyFrom(example)weighted_example.features.feature['weight'].float_list.value[:] = [1.0]return weighted_example # Group elements into batches keyed by the batch idgrouped_pcoll = (pcoll| 'Add weights' >> beam.Map(_add_weights)| 'Key by batch id' >> beam.FlatMap(generate_batch_ids)| 'Sample up to max_batch_size elements per batch'>> beam.combiners.Sample.FixedSizePerKey(self._max_batch_size)| 'Add padding' >> beam.Map(self._add_padding))return grouped_pcoll",
  "BTraining details": "We use a neural network with five layers and 78M parameters as the model. The first layer consistsof feature transforms for each of the categorical and integer features. Categorical features are mappedinto dense feature vectors using an embedding layer, where the embedding dimensions are fixed at 48.We apply a log transform for the remaining integer features, and concatenate all the features together.The next three layers are fully connected layers with 598 hidden units each and a ReLU activationfunction. The last layer consists of a fully connected layer which gives a scalar logit prediction. We use the Adam or Adagrad optimizer with a base learning rate in {0.0001, 0.0005, 0.001, 0.005,0.01, 0.05, 0.1, 0.5, 1}, which is scaled with a cosine decay, and we tune the norm boundC {1, 5, 10, 50}.For the experiments with varying batch sizes, we use batch sizes that are powers of 2 between 1 024 and 262 144, with corresponding maximum batch sizes B in{1 328, 2 469, 4 681, 9 007, 17 520, 34 355, 67 754, 134 172, 266 475}.For the experiments withvarying , we vary as powers of 2 between 1 and 256, with batch size 65 536 and correspondingmaximum batch sizes B in {67 642, 67 667, 67 725, 67 841, 68 059, 68 449, 69 106, 70 156, 71 760}.With these choices of the maximum batch sizes, the values for truncated Poisson subsampling areonly slightly larger than without truncation, as we observe from the nearly overlapping curves in and . The training is done using NVIDIA Tesla P100 GPUs, where each epoch oftraining takes 1-2 hours on a single GPU.",
  "CPrivacy Amplification by Shuffling": "We evaluate the upper bounds on S(, ) via privacy amplification by shuffling results of Feldmanet al. , as applied in the context of our experiments in . In particular, their Proposition5.3 states that if the unamplified (Gaussian) mechanism satisfies (0, 0)-DP, then ABLQS with Tsteps (in a single epoch setting) will satisfy (, + O(e0n)-DP for any and",
  "They also obtain a tighter numerical bound on with an implementation provided in a GitHubrepository.7": "We evaluate their bounds in an optimistic manner. Namely, for a given value of , we computean optimistic estimate on compared to the bound above by setting 0 = /n and setting 0 =D(0), and set = (note that this is optimistic because the above proposition requires setting = + O(e0n), which is larger than the we are claiming). We use the numerical analysismethod provided in the library by Feldman et al. to compute S (via a binary search on topof their method to compute an upper bound on S), and plot it in , along with the lowerbound on S as obtained by Chua et al. , as well as D. We find that the bounds by Feldmanet al. are vacuous for batch sizes 2 048 and above, in that they are even larger than the boundswithout any amplification."
}