{
  "Abstract": "Many important datasets contain samples that are missing one or more featurevalues. Maintaining the interpretability of machine learning models in the pres-ence of such missing data is challenging. Singly or multiply imputing missingvalues complicates the models mapping from features to labels. On the otherhand, reasoning on indicator variables that represent missingness introduces apotentially large number of additional terms, sacrificing sparsity. We solve theseproblems with M-GAM, a sparse, generalized, additive modeling approach thatincorporates missingness indicators and their interaction terms while maintainingsparsity through 0 regularization. We show that M-GAM provides similar orsuperior accuracy to prior methods while significantly improving sparsity relativeto either imputation or nave inclusion of indicator variables.",
  "Introduction": "Interpretability is essential for a wide range of machine learning applications Rudin et al. (2022).Missing data pose a challenge to interpretability, because many simple models (e.g., linear models)are not well-defined when data are missing. This raises the question: how can interpretability bemaintained for datasets with missing values? We introduce an interpretable model class, M-GAM, that extends Generalized Additive Models(GAMs) to handle missing data. GAMs take the form of a linear combination of univariate componentfunctions, with one function corresponding to each feature; this univariate nature is the core reason fortheir interpretability (Rudin et al., 2022). We introduce two sets of boolean variables for each feature.The first consists of missingness indicators that identify which features have missing values. Thesecond consists of missingness adjustment terms that adjust the shape curves for other features foreach missing features in a sample. This maintains our ability to view a GAM as a sum of univariateshape functions even when modeling interactions with missing data. As such, an M-GAM is muchsimpler to interpret than a GAM built on imputed data, since it avoids creating multivariate featuresas happens when imputing features from multiple others. This is illustrated in .",
  "Related Work": "Missing data is a well studied problem in statistics. Traditionally, mechanisms by which data can bemissing are organized into three categories: missing completely at random (MCAR), where missing-ness is independent of the value of all covariates; missing at random (MAR), where missingness in avariable X1 is conditionally independent of the value of X1 given all other variables; and missing notat random (MNAR), where missingness may depend on any variable (Little & Rubin, 2019). For supervised learning, there are two common approaches to dealing with missing data: impute-then-predict, in which a standard machine learning model is fit on top of imputed data and used forprediction, and incorporating missing data handling directly in the predictive model. A broad body of work studies imputation, particularly in the MAR setting for a more thoroughreview, see Shadbahr et al. (2023). Imputation methods can broadly be sorted into single imputationmethods (see Van Buuren, 2018, for a review of such methods), where each missing value isimputed once, and multiple imputation (Rubin, 1988; Van Buuren & Oudshoorn, 1999; Schafer &",
  "Dependent": "MissingnessDependent Variable 1Variable 2Variable 3 Variable 4Variable 5Variable 6Variable 7 -0.4 0.8 -0.4 0.8 -0.4 0.8 040007.501500100 : A generalized additive model (GAM) for the Explainable ML Challenge data from FICOet al. (2018) with missingness incorporated. This model handles missingness interpretably byexplicitly providing alternative shape functions when a variable is missing. For example, in thismodel the shape function for variable 2 is adjusted when variable 3 is missing, and the shape functionfor variable 3 is removed. This model achieves comparable performance to convoluted black boxapproaches (such as random forests and/or MICE), but provides global interpretability (the entiremodel can easily be inspected) and local interpretability (the shape functions applied for a givensample can be easily visualized). An expanded version of this figure with variable names can be foundin Appendix . Shape functions in the right section are shared across all missing variablecombinations. The type of missingness is indicated in parentheses next to the missing variable.Section F visualizes additional M-GAMs. Graham, 2002; Stekhoven & Buhlmann, 2012; Mattei & Frellsen, 2019), where many alternativeimputations are provided for each missing value. Multiple imputation is convenient because itintegrates uncertainty into its imputations by providing a range of alternatives (Van Buuren, 2018). Recent approaches directly incorporate missing data in the predictive model. Le Morvan et al. (2020)showed that, even when the target of prediction is a linear function, in the presence of missing data, theoptimal model need not be linear in the original features. Rather, their optimal model was linear in theobserved data and interactions between indicators for missing data and the observed data. Van Nesset al. (2023) showed that when missingness contains information about an outcome, linear modelsthat directly include missingness indicators outperform models excluding this information. Thisinclusion of missingness indicators is especially recommended in the practical setting of predictivemodeling, rather than the setting of statistical inference where MCAR, MAR and MNAR conceptsare more commonly used (Sperrin et al., 2020). There are also a wide variety of ad-hoc methods for handling missing data in tree-based models(Kapelner & Bleich, 2015; Twala et al., 2008; Beaulac & Rosenthal, 2020; Therneau et al., 1997)and boosting models (Wang & Feng, 2010; Chen & Guestrin, 2016) that involve prediction withoutexplicit imputation. Beaulac & Rosenthal (2020) learn decision trees which avoid splitting on missingdata when missingness follows a deterministic structure based on other known features. This sidestepsany need to query features when data is missing, but does not generalize to settings with less structuredmissingness. More generally, tree-based models can learn a branch direction for each split to usewhen the queried feature is unknown. This effectively imputes the response to the query, but keepsthe model itself simple. This is the method used in XGBoost (Chen & Guestrin, 2016) and SKLearnsdecision tree classifier (Pedregosa et al., 2011). Twala et al. (2008) and Kapelner & Bleich (2015)additionally incorporate the option to split on missingness itself, effectively encoding missingness asa value. Ding & Simonoff (2010) provides empirical support for incorporating this option to treatmissingness as a value. We compare to XGBoost and SKLearn in our experimental section, findingthat M-GAM better balances interpretability and performance, even when we allow missingness indicator splits like in Twala et al. (2008); Kapelner & Bleich (2015); Ding & Simonoff (2010); Wang& Feng (2010). Most similarly to our own approach, Therneau et al. (1997) and Breiman (2017)discuss an alternative approach of surrogate splits: when a feature that is split on is missing, a setof other splits is used in place of the missing feature to evaluate the split. This practice of adjustingwhich features are used when one feature is missing bears some similarities to our use of missingnessinteraction splits that adjust the shape functions for some features when other features are missing,though these surrogate splits do not optimize for sparsity like M-GAM, and underperform morestandard multiple imputation approaches Valdiviezo & Van Aelst (2015); Feelders (1999). Furtherwork explores the idea of developing distinct models for use under different cases of missing features(Fletcher Mercaldo & Blume, 2020; Stempfle et al., 2023) or developing additive logical models withdisjunctions, such that reliance on imputed values is low Stempfle & Johansson (2024). It is critical to note that, for a dataset with d features, adding indicators for missing data results in 2dfeatures, and adding first order interactions between features and missingness results in d(d 1) + 2dfeatures. As such, without careful regularization, these models that explicitly handle missingness arecomplex and uninterpretable. This poses a challenge for their application in high stakes domains suchas justice and medicine, where there have been calls to enshrine interpretability as a requirement forthe use of machine learning methods (US Food and Drug Administration, 2021; European Commision,2021). In contrast, M-GAM provides sparse, transparent models that handle missingness indicatorsand interactions by extending sparse generalized additive models (Liu et al., 2022). M-GAM providesan expressive model class for handling missingness while controlling the exploding number ofmissingness interaction terms through 0 regularization.",
  "Methodology": "We denote a dataset of n samples by D = (X, y) = {(xi, yi)}ni=1, where xi (R {NA})d is ad-dimensional vector of features, NA denotes a missing entry, and yi {0, 1} is our target label.We use xi,j to denote the j-th feature of the i-th sample. We use bold capital letters (X) to denotematrices, bold lowercase letters to denote vectors (xi), capital letters to denote random variables (X),and lowercase letters to represent scalars (xi,j). denotes noise; any other Greek characters denotemodel parameters. We encode all binary comparisons to the value NA as 0. That is, we follow theconvention that 1[NAa] = 0 for any value a R, where 1[] denotes the indicator function. Note that, in practice, data are often missing for distinct yet identifiable reasons; for example, ameasurement for one sample may be missing because it was never taken, while another may bemissing because a researcher spilled coffee on the notes containing the data. As such, we explicitlyconsider distinct reasons for missing data. For a dataset with c N potential reasons for data tobe missing, define the mapping mcat : R {NA} {0, 1, . . . , c} to map from an entry of X to anatural number indicating the reason that entry is missing (0 if the entry is not missing). With notation established, we begin with a motivational proposition. Proposition 3.1 states that evenif we can perfectly impute missing values, we may find greater predictive power by using missingnessitself as a feature rather than by imputing missing values. Proposition 3.1. Let I : (R NA)d Rd be an oracle imputation function that replaces allmissing values in a vector with the correct non-missing entry. For a random variable X Rd, letf1(X) := 1[E[Y |I(X)]>0.5] be the Bayes optimal model using perfectly imputed data and f2(X) :=1[E[Y |X]>0.5] be the Bayes optimal model using missingness as a value. There exist data generatingprocesses for X and Y where P(Y = f1(X)) < P(Y = f2(X)). Section A of the appendix provides a proof by construction for Proposition 3.1. The key insightbehind Proposition 3.1 is that, when missingness is dependent on the label Y, missingness itself canbe a powerful predictor of the label (this setting is called informative missingness in Van Ness et al.,2023). In particular, we can gain information about our label that is not available in other covariates(e.g., information from 1). Proposition 3.1 may appear to conflict with Theorem 3.1 of Le Morvan et al. (2021), which statesthat a Bayes optimal model may be produced using impute-then-predict with almost any imputationmodel. This theorem hinges on the idea that, for most imputations, it is still possible to distinguishimputed data entries from non-missing entries. This is not the case for perfect imputation, whichyields Corollary 3.2. Corollary 3.2. Let R(f, X, y) denote the risk of a model f for data X, y, and R the optimal risk.Let I : (R NA)d Rd denote the oracle imputation function of Proposition 3.1. Under perfectimputation, it is possible for there to be no Bayes optimal model built on imputed data. That is,",
  "(X, y) [f : R(f I, X, y) = R]": "Corollary 3.2 states that perfectly imputing missing data can reduce the best possible performance ofa predictive model. This has substantial implications for how imputation is understood for prediction:if perfect imputation is achieved, then impute-then-predict models sacrifice expressiveness. Ifimputation is optimized to maintain the downstream performance of impute-then-predict models, theimputed data loses some of its meaning since it is no longer our best guess of the missing datasvalue, as we need to deliberately avoid perfect imputation to guarantee that we maintain performance. Motivated by Proposition 3.1 and Corollary 3.2, we are interested in constructing predictive modelsthat explicitly use missingness as a value in their prediction rather than imputing first. More generally,we may also consider using the indicator for each type of missing data directly in a prediction.Generalized additive models (GAMs) provide a natural choice for such a model.",
  "j=1fj(xi,j; j).(1)": "In practice, it is common for each shape function to be a linear combination of different thresholds onits input variable, i.e., fj(xi,j; j) = len(tj)k=1j,k1[xi,jtj,k], where len(tj) N is the number ofthresholds applied to variable j, each tj,k R is a threshold value, and each j,k R is a learnedweight. These functions provide a convenient framework for considering missing values. In particular, we canform a new shape function hj(xi,j; j, missingj) that explicitly handles missing data by introducingadditional missingness indicator terms, such that our shape functions take the form",
  "m=1missj,m1[mcat(xi,j)=m],": "where missj Rc is an additional vector of parameters. Recall that there are c distinct reasons formissingness, with mcat(xi,j) = 0 if xi,j is not missing and mcat(xi,j) = m if xi,j is missing forthe m-th reason. We may further extend this augmentation to include interaction terms between missingness indicatorsand standard threshold functions. The missingness interaction function between feature j andfeature j takes the form",
  "m=1missj,m1[mcat(xi,j)=m]": "These augmentation terms are quite powerful. Theorem 3.4 shows that, for any impute-then-predictapproach using an affine imputer and a GAM predictor, we can construct an M-GAM that recoversthe expected classification score over imputations.Theorem 3.4. Consider any GAM g : Rd R, parameterized by , with shape functions defined aslinear combinations over boolean features (either thresholds fj(xi,j; j) = len(tj)k=1j,k1[xi,jtj,k]or a feature that was originally boolean). Suppose some observations are missing boolean feature b,and that this feature is imputed such that the modeled probability of xi,b being true, P(xi,b = 1|xi,b)(where xi,b refers to all covariates except b) is an affine function h : xi,b . For anyparameterization of a GAM g, let E[g(xi; )] := P(xi,b = 1|xi,b)g(x(b+)i; ) + P(xi,b =0|xi,b)g(x(b)i; ), where x(b+)idenotes xi with xi,b = 1 and x(b)idenotes xi with xi,b = 0. Then,there exists a model in the model class M-GAM (which does not use imputations), that recovers thisscore E[g(xi; )] for all i. More broadly, Theorem 3.4 suggests that M-GAM is able to express scores comparable to those ofany impute-then-predict GAM, if the imputation probabilities can be approximated by an additivemodel. One advantage is that M-GAM can be optimized directly for classification performance rather than first optimizing an imputation step to recover missing values, and then optimizinga model on the imputed data. Together, Proposition 3.1 and Theorem 3.4 show that M-GAM iscomparable to impute-then-predict in a broad range of settings and that M-GAM is strictly betterthan impute-then-predict in some settings. Appendix C contains the proof for Theorem 3.4.",
  "c dj": "j=j len(tj)+dj(len(tj))+cd+1 coefficients (or, d dj(len(tj))+d+1 when c = 1).This increases the risk of overfitting and may lead to complex, uninterpretable models. The sameproblem arises when adding similar interaction terms to a linear model, as diagnosed by Van Nesset al. (2023), who propose a hypothesis testing style framework for variable selection. Notably, thisframework does not explicitly encourage sparsity it only discourages overfitting. Rather than applying a pre-processing step for variable selection, we use 0 regularization, which wecan optimize directly alongside accuracy. This encourages the model coefficients to be 0, resultingin sparse models despite the potentially large number of input features. We optimize classificationperformance using the exponential loss, as it yields faster convergence rates than logistic loss duringoptimization (Liu et al., 2022). Thus, our goal is to solve the following optimization problem:",
  "We now evaluate the performance, runtime, and sparsity of M-GAM in comparison to other methods.To evaluate M-GAM in a realistic setting, we require datasets with some missing entries. We primarily": "consider four datasets: the Explainable Machine Learning Challenge dataset (FICO et al., 2018)(referred to as FICO), a breast cancer dataset introduced by Razavi et al. (2018) (referred to asBreast Cancer), the MIMIC-III critical care dataset (Johnson et al., 2016) (referred to as MIMIC),and a dataset concerning the prediction of pharyngitis introduced by Miyagi (2023) (referred to asPharyngitis). FICO contains 10,459 individuals, measuring 23 predictor variables used to predictwhether each individual will repay a line of credit within 2 years. FICO contains three distinctencodings for missingness: -7, indicating no information of a given type is available, -8, indicatingthere was no usable information, and -9, indicating that a credit bureau report was not investigatedor not found. Breast Cancer measures 27 features for 1,756 patients, MIMIC measures 49 featuresfor 30,238 patients, and Pharyngitis measures 19 features for 676 patients. We use AUC, ratherthan accuracy, when evaluating model performance for Breast Cancer and MIMIC because thesetwo datasets are heavily imbalanced. Breast Cancer, MIMIC, and Pharyngitis contain only onemissingness encoding. Two additional datasets are studied in Section E of the appendix. Each dataset contains missing entries. Because these are real datasets, we do not know the exactmechanism(s) (i.e., MCAR, MAR, or MNAR) by which data are missing. These datasets allow us toevaluate M-GAM on data with (.1) and without (.4) added MAR missingness. We then study the interpretability/accuracy tradeoff for M-GAM using sparsity versus accuracy plots(.2) and evaluate the runtime of M-GAM (.3). We use multivariate imputation bychained equations (MICE) (Van Buuren & Oudshoorn, 1999), MIWAE(Mattei & Frellsen, 2019), andMissForest(Stekhoven & Buhlmann, 2012) as multiple imputation baselines. We compare M-GAM to a variety of standard machine learning models used in an impute-then-predictframework. We further compare to standard machine learning models used with both the missingnessaugmentation described by Van Ness et al. (2023) and imputation. Section D.3 of the appendixcontains full experimental details.",
  "M-GAM Provides Superior Performance Given Informative MAR Missingness": "To demonstrate the added expressive capability of our model relative to impute-then-predict models,we created versions of each dataset with added synthetic missingness. Missingness is added to anarbitrary column of the data according to an MAR mechanism, where missingness is dependent onthe outcome Y and one other randomly chosen predictor variable. We encode this added missingnessas a new value distinct from the value(s) used to indicate missingness in the original dataset. Aconditional probability table for this synthetic missingness is provided in Appendix Section D.1. This adjustment falls under the MAR setting where imputation is often suggested. Nevertheless,as shown in , M-GAM with interactions provides much greater accuracy than imputation,because the missingness depends on the outcome. The gain in performance due to consideringinteraction terms grows larger with increasing MAR missingness (from left to right). Note that missingness depending on the outcome of interest is realistic. For example, individualswho are unlikely to have a particular disease are unlikely to receive medical tests related to thatdisease. One theory about why polls were wrong before the 2016 US presidential election was thatnon-response bias was associated with less education and distrust in the media, both predictors ofvotes for Donald Trump (Kurtzleben, 2016).",
  "The most important benefit of the missingness handling in M-GAM is that it enables simple, sparsemodels. As such, we show the tradeoff between complexity and performance for M-GAM": "demonstrates the sparsity-accuracy trade-off for M-GAM relative to a GAM fit using ScikitLearns logistic regression package (Pedregosa et al., 2011) over binarized features, trained on datafrom 10 imputations using a multiple imputation method. We also contrast two different levelsof missing variable parameterization: the full set of indicators and interactions versus using justindicators. We quantify interpretability using the number of nonzero coefficients selected by M-GAM, sincea large number of non-zero coefficients leads to a dense, complicated mapping from the inputdata to a prediction. Meanwhile, running impute-then-predict is not interpretable: the method",
  "# Nonzero Coecients": ": Sparsity of M-GAM when synthetic MAR missingness is added to up to 25% (left column)and 50% (right column) of entries in FICO (top row) and Breast Cancer (bottom row). We compareto several alternatives for GAMs with missing data: ensembling 10 GAMs fit on multiple imputation(for MIWAE, MICE, and MissForest), 0-value imputation (GAM), mean-value imputation (GAMw/ MVI), and selective addition of missingness indicators (SMIM). The number of non-zerocoefficients for multiple imputation cannot be evaluated because the models depend on both theGAM coefficients and the underlying imputation mechanisms, resulting in high dimensional shapefunctions as in . Error bars report standard error over 10 train-test splits. requires ensembling many different GAMs, and the imputations themselves introduce complicatedrelationships between the raw data and the classifications, similar to what was illustrated in . We show that with fewer than 40 total coefficients (including all step functions for all variables),M-GAM can achieve accuracy comparable to that of GAMs with multiple imputation. On FICO,M-GAM using just 20 non-zero coefficients achieves superior accuracy to a variety of dense,complicated alternatives.",
  "M-GAM is Faster than Impute-then-Predict": "We next turn to a runtime comparison between the impute-then-predict framework and M-GAM. Forimpute-then-predict models, we first imputed 10 datasets and recorded the time required to do so.We then fit a predictive model on each imputed training dataset and recorded the total time required.We recorded the time required to fit an M-GAM with missingness indicators and an M-GAM withmissingness interactions for comparison. This was repeated for each of ten distinct train-test splits ofthe original dataset. shows the runtime of our approach relative to each impute-then-predict baseline, as well asdecision trees and random forests without imputation. M-GAM consistently produces models at leastan order of magnitude more quickly than impute-then-predict with any non-trivial imputation. Whiledecision trees without imputation tend to be produced faster than M-GAM, they tend to have loweraccuracy than M-GAM as discussed in the next section. We repeat this experiment for four distinctsubsamples of each dataset (1/4, 1/2, 3/4, and all of the data) to study how each method scales in thenumber of samples in Appendix E.3.",
  "Pharyngitis": ": Runtime of different methods on Breast Cancer, FICO, MIMIC, and Pharyngitis. Foreach imputation method, we report the total time required to impute missing data and fit the bestperforming impute-then-predict classifier for that dataset and imputation method. M-GAM (Ind) isan M-GAM with indicators and M-GAM (Int) is an M-GAM with indicators and interaction terms.Error bars report standard error of total runtime over 10 train-test splits.",
  "M-GAM is as Accurate as Impute-then-Predict on Real Data": "While M-GAM outperforms imputation on semi-synthetic data, there is a risk that this comes atthe cost of performance on real data. To evaluate whether this is the case, we used several multipleimputation methods to impute 10 distinct datasets for each setting, then fit a variety of predictivemodels on these datasets. We used cross validation to select hyperparameters separately for eachimputed training dataset and ensembled the resulting 10 models for each model class to produce asingle predictive model for each model class. We repeated this procedure for ten distinct train-testsplits for each dataset considered. shows the test accuracy of each model. We find that, on real datasets, no alternativemethod substantially outperforms M-GAM. This suggests that M-GAM does not harm predictiveperformance on real datasets, while providing substantial benefits in interpretability (.2) andsuperior power under informative missingness (.1). : Box-and-whiskers plots comparing test performance of baseline models to M-GAM onfour datasets over ten train-test splits. All methods except M-GAMs, DecisionTree No Imputation,and RandomForest No Imputation impute ten datasets using MICE then ensemble the models fit oneach dataset.",
  "Conclusion": "We introduced M-GAM, a framework for producing accurate, sparse GAMs in the presence ofmissing data. We demonstrated that M-GAM achieves comparable accuracy to impute-then-predicton real datasets and superior accuracy under informative synthetic missingness. M-GAM producesmodels substantially more quickly than impute-then-predict models with multiple imputation, andprovides simple, transparent reasoning on missing data. While the 0 penalty in M-GAM encourages sparsity, it is limited in that the 0 regularization isapplied uniformly across all coefficients. Consider the case when we are adding interaction terms tohandle missingness. We might encourage the model to rely on features it is already using to predict y,rather than using new features. It may be more effective regularization and more interpretable tohave a reduced 0 penalty for such cases. Future work should investigate applying distinct levels ofregularization to observed variables, missingness indicators, and missingness interactions when avariable is already included in the model. An important caveat to models that reason on missing features is that missingness can be especiallyvulnerable to distribution shift, particularly in a medical domain (Sperrin et al., 2020; Groenwold,2020). The interpretability enabled by M-GAM is crucial in allowing models to be closely monitoredand adjusted in the presence of potential distribution shift. Future work could more thoroughlyinvestigate potential distribution shift and ways to adjust a model which reasons on missing data.",
  "Acknowledgements": "We acknowledge funding from the National Institutes of Health under 5R01-DA054994, the NationalScience Foundation under grant HRD-2222336 and the Department of Energy under grant DE-SC002135. Additionally, this material is based upon work supported by the National ScienceFoundation Graduate Research Fellowship under Grant No. DGE 2139754. Finally, we thankJiachang Liu for his helpful advice.",
  "Jeanselme, V., De-Arteaga, M., Zhang, Z., Barrett, J., and Tom, B. Imputation Strategies UnderClinical Presence: Impact on Algorithmic Fairness. In Machine Learning for Health, pp. 1234.PMLR, 2022": "Johnson, A. E., Pollard, T. J., Shen, L., Lehman, L.-w. H., Feng, M., Ghassemi, M., Moody, B.,Szolovits, P., Anthony Celi, L., and Mark, R. G. MIMIC-III, a freely accessible critical caredatabase. Scientific data, 3(1):19, 2016. Johnson, A. E. W., Stone, D. J., Celi, L. A., and Pollard, T. J. The mimic code repository: enablingreproducibility in critical care research. Journal of the American Medical Informatics Association,25(1):3239, 2018.",
  "Miyagi, Y. Identifying Group A Streptococcal Pharyngitis in Children Through Clinical VariablesUsing Machine Learning. Cureus, 15(4), 2023": "Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M.,Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M.,Perrot, M., and Duchesnay, E. Scikit-learn: Machine learning in Python. Journal of MachineLearning Research, 12:28252830, 2011. Razavi, P., Chang, M. T., Xu, G., Bandlamudi, C., Ross, D. S., Vasan, N., Cai, Y., Bielski, C. M.,Donoghue, M. T., Jonsson, P., et al. The Genomic Landscape of Endocrine-Resistant AdvancedBreast Cancers. Cancer cell, 34(3):427438, 2018.",
  "Schafer, J. L. and Graham, J. W. Missing data: Our view of the state of the art. Psychologicalmethods, 7(2):147, 2002": "Shadbahr, T., Roberts, M., Stanczuk, J., Gilbey, J., Teare, P., Dittmer, S., Thorpe, M., Torne, R. V.,Sala, E., Lio, P., et al. The impact of imputation quality on machine learning classifiers for datasetswith missing values. Communications Medicine, 3(1):139, 2023. Sperrin, M., Martin, G. P., Sisk, R., and Peek, N. Missing data should be handled differently forprediction than for description or causal explanation. Journal of clinical epidemiology, 125:183187, 2020.",
  "Stekhoven, D. J. and Buhlmann, P. Missforestnon-parametric missing value imputation for mixed-type data. Bioinformatics, 28(1):112118, 2012": "Stempfle, L. and Johansson, F. Minty: Rule-based models that minimize the need for imputingfeatures with missing values. In International Conference on Artificial Intelligence and Statistics,pp. 964972. PMLR, 2024. Stempfle, L., Panahi, A., and Johansson, F. D. Sharing pattern submodels for prediction withmissing values. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pp.98829890, 2023.",
  "Zhu, C. Q., Tian, M., Semenova, L., Liu, J., Xu, J., Scarpa, J., and Rudin, C. Fast and interpretablemortality risk scores for critical care patients, 2023": ": The case constructed to prove Proposition A.1. Model f1 takes as input variables in bluecircles, f2 takes as input variables in red circles, and dashed red circles indicate variables involved ingenerating missingness in X1, denoted by M. Here, 1 and 2 are unmeasured noise. By using M asan input, f2 can infer information about Y after noise from 1 is considered.",
  "AProof of Proposition 3.1": "First, recall Proposition 3.1:Proposition A.1. Let I : Rd Rd be an oracle imputation function that replaces all miss-ing values in a vector with the correct non-missing entry.For a random variable X Rd,let f1(X) := 1[E[Y |I(X)]>0.5] be the Bayes optimal model using perfectly imputed data andf2(X) := 1[E[Y |X]>0.5] be the Bayes optimal model using missingness as a value. There existdata generating processes for X and Y where P(Y = f1(X)) < P(Y = f2(X)).",
  "Proof. We prove Proposition 3.1 by construction": "Let X1, X2 Bernoulli(p = 0.5) and Y := |X1X2 1| where 1 Bernoulli(p = k1) isunobserved noise. Let M denote missingness in X1, where M := |Y 2| with 2 Bernoulli(p =k2) being unobserved noise. Let k2 < k1 < 0.5. Consider two oracle models f1 and f2, defined as:f1(X1, X2) := 1[E[Y |I(X)]0.5] = 1[E[Y |X1,X2]0.5]f2(M, X2) := 1[E[Y |X]0.5] = 1[E[Y |M,X2]0.5].Intuitively, f1 perfectly imputes X1 when X1 is missing, then predicts Y using X1 and X2, while f2predicts Y using M and X2.",
  "x1,x2P(f1(x1, x2) = Y |x1, x2)": "Noting that, when X1 = X2 = 1, we have Y = |1 1|, so P(Y = 1|X1 = 1, X2 = 1) = P(1 =0) = 1 k1. When at least one of X1 and X2 is 0, we have Y = |0 1|, so P(Y = 1|X1X2 =0) = P(1 = 1) = k1. Since f1 simply evaluates whether the expectation of Y given X1 and X2 isgreater than 0.5, if k1 < 0.5, we have f1(1, 1) = 1 and f1(0, 1) = f1(1, 0) = f1(0, 0) = 0. Thus,the expected accuracy is:",
  "= 0.5(1 k1 k2 + 2k1k2 + k2 + k1 2k1k2 + 0.5 + 0.5)(1 k2)= 1k2": "Thus, we have P(f1(X1, X2) = Y ) = 1 k1 and P(f2(M, X2) = Y ) 1 k2. Since k2 < k1, wehavek2 < k11 k2 > 1 k1P(f2(M, X2) = Y ) > P(f1(X1, X2) = Y ),as required, where the last step follows because P(f1(X1, X2) = Y ) 1 k2.",
  "A.1Alternative Proof": "While the above offers a proof of Proposition 3.1, one might wonder whether the proposition holdsoutside of the case we constructed. In the previous proof, we aimed for a case that resulted in an easyto follow proof. Here, we prove Proposition 3.1 by a second construction, to show that imputationcan be worse than missingness-as-a-value even if there is more noise in the informative missingnessthan in the data itself, and even when missingness is relatively rare.",
  "=1528": "When M = 0, we improve our classifiers accuracy by:P(Imputation model is wrong, model with missingness is right and M = 0) P(Imputation model is right, model with missingness is wrong and M = 0)=P(X1X2 = Y = X1X2X3, M = 0) P(X1X2 = Y = X1X2X3, M = 0)=P(X1X2 = 1, X3 = 0, Y = 0, M = 0) P(X1X2 = 1, X3 = 0, Y = 1, M = 0)=P(X1X2 = 1)P(Y = 0|X1X2 = 1)P(X3 = 0|Y = 0)P(M = 0|Y = 0) P(X1X2 = 1)P(Y = 1|X1X2 = 1)P(X3 = 0|Y = 1)P(M = 0|Y = 1)",
  "BProof of Corollary 3.2": "Corollary B.1. Let R(f, X, Y) denote the risk of a model f for data X, Y, and R the optimal risk.Let I : (R NA)d Rd denote the oracle imputation function of Proposition 3.1. Under perfectimputation, it is possible for there to be no Bayes optimal model built on imputed data. That is,",
  "Theorem 3.4 states:": "Theorem C.1. Consider any GAM g : Rd R, parameterized by , with shape functions defined aslinear combinations over boolean features (either thresholds fj(xi,j; j) = len(tj)k=1j,k1[xi,jtj,k]or a feature that was originally boolean). Suppose some observations are missing boolean featureb, and that this feature is imputed such that the modeled probability of xi,b being true, P(xi,b =1|xi,b) (where xi,b refers to all covariates except b) is an affine function h : xi,b .For any parameterization of g, let E[g(xi; )] := P(xi,b = 1|xi,b)g(x(b+)i; ) + P(xi,b =0|xi,b)g(x(b)i; ), where x(b+)idenotes xi with xi,b = 1 and x(b)idenotes xi with xi,b = 0. Then,there exists a model in the model class M-GAM (which does not use imputations), that recovers thisscore E[g(xi; )] for all i.",
  "+ b1[xi,b]": "For examples xi with feature b missing, the value 1[xi,b] must be imputed prior to being used. Underthe setting of this proof, the feature b is imputed by modeling the probability as an affine functionof an additive score. Without loss of generality, that means there exists some additive model withscore: s(xi,b) = C0 +",
  "+ b1[xi,b] + b1[mcat(xi,b)=0]P(xi,b = 1|xi,b)": "2Note that it is sufficient to show that this this theorem holds for a single missingness reason, m = 1.Showing that this theorem holds for M-GAM in the less expressive case where there is only a single reason formissingness, also shows that the theorem holds when there is added expressiveness to the model class. As such,we do not include notation for distinct missingness reasons in the proof.",
  "D.1Conditional Probability Table for Added MAR Missingness": "In .1, we evaluated the predictive power of M-GAM when additional synthetic missingnesswas added to each dataset we studied. Let X1 denote the variable missingness is being added to, X2another column from the dataset used to determine whether to add missingness, and Y our targetoutcome. Let M denote whether synthetic missingness is added to X1, and let QX2(p) denote thep-th quantile of X2. For a target missing rate r, Table D.1 shows the conditional probability of X1being missing given each value of Y and X2. Note that missingness does not depend on X1, makingthis an MAR setting.",
  "D.3Detailed Experimental Setup": "For every GAM we fit (M-GAM, FastSparse, and non-L0 GAMs), we created an indicator for eachof 8 quantiles (the 0.125 quantile, the 0.25 quantile, and so on). On FICO, we include missingnessindicator and interaction terms as appropriate for four encodings of missingness: the three types inthe original dataset (no information, no usable information, and no report available) and an addedindicator which is true for any type of missingness. All other datasets contain only one type ofmissingness. We fit all M-GAMs using FastSparse (Liu et al., 2022), and in all cases set the max support sizevariable to 100. This prevents the algorithm from exploring models with greater than 100 non-zerocoefficients. For all experiments that did not report complete sparsity versus accuracy curves, we used5-fold cross validation to select the value for the 0 sparsity penalty. We searched over the followingset of values for for each GAM: 20, 10, 5, 2, 1, 0.5, 0.4, 0.2, 0.1, 0.05, 0.02, 0.01, and 0.005. Weoptimized for AUC on Breast Cancer and accuracy on all other datasets when using cross validation.We fit all non-sparse GAMs using SKLearns implementation of logistic regression over binned data. We evaluated the performance of a variety of classifiers on all datasets in .4. For eachdatasets, we used MICE to impute 10 distinct datasets, and fit a variety of predictive models (alogistic regression, an AdaBoost model (Freund & Schapire, 1997), a random forest (Breiman, 2001),a decision tree, a shallow neural network, and an XGBoost classifier Chen & Guestrin (2016))on these datasets. For each baseline classifier, we also provide accuracy for a model fit with andwithout missingness indicators added via the SMIM procedure Van Ness et al. (2023). We used crossvalidation to select hyperparameters separately for each imputed training dataset, and ensembledthe 10 models for each model class to produce a single predictive model. Cross validation wasperformed using 5 folds via GridSearchCV from SKLearn (Pedregosa et al., 2011), and the SKLearnimplementation was used for each model class considered other than XGBoost. The hyperparameterswe considered are:",
  "D.4Computational Resources": "All experiments were performed on an institutional computing cluster. All experiments that involvedtiming were conducted using one Tensor TXR231-1000R D126 Intel(R) Xeon(R) CPU E5-2640 v4 @2.40GHz (512GB RAM - 40 cores), except for MIWAE timing experiments, which use one NVIDIATesla P100 GPU. When runtime was not reported, experiments were run on whatever hardware wasavailable on the cluster at that time.",
  "E.1Additional Datasets": "Throughout this section of the appendix, we will consider two new datasets in addition to the fourdatasets introduced in the main body of the paper (FICO, Breast Cancer, MIMIC, and Pharyngitis).We add the Chronic Kidney Disease Rubini et al. (2015) and Heart Disease Janosi et al. (1988)datasets from UCI, refered to simply as CKD and Heart Disease respectively. CKD consists of 400samples with 24 features, and involves prediction of chronic kidney disease using medical features.Heart Disease concerns predicting heart disease using medical and demographic features, with 303samples and 13 features. Note that the outcome in Heart Disease is an integer between 0 and 4; webinarize this label such that we classify no heart disease (0) versus any heart disease (1, 2, 3, 4). Bothof tshese datasets contain only one missingness encoding.",
  "E.2Evaluation of Alternative Imputation Methods": "In the main body of this paper, we focused our evaluation of baseline classifiers on impute-then-predictusing the MICE method for multiple imputation. However, a wide variety of multiple imputationmethods are available. Using all six datasets, we evaluate the runtime and accuracy of four imputationmethods: MICE (Van Buuren & Oudshoorn, 1999), MIWAE (Mattei & Frellsen, 2019), mean valueimputation, and MissForest (Stekhoven & Buhlmann, 2012). We impute ten alternative datasets foreach of ten distinct train-test splits for both FICO and Breast Cancer. For a given train-test split, wefit a model from each of the model classes described in D.3 for each imputed training dataset. Weensemble these ten models to produce a single predictive model per train-test split, and evaluate thetest accuracy (FICO) or test AUC (Breast Cancer) of this ensembled model. contains box-and-whiskers plots for the accuracy of each method considered across all sixdatasets. The imputation method does not generally have a substantial impact on the performance ofthe resulting impute-then-predict classifier. We also see that, across the two datasets not consideredin the main body of the paper (CKD and Heart Disease) M-GAM continues to provide comparableaccuracy to all baseline methods. shows the time required to produce a predictive model under each method for each dataset.As in the main body of the paper, we show the sum of the time required to impute data and thetime required to produce the most accurate model. We see that, despite resulting in models withcomparable accuracy to those produced by MICE and Mean imputation, MissForest and MIWAEtake longer to compute on the majority of datasets. On all six datasets datasets, M-GAM is at leastan order of magnitude faster than all three multiple imputation methods, although mean imputationtends to be fastest.",
  "E.3Scalability of M-GAM and Imputation Methods": "This section studies how well each method scales in terms of the number of samples in the dataset.For each dataset, we take subsamples of increasing size (25%, 50%, 75%, and 100% of samples ineach dataset) and run each impute-then-predict predict procedure, as well as M-GAM over 10 distincttrain-test splits. reports the total time taken to produce a model for each imputation methodand M-GAM on each dataset/subsample combination. We find that M-GAM scales no worse thanany of the imputation alternatives in terms of runtime.",
  "E.4Evaluation of Different Thresholds": "Throughout the main body of this work, we reported results using 8 evenly spaced quantiles tothreshold our input variables for both M-GAM and FastSparse GAMs fit on imputed data. In thissection, we evaluate the sparsity versus accuracy curve for M-GAM under different binning strategies.In particular, we evaluate the performance of M-GAM on FICO with 4, 8, 16, and 32 evenly spacedquantiles. shows the results of this analysis. As the number of quantiles increased, M-GAMremained sparse despite the exploding number of interaction terms, and in fact for 32 quantiles theinteraction terms lead to an especially sparse and accurate model. Beyond these observations, wefound that reasonable changes to the number of thresholds we consider did not significantly impactperformance. : Test accuracy of M-GAM compared against a variety of baselines for four imputationmethods and six datasets. Each column corresponds to a different imputation method, and each rowto a different dataset. : Runtime of different methods on Breast Cancer, CKD, FICO, Heart Disease, MIMIC, andPharyngitis. For each imputation method, we report the total time required to impute missing dataand fit the best performing impute-then-predict classifier for that dataset and imputation method.M-GAM (Ind) is an M-GAM with indicators and M-GAM (Int) is an M-GAM with interaction terms.Error bars report standard error of total runtime over 10 train-test splits.",
  "E.5Evaluation of MICE with Different Numbers of Imputations": "Since MICE is a multiple imputation method, we needed to choose how many datasets we allowMICE to impute for each of our experiments. In this section, we evaluate the runtime versus testaccuracy for models built on various numbers of imputed datasets for FICO. We evaluated eachnon-GAM baseling model considered in the main paper when ensembled over 1, 5, 10, 20, and 30MICE imputed datasets. shows the accuracy versus runtime for each number of imputations. In , wesee that there is a slight improvement in the accuracy of our classifiers when increasing from 1 to5 imputations, but no significant performance gain for any larger numbers of imputed datasets. Assuch, we opted to use the moderately fast and performant choice of 10 imputations.",
  "E.6Extension of Sparsity/Accuracy Results to Further Datasets": "We focus on the FICO and Breast Cancer datasets for much of the main paper, alongside Pharyngitisand MIMIC. In we show the superset of our sparsity-accuracy results that includes the twoUCI repository datasets, Heart Disease Janosi et al. (1988) and CKD Rubini et al. (2015). In we show the superset of our results for the data with added MAR missingness, for all 6 datasets.",
  "E.7Evaluation of Alternative Distinct Missingness Encodings": "As we discuss in the main text, allowing our model to encode different reasons for missingnessallows the ability to handle multiple reasons for missingness, improving the models power. However,using only a single, overall reason for missingness could potentially allow for handling a larger setof missing data cases with fewer coefficients. These two encodings are not mutually exclusive; wecould augment in both ways, as described in Equation E.1. For this reason, we investigate sparsityand test accuracy on the FICO dataset across a range of choices for whether to use distinct encodings, : Runtime of different methods over 25%, 50%, 75%, and all of Breast Cancer, CKD,FICO, Heart Disease, MIMIC, and Pharyngitis. For each imputation method, we report the total timerequired to impute missing data and fit the best performing impute-then-predict classifier for thatdataset and imputation method. M-GAM (Ind) is an M-GAM with indicators and M-GAM (Int) is anM-GAM with interaction terms. Error bars report standard error of total runtime over 10 train-testsplits. Test Performance vs Sparsity M-GAM (Indicators Only)M-GAM (w/ Interactions) SMIMGAM w/ MVI",
  ": Test accuracy vs sparsity for M-GAMs relative to competitor GAMs on 6 datasets": "overall encodings, or some combination of the two. We explore a variety of combinations in , allowing nonzero values for different subsets of (specific indicators), overall (overall indicators),miss (specific interactions), and overall miss (overall interactions). We find no dramatic differencesacross these choices.Definition E.1. Given parameters , overall, miss, overall miss, and , an M-GAM is defined as",
  "Test Accuracy": "* # Nonzero Coefficients 020400.70 0.71 0.72 0.73 0.74 0.75 * Indicator: Specific & Overall 020400.70 0.71 0.72 0.73 0.74 0.75 * 020400.70 0.71 0.72 0.73 0.74 0.75 * # Nonzero Coefficients 020400.70 0.71 0.72 0.73 0.74 0.75 * Interaction: Specific Indicator: Overall 020400.70 0.71 0.72 0.73 0.74 0.75 * Interaction: Specific & Overall 020400.70 0.71 0.72 0.73 0.74 0.75 * Interaction: Overall # Nonzero Coefficients : Results on the FICO dataset using different choices of missingness augmentation forindicators and interactions. Specific refers to the distinct missingness used throughout the text.Overall refers to augmenting our matrix while treating missing data as all having a single missingnessreason.",
  "GLicense Information for Used Assets": "In this section, we provide license information for every external asset used in this paper. TheBreast Cancer, CKD, Heart Disease, and Pharyngitis datasets are available under creative commons.FICO is used under its own license, the details of which can be found here: MIMIC-III isavailable under the MIT license. We use code from Liu et al. (2022) under the MIT license, and thecode from Shadbahr et al. (2023) under BSD 3-Clause.",
  "Trades": "Variable 4: AverageMInFile Variable 5: MaxDelq2Public RecLast12M Variable 6: NetFraction RevolvingBurden Variable 7: PercentInstallTrades -0.4 0.8 -0.4 0.8 -0.4 0.8 040007.501500100 : An expanded version of with variable names included. MSinceMostRecentDelqis the number of months since the individuals last delinquent payment and MSinceMostRecentIn-qexcl7day is the number of months since the individuals last inquiry, excluding those within thelast week; all features are described in the FICO challenge (FICO et al., 2018) data documentation. : A visualization of a M-GAM without interaction terms on FICO. The shape functions onthe left are selected based on which variables are missing, with the relevant missing variable noted tothe left. The shape functions on the right are used in all cases. : An additional visualization of a M-GAM with interaction terms on FICO. The shapefunctions within the left set of brackets are selected based on which variables are missing, with therelevant missing variable noted to the left. The shape functions in the right set of brackets are appliedin all cases. : A visualization of a M-GAM without interaction terms on Breast Cancer. The shapefunctions within the left set of brackets are selected based on which variables are missing, with therelevant missing variable noted to the left. The shape functions in the right set of brackets are appliedin all cases.",
  "The answer NA means that the paper has no limitation while the answer No means thatthe paper has limitations, but those are not discussed in the paper": "The authors are encouraged to create a separate Limitations section in their paper. The paper should point out any strong assumptions and how robust the results are toviolations of these assumptions (e.g., independence assumptions, noiseless settings,model well-specification, asymptotic approximations only holding locally). The authorsshould reflect on how these assumptions might be violated in practice and what theimplications would be. The authors should reflect on the scope of the claims made, e.g., if the approach wasonly tested on a few datasets or with a few runs. In general, empirical results oftendepend on implicit assumptions, which should be articulated. The authors should reflect on the factors that influence the performance of the approach.For example, a facial recognition algorithm may perform poorly when image resolutionis low or images are taken in low lighting. Or a speech-to-text system might not beused reliably to provide closed captions for online lectures because it fails to handletechnical jargon.",
  "If applicable, the authors should discuss possible limitations of their approach toaddress problems of privacy and fairness": "While the authors might fear that complete honesty about limitations might be used byreviewers as grounds for rejection, a worse outcome might be that reviewers discoverlimitations that arent acknowledged in the paper. The authors should use their bestjudgment and recognize that individual actions in favor of transparency play an impor-tant role in developing norms that preserve the integrity of the community. Reviewerswill be specifically instructed to not penalize honesty concerning limitations.",
  ". Experimental Result Reproducibility": "Question: Does the paper fully disclose all the information needed to reproduce the main ex-perimental results of the paper to the extent that it affects the main claims and/or conclusionsof the paper (regardless of whether the code and data are provided or not)?Answer: [Yes]Justification: A thorough description of our experimental framework is provided in theappendix. We also provide references to each dataset we use, and will release the code uponpublication.Guidelines: The answer NA means that the paper does not include experiments. If the paper includes experiments, a No answer to this question will not be perceivedwell by the reviewers: Making the paper reproducible is important, regardless ofwhether the code and data are provided or not.",
  "If the contribution is a dataset and/or model, the authors should describe the steps takento make their results reproducible or verifiable": "Depending on the contribution, reproducibility can be accomplished in various ways.For example, if the contribution is a novel architecture, describing the architecture fullymight suffice, or if the contribution is a specific model and empirical evaluation, it maybe necessary to either make it possible for others to replicate the model with the samedataset, or provide access to the model. In general. releasing code and data is oftenone good way to accomplish this, but reproducibility can also be provided via detailedinstructions for how to replicate the results, access to a hosted model (e.g., in the caseof a large language model), releasing of a model checkpoint, or other means that areappropriate to the research performed. While NeurIPS does not require releasing code, the conference does require all submis-sions to provide some reasonable avenue for reproducibility, which may depend on thenature of the contribution. For example(a) If the contribution is primarily a new algorithm, the paper should make it clear howto reproduce that algorithm.",
  "(b) If the contribution is primarily a new model architecture, the paper should describethe architecture clearly and fully": "(c) If the contribution is a new model (e.g., a large language model), then there shouldeither be a way to access this model for reproducing the results or a way to reproducethe model (e.g., with an open-source dataset or instructions for how to constructthe dataset). (d) We recognize that reproducibility may be tricky in some cases, in which caseauthors are welcome to describe the particular way they provide for reproducibility.In the case of closed-source models, it may be that access to the model is limited in",
  "The answer NA means that the paper does not include experiments": "The authors should answer Yes if the results are accompanied by error bars, confi-dence intervals, or statistical significance tests, at least for the experiments that supportthe main claims of the paper. The factors of variability that the error bars are capturing should be clearly stated (forexample, train/test split, initialization, random drawing of some parameter, or overallrun with given experimental conditions).",
  "Guidelines:": "The answer NA means that there is no societal impact of the work performed. If the authors answer NA or No, they should explain why their work has no societalimpact or why the paper does not address societal impact. Examples of negative societal impacts include potential malicious or unintended uses(e.g., disinformation, generating fake profiles, surveillance), fairness considerations(e.g., deployment of technologies that could make decisions that unfairly impact specificgroups), privacy considerations, and security considerations. The conference expects that many papers will be foundational research and not tiedto particular applications, let alone deployments. However, if there is a direct path toany negative applications, the authors should point it out. For example, it is legitimateto point out that an improvement in the quality of generative models could be used togenerate deepfakes for disinformation. On the other hand, it is not needed to point outthat a generic algorithm for optimizing neural networks could enable people to trainmodels that generate Deepfakes faster. The authors should consider possible harms that could arise when the technology isbeing used as intended and functioning correctly, harms that could arise when thetechnology is being used as intended but gives incorrect results, and harms followingfrom (intentional or unintentional) misuse of the technology. If there are negative societal impacts, the authors could also discuss possible mitigationstrategies (e.g., gated release of models, providing defenses in addition to attacks,mechanisms for monitoring misuse, mechanisms to monitor how a system learns fromfeedback over time, improving the efficiency and accessibility of ML).",
  "This work poses no substantial risks for misuse, beyond those associated with any predictivemodel.Guidelines:": "The answer NA means that the paper poses no such risks. Released models that have a high risk for misuse or dual-use should be released withnecessary safeguards to allow for controlled use of the model, for example by requiringthat users adhere to usage guidelines or restrictions to access the model or implementingsafety filters.",
  "The license under which each asset is used is given in the appendix, and we comply with thelicense for each asset.Guidelines:": "The answer NA means that the paper does not use existing assets. The authors should cite the original paper that produced the code package or dataset. The authors should state which version of the asset is used and, if possible, include aURL. The name of the license (e.g., CC-BY 4.0) should be included for each asset. For scraped data from a particular source (e.g., website), the copyright and terms ofservice of that source should be provided. If assets are released, the license, copyright information, and terms of use in thepackage should be provided. For popular datasets, paperswithcode.com/datasetshas curated licenses for some datasets. Their licensing guide can help determine thelicense of a dataset.",
  "The answer NA means that the paper does not involve crowdsourcing nor research withhuman subjects": "Depending on the country in which research is conducted, IRB approval (or equivalent)may be required for any human subjects research. If you obtained IRB approval, youshould clearly state this in the paper. We recognize that the procedures for this may vary significantly between institutionsand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and theguidelines for their institution."
}