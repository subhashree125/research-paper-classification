{
  "Abstract": "Hands are the primary means through which humans interact with the world.Reliable and always-available hand pose inference could yield new and intu-itive control schemes for human-computer interactions, particularly in virtualand augmented reality. Computer vision is effective but requires one or multi-ple cameras and can struggle with occlusions, limited field of view, and poorlighting. Wearable wrist-based surface electromyography (sEMG) presents apromising alternative as an always-available modality sensing muscle activi-ties that drive hand motion. However, sEMG signals are strongly dependent onuser anatomy and sensor placement, and existing sEMG models have requiredhundreds of users and device placements to effectively generalize. To facilitateprogress on sEMG pose inference, we introduce the emg2pose benchmark,the largest publicly available dataset of high-quality hand pose labels andwrist sEMG recordings. emg2pose contains 2kHz, 16 channel sEMG and poselabels from a 26-camera motion capture rig for 193 users, 370 hours, and 29stages with diverse gestures - a scale comparable to vision-based hand posedatasets. We provide competitive baselines and challenging tasks evaluatingreal-world generalization scenarios: held-out users, sensor placements, andstages. emg2pose provides the machine learning community a platform forexploring complex generalization problems, holding potential to significantlyenhance the development of sEMG-based human-computer interactions.",
  "Introduction": "Despite rapid progress in computing hardware and software, current input devices can be inefficientand non-intuitive for new and emerging computing platforms. This is particularly evident for spatialinteractions, such as those encountered in virtual and augmented reality, where conventional inputdevices like controllers, keyboards, and mice do not always offer intuitive control schemes norsufficient degrees of freedom to enable precise control (e.g., object manipulation). Interactionsbased on hand movements offer a high-dimensional continuous input that is instinctive, universal,and particularly well suited to spatial interactions [Han et al., 2020]. Furthermore, existing inputschemes can be viewed as low dimensional summaries of hand movements, for instance a mouseclick tells you that a finger has pressed a button. As such, hand kinematics is a potentially holisticand encompassing modality, covering existing inputs and extending them in a natural manner. Highfidelity hand tracking enables various AR/VR applications including gaming [Han et al., 2020],virtual teaching [Shrestha et al., 2022], teleoperations [Santos Carreras, 2012, Darvish et al., 2023],haptics [Scheggi et al., 2015], embodied realism [Wang et al., 2020], sports analytics [Gatt et al.,2020], and healthcare and rehabilitation [Krasoulis et al., 2017]. Given the high utility and broad appeal of effective hand pose estimation, there have been diverseapproaches developed across many sensing modalities: optical approaches (e.g. monocular, multi-view, depth-based, motion capture, infrared) using fixed [Cai et al., 2018, Mueller et al., 2018, Geet al., 2016, Supancic et al., 2018, Park et al., 2020] or head-mounted cameras [Han et al., 2018];wearable data gloves using magnetic [Parizi et al., 2019], inertial [Yang et al., 2021], capacitative[Truong et al., 2018], and stretch sensors [Shen et al., 2016, Tashakori et al., 2024, Luo et al., 2021];smart rings [Parizi et al., 2019]; wrist and forearm wearables that use impedance tomography [Zhangand Harrison, 2015], inertial measurement units [Laput and Harrison, 2019], acoustics [Laput et al.,2016] or ultrasound [McIntosh et al., 2017]. Each modality comes with its own hardware constraintsand limitations. Optical approaches can struggle with occlusions, poor lighting conditions, and limitedfield of view, and often require multiple cameras for effective inference, which places constraintson the overall size of the device. Alternatively, glove wearables can hinder dexterous manipulation[Roda-Sales et al., 2020] and forearm wearables typically only support discrete gesture classification. Surface electromyography (sEMG) sensing on the wrist or forearm provides an appealing alternativethat does not struggle with occlusion, field of view, poor lighting, or physical encumberance. sEMGuses electrodes on the skin to measure electrical potentials generated by muscles during movement[Stashuk, 2001]. Specifically, sEMG detects the electrical activity that occurs when spinal motorneurons activate the muscle fibers that drive motion [Merletti and Farina, 2016]. As such, sEMG isparticularly well suited for kinematic inference and numerous approaches have been developed [Liuet al., 2021, Quivira et al., 2018, Sosin et al., 2018, Smpetru et al., 2022b]. Nevertheless, learning auniversal sEMG-to-pose model that generalizes to new participants and kinematics is particularlychallenging. This is due to sEMG sensing containing many axes of variation, primarily: user anatomy,sensor placement, and hand kinematics [CTRL-labs at Reality Labs et al., 2024, Liu et al., 2021].User anatomy and sensor placement both influence the locations of the sensors relative to the muscles.Hand kinematics influence what combination of muscle activities are sensed. Given the numberof generative dimensions, sEMG models are particularly data-hungry [CTRL-labs at Reality Labset al., 2024], necessitating many samples across these axes to effectively learn universal models thatgeneralize (see .4 experiments). Existing datasets lack scale across each of these generativedimensions, thus hindering the development of generic models [Atzori et al., 2014]. Another complication of sEMG is that it encodes muscle activity, which relates more closely tomotion than the pose that we would like to recover. As such, direct pose inference from sEMG isparticularly challenging (see ), potentially requiring reasoning over long historical sEMGsequences to disambiguate pose from sequences of indirect motion measurements. Extracting relevantinformation from long sequences, or contexts, in the presence of ambiguity has been extensivelyexplored in fields such as CV [Brunetti et al., 2018, Kirillov et al., 2023, Pan et al., 2018], naturallanguage [Achiam et al., 2023, Kojima et al., 2022, Gu et al., 2021], and robotics [Lauri et al., 2022,Dunion et al., 2024, Jang et al., 2022]. Despite this, prior sEMG works have shown promising resultsfor personalized or single-user pose inference settings [Liu et al., 2021, Smpetru et al., 2022b]. To facilitate progress toward developing universal sEMG-to-pose models, we introduce the emg2posebenchmark dataset, a large-scale dataset of simultaneously recorded high-fidelity wrist sEMG record-ings and hand pose labels. High resolution sEMG recordings are obtained with the sEMG-RD wrist band [CTRL-labs at Reality Labs et al., 2024](see .1) and high precision poselabels are obtained from a 26-camera motion capture rig that offers benefits compared to multi-view computer vision [Liu et al., 2021, Sosin et al., 2018]. To our knowledge, this is the largestpublicly-available sEMG hand pose dataset, spanning 193 users, 370 hours, and 29 diverse kinematiccategories, called stages, each containing diverse low-level behaviors, called gestures. In addition,the 80M labelled frames that our dataset contains compares favourably with even the newest andlargest CV equivalents [Sener et al., 2022, Yu et al., 2020] in both number of frames as well assubjects (see ). We additionally provide three competitive baselines and challenging handpose inference benchmarks, investigating generalization to unseen users, stages, and user-stagecombinations. Instructions regarding accessing and using the emg2pose benchmark is provided in Given the high potential impact of sEMGinput devices, and the similar research challenges to existing fields, we believe this benchmark willbe of great value to the machine learning community.",
  "Ours751193450Yes": "sEMG Datasets:There are sev-eral publicly available sEMG datasetsfor tasks other than pose regression,specifically pose (sequence) classifi-cation. Data have been collected witheither clinical-grade high-density elec-trode arrays and amplifiers [Ammaet al., 2015, Du et al., 2017, Jianget al., 2021] or consumer-grade hard-ware that has fewer channels andlower temporal resolution [Palermoet al., 2017]. Clinical-grade hardwareoffers hundreds of recording channelsand acquisition rates >1 kHz but are impractical due to lengthy donning procedures that includeshaving the skin before applying conductive gel and the electrode arrays. In contrast, existingconsumer-grade hardware is easier to deploy, but is limited by lower bandwidth and channel countsand thus may not provide the level of fidelity required for pose estimation. In contrast, our dataset usesthe sEMG-RD band [CTRL-labs at Reality Labs et al., 2024], that can be quickly donned, records 16channels at >2 kHz and has proven performant for generalized pose classification modelling. Atzori et al. released a pose regression dataset on consumer-grade emg technologies. Thisdataset is composed of 37 hours of simultaneously recorded kinematics and sEMG, over a total of 67sensor placements. In contrast, our dataset includes 193 users, 370 hours, and 751 sessions, whichshould allow us to train models that generalize favourably across these axes (see for scalecomparison with existing datasets). Our dataset contains gesture categories as well as joint angles. Pose Regression from sEMG: Several papers have studied pose regression from sEMG, althoughwithout open sourcing datasets. Liu et al. use the MyoBand to estimate hand pose acrossdiverse movements in an 11 participant dataset. They test sEMG decoding models of hand pose acrossusers and sessions with both convolutional (NeuroPose; see .5) and LSTM architectures.Smpetru et al. [2022a] (SensingDynamics; see .5) use a clinic-grade system to collectseveral dozen minute datasets in a set of 13 participants. They use a custom 3D convolutionalarchitecture to predict hand joint angles, landmark positions, and grip force, reporting tracking withlow error in a held-out test set within each participant. These datasets are limited in scale, with only11 or 13 participants, and 15 or 20 minutes of data per participant for Liu et al. , Smpetru et al.[2022a], respectively, likely limiting generalization across users. In contrast, our dataset includes193 users and 370 hours, aiding the development of generic models that generalize across users (see.4). Pose from Computer Vision: Computer vision (CV) based hand pose estimation has receivedconsiderable attention in recent years, usually taking depth, RGB, or both as input, and leveraginglarge open-sourced datasets [Mueller et al., 2017, 2018, Spurr et al., 2018, 2020, 2021, Wan et al.,2019, Boukhayma et al., 2019]. Labels are either obtained using marker-based motion capture[Fan et al., 2023] - whose markers create an input distributional shift due to lack of markers during deployment - or using alternate approaches with lower quality labels or inputs, such as multi-viewcameras [Zimmermann et al., 2019, Moon et al., 2024], synthetic data [Zimmermann and Brox, 2017],and magnetic sensors [Yuan et al., 2017]. In contrast, motion capture markers afford high qualitylabels for sEMG, but they do not affect the data from which predictions are generated.",
  "Ours (per hand)80M19360Ours (across hands)40M19360": "The gestural diversity of CV-based datasets mostlyfocuses on exploring the full static pose space ofthe hand [Yuan et al., 2017, Zimmermann and Brox,2017, Zimmermann et al., 2019], interaction withobjects [Fan et al., 2023, Samarth et al., 2020, Ham-pali et al., 2020] or hand-hand interactions [Moonet al., 2020, 2024]. Conversely, our dataset focuseson movements of the hand because sEMG, unlike CV,is more closely related to motion than pose. Further-more, our dataset has 80M frames and 193 subjects,comparing favorably to CV datasets (see ,reporting million frames, subjects and fps). Pose from Other Modalities: In addition to visionand sEMG, there exists a diverse range of additionalwearable approaches to pose inference (see ), which typically focus on pose (sequence)classification. For example, Achenbach et al. released a dataset for pose classification usingcommercially available sensor gloves. Other datasets typically use bespoke hardware and are smallin scale, with the exception of a large (50 participant, 25 class) dataset available for classificationusing commercially available smartwatches [Laput and Harrison, 2019].",
  ".1sEMG Device": "Data are collected using the 16 channel bipolar sEMG-RD wrist band from CTRL-labs at RealityLabs et al. . They demonstrate the effectiveness of this device for generalized pose sequenceclassification across 6400 participants, the largest study to date. This high performance is achievedwithout the need for high-density sEMG platforms [Amma et al., 2015], with a similar form factorand ease of use to other low-density platforms [Rawat et al., 2016] (see Figs. 1 and 2 for a visualdepiction of the device). In contrast to the previously used low-density Thalmic Labs Myo band [Liuet al., 2021] that streams data at 200Hz, across 8 channels and with 8-bits, sEMG-RD senses at 2kHz,across 16 channels and with 12-bits. For more details see Appendix B.1.",
  "Dataset": ": emg2pose dataset statistics, reporting mean and standard deviation. Three separate test setsmeasure generalization to new users, types of behaviors (stages), and user-behavior combinations(user, stage). Note that the overall hours is the sum of the hours across all splits. The number of hourscounts the right-handed and left-handed data separately for each participant.",
  "UserUser, StageUserStageUser, Stage": "Subjects15815152015820193Unique stages23236236629Hours250.921.74.631.954.27.0370.3Hours / subject1.6 0.41.4 0.50.3 0.11.6 0.30.3 0.10.3 0.01.9 0.5Sessions / subject3.9 0.63.8 0.63.7 0.63.9 0.33.8 0.73.8 0.53.9 0.6 Consenting participants (see Appendix A) stood in a 26 camera motion capture array (Appendix B.2).A research assistant placed 19 motion capture markers on each of the participants hands (Han et al.) and an sEMG-RD band on each wrist [CTRL-labs at Reality Labs et al., 2024]. All sEMGand motion capture data were streamed to a real-time data acquisition system at 2kHz and 60 Hz,respectively. We time-aligned device streams using software timestamps, which we found to showless than 10ms relative latency between devices. Motion capture data were post-processed usingan offline inverse kinematics (IK) solver to reconstruct the joint angles of the hand (Appendices A : Dataset composition: a) sEMG-RD wrist-band and motion capture marker (white dots)setup. b) Dataset breakdown. i) Users are prompted to perform a sequence of movement types(gestures), such as counting up and down. sEMG and poses are recorded simultaneously. ii) Groupsof specific gesture types comprise a stage, such as counting. Stages are partitioned into train/val/testsplits (see .4). Our dataset consists of 29 diverse stages. iii) Each of the 193 users performvarious stages, donning on-and-off the wrist band. In total we record 370 hours of data.",
  "and B.2). The IK solver failed for 12.7% of frames, typically due to simultaneously occluded markers.Finally, joints angles were linearly interpolated to 2 kHz to match the sample rate of sEMG": "Participants followed a standardized data collection protocol across a diverse set of 45-120 s stagesin which participants were prompted to perform either a mix of 3-5 similar gestures in randomorderings (e.g. specific finger counting orderings such as ascending or descending) or unconstrainedfreeform movements (see Appendices A and B.3 for further details). Stages can be viewed as acategorization of gestures. For example, the Counting stage categorizes Counting Up and CountingDown gestures (see ). During data collection, the majority of users donned on-and-off thedevice 4 times, with a small fraction only thrice. Each group of stages with a single band placement isreferred to as a session. We report the prompted movements for each stage in detail in Appendix B.3.During each stage, we prompted participants using videos and verbal instructions by the researchassistant. Participants were instructed to move their hands across their body and between their waistand shoulders to ensure a range of different postures were sampled. See for a visualization ofthe data collection. The full dataset is organized hierarchically by participant, session, and stage. In total, we collecteddata from 193 participants, spanning 370 hours, 751 sessions, 29 diverse stages (see Appendix B.3for further details and for statistics). Note that the number of hours counts the right-handedand left-handed data separately for each participant, although they were collected simultaneously.To our knowledge, this is the only open-sourced sEMG and motion capture dataset and is of similarscale to those in the CV literature [Yuan et al., 2017, Brahmbhatt et al., 2020, Moon et al., 2020,2024]. The entire dataset consists of 25, 253 HDF5 files, each consisting of time-aligned sEMG andjoint angles for a single hand in a single stage.",
  "The emg2pose benchmark includes two benchmark tasks: pose regression and pose tracking": "Regression: For this task, previously explored in Liu et al. , Smpetru et al. [2022b], one mustregress from sEMG to hand joint angle sequences. Without knowledge of the initial hand pose andvelocity, this is a partially observable task [Spaan, 2012], and thus particularly challenging for the reasons mentioned in . Pose regression is the most challenging task and is meant to promotecontinued research with applications including unimodal pose prediction in settings where computervision is infeasible or unreliable. Tracking: For this simpler task, one must regress from sEMG to hand joint angle sequences whilstbeing provided with the initial hand pose in the sequence. Providing the initial pose addressesthe partial observability dilemma. Nevertheless, this task still poses the generalization challengesdiscussed in . The tracking task is meant to promote initial research and progress, and hasseveral real-world applications. An effective tracker would provide great value in settings where: theuser is prompted to match a given pose before tracking commences; visual pose prediction feedbackis provided, allowing the user to adjust their pose to correct for erroneous initial predictions; andwhen ground truth pose estimates are intermittently available, such as from computer vision settingswhenever partial or full occlusions occur. Evaluation: We evaluate on 5 second trajectories and report test set mean absolute joint angularerror () and mean (Euclidean) landmark distance (mm). Landmarks correspond to joint and fingertipCartesians. We do not regress to wrist angles, which were not recorded for this dataset. Landmarkscorresponding to the most proximal joint for fingers other than the thumb always have zero errorbecause the wrist does not move. These landmarks are therefore excluded from our metrics. Weobtain landmark locations by passing joint angles through a default hand model. This introduces bias,as it will not perfectly align with each users anatomy. We leave addressing this limitation for futurework. In real world applications, it will be important to not only improve mean performance for thesemetrics, but also lower percentile scores across the population.",
  "Held-Out Settings": "Effective pose inference requires models that generalize across device placements, users, and handkinematics. Prior works have only investigated generalization across a subset of these axes, such asuser [Liu et al., 2021, CTRL-labs at Reality Labs et al., 2024] or device placement [Liu et al., 2021,Palermo et al., 2017], but generalization to new types of kinematics has not been explicitly explored.In contrast, we provide three separate test sets intended to measure these axes independently. Thestatistics of each held-out scenario are reported in . In short, users corresponds to unseenusers, but in-distribution kinematics (stages). Stages represents unseen kinematic categories, butin-distribution users. Finally, users, stages constitute held-out users and stages, and is of greatestvalue as the most encompassing real-world deployment setting. Both held out user scenarios allconstitute new device placements, which vary across all sessions. We break down train, validationand test splits roughly using 0.7 : 0.1 : 0.2 ratio with exact splits shown in . Held-out usersare randomly sampled and held-out stages are chosen to be visually out-of-distribution with respectto the training stages. See for a breakdown of which stages are in the training and held-out sets, for details regarding each stage, and Appendix B.2.1 for further dataset details.",
  "Baselines": "We provide three baselines: open-source re-implementations of the NeuroPose and SensingDynamicsnetwork architectures [Liu et al., 2021, Smpetru et al., 2022a], and a new vemg2pose model.Algorithm details can be found in Appendix C. vemg2pose: sEMG meaures underlying muscle activity, and therefore relates more strongly to handmovements than the static pose of the hand. Therefore, vemg2pose (\"Velocity-based emg2pose\")predicts joint angular velocities, which are then integrated to produce joint angle predictions. sEMG isfirst embedded via a causal strided convolutional featurizer, which temporally down-samples sEMGfrom 2 kHz to 50 Hz. A Time-Depth Separable Convolution (TDS) network is used for the featurizer,as it has been shown to be effective and parameter-efficient in the automatic speech recognitionliterature [Hannun et al., 2019] (see Appendix C for implementation details). The features at eachtime-step are then concatenated to the joint angle predictions at the previous time step and fed toan LSTM decoder, which produces the next velocity prediction. Those velocities are added to theprevious joint angles to produce the next prediction. vemg2pose is therefore auto-regressive withrespect to its own predictions. Finally, predictions are linearly up-sampled to match the sample rateof the joint angles targets. For the tracking task, the initial joint angles are set to the ground truth,",
  "according to the motion capture labels. For the regression task, the initial state is also predicted bythe decoder (see Appendix C for further details)": "NeuroPose: NeuroPose and vemg2pose differ in their prediction spaces and network architectures.Whereas vemg2pose predicts angular velocities, NeuroPose predicts joint angles directly. NeuroPoseuses a U-Net architecture with residual bottleneck layers. Briefly, a convolutional encoder spatiallyand temporally down-samples sEMG while extracting features which are then refined via a stackof residual blocks. Finally, a decoder generates pose predictions at the original sample rate viaconvolutions and up-sampling layers. Because our sEMG device measures at 10x the temporalfrequency and 2x the spatial frequency of the MyoBand used in Liu et al. , we increase thetemporal and spatial down and up-sampling of NeuroPoses featurizer and decoder (by 8x and 2x,respectively), such that the receptive field remains comparable to the original model. See Liu et al. for full model details and Appendix C for further details. SensingDynamics: SensingDynamics and NeuroPose primarily differ in their architectures. Insteadof a U-Net, SensingDynamics featurizer comprises of 2d convolutions over sEMG channels and time,with learnable SMU activations [Biswas et al., 2021], batch normalisation, circular padding acrosschannels, and dropout layers. The decoder comprises of a 3-layered MLP. Uniquely, SensingDynamicsadditionally passes 20Hz low-passed filtered sEMG as input to the featurizer. See Smpetru et al.[2022a] for full model details and Appendix C for further details. Training Setup: All algorithms are trained to minimize the L1 error between predicted and groundtruth joint angles as well as the Euclidean error between between predicted and ground truth fingertiplocations. The joint angle loss term has a weight of 1 and the fingertip loss term has a weight of .01.We train on 1-6 seconds of non-overlapping trajectories. The training trajectory length - in additionto other hyperparameters - is optimized independently for each algorithm (see ). We train for500 epochs with a 50 epoch early stopping criterion. Time-points for which motion capture data arenot available are skipped during training and evaluation. We use a batch size of 64 per GPU. We trainon Amazon EC2 g5.48xlarge instances which have 8x NVIDIA T4 GPUs for less than a day.",
  "User, StageSensingDynamics18.7 1.627.2 2.0NeuroPose17.5 1.524.9 1.7vemg2pose15.8 1.421.6 2.0": "We report regression results in and tracking results in . We do not report standarddeviation across model seeds, as we observed these to be negligible. Results are further broken downby stage, finger, and joint in Figs. 3, 10 and 11, respectively. For the regression task, vemg2poseoutperforms both NeuroPose and SensingDynamics with respect to both angular errors and landmarkdistances. In general, accuracy degrades most for the held-out user, stage combination, which isthe hardest of all transfer scenarios. For the tracking task - in which the initial ground truth poseis provided - errors are lower overall, as expected (see .3). For this task, we do not report",
  "User, Stagevemg2pose11.0 1.015.4 1.4": "Performance varies considerably across users for all models and tasks, potentially due to anatomicaldifferences (Tables 4 and 5). Performance varies significantly across stages (), which islikely a result of the amount and type of movements in each stage. Stages with limited movement(StaticHands, WristFlex) may be easier for the model track because they involve very limited posturaltransitions. Stages with complex hand poses and dynamic articulation of individual fingers (Gesture2,Pointing) are more challenging and have higher errors. Moreover, shows that performancevaries significantly across fingers and finger joints, with the thumb the most reliably predicted,followed by the index, middle, ring, and pinky fingers. We also find that proximal joint angles of thefingers are easier to track than distal joint angles (). Together, this suggests that stages withhigh amounts of thumb movements (e.g. ThumbRotations) may be easier to track than those withmore general finger movements (e.g. Freestyle1). : vemg2pose tracking performance break down by stage and generalization condition.Distributions are over users. Note the variability in performance across stages. Each box shows themedian and interquartile range (IQR), and whiskers show the minimum and maximum values that arewithin 1.5 times the IQR of the lower and upper quartiles.",
  "Some stages were specifically designed to test behaviors that are known to be challenging for vision-based hand pose estimation (see Appendix D.1 for details). We found that stages with hand-hand": "interactions or hand-object interactions have similar model performance compared to stages withoutsuch interactions (, right), although differences in behavioral distribution across these stagesmakes direct comparison challenging. Furthermore, we find that visual occlusion does not impactsEMG based pose reconstruction, as expected. Stages in which the hand is occluded from a CV basedheadset tracking system have similar accuracy compared to stages without occlusion in which thesame behaviors are performed (, left).",
  ": Median percentile held-out user and stage (Counting2). Top: motion capture; bottom:vemg2pose, tracking predictions. Clips unroll evenly left-to-right over a 2 second segment": "We plot vemg2pose, tracking real-time online and offline kinematic predictions for held-out usersand stages in Figs. 1 and 5 (see Appendix C.5 for online setup details). This is the most challengingscenario, representing generalization to held-out kinematics, user anatomy, and device placement.For , we plot a median-performance representative held-out stage (Counting2, see ) anduser. As seen, individual finger movements are mostly tracked, but not always. We visualize top andbottom percentile (15% and 85%) offline kinematics for the held-out users and stages generalizationsetting in Figs. 12 to 15. In general, we observed three challenges specific to sEMG pose inference:angular drift due to sensing that strongly relates to pose derivatives (see the ring finger in );movements related to harder-to-sense intrinsic hand muscles, such as the finger adduction/abductionpresent in the \"vulcan\" gesture (); movements related to smaller and fewer muscles, such aspinky (see ) and distal joint motion (see ).",
  "Dataset Scale Analysis": ": Generalization vs. number of training users (left two) or stages (right three) for vemg2posetracking. We subsampled the training users/stages but evaluated on the same held-out users/stages. Asseen, performance improves with the number of training users/stages, demonstrating the importanceof our dataset scale for effective generalization. Box plots take the same format as . We ran experiments to demonstrate the importance of the scale of our dataset for effective generaliza-tion. In , we show that increasing the number of training users considerably reduces the errorfor held-out users, perhaps because models are exposed to sEMG from users with a variety of wristanatomies. We also show that increasing the number of stages per-user improves performance acrossall modes of generalization, demonstrating the importance of behavioural diversity.",
  "Quantifying Generalization Difficulty across Users and Stages": "To directly quantify the difficulty of generalizing across held-out stages and users, we performedexperiments in which a subset of the data from the held-out users and stages were either folded intothe training set or excluded entirely. shows that excluding specific users and stages from thetraining set markedly degrades performance, demonstrating the difficulty of generalizing across thesedimensions. Refer to for a detailed description of the experimental setup. : Excluding stages (left) or users (right) from the training set markedly decreases performancefor these stages/users. For the include stages/users condition, we include 70% of the data from theheld-out stages/users in the training set. For the exclude stages/users condition we exclude that 70%entirely. Both test sets are identical allowing us to isolate the influence of holding out stages/users.Data are from a tracking task with a vemg2pose model. Distributions are over users.",
  "Limitations and Future Work": "Modelling:We provide an initial investigation into generalized sEMG-to-pose modelling andopen-source our baselines to the community. Nevertheless, there remains a plethora of unexplored,potentially fruitful sequence modelling directions, such as state space and diffusion-based methods.Pose estimation in the presence of uncertainty introduced by sensor noise and anatomical variabilitycould also be addressed with probabilistic methods [Danelljan et al., 2020]. Model personalizationhas also been shown to be beneficial [CTRL-labs at Reality Labs et al., 2024, Liu et al., 2021],yet we do not explore this avenue here. In addition, our models obtain mean landmark distanceerrors that are higher than reported in the CV literature [Boukhayma et al., 2019, Mueller et al.,2017], despite having the advantage of not having to infer the wrist position or users anatomy.Addressing this performance gap will be of great importance. Finally, the lack of broader accessto the sEMG-RD wrist-band [CTRL-labs at Reality Labs et al., 2024] might be limiting, as thisprecludes human-in-the-loop testing of models. Metrics:Our landmark distance metrics use a default hand model to convert joint angles tojoint positions. The mismatch between the hand model and user anatomy will bias this metric. Ingeneral, our metrics do not capture the physical plausibility of model predictions. For example,we have observed that vemg2pose sometimes predicts unfeasible kinematics, such as intra-fingerpenetration. Providing metrics that capture these failure modes will be of value, especially forembodied applications [Yuan et al., 2023]. Simulators of the hand [Caggiano et al., 2022] couldbe leveraged in a manner similar to Yuan et al. to ensure physical constraints are adheredto. Finally, our held-out user, stage test scenario is meant to best represent real-world in the wildperformance. Nevertheless, it does not cover a potential range of signal aggressors such as: electrode-skin contact artifacts; impedance changes from sweat; electrical interference from external devices;and non-stationarity due to muscle fatigue. While these aggressors likely play a minor role in sEMGvariability, they may be important to include in future datasets and test sets.",
  "Conclusion": "We introduce the emg2pose benchmark, the largest, diverse, and open-source dataset of high-fidelitysEMG recordings and hand pose labels. We introduce competitive benchmark models that can trackor regress to hand pose for held-out users, stages and sessions, although there remains significantroom to improve these models in future research. Due to the myriad sources of variability in sEMGsignals, deciphering the relationship between sEMG and movement in a manner that generalizesacross people and kinematics will likely require new algorithmic advances, taking inspiration fromrelated machine learning fields. Large datasets like emg2pose should thus facilitate progress in bothsEMG decoding and machine learning applied to biosignals more broadly. Progress will enableintuitive, high-dimensional human-computer interfaces that we perceive as extensions of ourselves.",
  "Acknowledgements": "We thank Patrick Kaifosh and TR Reardon for their sponsorship and vision and the entire CTRL-labsteam for their collaboration and support. We thank Carl Hewitt and Migmar Tsering for help withdata collection, Steve Olsen and Mark Hogan for assistance setting up motion capture recordings,John Choi and Diogo Peixoto for technical assistance and advice, and Dano Morrison and SunainaRajani for assistance with visualizations.",
  "J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt,S. Altman, S. Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023": "C. Amma, T. Krings, J. Ber, and T. Schultz. Advancing muscle-computer interfaces with high-densityelectromyography. In Proceedings of the 33rd Annual ACM Conference on Human Factors inComputing Systems, pages 929938, 2015. M. Atzori, A. Gijsberts, C. Castellini, B. Caputo, A.-G. M. Hager, S. Elsig, G. Giatsidis, F. Bas-setto, and H. Mller. Electromyography data for non-invasive naturally-controlled robotic handprostheses. Scientific data, 1(1):113, 2014.",
  "S. Han, B. Liu, R. Wang, Y. Ye, C. D. Twigg, and K. Kin. Online optical marker-based hand trackingwith deep labels. Acm transactions on graphics (tog), 37(4):110, 2018": "S. Han, B. Liu, R. Cabezas, C. D. Twigg, P. Zhang, J. Petkau, T.-H. Yu, C.-J. Tai, M. Akbay, Z. Wang,et al. Megatrack: monochrome egocentric articulated hand-tracking for virtual reality. ACMTransactions on Graphics (ToG), 39(4):871, 2020. S. Han, P. Wu, Y. Zhang, B. Liu, L. Zhang, Z. Wang, W. Si, P. Zhang, Y. Cai, T. Hodan, R. Cabezas,L. Tran, M. Akbay, T. Yu, C. Keskin, and R. Wang. Umetrack: Unified multi-view end-to-endhand tracking for VR. In SIGGRAPH Asia 2022 Conference Papers, SA 2022, Daegu, Republicof Korea, December 6-9, 2022, 2022.",
  "J. N. Ingram, K. P. Krding, I. S. Howard, and D. M. Wolpert. The statistics of natural handmovements. Experimental brain research, 188:223236, 2008": "E. Jang, A. Irpan, M. Khansari, D. Kappler, F. Ebert, C. Lynch, S. Levine, and C. Finn. Bc-z:Zero-shot task generalization with robotic imitation learning. In Conference on Robot Learning,pages 9911002. PMLR, 2022. X. Jiang, X. Liu, J. Fan, X. Ye, C. Dai, E. A. Clancy, M. Akay, and W. Chen. Open access dataset,toolbox and benchmark processing results of high-density surface electromyogram recordings.IEEE Transactions on Neural Systems and Rehabilitation Engineering, 29:10351046, 2021.",
  "Proceedings of the Web Conference 2021, pages 14711482, 2021": "Y. Liu, Y. Liu, C. Jiang, K. Lyu, W. Wan, H. Shen, B. Liang, Z. Fu, H. Wang, and L. Yi. Hoi4d: A 4degocentric dataset for category-level human-object interaction. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition, pages 2101321022, 2022. Y. Luo, Y. Li, P. Sharma, W. Shou, K. Wu, M. Foshey, B. Li, T. Palacios, A. Torralba, and W. Matusik.Learning humanenvironment interactions using conformal tactile textiles. Nature Electronics, 4(3):193201, 2021. J. McIntosh, A. Marzo, M. Fraser, and C. Phillips. Echoflex: Hand gesture recognition usingultrasound imaging. In Proceedings of the 2017 CHI Conference on Human Factors in ComputingSystems, pages 19231934, 2017.",
  "R. Merletti and D. Farina. Surface electromyography: physiology, engineering, and applications.John Wiley & Sons, 2016": "G. Moon, S.-I. Yu, H. Wen, T. Shiratori, and K. M. Lee. Interhand2. 6m: A dataset and baseline for3d interacting hand pose estimation from a single rgb image. In Computer VisionECCV 2020:16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XX 16, pages548564. Springer, 2020. G. Moon, S. Saito, W. Xu, R. Joshi, J. Buffalini, H. Bellan, N. Rosen, J. Richardson, M. Mize,P. De Bree, et al. A dataset of relighted 3d interacting hands. Advances in Neural InformationProcessing Systems, 36, 2024. F. Mueller, D. Mehta, O. Sotnychenko, S. Sridhar, D. Casas, and C. Theobalt. Real-time handtracking under occlusion from an egocentric rgb-d sensor. In Proceedings of the IEEE InternationalConference on Computer Vision, pages 11541163, 2017. F. Mueller, F. Bernard, O. Sotnychenko, D. Mehta, S. Sridhar, D. Casas, and C. Theobalt. Ganeratedhands for real-time 3d hand tracking from monocular rgb. In Proceedings of the IEEE conferenceon computer vision and pattern recognition, pages 4959, 2018. F. Palermo, M. Cognolato, A. Gijsberts, H. Mller, B. Caputo, and M. Atzori. Repeatability ofgrasp recognition for robotic hand prosthesis control based on semg data. In 2017 InternationalConference on Rehabilitation Robotics (ICORR), pages 11541159. IEEE, 2017.",
  "L. Santos Carreras. Increasing haptic fidelity and ergonomics in teleoperated surgery. Technicalreport, EPFL, 2012": "S. Scheggi, L. Meli, C. Pacchierotti, and D. Prattichizzo. Touch the virtual reality: using the leapmotion controller for hand tracking and wearable tactile devices for immersive haptic rendering.In ACM SIGGRAPH 2015 Posters, pages 11. 2015. F. Sener, D. Chatterjee, D. Shelepov, K. He, D. Singhania, R. Wang, and A. Yao. Assembly101: Alarge-scale multi-view video dataset for understanding procedural activities. In Proceedings of theIEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2109621106, 2022.",
  "S. Shrestha, C. Fermller, T. Huang, P. T. Win, A. Zukerman, C. M. Parameshwara, and Y. Aloimonos.Aimusicguru: Music assisted human pose correction. arXiv preprint arXiv:2203.12829, 2022": "R. C. Smpetru, A. Arkudas, D. I. Braun, M. Osswald, D. S. de Oliveira, B. Eskofier, T. M. Kinfe,and A. Del Vecchio. Sensing the full dynamics of the human hand with a neural interface and deeplearning. bioRxiv, pages 202207, 2022a. R. C. Smpetru, M. Osswald, D. I. Braun, D. S. Oliveira, A. L. Cakici, and A. Del Vecchio. Accuratecontinuous prediction of 14 degrees of freedom of the hand from myoelectrical signals throughconvolutive deep learning. In 2022 44th Annual International Conference of the IEEE Engineeringin Medicine & Biology Society (EMBC), pages 702706. IEEE, 2022b. I. Sosin, D. Kudenko, and A. Shpilman. Continuous gesture recognition from semg sensor datawith recurrent neural networks and adversarial domain adaptation. In 2018 15Th internationalconference on control, automation, robotics and vision (ICARCV), pages 14361441. IEEE, 2018.",
  "A. Spurr, J. Song, S. Park, and O. Hilliges. Cross-modal deep variational hand pose estimation.In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 8998,2018": "A. Spurr, U. Iqbal, P. Molchanov, O. Hilliges, and J. Kautz. Weakly supervised 3d hand poseestimation via biomechanical constraints. In European conference on computer vision, pages211228. Springer, 2020. A. Spurr, A. Dahiya, X. Wang, X. Zhang, and O. Hilliges. Self-supervised 3d hand pose estimationfrom monocular rgb via contrastive learning. In Proceedings of the IEEE/CVF internationalconference on computer vision, pages 1123011239, 2021.",
  "J. S. Supancic, G. Rogez, Y. Yang, J. Shotton, and D. Ramanan. Depth-based hand pose estimation:methods, data, and challenges. International Journal of Computer Vision, 126:11801198, 2018": "A. Tashakori, Z. Jiang, A. Servati, S. Soltanian, H. Narayana, K. Le, C. Nakayama, C.-l. Yang, Z. J.Wang, J. J. Eng, et al. Capturing complex hand movements and object interactions using machinelearning-powered stretchable smart textile gloves. Nature Machine Intelligence, 6(1):106118,2024. H. Truong, S. Zhang, U. Muncuk, P. Nguyen, N. Bui, A. Nguyen, Q. Lv, K. Chowdhury, T. Dinh, andT. Vu. Capband: Battery-free successive capacitance sensing wristband for hand gesture recognition.In Proceedings of the 16th ACM Conference on Embedded Networked Sensor Systems, pages 5467, 2018. C. Wan, T. Probst, L. V. Gool, and A. Yao. Self-supervised 3d hand pose estimation throughtraining by fitting. In Proceedings of the IEEE/CVF conference on computer vision and patternrecognition, pages 1085310862, 2019. J. Wang, F. Mueller, F. Bernard, S. Sorli, O. Sotnychenko, N. Qian, M. A. Otaduy, D. Casas, andC. Theobalt. Rgb2hands: real-time tracking of 3d hand interactions from monocular rgb video.ACM Transactions on Graphics (ToG), 39(6):116, 2020.",
  "Z. Yang, S. Yan, B.-J. F. van Beijnum, B. Li, and P. H. Veltink. Hand-finger pose estimation usinginertial sensors, magnetic sensors and a magnet. IEEE sensors journal, 21(16):1811518122, 2021": "Z. Yu, J. S. Yoon, I. K. Lee, P. Venkatesh, J. Park, J. Yu, and H. S. Park. Humbi: A large multiviewdataset of human body expressions. In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 29903000, 2020. S. Yuan, Q. Ye, B. Stenger, S. Jain, and T.-K. Kim. Bighand2. 2m benchmark: Hand pose dataset andstate of the art analysis. In Proceedings of the IEEE conference on computer vision and patternrecognition, pages 48664874, 2017. Y. Yuan, J. Song, U. Iqbal, A. Vahdat, and J. Kautz. Physdiff: Physics-guided human motion diffusionmodel. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages1601016021, 2023. Y. Zhang and C. Harrison. Tomo: Wearable, low-cost electrical impedance tomography for handgesture recognition. In Proceedings of the 28th Annual ACM Symposium on User InterfaceSoftware & Technology, pages 167173, 2015.",
  "(b) Did you specify all the training details (e.g., data splits, hyperparameters, how theywere chosen)? [Yes] See , Appendices A to C, and Tables 3, 6 and 7": "(c) Did you report error bars (e.g., with respect to the random seed after running experi-ments multiple times)? [Yes] For example, see Tables 4 and 5 and . We includestandard deviation and percentile statistics calculated across users, stages. We donot report errors bars due to multiple model random seeds, as we saw very minimaldifferences.",
  ". If you are using existing assets (e.g., code, data, models) or curating/releasing new assets": "(a) If your work uses existing assets, did you cite the creators? [Yes] See .5for details on existing baselines that we provide and architectures we use. See Sec-tion 3.2 and Appendix B.2 for details on the Optitrack system used to collect data. SeeAppendix B.1 for wrist-band details. See Appendix A for details including pythonpackages we build on.",
  ". If you used crowdsourcing or conducted research with human subjects": "(a) Did you include the full text of instructions given to participants and screenshots, ifapplicable? [No] We provide a detailed description of the experimental instructions anda list of the movements participants were asked to perform (see Appendices A and B). (b) Did you describe any potential participant risks, with links to Institutional ReviewBoard (IRB) approvals, if applicable? [Yes] We discuss the consenting process inAppendix A and the approval of all research under an external IRB (c) Did you include the estimated hourly wage paid to participants and the total amountspent on participant compensation? [N/A] We recruited all participants through athird-party vendor that determined their compensation via market rates. We give detailsin Appendix A.",
  "We provide a datasheet in accordance with Gebru et al.": "Motivation: The motivation for emg2pose is to address the lack of wide-spread, sufficiently large,non-invasive surface electromyographic (sEMG) datasets with high-quality ground-truth annotationsfor a concrete task. sEMG as a technology has the potential to revolutionize how humans interactwith computers, and this public dataset is motivated to facilitate progress in this domain withoutneeding specialized hardware. The task we consider is hand pose inference, as a potentially holisticand encompassing modality, with many biomimetic applications. This dataset was created by theCTRL-Labs research group within Reality Labs, Meta. Composition: The entire dataset consists of 25, 253 HDF5 files, each consisting of time-alignedsEMG and joint angles for a single hand in a single stage. In total, we collected data from 193participants, spanning 370 hours and 29 diverse stages. The number of hours includes both theright-handed and left-handed data for each participant, which were collected simultaneously. EachHDF5 file includes sEMG data from one hand, the stage label, and the joint angles. sEMG is recordedat 2kHz, high pass filtered at 40 Hz, and rescaled such that the noise floor has a standard deviation of1. We also flip the sign of the left-handed EMG data to account for the reversal of polarity caused bywearing the band on the left vs. right hand. Additionally, the dataset includes a metadata file in CSVformat containing dataset split information (train, val, and test). All metadata have been de-identifiedto remove any personally identifiable information and does not identify any sub-population. See for additional details on the dataset and for statistics about the dataset such asthe number of participants, total duration, number of sessions and stages. See for furtherdetails with regards to the stage composition. The configuration for the precise data splits used inour experiments can be found in the following link: Collection Process:We recruited participants through a third-party vendor, who compensatedparticipants at market rates. All recruitment and on-boarding followed an external IRB-approvedprotocol. We provided participants with information about the study, and before study initiation askedthem to review and sign an IRB-reviewed consent form. We gave all participants the opportunityto ask questions before the study and were able to discontinue participation at any point. Toensure participant well-being, on-site research administrators monitored participants during the studyprotocol. All data have been de-identified to remove any personally identifiable metadata. Participantsstood in a 26 camera motion capture array (Appendix B.2). A research assistant placed 19 motioncapture markers on each of the participants hands (Han et al. ) and an sEMG-RD band on eachwrist [CTRL-labs at Reality Labs et al., 2024](Appendix B.2). All sEMG and motion capture datawere streamed to a real-time data acquisition system at 2kHz and 60 Hz, respectively (Appendices B.1and B.2). Participants followed a standardized data collection protocol across a diverse set of 30-120s stages in which participants were prompted to perform a mix of 3-5 gestures. We organized thedata collection into two repetitions of two different groups of 15 and 26 stages with a different bandplacement for each. Each group of stages with a single band placement is referred as a session. Forfurther stage and data collection details see Appendices B.2.1 and B.3. Preprocessing/Cleaning/Labeling: sEMG recordings in the dataset are sampled at 2 kHz with a bitdepth of 12 bits, with a maximum signal amplitude of 6.6 mV, and are bandpass filtered with -3 dBcutoffs at 20 Hz and 850 Hz before digitization (see Appendix B.1). Joint angles were estimated from motion capture recordings using a custom inverse kinematicspipeline using a personalized hand model according to Han et al. . Briefly, 19 reflectivemarkers were attached to each hand, and their 3D coordinates were tracked via a commercialOptitrack system with 26 cameras around the participant. A ConvNet then assigned labels to eachmarker. The labeled markers were registered to positions on a calibrated hand mesh to determinelandmark positions. An inverse kinematics solver produced the final joint angles. We applied aconservative 15 Hz low pass filter (Ingram et al. ) to the final joint angles to ensure there is noresidual jitter. The mean absolute difference between the filtered and unfiltered signal was only 0.32degrees across 500 recordings. This model produced an estimate of joint angles for the MCP, PIP and DIP joints for each finger aswell as the IP, MCP and CMC joints of the thumb. Each joint had a degree of freedom for flexion andextension, while each MCP joint had an additional degree of freedom for abduction and adduction. Following joint angle estimation, we used a forward kinematic algorithm using a generic hand modelto produce estimates of landmark positions [Han et al., 2022]. We used the center of each joint,as well as the fingertips, as landmark positions for evaluation. Finally, joint angles were low-passfiltered at 15 Hz to remove tracking noise, and temporally upsampled to match to 2 kHz sample rateof sEMG. Uses: The dataset and the associated tooling are meant to be used only to advance sEMG-basedresearch topics of interest within the academic community for purely non-commercial purposes andapplications. Our code for baseline models, built on top of frameworks such as PyTorch, PyTorchLightning and Hydra, is designed such that it can be easily extended to the exploration of differentmodels and novel techniques for this task. The dataset and the associated code are not intended to beused in conjunction with any other data types. Distribution and Maintenance: The dataset and the code to reproduce the baselines are accessiblevia The dataset is hosted on Amazon S3and the code to reproduce the baseline experiments on GitHub under the CC-BY-NC-SA 4.0 license.We welcome contributions from the research community. Any future update, as well as ongoingmaintenance such as tracking and resolving issues identified by the broader community, will beperformed and distributed through the GitHub repository.",
  "B.1sEMG Sensing": "sEMG data were collected using the sEMG-RD [CTRL-labs at Reality Labs et al., 2024] consistingof 16 differential electrode pairs utilizing dry gold-plated electrodes. The 16 electrodes are arrangedon a rigid ribbon, leaving a gap between electrodes 0 and 15 on the ulnar side of the wrist close to theulnar styloid. Identical bands are worn on the left and right hands, with the same electrode indicesaligning with the same anatomical features, but the polarity of the differential sensing being reversed.The band is tightened with an elastic strap and the size of the gap depends on the subjects wrist sizeand tightness. The band is manufactured in three different sizes to account for large changes in wristsize and the electrodes themselves are spring-loaded to further adjust across small variations in wristsizes. In contrast, the previously used low-density Thalmic Labs Myo band [Rawat et al., 2016] onlystreams data at 200Hz, across 8 channels and at 8-bits.",
  "All motion capture data were collected using a 26 camera motion camera array at 60 Hz (Prime13WOptitrack) in an external data collection facility. Before data collection participants donned an sEMG": "band on each wrist and 19 3mm facial motion capture markers in order. We placed markers at thebase of each fingernail and between the DIP and PIP and PIP and MCP joints of each finger. For thethumb we placed markers between the IP and MCP joint, on the MCP joint, and between the MCPand CMC joints. We additionally placed markers in a triangular pattern on the dorsal side of the hand[Han et al., 2018]. Participants additionally wore a 3D printed frame of an XR headset, tethered to aPC, that was not relevant to the present data collection. Before collection, participants were asked toperform a series of 17 calibration gestures with each hand. These gestures were used as input to acustom optimization software that estimated the size of the hand and the position of motion capturemarkers relative to joints [Han et al., 2018]. We saved personalized hand model information to aseparate file to be used offline to estimate joint angles from the collected motion capture data. During data collection, sEMG data were streamed over Bluetooth to a real-time data collectionapplication. Motion capture data were recorded over ethernet using Motive2 (Naturalpoint) and thenstreamed to the same data collection pipeline. sEMG and motion capture datastreams were assignedsoftware timestamps based on their arrival at the data pipeline. Internal testing bounded the relativelatency between the two recording pathways to below 10 ms, approximately the Nyquist limit of the60 Hz Optitrack recording.",
  "B.2.1Data Collection Protocol": "Data collection was divided into 4 different sessions (band placements). Participants performed tworepetitions of two different groups of prompted stages. In each stage participants were asked tofollow along a video of a set of example movements, either a mix of discrete gestures or freeformunprompted movements. Stages lasted 45 to 60s, while freeform stages lasted 60 to 120s. Duringdata collection, users donned on-and-off the device on average 3.9 times in total, see . We callthese sessions and are clearly annotated in our dataset. Participants performed all movements whilestanding or sitting on a tall stool. During each stage participants were asked to move their hand fromright to left and up and down to ensure a broad range of postures.",
  "B.4Dataset Limitations": "While our dataset is the largest and highest fidelity open-sourced to date, it is smaller than those usedin CTRL-labs at Reality Labs et al. , which may hinder generalization. While we provide highquality pose labels from motion capture using the inverse kinematics approach from Han et al. ,as a camera-based method it still suffers from occlusion, hindering label quality for gestures suchas fist clenching. We additionally do not track wrist movements, which are important for how weinteract with the world. Alternate labelling methods, such as stretch-sensing gloves, could addressthese limitations, at the potential expense of lower quality labels and impaired dexterity. Finally,future datasets could include both camera and sEMG sensors, which could be combined to improvepose inference in contexts where camera-based tracking fails such as occlusion.",
  "B.5Ethical and Societal Implications": "The broader usage of sEMG and the specific development of sEMG pose estimation models maypose novel ethical and societal considerations. A highly performant emg-to-pose model running on adevice placed on the wrist or forearm could store or transmit information about a persons actions,and appropriate safeguards to encrypt and limit access to this information may be warranted. Thereare numerous societal benefits for the development of sEMG models for pose estimation. sEMGallows one to directly interface a persons neuromotor intent with a computing device. This canbe used to create novel device controls for the general population and can also be used to developadaptive controllers for those who struggle to use existing computer interfaces.",
  "svt = (zt, st1)(2)st = st1 + svt(3)": "where is the decoder, st is the angular prediction at time t, and svt is the angular velocity predictionat time t. For the tracking task, the ground truth first state is provided: s0 := s0. For regression, theground truth state is unknown. Therefore, the decoder produces angle and angular velocity predictions(sp and sv, respectively). The angular predictions are used for the first P time steps (250 ms in ourcase), and velocities are integrated thereafter:",
  "st =sptif t < P,st1 + svtif t P(5)": "The LSTM has two hidden layers of size 512. We scale its output by .01, as we find that this improvestraining. A Time-Depth Separable Convolution (TDS) network is used for the featurizer, as it hasbeen shown to be effective in the automatic speech recognition literature [Hannun et al., 2019]. Thefeaturizer first applies three 1D convolutions over time with 256 features, kernel widths of 11, 5, and17, and strides of 5, 2, and 4. There are then 4 TDS blocks with channel and feature widths of 16 andkernel widths of 9, 9, 5, and 5. Overall, the featurizer reduces the sample rate to 25 Hz, and a finallinear up-sampling brings them to 50 Hz. We use layer norms as described in [Hannun et al., 2019].",
  "C.2NeuroPose": "We implement the NeuroPose U-Net architecture as described in Liu et al. , with minormodifications to account for differences in recording device and joint angle targets. The encoder ofthe original NeuroPose has 40x temporal down-sampling achieved via a series of strides. To accountfor the 10x greater sample rate of our device, we double each of 3 temporal strides to yield 360xdown-sampling. Similarly, we double the spatial stride of the final encoder convolution to account forthe 2x spatial resolution of our device. We similarly modify the up-sampling in the decoder by thesame factors and add a final linear project to achieve 20 dimensional angular predictions. Note that the original NeuroPose uses a velocity regularization term, which we do not explore here.We find that predicting velocities rather than joint angles is sufficient to achieve smooth predictions,and precludes having to tune the weight on the velocity regularization term.",
  "C.3SensingDynamics": "The original SensingDynamics was designed for a high-density sEMG device with 320 electrodesspread over 5 separate patches on the forearm and wrist [Smpetru et al., 2022a]. The architectureuses 3d convolutions over channels, patches, and time. In contrast, the sEMG-RD wrist band fromCTRL-labs at Reality Labs et al. does not have separate patches and has distinct channeldensities and temporal resolutions. To account for these discrepancies, we use 2d convolutions over",
  "C.4Training Setup": "For each algorithm, we performed a hyperparamter sweep over the following parameters, with eachexplored independently: training window length (2000-12000 samples at 2kHz, 3 different values),learning rate (.001 or .0001), gradient norm clipping (none or 1), and whether the decoder usedan MLP or LSTM (for (v)emg2pose). The most performant setting was used for each algorithm,as reported in (for emg2pose explanation see Appendix D.2). A learning rate of 0.001was universally optimal. To improve generalization across device placements, we use rotationaugmentation, wherein we spatially rotate the sEMG channels by 1, 0, or 1 (uniformly sampled).Augmentation is only applied during training.",
  "C.5Online vemg2pose": "To enable online deployment of vemg2pose it must be setup to handle sEMG data being receivedsequentially in discrete packets of variable temporal lengths. As such, we created a variant ofvemg2pose that uses buffers to append the current packet of data to the previous ones. In addition tostoring all received data in a buffer, we additionally keep track of which data have been processedalready and which have not (this is a function of the network receptive field and the stride), so asnot to make duplicate predictions. For , we trained a vemg2pose, tracking model with thisinternal variant. This internal variant achieved joint angular errors almost identical to those reportedin , as expected. We do not open-source this setup, as it is only useful with access to thesEMG-RD band for online testing.",
  "C.6Hand mesh visualizations of prediction trajectories": "We generated the articulated hand meshes representing prediction trajectories (as depicted in and Appendix D.5) from sequences of joint angles using the forward kinematic and a mesh-skinningalgorithms provided by UmeTrack [Han et al., 2022]. We use the generic, default hand model providedby UmeTrack. To generate the figures, we render the meshes using the Plotly and Plotly-Kaleidovisualization packages [Plotly Technologies Inc., 2015].",
  "C.7Statistical Analysis": "The Wilcoxon statistical analyses reported in were performed on data aggregated across timefor each user. That is, metrics were computed at each temporal sample, then averaged across time foreach user within each experimental condition. Statistics aggregated within each user and conditionare similarly used to construct distributions for all other plots and tables.",
  "D.1Analysis of Stages that are Challenging for Vision-Based Systems": "We compared the same stages with and without occlusion, and found that occlusion did not negativelyimpact model performance, as expected (, left). Each subject performed the CountingWigglingand FingerPinches stages under two conditions: with the hands in front them - such that they wouldbe visible to a headset based CV tracking system - and with the hands very close to or very far awayfrom the body - such that they would be occluded. We also compared stages with hand-object interactions, hand-hand interactions, and no interactions(, right). Hand-object interactions consisted of the Object1 and Object2 stages, in whichparticipants interacted with a cup, a soft toy, blocks, and chess pieces. Hand-hand interactions(HandHandInteractions stage) consisted of sliding the fingers across the opposite palm, clapping thehands together, and wiggling the fingers such that the fingertips of opposite hands tap against oneanother. These interaction types are known to be challenging for vision-based systems. Nonetheless,performance for these stages was comparable or superior to performance in stages without anyinteractions. Note, however, that the behavioral distributions are different across these stages, whichmakes direct comparison of metrics challenging.",
  ": vemg2pose vs. emg2pose for tracking and regression tasks. Distributions are over users.Box plots take the same format as": "We compared vemg2pose to emg2pose, an otherwise identical algorithm that directly predicts jointangles rather than joint angular velocities (). emg2pose has similar joint angular error in theregression task, but much worse performance on the tracking task. This is likely because vemg2poseis initialized to the ground truth initial state, whereas emg2pose is merely conditioned on the groundtruth initial state. For both tracking and regression tasks, vemg2pose has lower overall velocity thanemg2pose, suggesting that operating in velocity space encourages smoother predictions.",
  "D.3LSTM vs Transformer Decoders": "We ablated over decoder architectures, specifically LSTMs and transformers as the two most widelyadopted models for sequence modelling. For the transformer, we explored the widely adoptedtransformer encoder BERT setup [Kenton and Toutanova, 2019]. We swept over the number of layers(2, 4, 6) and number of heads (2, 4, 8) reporting the best for both regression and tracking tasks inTables 8 and 9. In order to fit into memory (Amazon EC2 g4dn.metal instances which have 8xNVIDIA T4 GPUs) we had to halve the feature dimensionality of the transformer decoder. In general,the transformer performs similarly or slightly worse than the LSTM.",
  "D.4Performance Decomposition across Fingers and Joints": ": Performance decomposition per finger: for tracking task, vemg2pose. Error per fingeris measured by averaging the errors of the joints associated with each finger. Distributions are overusers. Box plots take the same format as . We decompose vemg2pose tracking performance across fingers (), and proximal, mid, anddistal joint groups (). For the latter, proximal, mid, and distal joints are grouped accordingto their distance from the palm. See for further details. Reconstruction performance variesacross fingers and joint groups. Thumb and pinky fingers are consistently best and worst performers,and proximal joints are more easily predicted than distal joints. : Performance decomposition across joint groups: for tracking task, vemg2pose. Perfor-mance broken down by joint according to their proximal-distal location. Proximal is CMC for thethumb and MCP for other fingers; Mid is MCP for thumb and PIP for other fingers; and Distal is IPfor thumb and DIP for other fingers. Box plots take the same format as .",
  "D.5Tracking Trajectory Examples": "We provide representative vemg2pose, tracking prediction trajectories for the 15%, 50% and 85% userand stage percentiles for the held-out user, stage scenario described in .4. Performance variesconsiderably across held-out users and stages, as seen in Figs. 12 to 15. We note that there may existlarge variance within stages, for which these kinematic plots do not reflect. Minimizing these varianceswill be of great value. For video examples, visit"
}