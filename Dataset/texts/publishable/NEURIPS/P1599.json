{
  "Abstract": "Temporal context plays a significant role in temporal action segmentation. Inan offline setting, the context is typically captured by the segmentation networkafter observing the entire sequence. However, capturing and using such contextinformation in an online setting remains an under-explored problem. This workpresents the an online framework for temporal action segmentation. At the core ofthe framework is an adaptive memory designed to accommodate dynamic changesin context over time, alongside a feature augmentation module that enhances theframes with the memory. In addition, we propose a post-processing approach tomitigate the severe over-segmentation in the online setting. On three commonsegmentation benchmarks, our approach achieves state-of-the-art performance.",
  "Introduction": "This work addresses online temporal action segmentation (TAS) of untrimmed videos. Such videostypically feature procedural activities consisting of multiple actions or steps in a loose temporalsequence to achieve a goal . For example, making coffee has actions: take cup, pour coffee,pour water, pour milk, pour sugar and stir coffee. Standard TAS models areoffline and segment-only videos of complete procedural activities. An online TAS model, in contrast,segments only up to the current time point and does not have access to the entire video and, therefore,the entire activity. Online TAS faces challenges similar to other online tasks in establishing a scalable networkthat can retain useful information from an ever-increasing volume of data and facilitate effectiveretrieval when required. Additionally, over-segmentation is a common issue for offline TAS, where thesegmentation model divides an action into many discontinuous sub-segments, leading to fragmentedoutputs. This issue is exacerbated in the online setting, as partial data at the onset of an action maylead to erratic predictions and increased over-segmentation. Most relevant to our task is online temporal action detection (TAD) . Online TAD aims to identifywhether an action is taking place and the action category. TAD targets datasets like THUMOS ,TVSeries , and HACS Segment . Among these, 90.8% videos of THUMOS featureonly multiple instances of the same action, while TVSeries comprises diverse yet independentactions (e.g., open door, wave and write) in one video. These actions do not necessarily correlatewith one another or impose specific temporal constraints. As such, a direct adaptation of popularonline TAD approaches like LSTR and MAT to online segmentation is non-ideal. Forinstance, these models encode temporal context with a fixed set of tokens, which may limit theircapability to handle the relations of procedural videos. Furthermore, these models are typicallytrained to prioritize frame-level accuracy while neglecting temporal continuity, which invariably leadsto over-segmentation.",
  "arXiv:2411.01122v1 [cs.CV] 2 Nov 2024": "To address the online action segmentation task, this work proposes a novel framework centered ona context-aware feature augmentation module and an adaptive memory bank . The memory bank,per-video, tracks short-term and long-term context information. The augmentation module uses anattention mechanism to allow frame features to interact with context information from the memorybank and integrate temporal information into standard frame representations. Finally, we introducea post-processing technique for online boundary adjustment that imposes duration and predictionconfidence constraints to mitigate over-segmentation. Summarizing our contributions, 1) We establish an online framework for TAS; 2) We proposea feature augmentation module that generates context-aware representations by incorporating anadaptive memory, which accumulates temporal context collectively. The module operates on framefeatures independently of model architecture, enabling flexible integration; 3) We present a simplepost-processing technique for online prediction adjustment, which can effectively mitigate the over-segmentation problem; and 4) Our framework achieves the state-of-the-art online segmentationperformance on three TAS benchmarks.",
  "Related Work": "Online Action Understanding. Many video understanding tasks, such as action detection and video instance segmentation , have been explored in online contexts. For onlineaction detection, videos are classified frame by frame without access to future frames. Specifically,LSTR employs a novel memory mechanism to model long- and short-term temporal dependenciesby encoding them as query tokens. Follow-up works feature a segment-based long-term memorycompression and fusing short- and long-term histories via attention . However, the videos in the datasets commonly used in online TAD contain independent actions orsequences of limited actions , thus lacking temporal relations between the actions. In contrast,TAS deals with untrimmed procedural videos, where such relations are more prominent and mayspan over long temporal durations. There is also a growing trend in online TAD models to use actionanticipation as an auxiliary task to enhance action modeling . In our online segmentation task,we do not assume the availability of such information. Temporal Action Segmentation. In TAS , methods vary by their level of supervision, includingfully , semi-supervised , weakly , and unsupervised setups. An emerging direction is to learn TAS incrementally where proceduralactivities are learned sequentially. However, all existing works are offline, and complete videosequences can be used for inference. In contrast, our approach functions within an online setup. Themost related work investigates online TAS in a multi-view setup and leverages the offline modelto assist online model learning. Furthermore, it uses the frame-wise multi-view correspondenceto generate pseudo-labels for action segments. In contrast, we do not assume the availability ofmulti-view videos nor require assistance from a pre-trained offline model. Post-processing for Action Segmentation. Post-processing methods are either rule-based or leveragegraphical modelling. Rule-based approaches apply predefined rules to smooth outshort-duration predictions that are unlikely given the context. Graphical modelling approaches useConditional Random Fields (CRFs) to model the relationships and transitions betweenconsecutive actions. Online methods need post-processing to alleviate over-segmentation and helplocate the action boundaries.",
  "Online Action Segmentation": "Previous studies have demonstrated that the scope of the temporal context significantlyinfluences the performance of (offline) TAS models. This motivates our two lines of inquiry forour online setting: 1) how to consolidate temporal context over an extended period, and 2) how toenrich the frame representations with context to benefit the segmentation. This work introducesa context-aware feature augmentation module (Sec. 3.2) alongside an adaptive context memory(Sec. 3.3) to tackle the above questions.",
  "Preliminaries": "Consider an untrimmed video v = {xt}Tt=1 of T frames, where xt RD is the per-extracted framefeature at time t and D is the feature dimension. An action segmentation model partitions v intoN contiguous and non-overlapping temporal segments {si = (yi, i)}Ni=1 corresponding to actionsy Y present in the video, where indicates the segment length and Y defines the action space .A widely adopted strategy for TAS is to design a segmentation model M that predicts the actionlabel yt for each frame xt, akin to a frame-wise classification. In the offline setting , theper-frame prediction yt is based on the entire video sequence. The online setting uses only frames upto the current prediction time t, without access to future frames. Comparatively:",
  "wheret,y =t,y :t,y :otherwiseandt,y = |log pt(y) log pt1(y)|": "In this paper, we opt for the widely recognized convolution-based architecture MS-TCN as ourfoundational framework. This choice is driven by its relatively lower computational requirementsthan the attention- or diffusion-based models . A straightforward transition from offlinemode to an online mode of the segmentation model M is to substitute standard convolutions withcausal convolutions. Causal and standard convolutions differ in their receptive field in that causalconvolutions consider only past and present inputs while standard convolutions may incorporate bothpast and future inputs within a kernel. Mathematical details and illustrations of the two are shown inthe Appendix.",
  "Context-aware Feature Augmentation": "The context-aware feature augmentation (CFA) module generates enhanced clip-wise features throughinteractions with temporal context captured by an adaptive memory bank. The module operateson a clip-wise basis. During training, video v is split into K non-overlapping clips {ck}Kk=1. Eachclip has a window size w and is sampled from v with a stride of = w, where the final clipcK is padded if |cK| < w. The CFA module integrates the original pre-extracted frame featuresck = {xt}kwt=(k1)w+1 with temporal context to produce a context-enhanced version of representationsck = {xt}kwt=(k1)w+1. Like , our CFA module is also equipped with a simultaneouslyupdated memory bank Mk as a context resource for feature augmentation. The memory bank isfurther described in Sec. 3.3. At each step k, context is accumulated by feeding ck through a lightweight GRU to obtain cGRUk.The GRU is reliable in capturing information over long video sequences . The clip is then passedthrough a context aggregation block to be augmented. The context aggregation block incorporates theGRU features cGRUkwith the memory state Mk1 from the previous step for I iterations. Concretely,we pass cGRUkthrough a self-attention (SA) block to encourage information exchange with the localclip window. Additionally, we leverage a Transformer decoder to achieve a more effectivememory encoding M TDk1, i.e.,",
  "Adaptive Memory Bank": "In a similar spirit with , our memory is designed to account for both short- and long-termcontext, i.e., M = [M long, M short]. Short-term memory helps capture the local action dynamics whilelong-term memory retains information across extended durations important for TAS . Short Memory M short. Given that our enhancement module works on a per-clip basis with temporalstride w, we directly regard the enhanced feature ck1 from the last clip as the short-term memory,i.e., M shortk= ck1 RDw.",
  ": end for": "Training. Our final online segmentation model isconstructed by combining our CFA module witha single-stage TCN with causal convolutions.Specifically, TCN takes as input the enhanced repre-sentations ck and maps them to the same labels forck. We train the framework end-to-end with the lossfunction formulated in Eq. (2), but on a clip basiswith T replaced by w:",
  "enables the context memory to flexibly shift its attention between short and long-term information asthe video progresses. Algorithm 1 summarizes the update mechanism": "Discussion. Our module integrates context memory on top of a GRU layer. While the GRU capturestemporal dependencies, it may struggle, especially in thousands-frame long sequences common inTAS. The context memory supplements the GRU by allowing selective access and updates, therebyenabling the retrieval and manipulation of long-term information. Such a design is supported byour empirical study that the explicit memory can extend the capacity of the GRUs internal state.Supporting ablations are found in Sec. 4.1.",
  "we set = 0.15 following . Like , trainingon a clip basis provides better efficiency": "Inference. We present two distinct inference ap-proaches by manipulating the clip sampling stride parameter . The first mode of inference, referredto as online, is characterized by setting = 1. In such a setting, a video clip of w frames is processed,with emphasis placed solely on the prediction derived from the final frame and rest are discarded.This facilitates the scenario when frame-by-frame prediction is preferred. The alternate mode ofinference, termed semi-online, adheres to the training regime by setting = w. In this mode, videoclips are processed, and the dense predictions generated across all w frames are preserved as finaloutput. An illustration of two modes of inference is provided in .",
  "Post-processing": "Our intuition is that a valid action segment should not fall below a minimum length threshold unlessthere is high confidence in the prediction to justify a change in the action class. Specifically, weconsider the maximum softmax probability of a prediction as its confidence measure, denoted asqt = max(pt), for frame xt, as qt to some extent indicates its reliability. A prediction is consideredunreliable if its confidence measure scores below a certain threshold , i.e., qt < . For theframe with unreliable prediction, we disregard its current prediction and assign the action label ofits proceeding frame yt1, when the previous action segment is shorter than the minimum length.",
  "Experiments": "Datasets: We evaluate our model on three common TAS datasets. Breakfast comprises in total1,712 videos performing ten different activities with 48 actions. On average, each video contains sixaction instances. 50Salads has 50 videos with 17 action classes. GTEA contains 28 videosof seven kitchen activities composing 11 different actions. We use common I3D features as input. Evaluation Metrics: Standard evaluation metrics for TAS are reported for our online setting, whichincludes frame-wise accuracy (Acc), segmental edit score (Edit), and segmental F1 scores withvarying overlap thresholds 10%, 25%, and 50%. Implementation Details. In CFA, we stack 2 Transformer decoder layer with 8 heads, 2 Swin self- and cross attention with 4 heads. We use a single-stage TCN as segmentation backbone andsample non-overlapping clips i.e., = w for efficiency. We train the model end-to-end with a learningrate of 5e4 of total 50 epochs. Detailed hyperparameter settings can be found in Appendix.",
  "Experiment Results": "Effectiveness. We report the overall performance for online and semi-online inference (see Sec. 3.4)in in . Our baseline is a single-stage causal TCN and we build our framework on top ofit. Across all three datasets, the integration of our CFA module leads to a consistent boost in thesegmentation performance. Specifically, our approach gained 5.7% (75.2% vs. 80.9%) in Acc and9.2% (19.6% vs. 28.8%) in Edit on 50Salads. While the improvements on other datasets are not assignificant, they still show effectiveness, with a margin of about 2%. Generally, semi-online inferenceachieves better performance over online across all metrics. Such improvement is likely becauseclip-wise prediction better preserves the local temporal continuity of labels compared to step-by-stepsingle frame prediction. Comparing across the metrics, segmental scores appear to be significantly low. On breakfast with our online inference, a frame-wise accuracy of 56.7% only corresponds to a 9.3% F1 scorewith 50% IoU. Such score indicates a severe over-segmentation issue and necessitates an effectivepost-processing. However, a significant performance increase is observed on Edit and F1 scoresafter our proposed pose-processing. For example, the same F1 score increases to 30.5%, tripling itsoriginal value. Although post-processing could lead to a slight decrease in accuracy, it demonstratesgreat effectiveness in mitigating the over-segmentation problem.",
  ": Effect of interactions I": "Ablation study. evaluates the components in our CFA module. The first row is our single-layercausal TCN baseline with strong frame-wise accuracy but poor segmental metrics. The GRU boostssegment metrics (7-11%) over the baseline, showing its ability to accumulate context information.While CFA using the current clip as pseudo memory predictably leads to a performance drop (5%)compared to GRU due to lack of any context information. Combining either GRU or our adaptivememory with our CFA achieves very close performance (rows 4 and 5), highlighting the importanceof the context information for TAS. The complete model yields the best performance and boosts Accby 7% and average segmental scores by 15.3%. This validates the complementary effect of GRUsinternal state and our explicit memory design. Number of layers in CFA. explores the interaction iterations I in CFA. The results indicatethat the performance is not significantly affected by the number of iterations. In practice, we set thenumber of iterations to 2, as it achieves a good balance between performance and efficiency.",
  ": Effect of clip size and memory length.Seg. indicates the mean of Edit and F1 scores": "Memory composition (M short/M long). We assess the impact of memory types and present resultsin . It shows comparable performances for each memory type when considered individually.However, the combination of both yields a 2% improvement in Acc and 1.5% for averaged segmentalmetric, suggesting the significance of incorporating diverse memory for TAS. Clip size w / Memory size len(M). In our implementation, we set clip size and memory size tobe equal and we report its influence on performance in . It shows a larger clip size leads tobetter segmental results; this is because temporal continuity can be better modeled with longer clipsfor learning. However, the information of short actions could be diluted when compressed to formthe memory token if the window size is too large. For memory sizes of 16, 32, and 64, the earliestmemory are discarded as the average length of 50Salads is 5.8k frames. With the size reducing, theperformance gradually drops and reaches the lowest of 79.8% in Acc compared to the peak of 82.4%.Note that with the memory size set to 16, our approach only retains long-term information from up to192 frames, 30 less than the average video length. Post-processing hyperparameters. Two hyperparameters are defined in our post-processing: confi-dence threshold and the minimum segment length min. We vary its scaling factor to assess min.In , increasing greatly enhances the segment results, with a 18.1% increase observed when = 0.7. Although the accuracy tends to decrease as becomes larger, the drop is not as substantial(3.1%) compared to the improvements in segmental results. While shows the segmentalperformance stops increasing and stays stable when >116 with a fixed confidence score = 0.9. Inconclusion, employing a higher confidence threshold can help better mitigate the over-segmentationbecause it makes more sense to prioritize preserving the continuity of a segment that includes frameswith highly confident predictions given a fixed length budget.",
  "Comparison with State-of-the-Art Methods": "Tables 8 and 9 compare our approach against the state-of-the-art TAS approaches on all threebenchmarks. Due to the absence of dedicated online TAS methods, we benchmark against the onlineTAD approach LSTR . We train LSTR on TAS datasets using the official code implementation2.To ensure a fair comparison, we configure their working (short-term) memory to be the same as ours(w). Additionally, we adjust its long memory accordingly to provide access to the entire past sequence.As evident from Tables 8 and 9, LSTR consistently achieves relatively low performance,particularly with Edit scores of 5.0% and 4.9% on 50Salads and Breakfast datasets, respectively. Thissuggests severe over-segmentation in their predictions. Moreover, these performances are inferioreven to those of our baseline model (casual TCN), indicating that a direct adoption of online detectionmodels for the segmentation task is not ideal. Amongst all datasets, Breakfast is the most challenging, with a significant performance gap betweenoffline and online models, particularly on segmental metrics. Notably, the F1@50 score on Breakfastexperiences a drastic drop of 4/5, from 47.5% to 8.3%, highlighting the difficulty of the onlinesegmentation task with videos that are more complex. Nonetheless, we still achieve a modest absoluteperformance improvement of 2%. Furthermore, our post-processing technique, significantly boostssegmental performance, nearly tripling the original performance, albeit with a slight decrease inAcc. This underscores the effectiveness of our post-processing technique in mitigating the over-segmentation. MV-TAS tackles online segmentation but under a multi-view setting. It leveragesmulti-view information and an offline model as a reference for online segmentation. Despite this, evenour baseline model, depicted in the third-to-last row of , showcases a notable performanceimprovement (55.3% vs. 41.6%) over MV-TAS . This considerable margin emphasizes thecompetitiveness of our baseline model. When compared to offline models, our semi-online inference with post-processing manages to surpassthe offline model MS-TCN on 50Salads dataset across the segmental metrics and reaches around90% of the accuracy of the best-performing DiffAct . On Breakfast, our approach lags behind theoffline model in both frame-wise accuracy and segmental metrics.",
  ": Comparison with the state-of-the-art methods on Breakfast": "remains significant after the post-processing. Under the same configuration, our semi-online achievesslightly better results compared to the frame-by-frame online inference. Our post-processing, whenapplied, successfully removes the short fragments (blue boxes) in the raw prediction and refines thesegmentation output. However, it may reduce accuracy, particularly at action boundaries (red boxes). For failure cases, we have the following two observations: 1) Detection of action start often delaysdue to the need for more frame information to predict new actions, especially when facing semanticambiguities at action boundaries; 2) Persistent over-segmentation happens when the network makesincorrect but confident predictions, which could be improved with a stronger backbone or bettertemporal context modeling.",
  "Runtime Analysis": "We evaluate the runtime performance of our approach using an Nvidia A40 GPU with both pre-computed I3D features and raw streaming RGB inputs, and present the inference times in . Asshown, our approach can achieve up to 238.1 FPS when using pre-computed I3D features. To calculatethe runtime for the entire segmentation pipeline, we take into consideration of the computationaloverheads of optical flow calculation and the I3D feature extraction. By leveraging a GPU backendfor optical flow calculation, our full framework is able to achieve a runtime of 33.8 FPS. Inference Latency. The inference speed presented above is identical for both online and semi-onlineinference modes since their input sizes are the same. However, the latency can differ. In the onlinemode, inference is performed on a per-frame basis, meaning its latency is only dependent on the",
  "inference speed. In contrast, the semi-online mode incurs additional latency as it requires gatheringframes up to the clip lengths before forming inputs": "Online inference offers better real-time responsiveness compared to semi-online inference, but thelatter achieves superior performance as we discussed in Sec. 4.1. The choice between these twomodes depends on the applications priorities: if the real-time inference is critical, online inference ispreferable; however, if accuracy is more important and the task is less time-sensitive, semi-onlineinference is recommended.",
  ": Runtime profile (in ms and FPS)": "Limitation. In this work, we only evaluate our approach on cooking videos, however, handlingdiverse and real-world videos may present several additional challenges. One common scenarioinvolves interrupted actions, where a subject abruptly switches to a different action, leaving theongoing action unfinished. These interruptions can be challenging for the model to handle effectively.Additionally, the extended length of the video poses another challenge. Streaming videos can beinfinitely long, so effectively managing and preserving long-form history within a fixed memorybudget becomes a critical issue.",
  "Conclusion": "This paper presents the first framework for the online segmentation of actions in procedural videos.Specifically, we propose an adaptive memory bank designed to accumulate and condense temporalcontext, alongside a feature augmentation module capable of injecting context information into inputsand producing enhanced representations. In addition, we propose a fast and effective post-processingtechnique aimed at mitigating the over-segmentation problem. Extensive experiments on commonbenchmarks have shown the effectiveness of our approach in addressing the online segmentation task.",
  "and Disclosure of Funding": "This research is supported by the National Research Foundation, Singapore under its NRF Fellowshipfor AI (NRF-NRFFAI1-2019-0001). Any opinions, findings and conclusions or recommendationsexpressed in this material are those of the author(s) and do not reflect the views of National ResearchFoundation, Singapore. E. Bahrami, G. Francesca, and J. Gall. How much temporal long-term context is needed foraction segmentation? In Proceedings of the IEEE/CVF International Conference on ComputerVision, pages 1035110361, 2023.",
  "G. Ding and A. Yao. Temporal action segmentation with high-level complex activity labels.IEEE Transactions on Multimedia, 25:19281939, 2022": "Z. Du, X. Wang, G. Zhou, and Q. Wang. Fast and unsupervised action boundary detection foraction segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 33233332, 2022. Y. A. Farha and J. Gall. Ms-tcn: Multi-stage temporal convolutional network for actionsegmentation. In Proceedings of the IEEE/CVF conference on computer vision and patternrecognition, pages 35753584, 2019.",
  "A. Fathi, X. Ren, and J. M. Rehg. Learning to recognize objects in egocentric activities. InCVPR 2011, pages 32813288. IEEE, 2011": "R. Ghoddoosian, I. Dwivedi, N. Agarwal, C. Choi, and B. Dariush. Weakly-supervised onlineaction segmentation in multi-view instructional videos. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition, pages 1378013790, 2022. M. Guermal, A. Ali, R. Dai, and F. Brmond. Joadaa: joint online action detection and actionanticipation. In Proceedings of the IEEE/CVF Winter Conference on Applications of ComputerVision, pages 68896898, 2024.",
  "Y. Kong and Y. Fu. Human action recognition and prediction: A survey. International Journalof Computer Vision, 130(5):13661401, 2022": "H. Kuehne, A. Arslan, and T. Serre. The language of actions: Recovering the syntax andsemantics of goal-directed human activities. In Proc. IEEE Conference on Computer Vision andPattern Recognition (CVPR), pages 780787, 2014. S. Kumar, S. Haresh, A. Ahmed, A. Konin, M. Z. Zia, and Q.-H. Tran. Unsupervised actionsegmentation by joint representation learning and online clustering. In Proceedings of theIEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2017420185,2022. C. Lea, M. D. Flynn, R. Vidal, A. Reiter, and G. D. Hager. Temporal convolutional networksfor action segmentation and detection. In proceedings of the IEEE Conference on ComputerVision and Pattern Recognition, pages 156165, 2017.",
  "J. Li, P. Lei, and S. Todorovic. Weakly supervised energy-based learning for action segmentation.In Proceedings of the IEEE/CVF international conference on computer vision, pages 62436251,2019": "J. Li and S. Todorovic. Action shuffle alternating learning for unsupervised action segmentation.In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,pages 1262812636, 2021. S. Li, Y. A. Farha, Y. Liu, M.-M. Cheng, and J. Gall.Ms-tcn++: Multi-stage temporalconvolutional network for action segmentation. IEEE transactions on pattern analysis andmachine intelligence, 45(6):66476658, 2020. S. Lin, L. Yang, I. Saleemi, and S. Sengupta. Robust high-resolution video matting withtemporal guidance. In Proceedings of the IEEE/CVF Winter Conference on Applications ofComputer Vision, pages 238247, 2022.",
  "D. Liu, Q. Li, A.-D. Dinh, T. Jiang, M. Shah, and C. Xu. Diffusion action segmentation. InInternational Conference on Computer Vision (ICCV), 2023": "Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo. Swin transformer:Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVFinternational conference on computer vision, pages 1001210022, 2021. D. Moltisanti, S. Fidler, and D. Damen. Action recognition from single timestamp supervisionin untrimmed videos. In Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 99159924, 2019.",
  "M. K. Myers, N. Wright, A. S. McGough, and N. Martin. O-talc: Steps towards combatingoversegmentation within online action segmentation. arXiv preprint arXiv:2404.06894, 2024": "R. Rahaman, D. Singhania, A. Thiery, and A. Yao. A generalized and robust framework fortimestamp supervision in temporal action segmentation. In European Conference on ComputerVision, pages 279296. Springer, 2022. A. Richard, H. Kuehne, and J. Gall. Action sets: Weakly supervised action segmentationwithout ordering constraints. In Proceedings of the IEEE conference on Computer Vision andPattern Recognition, pages 59875996, 2018. A. Richard, H. Kuehne, A. Iqbal, and J. Gall. Neuralnetwork-viterbi: A framework for weaklysupervised video learning. In Proceedings of the IEEE conference on Computer Vision andPattern Recognition, pages 73867395, 2018. S. Sarfraz, N. Murray, V. Sharma, A. Diba, L. Van Gool, and R. Stiefelhagen. Temporally-weighted hierarchical clustering for unsupervised action segmentation. In Proceedings of theIEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1122511234, 2021. F. Sener, D. Singhania, and A. Yao. Temporal aggregate representations for long-range videounderstanding. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK,August 2328, 2020, Proceedings, Part XVI 16, pages 154171. Springer, 2020.",
  "F. Sener and A. Yao. Unsupervised learning and segmentation of complex activities from video.In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages83688376, 2018": "Z. Shou, D. Wang, and S.-F. Chang. Temporal action localization in untrimmed videos viamulti-stage cnns. In Proceedings of the IEEE conference on computer vision and patternrecognition, pages 10491058, 2016. D. Singhania, R. Rahaman, and A. Yao. Iterative contrast-classify for semi-supervised tem-poral action segmentation. Proceedings of the AAAI Conference on Artificial Intelligence,36(2):22622270, Jul 2022.",
  "S. Venkatesh, D. Moffat, and E. R. Miranda. Investigating the effects of training set synthesisfor audio segmentation of radio broadcast. Electronics, 10(7):827, 2021": "J. Wang, G. Chen, Y. Huang, L. Wang, and T. Lu. Memory-and-anticipation transformer foronline action understanding. In Proceedings of the IEEE/CVF International Conference onComputer Vision, pages 1382413835, 2023. X. Wang, S. Zhang, Z. Qing, Y. Shao, Z. Zuo, C. Gao, and N. Sang. Oadtr: Online actiondetection with transformers. In Proceedings of the IEEE/CVF International Conference onComputer Vision, pages 75657575, 2021. M. Xu, M. Gao, Y.-T. Chen, L. S. Davis, and D. J. Crandall. Temporal recurrent networks foronline action detection. In Proceedings of the IEEE/CVF international conference on computervision, pages 55325541, 2019.",
  "F. Yi, H. Wen, and T. Jiang. Asformer: Transformer for action segmentation. In BMVC, 2021": "K. Ying, Q. Zhong, W. Mao, Z. Wang, H. Chen, L. Y. Wu, Y. Liu, C. Fan, Y. Zhuge, andC. Shen. Ctvis: Consistent training for online video instance segmentation. In Proceedings ofthe IEEE/CVF International Conference on Computer Vision, pages 899908, 2023. T. Zhang, X. Tian, Y. Wu, S. Ji, X. Wang, Y. Zhang, and P. Wan. Dvis: Decoupled videoinstance segmentation framework. In Proceedings of the IEEE/CVF International Conferenceon Computer Vision, pages 12821291, 2023. H. Zhao, A. Torralba, L. Torresani, and Z. Yan. Hacs: Human action clips and segments datasetfor recognition and temporal localization. In Proceedings of the IEEE/CVF InternationalConference on Computer Vision, pages 86688678, 2019.",
  "A.3Implementation": "Hyper-parameters. As shown in , the hyper-parameter settings are generally the samefor each dataset. GTEA uses a shorter window size because the longest video is only about 2000frames, whereas the other datasets all use a window size of 128. The optimal confidence thresholdfor segmental metrics varies for each dataset: for GTEA is 0.6; for 50Salads is 0.9; and for Breakfastis 0.8.",
  "A.4AsFormer Performance": "We conduct experiments on three common TAS datasets, where we replace the MS-TCN backbonewith AsFormer. In MS-TCN, the transition to an online method is relatively straightforward, as itonly requires replacing all the standard convolution layers with causal convolution layers. However,in AsFormer, the transformation involves more extensive modifications. In addition to replacingthe convolution layers with causal convolutions, we also modify the standard attention layers intocausal attention layers. Furthermore, we incorporate our proposed GRU, CFA, Memory Bank, anda Post-processing module to ensure that AsFormer transitions from an offline method to an onlinemethod. Our approach remains highly effective in boosting online segmentation performance whilemaintaining the strengths of the AsFormer architecture."
}