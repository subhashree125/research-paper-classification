{
  "Abstract": "This paper aims at developing novel shuffling gradient-based methods for tacklingtwo classes of minimax problems: nonconvex-linear and nonconvex-strongly con-cave settings. The first algorithm addresses the nonconvex-linear minimax modeland achieves the state-of-the-art oracle complexity typically observed in nonconvexoptimization. It also employs a new shuffling estimator for the hyper-gradient,departing from standard shuffling techniques in optimization. The second methodconsists of two variants: semi-shuffling and full-shuffling schemes. These variantstackle the nonconvex-strongly concave minimax setting. We establish their oraclecomplexity bounds under standard assumptions, which, to our best knowledge, arethe best-known for this specific setting. Numerical examples demonstrate the per-formance of our algorithms and compare them with two other methods. Our resultsshow that the new methods achieve comparable performance with SGD, supportingthe potential of incorporating shuffling strategies into minimax algorithms. 1IntroductionMinimax problems arise in various applications across generative machine learning, game theory,robust optimization, online learning, and reinforcement learning (e.g., ). These models often involve stochastic settings or large finite-sum objective functions.To tackle these problems, existing methods frequently adapt stochastic gradient descent (SGD)principles to develop algorithms for solving the underlying minimax problems . For instance,in generative adversarial networks (GANs), early algorithms employed stochastic gradient descent-ascent methods where two routines, each using an SGD loop, ran iteratively . However, practicalimplementations of SGD often incorporate shuffling strategies, as seen in popular deep learninglibraries like TensorFlow and PyTorch. This has motivated recent research on developing shufflingtechniques specifically for optimization algorithms . Our work builds uponthis trend by developing shuffling methods for two specific classes of minimax problems.",
  "arXiv:2410.22297v1 [math.OC] 29 Oct 2024": "where f : Rp R {+} is a proper, closed, and convex function, Hi : Rp Rq R are smoothfor all i [n] := {1, 2, , n}, and h : Rq R {+} is also a proper, closed, and convexfunction. In this paper, we will focus on two classes of problems in (1), overlapped to each other.",
  "(NC) Hi is nonconvex in w and Hi(w, )h() is strongly concave in u for all (w, u) dom (L)": "Although (NC) looks more general than (NL), both cases can be overlapped, but one is not a specialcase of the other. Under these two settings, our approach will rely on a bilevel optimization approach,where the lower-level problem is to solve maxu L(w, u), while the upper-level one is minw L(w, u). Challenges. The setting (NL) is a special case of stochastic nonconvex-concave minimax problemsbecause the objective term H(w, u) := F(w), Ku is linear in u. It is equivalent to the compositionalmodel (CO) described below. However, if h is only merely convex and not strongly convex (e.g., theindicator of a standard simplex), then 0 in (CO) becomes nonsmooth regardless of Fs properties.This presents our first challenge. A natural approach to address this issue, as discussed in ,is to smooth 0. The second challenge arises from the composition between the outer function hand the finite sum F() in (CO). Unlike standard finite-sum optimization, this composition preventsany direct use of existing techniques, requiring a novel approach for algorithmic development andanalysis. The third challenge involves unbiased estimators for gradients or hyper-gradients inminimax problems. Most existing methods rely on unbiased estimators for objective gradients, withlimited work exploring biased estimators. While biased estimators can be used, they require variancereduction properties (see, e.g., ). The setting (NC) faces the same second and third challengesas the setting (NL). Additionally, when reformulating it as a minimization problem using a bileveloptimization approach (3), constructing a shuffling estimator for the hyper-gradient 0 becomesunclear. This requires solving the lower-level maximization problem (2). Therefore, it remains anopen question whether shuffling gradient-type methods can be extended to this bilevel optimizationapproach to address (1). In this paper, we address the following research question:",
  "Can we efficiently develop shuffling gradient methods to solve (1) for both (NL) and (NC) settings?": "Our attempt to tackle this question leads to a novel way of constructing shuffling estimators for thehyper-gradient 0 or its smoothed counterpart. This allows us to develop two shuffling gradient-based algorithms with rigorous theoretical guarantees on oracle complexity, matching state-of-the-artcomplexity results in shuffling-type algorithms for nonconvex optimization. Related work. Shuffling optimization algorithms have gained significant attention in optimization andmachine communities, demonstrating advantages over standard SGDs, see, e.g., . Nevertheless, applying these techniques to minimax problems like (1) remains challenging,with limited existing literature (e.g., ). Das et al. in explored a specific case of (1)without nonsmooth terms f and h, assuming strong monotonicity and L-Lipschitz continuity of thegradient H := [wH, uH] of the joint objective H. Their algorithm simplifies to a shufflingvariant of fixed-point iteration or a gradient descent-ascent scheme, not applicable to our settings.Cho and Yun in built upon by relaxing the strong monotonicity to Polyak-ojasiewicz (P)conditions. This work is perhaps the most closely related one to our algorithm, Algorithm 2, for the(NC) setting. Note that the method in exploits Nashs equilibrium perspective with a simultaneousupdate, which is different from our alternative update. Moreover, only considers the noncompositecase with f = 0 and h = 0. Though we only focus on a nonconvex-strongly-concave setting (NC),our results here can be extended to the P condition as in . Very recently, Konstantinos et al.in introduced shuffling extragradient methods for variational inequalities, which encompassconvex-concave minimax problems as a special case. However, this also falls outside the scope ofour work due to the nonconvexity of (1) in w. Again, all the existing works in utilize aNashs equilibrium perspective, while ours leverages a bilevel optimization technique. Besides, incontrast to our sampling-without-replacement approach, stochastic and randomized methods (i.e.using i.i.d. sampling strategies) have been extensively studied for minimax problems, see, e.g.,. A comprehensive comparison can be found, e.g., in .",
  "Contribution. Our main contribution can be summarized as follows": "(a) For setting (NL), we suggest to reformulate (1) into a compositional minimization and exploita smoothing technique to treat this reformulation. We propose a new way of constructingshuffling estimators for the hyper-gradient (cf. (10)) and establish their properties. (b) We propose a novel shuffling gradient-based algorithm (cf. Algorithm 1) to approximatean -KKT point of (1) for the setting (NL). Our method requires O(n3) evaluations ofFi and Fi under the strong convexity of h, and O(n7/2) evaluations of Fi and Fiwithout the strong convexity of h, for a desired accuracy > 0. (c) For setting (NC), we develop two variants of the shuffling gradient method: semi-shufflingand full-shuffling schemes (cf. Algorithm 2). The semi-shuffling variant combines bothgradient ascent and shuffling gradient methods to construct a new algorithm, which requiresO(n3) evaluations of both wHi and uHi. The full-shuffling scheme allows to performboth shuffling schemes on the maximization and the minimization alternatively, requiringeither O(n3) or O(n4) evaluations of uHi depending on our assumptions, whilemaintaining O(n3) evaluations of wHi for a given desired accuracy > 0. If a random shuffling strategy is used in our algorithms, then the oracle complexity in all the casespresented above is improved by a factor of n. Our settings (NL) and (NC) of (1) are different fromexisting works , as we work with general nonconvexity in w, and linearity or [strong]concavity in u, and both f and h are possibly nonsmooth. Our algorithms are not reduced or similarto existing shuffling methods for optimization, but we use shuffling strategies to form estimators forthe hyper-gradient 0 in (5). The oracle complexity in both settings (NL) and (NC) is similar tothe ones in nonconvex optimization and in a special case of (1) from (up to a constant factor). Paper outline. The rest of this paper is organized as follows. presents our bileveloptimization approach to (1) and recalls necessary preliminary results. develops ourshuffling algorithm to solve the setting (NL) of (1) and establishes its convergence. proposesnew shuffling methods to solve the setting (NC) and investigates their convergence. presentsnumerical experiments, while technical proofs and supporting results are deferred to Supp. Docs. Notations. For a function f, we use dom (f) to denote its effective domain, and f for its gradientor Jacobian. If f is convex, then f denotes a subgradient, f is its subdifferential, and proxf isits proximal operator. We use Ft to denote (w0, w1, , wt), a -algebra generated by randomvectors w0, w1, , wt, Et[] = E[|Ft] is a conditional expectation, and E[] is the full expectation.As usual, O() denotes Big-O notation in the theory of algorithm complexity.",
  "(w) := 0(w) + f(w).(3)": "Clearly, this approach is sequential, and only works if 0 is well-defined, i.e. (2) is globally solvable.Hence, the concavity of H(w, )h() w.r.t. to u is crucial for this approach as stated below. However,this assumption can be relaxed to a global solvability of (2) combined with a P condition as in .",
  "where h(v) := supu{v, u h(u)}, the Fenchel conjugate of h, and 0(w) = h(KT F(w)). Ifh is not strongly convex, then h is convex but possibly nonsmooth": "(b) Technical assumptions. To develop our algorithms, we also need the following assumptions.Assumption 2. h is h-strongly convex with h 0, and dom(h) is bounded by Mh < +.Assumption 3 (For Fi). For setting (NL) with Hi(w, u) := Fi(w), Ku (i [n]), assume that (a) Fi is continuously differentiable, and its Jacobian Fi is LFi-Lipschitz continuous.(b) Fi is also MFi-Lipschitz continuous or equivalently, its Jacobian Fi is MFi-bounded.(c) There exists a positive constant J (0, +) such that",
  "Condition (6) can be relaxed to the form 1": "nni=1 Fi(w) F(w)2 2J + J0(w)2 forsome J 0, where 0 is a [sub]gradient of 0 or (its smoothed approximation). Moreover,under Assumption 3, if h > 0, then h is Lh-Lipschitz continuous with Lh :=1h . Thus it ispossible (see ) to prove that 0 is differentiable, and 0 is also L0-Lipschitz continuous withL0 := MhKLF + M 2F K2",
  "Technical condition to handle the possible nonsmooth term f": "To handle the nonsmooth term f of (1) in our algorithms we require one more condition as in .Assumption 5. Let be defined by (10), which reduces to 0 given by (2) as 0+, and G bedefined by (18). Assume that there exist two constants 0 1 and 1 0 such that:",
  "(ii ) If f = W, the indicator of a nonempty, closed, convex, and bounded set W, thenAssumption 5 also holds by the same reason as in Example (i) (see Supp. Doc. A)": "3Shuffling Gradient Method for Nonconvex-Linear Minimax ProblemsWe first propose a new construction using shuffling techniques to approximate the true gradient in (11) for any 0. Next, we propose our algorithm and analyze its convergence. 3.1The shuffling gradient estimators for Challenges. To evaluate (w) in (11), we need to evaluate both F(w) and F(w) at each w.However, in SGD or shuffling gradient methods, we want to approximate both quantities at eachiteration. Note that this gradient can be written in a finite-sum 1",
  "Our estimators. Let F(t)(i)(w(t)i1) and F(t)(i)(w(t)i1) be the function value and the Jacobian": "component evaluated at w(t)i1 respectively for i [n], where (t) = ((t)(1), (t)(2), , (t)(n))and (t) = ((t)(1), (t)(2), , (t)(n)) are two permutations of [n] := {1, 2, , n}. We wantto use these quantities to approximate the function value F(w(t)0 ) and its Jacobian F(w(t)0 ) of F atw(t)0 , respectively, where w(t)0the iterate vector at the beginning of each epoch t.",
  "(w(t)i1) := (F (t)i)T (F (t)i) (F (t)i)T Ku(F (t)i).(24)": "Discussion. The estimator F (t)ifor F requires n i more function evaluations F(t)(j)(w(t)0 ) ateach epoch t. The first option (21) for F uses 2n function evaluations Fi, while the second one in(22) only needs n function evaluations at each epoch t 0. However, (21) uses the most updatedinformation up to the inner iteration i compared to (22), which is expected to perform better. TheJacobian estimator F (t)iis standard and only uses one sample or a mini-batch at each iteration i.",
  "Convergence Analysis of Algorithm 1 for Nonconvex-Linear Setting (NL)": "Now, we are ready to state the convergence result of Algorithm 1 in a short version: Theorem 1. Thefull version of this theorem is Theorem 6, which can be found in Supp. Doc. B.Theorem 1. Suppose that Assumptions 1, 2, 3, and 5 holds for the setting (NL) of (1) and > 0 is asufficiently small tolerance. Let { wt} be generated by Algorithm 1 after T = O(3) epochs usingarbitrarily permutations (t) and (t) and a learning rate t = := O() (see Theorem 6 in Supp.Doc. B for the exact formulas of T and ). Then, we have1",
  "T +1Tt=0 E[Gt( wt)2] 2": "Our first goal is to approximate a stationary point w of (CO) as E[G( w)2] 2, while Algorithm 1only provides an -stationary of (10). For a proper choice of , it is also an -stationary point of (3).Corollary 1. Let wT defined by (19) be generated from { wt} of Algorithm 1. Under the conditionsof Theorem 1 and any permutations (t) and (t), the following statements hold.",
  "If, in addition, (t) and (t) are sampled uniformly at random without replacement and independently,and 1 = O(n1), then the numbers of evaluations of Fi and Fi are reduced by a factor of n": "4Shuffling Method for Nonconvex-Strongly Concave Minimax ProblemsIn this section, we develop shuffling gradient-based methods to solve (1) under the nonconvex-strongly concave setting (NC). Since this setting does not cover the nonconvex-linear setting (NL) in as a special case, we need to treat it separately using different ideas and proof techniques.",
  "To make our method more flexible, we allow to perform either only one iteration (i.e. S = 1) ormultiple iterations (i.e. S > 1) of (26). Each iteration s requires n evaluations of uHi": "(a2) Shuffling gradient ascent scheme for the lower-level problem. We can also construct ut bya shuffling gradient ascent scheme. Again, we allow to run either only one epoch (i.e. S = 1) ormultiple epochs (i.e. S > 1) of the shuffling algorithm to update ut, leading to the following scheme:Starting from s := 1 with u(t)0:= ut1, at each epoch s = 1, 2, , S, having u(t)s1, we generate apermutation (s,t) of [n] and run a shuffling gradient ascent scheme as",
  ": end for": "Discussion. Algorithm 2 has a similar form as Algorithm 1, where u0( wt1) is approximated by ut.In Algorithm 1, u0( wt1) is approximated by u(F (t)i). Moreover, Algorithm 1 solves the smoothedproblem (10) of (3), while Algorithm 2 directly solves (3). Depending on the choice of method toapproximate u0( wt1), we obtain different variants of Algorithm 2. We have proposed two variants:",
  "Note that Theorem 2 holds for both S > 1 and S = 1 (i.e. we perform only one iteration of (26))": "(b) Convergence of the full-shuffling variant The case S > 1 with multiple epochs. We state ourresults for two separated cases: only Hi is H-strongly convex, and only h is h-strongly convex.Theorem 3 (Strong convexity of Hi). Suppose that Assumptions 1, 2, 4, and 5 hold, and Hi isH-strongly concave with H > 0 for i [n], but h is only merely convex. Let {( wt, ut)} be generated by Algorithm 2 using S epochs of the shuffling routine (27) andfixed learning rates t = := O() as given in Theorem 8 of Supp. Doc. C for a given > 0,t := = O(), S := ln(7/2)",
  "T +1Tt=0 G( wt)2 2": "Consequently, Algorithm 2 requires O(n3) evaluations of wHi and O(n4) evaluations ofuHi to achieve an -stationary point wT of (3) computed by (19).Theorem 4 (Strong convexity of h). Suppose that Assumptions 1, 2, 4, and 5 hold for (1), and h ish-strongly convex with h > 0, but Hi is only merely concave for all i [n]. Then, under the samesettings as in Theorem 3, but with S := ln(7/2)",
  "uH(w, u)2 0 G(u)2 + 1,(w, u) dom (L) .(29)": "Clearly, if h = 0, then G(u) = uH(w, u) and (20) automatically holds for 0 = 1 and 1 = 0.Assumption 6 is similar to Assumption 5, and it is required to handle the prox operator of h in (27).Assumption 7. For f in (1), there exists Lf 0 such that",
  "Under these additional assumptions, we have the following result.Theorem 5. Suppose that Assumptions 1, 2, 4, 5, 6, and 7 hold and G is defined by (18)": "Let {( wt, ut)} be generated by Algorithm 2 using one epoch (S = 1) of the shuffling routine (27),and fixed learning rates t = := O() as in Theorem 9 of Supp. Doc. C for a given > 0, t := = 302, and T := O(3), where :=Lu",
  "Consequently, Algorithm 2 requires O(n3) evaluations of both wHi and of uHi to achieve an-stationary point wT of (3) computed by (19)": "Similar to Algorithm 1, if (s,t) and (t) are generated randomly and independently, 1 = O(1/n),and 1 = O(1/n), then our complexity stated above can be improved by a factor of n. Nevertheless,we omit this analysis. Finally, we can combine each Theorem 2, 3, 4 or 5 and Lemma 2 to constructan -KKT point of (1). Theorem 5 has a better complexity than Theorems 3 and 4, but requiresstronger assumptions. Algorithm 2 is also different from the one in both in terms of algorithmicform and the underlying problem to be solved, while achieving the same oracle complexity. 5Numerical ExperimentsWe perform some experiments to illustrate Algorithm 1 and compare it with two existing and relatedalgorithms. Further details and additional experiments can be found in Supp. Doc. D.",
  "2u2": "Here, we apply our problem (31) to solve a model selection problem in binary classification withnonnegative nonconvex losses, see, e.g., . Each function Fi,j belongs to 4 different nonconvexlosses (m = 4): Fi,1(w, ) := 1 tanh(biai, w), Fi,2(w, ) := log(1 + exp(biai, w)) log(1 + exp(biai, w 1)), Fi,3(w, ) := (1 1/(exp(biai, w) + 1))2, and Fi,4(w, ) :=log(1 + exp(biai, w)) (see for more details), where (ai, bi) represents data samples.",
  "As shown in , the two variants of our SGM have a comparable performance with SGD andProx-Linear, providing supportive evidence for using shuffling strategies in minimax algorithms": "6ConclusionsThis work explores a bilevel optimization approach to address two prevalent classes of nonconvex-concave minimax problems. These problems find numerous applications in practice, including robustlearning and generative AIs. Motivated by the widespread use of shuffling strategies in implementinggradient-based methods within the machine learning community, we develop novel shuffling-basedalgorithms for solving these problems under standard assumptions. The first algorithm uses a non-standard shuffling strategy and achieves the state-of-the-art oracle complexity typically observed innonconvex optimization. The second algorithm is also new, flexible, and offers a promising possibilityfor further exploration. Our results are expected to provide theoretical justification for incorporatingshuffling strategies into minimax optimization algorithms, especially in nonconvex settings.",
  "uH(w, u0(w)) h(u0(w)).(33)": "(b) Details of Assumption 5 and Assumption 6. Both Assumptions 5 and 6 look relatively technical,though they have been used in previous works such as . Both assumptions are the same, but onefor f and the other for h, and thus we only discuss Assumption 5. Note that did not provide any example to motivate Assumption 5 for the case f = 0. As-sumption 5 extends the one from so that it holds for certain cases, including the two examplesdescribed after Assumption 5. Here, we further elaborate these examples in detail.",
  "Mf": "(ii) Example 2. It is also easy to check that if f = W, the indicator of a nonempty, closed,convex, and bounded set W, then for any w W, we also have proxf(w) w =projW(w) w 2diam(W), where diam(W) is the diameter of W. Hence, by thesame proof as in Example 1, Assumption 5 also holds.",
  "wH(w, u).Hence, combining this relation and 0 wH(w, u) + f(w), we have0 0(w) + f(w), which shows that w is a stationary point of (3). The converse state-ment is proved similarly, and we omit": "(b) If wT is an -stationary point of (3), then using a shorthand gT := G( wT ), we have E[gT 2] 2. From (18), we have gT = 1( wT proxf( wT 0( wT ))), which is equivalent togT 0( wT ) + f( wT gT ). Let us define wT as in Lemma 2 and eT as follows:",
  "iij=1F(t)(j)(w(t)0 ) F(w(t)0 )2": "For each epoch t = 1, , T, we denote by Ft := (w(1)0 , , w(t)0 ) as the -algebra generatedby the iterates of our algorithm (cf. Algorithm 1) up to the beginning of the epoch t. We observethat the permutation (t) used at time t is independent of the -algebra Ft. We also denote byEt[] := E[ | Ft] as the conditional expectation on the -algebra Ft.",
  "Combining the last two inequalities, and noting that 0 i n, we obtain (60)": "Finally, we can prove the necessary bound for ut u0( wt1)2. For simplicity of our proof, let usdenote gs,ti1() := H(s)(i)( wt1, ) and again ut := u0( wt1). By Assumption 4(a) and (b), it isclear that gs,ti1() is H-strongly convex and Lu-smooth. Let us consider the following the Bregmandistance constructed from gs,ti1:",
  "C.2Convergence of the semi-shuffling variant of Algorithm 2": "We now prove the convergence of the semi-shuffling variant of Algorithm 2 using (26).Lemma 16. Suppose that Assumptions 4 and 5 hold for (1). Let be defined by (3) and G bedefined by (18). Let {( wt, ut)} be generated by the semi-shuffling variant of Algorithm 2 using (26).For a fixed > 0, suppose that we choose t and t such that 1 3L2w2t 0 and 0 < t 2",
  "We can combine the results above to obtain the following lemma": "Lemma 17. Suppose that Assumptions 4 and 5 hold for (1), be defined by (3), and G be definedby (18). Let {( wt, ut)} be generated by the full-shuffling variant of Algorithm 2 using (27). For afixed > 0, assume that t and t are chosen such that 1 3L2w2t 0 and2L0t + 2L2u22t 1,",
  "Rearranging this inequality, we prove (84)": "The following theorem, Theorem 8, is the full version of Theorem 3 in the main text, where thelearning rates t and t, and the numbers of epochs S and T are given explicitly.Theorem 8 (Strong convexity of Hi). Suppose that Assumptions 1, 2, 4, and 5 hold for (1), and Hiis H-strongly concave with H > 0 for all i [n], but h is only merely convex. Let 0 be definedby (3), and G be defined by (18). We define Cw and Cu respectively as",
  "H = O(1), we get Tu := O(n4). The total number of evaluationsof wHi is Tw := Tn = O(n3) as stated": "Finally, since each epoch t, Algorithm 2 requires one evaluation of proxtf, and S evaluations ofproxth, but since S = O(1), the total number of proxtf evaluations is T = O(3), whilethe total number of proxth evaluations is TS = O(4). Overall, Algorithm 2 needs O(3)evaluations of proxtf and O(4) evaluations of proxth.",
  "which is exactly (112)": "Now, we are ready to prove the convergence of Algorithm 2 using only one epoch (i.e. S = 1) of theshuffling routine (27). The following theorem is the full version of Theorem 5 in the main text.Theorem 9. Suppose that Assumptions 1, 2, 4, 5, 6, and 7 hold for (1) under the (NC) setting. Let0 be defined by (3), and G be defined by (18). Let us denote Cw and Cu respectively by",
  "requires one evaluation of proxtf, and one evaluation of proxth, the total number of both proxtfand proxtf evaluations is T = O(3)": "DDetails and Additional Results of Numerical ExperimentsThis section provides the details of our experiments in and also adds more experimentsto illustrate our algorithms and compares them with two other methods. All the algorithms weexperiment in this paper are implemented in Python and are run on a MacBook Pro. 2.8GHzQuad-Core Intel Core I7, 16Gb Memory.",
  "D.1Details of Numerical Experiments in": "We have abbreviated Algorithm 1 by SGM in . Since we have two options to constructestimator F (t)ifor F( wt1), we name SGM-Option 1 for Algorithm 1 using (21), and SGM-Option2 for Algorithm 1 using (22). Implementation details and competitors. Since 0(v) = maxu11{v, u} in our model (31)is nonsmooth, we have implemented two other algorithms, SGD in a variant of the stochasticgradient method for compositional minimization, and Prox-Linear in a type of the Gauss-Newton method with variance-reduction using large mini-batches for compositional minimization.Since SGD only works for smooth 0, we have smoothed it as in our method, and utilized the estimatorand algorithm from , but also updated the smoothness parameter as in our method. Here, we onlycompare the performance of all algorithms in terms of epochs (i.e. the number of data passes) andignore their computational time since Prox-Linear becomes slower if p is getting large. This is dueto its expensive subproblem of evaluating the prox-linear operator. To compare with SGD and Prox-Linear, we only use Algorithm 1 since both SGD and Prox-Linearare designed to solve compositional minimization problems of the form (CO).However,Prox-Linear requires to solve a nonsmooth convex subproblem to evaluate the prox-linear op-erator. Therefore, we have implemented a first-order primal-dual scheme in to evaluate thisoperator, which we believe that it is an efficient method.",
  "Parameter selection. To boost the performance of all algorithms, we implement mini-batch variantsof these methods instead of a single sample variant. Our batch size b is computed by b := n": "kb , wheren is the number of data points and kb is the number of blocks. In our experiments, we have also variedthe number of blocks kb to observe the performance of these algorithms. Since we want to obtain goodperformance, instead of using their theoretical learning rates, we have carefully tuned the learning rate of all algorithms in a given set of candidates {100, 50, 10, 5, 1, 0.5, 0.1, 0.05, 0.01, 0.001, 0.0001}.We find = 5 (i.e. t = 104) for w8a and = 100 (i.e. t = 5 105) for rcv1 which work wellfor our method. We also update the smoothness parameter as :=1 2(t+1)1/3 w.r.t. to the epochcounter t instead of fixing it at a small value. For w8a, we find = 0.05 as a good learning rate forboth SGD and Prox-Linear. For rcv1, we get = 0.5 for both algorithms. All experiments are runup to 200 epochs.",
  ": The performance of 4 algorithms for solving (31) in terms of gradient mapping norm": "It seems that both options, SGM-Option 1 and SGM-Option 2 are almost identical for this test.For w8a, our methods look like having comparable performance with both SGD and Prox-Linear,just slightly better. For rcv1, our methods reach a better approximate solution earlier than SGD, butafter more than 200 epochs, SGD tends to approach a similar accuracy level. Prox-Linear has asignificantly worse performance than ours and SGD in this particular experiment.",
  "We provide additional experiments to test our algorithms and compare them with SGD andProx-Linear as in": "The effect of mini-batch size. Our first test is to verify if the mini-batch size b actually affects theperformance of these algorithms. We use the same datasets and the same parameters as in ,but reduce b by increasing kb from 32 to 64 blocks. reveals the performance of 4 algorithmson two datasets with kb = 64: w8a corresponding to b = 777 and rcv1 corresponding to b = 316.",
  "Prox-Linear requires a large mini-batch to achieve a variance reduce, and decreasing this mini-batchsize indeed affects its performance": "Different learning rates. Now, let us test our algorithms using different learning rates, we only focuson Option 2 as both options show similar results in our tests. For w8a, we choose 4 different learningrates = 0.5, 2.5, 5.0, and 7.5, while maintaining kb = 64. For rcv1, we also choose 4 differentlearning rates = 25, 50, 100, and 125. The results of this experiment are plotted in forboth w8a and rcv1 datasets.",
  "For rcv1, we also observe similar behaviors as in w8a, but with larger learning rates than = 125": "Large dataset. We have also run our algorithms and their competitors on a bigger dataset fromLIBSVM: url with n = 2, 396, 130 and p = 3, 231, 951. Here, we use a learning rate = 1 for ourmethods, which corresponds to t = 4.2 107. For SGD, we use a learning rate = 0.01 and forProx-Linear, we use a learning rate = 0.01 after tuning both methods. We also set kb = 64 forall algorithms. The results of this experiment are reported in .",
  "G. Gidel, H. Berard, G. Vignoud, P. Vincent, and S. Lacoste-Julien. A variational inequalityperspective on generative adversarial networks. pages 110, 2018": "I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, andY. Bengio. Generative adversarial nets. In Advances in neural information processing systems,pages 26722680, 2014. E. Gorbunov, H. Berard, G. Gidel, and N. Loizou. Stochastic extragradient: General analysisand improved rates. In International Conference on Artificial Intelligence and Statistics, pages78657901. PMLR, 2022.",
  "F. Lin, X. Fang, and Z. Gao. Distributionally robust optimization: A review on theory andapplications. Numerical Algebra, Control & Optimization, 12(1):159, 2022": "N. Loizou, H. Berard, G. Gidel, I. Mitliagkas, and S. Lacoste-Julien. Stochastic gradientdescent-ascent and consensus optimization for smooth games: Convergence analysis underexpected co-coercivity. Advances in Neural Information Processing Systems, 34:1909519108,2021. L. Luo, H. Ye, and T. Zhang. Stochastic recursive gradient descent ascent for stochasticnonconvex-strongly-concave minimax problems. Advances in Neural Information ProcessingSystems, 33:2056620577, 2020."
}