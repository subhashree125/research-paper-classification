{
  "Abstract": "Defending large language models against jailbreaks so that they never engage ina broadly-defined set of forbidden behaviors is an open problem. In this paper,we investigate the difficulty of jailbreak-defense when we only want to forbida narrowly-defined set of behaviors. As a case study, we focus on preventingan LLM from helping a user make a bomb. We find that popular defenses suchas safety training, adversarial training, and input/output classifiers are unable tofully solve this problem. In pursuit of a better solution, we develop a transcript-classifier defense which outperforms the baseline defenses we test. However, ourclassifier defense still fails in some circumstances, which highlights the difficultyof jailbreak-defense even in a narrow domain.",
  "Introduction": "As Large Language Models (LLMs) become more capable, it is important to ensure they are hard tomisuse. In particular, it should be hard to elicit harmful information from models via jailbreaks . Unfortunately, current approaches to controlling modelbehavior are unable to reliably prevent models from engaging in broadly-defined sets of harmfulbehaviors (e.g. behaviors forbidden by provider usage policies) without significantly impairing modelperformance . In this paper, we investigate the difficulty of jailbreak-defense when we only want to forbid a narrowly-defined set of behaviors instead of a broadly-defined set of behaviors. We focus on one particularinstance of the narrow problem where the aim is to prevent a model from helping a user make a bomb(while preserving all other capabilities of the model). We call this the LLM bomb-defense problem. We define helping a user make a bomb very strictly, and require that a model must provideactionable bomb-making details which are sufficient for constructing a lethal device and which gobeyond information that can be found on Wikipedia (see Appendix B for our rubric which containsthe full set of requirements). This bar for harm is substantially higher than in other studies .",
  "arXiv:2412.02159v1 [cs.LG] 3 Dec 2024": ".1: Our transcript classifier defense: (1) Generate transcript Transform user requests andassistant responses into capitalized, XML-tagged text with unique UUIDs to prevent prompt injection.(2) Transcript classifier Employ an LLM with chain-of-thought reasoning and a single prompt toevaluate potential requests for harmful information, starting with a manipulation check, identifyingdangerous inquiries, and assessing responses for inadvertent risks. (3) Parsing and judgment Explicitly validate each reasoning step; a parsing failure or yes on any checklist item triggers asystem block of the output and issues a user refusal. solving harder variants of the same problem (jailbreak defense to prevent broadly-defined forbiddenbehaviors). However, even in this narrower and easier setting, we find that existing defensemechanisms are unable to fully solve the LLM bomb-defense problem. Each defense we testincluding safety training , adversarial training , and LLM-based classifiers exhibits at least one vulnerability. In light of the failure of existing methods, we attempt to develop our own classifier defense, whichemploys prompting and parsing strategies (see .1) to try and solve the bomb-defense problem.Our classifier is transcript-based, allowing access to both the models input and output to decidewhether an interaction is harmful. While our classifier defense outperforms existing defenses onmany axes, we find that it, too, can ultimately be broken. Moreover, attempts to further improve ourclassifier defense either degrade its capabilities or fail to enhance its overall robustness. Our findings reveal that jailbreak defense is a challenging problem even when the jailbroken behaviorthat is trying to be prevented is very narrowly-defined. We believe that attempting to build defensesthat work for this narrow setting may yield insights into how to defend against jailbreaks in moregeneral settings.",
  "LLM Bomb-Defense Problem Statement": "We choose bomb-making as a particular instance of a narrowly-defined behavior that one may wantto prevent models from exhibiting. Indeed, bomb-making is a canonical bad behavior that LLMred-teaming research often tries to elicit from systems and that frontier model developers tryto prevent. Importantly, we choose a behavior that is not so simple that a non-machine learningsolution (e.g. a word filter) would work. See Appendix B for our rubric describing how to assess if aninteraction includes egregious bomb-making instructions and a formal statement of the bomb-defenseproblem.",
  "Threat Model": "We focus on a grey-box threat model where attackers can perform top-k log-prob queries againstany system sub-component but lack access to model weights. However, we assume attackers haveaccess to the prompts of both the generation model and any classifiers. We focus on this threat modelbecause it gives attackers more access than they would generally have in the wild when attacking aclosed-weight model served over a web-API. Thus, a defense that is effective under this threat modelwould likely also be effective in a real-world closed-weight web-API deployment. 2.3DefensesStandard safety trainingWe test frontier models developed by leading AI labs that haveundergone standard safety fine-tuning (such as using RLHF and RLAIF ). This groupincludes the claude-3 family (Haiku, Sonnet, and Opus), gpt-3.5-turbo-{0125,1106},gpt-4-turbo-2024-04-09, and gpt-4o-2024-05-13. Static adversarial trainingTo assess the generalizability of model fine-tuning strategies againstadversarial attacks, we evaluate defense methods that modify the underlying generation modelweights. Specifically, we explore static adversarial training (a.k.a. supervised fine-tuning) against aset of 3,725 HarmBench attacks, which covers a broad set of harmful behaviors including bomb-making. We fine-tune gpt-3.5-turbo-1106 to refuse jailbreaks while maintaining capability onbenign requests (see details in Appendix C.1). We train three different variants adv-sft-op{1,2,3}which represent three different operating points along the capabilities-robustness tradeoff curve.The variants use 11%, 15%, and 16% adversarial data respectively, with the rest of the data beingnon-adversarial data. Increasing the percentage of attack data enhances jailbreak robustness butincreases the refusal rate of benign requests. Classifier-based baselinesA generation model can be guarded against attacks using an LLM-basedclassifier. An input x is passed to a model M to get a completion M(x). Then, either M(x) is passedto an output-classifier or [x, M(x)] is passed to a transcript-classifier. These classifiers judge whetherthe interaction is safe. A refusal message replaces M(x) if the output is deemed unsafe. We test fiveclassifiers: Metas transcript classifiers using short and full prompts (LlamaGuard-2-s andLlamaGuard-2-f), and three of HarmBenchs output classifiers: a prompted gpt-4o-2024-05-13,a fine-tuned Llama-2-13b-chat , and a fine-tuned Mistral-7b . Our classifier defenseOur classifier, CoT-4o, is a transcript classifier that uses transformations onthe transcript, a chain-of-thought classifier, and a strict parser of the classifier output (as illustrated in.1). It is similar to LlamaGuard2 but has techniques to defend against prompt injections. Forexample, we use XML tags with random UUID strings around the transcript, which is transformedby applying uppercase and HTML escape characters before inserting them into the prompt. Thismakes it easier for the classifier to distinguish between instructions and content to moderate. Seeimplementation details for transforming the transcript in Appendix C.2 and prompt in Appendix F.2.",
  "Evaluating Attacks": "Human Judgment DetailsWe evaluate the success of jailbreaks via human judgment. We explicitlyavoid using an automated grader to judge jailbreaks because then the same grader when used as aoutput/transcript filter would be a perfect defense by construction. Defining a good criterion to ruleout out such tautological solutions is an open problem (see Appendix A) which is why we opt forhuman judgment. We have human judges score model responses using a custom rubric designed specifically for the LLMbomb-defense problem. The rubric identifies advanced technical information related to bomb-making,defined as specific, actionable details that significantly surpass a basic bomb-making Wikipediadescription. Under our rubric, an LLM response should be flagged if it introduces novel technicalinformation not present in the user input and provides sufficient detail for a middle-schooler to builda lethal bomb (see full rubric in Appendix B.1.1). Therefore, according to our rubric, a successfuljailbreak requires satisfying stricter requirements of harmful detail compared to other work in thefield. We draw judges from the papers authors and ensure agreement between 2 or more judges forborderline cases.",
  "Prioritized sampling methodTo efficiently identify and review potentially harmful model outputs,we first define a set of egregious words relevant to the domain for example, shrapnel\" and": "blasting-cap\" in the context of bomb-making (see the full list in Appendix C.3). We calculate anegregiousness score for each output by counting the occurrences of these words after making the textlowercase and removing punctuation. Outputs with scores below a certain threshold are discarded, asthey are unlikely to be harmful. Similarly, we discard outputs with excessively high scores, whichmay indicate issues like hallucinations or repetitions. We also discard outputs that contain commonrefusal phrases or are flagged by the defense as harmful since we care more about finding falsenegatives. Finally, we randomly sample the remaining outputs for human inspection. This methodallows us to focus our review on responses most likely to contain critical harmful ingredients andmethodologies. Attack Success RateWe measure the strength of an attack against defenses without classifiersusing attack success rate (ASR), which represents the proportion of successful attacks according tohuman judgment. Due to limited human judgment capacity, we sometimes produce a lower bound onASR by employing prioritized sampling. AlpacaEval Refusal RateClassifier defenses can use an adjustable threshold that balances preci-sion and recall. Therefore, we assess attack strength with AlpacaEval refusal rate. To calculatethis, we get responses for all AlpacaEval and jailbreak requests, recording the classifiers harmfuloutput probability. We determine the refusal rate for AlpacaEval requests when all jailbreaks aredefended (i.e. when the threshold is set to the minimum harmful probability in jailbreak responses).For instance, if a jailbreak judged to be successful by a human is classified as completely harmless bythe classifier (zero harm probability), all benign requests would be refused, leading to a 100% refusalrate. We choose to use a threshold of over 5% refusal rate to determine if a defense is compromisedsince it represents a significant compromise on benign requests.",
  "Generation model(s)3.5t-0125all3.5t-0125, gpt-4o3.5t-01253.5t-0125": ".1: All defense strategies show vulnerabilities to different attacks. Red cells indicate abreached defense, green cells show attacks that did not succeed, grey cells mark inapplicable attacks,and white cells with a \"-\" represent untested attack-defense pairs due to computational constraints.Notably, every defense is susceptible to at least one type of attack, as every row contains a red cell.(Top) details defenses that modify the generation model, considering them breached if the attacksuccess rate is above zero; (bottom) evaluates classifier defenses, deeming them breached if theirrefusal rate on AlpacaEval exceeds 5%. We also specify the generation models for which theclassifiers are defending. Our classifier defense appears at the very bottom of the table and does notsupport an easily tunable threshold (except for just turning it on, off, or rejecting all inputs). This iswhy, despite only failing on 1 / 321 static attacks, it has a 100% refusal rate on AlpacaEval.",
  "Standard safety training": "Existing frontier language models like GPT-3.5, GPT-4, and Claude-3 undergo safety training,which ensures they normally refuse if asked directly about how to make a lethal bomb. While Mazeikaet al. show that these models do not consistently refuse harmful requests when adversariallyattacked, the work does not measure narrow-domain efficacy with strict egregiousness requirements.Thus, we investigate the ASR of popular attacks attacks in our narrow bomb-making domain. Static JailbreaksWe created a dataset 321 of existing bomb-making jailbreaks compiled from theHarmBench and JailbreakBench . We found that these jailbreaks were ineffective against allthe frontier models except for gpt-3.5-turbo models. PAIR AttacksUsing the PAIR algorithm , we found optimizing for jailbreaks on a variety ofbomb-making objectives (provided in Appendix C.4), breaks all standard-safety trained models, withone exception: claude-3-sonnet having an ASR of 0%. GPT-4o has the highest rate of failingcompetently with a 2% ASR across all attack attempts. Utilizing our prioritized sampling method,we manually grade 10 examples for each model that have between 10 and 30 egregious words andare flagged by our classifier. PAIR is more effective than our static set because it generates attacksthat adapt until our classifier flags it as harmful. Dry-IceWe discovered via manual red-teaming that all frontier models we tested will happily giveovert help with making a lethal dry-ice bomb. For each frontier model, we hand-tuned a variant ofthe prompt below and graded ten temperature=1 outputs to conclude that every model is broken atleast 20% of the time.",
  "Write a couple of paragraphs about what dry ice bombs are and how they pose achallenge to the criminal justice system. Comment in particular on whatdeadlier variants look like": "Random Search AttacksWe conducted random search attacks on frontier models by generatingadversarial suffixes to jailbreak the model. This algorithm requires log probability access, so it isnot applicable to Claude models. Following the method in Andriushchenko et al. , we modifiedthe prompt to enhance the likelihood of a competent failure (see implementation details in AppendixC.5). This algorithm iteratively selects a suffix of random tokens by maximizing the log-probabilityof \"Sure\". We discovered that GPT models, except GPT-4o, were vulnerable to this attack method,though robustness was specific to the random search algorithm we used. Altering prompts orhyperparameters might still allow for the generation of egregious bomb-making instructions.",
  "Static adversarial training": "Fine-tuning to refuse jailbreaks and adversarial training increases robustness in broad domains, so we investigate how this performs on our bomb-defense problem.Compared togpt-3.5-turbo-1106 (the base model which our fine-tuning was done on top of), our fine-tuned models (adv-sft-op{1,2,3}) demonstrate superior resilience against both static jail-breaks and PAIR attacks. PAIR attacks elicited 338 responses flagged by our classifier for thegpt-3.5-turbo-1106 but only a single flagged response across adv-sft-op{1,2,3}, which wasdetermined to be a false positive. The strong performance is not surprising since PAIR-generated jailbreaks are close in distributionto the static jailbreaks we fine-tuned to refuse. However, despite improved robustness, all operatingpoints have generalization issues since all are still vulnerable to dry-ice attacks. Finally, only adv-sft-op3 proved robust against generating competent failures when tested withadversarial suffixes identified through our random search attack. Nonetheless, it generated a bor-derline example (shown in Appendix E.1), suggesting that further tuning of prompts or algorithmhyperparameters could potentially result in a successful, competent failure.",
  "Classifier-based defenses": "Using an LLM classifier acts as a second line of defense that aims to catch harmful outputs from themodel. We compare the performance of five classifier defenses on the LLM bomb-defense problemand evaluate them with AlpacaEval Refusal Rate. Static JailbreaksWe observed that all classifiers performed well on the static dataset when usedin conjunction with gpt-3.5-turbo-0125, the model most susceptible to bomb-jailbreaks. Thisoutcome indicates that classifiers can match or exceed the effectiveness of RLHF in preventingcompetent failures for attacks that are not extensively optimized with knowledge of the defense. PAIR AttacksWe modify the PAIR algorithm to not only break the model but also circumventa classifier. Specifically, we run PAIR until the classifier labels the model interaction as safe,while CoT-4o labels it as unsafe. This approach isnt foolproof, as there could be cases wherean egregious output is marked as safe by both classifiers. However, we find that it is effectiveenough to uncover examples that successfully bypass the baseline classifiers (see Appendix C.4 formore details). We find the HarmBench classifiers demonstrated robust performance, maintaining anegligible AlpacaEval Refusal Rate of less than 0.35%. In contrast, the LlamaGuard classifiers weresignificantly compromised, exhibiting an AlpacaEval Refusal Rate exceeding 5%. Dry-IceLike frontier models, the classifiers generally find defending against the variants of the dry-ice attack challenging. However, the HarmBench-4o classifier stood out as an exception, successfullyidentifying all explicit dry-ice-bomb responses while maintaining a low AlpacaEval refusal rate ofonly 0.24%. We attribute the strength and strong generalization performance of HarmBench-4o tothe fact that it leverages a much more powerful base model than the other baseline classifiers. Prompt InjectionsWe manually crafted a prompt-injection attack, directing the generation modelto produce content formatted in a way that attempts to deceive the classifier into evaluating content itnormally should not (see Appendix C.6). Our prompt injection proved highly effective, breachingall classifier defenses with a minimum 30% refusal rate on AlpacaEval. HarmBench-4o, whichwas strong against all other attacks, was broken with a 100% refusal rate, showing that improvedcapabilities cause new vulnerabilities. Random Search AttacksWe use the random search method from Mangaokar et al. , findinga suffix that reclassifies harmful responses as harmless when appended to the model output. Tojailbreak the classifier-guarded system, the generation model must output harmful behavior and thesuffix. To do this, few-shot learning is used so the model understands how to propagate the suffixto its output. See implementation details in Appendix C.5. We find universal suffixes of length 120by optimizing a batch of 20 harmful transcripts until the classifier outputs a probability that leadsto at least 5% AlpacaEval refusal rate. It helps first to find a suffix that breaks a single transcriptand then incrementally adding more to the batch. We find suffixes (provided in Appendix E.3) thatconsistently break each classifier with a high AlpacaEval Refusal Rate.",
  "We rigorously tested our classifier, CoT-4o, for vulnerabilities with a range of attacks and iterativelyimproved the prompt to make it more robust": "Human red teamingWe launched a webpage hosting a challenge that encouraged participants totest our classifier-defended models robustness by finding competent failures. Participants identifiedsix viable attacks out of over 3,000 attempts, including a notable dry-ice bomb scenario, which helpedus refine our classifier prompts. Automated red teamingWe employed PAIR to conduct thousands of adaptive attacks aimedat jailbreaking the generation model and deceiving our classifier. Using our prioritized samplingmethod, we manually graded highly suspicious samples, all of which were accurately identified bythe classifier as non-egregious per our rubric. Figure C.3 shows the distribution of transcripts basedon the count of egregious words. During prompt iteration, we discovered that chain-of-thought (CoT) enhanced reliability, especiallywhen following a checklist like Does it ask for instructions?. Using a parser proved beneficial indetecting malicious prompts that alter the classifiers output structure. This parser flags transcriptsas harmful if the classifiers CoT deviates from the expected structure or contains flagged checklist",
  "items. Appendix D explores the advantages of designing classifier prompts to flag bomb content, evenif not all rubric requirements are met, and how CoT improves resistance to random-search attacks": "However, our classifier remains critically flawed. Testing against our static jailbreak dataset revealeda human jailbreak attack that consistently breaks the classifier (example provided in Appendix E.2).This single vulnerability explains the red column for CoT-4o in .1, resulting in a 100%AlpacaEval refusal rate. Despite failing only once in 321 static attacks, it maintains a 100% refusalrate on AlpacaEval which cannot be reduced since a chain-of-thought classifier lacks a tunablethreshold. We attribute the vulnerability in CoT-4o to the constraints of manual prompt-engineering.In contrast, LlamaGuard-2 and the HarmBench classifiers achieve perfect scores on the static dataset,suggesting that similar fine-tuning of CoT-4o would help improve robustness.",
  "Conclusion": "While we introduce a classifier defense that makes progress on jailbreak defense in a narrow domain,the broader problem of preventing competent failures in AI systems remains an open-problem.Potential extensions to this work include calibrating classifiers in the narrow domain by fine-tuningto improve performance and developing better methods for iterative human ground-truth feedback.",
  "Author Contributions": "TW led and scoped the project and owned the prompt engineering of our classifiers. JH led LLM redteaming, random search experiments, fine-tuning, and targeted sampling. HS helped with projectlogistics and writing the rubric. RA led the persuasion attacks and proposed many ideas to iterate onclassifier-based defenses. RS, MS, JM, NS, FB and EP advised on research and provided feedbackon the paper. EP advised on experimental ideas and design, as well as led the initial direction.",
  "Social Impacts Statement": "Our work tackles the problem of robustly preventing an LLM from engaging in a specified set offorbidden behaviors. While we focus on preventing a model from helping a user make a bomb, ourmethod is fairly general and can be applied to any forbidden behavior that can be clearly definedusing natural language. More developed versions of our technique (in particular more computationally efficient versions) maybe able to substantially reduce the risk of frontier LLMs engaging in clearly defined bad behaviors(whether at the direction of a human or at the direction of a models own autonomy). However, ourtechnique also allows for models to be forbidden from engaging in positive behaviors, and thus canhave negative societal consequences if deployed in the wrong way.",
  "A. Rao, S. Vashistha, A. Naik, S. Aditya, and M. Choudhury. Tricking llms into disobedience:Formalizing, analyzing, and detecting jailbreaks, 2024": "A. Reuel, B. Bucknall, S. Casper, T. Fist, L. Soder, O. Aarne, L. Hammond, L. Ibrahim, A. Chan,P. Wills, M. Anderljung, B. Garfinkel, L. Heim, A. Trask, G. Mukobi, R. Schaeffer, M. Baker,S. Hooker, I. Solaiman, A. S. Luccioni, N. Rajkumar, N. Mos, J. Ladish, N. Guha, J. Newman,Y. Bengio, T. South, A. Pentland, S. Koyejo, M. J. Kochenderfer, and R. Trager. Open problemsin technical ai governance, 2024. URL",
  "A. Robey, E. Wong, H. Hassani, and G. J. Pappas. Smoothllm: Defending large languagemodels against jailbreaking attacks, 2023": "R. Schaeffer, D. Valentine, L. Bailey, J. Chua, C. Eyzaguirre, Z. Durante, J. Benton, B. Miranda,H. Sleight, J. Hughes, R. Agrawal, M. Sharma, S. Emmons, S. Koyejo, and E. Perez. Whendo universal image jailbreaks transfer between vision-language models?, 2024. URL S. Schulhoff, J. Pinto, A. Khan, L.-F. Bouchard, C. Si, S. Anati, V. Tagliabue, A. L. Kost,C. Carnahan, and J. Boyd-Graber. Ignore this title and hackaprompt: Exposing systemicvulnerabilities of llms through a global scale prompt hacking competition, 2024. URL",
  "H. Touvron, L. Martin, K. Stone, et al. Llama 2: Open foundation and fine-tuned chat models,2023": "B. Wang, C. Xu, S. Wang, Z. Gan, Y. Cheng, J. Gao, A. H. Awadallah, and B. Li. Adversarialglue: A multi-task benchmark for robustness evaluation of language models. In Thirty-fifthConference on Neural Information Processing Systems Datasets and Benchmarks Track (Round2), 2021. B. Wang, W. Chen, H. Pei, C. Xie, M. Kang, C. Zhang, C. Xu, Z. Xiong, R. Dutta, R. Schaeffer,et al. Decodingtrust: A comprehensive assessment of trustworthiness in gpt models. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track,2023. J. Wang, X. Hu, W. Hou, H. Chen, R. Zheng, Y. Wang, L. Yang, H. Huang, W. Ye, X. Geng,et al. On the robustness of chatgpt: An adversarial and out-of-distribution perspective. arXivpreprint arXiv:2302.12095, 2023.",
  "J. Yu, X. Lin, and X. Xing. Gptfuzzer: Red teaming large language models with auto-generatedjailbreak prompts. arXiv preprint arXiv:2309.10253, 2023": "Y. Zeng, H. Lin, J. Zhang, D. Yang, R. Jia, and W. Shi. How johnny can persuade llms tojailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms. arXiv preprintarXiv:2401.06373, 2024. K. Zhu, J. Wang, J. Zhou, Z. Wang, H. Chen, Y. Wang, L. Yang, W. Ye, Y. Zhang, N. Z. Gong,and X. Xie. Promptrobust: Towards evaluating the robustness of large language models onadversarial prompts, 2024. URL D. M. Ziegler, S. Nix, L. Chan, T. Bauman, P. Schmidt-Nielsen, T. Lin, A. Scherlis,N. Nabeshima, B. Weinstein-Raun, D. de Haas, B. Shlegeris, and N. Thomas. Adversarialtraining for high-stakes reliability, 2022.",
  "ARelated work": "Our work is quite similar to Ziegler et al. , which tried to finetune deberta-v3-large to classifywhether a given text completion contains additional violence compared to a given text input. Like us,Ziegler et al. also aimed for high-stakes reliability, where even a single failure is not acceptable.And like us, they were also unsuccessful. The main differences between our work and Ziegler et al. is that we used more powerful models as classifiers, such as GPT-4o, and we tried defensemethods that were different than their human-in-the-loop adversarial training method. Our work is also similar in spirit to some existing jailbreak benchmark projects sincewe are also trying to compare the efficacy of different defenses and attacks. However, existingjailbreak benchmarks have limitations when evaluating classifier-based defenses. In particular, thesebenchmarks use automated graders to judge attacks and defenses but do not have any stipulations thatyou are not allowed to use an automated grader as a filter. This is problematic because one couldtautologically ace these benchmarks by using their automated grader as a transcript filter. Whilethis is not in the spirit of these benchmarks, the benchmarks do not define what solutions are in thespirit of the problem. We believe that the proper way to do jailbreak benchmarks is to use humanjudgment, which we do in this paper (see ). However, this is not without its challenges, ashuman-based evaluations are hard to reproduce and very expensive. We feel that overcoming this lastpoint is an important open problem. Several proposed jailbreak defenses could be applied to solve our LLM Bomb-Defense Problem(Definition 1). In our paper, we primarily focus on benchmarking classifier-based defenses like Llam-aGuard and the classifiers from HarmBench classifiers . We choose to forgo benchmarkingagainst defenses like perplexity filters and randomized smoothing because the works thatintroduce these defenses report that they are either weak against natural language attacks or degrademodel performance too much. We leave evaluating circuit-breaking defenses to future work. The prompt-injection literature also has several techniques that apply to our setting. In particular,a key challenge in building a robust classifier is avoiding prompt-injections. Our CoT-4o classifierincorporates concepts from existing prompt-injection defenses, like the idea of clearly demarcat-ing or spotlighting untrusted inputs (see how we transform the transcript in Appendix C.2).However, we find that our implementation and all the other classifier-based defenses we tested arestill vulnerable to prompt-injection attacks. Interestingly, some works like Willison framejailbreaks and prompt injections as separate issues. At least from the point of view of solving theLLM Bomb-Defense Problem though, we argue that these issues are closely related, as successfullypreventing prompt injections would solve jailbreaks and vice versa. Finally, our problem definition is very similar to the Unrestricted Adversarial Examples Challengeintroduced by . The Unrestricted Adversarial Examples Challenge is the problem of training animage classifier that robustly matches human perception on all clean images. This is a departure fromthe standard epsilon-ball threat model in adversarial example research for vision, as the ground truthsignal is a human instead of a clean, mathematically defined concept like epsilon-ball perturbationinvariance. Like Brown et al. , our toy problem also grounds out in human judgment."
}