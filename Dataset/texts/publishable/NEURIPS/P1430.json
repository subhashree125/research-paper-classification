{
  "Abstract": "In many domains, the most successful AI models tend to be the largest, indeedoften too large to be handled by AI players with limited computational resources.To mitigate this, a number of compression methods have been developed, includingmethods that prune the network down to high sparsity whilst retaining performance.The best-performing pruning techniques are often those that use second-order cur-vature information (such as an estimate of the Fisher information matrix) to scorethe importance of each weight and to predict the optimal compensation for weightdeletion. However, these methods are difficult to scale to high-dimensional param-eter spaces without making heavy approximations. Here, we propose the FishLegsurgeon (FLS), a new second-order pruning method based on the Fisher-Legendre(FishLeg) optimizer. At the heart of FishLeg is a meta-learning approach to amor-tising the action of the inverse FIM, which brings a number of advantages. Firstly,the parameterisation enables the use of flexible tensor factorisation techniques toimprove computational and memory efficiency without sacrificing much accuracy,alleviating challenges associated with scalability of most second-order pruningmethods. Secondly, directly estimating the inverse FIM leads to less sensitivityto the amplification of stochasticity during inversion, thereby resulting in moreprecise estimates. Thirdly, our approach also allows for progressive assimilationof the curvature into the parameterization. In the gradual pruning regime, thisresults in a more efficient estimate refinement as opposed to re-estimation. Wefind that FishLeg achieves higher or comparable performance against two commonbaselines in the area, most notably in the high sparsity regime when considering aResNet18 model on CIFAR-10 (84% accuracy at 95% sparsity vs 60% for OBS)and TinyIM (53% accuracy at 80% sparsity vs 48% for OBS).",
  "arXiv:2412.02328v1 [cs.LG] 3 Dec 2024": "and must therefore compromise on performance. Large models are also expensive to serve and hard todeploy on low-power devices. Consequently, there is a growing need for methods that can compressthese models down to a fraction of their original size whilst retaining their performance (Liu andWang, 2023), or indeed train models from scratch to be sparse (Liu et al., 2023b). Some of the most promising directions for neural compression revolve around leveraging second orderinformation in order to selectively prune least important parameters, while simultaneously updatingthose that remain. Several recent studies have shown that second-order parameter importance scoresare more accurate than more rudimentary measures derived from weight magnitudes and/or gradients(Gale et al., 2019; Sanh et al., 2020), yielding more effective pruning in convolutional (Singh andAlistarh, 2020; Theis et al., 2018) or transformer (Kurtic et al., 2022; Kuznedelev et al., 2022a)architectures. Moreover, second-order methods have shown some promise in pruning benchmarksspecifically chosen to fail current sparse neural networks (Liu et al., 2023a). However to obtainstate-of-the-art performance, compressed models often require a period of retraining after or duringthe process of model compression which necessitates hand-crafted compression recipes to be designed usually switching between compression and training phases (Kuznedelev et al., 2022b). Despite the promise of OBS-derived approaches, they are faced with a severe tradeoff betweenscalability and accuracy that has proven hard to navigate. Specifically, both the importance scoresand the weight updates rely on estimating the action of the inverse Hessian H1 (or, in our case, theinverse Fisher matrix F 1) on a high-dimensional parameter space (v H1v), which inevitablycalls for approximations. Indeed, all recent applications of the OBS framework to pruning have hadto make significant simplifications, such as (i) ignoring correlations between most weights or groupsof weights (Kurtic et al., 2022; Kuznedelev et al., 2022a), even those that belong to the same layer, or(ii) making low-rank approximations to the Hessian (Frantar et al., 2021; Singh and Alistarh, 2020)which are as good as the memory they consume. Moreover, in the gradual pruning regime where themodel changes little from stage to stage, what has been learned about the curvature at the currentstage is often unduly discarded in the next one. In parallel to the advancements of second-order pruning techniques, optimization has also been thesubject of many improvements that tackle similar computational challenges. In particular, the FishLegoptimizer introduced by (Garcia et al., 2023) attacks the scalability-accuracy dilemma by learning todirectly amortize F 1v products in an easy-to-evaluate Q()v form. This is done by minimizingan auxiliary loss A() derived from Legendre duality principles, w.r.t. a set of auxiliary parameters (details in Appendix A). In contrast to low-rank approximations of the Fisher matrix that requirehundreds of gradients to be stored, FishLeg allows the progressive distillation of a large number ofgradients into the auxiliary parameter set . This direct and gradual learning of F 1 in Q() isparticularly relevant to the gradual pruning setting, where other methods typically have to recomputeF from scratch following pruning, and re-invert it. By means of low-parameter tensor factorizationtechniques, the size of can be kept within a small multiple of the size of the model itself, enablingpruning of large models with limited memory. Whilst such memory efficiency can also be attainedthrough KFAC-based methods (Martens and Grosse, 2015; Wang et al., 2019), FishLegs directestimation of the inverse Fisher is less sensitive to gradient noise. Moreover, the form of KFACsF 1 follows rigidly from approximate mathematical derivations, whereas FishLegs Q() can beany user-specified positive-definite quadratic form, yielding greater flexibility and accuracy. Weuse this flexibility to develop a novel variation on the well-known Kronecker-factored curvatureapproximation for dense layers, as well as new approximations for the convolutional layer. In this work, we introduce the FishLeg Surgeon (FLS) a novel pruning algorithm that exploits theinverse curvature estimation machinery of the Fisher-Legendre (FishLeg) optimizer (Garcia et al.,2023). We build on the Optimal Brain Surgeon (OBS; Hassibi and Stork, 1992; LeCun et al., 1989),a classical approach to pruning that approximates the networks loss function in quadratic form todetermine (i) the importance (or saliency) of each weight and (ii) the optimal way of compensatingfor their deletion.",
  ":wt+1 wS, t+1 S, t t + 1": "Under the standard assumption that the gradient at the current point w is negligible for a pretrainedmodel, the OBS formulas for the optimal weight to be pruned wp and the corresponding update pcan be derived by writing the locally quadratic problem under the constraint that element p of p isequal to wp, which means that wp is zero after applying the update to w. This problem has thefollowing Lagrangian:",
  "where H denotes the Hessian at w and ep is the p-th canonical basis vector": "The inverse FIM used to score and update the weights is typically recomputed at regular intervals incurrent second order pruning methods. The reason for this being that OBS derived methods are basedaround inverting an estimation of the empirical Fisher. However during pruning, as parametersare removed and others updated, the inverse FIM estimation becomes increasingly inaccurate,necessitating a complete re-estimation of the empirical Fisher before its explicit inversion. Here, we reason that FishLegs parametric estimation of the inverse FIM, Q(), can be activelyupdated in a rolling fashion between consecutive pruning steps by simply performing a certain numberof auxiliary loss minimization steps. Crucially, by amortizing the re-computation of the inverse FIMin this way, we can afford to update our model directly (for which we use the FishLeg optimizer, alsobased on the running estimate Q()), as outlined in Algorithm 1. Hence, unlike previous approachesto gradual second-order pruning, we need not re-estimate and re-invert the Fisher matrix from scratchafter each pruning step we simply refine our current estimate.",
  "Memory efficient parameterization of the inverse Fisher approximation": "For scalability, we approximate F 1 in block-diagonal form, with each layer contributing one block.Note that these blocks are orders of magnitude larger than the ones used in previous second-orderapproaches that implemented direct inversion (e.g. Kurtic et al., 2022 used blocks of size 50). Our choice of structure for Q() is slightly more constrained by our pruning objective than it is forthe FishLeg optimizer: we require efficient evaluation of not only Qv products (which essentiallysketch the curvature in the direction of steepest descent), but also diag(Q) (required in Equation 28).For dense layers with ni inputs and no outputs, we parameterize the corresponding inverse Fisherblock asQ() D(LL RR)D(2)where L Rnono and R Rnini are two parameter matrices, D is a diagonal matrix with(ni + 1)no parameters, and denotes the Kronecker product. This construction is such that, forV Rnoni,Q()vec(V ) = D vec(LL(V D)RR)(3)with the (unusual) convention that vec() vectorizes row-wise (corresponding to a no-copy reshapein numerical code), and denotes elementwise (Hadamard) product. Here, D Rno(ni+1) is theun-vectorized version of the diagonal of D. Similarly,",
  "diag(Q) = diag(D)2 (diag(LL) diag(RR))(4)": "can be evaluated efficiently, with diag(LL) = (L L)(1, . . . , 1). Note that the inclusion of Dmakes it more expressive than the standard KFAC approximation which is limited to the Kroneckerproduct. For completeness in Appendix F, we compare the above parameterisation with a purediagonal parameterisation and also a more restrictive block diagonal structure similar to other second-order pruning methods (i.e. oBERT & M-FAC). For convolutional layers, we follow a similar tensor factorization strategy. Filter parameters aretensors of dimensions no(output channels) ni(input channels) K(kernel size). Whilst we couldparameterize the inverse Fisher block as a 3-way Kronecker product, Grosse and Martens (2016)sKFAC derivation for convolutional layers suggests combining together the input and kernel-sizedimensions. We therefore use the same structure as in Equation 2, but with R of size niK and D ofsize noniK.",
  "Initialization of Q": "Our experiments with FishLeg have revealed that the minimization of the auxiliary loss is verysensitive to initialization to the point that getting it wrong can yield useless estimates of F 1 . Inthe context of neural network optimization, Garcia et al. (2023) advocated an identity initializationQ0 = I. To choose the value of , they observed that this identity initialization implied thatthe FishLeg update wt+1 wt Q()wL would initially correspond to SGD. Thus, given alearning rate SGD known to work well for SGD, they set SGD/. However, in the context ofpruning this rationale no longer applies; we therefore revisited the choice of . We found that good pruning results could only be obtained for sufficiently large . To understand this,we studied the idealized dynamics of auxiliary loss gradient descent (; see also Appendix C).Let F = UU be the eigendecomposition of the Fisher matrix, with = diag(1, . . . , n).Assuming u N(0, In), the auxiliary loss (Equation 9) reduces to A() = 1 2Tr(QFQ) Tr(Q)).Expressing Q in the eigenbasis of F as Q = UU , the gradient flow for this deterministic lossfunction takes the form = ( + I) + I with (0) = I. It is then easy to see that willremain diagonal throughout, and that the ith eigenvalue of Q has the following dynamics:",
  "withi(0) = .(5)": "Thus, the eigenvalues of Q all initially equal to converge at very different speeds dependingon their optimal steady states: eigenvalues that must reach large (resp. small) values evolve slowly(resp. fast). We therefore conclude that a good initialization is to set to be as large as the largesteigenvalues of F 1 , namely (min{i} + )1 1. This way, the eigenvalues of Q that would",
  "slow": ": The initialization of Q() matters much. In this toy experiment, the true Fisher matrix(n = 100) was chosen so that its ith eigenvalue is i 1/i2, and the damping parameter wasset to 103. Thus, the eigenvalues of F 1lie roughly in the range. The auxiliary lossA(Q) = 1 2Tr(QFQ) Tr(Q) (left) was minimized by gradient descent w.r.t. the Cholesky factor ofQ(), initialized such that Q() = I (black) or Q() = 1I = 1000 I (red). The learning ratewas optimized separately for each case. This simulation shows that it is clearly better to initialize Q tobe large rather than small. Indeed, a simple derivation shows that each eigenvalue i of Q approachesits target 1/(i + ) at a speed proportional to (i + ) (Equation 5). In other words, the eigenvaluesof Q that must end up large are also those that evolve the slowest. It, therefore makes sense toinitialize them to be large so they have less to travel; the eigenvalues that must end up small willbecome small rapidly anyway. The right panels illustrate this behaviour by plotting the eigenvaluesof Q against their respective targets, at regular intervals during optimization (color-coded), for bothinitialization schemes. The auxiliary loss is minimized when i = 1/(i + ), i.e. when the dots liealong the identity line (dashed grey). normally slowly evolve towards 1 are positioned there from the outset, and the eigenvalues thatare set to decrease do so rapidly. illustrates this behaviour and shows that large initializationof Q (with 1) results in faster minimization of the auxiliary loss.",
  "Preconditioning of the auxiliary loss": "Learning the full F 1 is a hard problem when F is ill-conditioned, as the auxiliary loss inheritsthis ill-conditioning. Our theoretical analysis of this problem (Appendix C) has led to the discoveryof a good preconditioner which only costs a single additional Q()v product per iteration. Thispreconditioner greatly accelerates asymptotic convergence of the auxiliary loss (A), leadingto better estimates of the inverse FIM.",
  "Empirical Investigations": "In order to validate our approach for pruning and training in the same breath using a second-orderoptimizer such as FishLeg, we provide initial studies with a ResNet18 model with a single-layerclassification layer on CIFAR-10 and TinyIM. We consider two pruning approaches that are commonin the field: unstructured and semi-structured pruning. Across all experiments, we consider the samedense models trained with Adam to 83.4% test accuracy on CIFAR-10 and 55.4% test accuracy onTinyIM. Results are reported as mean over 3 random seeds. All experiments were run on a singleNVIDIA GeForce RTX 2080 GPU with 8GB of VRAM. In addition to the results presented in this section, in Appendix F we provide extensive ablationstudies for the methods introduced in including a direct comparison between the blockdiagonal approximation used in OBS methods across multiple block sizes.",
  "Unstructured Pruning": "Unstructured pruning consists of choosing parameters across the entire network to prune and achievesome non-uniformly sparse model which maintains performance on a task. In these experiments, thenon-zero parameters were fine-tuned for 1 epoch (with SGDm for magnitude and OBS) after eachpruning step following a exponential sparsity schedule. displays a marked improvement Sparsity (%) Test Accuracy (%) MagnitudeOBSFLS (Ours) Sparsity (%) Test Accuracy (%) MagnitudeOBSFLS (Ours) : Test accuracy as a function of model sparsity for ResNet18 on CIFAR-10 (left) and TinyIM(right). Different pruning frameworks are used, which are magnitude pruning (blue), OBS (orange)and FLS (green). over magnitude and OBS pruning methods across both CIFAR-10 and TinyIM experiments withonly a reduction of 2.5% from dense performance at 99% sparsity on CIFAR-10. In addition tothis, FLS benefits from being able to continually adapt its second-order information between pruningsteps. By amortizing the overhead in recomputing the empirical FIM we afford efficient second-orderupdates during the finetuning phases which, combined with some potential effect from Occamsrazor (Blumer et al., 1987), explains the slight increase in performance across the experiment.",
  ": 2:4 semi-structured pruning performanceof ResNet18 model finetuned on CIFAR-10 andTinyIM data": "Semi-structured (N:M) pruning involves prun-ing N parameters in each block of M parame-ters. This repeated pattern can be readily appliedto Sparse Tensor Cores which perform calcula-tions on a compressed version of a sparse ma-trix. Here we consider the setting of 2:4 sparsitywhich results in a 50% sparse network. Simi-lar to the unstructured setting, all models werepruned with an exponential schedule up to a 2:4pattern and retrained for 1 epoch after each prun-ing step. Our method achieves greater or similarperformance when compared to the baselines, and even improves upon the dense model performancein the case of CIFAR-10.",
  "Discussion, limitations and future work": "We have introduced a new perspective on second-order pruning that blurs the lines between second-order optimization and compression. We have identified challenges with the naive approach ofusing an optimization\" sketch of the FIM for compression and addressed these with expandedparameterisations, which we have justified theoretically and thorough ablation studies. In addition,we have provided empirical evidence which demonstrates our method on ResNet18 with CIFAR-10and TinyIM. Despite this, pruning with FishLeg has several limitations. One of the key assumptions in our approachis that the inverse Fisher F 1 (w) can be well approximated by a specific form of positive definitematrix Q(); however, the structure chosen for Q is largely dictated by scalability requirements, andmay not be appropriate under certain conditions. We have proposed memory-efficient factorizationsof Q which we have found effective for dense and convolutional layers, and we leave the developmentof other types of neural network layers to future research. Indeed, it remains to be seen whether theseresults scale to larger models, other compression techniques (i.e., fully structured, quantisation, amixed setting etc.) and to a wider variety of layers. We hope that further research in these areas willlikely extend and refine the capabilities of the proposed method.",
  "Gale, T., Elsen, E., and Hooker, S. (2019). The state of sparsity in deep neural networks. CoRR,abs/1902.09574": "Garcia, J. R., Freddi, F., Fotiadis, S., Li, M., Vakili, S., Bernacchia, A., and Hennequin, G. (2023).Fisher-Legendre (FishLeg) optimization of deep neural networks. In The Eleventh InternationalConference on Learning Representations. George, T., Laurent, C., Bouthillier, X., Ballas, N., and Vincent, P. (2018). Fast approximate naturalgradient descent in a Kronecker factored eigenbasis. Advances in Neural Information ProcessingSystems, 31.",
  "F 1 [F + I]1 = u(0).(8)": "FishLeg meta-learns a parametric approximation (u, ) of (u), by minimizing the auxiliaryloss A(, u) H((u, )) u(u, ) w.r.t. meta-parameters , as prescribed by Equation 7.Importantly, Equation 8 shows that one only needs to learn the local behaviour of the vector field(u) around small u; thus, Garcia et al. directly parameterized its (symmetric, positive definite)Jacobian Q() at u = 0, corresponding to the choice (u, ) Q()u. Furthermore, consideringthe limit of small u and averaging over a relevant distribution (more on this below and in Appendix B),the auxiliary loss becomes",
  "Taking the gradient of Equation 9 w.r.t. makes is clear that Q() will learn to approximate the actionof F 1on the subspace spanned by the us. Given their application to natural gradient optimization,": "Garcia et al. took those us to be stochastic gradients of the models primary loss function. For ourpruning purposes, however, Equation 27 suggests that we must accurately estimate the action of F onthe entire parameter space; we will therefore work with a more isotropic distribution of u (). Directly estimating the inverse Fisher matrix, and doing so in this way, brings a number of advantages.First, the FishLeg approach is flexible: one can specify any form of Q(), and in particular combinestructured approximations obtained through mathematical derivations (as in e.g. KFAC; George et al.,2018; Grosse and Martens, 2016; Martens and Grosse, 2015) with a variety of parametric adjustmentsfor greater expressiveness. We give examples of such choices in .1. Second, the FishLegapproach is less biased than KFAC and related methods. These methods start by assuming that Fhas a certain structure (e.g. block diagonal), obtain a good approximation of F conforming to thisstructure, and then invert it. One expects both systematic errors as well as stochasticity in the estimateof F to propagate to F 1. In contrast, FishLeg fits a parametric approximation to F 1 directly,conveniently avoiding inversion. Relatedly, a key property of Equation 9 is that it is not biased bystochasticity in the estimate of F (Appendix D; ) unlike other seemingly sensible auxiliaryloss functions such as EuQ() Fu u2 or Eu FQ()u u2 whose quadratic terms in Fdo survive averaging.",
  "dQdt = P(FQ(t) I)(19)": "with initial condition Q(0) = I. Let F = UU be the eigendecomposition of the Fishermatrix, with = diagm(1, . . . , n) and U U = UU = I. We will assume that P has the sameeigenvectors as F, i.e. P = Udiagm(p1, . . . , pn)U . Rewriting the above gradient flow in theeigenbasis of F, we obtain",
  "= diagm(p1, . . . , pn)(( + I)U QU I)(22)": "We see that if U QU is diagonal at time t, it will remain diagonal. Given that U QU = U (I)U =I is diagonal, we conclude that at any time t, U Q(t)U = diagm(1(t), . . . , n(t)). Thus,Equation 22 boils down to a set of n decoupled, scalar flows,",
  "Kronecker productABCD": ": Assessing FishLegs inverse curvature estimation in a controlled setting. In thisfigure, the true Fisher matrix F R100100 is constructed to have a random orthonormal eigenbasisand eigenvalues i ei/30. All results are averaged over 20 independent realizations of thecorresponding experiment with different random seeds. (A): standard affine-invariant Riemanniandistance between FishLegs Q and F 1( = 0.01), as a function of the number of data mini-batchesof size m consumed so far. Each Adam step of auxiliary loss minimization consumes one minibatch.In this case, we use a full parameterization Q = LL that contains the solution F 1 ; in that case,FishLegs inverse curvature estimation is consistent and the error goes to zero. As a baseline, weshow the behaviour of a simple but biased estimator that estimates F on each new minibatch, invertsthat noisy estimate, and averages the result over minibatches; inverting noisy estimates yields a biasthat persists asymptotically. (B-D): In these panels, the inverse Fisher is estimated in structured form(B: diagonal; C: block-diagonal, 5 blocks; D: Kronecker product, (5 5) (20 20). This is doneeither by FishLeg assuming a correspondingly structured form for Q (red), or by (i) approximatingF in structured form for each minibatch (for the Kronecker approximation, we use a permuted SVDto find the nearest Kronecker product in the least-squares sense; Van Loan and Pitsianis (1993)), (ii)averaging over minibatches (for the Kronecker approximation the two factors are averaged separately,as in KFAC), and (iii) inverting the result (black; note that in this case, the inverse inherits thestructure). We report the squared error between Qu and F 1 u, averaged over u N(0, u), andnormalized by the average norm of F 1 u. Here, to reflect the need of accurately estimating theaction of F 1on the least salient parameter dimensions, we have chosen u = F 1.",
  "with i ipi.(25)": "For P = I, i.e. pi = 1, we recover the result of the main text (c.f. ): i convergesexponentially to its target i , but on a timescale i proportional to i itself. This is a problem whenF is poorly conditioned, such that there is a broad range of i : in this case, some is will convergerapidly, and some others will converge very slowly. Equation 25 suggests a solution based on a judicious choice of the preconditioner P. If somehow wecould precondition the loss with P = F 1 , then pi = i and therefore i = 1 for all i this casewe have rapid uniform convergence of the inverse Fisher in all directions. While we do not knowF 1(indeed this is what we are trying to learn ...), we do know that Q(t) is supposed to converge(albeit slowly) towards F 1. Thus, we propose a simple time-dependent preconditioner P(t) = Q(t).Empirically, we do find that this choice leads to better asymptotic convergence of the auxiliary loss,as illustrated in A. Note that this only costs a single additional Qv product in every iteration.",
  "In this section, we report on a series of simple experiments that show that FishLegs inverse curvatureestimation is typically more accurate and flexible than more conventional approaches": "First, A shows that when the parameterization of FishLegs Q is sufficiently expressive toinclude F 1, Q converges to F 1as desired, despite only having access to stochastic estimates of F.This is because, using standard unbiased estimates of the Fisher matrix (or, practically, Fisher-vectorproducts) on mini-batches in Equation 9, FishLegs auxiliary loss and its gradient are also unbiased.With sufficiently small learning rate, we therefore expect Q to converge to the inverse damped Fishersolution. In contrast, a more naive scheme that computes an average of inverses of noisy Fisherestimates (est. inv. avg. in A) yields a bias that persists asymptotically. Second, when F 1lies outside the domain of the structured approximation (e.g. when it is notexactly a single Kronecker product, or a block-diagonal matrix), there is an advantage to directlyapproximating F 1in the desired structured form Q (FishLegs strategy), rather than approximatingF in such a form and then inverting the result. For one, (Garcia et al., 2023) had already arguedthat the former is more flexible than the latter, because one can use structured forms that need notbe easily inverted (indeed FishLeg does not invert anything). Here, we show that even when thestructured form is easily inverted, FishLeg still has a marked advantage (B-D). In particular,the auxiliary loss allows the specification of a distribution of vectors u (specifically, their covariance)to promote learning the action of F 1on select directions in parameter space. This is not possible ina more conventional approach whereby the Fisher matrix is first approximated in structured form,then averaged, and finally inverted.",
  "These equations have also been extended to handle the semi-structured pruning setting whereby smallblocks of weights are treated as single units (Kurtic et al., 2022)": "Existing second-order pruning methods mostly differ in the way they estimate F 1v products tocompute Equations 27 and 28. All scalable methods make a block-diagonal approximation for F.WoodFisher (Singh and Alistarh, 2020) and oBERT (Kurtic et al., 2022) partition the parameter spaceinto small blocks assumed to be independent, and use the Woodbury identity to recursively update anestimate of the inverse empirical Fisher F 1Bfor each block B. These approaches have substantialmemory requirements (O(|B|n), where |B| is the block size and n is the total number of parametersin the model). M-FAC (Frantar et al., 2021) modifies this recursion to operate directly on F 1B vproducts, in a way that obviates the need for storing F 1B(some parts of the computation can becached and reused for any v). This is typically much slower but requires less memory. In our work,FLS too approximates F 1 in block-diagonal form, but with much larger blocks corresponding toentire layers, and with blocks structured to guarantee computational and memory efficiency.",
  "D": ": Ablation experiments on synthetic data in a toy setup to show: (A) the utility ofpreconditioning the auxiliary loss, (B) the predicted quality of the approximated Fisher in differentscenarios, (C) the one-shot pruning performance of various Fisher approximations (including otherbaselines) and (D) the effect of implementing a block diagonal FishLeg approximation and itscomparison to oBERT (an OBS-derived approach) at various block sizes. and their impact on one-shot pruning. In A-C we choose n = 100 and in D weset n = 500. The layer weights are drawn from N(0, 1/n), and inputs are drawn from N(0, x),where x is a random covariance matrix with eigenvalues {i ei/10}. Results are reported asmean s.e.m. over random seeds. Across all experiments, a batch size of 100 is chosen along witha damping parameter = 0.01. Note that in this toy example, the Fisher matrix is F = x, anddoes not depend on the weights. A shows the effect of preconditioning the FishLeg auxiliaryloss using the momentary approximation Q() of the inverse Fisher matrix. We observe that thispreconditioning does indeed lead to faster asymptotic convergence. This is shown here for the fullapproximation Q = LL, which in this case is as expressive as the Kronecker parameterizationof dense layers we have used in the experiments from the main text. B displays the quality of approximation of the inverse damped Fisher matrix, as measured byFishLegs auxiliary loss after convergence, for various parameterizations of Q(). We compare thefull parameterization Q = LL (orange), a positive diagonal parameterization (purple), and a set ofpositive-definite block-diagonal approximations with various block sizes (blues). These results showvery clearly that a full approximation can achieve a much lower auxiliary loss when compared to lesspowerful approximations in this case. Following from this, C is reporting the one-shot pruning performance (test MSE) for thevarious FishLeg parameterizations shown in B, as well as for magnitude pruning (black),MFAC (m = 10; green) and exact FLS with F = x appropriately masked and inverted before eachpruning step (red). One can observe that the full approximation achieves a far closer performanceto the exact result across all other baselines in this study. Note that in this case, the exact FLScharacterises the limit of performance for second-order pruning methods. In this setting, we thereforefind a strong correlation between the quality of the iFIM approximation (as measured by Garcia et al.(2023)s auxiliary loss after convergence) and one-shot pruning performance (comparing Band C). In particular, block-diagonal approximations (as used by OBS/oBERT) perform",
  "Where the Adam learning rate is separately tuned for each approximation": "worse than the Kronecker-factored approximation (in this case also exact) and, indeed not muchbetter than magnitude pruning or a simple diagonal approximation of the iFIM. Likewise, FLS with aKronecker-factored Q performs better than MFAC (with rank parameter m generously set to 10, i.e.10% of the parameter count, which would normally be intractable memory-wise). Finally, D provides a comparison between FLS with block-diagonal parameterization andoBERT for various block sizes (5, 10, 20, 50). In particular, this ablation study shows benefits ofdirectly estimating the inverse FIM than estimating the FIM and inverting it. oBERT utilizies theWSM formula for effective estimation without explicit inversion, resulting in iterative update of theinverse of moving average for the empricial Fisher matrix. In the top panels, we present one-shotpruning performance (test MSE) as a function of sparsity for the two methods. In the middle panels,the standard affine-invariant Riemannian distance between the masked approximate block-diagonalinverse and the true masked Fisher inverse are shown, for each method. In the bottom panels, thewall-clock time as a function of sparsity is shown. For these experiments, oBERT uses 512 gradientsat each pruning step, whereas FLS performs 20 steps of auxiliary loss minimization between pruningupdates. These results show a systematic improvement in the inverse FIM estimates when using FLS,which implies that directly approximating the inverse Fisher in block-diagonal form (FLS) is betterthan approximating the Fisher in block-diagonal form before inverting each block (oBERT).",
  "GAdditional Experimental Details": "Across all experiments we used a batch size of 128 and additionally applied standard flipping andcropping augmentations. show the hyperparameter values used for each of the experimentalsetups, for the FishLeg optimizer. For other methods, we used an implementation of SGDm (withlearning rate set at 1e 3 and a momentum value of 0.9 and all others set at the PyTorch SGDmdefault) which preserved the sparsity map. More details of the experiments can be found in ."
}