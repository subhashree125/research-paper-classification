{
  "Abstract": "Bayesian Optimisation (BO) is a state-of-the-art global optimisation technique for black-box prob-lems where derivative information is unavailable and sample efficiency is crucial. However, im-proving the general scalability of BO has proved challenging. Here, we explore Latent SpaceBayesian Optimisation (LSBO), that applies dimensionality reduction to perform BO in a reduced-dimensional subspace. While early LSBO methods used (linear) random projections (Wang et al.,2013 ), we employ Variational Autoencoders (VAEs) to manage more complex data structuresand general DR tasks. Building on Grosnit et al. (2021) , we analyse the VAE-based LSBOframework, focusing on VAE retraining and deep metric loss. We suggest a few key corrections intheir implementation, originally designed for tasks such as molecule generation, and reformulatethe algorithm for broader optimisation purposes. Our numerical results show that structured latentmanifolds improve BO performance. Additionally, we examine the use of the Matern- 5 2 kernel forGaussian Processes in this LSBO context. We also integrate Sequential Domain Reduction (SDR),a standard global optimization efficiency strategy, into BO. SDR is included in a GPU-based en-vironment using BoTorch, both in the original and VAE-generated latent spaces, marking the firstapplication of SDR within LSBO.",
  "f = minxX f(x),(P)": "where X RD represents a feasible region, and f is a black-box, continuous function in (high)dimensions D. Bayesian Optimisation (BO) is a state-of-the-art GO framework that constructs aprobabilistic model, typically a Gaussian Process (GP), of f, and uses an acquisition function toguide sampling and efficiently search for the global optimum . BO balances exploration andexploitation but suffers from scalability issues in high-dimensions . To mitigate this, Dimen-sionality Reduction (DR) techniques can be used, allowing BO to operate in a lower-dimensionalsubspace where it is more effective. Contributions.We investigate scaling up BO using DR techniques, focusing on VAEs and theRandom Embedding Global Optimisation (REGO) framework . Our work extends the algo-rithm , incorporating the Matern-5",
  "DIMENSIONALITY REDUCTION TECHNIQUES FOR GLOBAL BAYESIAN OPTIMISATION": "construct the VAE training dataset. For instance, samples can be generated from a multivariatenormal distribution with a large covariance matrix. This approach facilitates the VAE inlearning a meaningful low-dimensional data representation. 2. The second one involves constructing the latent datasets for a sample-efficient BO procedure,as it would be computationally inefficient to use the entire VAE training dataset. Therefore,instead of using the entire DL, we utilise only 1% of it by uniformly and randomly selectingN points, where N represents 1% of the size of DU at the current retraining stage l.",
  ". Preliminaries": "Bayesian Optimisation.BO relies on two fundamental components: a GP prior and an acquisi-tion function. Given a dataset of size n, Dn = {xi, f(xi)}ni=1, the function values f1:n are modelledas realisations of a Gaussian Random Vector (GRV) F1:n under the GP prior. The distributionis characterised by a mean EF1:n and covariance KF1:nF1:n, where F1:n N(EF1:n, KF1:nF1:n),using the Matern-5/2 kernel. For an arbitrary unsampled point x, the predicted function valuef(x) is inferred from the posterior distribution: F(x) N((x|Dn), 2(x|Dn)), with (x|Dn) =EF + KTF1:nF K1F1:nF1:n(f1:n EF1:n), 2(x|Dn) = KFF KTF1:nF K1F1:nF1:nKF1:nF . BO usesthe posterior mean () and variance 2() in an acquisition function to guide sampling. In thiswork, we focus on Expected Improvement (EI): u(x|Dn) = E[max{F(x) fn, 0}|Dn], wherefn = maxmn f(xm) is the highest observed value. To enhance BO, we incorporate SDR to refinethe search region based on the algorithms best-found values, updating the region every few iter-ations to avoid missing the global optimum. To accelerate BO process, we propose to implementSDR within the traditional BO framework such that the search region can be refined to locatethe global minimiser more efficiently according to the minimum function values found so far bythe algorithm. Compared to the traditional SDR implementation that updates the search region ateach iteration, we propose updating the region after a set number of iterations to avoid prematureexclusion of the global optimum. Algorithm 2 in Appendix B.1 outlines the BO-SDR approach. Variational Autoencoders.DR methods reduce the number of features in a dataset while preserv-ing essential information . They can often be framed as an Encoder-Decoder process, wherethe encoder maps high-dimensional (HD) data to a lower-dimensional latent space, and the decoderreconstructs the original data. We focus on VAEs , a DR technique using Bayesian Varia-tional Inference (VI) . VAEs utilise neural networks as encoders and decoders to generatelatent manifolds. The probabilistic framework of a VAE consists of the encoder q(|x) : X Zparameterised by which turns an input data x RD from some distribution into a distribution onthe latent variable z Rd (d D), and the decoder p(|z) : Z X parameterised by whichreconstructs x as x given samples from the latent distribution. The VAEs objective is to max-imise the Evidence Lower BOund (ELBO):L(, ; x) = ln p(x) DKL[q(z|x)p(z|x)] =Eq(z|x)[ln p(x|z)] DKL[q(z|x)p(z)], where ln p(x) is the marginal log-likelihood, andDKL() is the non-negative Kullback-Leibler Divergence between the true and the approximateposteriors. The prior is usually set to N(0, I), and the posterior is parametrised as Gaussians with di-agonal covariance matrices, making ELBO optimisation tractable via the reparameterisation trick",
  ". Algorithms": "As mentioned above, DR techniques help reduce the optimisation problems dimensionality. Usinga VAE within BO allows standard BO approach to be applied to larger scale problems, as then,we solve a GP regression sub-problem in the generated (smaller dimensional) latent space Z. TheBO-VAE approach1, instead of solving (P) directly, attempts to solve",
  "f = minzZ Ep(x|z) [f(x)] ,(1)": "where is the optimal decoder network parameter. Therefore, it is implicitly assumed that theoptimal point x can be obtained from the optimal decoder with some probability given some latentdata z by the associated optimal encoder q(z|x) , z Z, P [x p(|z)] > 0.When fitting the GP surrogate, we follow and use Deep Metric Loss (DML) to generatewell-structured VAE-generated latent spaces. Specifically, we apply the soft triplet loss and retrainthe VAEs following to adapt to new points from the GP and optimise the black-box objectiveefficiently. Additionally, we implement SDR in the latent space to accelerate the BO process. Algo-rithm 1 outlines our BO-VAE approach with SDR, consisting of the pre-training of a standard VAEon the unlabelled dataset DU (line 1) and optional retraining with soft triplet loss to structure thelatent space by gradually adjusting the network parameters of the encoder and decoder. When thesoft triplet loss is used in retraining the VAE, the modified VAE ELBO LDML() is used in line 4instead; see Appendix B.3 for details. The BO-VAE algorithm with DML is included in AppendixB.3 as Algorithm 4. For comparison, we provide a baseline BO-VAE algorithm without retrainingor DML (Appendix B.1, Algorithm 3). Theorem 1 in offers a regret analysis with a sub-linearconvergence rate, providing a valuable theoretical foundation. However, the proof relies on the as-sumption of a Gaussian kernel, limiting its direct applicability when using the Matern kernel, aswe do here. Despite this limitation, the theorem provides key insights supporting the BO-VAE ap-proach. Our ongoing work addresses this gap, and a similar result specifically tailored to the Maternkernel is delegated to future work.",
  ". Numerical Study": "We conduct numerical experiments with the three BO-VAE algorithms (Algorithms 1, 3, 4) withinthe BoTorch framework . We explore cases where d = 2, 5 for D = 10, and d = 2, 10, 50for D = 100. The results reveal that, for a fixed ambient dimension D, larger latent dimensions,particularly d = 50, tend to degrade performance due to the reduced VAE generalisation capacity.In contrast, smaller latent dimensions (d = 2, 5) yield better results, as VAEs then can give moreefficient latent data representations, and the BO can solve such reduced problems more efficiently.In this paper, we present experimental results for the case D = 100 and d = 2, which strongly high-light the advantages of SDR in latent space optimisation and illustrate the three BO-VAE algorithms.The encoder structure of the VAE used is 2, and the decoder structure is . For 1. For brevity, we use BO-VAE to refer the approach of combing VAEs with BO.2. It indicates a three-layer feedforward neural network: the input layer has 100 neurons, followed by a hidden layerwith 30 neurons, and finally an output layer with 2 neurons. Similarly for the others.",
  "end": "where (0, 1) is an accuracy level. If the criterion is not met, Np(s; ) = . The performanceprofile s,() is the fraction of problems where rp,s , representing the cumulative distributionof performance ratios.Data profiles. The data profile shows solver performance across different budgets. For a solvers, accuracy level , and problem set P, it is defined as:",
  "(Retrain DML BO-VAE) in solving 100D Ackley and Rosenbrock problems. The means andthe standard deviations (shaded areas) of the minimum function values found are plotted across 5repeated runs": "minyRd g(y), subject to y Y = d. Here A is a D d Gaussian matrix for randomembedding, with d D. Solving g(y) in the reduced subspace is equivalent to solving f(Ay).In this context, the Gaussian matrix A serves as a (linear) encoder (for dimensionality reduction),while AT acts as a decoder. For the REMBO comparisons, we used d = de + 1, where de is theeffective dimensionality, and set = 2.2de based on . Each (randomised) algorithm was runtwice on each problem in Test Set 1. Each (randomised) problem in Test Set 2 was run twice, yield-ing 10 test problems. The algorithms we are comparing are denoted by BO-SDR (Algorithm 2),V-BOVAE (Algorithm 3), R-BOVAE (Algorithm 1), and S-BOVAE (Algorithm 4).Results with accuracy levels = 101 and = 103 are summarised in Tables 1, 2 and Figure 3. From these results, it can be seen that BO-VAE algorithms solve more problems compared toBO-SDR and REMBO algorithms. While BO-SDR may struggle with scalability, REMBOs lowproblem-solving percentages are likely due to the over-exploration of the boundary projections and the embedding subspaces failing to accurately capture the global minimisers. To address this, recommends restarting REMBO to improve the success rate. Meanwhile, Algorithm 4 (S-BOVAE) consistently performed best due to its structured latent spaces.",
  ". Conclusion and Future Work": "In this work, we have explored dimensionality reduction techniques to enhance the scalabilityof BO. The use of VAEs offers an alternative and more general approach for GP fitting in low-dimensional latent subspaces, alleviating the curse of dimensionality. Unlike REMBO, which pri-marily targets low-rank functions, VAE-based LSBO is effective for both high-dimensional full-rank and low-rank functions. Although BO-VAE reduces function values effectively, the optimalitygap remains constrained by noise from the VAE loss, as seen in Figures 1 and 2. To address this,implementations of data weights and different GP initialisation are the potential future directions.Additionally, SDR struggles in high dimensions; adopting methods like domain refinement basedon threshold probabilities may improve performance.",
  "A.2. High-dimensional Low-rank Test Set": "The low-rank test set, or Test Set 2, comprises D-dimensional low-rank functions generated fromthe low-rank test functions listed in . To construct these D-dimensional functions with loweffective dimensionality, we adopt the methodology proposed in .Let h(x) be any function from with dimension de and the given domain scaled to de.The first step is to append D de fake dimensions with zero coefficients to h(x):",
  "xl,0i= x(0)i r(0)i2 , xu,0i= x(0)i+ r(0)i2 , i {1, . . . D},(2)": "where r(0)iis the initial range value computed from the upper and lower bounds of the initial searchdomain. Now, suppose we are progressing from iterations k 1 to k and that the best observationsare xk1 and xk up to the (k 1)-th and k-th iterations respectively. To update and contract on theRoI, we first determine an oscillation indicator along dimension i at iteration k as",
  ",(5)": "where the indices i, k have been intentionally omitted for clarity and to avoid complex notations.Here, the parameter o, typically set between 0.5 and 0.7, is a shrinkage factor to dampen oscillation.This parameter controls the reduction of the RoI, facilitating more stable and efficient convergencetowards the global optimum. Meanwhiel, p indicates the pure panning behaviour and is typicallyset as a unity. To shrink the RoI, we utilise a zooming parameter to update the range along eachdimension, i.e,r(k)i= ir(k1)i, where i = + |d(k)i|( ).(6) i represents the contraction rate along dimension i and typically lies in [0.5, 1). Below, wepresent the Bayesian Optimisation algorithms innovatively with SDR in the ambient and the VAE-generated latent spaces.",
  "B.3. Soft and Hard Triplet Losses": "In this subsection, we briefly present how integrates the triplet loss with VAEs. We referthe reader to for background knowlegde for triplet deep metric loss. introduces aparameter to create sets of positive Dp(x(b); ) = {x D : |f(x(b)) f(x)| < } and negativepoints Dn(x(b); ) = {x D : |f(x(b)) f(x)| } for a base point x(b) in a dataset D, basedon differences in function values. However, the classical triplet loss is discontinuous, which hindersGP models. To resolve this, a smooth version, the soft triplet loss, is proposed. Suppose we have alatent triplet zijk = zi, zj, zk associated with the triplet xijk = xi, xj, xk in the ambient space.Here, zi is the latent base point. The complete expression of the soft triplet loss is",
  "f(1 ),": "for any zj q(|xj), xj Dp(xi; ) and zk q(|xk), xk Dn(xi; ). Here, f(x) =tanh (a/(2)) is a smoothing function with being a hyperparameter such that Lstrip(zijk) ap-proaches the hard triplet loss since lim0 f(a) = 1. The function I{} is a indicator function.The penalisation weights ij and ik are introduced to smooth out the discontinuities. Thus, themodified ELBO of a VAE trained with soft triplet loss is",
  "i,j,k=1Eq(zijk|xijk)Lstrip(zijk),": "where q(zijk|xijk) = q(zi|xi)q(zj|xj)q(zk|xk).The BO-VAE algorithm with the soft triplet loss as the chosen deep metric loss is outlined 4. Wenote that Algorithm 4 is not implemented with SDR in the latent space, as experiments have shownthat SDR and DML methods conflict with each other in excluding the global optimum. Addressingthis conflict when implementing SDR in DML-structured latent spaces is left as future work.",
  "Augment the outer-loop datasets: Dl+1L Dl;qL , Dl+1Z Dl;qZ15 end": "where 0. The use of is a trade-off between reconstruction accuracy and the regularity of thelatent space and to avoid the case of the vanishing KLD term, where no useful information is learned. A common approach to implementing the beta-annealing technique involves initialising at 0 and gradually increasing it in uniform increments over equal intervals until reaches 1.",
  ": The retraining details of the VAE used for Test Set 1": "Experiment Configurations for Test Set 2The pre-training and retraining details for this VAEare consistent with as before, as shown in and , respectively. In addition to the twokey implementation details for BO-VAE algorithms listed in Appendix C, it is important to notethat the test problem domains must be scaled to D for a fair comparison with REMBO. Thisadjustment is due to the domain scaling used in constructing Test Set 2. The specific experimentalconfigurations for each BO-VAE algorithm are consistent with as before."
}