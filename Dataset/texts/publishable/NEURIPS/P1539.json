{
  "Abstract": "Due to privacy and security concerns, recent advancements in group fairnessadvocate for model training regardless of demographic information. However,most methods still require prior knowledge of demographics. In this study, weexplore the potential for achieving fairness without compromising its utility whenno prior demographics are provided to the training set, namely harmless Rawlsianfairness. We ascertain that such a fairness requirement with no prior demographicinformation essential promotes training losses to exhibit a Dirac delta distribution.To this end, we propose a simple but effective method named VFair to minimize thevariance of training losses inside the optimal set of empirical losses. This problem isthen optimized by a tailored dynamic update approach that operates in both loss andgradient dimensions, directing the model towards relatively fairer solutions whilepreserving its intact utility. Our experimental findings indicate that regression tasks,which are relatively unexplored from literature, can achieve significant fairnessimprovement through VFair regardless of any prior, whereas classification tasksusually do not because of their quantized utility measurements. The implementationof our method is publicly available at",
  "Introduction": "Fairness in machine learning has gained significant attention owing to its multifaceted ethicalimplications and its far-reaching potential to shape and influence various aspects of society Dworket al. , Barocas and Selbst , Ntoutsi et al. . In high-stakes decision-makingdomains, algorithms that merely prioritize model utility may yield biased models, resulting inunintentional discriminatory outcomes concerning factors such as gender and race. Group fairness,a.k.a. statistical fairness Carey and Wu , addresses this issue by explicitly encouraging themodel behavior to be independent of group indicators, such as disparate impact Feldman et al. ,or equalized odds Hardt et al. . However, with increasing privacy concerns applied in practicalsituations, sensitive attributes are not accessible which raises a new challenge for fairness learning. According to literature, numerous efforts have been directed towards achieving fairness regardlessof demographic information, which can be mainly categorized into two branches. One branch isto employ proxy-sensitive attributes Yan et al. , Grari et al. , Zhao et al. , Zhuet al. . These works assume that estimated or selected attributes are correlated with the actualsensitive attributes and thus can serve as a proxy of potential biases. The other branch follows",
  ": Illustration of our idea through different forms of loss curves": "Rawlsian fairness Rawls , which focuses on reducing the disparity in group utility. Unlike theformer branch, the group utility here is predefined and centered, and thus it is typically not meaningfulto test learned models on other group fairness metrics. Worst-case fairness methods Martinez et al., Hashimoto et al. , belonging to Rawlsian fairness, commonly leverage a prior aboutthe worst-off groups size to identify under-represented members and prioritize the utility of theapproximate worst-off group. Such a taxonomy overlooks the differences in tasks, as the majorityof aforementioned fairness approaches are designed for classification tasks. In applications wherediscrete outcomes (e.g. binary decisions) provide insufficient information, there is a crucial needfor fair regressors. As relatively few discussions Zhao , Agarwal et al. exist for fairregression tasks, our work bridges the gap by incorporating regression tasks into a general predictiveloss under Rawlsian fairness. The trade-off nature between model utility and fairness has emerged as a subject of dynamic dis-course Dutta et al. , Wei and Niethammer , Zhao and Gordon . Worst-case fairnessmethods which inherently prioritize the worst-off groups utility often come at the expense of theoverall utility Hashimoto et al. , Zhai et al. . In this work, we focus on scenarios whereno prior demographic information is provided, aligning the ingredients with the standard trainingsetup, and then advocate for a primary problem (harmless Rawlsian fairness):",
  "This problem is particularly important in utility-intensive scenarios Li and Liu , and weinvestigate it in both classification and regression tasks to answer the question": "Our idea. We approach the problem from a novel perspective. The crux of the desired fairness lies inpursuing minimum group utility disparity across all groups. Since during the training phase, we arenot aware of what the actual sensitive attributes are used for test data, the safest way is to ensure everypossible disjoint group of training data has the same utility. To this end, we expect the training lossfor each individual example to be very close, meaning that the loss variable approaches a Dirac deltadistribution. As shown in (a). The Dirac delta distribution essentially represents an Oraclemodel, where all the loss values are concentrated at zero, resulting in both a mean and variance of 0.In the distribution view, the motivation of our method which is dubbed as VFair, is to approximatethis ideal by minimizing both the mean and variance of the training losses. (b) also shows thecomparison between VFair and other methods, using a regression task as an example. Oracle denotesmodels with unlimited capability that make predictions with zero error. Empirical Risk Minimization(ERM) refers to models without any fairness design. Worst-case represents fairness methods thatrequire the prior of the worst-off group (e.g., lower bound of partition ratios). Uniform model, initiallyintroduced by Martinez et al. , represents a model that performs equally poorly across allgroups on classification tasks. Here, we extend it to regression. In a simplified logistic regression taskthat applies Mean Squared Error (MSE) loss with targets of 0 or 1, a uniform regressor predictsvalues close to 0.5 for each example, resulting in losses close to 0.25, as indicated by the yellowdashed line. As depicted by (b), we expect VFair to exhibit the following two properties. (1) VFair achieves a more flattened loss curve compared to ERM and Worst-case. A flattened curveindicates similar losses for each example, indicating a fairer solution for unknown group partitions.(2) VFair maintains an area under the curve comparable to that of ERM, reflecting the overall modelutility. Since a flattened curve may deteriorate into a uniform model that significantly sacrificesoverall utility, VFair prioritizes keeping the overall average loss at a low value. Statistically, our main idea can be understood as minimizing the loss distributions second moment(e.g., loss variance) while not increasing its first moment. By developing a dynamic approach operatedat both the loss and gradient levels, our idea is proven feasible and effective.",
  "Contributions. The key contributions of this research can be outlined as follows": "We introduce the setting of harmless Rawlsian fairness regardless of demographic prior in bothclassification and regression tasks. To well position this setting, we also discuss its connections withWorst-case fairness and harmless fairness from the view of variance reduction and re-weighting. We advocate that minimizing the variance of prediction losses is a straightforward yet effectivefairness proxy. By incorporating it as a secondary objective, the overall model performance canremain uncompromised.",
  "Related work": "Worst-case fairness without demographics. In alignment with the Rawlsian fairness principle, asequence of studies has followed the Worst-case scheme, which focuses on improving the performanceof the worst-off group without full demographics. DRO Hashimoto et al. identified the worst-off group members by a lower bound for the minimal group ratio. The behind insight is that examplesyielding larger losses are more likely sampled from an underprivileged group and thus should be up-weighted, which inherits the fairness strategy for handling group imbalance Abernethy et al. ,Chai and Wang . Similarly, Martinez et al. also considered training a fair model with agiven ratio of the protected group and connected such a fairness learning setting with the subgrouprobustness problem Liu et al. . In contrast to these studies, ARL Lahoti et al. introducedthe concept of Computational-Identifiability to enhance the Worst-case scheme. ARL presented anadversarial re-weighting method to identify the worst-off group in the computational-identifiableregion without relying on any demographic prior. This embodies the genuine essence of achievingfairness without demographics and is closest to our setting. Harmless fairness. In utility-intensive scenarios, a fair model is meaningful only when it preservesgood utility. Basically, these works engaged in discussing the extent to which fairness can be achievedwithout compromising model utility. Some Martinez et al. , Diana et al. searched forthe so-called minimax Pareto fair optimality for off-the-shelf binary attributes and then upgradedtheir method to the multi-value attribute cases with only side information about group size Martinezet al. . A pre-processing method Li and Liu accomplished cost-free fairness throughre-weighting training examples based on both fairness-related measures and predictive utility on avalidation set. Based on the concept of Rashomon Effect, Coston et al. achieved fairness fromgood-utility models under selective labels through a constrained optimization perspective, needinga proper upper bound for the average loss. The same fairness notation also applies to regressiontask Agarwal et al. , where the prediction error of protected groups remains below somepredefined threshold, and the fairness-accuracy frontier is experimentally achieved. Notably, theseworks more or less require direct or implicit demographic information and cannot adapt to ourproblem setting. A dynamic barrier gradient descent algorithm Gong and Liu was recentlyintroduced which allows models to prioritize must-satisfactory constraints. Inspired by this, weconceptualize harmless fairness within a similar framework, enabling us to move beyond a utility-onlysolution and obtain a fairer model that can narrow the utility gaps among possible data partitions.",
  "Problem setup": "Consider a supervised learning problem from input space X to a label space Y, with training set{zi}Ni=1, where zi = (xi, yi) X Y. For a model parameterized by and a trainingpoint z#, let (z; ) be the associated loss. Suppose for each zi, there exists a sensitive attributesi S. Thus a K-value sensitive attribute s will naturally partition data into K disjoint groups. Suchsensitive attributes are not observed during model training but can be accessible for fairness testing.Following the principle of Rawlsian fairness, the utility disparity over groups is used as a fairnessevaluation metric. For example, in classification tasks, denoting uk as the classification accuracyof the k-th group, we can define the maximum utility disparity, i.e., MUD = maxi,j[K] (ui uj),as a proper fairness metric. More metrics will be introduced in .1, and the same appliesto regression tasks. The fundamental objective of this work is to develop a model that minimizesgroup utility disparity Lahoti et al. , Martinez et al. , Agarwal et al. to the greatestextent possible while maintaining the overall predictive utility (compared to ERM) regardless ofdemographic prior.",
  "Fairness via minimizing variance of losses": "An ERM model may exhibit variable predictive utility across different groups. Conventionally, afair counterpart is achievable by properly incorporating the objective of minimizing group utilitydisparity (e.g., MUD), which is however not applicable when demographics are not accessible attraining stages. Intuitively, a predictive model that can be fair for any arbitrary partitions on the testset implies that the loss of each training example should be close to each other, exhibiting a Diracdelta distribution. A compelling piece of evidence is that an Oracle model, as depicted in (b),ensures that each individual loss (z; ) is sufficiently small, resulting in no disparity, i.e., MUD = 0.This case suggests that group fairness can be instance-wise characterized and hence bypasses theunobserved sensitive attributes. We present this insight by the following proposition.Proposition 1. u s holds for any s that splits data into a number of groups, if and only if the loss is (approximately) independent of the training example z, i.e., z.",
  "The proof of Proposition 1 is left to Appendix A, and the approximation arises from the quantizationof utility metrics, e.g., classification accuracy": "To achieve such a Dirac delta distribution, several fairness objectives can be adopted. We defer thediscussion to Appendix B and conclude that applying the variance of losses as a fairness objective issimple yet efficient. Intuitively, the small variance does encourage the loss to be invariant of input.Suppose that we intend to achieve a small MUD through minimizing the maximum group utilitydisparity, denoted by MUD. The following proposition shows that standard deviation of traininglosses essentially serves as a useful proxy.",
  "Vz[(z; )], where C is a constant": "B.1 gives the proof of Theorem 1. Although MUD is upper-bounded in the form of standarddeviation as stated in Theorem 1, we term it variance for convenience in statements where it doesnot introduce ambiguity. So far, we connect Rawlsian fairness with the variance of training losses,without using any prior of demographics.",
  ".(2)": "We use () and () to denote the primary and secondary objectives, respectively. Since weminimize () inside the optimal set of minimizing (), the eventually learned model is viewed tobe harmlessly fair with regard to the overall performance. Note that the objective of Eq. 1 looks similar to variance-bias research Maurer and Pontil ,Namkoong and Duchi . Following Bennetts inequality, the expected risk can be upper boundedby the empirical risk plus a variance-related term with a high probability:",
  "+ ,(4)": "where C =2(1/min 1)2 + 11/2 and min is a bound of the worst-off groups ratio. Given ant which is the optimal solution of the t-th inner optimization but also happens to be close to themean loss, i.e., t (t), Eq. 4 can be viewed as penalizing the upper semi-variance of trainingloss. This observation connects Worst-case fairness with variance penalization from a new aspect.Although focusing on the variance of training losses in our method as well, we penalize it inside theoptimal set of empirical losses. Our method is eventually capable of achieving harmless fairness.",
  "Harmless fairness update": "Directly calculating the optimal set of () in Eq. 2 can be very expensive. A common approachis to consider an unconstrained form of Eq. 2, i.e., Lagrangian function, which however needs to notonly specify a proper beforehand but also optimize the Lagrange multiplier to satisfy the constraintbest. Besides, the constrained form of Eq. 2 makes our task different from traditional multi-objectiveoptimization tasks. Recognizing that such re-balancing between two loss terms essentially operateson gradients, in a manner analogous to the approach outlined by Gong and Liu , we considerthe following gradient update scheme,",
  ": Two situations when updating primaryand secondary gradient simultaneously": "Gradient view. The idea of designing is tokeep decreasing when the constraint is notmet, meaning that the combined gradient shouldnever hurt the descent of . As depicted in (a), if the gradients and forms anobtuse angle, a detrimental component emergesin the direction of (indicated by the reddashed arrow). Otherwise, the gradient conflictdoes not happen, shown as (b). Conse-quently, should be sufficiently large to ensurethat the combined forces component in the primary direction remains non-negative, that is",
  "||||2 := 1.(6)": "Here, represents the extent to which we wish to update when two gradients are orthogonal. Wechoose = 1 in Eq. 6 because it keeps an intact update for the primary gradient in any cases. Theharmless component of optimizing , illustrated as the dotted yellow arrow in (a), undergoeswith equal strength. The derivation of 1 essentially assumes that the constraint of Eq. 2 is satisfied if = 0, which avoids an elaborately specified . When = 0 but |||| is small, indicating thatthe primary objective is nearly minimized, we set = max{1, 0} to prevent negative values. Loss view. Recall that takes as input according to Eq. 2, which inspires us to further inspect thecombined gradient, denoted by , beyond treating them separately as we do in the gradient view.Theorem 2. Given the objective of Eq. 2, the combined gradient derived by the update scheme ofEq. 5 can be expressed with an example-reweighting form,",
  "i .(7)": "The proof of Theorem 2 can be referred to Appendix C. Eq. 7 shows that our fairness formulationwith dynamic gradient update implicitly reweights each training example via an unnormalized weightwi, i.e., the Z-score of loss plus a coefficient . This finding connects our work with recent Worst-case fairness studies Hashimoto et al. , Martinez et al. , Chai and Wang whichup-weight the training examples whose losses are relatively larger, and also a harmless fairnessmethod Li and Liu which directly applies the re-weighting scheme.",
  "where the last inequality holds because predictive losses are typically designed to be non-negative,facilitating the elimination of the sorting procedure": "Remark 2. According to Eq. 8, 2 is positive. Notably, 2 can approach 0 if , where we mayobtain a model with good utility but poor fairness. Since Z-scores fall within the range of 3 to +3capturing a significant portion (99.7%) of the data in a normal distribution, 2 is often capped by 3.",
  "Experimental setup": "Datasets. Six datasets encompassing binary classification, multi-class classification, and regressionare employed. (i) UCI Adult Asuncion and Newman , (ii) Law School Wightman ,(iii) COMPAS Barenstein , (iv) CelebA Liu et al. , (v) Communities & Crime (C & C) Redmond and Baveja , (vi) AgeDB Moschoglou et al. . Note that datasets (i-iii) canbe transformed into a logistic regression task by applying MSE loss with the category label as thetarget. Following the convention established by Lahoti et al. , Agarwal et al. , we selectsex (or gender) and race (or Young on CelebA) as sensitive attributes for datasets (i-iv), four attributesfor C & C, and one for AgeDB datasets. Metrics. During the evaluation phase, we gain access to the sensitive attributes that partition thedataset into K disjoint groups. As discussed in .1, our training objective is to uphold ahigh overall predictive utility level while minimizing group utility disparities to the greatest extentfeasible. Henceforth, we assess the performance of our method across five distinct metrics: (i) Utility:Overall accuracy for classification (also specified by other metrics like F1-score and prediction errorwhen necessary) and MSE for regression tasks. (ii) WU: The worst group utility among all K groups.(iii) MUD: Maximum utility disparity, as described in .1. (iv) TUD: Total utility disparity.TUD = k[K](uk u), where u is the global average utility. (v) VAR: The variance of predictionerror. Since we are not able to exhaustively enumerate all possible sensitive attributes and test fairnessvia the metrics (ii-iv), VAR necessarily serves as a fairness proxy for any other possible selectedsensitive attributes during the test phase. To ensure the reliability of the results, we repeat all theexperiments 10 times and average the outcomes. Baselines. We compare VFair against seven baselines including ERM, DRO Hashimoto et al. ,ARL Lahoti et al. , FairRF Zhao et al. , MPFR Agarwal et al. , BPF Martinezet al. , and FKL Prez-Suay et al. . Note that DRO, BPF, FairRF, MPFR, and FKLall require some prior demographic information; DRO and BPF necessitate the identification ofthe worst-off group through a bound of group ratio, while FairRF selects some observed featuresas pseudo-sensitive attributes, which consequently constrain its application to image datasets (i.e.,CelebA); MPFR and FKL which are particularly designed for fair regression tasks also incorporatesensitive attributes. Methods take general loss functions like VFair which apply to both classificationtasks and regression tasks, i.e., DRO and ARL are implemented for regression tasks by using theMSE loss. Note that BPF, MPFR, and FKL are not designed with stochastic updates and they sufferfrom out-of-memory issues under our experimental setup on the UCI Adult and AgeDB datasets.Therefore, the experimental results of this part are not included. Please find more experimental setupdetails in Appendix E.",
  "Examine harmless fairness in regression tasks": "showcases the comparison results of different methods on regression tasks. The standarddeviation calculated from every 10 repeated experiments is presented in the bracket. In the Improvedrow, we computed the improvement of VFair compared to ERM, where + denotes improvementrather than a numerical increase. Results with significant changes at the 0.05 significance level arehighlighted in green, while others are in yellow. Note that our objective is to gain improvement infairness metrics while maintaining utility, non-significant changes in utility are desired. However,significant drops in utility violate the harmless setting. According to , we have the following findings. (1) VFair significantly improves most fairnessmetrics with non-significant changes in Utility. Exceptions on C & C and AgeDB are due to theirspecific group partition. C & C is split into 16 groups with some extremely small groups, limiting theimprovement on WU and MAD while VFair still outperforms others on TAD and VAR. AgeDB issplit by gender with a ratio of 4:6, where ERM can also be relatively fair. (2) VFair gains significantVAR improvement on all datasets, guaranteeing that the group utility disparity remains low for anydownstream sensitive attributes. (3) The utility of the test set turns out clear distinctions amongcompared methods because prediction error (MSE) is sensitive to both the possible distribution shiftof test data and model parameters. In this sense, only VFair and ARL can still approach the utility ofERM while the rest usually cannot. (4) DRO gains utility close to 0.25 on each group (i.e. a uniformregressor as shown in (b)) on Law School and COMPAS while using the real prior, shadowedin gray.",
  ": Per-example losses for all com-pared methods sorted in ascending orderon train set": "Our observation. (1) Regardless of the uniform classifierDRO, our method VFair exhibits a more flattened losscurve compared to others while maintaining a compara-ble area under the curve (filled with pink), signifying aharmless fairness solution. Such results align with ourinitial idea, as presented in (b). (2) Our methodVFair implies the Worst-case fairness, the average loss ofthe worst-off group for VFair will be consistently lowerthan any other method. Our claim is obviously true if thegroup size is small. Regarding a larger group size, thanksto the fact the total area under each curve is nearly equaland the curve of VFair is always above others at left, weconclude that the worst-off groups losses for VFair arealso the lowest. (3) The vertical dotted blue line representsthe threshold, where the intersection with the loss curveof VFair values -log(0.5). Divided by it, the samples onthe left are correctly classified, and conversely on the right. As evidenced by the figure, this thresholdis close to each methods intersection. Imagine a situation where the loss curve rotates around thedecision point with a smaller angle to the x-axis, obtaining a smaller sample disparity. However,due to the discrete metric and unchanged group partition, the accuracy-based metrics values for thismethod remain unchanged after rotation. Therefore, despite our method approaching a horizontalloss curve, thus providing a smaller disparity for any potential group split, the fairness improvementis still bounded by the overall utility. Beyond accuracy as utility. Classifying imbalanced data often applies F1-score as a metric, which isfree of the effect on true negative samples which can dominate the accuracy result. We test F1-score",
  "ERM75.0272.176.878.8891.4070.1719.3922.82DRO36.2716.0623.5941.1777.5274.293.94.78ARL74.9071.857.329.4991.6070.3920.1424.33VFair75.9872.745.827.4091.9175.7014.3918.50": "We observe that on UCI Adult, the earned fairness for each fairness method is still limited whileon CelebA, VFair yields superior performance. A reasonable explanation is that VFair has theopportunity to discover better solutions in a relatively larger solution space, where more diverseminima can be examined through fairness criteria. And though F1-score removes the influence oftrue negative samples, it takes the quantified property and hence may only help amply the gains. From quantized to continuous. For scenarios where prediction error (the difference betweenprediction and true label) is desired in classification, e.g., assessing whether a model overestimatesor underestimates, VFair should be more applicable. To justify this insight, we compare fairnessmethods (except for FairRF and DRO as they often fall short on utility) on the three datasets reusedfor regression tasks. Instead of evaluating specific attributes as we do in .2, we test VFair onall possible divisions of the test set by randomly splitting them into K groups. The three methods areranked based on their performance under each metric. From the best to the worst, the rank score is 1,2, and 3. The average rank over 100 times is reported.",
  "ERM2.52.212.562.572.51.892.572.562.51.532.582.62ARL2.52.442.432.422.51.982.432.442.51.562.422.38VFair11.351.011.0112.131112.9111": "Results in show that our method VFair has a better rank than other methods regardless ofthe choice of K, demonstrating that VFair prefers the utility metrics that are loss/error-related. Asmentioned in .1, VAR serves as an approximation for an extreme group split, where eachgroup consists of only one member. Thus, the significantly low VAR in and impliesgood results in random partitions on the test set, evidencing that variance can serve as an effectiveoptimized term in Rawlsian fairness tasks without prior demographic information.",
  "We examine our VFair through extensive experiments. Here we present the partial results and mainconclusions. One can refer to Appendix F.2, F.3, and F.4for more details": "Method efficacy. We monitor the performance of VFair during the training phase by evaluating itwith four utility-related metrics on the test set of COMPAS. (a) indicates these curves naturallyimprove in the desired direction under the variance penalty, verifying the effectiveness of our method. Ablation study. We train our model under four settings: = 1, = max(1, 0), = 2, and = max(1, 2). As depicted in (b), we present the per-dataset utility on five regressiondatasets (results are proportionally scaled on each dataset for a clearer presentation). The full version,considering both 1 and 2, exhibits the most stability in preserving low MSE, enabling a harmless",
  "solution. In (c), we demonstrate an example of 1 and 2 on C & C dataset during training,where both serve distinct and complementary roles in preventing the model from sacrificing utility": "Model examination. We scrutinize the fair models by studying their parameters and predictionsimilarity with ERM. Our experiments found that the model learned by VFair is more dissimilar fromERM than other methods. For example, on Law School, the cosine similarity of model parameters inARL and VFair with ERM is 0.6106 and 0.5839, respectively. This indicates that VFair may explorea larger model space to achieve better performance.",
  "Conclusion": "Towards harmless Rawlsian fairness regardless of demographics, we have introduced a straightforwardyet effective variance-based method VFair. VFair harnesses the principle of decreasing the variance oflosses to steer the models learning trajectory, thereby bridging the utility gaps appearing at potentialgroup partitions. The optimization with a devised dynamic weight parameter operated at both the lossand gradient levels, ensuring the model converges at the fairest point within the optimal solution set.By capping the Z-score, our dynamic weight parameter can also prevent the model from overfocusingon outliers with larger losses. The experiments affirm that regression can be a prior-free task toRawlsian harmless fairness because error-based metrics are more consistent with loss. Strong priorfor demographics may be needed for quantized metrics like accuracy in classification tasks. Asdiscussed in Appendix G, limitations may arise from computational costs, where VFair takes twicethe time of ERM to uncover more information without access to demographic prior. Future work willinvolve identifying and addressing further challenges that may arise when applying VFair for theprediction of non-IID data.",
  "Acknowledgement": "This research is supported by the National Research Foundation, Singapore and Infocomm MediaDevelopment Authority under its Trust Tech Funding Initiative (No. DTC-RGC-04). Any opinions,findings and conclusions or recommendations expressed in this material are those of the author(s)and do not reflect the views of National Research Foundation, Singapore and Infocomm MediaDevelopment Authority. Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness throughawareness. In Proceedings of the 3rd Innovations in Theoretical Computer Science Conference,Jan 2012. doi: 10.1145/2090236.2090255. URL",
  "Moritz Hardt, Eric Price, and Nati Srebro. Equality of opportunity in supervised learning. Advancesin neural information processing systems, 29, Oct 2016": "Shen Yan, Hsien-te Kao, and Emilio Ferrara. Fair class balancing: Enhancing model fairnesswithout observing sensitive attributes. In Proceedings of the 29th ACM International Conferenceon Information & Knowledge Management, Oct 2020. doi: 10.1145/3340531.3411980. URL Vincent Grari, Sylvain Lamprier, and Marcin Detyniecki. Fairness without the sensitive attributevia causal variational autoencoder. In Thirty-First International Joint Conference on ArtificialIntelligence {IJCAI-22}, pages 696702. International Joint Conferences on Artificial IntelligenceOrganization, 2022. Tianxiang Zhao, Enyan Dai, Kai Shu, and Suhang Wang. Towards fair classifiers without sensitiveattributes: Exploring biases in related features. In Proceedings of the Fifteenth ACM InternationalConference on Web Search and Data Mining, pages 14331442, 2022. Zhaowei Zhu, Yuanshun Yao, Jiankai Sun, Hang Li, and Yang Liu. Weak proxies are sufficient andpreferable for fairness with missing sensitive attributes. International Conference on MachineLearning, 202:4325843288, 2023.",
  "John Rawls. Justice as fairness: A restatement. Harvard University Press, May 2001": "Natalia L Martinez, Martin A Bertran, Afroditi Papadaki, Miguel Rodrigues, and Guillermo Sapiro.Blind pareto fairness and subgroup robustness. In International Conference on Machine Learning,pages 74927501. PMLR, 2021. Tatsunori Hashimoto, Megha Srivastava, Hongseok Namkoong, and Percy Liang. Fairness withoutdemographics in repeated loss minimization. In International Conference on Machine Learning,pages 19291938. PMLR, Jun 2018.",
  "Junyi Chai and Xiaoqian Wang. Fairness with adaptive weights. In International Conference onMachine Learning, pages 28532866. PMLR, 2022": "Evan Z Liu, Behzad Haghgoo, Annie S Chen, Aditi Raghunathan, Pang Wei Koh, Shiori Sagawa,Percy Liang, and Chelsea Finn. Just train twice: Improving group robustness without traininggroup information. In International Conference on Machine Learning, pages 67816792. PMLR,2021. Preethi Lahoti, Alex Beutel, Jilin Chen, Kang Lee, Flavien Prost, Nithum Thain, Xuezhi Wang, andEd Chi. Fairness without demographics through adversarially reweighted learning. Advances inneural information processing systems, 33:728740, 2020.",
  "Natalia Martinez, Martin Bertran, and Guillermo Sapiro. Minimax pareto fairness: A multi objectiveperspective. In International Conference on Machine Learning, pages 67556764. PMLR, 2020": "Emily Diana, Wesley Gill, Michael Kearns, Krishnaram Kenthapadi, and Aaron Roth. Minimaxgroup fairness: Algorithms and experiments. In Proceedings of the 2021 AAAI/ACM Conferenceon AI, Ethics, and Society, pages 6676, 2021. Amanda Coston, Ashesh Rambachan, and Alexandra Chouldechova. Characterizing fairness overthe set of good models under selective labels. In International Conference on Machine Learning,pages 21442155. PMLR, 2021.",
  "Matias Barenstein. Propublicas compas data revisited. arXiv preprint arXiv:1906.04711, Jun 2019": "Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild.In Proceedings of the IEEE international conference on computer vision, pages 37303738, Dec2015. doi: 10.1109/iccv.2015.425. URL Michael Redmond and Alok Baveja. A data-driven software tool for enabling cooperative infor-mation sharing among police departments. European Journal of Operational Research, 141(3):660678, Sep 2002. doi: 10.1016/s0377-2217(01)00264-8. URL Stylianos Moschoglou, Athanasios Papaioannou, Christos Sagonas, Jiankang Deng, Irene Kotsia, andStefanos Zafeiriou. Agedb: the first manually collected, in-the-wild age database. In Proceedingsof the IEEE Conference on Computer Vision and Pattern Recognition Workshop, volume 2, page 5,2017. Adrin Prez-Suay, Valero Laparra, Gonzalo Mateo-Garca, Jordi Muoz-Mar, Luis Gmez-Chova,and Gustau Camps-Valls. Fair kernel learning. In Joint European Conference on Machine Learningand Knowledge Discovery in Databases, pages 339355. Springer, 2017.",
  "Proposition 1. u s holds for any s that splits data into a number of groups, if and only if the loss is (approximately) independent of the training example z, i.e., z": "Proof. Suppose that s splits data into K disjoint groups. Let the random variable k represent thegroup index. We can rephrase the statement as s, u k|s z, which is established throughthe following two steps. Step 1. Since proving s, u k|s z is difficult, we consider its contrapositive, i.e., z s, u k. If the value of spreads across a large range, indicating some examples arewell-fitted (small loss) while others are not (large losses), we can simply let s split them according toif well-fitted. Since u1 = u2, u k follows. Step 2. The assertion, z s, u k, is true when the condition z is strictly satisfied.Particularly, if a quantized utility is applied, e.g., accuracy, the assertion holds even if we relaxthe condition exhibits approximate dependence on z. Two distinct scenarios arise. (i) All lossesare concentrated in proximity to the decision boundary, resembling the characteristics of a uniformclassifier. In the context of a finite partition by s, the accuracy of each subgroup within a uniformclassifier statistically converges towards 0.5 for a binary classification. (ii) All losses are conspicu-ously distanced from the decision boundary, akin to an ideal classifier. In this case, an ideal classifierconsistently achieves a subgroup accuracy of 1, irrespective of the chosen split. In both situations, wecan indeed conclude that s, u k|s.",
  "to 2 and further uses Ni<j |i j| L": "Note that Theorem 1 adopts the result of Eq. 12 which scales one side by a factor N, making it nota very tight bound. However, we justify that this option is more efficient than others in the nextsubsection. Additionally, although we start with MUD, it is easy to verify that the derived bound alsoserves as a proxy for other utility disparity metrics, e.g., TUD.",
  "i [N] + i 0 = 2 := 2(15)": "When operated on a mini-batch, unlike and 2, the pairwise difference objective does not considerglobal information, losing the relative relationships when attempting to identify the Worst-case group(also discussed in Appendix D). The instability arising from the difference of pairwise sample lossesmight mislead the upgrading process, as evidenced in our experiments. On challenging datasets suchas COMPAS and CelebA, the model tends to converge towards a uniform classifier, even constrainedby dynamic parameters.",
  "Consequently, we get 2 = 2 mini[N] i": "It can be observed from Eq. 13 that 2 serves as a broader constraint for MUD. As a result, it is a lessrestrictive objective for group disparity compared to . However, the square-version term penalizesmore on both smaller and larger losses, resulting in an unavoidable decrease in overall utility (e.g.,unreliable data with spurious correlation) and hence on all utility-based fairness metrics. Please seethe evidential experiments in that using 2 as objective results in lower variance but highergroup disparity.",
  "DImplementation and algorithm": "To enable the application on large datasets, we provide a mini-batch update strategy. It is worthnoting that the mean loss encompasses global information that could guide the update directionfor each sample. The variance on a mini-batch computed on a local mean loss may cause unstableoptimization, especially when the batch size is small. As such, we consider maintaining a globalmean which assists with the mini-batch update. To this end, we employ Exponential Moving Average(EMA) as an approximation for the global mean loss:",
  "EExperimental setup details": "All the deep-learning-based models, excluding FairRF, which operates within a distinct problemsetting, conform to a shared neural network framework. Specifically, for binary classification tasks,the core neural network architecture consists of an embedding layer followed by two hidden layers, with 64 and 32 neurons, respectively. In the ARL model, an additional adversarial component isintegrated, detailed in its respective paper, featuring one hidden layer with 32 neurons. For multi-classification tasks, the primary neural network transforms into Resnet18, and the embedding layertransitions to a Conv2d-based frontend. Throughout these experiments, the Adagrad optimizerwas employed. FairRF, utilizing its officially published code implementation, maintains the samebackbone network with nuanced variations in specific details. Particularly, fair regression methodsMPFR and FKL are implemented adapted from Agarwal et al. . As For the loss function, we implemented Binary Cross-Entropy, Cross-Entropy, and Mean SquareError for binary classification, multi-class classification, and regression tasks, respectively. Note thatour method is general and can be compatible with any other forms of loss.",
  "All experiments were conducted on Ubuntu 20.04 with one NVIDIA GeForce RTX 3090 graphicsprocessing unit (GPU), which has a memory capacity of 24 GB": "To compare all baselines under the harmless fairness setting, we implement them into the samescheme and select the epoch with the nearest loss compared to a converged ERM. Detailedly, eachmethod has an empirical loss, which in our method is denoted as and in ARL is denoted as learnerloss (compared to adversarial loss). Based on this loss, we select the harmless epoch which has thenearest loss value compared to a well-trained ERM model.",
  "ERM92.8089.7703.6404.7740.08DRO83.9782.192.372.721.48ARL93.2689.8404.0205.4137.38FairRF-----VFair93.4391.0902.7403.8511.70": "From the experimental results in , we can observe that: (1) VFair, without any prior, consis-tently achieves top-2 performances in classification tasks, competing with or outperforming baselinesthat use priors, e.g., DRO and FairRF. However, except for VAR, metrics earn limited improvements. We calculated the p-value for each metric between ERM and VFair to quantify the limitation. Theresults show that the p-value of metrics, except for VAR, remains high. Generally, a p-value less than0.05 is considered indicative of a significant difference between the two groups. Even with the samedatasets, especially on COMPAS, it is found that harmless Rawlsian fairness is difficult to earn forclassification while comparatively easier for regression tasks. (2) From the Utility dimension, FairRFand DRO sometimes fail to guarantee a comparable utility, because constraining group fairnesson their proxy attributes unavoidably hurts the overall model performance. With this cost, theysometimes achieve a noteworthy fairness improvement. Note that DRO turns into a uniform classifieron COMPAS, shadowed in gray. (3) CelebA seems an exception where VFair attains meaningfulfairness improvement while others do not. A reasonable explanation is that VFair has the opportunityto discover better solutions in a relatively larger solution space, where more diverse minima can beexamined through fairness criteria. (4) We also notice that because we explicitly optimize variance,VAR has been remarkably decreased in VFair across all datasets, showing flattened prediction errorson all test sets.",
  ": The loss curve of primary objective during the training process on four benchmark datasets": "We depict the full version of the per-sample losses for all compared methods sorted in ascendingorder on the training set in . From , we surprisingly see that across different datasets ourVFair is the unique one that has the flattened curve while DRO, ARL, and FairRF are essentiallyclose to ERM. The full version of test performance curves of four utility-related metrics during the training processon four benchmark datasets are present in . Our VFair effectively improves all utility-relatedmetrics. illustrates the convergence of training loss on four benchmark datasets. As the final combinedobjective is updated directly at the gradient level, which does not have a unified loss form, we displaythe curve of losses of the primary objective, representing the models utility. We observe that ourupgrading method in Eq. 7 effectively steers the model towards convergence. Note that our dynamicupdating strategy is similar to Gong and Liu , which is theoretically proven to converge.",
  "According to the ablation setting in .4, we conducted throughout experiments on theclassification task and the regression task, respectively": "As shown in , our method already achieves competitive results by solely employing 2. Thefull version which integrates both 1 and 2 demonstrates more stable results. Notably, on the LawSchool and COMPAS datasets, there exist situations when the model converges towards a uniformclassifier, as indicated by the gray region. These uniform classifiers predict all the samples near thedecision boundary, causing their losses to share very similar values and form variances at a scale ofaround 1e 7. This phenomenon underscores the effectiveness of 2 in preventing the model fromcollapsing to a low-utility model. Moreover, by adding 1, our method consistently improved in fourutility-related metrics. These results show that 1 effectively guides the model to converge to a betterpoint at the gradient level. : Comparison of classification ablation results (%) on four benchmark datasets. All of theresults are averaged over 10 repeated experiments to mitigate randomness, with the best resultshighlighted in red and the second-best in blue (excluding the uniform situation).",
  "F.4Model similarity with ERM": "We examine the similarity between fair models and an ERM model. We conduct experimentscomparing ERM, DRO, ARL (without adversary network), and VFair, as they share the same modelstructure. By calculating the Cosine similarity of model parameters and prediction similarity withERM as a reference, we get results shown in . : Comparison of regression ablation results (%) on four benchmark datasets. All of the resultsare averaged over 10 repeated experiments to mitigate randomness, with the best results highlightedin red and the second-best in blue (excluding the uniform situation).",
  "UCI Adult0.29560.99570.995532.75%97.45%96.55%Law School0.16930.61060.583937.19%95.61%95.91%CelebA0.16630.18860.147466.47%95.41%94.62%": "As evidenced by Law School and CelebA, similar predictions do not necessarily indicate similarmodel parameters. Note that VFair could be more distinct from ERM compared to other fair models,especially on complicated image dataset CelebA. The better fairness improvement in alsoproves that VFair can explore different minima in broader model space, guiding the model to convergeto a fairer point.",
  "COMPASPMG53.5249.149.9710.5813.21MMPF66.3963.912.155.44-VFair66.8063.866.258.471.86": "We have further supplemented control experiments, where the model has access to sensitive attributesand is optimized under constrained regularization. In detail, we reproduced the MMPF in Martinezet al. and further designed experiments that penalize the losses of the minority group, denotedas PMG. As MMPF is not applicable to image datasets, the results are conducted on three benchmarkdatasets shown in . By leveraging additional group information, MMPF achieves improvedfairness results, showing that group priors are indeed needed for significant fairness improvementin classification tasks. However, MMPF is not a harmless approach, particularly evident on LawSchool, where it sacrifices model utility for a fairer point. PMG yields unsatisfactory performanceconsistently due to its excessive focus on the minority group, missing general information from othergroups.",
  "GComputational costs": "Since the backward pass is the bottleneck of the total computation, we found that VFair requiresapproximately twice the computation time compared to the ERM method, as shown in withthe Law School dataset as an example. Note that ARL, an adversarial method, requires a comparablewall-clock time to VFair due to its inner and outer optimization nature.",
  "The answer NA means that the paper has no limitation while the answer No means thatthe paper has limitations, but those are not discussed in the paper": "The authors are encouraged to create a separate \"Limitations\" section in their paper. The paper should point out any strong assumptions and how robust the results are toviolations of these assumptions (e.g., independence assumptions, noiseless settings,model well-specification, asymptotic approximations only holding locally). The authorsshould reflect on how these assumptions might be violated in practice and what theimplications would be. The authors should reflect on the scope of the claims made, e.g., if the approach wasonly tested on a few datasets or with a few runs. In general, empirical results oftendepend on implicit assumptions, which should be articulated. The authors should reflect on the factors that influence the performance of the approach.For example, a facial recognition algorithm may perform poorly when image resolutionis low or images are taken in low lighting. Or a speech-to-text system might not beused reliably to provide closed captions for online lectures because it fails to handletechnical jargon.",
  "If applicable, the authors should discuss possible limitations of their approach toaddress problems of privacy and fairness": "While the authors might fear that complete honesty about limitations might be used byreviewers as grounds for rejection, a worse outcome might be that reviewers discoverlimitations that arent acknowledged in the paper. The authors should use their bestjudgment and recognize that individual actions in favor of transparency play an impor-tant role in developing norms that preserve the integrity of the community. Reviewerswill be specifically instructed to not penalize honesty concerning limitations.",
  ". Experimental Result Reproducibility": "Question: Does the paper fully disclose all the information needed to reproduce the main ex-perimental results of the paper to the extent that it affects the main claims and/or conclusionsof the paper (regardless of whether the code and data are provided or not)?Answer: [Yes]Justification: We provide the algorithm of our method in Appendix D and the full code insupplemental materials including our method and compared methods.Guidelines: The answer NA means that the paper does not include experiments. If the paper includes experiments, a No answer to this question will not be perceivedwell by the reviewers: Making the paper reproducible is important, regardless ofwhether the code and data are provided or not.",
  "If the contribution is a dataset and/or model, the authors should describe the steps takento make their results reproducible or verifiable": "Depending on the contribution, reproducibility can be accomplished in various ways.For example, if the contribution is a novel architecture, describing the architecture fullymight suffice, or if the contribution is a specific model and empirical evaluation, it maybe necessary to either make it possible for others to replicate the model with the samedataset, or provide access to the model. In general. releasing code and data is oftenone good way to accomplish this, but reproducibility can also be provided via detailedinstructions for how to replicate the results, access to a hosted model (e.g., in the caseof a large language model), releasing of a model checkpoint, or other means that areappropriate to the research performed. While NeurIPS does not require releasing code, the conference does require all submis-sions to provide some reasonable avenue for reproducibility, which may depend on thenature of the contribution. For example(a) If the contribution is primarily a new algorithm, the paper should make it clear howto reproduce that algorithm.",
  "(b) If the contribution is primarily a new model architecture, the paper should describethe architecture clearly and fully": "(c) If the contribution is a new model (e.g., a large language model), then there shouldeither be a way to access this model for reproducing the results or a way to reproducethe model (e.g., with an open-source dataset or instructions for how to constructthe dataset). (d) We recognize that reproducibility may be tricky in some cases, in which caseauthors are welcome to describe the particular way they provide for reproducibility.In the case of closed-source models, it may be that access to the model is limited insome way (e.g., to registered users), but it should be possible for other researchersto have some path to reproducing or verifying the results.",
  "Guidelines:": "The answer NA means that the paper does not use existing assets. The authors should cite the original paper that produced the code package or dataset. The authors should state which version of the asset is used and, if possible, include aURL. The name of the license (e.g., CC-BY 4.0) should be included for each asset. For scraped data from a particular source (e.g., website), the copyright and terms ofservice of that source should be provided. If assets are released, the license, copyright information, and terms of use in thepackage should be provided. For popular datasets, paperswithcode.com/datasetshas curated licenses for some datasets. Their licensing guide can help determine thelicense of a dataset.",
  "If the authors answer NA or No, they should explain why their work has no societalimpact or why the paper does not address societal impact": "Examples of negative societal impacts include potential malicious or unintended uses(e.g., disinformation, generating fake profiles, surveillance), fairness considerations(e.g., deployment of technologies that could make decisions that unfairly impact specificgroups), privacy considerations, and security considerations. The conference expects that many papers will be foundational research and not tiedto particular applications, let alone deployments. However, if there is a direct path toany negative applications, the authors should point it out. For example, it is legitimateto point out that an improvement in the quality of generative models could be used togenerate deepfakes for disinformation. On the other hand, it is not needed to point outthat a generic algorithm for optimizing neural networks could enable people to trainmodels that generate Deepfakes faster. The authors should consider possible harms that could arise when the technology isbeing used as intended and functioning correctly, harms that could arise when thetechnology is being used as intended but gives incorrect results, and harms followingfrom (intentional or unintentional) misuse of the technology. If there are negative societal impacts, the authors could also discuss possible mitigationstrategies (e.g., gated release of models, providing defenses in addition to attacks,mechanisms for monitoring misuse, mechanisms to monitor how a system learns fromfeedback over time, improving the efficiency and accessibility of ML).",
  "The answer NA means that the paper does not involve crowdsourcing nor research withhuman subjects": "Depending on the country in which research is conducted, IRB approval (or equivalent)may be required for any human subjects research. If you obtained IRB approval, youshould clearly state this in the paper. We recognize that the procedures for this may vary significantly between institutionsand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and theguidelines for their institution."
}