{
  "Abstract": "Temporal action segmentation and long-term action anticipation are two popularvision tasks for the temporal analysis of actions in videos. Despite apparent rele-vance and potential complementarity, these two problems have been investigatedas separate and distinct tasks. In this work, we tackle these two problems, ac-tion segmentation and action anticipation, jointly using a unified diffusion modeldubbed ActFusion. The key idea to unification is to train the model to effectivelyhandle both visible and invisible parts of the sequence in an integrated manner;the visible part is for temporal segmentation, and the invisible part is for futureanticipation. To this end, we introduce a new anticipative masking strategy duringtraining in which a late part of the video frames is masked as invisible, and learnabletokens replace these frames to learn to predict the invisible future. Experimentalresults demonstrate the bi-directional benefits between action segmentation andanticipation. ActFusion achieves the state-of-the-art performance across the stan-dard benchmarks of 50 Salads, Breakfast, and GTEA, outperforming task-specificmodels in both of the two tasks with a single unified model through joint learning.",
  "Introduction": "In everyday life, when interacting with people, we anticipate their future actions while recognizingtheir actions observed in the present and the past. Similarly, for effective human-robot interaction,robotic agents must recognize ongoing actions while anticipating future behaviors. Two essentialtasks in computer vision for such a temporal understanding of human actions are temporal actionsegmentation (TAS) and long-term action anticipation (LTA) . The task of TAS aims at translating observed video frames into a sequence of actionsegments, while the goal of LTA is to predict a plausible sequence of actions in the future based onthe observed video frames. These tasks are closely related in terms of understanding the relationsbetween actions; recognizing actions in the present and the past may improve anticipating action inthe future, and the ability to anticipate the future may also enhance recognizing observable actionswhen facing visual ambiguities. Despite the apparent relevance and potential complementarity, these two problems have been investi-gated as separate and distinct tasks. While a growing body of work has shown remarkable progresson both TAS and LTA, these methods are primarily designed for one task (see a) and showpoor generalization when applied to the other (e.g., see FUTR and DiffAct in c).Some prior methods tackle both tasks simultaneously, but they rely on task-specific architectures andrequire separate training for each individual task (e.g., see TempAgg in c). In this paper, we introduce ActFusion, a unified diffusion model that addresses TAS and LTA througha single architecture and training process, as illustrated in b. The core idea of our unificationlies in training the model to effectively handle two different parts of the sequence: visible partsfor action segmentation and invisible parts for action anticipation. Accordingly, we propose a",
  "Related work": "Temporal action segmentation (TAS). The goal of TAS is to classify frame-wise action labels in a video. Earlier approaches employ temporal slidingwindows for action segment detection, grammar-based methods have been intro-duced to incorporate a temporal hierarchy of actions during segmentation. Temporal convolutionnetworks and Transformer-based models are introduced based on deep learningmethods. Since learning long-term relations of actions from activity videos is challenging, a series ofwork has been proposed to develop refinement strategies that can beapplied to the TAS models . Recently, DiffAct is proposed to iteratively denoising actionpredictions conditioned on the input video features adopting the diffusion process. In a similar spirit,the proposed ActFusion is based on the diffusion process, focusing on unifying TAS and LTA throughanticipative masking. We demonstrate the bidirectional benefits between the two tasks, showing thatlearning segmentation along with anticipation is effective.",
  "Long-term action anticipation (LTA). LTA has recently emerged as acrucial task for predicting a sequence of future actions in long-term videos. Initial models use RNNs": "and CNNs , while time-conditioned anticipation introduces one-shot anticipation of specificfuture timestamps. A GRU-based model used cycle-consistent relations of past and futureactions. Sener et al. proposed TempAgg, a multi-granular temporal aggregation method foraction anticipation and recognition, utilizing different model architectures and task-specific losses forthe two tasks. Gong et al. propose a transformer model for parallel anticipation, dubbed FUTR,empirically showing that learning action segmentation as an auxiliary task is helpful for anticipation.Nawhal et al. propose a two-stage learning approach for LTA and Zhang et al. presentobject-centric representations using visual-language pre-trained models for LTA. While previouswork adopts TAS as an auxiliary task to help learn LTA, a unified model evaluated onboth tasks is rarely explored, often showing poor cross-task generalization performance (c).The exception is TempAgg, which requires separate training and model architecture for the two tasks.In this work, we present a unified model that tackles both TAS and LTA in a single training process. Diffusion models. Recent success in denoising diffusion models opens a new eraof computer vision research. The diffusion models learn the original data distribution through theiterative denoising process. Diffusion models have recently shown notable success in various domains,such as image generation , video generation , object detection , semanticsegmentation , temporal action segmentation , self-supervised learning , and etc .Recently, DiffMAE integrates masked autoencoders with the diffusion models, where the modellearns to denoise masked input while learning data distributions through generative pre-training ofvisual representations. In this work, we present a unified diffusion model that effectively integratesTAS and LTA through masking, where the model learns to denoise action labels conditioned on bothvisual and mask tokens. In this way, the model can effectively learn temporal relations betweenactions by classifying visual tokens and inferring missing actions of the mask tokens.",
  "Preliminary": "In this section, we give a brief overview of diffusion models . The diffusion models learn adata distribution by mapping and denoising noises from the original data distribution. The trainingprocess of diffusion models involves forward and reverse processes from random noise. During theforward process, Gaussian noise is added to the original data, while in the reverse process, a neuralnetwork learns to reconstruct the original data by iteratively removing noise.",
  "(s).(1)": "Here, a noise N(0, I) is added to the original data distribution x0 following the decreasingfunction (s) of time step s {1, 2, ..., S}, where S represents the entire forward time-step. Notethat the function (s) determines the intensity of the noise added to the original data following thepre-defined variance schedule .",
  "[M]": ": Overall pipeline of ActFusion. During training, we randomly select one of five maskingstrategies and apply it to input video frames F , replacing masked regions with learnable tokensto obtain masked features F . These features are processed by the encoder g to produce visualembeddings E, which condition the decoder h to denoise action labels from As to A0 at time-steps. For inference, we use different masking strategies depending on the task: no masking for TASand anticipative masking for LTA. The decoder then iteratively denoises action labels followingAS AS ... A0 using the DDIM update rule .",
  "Problem setup": "Temporal action segmentation (TAS) aims to classify input video frames into a sequence of predefinedaction classes, while long-term action anticipation (LTA) predicts future actions based on partiallyobserved video sequences. Formally, given a video sequence F = [F1, F2, , FT ] of length T,TAS predicts frame-wise action labels A = [A1, A2, , AT ], where each Ai is a one-hot vectorrepresenting the action class. In LTA, given the first N O = T observed frames, the goal is toanticipate action labels for the subsequent N A = T frames, where and represent the observation and anticipation ratios, respectively. Here, denotes the ceiling function.",
  "ActFusion": "ActFusion aims to unify TAS and LTA by leveraging an encoder-decoder architecture with adaptivemasking strategies. illustrates the overall pipeline, where ActFusion consists of a maskedencoder g and a denoising decoder h. The encoder obtains visual features and mask tokens as inputand generates embedded tokens as output. The decoder then progressively reduces noises through aniterative denoising process conditioned on the embedded tokens. During training, we randomly sample a time step s {1, 2, ..., S} for each iteration and add noise N(0, I) to the ground-truth action labels A0 following Eq. 1, resulting in noisy action labels As.The decoder then aims to denoise As to reconstruct the original action labels A0. During inference,the decoder starts with Gaussian noise AS and progressively denoises it following the DDIM update rule to generate final predictions A0, i.e., AS AS ... A0. The key to our unified approach lies in training the model to effectively process both visible andinvisible parts of the sequence, where the visible part corresponds to observed video frames andthe invisible part represents future frames to be anticipated. To this end, we introduce anticipativemasking that replaces unobserved frames with learnable mask tokens, enabling the model to learnfuture predictions. We apply this masking strategy consistently during both training and inferenceto achieve joint learning of action segmentation and anticipation. We further incorporate randommasking, where video frames are randomly masked to enhance robustness against visual ambiguities.Both masking strategies are detailed in Sec. 4.3. Input structuring. Given input video features F RT C with T frames of C feature dimensions,we start by defining a binary mask matrix M {0, 1}T 1. This mask serves as a frame selector,where a value of 0 indicates a frame to be masked and replaced by mask tokens, while a value of 1indicates an unmasked frame. The learnable mask token, denoted as m R1C, replaces the visual features in frames selected for masking, producing the input features F for the encoder:F = F M + (1T C M) m,(3)where denotes element-wise multiplication and 1ij represents a matrix of ones with dimensionsi j. Here, M and m are broadcasted along the channel and temporal dimensions, respectively. For our model architecture, we use a modified version of ASFormer used in DiffAct , whichemploys dilated attention to capture both local and global relations in the input sequence (See Fig. S2for the detailed model architecture). Encoder. The encoder consists of the N E layers, each consisting of a dilated 1-d convolutionfollowed by instance normalization, dilated attention, and a feed-forward network . In the dilatedattention mechanism, the receptive field is limited to a local window size w = 2i for the i-th layer,where the increasing dilation captures progressively broader temporal relations. The output of eachlayer is combined with its input via a residual connection before proceeding to the subsequent layer.Given the input features F , the encoder g produces embedded tokens E RT D as:E = g(F ),(4)where D represents the dimensions of embedded tokens. A fully-connected (FC) layer W enc RDK is applied to E to obtain frame-wise classification logits from the encoder followed by asoftmax:Aenc = (EW enc),(5)where K is the number of action classes and is the softmax. Decoder. The decoder is composed of N D sequential layers. Each layer consists of dilated 1-dconvolution, dilated attention, instance normalization, and feed-forward networks. As in the encoder,the dilation ratio for the i-th layer is set to w = 2i. The output of each layer is combined with itsinput through a residual connection before being passed into the subsequent layer.Given a time step s, noisy action labels As at time step s, and encoder embeddings E, the decoder hproduces output embeddings Y RT D according to:Y = h(As, s, E).(6)",
  "We introduce two distinct masking strategies: anticipative masking and random masking": "Anticipative masking. Anticipative masking enables joint learning of TAS and LTA by separatingvisible and invisible parts of the input sequence. This strategy masks the latter portion of video frames,requiring the model to predict future actions based on observed frames. Given a video length T, wedefine a binary anticipation mask M A {0, 1}T that sets visible frames to 1 and invisible frames to0: M Ai = 1i N O, where 1 is an indicator function and N o represents the number of observedframes. Unlike causal masking that prevents attending to future tokens, our anticipative maskingallows interactions among visible tokens while maintaining a clear boundary for anticipation. Random masking. Random masking aims to provide robustness in prediction when parts of videoframes are missing or ambiguous . A video is first divided into pre-defined clips of size Q,resulting in N P = T",
  "Q total clips. Then, N R clips are randomly selected to be masked. A binaryrandom mask is defined by M Ri = 1(j P, (j 1)Q < i jQ), where P is a randomly selectedsubset of {1, , N P} with |P| = N R": "For fully observable scenarios, we utilize a no mask strategy M N where all frames remain visible.Additionally, we adopt two masking strategies , the relation mask M S and the boundary maskM B, as illustrated in . The relation mask M S randomly masks segments associated with anaction class to learn inter-action dependencies. The boundary mask M B masks frames at actiontransitions to enhance boundary detection. See Sec. C for the detailed formulations of the masks. During training, one of five masking strategies is randomly selected: no mask M N, anticipative maskM A, random mask M R, relation mask M S, and boundary mask M B. For inference, no mask M N isused for TAS and anticipative mask M A for LTA. : Comparison with state of the art on TAS. The overall results demonstrate the efficacy ofActFusion on TAS, achieving state-of-the-art performance across benchmark datasets. Bold valuesrepresent the highest accuracy, while underlined values indicate the second-highest accuracy.",
  "F1@{10, 25, 50}editAcc.Avg.F1@{10, 25, 50}editAcc.Avg.F1@{10, 25, 50}EditAcc.Avg": "MS-TCN 76.3 / 74.0 / 64.567.980.772.752.6 / 48.1 / 37.961.766.353.385.8 / 83.4 / 69.879.076.378.9MS-TCN++ 80.7 / 78.5 / 70.174.383.777.564.1 / 58.6 / 45.965.667.660.488.8 / 85.7 / 76.083.580.182.8SSTDA 83.0 / 81.5 / 73.875.883.279.575.0 / 69.1 / 55.273.770.268.690.0 / 89.1 / 78.086.279.884.6GTRM 75.4 / 72.8 / 63.967.582.672.457.5 / 54.0 / 43.358.765.055.7- / - / ----BCN 82.3 / 81.3 / 74.074.384.479.368.7 / 65.5 / 55.066.270.465.288.5 / 87.1 / 77.384.479.883.4MTDA 82.0 / 80.1 / 72.575.283.278.674.2 / 68.6 / 56.573.671.068.890.5 / 88.4 / 76.285.880.084.2Global2local 80.3 / 78.0 / 69.873.482.276.774.9 / 69.0 / 55.273.370.768.689.9 / 87.3 / 75.884.678.583.2HASR 86.6 / 85.7 / 78.581.083.983.174.7 / 69.5 / 57.071.969.468.590.9 / 88.6 / 76.487.578.784.4ASRF 84.9 / 83.5 / 77.379.384.581.974.3 / 68.9 / 56.172.467.667.989.4 / 87.8 / 79.883.777.383.6ASFormer 85.1 / 83.4 / 76.079.685.681.976.0 / 70.6 / 57.475.073.570.590.1 / 88.8 / 79.284.679.784.5ASFormer + KARI 85.4 / 83.8 / 77.479.985.382.478.8 / 73.7 / 60.877.874.073.0- / -/ ----Temporal Agg. - / - / ----59.2 / 53.9 / 39.554.564.554.3- / - / ----UARL 85.3 / 83.5 / 77.878.284.181.865.2 / 59.4 / 47.466.267.861.292.7 / 91.5 / 82.888.179.686.9DPRN 87.8 / 86.3 / 79.482.087.284.575.6 / 70.5 / 57.675.171.770.192.9 / 92.0 / 82.990.982.088.1SEDT 89.9 / 88.7 / 81.184.786.586.2- / - / ----93.7 / 92.4 / 84.091.381.388.5 TCTr 87.5 / 86.1 / 80.283.486.684.876.6 / 71.1 / 58.576.177.572.091.3 / 90.1 / 80.087.981.186.1FAMMSDTN 86.2 / 84.4 / 77.979.986.483.078.5 / 72.9 / 60.277.574.872.891.6 / 90.9 / 80.988.380.786.5DTL 87.1 / 85.7 / 78.580.586.983.778.8 / 74.5 / 62.977.775.873.9- / - / ----UVAST 89.1 / 87.6 / 81.783.987.485.976.9 / 71.5 / 58.077.169.770.692.7 / 91.3 / 81.092.180.287.5BrPrompt 89.2 / 87.8 / 81.383.888.186.0- / - / ----94.1 / 92.0 / 83.091.681.288.4MCFM 90.6 / 89.5 / 84.284.690.387.8- / - / ----91.8 / 91.2 / 80.888.080.586.5LTContext 89.4 / 87.7 / 82.083.287.786.077.6 / 72.6 / 60.177.074.272.3- / - / ----DiffAct 90.1 / 89.2 / 83.785.088.987.480.3 / 75.9 / 64.678.476.475.192.5 / 91.5 / 84.789.682.288.1",
  "Training objective": "The model is trained with three types of losses: cross-entropy loss Lce for frame-wise classification,boundary smoothing loss Lsmo , and boundary alignment loss Lbd . These losses are appliedto both encoder and decoder, where the encoder serves as an auxiliary task to enhance discriminationability. Given the ground-truth action label A0 RT K and the predictions A RT K, the crossentropy loss is defined by:",
  "ActFusion (ours)28.2525.5224.6623.2535.7931.7629.6428.78": "method on three widely-used benchmark datasets: 50 Salads , Breakfast , and GTEA (see Sec. F for details). All three datasets are used to evaluate on TAS, while 50 Salads and Breakfastare used for evaluating LTA, following the protocols of the previous work . Evaluation metrics. For evaluation metrics for TAS, we report F1@{10, 25, 50} scores, the editscore, and frame-wise accuracy . The F1 scores and the edit score are segment-wisemetrics, and accuracy is a frame-wise metric. Mean over classes accuracy (MoC) is adopted as anevaluation metric for LTA . Implementation details We utilize the pre-trained I3D features as input video features for alldatasets provided by . For the diffusion process, we set the entire time step S as 1000 ,with a skipped time step for inference set to 25 . For the anticipation mask M A, we set theobservation ratio {0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8}. For the random mask M R, we fix the sizeof patch w to 10 for both tasks and randomly select the number of masked patches N R to 25, 10,and 20 for 50 Salads, Breakfast, and GTEA, respectively. For the encoder and decoder, we adopt amodified version of ASFormer using relative positional bias where the maximum number ofneighbors is set to 100. During inference, we set to 1 and to 0 for TAS . For LTA,we set {0.2, 0.3} and {0.1, 0.2, 0.3, 0.5}, following the evaluation protocols .See Sec. G for more details.",
  "Comparison with the state of the art on TAS and LTA": "Tables 1 and 2 present performance comparisons across benchmark datasets, where our singleunified model achieves superior results on both tasks. shows the results on TAS, whereActFusion outperforms all task-specific models demonstrating the benefits of joint learning. compares LTA performance across different datasets and input types: predicted action labels of thevisual features and pre-trained I3D features . Overall, ActFusion achieves state-of-the-artperformance on two benchmark datasets, demonstrating the efficacy of joint learning for TAS and LTAbased on the diffusion process. Note that we do not include the performance of ANTICIPATR due to differences in evaluation setup, as reported in .",
  "(1)42.8027.1123.4822.07(2)-35.2423.1515.729.38(3)-38.6025.5720.4218.62(4)-37.2325.2920.8220.35(5)-41.4227.3822.5419.32": "Segmentation helps anticipation. To validate the impact of learning TAS on LTA, we conductloss ablation experiments by removing the action segmentation loss on the observed N O frames. shows the results. Specifically, we exclude the encoder loss Lenc in Eq. 11, while omittingthe decoder loss Ldec in Eq. 12 on the observed frames. For simplicity, we denote the decoder lossfor the observed frames and unobserved frames to be anticipated as LdecOand LdecA , respectively. Notethat LdecOare utilized for effective anticipation learning. By comparing (1) and (3) in , weobserve a significant performance drop when removing the loss on the observed frames. We observethat applying segmentation loss on the observed frames in the decoder improves performance whencomparing (1) and (3). The overall results show that learning action segmentation plays a crucial rolein improving action anticipation. Anticipation helps segmentation. To validate the effect of jointly learning anticipation along withsegmentation, we conduct ablation studies on anticipative masking M A in . Comparing(1) and (2) in a, we find that using anticipation masking helps improve the performance ofTAS. In comparison to the results in , where segmentation greatly enhances anticipation, theperformance improvement without using anticipation masking is slightly lower. Nevertheless, wefind that anticipation does contribute positively to the overall metrics of segmentation. Ablation studies on masking types. To evaluate the effects of different types of masking on TASand LTA, we conduct ablation studies in . The performance on TAS and LTA are presented ina and 4b, respectively. We observe that anticipative masking M A plays a significant role inthe joint learning of TAS and LTA, as evidenced by the substantial performance drop in LTA whencomparing row (2) with the other rows in b. Random masking M R significantly reducesperformance in both TAS and LTA, as shown in row (3) of . Furthermore, the use of boundarymasking M B and relation masking M S is also essential for both tasks. Due to the limited space, weonly report the performance on LTA when the observation ratio is set to 0.3. Effects of learnable mask tokens. To verify the effects of learnable mask tokens m, we replace mwith zero vectors. shows the overall results. Comparing rows (1) and (2) in , we findthat using learnable mask tokens is more effective for both TAS and LTA. We conjecture that thisis due to the mask tokens being embedded with the visual tokens within the encoder, which aids ineffectively anticipating the invisible parts. Position of mask tokens. Instead of providing mask tokens as input to the encoder, we conductexperiments on providing mask tokens as input to the decoder, as shown in row (3) in . In thisexperiment, only the visible frames are given as input to the encoder, while mask tokens m R1Dare applied in the decoder to fill the original masked positions, similar to the approach used in maskedauto-encoder . By comparing rows (2) and (3) in , we observe that using mask tokensin the encoder is more effective than using them in the decoder. We hypothesize that embeddingmask tokens alongside visual tokens within the encoder benefits joint learning of TAS and LTA.Furthermore, when mask tokens are provided only to the decoder, they do not receive the encoderloss Lenc, which may negatively impact performance.",
  ": Inference steps of diffusion process": "The number of masked clips N R in random masking. To determine the number of masked clipsN R in random masking, we conduct experiments by adjusting N R from 10 to 100 while fixingthe window size Q of each masked clip N P to 10. illustrates the result, suggesting thatemploying an appropriate amount of masking is crucial. The number of inference steps in the diffusion process. We compare performance according tothe number of inference steps of the diffusion process in . We observe consistent performanceincreases as the number of inference steps increases. As the increasing step requires more computation,we choose to use 25 steps for inference in all of our experiments.",
  "Qualitative results": "presents qualitative results evaluated on both TAS and LTA using a single model. Eachfigure includes video frames, ground-truth action sequences, and predicted results for TAS and LTA.Only the visible parts (observed frames) are used as input during inference on LTA. Overall resultsshow that ActFusion effectively handles both visible and future segments, accurately classifyingcurrent actions and anticipating future ones. Additional results are provided in Fig. S3 and S4.",
  "Evaluation without using ground truth prediction length on LTA": "Benchmark evaluations in previous work, including those of and our results presented above,have been typically conducted following the evaluation protocol of where prediction length N A isset to T in testing; and T are the prediction ratio and the ground-truth video length, respectively.This can be seen as a leakage of ground-truth information in testing because the exact length offuture actions is supposed to be inherently unknown during inference in real-world scenarios. In thissubsection, we thus rectify the evaluation setting by determining the prediction length as rN O basedsolely on the number of observed frames N O, where r is a hyperparameter that adjusts the relativelength of future predictions. We then train our method with this modified anticipation maskingstrategy, denoted as ActFusion. In this experiment, r is set to 4. Using the rectified evaluation protocol, we compare ours with Cycle Cons. and FUTR ,whose codes are available1. For a fair comparison, we modify both models to use the same predictionlength rN O, denoted as Cycle Cons. and FUTR. The results are summarized in wherethe numbers in parentheses indicate relative performance changes compared to those in . shows that all the methods exhibit overall performance degradation when ground-truth lengthinformation is not used. These observations reveal that previous results have benefited from the useof ground-truth length T, leading to information leakage in testing. Therefore, we suggest that futureresearch avoid relying on this information for a more realistic and fair comparison. On the otherhand, the results also show that our approach consistently outperforms the methods across benchmarkdatasets, demonstrating its effectiveness even without ground-truth prediction length information.Please refer to Sec. G for further details of the experimental setup.",
  "Conclusion": "We have presented ActFusion, a unified diffusion model that jointly addresses temporal actionsegmentation and long-term action anticipation in videos. The key to our method is anticipativemasking, where learnable mask tokens replace unobserved frames, enabling simultaneous actionsegmentation of visible parts and anticipation of invisible parts. Our comprehensive experimentsdemonstrate that this unified approach not only outperforms existing task-specific models on bothtasks but also reveals the mutual benefits of joint learning. Additionally, by evaluating our method bothwith and without ground-truth length information during LTA inference, we hope to motivate futureresearch toward not using this information during testing. We believe that ActFusion demonstrates thepotential of unifying complementary tasks in temporal action understanding, opening new directionsfor future research.7Acknowledgement This work was supported by Samsung Electronics (IO201208-07822-01) and IITP grants (RS-2022-II220959: Few-Shot Learning of Causal Inference in Vision and Language for Decision Making(50%), RS-2022-II220264: Comprehensive Video Understanding and Generation with Knowledge-based Deep Logic Neural Network (45%), RS-2019-II191906: AI Graduate School Program atPOSTECH (5%)) funded by Ministry of Science and ICT, Korea. We thank reviewers for pointingout the issue of the evaluation protocol and providing insightful discussions.",
  "Z. Du and Q. Wang. Dilated transformer with feature aggregation module for action segmenta-tion. Neural Processing Letters, pages 117, 2022. 6": "W.-C. Fan, Y.-C. Chen, D. Chen, Y. Cheng, L. Yuan, and Y.-C. F. Wang. Frido: Featurepyramid diffusion for complex scene image synthesis. In Proceedings of the AAAI Conferenceon Artificial Intelligence, volume 37, pages 579587, 2023. 3 Y. A. Farha and J. Gall. Ms-tcn: Multi-stage temporal convolutional network for actionsegmentation. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR),pages 35753584, 2019. 1, 2, 6, 7, 17, 19 Y. A. Farha, Q. Ke, B. Schiele, and J. Gall. Long-Term Anticipation of Activities with CycleConsistency. In Proc. German Conference on Pattern Recognition (GCPR). Springer, 2020. 2,3, 7, 9, 10, 17, 20",
  "J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. In Proceedings of the34th International Conference on Neural Information Processing Systems, pages 68406851,2020. 3, 7, 15": "Y. Huang, Y. Sugano, and Y. Sato. Improving action segmentation via graph-based temporalreasoning. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR),pages 1402414034, 2020. 2, 6 K. Ishihara, G. Nakano, and T. Inoshita. Mcfm: Mutual cross fusion module for intermediatefusion-based action segmentation. In 2022 IEEE International Conference on Image Processing(ICIP), pages 17011705. IEEE, 2022. 6 Y. Ishikawa, S. Kasai, Y. Aoki, and H. Kataoka. Alleviating over-segmentation errors bydetecting action boundaries. In Proc. IEEE Winter Conference on Applications of ComputerVision (WACV), pages 23222331, 2021. 1, 2, 6",
  "J. Li, P. Lei, and S. Todorovic. Weakly supervised energy-based learning for action segmentation.In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), October2019. 21": "M. Li, L. Chen, Y. Duan, Z. Hu, J. Feng, J. Zhou, and J. Lu. Bridge-prompt: Towards ordinalaction understanding in instructional videos. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, pages 1988019889, 2022. 6 S.-J. Li, Y. AbuFarha, Y. Liu, M.-M. Cheng, and J. Gall. Ms-tcn++: Multi-stage temporalconvolutional network for action segmentation. IEEE transactions on pattern analysis andmachine intelligence, 2020. 6 X. Li, W. Chu, Y. Wu, W. Yuan, F. Liu, Q. Zhang, F. Li, H. Feng, E. Ding, and J. Wang. Videogen:A reference-guided latent diffusion approach for high definition text-to-video generation. arXivpreprint arXiv:2309.00398, 2023. 3 D. Liu, Q. Li, A.-D. Dinh, T. Jiang, M. Shah, and C. Xu. Diffusion action segmentation. InProceedings of the IEEE/CVF International Conference on Computer Vision, pages 1013910149, 2023. 1, 2, 3, 5, 6, 7, 17, 19, 20, 21",
  "J. Park, D. Kim, S. Huh, and S. Jo. Maximization and restoration: Action segmentation throughdilation passing and temporal reconstruction. Pattern Recognition, 129:108764, 2022. 6": "A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin,N. Gimelshein, L. Antiga, et al. Pytorch: An imperative style, high-performance deep learninglibrary. Advances in neural information processing systems, 32, 2019. 21 C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J.Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal ofmachine learning research, 21(140):167, 2020. 17, 18 A. Richard, H. Kuehne, and J. Gall. Weakly supervised action learning with rnn based fine-to-coarse modeling. In Proc. IEEE Conference on Computer Vision and Pattern Recognition(CVPR), pages 754763, 2017. 7, 21 M. Rohrbach, S. Amin, M. Andriluka, and B. Schiele. A database for fine grained activitydetection of cooking activities. In Proc. IEEE Conference on Computer Vision and PatternRecognition (CVPR), pages 11941201. IEEE, 2012. 2 R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesiswith latent diffusion models. In Proceedings of the IEEE/CVF conference on computer visionand pattern recognition, pages 1068410695, 2022. 3 F. Sener, D. Singhania, and A. Yao. Temporal aggregate representations for long-range videounderstanding. In Proc. European Conference on Computer Vision (ECCV), pages 154171.Springer, 2020. 1, 2, 3, 6, 7, 17, 20",
  "Y. Song and S. Ermon. Generative modeling by estimating gradients of the data distribution.Advances in Neural Information Processing Systems, 32, 2019. 3": "Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole. Score-basedgenerative modeling through stochastic differential equations. In International Conference onLearning Representations, 2020. 3 Y. Souri, Y. A. Farha, F. Despinoy, G. Francesca, and J. Gall. Fifa: Fast inference approximationfor action segmentation. In Pattern Recognition: 43rd DAGM German Conference, DAGMGCPR 2021, Bonn, Germany, September 28October 1, 2021, Proceedings, pages 282296.Springer, 2022. 2 S. Stein and S. J. McKenna. Combining embedded accelerometers with computer vision forrecognizing food preparation activities. In Proceedings of the 2013 ACM international jointconference on Pervasive and ubiquitous computing, pages 729738, 2013. 6, 7, 20 Z. Tong, Y. Song, J. Wang, and L. Wang. Videomae: Masked autoencoders are data-efficientlearners for self-supervised video pre-training. Advances in neural information processingsystems, 35:1007810093, 2022. 2, 5",
  "Z. Zhong, M. Martin, M. Voit, J. Gall, and J. Beyerer. A survey on deep learning techniques foraction anticipation. arXiv preprint arXiv:2309.17257, 2023. 7": "In this appendix, we offer detailed descriptions and additional results, which are omitted in the mainpaper due to the lack of space. We provide a detailed explanation of diffusion models in Sec. A,training and inference algorithms in Sec. B, masking types in Sec. C, encoder and decoder layersin Sec. D, additional experimental results in Sec. E, more information of datasets in Sec. F, moreexperimental details in Sec. G, more qualitative results in Sec. H, and discussions of limitations andbroader impacts in Sec. I.",
  "ADiffusion models": "In this section, we give an in-depth explanation of the diffusion models described in Sec. 3.The forward process adds noise to a data distribution x0 q(x0), following the noise distributionq based on the Markov property. Specifically, the forward process is defined by adding noise at anarbitrary time step s with a variance (s) as follows:",
  "(s)(s).(19)": "With a sufficiently large S and an appropriate variance schedule (s), the noisy data xS followsan isotropic Gaussian distribution. Consequently, if we know the reverse distribution q(xs1|xs),we can sample xS N(0, I) and reverse the process to obtain a sample from q(x0). However,since q(xs1|xs) relies on the entire data distribution, we employ a neural network to approximateq(xs1|xs) as follows:",
  "Figure S2: Encoder & decoder layer": "Figure S1 illustrates five types of masking. In Sec. 5.3, we introduced two types of masking:anticipative masking and random masking. The anticipation mask M A and the random mask M Rare defined for each masking, respectively. Additionally, we adopt the relation mask M S and theboundary mask M B introduced in DiffAct . The relation mask M S is proposed to help the modellearn relational dependencies between actions. It is defined by M S = 1(A0i,a = 1), i {1, 2, ..., T}and a {1, 2, ..., K}, with T representing the number of frames and K denoting the number ofaction classes. Here, a specific action class a is randomly selected for the mask. The boundarymask M B hides visual features near the boundaries based on the soft ground truth B to managethe uncertainty associated with action transitions. Specifically, it is defined by M Bi = 1( B < 0.5),where i {1, 2, ..., T}. During training, a masking type is randomly selected among five: no mask M N, anticipative maskM A, random mask M R, relation mask M S, and boundary mask M B, for each iteration as detailed inSec. 4.4. During inference, the no mask M N and the anticipative mask M A are utilized following theevaluation protocol .",
  "DModel architecture": "We employ a modified version of ASFormer utilized in DiffAct as the baseline model.An encoder g consists of N E number of layers and a decoder h consists of N D number of layersas described in Sec.4.2. Figure S2 illustrates detailed operations of the encoder and the decoderlayers. An encoder layer consists of dilated convolution followed by dilated attention, instancenormalization, and a feed-forward network. Additionally, we add relative position bias B to attentionscores to consider relative position relations among actions . Similarly, a decoder layer comprisesdilated convolution followed by dilated attention, instance normalization, and a feed-forward network.The output embedding E from the encoder is concatenated to the input of the decoder after dilatedconvolutions. Subsequently, they are embedded as queries and keys in the dilated attention operation.We refer the reader to for more details.",
  "We provide additional experimental results, maintaining the same experimental settings as describedin Sec.5.2 unless otherwise specified. The evaluation is conducted on the 50 Salads dataset": "Conditioning features. We investigate different types of conditioning features: masked features F ,output embeddings E from the encoder, and the encoder prediction A. Table S1 shows overall results,where using E is more effective than using other features. The effectiveness of utilizing the encoderin our approach becomes apparent when comparing the first and second rows, where a significantperformance drop is observed in the absence of the encoder. Since mask tokens replace visual tokensbefore going into the encoder, it is crucial for them to learn action relations through the encoder.Comparing the second and third rows, we also observe that utilizing features from intermediate layersyields slightly better results than using encoder predictions for both TAS and LTA. Position embedding. In Table S2, we explore different types of position embeddings: relative positionbias , relative position embedding , and absolute position embedding . Comparing thefirst and second rows, we find that employing relative position bias enhances the overall performancein TAS and LTA. From the second and third rows, we observe that relative position bias is also moreeffective than using learnable relative position embedding. Using absolute position embedding leadsto decreased performance in both TAS and LTA. We find that learnable embeddings often causeoverfitting problems during training. As a result, we adopt relative position bias in our model.",
  "-39.5528.6023.6119.9042.8027.1123.4822.0740.8031.0225.5913.9446.5626.2218.5616.15": "Loss ablations. We conduct ablation studies on the loss functions: boundary loss, smoothing loss,and cross-entropy loss. Table R4 presents the results, demonstrating that the combination of boundingloss and smoothing loss is effective for both TAS and LTA. While the effectiveness of these lossesin TAS is well-documented in previous research , their impact on LTA has been lessexplored. Notably, the smoothing loss leads to significant performance gains in both tasks, indicatingthat smoothed predictions are beneficial. Effects of reconstruction loss Lrecon Masked auto-encoding is a technique used in training NLPmodels like BERT and has recently been adapted to vision models . Inspired by thisapproach, we train our model to reconstruct input video features from the masked tokens as anauxiliary task. Specifically, we employ MLP layers on the encoder embeddings to reconstruct theinput video features and apply mean squared error (MSE) loss Lrecon during training. Table S4 shows the overall results on both TAS and LTA tasks. In TAS, overall performance increases.We conjecture that reconstruction helps the model gain a deeper understanding of the underlying datastructure and temporal dynamics by predicting the missing parts of the input. In LTA, we find thatreconstruction is more effective on relatively short-term anticipation. Since short-term predictions areoften based on more immediate context, there is less uncertainty. As a result, reconstructing maskedfeatures helps the model capture immediate patterns and transitions more accurately. However,for long-term predictions, as the model attempts to predict further into the future, the uncertaintyincreases significantly. Long-term predictions involve more variables and potential changes, makingthem inherently less predictable. This increased uncertainty might cause performance degradation,making reconstruction less effective for action anticipation.",
  "FDatasets": "The 50 Salads dataset consists of 50 videos depicting 25 individuals preparing a salad. With over4 hours of RGB-D video data, the annotations include 17 fine-grained action labels and 3 high-levelactivities. Notably, 50 Salads videos are longer than those in the Breakfast dataset, averaging 20actions per video. The dataset is partitioned into 5 splits for cross-validations, and we report theaverage performance across all splits. The Breakfast dataset is under the license of Creative CommonsAttribution-NonCommercial-ShareAlike 4.0 International License. The Breakfast dataset consists of 1,712 videos featuring 52 individuals preparing breakfast in18 different kitchens. Each video is assigned to one of the 10 activities associated with breakfastpreparation, utilizing 48 fine-grained action labels to define these activities. The average videoduration is approximately 2.3 minutes, encompassing around 6 actions per video. The dataset isdivided into 4 splits for cross-validations, and we report the average performance across all splits.The Breakfast dataset is under the license of Creative Commons Attribution 4.0 International License. The GTEA dataset comprises 28 videos capturing 11 action classes related to cooking activities.Each video includes 20 actions on average, and an average video duration is about a minute. Thedataset is divided into 4 splits for cross-validations, and we report average performance across allsplits. The GTEA dataset is under the license of Creative Commons Attribution 4.0 InternationalLicense.",
  "GExperimental details": "Cross-task generalization. In -(c), we conduct experiments on cross-task evaluations withDiffAct , FUTR , and TempAgg . To evaluate DiffAct on LTA, we limit the input videoframes to F1:T , with zero masks appended to future frame lengths concatenated to the encoderoutput embeddings, adhering to DiffActs masking strategies. Subsequently, the decoder predictsfuture actions based on the observed video frames. To evaluate FUTR on TAS, we utilize the encoderof FUTR as an action segmentation model. Note that we report the performance of TempAgg on bothTAS and LTA as provided in the original paper . Evaluation without using ground-truth prediction length on LTA. Baseline models of CycleCons. and FUTR anticipate future actions and their corresponding durations. The predicteddurations are then directly multiplied by the ground-truth prediction length, T, to generate the finalpredictions. In , we also experimented with using a modified prediction length, rT, instead",
  "of the ground-truth length. Please note that we use a reproduced model for Cycle Cons. and theofficial model checkpoints for FUTR2": "Implementation details. We provide additional implementation details to complement those de-scribed in Sec. 5.2. Table S5 presents the specific hyperparameters used in our experiments foreach dataset. All experiments are conducted on a single NVIDIA RTX-3080 GPU. We implementActFusion using Pytorch and some of the official code repository of DiffAct 3 licensed underan MIT License.",
  "HQualitative results": "We provide additional qualitative results for both successful and failure cases in Fig. S3 and Fig. S4,respectively. Figure S3 demonstrates the promising results on both TAS and LTA, showing theefficacy of joint learning these two tasks. In the failure cases, figure S4a highlights the importance ofaccurate segmentation on anticipation, where inaccurate action segmentation of the observed framesleads to wrong future anticipation. In Fig. S4b, ActFusion fails to detect the take knife action classin action segmentation. However, the model anticipates missing action classes that are not observedin the observed frames. This implies that the model can infer missing actions based on the actionrelations of the observed and unobserved actions.",
  "IDiscussion": "In this paper, we have proposed a unified diffusion model for action segmentation and anticipation,dubbed ActFusion. We have demonstrated the effectiveness of the proposed method through compre-hensive experimental results, showing the bi-directional benefits of joint learning the two tasks. Inthis section, we will discuss the limitations, future work, and broader impact of the proposed method. Limitations and future work. The proposed method shows the state-of-the-art results on TASacross three benchmark datasets. However, the frame-wise accuracy is slightly below comparedto DiffAct . We hypothesize that this discrepancy arises from the different masking strategiesemployed. In DiffAct, masking is applied to the output embeddings of the encoder, ensuring that theencoder always fully observes the visual features from the entire video. However, in our approach,masking is applied before the encoders, allowing for handling both visible and invisible tokens tounify the two tasks effectively. Consequently, the encoder may not always observe the entire visualfeatures, potentially leading to slightly lower accuracy. This issue could potentially be addressed byemploying reconstruction methods for masked features, similar to masked autoencoders . Byfurther training the model to reconstruct the original features from the masked features, the encodercould be empowered to handle the masked features better. We leave this as our future work. In future work, additional activity information could be integrated, particularly focusing on segment-wise action relations . Moreover, exploring weakly-supervised learning approaches could be beneficial for further enhancing the capabilities of our method. Broader impact. To the best of our knowledge, we are the first to integrate action segmentation andanticipation within a unified model. We believe that our work lays the foundation for integratingthese two tasks, offering substantial potential for real-world applications such as robots."
}