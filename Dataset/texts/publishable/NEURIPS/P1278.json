{
  "Abstract": "Generative language models are increasingly used for contract drafting and en-hancement, creating a scenario where competing parties deploy different languagemodels against each other. This introduces not only a game-theory challenge butalso significant concerns related to AI safety and security, as the language modelemployed by the opposing party can be unknown. These competitive interactionscan be seen as adversarial testing grounds, where models are effectively red-teamedto expose vulnerabilities such as generating biased, harmful or legally problem-atic text. Despite the importance of these challenges, the competitive robustnessand safety of these models in adversarial settings remain poorly understood. Inthis small study, we approach this problem by evaluating the performance andvulnerabilities of major open-source language models in head-to-head competi-tions, simulating real-world contract negotiations. We further explore how theseadversarial interactions can reveal potential risks, informing the development ofmore secure and reliable models. Our findings contribute to the growing body ofresearch on AI safety, offering insights into model selection and optimisation incompetitive legal contexts and providing actionable strategies for mitigating risks.",
  "Introduction": "The rise of generative language models has profoundly impacted various domains, with legal appli-cations, such as contract drafting and enhancement, emerging as a significant use case. Corporateentities often deploy specialised models offered by products like CoCounsel (Reuters, 2024), LexisNexis (LexisNexis, 2024) or Harvey AI (AI, 2024), which use either proprietary models or finely-tuned versions of publicly available models, whether closed or open source. Meanwhile, smallerentities or individual practitioners have access to a diverse range of models, including those from theGPT (Brown et al., 2020), Claude (Anthropic, 2024) and Llama (Touvron et al., 2023) families, aswell as fine-tuned legal versions of these models (Cheng et al., 2024) or specialised open-source legalmodels (Colombo et al., 2024). In real-world contract negotiations, it is increasingly likely that one or both parties will use differentgenerative language models to gain a competitive edge. This scenario introduces a multifacetedchallenge: not only must parties consider game theory in selecting a model that maximises favourableoutcomes, but they must also address significant AI safety and security concerns. The interactionbetween competing models can be viewed as a form of adversarial testing, or red teaming, wheremodels expose each other to potential vulnerabilities, such as generating biased, harmful or legallyproblematic content. Despite the critical importance of these issues, the competitive robustness and safety of generativelanguage models in adversarial settings are not well understood. Current benchmarks, such asLawBench (Fei et al., 2023) and the LinksAI benchmark (Linklaters LLP, 2023), primarily assess",
  "intrinsic and isolated model performance, which may not adequately reflect the challenges posed bycompetitive interactions in real-world legal contexts": "In this study, we aim to bridge this gap by evaluating and comparing the performance of majoropen-source language models in simulated contract negotiation scenarios. By pitting models againsteach other and determining the victor through a panel of peer models not involved in the negotiation,we seek to uncover potential risks and vulnerabilities. Our goal is to contribute to the field of AIsafety by providing insights into model selection and optimisation in competitive legal environments,ultimately offering actionable strategies for mitigating risks identified through red teaming.",
  "Experimental Setup": "In this study, we investigate the behaviours of eight generative language models tasked with negotiat-ing contracts on behalf of either a seller or a buyer. The contracts, which are also generated by themodels, pertain to the sale of 100 machines between two fictional companies: West ManufacturingInc. (the buyer) and Square Machines Inc. (the seller). These contracts serve as the foundation forevaluating the negotiation dynamics between the models.",
  "Create a very short legal contract for the sale of goods (100machines) between a buyer (West Manufacturing Inc.)and aseller (Square Machines Inc.).Return only the contract,nothing else": "The generated contract is then passed to the buyers model, which can amend the contract withthe goal of improving the terms for the buyer. Both the seller and buyer are instructed to avoidadding additional clauses to the contract and are restricted to improving existing terms. The buyersamendment process is guided by the following prompt:",
  "Is the following contract better for the buyer or the seller?Respond with exactly one letter.(A): Seller or (B): Buyer": "This consistent judging mechanism allows for a standardised evaluation across all experiments,ensuring comparability and fairness in the results. While we assume the LLM judges capability, werecognise the limitation in relying solely on automated evaluations without human expert input. Incases where there were equal votes for both sides, we did not record a win or loss for either model.",
  "Model Pairings and Matchups": "The experiment involves all pairwise combinations of the eight models, with each model representingeither the buyer or the seller in every possible matchup. Given that each model is paired with everyother model, the study includes 28 unique buyer-seller pairings. Since each matchup results in acontract generation, followed by judgements from six models, this leads to a total of 56 head-to-headmatchups and 336 individual judgements (6 judgements per contract).",
  "Models Used": "The eight models in this study include both general-purpose language models and those specificallydesigned for legal applications. The general-purpose models are Llama-3-8B Meta (2024), Llama-2-7B Touvron et al. (2023), Gemma-7B Team et al. (2024), Phi-3 Abdin et al. (2024) and Falcon-7BAlmazrouei et al. (2023), while the legal-specialised models are LawChat-7B Cheng et al. (2024) andSaul-7B Colombo et al. (2024).",
  "Fairness, Safety and Red Teaming Considerations": "While the primary objective of this experiment is to assess the negotiation capabilities of these models,it also serves as a probe into their safety and fairness features. The lack of specific guardrails beyondthose embedded by the models creators allows for exploration of potential safety risks. Some modelsmay take advantage of others that have more stringent safeguards, which we will discuss furtherin the results section. This setting provides valuable insight into how models handle adversarialinteractions in high-stakes negotiation scenarios without explicit mitigation strategies, aligning withthe red teaming focus of the workshop.",
  "Results and Discussion": "We compare and plot the performance of all eight models acting as both seller and buyer agents in. The results reveal distinct patterns in the negotiation behaviours of general-purpose andspecialised legal models, shedding light on their capabilities and limitations in contract negotiations. General-purpose models such as Llama-3-8B and Llama-2-7B exhibit strong adaptability in theirrespective roles. Llama-3-8B consistently performs well as a buyer agent, while Llama-2-7Bdominates as a seller. This adaptability suggests that general models can perform competitively acrossdifferent legal contexts without domain-specific training. In contrast, specialised legal models likeSaul-7B and LawChat-7B demonstrate more balanced performance across both roles, possibly due totheir fine-tuning on legal-specific datasets. This balance positions them as reliable choices in legalnegotiations where fairness is crucial. Interestingly, models such as Gemma-7B and Mistral-7B provide balanced and consistent outcomes,neither dominating in one role nor showing significant weaknesses. Their robustness makes themstrong candidates for environments where equity between buyer and seller is a priority. However,this also means they lack the decisive advantage seen in models like Llama-3-8B and Llama-2-7B,making them less suitable for scenarios where aggressive negotiation tactics might be preferred. Selecting the appropriate model is critical, as it can significantly influence negotiation outcomes. Forexample, Phi-3 shows a strong bias towards excelling as a seller agent, winning a disproportionatelyhigh number of seller-side judgements. However, its performance as a buyer agent is notably weaker,securing the fewest buyer wins among all models. This role-specific strength implies that models likePhi-3 might be strategically deployed in scenarios where sellers need to optimise their positions, butit would be a poor choice for buyer-side negotiations. The deployment of LLMs in legal negotiations raises questions around fairness and transparency.There is potential for misuse if models are manipulated to create biased or deceptive terms. Futurework should explore strategies to improve model transparency and fairness to mitigate such risks.",
  "Buyer Wins": "Mistral-7B Phi-3 Falcon-7B Llama-3-8B Llama-2-7B Gemma-7B Saul-7B LawChat-7B : Scatter plot depicting the head-to-head competition outcomes between different languagemodels acting as either sellers or buyers in contract negotiations. The x-axis represents the numberof seller wins, and the y-axis represents the number of buyer wins. Each point corresponds to aspecific language model. The Llama models, Llama-3 and Llama-2, show excellent buyer andseller performance, respectively, while models like Gemma and Mistral exhibit a more balancedperformance between roles. allowing us to explore how certain models perform against specific adversaries. Although Llama-3-8Bis the strongest overall buyer agent, it loses in direct negotiations against Saul-7B. This suggests thatmodels with certain guardrails (such as Llama-3-8B) may be outmanoeuvred by more specialisedmodels in certain contexts. For practitioners, this means that blindly selecting the best overall\" model may not always result inoptimal outcomes. For instance, if a buyer agent knows that the seller is using Saul-7B, choosingGemma-7B as the buyer agent may lead to a more favourable outcome than choosing Llama-3-8B.This highlights how model performance can vary significantly depending on the specific opponent,reinforcing the necessity for adaptive and context-aware AI deployment in legal applications. Importantly, no additional safety guardrails were implemented beyond those embedded by themodels creators. This decision was intentional, allowing us to explore how models with genericsafety mechanisms (such as Llama-3-8B) might behave when interacting with models like Saul-7B,which may include built-in legal-specific safety features. The results reveal how models with fewerconstraints can exploit the behaviours of more cautious models, raising critical safety concerns.",
  "Limitations and Future Work": "Our experiment highlights several key challenges in evaluating generative models for legal negotiationtasks. While the results provide insights into the performance and safety of models, further workis required to ensure robust evaluations across a broader range of contract types and legal contexts.Additionally, incorporating explicit safety guardrails and adversarial training may help mitigate therisks identified through this red-teaming exercise. Future research should also explore how modelscan be fine-tuned or augmented with real-time safety mechanisms to avoid generating biased orharmful outcomes, ensuring more trustworthy AI systems. Mistral-7B Phi-3 Falcon-7B Llama-3-8B Llama-2-7B Gemma-7B Saul-7B LawChat-7B",
  "Seller Agent": "0.50-0.33-0.200.200.331.000.67 0.201.000.000.671.00-0.601.00 0.00-0.60-1.00-0.50-0.600.00-0.20 -0.200.670.000.33-0.67-0.500.33 0.330.671.00-0.600.670.200.33 0.201.000.67-0.200.600.33-0.60 0.600.330.500.670.00-0.500.50 0.200.00-0.60-0.20-0.200.330.20 1.0 0.8 0.6 0.4 0.2 0.0 0.2 0.4 0.6 0.8 1.0 Normalized Win Difference (Negative: Buyer, Positive: Seller) : Normalised win differences between pairs of language models acting as buyer and selleragents in simulated contract negotiations. Each cell value is the normalised difference between thenumber of judges favouring the seller agent (positive, blue) and those favouring the buyer agent(negative, orange), centred at zero (equal wins). Another limitation of this experiment is the absence of an \"agreement mechanism\" to ensure boththe buyer and seller models mutually agree on the final contract terms. In the current setup, thebuyer model has the opportunity to make the final amendment, which could theoretically result incontracts heavily favouring the buyer. However, interestingly, the seller models won the majorityof cases (163 wins for sellers compared to 117 wins for buyers), suggesting that this limitation didnot critically impact the overall results. Nonetheless, the absence of a formal agreement step reflectsa potential imbalance in the negotiation process that should be addressed in future work to moreaccurately simulate real-world contract negotiations where both parties must agree to the final terms.Incorporating such mechanisms could also improve the fairness and robustness of model performancein such adversarial simulations. Another notable limitation of this study is the absence of human legal expert evaluation, which wasconstrained by available resources. Also due to resource constraints, each model pairing was testedin a single run. Future studies should increase the number of runs to better capture the variabilityinherent in LLM outputs and incorporate human judges to validate LLM-based evaluations. In future work, we aim to enhance the red teaming focus of this experiment by introducing morerigorous adversarial challenges and probing the models for vulnerabilities that go beyond contractnegotiation outcomes. This could include stress-testing the models with edge-case scenarios designedto exploit potential weaknesses, such as the generation of biased or harmful contract clauses andadversarial inputs aimed at circumventing built-in safety mechanisms. Additionally, implementingmore robust adversarial testing frameworks, where models actively attempt to exploit or manipulateeach others outputs, would provide deeper insights into their safety and reliability under high-stakesconditions. These enhancements will allow for a more comprehensive evaluation of the generativemodels robustness, bias resistance and their ability to maintain fairness and safety in adversariallegal contexts.",
  "Conclusion": "Our study reinforces the importance of red-teaming to uncover model vulnerabilities and providesvaluable guidance for legal practitioners and AI researchers alike. We emphasise the necessity ofcarefully selecting language models based on specific legal contexts and adversaries. The results alsopoint to the broader need for enhanced safety mechanisms, transparency, and fairness in AI systemsto ensure their trustworthy application in legal and other critical domains. Future work should focuson refining model evaluation frameworks, implementing more robust safety guardrails, and exploringfine-tuning strategies to mitigate potential risks in adversarial negotiations. Abdin, M., Jacobs, S. A., Awan, A. A., Aneja, J., Awadallah, A., Awadalla, H., Bach, N., Bahree, A.,Bakhtiari, A., Bao, J., Behl, H., Benhaim, A., Bilenko, M., Bjorck, J., Bubeck, S., Cai, Q., Cai, M.,Mendes, C. C. T., Chen, W., Chaudhary, V., Chen, D., Chen, D., Chen, Y.-C., Chen, Y.-L., Chopra,P., Dai, X., Giorno, A. D., de Rosa, G., Dixon, M., Eldan, R., Fragoso, V., Iter, D., Gao, M., Gao,M., Gao, J., Garg, A., Goswami, A., Gunasekar, S., Haider, E., Hao, J., Hewett, R. J., Huynh, J.,Javaheripi, M., Jin, X., Kauffmann, P., Karampatziakis, N., Kim, D., Khademi, M., Kurilenko,L., Lee, J. R., Lee, Y. T., Li, Y., Li, Y., Liang, C., Liden, L., Liu, C., Liu, M., Liu, W., Lin, E.,Lin, Z., Luo, C., Madan, P., Mazzola, M., Mitra, A., Modi, H., Nguyen, A., Norick, B., Patra, B.,Perez-Becker, D., Portet, T., Pryzant, R., Qin, H., Radmilac, M., Rosset, C., Roy, S., Ruwase, O.,Saarikivi, O., Saied, A., Salim, A., Santacroce, M., Shah, S., Shang, N., Sharma, H., Shukla, S.,Song, X., Tanaka, M., Tupini, A., Wang, X., Wang, L., Wang, C., Wang, Y., Ward, R., Wang, G.,Witte, P., Wu, H., Wyatt, M., Xiao, B., Xu, C., Xu, J., Xu, W., Yadav, S., Yang, F., Yang, J., Yang,Z., Yang, Y., Yu, D., Yuan, L., Zhang, C., Zhang, C., Zhang, J., Zhang, L. L., Zhang, Y., Zhang, Y.,Zhang, Y., and Zhou, X. (2024). Phi-3 technical report: A highly capable language model locallyon your phone.",
  "Reuters, T. (Accessed: 2024). Cocounsel: The genai for professionals": "Team, G., Mesnard, T., Hardin, C., Dadashi, R., Bhupatiraju, S., Pathak, S., Sifre, L., Rivire, M.,Kale, M. S., Love, J., Tafti, P., Hussenot, L., Sessa, P. G., Chowdhery, A., Roberts, A., Barua, A.,Botev, A., Castro-Ros, A., Slone, A., Hliou, A., Tacchetti, A., Bulanova, A., Paterson, A., Tsai,B., Shahriari, B., Lan, C. L., Choquette-Choo, C. A., Crepy, C., Cer, D., Ippolito, D., Reid, D., Buchatskaya, E., Ni, E., Noland, E., Yan, G., Tucker, G., Muraru, G.-C., Rozhdestvenskiy, G.,Michalewski, H., Tenney, I., Grishchenko, I., Austin, J., Keeling, J., Labanowski, J., Lespiau, J.-B.,Stanway, J., Brennan, J., Chen, J., Ferret, J., Chiu, J., Mao-Jones, J., Lee, K., Yu, K., Millican, K.,Sjoesund, L. L., Lee, L., Dixon, L., Reid, M., Mikua, M., Wirth, M., Sharman, M., Chinaev, N.,Thain, N., Bachem, O., Chang, O., Wahltinez, O., Bailey, P., Michel, P., Yotov, P., Chaabouni, R.,Comanescu, R., Jana, R., Anil, R., McIlroy, R., Liu, R., Mullins, R., Smith, S. L., Borgeaud, S.,Girgin, S., Douglas, S., Pandya, S., Shakeri, S., De, S., Klimenko, T., Hennigan, T., Feinberg, V.,Stokowiec, W., hui Chen, Y., Ahmed, Z., Gong, Z., Warkentin, T., Peran, L., Giang, M., Farabet,C., Vinyals, O., Dean, J., Kavukcuoglu, K., Hassabis, D., Ghahramani, Z., Eck, D., Barral, J.,Pereira, F., Collins, E., Joulin, A., Fiedel, N., Senter, E., Andreev, A., and Kenealy, K. (2024).Gemma: Open models based on gemini research and technology. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S.,Bhargava, P., Bhosale, S., Bikel, D., Blecher, L., Ferrer, C. C., Chen, M., Cucurull, G., Esiobu, D.,Fernandes, J., Fu, J., Fu, W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A., Hosseini,S., Hou, R., Inan, H., Kardas, M., Kerkez, V., Khabsa, M., Kloumann, I., Korenev, A., Koura, P. S.,Lachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y., Mao, Y., Martinet, X., Mihaylov, T.,Mishra, P., Molybog, I., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Saladi, K., Schelten, A.,Silva, R., Smith, E. M., Subramanian, R., Tan, X. E., Tang, B., Taylor, R., Williams, A., Kuan,J. X., Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur, M., Narang, S., Rodriguez, A.,Stojnic, R., Edunov, S., and Scialom, T. (2023). Llama 2: Open foundation and fine-tuned chatmodels."
}