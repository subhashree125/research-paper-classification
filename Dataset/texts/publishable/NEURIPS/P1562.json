{
  "Abstract": "Numerous industrial sectors necessitate models capable of providing robust fore-casts across various horizons. Despite the recent strides in crafting specific ar-chitectures for time-series forecasting and developing pre-trained universal mod-els, a comprehensive examination of their capability in accommodating varied-horizon forecasting during inference is still lacking. This paper bridges thisgap through the design and evaluation of the Elastic Time-Series Transformer(ElasTST). The ElasTST model incorporates a non-autoregressive design withplaceholders and structured self-attention masks, warranting future outputs thatare invariant to adjustments in inference horizons. A tunable version of rotaryposition embedding is also integrated into ElasTST to capture time-series-specificperiods and enhance adaptability to different horizons. Additionally, ElasTSTemploys a multi-scale patch design, effectively integrating both fine-grained andcoarse-grained information. During the training phase, ElasTST uses a hori-zon reweighting strategy that approximates the effect of random sampling acrossmultiple horizons with a single fixed horizon setting. Through comprehensiveexperiments and comparisons with state-of-the-art time-series architectures andcontemporary foundation models, we demonstrate the efficacy of ElasTSTs uniquedesign elements. Our findings position ElasTST as a robust solution for thepractical necessity of varied-horizon forecasting. ElasTST is open-sourced at",
  "Introduction": "Time-series forecasting plays a crucial role in diverse industries, where it is essential to provideforecasts over various time horizons, accommodating both short-term and long-term planning re-quirements. This includes predicting COVID-19 cases and fatalities one and four weeks ahead toallocate public health resources , estimating future electricity demand on an hourly, weekly, ormonthly basis to optimize power management , and projecting both immediate and long-termtraffic conditions for efficient road management , among others.",
  "arXiv:2411.01842v1 [cs.LG] 4 Nov 2024": "struggle to handle longer inference horizons once trained for a specific horizon, and may yieldsub-optimal performance when assessed for shorter horizons. These constraints lead to the practicalinconvenience of maintaining distinct model checkpoints for different forecasting horizons requiredby real-world applications. Even though recent studies on pre-training universal time-series foundation models have madesome progress in facilitating varied-horizon forecasting , they primarily concentrate onassessing the overall transfer performance from pre-training datasets to zero-shot scenarios. However,they lack an in-depth investigation into the challenges of generating robust forecasts for differenthorizons. To be specific, TimesFM , a decoder-only Transformer, is capable of arbitrary-horizonforecasting, but this approach could potentially lead to substantial error propagation in long-termforecasting scenarios due to autoregressive decoding. DAM , though free from this issue thanks toa novel output design composing sinusoidal functions, cannot effectively capture abrupt changes intime-series data, thereby limiting its utility in critical domains such as energy and traffic. Moreover,while MOIRAI employs a full-attention encoder-only Transformer architecture and supportsarbitrary-horizon forecasting via a non-autoregressive manner by introducing mask tokens intoforecasting horizons, it remains uncertain how well MOIRAI adapts to different horizons. Forexample, its architecture design does not ensure the horizon-invariant property: the model output fora specific future position should be invariant to arbitrary extensions in forecasting horizons beyondthat. Besides, its performance could drop significantly for moderate context lengths. To address this research gap, we introduce a comprehensive study to explore how to construct a time-series Transformer variant that can yield robust forecasts for varied inference horizons once trained.We name the developed model as Elastic Time-Series Transformer (ElasTST). ElasTST adopts anon-autoregressive design by incorporating placeholders into forecasting horizons, which is inspiredby diffusion Transformers and the success of SORA in video generation. Here we imposestructured self-attention masks, only allowing placeholders to attend to observed time-series patches.This design ensures the aforementioned horizon-invariant property by blocking the informationexchange across placeholders. Additionally, we devise a tunable version of rotary position embedding(RoPE) to capture customized period coefficients for time series and to learn the adaptation tovaried forecasting horizons. Furthermore, we introduce a multi-patch design to balance fine-grainedpatches beneficial to short-term forecasting with coarse-grained patches preferred by long-termforecasting, and use a shared Transformer backbone to handle these multi-scale patches. Alongsidecore model designs, during the training phase, we deploy a horizon reweighting approach thatapproximates the effects of random sampling across multiple training horizons using just one fixedhorizon, eliminating the need for additional sampling efforts. Collectively, these key customizationsfacilitate ElasTST to produce consistent and accurate forecasts across various horizons. Our extensive experiments affirm the effectiveness of ElasTST in varied-horizon forecasting. First,we evaluated ElasTST, trained with a fixed horizon and employing a reweighting scheme, againststate-of-the-art models trained for specific inference horizons. The results demonstrate that ElasTSTdelivers competitive performance without requiring per-horizon tuning. Then, we examined varied-horizon forecasting for these models, and the advantages of ElasTST are much more outstanding,demonstrating remarkable extrapolations to longer horizons while preserving robust results forshorter ones. Moreover, we also compared ElasTST with some pre-trained time-series models, suchas TimesFM and MOIRAI, and found that dataset-specific tuning still offers prominent advantagesover zero-shot inference in challenging datasets, such as Weather and Electricity, and that ElasTSTcan provide more robust performance across different forecasting horizons. At last, we conductedcomprehensive ablation tests to highlight the significance of each unique design element of ElasTST.",
  "Conducting a systematic study on varied-horizon forecasting, a critical requirement acrossvarious domains, yet an underexplored area in time-series research": "Developing a novel Transformer variant, ElasTST, which incorporates structured attentionmasks for horizon-invariance, tunable RoPE for time-series-specific periods, multi-patchrepresentations to balance fine-grained and coarse-grained information, and a horizonreweighting scheme to effectively simulate varied-horizon training. Demonstrating the effectiveness of ElasTST through experiments comparing it with state-of-the-art time-series architectures and some up-to-date foundation models. Our ablation testsfurther reveal the importance of its key design elements.",
  "Related Work": "Traditional Neural Architecture Designs for Time-Series ForecastingThe field of time-seriesforecasting has witnessed a significant evolution of neural architectures, transitioning from earlymulti-layer perceptrons , convolutional , and recurrent networks , to a more recent focuson various Transformer variants . However, the challenge of varied-horizon forecasting remains underexplored in these studies, as these models often require specifictuning to optimize performance for each inference horizon. Additionally, many models, includingPatchTST , iTransformer , and MTST , utilize horizon-specific projection heads, whichinherently complicates the extension of their forecasting horizons. Developing Foundation Models for Time-Series ForecastingInspired by the remarkable suc-cesses in the creation of foundational models in the language and vision domains , thetrend of pre-training universal foundation models has emerged in time-series forecasting research.Notable works in this area include Lag-Llama , DAM , TimesFM , and MOIRAI .These studies employ unique designs to address the challenges posed by varied variate numbers andforecasting horizons when adapting to new scenarios. Lag-Llama, DAM, and TimesFM adoptedthe univariate paradigm to circumvent the difficulties associated with handling different variates. Incontrast, MOIRAI has taken a different approach by flattening multi-variate time series into a singlesequence to facilitate cross-variate learning. While this method has its merits, it is worth noting that itmay introduce efficiency issues when handling a substantial number of variates and long forecastinghorizons. As a result, this paper also adopts the univariate setup to maintain efficiency. When it comesto varied forecasting horizons, Lag-Llama and TimesFM both utilized the decoder-only Transformerand relied on autoregressive decoding to manage arbitrarily long horizons. DAM introduced a noveloutput scheme that comprises numerous sinusoidal basis functions, enabling it to project into arbitraryfuture time points. MOIRAI, on the other hand, used a composite input scheme, combining observedtime-series patches with variable placeholders that indicate forecasting horizons, and built a full-attention encoder-only Transformer on top of this. Interestingly, this non-autoregressive generationparadigm originates from diffusion transformers used in video generation . In this paper,we also embrace this paradigm for generating variable-length time-series. Unlike MOIRAI, whichhas made considerable strides in time-series pre-training using a moderately designed Transformervariant, our focus lies in systematically examining critical architectural enhancements to improverobustness in time-series forecasting across various horizons. We believe that constructing a more ro-bust, resilient, and universal architecture will pave the way for more powerful foundational time-seriesmodels to be pre-trained in the future. Position Encoding in Time-Series TransformersPosition encoding plays a pivotal role in Trans-formers as both self-attention and feed-forward modules lack inherent position awareness. Themajority of existing time-series Transformer variants have roughly adopted absolute position en-coding with minor modifications across different studies. For instance, Informer andPyraformer have combined fixed absolute position embeddings with timestamp embeddingssuch as day, week, hour, minute, etc. Meanwhile, Autoformer and Fedformer have omit-ted absolute position embeddings and relied solely on timestamp embeddings. Other models likeLogTrans and PatchTST have explored learnable position embeddings. However, thechallenge with absolute position embedding is its inability to extrapolate into unseen horizons, posinga significant challenge for varied-horizon forecasting. To address this issue, MOIRAI has utilized arelative position embedding technique, RoPE , which has been broadly adopted in the languagedomain to handle variable-length sequences . In our work, we also adopt RoPE to introducerelative position information into self-attention operations. What we uniquely reveal is that the directapplication of the RoPE configuration from the language domain to time-series forecasting is not ideal.The reason being that the predefined coefficients do not align well with the typical periodic patternsobserved in time-series data. As a solution, we suggest redefining the period range encompassed bythe initial RoPE coefficients and making data-driven adjustments to these coefficients. Input Patches in Time-Series TransformersPatchTST spearheaded the concept of segment-ing time-series data into patches instead of feeding raw time-series values directly into Transformermodels. This straightforward yet effective approach has been widely adopted in subsequent studies,including MTST , TSMixer , HDMixer , and MOIRAI. It noteworthy that MOIRAI hasbeen trained with a diverse range of time-series patches with varying patch sizes. When adapting it to",
  "+Model Outputs": ": Overview of the ElasTST Architecture. ElasTST employs (a) structured attention masksfor placeholders to ensure consistent outputs across varied forecasting horizons. It incorporates (b)tunable RoPE customized to time series periodicities, enhancing its robustness. The architecture alsointegrates a (c) multi-scale patch assembly that merges fine-grained and coarse-grained details forimproved forecasting accuracy. Furthermore, we implement (d) training horizon reweighting schemeduring the training phase, which effectively simulates random sampling of forecasting horizons,reducing the need for additional sampling efforts. a new dataset, practitioners need to search through a range of patch sizes and rely on validation per-formance to select a single patch size. In our work, however, we have demonstrated that segmentingtime series into multiple patch sizes to create multi-scale patch representations is more advantageous.This approach further aids in stabilizing accurate forecasting across various horizons.",
  "Elastic Time-Series Transformers": "In , we present an overview of ElasTST. Different from other encoder-only Transformerarchitectures, ElasTST equipped three core designs to facilitate varied-horizon forecasting: structuredself-attention masks for placeholders, tunable rotary position embedding (TRoPE) with custimizedperiod coefficients, and a multi-scale patch representation learning. Additionally, we utilize a horizonreweighting scheme to achieve the effects of varied-horizon training. NotationsWe define a univariate time series as x1:T = {xt}Tt=1, with xt R indicating thevalue at time index t. The learning objective of a varied-horizon forecasting can be formulated as:max Exp(D),(t,L,T )p(T) log p(xt+1:t+T |xtL+1:t), where p(D) is the data distribution fromwhich time series samples are drawn, and p(T) is the task distribution, from which the timestamp t,look-back window L, and the prediction horizon T are sampled. Model InputsTo accommodate varied forecast horizons, our model combines the historicalcontext series xtL+1:t with placeholders 0 RT through concatenation, forming the input X =Concat(xtL+1:t, 0). This approach allows for flexible adjustment of the input and output dimensionsto suit different forecasting scenarios. We further segment X into non-overlapping patches Xp RNP , where P is the patch length and N = (L+T )",
  "Prepresents the number of patches. Each inputpatch is then transformed into latent space by the encoder H = Enc(Xp), H RND": "Masked Self-AttentionA robust varied-horizon forecasting method should deliver consistentoutputs across different forecasting horizons while maintaining high accuracy on unseen horizons.Existing time series Transformers, however, typically directly adapt techniques from video generationand natural language processing without considering the unique characteristics of time series. Toaddress this deficiency, ElasTST modifies a standard Transformer Encoder with two crucial en-",
  "am,n = f TRoPE(hmW q, m), f TRoPE(hnW k, n) Mm,n,(1)": "where W q, W k RDd denote the linear mappings for the query and key, respectively. A tunableRoPE f TRoPE dynamically adjusts the relative position encoding manner to best suit each dataset,with further details provided in the following subsection. The structured attention mask M,n is setto 0 for patches Xpn consisting solely of placeholders and 1 otherwise, ensuring that tokens attendonly to context-carrying patches. This structured masking, in conjunction with the relative positionencoding, prevents the influence of placeholders on prediction outcomes, thus ensuring consistentoutputs across varied forecasting horizons. Tunable Rotary Position EmbeddingPosition embedding is crucial for the attention mechanism tomaintain accuracy over unseen horizons. To overcome the limitations of absolute position embeddingin extrapolation scenarios, RoPE has been widely adopted in the NLP domain for handling variable-length sequences. It rotates a vector x Rd onto an embedding curve on a sphere in Cd/2,with the rotation parameterized by a base frequency b. The function is defined as f RoPE(x, t)j =(x2j1 + ix2j)eib2(j1)/dt, where j [1, 2, ..., d/2] . Typically in NLP, the base frequency bis set to a constant, such as 10,000. However, due to the unique characteristics of time series data,specific adaptations of RoPE are necessary. In this paper, we propose to use the period coefficientsPj =2",
  "d , thisapproach mirrors the original RoPE setup": "In addition to adjusting the period range, the distinct and varying periodicities inherent in timeseries data necessitate more flexible period coefficients. Therefore, in ElasTST, we consider periodcoefficients P as tunable parameters, optimizing it along with varied datasets and forecasting horizons.This adaptive approach allows for more precise and effective forecasting across diverse conditions.We provide a detailed exploration of this design in Section D.4, and illustrate the optimized periodcoefficients for each dataset in Appendix E.2. Multi-Scale Patch AssemblyTo ensure robust performance across various forecasting horizons,integrating both fine-grained and coarse-grained features from time series data is essential. Differentfrom earlier multi-patch models that utilize separate processing branches for each patch size ,ElasTST features a multi-scale patch design within a shared Transformer backbone, capable of bothparallel and sequential processing. We chose sequential processing for our implementation, keepingthe memory consumption comparable to baselines such as PatchTST. The implications of this designon memory usage are further discussed in Appendix F. Specifically, we define each patch size asp = {p1, . . . , pS}, with each size corresponding to a dedicated MLP encoder f Encpi: Rpi RD anddecoder f Decpi: RD Rpi. The outputs from each size are flattened and then averaged to produce thefinal forecast X. During training, losses under individual patch size are calculated and averaged withthe assembled forecast losses, to enhance accuracy and consistency across different scales. Furtherdetails on the effectiveness of this design is provided in .2. Training Horizon ReweightingTo effectively manage varied forecasting horizons, training modelsacross multiple horizon lengths, rather than using fixed ones, is a practical approach . In thisstudy, we propose to use reweighting scheme for loss computation that simulates this process, withoutthe need for additional sampling efforts. Formally, in the conventional implementation, at eachtraining step s, a forecasting horizon Ts is randomly selected from the range [1, Tmax].1 Then the loss",
  "Experiments": "To validate the effectiveness of ElasTST, we systematically assess its performance across variousforecasting scenarios, benchmarking it against established models. The results, detailed in .1,showcase ElasTSTs adaptability to diverse forecasting horizons. Subsequently, we perform anextensive ablation study in .2 to examine the impact of its key designs.2 DatasetsOur experiments leverage 8 well-recognized datasets, including 4 from the ETT series(ETTh1, ETTh2, ETTm1, ETTm2), and others include Electricity, Exchange, Traffic, and Weather.These datasets cover a wide array of real-world scenarios and are commonly used as benchmarksin the field. Detailed descriptions of each dataset are provided in Appendix C.1. Following thesetup described in , all models use a standard lookback window of 96, except TimesFM andMOIRAI , which utilize extended lookback windows of 512 and 5000, respectively. BaselinesFor our comparative analysis, we select 6 representative forecasting models as baselines:(1) Advanced but non-elastic forecasting models, such as iTransformer , PatchTST , andDLinear ; (2) Autoformer , which supports varied-horizon forecasting but requires horizon-specific tuning; (3) the cutting-edge time series foundation model like TimesFM and MOIRAI, which are pre-trained for general-purpose forecasting across varied horizons. Our analysisprimarily assesses the varied-horizon forecasting capabilities, considering their pre-training on subsetsof the datasets used. ImplementationElasTST is implemented using PyTorch Lightning , with a training regimenof 100 batches per epoch, a batch size of 32, and a total duration of 50 epochs. We use the Adamoptimizer with a learning rate of 0.001, and experiments are conducted on NVIDIA Tesla V100GPUs with CUDA 12.1. To ensure fairness, we conducted an extensive grid search for criticalhyperparameters across all models in this study. The range and specifics of these hyperparametersare documented in Appendix C.2. For parameters not mentioned in the table, we adhered to the bestpractice settings proposed in their respective original papers. For evaluation, we use NormalizedMean Absolute Error (NMAE) and Normalized Root Mean Squared Error (NRMSE) as they arescale-insensitive and widely accepted in recent studies . More details are in Appendix C.3.",
  "Main Results": "Comparing ElasTST with Horizon Reweighting to Neural Architectures Tuned for Specific In-ference HorizonsExperimental results demonstrate that ElasTST consistently delivers exceptionalperformance across all horizons without the need for per-horizon tuning. As evidenced in ,ElasTST outperformed SOTA models on diverse datasets including ETTm1, ETTh1, ETTh2, Traffic,Weather, and Exchange, despite these models undergoing specific horizon-based training and tuning.This clearly demonstrates ElasTSTs inherent robustness and its remarkable capacity to generalizeeffectively across varied forecasting scenarios.",
  "Unless stated otherwise, horizon reweighting scheme is deactivated in ablation study": ": Results (meanstd) on long-term forecasting scenarios with the best in bold and the secondunderlined. Each result contains three independent runs with different seeds. During the trainingphase, ElasTST utilizes a loss reweighting strategy where a single trained model is applied across allinference horizons, where the Hmax is set to 720. Other baseline models undergo horizon-specifictraining and tuning. Additional baseline results are detailed in Appendix D.1.",
  "0.059.000 0.089.0000.076.0000.114.0000.072.0000.106.0010.075.002 0.118.004 0.112.002 0.164.004": "Comparing the Robustness of Different Models for Varied Inference HorizonsThe resultsclearly position ElasTST as the most robust option for deploying a single, well-trained modelacross various inference horizons and application scenarios. As demonstrated in , ElasTSTconsistently maintains strong performance across both seen and unseen horizons, underscoring itsability to navigate beyond trained scopes with consistent accuracy across a wide range of forecasts. In contrast, other models face significant challenges in varied-horizon forecasting. State-of-the-art models like iTransformer and PatchTST excel within their trained horizons but struggle whenextended beyond these limits. Models that require horizon-specific tuning, such as Autoformer, oftenexperience abrupt declines in performance, illustrating that scalability alone is insufficient withouttailored optimization. TimesFM, with its autoregressive nature, shows substantial error propagation inunseen datasets like ETT and Exchange, and increased errors in pre-trained datasets such as Weatheras the horizon extends. While MOIRAI demonstrates strong zero-shot performance on datasetslike ETT and Exchange, we find that on the challenging datasets such as Weather and Electricity,dataset-specific tuning still offers advantages. Furthermore, MOIRAIs performance significantlydiminishes with shorter context lengths, as discussed in their paper . In comparison, ElasTSToperate effectively with much shorter context lengths.",
  "Ablation Study": "Structured Attention MasksThe ablation study confirms that structured attention masks areessential for robust inference across horizons that differ from the training phase. As illustratedin , removing structured masks from ElasTST results in significant performance declines, : Performance of trained once and inference over varying forecasting horizons. Models exceptTimesFM and MOIRAI are trained with a forecasting horizon of 720 and tasked with predictingacross multiple horizons. A vertical red dashed line distinguishes between their seen horizons (96,192, 336, 720) and unseen horizon (1024). We use a dashed line to denote the datasets on whichthe model was pre-trained, e.g., both TimesFM and MOIRAI have leveraged Traffic datasets fortheir pre-training. The ETT encompasses averaged results from datasets ETTh1, ETTh2, ETTm1,and ETTm2. Models lack inherent elasticity use a truncation strategy for shorter forecasts, and thefoundation models use their pre-trained checkpoints and recommended configurations for inference. 244896192336720 1024 Infer hor. 0.20 0.25 0.30 0.35 0.40 NMAE ETT 244896192336720 1024 Infer hor. 0.09 0.10 0.11 0.12",
  ": Ablation study for the structured attention masks, tunable RoPE, and multi-patch assembly.A vertical red dashed line indicates the training horizon": "particularly in the Weather dataset. Furthermore, as demonstrated in (see Appendix D.2),the benefits of structured masks are consistent across all forecasting horizons. This underscores theimportance of the horizon-invariant property for enhancing the stability of time series forecasting, anaspect often overlooked in current research. Tunable Rotary Position EmbeddingExperimental results indicate that tunable RoPE significantlyimproves the models ability to extrapolate. a shows that while other positional embeddingmethods are effective on seen horizons, they falter when applied to horizons extending beyondthe training range. Although the original RoPE excels in NLP tasks, it underperforms in timeseries forecasting. Besides, data-driven adjustments of these coefficients enable far more robustextrapolation. Dynamically tuning RoPE parameters according to the periodic patterns of the datasetproves highly beneficial, especially when inferring over unseen horizons. Furthermore, a range from 1 to 1000 for the period coefficients P is more suitable for time seriesforecasting. As demonstrated in b, using the commonly-used NLP settings with Pmin = 1and Pmax = 10000 does not fully exploit the potential of RoPE in time series forecasting. SettingPmax to 1000 results in better performance. We hypothesize that this is because, unlike textual datawhich benefits from attention over longer contexts, the time series data, especially when segmentedinto patches, benefits from focusing on shorter, more recent intervals. By adjusting the maximumperiod coefficient to a lower value, the model captures a richer spectrum of mid-to-high frequencypatterns, thereby enhancing its effectiveness. Detailed analyses of these findings are available inAppendix D.4. Appendix E includes visualizations demonstrating how different initial ranges impactfrequency components, along with illustrations of the tuned period coefficients for each dataset. Multi-Patch DesignThese experiments demonstrate that multi-patch configurations generallyoutperform single patch sizes across various forecasting horizons. shows that the con-figuration p = {8, 16, 32} consistently achieves the lowest NMAE values, effectively balancingthe capture of short-term dynamics and long-term trends. However, adding larger patches, such as 244896192336720 1024 Infer hor. 0.20 0.25 0.30 0.35 0.40 NMAE ETT 244896192336720 1024 Infer hor. 0.08 0.09 0.10 0.11 0.12",
  ": Performance of patch size selections. Results are averaged across all datasets and traininghorizons of {96, 192, 336, 720}. 8_16_32 represents a multi-patch configuration of p = {8, 16, 32}": "p = {8, 16, 32, 64}, does not consistently improve performance and can sometimes increase theNMAE. This suggests that more complex configurations may not always provide additional benefitsand could even be counterproductive. Moreover, the patch size selection is particularly critical in the varied-horizon forecasting scenarios.As demonstrated in the (see Appendix D.5), various combinations of training and forecastinghorizons exhibit distinct preferences for patch sizes. For instance, when the training forecastinghorizon is 720, during the inference stage, longer forecasting horizons prefer larger patch sizes.Conversely, on shorter training horizons, such as 96 and 192, choosing large patch sizes for longerhorizons can lead to performance collapse. This difference underscores the complexity and necessityof optimal patch size selection in achieving effective elastic forecasting. Detailed results for fourtraining horizons and further analysis are provided in Appendix D.5. The Impact of Training HorizonsFurther experiments validate the effectiveness of our proposedtraining horizon reweighting scheme in enhancing varied-horizon inference. As illustrated in ,reweighting longer horizons simplifies the training process, yielding better outcomes than selectinga fixed horizon and mitigating the uncertainties associated with random sampling. Crucially, thistraining approach is model-agnostic and can be applied to different forecasting training scenarios.These results also highlight the advantages of a flexible forecasting architecture, which allows traininghorizons to be customized to the unique characteristics of each dataset.",
  ": Impact of forecasting horizon selection during the training phase": "We also observe that different datasets have distinct preferences for training horizons. For example, inthe Exchange dataset, the longest training horizon led to worse results compared to a shorter horizonof 96, suggesting risks of overfitting or forecast instability with prolonged horizons. Besides, in theETTh1, employing random sampling for training horizons proved suboptimal. These insights showthat tailoring the training horizon selection strategy to the specific dataset can yield improvements.One potential enhancement could involve dynamically optimizing the horizon reweighting schemealongside model training.",
  "Conclusion": "This study introduces the Elastic Time-Series Transformer (ElasTST), a pioneering model designedto tackle the significant and insufficiently explored challenge of varied-horizon forecasting. ElasTSTintegrates a non-autoregressive framework with innovative elements such as structured self-attentionmasks, tunable Rotary Position Embedding (RoPE), and a versatile multi-scale patch system. Addi-tionally, we implement a training horizon reweighting scheme that simulates random sampling offorecasting horizons, thus eliminating the need for extra sampling efforts. Together, these elementsenable ElasTST to adapt to a wide range of forecasting horizons, delivering reliable and competitiveoutcomes even when facing horizons that were not encountered during the training phase. LimitationsWhile ElasTST demonstrates robust performance across various forecasting tasks,several limitations have been identified that highlight opportunities for future enhancements. First,the current version of ElasTST does not incorporate a pre-training phase, which could significantlyimprove the models initial grasp of time-series dynamics and boost its efficiency during task-specificfine-tuning. Further exploration is needed to ascertain optimal training methodologies that maximizethe architectural benefits of ElasTST. Additionally, while the training horizon reweighting scheme isstraightforward and effective in enhancing performance across different inference horizons, it is notthe optimal solution for all datasets. Moreover, the evaluation of ElasTST is limited to a select numberof datasets, which may not fully represent the broader challenges encountered in more complex ordiverse real-world scenarios. Future WorkIn response to these limitations, our forthcoming research efforts will concentrate ondeveloping and validating pre-training protocols for ElasTST to elevate its foundational performanceand extend its applicability across universal forecasting tasks. We aim to incorporate a reasonabletraining approach that will fine-tune the models ability to seamlessly manage forecasts of varyinglengths, thus bolstering its utility in dynamic real-world environments. Furthermore, by broadeningthe range of datasets used for model evaluations, we intend to rigorously test ElasTSTs effectivenessacross an expanded spectrum of industry-specific challenges. This comprehensive approach willnot only solidify ElasTSTs standing as a cutting-edge solution for time-series forecasting but alsoenhance our understanding of its practical implications and potential in diverse industrial applications. Abdul Fatir Ansari, Lorenzo Stella, Caner Turkmen, Xiyuan Zhang, Pedro Mercado, HuibinShen, Oleksandr Shchur, Syama Sundar Rangapuram, Sebastian Pineda Arango, ShubhamKapoor, et al. 2024.Chronos: Learning the language of time series.arXiv preprintarXiv:2403.07815 (2024). Joaquim Barros, Miguel Araujo, and Rosaldo JF Rossetti. 2015. Short-term real-time trafficprediction methods: A survey. In 2015 International Conference on Models and Technologiesfor Intelligent Transportation Systems. Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr,Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. 2024.Video generation models as world simulators. (2024). Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, ArielHerbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, ScottGray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, IlyaSutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. In NeurIPS.",
  "Mononito Goswami, Konrad Szafer, Arjun Choudhry, Yifu Cai, Shuo Li, and Artur Dubrawski.2024. MOMENT: A Family of Open Time-series Foundation Models. In ICML": "Luis Hernandez, Carlos Baladron, Javier M Aguiar, Beln Carro, Antonio J Sanchez-Esguevillas,Jaime Lloret, and Joaquim Massana. 2014. A survey on electric power demand forecasting:Future trends in smart grids, microgrids and smart buildings. IEEE Communications Surveys &Tutorials (2014). Qihe Huang, Lei Shen, Ruixin Zhang, Jiahuan Cheng, Shouhong Ding, Zhengyang Zhou, andYang Wang. 2024. HDMixer: Hierarchical Dependency with Extendable Patch for MultivariateTime Series Forecasting. In AAAI, Vol. 38. 1260812616. Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang, and XifengYan. 2019. Enhancing the Locality and Breaking the Memory Bottleneck of Transformer onTime Series Forecasting. In NeurIPS. 52445254. Shizhan Liu, Hang Yu, Cong Liao, Jianguo Li, Weiyao Lin, Alex X Liu, and Schahram Dustdar.2021. Pyraformer: Low-complexity pyramidal attention for long-range time series modelingand forecasting. In ICLR. Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, and MingshengLong. 2023. itransformer: Inverted transformers are effective for time series forecasting. arXivpreprint arXiv:2310.06625 (2023).",
  "William Peebles and Saining Xie. 2023. Scalable diffusion models with transformers. In ICCV.41954205": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferablevisual models from natural language supervision. In ICML. Kashif Rasul, Arjun Ashok, Andrew Robert Williams, Arian Khorasani, George Adamopoulos,Rishika Bhagwatkar, Marin Bilo, Hena Ghonia, Nadhir Vincent Hassen, Anderson Schneider,et al. 2023. Lag-llama: Towards foundation models for time series forecasting. arXiv preprintarXiv:2310.08278 (2023).",
  "Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. 2021. Autoformer: DecompositionTransformers with Auto-Correlation for Long-Term Series Forecasting. In NeurIPS. 2241922430": "Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, LouisMartin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, et al. 2023. Effectivelong-context scaling of foundation models. arXiv preprint arXiv:2309.16039 (2023). Jiexia Ye, Weiqi Zhang, Ke Yi, Yongzi Yu, Ziyue Li, Jia Li, and Fugee Tsung. 2024. A Survey ofTime Series Foundation Models: Generalizing Time Series Representation with Large LanguageMode. arXiv preprint arXiv:2405.02358 (2024).",
  "Jiawen Zhang, Shun Zheng, Wei Cao, Jiang Bian, and Jia Li. 2023. Warpformer: A multi-scalemodeling approach for irregular clinical time series. In SIGKDD. 32733285": "Yitian Zhang, Liheng Ma, Soumyasundar Pal, Yingxue Zhang, and Mark Coates. 2024. Multi-resolution time-series transformer for long-term forecasting. In International Conference onArtificial Intelligence and Statistics. 42224230. Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wan-cai Zhang. 2021. Informer: Beyond Efficient Transformer for Long Sequence Time-seriesForecasting. In AAAI. 1110611115.",
  "A.1Details of Rotary Position Embedding": "Rotary position embedding (RoPE) is a method used to encode the position of tokens in the inputsequence for transformer-based models, enhancing the capability to utilize the positional contextof tokens. RoPE uniquely incorporates the geometric property of vectors, transforming them into arotary matrix that interacts with the vector embeddings. Here, we have adopted the formal definition from the original RoPE paper. In the simplest two-dimensional (2D) case, RoPE considers a dimension d = 2, where each position vector is treated inits complex form. The formulation is given by:",
  "BAdditional Related Work on Foundation Models": "We summarize existing time series foundational models in , excluding the LLM-oriented ones. These models typically use standard architecture designs, position encodings, and patchingapproaches, primarily aiming to enhance transferability in zero-shot scenarios. However, theygenerally do not deeply explore the challenges of producing robust forecasts across varied horizons.Our work specifically addresses this gap by improving model design to enhance robustness forvaried-horizon forecasting.",
  "ModelBackboneDec. Scheme.Pos. Emb.Token": "TimeGPT-1 Enc-Dec TransformerARAbs PE-Lag-Llama Decoder-only TransformerARRoPE-Chronos Enc-Dec TransformerARSimplified relative PEQuantizationTimer Decoder-only TransformerARAbs PEPatchingTimesFM Decoder-only TransformerARAbs PEPatching UniTS Transformer EncoderNARLearnable PEPatchingDAM Transformer EncoderNARAbs PEToMETiny Time Mixers TSMixerNAR-PatchingMOIRAI Transformer EncoderNARRoPEPatchingMOMENT Transformer EncoderNARLearnable relative PEPatching datasets encompass a broad range of real-world applications and are frequently used as benchmarksin the field3. Consistent with common practices in long-term forecasting , all modelsare tested under the forecasting horizons T {96, 192, 336, 720}. Except for TimeFM , whichuses a lookback window of 512, a standard lookback window of 96 is employed across all othermodels as .",
  "C.2Implementation Details": "ElasTST is implemented using PyTorch Lightning . Training consists of 100 batches per epoch,capped at 20 epochs, with the NMAE metric used for model checkpointing. We use the Adamoptimizer with a learning rate of 0.001, and experiments are conducted on NVIDIA Tesla V100 GPUswith CUDA 12.1. The code for Transformer Block is adapted from .",
  "Datasets are available at under MIT License.4 MIT License.5 Apache-2.0 license.6 Apache-2.0 license.7 MIT License.8 Apache-2.0 license": "MOIRAI (UNI2TS) 9 : A foundation model for time series forecasting, pretrainedusing a masked encoder-based Transformer on the extensive Large-scale Open Time SeriesArchive (LOTSA) with over 27 billion observations. Hyper-parameter TuningTo ensure fairness, we conducted an extensive grid search for criticalhyperparameters across all models in this study. The range and specifics of these hyperparameters aredocumented in . For parameters not mentioned in the table, we adhered to the best practicesettings proposed in their respective original papers.",
  "C.3Evaluation Metrics": "We employ the Normalized Mean Absolute Error (NMAE) and Normalized Root Mean SquaredError (NRMSE) as our evaluation metrics because they provide a relative measure of error that isindependent of the data scale. Its important to note that some original papers reported metrics priorto re-scaling forecasts to their original magnitude, which can affect metric calculations. In this study,we have carefully ensured that our reproduced results are consistent with those reported in the originalstudies and have applied these unified metrics to enable a comprehensive and fair comparison. Normalized Mean Absolute Error (NMAE)The Normalized Mean Absolute Error (NMAE) is anormalized version of the MAE, which is dimensionless and facilitates the comparability of the errormagnitude across different datasets or scales. The mathematical representation of NMAE is given by:",
  "D.1Comparing ElasTST with More Neural Architectures": ": Results (meanstd) on long-term forecasting scenarios with the best in bold and the secondunderlined. Each result containing three independent runs with different seeds. During the trainingphase, ElasTST utilizes an loss reweighting strategy where a single trained model is applied across allinference horizons, the Hmax is set to 720. Other baseline models undergo horizon-specific trainingand tuning.",
  "D.2Performance Gains Across the Forecasting Horizon": "In , we compare the performance gains of each model design at different points withinthe forecasting window. The benefits of structured masks are consistent across the entire horizon,while the advantages of tunable RoPE and multi-patch designs become more prominent whenhandling unseen horizons. Notably, the tunable RoPE plays a critical role in enhancing the modelsextrapolation capability. Hor. 1.0 1.5 2.0 2.5 3.0 3.5 Model.MAE / ElasTST.MAE",
  "D.3More Analysis of the Impact of Training Horizon": "The scalable architecture of ElasTST allows it to treat the training horizon as a hyperparameter. Thisadaptability prompts us to evaluate performance across various training and inference horizons (see), yielding several interesting insights. Model performance deteriorates as the forecasting horizon increases, particularly in models trainedon shorter horizons, as seen in the ETT and Electricity datasets. This pattern suggests the importanceof training models on extended horizons to capture adequate contextual information. Tuning models specifically for a given horizon does not guarantee improved performance onthat horizon, as noted in the Weather dataset. This indicates that optimal model settings dependsignificantly on dataset-specific characteristics, and horizon-specific tuning may not be a reliablestrategy. The longest training horizons do not always produce the best forecasting results. In the Exchangedataset, for example, the longest horizon yielded poorer results compared to a shorter traininghorizon of 96. This points to the potential risks of overfitting or forecast instability when trainingwith long-term series only. These observations underscore the importance of tailoring training horizons to the unique characteris-tics of each dataset and underscore the benefits of an architecture designed for elastic forecasting.Furthermore, they suggest the potential advantages of implementing a mixed-horizon training strategy,which leverages multiple horizons to produce more resilient forecasts.",
  "D.4More Analysis of the Impact of Tunable Rotary Position Embedding": "Beyond its scalable architecture, the position embedding plays a crucial role in enhancing the elasticityof ElasTST. We analyze the Tunable RoPE in ElasTST by examining the effects of the initializationof period coefficients, specifically Pmin and Pmax, and the benefits of parameter optimization duringthe training process. Experimental results, presented in , indicate that using settings similar to the commonly-usedone in NLP, with Pmin = 1 and Pmax = 10000, does not fully exploit its potential in time seriesforecasting. This discrepancy stems from fundamental differences between the data types: in text,",
  "(c) The effect of tuning , with Pmin set at 1 and Pmax set at 10,000": ": Ablation study for designs in position embedding. Here we analyze the tunable RoPE inElasTST by examining the effects of the initialization of period coefficients, specifically Pmin andPmax, and the benefits of parameter optimization during the training process. Results are averagedacross all datasets. A vertical red dashed line distinguishes between seen horizons and unseenhorizons.",
  "discrete tokens are the smallest units, requiring attention over longer contexts, while time series data,particularly when patched, may benefit from focusing on shorter, more recent tokens": "Furthermore, our findings reveal that tuning parameters in RoPE during training significantly improvesforecasting accuracy, particularly over varying and extended horizons. When the training horizonis set to 96, a tunable feature shows minimal impact, suggesting that a short-seen horizon doesnot facilitate learning effective period coefficients. However, as the training horizon extends, theadvantages of a tunable theta become more pronounced, especially for unseen horizons. These results emphasize the importance of customizing RoPEs period coefficients settings andutilizing tunable RoPE to enable flexible and accurate forecasting in time series analysis. Appendix Eoffers visualizations demonstrating how different initial ranges impact the frequency components,along with a detailed analysis of the distribution of RoPE periods optimized for each dataset.",
  "D.5The Impact of Patch Size Selection": "These experiments highlight the critical impact of patch size selection, particularly in varied-horizonforecasting scenarios. As demonstrated in , various combinations of training and forecastinghorizons exhibit distinct preferences for patch sizes. For instance, when the training forecastinghorizon is 720, during the inference stage, longer forecasting horizons prefer larger patch sizes.Conversely, on shorter training horizons, such as 96 and 192, choosing large patch sizes for longerhorizons can lead to performance collapse. This difference underscores the complexity and necessityof optimal patch size selection in achieving effective elastic forecasting.",
  ": The performance of difference patch size selection": "Moreover, clearly show that multi-patch configurations typically surpass single patchsizes across most forecasting horizons. The configuration p = {8, 16, 32} consistently offers thelowest NMAE values, striking an optimal balance between capturing short-term dynamics and long-term trends. However, introducing larger patches (p = {8, 16, 32, 64}) does not always enhanceperformance and can sometimes increase the NMAE, indicating that overly complex configurationsmay not yield additional benefits and could be counterproductive. These findings emphasize the advantages of employing multi-patch configurations to improve theaccuracy of varied-horizon forecasting. They also highlights the importance of carefully selectingpatch size combinations to optimize performance and minimize computational expenses.",
  "Max GPU Mem. (GB)0.67470.05360.04150.03600.67510.0539NPARAMS (MB)5.01305.02675.04245.07385.12575.1228": "While our model uses a shared Transformer backbone to process all patch sizes, this can be doneeither in parallel or sequentially, depending on whether shorter computation times or lower memoryusage is prioritized. In practice, we chose the sequential approach, where each patch size is processedindividually, and the total forecast is assembled afterward. This approach ensures that the memorybottleneck depends on the smallest patch size used. As indicated in , memory usage is primarily influenced by the smallest patch size, not by thenumber of patch sizes employed. The additional parameters introduced by using multiple patch sizesare almost negligible. For example, the multi-patch setting 8,16,32 used in the paper requires thesame maximum memory as using a single patch size of 8. Under resource constraints, the minimumpatch size can be adjusted to balance model performance and memory usage."
}