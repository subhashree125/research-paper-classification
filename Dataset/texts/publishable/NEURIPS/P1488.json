{
  "Abstract": "Watermarking generative content serves as a vital tool for authentication, ownershipprotection, and mitigation of potential misuse. Existing watermarking methods facethe challenge of balancing robustness and concealment. They empirically inject awatermark that is both invisible and robust and passively achieve concealment bylimiting the strength of the watermark, thus reducing the robustness. In this paper,we propose to explicitly introduce a watermark hiding process to actively achieveconcealment, thus allowing the embedding of stronger watermarks. To be specific,we implant a robust watermark in an intermediate diffusion state and then guide themodel to hide the watermark in the final generated image. We employ an adversarialoptimization algorithm to produce the optimal hiding prompt guiding signal foreach watermark. The prompt embedding is optimized to minimize artifacts in thegenerated image, while the watermark is optimized to achieve maximum strength.The watermark can be verified by reversing the generation process. Experimentson various diffusion models demonstrate the watermark remains verifiable evenunder significant image tampering and shows superior invisibility compared toother state-of-the-art robust watermarking methods.",
  "Introduction": "Diffusion models (DMs) are revolutionizing content creation and generating stunningly realisticimagery across diverse domains . The advent of text-to-image diffusion models , coupled with personalized generation techniques , enables the creation ofhighly specific content by virtually anyone. However, it has raised concerns about authenticity andownership, including the risk of plagiarism and the potential misuse of images of publicfigures . Consequently, governments and businesses are increasingly advocating for robustmechanisms to verify the origins of generative content . Watermarking offers a proactive approach to authenticate the source of generated content. Thistechnique embeds imperceptible secret messages within the generated content. These messages serveas unique identifiers, confirming the images origin while remaining invisible to the human eye. Theyalso need to be robust enough to withstand potential distortions encountered during online sharing. Existing watermarking techniques face a significant challenge in striking a balance between conceal-ment and robustness. Traditional post-processing methods employ an empirical approach toidentify an invisible and robust watermark and embed it within the generated image. They passivelyachieve concealment by limiting the watermark strength, consequently compromising robustness.Conversely, stronger watermarks, while enhancing robustness, can introduce visible artifacts into thegenerated image. Recent advancements in in-processing watermarking for diffusion models expect",
  "arXiv:2411.03862v1 [cs.CV] 6 Nov 2024": "the generative model to learn this balance and directly produce watermarked content. However, thesemethods often require expensive model retraining or can lead to unintended semanticalterations within the generated images . Our ROBIN scheme introduces an explicit watermark hiding process to actively achieve concealment.This approach reduces the invisibility limitation of the watermark itself and thus enables the embed-ding of more robust watermarks. Specifically, we implant a robust watermark within an intermediatediffusion state, and then directionally guide the model to gradually conceal the implanted watermark,thus achieving invisibility in the final generated image. In this way, robust watermarks can be secretlyimplanted in the generated content without model retraining. We focus on the text-to-image diffusion models, which support an additional prompt signal to guidethe generation process. We employ an adversarial optimization algorithm to design an optimal promptguidance signal specifically tailored for each watermark. The prompt embedding is optimized tominimize artifacts in the generated image, and the watermark is optimized to achieve maximumstrength. The optimized watermark and prompt signal are universally applicable to all images.During the generation process, the watermark is implanted within an intermediate state followingthe semantic formation stage. Subsequently, the optimized prompt guidance signal is introducedthroughout the remaining diffusion steps. After image generation, following previous works ,we reverse the diffusion process to the watermark embedding point to verify the existence of thewatermark. This innovative approach offers a promising way to overcome the trade-off betweenwatermark strength and stealth by explicitly introducing an additional watermark hiding process.",
  "xt =tx0 +": "1 t(xt1, t 1).(6)This unique characteristic allows us to selectively mark and recover an inner noise representationwithin the diffusion process, which serves as a powerful tool for our watermarking approach. Watermarking generative models.The content watermark of generative models can be introducedeither after the generation (post-processing) or during the sampling process (in-processing). Post-processing methods can adopt traditional digital image watermarking technology. Popular methodsinclude frequency domain watermarking, which modifies the image representation in domainslike Discrete Wavelet Transform (DWT) or Discrete Cosine Transform (DCT) . DwtDctwatermarking is applied in open sourced model Stable Diffusion. Frequency domain watermarkscan be designed to be robust against common image manipulations like cropping, scaling, and evencompression . HiDDeN pioneered the end-to-end approach, utilizing an encoder-decoderarchitecture to directly generate watermarked images. RivaGAN leverages adversarial trainingto incorporate perturbations and image processing during model training for increased robustness. In-processing methods make the watermark become part of the generated image by interfering with thegeneration process. Early approaches explored adding watermarks to training data ,essentially building a watermark encoder into the model. Stable Signature simplified this processby fine-tuning only the external decoder of latent diffusion models. However, these methods alltreated watermarking as a separate goal from the generation task, limiting their flexibility. The recentTree-Ring watermarking shares similarities with our approach, modifying the initial noise toencode information semantically within the image. However, the semantic modifications inducedby Tree-Ring watermarks are random and may compromise the faithfulness of the original model.Therefore, we aim to preserve the original semantics exactly to guarantee a similar level of textalignment compared to the original generation. Our work shows that embedding the watermark withinthe intermediate diffusion state and guiding the model to hide it can achieve the secret embedding ofstrong watermarks without model retraining.",
  "Overview of ROBIN": "Task definition.Diffusion model watermarking aims to embed an invisible and verifiable watermarkwi within the generated image x0, using a watermark implantation function I. During Internettransmission, the generated content may be subjected to various image transformation operations T .The model owner aims to leverage a watermark extraction algorithm E to verify the presence of wiwithin the distorted sample T (x0), thereby establishing image ownership.",
  "Watermark implantation. ROBIN implants wi into an intermediate generation state xt after thesemantics have been formed as": "xt = I(xt, wi, M),(7)where I injects wi into the frequency domain of xt and M is the coverage area of the watermark.During the remaining DDIM generation, ROBIN incorporates the optimized prompt guidance signalwp to direct the model towards hiding the watermark wi to maintain the similarity between thegenerated image x0 and its unwatermarked counterpart x0. Let tinjection be the watermark injectionpoint, the generation of the watermarked image is as follows:",
  "Semantic formationDetail formation": ": The watermark optimization and implantation of ROBIN. A robust watermark is added atan intermediate state of generation, and an additional prompt guiding signal is optimized to direct themodel towards hiding the embedded watermark in the generated image. The image watermark andguiding signal are optimized adversarially to improve robustness and invisibility. After embedding the watermark, the model is guided by both the original input text prompt p andthe optimized prompt embedding wp to achieve reliable generation with the watermark hidden. Thepredicted noise then becomes",
  "where 1, 2 are the guidance scale parameters to weight the guidance of the original text prompt andthe optimized prompt signal": "Watermark verification. To verify the watermark, we reverse the transformed watermarked imageT (x0) to step tinjection and retrieve the intermediate state xt . The watermark information w =E(xt , M) is extracted from the frequency space of xt . L1 distance D is used to measure thesimilarity between w and w. When the distance falls below a threshold as",
  "Adversarial optimization algorithm": "We employ an adversarial optimization algorithm to generate the watermark and the correspondinghiding prompt guidance signal. The prompt signal is optimized in the embedding space and guidesthe model to conceal the embedded image watermark, while the watermark tries to be as strong aspossible while allowing for its targeted hiding by the prompt signal. The objective of the prompt guiding signal is to minimize the impact of the watermark on the finalgenerated image. We define the image retaining loss lret, which penalizes excessive deviations fromthe original images:",
  "(b) Predicted noise changes under perturbations": ": The impact of introducing frequency domain disturbances at different diffusion steps onthe predicted noise. Timestep 1000 signifies the Gaussian noise state and step 0 represents the finalgenerated image. The Uncondition curve (orange) and the Condition curve (gray) nearly overlap inboth figures. Guidance is the amplified difference of Uncondition and Condition. Full is the additionof Uncondition and Guidance.",
  "cons = MSE((xt , t, wp) (xt , t, ())).(14)": "To achieve robustness, we embed the watermark in the frequency domain of the image . Frequencydomain signals are more resistant to spatial operations compared to spatial domain signals .Similar to , we set the watermark as multiple concentric rings, but we further optimize its valueto the maximum within the aforementioned constraints for greater strength and better robustness. Theoptimization losses of wp and wi become",
  "Finding keypoints for implantation": "The selection of the optimal stage for watermark embedding within the diffusion process is crucialfor achieving both high image fidelity and semantic consistency with the input text prompt. We delveinto the sensitivity of the predicted noise to frequency domain disturbances in different diffusionsteps. According to classifier-free guidance method , the predicted noise in each step can bedepicted as Full = Uncondition + s (Condition Uncondition). Condition and Unconditionare predicted noise with and without text conditions. Parameter s is the scaling factor and the secondterm of the addition is called Guidance. Full noise is the final noise to be removed in the current step. (a) shows the evolution of mean values of various predicted noise terms throughout thegeneration process. We can find that after step 300, the slowdown in guidance rise indicates thecompletion of basic semantic formation and diminishing guidance influence. Additionally, (b)presents how the predicted noise changes when perturbations are added at different timesteps.When the timestep is greater than 200, the frequency domain noise interferes with the generationprocess mainly by disrupting the guidance term. After 200 steps, the intrinsic unconditional termis more affected. We can conclude that early generation stages establish the foundation for imagesemantics and excessive intervention at this point can disrupt the intended image content. Conversely,",
  "Watermark validation": "In the watermark verification phase, we reverse the diffusion process to get the state xt at thewatermark injection step. We extract the M region of xt Fourier space and calculate its L1 distancefrom the implanted watermark wi. However, the original prompt used for image generation isunknown during verification of online images. Similar to , we use the null-text prompt asthe condition text embedding and set the guidance scale to 1.0. We also found unexpectedly thatintroducing the optimized prompt signal during inversion hinders the watermark recovery, which weaim to explore in future work. Our watermark verification requires a reversible generation process,making it compatible with any reversible samplers such as DPM-Solver , DPM-Solver++ ,PNDM , and AMED-Solver .",
  "Experimental setting": "Model and dataset.We conducted experiments on two distinct diffusion models operating inlatent and image domains. For the latent diffusion model, we utilize the widely available StableDiffusion-v2 and the stable-diffusion-prompts dataset from Gustavosta . We also test on aguided diffusion model trained on the ImageNet , which operates directly on the pixel domainand can generate images of size 256 256 based on the category provided. Evaluation metrics.To assess the effectiveness of ROBIN, we compute the Area Under the ROCCurve (AUC-ROC) based on the L1 distance to measure the effectiveness of watermark verification.Specifically, we compute AUC using 1,000 watermarked and 1,000 clean images. For the qualityof watermarked images, we employ a suite of diverse metrics. We utilize classic measures likePSNR (Peak Signal-to-Noise Ratio), SSIM (Structural Similarity Index), and MSSIM (MultiscaleSSIM) to quantify the pixel-level differences between watermarked and original images. Weemploy the Frchet Inception Distance (FID) to evaluate the fidelity of the watermarked imagedistribution. We also leverage the CLIP score to measure the alignment between generatedimages and their corresponding text prompts. More details are provided in Appendix B.1. Implementation details.We utilize 50 steps of deterministic sampling for both models. StableDiffusion employs the second-order multistep DPM-Solver algorithm with a default guidancescale of 7.5. ImageNet diffusion model leverages the DDIM sampling algorithm . We optimizethe watermark and the hiding prompt using 50 generated images. The learning rates for the imagewatermark and prompt guidance are 0.8 and 5e-04, respectively, with a total of 1,000 optimizationrounds. The default image watermark covers 70% of the image frequency domain. All experimentsare conducted on an NVIDIA GeForce RTX 3090 GPU.",
  "Effectiveness and robustness": "We compare our method with five baselines: DwtDct , DwtDctSvd , RivaGAN , StableSignature , and Tree-Ring watermarks . To ensure the watermarks resilience in real-worldscenarios, we delve into its robustness under various image transformations. These include Gaussianblur with radius 4, Gaussian noise with intensity 10%, jpeg compression with quality 25, color jitterwith brightness 6, random rotation of 75 degrees, and random cropping of 75% and rescaling. Thesesettings are strict for watermark verification because the image has been significantly altered. ROBINis also evaluated under a combination of attacks where we randomly selected various combination ofthe six transformations. The processed samples are shown in in the Appendix. : Comparison of AUC under different attacks and verification time for Stable Diffusion .Clean represents the watermark verification on unmanipulated images. Avg is the average AUCacross all attack cases. Time is the time required to validate the watermark for a single image.",
  "Tree-Ring0.9690.8090.6990.5200.5460.509ROBIN0.9730.8140.7590.5790.5580.556": "Robustness.The comprehensive results of AUC comparison with baselines are presented in and . While most methods (except DwtDct) perform well for watermark verification in theabsence of attacks, their accuracy degrades with strong image manipulations. Traditional frequency-domain methods show significant vulnerability. RivaGAN falters with image rotations, and StableSignature exhibits sensitivity to blur, noise, and rotation. The Tree-Ring watermark displays betterrobustness due to its pattern design but remains less resilient than ROBIN. The performance of watermark verification for Stable Diffusion under different numbers of simultane-ous attacks is shown in . Note that due to the inherent potency of the individual attacks, theircombination leads to significant image quality deterioration. The resulting images are presented in. But ROBIN still demonstrates superior robustness compared to the state-of-the-art methodTree-Ring in such challenging scenarios. The robustness of ROBIN on the one hand comes from the introduction of an explicit hiding process,we can implant a stronger watermark. Furthermore, fewer inversion steps during verification comparedto Tree-Ring watermarks also mitigate the accumulation of DDIM inversion errors, further enhancingaccuracy. The evaluation of ROBIN under more attacks is presented in Appendix C.2. Time cost.The time cost of watermark verification associated with different watermarking schemesis presented in the last column of and . The simple DwtDct method demonstrates thefastest performance, achieving a validation time of less than 0.1s. DwtDctSvd exhibits a 4 slowdowncompared to DwtDct, while RivaGAN is 10 slower. StableSig decodes the watermark directlyfrom the image, but it requires fine-tuning the model. The verification of Tree-Ring watermarksnecessitates reversing the entire generation process, resulting in significant time costs. ROBINrequires reversing only a limited number of generation steps, resulting in consumption times of 0.531sand 0.986s for the two models, which are considerably lower compared to the Tree-Ring watermark.More experimental results are presented in Appendix C.1.",
  "Traditional post-hoc watermarking methods introduce subtle visual distortions into the generatedimages. In contrast, the objective of ROBIN aligns with the Tree-Ring in constructing a content": ": Quality of generated images. PSNR, SSIM and MSSIM measure the similarity betweenthe watermarked and unwatermarked images. CLIP evaluates how well the watermarked imagealigns with the user-provided textual description. FID measures the distribution similarity betweenthe watermarked dataset and a random dataset of real images. The subscripts indicate the standarddeviation of five independent experimental runs, each initialized with a different random seed.",
  ": The generated images with Tree-Ring and ROBIN watermarks": "watermark: seamlessly embedding the watermark within the image content without altering itssemantics. Due to this fundamental shift in watermarking philosophy, we only compare the imagequality with Tree-Ring watermarks. The Tree-Ring approach aims to find another watermarked image that aligns with the text prompt,even if it differs from the original image. However, it is more akin to random semantic modificationsand does not guarantee the same level of text alignment as the original generation. shows thatthe Tree-Ring approach significantly alters the generated images semantics, sometimes even failingto fulfill the text prompts intent. This occurs because it disrupts the essential Gaussian characteristicsof the initial noise, hindering the generation process. In contrast, ROBIN excels at preserving theoverall image content and semantic structure, providing a better lower bound for faithfulness bypreserving the original semantics. provides the quantitative results. ROBIN demonstratessignificant improvements in PSNR, SSIM, MSSSIM, and CLIP score, while a slight increase in FIDis observed. This is because the position of the watermark implanted in our scheme is at a laterstage of generation, resulting in a slightly greater influence on the overall generation distribution.This implies a negligible trade-off for achieving a strong watermark with minimal degradation of theoverall quality of the generated image. : Watermark accuracy and image quality under different settings. (1) random watermarkswi, (2) random watermarks with prompt signal wp for hiding, (3)-(5) different loss functions foroptimizing wi and wp, (6) full loss function for optimizing both wi and wp.",
  "Ablation study": "To gain further insights into the effectiveness of ROBIN, we conduct an ablation study, exploringthe influence of different design choices. We additionally introduce the Mean Squared Error ofWatermark (MSE) to represent the verification accuracy in some settings where the AUC is alwaysequal to 1. It is calculated as the mean of L1 distance between the extracted and original watermark. Setting variations.To explore the individual contributions of various components in our scheme,we conduct a series of experiments presented in . Experiments in Settings 1 and 2 demonstratethat the introduction of prompt-based watermark hiding signals improves image quality, as evidencedby a 1.6 increase in PSNR and a 1.44 decrease in FID score compared to Setting 1. Setting 3emphasizes the importance of the ret in controlling watermark strength. Without ret, ROBINprioritizes creating a highly robust watermark, leading to significant image distortion (PSNR: 18.95,SSIM: 0.48). Setting 4 presents that removing cons allows for stronger prompt guidance, but thisresults in increased DDIM inversion loss and a decrease of 0.13 in adversarial AUC. Setting 5prioritizes minimal impact on the generated image by weakening the watermark. This approach leadsto poorer watermark robustness and a decrease of 0.017 in adversarial AUC. Experiments underSettings 2 and 6 demonstrate that in the presence of the hiding prompt signal, the image watermarkcan be optimized to achieve stronger robustness while maintaining invisibility. Point of implantation.We evaluate the impact of implanting the watermark at different stagesin the diffusion process. The results are presented in . Watermark verification accuracyimproves with later implantation due to fewer DDIM inversion steps and reduced information loss.Early implantation, while initially maintaining image quality (low FID), can significantly change theimage content (low SSIM/PSNR) by disrupting semantic formation. Conversely, late implantationmay leave the watermark visible due to insufficient space for hiding, leading to high FID and deviationfrom the original image (low SSIM). This empowers us to pinpoint the optimal embedding stage(steps 15-10) for balancing visual quality and semantic preservation. Watermark strength.We also verify the influence of different watermarking strengths and theresults are shown in . Higher watermark strength (proportional coverage in the frequencydomain) generally benefits verification accuracy, as the watermark becomes more prominent. TheCLIP score and FID remain stable due to strategic embedding and guided hiding. Traditional metrics(SSIM, PSNR) decrease with stronger watermarks due to increased content modification. Thewatermarked images under different strengths are shown in . Compared to Tree-Ring, thequality of generated images with ROBIN watermarks is less sensitive to watermark strength. Morequalitative results are presented in Appendix C.5.",
  "Conclusion & Discussion": "This paper proposes a novel watermarking method for the diffusion model, which embeds a watermarkin the intermediate diffusion state and guides the model to conceal the watermark. By explicitlyintroducing the active hiding process, we can implant stronger watermarks without compromisingimage quality. We believe this method holds promise for expanding the possibilities of reliablewatermarking in diffusion models.",
  ": Generated images under different watermark strengths. The top row is the result of theTree-Ring scheme and the bottom row is the result of ROBIN": "Limitations.The verification of ROBIN watermarks relies on the reversible generation process,future advancements enabling the reversibility of other sampling algorithms would broaden theapplication of our method. Additionally, the inherent information loss during DDIM inversion can bereduced by exploring generative trajectories that can be reversed exactly . Social impact.Our ROBIN scheme, as a watermarking method, can help creators establishownership and discourage unauthorized use. Furthermore, ROBIN watermarks can be implanted in aone-shot manner without retraining the whole model, making it applicable to different diffusion-basedtext-to-image models.",
  "IJ Cox. Digital watermarking and steganography. Morgan Kaufmann google schola, 2:893914,2007": "Ingemar J Cox, Joe Kilian, Tom Leighton, and Talal Shamoon. Secure spread spectrumwatermarking for images, audio and video. In Proceedings of IEEE International Conferenceon Image Processing, volume 3, pages 243246. IEEE, 1996. Yingqian Cui, Jie Ren, Han Xu, Pengfei He, Hui Liu, Lichao Sun, and Jiliang Tang. Diffu-sionshield: A watermark for copyright protection against generative diffusion models. arXivpreprint arXiv:2306.04642, 2023. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scalehierarchical image database. In Proceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 248255, 2009.",
  "Luke Ditria and Tom Drummond. Hey thats mine imperceptible watermarks are preserved indiffusion generated outputs. arXiv preprint arXiv:2308.11123, 2023": "Pierre Fernandez, Guillaume Couairon, Herv Jgou, Matthijs Douze, and Teddy Furon. Thestable signature: Rooting watermarks in latent diffusion models. In IEEE/CVF InternationalConference on Computer Vision, pages 2240922420. IEEE, 2023. Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, andDaniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation usingtextual inversion. In International Conference on Learning Representations. OpenReview.net,2023. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances inNeural Information Processing Systems, 30, 2017.",
  "Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu.Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models. arXiv preprintarXiv:2211.01095, 2022": "Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversionfor editing real images using guided diffusion models. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition, pages 60386047, 2023. KA Navas, Mathews Cheriyan Ajay, M Lekshmi, Tampy S Archana, and M Sasikumar. Dwt-dct-svd based watermarking. In International Conference on Communication Systems Softwareand Middleware and Workshops, pages 271274. IEEE, 2008. Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew,Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editingwith text-guided diffusion models. In International Conference on Machine Learning, volume162, pages 1678416804. PMLR, 2022. Zhihong Pan, Riccardo Gherardi, Xiufeng Xie, and Stephen Huang. Effective real image editingwith accelerated iterative diffusion inversion. In Proceedings of the IEEE/CVF InternationalConference on Computer Vision, pages 1591215921, 2023. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visualmodels from natural language supervision. In International Conference on Machine Learning,pages 87488763. PMLR, 2021.",
  "Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchicaltext-conditional image generation with clip latents. arXiv e-prints, pages arXiv2204, 2022": "Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjrn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition, pages 1068410695, 2022. Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman.Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages2250022510, 2023. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton,Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al.Photorealistic text-to-image diffusion models with deep language understanding. Advances inNeural Information Processing Systems, 35:3647936494, 2022. Shawn Shan, Jenna Cryan, Emily Wenger, Haitao Zheng, Rana Hanocka, and Ben Y Zhao.Glaze: Protecting artists from style mimicry by text-to-image models. In USENIX SecuritySymposium, pages 21872204. USENIX Association, 2023.",
  "Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the datadistribution. Advances in Neural Information Processing Systems, 32, 2019": "Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, andBen Poole. Score-based generative modeling through stochastic differential equations. InInternational Conference on Learning Representations. OpenReview.net, 2021. Matthieu Urvoy, Dalila Goudia, and Florent Autrusseau. Perceptual dft watermarking withimproved detection and robustness to geometrical distortions. IEEE Transactions on InformationForensics and Security, 9(7):11081119, 2014. Thanh Van Le, Hao Phung, Thuan Hoang Nguyen, Quan Dao, Ngoc N Tran, and Anh Tran.Anti-dreambooth: Protecting users from personalized text-to-image synthesis. In Proceedingsof the IEEE/CVF International Conference on Computer Vision, pages 21162127, 2023. Bram Wallace, Akash Gokul, and Nikhil Naik. Edict: Exact diffusion inversion via coupledtransformations. In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 2253222541, 2023. Ruoyu Wang, Yongqi Yang, Zhihao Qian, Ye Zhu, and Yu Wu. Diffusion in diffusion: Cyclicone-way diffusion for text-vision-conditioned generation. In International Conference onLearning Representations, 2024.",
  "Kevin Alex Zhang, Lei Xu, Alfredo Cuesta-Infante, and Kalyan Veeramachaneni. Robustinvisible video watermarking with attention. arXiv preprint arXiv:1909.01285, 2019": "Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-imagediffusion models. In Proceedings of the IEEE/CVF International Conference on ComputerVision, pages 38363847, 2023. Xuandong Zhao, Kexun Zhang, Zihao Su, Saastha Vasan, Ilya Grishchenko, ChristopherKruegel, Giovanni Vigna, Yu-Xiang Wang, and Lei Li. Invisible image watermarks are provablyremovable using generative ai. Aug 2023.",
  "A.1Image watermark design": "To make the watermark less visible and more resistant to alterations, we embed the watermark intothe frequency domain of the selected diffusion latent. Frequency domain watermarks are provento be robust against common manipulations like cropping and compression and resilient againstgeometric distortions, such as scaling and rotation. Draw inspiration from Tree-Ring watermarks,we use a radiating watermark pattern, where the watermark information within each frequencyband holds equal values. This design choice enhances the watermarks robustness against imagerotations. Specifically, after each optimization round of the image watermark, the values within aspecific frequency band are averaged. This averaged pattern is then used for further prompt signaloptimization and another round of adversarial optimization.",
  "A.3Optimization algorithm design": "The details of the adversarial optimization algorithm are presented in Algorithm 1. Initially, the imagewatermark wi is randomly sampled, and guidance wp is set to NULL (representing no text prompt).In each round, we randomly select a generated sample x0 and obtain the noise representation xt atthe watermark embedding point. Then both wi and wp are optimized alternatively in an adversarialmanner. We experimentally set the hyperparameters as 1.0, as 1.0, and as 0.005.",
  "B.1Details about evaluation metric": "Setting for AUC computing.The AUC-ROC (Area Under the ROC Cure) metric is a statisticalmeasure used to evaluate the performance of a binary classification problem, which is watermarked ornot here. The ROC Curve is created by plotting the fraction of true positive results against the fractionof false positive results at various threshold settings. The AUC summarizes the overall performanceacross all possible thresholds. A higher AUC value means the test is more accurate in making thisdistinction. And an AUC of 1.0 represents perfect discrimination. For both our method and Tree-Ringwatermarking, we compare the extracted image watermarks using the L1 distance. For the other threesteganography-based methods, we utilize the Hamming distance between the implanted and decodedbinary sequences, as these methods typically operate on binary data representations. Setting for FID computing.For Stable Diffusion, we generate 5,000 watermarked images andcalculate FID against the MS-COCO-2017 dataset . For the ImageNet Diffusion model, wecalculate FID using 10,000 watermarked images against the ImageNet-1K training dataset . Setting for CLIP computing.For both models, we test 1,000 images using the OpenCLIP-ViTmodel . For Stable Diffusion, we work with the ground-truth text prompts, while for the ImageNetmodel, we construct prompts like \"a photo of x\", where \"x\" is the category of the generated image. About pixel-level metrics.The content watermarking scheme of ROBIN doesnt aim for exactreplication of the original image. Instead, it strives for a visually similar \"alternative generation\"that maintains both image quality and semantic integrity. While traditional watermarking schemesutilized metrics like PSNR/SSIM to assess image distortion introduced by the watermark (treatedas an additional signal), we utilize them in this research as supplementary indicators to reflect thedegree of semantic preservation within the watermarked image. Essentially, the higher the similaritybetween the watermarked and original image, the less semantic impact the watermark has introduced.",
  "C.1Time overhead": "We evaluate the time cost associated with different watermarking schemes. The results are presentedin . Traditional post-processing methods exhibit similar time requirements for watermarkaddition and verification. The simple DwtDct method demonstrates the fastest performance, achievingboth addition and validation times of less than 0.1s. DwtDctSvd exhibits a 3 slowdown comparedto DwtDct, while RivaGAN is 10 slower. Notably, the runtime of these methods is heavilyinfluenced by the input image size. For in-processing watermarking, StableSig directly fine-tunes themodel, incurring no additional time overhead during the generation process. The Tree-Ring methodintroduces minimal impact (0.003s) on generation time by solely modifying the initial random vector.However, verification necessitates reversing the entire generation process, resulting in significanttime consumption (2.6s for Stable Diffusion and 3.9s for Imagenet Diffusion). ROBIN employs aone-shot approach for watermark embedding during the intermediate diffusion stage. The impact",
  "Latent-level1.0000.9990.9400.9990.9740.9270.9940.972Pixel-level (ROBIN)1.0000.9990.9540.9990.9750.9570.9940.983": "on generation time arises from the introduction of additional guidance calculations, resulting in aminimal overhead of 0.068s and 0.113s, which is negligible compared to the generation time of2.614s and 3.479s for the two models. Verification of ROBIN watermarks requires reversing only alimited number of generation steps, resulting in consumption times of 0.531s and 0.986s for the twomodels, which are considerably lower compared to the Tree-Ring watermark.",
  "C.3Application to noise-to-image models": "ROBIN can also be applied to noise-to-image generation models, as it does not rely on the originaltext prompt input. Given that large-scale pretrained diffusion models are typically conditionalgenerative models, we chose to use the unconditional capability of Stable Diffusion to simulate thenoise-to-image generation process for this evaluation. We evaluate ROBIN on the unconditional generation of Stable Diffusion, where the original text isset to NULL (representing no text prompt). In this setup, the image is generated unconditionallybefore the watermark injection point. After that, we still utilize our watermarking hiding promptembedding to guide the generation process and actively erase the watermark. The results in indicate that ROBIN can still function well in noise-to-image generation.",
  "C.4Pixel-level optimization": "In our scheme, the watermark is embedded in the latent space while the loss function is calculatedat the pixel level. We believe that this approach, which combines pixel-level alignment with latentspace optimization, is beneficial for improving robustness. This is because different latent representations can map to similar pixel-level expressions, allowingus to find a latent code that maps to visually the same image but also contains robust watermarks.This provides more opportunities to embed strong and robust watermark signals without introducingnoticeable visual artifacts. The benefits of this optimization method are evident when we actively aimfor concealment, a feature not supported by other watermarking methods. To further validate our approach, we also test a variant of the ROBIN scheme where the loss functionis computed at the latent level rather than the pixel level. The results presented in demonstratethat latent-level alignment slightly decreases the robustness of the watermark, thereby underscoringthe effectiveness of our pixel-level alignment strategy.",
  "W/o WatermarkTree-RingROBIN": "Glowing cracks, elven princess, meditating, lotus pose, blossoming Full samurai ninja armor, spiderman, fantastic details full face Anime as Margot Robbie cute-fine-face, surprised realistic shaded face Oil painting portrait of a young black woman with long hair in a white dress A knitted Capybara wearing stylish sunglasses and dressed in a beanie cap Dark fantasy, evil magician portrait, dark surrealist",
  "The answer NA means that the paper has no limitation while the answer No means thatthe paper has limitations, but those are not discussed in the paper": "The authors are encouraged to create a separate \"Limitations\" section in their paper. The paper should point out any strong assumptions and how robust the results are toviolations of these assumptions (e.g., independence assumptions, noiseless settings,model well-specification, asymptotic approximations only holding locally). The authorsshould reflect on how these assumptions might be violated in practice and what theimplications would be. The authors should reflect on the scope of the claims made, e.g., if the approach wasonly tested on a few datasets or with a few runs. In general, empirical results oftendepend on implicit assumptions, which should be articulated. The authors should reflect on the factors that influence the performance of the approach.For example, a facial recognition algorithm may perform poorly when image resolutionis low or images are taken in low lighting. Or a speech-to-text system might not beused reliably to provide closed captions for online lectures because it fails to handletechnical jargon.",
  "If applicable, the authors should discuss possible limitations of their approach toaddress problems of privacy and fairness": "While the authors might fear that complete honesty about limitations might be used byreviewers as grounds for rejection, a worse outcome might be that reviewers discoverlimitations that arent acknowledged in the paper. The authors should use their bestjudgment and recognize that individual actions in favor of transparency play an impor-tant role in developing norms that preserve the integrity of the community. Reviewerswill be specifically instructed to not penalize honesty concerning limitations.",
  ". Experimental Result Reproducibility": "Question: Does the paper fully disclose all the information needed to reproduce the main ex-perimental results of the paper to the extent that it affects the main claims and/or conclusionsof the paper (regardless of whether the code and data are provided or not)?Answer: [Yes]Justification: The codes will be available at The answer NA means that the paper does not include experiments. If the paper includes experiments, a No answer to this question will not be perceivedwell by the reviewers: Making the paper reproducible is important, regardless ofwhether the code and data are provided or not.",
  "If the contribution is a dataset and/or model, the authors should describe the steps takento make their results reproducible or verifiable": "Depending on the contribution, reproducibility can be accomplished in various ways.For example, if the contribution is a novel architecture, describing the architecture fullymight suffice, or if the contribution is a specific model and empirical evaluation, it maybe necessary to either make it possible for others to replicate the model with the samedataset, or provide access to the model. In general. releasing code and data is oftenone good way to accomplish this, but reproducibility can also be provided via detailedinstructions for how to replicate the results, access to a hosted model (e.g., in the caseof a large language model), releasing of a model checkpoint, or other means that areappropriate to the research performed. While NeurIPS does not require releasing code, the conference does require all submis-sions to provide some reasonable avenue for reproducibility, which may depend on thenature of the contribution. For example(a) If the contribution is primarily a new algorithm, the paper should make it clear howto reproduce that algorithm.",
  "(b) If the contribution is primarily a new model architecture, the paper should describethe architecture clearly and fully": "(c) If the contribution is a new model (e.g., a large language model), then there shouldeither be a way to access this model for reproducing the results or a way to reproducethe model (e.g., with an open-source dataset or instructions for how to constructthe dataset). (d) We recognize that reproducibility may be tricky in some cases, in which caseauthors are welcome to describe the particular way they provide for reproducibility.In the case of closed-source models, it may be that access to the model is limited insome way (e.g., to registered users), but it should be possible for other researchersto have some path to reproducing or verifying the results.",
  "Guidelines:": "The answer NA means that the paper does not use existing assets. The authors should cite the original paper that produced the code package or dataset. The authors should state which version of the asset is used and, if possible, include aURL. The name of the license (e.g., CC-BY 4.0) should be included for each asset. For scraped data from a particular source (e.g., website), the copyright and terms ofservice of that source should be provided. If assets are released, the license, copyright information, and terms of use in thepackage should be provided. For popular datasets, paperswithcode.com/datasetshas curated licenses for some datasets. Their licensing guide can help determine thelicense of a dataset.",
  "According to the NeurIPS Code of Ethics, workers involved in data collection, curation,or other labor should be paid at least the minimum wage in the country of the datacollector": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with HumanSubjectsQuestion: Does the paper describe potential risks incurred by study participants, whethersuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)approvals (or an equivalent approval/review based on the requirements of your country orinstitution) were obtained?Answer: [NA]Justification: The paper does not involve crowdsourcing nor research with human subjects.Guidelines:",
  "The answer NA means that the paper does not involve crowdsourcing nor research withhuman subjects": "Depending on the country in which research is conducted, IRB approval (or equivalent)may be required for any human subjects research. If you obtained IRB approval, youshould clearly state this in the paper. We recognize that the procedures for this may vary significantly between institutionsand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and theguidelines for their institution."
}