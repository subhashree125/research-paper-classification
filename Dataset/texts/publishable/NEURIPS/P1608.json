{
  "Abstract": "Open-weight large language model (LLM) zoos allow users to quickly integratestate-of-the-art models into systems. Despite increasing availability, selectingthe most appropriate model for a given task still largely relies on public bench-mark leaderboards and educated guesses. This can be unsatisfactory for bothinference service providers and end users, where the providers usually prioritizecost efficiency, while the end users usually prioritize model output quality fortheir inference requests. In commercial settings, these two priorities are oftenbrought together in Service Level Agreements (SLA). We present MESS+, anonline stochastic optimization algorithm for energy-optimal model selection froma model zoo, which works on a per-inference-request basis. For a given SLA thatrequires high accuracy, we are up to 2.5 more energy efficient with MESS+ thanwith randomly selecting an LLM from the zoo while maintaining SLA qualityconstraints.",
  "Introduction": "As the number of open-weight large language models (LLMs), such as Llama , Mistral/Mixtral ,and Granite , is increasing rapidly, deep learning infrastructure providers and end users areconfronted with an abundance of models (model zoo) for their language modeling tasks. This leavesmany users questioning what model is best to choose and whether highly regarded benchmark resultsapply to their specific problem . Currently, the best way to approach model selection is educatedguessing. Since working with LLMs can be expensive , minimizing costs is an equally highpriority for end users and inference endpoint operators. This leaves us with a tri-fold problem: End-users primarily care about a correct model output. When inquiring about text information,e.g., by asking questions or requesting language translation, end users are mostly interested inobtaining factually correct and sufficiently clear language output . Additionally, many users areunfamiliar with the technical details of LLMs, making it challenging for them to select the right modelfor the job, i.e., their primary references are domain-specific benchmark rankings . However,there is no intuitive method to compare the complexity of individual requests with benchmark tasks.Thus, we require an automatic method to select the most appropriate LLM for any given request. Inference endpoint providers prioritize low operating costs. Operating infrastructure that can runstate-of-the-art LLMs can be costly. Microsoft has announced it will acquire a stake in the ThreeMile Island nuclear power plant in the United States to satisfy the energy demand of its planneddata center in Pennsylvania, which has two reasons: consistent energy delivery and low energy cost",
  "arXiv:2411.00889v1 [cs.LG] 31 Oct 2024": ". In times of globally rising energy costs, this underpins the necessity of energy-optimal serviceoperations. Currently, inference service providers only allow their users to query specific modelson serverless endpoints or to deploy individual models on dedicated hardware. To further optimizeoperating costs and improve user experience, we require a method that can choose the best model forany given request while minimizing energy consumption. Enterprise use-cases require a consistently high-quality model inference output while keepingcosts in check. Enterprise users unite the requirement for high-quality model outputs and pricesensitivity. Thus, commercial players typically rely on service-level agreements (SLAs) whensourcing services for their own products. This creates a legal basis for holding the model operatorresponsible for delivering high quality and performance. Such SLAs typically come with variouslevels where, in the case of model outputs, the primary quality metric is accuracy. Thus, we require amethod for formalizing and quantifying the SLA requirements.",
  "In summary, we ask the question:": "Can we select appropriate models from the model zoo to ensure energy efficiencywhile satisfying SLAs?In this paper, we address this question by using a stochastic optimization framework to developan optimal control algorithm, which enables end users to query an inference service and automaticallyselect the most appropriate Model with Energy-optimal Service-level GuaranteeS (MESS+).",
  "Our work is related to two major research directions: dynamic inference and inference requestscheduling. A broad overview is provided in": "Dynamic inference. Typically, dynamic inference approaches involve early exit strategies withina single model and require changes to the original model architecture. These changes are usuallyadditional layers that decide whether a request continues to subsequent layers or is evicted .While these approaches can save inference costs, they require additional training of the decision layers.Further, increasing the capabilities of dynamic inference models is difficult as it requires changing themodel architecture and re-training. MESS+ removes the need for altering the architecture of readilyavailable pre-trained models and automatically selects the most appropriate model with regard to agiven SLA for each request. Inference request scheduling. To this end, scheduling work primarily focuses on reducing latencyduring an inference call. Here, the main idea is to group , arrange , or early evict inferencerequests for faster processing. These approaches focus exclusively on scenarios with a single fixedmodel for all incoming requests and a priority for latency. In contrast, MESS+ is a decision-makingmethod to route requests for energy-optimal processing while maintaining a minimum accuracy overtime. As such, our approach contributes to establishing minimum quality guarantees but does notconsider latency directly. However, there is a relationship between latency and energy efficiency sincesmaller models are typically faster to execute an inference call . Since MESS+ routes inferencerequests to different models, it can build on top of existing scheduling techniques. Taken together, we enable dynamic inference across readily available pre-trained LLMs with servicelevel guarantees, while offering compatibility with existing inference load scheduling techniques.MESS+ can reduce the energy consumption of a given task by up to 4.6 compared to a fixedselection of a single model from a model zoo.",
  "We first formulate the optimization problem and introduce typical constraints that come with SLAs": "Inference Cost (Objective). The energy costs for querying a model can vary significantly overtime and depend on the chosen model, i.e., our computational costs are volatile in model zoos. Forinstance, a model zoo can include an LLM with 1B parameters and another with 13B parameters.The smaller LLM requires significantly fewer resources than the larger model. Service-Level Agreement (Constraints). In SLAs, we typically find a minimum service qualityrequirement. We consider a minimum average accuracy over requests, denoted by .1 We denoteeach models accuracy for processing the inference request t as Am(t), and we choose exactly onemodel for any t to identify the most appropriate model. Overall control problem. Taken together, the objective and constraints can be formalized into thefollowing problem of minimizing the average energy consumption per request under performanceconstraints defined in an SLA:",
  "where ym(t) = 1 if model m is chosen and ym(t) = 0 otherwise": "Challenges. Our optimization problem involves an inherent trade-off between model accuracy andenergy consumption, since larger LLMs, and thus more capable ones, typically yield higher accuracywhile consuming more energy at the same time. As such, we see a correlation over time between theobjective and constraints. Further, optimizing the energy costs involves a time average that is hardto estimate a priori as the properties of future requests are generally unknown and heterogeneous.Similarly, Am(t) is only available after querying the model and only if there is a feedback signal onwhether the response is satisfactory or not.",
  "Translation into an Online Decision Making Problem": "To address the aforementioned challenges, we introduce an online decision-making process. Here,the quantities of Em(t) and Am(t) are captured in every request, i.e., for every arriving t, withoutknowledge of future statistics. Methodology. We base our approach on the Lyapunov drift-plus-penalty framework . SinceSLAs are often volume-based, service quality guarantees are typically given for a maximum numberof requests or transactions. Thus, we deviate from and assume a finite T instead of an infinite T.",
  "Q(t + 1) max0, Q(t) + Mm=1 ym(t)Am(t),(2)": "where for t = 1, we initialize Q(1) to either zero or a small positive value. Intuitively, this capturesthe cumulative violations. Hence, we aim to collectively minimize our objective (1a) and the queuelength. We note that the average accuracy over requests needs to be greater than or equal to inthe constraint, while the direction of inequalities in the constraint is opposite in , thus our queueupdate equation in (2) is slightly different from that in .",
  "s.t.Constraints (1c), (1d),(3b)": "where Am(t) is a predictor of the estimated output of Am(t) because it is impossible to know theexact accuracy before the LLM processes the request.2 We will describe the computation of Am(t)in .3. We consider the energy consumption per request to be known, i.e., we measuredthe cost for an inference call before adding a model to the model zoo, which can depend on some 1In practice, should be chosen with a certain safety margin from the SLA requirement such that we do notviolate the SLA even if the average accuracy is slightly below .2Note that, in this work, we assume that the accuracy can be estimated immediately after the LLM generatesits output. Therefore, in (2), we use Am(t) to update the virtual queue length, but we use Am(t), m, to solvethe per-request optimization problem (3). The more realistic scenario where the accuracy is not known evenafter obtaining the LLM output is left for future work. characteristics of the input request such as its length. As SLAs come in various configurations, weintroduce the control parameter V > 0 that caters to different quality requirements. A high value ofV increases the priority of energy efficiency and lowers the need for accuracy. Solution to (3). Due to Constraints (1c), (1d), the problem in (3) can be easily solved using a linearsearch by setting ym(t) = 1 and ym(t) = 0 (for all m = m), for each m {1, 2, . . . , M}, andcomparing the values of the objective (3a). This procedure has a linear complexity of O(M). Usingsimilar proof techniques as those in , we can show constraint satisfaction guarantee, i.e., SLAguarantee, and optimality as T . The detailed proof is left for a future version of this work.",
  "Predicting the Accuracy for Each Request": "We now describe how to obtain the predicted accuracy Am(t), m, t, which is needed to solve (3).To facilitate the description, we write out Am(t) = A(xm,t, at), where xm,t is the parameter vectorof the (small) accuracy estimation model for the m-th LLM used for request t, and at is the t-thinput request. We omit the subscript t in xm,t in the following when it is unnecessary to specify theparameter used for a specific request t. We learn xm through a probabilistic exploration procedure.To explore the model zoo, we query multiple models with the same input request to obtain their actualaccuracies Am(t), allowing us to learn xm from Am(t) over time. More specifically, we samplefrom a distribution Xt Bernoulli(pt), where the exploration probability pt min(1,c3 t) andparameter c > 0 controls the exploration likelihood. The larger c, the more likely it is to do anexploration step. We decay the probability pt over time as the estimation Am(t) improves with eachexploration step.",
  "Get output from model m and its accuracy Am(t);// Virtual queue update14Q(t + 1) max{0, Q(t) + Am(t)};": "Especially for the first set of arriving re-quests, when we do not know how tochoose the optimal LLM for request t, wemust explore each m in the model zoo forthe best-performing model for t. Whiledoing so, we capture the actual model ac-curacy Am(t) of each m, which we useto learn xm. We define the mean squareerror (MSE) objective of accuracy predic-tion for an estimation over all the possibleincoming requests: L(xm) = EatA(xm, at) Am(t)2.(4)If we learn xm using stochastic gradientdescent (SGD), assuming that the distribu-tion of the input request at is IID acrosst, it is easy to prove that the convergenceupper bound is",
  "K,(5)where K is the number of exploration steps and tk is the request index corresponding to the k-thexploration step": "When exploring, we always use the output from the largest model as the final model output as wehave already spent the energy to query the largest model, i.e., we do not use the solution from (3) inthis case. Since we query all the models in an exploration step, it comes at additional energy costs.We can upper bound the average additional energy consumption per exploration step over time by",
  "Experiments": "We demonstrate the effectiveness of MESS+ with state-of-the-art language modeling tasks, includingWMT14 for language translation and CNNDailyMail for text summarization. We constructour model zoo from the TinyLlama 1.1B and Llama-2 13B models, each representing adifferent model class based on its number of model parameters. To measure the performance ofMESS+ in choosing the most appropriate model in the model zoo, we use the BLEU-1 metricto evaluate the language translation task and the ROUGE-1 score for the summarization task.We establish one SLA for each task, where the requirement is to reach an average BLEU score of = 0.52 and ROUGE score of = 0.315 for the translation and summarization tasks, respectively.We provide additional experimental details in Appendix A. Since V and c have to be chosen manually, we run a set of ablation studies to explore the sensitivityof both parameters. Varying the importance of energy efficiency V between [0.01, 100] for both taskswhile complying with their respective SLA shows that the maximum V we can choose to reliablyachieve is V = 0.1. For V > 0.1, we violate the SLA. Interestingly, when looking at values of for c, which controls the exploration likelihood, we observe that with c = 3, A(t) yields thelowest loss over time and thus the best performance. Hence, we choose V = 0.1 and c = 3 for ourexperiments. Further details on selecting V and c are in Appendices B.1 and B.2, respectively. We look at four different scenarios: (I) choosing the smallest model only (TinyLlama-1.1B), (II)choosing the largest model only (Llama-2 13B), (III) choosing a model randomly while satisfying theconstraints, and (IV) applying MESS+. The results are shown in . In the case of (I), we donot satisfy the SLA in either task, while for (II), we observe the highest accuracy and comply withthe SLAs, but the energy consumption is also the highest. Case (III) also complies with the SLAsand reduces the energy consumption by 1.9 and 1.8 for the translation and summarization tasks,respectively, when compared to (II). However, (IV) provides the same average accuracy over time as(III) while reducing the energy consumption by 3.5 and 4.6 for the translation and summarizationtasks, respectively, when compared to (II). Thus, we see that MESS+ strictly ensures SLA complianceand reliably reduces energy consumption compared to randomly choosing an available model thatsatisfies the SLA requirements. Since energy costs are directly proportional to energy usage, theenergy savings brought by MESS+ are directly related to cost reduction. With this, MESS+ can helpreduce the operating costs for model serving.",
  "Conclusions": "We present MESS+, a novel method for automatic model selection in model zoos for languageinference services on a per-request basis. The approach is particularly useful for inference providersthat cater to users with quality-sensitive workloads as it enables service level guarantees. Theexperimental evaluation of MESS+ demonstrates the effectiveness in routing requests to the mostappropriate model while achieving a minimum required accuracy and reducing energy consumptionat the same time. This will lead to overall lower operating costs for inference service providers. In future work, we will simplify the approximation mechanism that yields Am(t) by removing theneed for multiple linear classifiers as this would otherwise establish a new bottleneck with growingmodel zoos.",
  "Acknowledgements": "This work is partially funded by the Bavarian Ministry of Economic Affairs, Regional Developmentand Energy (Grant: DIK0446/01). We would like to thank the PANDORA project ( for our fruitful discussions. Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. Enriching word vectorswith subword information. Transactions of the Association for Computational Linguistics, 5:135146, 2017. ISSN 2307-387X. Ondrej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, JohannesLeveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, et al. Findings of the2014 workshop on statistical machine translation. In Proceedings of the ninth workshop onstatistical machine translation, pages 1258, 2014.",
  "IBM Granite Team. Granite 3.0 language models, Oct. 2024. URL": "Connor Holmes, Masahiro Tanaka, Michael Wyatt, Ammar Ahmad Awan, Jeff Rasley, SamyamRajbhandari, Reza Yazdani Aminabadi, Heyang Qin, Arash Bakhtiari, Lev Kurilenko, and Yux-iong He. Deepspeed-fastgen: High-throughput text generation for llms via mii and deepspeed-inference, 2024. URL Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, ChrisBamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand,Gianna Lengyel, Guillaume Bour, Guillaume Lample, Llio Renard Lavaud, Lucile Saulnier,Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak,Teven Le Scao, Thophile Gervet, Thibaut Lavril, Thomas Wang, Timothe Lacroix, andWilliam El Sayed. Mixtral of experts, 2024. URL Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. Bag of tricks forefficient text classification. In Proceedings of the 15th Conference of the European Chapterof the Association for Computational Linguistics: Volume 2, Short Papers, pages 427431.Association for Computational Linguistics, April 2017. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu,Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large languagemodel serving with pagedattention. In Proceedings of the 29th Symposium on OperatingSystems Principles, SOSP 23, page 611626, New York, NY, USA, 2023. Association forComputing Machinery. ISBN 9798400702297. doi: 10.1145/3600006.3613165. URL",
  "Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarizationbranches out, pages 7481, 2004": "Weijie Liu, Peng Zhou, Zhiruo Wang, Zhe Zhao, Haotang Deng, and Qi Ju. FastBERT: aself-distilling BERT with adaptive inference time. In Dan Jurafsky, Joyce Chai, Natalie Schluter,and Joel Tetreault, editors, Proceedings of the 58th Annual Meeting of the Association forComputational Linguistics, pages 60356044, Online, July 2020. Association for ComputationalLinguistics. doi: 10.18653/v1/2020.acl-main.537. URL",
  "Reuters.Microsoftdealpropelsthreemileislandrestart,withkeypermitsstillneeded. 24-09-2024]": "Siddharth Samsi, Dan Zhao, Joseph McDonald, Baolin Li, Adam Michaleas, Michael Jones,William Bergeron, Jeremy Kepner, Devesh Tiwari, and Vijay Gadepally. From words towatts: Benchmarking the energy costs of large language model inference. In 2023 IEEEHigh Performance Extreme Computing Conference (HPEC), pages 19, 2023. doi: 10.1109/HPEC58863.2023.10363447. Abigail See, Peter J. Liu, and Christopher D. Manning. Get to the point: Summarization withpointer-generator networks. In Proceedings of the 55th Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers), pages 10731083, Vancouver, Canada,July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1099. URL Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Openfoundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. Alexander Wettig, Aatmik Gupta, Saumya Malik, and Danqi Chen. QuRating: Selectinghigh-quality data for training language models. In Ruslan Salakhutdinov, Zico Kolter, Kather-ine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp, editors,Proceedings of the 41st International Conference on Machine Learning, volume 235 of Pro-ceedings of Machine Learning Research, pages 5291552971. PMLR, 2127 Jul 2024. URL",
  "Bingyang Wu, Yinmin Zhong, Zili Zhang, Shengyu Liu, Fangyue Liu, Yuanhang Sun, GangHuang, Xuanzhe Liu, and Xin Jin. Fast distributed inference serving for large language models,2023. URL": "Ji Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, and Jimmy Lin. DeeBERT: Dynamic earlyexiting for accelerating BERT inference. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and JoelTetreault, editors, Proceedings of the 58th Annual Meeting of the Association for ComputationalLinguistics, pages 22462251, Online, July 2020. Association for Computational Linguistics.doi: 10.18653/v1/2020.acl-main.204. URL",
  "AAdditional Experimental Details": "Benchmark datasets. We use the WMT14 dataset to evaluate how well our approach workswith language translation and the CNNDailyMail dataset for sequence-to-sequence modeling,specifically highlighting summarization from news articles. Metrics. Since the users sending inference requests are primarily interested in retrieving high-qualityresults, we use the BLEU-1 metric to evaluate the language translation task and the ROUGE-1score for the sequence-to-sequence task. To account for the service providers cost-savingpriority, we measure the energy consumption in total joules consumed. Inference models. We populate M with TinyLlama 1.1B and Llama-2 13B . These modelshave shown strong performance across language modeling tasks and represent two different modelclasses: small and large. MESS+ accuracy predictor. In our experiment, we use one linear model per inference model builtwith the fasttext library to learn xm . We query each linear model for every request t toestimate the corresponding inference model performance. Application scenarios. To demonstrate the sensitivity of control parameter V in MESS+, we exploredifferent service levels as they would be found in commercial services. For the WMT14 translationtask, users can choose from three different plans: silver, gold, and platinum, where the target accuracyis 0.49, 0.5, and 0.51, respectively. To discuss the energy efficiency implications, we aim to achieve = 0.52 for WMT14 and = 0.315 for CNNDailyMail. Both scores are considered high .",
  "B.1Varying the Priority for Energy Efficiency": "The parameter V serves as a tuning knob that adjusts the trade-off between prioritizing accuracyand minimizing energy usage. By systematically adjusting V , we observe how the system behaviorchanges in response to the three service level plans. The results are shown in . As mentioned above, we introduce three different SLAs that each require a different . Between thethree values, we vary the accuracy by exactly 1% to understand how much more energy we have touse in order to serve the next higher SLA. For the silver level, where the minimum accuracy requirement is low, the system is less sensitive tovariations of V . As we increase V from 0.01 to 100, the average BLEU score remains consistentlyabove the silver threshold. In such a scenario, it is beneficial to set V to a high value as this prioritizesenergy efficiency, which will route more requests to the smaller and faster model. For the gold level, the sensitivity to V becomes more notable. if V is set too high (e.g. V > 5, thesystem excessively prioritizes energy savings, leading to a noticeable drop in the average BLEU scorebelow the acceptable service requirement. At the platinum level, the sensitivity of V increases further compared to gold since the qualityrequirements have increased as well. However, the viable configurations must now emphasize modelaccuracy, which leads to V < 1. Here, a 1% performance gain costs 1.9 more energy than the goldplan. Analyzing V reveals that its optimal value is inversely related to the minimum service requirement. As increases, the range of acceptable V narrows, and the system demands a greater emphasison accuracy rather than cost reduction. Service providers can leverage this knowledge to adjust Vdynamically based on operational priorities, such as reducing energy costs during non-peak hoursand maximizing accuracy during critical periods.",
  "With parameter c, we control the exploration likelihood over time. The higher we set c, the higher thelikelihood of exploring the model zoo, since pt = min(1,c3": "t), and the number of training steps Kbecomes larger. However, we decay pt over time such that we increasingly rely on the model withparameter xm,t to predict the accuracies of different models for each request t. We explore the effects of varying levels of c ranging from on the loss of the accuracy predictors,defined in (4), for different models. This is an indicator of how well Am(t) approximates Am(t).The results are shown in . We find that balancing the level of exploration is important since c = 1 leads to less training; therefore,the training samples to learn xm are significantly fewer than with c = 10, for instance. A large cimplies more training of xm but can also lead to overfitting on the incoming requests. While theconvergence speed is similar for all values of c, we find that c = 3 leads to the lowest loss. Therefore,our systematic parameter search for c yields the best accuracy prediction performance with c = 3.",
  "We need between 200 and 250 exploration steps for the classifier to fully converge. We also see thatover time, each classifier overfits the data, which explains the need for decreasing pt over time": "The consequence of increased latency is observed as we increase c. As shown in , the averagetime per inference request increases as c increases. While MESS+ aims to establish minimum serviceguarantees and optimize energy consumption, it is important to consider the trade-off with latency.When c = 10, multiple models are queried more often compared to c = 1. The additional processingintroduces greater latency, as querying all M and training xm requires more time and resources.Increased latency can negatively impact user experience, especially in real-time services whereprompt response time is crucial."
}