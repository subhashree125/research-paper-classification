{
  "Abstract": "Model-based reinforcement learning (MBRL) is a promising route to sample-efficient policy optimization. However, a known vulnerability of reconstruction-based MBRL consists of scenarios in which detailed aspects of the world arehighly predictable, but irrelevant to learning a good policy. Such scenarios canlead the model to exhaust its capacity on meaningless content, at the cost of ne-glecting important environment dynamics. While existing approaches attempt tosolve this problem, we highlight its continuing impact on leading MBRL meth-ods including DreamerV3 and DreamerPro with a novel environment wherebackground distractions are intricate, predictable, and useless for planning futureactions. To address this challenge we develop a method for focusing the capacity ofthe world model through synergy of a pretrained segmentation model, a task-awarereconstruction loss, and adversarial learning. Our method outperforms a variety ofother approaches designed to reduce the impact of distractors, and is an advancetowards robust model-based reinforcement learning.",
  "Introduction": "Model-based reinforcement learning (MBRL) is a promising path to data-efficient policy learning,and recent advances show impressive performance with high dimensional sensory data [Hafner et al.,2023]. A central component of MBRL is a world model, which is trained to predict how an agentsactions impact future world states. However, the world is highly complex while the capacity of a worldmodel is finite, and ultimately only a subset of the components and dynamics of the environment canbe accurately modeled. In this setting, distracting stimuli can be particularly problematic, as theywaste the capacity of the world model on useless details. To address the challenge of distractors, a number of MBRL methods seek to isolate the most importantcomponents of an environment, including structural regularizations [Deng et al., 2022, Fu et al., 2021,Wang et al., 2022], pretraining the agents visual encoder [Seo et al., 2022, Wu et al., 2023], andvalue-equivalent world modeling [Schrittwieser et al., 2020], while environments such as DistractingControl Suite have been developed to assess distractor suppression [Stone et al., 2021]. In this paper we introduce a new method, Policy-Shaped Prediction (PSP), for identifying and focusingon the important parts of an image-based environment. Rather than relying on pre-imposed structuralregularizations, PSP learns to prioritize information that is important to the policy. We synergizetask-informed gradient-based loss weighting, use of a pre-trained segmentation model [Kirillovet al., 2023] and adversarial learning to create a distraction-suppressing agent that outperformsleading image-based MBRL agents. In addition to exhibiting similar performance in distraction-freesettings and on a standard benchmark of robustness to distractions, our method markedly improvesperformance in the face of particularly challenging distractors that are intricate but entirely learnable.Because learnable distractors can be accurately modeled, they straightforwardly contribute to reducingthe world models error, but needlessly exhaust the capacity of the world model.",
  "Policy-Shaped Prediction": "We introduce PSP, a method to reduce an agents sensitivity to useless distractions by focusing onsensory stimuli that are most relevant to its policy, rather than seeking to model everything in theenvironment. Our guiding intuition was that we can use the gradient from the policy to the inputimage to identify important pixels in the environment, and that we can aggregate these pixelwisesalience signals to identify important objects by using image segmentation. Specifically, we extendthe principles of VaGraM [Voelcker et al., 2022] to a high-dimensional vision model by usingexplainability-related notions of salience and aggregating an otherwise noisy gradient-based saliencesignal within objects. Additionally, inspired by the biological concept of efference copies [Crapseand Sommer, 2008], which are neural signals used to cancel out sensory consequences of an animalsactions, we incorporated a way to explicitly mitigate distractions caused by actions of the agent itself. PSP employs (1) gradients of the policy with respect to image inputs to identify task-relevant elementsof the image, (2) a segmentation model to aggregate gradients within each object in the image, and(3) an adversarial objective to the image encoder of the world model that discourages encodingof duplicate information about the previous action. illustrates the training modificationsmade by this method to the underlying DreamerV3 [Hafner et al., 2023] architecture. Notably, sincethese modifications only affect the training stage of the world model, the DreamerV3 agent remainsunaltered during inference. Below, we describe each of the three key components in detail. : Policy-Shaped Prediction in an environment with challenging distractions. (left) Trainingof an otherwise-unaltered DreamerV3 agent is modified in two ways: 1) A head is added to predictthe previous action based on the image encoding, and the gradient of the head is subtracted from thegradient of the image encoder, and 2) the loss is scaled pixelwise by a policy-shaped loss weight.(right) The loss weight uses the gradient of the policy to the input pixels. The image is segmented,and the pixel weights are averaged within each segmented object. Dashed lines signify gradient flow.",
  "Task-informed image reconstruction with policy-gradient weighting": "Our approach builds upon the core idea that signals most important to the actor and/or critic shouldbe given special importance in the world model. The concept of using the critic to inform model losswas applied in Value-Gradient weighted Model loss (VaGraM) [Voelcker et al., 2022], which weightsthe model loss according to the gradient of the value function with respect to the state. We extend thisconcept to high-dimensional image inputs, which previous work did not demonstrate. This extensionto the image domain is inspired by gradient-based interpretability methods such as saliency maps[Simonyan et al., 2013, Shrikumar et al., 2017, Ancona et al., 2019]. By upweighting the reconstruction loss for parts of the image that inform value estimation, we mightexpect to improve the performance of a downstream policy that aims to maximize value. Going onestep further, we propose using the gradient of the policy for weighting the model loss. While VaGraMfocused solely on the value function, we hypothesize that the gradient of the policy may provide aneven more informative signal because ultimately, the state representation must support effectiveaction selection. We hypothesize that the set of signals informing action selection may be richer thanthose that inform value estimation, which might rely primarily on simple cues such as whether anagent has flipped over. In contrast, the signals needed to select actions can be more subtle, such asthe distance of an agents leg from the platform it pushes off of in order to run.",
  "Object-based aggregation of gradient weights": "Gradient-based weighting of the world models reconstruction ultimately is a form of applyingmodel explainability methods, which attempt to highlight the most important elements of a modelsinput for its outputs. From model explainability literature, a known challenge with gradient-basedweighting is its noisiness, which is likely caused by the presence of sharp but meaningless fluctuationsin the derivative at small scales [Smilkov et al., 2017]. While this has been combated by morecomputationally demanding explainability approaches such as Integrated Gradients [Sundararajanet al., 2017] and SmoothGrad [Smilkov et al., 2017], we found these to be infeasible to run withinthe train loop, since this would require taking the derivative of the function with respect to multiplevarying inputs for every example in the original input batch. Instead, to combat this problem weintroduce a second novel contribution: object-based aggregation of an explainability signal using asegmentation model (SEG). In principle many models should work, but to ensure we had high qualitysegmentations, we used the Segment Anything Model (SAM) [Kirillov et al., 2023], a pre-trained andbroadly applicable segmentation model. Nothing prevents a different model from being utilized, solong as it is of sufficient quality. This idea follows from the observation that the saliency map tendsto show a broad, if noisy, correlation with relevant regions in the image (e.g. ). During datacollection, we segment each image into object masks using the existing method laid out by the authorsof SAM. We prompt the model with a grid of 256 points, filter the resulting masks with metrics forthe SAM algorithm (Intersection-Over-Union and a \"stability score\"), followed by non-maximumsuppression. We also include a mask to capture pixels that are not otherwise assigned to an object.Then, instead of weighting the image reconstruction loss with the raw gradient-based salience, weuse a salience score that is aggregated within each segmentation mask. The weight of a pixel xi thathas been assigned to segment SEG(xi) is the mean absolute value weight of the pixels in SEG(xi).",
  "jSEG(xi)|a/xj|(2)": "To ignore any exploding gradients, we clip the raw salience map to the 99th percentile beforeaggregation. Gradients are sometimes near zero in the beginning of training, and in the rare case thatall gradients are zero, we set Wi = 1 for all i, As a regularizer, we linearly interpolate between thesalience weighting and a uniform weighting, with = 0.9 for all our experiments, and using rescaledW i = width height Wi/",
  "i Wi to match the scale of the uniform background": "W i = W i + (1 )(3)This regularizer lets the world model maintain reasonable reconstruction of less-salient aspectsof the environment, which the actor-critic function can use as it learns. Without some degree ofreconstruction of less-salient regions, the model can become trapped in local minima with a bad policyand world model. Also, at the start of training, gradients from actor-critic functions are essentiallyrandom and often small. A uniform background offers a reasonable prior to begin the train loop.",
  "Adversarial action prediction head": "We additionally sought to explicitly reduce the distracting sensory impact of an agents own actions.As animals move, they experience sensory signals generated by their actions and the externalenvironment, and they have evolved the ability to distinguish these signals using efference copies[Crapse and Sommer, 2008]. We hypothesized that we could separate information about an agentsactions and its encoding of external stimuli through domain-adversarial training [Ganin et al., 2016].To this end, we introduce an adversarial action prediction head that prevents the model from wastingcapacity on irrelevant stimuli that are created by the agents own actions. The DreamerV3 world model consists of three main components: a convolutional neural network(CNN) image encoder zt q(zt|ht, et) with et = CNN(xt), which processes the input image,serves as a prior during training, and encodes the environment state during inference; a recurrentstate space machine (RSSM) consisting of ht = f(ht1, zt1, at1) and zt p(zt|ht) thatis trained to simulate the progression of latent states given actions; and an image decoder, xt p(xt|ht, zt) which reconstructs the image from the latent state. Problematically, the encoder cancapture information about previous actions from the image, despite this information already beingprovided directly to the RSSM through the action input. In other words, zt may source informationabout at1 directly through xt, despite at1 being an argument to f during the computation ofht. Unfortunately, our reconstruction loss weighting may not solve this problem, since duringbackpropagation from the actor-critic functions, we do not distinguish information about previousactions that comes from the image versus the action input to the RSSM. To prevent the CNN encoder from wasting capacity on encoding duplicate information about anagents actions, we add a small multilayer perceptron (MLP) head that is optimized to predict theprevious action from the image embedding.",
  "LAdvHead(at1, at1) = (at1 at1)2(5)": "When updating during world model training, we subtract the scaled gradient L(at1, at1)from the overall world model gradient, with = 1e3. This forces the latent states previous actioninformation to come solely from the provided action vector. Our training procedure for a DreamerV3 agent is shown in Algorithm 1. We note that it should bepossible to apply these concepts of gradient-based weighting, segmentation-based aggregation, andadversarial action prediction to world models other than our chosen DreamerV3 architecture.",
  "Experimental details": "BaselinesWe test four Model-Based RL approaches as baselines: DreamerV3 [Hafner et al., 2023],and three methods specifically designed to handle distractions Task Informed Abstractions [Fu et al.,2021], Denoised MDP (method in their b) [Wang et al., 2022], and DreamerPro [Deng et al.,2022]. Additionally, we choose DrQv2 [Yarats et al., 2021a] as a representative baseline Model-Freeapproach. For all agents, we use 3 random seeds per task, and default hyperparameters. Environment detailsVisual observations are 64 64 3 pixel renderings. We test performance inthree environments: DeepMind Control Suite (DMC) [Tassa et al., 2018], Reafferent DMC (describedbelow), and Distracting Control Suite [Stone et al., 2021] (with background video initialized to arandom frame each episode, 2,000 grayscale frames from the \"driving car\" Kinetics dataset [Kayet al., 2017]). For each environment, we test two tasks: Cheetah Run and Hopper Stand. Weselected these tasks because they present different levels of difficulty, allowing us to assess howdistraction-sensitivity depends on task difficulty. For ablation experiments, we test on Cheetah Run.",
  ": Schematic of the Reafferent DeepmindControl environment. The distracting backgroundis entirely predictable based on the agents previousaction and the elapsed time in the episode": "In the natural world, distractions can be highlycomplex, but in many cases are also highly pre-dictable. For instance, the creaking sound a rustybicycle makes as you pedal, or the movementof your own shadow as you dance outside. Wewanted an environment which would allow in-vestigation of how well existing methods wouldperform in scenarios where the representationalcomplexity of the distractions are very high, butthey cannot simply be ignored as unlearnablenoise. Distinguishing this type of partly self-generated distraction requires identifying whichparts of the world are relevant to taking action,not just those affected and unaffected by our ac-tion. To achieve this, we devised the ReafferentDeepmind Control environment, in which thedistracting background images have substantialcontent, but they depend deterministically onthe agents previous action and the elapsed time in the episode and are thus completely predictable(). We build on the Distracting Control Suite [Stone et al., 2021], using a background",
  ": Training curve comparisons on Reafferent Deepmind Control. Mean std. err": "consisting of 2,500 16 x 16 grids, with each grid cell filled by a randomly chosen color. We thenmap a tuple of time (625 possible values) and a discretized version of the first action dimension (4possible values) to an assigned background. We devised this to be analogous to the types of highcomplexity self-generated distractors found in the natural world (e.g. ones shadow). This environment allows us to stress test the structural regularizations (and associated priors) that formthe foundation of many existing distraction-avoidance methods. Many methods encode assumptionsabout the forms distractors will take (usually uncorrelated to agent actions, reward, or both), ratherthan a means of generally identifying and ignoring distractors. We hypothesize that a learning-basedapproach, in which we avoid distraction by learning what is actually important for the agent to getthings done, has the potential to overcome even learnable-but-not-useful distractions. : Reconstructed image comparison, PSPvs. DreamerV3 on Reafferent Cheetah Run, sameepisode and time point. True, reconstructed, differ-ence (true - recon.). DreamerV3 accurately repro-duces the background but not the cheetah. We find that none of the baseline MBRL meth-ods perform well on the Reafferent Environment,relative to their published results on the Distract-ing Control Suite or their performance on theunmodified Deepmind Control Suite (,). We find that the world models learnexcellent reproductions of the distracting back-ground. However, the cost of this is that thereconstruction of the agent becomes less well-defined or even replaced by the background, es-pecially in positions where the outcome of amovement is uncertain (see and A1for examples with DreamerV3 and A2 with De-noised MDPs). This matches our expectationthat the model will waste capacity on triviallypredictable dynamics, rather than on the muchmore important but uncertain agent dynamics.As expected, unlearnable distractions are lesschallenging (Figure A3).",
  ":Examplesalience maps (policy-shapedlossweights)highlight the agent": "Notably, the model-free DrQv2 agent shows a reduction in performancefrom its previous performance on the unmodified environment, but overalldemonstrates quite robust performance (). This also matches ourexpectations, since the CNN encoder is learned as part of the policyin model-free learning, unlike with model-based, where the learningobjective for the world model is separate from the learning objective forthe policy. Our new method demonstrates a substantial improvement over the existingbaselines (, ). Although it shows a higher than desiredlevel of variance between runs, especially on the more challenging HopperStand task, it nevertheless achieves scores beyond the reach of any of thebaselines. We believe this affirmatively answers Q1 by showing our new",
  ": Training curve comparison on Distracting Control. Mean std. err": "agent is in fact robust to challenging distractors. Addressing Q2, we also find that the salience maps,derived from the gradient of the policy and used to weight the world model loss, highlight the regionsof the image that we would expect (Figures 5 and A4). Interestingly, we see that sometimes thecheetahs rear leg is highlighted when it is the only leg close to the ground, though in other instancesthe entire cheetah is highlighted (Figure A4).",
  "Performance on unaltered DMC and Distracting Control Suite": "On Distracting Control tasks, in which the background distractor is uncoupled from the agentsactions, PSP produced consistently improved performance relative to baseline DreamerV3, in contrastto the more variable performance of DreamerPro, TIA, and Denoised MDP (, ). Thisaddresses Q3. Importantly, PSP also shows comparable performance to other methods (including DreamerV3) onthe unaltered Deepmind Control Suite, demonstrating that we have not introduced a tradeoff betweenperformance on distracting and non-distracting environments (, Figure A5), resolving Q4. In sum, PSP exhibits similar performance to baseline methods in commonly used tests of distractor-suppression and in non-distracting environments, while also demonstrating unmatched performanceon particularly challenging distractors that are complex but learnable.",
  "Ablation study": "To understand the contributions of each sub-component of the method (i.e. address Q5), we conductablations on the reafferent and unaltered Cheetah Run (). We find that some ablations trade offperformance between the environments, while our complete model has good performance on both. For instance, the top-performing method on the reafferent environment does not incorporate thesegmentation or adversarial components, and uses the value gradient rather than the policy gradient.However, the variance of its scores is higher than any other approach, and more problematically, itshows the worst performance of all experiments in the unaltered environment. We believe this occurs",
  "None521.1 136.3158.4 45.7": "because of the flaws in using only the gradient as an explanation for pixels that explain the actor-criticoutput. These flaws are more evident in an environment where the background never changes, asthe policy is not required to learn any robustness to shifts in the background. In other words, thegradient for changes in the static background may be quite large, since the model is immediately outof domain when the background changes. We find that segmentation-based aggregation is critical toimproving our models performance amid distractors, while also maintaining its performance in thenon-adversarial baseline. Overall, the results of the ablations confirm that combining segmentation,policy gradient sensory weighting, and adversarial action prediction results in the best scores acrossthe unaltered and reafferent environments. We also investigated the importance of the weight interpolation (Equation 3). We find that interpo-lation produces the expected benefit of allowing the agent to construct a useful world model, evenwhen the policy is not very good, and thus sidestepping the chicken-and-egg problem where theagent has neither a good policy nor a good world model (Figure A6). Furthermore, we tested theability of PSP to adapt to either a task change (Figure A7) or a change in the distractor (Figure A8),and we found that PSP was able to quickly adapt in both scenarios.",
  "Additional segmentation models": ": Performance comparison of different segmen-tation models. SAM2 is the tiny model size of the new,faster Segment Anything model. SAM2-tiny performssimilarly on Cheetah Run, but is worse on the more chal-lenging Hopper Stand. SAM2-large recovers some of thedecreased performance on distracting variants of Hopper.The full training plots are included in A10 and A11.",
  "Hopper Stand417.7 118.9187.4 172.0465.0 166.6": "We additionally tested the sensitivity ofPSP to the segmentation model. Giventhat segmentation models are likely tocontinue improving over time, we won-dered 1) whether PSP could be compati-ble with other models besides SAM, and2) how PSP performance might be mod-ulated by the performance of the segmen-tation model. To investigate, we usedthe recently released SAM2 [Ravi et al.,2024], which has multiple model sizesthat allow for trading off performance forsegmentation speed, with as high as 6xfaster segmentation speeds than the orig-inal SAM. We updated PSP to use thetiny SAM2 model, the smallest and low-est accuracy of the provided model sizes.Our basic implementation with SAM2-tiny immediately improved segmentationspeeds (and thus reduced the resources necessary for segmentation) by 2x. We found that PSPwith SAM2-tiny yielded nearly identical performance as the original PSP with SAM on all threeCheetah environments (). On Hopper, a substantially harder task, we observed increasedsensitivity to the quality of the segmentation. PSP with SAM2-tiny performed the same as PSPwith SAM in the unmodified environment. In the Reafferent environment, PSP with SAM2-tiny stilloutperformed all baselines, with one of three runs yielding a successful policy (compared with zeroout of twelve total runs across all baselines), though the successful run yielded a lower score (81.7) than the successful run with SAM (377.5), which was also one of three runs. Additionally, SAM2-tinyyielded significantly worse performance on Hopper in Distracting Control. We hypothesized thatthis environment is particularly challenging for segmentation because the distracting background is ablack and white video, which is difficult to discern from the platform. SAM2-tiny is less successfulat segmenting the platform, which is problematic for learning a good policy. We further hypothesizedthat this issue would be resolved by improving the performance of the segmentation model. We testedthis by using the SAM2-large model, and found that indeed this recovered performance back to thelevel of the original SAM, yielding a score of 465 166.6. We additionally tested SAM2-large onReafferent Control Hopper and observed a similar pattern with, again, one out of three runs yieldinga successful policy with a score (114.4) that improved on SAM2-tiny. Optimizations, includingusing SAM2s video segmentation capabilities and better utilization of the GPU, would likely furtherimprove segmentation speed. These results also suggest that as long as the segmentation is goodenough to properly segment the environment, PSP is not very sensitive to the segmentation algorithm.We also note that objects can be over-segmented into multiple segments without causing problems(Figure A9), and thus adequate segmentation is not a particularly stringent requirement.",
  "Related Work": "Distraction-sensitivity of model-based RLRecent advances in Model Based RL (MBRL) includ-ing World Models [Ha and Schmidhuber, 2018], SimPLe [Kaiser et al., 2019], MuZero [Schrittwieseret al., 2020], EfficientZero [Ye et al., 2021], DreamerV1 [Hafner et al., 2019], DreamerV2 [Hafneret al., 2020], and most recently DreamerV3 [Hafner et al., 2023] have surpassed model-free RL insettings such as Atari, Minecraft, and Deepmind Control Suite. One deficiency of current MBRLalgorithms is a susceptibility of the world model to become overwhelmed by easily predictabledistractors, in part due to mismatch between the objectives of the policy (maximizing reward) and theworld model (accurately predicting future states) [Lambert et al., 2020]. One line of work attempts to address the distractability of MBRL through structural regularizations.Deng et al. uses contrastive learning of prototypes instead of image reconstruction. Lambet al. introduces the Agent Control-Endogenous State Discovery algorithm, which discardsinformation not relevant to elements of the environment within the agents control. Task InformedAbstractions (TIA) identifies task-relevant and task-irrelevant features via an adversarial loss onreward-relevant information [Fu et al., 2021]. Denoised MDPs extends TIAs factorization to includenotions of controllability [Wang et al., 2022]. Clavera et al. use meta-learning and an ensembleof dynamics models. These works form a strong body of solutions, given prior knowledge of likelydistractors, but they can struggle if a distractor does not fall into the designed regularizations. A different approach instead learns what is important by using the actor-critic functions to scale theimportance of various learned dynamics. VaGraM uses value gradients to reweight state reconstructionloss [Voelcker et al., 2022], building on Lambert et al. and IterVAML [Farahmand, 2018],but VaGram does not operate on visual tasks. Eysenbach et al. propose a single objectivefor jointly training the model and policy. Goal-Aware Prediction learns a joint representation of thedynamics and a goal, by predicting a goal-state residual, although they describe this approach as likelystill susceptible to distractions [Nair et al., 2020]. Seo et al. decouples visual representationsand dynamics via an autoencoder, improving the performance of Dreamer on tasks involving smallobjects. Value-equivalent agents [Grimm et al., 2020], such as MuZero [Schrittwieser et al., 2020]or Value Prediction Networks [Oh et al., 2017], construct a world model that only aims to representdynamics relevant to predicting the value function, in contrast to methods such as Dreamer thataim to learn the broader dynamics of the environment. MuZero is very effective in settings withdiscrete actions such as Atari, Go, and chess. Adaptation to domains with complex action spacessuch as Deepmind Control Suite [Hubert et al., 2021] have shown some success, however Dreamer-based agents that include image reconstruction for world model learning can still exhibit superiorperformance, and the image-related signals have been shown to be essential to their performance[Hafner et al., 2020]. Building on these methods, our work investigates how to combine the benefitsof both image reconstruction and task-aware modeling, through policy-shaped image-based worldmodeling, by applying concepts from VaGraM to the image-based MBRL setting.",
  "Distraction-sensitivity of model-free RLA parallel track of Model Free RL (MFRL) has itsown body of literature, with a leading method DrQv2 [Yarats et al., 2021a] used in this paper": "for comparison. DrQv2 is an off-policy actor-critic RL algorithm that operates directly on imageobservations, using DDPG as the base RL algorithm alongside random shift image augmentation forimage generalization and sample efficiency. Although our work focuses on a solution to MBRLsdistraction-sensitivity, it is worth noting analogous deficiencies can exist in MFRL and there are anumber of works addressing these. For instance, Mott et al. uses an attention mechanism tomake the agent robust to environment changes, and Tomar et al. learn task-relevant inputsmask. Yarats et al. [2021b], an inspiration for DreamerPro, creates prototypes that compress thesensory data, Grooten et al. [2023a] apply dynamic sparse training to the input layer of the actor andcritic networks, and Grooten et al. [2023b] mask out non-salient pixels based on critic gradients.",
  "Discussion": "PSP combines three ideas to focus the capacity of an agents world model on aspects of the environ-ment that are useful to its policy. First, the gradient of the policy with respect to the input imageis used to identify pixels that influence the policy. Second, the importance of individual pixels areaggregated by object, using a segmentation model to identify objects. Third, wasteful encoding ofthe preceding action (which is known and does not need to be predicted) in the image embedding isremoved using an adversarial prediction head. Together, these allow an agent to construct a worldmodel that best informs its policy, and in doing so, use the policy to shape what information isprioritized by its world model. The outcome of this process is an agent that is selective about whatparts of the world it models, and that becomes resilient against enticingly learnable, but ultimatelyempty, distractions. Our work draws a connection between the use of the value function gradient in VaGraM and relatedconcepts from the vision model explainability literature. The value gradient can be seen as analogousto saliency maps [Simonyan et al., 2013]. Other gradient-based attribution methods, such as thosethat multiply saliency maps by input intensities [Shrikumar et al., 2017] or Integrated Gradients[Sundararajan et al., 2017, Ancona et al., 2019] offer additional ways to perform attribution. Somegradient-based attribution methods, such as Integrated Gradients, can be computationally expensivedue to the need to approximate an integral over the input space. Future work may investigateincorporation of more advanced explainability methods such as these into PSP, and the conceptof an agent interpreting itself may exhibit broader utility. Finally, recent work that uses SAMcombined with human supervision to improve the generalizability of model-free RL [Wang et al.,2023], together with our work, point towards the potential value of incorporating powerful objectsegmentation models into reinforcement learning systems. LimitationsLimitations of PSP include its fundamentally object-centric view, which assumes thatpixels belong to single objects, and that the objects can be ranked by their importance. Additionally,the SAM segmentation model requires significant compute, but these models will likely improve overtime and can also be application-tailored. Notably, however, segmentation is not necessary duringinference of the world model and policy, only training. Finally, it is not yet clear how well PSPwill adapt in environments where the reward structure or salient features change across time. PSPmay make the world model more task-specific than other approaches, although it does keep somereconstruction weight on non-task-relevant features and we observed initial evidence of resiliency(Figures A7, A8). OutlookOur work finds headroom to improve the robustness of MBRL to distractions by linkingthe actor-critic functions and the reconstruction loss and leveraging useful priors from pre-trainedfoundation models. The findings here open other lines of inquiry such as using better modelexplanation techniques or more explainable architectures, utilizing faster segmentation models, andutilizing segmentation models designed for videos, in order to do temporal aggregation. Substantialwork likely remains to improve the speed of this technique and find extensions that allow it toreliably work for harder problems, such as applied robotics. The adversarial action prediction headsinspiration from the biological concept of efference copies also suggests there is still space in MBRLto consider biological metaphors as helpful design principles for learning algorithms. In sum, wepresent PSP, a method for avoiding distractors by focusing the world model on the parts of theenvironment that are important for selecting actions.",
  "T. B. Crapse and M. A. Sommer. Corollary discharge across the animal kingdom. Nature ReviewsNeuroscience, 9(8):587600, 2008": "F. Deng, I. Jang, and S. Ahn. Dreamerpro: Reconstruction-free model-based reinforcement learningwith prototypical representations. In International Conference on Machine Learning, pages49564975. PMLR, 2022. B. Eysenbach, A. Khazatsky, S. Levine, and R. R. Salakhutdinov. Mismatched no more: Joint model-policy optimization for model-based rl. Advances in Neural Information Processing Systems, 35:2323023243, 2022.",
  "C. Grimm, A. Barreto, S. Singh, and D. Silver. The value equivalence principle for model-basedreinforcement learning. Advances in Neural Information Processing Systems, 33:55415552, 2020": "B. Grooten, G. Sokar, S. Dohare, E. Mocanu, M. E. Taylor, M. Pechenizkiy, and D. C. Mocanu.Automatic noise filtering with dynamic sparse training in deep reinforcement learning. arXivpreprint arXiv:2302.06548, 2023a. B. Grooten, T. Tomilin, G. Vasan, M. E. Taylor, A. R. Mahmood, M. Fang, M. Pechenizkiy, and D. C.Mocanu. Madi: Learning to mask distractions for generalization in visual deep reinforcementlearning. arXiv preprint arXiv:2312.15339, 2023b.",
  "D. Hafner, J. Pasukonis, J. Ba, and T. Lillicrap. Mastering diverse domains through world models.arXiv preprint arXiv:2301.04104, 2023": "T. Hubert, J. Schrittwieser, I. Antonoglou, M. Barekatain, S. Schmitt, and D. Silver. Learning andplanning in complex action spaces. In International Conference on Machine Learning, pages44764486. PMLR, 2021. L. Kaiser, M. Babaeizadeh, P. Milos, B. Osinski, R. H. Campbell, K. Czechowski, D. Erhan, C. Finn,P. Kozakowski, S. Levine, et al. Model-based reinforcement learning for atari. arXiv preprintarXiv:1903.00374, 2019. W. Kay, J. Carreira, K. Simonyan, B. Zhang, C. Hillier, S. Vijayanarasimhan, F. Viola, T. Green,T. Back, P. Natsev, M. Suleyman, and A. Zisserman. The kinetics human action video dataset.CoRR, abs/1705.06950, 2017. URL A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg,W.-Y. Lo, et al. Segment anything. In Proceedings of the IEEE/CVF International Conference onComputer Vision, pages 40154026, 2023. A. Lamb, R. Islam, Y. Efroni, A. Didolkar, D. Misra, D. Foster, L. Molu, R. Chari, A. Krishnamurthy,and J. Langford. Guaranteed discovery of control-endogenous latent states with multi-step inversemodels. arXiv preprint arXiv:2207.08229, 2022.",
  "Figure A2: Denoised MDP reconstructs the background with a high degree of fidelity, but does notclearly render the Cheetah agent": "Figure A3: To more thoroughly show that the reafferent environment impacts DreamerV3 because ofthe learnable time & action mapping to backgrounds (and not purely because of the presence of thebackgrounds themselves), we include this training curve from an environment which uses the samebackgrounds, but with a random choice of background at each timestep. This demonstrates effectivepolicy learning in spite of the distracting (but unlearnable) background.",
  "ABroader Impacts": "At the current stage, this work remains reasonably far from any large societal impacts, as it is limitedto agents interacting with small, simulated environments. Over the long term, however, if model-basedRL algorithms are used to control robots or internet-connected agents (such as large language modelagents), the potential for both large positive and negative societal impacts becomes relevant. On thepositive side, intelligent agents that are capable of modeling the world and avoiding distractors havethe potential to aid humans in a wide variety of scenarios, from housework, to medical applications,to exploration, to internet research. On the negative side, agents without proper safeguards have thepotential to inflict harm on humans and the environment, whether through negligence or malfeasance.Ultimately, our work is targeted at producing the positive impacts, while still allowing for mitigationof the negative impacts.",
  "Figure A5: Training curve comparison on unmodified Deepmind Control. Mean std. err., n=3 runs": "Figure A6: Interpolation of the gradient based reconstruction loss and a uniform reconstruction lossyields superior performance to a gradient based reconstruction loss without interpolation, whichmanifests as a faster initial rise in score with PSP. Interpolation allows the agent to construct a usefulworld model even when the policy is not yet very good (and hence minimizes the impact of thechicken-and-egg problem where the agent has neither a good policy nor a good world model). Mean std. err., n=3 runs. Figure A7: In order to test the adaptability of a model trained for one task via PSP to another taskfor the same agent, we switched from Walker Run to Walker Stand at step 1M, while leaving theReafferent background unchanged. We see that the learned model can quickly adapt to the new task,even with the Reafferent background. Mean std. err., n=3 runs. Figure A8: In order to test the adaptability of a PSP model to a dynamic distraction, we switched thesettings of the Reafferent distraction at step 1M. We see that the learned model can quickly adapt tothe new distraction. Mean std. err., n=2 runs. Figure A9: How high quality must the segmentation be? We note while training that SAM didnot have perfect segmentation performance on our tasks. In fact, the Cheetah agent was usuallysegmented into several (and a varying number of) different regions. The algorithm proves robustacross these segmentation variations. Figure A10: Training curve comparisons on Reafferent Deepmind Control. Mean std. err. of PSPwith different segmentation algorithms: SAM, SAM 2 Tiny, and SAM 2 Large. All baselines areshown in . Notably, all three PSP agents shown here achieve non-zero scores on HopperStand, in contrast to the baselines.",
  "BComputational Overhead": "We characterize the computational cost of the PSP algorithm by ablating various components andmeasuring its training speed (Table A1). There are four major components of PSP that affectperformance. In order of decreasing computational cost: (1) Policy gradient-based weighting, (2)image segmentation (e.g. with SAM) (3) action adversarial head, and (4) segmentation-basedaggregation of the gradient weighting. These are all costs that apply only during training, not duringinference. Policy gradient based weighting: For each latent state produced by the encoder RSSM in eachstep of a rollout, we take the gradient of the policy with regard to the image pixel inputs. In ourimplementation, this auto differentiation yields a complexity of O((E + R + P) S2 W H), whereE is the number of encoder parameters, R is the number of parameters of the RSSM, P is the numberof parameters of the policy, S is the number of steps in a rollout, W is the width of the input image,and H is the image height. There are a couple of major opportunities for optimization in future implementations. First, fordebugging and experimental reasons, we have been computing the gradient of the policy withrespect to all rollout steps, instead of just the current step. This reveals an opportunity to reducethe computational burden by a factor of S, where S is 64 in our implementation. Reducing thiscontribution could induce a significant speedup in the performance of future implementations. Second,we take the gradient of the policy with regard to the image during rollouts only to enable visualization",
  "for debugging and figures. This is strictly speaking unnecessary overhead and could be eliminated fora gain during training": "Image segmentation: The details of the additional complexity depend on the specific algorithmused. With our addition of results using the SAM2 tiny model, we now demonstrate the viability ofusing different segmentation algorithms. Notably, the segmentation process is entirely parallelizableseparately from the training process: images are segmented as they are collected and stored in thereplay buffer. We find that segmentation is not a bottleneck in training speed. Moreover we find thatadvances in segmentation algorithms (e.g. SAM2) reduce the computational resource requirementsfor this process. Adversarial head: For each training step, we take the gradient of the loss of the action predictionhead with regard to the parameters of the encoder. Therefore, the cost is O((E + R + A) S), whereA is the number of parameters of the action prediction head. These gradients are added with theregular gradients during a train step, so the adversarial head does not introduce additional iterationsduring training. Segmentation aggregation of gradients: We take the mean value of the gradient with respect toeach mask by multiplying each mask by the gradient weighting and then taking the hadamard productof the resulting image (matrix) with the inverse of the sum of mask elements equal to 1. This resultsin an additional cost of O(M W H), where M is the number of masks.",
  "CExperiments Compute Resources": "Each trial of the PSP method used 4 Nvidia A40 GPUs to train the modified DreamerV3 model, and 4A40 GPUs to run the Segment Anything model in parallel. Given an estimated 17 unique experimentsfor the final paper, 3 trials per experiment with our method, and about 1.5 days per training run, weused about 17 * 3 * 1.5 * 8 GPUs = 612 GPU days on A40 accelerators. Early experiments withthis methodology likely used an additional 300. Baseline trials could be run on only a single A40GPU or a desktop NVIDIA 2070 SUPER, usually in less than a day, and accounted for a comparablynegligible level of resources. We believe this level of resource consumption could be easily reduced. The modifications to theDreamerV3 model do not attempt to benchmark the most costly components. We suspect our methodof parallelizing the new backpropagation from the policy to the image could be optimized furtherfrom its naive Jax implementation. Additionally, SAM could be supplanted by a more efficientsegmentation model. We focused on establishing the basic technique with SAM, and replacing it withmore efficient methods should be the subject of future work."
}