{
  "Abstract": "Subgraph-based methods have proven to be effective and interpretable in predict-ing drug-drug interactions (DDIs), which are essential for medical practice anddrug development. Subgraph selection and encoding are critical stages in thesemethods, yet customizing these components remains underexplored due to thehigh cost of manual adjustments. In this study, inspired by the success of neuralarchitecture search (NAS), we propose a method to search for data-specific com-ponents within subgraph-based frameworks. Specifically, we introduce extensivesubgraph selection and encoding spaces that account for the diverse contexts ofdrug interactions in DDI prediction. To address the challenge of large searchspaces and high sampling costs, we design a relaxation mechanism that uses anapproximation strategy to efficiently explore optimal subgraph configurations. Thisapproach allows for robust exploration of the search space. Extensive experimentsdemonstrate the effectiveness and superiority of the proposed method, with thediscovered subgraphs and encoding functions highlighting the models adaptability.",
  "Introduction": "Precise prediction of drug-drug interactions (DDIs) is essential in biomedicine and healthcareresearch . Drug combination therapy can enhance treatment effectiveness for certain diseases;however, it also increases the risk of adverse drug reactions, potentially threatening patient safety .Identifying DDIs through laboratory experiments is both costly and time-consuming . Withthe success of deep learning, researchers have increasingly explored computational methods forDDI prediction. Early approaches primarily relied on molecular fingerprint information orhand-engineered features , often neglecting the pre-existing interaction properties between drugs. Considering drugs as nodes and their interactions as edges, DDI prediction can be framed as amulti-relational link prediction problem within the constructed drug interaction network. Recentadvancements in graph neural networks (GNNs) have consistently achieved superior per-formance in this task. Specifically, subgraph-based methods, such as SumGNN , EmerGNN ,and KnowDDI , have shown promising results by selecting subgraphs around query edgesand applying sophisticated encoding functions (message passing functions) to represent these sub-graphs, Such methods transform the multi-relational link prediction task into a multi-type subgraphclassification problem. illustrates the pipeline of subgraph-based methods. However, due to the dense nature of drug interaction networks and their complex in-teraction semantics , existing hand-designed subgraph methods often fail to capture the nu-anced but crucial information across different data inputs. In the initial phase of the reasoningpipeline, the subgraph sampler must have the capability to customize the selection of drug sub-graphs for different queries, thereby ensuring precise contextualization of the reasoning evidence.",
  "CSSE-DDI": "Without customized subgraph selection, SumGNNsamples subgraphs using a fixed subgraph range k,selecting the k-hop neighbors of each drug as associ-ated subgraphs for predicting drug-drug interactions(DDIs). This coarse-grained approach is straightfor-ward and easy to implement, but it may introducenoise or omit valuable information needed to reasonabout diverse drug pair interactions. In terms of encoding process, the encoding functionmust be capable of modeling a wide variety of drug in-teractions within the drug interaction network. Real-world drug interactions exhibit complex mechanisms,for instance, metabolism-based interactions displayasymmetric semantic patterns, whereas phenotype-based interactions are symmetric. Manually designedencoding functions are limited in their ability to ac-commodate both types of distinct semantic patterns simultaneously . Therefore, designing acustomized and data-adaptive subgraph-based pipeline is essential for effective DDI prediction. Neural architecture search (NAS) has achieved remarkable success in designing data-specificmodels, often surpassing architectures crafted by human experts in various fields, such as computervision , graph neural network , and knowledge graph learning . However, effectivelyselecting suitable subgraphs from the vast space of candidates and efficiently optimizing the jointsearch process of subgraph selection and encoding remain open challenges. In this paper, we leverage NAS to search for data-specific components in the subgraph-based pipeline.Specifically, we design search spaces for pipeline components, including subgraph selection andencoding spaces, to capture various drug interaction patterns. To enable efficient exploration ofthe extensive subgraph selection space, we introduce a relaxation mechanism that continuouslyselects subgraphs in a structured manner. Additionally, we propose a subgraph representationapproximation strategy to reduce the high cost of explicit subgraph sampling, enabling efficient androbust search. Compared with existing methods in , our proposed Customized SubgraphSelection and Encoding for Drug-Drug Interaction prediction (CSSE-DDI) achieves fine-grainedsubgrpah selection and data-specific encoding functions, providing an efficient and precise methodfor drug interaction prediction. Our main contributions are summarized as follows: We present CSSE-DDI, a searchable framework for DDI prediction that adaptively customizesthe subgraph selection and encoding processes. To the best of our knowledge, this is the firstapplication of NAS techniques to tailor an adaptive subgraph-based pipeline for the DDI predictiontask. We construct expressive search spaces to ensure precise capture of evidence for drug interactionprediction. Additionally, we devise a relaxation mechanism to transform the discrete subgraphselection space into a continuous form, enabling differentiable search. Simultaneously, we apply asubgraph representation approximation strategy to mitigate the inefficiencies of explicit subgraphsampling, thereby accelerating the search process. Extensive experiments on benchmark datasets demonstrate that our method, which searchesfor customized pipelines, achieves superior performance compared to hand-designed methods.Additionally, our approach effectively captures the underlying biological mechanisms of drug-druginteractions.",
  "Related Works": "Subgraph-based Link PredictionRecently, subgraph-based methods have emergedas a promising approach, showing superior performance in link prediction tasks. Different fromcanonical GNNs, subgraph-based method extracts a subgraph patch for each training and test query,learning a representation of the extracted patch for final prediction, as illustrated in . Existing works has primarily focused on designing more informative subgraphs and more expressiveencoding functions. However, they do not take into account customizing these components to dealwith various data. Specifically, in terms of subgraph sampler, current approaches lack fine-grainedand adaptive extraction for different query subgraphs. While PS2 demonstrates the effectivenessof identifying optimal subgraphs for each edge in homogeneous graph link prediction, there is nocomparable work in multi-relational graph link prediction. In dense DDI networks, fine-grainedidentification of subgraph for different queries is even more crucial. As for the encoding function, existing works overlook the importance of data-specific encoding, whichhas been emphasized in recent literature . Customized encoding functions are especiallyadvantageous for drug interaction networks with complex and diverse interactions. GNN-based DDI PredictionRecently, there has been growing interest in applying GNNs forDDI prediction . However, these works execute message-passing functions over the entiregraph, which limits their ability to capture explicit local evidence for specific query drug pairs andlack interpretability. In contrast, subgraph-based DDI prediction methods transformthe multi-relational link prediction problem into a subgraph classification problem by extractingsubgraphs around query nodes, achieving strong performance. Nevertheless, these works use thesame subgraph extraction strategy for all queries and rely on a fixed message-passing function tohandle complex DDIs, which limits their flexibility and adaptivity in dense DDI networks. Graph Neural Architecture SearchGraph neural architecture search (GNAS) aims to findhigh-performing GNN architectures using NAS techniques. Recent studies have exploredGNAS to create more expressive GNN models across various tasks. AutoDDI , for instance,automatically designs GNN architectures to learn molecular graph representations of drugs for DDIprediction. However, research on optimizing graph sampling for GNAS remains limited due to thediversity of graph-structured data. Regarding search strategy, early approaches explores the search space using reinforcement learn-ing or evolutionary algorithms , which is highly inefficient. One-shot approaches instead construct an over-parameterized network (supernet) and optimize it using gradient descent,leveraging continuous relaxation of the search space to improve search efficiency. The recently pro-posed few-shot NAS paradigm further enhances supernet evaluation consistency by generatingmultiple sub-supernets.",
  "Problem Formulation": "Given a set of drugs V and interaction relations R among them, the drug interaction network isdenoted as GDDI = {(u, r, v) | u, v V, r R}, with each tuple (u, r, v) describes an interactionbetween drug u and drug v. Consequently, drug-drug interaction (DDI) prediction can be frameda multi-relational link prediction task within the drug interaction network GDDI. The objective is topredict the types of interactions between two given drug nodes, which can be denoted as a query(u, ?, v), i.e., given the query drug-pair entities u and v, to determine the interaction r that makes(u, r, v) valid. Moreover, instead of directly predicting on the entire graph GDDI, subgraph-based methods decouplethe prediction process into two stages: (1) selecting a query-specific subgraph and (2) encoding thesubgraph to predict interactions, as shown in . The prediction pipeline then becomes",
  "Building on previous analysis and existing research, and inspired by NAS, we propose to searchfor data-adaptive subgraph selection and encoding components to obtain a customized subgraph": "pipeline. In .2, we first introduce the well-designed subgraph selection and encodingspaces to ensure comprehensive coverage of cricual information in various drug interaction networks.Further, in .3 we present a subgraph relaxation strategy and approximation mechanismsfor subgraph representations to facilitate efficient differentiable search. Finally, we develop a robustsearch algorithm to address the customized search problem with stability and precision.",
  "Search Space3.2.1Subgraph Selection Space": "In practice, subgraph-based methods define the drug-pair subgraph between drug pairs as the union orinteraction of k-hop ego-network 2 of query drugs. Here, k is a key hyperparameter that determinesthe range of message propagation aggregated by the central node. Selecting k is crucial to modelperformance, as it dictates whether the model has access to high-quality evidence context for accurateprediction. Prior works typically utilize a fixed hyperparameter for all drug pairs, i.e., selecting theunion of a fixed k-hop ego-network for arbitrary queries. Nevertheless, this approach can lead to animprecise collection of evidence for interaction reasoning, potentially undermining the reasoningprocess due to missing critical information or the inclusion of excessive irrelevant information.",
  "Su,v = {Gi,ju,v |1 i, j },(2)": "where Gi,ju,v is generated by taking the union of the i-hop ego-network of node u and the j-hopego-network of node v, i.e., Gi,ju,v = {z V | z (u Ni(u) v Nj(v))}, where Ni(u) andNj(v) are the i-hop and the j-hop neighbors of u and v, respectively. The threshold constrains themaximum subgraph range. Since each drug-pair has a specific subgraph selection space, the overall size of space in the entiregraph is 2|E|, where |E| represents the number of edges in the drug interaction network. A larger |E|result in a subgraph selection space that grows exponentially with the number of edges. Therefore,efficiently searching for the optimal subgraph configurations for different queries is challenging.",
  "where hu Rd and hr Rd represent the embeddings of node u and interaction r, respectively, andmu is the intermediate message representation of u aggregated from its neighborhood N1(u)": "A substantial amount of literature has focused on manually designing these modules toimprove performance. However, such encoding functions are inflexible for handling diverse interac-tion patterns across different drug interaction network. For example, interactions in DrugBank describe how one drug affects the metabolism of another one. The excretion of Acamprosate, for in-stance, may be decreased when combined with Acetylsalicylic acid (Aspirin). Such interaction patternis asymmetric, meaning r(x, y) r(y, x). Conversely, interactions in the TWOSIDES dataset are primarily at the phenotypic level, such as headache or pain in throat, representing symmetricpatterns where r(x, y) r(y, x). These two relational semantics are distinctly different, and existinghand-designed encoding functions struggle to capture such diverse semantics effectively . Here, we aim to perform an adaptive searching for the encoding function in the context of druginteraction prediction. Based on the framework presented in Eq. (3), we design an expressive subgraphencoding space with a set of candidate operations. Detailed explanations of these modules can befound in the Appendix A.1.",
  "Search Strategy3.3.1Search Problem": "Based on the well-designed search space described above, we formulate a bi-level optimizationproblem to adaptively search for the optimal configuration of subgraph-based pipelines.Definition 1 (Customized Subgraph-based Pipeline Search Problem). Let A denote the subgraphencoding space, Su,v represent the subgraph selection space for the query (u, v), be a candi-date encoding function in A, W represent the parameters of a model from the search space, andW(Gu,v; ) denote the trained operation parameters. Let Dtra and Dval denote the training andvalidation sets, respectively. The search problem is formulated as follows:",
  "(u,r,v)DtraL(W; Gu,v; ),(5)": "where the classification loss L is minimized for all interactions, while the performance measurementM is expected to be maximized.In this work, we adopt the differentiable search paradigm to solve the bi-level optimizationproblem, which is widely used in recent NAS literature and enables efficient exploration of thesearch space. Nevertheless, our proposed subgraph selection space poses two technical challenges:First, we cannot directly apply relaxation strategies, which is a prerequisite for differentiable NASmethods, to make the discrete selection space continuous. This limitation arises because differentsubgraphs in the selection space contain diverse nodes and edges, making it challenging to designa relaxation function that unifies subgraphs of varying sizes. Second, to enable searching withinthe subgraph selection space, we would need to first generate all subgraphs in the space. However,sampling such a large number of subgraphs is computationally intractable. To address these challenges, we design a subgraph selection space relaxation mechanism in Sec-tion 3.3.2 . Additionally, we introduce an intuitive subgraph representation approximation strategy in.3.3 to reduce the high costs associated with explicit sampling.",
  "Relaxation of Subgraph Selection Space": "Technically, as in existing NAS works , one typically needs to relax the search space intocontinuous form to enable effective backpropagation training. However, for the subgraph selectionspace, the traditional continuous relaxation strategy is not directly applicable due to the structuralmismatch between graphs and vectors. To address this, we first utilize encoding function f() to encode subgraphs with different scopes.This approach provides all subgraphs with representations of the same dimension, making it feasibleto implement a relaxation strategy. Additionally, inspired by the reparameterization trick , weadopt the Gumbel-Softmax function to facilitate differentiable learning over a discrete space:",
  "Subgraph Representation Approximation Strategy": "To solving the optimization problem as Eq. (4) and (5), we need to explicitly sample all the candidatesubgraphs within the subgraph selection space Su,v for each query. However, one of the mostchallenging aspects of subgraph-based approaches is their inefficient subgraph sampling process . Upon examining our subgraph selection space, we observe that all subgraphs are generated bycombining multi-hop ego-networks of the target nodes, encompassing multiple neighborhood hops.Inspired by the k-subtree extractor , we apply an encoding function to the entire graph and usethe resulting node representations of u and v as the ego-network representations for these nodes. Therepresentation of the drug pair can then be obtained by concatenating the ego-network representationsof u and v. Formally, if we denote by f(GDDI, u, i) the i-layer hidden representation of node uproduced by encoding function applied to GDDI, then",
  "f(Gi,ju,v) CONCAT(f(GDDI, u, i), f(GDDI, v, j)),(7)": "The k-subtree extractor represents the k-subtree structure rooted at a given node, which mirrors thestructure as the k-hop ego-network. This approximation strategy only requires executing the encodingfunction on the entire drug interaction network, thereby efficiently yielding subgraph representationsof varying scopes, which significantly improves the efficiency in solving the bi-level optimizationproblem.",
  "Robust Search Algorithm": "Using the proposed subgraph selection relaxation mechanism, we can transform the overall discretesearch space in Definition. 1 into a continuous form, allowing the search problem to be solved by theone-shot NAS paradigm. Additionally, our subgraph representation approximation strategy efficientlyobtains subgraph representations and reduces search costs Following , we adopt the single path one-shot training strategy (SPOS) to reduce thecomputational cost of supernet training. However, the one-shot approach , i.e., using thesame supernet parameters W for all architectures, can decrease the consistency between the supernetsperformance estimation and the ground-truth performance . Inspired by few-shot NAS , wepropose a message-aware partitioned supernet training strategy to mitigate the coupling effect ofdifferent message-computing operators . By partitioning the superent to form sub-supernets basedon the type of message-computing function, this strategy improves the consistency and accuracyof supernet, enabling the search algorithm more stable and robust. Algorithm 1 delineates the fullprocedure, with further details provided in Appendix A.2.",
  "Comparison with Existing Works": "While many works have explored DDI prediction using subgraph-based methods, ourapproach introduces two significant advancements. First, to the best of our knowledge, our method(CSSE-DDI) is the first to customize the subgraph selection and encoding processes specificallyfor subgraph-based DDI prediction. In contrast, previous methods rely on fixed subgraph selectionstrategy to sample subgraphs and employ hand-designed functions for encoding, as summarizedin . Consequently, our method can adapt data-specific components within subgraph-basedpipelines, outperforming existing methods in both performance and efficiency (.2). Moreover,our approach not only selects fine-grained drug-pair subgraphs that enhance interpretability throughpotential pharmacokinetic and metabolic concepts (.6.1), but also searches for data-specificencoding functions that accurately capture the semantic features of drug interactions (.6.2).",
  "DatasetsExperiments are conducted on two public benchmark DDI datasets: DrugBank andTWOSIDES . Detailed descriptions of these datasets are presented in Appendix B.1": "Experimental SettingsFollowing , we examine two DDI prediction task settings: S0 and S1.Let the drug pairs for DDI prediction be denoted as (u, v). In the S0 setting, both drug nodes u and vare present in the known DDI graph. Existing DDI prediction methods are typically evaluated in thissetting. In contrast, the S1 setting involves a pair (u, v) where one drug is known and the other is anovel drug not represented in the known DDI graph. This scenario highlights the critical need forDDI predictions involving new drugs in real-world applications. Evaluation MetricWe follow to evaluate our method. For the DrugBank dataset, whereeach drug pair contains only one interaction, we use the following metrics: F1 Score, Accuracy andCohens . For the TWOSIDES dataset, where multiple interactions may exist between a pair ofdrugs, we consider the following metrics: ROC-AUC, PR-AUC and AP@50. Additional details areprovided in Appendix B.2. BaselinesWe compare CSSE-DDI with the following representative DDI prediction method: (i)GNN-based methods include Decagon , GAT , SkipGNN , CompGCN , ACDGNN ,and TransFOL . (ii) Subgraph-based methods include SEAL , GraIL , SumGNN ,SNRI , KnowDDI and LaGAT . (iii) NAS-based method include MR-GNAS , andAutoGEL . We also compare our method with two variants, including CSSE-DDI-FS and CSSE-DDI-FF. Theconfigurations of these variants are as follows: (i) CSSE-DDI-FS: This variant omits fine-grainedsubgraph selection for each query, using fixed k-layer drug node representations to generate thesubgraph representation. (ii) CSSE-DDI-FF: This variant does not search for the encoding function,instead using a fixed encoding function backbone to capture semantic and topological features in thedrug interaction network. In this case, we employ a 3-layer CompGCN model as the backbone. Forall baselines, we obtain the results by rerunning the released codes. ImplementationWe implement our method3 based on PyTorch framework . Following existingGNN-based methods , we select a 3-layer encoding function backbone for both datasets. Themaximum threshold for the subgraph selection space is set to 3. More experimental details aregiven in the Appendix B.3.",
  "Performance Comparison in S0 settings": "shows the overall results across all benchmarks in S0 setting. As can be seen, CSSE-DDIconsistently outperforms all baselines on each dataset, demonstrating its effectiveness in searching fordata-specific subgraph-based pipelines for DDI prediction task. Among the baselines, subgraph-basedmethods significantly outperform full-graph-based methods due to their enhanced ability to reasonover local subgraph contexts. Within the subgraph-based methods, SEAL, GraIL, SumGNN, andSNRI use a fixed sample strategy to select subgraphs, which may not be optimal for different drug-pairqueries. When it comes to NAS-based method, MR-GNAS and AutoGEL contain well-established searchspaces that embrace multi-relational message-passing schema, focusing primarily on automatedencoding function design using the one-shot NAS paradigm. While CSSE-DDI adopts a single pathsupernet training strategy and a message-aware partitioning approach to search for data-adaptivesubgraph-based pipelines with stability and robustness, enabling the model to achieve excellentperformance across various datasets. Moreover, the consistent performance gains of CSSE-DDI overits two variants validate the importance of jointly customizing subgraph-based pipeline components,i.e., fine-grained subgraphs and data-specific encoding functions, to fit datasets rather than relying ona fixed approach. shows the learning curves of several competitive methods on both datasets, includingCompGCN, KnowDDI and the proposed CSSE-DDI. As can be seen, the searched models not onlyoutperform the baselines but also demonstrate a clear advantage in efficiency, highlighting thatenhancing model flexibility and adaptability is essential for improving performance and efficiency.",
  ": Comparison on convergence between the searchedarchitectures by CSSE-DDI and human-designed methods": "To demonstrate the effectiveness ofour search strategy, we introduce twovariants with different search strate-gies: (i) CSSE-DDI w/o MAP: Thisvariant uses only one trained supernetto serve as a performance evaluatorfor candidate architectures, instead ofgenerating multiple sub-supernets byMessage-Aware Partition (MAP) strat-egy. (ii) CSSE-DDI w/o SPOS: Thisvariant utilizes the message-aware par-tition strategy to jointly optimize thesupernet weights and architectural pa-rameters, without using the SinglePath One-Shot (SPOS) strategy .",
  "CSSE-DDI92.080.2295.470.02": "In , we compare CSSE-DDIwith other variants. As can be seen,the absence of either message-awarepartition strategy or sampling-basedNAS strategy negatively impacts per-formance.The performance gainsachieved through the message-awarepartition strategy arise from usingmultiple sub-supernets, which providemore accurate performance estima-tions to guide the search process. Regarding the SPOS strategy, it decouples supernet trainingfrom architecture search, making it more efficient and robust in practice. 4.4Sensitivity Analysis of the Threshold Here, we analyze the effect of the threshold used in subgraph selection space. showsthe impact of varying .As can be observed, model performance continues to get better asthe threshold grows.When the threshold = 3, the model performance nears saturation, Test Performance (%) F1 ScoreAccuracyCohen's Kappa",
  ": Performance given different hyperparameter": "as larger thresholds do not lead to fur-ther improvements. This is likely be-cause most of the essential informa-tion for DDI prediction is containedwithin the 3-hop ego-subgraphs oftarget drugs. Intuitively, larger sub-graphs may provide additional use-ful information. However, in prac-tice, due to the inherent biases of thesearch algorithm, achieving an opti-mal model may be challenging. When is too large, it may introduce noiseand dilute the critical information. Asimilar phenomenon has been found in the existing work SumGNN . Besides, excessively largethresholds will only lead to unnecessary expansion of the search space and higher computationalcosts.",
  "Performance Comparison in S1 settings": "To further validate the effectiveness of our method, we use the S1 setting in the EmerGNN method,to predict drug-drug interactions between emerging drugs and existing drugs. The experimentalresults are shown in . A significant performance drop from the transductive setting (S0) to theinductive setting (S1) demonstrates that DDI prediction for new drugs is more challenging. AlthoughEmergnn, which is specifically designed for new drug prediction, achieves optimal performance,CSSE-DDI still demonstrates impressive results, outperforming existing GNN-based and subgraph-based methods. This strong performance is largely due to the robust learning capability of NAStechnology in handling unknown data.",
  ": Visualization of the searched subgraphs corre-sponding to the specific drug pairs": "We visualize exemplar query-specificsubgraphs from the DrugBank datasetin , highlighting domainconcepts such as pharmacokinetics,metabolism, and receptor interactions.As shown, CSSE-DDI can identify dis-tinctive subgraphs containing seman-tic information to support inferencefor different queries, revealing pharma-cokinetic and metabolic relationships. For example, to predict the interaction between DB00945 (Aspirin) and DB00682 (Warfarin), CSSE-DDI searches out the subgraph scope (1, 1), as depicted on the left part of . Firstly, it can beseen from the figure that the therapeutic efficacy of DB00233 (Aminosalicylic acid) can decreasewhen combined with DB00945 (Aspirin), suggesting similarity between the two drugs Giventhat DB00233 (Aminosalicylic acid) may increase the anticoagulant activity of DB00682 (Warfarin)and that DB00233 resembles DB00945 (Aspirin), it can be inferred that DB00945 (Aspirin) maysimilarly increase the anticoagulant activity of DB00682 (Warfarin). This example demonstratesthat the identified subgraph contains sufficient semantic information to reason about the interactionbetween DB00945 (Aspirin) and DB00682 (Warfarin).",
  ": The searched encoding functions on all benchmarkdatasets": "Furthermore,wevisualizethesearchedstructureofencodingfunctionsacrossalldatasetsin.It is clearly illustratedthat different combinations of thedesigned operations, i.e., data-specificencoding functions, are obtained. In particular, the searched message-computing functions contain moreCORR operations in the DrugBankdataset, while more MULT functionsare searched in the TWOSIDESdataset. The CORR function is non-commutative , making it suitablefor modeling asymmetric interactions(e.g., metabolic-based interactions) present in DrugBank. While MULT is suitable for modelingsymmetric relations (phenotype-based interactions) due to its exchangeability .",
  "Conclusion": "We propose a searchable framework, CSSE-DDI, for DDI prediction. Specifically, we designrefined search spaces to enable fine-grained subgraph selection and data-specific encoding functionoptimization. To facilitate efficient search, we introduce a relaxation mechanism to convert the discretesubgraph selection space into a continuous one. Additionally, we employ a subgraph representationapproximation strategy to accelerate the search process, addressing the inefficiencies of explicitsubgraph sampling. Extensive experiments demonstrate that CSSE-DDI significantly outperformsstate-of-the-art methods. Moreover, the search results generated by CSSE-DDI offer interpretabilityin the context of drug interactions, revealing domain-specific concepts such as pharmacokinetics andmetabolism.",
  "Acknowledgements": "We thank the anonymous reviewers for their valuable comments. This work was supported in partby the National Key Research and Development Program of China (Grant No. 2022ZD0160300),in part by the National Science Fund for Distinguished Young Scholars (Grant No. 62025602),in part by the National Natural Science Foundation of China (Grant Nos. U22B2036, 11931015,62203363, and 92270106), in part by the Technology Innovation Leading Program of Shaanxi (GrantNo. 2023GXLH-086), in part by the Beijing Natural Science Foundation (Grant No. 4242039). in partby the Fok Ying-Tong Education Foundation, China (Grant No. 171105), in part by the FundamentalResearch Funds for the Central Universities (Grant Nos. G2024WD0151 and D5000240309), and inpart by the Tencent Foundation and XPLORER PRIZE. Xuan Lin, Lichang Dai, Yafang Zhou, Zu-Guo Yu, Wen Zhang, Jian-Yu Shi, Dong-Sheng Cao,Li Zeng, Haowen Chen, Bosheng Song, et al. Comprehensive evaluation of deep and graphlearning on drug-drug interactions prediction. arXiv preprint arXiv:2306.05257, 2023. Timo Mttnen, Pekka Hannonen, Marjatta Leirisalo-Repo, Martti Nissil, Hannu Kautiainen,Markku Korpela, Leena Laasonen, Heikki Julkunen, Reijo Luukkainen, Kaisa Vuori, et al.Comparison of combination therapy with single-drug therapy in early rheumatoid arthritis: arandomised trial. The Lancet, 353(9164):15681573, 1999. David N Juurlink, Muhammad Mamdani, Alexander Kopp, Andreas Laupacis, and Donald ARedelmeier. Drug-drug interactions among elderly patients hospitalized for drug toxicity. Jama,289(13):16521658, 2003.",
  "Nguyen Quoc Khanh Le. Predicting emerging drug interactions using gnns. Nature Computa-tional Science, 3(12):10071008, 2023": "Yue Yu, Kexin Huang, Chao Zhang, Lucas M Glass, Jimeng Sun, and Cao Xiao. Sumgnn: multi-typed drug interaction prediction via efficient knowledge graph summarization. Bioinformatics,37(18):29882995, 2021. Yongqi Zhang, Quanming Yao, Ling Yue, Xian Wu, Ziheng Zhang, Zhenxi Lin, and YefengZheng. Emerging drug interaction prediction enabled by a flow-based graph neural networkwith biomedical network. Nature Computational Science, 3(12):10231033, 2023.",
  "Yaqing Wang, Zaifei Yang, and Quanming Yao. Accurate and interpretable drug-drug interactionprediction enabled by knowledge subgraph learning. Communications Medicine, 4(1):59, 2024": "Shonosuke Harada, Hirotaka Akita, Masashi Tsubaki, Yukino Baba, Ichigaku Takigawa, Yoshi-hiro Yamanishi, and Hisashi Kashima. Dual graph convolutional neural network for predictingchemical networks. BMC bioinformatics, 21:113, 2020. Mihai Udrescu, Sebastian Mihai Ardelean, and Lucretia Udrescu. The curse and blessingof abundancethe evolution of drug interaction databases and their impact on drug networkanalysis. GigaScience, 12:giad011, 2023.",
  "Lanning Wei, Huan Zhao, Zhiqiang He, and Quanming Yao. Neural architecture search forgnn-based graph classification. ACM Transactions on Information Systems, 2023": "Sijie Mai, Shuangjia Zheng, Yuedong Yang, and Haifeng Hu. Communicative message passingfor inductive relation reasoning. In Proceedings of the AAAI Conference on Artificial Intelligence,volume 35, pages 42944302, 2021. Qiaoyu Tan, Xin Zhang, Ninghao Liu, Daochen Zha, Li Li, Rui Chen, Soo-Hyun Choi, andXia Hu. Bring your own view: Graph neural networks for link prediction with personalizedsubgraph selection. In Proceedings of the Sixteenth ACM International Conference on WebSearch and Data Mining, pages 625633, 2023. Zhen Wang, Haotong Du, Quanming Yao, and Xuelong Li. Search to pass messages for temporalknowledge graph completion. In Findings of the Association for Computational Linguistics:EMNLP 2022, pages 61606172, 2022.",
  "Ziwei Zhang, Xin Wang, and Wenwu Zhu. Automated machine learning on graphs: A survey.pages 47044712, 2021": "Jianliang Gao, Zhenpeng Wu, Raeed Al-Sabri, Babatounde Moctard Oloulade, and Jiamin Chen.Autoddi: Drugdrug interaction prediction with automated graph neural network. IEEE Journalof Biomedical and Health Informatics, 2024. Kwei-Herng Lai, Daochen Zha, Kaixiong Zhou, and Xia Hu. Policy-gnn: Aggregation opti-mization for graph neural networks. In Proceedings of the 26th ACM SIGKDD InternationalConference on Knowledge Discovery & Data Mining, pages 461471, 2020. Yaoman Li and Irwin King. Autograph: Automated graph neural network. In Neural InformationProcessing: 27th International Conference, ICONIP 2020, Bangkok, Thailand, November 2327,2020, Proceedings, Part II 27, pages 189201. Springer, 2020.",
  "Jiaxuan You, Zhitao Ying, and Jure Leskovec. Design space for graph neural networks. Advancesin Neural Information Processing Systems, 33:1700917021, 2020": "Gabriele Corso, Luca Cavalleri, Dominique Beaini, Pietro Li, and Petar Velickovic. Principalneighbourhood aggregation for graph nets. Advances in Neural Information Processing Systems,33:1326013271, 2020. David S Wishart, Yannick D Feunang, An C Guo, Elvis J Lo, Ana Marcu, Jason R Grant, TanvirSajed, Daniel Johnson, Carin Li, Zinat Sayeeda, et al. Drugbank 5.0: a major update to thedrugbank database for 2018. Nucleic acids research, 46(D1):D1074D1082, 2018.",
  "Hieu Pham, Melody Guan, Barret Zoph, Quoc Le, and Jeff Dean. Efficient neural architecturesearch via parameters sharing. In International conference on machine learning, pages 40954104. PMLR, 2018": "Kaicheng Yu, Christian Sciuto, Martin Jaggi, Claudiu Musat, and Mathieu Salzmann. Evaluatingthe search phase of neural architecture search. In International Conference on LearningRepresentations, 2020. Zhaoxuan Tan, Zilong Chen, Shangbin Feng, Qingyue Zhang, Qinghua Zheng, Jundong Li, andMinnan Luo. Kracl: Contrastive learning with graph context modeling for sparse knowledgegraph completion. In Proceedings of the ACM Web Conference 2023, pages 25482559, 2023.",
  "Shikhar Vashishth, Soumya Sanyal, Vikram Nitin, and Partha Talukdar. Composition-basedmulti-relational graph convolutional networks. In International Conference on Learning Repre-sentations, 2020": "Hui Yu, KangKang Li, WenMin Dong, ShuangHong Song, Chen Gao, and JianYu Shi. Attention-based cross domain graph neural network for prediction of drugdrug interactions. Briefings inBioinformatics, 24(4):bbad155, 2023. Junkai Cheng, Yijia Zhang, Hengyi Zhang, Shaoxiong Ji, and Mingyu Lu. Transfol: A logicalquery model for complex relational reasoning in drug-drug interaction. IEEE Journal ofBiomedical and Health Informatics, 2024.",
  "Activation FunctionRELU, TANH,IDENTITY": "In particular, given the embedding hu of node u and the embedding hr of interaction r, the messagecomputing function takes the following form: MESSUB = hu hr, MESMULT = hu hr, MESCORR =hu hr, MESROTATE = hu hr, where stands for the circular correlation operation , representsthe rotation operation .",
  "where the discrete distribution (A) is set to uniform distribution": "First, we need to perform single path sampling to train the supernet until it converges. In the next step,we need to partition the supernet into sub-supernets. which is a key step aiming to isolate operationsthat are coupled with each other. This allows the supernet to be trained and converge more stably. In our supernet, we use a message-aware partitions strategy due to the fact that the degree ofdissimilarity between the operations in the message computing function MES is much higher comparedwith others. These operations focus on capture different semantic types of interactions, which hasbeen discussed in existing works . Therefore, we partition four operations of the messagecomputing function of the first layer of the supernet, to improve the accuracy of the performanceestimation. After partitioning operation, we initialize four sub-supernets with weights transferred from theoriginal supernet. Next, we train these sub-supernets to convergence by sampling single path. Here,the supernet training phase is all done.",
  "B.2Evaluation Metric": "We follow to evaluate our method. Specifically, in terms of the multi-class prediction onDrugBank, we followc and evaluate the performance by three metrics: (i) Macro F1 score (MacroF1) is computed by taking the arithmetic mean (aka unweighted mean) of all the per-class F1 scores.(ii) Accuracy (ACC) is calculated by dividing the number of correct predictions by the total prediction number. (iii) Cokens Kappa (Cohens ) measures inter-rater reliability. As to the multi-labelprediction on TWOSIDES, we consider the following measure and use the average performanceover all interaction types: (i) ROC-AUC (AUROC) stands for Area Under the Curve (AUC) of theReceiver Operating Characteristic (ROC) curve. (ii) PR-AUC (AUPRC) is the average area underprecision-recall curve. (iii) AP@50 is the average precision at 50.",
  "B.3Implementation and Hyperparameters": "All the experiments are implemented in Python with the PyTorch framework and run on a servermachine with single NVIDIA RTX 3090 GPU with 24GB memory and 64GB of RAM. Our code isadded in the supplementary material. For CSSE-DDI, we set the epoch to 400 for training supernet and set the epoch to 400 for trainingsub-supernets. We set the the temperature parameter as 0.05. Repeat 5 times with different seeds, wecan get 5 candidates. The searched candidates are finetuned individually with the hyper-parameters.In the stage of fine-tuning, we use the ReduceLROnPlateau scheduler to adjust the learning ratedynamically. Each candidate has 10 hyper steps. In each hyper step, a set of hyperparameter will besampled from .",
  "C.1Subgraph Scope Distribution Analysis": "We visualize the learned distributions of subgraph scope on all datasets by using CSSE-DDI in. By comparing the distributions across different benchmarks, we have the followingobservation: CSSE-DDI can effectively learn different subgraph scope distributions for variousdatasets. By identifing specific subgraph scopes for different queries, CSSE-DDI is able to preciselycontrol the extent of information propagation required for reasoning about the interactions of differentdrug pairs. In addition, our method can skip some subgraph scopes if they are not optimal for anyqueries. For example, no queries are assigned to the propagation scope (3, 3) on TWOSIDES dataset.It is worth mentioning that our searched subgraph scopes are consistent with the sensitivity analysisresults for the hop of subgraph in SumGNN , which further validates the effectiveness of ourapproach. (1,1) (1,2) (1,3) (2,1) (2,2) (2,3) (3,1) (3,2) (3,3)",
  "D.1Limitations": "There are three limitations for CSSE-DDI. (1) CSSE-DDI is focused on method design rather thansystem design. In the future, we will co-design the algorithm and the system to further improve theefficiency. (2) At present, CSSE-DDI only search for data-specific components of subgraph-basedpipeline, while hyper-parameters are also important for DDI prediction. A promising direction is toexplore how to efficiently search network architectures and hyper-parameters simultaneously.",
  "NeurIPS Paper Checklist": "The checklist is designed to encourage best practices for responsible machine learning research,addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not removethe checklist: The papers not including the checklist will be desk rejected. The checklist shouldfollow the references and follow the (optional) supplemental material. The checklist does NOT counttowards the page limit.",
  "Please provide a short (12 sentence) justification right after your answer (even for NA)": "The checklist answers are an integral part of your paper submission. They are visible to thereviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it(after eventual revisions) with the final version of your paper, and its final version will be publishedwith the paper. The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation.While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided aproper justification is given (e.g., \"error bars are not reported because it would be too computationallyexpensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering\"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, weacknowledge that the true answer is often more nuanced, so please just use your best judgment andwrite a justification to elaborate. All supporting evidence can appear either in the main paper or thesupplemental material, provided in appendix. If you answer [Yes] to a question, in the justificationplease point to the section(s) where related material for the question can be found.",
  "Answer: [Yes]": "Justification: We discuss the limitations in Section D.1 of the Appendix.Guidelines: The answer NA means that the paper has no limitation while the answer No means thatthe paper has limitations, but those are not discussed in the paper. The authors are encouraged to create a separate \"Limitations\" section in their paper. The paper should point out any strong assumptions and how robust the results are toviolations of these assumptions (e.g., independence assumptions, noiseless settings, modelwell-specification, asymptotic approximations only holding locally). The authors shouldreflect on how these assumptions might be violated in practice and what the implicationswould be. The authors should reflect on the scope of the claims made, e.g., if the approach was onlytested on a few datasets or with a few runs. In general, empirical results often depend onimplicit assumptions, which should be articulated. The authors should reflect on the factors that influence the performance of the approach.For example, a facial recognition algorithm may perform poorly when image resolution islow or images are taken in low lighting. Or a speech-to-text system might not be usedreliably to provide closed captions for online lectures because it fails to handle technicaljargon.",
  "If applicable, the authors should discuss possible limitations of their approach to addressproblems of privacy and fairness": "While the authors might fear that complete honesty about limitations might be used byreviewers as grounds for rejection, a worse outcome might be that reviewers discoverlimitations that arent acknowledged in the paper. The authors should use their bestjudgment and recognize that individual actions in favor of transparency play an importantrole in developing norms that preserve the integrity of the community. Reviewers will bespecifically instructed to not penalize honesty concerning limitations.",
  ". Theory Assumptions and Proofs": "Question: For each theoretical result, does the paper provide the full set of assumptions anda complete (and correct) proof?Answer: [NA]Justification: Our paper does not include theoretical results.Guidelines: The answer NA means that the paper does not include theoretical results. All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. All assumptions should be clearly stated or referenced in the statement of any theorems. The proofs can either appear in the main paper or the supplemental material, but if theyappear in the supplemental material, the authors are encouraged to provide a short proofsketch to provide intuition.",
  ". Experimental Result Reproducibility": "Question: Does the paper fully disclose all the information needed to reproduce the main ex-perimental results of the paper to the extent that it affects the main claims and/or conclusionsof the paper (regardless of whether the code and data are provided or not)?Answer: [Yes]Justification: We provide the complete code that runs correctly and hyperparameter con-figurations in the supplemental material and Section B.3 to ensure reproducibility andtransparency.Guidelines: The answer NA means that the paper does not include experiments. If the paper includes experiments, a No answer to this question will not be perceived wellby the reviewers: Making the paper reproducible is important, regardless of whether thecode and data are provided or not.",
  "If the contribution is a dataset and/or model, the authors should describe the steps taken tomake their results reproducible or verifiable": "Depending on the contribution, reproducibility can be accomplished in various ways.For example, if the contribution is a novel architecture, describing the architecture fullymight suffice, or if the contribution is a specific model and empirical evaluation, it may benecessary to either make it possible for others to replicate the model with the same dataset,or provide access to the model. In general. releasing code and data is often one goodway to accomplish this, but reproducibility can also be provided via detailed instructionsfor how to replicate the results, access to a hosted model (e.g., in the case of a largelanguage model), releasing of a model checkpoint, or other means that are appropriate tothe research performed. While NeurIPS does not require releasing code, the conference does require all submis-sions to provide some reasonable avenue for reproducibility, which may depend on thenature of the contribution. For example(a) If the contribution is primarily a new algorithm, the paper should make it clear how toreproduce that algorithm.",
  "(b) If the contribution is primarily a new model architecture, the paper should describethe architecture clearly and fully": "(c) If the contribution is a new model (e.g., a large language model), then there shouldeither be a way to access this model for reproducing the results or a way to reproducethe model (e.g., with an open-source dataset or instructions for how to construct thedataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authorsare welcome to describe the particular way they provide for reproducibility. In thecase of closed-source models, it may be that access to the model is limited in someway (e.g., to registered users), but it should be possible for other researchers to havesome path to reproducing or verifying the results.",
  ". Experimental Setting/Details": "Question: Does the paper specify all the training and test details (e.g., data splits, hyper-parameters, how they were chosen, type of optimizer, etc.) necessary to understand theresults?Answer: [Yes]Justification: We provide the data splits, hyperparameter configurations, and other experi-mental details in the supplemental material and Section B.3 to ensure reproducibility andtransparency.Guidelines: The answer NA means that the paper does not include experiments. The experimental setting should be presented in the core of the paper to a level of detailthat is necessary to appreciate the results and make sense of them.",
  ". Experiment Statistical Significance": "Question: Does the paper report error bars suitably and correctly defined or other appropriateinformation about the statistical significance of the experiments?Answer: [Yes]Justification: All of the methods are run for five times on the different random seeds withmean value and standard deviation reported on the testing data, as shown in .Guidelines: The answer NA means that the paper does not include experiments. The authors should answer \"Yes\" if the results are accompanied by error bars, confidenceintervals, or statistical significance tests, at least for the experiments that support the mainclaims of the paper. The factors of variability that the error bars are capturing should be clearly stated (forexample, train/test split, initialization, random drawing of some parameter, or overall runwith given experimental conditions).",
  "Guidelines:": "The answer NA means that there is no societal impact of the work performed. If the authors answer NA or No, they should explain why their work has no societal impactor why the paper does not address societal impact. Examples of negative societal impacts include potential malicious or unintended uses(e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g.,deployment of technologies that could make decisions that unfairly impact specific groups),privacy considerations, and security considerations. The conference expects that many papers will be foundational research and not tied toparticular applications, let alone deployments. However, if there is a direct path to anynegative applications, the authors should point it out. For example, it is legitimate to pointout that an improvement in the quality of generative models could be used to generatedeepfakes for disinformation. On the other hand, it is not needed to point out that ageneric algorithm for optimizing neural networks could enable people to train models thatgenerate Deepfakes faster. The authors should consider possible harms that could arise when the technology is beingused as intended and functioning correctly, harms that could arise when the technology isbeing used as intended but gives incorrect results, and harms following from (intentionalor unintentional) misuse of the technology. If there are negative societal impacts, the authors could also discuss possible mitigationstrategies (e.g., gated release of models, providing defenses in addition to attacks, mecha-nisms for monitoring misuse, mechanisms to monitor how a system learns from feedbackover time, improving the efficiency and accessibility of ML).",
  ". Safeguards": "Question: Does the paper describe safeguards that have been put in place for responsiblerelease of data or models that have a high risk for misuse (e.g., pretrained language models,image generators, or scraped datasets)?Answer: [NA]Justification: This paper poses no such risks.Guidelines: The answer NA means that the paper poses no such risks. Released models that have a high risk for misuse or dual-use should be released withnecessary safeguards to allow for controlled use of the model, for example by requiringthat users adhere to usage guidelines or restrictions to access the model or implementingsafety filters.",
  ". Licenses for existing assets": "Question: Are the creators or original owners of assets (e.g., code, data, models), used inthe paper, properly credited and are the license and terms of use explicitly mentioned andproperly respected?Answer: [Yes]Justification: We cite the original paper that produced the code package or dataset.Guidelines: The answer NA means that the paper does not use existing assets. The authors should cite the original paper that produced the code package or dataset. The authors should state which version of the asset is used and, if possible, include a URL. The name of the license (e.g., CC-BY 4.0) should be included for each asset. For scraped data from a particular source (e.g., website), the copyright and terms of serviceof that source should be provided. If assets are released, the license, copyright information, and terms of use in the packageshould be provided. For popular datasets, paperswithcode.com/datasets has curatedlicenses for some datasets. Their licensing guide can help determine the license of adataset.",
  ". New Assets": "Question: Are new assets introduced in the paper well documented and is the documentationprovided alongside the assets?Answer: [Yes]Justification: The assets we submitted have detailed documentation.Guidelines: The answer NA means that the paper does not release new assets. Researchers should communicate the details of the dataset/code/model as part of their sub-missions via structured templates. This includes details about training, license, limitations,etc.",
  ". Crowdsourcing and Research with Human Subjects": "Question: For crowdsourcing experiments and research with human subjects, does the paperinclude the full text of instructions given to participants and screenshots, if applicable, aswell as details about compensation (if any)?Answer: [NA]Justification: The paper does not involve crowdsourcing nor research with human subjects.Guidelines: The answer NA means that the paper does not involve crowdsourcing nor research withhuman subjects.",
  "According to the NeurIPS Code of Ethics, workers involved in data collection, curation, orother labor should be paid at least the minimum wage in the country of the data collector": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with HumanSubjectsQuestion: Does the paper describe potential risks incurred by study participants, whethersuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)approvals (or an equivalent approval/review based on the requirements of your country orinstitution) were obtained?Answer: [NA]Justification: The paper does not involve crowdsourcing nor research with human subjects.Guidelines: The answer NA means that the paper does not involve crowdsourcing nor research withhuman subjects. Depending on the country in which research is conducted, IRB approval (or equivalent)may be required for any human subjects research. If you obtained IRB approval, youshould clearly state this in the paper. We recognize that the procedures for this may vary significantly between institutionsand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and theguidelines for their institution."
}