{
  "Abstract": "Recent advancements in Multimodal Large Language Models (MLLMs) havegreatly improved their abilities in image understanding. However, these modelsoften struggle with grasping pixel-level semantic details, e.g., the keypoints ofan object. To bridge this gap, we introduce the novel challenge of SemanticKeypoint Comprehension, which aims to comprehend keypoints across differenttask scenarios, including keypoint semantic understanding, visual prompt-basedkeypoint detection, and textual prompt-based keypoint detection. Moreover, weintroduce KptLLM, a unified multimodal model that utilizes an identify-then-detectstrategy to effectively address these challenges. KptLLM underscores the initialdiscernment of semantics in keypoints, followed by the precise determinationof their positions through a chain-of-thought process. With several carefullydesigned modules, KptLLM adeptly handles various modality inputs, facilitatingthe interpretation of both semantic contents and keypoint locations. Our extensiveexperiments demonstrate KptLLMs superiority in various keypoint detectionbenchmarks and its unique semantic capabilities in interpreting keypoints.",
  "Introduction": "Recent advancements in deep learning and natural language processing have facilitated the riseof Large Language Models (LLMs) that display human-like fluency in text comprehension andgeneration . By incorporating visual information, researchers have developedMultimodal Large Language Models (MLLMs) , specifically designed for visual-language tasks, showcasing remarkable abilities in image understanding. However, these modelsencounter difficulties in capturing fine-grained semantic details, particularly at the point level,which are crucial for various real-world applications. The exploration of MLLMs for keypointcomprehension remains under-explored in the literature. Keypoint detection is a fundamental aspect of computer vision that supports various applications suchas controllable image/video generation , human-centric perception , and AR/VRsystems . Initially, research in this field focused on closed-set problems, aiming to predictthe locations of predefined semantic keypoints of a certain object category (e.g. human body). Asthe demand for generalization grew, researchers started investigating the detection of keypoints fornovel objects by providing visual prompts (i.e., a support image of a novel object with its keypointdefinitions) or utilizing textual prompts (i.e., keypoint names) . Despite significantprogress in these areas, existing models still fall short of achieving genuine semantic comprehension",
  "(c) Textual Prompt-based Keypoint Detection": ": This work aims to address the problem of semantic keypoint comprehension, which aims tounderstand keypoints across different task scenarios: (a) Keypoint Semantic Understanding takes theobject image and a keypoint prompt (i.e., the position of the target keypoint) as inputs, then generateresponses that interpret keypoint semantics; (b) Visual Prompt-based Keypoint Detection takes aquery image and a support image with a keypoint prompt as inputs and then outputs the correspondingkeypoint positions and semantics of the query image; (c) Textual Prompt-based Keypoint Detectionutilizes detailed descriptions of keypoints through extensive text, to perform more generalizablekeypoint detection. of keypoints akin to humans. These models primarily rely on direct learning of visual patterns forkeypoint localization through extensive data fitting, while neglecting semantic understanding of thekeypoints, thus leading to misinterpretation of the prompts and inaccurate predictions. Moreover,the input-output structures are designed in fixed and predefined formats, restricting their usage topredetermined methods and impeding the flexibility required for interfacing with users. Motivated by the aforementioned challenges, this paper delves into a more comprehensive problem ofSemantic Keypoint Comprehension to evaluate the model capability of comprehensively understandingkeypoints both visually and semantically. As shown in , we investigate three distinct capabilitiesvia different task instructions: (a) Keypoint Semantic Understanding aims to infer the desiredkeypoint semantics, given the target image and a keypoint prompt (i.e., the position of the targetkeypoint) as inputs. It provides the potential for an AI model with high-level visual understandingand analytical capabilities, crucial for tasks such as structural comprehension, action recognition,and medical image analysis. (b) Visual Prompt-based Keypoint Detection, also referred to ascategory-agnostic pose estimation, takes a query image and a labeled support image with the keypointannotation as inputs and then outputs the corresponding keypoint positions in the query image. Thiscapability requires the model to acquire keypoint definitions from visual prompts, enabling it toperform cross-class and cross-keypoint localization tasks using sample images provided by users.(c) Textual Prompt-based Keypoint Detection, also known as open-vocabulary keypoint detection,aims to utilize detailed descriptions of keypoints through extensive text for keypoint localization. Thekeypoint detectors directly receive the human language guidance, facilitating keypoint localization onarbitrary object and keypoint categories in a zero-shot manner. We introduce KptLLM, a novel framework that utilizes an identify-then-detect strategy to addressthe challenging problem of semantic keypoint comprehension. It formulates all three capabilitiesdepicted in , by first identifying the semantic meaning of keypoints and then detecting theirpositions via a chain-of-thought approach, akin to human cognition. KptLLM is a unified framework that comprises four key components designed to accommodate various modality inputs and inferboth the semantics and location of the keypoint. Specifically, we first extract visual features ofboth query and support images to obtain query visual tokens and support image features. Secondly,we encode the support keypoint prompt, which describes the position of keypoint on the supportimage, to generate keypoint prompt embedding. Thirdly, prompt-oriented features are derived byintegrating support image features with keypoint prompt embedding, and are utilized to form keypointprompt tokens. Lastly, LLMs take query visual tokens, keypoint prompt tokens, and task-relatedlanguage tokens as input, and then generate the semantic description of the target keypoint andits corresponding position on the query image. By harnessing commonsense knowledge in LLMs,KptLLM can assist in keypoint localization of novel object categories, potentially leading to enhancedgeneralizability in performance. In addition, the chain-of-thought design elicits the powerful keypointunderstanding capabilities of LLMs, which helps to distinguish visually ambiguous keypoints (e.g.left and right arms). Extensive experiments demonstrate KptLLMs superiority on semantic keypointcomprehension, showcasing its unique semantic understanding capabilities in interpreting keypointsand state-of-the-art performance in various keypoint detection benchmarks. In summary, the contributions of this work are three-fold: (1) We pioneer the investigation of a novelproblem in semantically interpretable keypoint analysis, termed Semantic Keypoint Comprehension,which aims to enhance MLLMs with improved image understanding at a finer-grained keypoint level;(2) We introduce KptLLM, a unified multimodal model that utilizes an identify-then-detect strategy toeffectively address three tasks of semantic keypoint comprehension. KptLLM underscores the initialdiscernment of semantic significance in keypoints, followed by the precise determination of theirpositions through a chain-of-thought process. (3) We demonstrate KptLLMs superiority in variousexisting keypoint detection benchmarks and its unique semantic capabilities in interpreting keypoints.We hope our work could inspire future research on keypoint understanding and localization, whilealso fostering enhanced human-AI interface in fine-grained visual understanding.",
  "Keypoint Detection": "Keypoint detection, also referred to as pose estimation, focuses on localizing the 2D keypoints ofobjects in the image. Traditional models for keypoint detection are typically designed for a singlecategory, e.g., human , animal and clothes . Based on thelocalization strategy, existing methods are generally divided into regression-based methods and heatmap-based methods . More recently,methods that can recognize and localize keypoints for unseen object categories in the training datasets,are gaining increasing attention from the community. Category-agnostic pose estimation ,also referred to few-shot keypoint detection, aims to estimate the pose of any category in queryimages with visual prompts (i.e., a few support images of a novel class and its corresponding keypointannotations). Another line of research explores open-vocabulary keypoint detection , whichaims to localize keypoints based on text prompts in zero-shot settings. In this work, we investigatesemantic keypoint comprehension and propose a novel unified framework to comprehend keypointsacross three different task scenarios, including (a) keypoint semantic understanding; (b) visualprompt-based keypoint detection; (c) textual prompt-based keypoint detection.",
  "Multimodal Large Language Model": "Inspired by the success of Large Language Models (LLMs) , researchers areexploring ways to transfer the formidable capabilities of LLMs into the realm of vision, developingMultimodal Large Language Models (MLLMs) . These models exemplify an autoregressive mechanism predicated on a transformer decoderarchitecture . The integration of visual representation from vision encoders into thedomain of LLMs ushers a new era of visual comprehension and reasoning. Such integration ispredominantly facilitated through a Multilayer Perceptron (MLP) that seamlessly transforms visualfeatures into the input embedding space of LLMs , or via a cross-attention mechanismthat attends to visual contents through a series of attention layers . However, most ofthese VLMs can only provide text outputs, inhibiting the complex applications requiring detailedvisual perception. VisionLLM tackles a range of conventional vision-centric tasks by instruction",
  "Linear": ": We introduce KptLLM, a unified framework designed to address three tasks of semantickeypoint comprehension: Keypoint Semantic Undertanding, which processes a support image Isand a support keypoint prompt x to generate responses that interpret the semantic information of thespecified keypoint; Visual Prompt-based Keypoint Detection aims to detect the correspondingkeypoint in the query image Iq based on the understanding of the support keypoint prompt; Textual Prompt-based Keypoint Detection leverages textual keypoint descriptions to directly inferthe corresponding keypoint positions in the query image. tuning LLMs. However, it may fall short of fully leveraging the comprehensive reasoning faculties ofLLMs. Kosmos-2 , Qwen-VL and DetGPT further exploit the power of LLMs to enableuser-guided detection. Moreover, GPT4RoI , Ferret , Shikra , and PerceptionGPT innovates by incorporating spatial boxes or masks as inputs and training with region-text pairs,offering region-level visual comprehension. Notably, a concurrent work, LocLLM , utilizesLLMs for human keypoint localization via textual description. In contrast, we take a step further byenabling LLMs to comprehend keypoints of various objects via multi-modal (e.g., textual or visual)prompts under different task formulations. This advancement not only broadens the utility of MLLMsfor keypoint detection but also enhances interpretive depth, allowing for a more comprehensiveunderstanding and grounding across a wider range of visual information.",
  "Methodology": "This section introduces our proposed unified framework, referred to as KptLLM, which effectivelyaddresses three semantic keypoint comprehension scenarios. As illustrated in , KptLLM acceptsmultiple images (query and support images) along with a support keypoint prompt (i.e., the positionof the target keypoint in the support image) and textual user instructions as the input. The outputcomprises both the response text and the desired keypoint position. Specifically, KptLLM comprisesfour key architectural components: (1) A visual encoder that extracts features from both query andsupport images (see Sec. 3.1); (2) A prompt encoder that converts support keypoint prompts intoprompt embeddings (see Sec. 3.2); (3) A prompt feature extractor that derives prompt-orientedfeatures from the corresponding image features (see Sec. 3.3); (4) A pre-trained LLM that processesmultimodal tokens for keypoint comprehension (see Sec. 3.4).",
  "Visual Encoder": "The Visual Encoder is designed to process two types of images in parallel: query and support images.Generally, it receives an input image I RHW 3 and generates a feature map F = V(I) Rhwd. Here, d represents the feature dimension, and h and w are the spatial dimensions obtainedby downsampling the original image dimensions H and W. Query Image. The query image represents the image that is to be analyzed. We extract its spatialfeatures through the vision encoder V, resulting in Fq. Following LLaVA , we apply a linearlayer to project Fq into language space: zq = Linear(Fq). As a result, query visual tokens alignedwith the LLM dimension are obtained and fed to the LLM. Support Image. The support image serves as a reference example. We extract its spatial features,which are represented as Fs. Unlike the query image features, Fs is not directly input into LLM.Instead, it is processed by the prompt feature extractor to derive prompt-oriented features.",
  "Prompt Encoder": "In addition to processing images, we need to incorporate an additional prompt consisting of 2Dcoordinates x R2, which describes the keypoint location within the image. Inspired by SAM ,we introduce a prompt encoder to adapt this prompt input to be aligned with the image feature spaceF. The prompt encoder encodes the keypoint coordinates using a sine-cosine position embedding(PE), followed by a Multi-Layer Perceptron (MLP):Fp = MLP(PE(x)).(1)",
  "Prompt Feature Extractor": "The Prompt Feature Extractor is designed to extract the prompt-specific features from image features.As illustrated in , the semantics of the keypoint prompt directly correspond to the supportimage. We initialize the prompt feature extractor with a two-layer transformer that incorporates thecross-attention mechanism (CrossAttnLayers). This mechanism employs Fp as the query and Fsas the key and value to extract keypoint-specific visual features indicated by the prompt:zp = CrossAttnLayers(Fp, Fs),(2) where zp denotes the keypoint-specific visual features. In essence, compared with average pooling-based feature extraction method , the prompt feature extractor is trainable and capable ofincorporating global image features to enhance keypoint identification. This is particularly beneficialfor distinguishing mirror-symmetric keypoints, such as the left and right eyes, which can be highlyambiguous when relying solely on local image features. Our ablation study demonstrates theperformance improvements achieved through the utilization of this component.",
  "Multimodal LLM for Keypoint Comprehension": "Given a query image and an optional prompt specifying the keypoint of interest, our goal is togenerate textual descriptions and keypoint locations that convey fine-grained keypoint informationwithin the image. Recognizing the exceptional ability of LLMs in handling multimodal tokens fordifferent perception tasks , we further leverage LLM for keypoint comprehension,which could effectively process various inputs: (1) the visual tokens zq of the query image, (2) theprompt tokens zp, and (3) a sequence of language tokens t, which depend on the three semantickeypoint comprehension scenarios. Keypoint Semantic Decoding. We design the model to directly generate textual descriptions thatinterpret keypoint semantics, following the standard approach used by LLMs for text generation.Generally, the architecture of an LLM typically comprises Transformer layers (TransformerLayers)followed by a final Feed Forward Network (FFN). The latent embedding u, which captures the fusedmultimodal information, can be computed as:u = TransformerLayers([zq, zp, t]),(3) where [zq, zp, t] denotes the concatenation of the visual, prompt, and language tokens. This embed-ding u is then passed through the FFN and a Softmax function to generate the probability distributionp over the vocabulary for the next token:p = Softmax(FFN(u)).(4) Keypoint Position Decoding. Inspired by , we introduce a special token, <keypoint>, intothe vocabulary. Consequently, the 2D keypoint position y can be computed from the output latentembedding ukpt of the special token using another FFN prediction head:",
  "Training and Inference Details": "To retain the learned general knowledge of the pre-trained LLM, we employ LoRA for efficientfine-tuning of LLM, while fully fine-tuning other modules of the framework. The training andinference processes for different tasks are outlined below. Keypoint Semantic Understanding. As shown in -(a), this task focuses on extracting semantictextual information associated with specific keypoints within an image. The training objective is tominimize the language modeling loss, computed as the cross-entropy loss over the vocabulary of theLLMs tokenizer. Specifically, the loss function is defined as:",
  "L = Llm(a, a),(6)": "where a is the text response predicted by the model, and a is the ground-truth text response. Duringinference, given an image provided by the user and the corresponding keypoint position as a prompt,our model comprehends and generates the semantic meaning of the specified keypoint. Visual Prompt-based Keypoint Detection. This task involves simultaneously comprehending thesemantics of keypoints, generating textual descriptions of this understanding, and precisely localizingthe keypoint coordinates, as shown in -(b). The overall training function further incorporatesthe L1 loss for keypoint regression:",
  "L = y y + Llm(a, a),(7)": "where y is the ground-truth keypoint position, and is the loss weight that balances the learningof keypoint regression and text generation ( = 2 in our implementation). During inference, theuser provides two images: one as the query image for testing and the other as the support imagefor reference. Additionally, the keypoint definition for the support image should be provided as thesupport keypoint prompt. Our model then comprehends the semantics of the desired keypoint anddetects its corresponding position in the query image. Textual Prompt-based Keypoint Detection. As illustrated in -(c), this task aims to accuratelylocalize keypoints based on the detailed keypoint descriptions. The loss function aligns with thatof the visual prompt-based keypoint detection. During inference, users have the option to providedetailed descriptions of the desired keypoints or simply the keypoint names, based on which ourmodel can detect the corresponding keypoints.",
  "Datasets": "In our experiments, we employ two datasets to evaluate the semantic keypoint comprehension in threescenarios: (1) The MP-100 dataset for both Keypoint Semantic Undertanding and Visual Prompt-based Keypoint Detection: This dataset is a pioneering dataset for category-agnostic pose estimation,which encompasses 100 different object categories with over 20,000 instances. The number ofkeypoints varies across categories, ranging from 8 to 68. Following the protocols established byPOMNet , the dataset is divided into five distinct splits to ensure comprehensive coverage acrossdifferent model training and validation scenarios. Each split contains all 100 categories, with 70 fortraining, 10 for validation, and 20 for testing. The splits are carefully designed to avoid categoryoverlap, maintaining the independence and integrity of training and testing scenarios. (2) The AP-10K dataset for Textual Prompt-based Keypoint Detection: The dataset comprises23 animal families and 54 species, totaling 10,015 images. Each image is annotated with 17 keypoints,including two eyes, one nose, one neck, two shoulders, two elbows, two knees, two hips, four paws,and one tail. We follow CLAMP to assess the models ability to generalize to previously unseenanimal species within a zero-shot learning paradigm. We establish two experimental scenarios basedon the taxonomic relationship between the species in the training and test setsspecifically, whetherthey belong to the same animal order. Species within the same order typically share similar visualcharacteristics, whereas those from different orders exhibit greater diversity in appearance. Thesescenarios enable us to assess how different methods perform when generalizing to unseen speciesunder varying conditions. Following CLAMP, we assign Bovidae and Canidae as the training and",
  "Evaluation & Metrics": "Different evaluation methods and metrics are used for different tasks of semantic keypoint compre-hension. (1) Keypoint Semantic Undertanding: We use the MP-100 dataset (Split-1), with thekeypoint semantic labels adopted from X-Pose . Some keypoints are excluded from the evaluationdue to ambiguity or inadequacy in their descriptions, such as those used to describe the collar in theclothing category. By aggregating the results of tested keypoints, we derive corresponding accuracyrates (%). (2) Visual Prompt-based Keypoint Detection: We employ the Probability of CorrectKeypoint (PCK) metric, which is the standard evaluation measure for this task. Consistent withPOMNet , we uniformly set the PCK threshold to 0.2 across all categories. Additionally, wecompute and report the average PCK over all five data splits to provide a comprehensive indication ofthe models overall effectiveness. (3) Textual Prompt-based Keypoint Detection: Following CLAMP,we employ average precision (AP) as the primary metric for AP-10K. This metric is computed basedon the object keypoint similarity (OKS). For detailed protocol definitions, please refer to .",
  "Implementation Details": "Architecture. We utilize LLaVA-V1.5-7B as our base model, which incorporates the ViT-basedvisual encoder of CLIP for image encoding and Vicuna-7B (fine-tuned from Llama-2) as the LLMbackbone. We employ LoRA for efficient fine-tuning LLM. Instead, all other modules, including thevisual encoder, prompt encoder, prompt feature extractor, and a series of linear layers and feed forwardnetworks, undergo full fine-tuning. The input image only contains a single object of interest, croppedaccording to the ground-truth bounding box and resized to 336336, consistent with CLIP-ViT-L. Training Details. LoRA parameters are configured with a rank of 128 and an alpha of 256. Optimiza-tion is conducted using AdamW, with a learning rate of 2e4 and weight decay of 0. We utilize 8NVIDIA A100-80G GPUs for training, and use the DeepSpeed engine to enhance training efficiency.Each GPU operates with a batch size of 16, and we employ a gradient accumulation step of 1.",
  "LLaVA 3%LLaVA 72%KptLLM83%": "As depicted in Tab. 1, we present the accuracy for key-point semantic understanding on MP-100 Split-1 set. To facilitate a comprehensive comparison, wehighlight the keypoint area in the image and feed theprocessed image, along with the task instruction, intoLLaVA . We report the performance of both theoriginal LLaVA model and a version fine-tuned on theMP-100 dataset. The original LLaVA performs notablypoorly in grasping keypoint semantics, indicating theinadequacy of traditional multimodal large language models in capturing fine-grained semantic details.Conversely, the fine-tuned LLaVA demonstrates significantly enhanced performance, thereby validat-ing the efficacy of our training pipeline. Furthermore, our KptLLM surpasses the fine-tuned LLaVAby a substantial margin, particularly in terms of keypoint accuracy (83% vs 72%). It demonstratesthe effectiveness of our keypoint prompt token in guiding attention to the fine-grained keypoint area.",
  "Visual Prompt-based Keypoint Detection": "1-shot & 5-shot Evaluation. We compare our method with the previous visual prompt-based methodsProtoNet , MAML , Fine-tune , POMNet , and CapeFormer . Tab. 2 presents thePCK results of different approaches on the MP-100 dataset under both 1-shot and 5-shot settings.Compared with previous methods, KptLLM showcases the potential of MLLM in detecting keypointsthrough the use of visual prompts, consistently outperforming across all settings and data splits. Moreimportantly, we integrate keypoint semantic understanding into the output response, introducingnovel functionalities for comprehending the semantic aspects of support image keypoints. Cross Super Category Evaluation. To thoroughly assess generalization across markedly differentcategories, we conduct a cross-supercategory evaluation following the protocol of POMNet .While the MP-100 dataset ensures that training, validation, and test categories are non-overlapping,some categories may still exhibit similar features, e.g., body characteristics commonly shared amongdifferent quadruped animals. To address this, we designate four supercategorieshuman face, humanbody, vehicle, and furniturefrom the MP-100 dataset as test categories. The remaining categoriesare utilized for training, allowing us to better evaluate the models ability to generalize acrosssignificantly diverse categories. As shown in Tab. 3, KptLLM consistently outperforms previousmethods, highlighting the robustness and excellent generalization ability of our proposed method. Qualitative Results. For the visual prompt-based keypoint detection task, the input necessitates asupport image of the object to be tested, as well as keypoint positions that represent the definitions ofthose keypoints. In this study, we examine how the visual disparity between support images and queryimages affects the models performance. As depicted in , our model is capable of effectivelydetecting keypoints in various query images when provided with the same support image and itscorresponding keypoints. This effectiveness is maintained even in the presence of differences inobject poses, appearances, and environmental conditions.",
  "Textual Prompt-based Keypoint Detection": "The results are presented in Tab. 4. Compared to previous textual prompt-based model-CLAMP ,KptLLM achieves superior cross-species generalization. Specifically, our model demonstrates a 15.3average precision (AP) improvement in the different order setting and a 21.8 AP increase in the sameorder setting. Notably, our model performs better in the same order setting, where species often sharesimilar visual characteristics. Overall, we show that leveraging detailed keypoint descriptions throughcomprehensive text, combined with commonsense knowledge from LLMs, effectively enhancesgeneralizable performance in keypoint localization.",
  "Ablation Study": "In this subsection, we perform ablation study on the design choices of our model. The experimentsare conducted on the visual prompt-based keypoint detection task using the MP-100 Split-1 settingwith the PCK metric reported. Indentify-then-Detect (ItD) Strategy. KptLLM follows the identify-then-detect paradigm, wherethe model learns to first interpret the semantic information of the keypoint to be detected, and thenpredict the precise location of the keypoint. In Tab. 5, we validate the effectiveness of our ItD strategy.We observe notable enhancement, which can be attributed to the inter-task synergy that arises fromthe ItD mechanism. Prompt Feature Extractor. In Tab. 6, we compare our prompt feature extractor (Sec. 3.3) withthe average pooling based feature extraction method . The results show that our prompt featureextractor significantly outperforms the baseline method (91.66 vs 89.78), which validates the efficacyof our prompt feature extractor in enhancing focus on fine-grained keypoint areas. Combining Visual and Textual Prompts. In Tab. 7, rather than relying solely on the visualprompt for localization, we further incorporate the textual prompt to demonstrate the effect of thiscombination. The improved results indicate that the textual prompt could provide valuable high-leveland semantically rich guidance, enhancing keypoint localization.",
  "Conclusion": "This paper introduces the novel challenge of Semantic Keypoint Comprehension, which aims tocomprehend keypoints across different task scenarios. To address this challenge, we present KptLLM,a novel and unified multimodal large language model designed to adeptly process various modalityinputs, facilitating the interpretation of both semantic contents and keypoint locations. Extensiveexperiments show the superiority of our model in three different tasks for comprehending keypoints,including keypoint semantic understanding, visual prompt-based keypoint detection, and textualprompt-based keypoint detection. We hope this work can open up new possibilities for more fine-grained multimodal vision-language understanding and provide valuable insights for future research.",
  "Discussion": "Limitations. (1) A major limitation of our work is the models size and computational efficiency,which is a common challenge for MLLMs compared to traditional vision models. However, this isacceptable because, as a pioneering effort in utilizing LLMs for keypoint comprehension, our maincontribution is demonstrating the potential of LLMs to understand and locate pixel-level details atkeypoints. (2) Additionally, the datasets used in our experiments have constraints in the diversity ofobject and keypoint categories for both training and testing. This highlights the need to expand thesedatasets to validate the models applicability in more diverse, real-world scenarios. Future Work. (1) Improving the Capacity of the Vision Encoder: Our work follows LLaVA by employing a CLIP-based ViT as the vision encoder. However, some studies have demon-strated that stronger vision encoders can lead to more significant improvements, e.g., DINOv2 . (2)Refining Keypoint Decoding Strategy: Inspired by previous MLLMs for perception tasks ,we introduce a special token <keypoint> into the models vocabulary. When the model generatesthis <keypoint> token, its hidden embedding is decoded to the corresponding keypoint position.Although this strategy has shown promising results, it remains sub-optimal for user interaction. Amore direct approach is to output the keypoint coordinates as textual descriptions. However, traininga model to express numerical values in text using cross-entropy loss is challenging because slightdeviations in numerical values can lead to significant differences in the generated text. Therefore,it intuitively requires more data for effective training. (3) Expanding Data Scale and CategoryDiversity: The datasets used in our experiments follow standard benchmarks. However, both theMP-100 dataset for visual prompt-based keypoint detection and the AP-10K dataset fortextual prompt-based keypoint detection contain only a small amount of data, which limits the modelsgeneralization performance. Furthermore, the limited diversity of object and keypoint categoriesgreatly reduces the models applicability, making it insufficient for handling open-world scenarios. Apromising direction is to leverage large-scale keypoint datasets for training, such as UniKPT ,which could further explore the upper bounds of MLLMs for keypoint comprehension. Broader Impact. The study aims to enhance MLLMs for understanding images at a more granularkeypoint level. We also propose a new challenge of keypoint semantic understanding, which holdspromise for benefiting tasks such as structural understanding, action recognition, and medical imageanalysis. Nevertheless, recognizing the potential negative impacts that are common to many MLLMs,our model also carries risks, including the amplification of societal biases and concerns regardingprivacy and ethics. To address these issues, we are committed to implementing safeguards, includingstrict access controls and the establishment of clear usage policies and agreements.",
  "Acknowledgements": "The work is partially supported by the Young Scientists Fund of the National Natural ScienceFoundation of China under grant No.62106154, by the Natural Science Foundation of GuangdongProvince, China (General Program) under grant No.2022A1515011524, and by Shenzhen Science andTechnology Program JCYJ20220818103001002, and by the Guangdong Provincial Key Laboratoryof Big Data Computing, The Chinese University of Hong Kong (Shenzhen). Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models arefew-shot learners. Advances in neural information processing systems, 33:18771901, 2020. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia LeoniAleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4technical report. arXiv preprint arXiv:2303.08774, 2023. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-the Lacroix, Baptiste Rozire, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Openand efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Openfoundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, AdamRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1113, 2023.",
  "Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuningwith gpt-4. arXiv preprint arXiv:2304.03277, 2023": "Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, LaurentSifre, Morgane Rivire, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open models based ongemini research and technology. arXiv preprint arXiv:2403.08295, 2024. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-imagepre-training with frozen image encoders and large language models. In International conferenceon machine learning, pages 1973019742. PMLR, 2023. Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, WeishengWang, Boyang Li, Pascale N Fung, and Steven Hoi. Instructblip: Towards general-purposevision-language models with instruction tuning. Advances in Neural Information ProcessingSystems, 36, 2024.",
  "Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Fanyi Pu, Jingkang Yang, ChunyuanLi, and Ziwei Liu. Mimic-it: Multi-modal in-context instruction tuning. arXiv preprintarXiv:2306.05425, 2023": "Jianzhu Guo, Dingyun Zhang, Xiaoqiang Liu, Zhizhou Zhong, Yuan Zhang, Pengfei Wan, andDi Zhang. Liveportrait: Efficient portrait animation with stitching and retargeting control. arXivpreprint arXiv:2407.03168, 2024. Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-imagediffusion models. In Proceedings of the IEEE/CVF International Conference on ComputerVision, pages 38363847, 2023. Xuan Ju, Ailing Zeng, Chenchen Zhao, Jianan Wang, Lei Zhang, and Qiang Xu. Humansd:A native skeleton-guided diffusion model for human image generation. In Proceedings of theIEEE/CVF International Conference on Computer Vision, pages 1598815998, 2023. Jie Yang, Chaoqun Wang, Zhen Li, Junle Wang, and Ruimao Zhang. Semantic human parsingvia scalable semantic transfer over multiple label domains. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition, pages 1942419433, 2023.",
  "Shunlin Lu, Ling-Hao Chen, Ailing Zeng, Jing Lin, Ruimao Zhang, Lei Zhang, and Heung-Yeung Shum. Humantomato: Text-aligned whole-body motion generation. arXiv preprintarXiv:2310.12978, 2023": "Lumin Xu, Sheng Jin, Wang Zeng, Wentao Liu, Chen Qian, Wanli Ouyang, Ping Luo, andXiaogang Wang. Pose for everything: Towards category-agnostic pose estimation. In Europeanconference on computer vision, pages 398416. Springer, 2022. Min Shi, Zihao Huang, Xianzheng Ma, Xiaowei Hu, and Zhiguo Cao. Matching is not enough:A two-stage framework for category-agnostic pose estimation. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition, pages 73087317, 2023. Xu Zhang, Wen Wang, Zhe Chen, Yufei Xu, Jing Zhang, and Dacheng Tao. Clamp: Prompt-based contrastive learning for connecting language and animal pose. In Proceedings of theIEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2327223281, 2023.",
  "Hang Yu, Yufei Xu, Jing Zhang, Wei Zhao, Ziyu Guan, and Dacheng Tao. Ap-10k: A benchmarkfor animal pose estimation in the wild. arXiv preprint arXiv:2108.12617, 2021": "Muhammad Haris Khan, John McDonagh, Salman Khan, Muhammad Shahabuddin, AdityaArora, Fahad Shahbaz Khan, Ling Shao, and Georgios Tzimiropoulos. Animalweb: A large-scale hierarchical dataset of annotated animal faces. In IEEE Conf. Comput. Vis. Pattern Recog.,2020. Yuying Ge, Ruimao Zhang, Xiaogang Wang, Xiaoou Tang, and Ping Luo. Deepfashion2:A versatile benchmark for detection, pose estimation, segmentation and re-identification ofclothing images. In IEEE Conf. Comput. Vis. Pattern Recog., 2019.",
  "Jie Yang, Ailing Zeng, Ruimao Zhang, and Lei Zhang. Unipose: Detecting any keypoints. arXivpreprint arXiv:2310.08530, 2023": "Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen,Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trainedtransformer language models. arXiv preprint arXiv:2205.01068, 2022. Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra SinghChaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, LucileSaulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson,Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visuallanguage model for few-shot learning. Advances in neural information processing systems,35:2371623736, 2022.",
  "Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee.Llava-next: Improved reasoning, ocr, and world knowledge, January 2024": "Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz,Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models. arXivpreprint arXiv:2312.07533, 2023. Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, PhilippDufter, Dhruti Shah, Xianzhi Du, Futang Peng, Floris Weers, et al. Mm1: Methods, analysis &insights from multimodal llm pre-training. arXiv preprint arXiv:2403.09611, 2024. Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, TongzhengRen, Zhuoshu Li, Hao Yang, Yaofeng Sun, Chengqi Deng, Hanwei Xu, Zhenda Xie, and ChongRuan. Deepseek-vl: Towards real-world vision-language understanding, 2024. Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu,Shaoteng Liu, and Jiaya Jia. Mini-gemini: Mining the potential of multi-modality visionlanguage models. arXiv:2403.18814, 2023.",
  "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Adv. Neural Inform. Process.Syst., 2017": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visualmodels from natural language supervision. In Int. Conf. Mach. Learn., 2021. Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss forlanguage image pre-training. In Proceedings of the IEEE/CVF International Conference onComputer Vision, pages 1197511986, 2023. Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo,Tong Lu, Jie Zhou, Yu Qiao, et al. Visionllm: Large language model is also an open-endeddecoder for vision-centric tasks. Advances in Neural Information Processing Systems, 36, 2024.",
  "Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and FuruWei. Kosmos-2: Grounding multimodal large language models to the world. arXiv preprintarXiv:2306.14824, 2023": "Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, ChangZhou, and Jingren Zhou. Qwen-vl: A frontier large vision-language model with versatileabilities. arXiv preprint arXiv:2308.12966, 2023. Renjie Pi, Jiahui Gao, Shizhe Diao, Rui Pan, Hanze Dong, Jipeng Zhang, Lewei Yao, JianhuaHan, Hang Xu, and Lingpeng Kong Tong Zhang. Detgpt: Detect what you need via reasoning.arXiv preprint arXiv:2305.14167, 2023. Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi Shao, Wenwei Zhang, Kai Chen,and Ping Luo. Gpt4roi: Instruction tuning large language model on region-of-interest. arXivpreprint arXiv:2307.03601, 2023. Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, LiangliangCao, Shih-Fu Chang, and Yinfei Yang. Ferret: Refer and ground anything anywhere at anygranularity. arXiv preprint arXiv:2310.07704, 2023.",
  "Renjie Pi, Lewei Yao, Jiahui Gao, Jipeng Zhang, and Tong Zhang. Perceptiongpt: Effectivelyfusing visual perception into llm. arXiv preprint arXiv:2311.06612, 2023": "Dongkai Wang, Shiyu Xuan, and Shiliang Zhang. Locllm: Exploiting generalizable humankeypoint localization via large language model. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 614623, 2024. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson,Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. InProceedings of the IEEE/CVF International Conference on Computer Vision, pages 40154026,2023."
}