{
  "Abstract": "Recent advances in text-to-image generation have enabled the creation of high-quality images with diverse applications. However, accurately describing desiredvisual attributes can be challenging, especially for non-experts in art andphotography. An intuitive solution involves adopting favorable attributes from thesource images. Current methods attempt to distill identity and style from sourceimages. However, \"style\" is a broad concept that includes texture, color, andartistic elements, but does not cover other important attributes such as lightingand dynamics. Additionally, a simplified \"style\" adaptation prevents combiningmultiple attributes from different sources into one generated image. In this work,we formulate a more effective approach to decompose the aesthetics of a picture intospecific visual attributes, allowing users to apply characteristics such as lighting,texture, and dynamics from different images. To achieve this goal, we constructedthe first fine-grained visual attributes dataset (FiVA) to the best of our knowledge.This FiVA dataset features a well-organized taxonomy for visual attributes andincludes around 1 M high-quality generated images with visual attribute annotations.Leveraging this dataset, we propose a fine-grained visual attribute adaptationframework (FiVA-Adapter), which decouples and adapts visual attributes fromone or more source images into a generated one. This approach enhances user-friendly customization, allowing users to selectively apply desired attributes tocreate images that meet their unique preferences and specific content requirements.",
  "Introduction": "Imagine an artist drawing a picture; he or she not only exhibits unique styles, identities, andspatial structures but also frequently integrates personal elements such as brushstrokes, composition,and lighting effects into their creations. These detailed visual features capture profound personalemotions and artistic expressions. However, despite the capability of current text-to-image models togenerate high-quality images from textual or visual prompts, they encounter substantial challenges ineffectively controlling these fine-grained visual concepts, which vary widely across different artisticdomains. This limits the practical applications of text-to-image models in various fields. Recently, significant research efforts have been made to advance controllable image generation. Inparticular, numerous studies have explored the use of personalization techniques to preserve theidentity of an object or person across different scenarios . On the other hand, there havebeen attempts to control the generation process by conditioning on the style and spatial structure ofreference images , which involves mimicking abstract styles or leveraging spatialcues such as edge maps, semantic masks, or depth. Yet, these methods are limited to specific aspects",
  ": Overview. We propose the FiVA dataset and adapter to learn fine-grained visual attributesfor better controllable image generation": "and fall short in terms of generalizability. These methods use complex images as unified referenceswithout adequately disentangling visual attributes, resulting in a lack of control over generationbased on specific attributes of the conditional images. This underscores the importance of extractingfine-grained visual concepts from images to achieve controllable generation. However, achievingthis requires a well-annotated image dataset that displays various fine-grained attributes within theimages and a unified framework to adapt these different attributes to facilitate generation. In this paper, we present a comprehensive fine-grained visual attributes dataset (FiVA), featuringimage pairs that meticulously delineate a variety of visual attributes, as shown in . Insteadof annotating the real-world images, we propose to leverage advanced 2D generative models withinan automated data generation pipeline for data collection. We develop a systematic approach thatincludes attribute and subject definition, prompt creation, LLM-based filtering, and human validationto construct a dataset annotated with diverse visual attributes across around 1 M images. Each imagecan further be split into 2x2 sub-images generated from different seeds, available for sampling duringtraining. Building on this dataset, we introduce a fine-grained visual attributes adaptation framework(FiVA-Adapter) designed to control the fine-grained visual attributes during the generation process.Specifically, we propose to integrate a multimodal encoder, Q-former, into the image feature encoderbefore its insertion into the cross-attention modules. This integration aids in understanding tagsor brief instructions for extracting image information. During inference, our approach allows forthe isolation of specific visual attributes from the reference image before applying them to a targetsubject, and even the combination of different attribute types. This capability enables free and diverseuser choices, significantly enhancing the adaptability and applicability of our method. We conduct extensive experiments across a variety of attribute types on both synthetic and real-worldtest sets. Our results demonstrate that our method outperforms baseline methods in terms of precisecontrollability in attribute extraction, high textual alignment regarding the target prompt, and theflexibility to combine different attributes. This work aims to cater to an increasingly diverse array ofuser needs, recognizing the rich informational content that visual media embodies. Our hope is thatit will pave the way for innovative applications that harness the full potential of visual attributes inmyriad contexts.",
  "Related Words": "Image Generation with Diffusion Models Diffusion models have seen significant advancementsin the field of image generation, driven by their impressive generative capabilities. DDPM employs an iterative denoising process to transform Gaussian noise into images. The Latent DiffusionModel enhances traditional DDPMs by employing score-matching in the images latent spaceand introducing cross-attention-based controls. Rectified flow improves training stability andenhances image synthesis by introducing flow matching. Another line of research focuses onarchitectural variants of diffusion models. The Diffusion Transformer and its variants replace the U-Net backbone with transformers to increase scalability on large datasets. Controllable Image Generation The ambiguity of relying solely on text conditioning often resultsin weak control for image diffusion models. To enhance guidance, some works adopt the scenegraph as an abstract condition signal to control the visual content. To allow for more spatial control,several attempts such as ControlNet and IP-Adapter propose to build adaptors for visualgeneration by incorporating additional encoding layers, thus facilitating controlled generation undervarious conditions such as pose, edge, and depth. Further research investigates image generationunder specified image conditions. Techniques like ObjectStitch , Paint-by-Example , andAnyDoor leverage the CLIP model and propose diffusion-based image editing methodsconditioned on images. AnimateAnyone suggests using a reference network to incorporate skeletalstructures for image animation. However, these methods primarily focus on the visual attributeslike identity and spatial structure of reference images. Other works such as DreamBooth ,CustomDiffusion , StyleDrop , and StyleAligned approach controlled generation byenforcing consistent style between reference and generated images. Yet even among these methods,the definition of style is inconsistent. We propose a fine-grained visual attribute adapter that decouplesand adapts visual attributes from one or more source images into generated images, enhancing thespecificity and applicability of the generated content. Datasets for Image Generation Datasets provide a foundation for the rapid progress of diffusionmodels in image generation. MS-COCO and Visual Genome , built with human annotation,are limited in scale to 0.3 and 5 million samples, respectively. The YFCC-100M datasetscales up further, containing 99 million images sourced from the web with user-generated metadata.However, the noisy labels from the web often result in text annotations that are unrelated to theactual image content. The CC3M and its variant CC12M utilize web-collected images andalt-text, undergoing additional cleaning to enhance data quality. LAION-5B further scales up thedataset to 5 billion samples for the research community, aiding in the development of foundationalmodels. DiffusionDB leverages large-scale pre-trained image generation models to create asynthetic prompt-image dataset, containing imaged generated by Stable Diffusion using prompts andhyperparameters specified by real users. However, these datasets only provide a coarse descriptionof the images. DreamBooth and CustomDiffusion , techniques focused on text-to-imagecustomization, also provide their own dataset, containing sets of images capturing the same concept.However, such datasets focus only on individual concepts, and are limited in sample volume. Thus,we propose the fine-grained visual attributes dataset (FiVA) with high-quality attribute annotations tofacilitate precise control of generative models with specific attribute instructions from users.",
  "Dataset Overview": "Visual attributes encompass the distinctive features present in photography or art. However, thereexists no dataset annotated with fine-grained visual attributes, making the fine-grained control ingenerative models inapplicable. In the following, we present the details for how to construct theFiVA dataset with fine-grained visual attributes annotation. We first introduce our taxonomy for thefine-grained visual attributes. We then illustrate how to create large-scale text prompts with diverseattributes, which are further used for generating image data pairs using state-of-the-art text-to-imagemodels. Considering the imperfect text-image alignment of current generative models, a data filteringstage is implemented to further refine the precision. Additionally, we conduct thorough humanvalidation to ensure the quality of the dataset. We perform a comprehensive statistical analysis of thecompiled dataset in the supplementary material.",
  "The attribute shares across all subjects.The attribute is range-sensitive": ": Examples of visual consistency application range. Some visual attributes, such as colorand stroke, are easily transferable across different subjects (left). However, other attributes, likelighting and dynamics, are range-sensitive, meaning they produce varying visual effects dependingon the domain (right), resulting in more fine-grained, subject-specific definitions of sub-attributes. : Statistics and human validation results. We report the number of individual imagescontaining each specific visual attribute (some images could contain multiple attributes), along withhuman validation accuracy and cross-agreement measured by standard deviation.",
  "Data Construction Pipeline": "Taxonomy of Visual Attributes. Visual attributes are a broad concept, varying significantlyin different use cases. Therefore, we choose some of the most general types to cover a widerange of application scenarios.Specifically, we categorize these attributes into the followinggroups: color, lighting, focus and depth of field, artistic stroke, dynamics,rhythm, and design. Within each group, we further refine them into comprehensive subcategories.Initially, we reference examples from professional texts and then use GPT-4 to generate additionalentries. We manually filter out any redundant or unreasonable suggestions. (Please refer to thesupplementary material for more details). This comprehensive taxonomy enables us to encompass awide range of applications in photography, art, and design, ensuring that our dataset is versatile andapplicable across various visual domains. Prompts and Paired Images Generation.We aim to construct image-image pairs with thesame specific visual attribute. However, achieving this by filtering existing text-image datasets,LAION5B , etc. is very challenging due to the ambiguity in textual descriptions, which makes itdifficult to accurately select the image pairs from the datasets. Therefore, we opt to construct thedataset using generative models: first, we generate prompts containing various visual attributes inbulk, and then we use state-of-the-art text-to-image models to generate images with these prompts. For prompt generation, we augment the attribute names and descriptions with GPT-4 based on ourtaxonomy. Then, we combine each of the augment names or their combinations with specific objectsto generate the final prompts. However, this process is non-trivial, as a specific branch of visualattributes may not be suitable for all kinds of random subjects. For example, motion blur can onlybe applied to dynamic subjects, and candle light cannot be applied to landscapes. We employGPT-4 to construct a hierarchical subject tree, with parent nodes representing the primary categoriesof subjects and child nodes denoting specific objects within these main categories. We also utilizeGPT-4 to associate visual attributes with the parent nodes in the subject tree to make the combinationof them into a reasonable case. When creating prompts, we select n (e.g., 1, 2, 3) attributes along withone associated subject according to the subject tree. We construct over 1.5 million initial prompts intotal and used playground-v2.5 to generate four images for each prompt. The generated imageswith the same attribute are considered to be positive pairs. However, we observe some disparitycaused by the imperfect alignment with text as well as the different manifestations of the same",
  "attribute on different subjects. A further cleaning and filtering process is therefore needed to enhancequality and precision": "Range-sensitive Data Filtering. Not all generated images with text prompts containing the sameattribute exhibit similar visual effects. illustrates examples of varying application ranges fordifferent attributes where visual consistency is present. To achieve attribute-consistent image pairs,we define specific ranges for each attribute within which two images are highly likely to maintainvisual consistency. Utilizing our subject tree, we apply a range-sensitive data filtering approachhierarchically: if an attribute shows high visual consistency across subjects at the parent node, werecord it as the range; if results are inconsistent, we proceed to its child nodes for further validation.The validation process involves constructing a 3 3 image grid generated with prompts containingthe attribute and subjects sampled from a specific node. Using GPT-4, we assess image consistencyfor a given visual attribute by prompting: I have a set of images and need to determine if they exhibitconsistent <major attribute> traits of <specific attribute>. We ask GPT-4 to identify any inconsistentimage IDs. This sampling is repeated multiple times, and if the proportion of inconsistent imagesremains below a predefined threshold, we consider the images consistent for the specified visualattribute within the given range.For attributes that cannot maintain consistency even at the leaf nodes,we simply remove them. After filtering, we further compile statistics on the number of individual images for each major visualattribute, as shown in . Note that these numbers do not add up to the total image count, aseach image can possess one or multiple different attributes.",
  "Human Validation": "After filtering the data, we randomly selected 1,400 image pairs (200 per attribute) for humanvalidation. Each pair was evaluated by three of eight trained annotators for attribute similarity, withthe final result determined by majority vote. These pairs were randomly matched based on sharedattribute descriptions, and annotators focused solely on judging the visual similarity of each pairsattributesalignment with the original text prompt was not required. displays the accuracy for each attribute type, along with the overall average, underscoringthe datasets high precision and robustness for this study. To assess annotator agreement, all eightannotators also evaluated a subset of 350 image pairs (50 per attribute). The table further includes thestandard deviation of these assessments, demonstrating strong consistency among annotators.",
  "Preliminary": "Image Prompt Adapter (IP-Adapter) is crafted to enable a pre-trained text-to-image diffusionmodel to generate images using image prompts. It comprises two main components: an imageencoder for extracting image features, and adapted modules with decoupled cross-attention to embedthese features into the pre-trained text-to-image diffusion model. The image encoder employs apre-trained CLIP image encoder followed by a compact trainable projection network, whichprojects the feature embedding into a sequence of features of length N. The projected image featuresare then injected into the pre-trained U-Net model via decoupled cross-attention, where differentcross-attention layers handle text and image features separately. The formulation can be defined as:",
  "V ,(1)": "where Q = ZWq, K = FtWk, V = FtWv, K = FvW k, V = FvW v. Ft and Fv indicate the textand image condition features, respectively. W k and W v represent the weight matrices in the newcross-attention layer dedicated to the visual features. These matrices are the sole new parameters to",
  "be trained within this module and are initialized from Wk and Wv to facilitate faster convergence. Inthis paper, we follow the decoupled cross-attention mechanism for adapting image prompts": "Q-Former was initially introduced as a trainable module designed to bridge the representationgap between a image encoder and LLM. It comprises two transformer submodules: one functions asan image transformer to extract visual features, and the other serves as a text encoder and decoder. Ittakes learnable query tokens as input and associate with text via self-attention layers, and with frozenimage features through cross-attention layers, ultimately producing text-aligned image features asoutput. Blip-Diffusion utilizes Q-Former to derive text-aligned visual representations, allowing adiffusion model to generate new subject renditions from these representations. DEADiff employstwo Q-Formers, each focusing on style and content to extract distinct features for separate cross-attention layers, thereby enhancing the disentanglement of semantics and styles. It makes theStylization Diffusion Model follow the style of reference images better. In this work, we proposethe FiVA-Adapter that focuses on fine-grained control of visual attributes. Thus, disentangling thecontent and style is not our goal.",
  "where G() is the generator, V = {Vk|k [1, N]} represents the visual prompts, A = {ak|k [1, N]} denotes the corresponding attribute instructions, and y is the text prompts": "To achieve our goal, our framework is composed of two key components: 1) Attribute-specific VisualPrompt Extractor: A feature extractor that can extract the attribute-specific image condition featureFk from image Vk with respect to ak. 2) Multi-image Dual Cross-Attention Module: A module canembed both text prompt conditions and multiple image conditions into U-Net with two dedicatedCross-Attention Modules for multi-conditional image generation.",
  "The overall pipeline of our method is illustrated in": "Attribute-specific Visual Prompt Extractor. Inspired by BLIP-Diffusion , we employ theQ-former module to extract the attribute-specific image condition features. Specifically, the Q-formertakes both image Vk and attribute instruction ak as inputs. It models the semantic relationshipbetween the image and the attribute instruction, expected to extract condition feature that alignedwith the given attribute. The extracted feature is then further projected by a projector comprises aLinear Layer and LayerNorm into the image condition Fk to meet the input channel of the followingcross-attention module. Multi-image Dual Cross-Attention Module. IP-Adapter introduces a Dual Cross-AttentionModule where one cross-attention for text prompt condition and the other one for a single-imagecondition. In our work, we extend this module to adapt to multi-image conditions, facilitatingmulti-image controls. Specifically, we set a fixed number of attributes N to control the generated",
  ": Qualitative comparisons on single attribute transferring": "image simultaneously. This allows us to use a fixed number of tokens as input for the cross-attention.The image condition features Fk prepared by the Q-Former and channel projector are concatenatedinto F = [F0, F1, . . . , FN]. Notably, since not all images in our dataset have N attributes, for atarget image I with fewer than N attributes, we use unconditional image features Fzero to pad thesequence. The unconditional feature Fzero is generated by feeding a zero-image and an empty textinto the Q-Former and followed channel projector. To ensure the condition signals are invariant tothe order of prompts, we randomly shuffle the multi-image conditions during training. Finally, themulti-image conditions are passed to the cross-attention module as described in Equation 1. Image Prompts and Attribute Sampling . For a target image characterized by a series of visualattributes and a specific subject, we randomly select reference images from the training set that sharethe same attribute. We incorporate LLM-based data filtering to enhance data sampling. It ensuresthat the subjects of sampled image prompts can share the attribute with the target images subject.",
  "out-domain0.1770.1350.2050.1890.229": "Additionally, we enhance the flexibility of attribute instructions by implementing tag augmentation onthe attribute text, broadening the range of instructional keywords to accommodate various user inputs.Specifically, we prepare a list of augmented tags for each attribute using GPT-4. The augmentedattribute text for prompt images is then randomly sampled from this candidate list of tags. Training and Inference The training and inference setting of our framework are similar with theIP-Adapter . The learning rate is set to 2e-5 and weight decay is set to 1e-3 for stabling thetraining. The Q-former, channel projector, and multi-image cross-attention are trained and otherparameters are frozen.",
  "Experimental Settings": "Baselines We compare our method with the state-of-the-art methods for customized image generation,including Dreambooth-Lora , IP-Adapter , DEADiff , and StyleAligned . Please findthe implementation details of both our method and the baselines in the supplementary material. Evaluation Metrics In order to evaluate the results systematically, we propose a small validation setwith 100 reference images covering the seven attributes, together with 4 target subjects for each ofthem. The target subjects include 2 in-domain ones, which are randomly selected from the same majorcategory with the subject from the reference image, and 2 out-domain ones, which are randomlyselected from the other major categories. For the evaluation, we focus on two aspects: attribute-",
  ": Ablation on range-sensitive data filter. It helps improve the attribute accuracy and protectthe original generation capacity": "accuracy and subject-accuracy. Given the subjectivity of the issue and the lack of quantitative metrics,we incorporate both user studies and GPT scoring on our proposed validation set. Specifically, bothusers and GPT are instructed as follows: subject-accuracy is determined by whether the subject inthe generated image is correct, while attribute-accuracy is evaluated in conjunction with the subject,referred to as attr&sub-accuracy, to eliminate the image variance solution that might artificiallyinflate accuracy. Please refer to the supplementary materials for more details about the validation set,metric implementation, and GPT scoring results.",
  "Quantitative Evaluation": "We show quantitative evaluation results in , where both the CLIP-Score and user study resultsindicate that our method benefits from a higher subject accuracy. The user study further demonstratesthat our method also excels in the joint accuracy of both subject and attribute. However, it is importantto note that the joint accuracy is not high overall due to the challenging nature of this new task.",
  "Qualitative Evaluation": "Comparisons with previous methods. demonstrates that our method effectively transfersthe specific attribute to the target. In contrast, other methods exhibit various issues. The DB-Lora andIP-Adapter usually tend to create image variations without accurately following the target subject.While DEADiff follows the target subject better, its attribute accuracy is poor, and often resulting inround images. Additionally, the StyleAligned model produces unstable image quality, with a highrisk of generating anomalous images. Visual attributes decomposition from the reference image. Based on our method, the visualinformation extracted from the same reference image can be different, depending on the taginput. shows an example of this, demonstrating the effectiveness of fine-grained attributedecomposition. Combination of multiple visual attributes into the target subject. We demonstrate how multiplereference images can be used to combine specific attributes from each into a new image with a targetsubject. As shown in , our method accurately extracts the specified attribute from two or eventhree reference images and combines them with the target subject. To further validate the methodssensitivity to the input tags, we conducted experiments by exchanging the tags for the same referenceimages. The results confirm that the visual attribute extraction is both valid and distinct for differenttag inputs, highlighting the methods flexibility. Effect of range-sensitive data filter. The filter introduced in .1 is aimed for regularizingthe similarity application range of some attributes like lighting and dynamic modelling. In ,we show some qualitative results of its effects. First, it slightly enhances the attribute accuracy rateregarding the difficult visual attributes like lighting. Whats more important, it also helps protect thegeneration capacity of the original pre-trained model from being harmed by the heavy noise in thepaired data without filtering, leading to highly image quality and lower rate in producing failure caseslike distorted human face and body.",
  "Conclusion": "In conclusion, our work addresses the limitations of current text-to-image models in controlling fine-grained visual concepts by introducing a comprehensive dataset with fine-grained visual attributes,FiVA dataset, and a novel visual prompt adapter along with it. Our approach enables precisemanipulation of specific visual attributes, offering greater flexibility and applicability across variousphotography, artistic, and other practical domains. We believe that our contributions will pave the wayfor more sophisticated and user-driven image generation technologies. We will discuss the limitationsand future works in the supplementary material. Limitations and Future Works. The main limitation of the dataset is its heavy reliance onthe capacity of the generative model, which might constrain the realism, range of availablevisual attributes, and attribute accuracy between paired data. For example, specific attributes likephotographic composition techniques or creative photography can hardly be created in this way.This might also introduce some bias in appearance distribution introduced by the generative model.In the future, we will consider collecting some high-quality data from platforms with professionalphotographers and designers, and involve human annotation to create paired data, which can furtherenhance the dataset with a more realistic data distribution and more complex visual attributes.",
  "Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12M: Pushing web-scaleimage-text pre-training to recognize long-tail visual concepts. In CVPR, 2021": "Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, JamesKwok, Ping Luo, Huchuan Lu, et al. Pixart-: Fast training of diffusion transformer for photorealistictext-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo,Huchuan Lu, and Zhenguo Li. Pixart-\\sigma: Weak-to-strong training of diffusion transformer for 4ktext-to-image generation. arXiv preprint arXiv:2403.04692, 2024. Junsong Chen, Yue Wu, Simian Luo, Enze Xie, Sayak Paul, Ping Luo, Hang Zhao, and Zhenguo Li.Pixart-{\\delta}: Fast and controllable image generation with latent consistency models. arXiv preprintarXiv:2401.05252, 2024.",
  "Liucheng Hu, Xin Gao, Peng Zhang, Ke Sun, Bang Zhang, and Liefeng Bo. Animate anyone: Consistentand controllable image-to-video synthesis for character animation. ArXiv, abs/2311.17117, 2023": "Xin Jing, Yi Chang, Zijiang Yang, Jiangjian Xie, Andreas Triantafyllopoulos, and Bjoern W Schuller. U-dittts: U-diffusion vision transformer for text-to-speech. In Speech Communication; 15th ITG Conference,pages 5660. VDE, 2023. Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen,Yannis Kalantidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, and Li Fei-Fei. Visual genome:Connecting language and vision using crowdsourced dense image annotations. International Journal ofComputer Vision, 123:32 73, 2016.",
  "Daiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sabet, Linmiao Xu, and Suhail Doshi. Playground v2.5:Three insights towards enhancing aesthetic quality in text-to-image generation, 2024": "Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi. BLIP-2: bootstrapping language-imagepre-training with frozen image encoders and large language models. In International Conference onMachine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA. PMLR, 2023. Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollr,and C. Lawrence Zitnick. Microsoft coco: Common objects in context. In European Conference onComputer Vision, 2014.",
  "Tianhao Qi, Shancheng Fang, Yanze Wu, Hongtao Xie, Jiawei Liu, Lang Chen, Qian He, and YongdongZhang. Deadiff: An efficient stylization diffusion model with disentangled representations. 2024": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, GirishSastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learningtransferable visual models from natural language supervision. In International Conference on MachineLearning, 2021. Robin Rombach, A. Blattmann, Dominik Lorenz, Patrick Esser, and Bjrn Ommer. High-resolution imagesynthesis with latent diffusion models. 2022 IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR), pages 1067410685, 2021. Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedicalimage segmentation. In Medical image computing and computer-assisted interventionMICCAI 2015: 18thinternational conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, pages 234241.Springer, 2015. Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman.Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In CVPR, pages2250022510, 2023. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti,Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, SrivatsaKundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. Laion-5b:An open large-scale dataset for training next generation image-text models, 2022. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti,Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scaledataset for training next generation image-text models. Advances in Neural Information ProcessingSystems, 35:2527825294, 2022. Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned,hypernymed, image alt-text dataset for automatic image captioning. In Annual Meeting of the Associationfor Computational Linguistics, 2018. Kihyuk Sohn, Nataniel Ruiz, Kimin Lee, Daniel Castro Chin, Irina Blok, Huiwen Chang, Jarred Barber, LuJiang, Glenn Entis, Yuanzhen Li, Yuan Hao, Irfan Essa, Michael Rubinstein, and Dilip Krishnan. Styledrop:Text-to-image generation in any style. ArXiv, abs/2306.00983, 2023. Kihyuk Sohn, Nataniel Ruiz, Kimin Lee, Daniel Castro Chin, Irina Blok, Huiwen Chang, Jarred Barber,Lu Jiang, Glenn Entis, Yuanzhen Li, et al. Styledrop: Text-to-image generation in any style. arXiv preprintarXiv:2306.00983, 2023.",
  "Zhouxia Wang, Xintao Wang, Liangbin Xie, Zhongang Qi, Ying Shan, Wenping Wang, and Ping Luo.Styleadapter: A single-pass lora-free model for stylized image generation. ArXiv, abs/2309.01770, 2023": "Zijie J. Wang, Evan Montoya, David Munechika, Haoyang Yang, Benjamin Hoover, and Duen Horng Chau.DiffusionDB: A large-scale prompt gallery dataset for text-to-image generative models. arXiv:2210.14896[cs], 2022. Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin Chen, Xiaoyan Sun, Dong Chen, and Fang Wen.Paint by example: Exemplar-based image editing with diffusion models. 2023 IEEE/CVF Conference onComputer Vision and Pattern Recognition (CVPR), pages 1838118391, 2022.",
  "Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusionmodels. 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pages 38133824, 2023": "In the supplementary material, we include links to the dataset, metadata, and documentation inSection A. We then introduce additional details on dataset construction and statistics in Section B.Further, we present more details on the experimental setup and additional experimental results inSection C. Finally, please also find the datasheet for the dataset in Section D.",
  "A.1Dataset Link and Documentation": "Our dataset, metadata, and its license are currently maintained on huggingface 1 for users to download: It contains the generated images and theirmetadata, the the original taxonomy of visual attributes and subjects to create the prompts, and thedata filtering file. For each of the images, the main visual attribute type, keyword, subject, and promptis stored in the metadata. A detailed documentation of dataset structure and usage as well as anexample of the metadata can be found in the dataset card via the URL above. The Croissant link canbe find here",
  "BAdditional Details on Dataset Construction": "Details on attribute taxonomy and statistics. When constructing the attribute library, for color,lighting, dynamics, artistic stroke, and focus and depth of field, we create a list ofshort descriptions or keywords for each kind of subcategory together with a list of major subjectsthat can fit into the description. When constructing the prompt, we simply link the attribute andthe subject with a comma. For two specific attribute types, namely rhythm and design, the visualresults can hardly be presented simply via short descriptions or keywords. We use long descriptionswith [sks] denoting the placeholder for subjects that might fit into the sentence. Prompts are createdby replacing [sks] with each of the subject candidates. We show the visualization of a roughdistribution of attributes and subjects in Figure S2, as well as an example of constructing a pair ofimages that share similar lighting conditions. We also show some more examples of images withdifferent visual attributes in Figure S1. Details on the Range-sensitive Data Filtering. To achieve attribute-consistent image pairs, weneed to establish a set of ranges for each attribute where any two images maintain consistency. Weorganize images into a hierarchy of Set/Major-subject/Sub-subject, with the largest set beingthe aforementioned group of suitable subjects. Figure S3a shows an example of the hierarchicalstructure of images related to the attribute lighting: moonlight featuring 7 major-subjects and over100 sub-subjects. Within this hierarchy, each sub-subject corresponds to a list of images, where eachimage belongs to that sub-subject and possesses the visual attribute of lighting: moonlight. We apply Range-sensitive Data Filtering to this hierarchy: We first validate the consistency withineach specific Major-subject. Subsequently, we validate the Set encompassing all validated major-subjects. For any major-subject that failed to pass the validation, we then check their Sub-subjects. As shown in Figure S3b, from the range we want to verify, we sample 9 images and arrange them ina grid. Using GPT-4V, we assessed image consistency for a specific visual attribute. In our example,<major attribute> is lighting, and <specific attribute> is moonlight. For each range we want to verify,this sampling is repeated multiple times. If the mean proportion of inconsistent images remains belowa predefined threshold of 0.1, we consider the images consistent for the selected visual attributewithin the specified range.",
  "C.1Details on Experimental Setup": "Implementation Details. For our methods, our frameworks training and inference setting are similarto the IP-Adapter . The learning rate is set to 2e-5, and weight decay is set to 1e-3 for stabilizingthe training. The Q-former, channel projector, and multi-image cross-attention are trained, and otherparameters are frozen. The training images are resized to 512 512. The model is trained for threeepochs with the randomly shuffled training dataset. For each target image, the attribute images arerandomly sampled.",
  "No": "Image 4 deviates from the consistent lighting traits of 'moonlight'. The lighting in this picture is emanating from a large, orange-yellow moon (or celestial body), creating an intense warm glow that is not consistent with the cool, often bluish or neutral silver lighting associated with natural moonlight. The other images exhibit either a blueish or neutral moonlight, creating a more natural, nocturnal atmosphere.",
  "(b) GPT4V based Range-sensitive Data Filtering": "Figure S3: Range-sensitive Data Filtering. Taking the attribute lighting: moonlight as an example,(a) demonstrates the hierarchy of Set/Major-subject/Sub-subject. It lists the \"group of suitablesubjects\" chosen for generating images with the visual attribute lighting: moonlight, along withsub-subjects under each major-subject. The \"group of suitable subjects\" refers to the pre-definedmajor-subjects that are applicable for the attribute. Due to space limitations, only 15 sub-subjectsare listed for each major-subject. (b) verifies whether the images under major-subject: architectureexhibit consistent lighting traits of moonlight. The result shows that Image 4 exhibits inconsistencies,with the reasons provided. For the baseline methods, we adopt the official code base and hyper-parameters for IP-Adapter ,DEADiff , and Style-Aligned , and we use the implementation in diffusers 2 for Dreambooth-Lora with only the reference image as training source. Details on Evaluation The validation set for the user study contains 100 reference images withdifferent visual attribute types. The distribution of the validation set reflects the inherent diversityof each attribute. We involve three times more data for the GPT study under the same distribution,thanks to its ability to scale up.",
  "C.2More Results": "GPT Study Results Multi-modal Large Language Models (e.g., GPT-4V(ision)) can offer a morescalable alternative to user studies, providing comprehensive analysis and judgment simultaneously.Specifically, we instruct the GPT-4V model to complete similar questionnaires as in the user study. Anexample of the instruction and GPTs output can be found in Figure S6. The GPT study results, shownin Table R1, demonstrate that our method outperforms the baselines in most attributes. However, theresults for Design and Rhythm are not as strong, possibly due to the relatively small data scale forthese two attributes. Effect of the input attribute augmentation. During inference, users may present visual informationin various ways. For example, color might be referred to as hue or palette, and dynamic asmotion capture or action shot. Therefore, we add attribute name augmentation during Q-formertraining to accommodate diverse user inputs. As shown in Figure S4, when the input text slightlydiffers from the standard attribute names during inference, models trained with tag augmentation canstill perform effectively, whereas those without augmentation fail to do so. Results on real-world data. We show the generalization ability of the model to some real-worlddata collected from Unsplash 3 to verify the models generation ability to some attributes beyond thetraining set. Results in Figure S5 shows that our adapter can be effectively extended to real-worldimages, which have a different distribution than generated images.",
  "Ours0.7800.6470.3960.7270.5600.5100.5210.592": "In this row, there are 6 images, with the leftmost one being the reference image. Now We need to retain the colorpalette attribute from the reference image and transfer it to the new subject stage pyrotechnics. Images 1 to 5 are the results from 5 different methods. Please determine two things:",
  ". Do these 5 images match my target subject stage pyrotechnics?2. Do these 5 images satisfy the color palette\" visual attribute from the reference image?": "Please provide a simple analysis and then output the final answers in the last two lines. They will each contain a list, with the list saving the image numbers that meet the respective requirements. For example, if images 1, 3, and 5 match the subject, and image 1 matches the attribute, then the final output will be:",
  "THE LAST TWO LINES SHOULD ONLY CONTAIN THESE TWO LISTS!!!": "1. Image 1 contains fireworks integrated within a frame structure, not matching the pyrotechnic display often associated with stage pyrotechnics.2. Image 2 depicts a fiery display within a heart-shaped structure, somewhat representing a pyrotechnic element but still not typical of a staged event.3. Image 3 clearly shows a stage set with a pyrotechnics display, fitting the \"stage pyrotechnics\" description accurately.4. Image 4 again features a stage with vibrant fireworks, closely matching what you'd expect from \"stage pyrotechnics.\"5. Image 5, while beautifully matching the helmet in the reference in terms of subjects, does not relate to \"stage pyrotechnics.\" Regarding the \"same color palette\" attribute from the reference image:- The reference image features a dynamic and vivid color range with predominant reds, blues, greens, and touches of yellow.1. Image 1 attempts to incorporate similar vibrant colors but the colors are less vivid and more subdued compared to the reference.2. Image 2 features intense orange and yellow tones, lacking the broader color variety of the reference.3. Image 3, while mostly featuring golden and warm tones, lacks the varied color palette of the reference.4. Image 4 infuses a rich variety of colors, aligning well with the diverse palette of the reference.5. Image 5 also diligently replicates the reference's color palette with the same rich, vivid colors.",
  "D.2Composition": "What do the instances that comprise the dataset represent (e.g., documents, photos, people,countries)?The FiVA dataset consists of a number of pairs of images that share similar visual attributesand corresponding meta data like attribute type and subject. How many instances are there in total (of each type, if appropriate)?The FiVA dataset contains 1M images generated by Playground-V2.5. More examples fromother generative models are planned to be further added. Does the dataset contain all possible instances or is it a sample (not necessarily random) ofinstances from a larger set?The FiVA dataset is a new dataset generated using existing 2D generative models.",
  "How was the data associated with each instance acquired?We used the open-source 2D generative model, Playground-V2.5 to generate the dataset": "What mechanisms or procedures were used to collect the data (e.g., hardware apparatuses orsensors, manual human curation, software programs, software APIs)?We develop an attribute library and subject tree to create the prompts, generate the images,and develop a range-sensitive filtering to enhance the pair-wise attribute alignment. We alsoperform human validation to verify the accuracy of the attribute alignment.",
  "How will the dataset will be distributed (e.g., tarball on website, API, GitHub)?The dataset are released on Huggingface:": "When will the dataset be distributed?The dataset will be gradually released starting from June 2024. Due to its large scale, it willtake some time for the dataset to be fully released, considering the uploading speed. Will the dataset be distributed under a copyright or other intellectual property (IP) license,and/or under applicable terms of use (ToU)?The dataset will be released under the Playground v2.5 Community License license.",
  "Checklist": "The checklist follows the references. Please read the checklist guidelines carefully for information onhow to answer these questions. For each question, change the default [TODO] to [Yes] , [No] , or[N/A] . You are strongly encouraged to include a justification to your answer, either by referencingthe appropriate section of your paper or providing a brief inline description. For example:",
  ". If you are using existing assets (e.g., code, data, models) or curating/releasing new assets": "(a) If your work uses existing assets, did you cite the creators? [Yes] See reference.(b) Did you mention the license of the assets? [Yes] See supplementary material.(c) Did you include any new assets either in the supplemental material or as a URL? [Yes](d) Did you discuss whether and how consent was obtained from people whose data youreusing/curating? [Yes]"
}