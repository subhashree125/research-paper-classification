{
  "Abstract": "Data curation is a field with origins in librarianship and archives, whose schol-arship and thinking on data issues go back centuries, if not millennia. The fieldof machine learning is increasingly observing the importance of data curation tothe advancement of both applications and fundamental understanding of machinelearning models evidenced not least by the creation of the Datasets and Bench-marks track itself. This work provides an analysis of recent dataset developmentpractices at NeurIPS through the lens of data curation. We present an evaluationframework for dataset documentation, consisting of a rubric and toolkit devel-oped through a thorough literature review of data curation principles. We usethe framework to systematically assess the strengths and weaknesses in currentdataset development practices of 60 datasets published in the NeurIPS Datasetsand Benchmarks track from 2021-2023. We summarize key findings and trends.Results indicate greater need for documentation about environmental footprint,ethical considerations, and data management. We suggest targeted strategies andresources to improve documentation in these areas and provide recommendationsfor the NeurIPS peer-review process that prioritize rigorous data curation in ML.We also provide guidelines for dataset developers on the use of our rubric as astandalone tool. Finally, we provide results in the format of a dataset that show-cases aspects of recommended data curation practices. Our rubric and results are ofinterest for improving data curation practices broadly in the field of ML as well asto data curation and science and technology studies scholars studying practices inML. Our aim is to support continued improvement in interdisciplinary research ondataset practices, ultimately improving the reusability and reproducibility of newdatasets and benchmarks, enabling standardized and informed human oversight,and strengthening the foundation of rigorous and responsible ML research.",
  "Introduction": "The NeurIPS Datasets and Benchmarks (D&B) track was created in 2021 to enhance the developmentof datasets in line with the exponential growth of applications of machine learning (ML). A keychallenge this track aimed to address is that of datasets being used outside their original scope asbenchmarks among other potential issues, this creates the possibility of field-leveloverfitting, as well as unanticipated ethical and privacy problems. NeurIPS sought to address thisby encouraging the development of new datasets for ML, improving the quality of datasets beingproduced, and emphasizing the importance of the role of data within ML. The introduction of new,tailored peer-review guidelines also enabled and incentivized publication of datasets and benchmarks.The D&B track is thus uniquely positioned to influence and guide the quality and ethicality of datasetsbeing released and bolster responsible dataset development practices.",
  "arXiv:2410.22473v2 [cs.CY] 3 Jan 2025": "Recent research in fairness, accountability, and transparency has proposed to reduce bias in models,via datasets, through the improvement of dataset documentation .Particularly, recent research on ethical data curation for ML datasets emphasizes the adoptionof concepts and processes from library and archival studies and digital curation in order to improvethe documentation of dataset contents and data design decisions . Data curation, asubset of digital curation and information science, is ...the activity of managing data throughoutits life cycle; appropriately maintaining its integrity and authenticity; ensuring that it is properlyappraised, selected, securely stored, and made accessible; and supporting its usability in subsequenttechnology environments. [75, p. 203]. We recently translated data curation concepts for the MLdataset development context . The results enable us to apply data curation to the field of ML sothat dataset development practices can be evaluated and improved. Contributions. Our goal is to document and improve the standard of dataset development in NeurIPSso that future benchmarks and datasets can be effectively found, easily accessed, ethically used,consistently evaluated, and appropriately reused. To these ends, we present a systematic datasetdocumentation evaluation framework to organize the assessment of curation practices for ML datasetdevelopment. The framework is composed of a rubric and toolkit developed through an iterativemulti-stage process to arrive at the rubric elements relevant for evaluation and corresponding criteriafor their assessment, as well as supplementary material packaged as a toolkit to aid in the applicationof the rubric. Here, we establish the feasibility of this framework as an auditing tool for datasetdocumentation by performing a systematic evaluation of a sample set of 60 datasets published inthe NeurIPS D&B track between 2021-2023. We analyze the assessments to evaluate how datacuration is currently performed in ML and show how it can be improved. Our results demonstrate thatdocumentation quality varies widely across datasets and reveal a lack of documentation and reflexivityon environmental footprint, the situatedness and non-neutral nature of data, ethical considerations,and data management. We recommend how to improve this in and make a proposal forNeurIPS peer-review changes.",
  "The Importance of Data in Machine Learning": "Increasingly, machine learning research has turned towards the improvement of data to improvemodel results and fundamental understanding. Research areas include the role of data (distribution)in learning theory and generalization , explicitly data-centric machine learning, the construction of datasets , addressing a greater number of areas andproblem domains , the development of ethical models/frameworks around AIand data within sound and music computing, computer vision, natural language interfaces, andmore . NeurIPS has responded to the rising urgency and recognized impact of data through the introductionof the Datasets and Benchmarks (D&B) track . Submissions to the NeurIPS D&B track highlightaspects of data work critical to the development of machine learning datasets. Specifically, looks at impacts of unmanaged citations of dataset derivatives, continued usage of datasets after theirretraction, ambiguous dataset documentation, non-restrictive and ineffective licensing, and lack oflong-term data stewardship. A data quality framework developed for datathons , is also applicablefor the examination of data quality of ML datasets as it covers 5 broad quality dimensions. A checklistfor ethical data collection of human-centric computer vision datasets highlights that further emphasisshould be placed on moving away from general-purpose datasets to clearly-defined dataset collection,providing recommendations for obtaining consent and maintaining privacy, and proposing how toexamine, acknowledge, and enhance dataset diversity . In 2023 NeurIPS released a Code of Ethics to supplement the NeurIPS Code of Conduct .The code of ethics includes information on how to report and prevent harms from research thatinvolves human participants, data concerns including privacy, consent, etc., and societal harms suchas concerns of safety, security, discrimination, harassment, environment, human rights, and bias.Alongside this, there were ethics guidelines released for reviewers which point reviewers tochecklists and a framework for evaluating general ethical conduct, as well specifications for human-related data, data concerns around compliance, consent, and regulations, and negative societal impacts.Guides and further information about how to report on these areas have been released by NeurIPS",
  "and others . Since 2021, the D&B track has seen immense growth and success, from 174 of484 submitted papers accepted in 2021 to 322 of 987 submitted papers accepted in 2023": "As the landscape of data-focussed ML research continues to evolve, there remain open challenges forthe field to tackle. For example, a review of AI impact statements in NeurIPS papers from 2021 finds that agency and responsibility are two key themes in the statements whereby dataset creatorsfeel they are not in control of the negative impacts of their work if it was to be misused by malicioususers. Authors typically reassign the responsibility to identify and safeguard against potential negativeimpacts to other practitioners, or state that the potential negative impacts of their work are the same asthose that exist for that domain, i.e., further work is needed to recognize contributions as non-neutraland take accountability of how design decisions impact outcomes .",
  "Data Curation": "Data curations influences as a field include information sciences, digital libraries, and archivalsciences . It thus has deep-rooted and established methods and discourse onhow to maintain large amounts of data and manage ethical concerns. Data curation as a componentof digital curation takes a lifecycle approach to the management of digital data . Lifecycleapproaches divide the digital curation practice into stages, which differ in their details but havein common a broad view including the pre-use, use, and post-use of a dataset. For example, theDigital Curation Centers model specifies conceptualize, create or receive, appraise and select,ingest, preservation action, store, access, use, and reuse, and transform . Each stage ofcuration consists of activities, designated roles and responsibilities, specified technical, legal, ethical,and operational considerations as well as policies that institutionalize the discussed activities andresponsible parties . ML studies on data practices (...what and how data are collected, managed, used, interpreted, reused,deposited, curated, and so on... [12, p. 55]) also called dataset development and data work mostclosely resemble examinations of the processes of data curation (despite often using the term toconnote data collection, e.g., ). Other works in this space discuss the importance ofdocumentation and propose new frameworks for it , review intrinsic and extrinsic biasesin the dataset development process , and highlight the power dynamics involved in dataquality, data work, and documentation . In recent work , we performed a thorough literature review of data curation practices, translatedthem to be applied in a machine learning setting, and presented a preliminary analysis of a fewNeurIPS D&B track datasets, focussed on the interdisciplinary process of adopting data curation forML. Particularly, we highlighted the need for tools to translate the standards for transparency andaccountability. In contrast to previously mentioned studies, our framework enables the application ofdata curation principles and concepts in practice. Other authors also point to the need for dataset creators to actively document and steward theirdatasets , in contrast to much of the static documentation which is common in ML. Theserecommendations can be translated into practice-based processes: by seeing dataset development inML as data curation and adopting its norms and practices, the NeurIPS community and ML researchpractices broadly can gain an elevated standard of documentation and resulting benefits to modelperformance, responsibility, and fundamental understanding.",
  "Methods": "Research Questions. What are the strengths and weaknesses of NeurIPS dataset documentationpractices considered through a data curation lens? In other words, how well curated are NeurIPSdatasets and benchmarks? To study this, we examine (1) What constitutes a well curated dataset? (2)How feasible is the adoption of data curation principles to assess ML datasets? and (3) What is thestate of data curation at NeurIPS and how can it be further advanced? Approach. We developed an evaluation framework to assess data documentation practices, i.e.,curation processes, and applied it to recently published ML datasets in the NeurIPS datasets andbenchmarks track. This track was precisely chosen because of its relevance in publishing suchcontributions but also in influencing the quality of datasets that are accepted. The evaluationframework consists of a rubric used to evaluate dataset documentation and design decisions and a",
  "toolkit which supplements the rubric by providing additional information on how to apply the rubriceffectively. This framework was applied to manually assess 60 ML datasets in three steps": "1. We established the initial construction and design of our evaluation framework consistingof the rubric and toolkit and reviewed the preliminary feasibility of the framework byapplying it to 25 datasets across 4 rounds . In these rounds, we continued to develop theframework iteratively based on the evaluation results . We reflected and reported on theinitial process of designing the framework such as the benefits resulting from the diverseperspectives of an interdisciplinary team, the lessons learned while applying the framework,and how we used the data from the initial application of the rubric to iteratively refine it andyield more consistent evaluations . 2. We examined the consistency in application by measuring inter-rater reliability (IRR). Toclaim that our framework is consistent, reliable, and accordingly feasible, we conductedanother round of evaluations consisting of 5 datasets (round 4 and disagreement review)with the framework fully developed. We therefore address RQ 1 with our most updatedversion of the framework and RQ 2 with the final fourth round of evaluations that firmlyestablishes the frameworks reliability through iteratively improving IRR results.",
  ". We applied this framework to assess 30 additional datasets to evaluate current practices ofdata curation in ML dataset development and areas where improvement was needed (RQ 3)": "Evaluation Framework. We grounded our framework in data curation principles, emphasizingdocumentation, transparency, and ethical considerations. We started with key aspects of data curationrelevant to ML and followed with iterative refinement through internal reviews and adjustments toevaluation criteria, guided by digital curation lifecycle models, FAIR data principles, and environ-mental sustainability and justice considerations. The rubric, provided in the Appendix, consists of 18elements across five categories. In , we presented results from a training round and rounds 1-3.After round 3, we updated and refined the criteria for 13 elements and added additional guidance inthe toolkit for interpreting authenticity, reliability, and representativeness. We present the up-to-dateversion of the framework along with the changes made between versions and their rationale in theAppendix. The scope category has 2 elements, context, purpose, motivation and requirements, which em-phasize the requirement for a dataset creation plan and addressing intrinsic biases. The ethicalityand reflexivity category has 4 elements, ethicality, domain knowledge and data practices, con-text awareness, and environmental footprint, covering a range of documentation requirements toincrease reflection and accountability in the dataset creation process. The data pipeline categoryincludes data collection, data processing, and data annotation, prompting reflection on howand why choices were made and their implications. The data quality category contains suitabil-ity, representativeness, authenticity, reliability, and structured documentation, to ensure theconsideration of a broad set of qualities that impact how well a dataset can be appropriately andresponsibly reused. The data management category covers FAIR principles - findability, acces-sibility, interoperability, and reusability - included to evaluate the transparency of data managementconsiderations. Each rubric element is assessed on minimum standard criteria (with a score of passor fail) that detail the expected level of documentation. Elements that pass the minimum standardare also assessed on a standard of excellence (with a score of full, partial, or none). Therefore,the conceptualization of the rubric defines what a well-curated dataset must document. The toolkit isa supplementary resource that introduces concepts from data curation and serves as a manual to therubric. It contains instructions and guidance on how to evaluate datasets, how to interpret specificelements, guiding principles, recommendations, FAQ, sample evaluations, a glossary, and furtherreadings. The toolkit is provided in the Appendix. Iterations. In order for data curation to provide robust norms for ML dataset development, ourframework has to enable consistent use. To evaluate consistency, we measured inter-rater reliability(IRR) as we iteratively refined the rubric over multiple rounds of evaluation. The preliminary stage ofrefining the rubric occurred across the first 4 rounds of evaluations (namely training, round 1, round2, round 3) . Each round involved improvements based on feedback and observations, ensuringthe rubric and toolkit were effectively refined and validated. This was structured around several keyactivities. We began with a training round to help reviewers become acquainted with the rubric andfoundational data curation concepts, significantly reducing initial discrepancies and increasing IRRin the upcoming rounds. Following each evaluation round, we gathered feedback from reviewers and identified specific areas of the rubric that needed adjustments to better convey the expectations andreduce ambiguity. We refined definitions, provided clearer examples, and better aligned the rubricelements with practical evaluation scenarios. This established preliminary feasibility. Consistency. To establish the frameworks feasibility and consistency, we performed additionalrounds of evaluations. Across training to round 4, three reviewers assessed each of the 30 datasetsin a fully crossed design thus we calculated IRR using a two-way mixed, consistent, average-measures intra-class coefficient (ICC) to assess the consistency of the raters evaluations of rubricelements measured on an ordinal scale across subjects . In rounds 3 and 4, we additionallyperformed a disagreement review. Once reviewers had completed their evaluations, they reviewedother evaluations and engaged in a brief discussion period in which they could debate, review, andupdate their scores and comments. Given the interpretative nature of the framework, this collaborativedisagreement review enabled improved understanding of the rubric concepts and mitigated potentialerrors such as overlooking provided documentation. It also led to greater consistency while simulta-neously leveraging the diverse perspectives of reviewers to enhance the richness and accuracy of thedataset evaluations. With the frameworks feasibility established, we evaluated additional datasets. Application. To understand precisely how data curation can contribute to the advancement of MLdataset documentation practices, we performed a final round of evaluations (round 5). In this round,we evaluated 30 datasets published in the NeurIPS D&B track from 2021-2023. A full list of evaluateddatasets from all rounds can be found in the Appendix. The datasets were randomly selected afterfiltering all published papers at the NeurIPS D&B track for dataset contributions. The filtering processis described in the Appendix. In the final round, four reviewers performed double coding for 30datasets, each reviewing on average 15 datasets, including a disagreement review. Accordingly, wemeasured IRR with a one-way mixed, consistent, average-measures intra-class coefficient (ICC).After the disagreement review, additional corrections were made for consistency, see Appendix. All60 dataset evaluations and analysis files can be found hosted on Zenodo . Analysis. We analyze to what extent criteria were fulfilled for 1) each dataset and 2) each rubricelement. This enables a review of whether data curation can provide guidance for documentation forNeurIPS datasets and precisely in what capacity that guidance is needed.",
  "Results": "R1. Inter-rater reliability suggests the evaluations are consistent and reliable. We observed aquantifiable improvement in IRR per dataset across successive evaluation rounds. The ICC valuesprogressively increased moving from fair to excellent agreement among raters. In the final round,the median ICC value for the 30 datasets evaluated was 0.90 (a). Despite the high level ofqualitative human interpretation present when evaluating IRR across elements as compared to datasets,the final round had very high agreement, with ICC values with medians ranging from 0.83-0.98 acrossrubric categories (b). The improvements in IRR confirm the effectiveness of our iterativerefinement approach. By continuously enhancing the rubric and its guidelines, we achieved a highlevel of consistency in evaluations, demonstrating the rubrics potential as a reliable tool for assessingdataset documentation in machine learning. This high level of agreement underscores the clarity andeffectiveness of the rubrics criteria in guiding evaluators to consistent outcomes. These findingsare critical as they establish the rubrics credibility and pave the way for its broader application andacceptance within the ML community. Additionally, the findings demonstrate the utility and rigorof qualitative human evaluations. ICC values for each of 5 rounds is shown in a and acrossrubric categories in b. Additional results are provided in the Appendix. R2. Documentation quality varies widely across datasets. To gauge the extent of documentationprovided, we calculated for each dataset the percentage of rubric elements that received a pass andfail score for the minimum standard and a full, partial, and none score for the standard ofexcellence. Since each dataset was evaluated by 2 reviewers and the rubric consisted of 18 elements,we averaged the score across both reviewers and divided by 18. The results are shown in Fig 2a and2b. Across all datasets evaluated during the final round, one fulfilled 86% of the minimum standardcriteria (highest pass rate), while another fulfilled only 39% (lowest pass rate), a 47% difference. Theabsolute difference between the best and worst performing papers at the standard of excellence criteriais similar, with the best-performing paper scoring full on 50% of the standard of excellence and thetwo worst-performing receiving no full scores. These results demonstrate that documentation varies",
  "(b) IRR for rubric categories in round 5": ": Inter-rater reliability (IRR) (a) Across evaluation rounds, and (b) Within round 5 acrossrubric categories. Improvement of IRR across rounds and ultimate high IRR across categories providesevidence that the multi-stage quality and consistency process described in Sec. 3 was successful.In addition to this quantitative measure, we conducted qualitative participatory evaluations withreviewers in each round; see R1 and Appendix.",
  "widely across datasets and there is great scope for improvement in documentation practices from adata curation lens, particularly to meet a standard of excellence": "R3. NeurIPS prioritizes model-work adjacent documentation. To analyze where data practicescould be improved, we measured the completion of documentation for each rubric element andcategory by calculating the number of pass, fail scores and full, partial, none scores for all 60evaluations (2 per each dataset in round 5) and divided the number by 60. Notably, NeurIPS paperstended to perform better at certain rubric elements than at others (see c, 2d). Documentationfor the minimum standard for context, purpose, motivation, suitability, and reliability was 100%fulfilled. Additionally, all 3 elements in the data pipeline category were 93% fulfilled. This highlightsthe areas that are prioritized and considered primary for documentation by dataset creators. These arealso areas that are standard to report for publication. NeurIPS has also been able to guide and encour-age greater focus through the suggested submission requirements for structured documentation (i.e.,datasheets , data statements , etc.) and some aspects of the data management rubric elements.For example, documentation for a dataset clearly stated the problem domain of NLP and computervision and the relevance of the new dataset being introduced in creating speech-based rather thantext-based input for assistive devices (context, purpose, motivation), discussed the feasibility oftheir dataset (suitability), and provided a datasheet (structured documentation). R4. Documentation is rarely context-aware and typically does not quantify environmentalfootprint. The rubric elements with the worst performance across round 5 evaluations are contextawareness and environmental footprint, both with 0% pass rates of the minimum standard (andsubsequently of the standard of excellence). Papers fail the context awareness rubric criteria bynot including a dedicated positionality statement (a statement of authors institutional affiliations isnot considered as a statement of or reflection on positionality). For the standard of excellence, lessthan 20% of papers receive a full or partial score for the ethicality standard of excellence. Thatis: even those papers that make use of the proportionality principle and document informed consenttend to do so only as much as required by ethics checklists, with additional ethical discussion rarelyincluded. The evaluated datasets also fail the environmental footprint criteria because none of themquantitatively assess the environmental footprint associated with dataset creation. R5. Documentation often remains incomplete. The results indicate that even for those datasets withwell-documented elements for the minimum standard, rigorous documentation is ultimately lacking.For example, in the case of reliability, papers tend to pass the minimum standard by describing thephenomena represented by the data (e.g., describing the videos from which screen capture data weregenerated), but fail the standard of excellence by not providing a mechanism by which others couldverify what was being represented (e.g., no way for anyone else to access the videos used to producescreen captures). As in the case of reliability, and as intended in the rubrics design, papers perform",
  "(d) Per element, Standard of excellence": ": Percentage of completed documentation per dataset (a,b) and per element (c,d) in round 5(i.e. after a multi-step iterative process to improve quality). In (a) we observe that the highest scoringdataset fulfilled 86% of criteria to meet the minimum standard of quality while the lowest fulfilledonly 39%; in (b) for the standard of excellence we see similar spread (approximately 50% differencebut lower attainment (highest fulfilled 50% of criteria, lowest two fulfill none of the criteria forexcellence); see R2. In both (c) minimum standard and (d) excellence we observe that those elementsmore closely related to model-work (such as suitability and reliability) are more consistentlyfulfilled; see R3. better at the minimum standard than at the standard of excellence: 14 out of 18 rubric criteria have aminimum standard pass rate over 50%, compared to 1 of 18 with full scores over 50% and 3 of 18with partial scores over 50% for the standard of excellence. R6. Findings suggest no improvements occurred over time. We evaluated an even distribution ofdatasets published in 2021, 2022, and 2023 for round 5. shows the results of the percentageof pass and full scores across elements for each dataset summarized by year. Particularly, wecan observe a slight downward trend in documentation scores across the years evaluated: from 2021to 2023, the median percentage of pass scores per dataset for the minimum standard goes from78%, to 67%, to 61%, while the median percentage of full scores per dataset for the standard ofexcellence goes from 29%, 25%, and 13%, respectively. In 2021, the call for papers for the D&Btrack required the submission of dataset documentation, URL for accessing the dataset, details aboutdata licensing, hosting, and maintenance, and for authors to ensure easy reproducibility . In thefollowing years, additional requirements around datasets being in widely used formats, long-termpreservation, inclusion of and access to metadata, and usage of persistent identifiers were added. Despite the increasing stringency of requirements, we could not find any evidence in thissample to suggest that the extent of provided documentation improved over time, pointing to theneed for an encompassing structure and framework by which to assess documentation practices andpinpoint areas of improvement. Furthermore, the use of structured documentation also reduced overtime, from 80% in 2021 to 50% in 2023.",
  "How to Improve Data Curation at NeurIPS: Strategies and Resources": "Our findings identify areas for which datasets have low or no documentation. To aid dataset creators instrengthening their curation processes, we recommend the adoption of the following methods and re-sources, particularly for the elements requirements, ethicality, context awareness, environmentalfootprint, findability, and reusability. First, to address requirements we echo recommendations regarding the creation of purposestatements . Stating how dataset creators translated the real-world problem they are addressinginto a ML problem for which the dataset is created promotes transparency. This processconsists of numerous decisions, expertise, and assumptions that should be documented in order tounderstand the context in which the problem situation was framed. In cases of harmful ML models,it has been seen that this translation process can lead to the creation of bad proxy data that unfairlyrepresents the real-world scenario . Furthermore, sharing this process reveals the practicalities ofperforming data work which is often hidden and considered under-valued . Particularly, it isimportant for dataset creators to distinguish between the initial formulation of the problem vs. thedataset creation scheme detailing how the dataset development was actually executed. The latter isoften documented after the development of the data and can be impaired by the forgetting practiceof only recording conclusions . As Muller et al. distinctly point out, Measurement plans tend torecord conclusions, not rationales... Other people then work with those conclusions, and have noway to access those unrecorded rationales. [64, p. 9]. We urge dataset creators to explicitly document how the benefits of developing their dataset outweighthe harms of creating it to improve reflection on ethicality. In other words, the proportionalityprinciple must be considered. In ethics, it is understood that actions have positive and negativeeffects simultaneously. This is called the double effect. Applications of double effect alwayspresuppose that some kind of proportionality condition has been satisfied. Traditional formulations ofthe proportionality condition require that the value of promoting the good end outweigh the disvalueof the harmful side effect. . The submission checklist for NeurIPS requires authors to documentpotential positive and negative societal impacts of their work; we support this and additionallyrecommend the checklist be amended to encourage comparison and reflection on these in proportionto each other, which very few datasets from our evaluations explicitly do. Context awareness was not well demonstrated in any dataset we evaluated, correlated with thelack of positionality statements in the NeurIPS D&B track as a whole. Past research from thetrack has pointed to the importance of annotator positionality, as researchers social identity andimplicit biases impact data-related choices ; this recommendation is also made from a feminist",
  "HCI (human-computer interaction) perspective with the goal to increase reflexivity in ML datasetdevelopment . See Appendix for examples of positionality and reflection statements of this work": "Data curation in ML encompasses many phases, each responsible for significant emissions dueto energy consumption . Several datasets in our sample ranged from overone million instances to several billion instances . Owing to theirvolume, these datasets are anticipated to have a significant environmental footprint, beyond theimpacts associated with model training which have tended to be the focus in ML. There were noquantitative assessments of environmental footprint in the evaluated datasets. This quantificationis crucial for several reasons: it allows for the assessment and comparison of carbon footprint acrossdifferent projects, facilitating more informed decisions about resource allocation and model design. Understanding these impacts can also drive the development of more energy-efficientalgorithms and hardware, contributing to broader efforts to mitigate climate change .Moreover, as public awareness of environmental issues grows, the ML community faces increasingpressure to demonstrate accountability and progress toward sustainability . Transparentreporting of carbon emissions can enhance the credibility of research institutions and companies in thefield. By understanding where these emissions originate, researchers and engineers can better targetinterventions, such as optimizing algorithms for efficiency or sourcing providers with renewableenergy for power-intensive tasks, to mitigate the ecological consequences of their work .We provide a list of strategies in the Appendix on how to report environmental footprint for datasetdevelopment. To improve findability, we urge NeurIPS to require datasets to have metadata (not just data)assigned a persistent identifier and hosted in a searchable repository (such as Zenodo). While werecognize that having this requirement for all datasets may be infeasible due to volume, sensitivity,and other factors, having this information for metadata will help provide information about thedataset that will be available in the long-term, even if the data are gone. Although many datasetsare provided on GitHub or platforms hosted by the dataset creators, these URLs suffer from a highlikelihood of link rot (that the link will no longer be available over time) . Lastly, to improvereusability, we echo the inclusion of identifier information, dataset characteristics, and datasetprovenance as outlined in The Data Provenance Initiative .",
  "What Next: A Proposal for Peer-Review": "Peer-review processes are constantly progressing with evolving needs, and they require multiplelenses . We propose that our evaluation framework can provide a structure that enhancesthe submission and peer-review process for the NeurIPS Datasets and Benchmarks track. Datasetcreators, by applying the rubric and associated resources, would be more easily able to conduct theirdataset development processes with data curation in mind. On the other side, dataset reviewers canuse the rubric to evaluate dataset documentation, highlight targeted areas of improvement where datacuration standards can be applied, and provide recommendations. The framework speaks directly tothe evaluation of documentation required from peer-reviewers in the review form - Documentation:For datasets, is there sufficient detail on data collection and organization, availability and maintenance,and ethical and responsible use? Note that dataset submissions should include documentation andintended uses; a URL for reviewer access to the dataset; and a hosting, licensing and maintenance plan.. The presented framework allows for the systematic, precise, and encompassing evaluation ofthese details beyond the prompts present in the current review form, through the criteria presented forthe elements in the data pipeline category (i.e., data collection and organization), the data managementcategory based on the FAIR principles (i.e., availability and maintenance), and the ethicality andreflexivity category (i.e., ethical and responsible use). The use of this framework would also enablereviewers to consistently determine whether an additional ethics review is needed. A past consistencyexperiment for peer-review at NeurIPS in 2014 showed that if the conference reviewing had beenrun with a different committee, only half of the papers presented at the conference would have beenthe same [20, p. 3]. The 2021 version of the experiment was ...consistent with the 2014 experimentwhen the conference was an order of magnitude smaller. . Our framework can thus help provide ameans of consistency in terms of dataset documentation. We suggest incorporating the framework byintroducing a dedicated dataset documentation reviewer role to the NeurIPS D&B track. This caninitially be similar to the ethics reviewer, who performs reviews only for those papers that are flagged,but may later evolve to be a part of the core peer-review team. We recommend a dedicated reviewerbecause of the relatively intense process of evaluating each dataset and the specialized skillset that it will require. Although we found that the time requirement to evaluate each dataset ranged from35 minutes to 2 hours, the average time in later rounds was limited to 1 hour. Additionally, as withethics reviews, the results from the dataset documentation review should advise the acceptance of thepaper as poor documentation ultimately leads to poor reusability and reproducibility which woulddefeat the purpose of the D&B track.",
  "Limitations": "We identify five limitations of our work. 1. The results we showcase are based on the sample set ofdatasets we evaluated. Although these datasets were randomly chosen and evenly distributed between2021-2023, there is a chance for the selected datasets to be unrepresentative of the datasets andbenchmarks published at the NeurIPS D&B track as a whole. 2. Our analysis is based on descriptiveand interpretative evaluations completed by a mix of reviewers. Although we took careful steps inour iterative process to clarify and normalize standards of interpretation, all results are contingenton the human processes of differing evaluation styles. As with all peer-review, the results are thusdependent on the reviewers. 3. It is understood that a given reviewer would evaluate each elementin the rubric similarly across all datasets. However, we also conducted a disagreement review. Thismeans that a reviewer could change their perspective for a specific element based on the comments ofanother reviewer, if a disagreement was flagged. This does not mean that the reviewer would thenupdate their scores and comments for that element for all other datasets they evaluated. Thus someinconsistencies in how the one element is evaluated across all the datasets might be introduced as thecost of improving within-dataset review quality. We believe the impact of this limitation is low, as ourresults showed minimal disagreement in the final round (see a). 4. Our rubric is designed toenable qualitative and holistic evaluation of each paper on a standardized basis. We believe stronglyin the merits of this approach, however it does limit the amount of insight we can have about howproperties of the dataset itself influence curation practices. An interesting complementary futureapproach to study this would be to code characteristics of datasets and see if the trends we identifyvary when decomposed by these characteristics, i.e., whether there are specific documentation trendsacross types of ML datasets or metadata. 5. Using documentation to understand the curation processis not a substitute for being directly involved in the process or communicating with the datasetcreators. As such, it is possible and ultimately a limitation that the reality of data curation is morecomplex than what is covered in documentation or our framework. In such cases, things can becomeoverly simplified in the documentation and auditing process, e.g., box ticking instead of genuinereflection and evaluation. An ethnographic study of data curation would yield different, additional,insights that we cannot provide. This is currently a limitation and opportunity for future work.",
  "Conclusion": "By giving datasets and benchmarks a dedicated venue, NeurIPS has sent a clear message that datasetquality is the foundation of continued progress in ML applications and fundamental understanding.There is no better database of knowledge than data curation to aid in this venture. Our evaluationframework adopts concepts from these fields for ML and provides a practical lens on how NeurIPScan spearhead the requirement for rigorous data curation in ML. The enhancements due to theframework are designed to improve the quality, ethicality, and human oversight of new datasets andbenchmarks, fostering greater scientific integrity and advancement.",
  "AIES. [n. d.]. Call for Papers AIES 2024": "Saad Almohaimeed, Saleh Almohaimeed, Ashfaq Ali Shafin, Bogdan Carbunar, and LadislauBlni. 2023. THOS: A Benchmark Dataset for Targeted Hate and Offensive Speech. InWorkshop on Data-centric machine learning. Jerone Andrews, Dora Zhao, William Thong, Apostolos Modas, Orestis Papakyriakopoulos,and Alice Xiang. 2023. Ethical Considerations for Responsible Data Curation. Advances inNeural Information Processing Systems.",
  "Alina Beygelzimer, Yann Dauphin, Percy Liang, and Jennifer Wortman Vaughan. 2021.The NeurIPS 2021 Consistency Experiment": "Eshta Bhardwaj, Harshit Gujral, Siyi Wu, Ciara Zogheib, Tegan Maharaj, and ChristophBecker. 2024. Dataset for The State of Data Curation at NeurIPS: An Assessment of DatasetDevelopment Practices in the Datasets and Benchmarks Track. Eshta Bhardwaj, Harshit Gujral, Siyi Wu, Ciara Zogheib, Tegan Maharaj, and ChristophBecker. 2024. Machine Learning Data Practices through a Data Curation Lens: An Evalua-tion Framework. In 2024 ACM Conference on Fairness, Accountability, and Transparency.Association for Computing Machinery, New York, NY, USA. arXiv:2405.02703 [cs].",
  "NeurIPSCommunicationChairs.2021.ARetrospectiveontheNeurIPS2021EthicsReviewProcess": "Tiffany C. Chao, Melissa H. Cragin, and Carole L. Palmer. 2014. Data Practices and CurationVocabulary (DPCVocab): An empirically derived framework of scientific data practices andcuratorial processes. Journal of the Association for Information Science and Technology (June2014), n/an/a. Valeriia Cherepanova, Steven Reich, Samuel Dooley, Hossein Souri, John Dickerson, MicahGoldblum, and Tom Goldstein. 2023. A Deep Dive into Dataset Imbalance and Bias in FaceIdentification. In Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society.229247. Kasia S. Chmielinski, Sarah Newman, Matt Taylor, Josh Joseph, Kemi Thomas, JessicaYurkofsky, and Yue Chelsea Qiu. 2022. The Dataset Nutrition Label (2nd Gen): LeveragingContext to Mitigate Harms in Artificial Intelligence. In NeurIPS 2020 Workshop on DatasetCuration and Security. Giovanni Colavizza, Tobias Blanke, Charles Jeurgens, and Julia Noordegraaf. 2022. Archivesand AI: An Overview of Current Debates and Future Perspectives. Journal on Computing andCultural Heritage 15, 1 (Feb. 2022), 115.",
  "DMLR. [n. d.]. Call for Papers DMLR 2024": "Gintare Karolina Dziugaite, Kyle Hsu, Waseem Gharbieh, Gabriel Arpino, and Daniel Roy.2021. On the Role of Data in PAC-Bayes Bounds. In Proceedings of The 24th InternationalConference on Artificial Intelligence and Statistics (Proceedings of Machine Learning Research,Vol. 130), Arindam Banerjee and Kenji Fukumizu (Eds.). PMLR, 604612. Gintare Karolina Dziugaite and Daniel M Roy. 2017. Computing nonvacuous generalizationbounds for deep (stochastic) neural networks with many more parameters than training data.Association for Uncertainty in Artificial Intelligence (2017). Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, ThaoNguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, and others. 2024.Datacomp: In search of the next generation of multimodal datasets. Advances in NeuralInformation Processing Systems 36 (2024).",
  "Kevin A. Hallgren. 2012. Computing Inter-Rater Reliability for Observational Data: AnOverview and Tutorial. Tutorials in quantitative methods for psychology 8, 1 (2012), 2334": "Eric Hambro, Roberta Raileanu, Danielle Rothermel, Vegard Mella, Tim Rocktschel, HeinrichKttler, and Naila Murray. 2022. Dungeons and data: A large-scale nethack dataset. Advancesin Neural Information Processing Systems 35 (2022), 2486424878. Amy K. Heger, Liz B. Marquis, Mihaela Vorvoreanu, Hanna Wallach, and Jennifer Wort-man Vaughan. 2022. Understanding Machine Learning Practitioners Data DocumentationPerceptions, Needs, Challenges, and Desiderata. Proceedings of the ACM on Human-ComputerInteraction 6, CSCW2 (2022), 129.",
  "EnergyInnovation.2020.HowMuchEnergyDoDataCentersRe-allyUse?": "Md Mofijul Islam, Reza Mirzaiee, Alexi Gladstone, Haley Green, and Tariq Iqbal. 2022.CAESAR: An embodied simulator for generating multimodal referring expression datasets.Advances in Neural Information Processing Systems 35 (2022), 2100121015. Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy Bengio. 2020.Fantastic Generalization Measures and Where to Find Them. International Conference onLearning Representations (ICLR) (2020). Eun Seo Jo and Timnit Gebru. 2020. Lessons from archives: strategies for collecting so-ciocultural data in machine learning. In Proceedings of the 2020 Conference on Fairness,Accountability, and Transparency. ACM, Barcelona Spain, 306316. Lynn H Kaack, Priya L Donti, Emma Strubell, George Kamiya, Felix Creutzig, and DavidRolnick. 2022. Aligning artificial intelligence with climate change mitigation. Nature ClimateChange 12, 6 (2022), 518527. Publisher: Nature Publishing Group UK London. Julia Kaltenborn, Charlotte Lange, Venkatesh Ramesh, Philippe Brouillard, Yaniv Gurwicz,Chandni Nagda, Jakob Runge, Peer Nowack, and David Rolnick. 2023. ClimateSet: A large-scale climate model dataset for machine learning. Advances in Neural Information ProcessingSystems 36 (2023), 2175721792. Martin Klein, Herbert Van de Sompel, Robert Sanderson, Harihar Shankar, LyudmilaBalakireva, Ke Zhou, and Richard Tobin. 2014. Scholarly Context Not Found: One inFive Articles Suffers from Reference Rot. PLOS ONE 9, 12 (2014), e115253. Bernard Koch, Emily Denton, Alex Hanna, and Jacob Gates Foster. 2021. Reduced, Reusedand Recycled: The Life of a Dataset in Machine Learning Research. Advances in NeuralInformation Processing Systems. Ravin Kohli, Matthias Feurer, Katharina Eggensperger, Bernd Bischl, and Frank Hutter. 2024.Towards Quantifying the Effect of Datasets for Benchmarking: A Look at Tabular MachineLearning. In Workshop on Data-centric machine learning.",
  "Bianca Lamm and Janis Keuper. 2024. Retail-786k: a Large-Scale Dataset for Visual EntityMatching. In Workshop on Data-centric machine learning": "Susan Leavy, Eugenia Siapera, and Barry OSullivan. 2021. Ethical Data Curation for AI: AnApproach based on Feminist Epistemology and Critical Theories of Race. In Proc. of 2021AAAI/ACM Conf. on AI, Ethics, and Society. ACM, Virtual Event USA, 695703. Alycia Lee, Brando Miranda, Sudharsan Sundar, Allison Casasola, and Sanmi Koyeyo. 2024.Beyond Scale: The Diversity Coefficient as a Data Quality Metric for Variability in NaturalLanguage Data. In Workshop on Data-centric machine learning.",
  "Christopher A. Lee and Helen Tibbo. 2011. Wheres the Archivist in Digital Curation? Explor-ing the Possibilities through a Matrix of Knowledge and Skills. Archivaria 72, 0 (Dec. 2011),123168": "Nianyun Li, Naman Goel, and Elliott Ash. 2022. Data-Centric Factors in Algorithmic Fairness.In Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society. ACM, OxfordUnited Kingdom, 396410. Yuanzhi Li and Yingyu Liang. 2018. Learning Overparameterized Neural Networks viaStochastic Gradient Descent on Structured Data. In Advances in Neural Information ProcessingSystems, S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett(Eds.), Vol. 31. Curran Associates, Inc. Weixin Liang, Girmaw Abebe Tadesse, Daniel Ho, L. Fei-Fei, Matei Zaharia, Ce Zhang, andJames Zou. 2022. Advances, challenges and opportunities in creating data for trustworthyAI. Nature Machine Intelligence 4, 8 (2022), 669677. David Liu, Priyanka Nanayakkara, Sarah Ariyan Sakha, Grace Abuhamad, Su Lin Blod-gett, Nicholas Diakopoulos, Jessica R. Hullman, and Tina Eliassi-Rad. 2022.Examin-ing Responsibility and Deliberation in AI Impact Statements and Ethics Reviews. In Pro-ceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society. ACM, 424435. Shayne Longpre, Robert Mahari, Anthony Chen, Naana Obeng-Marnu, Damien Sileo, WilliamBrannon, Niklas Muennighoff, Nathan Khazam, Jad Kabbara, Kartik Perisetla, Xinyi Wu,Enrico Shippole, Kurt Bollacker, Tongshuang Wu, Luis Villa, Sandy Pentland, and SaraHooker. 2023. The Data Provenance Initiative: A Large Scale Audit of Dataset Licensing &Attribution in AI. arXiv:2310.16787 [cs].",
  "Alison McIntyre. 2023. Doctrine of Double Effect": "Milagros Miceli, Julian Posada, and Tianling Yang. 2022. Studying Up Machine Learning Data:Why Talk About Bias When We Mean Power? Proceedings of the ACM on Human-ComputerInteraction 6, GROUP (2022), 114. Milagros Miceli, Tianling Yang, Laurens Naudts, Martin Schuessler, Diana Serbanescu, andAlex Hanna. 2021. Documenting Computer Vision Datasets: An Invitation to ReflexiveData Practices. In Proceedings of the 2021 ACM Conference on Fairness, Accountability,and Transparency. ACM, Virtual Event Canada, 161172. Carlos Mougan, Richard Plant, Clare Teng, Marya Bazzi, Alvaro Cabrejas-Egea, Ryan Sze-YinChan, David Salvador Jasin, Martin Stoffel, Kirstie Jane Whitaker, and Jules Manser. 2023.How to Data in Datathons. Advances in Neural Information Processing Systems. Michael Muller, Ingrid Lange, Dakuo Wang, David Piorkowski, Jason Tsay, Q. Vera Liao,Casey Dugan, and Thomas Erickson. 2019. How Data Science Workers Work with Data:Discovery, Capture, Curation, Design, Creation. In Proceedings of the 2019 CHI Conferenceon Human Factors in Computing Systems. ACM, Glasgow Scotland Uk, 115.",
  "Carole L. Palmer, Nicholas Weber, Trevor Muoz, and Allen Renear. 2013. Foundations ofData Curation: The Pedagogy and Practice of Purposeful Work with Research Data. ArchiveJournal 3 (2013)": "Samir Passi and Solon Barocas. 2019. Problem Formulation and Fairness. In Proceedingsof the Conference on Fairness, Accountability, and Transparency (FAT* 19). Associationfor Computing Machinery, New York, NY, USA, 3948. David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, DanielRothchild, David So, Maud Texier, and Jeff Dean. 2021. Carbon emissions and large neuralnetwork training. arXiv preprint arXiv:2104.10350 (2021). Amandalynne Paullada, Inioluwa Deborah Raji, Emily M. Bender, Emily Denton, and AlexHanna. 2021. Data and its (dis)contents: A survey of dataset development and use in machinelearning research. Patterns 2, 11 (2021), 100336. Rohith Peddi, Shivvrat Arya, Bhrath Challa, Likhitha Pallapothula, Akshay Vyas, QifanZhang, Jikai Wang, Vasundhara Komaragiri, Eric Ragan, Nicholas Ruozzi, Yu Xiang, andVibhav Gogate. 2023. Put on your detective hat: Whats wrong in this video?. In Workshop onData-centric machine learning. Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cap-pelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. 2023. TheRefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and webdata only. arXiv preprint arXiv:2306.01116 (2023).",
  "Kenny Peng, Arunesh Mathur, and Arvind Narayanan. 2021. Mitigating Dataset HarmsRequires Stewardship: Lessons from 1000 Papers. Advances in Neural Information ProcessingSystems": "Aabha Pingle, Aditya Vyawahare, Isha Joshi, Rahul Tangsali, and Raviraj Joshi. 2023. L3Cube-MahaSent-MD: A Multi-domain Marathi Sentiment Analysis Dataset and Transformer Models.In Workshop on Data-centric machine learning. Mahima Pushkarna, Andrew Zaldivar, and Oddur Kjartansson. 2022. Data Cards: Purposefuland Transparent Dataset Documentation for Responsible AI. In 2022 ACM Conference onFairness, Accountability, and Transparency. ACM, Seoul Republic of Korea, 17761826. Inioluwa Deborah Raji, Emily Denton, Emily M. Bender, Alex Hanna, and AmandalynnePaullada. 2021. AI and the Everything in the Whole Wide World Benchmark. Advances inNeural Information Processing Systems.",
  "Seamus Ross. 2012. Digital Preservation, Archival Science and Methodological Foundationsfor Digital Libraries. New Review of Information Networking 17, 1 (2012), 4368": "Yuta Saito, Shunsuke Aihara, Megumi Matsutani, and Yusuke Narita. 2020. Open banditdataset and pipeline: Towards realistic and reproducible off-policy evaluation. arXiv preprintarXiv:2008.07146 (2020). Nithya Sambasivan, Shivani Kapania, Hannah Highfill, Diana Akrong, Praveen Paritosh, andLora M Aroyo. 2021. Everyone wants to do the model work, not the data work: DataCascades in High-Stakes AI. In Proceedings of the 2021 CHI Conference on Human Factors inComputing Systems. ACM, Yokohama Japan, 115. Morgan Klaus Scheuerman, Alex Hanna, and Emily Denton. 2021. Do Datasets Have Politics?Disciplinary Values in Computer Vision Dataset Development. Proceedings of the ACM onHuman-Computer Interaction 5, CSCW2 (Oct. 2021), 137. William Seymour, Xiao Zhan, Mark Cot, and Jose Such. 2023. A Systematic Review ofEthical Concerns with Voice Assistants. In Proceedings of the 2023 AAAI/ACM Conference onAI, Ethics, and Society. 131145. Rajat Shinde, Sujit Roy, Christopher E Phillips, Aman Gupta, Aditi Sheshadri, Manil Maskey,and Rahul Ramachandran. 2024. WINDSET: Weather Insights and Novel Data for SystematicEvaluation and Testing. In Workshop on Data-centric machine learning.",
  "Artem Vysogorets and Julia Kempe. 2024. Towards Robust Data Pruning. In Workshop onData-centric machine learning": "Mark D. Wilkinson, Michel Dumontier, IJsbrand Jan Aalbersberg, Gabrielle Appleton, MylesAxton, Arie Baak, Niklas Blomberg, Jan-Willem Boiten, Luiz Bonino da Silva Santos, Philip E.Bourne, Jildau Bouwman, Anthony J. Brookes, Tim Clark, Merc Crosas, Ingrid Dillo,Olivier Dumon, Scott Edmunds, Chris T. Evelo, Richard Finkers, Alejandra Gonzalez-Beltran,Alasdair J. G. Gray, Paul Groth, Carole Goble, Jeffrey S. Grethe, Jaap Heringa, Peter A. C. tHoen, Rob Hooft, Tobias Kuhn, Ruben Kok, Joost Kok, Scott J. Lusher, Maryann E. Martone,Albert Mons, Abel L. Packer, Bengt Persson, Philippe Rocca-Serra, Marco Roos, Rene vanSchaik, Susanna-Assunta Sansone, Erik Schultes, Thierry Sengstag, Ted Slater, George Strawn,Morris A. Swertz, Mark Thompson, Johan van der Lei, Erik van Mulligen, Jan Velterop, AndraWaagmeester, Peter Wittenburg, Katherine Wolstencroft, Jun Zhao, and Barend Mons. 2016.The FAIR Guiding Principles for scientific data management and stewardship. Scientific Data3, 1 (2016), 160018. Huang Xuanwen, Yang Yang, Wang Yang, Wang Chunping, Zhang Zhisheng, Xu Jiarong, ChenLei, and Vazirgiannis Michalis. 2022. DGraph: A Large-Scale Financial Dataset for GraphAnomaly Detection. In Advances in Neural Information Processing Systems 35 (NeurIPS2022). Rubing Yang, Jialin Mao, and Pratik Chaudhari. 2022. Does the Data Induce Capacity Controlin Deep Learning?. In Proceedings of the 39th International Conference on Machine Learning(Proceedings of Machine Learning Research, Vol. 162), Kamalika Chaudhuri, Stefanie Jegelka,Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (Eds.). PMLR, 2516625197.",
  ". Claims": "Question: Do the main claims made in the abstract and introduction accurately reflect thepapers contributions and scope?Answer: [Yes]Justification: We claim that assessing dataset development from a data curation lens canimprove the documentation practices in the NeurIPS Datasets and Benchmarks track. Wecontribute an evaluation framework to support this claim. The abstract and introductionprovide further details on our papers contributions and scope.Guidelines:",
  "The answer NA means that the paper has no limitation while the answer No means thatthe paper has limitations, but those are not discussed in the paper": "The authors are encouraged to create a separate \"Limitations\" section in their paper. The paper should point out any strong assumptions and how robust the results are toviolations of these assumptions (e.g., independence assumptions, noiseless settings,model well-specification, asymptotic approximations only holding locally). The authorsshould reflect on how these assumptions might be violated in practice and what theimplications would be. The authors should reflect on the scope of the claims made, e.g., if the approach wasonly tested on a few datasets or with a few runs. In general, empirical results oftendepend on implicit assumptions, which should be articulated. The authors should reflect on the factors that influence the performance of the approach.For example, a facial recognition algorithm may perform poorly when image resolutionis low or images are taken in low lighting. Or a speech-to-text system might not beused reliably to provide closed captions for online lectures because it fails to handletechnical jargon.",
  "If applicable, the authors should discuss possible limitations of their approach toaddress problems of privacy and fairness": "While the authors might fear that complete honesty about limitations might be used byreviewers as grounds for rejection, a worse outcome might be that reviewers discoverlimitations that arent acknowledged in the paper. The authors should use their bestjudgment and recognize that individual actions in favor of transparency play an impor-tant role in developing norms that preserve the integrity of the community. Reviewerswill be specifically instructed to not penalize honesty concerning limitations.",
  "If the contribution is a dataset and/or model, the authors should describe the steps takento make their results reproducible or verifiable": "Depending on the contribution, reproducibility can be accomplished in various ways.For example, if the contribution is a novel architecture, describing the architecture fullymight suffice, or if the contribution is a specific model and empirical evaluation, it maybe necessary to either make it possible for others to replicate the model with the samedataset, or provide access to the model. In general. releasing code and data is oftenone good way to accomplish this, but reproducibility can also be provided via detailedinstructions for how to replicate the results, access to a hosted model (e.g., in the caseof a large language model), releasing of a model checkpoint, or other means that areappropriate to the research performed. While NeurIPS does not require releasing code, the conference does require all submis-sions to provide some reasonable avenue for reproducibility, which may depend on thenature of the contribution. For example(a) If the contribution is primarily a new algorithm, the paper should make it clear howto reproduce that algorithm.",
  "(b) If the contribution is primarily a new model architecture, the paper should describethe architecture clearly and fully": "(c) If the contribution is a new model (e.g., a large language model), then there shouldeither be a way to access this model for reproducing the results or a way to reproducethe model (e.g., with an open-source dataset or instructions for how to constructthe dataset). (d) We recognize that reproducibility may be tricky in some cases, in which caseauthors are welcome to describe the particular way they provide for reproducibility.In the case of closed-source models, it may be that access to the model is limited insome way (e.g., to registered users), but it should be possible for other researchersto have some path to reproducing or verifying the results.",
  "Guidelines:": "The answer NA means that there is no societal impact of the work performed. If the authors answer NA or No, they should explain why their work has no societalimpact or why the paper does not address societal impact. Examples of negative societal impacts include potential malicious or unintended uses(e.g., disinformation, generating fake profiles, surveillance), fairness considerations(e.g., deployment of technologies that could make decisions that unfairly impact specificgroups), privacy considerations, and security considerations. The conference expects that many papers will be foundational research and not tiedto particular applications, let alone deployments. However, if there is a direct path toany negative applications, the authors should point it out. For example, it is legitimateto point out that an improvement in the quality of generative models could be used togenerate deepfakes for disinformation. On the other hand, it is not needed to point outthat a generic algorithm for optimizing neural networks could enable people to trainmodels that generate Deepfakes faster. The authors should consider possible harms that could arise when the technology isbeing used as intended and functioning correctly, harms that could arise when thetechnology is being used as intended but gives incorrect results, and harms followingfrom (intentional or unintentional) misuse of the technology. If there are negative societal impacts, the authors could also discuss possible mitigationstrategies (e.g., gated release of models, providing defenses in addition to attacks,mechanisms for monitoring misuse, mechanisms to monitor how a system learns fromfeedback over time, improving the efficiency and accessibility of ML).",
  ". Safeguards": "Question: Does the paper describe safeguards that have been put in place for responsiblerelease of data or models that have a high risk for misuse (e.g., pretrained language models,image generators, or scraped datasets)?Answer: [N/A]Justification: Our paper does not pose any such risks.Guidelines: The answer NA means that the paper poses no such risks. Released models that have a high risk for misuse or dual-use should be released withnecessary safeguards to allow for controlled use of the model, for example by requiringthat users adhere to usage guidelines or restrictions to access the model or implementingsafety filters.",
  ". Licenses for existing assets": "Question: Are the creators or original owners of assets (e.g., code, data, models), used inthe paper, properly credited and are the license and terms of use explicitly mentioned andproperly respected?Answer: [N/A]Justification: The paper does not use existing assets.Guidelines: The answer NA means that the paper does not use existing assets. The authors should cite the original paper that produced the code package or dataset. The authors should state which version of the asset is used and, if possible, include aURL. The name of the license (e.g., CC-BY 4.0) should be included for each asset. For scraped data from a particular source (e.g., website), the copyright and terms ofservice of that source should be provided. If assets are released, the license, copyright information, and terms of use in thepackage should be provided. For popular datasets, paperswithcode.com/datasetshas curated licenses for some datasets. Their licensing guide can help determine thelicense of a dataset.",
  ". New Assets": "Question: Are new assets introduced in the paper well documented and is the documentationprovided alongside the assets?Answer: [Yes]Justification: We introduce new assets in the form of evaluations of datasets using ourframework. The process of performing these evaluations is discussed in .Guidelines: The answer NA means that the paper does not release new assets. Researchers should communicate the details of the dataset/code/model as part of theirsubmissions via structured templates. This includes details about training, license,limitations, etc.",
  "According to the NeurIPS Code of Ethics, workers involved in data collection, curation,or other labor should be paid at least the minimum wage in the country of the datacollector": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with HumanSubjectsQuestion: Does the paper describe potential risks incurred by study participants, whethersuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)approvals (or an equivalent approval/review based on the requirements of your country orinstitution) were obtained?Answer: [N/A]Justification: The paper does not involve crowdsourcing nor research with human subjects.Guidelines:",
  "The answer NA means that the paper does not involve crowdsourcing nor research withhuman subjects": "Depending on the country in which research is conducted, IRB approval (or equivalent)may be required for any human subjects research. If you obtained IRB approval, youshould clearly state this in the paper. We recognize that the procedures for this may vary significantly between institutionsand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and theguidelines for their institution.",
  "C.1 Overview of research": "Background and Motivation: The usage of artificial intelligence has increased exponentially with applications in predicting outcomes related to education, employment, housing, and many more social, economic, and financial aspects of our lives. Archival studies have long dealt with large amounts of data and concerns of representativeness, ethics, integrity, and more with the use of data curation methods, theories, and frameworks. Machine learning research (MLR) has pinpointed the data underlying predictive models to be the largest contributor in introducing bias . Emerging studies have advocated for the prioritization of rigorous data curation practices often referred to as data work or dataset development in MLR . Introducing data curation concepts and principles can therefore improve the transparency and accountability of the dataset creation process within MLR. Objectives: We assess ML dataset development processes using principles and methods from archival studies and digital curation. We perform a synthesis and organization of existing work to enable the coherent usage of data curation frameworks, a taxonomy of data curation terms used within machine learning research, and a review of gaps and opportunities for data curation in machine learning. Method: Our research design for this study consists of the following: 1. Synthesizing literature on data curation concepts and principles central to ML data work. 2. Exploring the relevance of data curation concepts and principles through an illustration of how they can be",
  "curation is discussed in MLR, and how data curation can be further adopted": "Goals and contributions: This project deepens the scholarly and practical connections between the data curation and machine learning research communities and initiate directions for improvement within MLRs data practices. The outcomes present a novel perspective on improving documentation practices in machine learning through data curation. Through this project, we aim to further establish the connections between the data curation and machine learning research communities.",
  ". Secondly, existing datasets can be evaluated prior to publishing or reuse by applying the rubric to": "determine gaps that require further documentation and areas where bias can be introduced. In both cases, we aim for the rubric to be a practical and useful resource for researchers to engage with the dataset creation process using a data curation lens. The rubric was developed for the evaluation of ML datasets and has elements specific to the domain, including: requirements, data annotation, environmental footprint, and structured documentation.",
  ". Read the rubric to get familiarized with the elements and details that will be needed. 2. Review each element in the rubric individually": "a. For each element, first assess whether the minimum standard of documentation has been fulfilled. To do this, provide a pass/fail evaluation, where a pass is granted if all aspects specified under the minimum standard were discussed and a fail if they were only partially discussed or not discussed at all.",
  "granted if all aspects specified in the standard of excellence column were discussed, a partial is granted if one or more (but not all) were discussed, and a fail if none were discussed": "c. It is important to note both for points 2a and 2b that the quality of the responses/documentation is not being assessed but rather if the element was considered and reflected on in any capacity. The purpose of the rubric is to demonstrate the dataset creators thought process and provide transparency so that its reuse is based on a complete understanding of the dataset.",
  "b. Next, assess whether the documentation meets a standard of excellence, only if the minimum": "criteria received a pass. The standard of excellence is a full/partial/none evaluation. A full is granted if all aspects specified in the standard of excellence column were discussed, a partial is granted if one or more (but not all) were discussed, and a fail if none were discussed. c. It is important to note both for points 2a and 2b that the quality of the responses/documentation is not being assessed nor the correctness of the technicalities but rather if the element was considered and reflected on in any capacity. The purpose of the rubric is to demonstrate the dataset creators thought process and provide transparency so that its reuse is based on a complete understanding of the dataset and how it was developed.",
  "C.2.3 How to interpret authenticity, reliability, and representativeness": "It may be worth noting that the archival and digital curation perspectives that inform the evaluation framework are particularly important to interpreting the meaning of certain dimensions. Above all, the cluster of authenticity, integrity and reliability needs to be understood from this angle. They are closely related aspects, often treated or addressed by similar mechanisms, but they can be seen as analytically separate concepts. Here is an example. When you download a data set of weather observations from a platform, you may want to verify if the file you have downloaded in fact is the data set you wanted to get, i.e., is it an authentic copy? You may be able to verify this with various checksums, both on the level of the file (e.g. a hashcode of the file, as commonly provided for downloads) and on the level of observations in some cases. In this case, you are concerned with authenticity - you want to verify that the data set is what it purports to be.",
  "C. Toolkit Page 9": "Authenticity does not guarantee you, however, that the observations in the data set are any good. A good observation of weather data is one that you can rely on to accurately represent how the weather actually was at the temporal and spatial locations covered by the data. Other aspects of goodness are reflected in the many quality standards for data, but when you want the data set to be able to stand in for the facts it represents, you are concerned with reliability. In other words, reliability is very much about the relationship of the data to whatever it represents. If the data set is a compilation of social media posts, then reliability will relate to the question whether these contributions were really posted, etc. Integrity on the other hand refers to questions of tampering, errors, etc. For example, a dataset that lacks integrity is one for which we cannot assert that it contains all the items it originally contained, or that none of the items have been altered, falsified, or faked. Consider a textbook case for records and archives for the difference between the three. A passport is a document that comes with very special features to prove that it can stand in for the fact that you are a citizen of the issuing country. Its integrity refers to the question whether it has been tampered with - has the photo been peeled off, have pages been removed or added? etc. The passport comes with features to prevent and check integrity. Its authenticity refers to the fact that it is indeed a passport of that country and that it indeed asserts the facts it states. Most of its special features are designed to make it easy to verify that (cf. banknotes). But imagine: a government could issue a perfectly authentic passport for a person who doesnt exist. That would be authentic, but it would not be reliable. The reliability rests on the relationship to the person it represents. We trust an authentic passport to be reliable because we trust the processes that governments have instituted and honed over the centuries to ensure that passports are only issued to authenticated citizens. But border control will use a machine-readable passport to look up and compare the information shown with the information stored in a database. When they do that, they verify reliability. For a deep dive into the archival perspective on what makes records authentic and reliable, see . Consider next a digital photograph taken during sunlight with a pro-grade digital lens reflex camera of a pantone color set of whites with standardized, specified colors, where the white balance is erroneously set at fluorescent light. White balance relates to the color temperature of light: our eyes automatically adjust to different color temperatures, but a digital sensor does not. How an image looks on a screen is the result of computing it. In this case, the colors will not look very white on the photo without corrections to where the white point should be located. The photo as taken is an authentic photo providing an unreliable representation of its subject. If you transfer the photograph yourself out of the camera you can also put in place mechanisms to verify integrity (including fixity checks and integrity checks using hash sums and the like on the file). If you notice the error in color and then manually edit the binary code of the RAW file to set the white balance to the correct sunlight setting, the photograph would in fact lose the property of integrity since it has been tampered with (the hashcodes wont match), and it would lose the property of authenticity since that was not the original setting, but it would gain in reliability since the resulting color rendering would be a more accurate representation of how the colors should look. In this particular case, the fact that the subject of the photograph is standardized provides a ground truth that aids in verifying and assessing the photograph. Professional image processing software will be able to document both the as-taken setting and to-use setting of the photograph. Most photos, of course, are of subjects where this is much harder, and if the photograph is directly processed into a JPEG file, correcting white balance is much more difficult. Finally, representativeness is related to reliability, but its perspective is much more narrowly focused on the question whether a data set accurately represents the overall set of observations or entities that it claims to be a sample of. For instance, for a data set of social media posts, the question will arise if its representative of all platforms, all users, all topics, all media types, or various combinations of dimensions. All the statistical concepts around sampling apply as usual. Other data sets are not sampled out of an identified population but claim to stand for a general category so that representativeness is evaluated analytically, and so on.",
  "C.2.4 How to interpret findability, accessibility, interoperability, and reusability (FAIR)": "Note that this group of criteria are a direct representation of the widely used FAIR principles for research data sets, adopted and adapted for machine learning. We provide a simple checklist to assess whether the documentation of the dataset discusses the application of FAIR principles. This checklist is derived from the following tools and resources:",
  ". Minimum is easy, excellence is hard": "The evaluations for the minimum standard are meant to be generous. On the other hand, the standard of excellence criteria advocates for a high level of criticality, which is significantly harder to attain (compared to the minimum standard). The evaluations should only grant a Full if all criteria are satisfied.",
  "C. Toolkit Page 11": "If there is no documentation provided to evaluate an element, then dont make excuses for the dataset creators and evaluate it yourself or think of it as unnecessary. If you truly feel the element does not apply for that dataset, then that means its feedback for the rubric and that the element needs further work so it applies to all types of datasets.",
  ". Completing an evaluation using the rubric requires iteration. A single pass through the rubric is often": "insufficient, especially for datasets that include various sources of documentation. The first iteration should be a step-by-step completion of each element in the rubric by looking for relevant information, keywords in the research paper or other dataset documentation. However, in doing so, sections of the documentation may be missed. It is therefore suggested to first evaluate the dataset by applying the rubric sequentially and then reviewing all the dataset documentation sequentially. The final step should be iterating as needed and zooming out.",
  "only provide information to fill in gaps rather than be sufficient to completely evaluate any element": "4. None of the elements should receive an N/A comment or grade. 5. The standard of excellence criteria should only be evaluated if the minimum standard criteria passes. 6. A failure for any element should not be provided based on the quality of the dataset but rather the documentation and reflection on the process of developing the dataset. For example, if the documentation acknowledges that the sample is not representative and can therefore introduce a bias- this is not considered a Fail.",
  ". It is important to not evaluate the technical details provided but only evaluate the documentation. This": "means that evaluators should refrain from inferring the thought process or intention of the dataset creators based on their technical understanding of why the creators would develop their dataset in one way versus another. It is key to rely on explicit documentation only. This is important because the rubric assesses critical reflection around the dataset process not the quality of the dataset developed.",
  ". How to evaluate consistency and timeliness for suitability?": "Data quality is often defined as fitness for purpose and is multi-dimensional, meaning that its measured through more than one data quality dimension such as accuracy, completeness, etc. Suitability, in the rubric, evaluates whether dataset creators ensure that their datasets quality meets the purpose defined. For example, a dataset of math problems may not require timely data but may require consistent data (i.e., data presented in the same format). For standard of excellence, multiple data quality dimensions will apply for evaluation but potentially not all.",
  ". Why does the evaluation criteria for authenticity discuss data processing specifically?": "Data processing alters the authenticity of a digital object. Authenticity is dependent on the bits of information in a file. For example, if you download a dataset with a hash code and make copies of it, all copies will have the same hash code. However, if you perform data processing (which changes the bits), the hash code will no longer be the same. In the rubric, for the minimum standard, you evaluate whether the dataset creators validate and verify the authenticity of the data they are collecting. Whereas for standard of excellence, you evaluate whether they have",
  "representative, and is reliable OR that the dataset creators discuss their processes for ensuring these? If there is no mention of these qualities specifically, how do we evaluate them?": "For data quality elements, you are evaluating whether the dataset creators discussed their processes for ensuring that their dataset is suitable, authentic, reliable, has integrity, and the extent to which it is representative (and why if it is not). Remember the guiding principle- evaluate explicit documentation. We have added another guiding principle- dont make excuses. If no documentation is provided for these data quality elements, then dont make excuses for the dataset creators and evaluate it yourself or think of it as unnecessary. If you truly feel the element does not apply for that dataset, then that means its feedback for the rubric and that the element needs further work so it applies to all types of datasets.",
  ". Why are URLs not acceptable for findability?": "URLs are not considered findable because of the high likelihood of link rot (that the link over time will no longer be available). There are studies that show that academic papers are highly perceptible to link rot, eg: see . Instead, we want persistent identifiers like DOIs to make sure the dataset is findable in the future.",
  ". What is the difference between findability and accessibility?": "Findability is about a dataset being easily located. For example, if a publication provides a zenodo link to a dataset, that would make it findable (zenodo assigns a DOI to everything it publishes). So here were looking for a dataset being easily located, indexed, catalogued, etc. Accessibility is about whether a dataset can be opened and used and read. For example, is it in a format you can read, can you download it (i.e., is it retrievable), is the access blocked off via password-protection, are there access and authorization protocols? A dataset would then be findable if there was a link pointing to it but not accessible if you couldnt open it because you didnt have the password for it and there was no documentation of an access protocol. On the other hand, if a dataset was open-access (eg, through github) but didnt have a persistent identifier (eg DOI) and wasnt indexed in a repository like zenodo then it would be accessible but not findable. Since accessibility rests on accessing the content, a URL alone is not enough to make it accessible either. So even if the dataset is available through github there must be other documentation that provides any further information needed to access the content and metadata.",
  ". Can you provide further clarification for evaluating interoperability (especially standard of excellence)?": "For the minimum standard, the documentation must explain how the dataset can be integrated with other data and workflows. An example of that is that the data can be exported to popular, standard formats. For the standard of excellence, the data and metadata must use controlled vocabularies and link to other resources with qualified references. For example, metadata can be created using controlled vocabularies like the W3C's Data Catalog Vocabulaire (DCAT) model which defines terms like dataset vs data service, catalog (as a subclass of dataset), and so on. Please see this blurb from FAIR about qualified references: A qualified reference is a cross-reference that explains its intent. For example, X is regulator of Y is a much more qualified reference than X is associated with Y, or X see also Y. The goal therefore is to create as many meaningful links as possible between (meta)data resources to enrich the contextual knowledge about the data, balanced against the time/energy involved in making a good data model. To be more concrete, you should specify if one dataset builds on another data set, if additional datasets are needed to complete the data, or if complementary information is stored in a different dataset. In particular, the scientific links between the datasets need to be described. Furthermore, all datasets need to be properly cited (i.e., including their globally unique and persistent identifiers). .",
  "Extrinsic bias Extrinsic bias refers to bias that exists within the dataset which are reflections of social, historical biases": "Extrinsic bias is concerned with a view of a biased dataset from the outside. The argument is that an already-biased dataset can cause even innocent software to produce a biased outcome - and may look like people saying things such as the data made me do it. If we fail to remember that a dataset is biased, then we may treat it as fair or representative, harming people who have been excluded from it.",
  "Its application in online environments is complicated by the shift in technology and methods (see ), but the principle remains important": "In conventional human subject studies such as interviews, an IRB reviews ethics protocols and evaluates if the research is compliant with principles such as the proportionality principle. In social media research, things get complicated. In some situations, implied consent (see ) can be present but must be justified. In the case of LLMS, widespread data collection without consent has prompted massive ethical and legal concerns.",
  "A discussion of Twitter research ethics is a good start": "Intrinsic bias The ways in which we change the data from the inside of data science work-processes while we are preparing the data for modeling. Intrinsic bias is the bias data workers introduce to the dataset. Through practices of data wrangling, curation, and feature-engineering, humans make a series of decisions about how to treat their data, and those decisions may inadvertently introduce bias into the data.",
  "PID: persistent identifier": "Globally unique and persistent identifiers remove ambiguity in the meaning of your published data by assigning a unique identifier to every element of metadata and every concept/measurement in your dataset. [IDs] must be persistent. It takes time and money to keep web links active, so links tend to become invalid over time. Registry services guarantee resolvability of that link into the future, at least to some degree.",
  "Problem formulation": "The process with which a problem is formulated and the methods we use to define and measure it define the lens with which we see the problem. ...the problems we solve with data science are never insulated from the larger process of getting data science to return actionable results, these ends are very much an artifact of a contingent process of arriving at a successful formulation of the problem, and they cannot be easily decoupled from the process at arriving at these ends.[105:9]. For example, ONeil describes how proxies are used to quantify university excellence through indicators that are easily collected such as SAT scores, student-teacher ratios, and acceptance rates rather than through students learning experience, happiness, productivity, personal fulfillment, etc. . See for additional examples.",
  "Proportionality principle": "In ethics, it is understood that actions have positive and negative effects simultaneously. This is called double effect. Applications of double effect always presuppose that some kind of proportionality condition has been satisfied. Traditional formulations of the proportionality condition require that the value of promoting the good end outweigh the disvalue of the harmful side effect. In medicine, a surgeon may cause harm to a patients skin (negative) in order to save their heart (positive). It would not be permissible for a surgeon to open up someones chest just to get a better look or take a selfie with it because that would violate the proportionality principle.",
  "C. Toolkit Page 21": "wish to be acknowledged. Include a description of the workflow that led to your data: Who generated or collected it? How has it been processed? Has it been published before? Does it contain data from someone else that you may have transformed or completed? Reflexivity Questions of reflexivity ask us to consider who we should listen to and why, how to place actors ideas in a larger field of power, questions about our own relationship to actors theories of the world. Reflexivity asks us to approach our work with epistemological unease because we are always at risk of reproducing categories that reify power.",
  "C.7.1 Data curation in data science": "A vast amount of literature points to the datasets used for training machine learning models to be the source for introducing bias in model results leading to a call for increased documentation of datasets used in ML. Emerging research has proposed context documents interventions designed to accompany a dataset or ML model, allowing builders to communicate with users . The following are types of relevant context documents.",
  "Mitigating System Bias and Enabling Better Science. Transactions of the Association for Computational Linguistics 6, (2018), 587604": "Bender and Friedman develop data statements- a resource for NLP training datasets to be documented in order to mitigate bias and exclusion . Topics like dataset documentation in ML are often discussed as a part of data practices, data work, or dataset development. The following studies talk about stages of dataset development processes, how data scientists or data workers approach their data work, and the importance and impact of decisions made during the dataset development.",
  "Values in Computer Vision Dataset Development. Proc. ACM Hum.-Comput. Interact. 5, CSCW2 (October 2021), 137": "This paper discusses how documentation captures underlying values of data practices in machine learning (specifically computer vision tasks) . Specifically, publications are analyzed to understand the documentation and communication of datasets. The findings showcase the practices that are silenced (such as data work, context, positionality, and care) over those that are (wrongly) embraced such as model work, universality, and so on. This reading helps reflect on and understand how intrinsic bias can be introduced within datasets.",
  ". Nithya Sambasivan, Shivani Kapania, Hannah Highfill, Diana Akrong, Praveen Paritosh, and Lora M": "Aroyo. 2021. Everyone wants to do the model work, not the data work: Data Cascades in High-Stakes AI. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, May 06, 2021, Yokohama Japan. ACM, Yokohama Japan, 115. Through interviews with AI practitioners, Sambasivan et al. find that poor data practices in high-stakes AI domains (i.e., practices that do not prioritize data quality) lead to data cascades which are negative impacts of data issues .",
  "About Bias When We Mean Power? Proc. ACM Hum.-Comput. Interact. 6, GROUP (January 2022), 114": "Miceli et al. discuss that while we often recognize that there is bias in the datasets and their processes used for ML models, it is often ignored that this bias is a result of power inequities . The authors analyze data bias, data work, and data documentation from a power-aware framing as compared to a bias-oriented one. This paper provides an interesting shift in perspective which further illuminates the importance of reflexivity in data work.",
  "Conference on Human Factors in Computing Systems, April 29, 2022, New Orleans LA USA. ACM, New Orleans LA USA, 119": "This paper studies how data processing leads to different types of forgetting and where and how each type of forgetting occurs in the machine learning stack . Forgetting is conceptualized as the practice that occurs when choices are made about what data is kept, what it represents and so forth (therefore by designing a dataset in a given way, we remember only its current state, and forget the decisions, the erased data, etc.). This is a great paper for a deep dive into the various types of design decisions that impact the eventual dataset. The previous studies discuss aspects of data curation as dataset development. However, some ML studies have started discussing the importance of data curation by referencing archival studies and digital curation directly. These are included below.",
  ". Susan Leavy, Eugenia Siapera, and Barry OSullivan. 2021. Ethical Data Curation for AI: An Approach": "based on Feminist Epistemology and Critical Theories of Race. In Proc. of 2021 AAAI/ACM Conf. on AI, Ethics, and Society, July 21, 2021, Virtual Event USA. ACM, Virtual Event USA, 695703. Retrieved November 11, 2022 from This study discusses principles for ethical data curation based on race critical race theory and data feminism to improve the reflection of power, bias, and values in data processes and thereby improve transparency and accountability of AI systems .",
  ". Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On the": "Dangers of Stochastic Parrots: Can Language Models Be Too Big? . In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (FAccT 21), March 01, 2021, New York, NY, USA. Association for Computing Machinery, New York, NY, USA, 610623. . This paper discusses the potential risks of language models (and by extension other ML/AI systems) . The authors recommend a shift towards careful, reflective practices around datasets and model development along with a greater focus towards documentation.",
  "D.3 Evaluation consistency": "Additional note on methods: In the final round, four reviewers performed double coding for 30 datasets, each reviewing on average 15 datasets. Accordingly, we measured IRR with a one-way mixed, consistent, average-measures intra-class coefficient (ICC). The final round, as with Round 3 and 4, also consisted of a disagreement review. After the disagreement review, additional corrections were made for consistency. This included:",
  "score was automatically changed to match the 2nd reviewers score": "Additional results (IRR): We measured IRR per datasets and rounds as well as rubric categories. Specifically, the lowest ICC value for a given dataset in the initial rounds (training to round 4) was 0.45, indicating fair agreement, while the highest was 0.94, signifying excellent agreement. Subsequently, in the final round, the median ICC value for the 30 datasets evaluated was 0.90, with the highest ICC value for a given dataset as 1 indicating perfect agreement ().",
  ". IRR for datasets. Datasets 1-30 measured with two-way ICC and 31-60 with one-way ICC": "Despite the greater level of interpretation present when evaluating IRR across elements as compared to datasets, the final round had a median ICC value >0.82 across all rubric categories (Figures 3a and 3b). Greater degree of variability can also be seen for the earlier rounds as compared to round 5, with round 5 ICC values having a median of 0.83-0.98 across rubric categories. Furthermore, many elements had perfect agreement especially for the minimum standard criteria (context, purpose, motivation, context awareness, environmental footprint, data collection, data processing, data annotation, suitability, reliability, structured documentation, findability, and reusability). The IRR for rubric categories from rounds 1-4 is shown in .",
  "No disagreement, standard of excellence: instances where 1 of 2 reviewers gives a Partial evaluation, e.g., Partial, None, None": "We observed that the inconsistencies across datasets had markedly decreased by the final round. illustrates the trend in inconsistencies across datasets throughout multiple rounds of evaluations. The trend in major disagreements showed an even more pronounced reduction: for major inconsistencies under the minimum standard (Major, min), the inconsistency rates decreased significantly from initially high levels in the training and early rounds to much lower levels by Round 5. Similarly, major inconsistencies under the standard of excellence (Major, exc) see a sharp reduction, highlighting the impact of targeted improvements in rubric clarity and rater understanding. When considering all types of inconsistencies combined, the graph shows a substantial decrease from over 30% in the earliest phases to around 2% by the end of the final round, demonstrating nearly complete alignment among evaluators. This uniformity is indicative of the rubrics maturity as a tool for assessing dataset documentation quality.",
  ". Inconsistencies in the rubric application across datasets": "In analyzing the inconsistencies among elements across several rounds, we found a clear trend of decreasing inconsistencies across almost all rubric elements. Progressing through rounds, the inconsistencies in even the most challenging elements had been markedly reduced (a). By round 5, the percentage of inconsistencies reduced to 10% and under and only persisted for 7 of the 18 rubric elements (b).",
  "D.4 Positionality, reflection, and contributions": "Positionality begins by recognizing that knowledge is always produced from a certain perspective and that this perspective is always dependent on where we stand. Feminist standpoint theory emphasizes that there is no knowledge without a subject who knows. By identifying the location and situatedness of the knowing subject, we gain a better understanding of the knowledge in question. This makes the knowledge more robust, not less - it is a stronger version of objectivity than the flawed idea that knowledge can be fully disassociated from a subject and would embody a perspective of nowhere . By clarifying from where we speak, we can be more objective . This is true for individuals as much as for groups. A key emphasis is often on recognized dimensions of intersectionality including but not limited to gender, age, sex, race, class, religion, dis/abilities, or geographies. In addition, the knowledges each team member brings to a group are also highly relevant. Positionality statements can take many different shapes dependent on context and purpose. Beyond simply declaring aspects of identity, there is most value in reflecting how specifically these aspects intersect and shape the work being presented. Below, we will include positionality statements from each person disclosing what they are comfortable to disclose, followed by joint reflections on how the interaction of these perspectives have influenced this project. Tegan: My primary academic training is in machine learning, specifically deep learning. Much of my master's and doctoral work examined generalization and learning behaviour of deep networks on real-world data (e.g. for climate and video data) using empirical methodology, giving me a strong and grounded appreciation for the importance of data in machine learning. As a professor, I am increasingly engaged in the interdisciplinary research I believe is necessary to make the field of machine learning/AI more responsible and empirically rigorous. All of my training and much of my perspective remain influenced by the dominant paradigms of the field, which generally have strong western, white, colonial/extractive, heteronormative, and patriarchal biases; my identity as a mixed-race, female-presenting, first-generation scholar has likely sensitized me to these influences. My work seeking change in the field (e.g. to our notions of novelty, representation, intelligence, rigor, or appropriate scientific practice) has been mostly internally reform-oriented. Christoph: I am a white central European immigrant settler in Toronto, Canada, with an invisible disability. This identity has afforded me an odd juxtaposition of experiencing how privilege and oppression can intersect in our societies and has sensitized me over the years to theories and frameworks that help us make sense of these issues, including the intersectional feminist theories that shape this positionality statement. My education was in computer science and business informatics, but I am now a professor of information and am interacting closely with fields outside of computer science. My first published paper applied self-organizing maps to software cost estimation, but my doctoral and postdoctoral work developed computing and decision analysis frameworks for large-scale digital curation problems in libraries and archives. I became interested in long-term perspectives of environmental sustainability and the broader implications of technology on social justice partially because of the longer-term perspectives offered by ideals of stewardship and archiving. In the past decade I have grappled with the myths and illusions that are common in computing and that I too inherited via my education - including the flawed idea that technology is or can be neutral and that people's minds are information processors - and have built critical friendships with other fields that can help computing researchers, educators and practitioners see beyond these conventional horizons and make better sense of the role of computing in our societies in order to help reorient computing for environmental sustainability and social justice. This negotiation between different fields and disciplines can be uncomfortable especially when it implies critique. In this project I aim to be pragmatic in building bridges between the conceptual spaces of these fields and to translate valuable insights from the stewardship perspective of digital curation into practical guidelines that ML can adopt and use. Eshta: As a brown, female PhD student, I feel a lot of privilege in the opportunity to be a researcher where so many like me have not had the same opportunities, spaces, and circumstances. While I am a minority in these social identities, I am not in others and that is likely what has led to a position of privilege that I acknowledge. I am therefore also in a position where I can discuss and reflect on the impacts of my identities and worldviews on how and what I research. My academic background is rooted in information technology and data science, but my doctoral work is within a sociotechnical space. My perspective has thus shifted from seeing technology as a neutral entity, a tool that must be used and advanced further, to recognition of the need for a critical approach to technology that takes an ethics and justice based lens. The exposure to different worldviews that I have gained during my doctoral research has altered how I perceive the use of technology. For this project, I have pursued balancing a technical and",
  "critical perspective so that the adoption of data curation that we recommend has practical benefits and uptake within the ML community": "Harshit: HG brings a perspective informed by his background in data science and information technology, shaped by his studies in South Asia. He is attuned to both the potential and the challenges of scaling technology in contexts vulnerable to climate change. In his work at the Department of Computer Science, HG evaluates how technological advancements can be balanced against their environmental impacts, striving to produce evidence that supports stringent environmental standards through a public health lens. His position is situated at the intersections between machine learning, environmental policy, and health outcomes. Reyna: I am an Asian female PhD student. My primary academic training is in computer science and statistics, with a focus on climate informatics. Much of my academic research uses empirical methodologies to build machine learning and statistical models on real-world datasets, such as climate and scRNA-seq data. Throughout my PhD journey, I have recognized the importance of adopting a critical approach that incorporates ethics, responsibility, and sustainability in technology. This understanding is driven by my passion for studying climate change and the potential and limitations of machine learning to address it. I am particularly committed to promoting explainable AI, ethical AI, and responsible AI for climate change. Through my doctoral research, I have embraced a multifaceted and inclusive approach, striving to balance technical excellence with ethical considerations. This includes advocating for the responsible use of machine learning to tackle climate change emphasizing the importance of sustainability and social justice in my work. Ciara: I am a PhD candidate at a faculty of information, where my primary research focus is on how people work with cross-domain data. I also work as a data scientist with the federal government, where I develop AI/ML governance and support the implementation of AI/ML projects. In my academic research, I draw on data and information practice scholarship, intersectional feminist and queer data studies, and interdisciplinary studies. My commitments to interdisciplinarity and feminist perspectives are influenced by my positions as a mixed-race woman, first generation university student, and settler in Canada. These commitments mean that I approached this project centering the situatedness of the rubric (that is: we are proposing a set of data curation best practices for ML rather than implying a sole universal right way to do data work) and paying attention to the ways in which subject matter domain might have impacted the documentation of the datasets we evaluated. Note: The fact that these statements above are all slightly different is an expression of their authenticity - we refrained from homogenizing them to fit a common structure, choosing instead to leave our individual voices here and present additional context. Our perspectives interacted most directly in the making of the rubric. In our previous work, we discussed how disagreements in evaluations occurred because of differences in perspectives and areas of knowledge, including overlapping technologies, negotiating the depth of data curation expertise needed to apply the rubric, and challenges in scoping the extent of documentation dataset creators are responsible for . These disagreements were deliberated to reach a balance between points of view. Particularly, it is the different points of view that enabled the rubric to take a shape in which both data curation and ML concepts were harmonized. When assembling the team, the senior researchers intentionally selected candidates with diverse perspectives. The composition of this team proved well-suited for doing the translation and operationalizing process of data curation for ML. Simply having two varying perspectives on the same dataset and the same criterion helped create a more nuanced view, and this triangulation of perspectives often enriched the debate and reflection on data practices. Other teams with diverse compositions with an interest in ML data practices as well as interdisciplinarity complementarity may produce different assessment results, but a combination of perspectives will be valuable. We thus recommend having diversity and interdisciplinary complementarity in the application of the rubric. We did not aim to neutralize the specifics of each individual viewpoint in a supposedly neutral rubric - something that is never feasible - but instead sought consistency in the evaluation process. It is important to overcome misleading ideals of curation as neutral. As with other technical work, the neutral stance is illusory. Fields concerned with curation have also grappled with the realization that they cannot be neutral at all . As a consequence, current archival thought now recognizes and explores the implications of the subjective and inherently political nature of archival processes such as appraisal: the decision what to keep and what to discard [122:162].",
  "D. Additional information about rubric evaluations Page 32": "Our work in developing this framework extends a critical friendship from data curation towards machine learning. While machine learning already performs curation, it does so without adopting the fields standards which can aid in advancing the state of the art. Our aims were to clearly communicate knowledge from the data curation literature and communicate with machine learning researchers, in order to normatively encourage better practice.",
  "Author Contributions": "Eshta Bhardwaj (she/her) Her contributions for this project include the conceptualization of the framework, aiding with project administration, developing the methodology of conducting the evaluations, conducting evaluations, analyzing and visualizing the results, and writing, editing, and reviewing all drafts of the published work. Harshit Gujral (he/him) His contributions to this paper include the development and iteration of the evaluations, conducting evaluations, its writing, particularly methods, and results, and a discussion about reporting the environmental footprint of machine learning.",
  "D.5 How to report environmental footprint": "We recommend the following strategies to quantify the environmental footprint of dataset development: 1. Carbon Footprint Estimation Tools: Tools like LLMCarbon, which has been designed to provide end-to-end carbon footprint estimations for large language models, offer a valuable resource for dataset creators in ML . These tools allow for the prediction of carbon outputs based on diverse parameters such as hardware use, model architecture, and operational practices before the actual computational tasks begin.",
  ". Efficient Data Management Strategies: Research shows that end-to-end utilization of each GB stored in a data": "center is associated with approximately 5.12 kWh of energy consumption . Reducing data redundancy and implementing data pruning techniques can decrease the volume of data that needs to be stored and processed, subsequently reducing the energy consumed during these stages. Several researchers have called for using end-to-end efficiency as an evaluation criterion for publishing ML research on computationally intensive models besides accuracy and related measures .",
  ". Standardize Carbon Reporting: Developing a standardized protocol for reporting the carbon emissions of ML": "projects, as suggested by existing research , would facilitate greater transparency and accountability within the industry. For the very least, researchers can gather a rough estimate of the electricity consumption of their ML projects and can get an estimate of corresponding carbon dioxide equivalent emissions using tools like the ML Emissions Calculator and Green Algorithms tool . This could also involve detailed reporting of energy sources, hardware specifications, and operational efficiencies.",
  ". Large but Sparsely Activated Networks: The previous research discusses the energy efficiency of using large": "but sparsely activated deep neural networks (DNNs), which can consume less than one-tenth the energy of large, densely activated DNNs without sacrificing accuracy . For data processing, training, and deployment, the paper also emphasizes the impact of choosing energy-efficient data centers and hardware, along with strategically choosing data centers at a geographic location with a high renewable energy mix in the electricity grid .",
  "E. Changes to Rubric and Toolkit Page 33": "equipment manufacturing to operational use and beyond . This methodology, as discussed in the BLOOM model analysis , includes the energy consumed during model training and the emissions during model deployment and inference. We hope these recommendations provide a much-needed starting point for the ML community to begin quantifying the environmental footprint of their projects. However, reporting alone will not mitigate the environmental impacts of data curation in ML. Existing research encourages the ML community to engage in efficient data management strategies, optimize model architectures, and adopt green computing practices like selecting data centers that use renewable energy . Research suggests that while efficiency improvements are crucial, they are not always sufficient on their own to reduce overall carbon emissions due to these rebound effects . Rebound effects manifest when improvements in computational efficiency lead to an increased use of ML technologies, as lower operational costs and enhanced capabilities encourage more frequent training and deployment of larger models, potentially increasing total energy consumption . Historical data in sectors like automotive and residential energy use demonstrate that efficiency gains often lead to increased consumption, as savings are reinvested into more or expanded use of the technology, rather than resulting in a net decrease in energy use . This can be exacerbated by indirect rebound effects that occur when the application of energy-efficient ML technologies in various sectors leads to broader and more intensive use of these technologies, subsequently increasing energy demand across those sectors, despite individual efficiency improvements. In addition to energy efficiency-based measures, we call for the inclusion of digital sufficiency-based measures to mitigate potential rebound effects . These measures include limiting the growth of computational demands by setting strict computational budgets that reflect actual needs rather than maximum capacities. Additionally, it involves the creation of algorithms designed to perform effectively with minimal energy use, emphasizing necessity over excess. Regulatory measures are also critical, aimed at enforcing sustainable practices across the digital and computing sectors to ensure that efficiency improvements result in genuine reductions in carbon emissions. These mitigation strategies integrate sufficiency with technological innovation, ensuring that the advancement of ML contributes positively to environmental sustainability.",
  "Toolkit: Application Guidance": "The evaluation of the minimum standard of documentation was updated. It previously stated that a pass is granted for any amount or type of discussion around the element and a fail is granted only if there is no discussion around the element at all. In the new version of the toolkit, it states that a pass is granted if all aspects specified under the minimum standard were discussed and a fail if they were only partially discussed or not discussed at all. Based on the changes made to criteria of the rubric elements, the minimum standard could only be achieved if all criteria were fulfilled (not just one or few).",
  "Rubric: Context, purpose, motivation": "The criteria for the standard of excellence were made more explicit so that evaluators would have similar interpretations. Previously it stated, documentation explains how dataset can be reused beyond its original context. The current version expects documentation to discuss whether and how reuse is possible. Rubric: Requirements The criteria for the standard of excellence were simplified, and the requirement to state different approaches in formulating the problem apart from the final presented plan was removed.",
  "Rubric: Context awareness": "The criterion for the minimum standard was simplified by moving the requirement for a reflection on the dataset creators awareness of social, political, and historical context to the standard of excellence criteria for context, purpose, motivation. The criterion was moved for clarity: the reflection on context ties in with purpose and motivation, while the context awareness dimension focuses on positionality and reflexivity.",
  "Page 36": "M. Pratt, Vivek Ramanujan, Yonatan Bitton, Kalyani Marathe, Stephen Mussmann, Richard Vencu, Mehdi Cherti, Ranjay Krishna, Pang Wei Koh, Olga Saukh, Alexander Ratner, Shuran Song, Hannaneh Hajishirzi, Ali Farhadi, Romain Beaumont, Sewoong Oh, Alex Dimakis, Jenia Jitsev, Yair Carmon, Vaishaal Shankar, and Ludwig Schmidt. 2023. DataComp: In search of the next generation of multimodal datasets. November 02, 2023. Advances in Neural Information Processing Systems.",
  "Mark D. Wilkinson, Michel Dumontier, IJsbrand Jan Aalbersberg, Gabrielle Appleton, Myles Axton, Arie": "Baak, Niklas Blomberg, Jan-Willem Boiten, Luiz Bonino da Silva Santos, Philip E. Bourne, Jildau Bouwman, Anthony J. Brookes, Tim Clark, Merc Crosas, Ingrid Dillo, Olivier Dumon, Scott Edmunds, Chris T. Evelo, Richard Finkers, Alejandra Gonzalez-Beltran, Alasdair J. G. Gray, Paul Groth, Carole Goble, Jeffrey S. Grethe, Jaap Heringa, Peter A. C. t Hoen, Rob Hooft, Tobias Kuhn, Ruben Kok, Joost Kok, Scott J. Lusher, Maryann E. Martone, Albert Mons, Abel L. Packer, Bengt Persson, Philippe Rocca-Serra, Marco Roos, Rene van Schaik, Susanna-Assunta Sansone, Erik Schultes, Thierry Sengstag, Ted Slater, George Strawn, Morris A. Swertz, Mark Thompson, Johan van der Lei, Erik van Mulligen, Jan Velterop, Andra Waagmeester, Peter Wittenburg, Katherine Wolstencroft, Jun Zhao, and Barend Mons. 2016. The FAIR Guiding Principles for scientific data management and stewardship. Sci Data 3, 1 (2016), 160018."
}