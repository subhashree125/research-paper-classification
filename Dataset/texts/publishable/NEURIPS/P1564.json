{
  "Abstract": "This paper introduces the risk-sensitive control as inference (RCaI) that extends CaIby using Rnyi divergence variational inference. RCaI is shown to be equivalentto log-probability regularized risk-sensitive control, which is an extension of themaximum entropy (MaxEnt) control. We also prove that the risk-sensitive optimalpolicy can be obtained by solving a soft Bellman equation, which reveals severalequivalences between RCaI, MaxEnt control, the optimal posterior for CaI, andlinearly-solvable control. Moreover, based on RCaI, we derive the risk-sensitivereinforcement learning (RL) methods: the policy gradient and the soft actor-critic.As the risk-sensitivity parameter vanishes, we recover the risk-neutral CaI and RL,which means that RCaI is a unifying framework. Furthermore, we give another risk-sensitive generalization of the MaxEnt control using Rnyi entropy regularization.We show that in both of our extensions, the optimal policies have the same structureeven though the derivations are very different.",
  "Introduction": "Optimal control theory is a powerful framework for sequential decision making . In optimal controlproblems, one seeks to find a control policy that minimizes a given cost functional and typicallyassumes the full knowledge of the systems dynamics. Optimal control with unknown or partiallyknown dynamics is called reinforcement learning (RL) , which has been successfully applied tohighly complex and uncertain systems, e.g., robotics , self-driving vehicles . However, solvingoptimal control and RL problems is still challenging, especially for continuous spaces. Control as Inference (CaI), which connects optimal control and Bayesian inference, is a promisingparadigm for overcoming the challenges of RL . In CaI, the optimality of a state and controltrajectory is defined by introducing optimality variables rather than explicit costs. Consequently,an optimal control problem can be formulated as a probabilistic inference problem. In particular,maximum entropy (MaxEnt) control is equivalent to a variational inference problem using theKullbackLeibler (KL) divergence. MaxEnt control has entropy regularization of a control policy,and as a result, the optimal policy is stochastic. Several works have revealed the advantages ofthe regularization such as robustness against disturbances , natural exploration induced by thestochasticity , fast convergence of the MaxEnt policy gradient method . On the other hand, the KL divergence is not the only option available for variational inference. In, the variational inference was extended to Rnyi -divergence , which is a rich familyof divergences including the KL divergence. Similar to the traditional variational inference, thisextension optimizes a lower bound of the evidence, which is called the variational Rnyi bound.The parameter of Rnyi divergence controls the balance between mass-covering and zero-forcingeffects for approximate inference . However, if we use Rnyi divergence for CaI, it remainsunclear how affects the optimal policy, and a natural question arises: what objective does CaI usingRnyi divergence optimize?",
  "ContributionsThe contributions of this work are as follows:": "1. We reveal that CaI with Rnyi divergence solves a log-probability (LP) regularized risk-sensitive control problem with exponential utility (Theorem 2). The order parameter of Rnyi divergence plays a role of the risk-sensitivity parameter, which determineswhether the resulting policy is risk-averse or risk-seeking. Based on the result, we refer toCaI using Rnyi divergence as risk-sensitive CaI (RCaI). Since Rnyi divergence includesthe KL divergence, RCaI is a unifying framework of CaI. Additionally, we show that therisk-sensitive optimal policy takes the form of the Gibbs distribution whose energy is givenby the Q-function, which can be obtained by solving a soft Bellman equation (Theorem 3).Furthermore, this reveals several equivalence results between RCaI, MaxEnt control, theoptimal posterior for CaI, and linearly-solvable control . 2. Based on RCaI, we derive risk-sensitive RL methods. First, we provide a policy gradientmethod for the regularized risk-sensitive RL (Proposition 7). Next, we derive therisk-sensitive counterpart of the soft actor-critic algorithm through the maximization ofthe variational Rnyi bound (Subsection 4.2). As the risk-sensitivity parameter vanishes,the proposed methods converge to REINFORCE with entropy regularization andrisk-neutral soft actor-critic , respectively. One of their advantages over other risk-sensitive approaches, including distributional RL , is that they require only minormodifications to the standard REINFORCE and soft actor-critic. The behavior of therisk-sensitive soft actor-critic is examined via an experiment. 3. Although the risk-sensitive control induced by RCaI has LP regularization of the policy,it is not entropy, unlike the MaxEnt control with the Shannon entropy regularization. Tobridge this gap, we provide another risk-sensitive generalization of the MaxEnt control usingRnyi entropy regularization. We prove that the resulting optimal policy and the Bellmanequation have the same structure as the LP regularized risk-sensitive control (Theorem 6).The derivation differs significantly from that for the LP regularization, and for the analysis,we establish the duality between exponential integrals and Rnyi entropy (Lemma 5).",
  ": Relations of control problems": "Related workThe duality between con-trol and inference has been extensivelystudied .Inspired by CaI, reformulated model predictive con-trol (MPC) as a variational inference prob-lem. In , variational inference MPC us-ing Tsallis divergence, which is equivalentto Rnyi divergence, was proposed. Thedifference between our results and theirs isthat variational inference MPC infers feed-forward optimal control while RCaI infersfeedback optimal control. Consequently,the equivalence of risk-sensitive controland Tsallis variational inference MPC is not derived, unlike RCaI. The work proposed anEM-style algorithm for RL based on CaI, where the resulting policy is risk-seeking. However, risk-averse policies cannot be derived from CaI by this approach. Our framework provides the equivalencebetween CaI and risk-sensitive control both for risk-seeking and risk-averse cases. Risk-averse policies are known to yield robust control , and risk-seeking policies are usefulfor balancing exploration and exploitation for RL . Because of these merits, many effortshave been devoted to risk-sensitive RL . In , risk-sensitive RL with Shannon entropyregularization was investigated. However, their theoretical results are valid only for almost risk-neutralcases. Our results imply that LP and Rnyi entropy regularization are suitable for the risk-sensitiveRL. In , risk-sensitive control whose control cost is defined by Rnyi divergence was investigated,and it was shown that the associated Bellman equation can be linearized. However, it is assumedthat the transition distribution can be controlled as desired, which is not satisfied in general aspointed out in . On the other hand, our result shows that when the dynamics is deterministic, LP",
  "and Rnyi entropy regularized risk-sensitive control problems are linearly solvable without the fullcontrollability assumption of the transition distribution": "NotationFor simplicity, by abuse of notation, we write the density (or probability mass) functionsof random variables x, y as p(x), p(y), and the expectation with respect to p(x) is denoted by Ep(x).For a set S, the set of all densities on S is denoted by P(S). Rnyi entropy and divergence withparameter > 0, = 1 are defined as H(p) :=1",
  "{u:p1(u)p2(u)>0} p1(u)p2(u)1du. For the factor1": "(1) of H, we follow because this choice is convenient for the analysis in Subsection 3.2 rather than another commonchoice 1/(1 ). We formally extend the definition of H to < 0. Denote the Shannonentropy and KL divergence by H1(p), D1(p1p2), respectively because lim1 H(p) = H1(p),lim1 D(p1p2) = D1(p1p2). For further properties of the Rnyi entropy and divergence,see e.g., . The set of integers {k, k + 1, . . . , s}, k < s is denoted by [[k, s]]. A sequence{xk, xk+1, . . . , xs} is denoted by xk:s. The set of non-negative real numbers is denoted by R0.",
  ": Graphical model for CaI": "First, we briefly introduce the framework of CaI. For the de-tailed derivation, see Appendix A and . Throughout the pa-per, xt and ut denote X-valued state and U-valued control vari-ables at time t, respectively, where X Rnx, U Rnu, andL(U) > 0. Here, L denotes the Lebesgue measure on Rnu.The initial distribution is p(x0), and the transition density isdenoted by p(xt+1|xt, ut), which depends only on the currentstate and control input. Let T > 0 be a finite time horizon.CaI connects control and probabilistic inference problems byintroducing optimality variables Ot {0, 1} as in . Forct : X U R0, cT : X R0, which will serve as cost functions, the distribution of Ot is givenby p(Ot = 1|xt, ut) = exp(ct(xt, ut)), t [[0, T 1]] and p(OT = 1|xT ) = exp(cT (xT )). IfOt = 1, then (xt, ut) at time t is said to be optimal. The control posterior p(ut|xt, Ot:T = 1) iscalled the optimal policy. Let the prior of ut be uniform: p(ut) = 1/L(U), ut U. Although thischoice is common for CaI, the arguments in this paper may be extended to non-uniform priors. Then,for the graphical model in , the distribution of the optimal state and control input trajectory := (x0:T , u0:T 1) satisfies",
  "Qt(xt, ut) = ct(xt, ut) log Ep(xt+1|xt,ut) [exp(Vt+1(xt+1))] , t [[0, T 1]].(5)": "The recursive computation (4), (5) is similar to the Bellman equation for the risk-seeking control.However, it is not still clear what kind of performance index the optimal trajectory p(|Ot:T )optimizes because (4) does not coincide with that of the conventional risk-seeking control. Anindirect way to make this clear is variational inference. Let us consider finding the closest trajectorydistribution p() to the optimal distribution p(|O0:T ). The variational distribution is chosen as",
  "Derivation of optimal control and further equivalence results": "In this subsection, we derive the optimal policy of (9) and give its characterizations. For the analysis,we do not need the non-negativity of the cost ct. We only sketch the derivation, and the detailed proofis given in Appendix B. Similar to the conventional optimal control problems, we adopt the dynamicprogramming. Another approach based on variational inference will be given in Subsection 4.2.Define the optimal (state-)value function Vt : X R and the Q-function Qt : XU R as follows:",
  "Because of the softmin operation above, the left equation in (13) is called the soft Bellman equation.Theorem 3. Assume that": "U exp (Qt(x, u)) du < holds for any t [[0, T 1]] and x X.Let > 1, = 0. Then, the unique optimal policy of Problem (9) is given by (13). Especially whenthe dynamics is deterministic, i.e., p(xt+1|xt, ut) = (xt+1 ft(xt, ut)) for some ft : X U Xand the Dirac delta function , it holds that",
  "U exp (Qt(x, u)) du < is satisfied for example when ct is bounded for anyt [[0, T]] and L(U) < . The linear quadratic setting also fulfills this assumption; see (16)": "Theorem 3 suggests several equivalence results:RCaI and MaxEnt control for deterministic systems. First, we emphasize that even thoughthe equivalence between unregularized risk-neutral and risk-sensitive controls for deterministicsystems is already known, our equivalence result for MaxEnt and regularized risk-sensitive controlsis nontrivial. This is because the regularized policy t makes a system stochastic even though theoriginal system is deterministic, and for stochastic systems, the unregularized risk-sensitive controldoes not coincide with the risk-neutral control. This implies that the optimal randomness introducedby the regularization does not affect the risk sensitivity of the policy. This provides insight into therobustness of MaxEnt control . Note that mentioned that the MaxEnt control objective canbe reconstructed by the risk-sensitive control objective under the heuristic assumption that the costfollows a uniform distribution. However, this assumption is not satisfied in general. Our equivalenceresult does not require such an unrealistic assumption.",
  "RCaI and optimal posterior. Although the optimal posterior p(ut|xt, Ot:T ) yields the MaxEnt con-trol for deterministic systems as mentioned in , it is not known what objective p(ut|xt, Ot:T )": "optimizes for stochastic systems. Theorem 3 gives a new characterization of p(ut|xt, Ot:T ). Byformally substituting = 1 into (11), the Bellman equation for computing t becomes (4), (5) forthe optimal posterior p(ut|xt, Ot:T ). Note that even if the cost function ct in (9) is replaced by ct inProposition 1, {t } is still optimal. Therefore, by taking the limit as 1, the policy t (ut|xt)in Theorem 3 converges to p(ut|xt, Ot:T ), and in this sense, the policy p(ut|xt, Ot:T ) is risk-seeking.Corollary 4. Under the assumptions in Proposition 1, it holds that",
  "where Vt and Qt are given by (11), (13) with = 1": "RCaI for deterministic systems and linearly-solvable control. For deterministic systems, bythe transformation Et(xt) := exp(Vt(xt)), the Bellman equation (14) becomes linear: Et(xt) =exp(ct(xt, u))Et+1( ft(xt, u))du. That is, when the system is deterministic, the LP-regularizedrisk-sensitive control, or equivalently, the MaxEnt control is linearly solvable , whichenables efficient computation of RL. Even for the MaxEnt control, this fact seems not to be mentionedexplicitly in the literature. RCaI and unregularized risk-sensitive control in linear quadratic setting. Similar to the un-regularized and MaxEnt problems , Problem (9) with a linear system p(xt+1|xt, ut) =N(xt+1|Atxt + Btut, t) and quadratic costs ct(xt, ut) = (xt Qtxt + ut Rtut)/2, cT (xT ) =xT QT xT /2 admits an explicit form of the optimal policy:",
  "(Rt + Btt+1(I tt+1)1Bt)1.(16)": "Here, N(|, ) denotes the Gaussian density with mean and covariance . The definition of tand the proof are given in Appendix C. In general, the mean of the regularized risk-sensitive controldeviates from the unregularized risk-sensitive control. However, in the linear quadratic Gaussian(LQG) case, the mean of the optimal policy (16) coincides with the optimal control of risk-sensitiveLQG control without the regularization .",
  "Another risk-sensitive generalization of MaxEnt control via Rnyi entropy": "The Shannon entropy regularization E[H1(t(|xt))] of the MaxEnt control problem (7) can berewritten as E[log t(ut|xt)]. In this sense, the risk-sensitive control (9) is a natural extension of (7).Nevertheless, for the risk-sensitive case, the interpretation of log t(ut|xt) as entropy is no longeravailable. In this subsection, we provide another risk-sensitive extension of the MaxEnt control.Inspired by the Rnyi divergence utilized so far, we employ Rnyi entropy regularization:",
  "U exp(( )g(u))du , u U.(20)": "For the precise statement and the proof, see Appendix D. By applying Lemma 5 with = 1, = to (18), we obtain the optimal policy of (17) as follows.Theorem 6. Assume that ct is bounded below for any t [[0, T]]. Assume further that for any x Xand t [[0, T 1]], it holds that",
  "Uexp ((1 )Qt(xt, u)) du, t [[0, T 1]], xt X.(22)": "Recall that the LP regularized risk-sensitive optimal control is given by (11), (13) while theRnyi entropy regularized control is determined by (21), (22), and Qt(xt, ut) = ct(xt, ut) +1 log Ep(xt+1|xt,ut)[exp(Vt+1(xt+1))]. Hence, the only difference between the risk-sensitive con-trols for the LP and Rnyi regularization is the coefficient in the soft Bellman equations (13), (22).",
  "Risk-sensitive policy gradient": "In this subsection, we consider minimizing the cost (9) by a time-invariant policy parameterized ast(u|x) = ()(u|x), Rn. Let C() := cT (xT )+T 1t=0 (ct(xt, ut)+log ()(ut|xt)) and pbe the density of the trajectory under the policy (). Then, Problem (9) can be reformulated as theminimization of J()/ where J() :=p() exp(C())d. To optimize J()/ by gradientdescent, we give the gradient J(). The proof is shown in Appendix F. Proposition 7. Assume the existence of densities p(xt+1|xt, ut), p(x0). Assume further that ()is differentiable in , and the derivative and the integral can be interchanged as J() =[p() exp(C())]d. Then, for any function b : Rnx R, it holds that",
  "Risk-sensitive soft actor-critic": "In Subsection 3.2, we used dynamic programming to obtain the optimal policy {t }. Rather, in thissection, we adopt a standard procedure of variational inference . First, we find the optimal factort for fixed s, s = t as follows. The proof is deferred to Appendix G.Proposition 8. For t [[0, T 1]], let s, s = t be fixed. Let > 1, = 0. Then, the optimalfactor t := arg mintP(U) D1+(pp(|O0:T = 1)) is given by",
  "where Zt(xt) is the normalizing constant": "By (24), the optimal factor t is independent of the past factors s, s [[0, t 1]]. Therefore, thevariational Rnyi bound in (8) is maximized by optimizing t in backward order from t = T 1 tot = 0, which is consistent with the dynamic programming. Associated with (24), we define",
  "Uexp (Qt (xt, u)) du.(30)": "Especially when t(ut|xt) = t (ut|xt), it holds that V t (xt) = logexp(Qt (xt, u))du,which coincides with the soft Bellman equation in (13). In summary, in order to obtain the optimalfactor t , it is sufficient to compute V t and Qt in a backward manner. Next, we consider the situation when the policy is parameterized as ()t(ut|xt), Rn and thereis no parameter that gives the optimal factor ()t= t . To accommodate this situation, we utilizethe variational Rnyi bound. One can easily see that the maximization of the Rnyi bound in (8) withrespect to a single factor t is equivalent to the following problem.",
  "J() = ( + 1)Ep(xt,ut) log ()(ut|xt)TQ()(xt, ut) + log ()(ut|xt). (36)": "Thanks to the transformation T, the expectations appear linearly, and an unbiased gradient estimatorcan be obtained by removing them. By simply replacing the gradients of the soft actor-critic with(34)(36), we obtain the risk-sensitive soft actor-critic (RSAC). It is worth mentioning that sinceRSAC requires only minor modifications to SAC, techniques for stabilizing SAC, e.g., reparameteri-zation, minibatch sampling with a replay buffer, target networks, double Q-network, can be directlyused for RSAC.",
  ": Average episode cost forRSAC with some and standard SAC": "Unregularized risk-averse control is known to be robustagainst perturbations in systems . Since the robustnessof the regularized cases has not yet been established the-oretically, we verify the robustness of policies learned byRSAC through a numerical example. The environment isPendulum-v1 in OpenAI Gymnasium. We trained controlpolicies using the hyperparameters shown in Appendix H.There were no significant differences in the control perfor-mance obtained or the behavior during training. On the otherhand, for each , one control policy was selected and wasapplied to a slightly different environment without retrain-ing. To be more precise, the pendulum length l, which is 1.0during training, is changed to 1.25 and 1.5; See . Inthis example, it can be seen that the control policy obtainedwith larger has a smaller performance degradation due toenvironmental changes. This robustness can be considered a benefit of risk-sensitive control.",
  ": Empirical distributions of the costs for different risk-sensitivity parameters": "distribution for SAC ( = 0) with l = 1.5 deviates from the original one (l = 1.0), and anotherpeak of the distribution appears in the high-cost area. This means that there is a high probabilityof incurring a high cost, which clarifies the advantage of RSAC. The more risk-seeking the policybecomes, the less robust it becomes against the system perturbation.",
  "Conclusions": "In this paper, we proposed a unifying framework of CaI, named RCaI, using Rnyi divergencevariational inference. We revealed that RCaI yields the LP regularized risk-sensitive control withexponential performance criteria. Moreover, we showed the equivalences for risk-sensitive control,MaxEnt control, the optimal posterior for CaI, and linearly-solvable control. In addition to theseconnections, we derived the policy gradient method and the soft actor-critic method for the risk-sensitive RL via RCaI. Interestingly, Rnyi entropy regularization also results in the same form of therisk-sensitive optimal policy and the soft Bellman equation as the LP regularization. From a practical point of view, a major limitation of the proposed risk-sensitive soft actor-critic isits numerical instability for large || cases. Since appears, for example, as exp(Q()(xt, ut)) inthe gradients (34)(36), the magnitude of that does not cause the numerical instability depends onthe scale of costs. Therefore, we need to choose depending on environments. In the experimentusing Pendulum-v1, || that is larger than 0.03 results in the failure of learning due to the numericalinstability. Although it is an important future work to address this issue, we would like to note that thisissue is not specific to our algorithms, but occurs in general risk-sensitive RL with exponential utility.It is also important how to choose a specific value of the order parameter 1 + of Rnyi divergence.Since we showed that determines the risk sensitivity of the optimal policy, we can follow previousstudies on the choice of the sensitivity parameter of the risk-sensitive control without regularization.The properties of the derived algorithms also need to be explored in future work, e.g., the compatibilityof a function approximator for RSAC .",
  "Benjamin Eysenbach and Sergey Levine, Maximum entropy RL (provably) solves some robustRL problems, in International Conference on Learning Representations, 2022": "Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan,Vikash Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, and Sergey Levine, Soft actor-criticalgorithms and applications, arXiv preprint arXiv:1812.05905, 2018. Jincheng Mei, Chenjun Xiao, Csaba Szepesvari, and Dale Schuurmans, On the global con-vergence rates of softmax policy gradient methods, in International Conference on MachineLearning. PMLR, 2020, vol. 119, pp. 68206829.",
  "David Nass, Boris Belousov, and Jan Peters, Entropic risk measure in policy search, in 2019IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2019, pp.11011106": "Erfaun Noorani and John S. Baras, Risk-sensitive REINFORCE: A Monte Carlo policygradient algorithm for exponential performance criteria, in 2021 60th IEEE Conference onDecision and Control (CDC). IEEE, 2021, pp. 15221527. Jingliang Duan, Yang Guan, Shengbo Eben Li, Yangang Ren, Qi Sun, and Bo Cheng, Dis-tributional soft actor-critic: Off-policy reinforcement learning for addressing value estimationerrors, IEEE Transactions on Neural Networks and Learning Systems, vol. 33, no. 11, pp.65846598, 2022. Jinyoung Choi, Christopher Dance, Jung-Eun Kim, Seulbin Hwang, and Kyung-sik Park,Risk-conditioned distributional soft actor-critic for risk-sensitive navigation, in 2021 IEEEInternational Conference on Robotics and Automation (ICRA). IEEE, 2021, pp. 83378344.",
  "Christopher M. Bishop, Pattern Recognition and Machine Learning, Springer, 2006": "Richard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour, Policy gradientmethods for reinforcement learning with function approximation, in Advances in NeuralInformation Processing Systems, 1999, vol. 12, pp. 10571063. Antonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, and NoahDormann, Stable-baselines3: Reliable reinforcement learning implementations, Journal ofMachine Learning Research, vol. 22, no. 268, pp. 18, 2021.",
  "subject toxt+1 = ft(xt, ut, wt), ut U, t [[0, T 1]],(48)ut t(|x) given xt = x,(49)x0 Px0.(50)": "Here, {wt}T 1t=0 is an independent sequence, x0 is independent of {wt}, > 0 is the regularizationparameter, and is the risk-sensitivity parameter satisfying > 1, = 0. Note that we donot assume the existence of densities p(xt+1|xt, ut), p(x0). To perform dynamic programming forProblem (47), define the value function and the Q-function as",
  "CLinear quadratic Gaussian setting": "In this appendix, we derive the regularized risk-sensitive optimal policy in the linear quadraticGaussian setting.Theorem 9. Let p(xt+1|xt, ut)=N(Atxt + Btut, t) and ct(xt, ut)=(xt Qtxt +ut Rtut)/2, cT (xT ) = xT QT xT /2, where t, Qt, and Rt are positive definite matrices for anyt, and N(, ) denotes the Gaussian distribution with mean and covariance . Let X = Rnx,U = Rnu. Assume that there exists a solution {t}Tt=0 to the following Riccati difference equation:",
  "(u) := exp(g(u))exp(g(u))du , (u) := exp(h(u))exp(h(u))du(74)": "are the unique optimal solutions to (72), (73), respectively. To see this, note that if (72), (73) hold for > 0, = 1, they hold for any R \\ {0, 1}. Indeed, when < 0, let := 1 > 1 and forh B{,1}, let g := h. Since g B{1,1}, by (72), we have",
  "HDetails of the experiment": "The implementation of the risk-sensitive SAC (RSAC) algorithm follows the stable-baselines3 version of the SAC algorithm, which means that the RSAC algorithm also implements some tricksincluding reparameterization, minibatch sampling with a replay buffer, target networks, and doubleQ-network. Now, we introduce a series of hyperparameters listed in shared for both SAC andRSAC algorithms.",
  "ParameterValue": "optimizerAdam learning rate103discount factor0.99regularization coefficient0.1target smoothing coefficient0.005replay buffer size105number of critic networks2number of hidden layers (all networks)2number of hidden units per layer256number of samples per minibatch256activation functionReLU As mentioned in , there were no significant differences in the control performance obtainedor the behavior during training shown in with those hyperparameters. However, when istoo small or too large, the training process becomes unstable due to the gradient vanishing problemand the gradient exponential growth problem, respectively, leading to training failure. To this end,we compare the robustness of the trained policies with RSAC ( {0.02, 0.01, 0.01, 0.02}) andthe standard SAC, which corresponds to = 0, in the experiment. For each learned policy, wedo trail for 20 times. For each trail, we take 100 sampling paths to calculate the average episodecost. In , the error bars depict the max and min values, and the points depict the mean valueamong the 20 trails. We change the length of the pole l in the Pendulum-v1 environment to testthe robustness of the learned policies (l = 1.0 m in the original environment). For the training,we used an Ubuntu 20.04 server (GPU: NVIDIA GeForce RTX 2080Ti). The code is available at"
}