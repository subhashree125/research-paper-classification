{
  "Abstract": "In-context learning allows models like transformers to adapt to new tasks from a fewexamples without updating their weights, a desirable trait for reinforcement learning(RL). However, existing in-context RL methods, such as Algorithm Distillation(AD), demand large, carefully curated datasets and can be unstable and costlyto train due to the transient nature of in-context learning abilities. In this workwe integrated the n-gram induction heads into transformers for in-context RL.By incorporating these n-gram attention patterns, we significantly reduced thedata required for generalization up to 27 times fewer transitions in the Key-to-Door environment and eased the training process by making models lesssensitive to hyperparameters. Our approach not only matches but often surpassesthe performance of AD, demonstrating the potential of n-gram induction heads toenhance the efficiency of in-context RL.",
  "Return": "baselinengram : Performance comparison for dif-ferent number of training goals between ourmethod and Algorithm Distillation (AD), anin-context reinforcement learning method(Laskin et al., 2022). Our method demon-strates similar performance with less traininggoals (128 vs. 512) and in general outper-forms the baseline in Key-to-Door environ-ment. In such stricter data scenario it is ableto perform on par with AD using 27x lesstransitions in total. See for details. In-context learning is a powerful ability of autore-gressive models, such as transformers (Vaswani et al.,2023) or state-space models (Gu et al., 2022), allow-ing them to infer and solve tasks from just a fewexamples without updating models weights (Brownet al., 2020). Such an adaptation ability is usefulin Reinforcement Learning (RL), where after exten-sive pre-training, the agent can adapt on-the-fly tounseen tasks or environments (Grigsby et al., 2023;Ramrakhya et al.). In-context Reinforcement Learning (ICRL) methodsthat learn from offline datasets were first introducedby Laskin et al. (2022) and Lee et al. (2023). In theformer work, Algorithm Distillation (AD), authorspropose to distill the policy improvement operatorfrom a collection of learning histories of RL algo-rithms, after which an agents is able to generalize to",
  "unseen tasks in-context. In the latter, authors similarly show it is possible to generalize from a datasetof interactions, provided that the optimal actions are also available": "Both methods require specifically curated data, which is demanding to obtain (Nikulin et al., 2024b).Several methods were proposed to tackle this problem (Zisman et al., 2024; Kirsch et al., 2023) byeasing the data acquisition process. In addition, the in-context ability itself is transient (Singh et al.,2024) and hard to predict its emergence from the cross-entropy loss alone (Agarwal et al., 2024),making the training of such models unstable and expensive in terms of training budget. Our workaims to solve aforementioned obstacles and presents changes made to transformers attention heads,which can significantly speed up training process and decrease the total amount of data needed forin-context learning to emerge. It has been shown that a central mechanism that enables in-context learning in transformers isinduction heads (Olsson et al., 2022). Edelman et al. (2024) studied the emergence of these statisticalinduction heads on synthetic data and concluded that transformers obtain a simplicity bias towardsplain uni-grams. Akyrek et al. (2024) take a step forward in this direction, showing that during in-context learning of transformers, there appear higher-order induction heads in the attention mechanism,which capture different n-grams in the sequence. They propose to hardcode this mechanism intoa transformer, creating an n-gram layer which is used interchangeably with standard multi-headattention mechanism. Intuitively, a transformer benefits from it by not learning this complicatedbehaviour by itself, rather it straightforwardly receives an inductive bias n-gram heads provide. Thisapproach significantly boosts perplexity even when applied to recurrent sequential models, indicatingthat n-grams are a central mechanism for in-context learning.",
  "We bring up these findings to ICRL setting and show that:": "N-grams heads decrease the amount of data needed for generalization on novel tasks.By leveraging them, it is possible to reduce the total amount of transitions in training databy 27x compared to the original method of Laskin et al. (2022). The results are presented in. N-grams help to ease training of in-context models. By employing n-gram heads, oneneeds considerable less time doing hyperparameters search, thus making a model lesssensitive to hyperparameters and making it cheaper to train. The results are presented in.",
  "Related Work": "In-context RL. The key feature behind ICRL is the adaptation ability of a pretrained agent . Ingeneral, it relies on the transformers ability to infer a task from the history of interactions withan environment. Mller et al. (2021) show that transformers are capable of performing Bayesianinference, which is known for its applicability for reasoning under uncertainty (Ghavamzadeh et al.,2015). Laskin et al. (2022) proposed to pretrain a transformer on learning histories of RL algorithmswhich allows it to implicitly learn policy improvement operator. During inference on unseen tasks, atransformer is able to improve its policy by observing a context and to infer a task from it. However,such approach requires specific datasets, which may be expensive to collect (Nikulin et al., 2024b).To tackle this, it has been proposed to generate datasets following noise curriculum instead of trainingthousands of agents (Zisman et al., 2024) or make augmentations to existing data (Kirsch et al., 2023).Our work follows the direction of democratizing data restrictions, but instead of working with data,we introduce a model-centric approach, making a transformer to perform in-context reinforcementlearning with less data. N-Gram and Transformers. N-Gram statistical models has been known for decades and used inthe statistical approach to language modelling (Brown et al., 1992; Kneser & Ney, 1995). Morerecent approaches (Roy et al., 2022; Liu et al., 2024) study the application of n-grams to transformermodels, finding that they can increase the overall performance. Akyrek et al. (2024) discover that atransformer implicitly implements 2-gram attention pattern when solving in-context learning task,which authors denote as a higher order of induction head (Olsson et al., 2022). They explicitlyimplement 1-, 2- and 3-gram attention layers and observe a significant reduction in perplexity.Another work (Edelman et al., 2024) directly investigates the behavior of n-gram induction headsduring training process. Authors find that transformers are biased towards simple solutions, thus",
  "Method": "We build our method on AD (Laskin et al., 2022) as our baseline. In its core, it uses learning historiesof RL algorithms that are trained to solve a single task in an environment. The data with learningprogress of multiple RL agents then passed to a transformer which learns to predict the next actionvia cross-entropy loss. This results in a transformer that is able to adapt to unseen tasks on inferencewithout any weight updates. The details of implementation can be found in Appendix A. However, AD suffers the same problems as any in-context algorithm does. Learning of the optimalsolution can be delayed by a tendency of transformers to learn simple structures at first (Edelmanet al., 2024). Besides, the nature of in-context ability is unstable and can fade into in-weights regimeas the training progresses, considerably complicating the emergence of adaptation ability (Singhet al., 2024). To combat these obstacles, we implement n-gram attention layer (Akyrek et al., 2024) as one of thelayers of a transformer. In essence, it hardcodes computations of n-gram statistics into the transformeritself, rather than waiting for them to emerge naturally. The attention pattern that is calculated fromthe input sentence is defined as:",
  "NGHn hl= W1hl + W2A(n)hl": "Where n is the n-grams length, W1 and W2 are learnable projection matrices and hl is an embeddingfrom a previous transformer layer. In simple terms, we look for n-gram occurrences and with the helpof A(n) attention pattern force gradients to flow only through tokens that co-occur in the sequence. To count n-gram statistics we use raw input sequence. However, since we are working in RL setting,the input sequence has a form of (s0, a0, r0, ..., sn, an, r0), so in our experiments we tested twoapproaches. We either compare the equivalence of full transitions (si, ai, ri) = (sj, aj, rj) or juststates (si = sj). Throughout the text we use the terms learning histories and tasks. The task is a predefined grid ora pair of grids an agent must come to upon it receives a reward. The learning history is an orderedcollection of states, actions and rewards an RL algorithm observed (or produced) while learning tosolve a single task. When we say we generated a dataset of n tasks with m learning histories, itmeans for each of the task there are at least m n learning histories per task. Unlike Laskin et al.(2022), we distinguish between tasks and learning histories, as it is often the case with real data whenmany trajectories correspond to only a few tasks (Yu et al., 2019; Galloudec et al., 2024).",
  "Experiment Setup": "We test our method on two environments, Dark Room and Dark Key-to-Door, originally presentedby Laskin et al. (2022). Both environments are grid-worlds of size 9x9. In Dark Room, an agent isspawned at the center and needs to find a goal, after which it receives a reward of 1. The task forKey-to-Door is similar, but at first an agents seeks for the key which allows it to open the door (bothof these actions lead to getting a reward of 1). For each environment, we access the performance ofICRL algorithms only on unseen tasks. The total number of goals is 81 and 6561 for Dark Room andKey-to-Door respectively. The more detailed description can be found in Appendix C. To show that our method is more stable, we choose to report the results using the Expected MaxPerformance protocol (EMP) (Dodge et al., 2019; Kurenkov & Kolesnikov, 2022). By doing so,we do not report the maximum performance of a single checkpoint, rather we show the expectedperformance for a certain computational budget. By using this approach we simultaneously compareour method with a baseline in terms of easiness of training and maximum achieved performance. 0.5 1.0",
  "Expected Max Return": "500 hist # hyperparameter assignments 0.50 1.00 1.50 1.79 2.00 750 hist 0.50 1.00 1.50 1.852.00 1000 hist states[s, a, r]baseline : Results on Key-to-Door. We demonstrate the ability of our method to generalize when thedata is extremely low in a more complex environment than Dark Room. We fix the total number ofgoals with 100, significantly shrinking the number of learning histories. Keep in mind that for thebaseline method to converge to a model with the same return, it needs 2048 goals and 2048 learninghistories (Laskin et al., 2022). We show that our method needs 27x data. The baseline method canno longer converge with that few data and its performance plateaus with the increasing number ofhyperparameter assignments. Next, we further restrict the amount and diversity of data available to train. We set up an experimentin Key-to-Door, a more comprehensive environment with the total of 6561 tasks, with only 100training tasks and 500, 750, 1000 learning histories. Comparing to Laskin et al. (2022), we use 27xless data. The detailed calculations are provided in Appendix B. It can be observed from thatthe baseline method cannot find a model that is able to generalize to unseen goals in such a setting.In turn, out method demonstrates performance on par with what Laskin et al. (2022) report in theirwork. To ensure that our implementation of a baseline (AD) can solve the environments, we presentthe performance of a baseline that is trained on optimal hyperparameters in Appendix F.",
  "Results": "We start with showing the ability of our method to significantly reduce the hyperparameter sensitivityof AD. In the top row of we vary the number of learning histories, when the number oftraining goals is fixed. It can be seen that for our method to converge to the optimal hyperparameters,it needs only about 20 hyperparameter assignments. At the same time, for a baseline AD one needsto search through 400 different hyperparamenter combinations on average to find a model that cansolve this relatively small environment. In the next set of experiments we investigate an ability of our method to work in low-data regime.We fix the number of learning histories and vary the number of goals only. Note that in the previousexperiment we had 60 tasks for training, but now we considerably reduce their quantity. In thebottom row of the experiments with 10, 20, 30 training goals are shown. Neither methodcan generalize to unseen task when presented only with 10 training goals. However, it is differentfor 20 and 30 goals, where our method finds the optimal model in approximately 15 hyperparameterassignments. Noticeably, the baseline method completely fails to learn from the constrained data,which highlights the applicability of our method when no large dataset is available. For transparencyreasons, we show the full-length plots in Appendix E. 0.50 1.00 1.50 1.76 2.00",
  "Conclusion and Future Work": "In our work we show that incorporating n-gram induction heads can significantly ease training ofin-context reinforcement learning algorithms. Our findings are twofold: (i) we show that n-gramheads can notably decrease a sensitivity to hyperparameters of in-context RL algorithm and (ii) wedemonstrate that our method is able to generalize from much less data than the baseline AlgorithmDistillation (Laskin et al., 2022) approach. We speculate these findings are mainly attributed tothe imperfect nature of in-context learning itself: a tendency of transformers to converge to simplesolutions first (Edelman et al., 2024) and transitivity of in-context ability itself (Singh et al., 2024). While we believe our findings are promising, there are some limitations of current work. Furtherresearch is needed to make our method compatible with continuous observations, which can greatlyexpand the applicability of the method. Also, one might consider scaling to larger models and morecomprehensive environments, e.g. XLand-Minigrid (Nikulin et al., 2024a) or Meta-World (Yu et al.,2019), which are not yet solved.",
  "Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam,P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R.,": "Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray,S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D.Language models are few-shot learners. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan,M., and Lin, H. (eds.), Advances in Neural Information Processing Systems, volume 33, pp.18771901. Curran Associates, Inc., 2020. URL",
  "Gu, A., Goel, K., and R, C. Efficiently modeling long sequences with structured state spaces, 2022.URL": "Kirsch, L., Harrison, J., Freeman, D., Sohl-Dickstein, J., and Schmidhuber, J. Towards general-purpose in-context learning agents. Workshop on Distribution Shifts, 37th Conference on NeuralInformation ..., 2023. Kneser, R. and Ney, H. Improved backing-off for m-gram language modeling. In 1995 InternationalConference on Acoustics, Speech, and Signal Processing, volume 1, pp. 181184 vol.1, 1995. doi:10.1109/ICASSP.1995.479394. Kurenkov, V. and Kolesnikov, S. Showing your offline reinforcement learning work: Online evaluationbudget matters. In Chaudhuri, K., Jegelka, S., Song, L., Szepesvari, C., Niu, G., and Sabato, S.(eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 ofProceedings of Machine Learning Research, pp. 1172911752. PMLR, 1723 Jul 2022. Laskin, M., Wang, L., Oh, J., Parisotto, E., Spencer, S., Steigerwald, R., Strouse, D., Hansen, S.,Filos, A., Brooks, E., et al. In-context reinforcement learning with algorithm distillation. arXivpreprint arXiv:2210.14215, 2022.",
  "Mller, S., Hollmann, N., Arango, S. P., Grabocka, J., and Hutter, F. Transformers can do bayesianinference. arXiv preprint arXiv:2112.10510, 2021": "Nikulin, A., Kurenkov, V., Zisman, I., Agarkov, A. S., Sinii, V., and Kolesnikov, S. Xland-minigrid:Scalable meta-reinforcement learning environments in jax. In Automated Reinforcement Learning:Exploring Meta-Learning, AutoML, and LLMs, 2024a. Nikulin, A., Zisman, I., Zemtsov, A., Sinii, V., Kurenkov, V., and Kolesnikov, S. Xland-100b: A large-scale multi-task dataset for in-context reinforcement learning. arXiv preprint arXiv:2406.08973,2024b. Olsson, C., Elhage, N., Nanda, N., Joseph, N., DasSarma, N., Henighan, T., Mann, B., Askell, A.,Bai, Y., Chen, A., et al. In-context learning and induction heads. arXiv preprint arXiv:2209.11895,2022.",
  "Singh, A., Chan, S., Moskovitz, T., Grant, E., Saxe, A., and Hill, F. The transient nature of emergentin-context learning in transformers. Advances in Neural Information Processing Systems, 36, 2024": "Sinii, V., Nikulin, A., Kurenkov, V., Zisman, I., and Kolesnikov, S. In-context reinforcement learningfor variable action spaces. In Salakhutdinov, R., Kolter, Z., Heller, K., Weller, A., Oliver, N.,Scarlett, J., and Berkenkamp, F. (eds.), Proceedings of the 41st International Conference onMachine Learning, volume 235 of Proceedings of Machine Learning Research, pp. 4577345793.PMLR, 2127 Jul 2024. URL",
  "Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., andPolosukhin, I. Attention is all you need, 2023. URL": "Yu, T., Quillen, D., He, Z., Julian, R., Hausman, K., Finn, C., and Levine, S. Meta-world: Abenchmark and evaluation for multi-task and meta reinforcement learning. In Conference on RobotLearning (CoRL), 2019. URL Zisman, I., Kurenkov, V., Nikulin, A., Sinii, V., and Kolesnikov, S. Emergence of in-contextreinforcement learning from noise distillation. In Proceedings of the 41st International Conferenceon Machine Learning, 2024.",
  "AModel Implementation": "We build our model on the base of Decision Transformer (Chen et al., 2021), which implementationis taken from CORL (Tarasov et al., 2024). We modify it by removing return-to-go completely. Sincein RL we operate with tuples of states, actions and rewards, we concatenate them into one large\"token\" to preserve sequence length size as in (Lee et al., 2023; Sinii et al., 2024).",
  "CEnvironments": "Dark Room. 2D POMDP with discrete state and action spaces (Laskin et al., 2022). The grid size is9 9, where an agent has 5 possible actions: up, down, left, right and do nothing. The goal is to finda target cell, the location of which is not known to the agent in advance. The episode length is fixedat 20 time steps, after which the agent is reset to the middle of the grid. The reward r = 1 is given forevery time step the agent is on the goal grid, otherwise r = 0. The agent does not know the positionof the goal, hence it is driven to explore the grid. In total, there are 81 goals. Key-to-Door. Similar to Dark Room, but it first requires an agent to find an invisible key and then thedoor. Without a key, the door will not open. The reward is given when the key is found (r = 1) andonce the door is opened (also r = 1), after which the game terminates. The agent then resets to arandom grid. The maximum episode length is 40, and since we can control the location of the keyand door, there are around 6.5k possible tasks."
}