{
  "Abstract": "Neural network architecture design requires making many crucial decisions. Thecommon desiderata is that similar decisions, with little modifications, can be reusedin a variety of tasks and applications. To satisfy that, architectures must providepromising latency and performance trade-offs, support a variety of tasks, scaleefficiently with respect to the amounts of data and compute, leverage availabledata from other tasks, and efficiently support various hardware. To this end,we introduce AsCANa hybrid architecture, combining both convolutional andtransformer blocks. We revisit the key design principles of hybrid architectures andpropose a simple and effective asymmetric architecture, where the distribution ofconvolutional and transformer blocks is asymmetric, containing more convolutionalblocks in the earlier stages, followed by more transformer blocks in later stages.AsCAN supports a variety of tasks: recognition, segmentation, class-conditionalimage generation, and features a superior trade-off between performance andlatency. We then scale the same architecture to solve a large-scale text-to-imagetask and show state-of-the-art performance compared to the most recent public",
  "Introduction": "Convolutional neural networks (CNNs) and transformers have been deployed in a wide spectrumof real-world applications, addressing various computer vision tasks, e.g., image recognition and image generation . CNNs encode many desirable properties like translationequivariance facilitated through the convolutional operators . However, they lack the input-adaptiveweighting and the global receptive field capabilities offered by transformers . By recognizingthe potential benefits of combining these complementary strengths, research endeavors explorehybrid architectures that integrate both convolutional and attention mechanisms .Recently, such architectures witness huge success when scaled up to train large-scale text-to-image(T2I) diffusion models , enabling a vast range of visual applications, such as contentediting and video generation . One prominent research area for hybrid models involves the creation of building blocks that caneffectively combine convolution and attention operators . While these efforts seek to use thestrengths of both operators, their faster attention alternatives only approximate the global attention,leading to compromised model performance as lacking a global receptive field. Thus, they neces-sitate incorporating additional layers to compensate for the capacity reduction due to the attentionapproximation. On the other hand, minimal effort is directed toward optimizing the entire hybridarchitecture. This raises the question: Is the current macro design of hybrid architecture optimal? In this work, we propose a simple yet effective hybrid architecture, wherein the number of convolutionand transformer blocks is asymmetric in different stages. Specifically, we adopt more convolutionalblocks in the early stages, where the feature maps have relatively large spatial sizes, and more trans-former blocks at the later stages. This design is verified across different tasks. For example, in ,we demonstrate superior advantages of our model for the latency-performance trade-off on ImageNet-1K classification task. Particularly, our model achieves even faster speed than many works fea-turing efficient attention operations. Additionally, we scale up our architecture to train the large-scaleT2I diffusion model for high-fidelity generation (, , and Tab. 3). Furthermore, consideringthe high training cost for the large-scale T2I diffusion models, we introduce a multi-stage trainingpipeline to improve the training efficiency. Overall, our contributions can be summarized as follows:",
  "We revisit the macro design principles of hybrid convolutional-transformer architectures andpropose one with asymmetrically distributed convolutional and transformer blocks": "For the image classification task, we perform extensive latency analysis on the ImageNet-1K datasetand show our models achieve superior throughput-performance trade-offs than existing works (see). Notably, we show that the model runtime can be significantly accelerated even without anyacceleration optimization on attention operations. Additionally, we show our pre-trained modelson ImageNet-1K can be applied to downstream tasks such as semantic segmentation.",
  "For the class-conditional generation on ImageNet-1K (256 256), our asymmetric UNet achievessimilar performance as state-of-the-art models with half the compute resources (see Tab. 4)": "For the text-to-image generation task, we demonstrate that our network can be scaled up for thelarge-scale T2I generation with a better performance-latency trade-off than existing public models(as in Tab. 3). Additionally, we improve the training efficiency through a multi-stage trainingpipeline, where we first train the model on a small dataset, i.e., ImageNet-1K, for T2I generation,then fine-tune the model on a large-scale dataset.",
  "Related Works": "Efficient Hybrid Architectures. Over the past decade, convolutional neural networks (CNNs) have achieved unprecedented performance in various computer vision tasks . Despitenumerous attempts to improve CNNs , they still face limitations, particularly interms of their local and restrictive receptive fields. On the other hand, the vision transformer (ViT) treats images as sequences of tokens , facilitating the computation of global dependencies betweentokens to enhance the receptive field. However, ViTs come with quadratic computation complexityconcerning input resolution. To address this, several studies have aimed to develop more efficientattention operations . Recently, there has been a growing interest in exploringmodels that go beyond pure convolutional or transformer blocks, combining them within a singlearchitecture to harness the spatial and translational priors from convolutions and the global receptivefields from the attention mechanism . We revisit the design choices of hybridarchitectures and propose a new design with a fast runtime while maintaining high performance. Wereveal that optimizing the macro architecture directly allows us to use the original attention for asuperior latency-performance trade-off compared to existing works. Most importantly, our designednetworks can be applied to different domains, e.g., image recognition and image generation, withboth superior latency-performance trade-offs. Text-to-Image Diffusion Models.The development of text-to-image models, such as GAN-based models , autoregressive models , and diffusion models , has en-abled the generation of high-fidelity images by using textual descriptions. Among them, dif-fusion models demonstrate advantages in stable training processes and the scalability of large-scale neural networks . Recent studies explore the enhancements of text-to-image diffusion models through various directions, such as designing new network architec-tures , improving inference efficiency during multi-steps sam-pling , and accelerating the training convergence . Our designednetwork can be scaled up for the T2I generation when modified to a UNet architecture . Themodel achieves better performance-latency trade-offs than open-sourced models. Furthermore, wecan reduce the training cost through the proposed two-stage training pipeline.",
  "Methods": "This section describes our network and training designs in detail. First, we justify our choices ofutilizing the convolutional and transformer operations for the building blocks, which we then useto design the asymmetric architecture (Sec. 3.1). Second, we scale up the network architecture fortraining the text-to-image diffusion models (Sec. 3.3). We point out that it is easier to ablate thedesign of building blocks on the image classification task due to the lower resource requirements(training/evaluation compute) in comparison to image generation.",
  "Architecture Design: Asymmetric Convolution-Attention Networks (AsCAN)": "Our hybrid architecture consists of a mix of convolutional and transformer blocks, which enables oper-ating on different input resolutions seamlessly and allows us to be pareto efficient w.r.t. performance-compute trade-off compared to other architectures. Before delving into the exact architectureconfiguration, we discuss our choice of the building blocks. We use X RHW C to representthe input feature map X that has H W spatial dimensions along with C channels. We denoteY RHWCas the output of a building block (convolution or transformer) and symbolfor the function composition operator. In the following, we present our design choices based onImageNet-1K classification task by varying different convolution and transformer blocks. Convolutional Block (C). There are various choices for a convolutional block that can be used inour architecture, e.g., MBConv , FusedMBConv , and ConvNeXt . While MBConv blockhas been used in many networks , the presence of depthwise convolutions results in lowaccelerator utilization for high-end GPUs . To better understand the latency-performance trade-offfor various convolutional blocks, we experiment with the same hybrid architecture yet with differentconvolutional blocks. As in Tab. 1 (more experimental settings in Sec. 4.1), FusedMBConv has betterlatency on A100 and V100 GPUs than others, while maintaining high performance. Therefore, weadopt FusedMBConv (C) for the convolutional block and represent it as Y = X + P SE C(X),where C is a full 3 3 convolution with 4 channel expansion followed by batch norm and GeLUnon-linearity , SE is a squeeze-and-excite operator with shrink ratio of 0.25, and P is a1 1 convolution to project to C channels. Transformer Block (T). Similar to the convolution blocks, there are many choices for attentionmechanism in transformers, e.g., blocks containing efficient attention mechanisms like multi-axialattention and hierarchical attention . Tab. 2 shows that vanilla attention mechanism provides",
  "(b)(c)": ": Example AsCAN architectures for Image Classification & Text-to-Image Generation.(a): The architecture for the image classification and details of the convolutional (C) and transformerblocks (T). AsCAN includes Stem (consisting of convolutional layers) and four stages followed bypooling and classifier. (b): The UNet architecture for the image generation. The Down blocks (thefirst three blocks starting from left) have the reverted reflection as the Up blocks (the first three blocksstarting from right). (c): The details for C and T used in UNet. For the T that performs the crossattention between latent image features and textural embedding, the Q matrix comes from the texturalembedding. Note that, compared to image classification, the C and T blocks for image generationonly adds extra components to incorporate the input time-step and textual embeddings.",
  "Y = X + Yattn + Ymlp; Yattn = P A PQKV( X); Ymlp = P PMLP( X),(1)": "where X = LN (X), LN denotes layer normalization, denotes the GeLU non-linearity ,A is the multi-headed self-attention function, PQKV & PMLP denote the linear projection to the QKVand MLP space, respectively, and P denotes the projection operator to the same space as the input.Note that these update equations are inspired by recent works , where the feed-forward andthe self-attention operators are arranged in parallel in order to get improved throughput with marginalreduction in performance. Design Choices. Given the FusedMBConv (C) and Vanilla Transformer (T) blocks, we introduce themacro design for our hybrid architecture. For the image classification task, we follow the existingworks to utilize a four-stage architecture (excluding the convolutional Stem at thebeginning and classifier components at the end). However, before finalizing our architecture, we stillhave a design question to answer, namely, in which configuration should we arrange these buildingblocks? For instance, CoAtNet chooses to stack convolutional blocks in the first two stages andtransformer blocks in the remaining stages while MaxViT stacks convolutional and transformerblocks alternatively throughout the entire network. A formal algorithm requires evaluating all possibleC and T configurations, which is computationally expensive. Even neural architecture search leads toexponential search space. Thus, we follow a naive strategy that is based on the following principles: C before T. In any stage, we prefer convolutions followed by transformer blocks to capture theglobal dependence between the features aggregated by the convolutions, as they can capture scaleand translation-aware information. Our ablations in Appendix Tab. 9 justify this design choice. Fixed first stage. As transformer blocks have quadratic computation complexity in terms of thesequence length, we prefer the first stage to contain only convolutional blocks to improve theinference latency. Equal blocks in remaining stages. For ease of analysis, we fix the number of blocks in theremaining stages, i.e., stages 2 to 4, to be four. Once we finalize the basic configuration, we canscale these stages similar to earlier works to achieve larger models.",
  "symmetric since both C and T blocks are equal within a stage. In contrast, the configuration ofCCCT-CCTT-CTTT is asymmetric since C and T blocks are not equal in stages 2 and 4": "Final Architecture. Given these design principles, we list various promising configurations for thebuilding blocks (C & T), and analyze their inference throughput and top-1 accuracy. Tab. 2 providesthese configurations along with performance and runtime on different GPUs. For a better comparisonwith existing works, we also add the configurations of CoAtNet and MaxViT for reference. From theresults, we can draw the following conclusions:",
  "While increasing the number of transformer blocks in the network improves the throughput, it doesnot result in improved accuracy, as demonstrated by C6 vs C9": "Given above analysis, we prefer the C1 configuration for simplicity along with better accuracy vslatency trade-off. Since transformer blocks capture global dependencies, we prefer at least someblocks in the early layers as well in conjunction with the convolutional blocks. Similarly, we preferhaving few convolutional blocks in the later stages to capture the spatial, translation, or scale awarefeatures. To be concrete, our final architecture includes a convolutional stem followed by four stagesand the classifier head. In the first stage, we only keep convolutional blocks. In the second stage, wekeep 75% convolutional and 25% transformer blocks. This trend is reversed in the final stage. Forthe third stage, we keep equal number of convolutional and transformer blocks. We visualize thisconfiguration in along with the diagrams representing the convolutional and transformer blocks. Remarks. For simplicity, in this work, we focus on the configuration in which to combine the C & Tblocks, and leverage vanilla quadratic attention and convolutional mechanisms. We can incorporatefaster alternatives to quadratic attention to further boost the performance and latency trade-offs. Weleave this exploration to future research.",
  "Discussion": "While many works in the literature focus on improving the trade-off between performance andmultiply-add operation counts (MACs), most of the time, MACs do not translate to throughput gains.It is primarily due to the following reasons: Excessive use of operations that do not contribute to MACs. Such tensor operators include reshape,permute, concatenate, stack, etc. While these operations do not increase MACs, they burdenthe accelerator with tensor rearrangement. The cost of such rearrangement grows with the sizeof the feature maps. Thus, whenever these operations occur frequently, the throughput gainsdrop significantly. For instance, MaxViT uses axial attention that includes many permuteoperations for window/grid partitioning of the spatial features. Similarly, SMT includes manyconcatenation and reshape operations in the SMT-Block. It reduces the throughput significantlyeven though their MACs are lower than AsCAN (see Appendix Tab. 7). MACs do not account for non-linear accelerator behavior in batched inference. Another issue isthat MACs do not account for the non-linear behavior of the GPU accelerators in the presence oflarger batch sizes. For instance, with small batch sizes (B=1), the GPU accelerator is not fullyutilized. Thus, the benchmark at this batch size is not enough. Instead, one should benchmark atlarger batch sizes to see consistency between architectures. Lack of efficient CUDA operators for specialized building blocks. Many architectures proposespecialized and complex attention or convolution building blocks. While these blocks offer betterMACs-vs-performance trade-offs, it is likely that their implementation relies on naive CUDAconstructs and does not result in significant throughput gains. For instance, Bi-Former computes attention between top-k close regions using a top-k sorting operation and performs manygather operations on the queries and keys. Similarly, RMT computes the Manhattan distancebetween the tokens in the image. It includes two separate attention along the height and width ofthe image. This process invokes many small kernels along with reshape and permute operations.These specialized blocks would benefit from efficient CUDA kernels.",
  "Using accelerator-friendly operators. Depending on the hardware, some operators are better thanothers. Depth-wise separable convolutions reduce the MACs, yet they may not be efficient for": ":Analysis of the Configura-tion of Convolution and TransformerBlocks. We analyze various convolutionaland transformer blocks by training hybridarchitectures with different options on theImageNet-1K dataset. We provide the in-ference latency (as throughput) for differ-ent GPUs. Results show that FusedMB-Conv and Vanilla Transformer blocks pro-vide a better trade-off over accuracy andlatency than others.",
  "Vanilla55M4295114883.44%": ": Analysis of Architecture Configuration. Weanalyze the distribution of convolution and transformerblocks by training hybrid architecture with differentdistributions of blocks on ImageNet-1K dataset . Itdemonstrates that the design in provides a bettertrade-off over accuracy and latency. Symbol M denotesMaxViT block composed of MBConv andMulti-Axial Attention blocks, which is equivalent to CT.Note that CoAtNet uses MBConv blocks comparedto the FusedMBConv blocks in our design.",
  "Scaling Up Architecture for Image Generation": "We further scale up our architecture for the image generation task, which requires more computationthan the image recognition task. We train our network by utilizing the denoising diffusion probabilisticmodels (DDPM) in the latent space . The latent diffusion model includes a variationalautoencoder (VAE) that encodes the image x into latent z and a diffusion model () withparameters . We utilize UNet as the network for the diffusion model. For the T2I generation,we get the text embedding with Flan-T5-XXL encoder . We train the diffusion model followingthe noise prediction :",
  "Asymmetric UNet Architecture": "We follow the existing literature to design the UNet architecture for the T2I diffusionmodel. gives an overview of the overall architecture. It consists of three main stages, namely,Down blocks, Middle Block, and Up blocks. The Up blocks are the reverted reflection of the Downblocks. In addition, there are skip connections to add the features from Down blocks to Up blocks.We adopt the VAE from SDXL to transform the image to the latent space and carry out thediffusion in this space. Since the text-to-image generation task requires additional inputs (i.e., timeand text embeddings), we modify the C and T blocks to incorporate these conditions. Similar to theexisting literature , we add the time embedding to the C blocks. Additionally, we add the textembedding to the T blocks through a cross-attention operation carried in parallel to the self-attentionoperation as shown in (c). While the UNet can take arbitrary resolution as input, we notice that adding positional embeddings inthe self-attention operation reduces the artifacts such as duplicate patterns. For this purpose, we addRoPE embeddings for encoding the position information in the T blocks. Further, we incorporatequery-key normalization using the RMSNorm for stable training in lower precision (bfloat16).",
  "Improved Multi-Stage Training Pipeline": "Instead of training the T2I network from scratch, we first train the model on a small-scale datasetand then fine-tune the model on a much larger dataset. It effectively reduces the training cost on thelarge-scale scale dataset (see ablation in Appendix Sec. A.6 , Fig 8). This strategy differs from existingworks that perform multi-stage training using different architectures. For instance, PixArt- trains a class conditional image generation network and modifies it to fine-tune the network fortext-to-image generation. Both our pre-training and fine-tuning tasks use the same architecture fortext-to-image generation. We use the AdamW optimizer with 1 = 0.9 and 2 = 0.99. Specifically, in the first stage, we train our model using ImageNet-1K for the text-to-imagegeneration. Following Esser et al. , we form the conditioned text prompt as \"a photo of a <classname>\", where class name is randomly chosen from the label of each image. The model is trainedto generate an image with a resolution of 256 256. In the second stage, we fine-tune the modelfrom the first stage on a much larger dataset. Here we train the model in four phases: First, weconduct training at the resolution of 256 256 for 300K iterations with the batch size as 16, 384 and4e 4 as the learning rate. Second, we continue the training at the resolution of 512 512 for 200Kiterations with the batch size as 6, 144 and 1e 4 as the learning rate. Third, we train the model for1024 1024 for 100K iterations with the batch size as 1, 536 and 5e 5 as the learning rate. Finally,we perform the multi-aspect ratio training such that the model can synthesize images at variousresolution . Additionally, we adjust the added noise at different resolution , i.e., T in Eq. (3)is chosen as 0.01 for 256 256 and 0.02 for higher resolution (see ablations in Appendix Sec. A.6).We add an offset noise of 0.05 during multi-aspect ratio training similar to earlier works.",
  "Top-1 Accuracy on ImageNet-1K (%)": ": Top-1 Accuracy vs Inference Latency on ImageNet-1K Classification. We plot thelatency measured as images inferred per second on a single V100 GPU (Left)/A100 GPU (Right) withbatch-size 16 with 224 224 resolution. The plot compares state-of-the-art models (convolutional,transformer, hybrid architectures) against the proposed AsCAN architecture. The area of each circleis proportional to the model size. Our model consistently achieves better accuracy vs latency trade-offs. While some models regress between two hardware (e.g., MaxViT-S vs SMT-B ), our modelconsistently achieves better accuracy vs latency trade-offs. We report additional baselines along withmultiply-add operations count and different batch sizes in Appendix Tab. 7.",
  "Image Recognition": "We scale the AsCAN architecture discussed in Sec. 3.1 to base and large variants following earlierworks . We train these variants on the ImageNet-1K classification task. We provide theexperimental details (dataset description, architectures, training hyper-parameters) in AppendixSec. A.2. plots the inference speed on a V100 GPU with batch size 16 (measured in imagesprocessed per second) and top-1 accuracy achieved on this task for various models. In addition,Appendix Tab. 7 shows the parameter count and inference speed on both V100 and A100 GPUs alongwith additional details such as floating-point multiply-add count (FLOPs/MACs) and inference speedacross different batch sizes. Below, we highlight the salient observations from these experiments. More than 2 higher throughput across accelerators. Compared to the existing hybrid architecturessuch as FasterViT and MaxViT , the proposed AsCAN family achieves similar or bettertop-1 accuracy with more than 2 higher throughput. This trend holds true for both A100 andV100 GPUs. For instance, on A100 with batch=16, FasterViT-1 achieves 83.2% top-1 accuracywith throughput as 1123 images/s, while AsCAN-T has 83.44% with throughput as 3224 images/s.",
  "Better throughput across different batch sizes. AsCAN consistently achieves better throughputacross batch sizes for both the accelerators compared to baselines": "Better storage and computational footprint. AsCAN-family of architectures requires less numberof parameters and float operations to achieve similar performance. For example, to achieve nearly85.2% accuracy, MaxViT-L requires 212M parameters and 43.9G MACs whereas AsCAN-Lrequires 173M parameters and 30.7G MACs. We achieve better latency vs. accuracy trade-off than hybrid architectures with better alternatives tothe attention mechanisms in transformer. For instance, we outperform newer architectures such asPVTv2, MOAT, EfficientViT, and Scale-Aware Modulation Transformers.",
  "Class Conditional Generation": "We apply our asymmetric UNet architecture to learn class conditional image generation on theImageNet-1K dataset with 256 256 resolution. We train a smaller variant of our asymmetricUNet architecture (as in Sec. 3.3.1) with nearly 400M parameters and inject the class conditionthrough cross-attention mechanism. We train this model using DDPM (more details in AppendixSec. A.7), and provide results in Tab. 4. As can be seen, we achieve Frchet Inception Distance(FID) close to state-of-the-art models with nearly half the FLOPs (e.g., Ours vs. DiT-XL/2-G),and better FID than the existing work with similar computation (e.g., Ours vs. U-ViT-L/2).",
  "Below, we evaluate our T2I asymmetric UNet architecture trained using the multi-stage training": "GenEval Benchmark. We evaluate our model on the GenEval benchmark that studies variousaspects of an image generation model. Tab. 3 shows that our model outperforms fast training pipelinessuch as PixArt- by a significant margin. It beats even larger models such as SDXL which consumesignificantly more training data and computational resources.",
  "super cute extremely fluffy little dog": "A white dog standing next to a person and the person is wearing light blue jeans and brown shoes. The dog's head turned left and he had beige patches and a black head. The floor is made up of gray tiles. There is a blurred background with a white car on a wet grey stone floor very realistic, detailed, cat with big hat and white sunglasses, posing, hyperrealistic, atmospheric, in city, street, new york, cinematic, dramatic lighting, photorealistic, .. M10R 8k .. 35mm lens, Tilt Blur, Shutter Speed 1 1000, F 5.6, Super Resolution A young woman standing outside slightly turned to her left. Her head is slightly turned to her right, her arms are bent at the elbow and her arms are at stomach level. She has black hair, dark brown eyes, red lipstick, face paint with black and white squiggles on her cheeks, and a happy facial expression with a slight smile. She wears an elaborate headdress made of feathers with black, orange, and white decorations, and golden earrings. She wears a black T-shirt underneath a shoulder piece with elaborate feathers and decorations that are orange and black. The background is blurred and shows grey ground A woman is standing straight near the shore of a lake with her back facing towards the camera. She has shoulder-length blonde hair. She is dressed in a beige jacket with a grey scarf around her neck. In the background, the lake has reflections of trees and the sky, brown trees on both sides of the lake, and the blue sky is covered with gray rainy clouds during the daytime In the daytime, a young man standing slightly turned to his right, while holding a gray rabbit behind his left shoulder. His right arm is bent at the elbow and resting on his left shoulder, while his left arm is down and his head turned to the right. He has a stubbled mustache, a beard, grey eyes and a happy facial expression with a wide smile. He is wearing a gray knitted hat with dark blue-brown patterns, a dark brown jacket, and a black sweater. There is a snowy forest with snow-capped trees in the background. : Qualitative Comparison against open source and commercial models. We compareour T2I model against generations from different baselines. We illustrate that many times existingmodels generate images with less photo-realism (either lot less details or more on the cartoonish side),specially for PixArt- and PixArt-. Further, they frequently miss the fine-grained details explicitlyasked in the prompts. We highlight these mistakes in red color in the input prompt. For instance, inthe above generations (ordered A F from top to bottom row), baselines miss details such as, (A)lack of realism (B) light blue jeans, (C) white sunglasses, (D) black, orange, and white feathers, (E)grey scarf & back towards camera, and (F) gray knitted hat with dark blue-brown patterns. Qualitative Comparison. shows a qualitative comparison between different methods. Wehighlight salient differences between the baselines and our generations. Baselines such as PixArt-and PixArt- tend to generate images with much less photo-realism and more often it is much moreon the cartoonish side or it has grainy artifacts. Similarly, SDXL with refiner framework is unable toadapt to long text prompts due to limitations of the CLIP text-encoder. Hence, it misses many keyfeatures described in the prompts. In contrast, our model is able to follow the long prompts whileadhering to the required semantics as well as photo-realism. Human Preference study. We perform a user study to compare open-sourced models to evaluatetheir image-text alignment characteristics. We select 1000 prompts from our validation set andgenerate images from SDXL, PixArt-, PixArt-, and our multi-aspect ratio model. We show theseresults in the . It shows that our model convincingly outperforms both SDXL and PixArt-generations, while it has similar image-text alignment as PixArt- model. To further evaluate thesemodels, we generate 10K images from our validation set (described in Appendix A.5.2) and computethe FID between the generated and original images. We show these scores in Tab. 5, which : ImageNet-1K Class Conditional Generation. We train a smaller variant of our T2IUNet architecture to perform class conditional generation on ImageNet-1K. We train this model at256 256 and 512 512. Our asymmetric architecture achieves similar FID as the state-of-the-artmodels with less than half the floating point operations (e.g., Ours vs. DiT-XL/2-G), and better FIDthan the existing work with similar computation (e.g., Ours vs. U-ViT-L/2).",
  "clearly indicates that our generations align well with the real-world images. This is also evident fromour earlier comparison on qualitative visualization in": "Resource Efficiency. We compare the resource requirements of our model against various baselinesin Appendix Tab. 12. It shows that we achieve better inference latency compared to many existingmodels. Further, we consume considerably less compute to achieve much better performance thanmany existing baselines as illustrated by evaluations in the previous section.",
  "Conclusion": "In this work, we design hybrid architectures comprising convolutional and transformer blocks withapplications to many computer vision tasks. We focus on developing a simple hybrid model withbetter throughput and performance trade-offs. Instead of designing efficient alternatives to theconvolutional and transformer (mainly attention mechanism) blocks, we leverage existing vanillaattention along with the FusedMBConv block to design the new architecture, called AsCAN. Ourmain philosophy revolves around the uneven distribution of the convolutional and transformer blocksin the different stages of the network. We refer to this distribution as asymmetric, in the sense thatit favors more convolutional blocks in the early stages with a mix of few transformer blocks, whileit reverses this trend favoring more transformer blocks in the later stages with fewer convolutionalblocks. We demonstrate the superiority of the proposed architecture through extensive evaluationsacross the image recognition task, class conditional generation, and text-to-image generation.",
  "Mingxing Tan and Quoc Le. Efficientnetv2: Smaller models and faster training. In Internationalconference on machine learning, pages 1009610106. PMLR, 2021": "Yanyu Li, Geng Yuan, Yang Wen, Ju Hu, Georgios Evangelidis, Sergey Tulyakov, Yanzhi Wang, andJian Ren. Efficientformer: Vision transformers at mobilenet speed. Advances in Neural InformationProcessing Systems, 35:1293412949, 2022. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, KamyarGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information ProcessingSystems, 35:3647936494, 2022.",
  "William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of theIEEE/CVF International Conference on Computer Vision, pages 41954205, 2023": "Yanyu Li, Huan Wang, Qing Jin, Ju Hu, Pavlo Chemerys, Yun Fu, Yanzhi Wang, Sergey Tulyakov, andJian Ren. Snapfusion: Text-to-image diffusion model on mobile devices within two seconds. arXivpreprint arXiv:2306.00980, 2023. Xian Liu, Jian Ren, Aliaksandr Siarohin, Ivan Skorokhodov, Yanyu Li, Dahua Lin, Xihui Liu, Ziwei Liu,and Sergey Tulyakov. Hyperhuman: Hyper-realistic human generation with latent structural diffusion.arXiv preprint arXiv:2310.08579, 2023. Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, and Taesung Park.Scaling up gans for text-to-image synthesis. In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 1012410134, 2023. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Mller, Joe Penna,and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXivpreprint arXiv:2307.01952, 2023. Risi Kondor and Shubhendu Trivedi. On the generalization of equivariance and convolution in neuralnetworks to the action of compact groups. In International Conference on Machine Learning, pages27472755. PMLR, 2018. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, ThomasUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit,and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. InInternational Conference on Learning Representations, 2021. Zihang Dai, Hanxiao Liu, Quoc V Le, and Mingxing Tan. Coatnet: Marrying convolution and attentionfor all data sizes. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, Advancesin Neural Information Processing Systems, 2021. Yanyu Li, Ju Hu, Yang Wen, Georgios Evangelidis, Kamyar Salahi, Yanzhi Wang, Sergey Tulyakov, andJian Ren. Rethinking vision transformers for mobilenet size and speed. In Proceedings of the IEEE/CVFInternational Conference on Computer Vision, pages 1688916900, 2023. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjrn Ommer. High-resolutionimage synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 1068410695, 2022. Hao Li, Yang Zou, Ying Wang, Orchid Majumder, Yusheng Xie, R Manmatha, Ashwin Swaminathan,Zhuowen Tu, Stefano Ermon, and Stefano Soatto. On the scalability of diffusion-based text-to-imagegeneration. arXiv preprint arXiv:2404.02883, 2024.",
  "Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-basedgenerative models. Advances in Neural Information Processing Systems, 35:2656526577, 2022": "Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, IlyaSutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guideddiffusion models. arXiv preprint arXiv:2112.10741, 2021. Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee, Jonathan Ho, Tim Salimans, David Fleet, andMohammad Norouzi. Palette: Image-to-image diffusion models. In ACM SIGGRAPH 2022 ConferenceProceedings, pages 110, 2022.",
  "Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditionalimage generation with clip latents. arXiv preprint arXiv:2204.06125, 2022": "Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang,Kevin Murphy, William T Freeman, Michael Rubinstein, et al. Muse: Text-to-image generation viamasked generative transformers. arXiv preprint arXiv:2301.00704, 2023. Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika Aittala,Timo Aila, Samuli Laine, Bryan Catanzaro, et al. ediffi: Text-to-image diffusion models with an ensembleof expert denoisers. arXiv preprint arXiv:2211.01324, 2022. Zhixing Zhang, Ligong Han, Arnab Ghosh, Dimitris N Metaxas, and Jian Ren. Sine: Single image editingwith text-to-image diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 60276037, 2023. Shelly Sheynin, Adam Polyak, Uriel Singer, Yuval Kirstain, Amit Zohar, Oron Ashual, Devi Parikh, andYaniv Taigman. Emu edit: Precise image editing via recognition and generation tasks. arXiv preprintarXiv:2311.10089, 2023. Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik PKingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition videogeneration with diffusion models. arXiv preprint arXiv:2210.02303, 2022. Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera Hahn, Li Fei-Fei, Irfan Essa, Lu Jiang, and JosLezama. Photorealistic video generation with diffusion models. arXiv preprint arXiv:2312.06662, 2023. Rohit Girdhar, Mannat Singh, Andrew Brown, Quentin Duval, Samaneh Azadi, Sai Saketh Rambhatla,Akbar Shah, Xi Yin, Devi Parikh, and Ishan Misra. Emu video: Factorizing text-to-video generation byexplicit image conditioning. arXiv preprint arXiv:2311.10709, 2023. Willi Menapace, Aliaksandr Siarohin, Ivan Skorokhodov, Ekaterina Deyneka, Tsai-Shien Chen, Anil Kag,Yuwei Fang, Aleksei Stoliar, Elisa Ricci, Jian Ren, et al. Snap video: Scaled spatiotemporal transformersfor text-to-video synthesis. arXiv preprint arXiv:2402.14797, 2024.",
  "Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, and Yinxiao Li.Maxvit: Multi-axis vision transformer. ECCV, 2022": "Ali Hatamizadeh, Greg Heinrich, Hongxu Yin, Andrew Tao, Jose M Alvarez, Jan Kautz, andPavlo Molchanov.Fastervit: Fast vision transformers with hierarchical attention.arXiv preprintarXiv:2306.06189, 2023. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scalehierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages248255. Ieee, 2009. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770778, 2016.",
  "Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differentialequations. Advances in neural information processing systems, 31, 2018": "Xiaohan Ding, Xiangyu Zhang, Jungong Han, and Guiguang Ding. Scaling up your kernels to 31x31:Revisiting large kernel design in cnns. In Proceedings of the IEEE/CVF conference on computer visionand pattern recognition, pages 1196311975, 2022. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, ThomasUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image isworth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.",
  "Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention withlinear complexity. arXiv preprint arXiv:2006.04768, 2020": "Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swintransformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVFinternational conference on computer vision, pages 1001210022, 2021. Xiaoyi Dong, Jianmin Bao, Dongdong Chen, Weiming Zhang, Nenghai Yu, Lu Yuan, Dong Chen, andBaining Guo. Cswin transformer: A general vision transformer backbone with cross-shaped windows. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1212412134, 2022. Junting Pan, Adrian Bulat, Fuwen Tan, Xiatian Zhu, Lukasz Dudziak, Hongsheng Li, Georgios Tz-imiropoulos, and Brais Martinez. Edgevits: Competing light-weight cnns on mobile devices with visiontransformers. In European Conference on Computer Vision, pages 294311. Springer, 2022.",
  "Axel Sauer, Tero Karras, Samuli Laine, Andreas Geiger, and Timo Aila. Stylegan-t: Unlocking the powerof gans for fast large-scale text-to-image synthesis. arXiv preprint arXiv:2301.09515, 2023": "Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan,Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-richtext-to-image generation. arXiv preprint arXiv:2206.10789, 2022. Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman. Make-a-scene:Scene-based text-to-image generation with human priors. In European Conference on Computer Vision,pages 89106. Springer, 2022. Xiaoliang Dai, Ji Hou, Chih-Yao Ma, Sam Tsai, Jialiang Wang, Rui Wang, Peizhao Zhang, SimonVandenhende, Xiaofang Wang, Abhimanyu Dubey, et al. Emu: Enhancing image generation models usingphotogenic needles in a haystack. arXiv preprint arXiv:2309.15807, 2023. Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervisedlearning using nonequilibrium thermodynamics. In International Conference on Machine Learning, pages22562265. PMLR, 2015.",
  "Ali Hatamizadeh, Jiaming Song, Guilin Liu, Jan Kautz, and Arash Vahdat. Diffit: Diffusion visiontransformers for image generation. arXiv preprint arXiv:2312.02139, 2023": "Zhida Feng, Zhenyu Zhang, Xintong Yu, Yewei Fang, Lanxin Li, Xuyi Chen, Yuxiang Lu, JiaxiangLiu, Weichong Yin, Shikun Feng, et al. Ernie-vilg 2.0: Improving text-to-image diffusion model withknowledge-enhanced mixture-of-denoising-experts. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, pages 1013510145, 2023. Zeyue Xue, Guanglu Song, Qiushan Guo, Boxiao Liu, Zhuofan Zong, Yu Liu, and Ping Luo. Raphael:Text-to-image generation via large mixture of diffusion paths. Advances in Neural Information ProcessingSystems, 36, 2024.",
  "Xinyin Ma, Gongfan Fang, and Xinchao Wang. Deepcache: Accelerating diffusion models for free. arXivpreprint arXiv:2312.00858, 2023": "Muyang Li, Tianle Cai, Jiaxin Cao, Qinsheng Zhang, Han Cai, Junjie Bai, Yangqing Jia, Ming-Yu Liu,Kai Li, and Song Han. Distrifusion: Distributed parallel inference for high-resolution diffusion models.In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. Felix Wimbauer, Bichen Wu, Edgar Schoenfeld, Xiaoliang Dai, Ji Hou, Zijian He, Artsiom Sanakoyeu,Peizhao Zhang, Sam Tsai, Jonas Kohler, et al. Cache me if you can: Accelerating diffusion modelsthrough block caching. arXiv preprint arXiv:2312.03209, 2023.",
  "Ryan Webster, Julien Rabin, Loic Simon, and Frederic Jurie. On the de-duplication of laion-2b. arXivpreprint arXiv:2303.12733, 2023": "Aaron Gokaslan, A Feder Cooper, Jasmine Collins, Landan Seguin, Austin Jacobson, Mihir Patel,Jonathan Frankle, Cory Stephenson, and Volodymyr Kuleshov. Commoncanvas: An open diffusion modeltrained with creative-commons images. arXiv preprint arXiv:2310.16825, 2023. Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, JamesKwok, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-: Fast training of diffusion transformer forphotorealistic text-to-image synthesis, 2023. Pablo Pernias, Dominic Rampas, Mats Leon Richter, Christopher Pal, and Marc Aubreville. Wrstchen:An efficient architecture for large-scale text-to-image diffusion models. In The Twelfth InternationalConference on Learning Representations, 2023. Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo,Huchuan Lu, and Zhenguo Li. Pixart-\\sigma: Weak-to-strong training of diffusion transformer for 4ktext-to-image generation. arXiv preprint arXiv:2403.04692, 2024. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognitionchallenge. International journal of computer vision, 115:211252, 2015.",
  "Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks.In International conference on machine learning, pages 61056114. PMLR, 2019": "Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. Aconvnet for the 2020s. In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR), pages 1197611986, June 2022. Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang,Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et al. Searching for mobilenetv3. In Proceedings of theIEEE/CVF international conference on computer vision, pages 13141324, 2019.",
  "Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114,2013": "Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and ap-proximate inference in deep generative models. In International conference on machine learning, pages12781286. PMLR, 2014. Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, XuezhiWang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models.Journal of Machine Learning Research, 25(70):153, 2024. Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth words:A vit backbone for diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 2266922679, 2023.",
  "Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conferenceon Learning Representations, 2019": "Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Mller, Harry Saini, Yam Levi,Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolutionimage synthesis. arXiv preprint arXiv:2403.03206, 2024. Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, andLing Shao. Pvt v2: Improved baselines with pyramid vision transformer. Computational Visual Media,8(3):415424, March 2022. Chenglin Yang, Siyuan Qiao, Qihang Yu, Xiaoding Yuan, Yukun Zhu, Alan Yuille, Hartwig Adam, andLiang-Chieh Chen. MOAT: Alternating mobile convolution and attention brings strong vision models. InThe Eleventh International Conference on Learning Representations, 2023.",
  "Donghyun Kim, Byeongho Heo, and Dongyoon Han. Densenets reloaded: Paradigm shift beyond resnetsand vits, 2024": "Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and HervJgou. Training data-efficient image transformers & distillation through attention. In Internationalconference on machine learning, pages 1034710357. PMLR, 2021. Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsingthrough ade20k dataset. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),pages 51225130, 2017.",
  "Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. Unified perceptual parsing for sceneunderstanding. In European Conference on Computer Vision. Springer, 2018": "Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In Andreas Krause, Emma Brunskill,Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the40th International Conference on Machine Learning, volume 202 of Proceedings of Machine LearningResearch, pages 1973019742. PMLR, 2329 Jul 2023.",
  "A.1Image Recognition Architecture Details": "We describe the three AsCAN variants (tiny, base, and large) used in the experiments (see Sec. 4.1) inthe Tab. 6. We follow earlier works to scale the tiny variant discussed in Sec. 3.1. Note that thestem consists of two convolutional layers that downsample the input to half the spatial resolution.The classifier stage projects the final feature map to the corresponding embedding size and performsadaptive average pooling to reduce the spatial dimensions to 1 1, in order to apply a feed-forwardlayer that acts as classifier head on top of these pooled features. Our convolution blocks use thebatch-norm as the normalization layer while the transformer blocks leverage the layer-norm as thenormalization layer. We also learn the relative positional embeddings in our attention mechanismsimilar to earlier works . : Architecture Details. We provide detailed configuration of the three variants of theAsCAN proposed in this work, namely, tiny, base, and large. As per our method section, we useFusedMBConv (C) as the convolution block and Vanilla Transformer (T) as the transformer block.We refer the reader to the for their detailed description and schematic of these building blocks.We use K to denote the number of classes. Note that we hide the activation and normalization layersin the blocks. We use GeLU activation and batch-normalization in the Stem and the classifier stages.",
  "A.2ImageNet Classification (Training Procedure & Hyper-parameters)": "We follow the same training strategy for our ImageNet experiments in Sec. 3.1 and Sec. 4.1. Wereport the top-1 accuracy on the single center crop image. For training ImageNet-1K models with224 224 resolution, we use the AdamW optimizer with a peak learning rate of 3e 3 for 300epochs. We use a batch size of 4096 images during this training period. We follow a cosine schedulefor decaying the learning rate to the minimum learning rate of 5e 6. We also perform a learningrate warm-up to avoid instabilities during the training. We follow a 20 epoch warm-up schedule withan initial learning rate of 5e 7 that gets warmed up to the peak learning rate. For all our experiments, we use standard data augmentation strategies. We use RandAugment with parameters (2, 15), MixUp with = 0.8, color jittering with 0.4 as the weight, andlabel smoothing with 0.1 as the smoothing parameter. We also use 0.05 value for the weight decayregularization and perform an exponential model averaging with a decay value of 0.9999. In addition,we adopt gradient clipping with a gradient norm of 1.0 to avoid instabilities during the training ofsuch large models. We also enable stochastic depth for regularization. We use the stochastic depth of0.3/0.4/0.5 for the three variants in our experiments.",
  "A.2.2Ablations on Architecture Configuration": "One of the AsCAN design principles include preferring C blocks over T blocks in a stage. We demon-strated various architecture configurations using this design choice in Tab. 2. For completeness, wereverse this design choice and prefer T blocks over C blocks in a stage. We show these configurationsin Tab. 9. Using T block in stem / early layers result in lower throughput. Further, the performance ofconfigurations with T before C yields lower accuracy vs throughput trade-off.",
  "A.2.3ImageNet-21K": "We also study the effects of pre-training the AsCAN family on larger datasets such as ImageNet-21K . Similar to earlier works, we pre-train our models on the ImageNet-21K dataset. We use thepre-processed version of this dataset for ease of usage . Following previous works , wetrain the AsCAN model for 90 epochs on the ImageNet-21K dataset and fine-tune these weights on theImageNet-1K classification task. Similar to the ImageNet-1K experiments, we use RandAugment with parameters (2, 5), MixUp with = 0.2, color jittering with 0.4 as the weight and labelsmoothing with 0.01 as the smoothing parameter. We also use 0.01 value for the weight decayregularization. Additionally, we perform an exponential model averaging with a decay value of0.9999 and gradient clipping with a gradient norm of 1.0 to avoid instabilities during training suchlarge models. We also enable stochastic depth for regularization. We use the stochastic depth of0.4/0.5/0.6 for the three variants in our experiments. We report the performance in Tab. 10, where we observe similar top-1 accuracy as the other baselineswhile achieving much better inference throughput. This trend is similar to the one we observed inSec. 4.1 when these models trained only on the ImageNet-1K dataset without any pre-training on theImageNet-21K dataset.",
  "A.3ADE20K Semantic Segmentation": "Dataset Details. ADE20K is a popular scene-parsing dataset used to evaluate the semanticsegmentation performance. It consists of 20K train and 2K validation images over 150 fine-grainedsemantic categories. Images are resized and cropped to 512 512 resolution for training. Training Procedure & Hyper-parameters. We base our semantic segmentation experiments on thewidely used and publicly available mmsegmentation library . We use the UPerNet as oursemantic segmentation architecture wherein different hybrid architectures are used as the backbonesto extract the spatial feature maps. We extract the spatial feature maps at stages S2, S3, and S4,and forward these feature maps to the semantic segmentation network. All three AsCAN variants(tiny, base, large) have been initialized with the weights pre-trained on the ImageNet-1K task with224 224 resolution. Training is performed in 512 512 resolution. Similar to FasterViT , wefollow a similar schedule for training these segmentation models with an AdamW optimizer witha learning rate of 1e 4, and a weight decay of 0.05. We use a batch size of 16 on 8 A100 GPUs.We also use stochastic depth similar to ImageNet training for controlling overfitting during theseexperiments. Experimental Setup. We train UperNet as segmentation architecture on the ADE20K dataset.We use the proposed AsCAN as the backbone pre-trained on the ImageNet-1K dataset. We usethe AdamW optimizer with learning rate 1e 4 and weight decay 0.05. Following earlierworks , we train the models for 160K iterations using the mmsegmentation library . Wetrain these networks on 8 NVIDIA A100 GPUs using 16 as the batch size. We compute the variousinference statistics with 512 512 as the image resolution. Experimental Results. We compare the performance of various backbones in Tab. 11. It showsthat the proposed backbone achieves competitive mIoU while achieving nearly 1.5 faster inferencelatency compared to the baselines, measured using the frames per second metrics. This fact can beobserved across different scaling of the backbones. For instance, the Swin-T backbone achieveslatency of 44FPS while AsCAN-T achieves 64FPS as the latency with similar mIoU. Thus, our",
  "A.4T2I Dataset Details": "We curate our training dataset with careful filtration on diverse data sources using first-party andlicensed datasets. We depict this entire process in . We start by collating all these data sources tocreate a stream of images and if available, relevant text captions. Next stage, we add a series of filtersto improve the quality of the selected images (resolution, de-duplication, aesthetic score, nsfw filtering,etc.). Since this data mostly contains very short descriptions or almost non-existent descriptions ofthe image, we unify the captioning process using an image captioning model. We collect a subset of200K data for human annotations, where we ask the annotators to add details such as the <angleshot of the image>, <image background information>, <human attributes>, etc. Weuse this labeled data to fine-tune a BLIP2 model which is used to generate long captions similarto this format.",
  "A.5.2Evaluation on Internal Data": "We evaluate our models against various baselines on two internally captioned datasets (Set-A-10Kand Set-B-10K). We draw these datasets from our validation set. We compute the FID betweenthe generated 10K images against the reference images in Tab. 5. This table also shows that fasttraining pipelines like PixArt- and PixArt- suffer considerably when evaluated on real-worldimages compared to models like SDXL and Ours. We also point out that SDXL has been trained witha considerably larger dataset compared to ours and uses much more computational resources.",
  "A.6Hyper-parameter Setup for Text-to-Image Generation": "Since full-scale text-to-image generation task is a computationally expensive task, it becomes crucialthat we choose appropriate hyper-parameters and initialization of the model before training it on theentire dataset. Thus, we choose a proxy task that can be run on a small scale and guide us in thesuitable training configurations. ImageNet-1K T2I Generation Task. We resort to the ImageNet-1K dataset since it contains a varietyof objects (1000 objects) in different settings. Earlier works such as Pixart-, train their transformerarchitecture on the ImageNet-1K class-conditional generation tasks, wherein they incorporate thetext condition via a cross-attention mechanism. This process requires the model to forget the classcondition and adapt to the text condition. In this work, along the lines of , we simply convertthe ImageNet-1K classification dataset to Text-to-Image dataset by adding the text correspondingto the <class name>, with the template \"a photo of a <class name>\". Specifically, the datasetprovides various different names corresponding to each class, we pick up one at random duringtraining to learn diverse text mappings. We evaluate the models by computing the FID between the50K generated images and the reference images from the dataset. Embedding Pre-Computation. Our T2I model performs diffusion in the latent space. To keepparity between this task and the full training, we also pre-compute the SDXL VAE embeddings of theImageNet-1K dataset. Similarly, we pre-compute Flan-T5-XXL embeddings for the text condition.This saves training resources since loading VAE and Flan-T5-XXL encoders consumes significantGPU memory as well as non-trivial computation time. Thus, by pre-computing these embeddings,we increase the batch size available for training the UNet model. Architecture Details. We use similar configuration for the C blocks as in the image recognitionarchitecture. All our convolutional operators use 3 3 kernel sizes. The number of output channelsin the three Down blocks are {320, 640, 1280}. Since the three Up blocks are reflections of the Downblocks, their output channels are in the reverse order, i.e., {1280, 640, 320}. Given that we use theSDXL VAE to convert the 3channel RGB-image into 4channel latent space, our number of inputchannels is 4 for the convolution operator in the first Down block. The number of attention heads inthe three T blocks in the three Down stages are {5, 10, 20}. Since our T blocks are applied after Cblocks, the number of output channels of the convolutional blocks along with the number of attentionheads, determine the attention dimension for the transformer block. Finally, we compute the time-stepand textual embeddings. The time-step integer values are converted to timestep embedding by firstconverting it into sinusoidal space and then projecting it through two linear layers. We process thetextual embeddings using two T blocks for adapting the frozen embeddings (dimension 4096) for T2Igeneration with just one head and the same dimension as the frozen embeddings. Noise Level Ablation for Different Resolutions. Since our training strategy involves jumpingresolutions from 256 512 1024, we ablate on the noise levels at each of these resolutions. Weuse the DDPM noise scheduler for injecting the noise and the level of noise is controlled by the Thyper-parameter. We try four different noise levels {0.01, 0.02, 0.03, 0.04}, and compute the FIDscores. We plot these scores in . We find that at resolution 256, noise level 0.01 produces thebest FID score, and at remaining resolutions, noise level 0.02 works the best.",
  "FID": ": Noise Level Selection for Different Resolution. We ablate on the end noise level duringthe epsilon-prediction for the ImageNet-1k T2I task for the 256 256 (Left) and 512 512 (Right)resolutions with different learning rate and weight decay parameters. We plot the FID scores againsta reference image set from the dataset. T2I Task Initialization Ablation. We analyze initializing the 256 256 resolution training with andwithout pre-training from the ImageNet-1K T2I pre-trained model. As shown in , pre-traininghelps improve the performance of the model compared to training from scratch.",
  "A.7ImageNet-1K Class Conditional Generation": "We learn a class conditional image generation on the ImageNet-1K dataset with image resolution256 256. We train a smaller variant of our asymmetric UNet architecture (as in Sec. 3.3.1) withnearly 400M parameters and inject the class condition through the cross-attention mechanism. Wetrain this model using the DDPM scaled linear noise schedule for 1000 time steps. The training isconducted for 1000 epochs with a batch size of 2048. We use AdamW optimizer with 6e 4 as thelearning rate, 0.01 as the weight decay, and (1, 2) = (0.9, 0.99). We use an 8channel VAE toconvert the input image space into a latent space for latent diffusion. We compare our results withother class conditional models in Tab. 4. Our asymmetric architecture for this task is similar to the T2IUNet architecture described earlier. To lower down the compute footprint, we reduce the number ofchannels to {160, 320, 640} and reduce the cross-attention dimension from 4096 to 768. We evaluateour model with two configurations and report these two FID scores. In the first configuration, weapply a classifier-free guidance (cfg) scale of 1.78 to generate the class-conditioned images. Inthe second configuration (sampled cfg), we apply the guidance in the steps with a cfg scale inthe increasing range [1.1, 3.6]. This sampling procedure is similar to the one proposed in MUSE .We use the Heun discrete scheduler for inference with 30 sampling steps.",
  "A.8Limitations": "We develop hybrid architectures with asymmetric distribution of the convolution and transformerblocks. We show that this design paradigm is generic and applicable to many vision tasks such asimage recognition, semantic segmentation, and text-to-image generation. We expect this architecturedesign to be widely useful for other applications. From the architecture design perspective, ourbuilding blocks are not rigid and can be replaced with other efficient alternatives of the chosen C and Tblocks. Currently, we focus on the vanilla attention mechanism, but we can certainly incorporate fasteralternatives to quadratic attention without sacrificing any efficiency advantages. Since we demonstratean application of the asymmetric design to the UNet framework, we list out some limitations of ourtext-to-image generation model, which is broadly similar to various existing text-to-image models. Extra limbs. While our model is able to generate limbs for humans and animals very well in mostcases. There are instances where we can observe extra limbs in generations. It is unclear if this isdue to a poorly learned diffusion process, poor captioning guidance, lack of granular information inimage-caption pairs, or just an artifact of the VAE encoder while converting to the latent space. Artifacts in extreme resolutions. We train our T2I model with multi-aspect ratio bucketing. While itis able to adapt to many different resolutions in a graceful manner, we find that the model generatesduplicate patterns in the extremes of the aspect ratio (significantly higher width than height orvice-versa). We believe this is due to the lack of training data in such resolutions. While we are much better at avoiding NSFW generations compared to baselines (due to carefulcuration of the training data), there might be a slight percentage of NSFW content, due to leakagein various filtration processes in the dataset curation. Thus, we incorporate an additional NFSWfilter on top of the generation to remove such edge cases.",
  "A.9Societal Impact": "We design the asymmetric architecture and show the advantages it brings in many applications (imagerecognition, semantic segmentation, and text-to-image generation). Our architecture design targetsa better trade-off between efficiency (training/inference latency) while achieving achieving goodperformance. In itself, the asymmetric design would reduce the amount of resources utilized by deepneural networks used commonly in vision tasks. The downstream applications of this architecture,especially in Text-to-Image generation need to be deployed with care. We expect the communityto utilize proper care w.r.t. training data as well as the generated output. There needs to be specialchecks in place while preparing the training data for T2I tasks. We specially add various filters toremove malicious and harmful content from the dataset. Further, even generated images need to beprocessed with some filtration to remove similar harmful and malicious content.",
  "A.10Text-to-Image Generation Results": "For visualization purposes, we generate more images with different prompts using our multi-aspectratio model. We present these images in , 10, and 11. It shows that our model is able to generatea wide range of images, both very realistic as well as stylistic images. In addition, it is able to graspvarious concepts and compositions.",
  ": Example text-to-image generation results": ": Results on ImageNet-1K for Classification Task. We compare the performanceof our asymmetric architectures, AsCAN, against baselines. We report the inference latency asthe throughput (images per second) that is measured by inferring images with batch size as B inhalf-precision, i.e., fp16, on an A100 GPU using torch-compile and benchmark utility from timmlibrary . A similar procedure (without torch-compile) is followed to obtain the throughput onthe V100 GPU.",
  "B=1 B=16 B=64 B=1B=16": "EfficientNet-B6 52843M19.0G885295892720584.0%EfficientNet-B7 60066M37.0G723213502412484.3%NFNet-F0 25672M12.4G 199 2470 3348 4781383.6%NFNet-F1 320132M35.5G 106 998 1192 264884.7%ConvNet EfficientNetV2-S 38424M8.8G112 1884 2744 3456183.9%EfficientNetV2-M 48055M24.0G84918 1094 2632985.1%ConvNeXt-S 22450M8.7G174 2291 2889 5683683.1%ConvNeXt-B 22489M15.4G 167 1760 2073 5861983.8%ConvNeXt-L 224198M34.4G 168 1045 1127 5836284.3%RDNet-S 22450M8.7G102 1782 2761 5378083.7%RDNet-B 22487M15.4G80 1578 1891 4858984.4%RDNet-L 224186M34.7G786409903229084.8% ViT-B/16 38486M55.4G 212 1006 1266 1048677.9%ViT-B/32 384307M 190.7G 112 893924542776.5%ViTDeiT-B 38486M55.4G 189 1058 1192 13048883.1%Swin-S 22450M8.7G50841 2221 3443683.0%Swin-B 38488M47.0G51458486208584.5% PVTv2-B322445M7G49933 3782 3354883.2%PVTv2-B522482M11.8G32511 2112 1729183.8%EfficientViT-B222424M1.6G142 2171 5132 4878282.1%EfficientViT-B322449M4G114 1882 2860 3255283.5%MOAT-222473M17.2G 132 1367 2087 3842484.7%MOAT-3224190M44.9G708059622124685.3%SMT-S22421M4.7G38566 2211 4334883.7%SMT-B22432M7.7G27388 1412 1424384.3% MogaNet-L 22483M15.9G345238822429084.7%MogaNet-XL 224181M34.5G324715761921085.1%BiFormer-S 22426M4.5G45937 2139 3659583.8%BiFormer-B 22457M9.8G50840 1439 2844084.3%RMT-S 22427M4.5G46790 2439 3848084.1%Hybrid CoAtNet-0 22425M4.2G214 3537 5221 6197681.6%CoAtNet-1 22442M8.4G141 2221 2907 4562983.3%CoAtNet-2 22475M15.7G 133 1718 2040 3854084.1%CoAtNet-3 224168M34.7G 132 1085 1105 3738884.5%",
  "AsCAN-L224173M30.7G 120 1381 16174044085.24%21.2": ": Analysis of Architecture Configuration (with T before C). We extend Tab. 2 by ablatingover the preference of C and T blocks in a stage. Using T block in stem / early layers result in lowerthroughput. Further, the performance of configurations with T before C yields lower accuracy vsthroughput trade-off.",
  "B=16B=64B=16": "CC-CCCT-CCTT-CTTT (C1)55M3224 4295114883.4%CC-CCCT-CCTT-CCTT (C2)73M3217 4179103683.2%CC-CCCT-CCTT-TTTT (C3)41M3384 4472122482.9%CC-CCCT-CCCC-TTTT (C4)50M3434 4411118283.1%CC-CCCT-CCCT-CCCT (C5)95M3135 406699182.7%CC-TCCC-CCTT-CTTT (T1)56M3029 402194582.8%CC-CCCT-TTCC-CTTT (T2)57M3100 4092102182.9%CC-CCCT-CCTT-TTTC (T3)64M3190 4193104583.1%TT-CCCT-CCTT-CTTT (T4)55M1428 158448783.1%TT-TTTC-TTCC-TTTC (T5) 100M 1280 144039083.5% : ImageNet-21K Pre-training. We report the performance of models pre-trained onImageNet-21K with 224 resolution and fine-tuned on ImageNet-1K dataset. We report the inferencelatency as the throughput (images per second) that is measured by inferring images with batch sizeas B in half-precision, i.e., fp16, on an A100 GPU using torch-compile and benchmark utility fromtimm library .",
  "A mind-blowing sunset in the mountainsA lush forest with a winding river": "anthropomorphic profile of the white snow owl Crystal priestess , art deco painting, pretty and expressive eyes, ornate costume, mythical, ethereal, intricate, elaborate, hyperrealism, hyper detailed, 3D, 8K, Ultra Realistic, high octane, ultra resolution, amazing detail, perfection, In frame, photorealistic, cinematic lighting, visual clarity, shading , Lumen Reflections, Super-Resolution, gigapixel, color grading, retouch, enhanced, PBR, Blender, V-ray, Procreate, zBrush, Unreal Engine 5, cinematic, volumetric, dramatic, neon lighting, wide angle lens ,no digital painting blur Digital illustration of a beach scene crafted from yarn. The sandy beach is depicted with beige yarn, waves are made of blue and white yarn crashing onto the shore. A yarn sun sets on the horizon, casting a warm glow. Yarn palm trees sway gently, and little yarn seashells dot the shoreline. A young man stands with his left shoulder slightly raised. He can be seen up to his shoulders. He has dark gray eyes looking directly into the camera, light brown stubble, and a calm expression. He is wearing a blue safety helmet and a gray hooded sweatshirt under an orange safety jacket with gray detailing vertically around the shoulder. The blurred background has gray equipment on the left and a black curtain with a white wall on the right.",
  "The answer NA means that the paper has no limitation while the answer No means thatthe paper has limitations, but those are not discussed in the paper": "The authors are encouraged to create a separate \"Limitations\" section in their paper. The paper should point out any strong assumptions and how robust the results are toviolations of these assumptions (e.g., independence assumptions, noiseless settings,model well-specification, asymptotic approximations only holding locally). The authorsshould reflect on how these assumptions might be violated in practice and what theimplications would be. The authors should reflect on the scope of the claims made, e.g., if the approach wasonly tested on a few datasets or with a few runs. In general, empirical results oftendepend on implicit assumptions, which should be articulated. The authors should reflect on the factors that influence the performance of the approach.For example, a facial recognition algorithm may perform poorly when image resolutionis low or images are taken in low lighting. Or a speech-to-text system might not beused reliably to provide closed captions for online lectures because it fails to handletechnical jargon.",
  "If applicable, the authors should discuss possible limitations of their approach toaddress problems of privacy and fairness": "While the authors might fear that complete honesty about limitations might be used byreviewers as grounds for rejection, a worse outcome might be that reviewers discoverlimitations that arent acknowledged in the paper. The authors should use their bestjudgment and recognize that individual actions in favor of transparency play an impor-tant role in developing norms that preserve the integrity of the community. Reviewerswill be specifically instructed to not penalize honesty concerning limitations.",
  ". Experimental Result Reproducibility": "Question: Does the paper fully disclose all the information needed to reproduce the main ex-perimental results of the paper to the extent that it affects the main claims and/or conclusionsof the paper (regardless of whether the code and data are provided or not)?Answer: [Yes]Justification: We describe the architecture configurations used in this work in detail inthe main and appendix sections. We also describe the hyper-parameter configurationsused to train this model (training iterations, optimizer, learning rate, weight decay, dataaugmentation, etc.)Guidelines: The answer NA means that the paper does not include experiments. If the paper includes experiments, a No answer to this question will not be perceivedwell by the reviewers: Making the paper reproducible is important, regardless ofwhether the code and data are provided or not.",
  "If the contribution is a dataset and/or model, the authors should describe the steps takento make their results reproducible or verifiable": "Depending on the contribution, reproducibility can be accomplished in various ways.For example, if the contribution is a novel architecture, describing the architecture fullymight suffice, or if the contribution is a specific model and empirical evaluation, it maybe necessary to either make it possible for others to replicate the model with the samedataset, or provide access to the model. In general. releasing code and data is oftenone good way to accomplish this, but reproducibility can also be provided via detailedinstructions for how to replicate the results, access to a hosted model (e.g., in the caseof a large language model), releasing of a model checkpoint, or other means that areappropriate to the research performed. While NeurIPS does not require releasing code, the conference does require all submis-sions to provide some reasonable avenue for reproducibility, which may depend on thenature of the contribution. For example(a) If the contribution is primarily a new algorithm, the paper should make it clear howto reproduce that algorithm.",
  "(b) If the contribution is primarily a new model architecture, the paper should describethe architecture clearly and fully": "(c) If the contribution is a new model (e.g., a large language model), then there shouldeither be a way to access this model for reproducing the results or a way to reproducethe model (e.g., with an open-source dataset or instructions for how to constructthe dataset). (d) We recognize that reproducibility may be tricky in some cases, in which caseauthors are welcome to describe the particular way they provide for reproducibility.In the case of closed-source models, it may be that access to the model is limited in",
  "The answer NA means that the paper does not include experiments": "The authors should answer \"Yes\" if the results are accompanied by error bars, confi-dence intervals, or statistical significance tests, at least for the experiments that supportthe main claims of the paper. The factors of variability that the error bars are capturing should be clearly stated (forexample, train/test split, initialization, random drawing of some parameter, or overallrun with given experimental conditions).",
  ". Experiments Compute Resources": "Question: For each experiment, does the paper provide sufficient information on the com-puter resources (type of compute workers, memory, time of execution) needed to reproducethe experiments?Answer: [Yes]Justification: Yes, we describe the amount of resources required to train our models, includ-ing the hardware type. We also provide the total number of iterations/epochs required toreproduce the reported results.Guidelines: The answer NA means that the paper does not include experiments. The paper should indicate the type of compute workers CPU or GPU, internal cluster,or cloud provider, including relevant memory and storage.",
  ". Code Of Ethics": "Question: Does the research conducted in the paper conform, in every respect, with theNeurIPS Code of Ethics [Yes]Justification: We have reviewed the code of ethics and our research work conforms to thiscode.Guidelines: The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. If the authors answer No, they should explain the special circumstances that require adeviation from the Code of Ethics.",
  "Justification: We discuss the societal impacts of this work in the appendix Sec. A.9.Guidelines:": "The answer NA means that there is no societal impact of the work performed. If the authors answer NA or No, they should explain why their work has no societalimpact or why the paper does not address societal impact. Examples of negative societal impacts include potential malicious or unintended uses(e.g., disinformation, generating fake profiles, surveillance), fairness considerations(e.g., deployment of technologies that could make decisions that unfairly impact specificgroups), privacy considerations, and security considerations. The conference expects that many papers will be foundational research and not tiedto particular applications, let alone deployments. However, if there is a direct path toany negative applications, the authors should point it out. For example, it is legitimateto point out that an improvement in the quality of generative models could be used togenerate deepfakes for disinformation. On the other hand, it is not needed to point outthat a generic algorithm for optimizing neural networks could enable people to trainmodels that generate Deepfakes faster. The authors should consider possible harms that could arise when the technology isbeing used as intended and functioning correctly, harms that could arise when thetechnology is being used as intended but gives incorrect results, and harms followingfrom (intentional or unintentional) misuse of the technology. If there are negative societal impacts, the authors could also discuss possible mitigationstrategies (e.g., gated release of models, providing defenses in addition to attacks,mechanisms for monitoring misuse, mechanisms to monitor how a system learns fromfeedback over time, improving the efficiency and accessibility of ML).",
  ". Safeguards": "Question: Does the paper describe safeguards that have been put in place for responsiblerelease of data or models that have a high risk for misuse (e.g., pretrained language models,image generators, or scraped datasets)?Answer: [Yes]Justification: Yes, we discuss various safeguards both for preparing training data as well asserving the generated images for our text-to-image generation model.Guidelines: The answer NA means that the paper poses no such risks. Released models that have a high risk for misuse or dual-use should be released withnecessary safeguards to allow for controlled use of the model, for example by requiringthat users adhere to usage guidelines or restrictions to access the model or implementingsafety filters.",
  ". Licenses for existing assets": "Question: Are the creators or original owners of assets (e.g., code, data, models), used inthe paper, properly credited and are the license and terms of use explicitly mentioned andproperly respected?Answer: [Yes]Justification: For our image recognition and semantic segmentation experiments, we usethe widely available ImageNet-1K and ADE20K datasets. We cite these resources in thework. For the T2I experiments, we use first party and licensed datasets. We depict this entireprocess in . We start by collating all these data sources to create a stream of imagesand if available, relevant text captions. Next stage, we add a series of filters to improve thequality of the selected images (resolution, de-duplication, aesthetic score, nsfw filtering,etc.). Since this data most contains very short description or almost non-existent description of the image, we unify the captioning process using an image captioning model. We collect asubset of 200K data for human annotations, where we ask the annotators to add details suchas the <angle shot of the image>, <image background information>, <human attributes>,etc. We use this labelled data to fine-tune a BLIP2 model which is used to generate longcaptions similar to this format.Guidelines: The answer NA means that the paper does not use existing assets. The authors should cite the original paper that produced the code package or dataset. The authors should state which version of the asset is used and, if possible, include aURL. The name of the license (e.g., CC-BY 4.0) should be included for each asset. For scraped data from a particular source (e.g., website), the copyright and terms ofservice of that source should be provided. If assets are released, the license, copyright information, and terms of use in thepackage should be provided. For popular datasets, paperswithcode.com/datasetshas curated licenses for some datasets. Their licensing guide can help determine thelicense of a dataset.",
  "The answer NA means that the paper does not involve crowdsourcing nor research withhuman subjects": "Depending on the country in which research is conducted, IRB approval (or equivalent)may be required for any human subjects research. If you obtained IRB approval, youshould clearly state this in the paper. We recognize that the procedures for this may vary significantly between institutionsand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and theguidelines for their institution."
}