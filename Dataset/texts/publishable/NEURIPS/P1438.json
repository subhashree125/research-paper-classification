{
  "Abstract": "Auditing Differentially Private Stochastic Gradient Descent (DP-SGD) in the finalmodel setting is challenging and often results in empirical lower bounds that aresignificantly looser than theoretical privacy guarantees. We introduce a novelauditing method that achieves tighter empirical lower bounds without additionalassumptions by crafting worst-case adversarial samples through loss-based input-space auditing. Our approach surpasses traditional canary-based heuristics and iseffective in both white-box and black-box scenarios. Specifically, with a theoreticalprivacy budget of = 10.0, our method achieves empirical lower bounds of 6.68in white-box settings and 4.51 in black-box settings, compared to the baselineof 4.11 for MNIST. Moreover, we demonstrate that significant privacy auditingresults can be achieved using in-distribution (ID) samples as canaries, obtainingan empirical lower bound of 4.33 where traditional methods produce near-zeroleakage detection. Our work offers a practical framework for reliable and accurateprivacy auditing in differentially private machine learning.",
  "Introduction": "Differentially Private Stochastic Gradient Descent (DP-SGD) [Abadi et al., 2016] was introduced toprevent sensitive information leakage from training data in models trained using Stochastic GradientDescent (SGD) [Shokri et al., 2017, Hayes et al., 2017, Yeom et al., 2018, Bichsel et al., 2021, Balleet al., 2022, Haim et al., 2022, Carlini et al., 2022]. DP-SGD mitigates privacy leakage by clippingindividual gradients and adding Gaussian noise to the aggregated gradient updates. However, correctly implementing DP-SGD is challenging, and several implementations have revealedbugs that compromise its privacy guarantees [Ding et al., 2018, Bichsel et al., 2021, Stadler et al.,2022, Tramer et al., 2022]. These issues introduce potential privacy leakage, making thorough auditsessential to ensure that privacy guarantees hold in practice. Audits commonly employ membershipinference attacks (MIAs) [Shokri et al., 2017], using success rates to empirically estimate privacyleakage and compare it against theoretical upper bounds [Jagielski et al., 2020, Nasr et al., 2021,2023]. If the empirical privacy leakage, represented by the lower bound on , is significantly lowerthan the theoretical upper bound, the audit results are considered loose; conversely, when the lowerbound closely approaches the upper bound, the results are regarded as tight. In intermediate modelsettings, where adversaries can observe gradients throughout the entire training process, previousworks [Nasr et al., 2021, 2023] have shown that achieving tight privacy auditing is feasible. However,",
  "Adversarial Sample": ": Overview of privacy auditing process. The Crafter crafts a canary and constructs twoneighboring datasets, D and D, where D contains the canary. The Trainer trains a model on eitherD or D using the DP-SGD. The Distinguisher observes the loss values for specific input to inferwhether the model was trained on D or D. We suggests an adversarial sample for tighter auditing. this approach is often impractical, as real-world scenarios usually restrict adversaries access to onlythe final model. Achieving rigorous audit results in the final model settings remains challenging,leaving the feasibility of final model auditing for DP-SGD an unresolved issue. To address these challenges, the final model setting has gained attention in practical applicationsas a promising method to improve the privacy-utility trade-off. By concealing intermediate stepsand exposing only the final model, this approach aligns more closely with real-world constraints.Theoretically, in the final model setting, privacy amplification by iteration can provide additionalprivacy protection for data points involved in earlier stages of training [Feldman et al., 2018, Balleet al., 2019, Chourasia et al., 2021, Ye and Shokri, 2022, Altschuler and Talwar, 2022, Bok et al.,2024]. However, this effect has primarily been observed in convex or smooth loss functions, leavingits applicability to non-convex problems unresolved. Building on this theoretical foundation, several works [Nasr et al., 2021, 2023, Andrew et al., 2023,Steinke et al., 2024b,a, Cebere et al., 2024] have explored privacy amplification in final model settingswith non-convex loss functions. They observed a significant gap between empirical lower bounds andtheoretical upper bounds, suggesting that privacy amplification by iteration may extend to generalnon-convex loss functions. However, there is no formal proof to support this, and the observed gapmay instead reflect that adversaries have not fully exploited their potential capabilities. Conversely, recent works [Annamalai, 2024, Cebere et al., 2024] have observed that privacy amplifi-cation does not occur in non-convex settings. These findings, however, apply only to specific cases,such as constructing a worst-case non-convex loss function for DP-SGD where information from allprevious iterations is encoded in the final iteration, or manually designing a gradient sequence. Theseefforts remain constrained to additional assumptions or highly specific scenarios, leaving the broaderquestion of whether privacy amplification can occur in general non-convex settings unresolved. In this work, we introduce a novel auditing approach designed to establish tighter lower bounds ingeneral non-convex settings for final model scenarios, without requiring any additional assumptions.Our method leverages loss-based input-space auditing to craft worst-case adversarial samples, en-hancing the precision of privacy auditing results. Previous works [Jagielski et al., 2020, Nasr et al.,2021, Tramer et al., 2022, Aerni et al., 2024] on input-space auditing have implicitly assumed thatthe canary sample represents the worst-case privacy leakage, using it as an adversarial sample forMIAs based on its loss outputs. However, relying solely on canary-based approaches may not yieldtighter empirical lower bounds. By leveraging loss outputs from an alternative adversarial sample,our approach identifies increased privacy leakage. Consequently, we craft worst-case adversarialsamples without relying on canaries, providing tighter empirical lower bounds for auditing. Our auditing method addresses practical challenges in two scenarios within the final model setting.First, for the white-box auditing scenario, adversaries have access to the final models weights, asin open-source cases. We craft the worst-case adversarial sample directly using these weights. Forexample, with a theoretical privacy budget of 10.0, our approach achieves an empirical lower boundof 6.68, significantly surpassing the 4.11 obtained with the canary approach. Secondly, for theblack-box auditing scenario, only the models outputs are accessible, such as through APIs. Weemploy surrogate models to approximate the final models and use their weights to craft the worst-case adversarial sample. While adversaries capabilities are more limited in the black-box settingcompared to the white-box setting, our approach increases the empirical lower bound from 4.11, achieved using the canary approach, to 4.51 for a theoretical upper bound of 10.0. This demonstratesthat our approach provides generalizable and effective privacy auditing results, even in scenarios withrestricted adversarial access. Additionally, we show that significant privacy auditing results can be achieved by using in-distribution(ID) samples from the training dataset as canaries, eliminating the need for out-of-distribution (OOD)samples. When an ID sample is used as the canary, the canary approach yields empirical privacyleakage close to zero, indicating little privacy leakage detection. In contrast, we achieve an empiricallower bound of 4.33 even with ID canaries.",
  "Preliminary": "Differentially Private Training.Differential Privacy (DP) [Dwork et al., 2006] is a widely usedstandard for preserving individual privacy in data analysis. A randomized mechanism M is considered(, )-differentially private if, for any two neighboring datasets D and D that differ by only one datapoint known as the canary (c), and for any possible output subset O Range(M), the followinginequality holds:Pr[M(D) O] e Pr[M(D) O] +",
  "In this formulation, represents the loss of privacy, with smaller indicating stronger privacyguarantees. The parameter (0, 1) represents a small probability that the privacy guarantee maynot hold": "Numerous mechanisms have been developed to ensure differential privacy during the training ofmachine learning models. [Abadi et al., 2016, Papernot et al., 2016, Zhu et al., 2020]. DifferentiallyPrivate Stochastic Gradient Descent (DP-SGD) [Abadi et al., 2016] is widely used to achievedifferential privacy. DP-SGD clips gradients to a maximum norm C to limit the impact of individualdata points on model parameters, then adds Gaussian noise scaled by to ensure privacy acrosstraining steps. This mechanism allows DP-SGD to provide a theoretical upper bound on cumulativeprivacy loss, ensuring privacy throughout training (known as privacy accounting). The completeDP-SGD algorithm is shown in Algorithm 1.",
  ": return T": "Privacy Auditing.Although DP-SGD provides a theoretical upper bound on privacy budgets(, ), relying solely on privacy accounting poses challenges. First, Theoretical analyses, whilecrucial, are demonstrated to be conservative [Bassily et al., 2014, Kairouz et al., 2015, Abadi et al.,2016, Mironov, 2017, Koskela et al., 2020, Gopi et al., 2021, Doroshenko et al., 2022], which canlead to overestimating the required noise and subsequently reducing the utility of the model [Nasret al., 2021]. Second, the complexity of DP-SGD training can lead to implementation errors thatcompromise privacy guarantees [Ding et al., 2018, Tramer et al., 2022]. Privacy auditing addressesthese issues by establishing empirical lower bounds on privacy, providing a more realistic assessmentof privacy loss. In privacy auditing, Nasr et al. decomposes the attack process into three main components:Crafter, Trainer, and Distinguisher. The Crafter generates a canary sample c to distinguish betweenneighboring datasets D and D (i.e., D = D {c}). The Trainer then trains a model on one of thesetwo datasets using the DP-SGD training algorithm. Finally, the Distinguisher receives the datasets D and D from the Crafter, along with the trained model as input. The distinguisher then infers whichdataset was used, with the accuracy of this inference indicating the level of privacy leakage. Together,the Crafter and the Distinguisher form the adversary (A). Privacy Auditing Setup.Privacy auditing can be categorized into two settings based on adversariesaccess levels: intermediate models [Nasr et al., 2021, 2023, Andrew et al., 2023, Steinke et al., 2024b,Mahloujifar et al., 2024], final model [Jagielski et al., 2020, Nasr et al., 2021, Annamalai andDe Cristofaro, 2024, Cebere et al., 2024, Steinke et al., 2024a]. For the intermediate modelssetup, the adversaries have full access to the model parameters during all training steps. Previousworks [Nasr et al., 2021, 2023] have demonstrated that privacy auditing in this setup achieves a tightlower bound, where the theoretical upper bound and the empirical lower bound (emp) can align. However, in practice, adversaries are more likely to have access only to the final model rather thanto all intermediate training steps. Access to the final model is available either through its outputs,such as via APIs, or through direct access to its weights, as in open-source models. In the case ofaccess to the final model through an API, a substantial gap remains between and emp. AlthoughAnnamalai and De Cristofaro attempts to address this, their approach requires additionalassumptions on initialized parameters and still lacks tightness. In contrast, recent works [Annamalai,2024, Cebere et al., 2024] aim for tight auditing via direct access to model weights in the final modelsetting. However, their auditing methods focus on specific scenarios, such as constructing worst-case non-convex loss functions for DP-SGD, manually designing gradient sequences, or assumingaccess to the initial parameters (0). Our work delivers tighter privacy auditing without additionalassumptions, relying solely on final model. Privacy auditing techniques can be broadly classified into gradient-space auditing and input-spaceauditing, based on the modifications allowed by the Crafter. In gradient-space auditing [Nasr et al.,2021, Maddock et al., 2022, Nasr et al., 2023, Andrew et al., 2023, Steinke et al., 2024b], the gradientcanary is embedded directly into the gradients during training, allowing targeted gradient-levelinterventions. In contrast, input space auditing [Jagielski et al., 2020, Zanella-Beguelin et al., 2023,Andrew et al., 2023, Steinke et al., 2024a] uses a canary in the form of an input sample, focusing onthe training data itself. In our experiments, we adopt input-space auditing, which is more practical.",
  "DP-SGD Auditing procedure": "Recent privacy auditing studies [Nasr et al., 2021, Tramer et al., 2022, Nasr et al., 2023, Steinkeet al., 2024a, Cebere et al., 2024, Chadha et al., 2024] often employ membership inference attacks(MIAs) [Shokri et al., 2017] to determine whether a specific sample was included in the training databy analyzing the outputs of M(D) or M(D). Similarly, our auditing procedure in Algorithm 2 usesMIAs to evaluate the privacy guarantees of the model, consistent with their widespread adoption asthe de facto standard in many privacy studies [Jeon et al., 2024, Thaker et al., 2024]. The procedure begins with Crafter phase. A canary Crafter crafts a canary sample c as an outlierwithin the training dataset D. This canary sample is then inserted into the original dataset to createa neighboring dataset, D = D {c}. In our experiments, following prior works [De et al., 2022,Nasr et al., 2023, Steinke et al., 2024a], we primarily use a blank image as the canary to maximize itsdistinctiveness from typical training samples. In the Model Trainer phase, trainer applies the DP-SGD mechanism M to both D and D, producingN models for each dataset, denoted as {Mi}Ni=1 and {M i}Ni=1, respectively. These models are trainedindependently to analyze the effect of canary insertion across multiple models. The Distinguisher phase performs the core auditing process. It first crafts an adversarial samplea, then computes the loss values of this sample across all models, forming output sets O and Ofor {Mi}Ni=1 and {M i}Ni=1, respectively. Using a decision threshold determined by a separatedataset (e.g., a validation set), the Distinguisher calculates the False Positive Rate (FPR) and FalseNegative Rate (FNR) based on O and O. It then computes the upper bounds of FPR and FNR usingClopper-Pearson confidence intervals [Clopper and Pearson, 1934]. Finally, based on the empiricalupper bounds FPR and FNR, the Distinguisher estimates the empirical lower bound for the privacyparameter at a given , denoted as emp.",
  ": FPR Clopper-Pearson(FPR,N,)15: FNR Clopper-Pearson(FNR,N,)16: emp EstimateDP(FPR,FNR,)17: return emp": "Nasr et al. demonstrated that the privacy region for DP-SGD aligns more closely with the-Gaussian Differential Privacy guarantee (-GDP) [Dong et al., 2022] than with the (, )-DPguarantee, establishing a direct relationship between FPR, FNR, and . Furthermore, it enables atighter estimate of privacy leakage with fewer training runs than (, )-DP. In addition, when decidingthe decision threshold, any value for -GDP has an equal likelihood of maximizing the lower boundgiven a sufficient number of observations [Nasr et al., 2023]. Following previous work [Nasr et al.,2021, Maddock et al., 2022, Zanella-Beguelin et al., 2023], we use the threshold that maximizes the lower bound for the same set of observations. Calculating a empirical bound for in -GDP, which can be converted to a lower bound for in(, )-DP, provides an effective estimate of privacy leakage. To compute a lower bound on the privacyparameters of the Gaussian mechanism (i.e., ), we have:",
  "Crafting Adversarial Samples for Tighter Privacy Auditing": "To obtain tighter lower bounds for membership inference attacks (MIAs), some studies [Nasr et al.,2021, 2023] craft input canaries based on the final model weights during line 3 of Algorithm 2. Incontrast, most prior works [Carlini et al., 2019, Jagielski et al., 2020, Nasr et al., 2021, Aerni et al.,2024, Annamalai and De Cristofaro, 2024] rely on the heuristic assumption that the most vulnerabledata point serves as the canary sample in the line 8 of the same algorithm. In our study, instead ofrelying on a canary-based approach, we directly construct the worst-case adversarial sample, adheringto the requirement that every output must satisfy the differential privacy guarantee, thereby ensuringfundamental indistinguishability across all possible input samples. To the best of our knowledge,this is the first attempt in loss-based input-space auditing to achieve tighter lower bounds by using aworst-case adversarial sample instead of a traditional canary.",
  "(c) aade": ": The loss distributions of three adversarial samples (ac, aude, aade) at a fixed privacy budget = 10.0. The green distribution represents the loss outputs of models M, while the blue distributionrepresents the loss outputs of models M . To enable tighter privacy auditing, we craft a worst-case adversarial sample aw that maximizes thedistinguishable difference between the distributions of outputs O and O. This involves designinga specialized loss function to enhance the separability of these distributions. When crafting aw,we leverage the Distinguishers knowledge of the canary by initializing the adversarial samplea = (xa, ya) with the canarys values c = (xc, yc). Each pixel of xa is treated as an learnablevariable to form the worst-case scenario, while the label ya is fixed as a constant yc. Our initial loss function, Lude (Uniform Distance Expansion), is based on the observation that thecanarys loss is naturally higher in models trained on dataset D (denoted as Mi) than in modelstrained on D (denoted as M i). Therefore, we train the adversarial sample to ensure that models Miproduce a higher loss and models M i produce a lower loss when evaluating aw, aiming to maximizethe distinguishability between their output distributions. The loss function Lude is defined as:",
  "i=1( (M i(xa), ya) (Mi(xa), ya)) ,": "and the corresponding minimizer, aude. However, as illustrated in b, using Lude can lead tounnecessary training on the non-overlapping regions of the two distributions. This results in increaseddispersion rather than improved separation between the distributions. To address this issue, we propose an adaptive loss function Lade (Adaptive Distance Expansion)that focuses solely on the overlapping regions of the loss distributions, avoiding training on alreadydistinguishable values. This targeted approach improves the distinction between the distributions, asdepicted in c. The adaptive loss function Lade is defined as:",
  "and the corresponding minimizer, aade": "In this equation, Lade adjusts the loss based on how much each M i loss deviates from the mean lossof all models Mj. The function ReLU ensures that only positive deviations contribute to the loss,effectively limiting unnecessary updates when M i loss is already sufficiently separated from themean. This allows the training process to focus on further separating less distinct distributions. Here, is a tunable margin that controls the sensitivity of the separation adjustment. Increasing seeksto further separate the distributions but may risk isolating only a few samples. In this work, we set = 0.2 for all experiments. Further analysis of the impact of is provided in .4.",
  "of Annamalai and De Cristofaro . We set the learning rate to = 4 for MNIST and = 2 forCIFAR-10. The training process runs for T = 100 iterations on MNIST and T = 200 iterations onCIFAR-10": "For models trained with DP-SGD, the accuracy on MNIST is nearly 95% for all , while the accuracyon CIFAR-10 is around 52% at = 10.0. This is due to the fact that models on CIFAR-10 convergemuch more slowly [Annamalai and De Cristofaro, 2024]; however, increasing the number of trainingepochs can raise the accuracy to around 70%, making it comparable to the state-of-the-art [De et al.,2022]. Implementation Details.To ensure robustness, we perform 10 independent runs for each experi-ment and report the average value of emp along with its standard deviation, and the theoretical upperbound for is computed using the privacy accountant provided by Opacus [Yousefpour et al., 2021].In each run, we generate 2N = 256 models128 with canary (M ) and 128 without canary (M). Wecompute empirical lower bounds using the -GDP approach, following previous works [Nasr et al.,2023, Cebere et al., 2024], and report the lower bounds with 95% confidence intervals. Additionally, to simplify the privacy analysis, we set the sampling rate to 1, performing DP-SGDwith full-batch training (B = |D|). While, analyzing DP-SGD with both subsampling amplificationand multi-step composition is challenging, and relying on the central limit theorem for subsamplingmay underestimate the privacy budget due to the emergence of mixture distributions [Nasr et al.,2023]. Although the Privacy Loss Distribution (PLD) [Koskela et al., 2020] is used to approximatethe trade-off function and address this issue [Nasr et al., 2023], PLD lacks a closed-form trade-offfunction and provides a looser bound for . For these reasons, we focus our analysis on the full-batchsetting.",
  "White-Box Auditing in Final Model": "In , we compare the empirical privacy bounds emp between the canary sample (ac) and ourcrafted worst-case samples (aw). For ac, we use the default canary that differentiates D and D. Foraw, we employ samples generated using the loss functions Lude and Lade, resulting in aude and aade,respectively. Utilizing aw for privacy auditing consistently yields tighter empirical lower boundsacross various theoretical upper bounds compared to the baseline ac. For instance, with a theoreticalprivacy budget of = 10.0, a canary sample ac produces empirical lower bounds of 4.11 for MNISTand 0.30 for CIFAR-10. In contrast, aude and aade yield estimates of 5.61 and 6.68 for MNIST, and1.05 and 4.10 for CIFAR-10, respectively. Notably, when using ac on CIFAR-10, we observe almostno detectable privacy leakage. However, employing aade reveals significant privacy leakage. Thisdemonstrates aades superior effectiveness in providing tighter empirical lower bounds on privacy.",
  "Black-box Auditing in Final Model": "We also conduct privacy auditing in the final model setting when the Distinguisher has access only tothe model output. In this setting, since the Distinguisher cannot directly access the weights of the finalmodel, it is not feasible to directly train adversarial sample through final model. Instead, leveragingthe fact that the Distinguisher knows both neighboring datasets D and D, we apply distillation techniques to train 128 surrogate models using the output of D for each model M and M , resultingin a total of 2N = 256 models. These surrogate models, distilled from the final model output in D,serve as proxies for M and M (Msur and M sur). We then train the adversarial sample using Msurand M sur going through the same process as white-box auditing. 1.02.04.010.0",
  ": Black-box privacy auditing re-sults on the MNIST dataset, comparingthree auditing samples: the canary sam-ple, aude, and aade, across privacy bud-gets {1.0, 2.0, 4.0, 10.0}": "As shown in , although black-box auditing pro-vides less tighter results than white-box auditing due tothe attackers limited access, the aw trained by surrogatemodels consistently yields tighter lower bounds comparedto the baseline ac = c. Specifically, when the theoreticalupper bound is 10.0, the empirical lower bound using thebaseline ac was 4.11, but it increased to 4.40 with audeand 4.51 with aade. Furthermore, while aade providedsignificantly tighter auditing results than aude in all pri-vacy budgets in white-box auditing, both methods showsimilar lower bounds in black-box auditing. Since black-box auditing is typically limited to training interventions,crafting adversarial samples offers a promising approachfor differential privacy auditing.",
  "Theoretical": "Empirical emp sample acsample audesample aade : White-box privacy auditingresults on the MNIST dataset when or-dinary data is used as the canary sam-ple, evaluated at privacy budgets {1.0, 2.0, 4.0, 10.0}.The adversarialsample successfully audits the model,while the default canary fails. Canary type.Typically, the canary sample used exclu-sively for training M is an out-of-distribution (OOD) sam-ple that differs significantly from the training dataset D.Without an OOD canary, obtaining tighter lower boundsbecomes challenging, as the diffrence between M and M is minimal. However, leveraging a worst-case adversarialsample enables significantly tighter lower bounds, evenwhen using in-distribution (ID) canary. As shown in , audits using a standard ID canarysample yield zero empirical privacy leakage estimates(emp) across all privacy budgets when applying the base-line attack ac. In contrast, adversarial samples improveauditing performance significantly. At = 10.0, the em-pirical lower bounds rise to 4.05 for aude and 4.33 foraade, demonstrating the effectiveness of aw in exposingprivacy leakage. Furthermore, aade consistently achievesslightly tighter bounds than aude across all privacy bud-gets, highlighting the robustness of adaptive attacks in privacy auditing. These results emphasize theimportance of leveraging adversarial sample when auditing with ID canary. ordinaryblankmislabelclipbkd",
  "Canary Type": "Empirical emp sample acsample audesample aadeUpper Bound :Privacy auditing resultson the MNIST dataset using both in-distribution (ID) and out-of-distribution(OOD) data as the canary samples(blank, mislabel, clip-bkd), evaluated ata privacy budget of = 10.0. Additionally, adversarial samples are effective across di-verse canary types, including ID samples, blank samples,mislabeled samples and a clip-bkd samples. The misla-beled image is an OOD sample intentionally assigned thewrong label, and the clip-bkd image, crafted as proposedby Jagielski et al. , target the direction of least vari-ance to create a robust attack against the gradient clippingnorm. As shown in , aade consistently achieves tighterlower bounds compared to the baseline (ac) across all ca-nary types. Specifically, for ID canary, blank image, mis-labeled image, and clip-bkd image, our method achievesemp = 3.03, 3.94, 3.96, and 3.91, respectively, signifi-cantly outperforming the baseline values of emp = 0.00,1.29, 0.93, and 1.56. These results highlight the robust-ness of our approach in privacy auditing, demonstratingits effectiveness across ID and OOD canary. 1.02.04.010.0",
  ": Privacy auditing results on the MNIST and CIFAR-10 datasets using worst-case initializa-tion with adversarial sample, evaluated across privacy budgets {1.0, 2.0, 4.0, 10.0}": "Worst case initialization.Recent work [Annamalai and De Cristofaro, 2024] leverages the insightthat minimizing the gradients of non-canary samples enables tight auditing in DP-SGD, aiming toachieve this in black-box auditing of the final model by pre-training on an auxiliary dataset matchingthe training data distribution. These crafted worst-case initial model parameters are then used as themodels initial weights, replacing random initialization to obtain tighter lower bounds in a black-boxsetting in final model. In this section, we present our results on tighter privacy auditing for the final model trained on MNISTand CIFAR-10 datasets by combining the worst-case model parameters (worst) with a worst-caseadversarial sample (aw). As illustrated in , using the canonical attack ac with the averagemodel parameters avg yielded lower bounds of 4.11 on MNIST and 0.41 on CIFAR-10. When weswitched to the worst-case model parameters worst while using the same attack ac, these lower boundsincreased to 6.83 and 5.31, respectively, indicating tighter estimates of privacy leakage. Furtherimprovements were achieved by employing the adaptive attack aade with worst, which resulted ineven higher lower bounds of 9.9 on MNIST and 9.93 on CIFAR-10. Notably, across all theoreticalprivacy budgets, the auditing results using worst with aade closely aligned with the theoretical upperbounds. These findings demonstrate that we achieved tight privacy auditing, reinforcing the argumentthat privacy amplification in the final model setting does not occur with general non-convex loss. 10.01.00.1 Max Clipping Norm (C) Empirical emp AcAUDEAADE",
  ": Auditing models trained withvarying gradient clipping norms C, eval-uated at privacy budgets = 10.0": "Clipping norm sensitivity.Based on the experimentalresults, we analyze the impact of varying gradient clippingnorms on the empirical privacy leakage estimates (emp). presents the outcomes for three attack scenarios:ac, aude, and aade. When the gradient clipping norm is small (C = 0.1),the privacy leakage estimates are 5.90, 7.81 and 7.85 forac, aude and aade, respectively, indicating tight auditsdue to a high signal-to-noise ratio. As the clipping normincreases to C = 1.0, emp decreases across all scenarios;however, the rate of decrease varies. Notably, aade is lessaffected compared to ac and aude, retaining a relativelyhigh estimate of 4.24. This suggests that aade is morerobust and capable of effectively auditing privacy leakageeven with larger clipping norms. At a higher clippingnorm (C = 10.0), emp decreases significantly for all attacks, with values of 0.00, 0.59, and 0.87 forac, aude, and aade, respectively. This reduction is attributed to excessive noise overwhelming theuseful signal, resulting in looser audits.",
  "emp1.900.511.840.321.830.341.870.212.030.24": "These results highlight the trade-offs betweengradient clipping norms and the tightness of pri-vacy audits, emphasizing that adaptive attackslike aade can provide more effective auditingeven under higher clipping norms. sensitivity.Although the worst-case adversarial sample demonstrates tighter privacy auditingperformance, it relies on selecting an appropriate value for the hyperparameter . To assess the effectof , we trained adversarial samples using the Lade with set to various values: 0.05, 0.1, 0.15, 0.2,and 0.25. As shown in , the Lade exhibits similar auditing performance across these values,indicating that our loss function is robust to the choice of , which is advantageous. Empirically, wefind that values between 0.02 and 0.5 do not result in significant performance degradation.",
  "Conclusion": "We introduce a novel method for crafting worst-case adversarial samples to achieve tighter lowerbounds in privacy auditing of differentially private models, focusing on the final model setting. Unliketraditional canary-based approaches, our technique maximizes the distinguishability between outputdistributions by designing specialized loss functions, particularly the adaptive loss function Lade.This approach enhances the separability of overlapping regions in the loss distributions, leading tomore effective auditing even when only the final model is accessible. Our experiments on MNIST and CIFAR-10 datasets demonstrate that our method consistentlyprovides tighter empirical lower bounds on privacy leakage compared to baseline methods, both inwhite-box and black-box settings. For instance, on CIFAR-10 with a theoretical privacy budget of = 10.0, the traditional canary-based method yields an empirical lower bound of only 0.30, whereasour approach achieves a significantly tighter lower bound of 4.10. Notably, our approach is effectiveeven when using in-distribution (ID) samples as canaries, eliminating the need for out-of-distribution(OOD) samples. This robustness across various canary types highlights the practical applicability ofour method. By enabling more precise privacy auditing without relying on specific canary samples or additionalassumptions, our work significantly advances the evaluation of differential privacy guarantees in thefinal model setting. Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, andLi Zhang. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSACconference on computer and communications security, pages 308318, 2016.",
  "Borja Balle, Giovanni Cherubin, and Jamie Hayes. Reconstructing training data with informedadversaries. In 2022 IEEE Symposium on Security and Privacy (SP), pages 11381156. IEEE,2022": "Raef Bassily, Adam Smith, and Abhradeep Thakurta. Private empirical risk minimization: Efficientalgorithms and tight error bounds. In 2014 IEEE 55th annual symposium on foundations ofcomputer science, pages 464473. IEEE, 2014. Benjamin Bichsel, Samuel Steffen, Ilija Bogunovic, and Martin Vechev. Dp-sniper: Black-boxdiscovery of differential privacy violations using classifiers. In 2021 IEEE Symposium on Securityand Privacy (SP), pages 391409. IEEE, 2021.",
  "Jinho Bok, Weijie Su, and Jason M Altschuler. Shifted interpolation for differential privacy. arXivpreprint arXiv:2403.00278, 2024": "Nicholas Carlini, Chang Liu, lfar Erlingsson, Jernej Kos, and Dawn Song. The secret sharer:Evaluating and testing unintended memorization in neural networks. In 28th USENIX securitysymposium (USENIX security 19), pages 267284, 2019. Nicholas Carlini, Steve Chien, Milad Nasr, Shuang Song, Andreas Terzis, and Florian Tramer.Membership inference attacks from first principles. In 2022 IEEE Symposium on Security andPrivacy (SP), pages 18971914. IEEE, 2022.",
  "Vadym Doroshenko, Badih Ghazi, Pritish Kamath, Ravi Kumar, and Pasin Manurangsi. Con-nect the dots: Tighter discrete approximations of privacy loss distributions.arXiv preprintarXiv:2207.04380, 2022": "Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity inprivate data analysis. In Theory of Cryptography: Third Theory of Cryptography Conference, TCC2006, New York, NY, USA, March 4-7, 2006. Proceedings 3, pages 265284. Springer, 2006. Vitaly Feldman, Ilya Mironov, Kunal Talwar, and Abhradeep Thakurta. Privacy amplification byiteration. In 2018 IEEE 59th Annual Symposium on Foundations of Computer Science (FOCS),pages 521532. IEEE, 2018.",
  "Ilya Mironov. Rnyi differential privacy. In 2017 IEEE 30th computer security foundations symposium(CSF), pages 263275. IEEE, 2017": "Milad Nasr, Shuang Songi, Abhradeep Thakurta, Nicolas Papernot, and Nicholas Carlin. Adversaryinstantiation: Lower bounds for differentially private machine learning. In 2021 IEEE Symposiumon security and privacy (SP), pages 866882. IEEE, 2021. Milad Nasr, Jamie Hayes, Thomas Steinke, Borja Balle, Florian Tramr, Matthew Jagielski, NicholasCarlini, and Andreas Terzis. Tight auditing of differentially private machine learning. In 32ndUSENIX Security Symposium (USENIX Security 23), pages 16311648, 2023.",
  "Jiayuan Ye and Reza Shokri. Differentially private learning needs hidden state (or much fasterconvergence). Advances in Neural Information Processing Systems, 35:703715, 2022": "Samuel Yeom, Irene Giacomelli, Matt Fredrikson, and Somesh Jha. Privacy risk in machine learning:Analyzing the connection to overfitting.In 2018 IEEE 31st computer security foundationssymposium (CSF), pages 268282. IEEE, 2018. Ashkan Yousefpour, Igor Shilov, Alexandre Sablayrolles, Davide Testuggine, Karthik Prasad, ManiMalek, John Nguyen, Sayan Ghosh, Akash Bharadwaj, Jessica Zhao, et al. Opacus: User-friendlydifferential privacy library in pytorch. arXiv preprint arXiv:2109.12298, 2021. Santiago Zanella-Beguelin, Lukas Wutschitz, Shruti Tople, Ahmed Salem, Victor Rhle, AndrewPaverd, Mohammad Naseri, Boris Kpf, and Daniel Jones. Bayesian estimation of differentialprivacy. In International Conference on Machine Learning, pages 4062440636. PMLR, 2023. Yuqing Zhu, Xiang Yu, Manmohan Chandraker, and Yu-Xiang Wang. Private-knn: Practical differen-tial privacy for computer vision. In Proceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 1185411862, 2020."
}