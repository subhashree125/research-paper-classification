{
  "Abstract": "This study presents the conditional neural fields for reduced-order modeling (CNF-ROM) framework to approximate solutions of parametrized partial differentialequations (PDEs). The approach combines a parametric neural ODE (PNODE) formodeling latent dynamics over time with a decoder that reconstructs PDE solutionsfrom the corresponding latent states. We introduce a physics-informed learningobjective for CNF-ROM, which includes two key components. First, the frameworkuses coordinate-based neural networks to calculate and minimize PDE residualsby computing spatial derivatives via automatic differentiation and applying thechain rule for time derivatives. Second, exact initial and boundary conditions(IC/BC) are imposed using approximate distance functions (ADFs) [Sukumar andSrivastava, CMAME, 2022]. However, ADFs introduce a trade-off as their second-or higher-order derivatives become unstable at the joining points of boundaries.To address this, we introduce an auxiliary network inspired by [Gladstone et al.,NeurIPS ML4PS workshop, 2022]. Our method is validated through parameterextrapolation and interpolation, temporal extrapolation, and comparisons withanalytical solutions.",
  "Introduction": "Numerical simulations for solving nonlinear partial differential equations (PDEs) have advancedscientific understanding by enabling accurate modeling of complex physical phenomena. However,the computational demands of high-fidelity simulations have led to the development of varioussurrogate and reduced-order modeling (ROM) techniques. Some of these techniques simplify theunderlying physics, while another approach is to accelerate computations by leveraging data, as seenin linear subspace projection-based ROMs and nonlinear manifold ROMs . In recent years, neural networks have been employed increasingly to approximate PDE solutions, withphysics-informed neural networks (PINNs) emerging as an approach that learns physicsby incorporating the governing PDE directly into the loss function. A more recent direction hasemerged in coordinate-based neural networks, originally developed to learn implicit representationsof complex signals, commonly referred to as implicit neural representations (INRs) . Thesenetworks are particularly attractive for PDE-related tasks because they can learn continuous functionsover domains. Building on this capability, a series of works has been proposed that use data-drivenapproaches to learn PDE solutions using an INR-based decoder . This approach leverages",
  "the idea of modeling a vector field with a neural function that is conditioned on a latent state, aframework we refer to as conditional neural fields ROMs (CNF-ROMs)": "Building on these developments, our work extends the CNF-ROM framework to support both data-driven and physics-informed learning. To the best of our knowledge, this is the first application of aspace- and time-separated CNF for training PINNs. Our contributions are summarized as follows: Establishing a physics-informed learning objective for CNF-ROM framework, Enabling the framework to handle parametrized PDEs using PNODEs , Addressing challenges in enforcing exact initial and boundary conditions (IC and BC) by introduc-ing an auxiliary network to approximate first-order derivatives, following , Introducing training objectives with simultaneous optimization of decoder and PNODE parameters, Verifying the performance in parameter interpolation/extrapolation (with PINN fine-tuning forunseen parameters), temporal extrapolation, and comparison with analytical solutions.",
  "tu = L(u; ),u(x, 0, ) = u0(x, ),B(u; ) = 0,t [0, T],x IRd,(1)": "where L is a differential operator and B is a boundary operator. u : (0, T] D IR is thesolution of the PDE that represents a physical quantity such as velocity or pressure. u(x, t, ) can beapproximated by a neural network u(x, t, ), where IRd is a vector of tunable parameters. Conditional neural fieldsWhen coordinate-based neural networks specifically operate in spatialand temporal domains, we refer to them as neural fields (NFs) , borrowing the term fields\"from physics. CNFs extend NFs by introducing a conditioning latent factor . A commonapproach uses a hypernetwork h :IRdIRd to generate (a part of) high-dimensional parameters = h() IRd from a low-dimensional latent state IRd. This use of a hypernetworkenables reduced-order modeling by casting the problem as one of modeling latent states. We applythis framework to approximate solutions of spatio-temporal governing PDEs, referring to it as theconditional neural field for reduced-order modeling (CNF-ROM). This structure is illustrated in thediagram shown in . The approach combines a parametric neural ODE (PNODE) , whichmodels latent dynamics, with a decoder that reconstructs PDE solutions. The decoder maps the latentstate and coordinates to the solution, while the PNODE captures distinct latent trajectories for each . DecoderBuilding on the DINo architecture , we implement a decoder by employing a spatialcoordinate-based network based on FourierNet . To separate the spatial and temporal domains,DINo introduces the hypernetwork whose parameters are determined conditionally on the time-dependent latent state, which in our framework is extended to also depend on PDE parameters. Theexpanded section on the right side of illustrates the detailed DINo architecture integratedwith PNODE. Following this approach, we define our decoder as",
  "u(x, t, ) = D(x, t,),(2)": "where sinusoidal filters s((l)x) = [sin((l)x), cos((l)x)] are used as Fourier basis. At each timestep t and parameter , a latent state t, determines the high-dimensional parameters of the decoder,thereby shifting the learning of the decoder to the training of the hypernetwork parameters . PNODERecent advancements in PINNs emphasize the importance of learning parameterizedsolutions to generalize across multiple PDE parameter values () . These extensions aimto capture diverse solution behaviors as a function of parameters, avoid retraining for every newparameter, and identify patterns across the parameter space, enabling generalization to unseenparameter values. Parameterized neural ODEs (PNODEs) address these challenges by extendingneural ODEs (NODEs) to incorporate as inputs, allowing them to model the evolution of latentstates for multiple trajectories. In our model, PNODEs learn the function f:",
  "Methods: Physics-informed training with exact satisfaction of IC/BC": "Physics-informed learning with parametrized CNF-ROMWith the CNF-ROM frameworkintroduced in , we propose a physics-informed learning objective for PINNs. A physics-informed learning objective typically includes loss terms for the governing PDE residual, as well asfor IC and BC. These terms ensure that the solution satisfies the PDE and adheres to the given ICand BC . To compute the necessary spatial and temporal derivatives for the PDE residual loss, theCNF-ROM leverages coordinate-based neural networks to directly compute spatial derivatives viaautomatic differentiation. Temporal dynamics, on the other hand, are modeled through a latent statet, that evolves over time for each . Using the chain rule, temporal derivatives are expressed as:",
  "Instead of incorporating IC and BC as loss terms, we address strategies for imposing exact IC andBC and discuss the trade-offs in this approach in the following paragraphs": "Exact imposition of IC and BCExact imposition of IC and BC reduces the number of lossconstraints while ensuring the uniqueness of the solution, which improves training convergence andpredictive accuracy. The R-function-based approximate distance functions (ADFs) proposed by were developed to enforce boundary conditions while satisfying other desirable properties suchas differentiability. These functions can be constructed to satisfy Dirichlet, Neumann, and Robinconditions a priori, even on complex and irregular geometries; see for more details. Buildingon this foundation, we extend ADFs to the temporal domain to address initial conditions as well byensuring (x, t) = 0 when x or t = 0. For Dirichlet boundary conditions, where u = g isimposed on boundaries, we use the following construction:",
  "Here, is an ADF and D(x, t,) denotes the decoder output in Eq. (2)": "When using the R-function-based ADFs, the following challenges arise when the input dimension isgreater than two. In our case, this holds as the ADF includes both time and space coordinates. To bespecific, (i) ADF is not well-defined on the boundary, and (ii) its second (or higher) derivativesexplode near boundary junctions, as noted in . To mitigate these challenges, (i) we excludethe boundary set from the PINN loss definition in Eq. (8) and (ii) adopt a more delicate approachproposed in . This approach addresses the trade-off in imposing IC and BC on u by approximatingits first-order derivatives. For example, to compute xxu, instead of directly computing the second-order derivatives, we first train an auxiliary network to approximate the first-order derivatives and",
  "In the following paragraph, we describe two learning modes: data-driven learning and physics-informed learning. In both modes, the solution is constructed as in Eq. (5) to impose IC and BC": "Training objectivesOne key advantage of imposing exact IC and BC is that it eliminates the needfor an encoder. Since the output satisfies the initial condition for any arbitrary 0, we initialize0 = 0 IRd. Once the initial latent state is obtained, we use ODE solvers to evolve itaccording to Eq. (3). The use of PNODEs allows the initial trajectory to adapt based on differentvalues of , enabling the model to generate distinct outputs. Note that for v in Eq. (6), 0, needs tobe learned, and we use an auto-decoding approach as suggested in .",
  ",(10)": "with the initial condition determined by (10) at t = 0. The sets of training and test parameters train ={20, 30, 40, 50, 60, 70, 80, 100} and test = {15, 25, 45, 90, 110} and the latent state dimensiond = 10 are used. The numerical solution of the full-order model was obtained using a uniformspatial and temporal discretization (nx = 64, nt = 100) and a backward Euler time integrator. Weconsider the following training scenarios:",
  "(b) Fine-tuning with PINN loss: LPDE() + Lderiv(), T = 1, train test": ": Performance verification through: (top left) loss trajectories, (bottom left) evaluation ontraining and test set parameters, and temporal extrapolation at t = 1.25; (top right) heatmap of lossfor model (a) at = 20; (bottom right) heatmap of loss for model (b) at = 20. The goal of these scenarios is to highlight the advantage of using PINN as a fine-tuning objective,leveraging its ability to learn without data. For scenario (a), both the decoder and the auxiliarynetwork are optimized during training. For scenario (b), starting from the pre-trained model from (a),we freeze the decoder and use the PDE loss to update only the PNODE (latent dynamics) parametersfor the target parameters. This PINN fine-tuning stage models only the low-dimensional latent state,aligning with the ROM perspective. presents the performance evaluation from multiple perspectives. The top left panel ofpresents the loss trajectory, displaying the loss with respect to exact solutions in Eq. (10) (ExactLoss\", solid black), its relative error (Exact Rel. Loss\", dashed black) and the PDE loss for PINN(PINN Loss\", solid green). The model was pre-trained under scenario (a) up to epoch 4500, followedby fine-tuning under scenario (b) for the fixed parameter = 20. The purpose of this figure is tovalidate the PDE loss calculation by showing that both exact and PINN losses decrease in parallel,regardless of whether the training was based on (a) data loss or (b) PINN loss, confirming that thePDE residual loss aligns with the exact solution. The right panels of present heatmaps of theloss for = 20. The top right heatmap corresponds to the model trained under scenario (a), while thebottom right heatmap shows the result of the fine-tuned model (b). We first observe that additionalfine-tuning with PINN loss led to additional error reduction. Notably, the region t > 1 represents anextrapolation beyond the trained domain (i.e. forecasting). The error plots reveal that forecast errorsare more pronounced in the pre-trained model, whereas the fine-tuned PINN loss model exhibitsrobust result in this region. The bottom left panel of offers a summary of the performance of the model trained under thepre-trained scenario (a, ) and the fine-tuned scenario (b, ) for each parameter train test.This panel compares fine-tuning results for the training set (blue), inter-/extrapolation results in theparameter space (red), and temporal extrapolation (black). For temporal extrapolation, all models(a) and (b) were trained up to T = 1 and used to evaluate up to t = 1.25. Specifically, fine-tuningwith PINN loss demonstrated clear improvements across all regions except temporal extrapolationperformance at = 30, with a particularly significant error reduction in parameter extrapolation areaswhere = 15 or 110. In general, we observed that the fine-tuning with PINN loss improves modelperformance by learning without additional data, enabling further learning for unseen parameters, andenhancing robustness in forecast regions. Finally, we conclude with the remark that the CNF-ROMframework can be extended to higher-dimensional problems, which we leave their exploration as afuture direction.",
  "and Disclosure of Funding": "K. Lee was supported by NSF under grant IIS #2338909. Y. Choi was supported by the U.S.Department of Energy, Office of Science, Office of Advanced Scientific Computing Research, as partof the CHaRMNET Mathematical Multifaceted Integrated Capability Center (MMICC) program,under Award Number DE-SC0023164. Lawrence Livermore National Laboratory is operated byLawrence Livermore National Security, LLC, for the U.S. Department of Energy, National NuclearSecurity Administration under Contract DE-AC52-07NA27344. IM release number: LLNL-CONF-869137. Kevin Carlberg, Charbel Farhat, Julien Cortial, and David Amsallem. The gnat method for nonlinear modelreduction: Effective implementation and application to computational fluid dynamics and turbulent flows.Journal of Computational Physics, 242:623647, 2013.",
  "Youngsoo Choi and Kevin Carlberg. Spacetime least-squares petrovgalerkin projection for nonlinearmodel reduction. SIAM Journal on Scientific Computing, 41(1):A26A58, 2019": "Youngsoo Choi, Deshawn Coombs, and Robert Anderson. SNS: A solution-based nonlinear subspacemethod for time-dependent model order reduction. SIAM Journal on Scientific Computing, 42(2):A1116A1146, 2020. Lawson Fulton, Vismay Modi, David Duvenaud, David IW Levin, and Alec Jacobson. Latent-spacedynamics for reduced deformable simulation. In Computer graphics forum, volume 38, pages 379391.Wiley Online Library, 2019.",
  "Kookjin Lee and Kevin T. Carlberg. Model reduction of dynamical systems on nonlinear manifolds usingdeep convolutional autoencoders. Journal of Computational Physics, 404:108973, 2020": "Youngkyu Kim, Youngsoo Choi, David Widemann, and Tarek Zohdi. A fast and accurate physics-informedneural network reduced order model with shallow masked autoencoder. Journal of Computational Physics,451:110841, 2022. Maziar Raissi, Paris Perdikaris, and George Em Karniadakis. Physics-informed neural networks: A deeplearning framework for solving forward and inverse problems involving nonlinear partial differentialequations. Journal of Computational Physics, 378:686707, 2019.",
  "Woojin Cho, Minju Jo, Haksoo Lim, Kookjin Lee, Dongeun Lee, Sanghyun Hong, and Noseong Park.Parameterized physics-informed neural networks for parameterized PDEs. Preprint, arXiv:2408.09446,2024": "Vincent Sitzmann, Julien Martel, Alexander Bergman, David Lindell, and Gordon Wetzstein. Implicitneural representations with periodic activation functions. Advances in Neural Information ProcessingSystems, 33:74627473, 2020. Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, UtkarshSinghal, Ravi Ramamoorthi, Jonathan Barron, and Ren Ng. Fourier features let networks learn highfrequency functions in low dimensional domains. Advances in neural information processing systems,33:75377547, 2020. Yuan Yin, Matthieu Kirchmeyer, Jean-Yves Franceschi, Alain Rakotomamonjy, and Patrick Gallinari.Continuous PDE dynamics forecasting with implicit neural representations. In The Eleventh InternationalConference on Learning Representations, 2023. Peter Yichen Chen, Jinxu Xiang, Dong Heon Cho, Yue Chang, GA Pershing, Henrique Teles Maia,Maurizio M Chiaramonte, Kevin Thomas Carlberg, and Eitan Grinspun. Crom: Continuous reduced-ordermodeling of pdes using implicit neural representations. In The Eleventh International Conference onLearning Representations, 2023. Zhong Yi Wan, Leonardo Zepeda-Nunez, Anudhyan Boral, and Fei Sha. Evolve smoothly, fit consis-tently: Learning smooth latent dynamics for advection-dominated systems. In The Eleventh InternationalConference on Learning Representations, 2023.",
  "Kookjin Lee and Eric J. Parish. Parameterized neural ordinary differential equations: applications tocomputational physics problems. Proceedings of the Royal Society A, 477(2251):20210162, 2021": "Tianshu Wen, Kookjin Lee, and Youngsoo Choi. Reduced-order modeling for parameterized pdes viaimplicit neural representations. In Proceedings of the Machine Learning and the Physical SciencesWorkshop at the 37th Conference on Neural Information Processing Systems (NeurIPS), 2023. N. Sukumar and Ankit Srivastava. Exact imposition of boundary conditions with distance functions inphysics-informed deep neural networks. Computer Methods in Applied Mechanics and Engineering,389:114333, 2022. Rini J. Gladstone, Mohammad A. Nabian, N. Sukumar, Ankit Srivastava, and Hadi Meidani. FO-PINNs:A first-order formulation for physics informed neural networks. In Proceedings of the Machine Learningand the Physical Sciences Workshop at the 36th Conference on Neural Information Processing Systems(NeurIPS), 2022. Yiheng Xie, Towaki Takikawa, Shunsuke Saito, Or Litany, Shiqin Yan, Numair Khan, Federico Tombari,James Tompkin, Vincent Sitzmann, and Srinath Sridhar. Neural fields in visual computing and beyond.Computer Graphics Forum, 2022. Jan Hagnberger, Marimuthu Kalimuthu, Daniel Musekamp, and Mathias Niepert. Vectorized conditionalneural fields: A framework for solving time-dependent parametric partial differential equations. In RuslanSalakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and FelixBerkenkamp, editors, Proceedings of the 41st International Conference on Machine Learning, volume 235of Proceedings of Machine Learning Research, pages 1718917223. PMLR, 2127 Jul 2024.",
  "Ricky T.Q. Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differentialequations. Advances in neural information processing systems, 31, 2018": "Stefano Berrone, Claudio Canuto, Moreno Pintore, and N. Sukumar. Enforcing dirichlet boundaryconditions in physics-informed neural networks and variational physics-informed neural networks. Heliyon,9(8):e18820, 2023. Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. Deepsdf:Learning continuous signed distance functions for shape representation. In The IEEE Conference onComputer Vision and Pattern Recognition (CVPR), June 2019."
}