{
  "Abstract": "The word embedding space in neural models is skewed, and correcting this canimprove task performance. We point out that most approaches for modeling,correcting, and measuring the symmetry of an embedding space implicitly assumethat the word frequencies are uniform; in reality, word frequencies follow a highlynon-uniform distribution, known as Zipfs law. Surprisingly, simply performingPCA whitening weighted by the empirical word frequency that follows Zipfs lawsignificantly improves task performance, surpassing established baselines. Froma theoretical perspective, both our approach and existing methods can be clearlycategorized: word representations are distributed according to an exponential familywith either uniform or Zipfian base measures. By adopting the latter approach, wecan naturally emphasize informative low-frequency words in terms of their vectornorm, which becomes evident from the information-geometric perspective ,and in terms of the loss functions for imbalanced classification . Additionally,our theory corroborates that popular natural language processing methods, such asskip-gram negative sampling , WhiteningBERT , and headless languagemodels , work well just because their word embeddings encode the empiricalword frequency into the underlying probabilistic model.",
  "Introduction": "Representing discrete words by continuous vectors is a fundamental and powerful framework ofmodern deep-learning-based natural language processing (NLP). Static word embeddings ,dynamic word embeddings , and causal language models have caused a paradigmshiftthey have greatly improved the performance of virtually all kinds of NLP applications andhave been actively used in relevant areas as well. While the embedded units may be characters orsubwords instead of words, we simply refer to them collectively as word. Recently, the machine learning and NLP communities have discovered that the word embeddingspace is skewed and that correcting this can lead to better performance in downstream tasks . The isotropy of the embedding space would be one factor: vectors dispersing more evenlyshould be more discriminative than those clustered in the same direction . Typically, suchspatial symmetry in the embedding space is enhanced through centering/whitening . Nevertheless, we would like to point out that most existing approaches implicitly assume uniformword frequency to formalize spatial symmetry. Consider the classical centering operation as anexample: we first calculate the mean of the word vectors, and then subtract it to ensure they arezero-meaned. This method, however, has an unexpected pitfall. Recall that the definition of thecentroid or barycenter of a random vector x p, assuming it has a finite set of distinct realizations,",
  "arXiv:2411.00680v1 [cs.CL] 1 Nov 2024": "is given by Exp[x] = i p(xi)xi. The classical centering, based on the standard (unweighted)mean, implicitly assumes that all words occur uniformly p(w1) = = p(wn). In reality, however,word frequencies are known to follow a highly non-uniform distribution1, creating a significant gapbetween the methodology and the actual usage of words. This seemingly obvious issue does not arisewhen addressing classical statistical estimation problems, as data vectors in our hands are usuallyrepresentations of observations or instances. In contrast, word vectors used in NLP are representationsof types or classes; each of them (such as the vector for the) abstracts the numerous instances(such as the tokens of the) appearing in the data. This problem of hidden frequencies becomesapparent in the cases where the type-token distinction is crucial, such as when dealing withnatural language data ( 2). The take-home message of this paper can be summarized as follows: useempirical word frequencies when calculating expected values. Following this very simple guidelineleads to strong empirical outcomes ( 3.2, 3.3) and opens a rich theoretical landscape ( 4, 5).",
  "Motivation: type-token distinction and expected values": "Why have word frequencies been overlooked when considering the geometric properties of em-bedding spaces? This can be explained through the concept of type-token distinction , whichis a fundamental concept in linguistics and related fields but generally not required in statisticalmachine learning. Here, type represents a class and token represents an instance. For example, thephrase perform natural language processing in a natural way contains eight tokensand seven types. The instances natural appear twice, but as a word type, it is counted only once. With the type-token distinction in mind, let us take a fresh look at data matrices and their expectedvalues. Typically, each row in a data matrix represents one observation, i.e., one instance token. Ifwe want to centralize a set of data vectors, computing the unweighted mean is a natural way in themachine learning pipeline. On the other hand, each row of a word embedding matrix, i.e., wordvector, is a type embedding. Each word vector abstracts the numerous instances appearing repeatedlyin a corpus, though information on the frequency of instances for each word type is not encoded in it.The unweighted mean of word vectors treats type vectors as token vectors, resulting in the completeomission of word frequency information.",
  "agglutinative": ": Low-frequent words {} andhigh-frequent words {} are unevenly dis-tributed in the embedding space . Consequently, the apparent mean cal-culated by unweighted averaging oftendiffers from the actual centroid . Let us describe the above idea formally. The data ma-trix X Rnd or the set of data vectors {xi}ni=1 Rd represents a collection of instances, observations,or tokens; then the empirical distribution is X =ni=1",
  "1n(xi), where is the Dirac delta function.Here, the unweighted mean can be seen as the expecta-tion ExX[x] = ni=1": "1nxi with the empirical distri-bution. On the other hand, the word embedding matrixW Rnd or the set of word vectors {wi}ni=1 Rdrepresents a collection of types. When describing theempirical distribution, the hidden frequency p of to-kens is necessary. Given p, the empirical distributionis W = p(wi)(wi). From this perspective, the cen-troid of the word vectors should be written as the ex-pectation EwW [w] =",
  "i p(wi)wi over p": "The distinction is not just theoretical. First, refer to . Word vectors are known to cluster byfrequency . In this situation, the centroid weighted by the word frequencies islocated near the narrow region where high-frequent words are concentrated (a region with a lightblue background), and thus differs from the unweighted mean . Second, see , which shows 1As known as Zipfs law. If we count the frequencies of words in huge English corpora, we find that thehas a frequency of about 5.89 102 and isotropy has a frequency of about 3.47 108, a difference of amillion times greater. 50 words sampled from each of types and tokens. Uniform sampling from types, corresponding toan unweighted mean, tends to select mostly rare words from the heavy tail. Sampling from tokensclearly captures a more natural representation of language as it typically appears in text.",
  "Words sampled from typesWords sampled from tokens": "scintillation, fanon, rubato, upstanding, collard,creeks, skookum, unbelievers, monocyte, nishikawa,crusher,gerwen,abrah,silverchair,hangman,unitary,klausen,arousal,heat,bridgnorth,mildred, porton, aquasox, wylie, hipaa, krimuk,hexahedron,kuei,barbera,dalvi,gilding,visakhapatnam, tatsuo, tarascon, bajram, scholes,hadad, incidental, theodosius, reichskommissariat,boeheim, amsl, buencamino, thrasyvoulos, insulated,discourtesy, nisra, ycko, luen, dooku nine, ranked, zero, the, garcia,rank, station, the, for, four,williams, drunken, a, one, eight,of, were, zero, debate, orchestra,of, wrist, points, fractured, the,to, redirect, adnan, white, car,fond, concluded, under, two, by,five, his, infection, the, the,pop, in, one, in, one, one, fram,handled, battle, mutual",
  "Definition of embedding symmetry": "In mathematical science fields, such as high-dimensional probability theory and the volume ofconvex bodies , there are numerous intriguing definitions of spatial symmetry. Among them, webegin with the definition of the symmetry of random vectors with their frequencies . This issuited for dealing with word vectors because they entail word frequencies, unlike usual data instances.",
  "From these definitions, we will develop methods to adjust given word vectors to be symmetric in 3.2, and to evaluate the symmetry of given word vectors in 3.3": "In machine learning and NLP, the spatial symmetry of embedding spaces is a hot topic, and numeroustheories and algorithms have been proposed . However, the approach in manyresearches implicitly treats all vectors equally, ignoring word frequency information. In the followingsections, we will detail both the empirical and theoretical issues that a uniform approach can cause,especially when applied to NLP tasks. Furthermore, when embeddings correspond to tokens ratherthan typessuch as in the internal representations of masked or causal language modelsa uniformapproach tends to be effective. This point will be discussed in 5.1.",
  "Enhancement of embedding symmetry": "This section proposes Zipfian whitening2, which symmetrizes a given set of word vectors withword frequency. At a glance, the most natural method to achieve Def. 1 and Def. 2 would be PCAwhitening, also known as sphering. Notably, each step of whiteningcentering, decorrelation, andstandardizationimplicitly involves calculating expected values. Our approach is simple: each timewe calculate an expected value, we should weight it by the empirical word frequency. The specificalgorithm is as shown in Algorithm 1. The only difference from general whitening is that it usesword frequency in the part highlighted in blue . Please refer to Appendix A for a formal explanationshowing that the word vectors obtained by the proposed algorithm actually satisfy Def. 1 and Def. 2. 2In this paper, Zipfian is simply used to denote a highly non-uniform distribution. Our focus is on themismatch between actual word frequencies and uniform distribution, and we have not constructed arguments orexperiments that rely on specific properties of power laws. Refining experiments and theory based on the degreeof tail heaviness is an interesting direction for future work.",
  "+ ABTT 56.98+ SIF + CCR 63.04": "Empirical evaluation: We confirm the effectiveness of Zipfian whitening (Algorithm 1) by measuringperformance on standard sentence-level downstream tasks using post-processed word vectors. Weemployed the most standard word embeddingsGloVe , word2vec , and fastText andutilized the widely adopted evaluation tasks, including STS-B and related benchmarks. Detailedexperimental settings can be found in Appendix B. shows the results on the STS-B task.Remarkably, the proposed Zipfian whitening shows significant advantages not only over standard(uniform) centering and whitening but also over the strong baseline method specifically designedto create powerful sentence vectors. Consistent results were obtained with various benchmark datasets,multiple empirical word probabilities, and a language other than English (Appendix C)3. In 4.2,one reason for this remarkable performance is clarified from the perspective of information geometry.",
  "Degree of centralitythe 1st moment of symmetry: Recall that, if the barycenter E[v] is closeto 0, then the random vector v can be considered symmetric in terms of the first moment (Def. 1)": "3Notably, we observed improved scores when using word frequencies from the evaluation dataset itself asp(w). In general, for NLP tasks, p(w) refers to word frequencies derived from the embedding training dataor from a standard large corpus. However, to optimize downstream task performance, it is preferable to basep(w) on word frequencies within the evaluation dataset itself used for those tasks. This adjustment exemplifiescovariate shift in machine learning, where the distribution of training data differs from that of test data. Thue, examining the value of E[v] := E[v] 0 appears to be a reasonable way to measure thesymmetry of the first moment. However, random vectors v and v ( R>0) should be consideredequivalent in terms of spatial symmetry. Thus, we define the scale-invariant metric (Def. 3), obtainedby dividing E[v] by the average length E[v].",
  "By definition, Sym1(v) takes values in , and Sym1(v) = 1 if and only if v is zero mean": "Degree of isotropythe 2nd moment of symmetry: If the covariance matrix E[(v E[v])(v E[v])] is a constant multiple of the identity matrix Id, i.e., if the random vector v has an equalspread in all directions, v is symmetric in terms of the second moment (Def. 2). Following convention,this degree can be confirmed by examining the flatness of the eigenspectrum.",
  "Proposition 1. Sym2(v) takes values in , and Sym2(v) = 1 if and only if v is isotropic aroundits barycenter (Def. 2).Proof. Please refer to Appendix D": "Note that the approach of measuring the entropy of the spectrum to evaluate the flatness of a signalcan be found in many fields. For example, similar definitions are seen in probability processes and signal processing . We also follow this standard and powerful line. Algorithm: To compute the evaluation metrics of symmetry (Def. 3, Def. 4) for given word vectors,again, one should just use the empirical word frequency when calculating the expectations. Apseudocode for measuring symmetry is provided in Appendix E. Empirical evaluation: To what extent does our symmetry score (an intrinsic evaluation of embeddingspaces) correlate with downstream task performance (an extrinsic evaluation of those)? As baselines,we use versions of our symmetry score that do not account for word frequency, calculated in a uniformmanner. We also compare with popular symmetry scores in NLP, the average of cosine similarity(Ave. Cos.) and the recently proposed IsoScore . Note that all these baselines implicitlyassume uniform word frequency. Additional experimental settings can be found in Appendix B. shows the results. The right side of demonstrates the superiority of the Zipfian approach.Moving from the bottom-left to the top-right of the figurei.e. as both the 1st (x-axis) and 2ndmoments (y-axis) of the symmetry score increaseit is clearly visible that the downstream taskperformance increases (the color becomes more red). In contrast, in the left-hand plot, which assumesuniform word frequency, there is no observed relationship between the symmetry score (x and y-axis)and the downstream task performance (color). lists the correlation coefficients between thesymmetry scores and downstream task performance in more detail. It can be seen that the symmetryscores considering word frequency can predict downstream task performance with remarkablyhigh correlation. On the other hand, the prediction performance of other metrics, including Ave.Cos. and IsoScore that implicitly assume uniform word frequency, is unsatisfactory. Surprisingly,when the most popular Ave. Cos. metric shows almost no correlation (0.04) with downstream taskperformance (STS-B), Zipfian symmetry metric has a strong positive correlation (0.83) with it.",
  "Why is Zipfian whitening better than uniform whitening?": "A natural question is why the Zipfian approach empirically dramatically outperforms the uniformapproach. We provide a theoretical explanation using . In a nutshell, a significant differencearises depending on whether the base measure of an exponential family is uniform or Zipfian. : The relationship between the 1st-order symmetry (Def. 3, x-axis), the 2nd-order symmetry(Def. 4, y-axis), and task performance (color). Each point represents either pre-trained or post-processed word embeddings (GloVe, word2Vec, and fastText). The Zipfian measure well captures thedownstream task performance (right), while the uniform isotropic measure cannot (left). : Spearmans 100 (each cell) between the symmetry scores (each column) and downstreamSTS-B performance (each row), on pre-trained and post-processed embeddings (GloVe, word2Vec,and fastText). The scores based on the Zipfian prior show a significantly higher correlation with taskperformance compared to those based on the uniform prior including Ave. Cos. and IsoScore.",
  "Characterization through generative model, partition function, and whitening": "Exponential families: Hereafter, we interpret two salient generative models from the viewpoint ofexponential families: one given by Arora et al. and the other generalizing the LevyGoldbergformula [32, Eq. (7)]. Details of these models will be provided shortly. An exponential family is aclass of probability distributions of a random variable x parametrized by a parameter , written inthe following (canonical) form:",
  ", (18)": "where x is a sufficient statistic, is called a natural parameter, is the base measure (or prior), and is the log-partition function. Once we specify the base measure and the canonical pair (x, ),the log-partition function is determined. That being said, the base measure is the design choice ofan exponential family left for us. In the following, we specifically examine an exponential family ofdistributions in the form p(w | c), where word w is predicted given context c. Specifically, the contextrepresents a co-occurring word (in static word embeddings), a cloze sentence (in masked languagemodels), or a sentence prefix (in causal language models). In all of these cases, we predict a wordwith the logit w, c, making the exponential family a natural probabilistic model. Here, the vector crepresents the vector expression of the context c, known as the context vector. Note that, even forthe same word t, the predicted word vector w(t) and the predicting context vector c(t) are distinct. Uniform prior: Arora et al. firstly considered a log-linear generative model of word embeddingsgiven a context (6) and demonstrated that when the generative model is adopted with normalizedcontext vectors and a huge vocabulary, the partition function asymptotically becomes constant (8) [6,Lemma 2.1]. Here, we can regard that this model belongs to the exponential family with the uniformbase measure (w) = (c) = 1/|V| 4. 4Although Arora et al. s generative model treats a context vector c as a model parameter drifting by arandom walk, we can cast their model into an exponential family because they did not specify how the initialcontext vector is generated. Hence, by regarding c as an observed token with the uniform prior (c) = 1/|V|,",
  "wVPc|w[w arg maxww, c](17)": "Zipfian prior: An exponential family adopted with the Zipfian measure can be written as (7).This generative model can be naturally derived from the skip-gram model with negative sampling(SGNS) . By assuming that the linear model c w, c is sufficiently capable of discriminatingcooccurring words and negative samples (as in the realizable case), we can see that the generativemodel of the word embeddings must comply with the following formula:",
  "p(w)p(c) log k = w, c,(19)": "where k is the number of negative samples. This optimality formula owes to Levy and Goldberg ,and we call (19) the LevyGoldberg formula. A more concise derivation is later given by Oyamaet al. . We can regard the LevyGoldberg formula as an exponential family with the Zipfianbase measure, (w) = p(w) and (c) = p(c), and the constant log-partition function Z z (c) k1.The generative model (7) is a relaxation of the LevyGoldberg formula since we do not impose therealizability assumption necessary for the derivation of (19).",
  "their model is reduced to (6). The static context prior does not contradict Arora et al. s model with sufficientlylarge d, where the random walk drifts extremely slowly": "What does whitening do? Mu and Viswanath proposed a method to approximately make thepartition function of the uniform prior model constant by centering the word vectors and removingthe top principal components (10). Our Zipfian whitening corresponds to Mu and Viswanathspost-processing method, in the sense that ours and theirs make the partition function constant up tothe second moment (11) and (10), respectively. In summary, Zipfian whitening (11) transforms aprobabilistic model into an exponential family adopted with the Zipfian base measure (7), making itcloser to the LevyGoldberg formula (19).",
  "Emphasis on rare words by Zipfian prior": "Let us explore further why the Zipfian prior results in good performance in downstream tasks ( 3.2).In summary, the Zipfian prior approach emphasizes low-frequency words, while the uniform priorapproach emphasizes high-frequency words, both from perspectives of vector norms and errors/losses.So far in this paper, we have repeatedly discussed weighting each word according to frequency,so it may seem contradictory that Zipfian approach emphasizes low-frequency words as a result.To illustrate, let us reconsider centering. In centering, the mean vector is subtracted from eachvector. Weighting each word vector by frequency when constructing the mean vector means thatsignals corresponding to high-frequency words are removed more substantially from each vector. Theemphasis on low-frequency words has been repeatedly supported throughout the history of NLP andinformation retrieval, such as Luhns hypothesis , inverse document frequency (IDF) , andsmooth inverse frequency (SIF) . For instance, it is reasonable to emphasize the word isotropywhen creating a sentence embedding containing both words the and isotropy.",
  "From the perspective of vector norm": "Under the Zipfian prior model, words with larger information content have longer (emphasized)vector representations. Conversely, under the uniform prior model, words with smaller informationcontent have longer (emphasized) vector representations. As a representative example of uniform prior models, the norms of word vectors learned by randomwalk language models are theoretically and empirically proportional to word frequency (12) (seeEq. (2.4) and in Arora et al. ). That is, in such embedding space, words with less information(e.g., the) are emphasized. This tendency is consistently observed in dynamic language modelsand causal language models that adopt the softmax cross-entropy loss, another typical example ofthe uniform prior family . By contrast, when training word embeddings with skip-gram negativesampling , the word embeddings follow the Zipfian prior family, and their norms become largerwith greater information, which we show subsequently . Based on the formulation ofthe exponential family and following Eq. (12) of Oyama et al. , we formally describe the normproperties of the word vectors obtained from the Zipfian prior model. Theorem 1 (The norm of a word vector learned with empirical Zipfian prior models reflect theinformation amount of the word; a refined version of Eq. (12)). Assume that word embeddings{wi}i follow the Zipfian prior model (7), For the same word t, the vector w on the predicted side andthe vector c on the predicting side are shared: w(t) c(t) (weight tying), and",
  "w2": ": Relationships between the information content log p(w) and the vector norms w2 fortop 500 frequent words w. The figure in the center represents the pre-trained GloVe model. By usingZipfian whitening, the information content gets encoded in the norm (center to right). Conversely,with uniform whitening, this phenomenon does not occur (center to left).",
  "From the perspective of error and loss": "The error and loss functions associated with the Zipfian prior model emphasize low-frequency words.In contrast, the error and loss functions of the uniform prior model focus on the average loss acrossthe entire dataset, resulting in a greater emphasis on high-frequency words. The standard classification loss is the softmax cross-entropy loss (14). By taking its expectationover the dataset {(w, c)}, embeddings associated with higher-frequency words receive more updatesbecause the softmax is the uniform inverse link, corresponding to the uniform prior model. Bycontrast, the logit-adjusted loss (15) has been proposed to tackle class imbalance . From ourviewpoint, the logit adjustment term p(w) makes the inverse link belong to the Zipfian prior model.The softmax and logit-adjusted losses are Fisher consistent to the misclassification (16) and balanced(17) error rates, respectively. As the latter tends to stress minor classes, the logit-adjusted loss andZipfian prior model are suitable for emphasizing low-frequency words during the learning process. Another prominent loss function for representation learning is contrastive loss, with the SGNS loss(word2vec) as a representative example in the context of word representation learning. This losssimilarly uses a loss aligned with the Zipfian prior:",
  ",(21)": "where is sigmoid function, and k is the number of negative samples. Since high-frequency wordsare more likely to be sampled as negative examples, the loss has less impact on high-frequencywords in positive examples. Consequently, low-frequency positive words are relatively emphasized inrepresentation learning. The LevyGoldberg formula in the previous section describes the propertiesof an ideally trained word2vec model, which are essentially the properties of Zipfian prior models.",
  "Uniform whitening of token embeddings Zipfian whitening of type embeddings": "Masked language models like BERT and RoBERTa produce dynamic (contextualized)token embeddings. Adding up such token embeddings of constituent tokens to create sentenceembeddings often leads to poor empirical performance . However, symmetrizing significantlyimproves their performance; such methods including batch centering, WhiteningBERT, andcontrastive learning methods . This improvement can also be explainedfrom the perspective of the Zipfian prior. A dataset or corpus is first fed into the model to obtaintoken embeddings7. Centering/whitening is then applied to this entire set of embeddings. As thistoken embedding (multi)set has the multiplicity asymptotically proportional to the word frequency,",
  "ws w can be ignored without major issuesin formal discussions of spatial symmetry. This is because the words in a sentence are generated based on word": ": The empirical performance difference between uniformenforced centering and whiteningwith a uniform prior for dynamic embeddings, and Zipfianconventional uniform centering andwhitening over tokens with an implicit Zipfian prior over types. Each cell shows the STS-B score 100. This comparison reveals that token-level uniform centering/whitening, corresponding totype-level Zipfian centering/whitening, leads to empirically better performance.",
  "UniformZipfian+ Centering60.3461.30+ Whitening61.3165.59": "this uniform centering/whitening of token embeddings corresponds to the word-frequency-weighted(Zipfian) centering/whitening of type embeddings. For a more formal description of the aboveexplanations, please refer to Appendix H. Additionally, recent work has found that contrastiveadditive sentence encoders implicitly weight words by their information content . This finding isconsistent with the previous discussion on vector norms ( 4.2), and can be seen as indirect evidencesupporting the idea that these models belong to the Zipfian prior family. This idea can also be supported by empirical evidence. This idea is also supported by empiricalevidence. To establish a baseline for centering and whitening token embeddings under a uniformprior, we scale each embedding by the reciprocal of its type frequency, ensuring uniform treatmentacross types. Refer to the Appendix H for the detailed computation of this pseudo uniform approachand a formal explanation of how it achieves type uniformity. shows the results. Comparing thepseudo-uniform centering/whitening (which assumes a uniform prior over types) with the conventionaltoken-level uniform centering/whitening (which implicitly assumes a Zipfian prior over types) revealsthat the latter approach based on a Zipfian prior empirically achieves better performance. Additionalexperimental settings and results can be found in Appendix B and Appendix I.",
  "Headless causal language model roughly belongs to Zipfian prior family": "The recently proposed headless language model uses only words within the same batch topredict next tokens with a pseudo-softmax function. This method originally aimed to reduce thecomputational cost of the softmax function in the |V| direction, but an interesting side effect is theimprovement in the performance. This success can also be explained from the perspective of Zipfianpriors. If we repeatedly sample small batches, the sampling frequency of each word will increasinglyreflect its true frequency as the batch size approaches 1.",
  "Conclusion": "Standard methods for adjusting and measuring symmetries in word embedding spacessuch as cen-tering and whiteningimplicitly assume uniformly distributed word frequencies, which is unrealistic.We hypothesize that, based on the type-token distinction, using empirical Zipfian word frequenciesis essential when calculating the expectation ( 2). Based on the idea and the definitions of first-and second-order symmetry in random vectors, we derived Zipfian whitening, which enhances thesymmetry of the word embedding space. Even though it is nearly identical to standard PCA whitening,Zipfian whitening significantly outperforms existing methods ( 3.2). Similarly, we derived a metricto evaluate the symmetry of word embedding spaces. Our intrinsic metrics showed a strong correlationwith extrinsic task performance, even when popular metrics show almost none ( 3.3). We thenpresented a framework explaining the differences in effect between whitening based on uniform andZipfian approaches, by attributing them to differences in the base measure of the exponential family( 4.1). By further exploring this viewpoint through information geometry and loss functions, weshowed how the Zipfian approach emphasizes the informativeness of low-frequency words ( 4.2).Lastly, through our proposed viewpoint, we found that popular NLP methods perform well becausetheir word embeddings end up encoding a Zipfian prior; such models include word2vec (.2),WhiteningBERT ( 5.1), and headless language models ( 5.2).",
  "Acknowledgements": "This work is supported by JST ACT-X Grant Number JPMJAX200S and JSPS KAKENHI GrantNumber 22H05106. We received numerous constructive and valuable comments from the anonymousreviewers of NeurIPS 2024, which have significantly contributed to the quality improvements fromthe submission version to the camera-ready version. We would also like to thank Hayato Tsukagoshiof Nagoya University for his insightful comments on the handling of dynamic embeddings andon the experimental setup of the SimCSE paper , including minor discrepancies between thepapers description and its actual implementation. We also extend our gratitude to the organizersand participants of MLSS 2024, the Tohoku NLP group, the Shimodaira lab at Kyoto University,and many members of the Japanese NLP and machine learning community, for their constructivefeedback and motivating encouragement throughout our research discussions.",
  "How these assumptions might be violated in practice": "In our theoretical analysis concerning norms, and in the discussion on the relationship betweenwhitening and normalization constants, we have proceeded by ignoring the residual terms beyond thesecond order. Empirically, focusing only on the first and second order has yielded significant results.However, to accurately identify cases where the proposed method might fail, a detailed theoreticaland empirical examination of the asymptotic behavior of higher-order moments might be crucial.This remains an important future work. The condition that the partition function is constant is only a necessary condition from the perspectiveof both the generative models optimal solution and whitening. The true logical relationship betweenwhitening and the generative model has not been clarified. In particular, verifying whether theprojection through whitening allows us to transition between the two model families (the uniformfamily and the Zipfian family) is an intriguing and valuable direction for both theoretical explorationand practical application.",
  "The scope of the empirical claims made": "Our experiments primarily focused on static and dynamic word embeddings, as many of theirtheoretical properties have been understood and they have been central to the rise of isotropization.Admittedly, this paper also advances our understanding of causal language models. However, tomake a more significant practical impact in the era of large language models, employing the proposedmethod as a regularization term for next-token prediction holds great promise for future work. The experiments utilized typical downstream NLP tasks, particularly popular datasets for sentence-level semantic tasks. By scaling up the task set to include word-level tasks or leveraging a broaderrange of multilingual data, we can more robustly demonstrate the practical utility of the proposedframework.",
  "The factors that influence the performance of our approach": "The proposed method inherently involves numerically unstable calculations, such as multiplyingby the inverse of small singular values. Embeddings for low-frequency words are often far fromconverged even after extensive pre-training, and the eigenvalues of the embedding space are known todecay. Given these situations, the adverse effects of small singular values are plausible. Consideringrecent advancements in whitening techniques, developing a more numerically stable algorithm is animportant direction for future work.",
  "culturally unique expressions. Our Zipfian whitening and similar regularization methods couldenhance output diversity, enriching the linguistic landscape": "Potential negative societal impacts The sentence similarity tasks used in our evaluation experimentsare now considered core technologies for RAG (retrieval-augmented generation), which is essentialwhen large language models leverage external resources. If chatbots generate responses tailored touser ideologies or preferred information sources, it may result in negative societal impacts, includingpolitical agitation. E. Agirre, D. Cer, M. Diab, and A. Gonzalez-Agirre. SemEval-2012 Task 6: A Pilot on Semantic TextualSimilarity. In E. Agirre, J. Bos, M. Diab, S. Manandhar, Y. Marton, and D. Yuret, editors, *SEM 2012:The First Joint Conference on Lexical and Computational Semantics Volume 1: Proceedings of themain conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop onSemantic Evaluation (SemEval 2012), pages 385393, Montral, Canada, 7-8 June 2012. Association forComputational Linguistics. URL E. Agirre, D. Cer, M. Diab, A. Gonzalez-Agirre, and W. Guo. *SEM 2013 shared task: SemanticTextual Similarity. In M. Diab, T. Baldwin, and M. Baroni, editors, Second Joint Conference on Lexicaland Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the SharedTask: Semantic Textual Similarity, pages 3243, Atlanta, Georgia, USA, June 2013. Association forComputational Linguistics. URL E. Agirre, C. Banea, C. Cardie, D. Cer, M. Diab, A. Gonzalez-Agirre, W. Guo, R. Mihalcea, G. Rigau, andJ. Wiebe. SemEval-2014 Task 10: Multilingual Semantic Textual Similarity. In P. Nakov and T. Zesch,editors, Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages8191, Dublin, Ireland, Aug. 2014. Association for Computational Linguistics. doi: 10.3115/v1/S14-2010.URL E. Agirre, C. Banea, C. Cardie, D. Cer, M. Diab, A. Gonzalez-Agirre, W. Guo, I. Lopez-Gazpio, M. Mar-itxalar, R. Mihalcea, G. Rigau, L. Uria, and J. Wiebe. SemEval-2015 Task 2: Semantic Textual Similarity,English, Spanish and Pilot on Interpretability. In P. Nakov, T. Zesch, D. Cer, and D. Jurgens, editors,Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 252263,Denver, Colorado, June 2015. Association for Computational Linguistics. doi: 10.18653/v1/S15-2045.URL E. Agirre, C. Banea, D. Cer, M. Diab, A. Gonzalez-Agirre, R. Mihalcea, G. Rigau, and J. Wiebe. SemEval-2016 Task 1: Semantic Textual Similarity, Monolingual and Cross-Lingual Evaluation. In S. Bethard,M. Carpuat, D. Cer, D. Jurgens, P. Nakov, and T. Zesch, editors, Proceedings of the 10th InternationalWorkshop on Semantic Evaluation (SemEval-2016), pages 497511, San Diego, California, June 2016.Association for Computational Linguistics. doi: 10.18653/v1/S16-1081. URL",
  "P. Bojanowski, E. Grave, A. Joulin, and T. Mikolov. Enriching Word Vectors with Subword Information.TACL, 5:135146, 2017. doi: 10.1162/tacl_a_00051. URL": "T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry,A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. Ziegler,J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner,S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language Models are Few-Shot Learners.In NeurIPS, volume 33, pages 18771901, 2020. URL",
  "L. L. Campbell. Minimum coefficient rate for stationary random processes. Information and Control, 3(4):360371, Dec. 1960": "D. Cer, M. Diab, E. Agirre, I. Lopez-Gazpio, and L. Specia. SemEval-2017 Task 1: Semantic TextualSimilarity Multilingual and Crosslingual Focused Evaluation. In SemEval, pages 114, 8 2017. doi:10.18653/v1/S17-2001. URL X. Chen, N. Ding, T. Levinboim, and R. Soricut. Improving Text Generation Evaluation with BatchCentering and Tempered Word Mover Distance. In First Workshop on Evaluation and Comparison of NLPSystems, pages 5159, Online, 11 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.eval4nlp-1.6. URL",
  "P. He, X. Liu, J. Gao, and W. Chen. {DEBERTA}: {DECODING}-{enhanced} {bert} {with} {disentan-gled} {attention}. In ICLR, 2021. URL": "J. Huang, D. Tang, W. Zhong, S. Lu, L. Shou, M. Gong, D. Jiang, and N. Duan. WhiteningBERT: Aneasy unsupervised sentence embedding approach. In M.-F. Moens, X. Huang, L. Specia, and S. W.-T. Yih,editors, Findings of the Association for Computational Linguistics: EMNLP 2021, pages 238244, PuntaCana, Dominican Republic, Nov. 2021. Association for Computational Linguistics.",
  "G. Kobayashi, T. Kuribayashi, S. Yokoi, and K. Inui. Transformer language models handle word frequencyin prediction head. pages 45234535, July 2023": "K. Kurihara, D. Kawahara, and T. Shibata. JGLUE: Japanese general language understanding evaluation.In N. Calzolari, F. Bchet, P. Blache, K. Choukri, C. Cieri, T. Declerck, S. Goggi, H. Isahara, B. Maegaard,J. Mariani, H. Mazo, J. Odijk, and S. Piperidis, editors, Proceedings of the Thirteenth Language Re-sources and Evaluation Conference, pages 29572966, Marseille, France, June 2022. European LanguageResources Association. URL H. Kurita, G. Kobayashi, S. Yokoi, and K. Inui. Contrastive learning-based sentence encoders implicitlyweight informative words. In H. Bouamor, J. Pino, and K. Bali, editors, Findings of the Association forComputational Linguistics: EMNLP 2023, pages 1093210947, Singapore, Dec. 2023. Association forComputational Linguistics. Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, EytanRuppin. Placing search in context: the concept revisited. ACM Trans. Inf. Syst. Secur., 20(1):116131, Jan.2002.",
  "A. K. Menon, S. Jayasumana, A. S. Rawat, H. Jain, A. Veit, and S. Kumar. Long-tail learning via logitadjustment. Sept. 2020": "T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean. Distributed Representations of Words andPhrases and their Compositionality. In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, andK. Q. Weinberger, editors, NIPS, pages 31113119, 2013. URL D. Mimno and L. Thompson. The strange geometry of skip-gram with negative sampling. In Proceedingsof the 2017 Conference on Empirical Methods in Natural Language Processing, pages 28732878,Copenhagen, Denmark, Sept. 2017. Association for Computational Linguistics.",
  "L. Wetzel. Types and tokens. The Stanford Encyclopedia of Philosophy (Fall 2018 Edition), Apr. 2006": "Y. Yan, R. Li, S. Wang, F. Zhang, W. Wu, and W. Xu. ConSERT: A contrastive framework for self-supervised sentence representation transfer. In C. Zong, F. Xia, W. Li, and R. Navigli, editors, ACL-IJCNLP,pages 50655075, Online, 8 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.393. URL S. Yokoi, R. Takahashi, R. Akama, J. Suzuki, and K. Inui. Word Rotators Distance. In EMNLP, pages 29442960, Online, 11 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.236.URL",
  "w := E[w w]1/2w.(22)": "Actually, E[ w w] = Id holds; w satisfies Def. 2. Computationally, the estimation of E[w w]1/2can be performed efficiently via singular value decomposition (SVD) of the centered data matrixW := [w1 , . . . , wn ]. Note again that, this standard method assumes that the frequency of eachword (i.e., each row) is uniform, which presents the issues discussed in 2. To account for wordfrequency, SVD should be performed on the matrix",
  "B.2Empirical word frequency and vocabulary": "As the empirical word probability p(w) of English words, we used the enwiki dataset preprocessedby Arora et al. 17. For the Japanese word probability, we used Japanese Wikipedia word frequencyfrom Wiktionary, denoted as jawiki 18. Furthermore, we also used the frequency of words in theevaluation data itself (test set probability). The word frequency in the test set is implicitly utilizedin s sentence embedding method and is also a natural approach in the context of covariateshift .",
  "B.3Baseline methods": "As baselines for post-processing of word vectors, we used ABTT (all-but-the-top) , whichestablished the trend of post-processing word vectors; and the strong baseline method by , the com-bination of SIF (smoothed inverse frequency) weighting and CCR (common component removal)19.We followed the hyperparameter choices of the original papers, with the dimensionality reductionparameter for ABTT set to D := 3, and the weighting parameter for SIF set to a := 103.",
  "B.4Extrinsic tasks": "As downstream tasks, we used the most commonly utilized ones in the community, STS12-16 , STS-B and SICK-R . For the multilingual experiments, we used JSTS (Japaneseversion of the STS) from JGLUE benchmark . They are sentence-level similarity tasks and arestandard for empirically evaluating the performance of word vectors20 21. These datasets consist ofpairs of sentences and their semantic similarity rated by annotators. We first tokenized the dataset 15Note that, we apply centering/whitening operations to such token embeddings, not to the final sentenceembeddings, in order to match the setting in the theoretical analysis and the static word embedding experiments.16Though we followed the experimental setting from the prior work , there is a slight discrepancy in theexperimental results of the baseline setting. We found that this was due to prior work inadvertently taking theaverage of the hidden states of the zero-th layer (i.e., static word embedding layer) and the final dynamic layer.See the discussion at for more details.17 19CCR is a process applied to sentence vectors, but due to its linearity, it can be adapted to word vectors. Formore details, please refer to Yokoi et al. .20For those outside the field of NLP research or practice, the question, Why not run word-level evaluationmetrics? is a natural and valid one. Our language has a property known as compositionality, allowing infinitesemantic content to be conveyed through a finite vocabulary as building blocks. This principle underlies modelslike word2vec , BERT , and the GPT series , where the fundamental unit of representation is theword; these models are then applied to solve tasks with larger components, such as sentences. Our researchadheres to this foundational principle of NLP. Also, existing word-level similarity datasets have significantissues that make them less suitable for our work (see Bakarov [8, .1.1]). Given that whitening reflectsword information content in vector norms, classic tasks like keyword extractionwhich selects words with highinformation contentcould be good candidates; results from a prior study using methods similar to ours wouldalso be informative [42, .1].21Setting aside the criticisms from previous studies for now, we conducted an evaluation using the twomost well-known lexical similarity datasets. shows the results. We found that the process of raw Zipfian centering Zipfian whitening consistently improves lexical properties. However, note that the findingdirection: uniform whitening > direction: Zipfian whitening contradicts the experimental results in Appendix G,which showed direction: uniform whitening, norm: Zipfian whitening < direction: Zipfian whitening, norm:Zipfian whitening. Here, lexical similarity tasks rely solely on vector direction and do not reference vectornorms, as only the cosine similarity between word vectors is used to predict similarity. This discrepancy likelyarises because these datasets are not representative of natural language, as discussed in Bakarov [8, .1.1]. For example, the widely used dataset WordSim353 includes only about 200 subjective ratings on by NLTK with some post-processing following 22, then lowercased all tokens. The typicalexperimental protocol we followed is to sum the word vectors to form a sentence vector and thencheck if the angles (cosine similarity) between them correlate well with the gold scores. We reportedSpearmans rank correlation between the predictions (cosine scores) and human-annotated goldscores23.",
  "B.5Computational resources for experiments": "We conducted all experiments using a single NVIDIA RTX 6000 Ada GPU with 48GB VRAM. EachSTS task required 10 seconds per model and whitening method, totaling approximately 10 minutesfor the entire experiment, excluding the embedding loading time to the GPU. For the calculation of the symmetry scores, each setting took one minute, resulting in a total of 5minutes, again excluding the embedding loading time and the average cosine similarity (Ave. Cos.)setting. The Ave. Cos. score computation took 10 minutes per model, totaling 20 minutes for the twomodels.",
  "CExperimental results on all benchmark datasets to evaluate the effects ofZipfian whitening": "In 3.2, we evaluated the empirical performance of Zipfian whitening on the STS-B dataset. In thissection, we present experimental results using more comprehensive datasets. Detailed experimentalsettings can be found in Appendix B. , and show the results. Across alldatasets, the method incorporating a Zipfian prior consistently outperforms the method employing auniform prior.",
  "FProof of Thm. 1": "Proof. By the assumption, the word and context vectors for the same word t V, w(t) and c(t),are obtained through the linear embedding layer with weight tying, namely, w(t) = U1t andc(t) = U1t, where U Rd|V| is the embedding matrix and 1t R|V| is the one-hot vectorindicating the token t. To derive the KL divergence for the model p(c | w), we need to begin with thegenerative model p(w | c) (7) and confirm that p(c | w) belongs to an exponential family.",
  "cVp(c | w)1c1c = diag[. . . p(c | w) . . . ],": "which is the |V| |V| diagonal matrix with p(c | w) being the (c, c)-th diagonal entry. Now, weare ready to derive the KL divergence. For two tokens w, w |V|, if we write := U1w and := U1w, the KL divergence of the exponential family can be expanded as the following quadraticform in their parameters and :",
  "GExperiments with a mix of uniform and Zipfian settings": "Based on the findings that Zipfian whitening positively impacts word vector norms ( 4.2), we presentexperimental results for a baseline: first, uniform whitening is applied, followed by rescaling normsaccording to information content through Zipfian whitening. presents the results, with the basic settings identical to those in , but uses Pearsonsr as the evaluation metric. Here, Uniform + refers to the process of correcting word vectorsusing a uniform prior, then replacing only the norm with that obtained from Zipfian whitening. Wefound that appropriately weighting by norm has a critical effect on task performance. Notably, pureZipfian centering/whitening performs even better, suggesting that Zipfian correction has two effects:(i) the norm becomes representative of information content ( 4.2), and (ii) vectors are more evenlydispersed (isotropic), resulting in appropriate positioning in terms of direction as well.",
  "H.2Pseudo-uniform whitening of token embeddings uniform whitening of typeembeddings": "To establish a baseline for centering/whitening token embeddings under uniform prior, we can applya coefficient 1/|VD| 1/cD(type(t)) to each token embedding t, for removing type frequencies that areimplicitly referenced. Here, VD denotes the vocabulary contained in D: VD := {w V : t D, type(t) = w}. The pseudo-uniform average E u [t] calculated in this way is asymptoticallyequivalent to the uniform average of type embeddings E u [w], under the previous assumption(Assump. 1) that ignores the dynamic nature of token embeddings:",
  "IExperimental results on all benchmark datasets to evaluate the effects ofuniform whitening on token embeddings": "In 5.1, we evaluated the empirical performance of uniform whitening of dynamic token embeddingson the STS-B dataset. In this section, we present experimental results using more comprehensivedatasets. shows the results. Across all datasets, the methods implicitly incorporating aZipfian prior consistently outperforms the method employing a uniform prior. : Full results of the empirical performance of Zipfian whitening. Each cell shows the STSscore 100. As empirical word frequency p(w), we used enwiki. Across all models and tasks,Zipfian whitening outperforms powerful baseline methods.",
  "+ Centering49.2143.1349.8962.0349.7054.5646.9150.78Uniform+ Whitening45.1241.0047.3062.0848.8554.8043.5548.96": "+ Centering48.6855.0354.0760.2358.4154.6450.3854.49Zipfian+ Whitening61.2260.6863.1873.5969.8759.8268.2065.22ABTT49.6441.7948.8160.8447.5755.0944.2349.71SIF + CCR57.2854.5060.7768.8261.6356.8360.3660.03 : Full results of the empirical performance of Zipfian whitening, test set frequency setting.Each cell shows the STS score 100. As empirical word frequency p(w), we used test set frequency.Across all models and tasks, Zipfian whitening outperforms powerful baseline methods. Besides, thetest set frequency setting consistently outperforms the enwiki setting in , demonstrating thatthe models benefit from using task-specific statistics in line with a covariate shift approach ."
}