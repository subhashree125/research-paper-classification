{
  "Abstract": "Multimodal Large Language Models (MLLMs) have demonstrated remarkablecomprehension and reasoning capabilities with complex language and visual data.These advances have spurred the vision of establishing a generalist robotic MLLMproficient in understanding complex human instructions and accomplishing variousembodied tasks. However, developing MLLMs for real-world robots is challengingdue to the typically limited computation and memory capacities available on roboticplatforms. In contrast, the inference of MLLMs involves storing billions of pa-rameters and performing tremendous computation, imposing significant hardwaredemands. In our paper, we seek to address this challenge by leveraging an intrigu-ing observation: relatively easier situations make up the bulk of the procedure ofcontrolling robots to fulfill diverse tasks, and they generally require far smaller mod-els to obtain the correct robotic actions. Motivated by this observation, we proposea Dynamic Early-Exit Framework for Robotic Vision-Language-Action Model(DeeR-VLA, or simply DeeR) that automatically adjusts the size of the activatedMLLM based on each situation at hand. The approach leverages a multi-exit archi-tecture in MLLMs, which allows the model to terminate processing once a propersize of the model has been activated for a specific situation, thus avoiding furtherredundant computation. Additionally, we develop novel algorithms that establishearly-termination criteria for DeeR, conditioned on predefined demands such as av-erage computational cost (i.e., power consumption), as well as peak computationalconsumption (i.e., latency) and GPU memory usage. These enhancements ensurethat DeeR operates efficiently under varying resource constraints while maintainingcompetitive performance. Moreover, we design a tailored training method forintegrating temporal information on top of such multi-exit architectures to pre-dict actions reasonably. On the CALVIN robot manipulation benchmark, DeeRdemonstrates significant reductions in computational costs of LLM by 5.2-6.5xand GPU memory of LLM by 2-6x without compromising performance. Code andcheckpoints are available at",
  "arXiv:2411.02359v1 [cs.RO] 4 Nov 2024": "proficient in interacting with humans and the physical world to flexibly execute complex manipulationtasks . An encouraging preliminary work, RT-2 , has demonstrated the feasibility ofadopting MLLMs to control robots in end-to-end. This not only yields performant robotic policies,but also exhibits some emergent abilities obtained from large models, such as understanding novelcommands, generalizing to objects never seen before, and reasoning. Despite these favorable findings, the high demands of MLLMs on hardware are usually an importantbottleneck that inhibits the establishment of generalist robots with advanced MLLMs. Typically,robotic applications are based on resource-hungry platforms with limited computational capability,memory space, and battery capacity, yet usually necessitate acting in real-time and performinglow-latency interactions with humans or the physical environments. However, every time MLLMsare activated to obtain a robotic action involves utilizing billions of parameters to accomplisha computationally intensive inference process. Such inefficiencies may lead to considerable GPUmemory requirements, tremendous power consumption, as well as nontrivial time delays in controllingrobots. These weaknesses make it challenging to deploy MLLMs on real embodied robotic systems. To alleviate this problem, we propose an approach based on dynamic neural networks. Our workis inspired by an intriguing observation: in the procedure of controlling a robot to fulfill varioustasks, relatively easier circumstances make up the bulk of all the situations confronted by the robot.When encountered with these easier situations, an embodied agent can actually acquire properrobotic actions with a much smaller model compared to the full MLLMs. Or more precisely, onlythe remaining small number of more difficult circumstances necessitate the full capacity of largeMLLMs. This phenomenon can be demonstrated using the representative example in , wherewe train RoboFlamingo with varying model sizes, and report the FLOPs and task successful ratein the Calvin Long-Horizon Multi-Task Language Control (LH-MTLC) challenge . Adopting theofficially recommended 24-layer Flamingo only correctly finishes 3.2% (78.9% v.s. 75.7%) moretasks compared to using 6 layers, but it increases the computational cost by 4x. In other words,computational resources are wasted on activating larger models in many easy circumstances for whichsmaller models are sufficient. : Computation cost v.s. task successfulrate1(RoboFlamingo++) on CALVIN LH-MTLCchanllenge DD. Notably, we mainly focus onthe core component, LLM, of the MLLM, whichcomprises the majority of parameters. We varythe size of the LLM to examine its impact. Fora focused comparison, we report the FLOPs (andGPU memory usage) of the LLM in our paper,unless otherwise specified.",
  "GFLOPs/action (LLM)31.215.67.8Task success rate %78.978.075.7": "Motivated by this observation, we propose a Dy-namic Early-Exit for Robotic MLLM (DeeR) frame-work, seeking to automatically configure the size ofMLLMs conditioned on each situation confrontedby an embodied agent. Specifically, we introducea MLLM architecture featuring multiple intermedi-ate exits, with which a correct robotic action canbe immediately obtained once a proper size of themodel has been activated, eliminating further redun-dant computation. Additionally, we develop novelalgorithms that are able to establish early-terminationcriteria for DeeR conditioned on arbitrarily specifieddemands of average computational cost (i.e., powerconsumption), and peak computational cost (i.e., latency) or GPU memory overhead. At inference,DeeR can adaptively activate smaller models for less complex situations and larger models for morechallenging cases. Consequently, computation is unevenly allocated among situations, yielding a con-siderable improvement in efficiency. Besides, the computational cost of DeeR can be adjusted onlineby simply modifying the termination criterion on top of a fixed main model, making it appealing inflexibility. Moreover, we design a tailored training method for DeeR, enabling integrating temporalinformation on top of such multi-exit architectures to control robots reasonably. The performance of DeeR is evaluated on 3 CALVIN LH-MTLC challenges with RoboFlamingo .Extensive robot experiments show that DeeR reduces the LLM computational cost by 5.2-6.5xwithout sacrificing performance. Surprisingly, even when considering GPU memory limitations in thetermination criterion, DeeR remains competitive with other SOTA methods while only utilizing 2GBmemory for the activated LLM. Consequently, DeeR demonstrates the potential to enable a widerrange of users to operate their own robots equipped with MLLMs on resource-limited platforms.",
  "Related Works": "LLM/MLLM for language-conditioned robot control. A range of studies have explored the useof natural language to instruct robots in performing tasks . Methods such asSayCan and PaLM-E utilize LLMs as high-level planners to translate commands intoindividual primitives that are then executed by low-level controllers. However, these controllers areusually domain-specific small models and lack the semantic understanding and reasoning capabilitiesthat LLMs/MLLMs possess. To fully leverage LLMs astonishing capabilities, RT-2 proposes anend-to-end MLLM that directly generates low-level robotic actions via co-finetuning on robotic dataand image-language data. It exhibits some emergent abilities obtained from large MLLMs, such asgeneralizing to instructions and objects never seen before, and reasoning. Further, RoboFlamingo proposes to adapt existing MLLMs to a low-level robotic policy through straightforward fine-tuningon robotics data. While representative projects like RT-2 and RoboFlamingo have showcased thepromising potential in enabling generalist robots, the use of MLLMs for such low-level control iscomputationally intensive. This is because each robotic action requires processing through all layersof an MLLM, whose inefficiencies often yield significant bottlenecks in practical robotic applications. Efficient LLM. Considerable strides have been made to improve the inference efficiency of LLMs . Research in this domain typically falls into three categories: efficient structural design, model compression , and dynamic networks . Our research focuses on the third category, dynamic networks, which optimize computationalresources based on input data to reduce unnecessary computation. A key strategy within this categoryis early exiting, discussed further below. Early exiting is an innovative method for dynamically halting forward propagation at a certain layerbased on intermediate predictions. This technique has been well explored in both computer vision , language processing , and multimodality . Achallenge in implementing early-exiting models is devising an appropriate metric to determine whento issue an intermediate prediction. Traditionally, in tasks such as image classification, metrics suchas Softmax confidence or entropy are utilized. Alternative approaches include traininglearning-based exit strategies with pseudo labels . Recent advancements haveexpanded early exiting to encompass the next-token prediction of LLMs focused on, treating it asa classification task. Diverging from these methods, our work adapts an MLLM to generate actionoutputs for sequential decision-making. We introduce a novel dynamic paradigm that integratestemporal information to predict action. Further, we devise a novel early-exiting metric based on actionconsistency, necessary because typical metrics like confidence and entropy are infeasible withoutdirect Softmax outputs. Lastly, we develop an algorithm to derive termination criteria through onlineenvironmental interactiona strategy not explored in prior early-exiting research in vision or NLP.",
  "Dynamic Early-Exit for Robotic MLLM": "The strong task instruction understanding and visual grounding capabilities of MLLMs haveexhibited great promise for language-instructed multitask robotic manipulation . However,existing works tend to be computationally intensive since the actions of the robot are obtained byinferring all layers of an MLLM. At each timestep, this process may activate billions of parameters,necessitating substantial computation and memory, and yielding a significant latency and powerconsumption. These inefficiencies are usually important bottlenecks for practical robotic applications. Overview. We seek to address this challenge by leveraging an intriguing observation: relativelyeasier situations make up the bulk of the procedure of controlling robots to fulfill various tasks, andthey generally require far smaller models to obtain the correct robotic actions (as shown in ).Inspired by this phenomenon, we propose Dynamic Early-Exit for Robotic MLLM (DeeR), aimingto improve the computational efficiency of the robotic MLLM systems by dynamically adopting aproper size of MLLM for each situation. In specific, we first develop a novel MLLM architecture withmultiple intermediate exits (.1). Consequently, given an input, one can immediately acquirea proper robotic action once a sufficient number of model parameters have been activated, avoidingfurther redundant computation. Then, .2 establishes early-termination criteria for DeeRconditioned on arbitrarily specified demands of average computational cost, and peak computationalcost or GPU memory overhead. Finally, .3 proposes a tailored training algorithm for ourmodel, demonstrating how to integrate temporal information on top of this dynamic network andreasonably predict robotic actions.",
  "History hidden state": ": Left: Dynamic inference of DeeR. For inference, we adaptively activate an appropriate size of MLLMbased on an exit criterion c, which accounts for the current situation (including task instruction l and observationot) and predefined computational and GPU memory budgets. The language instruction and gripper cameraimage, not shown in this figure, are also inputs to the MLLM. An action is then obtained using the intermediatefeature xc(t)tand historical information. Right: Training of DeeR.We randomly sample features from all exitsduring training. This strategy helps minimize the discrepancy between training and dynamic inference. Moreover,we employ several auxiliary action heads (AuxH) to better optimize the MLLM.",
  "We first introduce an MLLM architecture featuringmultiple intermediate exits, enabling the dynamicadaptation of the MLLMs size to suit the varyingsituations encountered by robots": "Basic architecture. Tasked with a language instruc-tion l, a robot receives an observation ot from sen-sors (e.g., RGB image from the camera in our paper)at timestep t and predicts an action at to execute.To correctly predict the action, the robot should notonly sufficiently understand the language instructions,but also extract task-relevant information from theimages . Built upon an existing work , weachieve this by employing a pretrained MLLM, i.e.,Flamingo , to process and integrate both vi-sion and language inputs, thus obtaining fused mul-timodal features for decision-making. Our basic MLLM mainly consists of a vision encoderEI and a LLM. The vision encoder EI comprises aVision Transformer (ViT) paired with a Perceiver Resampler , which encodes an input imageot into a sequence of informative tokens. For multimodal fusion, an LLM is established on topof the visual representations generated by the vision encoder EI. More specifically, we interleavethe self-attention blocks of a pretrained, frozen text-only LLM with newly introduced, learnablecross-attention blocks that cross-attend to the visual tokens. This configuration allows the originalMLLM to function as an effective multimodal feature extractor F, formalized as follows:xt = F(l, EI(ot)),(1)where l denotes the input language instruction tokens with a length L, and the output xt =(xt,1, xt,2, . . . , xt,L) represents the hidden state sequence from the last layer of our MLLM attimestep t. Notably, despite the effectiveness of LLMs in multimodal feature integration, theirreliance on billions of parameters results in substantial computational costs and memory usage. Visual language model with intermediate exits. We dynamically adapt the size of the LLM to thespecific requirements of each situation encountered by a robot by introducing a model with intermedi-ate exits. Specifically, we divide the LLM layers into N consecutive groups, noted as F 1 , F 2 , . . . , F N .Each group F i outputs an intermediate hidden state sequence xit =(xit,1, xit,2, . . . , xit,L). When com-putation terminates at an intermediate exit i, we apply a max-pooling operator P to aggregate the infor-mation across the token dimension, resulting in a compact representation xit =P(xit,1, xit,2, . . . , xit,L) that effectively summarizes the image ot and instruction l. This representation serves as the input forthe subsequent action prediction module. With such a multi-exit MLLM architecture, we can obtain aseries of informative representations x1t, x2t, ..., xNt at varying scales of LLM processing. This allowsus to dynamically select the most suitable LLM size conditioned on the situation complexity withoutactivating parameters beyond the chosen exit point. Our multi-exit MLLM is illustrated in . Predicting robotic actions with an action head. After the LLM processes to an appropriate level, theoutput xit from the i-th intermediate exit is transformed into low-level actions by a lightweight actionhead. In this paper, we consider a 7 DoF end-effector action as a representative example of low-levelactions, where the first six continuous dimensions specify the position and orientation of the end-effector, and the seventh discrete value indicates whether the gripper is open or closed. Notably, giventhat the decision-making environment is typically characterized as a Partially Observable MarkovDecision Process (POMDP) , optimal decisions rely not only on the current observation ot butalso on historical observations. Thus, we employ a sequence model as the action head to aggregateinformation across a history window of size H. Without loss of generality, this paper considers alightweight LSTM as an example. On the top of LSTM are two distinct MLP modules: onededicated to predicting the pose of the end-effector, and the other to predict the discrete gripper status.The lightweight action head computes actions efficiently with minimal computational overhead. Early-terminated inference of robotic actions. Equipped with the action head, we assume that acriterion c is defined to determine the optimal point for the conditional exiting from an appropriatelysized LLM at the current timestep t (we will discuss the details of criteria in .2). The indexof the selected exit, denoted as c(t), ranges from 1 to N. Consequently, we utilize the feature xc(t)tfrom the c(t)-th LLM block to compute the predicted action at for the current timestep as follows:",
  "Adaptive Inference": "This section demonstrates how DeeR efficiently executes robot tasks by adaptively activating a propersize of the MLLM under predefined computation and GPU memory budgets. Specifically, we firstdiscuss the termination criterion utilized by DeeR, designed to activate smaller models for lesscomplex scenarios and larger models for more challenging conditions. Next, we explore our approachto devising an effective resource allocation strategy that addresses limitations in computation andGPU memory. The inference process of DeeR is illustrated in . Termination criterion. As mentioned in related works, many previous works utilize confidence-based criteria for determining when to terminate, typically involving metrics such as the maximumelement or entropy of the SoftMax output . In our case, where the goal is actionprediction and SoftMax output is not readily available, we adopt a different approach by leveraging theconsistency of action predictions from adjacent intermediate features as our criterion. The underlyingintuition is that if the action predictions from two differently sized MLLMs remain consistent, itsuggests that the computational model may have reached saturation, and further processing is unlikelyto yield any further improvements. For a given timestep t, we identify the smallest i within the range[1, 2, ..., N] that satisfies the following action consistency condition as termination exit: (xit, ht1) (xi1t, ht1)2 < i,(3)where we disregard the hidden state outputs of and focus solely on comparing the L2 norm of thedifference in predicted actions against a predefined threshold i. We always adopt infinity as N toensure all samples can exit. For i = 1 , we use the input features to the LLM layer as xi1t. Budgeted task execution. Given the predefined constraints of computation and memory budgets, itcan be challenging to manually set optimal threshold values {1, 2, . . .} to ensure that the roboticMLLM policy achieves peak performance while adhering to budget limitations. In contrast, wepropose to determine these values by formulating an optimization problem. We operate under aBudgeted Task Execution Setting, where DeeR is required to perform a set of tasks T within aspecified total computational budget B > 0. To ensure that each action is delivered within anacceptable waiting time, we impose constraints on peak computation where G > 0. Additionally, welimit GPU memory usage to M > 0 to accommodate scenarios where users may not have accessto high-memory GPUs. Let Scc(T , {1, 2, . . .}) represent the success rate of tasks in T , and let FLOPs(T , {1, 2, . . .}) denote the computational cost for executing these tasks under specifiedconstraints. Furthermore, MFLOPs(T , {1, 2, . . .}) denotes the peak FLOPs across all timesteps,and Mem(T , {1, 2, . . .}) indicates GPU memory used during task execution. We seek the optimalthresholds by addressing the following optimization problem:",
  "FLOPs(T , {1, 2, . . .}) < B,(average FLOPs constraint)MFLOPs(T , {1, 2, . . .}) < G,(peak FLOPs constraint)Mem(T , {1, 2, . . .}) < M.(GPU memory constraint)": "Due to the non-differentiability of the success rate function Scc(, ), we may leverage heuristicalgorithms to solve for the thresholds that maximize success within the computational constraints.We discuss strategies for determining optimal thresholds under two conditions: one where we onlyhave access to a demonstration dataset, and another where real environment interaction is permitted. Solving problem (4) using a demonstration dataset. We denote by 0 < q 1 the probabilitythat a sample reaching an exit point will meet the termination criterion and thus exit at that point.When accessing only a demonstration dataset, we assume q is constant across all layers . Thissuggests that the proportion of samples exiting at exit i can be represented as qi = zqi, where z isa normalizing constant ensuring that ni=1 qi = 1. Here, n N denotes the maximum allowableexit index where the corresponding activated LLM meets the constraints of peak GFLOPs and GPUmemory. The proportions of samples at the exits whose index are greater than n is set to zero. Attesting time, we must adhere to the computational budget constraint:",
  "i=1 qiCi B,(5)": "where |T | is the number of tasks to perform, L represents the average length of tasks as derived fromthe dataset statistics and Ci is the computational cost when the LLM inference terminates at i-th exit.Equation (5) allows us to solve for q and determine qi. Using these target proportions qi for each exit,we then determine the threshold values i on the dataset to ensure that approximately qi proportionof timesteps exits at the i-th exit . Solving with online interactions. If the interaction with a real environment is feasible, we can utilizeonline learning algorithms that iteratively adjust thresholds based on feedback regarding successrates. To solve Equation (4) under budget constraints, we implement Bayesian Optimization . Weconstruct the objective function for Bayesian Optimization to maximize as follows:",
  "fobj = Scc(T , {1, 2, . . .}) P,(6)": "where P = 0 if budget constraints are satisfied otherwise a significant penalty term. This onlineparadigm allows us to determine thresholds without assuming an exponential distribution, enablingthe acquisition of more effective thresholds through real-time feedback. Here, we employ BayesianOptimization as a representative example of an online solving approach. Future work could extend toother online algorithms such as multi-armed bandit or reinforcement learning . 3.3Training AlgorithmNotably, it is nontrivial to train our dynamic robotic MLLM properly. Specifically, dynamic adjust-ment of the network architecture leads to a discrepancy between training and inference. Duringinference, we use a deterministic criterion to select a proper intermediate feature at each timestep.Nevertheless, during training, we lack a well-defined termination criterion and remain unaware ofthe distribution of features across the exits. To enable our model to learn to integrate temporalinformation effectively, we propose a tailored training algorithm, as introduced in the following. Learning with an arbitrary size of models. To reduce the aforementioned discrepancy, we proposea simple yet effective random sampling strategy during training. As depicted by the winding curveson the right side of , our approach involves sampling an exit index from 1 to N at eachtimestep. We implement two types of sampling strategies. The first strategy, denoted as s1, is touniformly sample an exit index from 1 to N at each step. This ensures that features from all possibleexits are effectively captured in the action head during training. It simulates scenarios where the actionhead might encounter features from all exits within a given time window, thus accommodating an : Comparison with baselines. GR-1 uses extra proprioceptive information as input. Note that somebaselines mainly focus on one or two settings, and we present results following their original papers. We reportthe performance of our method at the last epoch. The value in parentheses indicates the LLM FLOPs required toachieve the reported score. The success rates for the 1st to 5th subtasks are in Appendix B.1.",
  "DeeR w. online (ours)RGBLANGOpenFlamingo 3B2.92 (8.5)4.13 (9.7)2.90 (9.5)": "arbitrary inference pattern and reducing the training-inference discrepancy. Moreover, we observe thatin practice, the dynamic model often terminates at the same exit for multiple consecutive timesteps,as the neighboring observations tend to be quite similar. The model then switches to another exitfor a sequence of subsequent timesteps. To better emulate this pattern during training, we adopt asecond sampling strategy denoted as s2. Specifically, we split the time window ot:t+H1 into twoconsecutive segments ot:t+i and ot+i+1:t+H1, with i chosen randomly. In each segment, a singleuniformly sampled index is assigned and shared across all timesteps. On top of these two sampling strategies, we can define our training loss function. We samplefrom a robot demonstration dataset a language instruction l and a clip of observation-actions{ot, at, ot+1, at+1, . . . , ot+H1, at+H1}. For each sampling strategy s {s1, s2}, we use Equa-tion (1) and Equation (2) to predict each action a,st+i for s = s1, s2 and i = 0, 1, . . . , H 1, wherec(t) in Equation (2) is replaced by the sampling strategy s. For each pair of the predicted action aand the actual action a, we define a single-action loss function L(a, a) that incorporates both meansquared error (MSE) for pose prediction and cross-entropy loss for gripper status prediction with acoefficient to balance the two terms . The total loss for a sequence is then the sum of the lossesover timesteps:",
  "i=0 L(a,st+i, at+i).(7)": "Auxiliary losses. The intermediate features from the original MLLM, intended as input for subsequentlayers, may not be optimal for output prediction. To ensure that each activated size of the MLLMin our framework produces features suitable for predicting actions, we introduce auxiliary losses.Specifically, we attach N auxiliary action heads (denoted as UAH in ) at the exits. The j-thauxiliary head processes temporal features from the j-th exit and predicts the action ajt. We jointlytrain the auxiliary heads and the MLLM using the loss function:",
  "These auxiliary heads are employed only during training and are not used for inference": "Total Loss. The full training pipeline is depicted in . We fine-tune only the parameters of theperceiver sampler and cross-attention layers in the MLLM, with the randomly initialized action head and auxiliary action heads. The visual encoder and the other LLM components, remain frozen.The total loss for the training process is expressed as Ltotal = L + Laux.",
  "Experiments": "Setup. In this section, we conduct experiments to validate the effectiveness of DeeR as an efficientrobot policy. Specifically, we build DeeR upon the RoboFlamingo++ codebase. We preserve hyper-parameters from RoboFlamingo++ for fair comparison, except for the number of LLM layers and ourproposed dynamic early-exit paradigm. we compare DeeR in terms of budget v.s. performance withsimilarly sized RoboFlamingo++ models and other SOTA baselines. We provide implementationdetails in Appendix A.",
  "Measures of efficiency. In modern foundation models, the LLM typically plays a pivotal role withinan MLLM in terms of reasoning and problem-solving tasks, and it usually contains the majority of the": "models parameters . Our work mainly focuses on improving the efficiency of LLMs withina robotic context. To facilitate a focused comparison, in our experiments, we report the number offloating point operations (FLOPs) and GPU memory usage for LLMs inference. Benchmark. We utilize the CALVIN Long-Horizon Multi-Task Language Control benchmark(LH-MTLC) as the testbed to test our learned multi-task, language-conditioned policy. In theCALVIN, the objective is for the agent to successfully complete task sequences, each with fivesubtasks described in natural language. Following previous works , model performanceis evaluated based on the average successful length (0 to 5) across 1000 task sequences. Datasets. The CALVIN dataset is divided into four environmental splits, labeled A throughD, each characterized by unique backgrounds and object configurations. Each split contains over 2million robot manipulation trajectories (denoted as ALL). Of these, only about 1%, approximately24 thousand trajectories, are annotated with language instructions (denoted as LANG). For trainingDeeR, we exclusively utilize the LANG data. In our study, we evaluate models across three settingsto thoroughly assess their imitation and generalization capabilities: 1) DD: Train and evaluate in asingle environment, 2) ABCD: Zero-Shot Multi-Environment, 3) ABCDD: Multi-Environment. Baselines. For a comprehensive comparison, we consider various baselines. We include HULC and SPIL as representatives of approaches reliant on hierarchical planning and skill priors . Additionally, we evaluate models using pretrained or foundation models, such as RT-1 ,SuSIE , GR-1 , and RoboFlamingo . RoboFlamingo++ is our reproduced RoboFlamingo.",
  "x": "Maximal FLOPs for LLM (G) RoboFla- mingo++ 3B DeeR-B DeeR-S 2x 6x GPU Memory for LLM (G) 2x 6x : Results atop OpenFlamingo 3B. Upper: Avg. successful len v.s. avg. LLM GFLOPs. Bottom:Peak GLOPs and GPU memory for LLM. Different colors indicate different peak FLOPs and GPU memorybudgets, denoted as DeeR-S and DeeR-B (they share a fixed model). DeeR preserve all the architecture andhyperparameters from RoboFlamingo++ for fair comparisons, except for our dynamic early-exit paradigm.Results on Flamingo 3B are presented in . We train just a single model in each CALVINsetting. Given the predefined total computational budget B, the maximum FLOPs G, and the GPUmemory M, we adhere to these budgets by adjusting the termination thresholds, which are determinedby solving Equation (4) with the CALVIN dataset. Then we assess the average successful length ofDeeR under different thresholds to plot the curves. It can be observed that DeeR consistently reducesthe computational cost of the LLMs across all settings. For instance, in the setting DD, DeeRachieves an average successful length 2.71 with 5.9x fewer average FLOPs, 2x fewer maximumFLOPs and 2x fewer GPU memory. Surprisingly, DeeR-S achieves relatively high performance withonly 2GB memory consumed by LLM, which is affordable to most users. Thus, DeeR demonstratesthe potential to enable a broader range of users to operate their own robots effectively with LLMs. Comparison with SOTA baselines. In , we benchmark the DeeR model against recentlySOTA methods in the CALVIN benchmark. Our analysis reveals that DeeR achieves competitiveperformance compared to the latest SOTA model GR-1 which uses additional proprioceptive informa-tion. When compared with traditional imitation learning methods without foundation model, DeeRdemonstrates superior performance, particularly in generalization scenarios (ABCD). Moreover,DeeR slightly outperforms RoboFlamingo while requiring less computation. Solve thresholds with online interaction. When interaction with the environment is feasible, weutilize Bayesian Optimization to solve Equation (4) as we stated in .2. As shown in ,we discovered that finding thresholds via online interaction is particularly effective in challengingscenarios such as low-data environments (DD) and generalization to unseen situations (ABCD). Scalability of DeeR. We developed DeeR on top of OpenFlamingo 9B to evaluate its efficiencywhen scaling up the foundation model. The results, detailed in , indicate that DeeR reduces1.8-5.7x computation and 2.7x-4.0x peak FLOPs and memory for the same performance.",
  "ABCD4.92.292.462.629.12.452.712.75": "Early-termination criterion. Based on a fixed DeeRmodel, we explore various criteria for adaptive infer-ence. We consider using cosine similarity betweenexit points to determine termination . Specifi-cally, if the similarity value exceeds a threshold, theprocess is terminated. We introduce another met-ric that progressively increases the size of the acti-vated LLM as a task progresses, based on the obser-vation that the initial stage of a task generally presentsimpler scenarios. Our results, detailed in ,demonstrate that our straightforward yet effective ac-tion consistency criterion outperforms other criteria across several average computational budgets.",
  "Robo++4.0731.255msDeeR4.086.017.5ms": "Real inference efficiency. We conducted evaluations ofreal-world operational efficiency. Both RoboFlamingo++and DeeR were tested on the same Nvidia V100 GPU.As shown in , DeeR achieved a 68.1% reductionin LLM inference time compared to RoboFlamingo++(abbreviated as Robo++ in ) when both modelsachieved the same performance, which aligns with thetheoretical 80.7% reduction in FLOPs. This evaluation was performed without code optimizations forearly-exit implementation. We expect that with further optimizations, DeeRs real-world operationalefficiency will improve, potentially aligning even more closely with the predicted FLOPs reduction.",
  "float326G4.13float163G4.12int41.7G3.91": "DeeR with Quantization. Model compression techniques, suchas quantization and pruning, along with efficient structural designs,complement early-exit strategies like DeeR. These methods im-prove efficiency from different perspectives: quantization reducesmemory usage by lowering parameter precision, while early-exitstrategies like DeeR reduce computational load by dynamicallyskipping unnecessary layers. In , we present quantization as an example to illustrate howDeeR can integrate with these techniques to reduce memory cost.",
  "left": ": Visualization of DeeR rollouts in the CALVINenvironment. Please zoom in to view details. The numbersindicate the termination exit index. Situations with a lowerexit index are recognized as easier ones. displays rollouts of DeeR with thetermination points. Situations with a higherexit index are considered \"harder\" by DeeRand thus are allocated more computationalresources. One can observe that \"hard\"situations often involve relatively complexand delicate operations, while \"easy\" sit-uations typically involve straightforwardmovements toward target objects. For ex-ample, in the task of stacking blocks (1strow), lifting the blue block from the ta-ble (1st image) and placing it down on thepink block (images 4 and 5) are allocatedmore computation, whereas simply mov-ing towards the pink block (images 2 and3)requires only the smallest LLM to han-dle. Similar observations occur in the 2ndand 3rd rows, where the stage of movingtoward the target object require minimal computation, while pushing the lightbulb switch or movingthe sliding door are sophisticated operations that necessitate more LLM processing.",
  "Conclusion and Limitations": "In this paper, we introduced the Dynamic Early-Exit for Robotic MLLM (DeeR) framework, aimingto dynamically configure the size of MLLMs based on the specific requirements of each situationencountered by a robotic agent. In specific, we proposed a novel MLLM architecture with multipleintermediate exits. Further, we establish early-termination criteria for DeeR based on action con-sistency and solve thresholds via a dataset or online interaction. Additionally, we crafted a tailoredtraining method that integrates temporal information within this multi-exit framework to enhancerobotic control. Extensive robotic experiments demonstrated that DeeR significantly reduces LLMcomputational costs and GPU memory usage, highlighting its potential to empower a broader rangeof users to manage their robots on resource-constrained platforms. While our study shows promisingresults, it has some limitations. We focused on improving LLM efficiency for robotic executionsince LLMs account for most of the parameters and GFLOPs. However, the computational cost ofthe visual encoder is also significant. We expect this limitation to be alleviated as more efficient,lightweight visual encoders are developed. Besides, our experiments were limited to a simulatedbenchmark. Future work will aim to improve the inference efficiency of the entire MLLM-basedrobotic system in realistic environments.",
  "The Tsinghua University team is supported in part by the National Key R&D Program of China(2022ZD0114903)": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia LeoniAleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4technical report. arXiv preprint arXiv:2303.08774, 2023. 1 Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, andLijuan Wang. The dawn of lmms: Preliminary explorations with gpt-4v (ision). arXiv preprintarXiv:2309.17421, 2023. 1 Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-the Lacroix, Baptiste Rozire, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Openand efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 1",
  "Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. NeurIPS,2024. 1, 3, 8": "Naoki Wake, Atsushi Kanehira, Kazuhiro Sasabuchi, Jun Takamatsu, and Katsushi Ikeuchi.Gpt-4v (ision) for robotics: Multimodal task planning from human demonstration. arXivpreprint arXiv:2311.12015, 2023. 2, 3 Liang Chen, Yichi Zhang, Shuhuai Ren, Haozhe Zhao, Zefan Cai, Yuchi Wang, Peiyi Wang,Tianyu Liu, and Baobao Chang. Towards end-to-end embodied decision making via multi-modal large language model: Explorations with gpt4-vision and beyond.arXiv preprintarXiv:2310.02071, 2023. 2 Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choro-manski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. CoRL, 2023. 2, 3 Abhishek Padalkar, Acorn Pooley, Ajinkya Jain, Alex Bewley, Alex Herzog, Alex Irpan,Alexander Khazatsky, Anant Rai, Anikait Singh, Anthony Brohan, et al. Open x-embodiment:Robotic learning datasets and rt-x models. arXiv preprint arXiv:2310.08864, 2023. 2, 3 Xinghang Li, Minghuan Liu, Hanbo Zhang, Cunjun Yu, Jie Xu, Hongtao Wu, Chilam Cheang,Ya Jing, Weinan Zhang, Huaping Liu, et al. Vision-language foundation models as effectiverobot imitators. ICLR, 2024. 2, 3, 4, 7, 8 Oier Mees, Lukas Hermann, Erick Rosete-Beas, and Wolfram Burgard. Calvin: A benchmarkfor language-conditioned policy learning for long-horizon robot manipulation tasks. IEEERobotics and Automation Letters, 7(3):73277334, 2022. 2, 8",
  "Oier Mees, Jessica Borja-Diaz, and Wolfram Burgard. Grounding language with visual affor-dances over unstructured data. In ICRA, 2023. 3, 8": "Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn,Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. Rt-1: Roboticstransformer for real-world control at scale. Proceedings of Robotics: Science and Systems, 2024.3, 7, 8 Octo Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, SudeepDasari, Joey Hejna, Charles Xu, Jianlan Luo, et al. Octo: An open-source generalist robotpolicy. First Workshop on Vision-Language Models for Navigation and Manipulation at ICRA2024, 2024. 3 Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter,Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodiedmultimodal language model. ICML, 2023. 3 Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David,Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, et al. Do as i can, notas i say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022. 3 Yao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang, Mingyu Ding, Jun Jin, Bin Wang,Jifeng Dai, Yu Qiao, and Ping Luo. Embodiedgpt: Vision-language pre-training via embodiedchain of thought. NeurIPS, 2023. 3 Zhongwei Wan, Xin Wang, Che Liu, Samiul Alam, Yu Zheng, Zhongnan Qu, Shen Yan, Yi Zhu,Quanlu Zhang, Mosharaf Chowdhury, et al. Efficient large language models: A survey. arXivpreprint arXiv:2312.03863, 1, 2023. 3 Siddharth Samsi, Dan Zhao, Joseph McDonald, Baolin Li, Adam Michaleas, Michael Jones,William Bergeron, Jeremy Kepner, Devesh Tiwari, and Vijay Gadepally. From words towatts: Benchmarking the energy costs of large language model inference. In 2023 IEEE HighPerformance Extreme Computing Conference (HPEC), pages 19. IEEE, 2023. 3 Zixuan Zhou, Xuefei Ning, Ke Hong, Tianyu Fu, Jiaming Xu, Shiyao Li, Yuming Lou, LuningWang, Zhihang Yuan, Xiuhong Li, et al. A survey on efficient inference for large languagemodels. arXiv preprint arXiv:2404.14294, 2024. 3 Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, andSumit Sanghai. Gqa: Training generalized multi-query transformer models from multi-headcheckpoints. In EMNLP, 2023. 3",
  "Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces.arXiv preprint arXiv:2312.00752, 2023. 3": "Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, XinCheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, et al. Rwkv: Reinventing rnns forthe transformer era. Findings of EMNLP, 2023. 3 Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang,and Furu Wei. Retentive network: A successor to transformer for large language models. arXivpreprint arXiv:2307.08621, 2023. 3",
  "Dongchen Han, Xuran Pan, Yizeng Han, Shiji Song, and Gao Huang. Flatten transformer:Vision transformer using focused linear attention. In ICCV, 2023. 3": "Zechun Liu, Changsheng Zhao, Forrest Iandola, Chen Lai, Yuandong Tian, Igor Fedorov,Yunyang Xiong, Ernie Chang, Yangyang Shi, Raghuraman Krishnamoorthi, et al. Mobilellm:Optimizing sub-billion parameter language models for on-device use cases. arXiv preprintarXiv:2402.14905, 2024. 3 Huanqian Wang, Yang Yue, Rui Lu, Jingxin Shi, Andrew Zhao, Shenzhi Wang, Shiji Song, andGao Huang. Model surgery: Modulating llms behavior via simple parameter editing. arXivpreprint arXiv:2407.08770, 2024. 3",
  "Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, and Christopher M De Sa. Quip: 2-bit quantiza-tion of large language models with guarantees. NeurIPS, 2023. 3": "Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han.Smoothquant: Accurate and efficient post-training quantization for large language models.In ICML, pages 3808738099, 2023. 3 Shiyao Li, Xuefei Ning, Ke Hong, Tengxuan Liu, Luning Wang, Xiuhong Li, Kai Zhong,Guohao Dai, Huazhong Yang, and Yu Wang. Llm-mq: Mixed-precision quantization forefficient llm deployment. In The Efficient Natural Language and Speech Processing Workshopwith NeurIPS, volume 9, 2023. 3",
  "Yizeng Han, Gao Huang, Shiji Song, Le Yang, Honghui Wang, and Yulin Wang. Dynamicneural networks: A survey. TPAMI, 44(11):74367456, 2021. 3, 5": "Luciano Del Corro, Allie Del Giorno, Sahaj Agarwal, Bin Yu, Ahmed Awadallah, and Sub-habrata Mukherjee. Skipdecode: Autoregressive skip decoding with batching and caching forefficient llm inference. arXiv preprint arXiv:2307.02628, 2023. 3 Mostafa Elhoushi, Akshat Shrivastava, Diana Liskovich, Basil Hosmer, Bram Wasti, LiangzhenLai, Anas Mahmoud, Bilge Acun, Saurabh Agarwal, Ahmed Roman, et al. Layer skip: Enablingearly exit inference and self-speculative decoding. arXiv preprint arXiv:2404.16710, 2024. 3 David Raposo, Sam Ritter, Blake Richards, Timothy Lillicrap, Peter Conway Humphreys,and Adam Santoro. Mixture-of-depths: Dynamically allocating compute in transformer-basedlanguage models. arXiv preprint arXiv:2404.02258, 2024. 3 Yulin Wang, Rui Huang, Shiji Song, Zeyi Huang, and Gao Huang. Not all images are worth16x16 words: Dynamic transformers for efficient image recognition. Advances in neuralinformation processing systems, 34:1196011973, 2021. 3",
  "Yang Yue, Bingyi Kang, Zhongwen Xu, Gao Huang, and Shuicheng Yan. Value-consistentrepresentation learning for data-efficient reinforcement learning. In AAAI, 2023. 4": "Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson,Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visuallanguage model for few-shot learning. NeurIPS, 2022. 4 Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, KalyaniMarathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al.Openflamingo: An open-source framework for training large autoregressive vision-language models. arXiv preprintarXiv:2308.01390, 2023. 4, 8, 9 Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zi-Hang Jiang, Francis EH Tay,Jiashi Feng, and Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratchon imagenet. In ICCV, 2021. 4",
  "Richard S. Sutton and Andrew G. Barto. Reinforcement learning: An introduction. 1998. 6": "Hongtao Wu, Ya Jing, Chilam Cheang, Guangzeng Chen, Jiafeng Xu, Xinghang Li, MinghuanLiu, Hang Li, and Tao Kong. Unleashing large-scale video generative pre-training for visualrobot manipulation, 2023. 7, 8 Hongkuan Zhou, Zhenshan Bing, Xiangtong Yao, Xiaojie Su, Chenguang Yang, Kai Huang, andAlois Knoll. Language-conditioned imitation learning with base skill priors under unstructureddata. ICML, 2024. 7, 8 Kevin Black, Mitsuhiko Nakamoto, Pranav Atreya, Homer Walke, Chelsea Finn, Aviral Kumar,and Sergey Levine. Zero-shot robotic manipulation with pretrained image-editing diffusionmodels. ICLR, 2024. 7, 8",
  "A.1Network Architecture": "For the MLLM, we utilize the pretrained model OpenFlamingo, which includes a frozen LLM andvision encoder. The vision-language fusion modules, specifically a perceiver sampler and cross-attention, are trained using the web-scraped image-text datasets LAION-2B and Multimodal C4.The architecture specifics are outlined in . For the action head, which integrates temporalinformation for action prediction, we employ a 4-layer LSTM to process temporal information and a3-layer MLP for predicting actions. To mitigate overfitting, we implement dropout for the LSTM andMLP. Additionally, LayerNorm is applied prior to activation functions. We configured exit points after every two self-attention layers in all MLLM models. To conservetraining resources, we employed a subset of the OpenFlamingo model as our backbone. Specifically,for OpenFlamingo3B (which has 24 LLM layers) and OpenFlamingo9B (which has 32 LLM layers),we used only the first 12 layers for DeeR multiexit architecture, resulting in 6 exit points. ForRoboFlamingo++, we utilize 6/12/24 LLM layers from OpenFlamingo3B and 8/16/32 LLM layersfrom OpenFlamingo9B to create a range of model sizes for comparing budget versus performancecurve with DeeR.",
  "A.2Inference Details": "After training, we obtain a multiexit model. During inference, the model remains fixed, but thecomputational cost of DeeR can be adjusted dynamically based on computational constraints, withoutmodifying the multiexit model. For the 12-layer multiexit model built on top of OpenFlamingo3B, each LLM layer consumes approximately 0.5GB of memory and requires 1.3G FLOPs duringinference (with a batch size of 1, excluding the vision encoder). Given a GPU memory limit of6GB and a maximum of 15.6G FLOPs, DeeR-B loads all 12 LLM layers, offering 6 exit points. Formore resource-constrained scenarios, DeeR-S loads only the first 4 LLM layers, with 2 exit points.For the OpenFlamingo 9B model, each LLM layer consumes approximately 1.0GB of memory andrequires 2.85G FLOPs. DeeR-B loads 12 LLM layers with 6 exit points, requiring about 12GB ofGPU memory, while DeeR-S loads 8 LLM layers with 4 exit points, using around 8GB of memory.Crucially, users have the flexibility to define custom inference-time models beyond DeeR-S andDeeR-B by selecting the number of LLM layers to load, based on memory or FLOPs constraints. After determining the number of LLM layers and exit points, the average FLOPs per action can befurther dynamically adjusted by adjusting the exit thresholds based on the criteria in Equation (5)or Equation (6). We compute the thresholds using the validation set from environment D. However,we found that thresholds computed using just 1% of the training set achieve similar performance,demonstrating that these thresholds effectively generalize and are robust enough.",
  "A.3Training Details": "For both RoboFlamingo++ and the multiexit model for DeeR, we employ a two-phase trainingschedule. Initially, we jointly train the trainable components of the MLLM (the perceiver samplerand cross-attention layers) alongside the action head. Since the backbone MLLM is pretrained andconverges more rapidly, we later freeze the MLLM and finetune only the action head, which we referto as post-training for the action head. Our experiments indicate that this additional finetuning stepfor the action head results in slightly better performance. The hyperparameters used during trainingare detailed in . Note that the dropout rates for LSTM and MLP are 0.3 and 0.4, respectively, except during post-training for ABCDD, where slightly higher values of 0.4 and 0.5 are used. Wereport the results based on the final epochs checkpoint, rather than selecting the best-performingcheckpoint.",
  "A.4Training Cost": "We leverage PyTorchs Automatic Mixed Precision (AMP) acceleration to optimize training efficiency.For DeeR using the OpenFlamingo 3B model, training is conducted on 8 NVIDIA V100 32G GPUs,taking approximately 14 hours for 4+4 epochs on Dataset D, 24 hours for 4+1 epochs on DatasetABC, and 25 hours for 3+1 epochs on Dataset ABCD. When scaling DeeR to the OpenFlamingo 9B,the model is trained on 8 NVIDIA A100 80G GPUs. The training for 4+4 epochs on Dataset D takesaround 24 hours. Multi-node parallel training support is included in the code, offering faster training.",
  ": Visualization of DeeR rollouts of five subtasks in a task chain": "The paper should point out any strong assumptions and how robust the results are toviolations of these assumptions (e.g., independence assumptions, noiseless settings,model well-specification, asymptotic approximations only holding locally). The authorsshould reflect on how these assumptions might be violated in practice and what theimplications would be. The authors should reflect on the scope of the claims made, e.g., if the approach wasonly tested on a few datasets or with a few runs. In general, empirical results oftendepend on implicit assumptions, which should be articulated. The authors should reflect on the factors that influence the performance of the approach.For example, a facial recognition algorithm may perform poorly when image resolutionis low or images are taken in low lighting. Or a speech-to-text system might not beused reliably to provide closed captions for online lectures because it fails to handletechnical jargon.",
  "If applicable, the authors should discuss possible limitations of their approach toaddress problems of privacy and fairness": "While the authors might fear that complete honesty about limitations might be used byreviewers as grounds for rejection, a worse outcome might be that reviewers discoverlimitations that arent acknowledged in the paper. The authors should use their bestjudgment and recognize that individual actions in favor of transparency play an impor-",
  "If the contribution is a dataset and/or model, the authors should describe the steps takento make their results reproducible or verifiable": "Depending on the contribution, reproducibility can be accomplished in various ways.For example, if the contribution is a novel architecture, describing the architecture fullymight suffice, or if the contribution is a specific model and empirical evaluation, it maybe necessary to either make it possible for others to replicate the model with the samedataset, or provide access to the model. In general. releasing code and data is oftenone good way to accomplish this, but reproducibility can also be provided via detailedinstructions for how to replicate the results, access to a hosted model (e.g., in the caseof a large language model), releasing of a model checkpoint, or other means that areappropriate to the research performed. While NeurIPS does not require releasing code, the conference does require all submis-sions to provide some reasonable avenue for reproducibility, which may depend on thenature of the contribution. For example(a) If the contribution is primarily a new algorithm, the paper should make it clear howto reproduce that algorithm.",
  "(b) If the contribution is primarily a new model architecture, the paper should describethe architecture clearly and fully": "(c) If the contribution is a new model (e.g., a large language model), then there shouldeither be a way to access this model for reproducing the results or a way to reproducethe model (e.g., with an open-source dataset or instructions for how to constructthe dataset). (d) We recognize that reproducibility may be tricky in some cases, in which caseauthors are welcome to describe the particular way they provide for reproducibility.In the case of closed-source models, it may be that access to the model is limited insome way (e.g., to registered users), but it should be possible for other researchersto have some path to reproducing or verifying the results.",
  "Guidelines:": "The answer NA means that the paper does not include experiments. The authors should answer \"Yes\" if the results are accompanied by error bars, confi-dence intervals, or statistical significance tests, at least for the experiments that supportthe main claims of the paper. The factors of variability that the error bars are capturing should be clearly stated (forexample, train/test split, initialization, random drawing of some parameter, or overallrun with given experimental conditions).",
  ". Experiments Compute Resources": "Question: For each experiment, does the paper provide sufficient information on the com-puter resources (type of compute workers, memory, time of execution) needed to reproducethe experiments?Answer: [Yes]Justification: We provided sufficient information on the computer resources in the main textand appendix.Guidelines: The answer NA means that the paper does not include experiments. The paper should indicate the type of compute workers CPU or GPU, internal cluster,or cloud provider, including relevant memory and storage.",
  ". Code Of Ethics": "Question: Does the research conducted in the paper conform, in every respect, with theNeurIPS Code of Ethics [Yes]Justification: the research conducted in the paper conformed, in every respect, with theNeurIPS Code of Ethics.Guidelines: The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. If the authors answer No, they should explain the special circumstances that require adeviation from the Code of Ethics.",
  "Justification: discuss both potential positive societal impacts and negative societal impactsof the work in the appendix.Guidelines:": "The answer NA means that there is no societal impact of the work performed. If the authors answer NA or No, they should explain why their work has no societalimpact or why the paper does not address societal impact. Examples of negative societal impacts include potential malicious or unintended uses(e.g., disinformation, generating fake profiles, surveillance), fairness considerations(e.g., deployment of technologies that could make decisions that unfairly impact specificgroups), privacy considerations, and security considerations. The conference expects that many papers will be foundational research and not tiedto particular applications, let alone deployments. However, if there is a direct path toany negative applications, the authors should point it out. For example, it is legitimateto point out that an improvement in the quality of generative models could be used togenerate deepfakes for disinformation. On the other hand, it is not needed to point outthat a generic algorithm for optimizing neural networks could enable people to trainmodels that generate Deepfakes faster. The authors should consider possible harms that could arise when the technology isbeing used as intended and functioning correctly, harms that could arise when thetechnology is being used as intended but gives incorrect results, and harms followingfrom (intentional or unintentional) misuse of the technology. If there are negative societal impacts, the authors could also discuss possible mitigationstrategies (e.g., gated release of models, providing defenses in addition to attacks,mechanisms for monitoring misuse, mechanisms to monitor how a system learns fromfeedback over time, improving the efficiency and accessibility of ML).",
  ". Safeguards": "Question: Does the paper describe safeguards that have been put in place for responsiblerelease of data or models that have a high risk for misuse (e.g., pretrained language models,image generators, or scraped datasets)?Answer: [Yes]Justification: We describe safeguards for responsible release of models in the social impactssection.Guidelines: The answer NA means that the paper poses no such risks. Released models that have a high risk for misuse or dual-use should be released withnecessary safeguards to allow for controlled use of the model, for example by requiringthat users adhere to usage guidelines or restrictions to access the model or implementingsafety filters.",
  ". Licenses for existing assets": "Question: Are the creators or original owners of assets (e.g., code, data, models), used inthe paper, properly credited and are the license and terms of use explicitly mentioned andproperly respected?Answer: [Yes]Justification: We properly credited the creators or original owners of assets (e.g., code, data,models), used in the paper and conformed the license and terms.Guidelines:",
  "The authors should state which version of the asset is used and, if possible, include aURL": "The name of the license (e.g., CC-BY 4.0) should be included for each asset. For scraped data from a particular source (e.g., website), the copyright and terms ofservice of that source should be provided. If assets are released, the license, copyright information, and terms of use in thepackage should be provided. For popular datasets, paperswithcode.com/datasetshas curated licenses for some datasets. Their licensing guide can help determine thelicense of a dataset.",
  ". New Assets": "Question: Are new assets introduced in the paper well documented and is the documentationprovided alongside the assets?Answer: [Yes]Justification: We communicated the details of the dataset/code/model as part of theirsubmission.Guidelines: The answer NA means that the paper does not release new assets. Researchers should communicate the details of the dataset/code/model as part of theirsubmissions via structured templates. This includes details about training, license,limitations, etc.",
  "According to the NeurIPS Code of Ethics, workers involved in data collection, curation,or other labor should be paid at least the minimum wage in the country of the datacollector": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with HumanSubjectsQuestion: Does the paper describe potential risks incurred by study participants, whethersuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)approvals (or an equivalent approval/review based on the requirements of your country orinstitution) were obtained?Answer: [NA]Justification: Our paper does not involve study participants.Guidelines:",
  "The answer NA means that the paper does not involve crowdsourcing nor research withhuman subjects": "Depending on the country in which research is conducted, IRB approval (or equivalent)may be required for any human subjects research. If you obtained IRB approval, youshould clearly state this in the paper. We recognize that the procedures for this may vary significantly between institutionsand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and theguidelines for their institution."
}