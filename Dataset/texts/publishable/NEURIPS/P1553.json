{
  "Yiheng Zhu1, Jialu Wu2, Qiuyi Li3, Jiahuan Yan1, Mingze Yin4, Wei Wu5,Mingyang Li3, Jieping Ye3, Zheng Wang3, Jian Wu1,4,6": "1College of Computer Science & Technology and Liangzhu Laboratory, Zhejiang University2College of Pharmaceutical Sciences, Zhejiang University3Alibaba Cloud Computing4School of Public Health, Zhejiang University5School of Artificial Intelligence and Data Science, University of Science and Technology of China6The Second Affiliated Hospital Zhejiang University School of Medicine{zhuyiheng2020, jialuwu, jyansir, yinmingze, wujian2000}@zju.edu.cn{liqiuyi.lqy, sangheng.lmy, yejieping.ye, wz388779}@",
  "Abstract": "Inverse protein folding is a fundamental task in computational protein design,which aims to design protein sequences that fold into the desired backbone struc-tures. While the development of machine learning algorithms for this task hasseen significant success, the prevailing approaches, which predominantly employa discriminative formulation, frequently encounter the error accumulation issueand often fail to capture the extensive variety of plausible sequences. To fill thesegaps, we propose Bridge-IF, a generative diffusion bridge model for inverse folding,which is designed to learn the probabilistic dependency between the distributions ofbackbone structures and protein sequences. Specifically, we harness an expressivestructure encoder to propose a discrete, informative prior derived from structures,and establish a Markov bridge to connect this prior with native sequences. Duringthe inference stage, Bridge-IF progressively refines the prior sequence, culminatingin a more plausible design. Moreover, we introduce a reparameterization perspec-tive on Markov bridge models, from which we derive a simplified loss functionthat facilitates more effective training. We also modulate protein language models(PLMs) with structural conditions to precisely approximate the Markov bridgeprocess, thereby significantly enhancing generation performance while maintainingparameter-efficient training. Extensive experiments on well-established bench-marks demonstrate that Bridge-IF predominantly surpasses existing baselines insequence recovery and excels in the design of plausible proteins with high foldabil-ity. The code is available at",
  "Introduction": "Proteins are 3D folded linear chains of amino acids that execute the myriad of biological processesfundamental to life, such as catalysing metabolic reactions, mediating immune responses, andresponding to stimuli . Designing protein sequences that fold into desired 3D structures, known asinverse protein folding, is a crucial task with great potential for applications in protein engineering . Beyond long-established physics-based methods like Rosetta , the considerable promiseof leveraging geometric deep learning for protein structure modeling has given rise to an ongoing",
  "paradigm. This paradigm is centered on deciphering the principles of protein design directly fromdata and on predicting sequences corresponding to specific structures": "Despite substantial advancements, most existing approaches follow a discriminative formulationfor learning inverse folding , consequently encountering two principal obstacles: (i) Erroraccumulation issue. For instance, Transformer-based autoregressive models are constrained by theirinherent sequential generation process and exposure bias, which prevents them from correctingpreceding erroneous predictions. (ii) One-to-many mapping nature of the inverse folding problem.A multitude of distinct amino acid sequences possess the capability to fold into an identical proteinbackbone structure, a phenomenon exemplified by homologous proteins. Discriminative models areincapable of capturing the one-to-many mapping from the protein structure to non-unique sequences,thereby facing difficulties in covering the broad spectrum of plausible solutions . Recent studies have advanced the iterative refinement strategy to optimize the previously generatedresults, aiming to reduce prediction errors . These approaches employ a refinementmodule to identify and correct inaccurately predicted amino acids. However, as the number ofrefinement iterations grows, managing the intermediate stages effectively becomes more challenging,potentially hindering sustained performance gains. Diffusion-based generative models , particularly their discrete extensions , which offera structured iterative refinement process with probabilistic interpretation, appear to be a promisingsolution. GraDe-IF is a pioneer in investigating diffusion models for inverse folding, leveragingthe backbone structure to guide the denoising process on the amino acid residues. However, asdiffusion models are designed to learn a single intractable data distribution, the prior distributionutilized by GraDe-IF is restricted to a simple noise distribution (i.e., a uniform distribution acrossall residue types), which has little or no information about the distribution of native sequences. Itremains unclear whether this default formulation best suits conditional generative problems such asinverse protein folding, where the backbone structures provide significantly more information thanrandom noise. Thus, an exciting research question naturally arises: Can we propose a more strongand informative prior based on desired backbone structures to enhance the quality of samples andaccelerate the inference process?",
  "In this work, we propose Bridge-IF, a novel generative diffusion bridge model for inverse folding.Its core design is aimed at generating protein sequences from a structure-aware prior. As shown": "in , we leverage an expressive structure encoder supervised by native sequences to propose adiscrete, deterministic prior based on desired structures, and build a Markov bridge betweenit and the native sequence. By approximating the reference Markov bridge process, Bridge-IF learnsto progressively refine the prior sequence, resulting in a more plausible design. Furthermore, wepresent a fresh reparameterization perspective on Markov bridge models and derive a simplifiedloss function that yields enhanced training effectiveness. Inspired by significant advances in proteinlanguage models (PLMs) for understanding proteins , we innovatively integrate conditions,including timestep and structures, into PLMs to accurately approximate the Markov bridge process.This approach notably improves generation performance while ensuring parameter-efficient training.Empirically, we demonstrate that Bridge-IF outperforms state-of-the-art baselines on several standardbenchmarks and excels in the design of plausible proteins with high foldability.",
  "Inverse protein folding": "Recently, AI algorithms have spurred a revolution in modeling protein folding . Meanwhile,the inverse problem of protein folding, which aims to infer an amino acid sequence that will fold intothe desired structure, is gaining increasing attention . By representing protein backbone structuresas a k-NN graph, geometric deep learning has achieved remarkable progress in learning inversefolding , surpassing traditional physics-based approaches , and even facilitating thedesign of a range of experimentally validated proteins . Modern deep learning-based inversefolding approaches typically comprise a structure encoder and a sequence decoder. Depending ontheir decoding strategies, these approaches can be classified into three categories: autoregressivemodels, one-shot models, and iterative models. Most methods adopt the autoregressive decodingscheme to generate amino acid sequences . Given that autoregressive models tend tohave low inference speed, some researchers have investigated one-shot methods that facilitate theparallel generation of multiple tokens . Since directly predicting highly plausible sequencesis challenging, some works have shifted their attention to iterative refinement .For instance, LM-Design and KW-design utilize the pre-trained knowledge from PLMs toreconstruct a native sequence from a corrupted version. The Potts model-based ChromaDesign and CarbonDesign employ iterative sampling techniques, including Markov chain Monte Carlo,to design protein sequences. GraDe-IF further leverages the principles of discrete denoisingdiffusion probabilistic models , demonstrating a strong capacity to encompass diverse plausiblesolutions. In this work, we present the first generative diffusion bridge model for inverse folding.",
  "Diffusion models": "Diffusion-based generative models have showcased remarkable successes in a wide rangeof applications, ranging from image synthesis , audio synthesis , to video generation .Generally, the essential idea behind these models is to define a forward diffusion process thatgradually transforms the data into a simple prior distribution and learn a reverse denoising process togradually recover original data samples from the prior distribution. While most existing methods aredesigned for modeling continuous data, a few efforts have extended diffusion models to discrete datadomains . Recently, diffusion models have also found utility in scientific discovery ,particularly in protein design .",
  "Schrdinger bridge problem": "The Schrdinger bridge (SB) problem is a classical entropy-regularized optimal transport problem . Given a data distribution, a prior distribution, and a reference stochastic process betweenthem, solving the SB problem amounts to finding the closest process to the reference in termsof Kullback-Leibler divergence on path spaces. This concept exhibits fundamental similarities todiffusion models , particularly in the field of unconditional generative modeling ,where the prior distribution assumes the form of Gaussian noise. Notably, SB formalism offers ageneral framework for approximating the reference stochastic process by training on coupled samplesfrom two continuous distributions . The recently proposed Markov bridge has broadened the scope of the SB, enabling it to model categorical distributions. In this work, wepresent the first diffusion bridge model for inverse protein folding.",
  "Problem formulation and notation": "Generally, a protein can be represented as a pair of amino acid sequence and structure (y, s), wherey = [y1, y2, . . . , yn] denotes its sequence of n residues with yi {1, 2, . . . , 20} indicating the typeof the i-th residue, and s = [s1, s2, . . . , sn] Rn43 denotes its structure with si representing theCartesian coordinates of the i-th residues backbone atoms (i.e., N, C-, and C, with O optionally).The inverse protein folding problem aims to automatically identify the protein sequence y that canfold into the given structure s. Given that homologous proteins invariably exhibit similar structures,the solution for a given structure is not unique . Hence, an ideal model, parameterized by ,should be capable of learning the underlying mapping from protein backbone structures to theircorresponding sequence distributions p(y|s).",
  "Markov bridge models": "Markov bridge model is a general framework for learning the probabilistic dependency betweentwo intractable discrete-valued distributions pX and pY. For a pair of samples (x, y) pX,Y(x, y),it defines a Markov process pinned to fixed start and end points z0 = x and zT = y through asequence of random variables (zt)Tt=0 that satisfies the Markov property,p(zt|z0, z1, . . . , zt1, y) = p(zt|zt1, y).(1)To pin the process at the end point zT = y, we have an additional requirement,p(zT = y|zT 1, y) = 1.(2)Assuming that both pX and pY are categorical distributions with a finite sample space {1, . . . , K}, wecan represent data points as one-hot vectors: x, y, zt {0, 1}K, and define the transition probabilities(Equation 1) as follows,p(zt+1|zt, y) = Cat (zt+1; Qtzt) ,(3)where Cat( ; p) is a categorical distribution with probabilities given by p, and Qt is a transitionmatrix parameterized asQt := Qt(y) = tIK + (1 t)y1K,(4)where t is a schedule parameter transitioning from 0 = 1 to T 1 = 0. It is easy to see that zt canbe efficiently sampled from p(zt+1|z0, zT ) = Catzt+1; Qtz0with a cumulative product matrix",
  "Qt = QtQt1...Q0 = tIK + (1 t)y1K, where t = ts=0 s": "TrainingUsing the finite set of coupled samples {(xi, yi)}Di=1 pX,Y, Markov bridge modellearns to sample y when only x is available by approximating y with a neural network :y = (zt, t),(5)and defining an approximated transition kernel,q(zt+1|zt) = Cat (zt+1; Qt(y)zt) .(6) is trained by optimizing the variational bound on negative log-likelihood log q(y|x), which hasthe following closed-form expression, log q(y|x) T EtU(0,...,T 1) Eztp(zt|x,y)DKL (p(zt+1|zt, y)q(zt+1|zt))Lt",
  "Methods": "In this section, we introduce Bridge-IF, a Markov bridge-based model for inverse protein folding. shows an overview of our proposed Bridge-IF. Due to space limitation, we present thedetailed algorithm in Appendix A. To begin, we describe how to extend Markov bridge techniques tofacilitate the inverse protein folding task. Next, we propose a simplified training objective. Finally,we elucidate how to modulate pre-trained PLMs with structural conditions to approximate the Markovbridge process.",
  "Overview of Bridge-IF": "We frame the inverse protein folding problem as a generative problem of modeling a stochasticprocess between the distributions of backbone structures pS(s) and protein sequences pY(y). Aspreviously discussed, diffusion bridge models, with their general properties of an unrestrictedprior form, serves as an ideal substitution for diffusion models in the presence of a well-definedinformative prior. Regrettably, to the best of our knowledge, no existing method can directly modelthe dependency between two distinct types of distributions: specifically, the continuous sourcedistribution of backbone structures and the discrete target distribution of protein sequences. To reconcile the differences between source and target distributions and streamline the modelingprocess, we propose introducing a discrete proposal distribution to serve as a deterministic prior. Weparameterize the proposal distribution using a structure encoder E : S X that is supervised byground-truth target sequences. Recent advancements have demonstrated that an expressive encoderis capable of directly predicting pretty good protein sequences in a one-shot manner . Thisapproach enables us to utilize structural information more effectively, rather than simply employingit to guide the denoising process as in previous diffusion-based methods like GraDe-IF . In thiswork, we will take the discriminative model PiFold as the structure encoder to produce a cleanand deterministic prior x = E(s). Upon this deterministic mapping from structure to sequence, wesimplify the originally complex problem of modeling p(s, y) into the more tractable problem ofmodeling p(x, y). Then, we build a Markov bridge between the prior sequence and thenative sequence to model the stochastic process, leading to a data-to-data process. As depicted in thelower half of , each sampling step progressively refines the prior sequence, which containssignificant information about the target sequence, ultimately resulting in a more precise prediction. Recall that the Markov bridge models are typically trained by optimizing the variational boundon negative log-likelihood log q(y|x) (Equation 7), which is analytically complicated and hard tooptimize in practice . Therefore, we here propose a reparameterization perspective on Markovbridge models, deriving a simplified loss function for easier optimization (4.2). We build Markov bridges in the sequence space, treating the sequence representation as a set ofindependent categorical random variables. To model the Markov bridge process, Qt is appliedseparately to each residue within a protein sequence. Motivated by the impressive advancements inPLMs for understanding and generating proteins , we advocate for employing PLMsto approximate the Markov bridge process. This approach capitalizes on the emergent evolutionaryknowledge of proteins, learned from an extensive dataset of protein sequences. Additionally, weutilize the latent structural features extracted by the structure encoder to prompt PLMs, therebyguiding the generation of structurally coherent proteins. Formally, the final state of the Markovbridge process is approximated by y = (zt, s, t), foregoing the use of Equation 5. We investigatethe integration of conditional information, such as timestep and structure, into PLMs, focusing onpreserving their emergent knowledge and achieving parameter-efficient training (4.3).",
  "where t = 1 t": "The full derivation is provided in C. This derived expression of Lt() formulates the training lossas a re-weighted standard multi-class cross-entropy loss function, which is computed over tokensthat have not been transformed to the ground truth y = zT . Following Ho et al. , we set t toa constant 1 in practice. Compared to the simpler cross-entropy loss calculated across all tokens,this new formulation places greater weight on tokens that require refinement. On the other hand, itis conceptually simpler than the original training loss (Equation 7), which requires calculating thecomplicated KL divergence between two categorical distributions DKL[p(zt+1|zt, y)q(zt+1|zt)].",
  "Network architecture design space": "We adopt pre-trained PLMs as the base network to approximate the final state of the Markovbridge process. Typically, PLMs exclusively take protein sequences as input during the pre-trainingstage, making it non-trivial to integrate timestep and structural conditions into the PLMs. Hence,we innovatively tailor the Transformer blocks to effectively capture timestep and structuralinformation, as depicted in . To facilitate efficient training, the architecture of our modelis delicately designed for compatibility with the pre-trained weights. Our exposition emphasizesfundamental principles and the corresponding modifications to the base network.",
  ": Model architecture of Bridge-IF": "Inspired by DiT , we explore replacing stan-dard layer norm layers in transformer blockswith adaptive layer norm (adaLN) to modulatethe normalizations output based on both thetimestep of the Markov bridge process and thebackbone structure. The key idea is to regressthe dimension-wise scale and shift parameters and of the layer norm from the sum of thetimestep embedding and the pooled structurerepresentation. In our situation, meaningful pre-trained parameters and are readily accessi-ble. Upon commencing the fine-tuning stage, itis crucial that these parameters are close to thepre-trained values to preserve the effectivenessof the original model, since a poor initializationcould significantly deteriorate performance. Forsimplicity, we propose to predict bias and on the frozen original scalars and initializethe multi-layer perception (MLP) to output thezero-vector for all and . We term theproposed variant of adaLN as adaLN-Bias. : Results comparison on the CATH dataset. Benchmarked results are quoted from Hsu et al., Zheng et al. , Yi et al. , Gao et al. . : Single-chain in Hsu et al. is defineddifferently. The best and suboptimal results are labeled with bold and underline.",
  "Structural adapter": "Considering that the pooled structure representation might only retain coarse-grained information,the network could consequently lack a detailed understanding of the structure input and necessitateinformation derived from original structural features to compensate. We incorporate a multi-headcross-attention module to the transformer block, enabling the network to flexibly interact with thestructural features extracted from the structure encoder . To facilitate pre-trained weights, wefurther integrate it into a bottleneck adapter layer with residual connection, preserving the inputfor the subsequent layers.",
  "Experiments": "In this section, we first demonstrate the effectiveness of our Bridge-IF on the standard CATHbenchmark . Next, we assess Bridge-IF for its applicability in de novo protein design. Moreover,we conduct several ablation studies to empirically justify the key design choices. Further resultspertaining to the design of multi-chain protein complexes can be found in Appendix B.1.",
  "Experimental protocol": "Training setupWe conduct experiments on both CATH v4.2 and CATH v4.3, where proteinsare categorized based on the CATH hierarchical classification of protein structure, to ensure acomprehensive analysis. Following the standard data splitting provided by Ingraham et al. ,CATH v4.2 dataset consists of 18,024 proteins for training, 608 proteins for validation, and 1,120proteins for testing. Following the standard data splitting provided by Hsu et al. , CATH v4.3dataset consists of 16,153 proteins for training, 1,457 proteins for validation, and 1,797 proteinsfor testing. For a fair comparison with iterative models , we use pre-trained PiFold topropose the prior distribution. We use the cosine schedule with number of timestep T = 25.The model is trained up to 50 epochs by default on an NVIDIA 3090. We used the same training",
  "settings as ProteinMPNN , where the batch size was set to approximately 6000 residues, and Adamoptimizer with noam learning rate scheduler was used": "BaselinesWe compare Bridge-IF with several state-of-the-art baselines, categorized into threegroups: (1) autoregressive models, including StructGNN , GraphTrans , GCA , GVP ,AlphaDesign , ESM-IF , and ProteinMPNN ; (2) the one-shot model, PiFold ; (3)iterative models, including LM-Design , KW-Design , and diffusion-based GraDe-IF . EvaluationWe evaluate the generative quality using perplexity and recovery rate. Followingprevious studies , we report perplexity and median recovery rate on three settings, namelyshort proteins (length 100), single-chain proteins (labeled with 1 chain in CATH), and all proteins.",
  "Inverse folding": "The performance of Bridge-IF, compared to competitive baselines, is summarized in . Bridge-IF demonstrates superior performance over previous methods. We highlight the following: (1)Iterative models comprehensively surpass the previously dominant autoregressive and one-shotmethods. (2) Our Bridge-IF outperforms LM-Design and KW-Design with the same pre-trainedPLMs, supporting our hypothesis that the iterative refinement process should be modeled in aprobabilistic framework. (3) Compared with diffusion-based GraDe-IF, our Bridge-IF achieves betterperformance with fewer diffusion steps (25 vs. 500), demonstrating that our bridge-based formulationcan better leverage the structural prior.",
  ": Performance comparison w.r.t. modelscales of pLMs using ESM-2 series on CATH 4.3": "Following Zheng et al. , we also study theimpact of the scale of PLMs on CATH v4.3.We use ESM-2 series, with parameters rang-ing from 8M to 3B. As depicted in ,the performance of Bridge-IF improves withmodel scaling, exhibiting a distinct scaling lawin logarithmic scale. Using ESM-2 at the samescale, we observe that Bridge-IF consistentlyobtains greater enhancements relative to LM-Design. Besides, Bridge-IF does not exhibit anyperformance degradation, even when the small-est model (i.e, ESM-2 8M) is employed. Re-markably, the largest ESM2-3B-based variant ofBridge-IF attains a record-setting recovery rateof 61.27% on CATH v4.3.",
  "Bridge-IF0.8154.08": "While perplexity and recovery rate serve as ef-fective proxy metrics, it is imperative to recog-nize that these measurements may not accuratelyreflect the foldability of the designed proteinsequences in real-world scenarios .Given that wet-lab assessment is extremelycostly, we leverage the in silico structure predic-tion model ESMFold , to evaluate whetherour designs can adhere to the structure condi-tion. Here we assess the agreement of the nativestructures with the predicted structures using theTM-score , and follow the evaluation con-figurations as in Wang et al. . Specifically,we use the small, high-quality test set of 82 sam-ples curated by Wang et al. and randomlygenerate 100 sequences for each structure.",
  "De novo protein design": "Thus far, our experiments have been limited to accurate experimentally-determined structures. How-ever, in real-world applications like de novo protein design, inverse folding models are commonlyused to design sequences for novel structures generated by backbone generation models .Consequently, we next evaluate Bridge-IF for its potential in such a scenario. The experimentalmethodology is detailed as follows: we sample 10 backbones at every length [100, 105, . . . , 500] inintervals of 5 using Chroma . For each de novo structure, we employ inverse folding models todesign 8 sequences. Subsequently, these sequences are folded using ESMFold to identify the sequencewith the highest TM-score (scTM). We compare Bridge-IF with ProteinMPNN , which is widelyused in de novo protein design . Our results show that Bridge-IF surpasses ProteinMPNN interms of scTM (0.73 vs. 0.69) and designability (0.85 vs. 0.80), using scTM > 0.5 as the criterion.",
  "Prior": "We investigate two training strategies distinguished by their prior: 1) the structure encoder and thePLM are jointly trained; 2) the structure encoder is first pre-trained and remains frozen during thesubsequent training of the PLM. We noted that the structure encoder is trained with an equivalentobjective in both strategies. The latter consistently yields higher-quality protein sequences. Hence, ithas been established as our default configuration.",
  "Conclusion": "In this work, we introduce Bridge-IF, the first diffusion bridge model based on the Markov bridgeprocess for inverse protein folding. Bridge-IF can gradually generate high-quality protein sequencesfrom a deterministic prior. Bridge-IF achieves state-of-the-art performance in sequence recoveryand foldability. Future work will focus on investigating more advanced structural encoders and pre-training Bridge-IF using more protein structure data predicted by AlphaFold2 to furtherenhance performance. We also intend to apply Bridge-IF to guide protein engineering aimed atdesigning novel functional proteins. One potential limitation of the proposed Bridge-IF is its lackof validation through wet-lab experiments in practical applications.",
  "and Disclosure of Funding": "This research was partially supported by National Natural Science Foundation of China undergrants No.12326612, Zhejiang Key R&D Program of China under grant No. 2023C03053 and No.2024SSYS0026, Alibaba Research Intern Program. Sarah Alamdari, Nitya Thakkar, Rianne van den Berg, Alex Xijie Lu, Nicolo Fusi, Ava PardisAmini, and Kevin K Yang. Protein generation with evolutionary diffusion: sequence is all youneed. bioRxiv, pages 202309, 2023. Rebecca F Alford, Andrew Leaver-Fay, Jeliazko R Jeliazkov, Matthew J OMeara, Frank PDiMaio, Hahnbeom Park, Maxim V Shapovalov, P Douglas Renfrew, Vikram K Mulligan, KalliKappel, et al. The rosetta all-atom energy function for macromolecular modeling and design.Journal of chemical theory and computation, 13(6):30313048, 2017. Jacob Austin, Daniel D Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg.Structured denoising diffusion models in discrete state-spaces. Advances in Neural InformationProcessing Systems, 34:1798117993, 2021.",
  "Alexander E Chu, Tianyu Lu, and Po-Ssu Huang. Sparks of function by de novo protein design.Nature Biotechnology, 42(2):203215, 2024": "Justas Dauparas, Ivan Anishchenko, Nathaniel Bennett, Hua Bai, Robert J Ragotte, Lukas FMilles, Basile IM Wicky, Alexis Courbet, Rob J de Haas, Neville Bethel, et al. Robust deeplearningbased protein sequence design using proteinmpnn. Science, 378(6615):4956, 2022. Valentin De Bortoli, James Thornton, Jeremy Heng, and Arnaud Doucet. Diffusion schrdingerbridge with applications to score-based generative modeling. Advances in Neural InformationProcessing Systems, 34:1769517709, 2021.",
  "Zhangyang Gao, Cheng Tan, and Stan Z. Li. Pifold: Toward effective and efficient proteininverse folding. In The Eleventh International Conference on Learning Representations, 2023.URL": "Zhangyang Gao, Cheng Tan, Yijie Zhang, Xingran Chen, Lirong Wu, and Stan Z. Li. Pro-teininvbench: Benchmarking protein inverse folding on diverse tasks, models, and metrics. InThirty-seventh Conference on Neural Information Processing Systems Datasets and BenchmarksTrack, 2023. URL Zhangyang Gao, Cheng Tan, Xingran Chen, Yijie Zhang, Jun Xia, Siyuan Li, and Stan Z. Li.KW-design: Pushing the limit of protein design via knowledge refinement. In The TwelfthInternational Conference on Learning Representations, 2024. URL Nate Gruver, Samuel Don Stanton, Nathan C. Frey, Tim G. J. Rudner, Isidro Hotzel, JulienLafrance-Vanasse, Arvind Rajpal, Kyunghyun Cho, and Andrew Gordon Wilson. Protein designwith guided discrete diffusion. In Thirty-seventh Conference on Neural Information ProcessingSystems, 2023. URL Tymor Hamamsy, James T Morton, Robert Blackwell, Daniel Berenberg, Nicholas Carriero,Vladimir Gligorijevic, Charlie EM Strauss, Julia Koehler Leman, Kyunghyun Cho, and RichardBonneau. Protein remote homology detection and structural alignment using deep learning.Nature biotechnology, pages 111, 2023.",
  "Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, andDavid J Fleet. Video diffusion models. Advances in Neural Information Processing Systems,35:86338646, 2022": "Lars Holdijk, Yuanqi Du, Priyank Jaini, Ferry Hooft, Bernd Ensing, and Max Welling. Pathintegral stochastic optimal control for sampling transition paths. In ICML 2022 2nd AI forScience Workshop, 2022. Emiel Hoogeboom, Alexey A. Gritsenko, Jasmijn Bastings, Ben Poole, Rianne van den Berg,and Tim Salimans. Autoregressive diffusion models. In International Conference on LearningRepresentations, 2022. URL Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe,Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learningfor nlp. In International conference on machine learning, pages 27902799. PMLR, 2019. Chloe Hsu, Robert Verkuil, Jason Liu, Zeming Lin, Brian Hie, Tom Sercu, Adam Lerer, andAlexander Rives. Learning inverse folding from millions of predicted structures. In Internationalconference on machine learning, pages 89468970. PMLR, 2022.",
  "John Ingraham, Vikas Garg, Regina Barzilay, and Tommi Jaakkola. Generative models forgraph-based protein design. Advances in neural information processing systems, 32, 2019": "John B Ingraham, Max Baranov, Zak Costello, Karl W Barber, Wujie Wang, Ahmed Ismail,Vincent Frappier, Dana M Lord, Christopher Ng-Thow-Hing, Erik R Van Vlack, et al. Illumi-nating protein space with a programmable generative model. Nature, 623(7989):10701078,2023. Bowen Jing, Stephan Eismann, Patricia Suriana, Raphael John Lamarre Townshend, and RonDror. Learning from protein structure with geometric vector perceptrons. In InternationalConference on Learning Representations, 2021. URL John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ron-neberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin dek, Anna Potapenko, et al.Highly accurate protein structure prediction with alphafold. Nature, 596(7873):583589, 2021.",
  "Christian Lonard. A survey of the schrdinger problem and some of its connections withoptimal transport. Discrete & Continuous Dynamical Systems-A, 34(4):15331574, 2014": "Xiang Li, John Thickstun, Ishaan Gulrajani, Percy S Liang, and Tatsunori B Hashimoto.Diffusion-lm improves controllable text generation. Advances in Neural Information ProcessingSystems, 35:43284343, 2022. Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin,Robert Verkuil, Ori Kabeli, Yaniv Shmueli, et al. Evolutionary-scale prediction of atomic-levelprotein structure with a language model. Science, 379(6637):11231130, 2023. Guan-Horng Liu, Arash Vahdat, De-An Huang, Evangelos A Theodorou, Weili Nie, and AnimaAnandkumar. I2sb: image-to-image schrdinger bridge. In Proceedings of the 40th InternationalConference on Machine Learning, pages 2204222062, 2023.",
  "Aaron Lou, Chenlin Meng, and Stefano Ermon. Discrete diffusion modeling by estimating theratios of the data distribution. In Forty-first International Conference on Machine Learning,2024. URL": "Ali Madani, Ben Krause, Eric R Greene, Subu Subramanian, Benjamin P Mohr, James M Holton,Jose Luis Olmos, Caiming Xiong, Zachary Z Sun, Richard Socher, et al. Large language modelsgenerate functional protein sequences across diverse families. Nature Biotechnology, 41(8):10991106, 2023. Weian Mao, Muzhi Zhu, Zheng Sun, Shuaike Shen, Lin Yuanbo Wu, Hao Chen, and ChunhuaShen. De novo protein design using geometric vector field networks. In The Twelfth InternationalConference on Learning Representations, 2024. URL",
  "Yuyang Shi, Valentin De Bortoli, Andrew Campbell, and Arnaud Doucet. Diffusion schrdingerbridge matching. Advances in Neural Information Processing Systems, 36, 2024": "Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsuper-vised learning using nonequilibrium thermodynamics. In International conference on machinelearning, pages 22562265. PMLR, 2015. Vignesh Ram Somnath, Matteo Pariset, Ya-Ping Hsieh, Maria Rodriguez Martinez, AndreasKrause, and Charlotte Bunne. Aligned diffusion schrdinger bridges. In Uncertainty in ArtificialIntelligence, pages 19851995. PMLR, 2023. Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, andBen Poole. Score-based generative modeling through stochastic differential equations. InInternational Conference on Learning Representations, 2021. URL Cheng Tan, Zhangyang Gao, Jun Xia, Bozhen Hu, and Stan Z Li. Global-context awaregenerative protein design. In ICASSP 2023-2023 IEEE International Conference on Acoustics,Speech and Signal Processing (ICASSP), pages 15. IEEE, 2023.",
  "Francisco Vargas, Pierre Thodoroff, Austen Lamacraft, and Neil Lawrence. Solving schrdingerbridges via maximum likelihood. Entropy, 23(9):1134, 2021": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan NGomez, ukasz Kaiser, and Illia Polosukhin.Attention is all you need.In I. Guyon,U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, ed-itors, Advances in Neural Information Processing Systems, volume 30. Curran Associates,Inc., 2017. URL Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural informationprocessing systems, 30, 2017.",
  "Gefei Wang, Yuling Jiao, Qian Xu, Yang Wang, and Can Yang. Deep generative learning viaschrdinger bridge. In International conference on machine learning, pages 1079410804.PMLR, 2021": "Hanchen Wang, Tianfan Fu, Yuanqi Du, Wenhao Gao, Kexin Huang, Ziming Liu, PayalChandak, Shengchao Liu, Peter Van Katwyk, Andreea Deac, et al. Scientific discovery in theage of artificial intelligence. Nature, 620(7972):4760, 2023. Joseph L Watson, David Juergens, Nathaniel R Bennett, Brian L Trippe, Jason Yim, Helen EEisenach, Woody Ahern, Andrew J Borst, Robert J Ragotte, Lukas F Milles, et al. De novodesign of protein structure and function with rfdiffusion. Nature, 620(7976):10891100, 2023.",
  "Kai Yi, Bingxin Zhou, Yiqing Shen, Pietro Lio, and Yu Guang Wang. Graph denoising diffusionfor inverse protein folding. In Thirty-seventh Conference on Neural Information ProcessingSystems, 2023. URL": "Jason Yim, Brian L Trippe, Valentin De Bortoli, Emile Mathieu, Arnaud Doucet, ReginaBarzilay, and Tommi Jaakkola. Se (3) diffusion model with application to protein backbonegeneration. In International Conference on Machine Learning, pages 4000140039. PMLR,2023. Mingze Yin, Hanjing Zhou, Yiheng Zhu, Miao Lin, Yixuan Wu, Jialu Wu, Hongxia Xu, Chang-Yu Hsieh, Tingjun Hou, Jintai Chen, et al. Multi-modal clip-informed protein editing. arXivpreprint arXiv:2407.19296, 2024.",
  "The overall workflow of the training and sampling process are provided in Algorithm 1 and Algo-rithm 2": "Algorithm 1 Training of the Bridge-IFInput: coupled sample (s, y) pS,Y, structure encoder E, neural network x = E(s) Deterministic mapping from structure to sequencet U(0, . . . , T 1), zt Catzt; Qt1x Sample time step and intermediate statey (zt, t) Output of is a vector of probabilitiesp(zt+1|zt, y) Cat (zt+1; Qt(y)zt) Reference transition distributionq(zt+1|zt) Cat (zt+1; Qt(y)zt) Approximated transition distributionMinimize DKL (p(zt+1|zt, y)q(zt+1|zt)) Algorithm 2 SamplingInput: starting point s pS, structure encoder E, neural network z0 E(s)for t in 0, ..., T 1:y (zt, t) Output of is a vector of probabilitiesq(zt+1|zt) Cat (zt+1; Qt(y)zt) Approximated transition distributionzt+1 q(zt+1|zt)Return zT",
  "Bridge-IF (pretrained PiFold:freeze + ESM-2 650M)61.26": "Studying protein sequence design for multi-chain assemble structures is crucial for drugdesign. Next, we assess the capabilities of de-signing multi-chain complexes using the PDBdataset curated by Dauparas et al. , wheresequences were clustered at 30% identity, re-sulting in 25,361 clusters. Following the stan-dard data splitting, we divided those clusters ran-domly into three groups for training (23,358),validation (1,464), ensuring that neither thechains from the target chain nor the chains fromthe biounits of the target chain would be presentin the other two groups.",
  "DBroader impacts": "Inverse protein folding models, operating within the broader realm of bioinformatics and computa-tional biology, have significant impacts across various scientific and practical domains. These models,by enabling the design or prediction of protein sequences that fold into specific three-dimensionalstructures, foster advancements in numerous fields. The broader impacts encompass several areas,including drug discovery, enzyme design, and synthetic biology.",
  "The answer NA means that the paper has no limitation while the answer No means thatthe paper has limitations, but those are not discussed in the paper": "The authors are encouraged to create a separate \"Limitations\" section in their paper. The paper should point out any strong assumptions and how robust the results are toviolations of these assumptions (e.g., independence assumptions, noiseless settings,model well-specification, asymptotic approximations only holding locally). The authorsshould reflect on how these assumptions might be violated in practice and what theimplications would be. The authors should reflect on the scope of the claims made, e.g., if the approach wasonly tested on a few datasets or with a few runs. In general, empirical results oftendepend on implicit assumptions, which should be articulated. The authors should reflect on the factors that influence the performance of the approach.For example, a facial recognition algorithm may perform poorly when image resolutionis low or images are taken in low lighting. Or a speech-to-text system might not beused reliably to provide closed captions for online lectures because it fails to handletechnical jargon.",
  "If applicable, the authors should discuss possible limitations of their approach toaddress problems of privacy and fairness": "While the authors might fear that complete honesty about limitations might be used byreviewers as grounds for rejection, a worse outcome might be that reviewers discoverlimitations that arent acknowledged in the paper. The authors should use their bestjudgment and recognize that individual actions in favor of transparency play an impor-tant role in developing norms that preserve the integrity of the community. Reviewerswill be specifically instructed to not penalize honesty concerning limitations.",
  ". Experimental Result Reproducibility": "Question: Does the paper fully disclose all the information needed to reproduce the main ex-perimental results of the paper to the extent that it affects the main claims and/or conclusionsof the paper (regardless of whether the code and data are provided or not)?Answer: [Yes]Justification: See 5.Guidelines: The answer NA means that the paper does not include experiments. If the paper includes experiments, a No answer to this question will not be perceivedwell by the reviewers: Making the paper reproducible is important, regardless ofwhether the code and data are provided or not.",
  "If the contribution is a dataset and/or model, the authors should describe the steps takento make their results reproducible or verifiable": "Depending on the contribution, reproducibility can be accomplished in various ways.For example, if the contribution is a novel architecture, describing the architecture fullymight suffice, or if the contribution is a specific model and empirical evaluation, it maybe necessary to either make it possible for others to replicate the model with the samedataset, or provide access to the model. In general. releasing code and data is oftenone good way to accomplish this, but reproducibility can also be provided via detailedinstructions for how to replicate the results, access to a hosted model (e.g., in the caseof a large language model), releasing of a model checkpoint, or other means that areappropriate to the research performed. While NeurIPS does not require releasing code, the conference does require all submis-sions to provide some reasonable avenue for reproducibility, which may depend on thenature of the contribution. For example(a) If the contribution is primarily a new algorithm, the paper should make it clear howto reproduce that algorithm.",
  "(b) If the contribution is primarily a new model architecture, the paper should describethe architecture clearly and fully": "(c) If the contribution is a new model (e.g., a large language model), then there shouldeither be a way to access this model for reproducing the results or a way to reproducethe model (e.g., with an open-source dataset or instructions for how to constructthe dataset). (d) We recognize that reproducibility may be tricky in some cases, in which caseauthors are welcome to describe the particular way they provide for reproducibility.In the case of closed-source models, it may be that access to the model is limited insome way (e.g., to registered users), but it should be possible for other researchersto have some path to reproducing or verifying the results.",
  "Guidelines:": "The answer NA means that the paper does not use existing assets. The authors should cite the original paper that produced the code package or dataset. The authors should state which version of the asset is used and, if possible, include aURL. The name of the license (e.g., CC-BY 4.0) should be included for each asset. For scraped data from a particular source (e.g., website), the copyright and terms ofservice of that source should be provided. If assets are released, the license, copyright information, and terms of use in thepackage should be provided. For popular datasets, paperswithcode.com/datasetshas curated licenses for some datasets. Their licensing guide can help determine thelicense of a dataset.",
  "According to the NeurIPS Code of Ethics, workers involved in data collection, curation,or other labor should be paid at least the minimum wage in the country of the datacollector": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with HumanSubjectsQuestion: Does the paper describe potential risks incurred by study participants, whethersuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)approvals (or an equivalent approval/review based on the requirements of your country orinstitution) were obtained?Answer: [NA]Justification: The paper does not involve crowdsourcing nor research with human subjects.Guidelines:",
  "The answer NA means that the paper does not involve crowdsourcing nor research withhuman subjects": "Depending on the country in which research is conducted, IRB approval (or equivalent)may be required for any human subjects research. If you obtained IRB approval, youshould clearly state this in the paper. We recognize that the procedures for this may vary significantly between institutionsand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and theguidelines for their institution."
}