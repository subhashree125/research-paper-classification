{
  "Abstract": "Remote photoplethysmography (rPPG) enables non-invasive extraction of blood volume pulsesignals through imaging, transforming spatial-temporal data into time series signals. Advances inend-to-end rPPG approaches have focused on this transformation where attention mechanismsare crucial for feature extraction. However, existing methods compute attention disjointly acrossspatial, temporal, and channel dimensions. Here, we propose the Factorized Self-AttentionModule (FSAM), which jointly computes multidimensional attention from voxel embeddingsusing nonnegative matrix factorization. To demonstrate FSAMs effectiveness, we developed Fac-torizePhys, an end-to-end 3D-CNN architecture for estimating blood volume pulse signals fromraw video frames. Our approach adeptly factorizes voxel embeddings to achieve comprehensivespatial, temporal, and channel attention, enhancing performance of generic signal extraction tasks.Furthermore, we deploy FSAM within an existing 2D-CNN-based rPPG architecture to illustrateits versatility. FSAM and FactorizePhys are thoroughly evaluated against state-of-the-art rPPGmethods, each representing different types of architecture and attention mechanism. We performablation studies to investigate the architectural decisions and hyperparameters of FSAM. Experi-ments on four publicly available datasets and intuitive visualization of learned spatial-temporalfeatures substantiate the effectiveness of FSAM and enhanced cross-dataset generalization in esti-mating rPPG signals, suggesting its broader potential as a multidimensional attention mechanism.The code is accessible at",
  "Introduction": "Attention mechanisms in computer vision are inspired by the human ability to identify salient regionsin complex scenes. Such mechanisms can be interpreted as a dynamic weight adjustment process thatselects useful features and disregards irrelevant ones in a multidimensional feature space. Recent surveys provide a comprehensive overview of attention mechanisms and distinctly categorize existing attentionmechanisms. Amidst a spectrum of research from convolution block attention to computationallyintensive multi-head attention , an effective, yet computation and memory efficient, attention mechanismhas remained desirable for real-world applications. Matrix decomposition , a dimensionalityreduction technique, has captured the interest of researchers and has been explored in deep learning researchfor different objectives . This work investigates nonnegative matrix factorization (NMF),a matrix decomposition technique, for its potential to efficiently perform multidimensional attention andevaluates its effectiveness in the spatial-temporal context of estimating rPPG signal from video frames.",
  "arXiv:2411.01542v1 [cs.CV] 3 Nov 2024": "Verkruysse s pioneering investigation on extracting photoplethysmography (PPG) or blood volumepulse (BVP) signals from RGB cameras in a contactless manner led to an exciting research field of imaging-based physiological sensing. There exist several potential applications and contexts of noninvasive andcontactless measurement techniques, such as stress and mental workload recognition , driver drowsinessmonitoring and social biofeedback interaction . The seminal works on unsupervised rPPG methods either used video frames acquired under stationary conditions or performed skin segmentation or region of interest (RoI) tracking as a preprocessing step. This preprocessing step can be consideredas a basic form of attention mechanism that enables the unsupervised models to process only the relevantregions. Some of the supervised rPPG methods, including HR-CNN , RhythmNet , NAS-HR ,PulseGAN , and Dual-GAN also relied on extracting spatial-temporal features from the tracked RoIsas a preprocessing step.As end-to-end rPPG methods, such as DeepPhys , and MTTS-CAN among several others, takewhole facial frames as input, they rely on attention mechanisms that enable models to emphasize the relevantspatial-temporal features. Estimating BVP signal from raw facial video frames in an end-to-end manner istherefore an interesting downstream task to investigate the attention mechanism in multidimensional featurespace. This requires networks to learn to pick the spatial features having the desired temporal signature, whilediscarding the variance related to head-motion, illumination, and skin-tones, thus representing one of the chal-lenging spatial-temporal tasks. Few other notable end-to-end rPPG methods include PhysNet , 3DCNN ,SAM-rPPGNet , RTrPPG , and transformer-network-based methods such as PhysFormer , Phys-Former++ , EfficientPhys , JAMSNet , and GLISNet . A recent survey article on visualcontactless physiological monitoring in clinical settings highlights susceptibility to disturbance, such ashead movement, as one of the key challenges, among others. Some of the recent end-to-end rPPG methods further highlight the need for multidimensional attention, as squeezing features in selective dimensionsfor deriving attention reduces the feature space to a single dimension and is therefore not well suited for thetask of signal extraction.To address this, our work draws inspiration from the seminal work on NMF which a recent workformulated as an approach to design the global information block, referred to as Hamburger . Hamburger implements NMF to derive low-rank embeddings, which serve as a global context block. Despite the lowcomputational complexity of O(n), Hamburger outperformed various attention modules in the semanticsegmentation and image generation tasks. In addition, researchers have combined matrix factorizationwith deep architectures in several ways for different applications such as layer-wise learning of dictionary forclassification and clustering , adaptive learning of dictionary for image denoising , multi-attentionmodel for recommendation systems , and linearly scalable approach to context modeling for medicalimage segmentation among several others. Drawing inspiration from these studies, especially those thatuse matrix factorization to model global context in vision tasks, we investigate the application of NMFas a multidimensional attention block. Although matrix factorization in deep learning has remained a topic ofsignificant interest, it has not been investigated in the realm of rPPG, which stands to gain from joint spatial,temporal, and channel attention.We introduce the Factorized Self-Attention Module (FSAM), which implements NMF to jointly computespatial-temporal attention and describe an appropriate formulation for the low-rank recovery problem. Toinvestigate the relevance and effectiveness of FSAM in computing multidimensional attention, we build a3D-CNN architecture FactorizePhys that implements FSAM. We further adapt FSAM for EfficientPhys ,an end-to-end rPPG architecture that builds on the Temporal Shift Module (TSM) , to uniquely learnspatial-temporal features using 2D-CNN layers. Evaluation of FactorizePhys and EfficientPhys withFSAM, against existing SOTA rPPG methods, demonstrates the versatility of FSAM as multidimensionalattention along with its effectiveness for the downstream task of estimating time series from spatial-temporaldata. In summary, we make the following contributions. Factorized Self-Attention Module (FSAM): NMF -based novel approach that jointly computes multidi-mensional attention within voxel embeddings.",
  "Attention Mechanisms in Vision": "Varied forms of attention mechanisms have been successful in different visual tasks such as image classification, object detection , semantic segmentation , video understanding , 3D vision , and multimodal tasks among others . The most widely used attentionmechanisms are channel attention , spatial attention , temporal attention , self-attentionor transformer-based approaches , multimodal attention , graph-based approaches , aswell as different combinations of these types . In addition, researchers have proposed attentionmechanisms for video understanding as well as 3D vision . Despite notable advancesin different forms of attention mechanisms, some of the existing challenges include the requirement for highcomputational costs, large training data, the overall efficiency of the model, and a cost-benefit analysis ofperformance improvement . Additionally, for rPPG research, the impact of attention mechanisms on anability of models to generalize on unseen datasets is not systematically studied, which we address in this work.",
  "Attention Mechanisms in rPPG Methods": "End-to-end rPPG methods can be categorized into convolution neural networks (CNN) architectures suchas PhysNet , EfficientPhys-C , 3DCNN , SAM-rPPGNet , and RTrPPG , and transformer-network-based architectures such as PhysFormer , PhysFormer++ , and EfficientPhys-T . Amongend-to-end rPPG methods, DeepPhys first implemented a novel convolutional attention mechanism in anarchitecture that comprised separate motion and appearance branches, with the latter intended to computeattention for the main motion branch. Inspired by the CBAM attention mechanism , originally validatedfor classification and detection tasks, ST-Attention was proposed to filter salient information from spatial-temporal maps, thus improving remote HR estimation. The similar dual attention mechanism was also foundto be effective in the SMP-Net framework , which jointly learned the features of RGB and infraredspatial-temporal maps to estimate multiple physiological signals.Recently, EfficientPhys , an end-to-end network, presented an efficient single-branch approach witha gated attention mechanism. The Swin-Transformer based version of EfficientPhys insightfullyadded the TSM module to the Swin transformer , enabling the architecture to perform efficientspatial-temporal modeling and compute attention by combining shifting window partitions spatially andshifting frames temporally. It should be noted that the convolution-based version of EfficientPhys , whichcombined the TSM module and a convolutional attention mechanism showed superior accuracyalong with significantly low latency, making it highly suitable for deployment on mobile devices. Recently,there has been an upsurge in transformer-based rPPG architectures, some of which include PhysFormer ,PhysFormer++ , TransPhys , and RADIANT .Unlike other transformer-based architectures that rely on spatial-temporal maps as input, PhysFormer and PhysFormer++ are end-to-end video transformer-based architectures, which adaptively aggregate bothlocal and global spatial-temporal features. PhysFormer++ extends PhysFormer by better exploitingtemporal contextual and periodic rPPG clues, as it extracts and fuses attentional features from slow andfast pathways. In addition, both architectures are trained using label distribution learning and acurriculum learning-inspired dynamic constraint in the frequency domain, which helps to alleviate overfitting.Although unlike convolution-based EfficientPhys , transformer architectures require significantly highercomputational resources.Most of the light-weight convolutional attention mechanisms require attention to be separately derived inspatial, temporal, and channel dimensions, which is later merged . Although 3D-CNN architecturessuch as PhysNet and iBVPNet have shown promising performances, they have not explored attentionmechanisms that can potentially enhance performance in unseen datasets. JAMSNet and GLISNet are recent 3D-CNN architectures that benefit significantly from channel-temporal joint attention (CTJA) andspatial-temporal joint attention (STJA). However, unlike CTJA and STJA , we jointly derive attentionin temporal, spatial, and channel dimensions, without squeezing any dimension of multidimensional features.",
  "Primer: Nonnegative Matrix Factorization": "Nonnegative matrix factorization is a dimensionality reduction paradigm that decomposes M N matrixV = [v1, v2, ..., vN] RMN0into nonnegative M L basis matrix W = [w1, w2, ..., wL] RML0andnonnegative L N coefficient matrix H = [h1, h2, ..., hN] RLN0, as depicted in fig. 1 and expressed as:",
  "iwiHij(2)": "The objective to represent high-dimensional matrix with fewer basis can be achieved only when L is chosensuch that L min(M, N), while when L is larger than M, it results in over-complete basis. An optimizationin W and H to achieve the optimal approximation effectively results in the discovery of inherent correlationsbetween the basis vectors in W and the corresponding coefficients in H . The optimization objectiveis formulated as:minW,HV WH2F Wml 0, Hln 0(3) Further, the imposed non-negativity constraints on W and H enable parts-based representations, whereactivation of one or many of the coefficients in H together with the basis vectors in W can reconstructdifferent interpretable parts of V . For a detailed primer on NMF, we refer the reader to the seminal work and a survey article that summarizes different NMF models and algorithms. The factorization ofdeeper layer embeddings can be elucidated as the squeeze of information without reducing the dimensionsof the embeddings, unlike the existing attention mechanisms . Therefore, exciting or multiplying withthe resultant low-rank (information squeezed) embeddings can potentially serve as an attention mechanism.While factorization is formulated for two-dimensional matrix, high-dimensional embeddings can be mappedto two-dimensional matrix. In PyTorch , this is achieved with the view operation.",
  "Factorized Self-Attention Module (FSAM)": "For the downstream task of estimating rPPG from video frames, spatial-temporal input data can be expressedas I RT CHW , where T, C, H, and W represents total frames (temporal dimension), channels in aframe (e.g., for RGB frames, C = 3), height and width of pixels in a frame, respectively. I is passed through afeature extractor that generates voxel embeddings R, with temporal (), channel () and spatial(, ) dimensions.The goal is to jointly derive the attention in the multidimensional space of , without squeezing individualdimensions. For this, we deploy NMF-based matrix factorization to compute low-rank by reconstructing itfrom the factorized basis matrix W and a coefficient matrix H. It is essential to factorize in a way that approximated through the computed basis and coefficient matrices serves as an effective self-attention. Amongseveral parameters that govern factorization, here we delve into the ones most relevant for the time seriesestimation task. These include: i) the transformation of the voxel embeddings that maps R to thefactorization matrix V st RMN and ii) the rank of the factorization. For a 2D-CNN architecture with channels and spatial features, the transformation (MN) implemented in the Hamburger module",
  "V s RMN = MN( R) M, N(4)": "where channels are mapped to M and spatial features are mapped to N, with an underlying assumptionthat spatial features are inherently correlated due to learnt CNN kernels. However, for 3D-CNN architectures,as R encodes temporal, channel, and spatial features, it is required to revisit these mappings.While it can be argued that similar to 2D-CNN architectures, as 3D-CNN architectures have 3D kernels, thespatial-temporal features are inherently correlated. However, it should be noted that the scales of spatial andtemporal dimensions are very distinct, owing to which the spatial-temporal patterns to be learned may notbe uniformly captured through typical convolutional kernels (e.g. 3 3 3). Adjusting the spatial-temporalkernel sizes can be heuristic task, and does not guarantee the extraction of desired features, while drasticallyincreasing the model complexity (since for time series estimation, >> , ). Also, channels are notinherently correlated, and therefore it is crucial to devise a multidimensional attention that jointly computesthe spatial-temporal and channel attention. To address this, we first consider negative Pearson correlation, aloss function that is commonly deployed to optimize end-to-end rPPG methods, expressed as:",
  "(5)": "where, rppg R1T is an estimated rPPG signal and gppg R1T corresponds to the ground-truth BVPsignal. The optimization of end-to-end model to estimate a vector in temporal dimension (rppg R1T ) canbe leveraged by establishing the correlation of features in spatial and channel dimensions with the featuresin temporal dimension. Factorization of a matrix that consists of vectors in temporal domain and spatial andchannel dimension as the features of the vectors, uniquely offers an opportunity to design the requisite attention.Prior to transforming to V st RMN, it is pre-processed through a convolution layer (with 1 1 1kernels), and a ReLU activation to ensure non-negativity of the embeddings. Following this preprocessing, thetemporal features of are mapped to the vector dimension (M) in V st, while spatial and channel dimensionsare mapped to the feature dimension (N) of V st. This transformation of as depicted in fig. 2, can beexpressed as:",
  "V st RMN = MN(pre( R)) M, N(6)": "where, pre represents preprocessing operation. Factorization of thus formed matrix V st with temporal vectorsshall result in a low-rank matrixV st which is approximated based on the latent structure that establishescorrelation of temporal features with spatial and channel features.V st = (V st)(7) where represents factorization operation.V st is transformed back to the embedding space, resulting inan approximated voxel embeddings that selectively retains the spatial and channel features that contributetowards the recovery of salient temporal features in . The resultant can be expressed as:",
  "= MN( V st RMN)(8)": "where MN represents matrix transformation operations. We use the one-step gradient optimizationbased approach to factorize V st. This approach is a linear approximation of the conventional back-propagation through time algorithm (for time t ) , as proposed with the Hamburger module .Approximated low-rank matrix V st is transformed back to the embedding space through MN, resultingin that can potentially serve as the requisite attention. is post-processed with a convolution layer (with1 1 1 kernels), and a ReLU activation, followed by element-wise multiplication with . This multiplicationoperation serves as an excitation operation, which can be distinctly effective as retains the dimensionof while computing the attention. The product is instance-normalized, and added with that serves asresidual connection as depicted in fig. 2. It is to be noted that for each single forward pass through the model,approximation ofV st requires 4-8 steps, however, FSAM implements NMF within no_grad block, thatdoes not require back-propagation through the NMF for model optimization. Representing the network headas , post as post-processing operation and IN as instance normalization, the estimated rppg signal can beexpressed as:rppg = ( + IN( post()))(9) Next, we look at the rank of the factorization that affects the approximation of V st. The primary considerationfor the rank L min(M, N) as mentioned in 3.1 ensures that V st is of low rank. Although the choice of Lis generally governed by the downstream task, it is often derived empirically. In the context of rP P Gestimation,we revisit the formulation of factorization matrix through MN that maps temporal features along theM dimension. As we expect only a single signal underlying source of BVP signal across all facial regions,single vector estimation vst0 corresponding to rank-1 (i.e., L = 1) shall be sufficient to capture the spatial,temporal, and channel features that contribute to the rP P G estimation. Experimentation with rank-1 and higherrank factorization (appendix A.3) shows that for the higher ranks, the performance remains at par with thatof the network without the FSAM, indicating that for rPPG estimation task, rank-1 factorization offers theoptimal multidimensional attention, confirming our understanding.",
  "We deploy FSAM in our proposed 3D-CNN model, FactorizePhys and integrate it in an existing 2D-CNNarchitecture, EfficientPhys to assess its versatility": "FactorizePhys Architecture:FactorizePhys, as depicted in fig. 3[A], is an end-to-end 3D-CNN architecturefor estimating rPPG signal from raw video frames. Skin reflection models discuss the presenceof several unrelated stationary and time-varying temporal components, and a relatively weaker pulsatilecomponent of interest. To eliminate stationary components, FactorizePhys implements a Diff as first layer,inspired by existing rPPG architectures . The resultant Diff frames are normalized with IN, unlikeexisting architectures that use BatchNorm. The size of the kernel and the strides of each convolution layer are",
  ": (A) Proposed FactorizePhys with FSAM; (B) FSAM Adapted for EfficientPhys": "depicted in fig. 3[A]. For each layer, we use TanH activation followed by IN. Spatial features are graduallyaggregated by not padding the features, while we deploy spatial convolution strides only on the 3rd and 6th layers. For temporal features, same padding retains the input temporal dimension throughout the network, asdepicted in fig. 3[A]. Downsizing of temporal features may result in high-amplitude unrelated time-varyingcomponents to outweigh the weaker rPPG related pulsatile component. To provide a clearer overview of thearchitecture of FactorizePhys, only the spatial and temporal dimensions of the features at multiple layers areshown in fig. 3[A], while the channel dimension is skipped. FSAM, as elaborated in 3.2, is deployed tojointly compute multidimensional attention, at the layer where the spatial dimension is reduced to 7 7, asreported as the optimal spatial dimension in a recent work . Adaptation of FSAM for 2D-CNN Architecture:Several SOTA rPPG methods leverage the TSM that efficiently models spatial-temporal features using 2D-CNN architectures. The parameter calledFrame Depth ( channels) controls the number of channels that are shifted along the temporaldimension for modeling temporal features. We investigated the effectiveness of the proposed FSAM with amore recent TSM-based SOTA rPPG architecture, EfficientPhys , which also deploys the Self-AttentionShifted Network (SASN) as the attention module. As controls the amount of temporal information that islearned, we use this to formulate the mapping for the factorization matrix. Equation (6) can be adapted forTSM based architectures to appropriately transform the embeddings to factorization matrix as:",
  "V tsm RMN = MN(tsm R) M, N(10)": "[B] shows modified EfficientPhys architecture, in which we drop SASN blocks and adda single FSAM. Unlike SASN , FSAM derives attention without squeezing any individual dimension,which in turn can strengthen the correlation between temporal, channel and spatial features. This adaptioncritically evaluates the proposed FSAM against its counterpart in the SOTA architecture, addressing therecommendations of a recent survey article on visual attention methods.",
  "Experiments": "We perform an evaluation with carefully selected end-to-end SOTA rPPG methods that include PhysNet, a 3D-CNN architecture without the attention mechanism, EfficientPhys , a 2D-CNN architecture withself-attention, and PhysFormer , a transformer-based 3D-CNN architecture with multi-head self-attention.Comparison of FactorizePhys and PhysNet can indicate the importance of the attention mechanism in3D-CNN rPPG architectures, while comparison of EfficientPhys with SASN and EfficientPhys withFSAM allows evaluating the effectiveness of the proposed FSAM in 2D-CNN architectures, and thus allowsassessing the versatility of FSAM. Similarly, the comparison of FactorizePhys and PhysFormer offers athorough evaluation of the proposed FSAM against multi-head self-attention in 3D-CNN architectures.Each model was trained on one of the four existing datasets that include iBVP , PURE , UBFC-rPPG and SCAMPS and evaluated on the other three. Appendix A.1 provides our detailed descriptionof these datasets. Our code is based on the rPPG-Toolbox , with specific adaptations described inappendix A.2. We train all models uniformly with 10 epochs on iBVP , PURE , and UBFC-rPPG datasets, and with one epoch on SCAMPS dataset. For fair evaluation, all model-specifichyperparameters were maintained as provided by the respective SOTA rPPG methods, while the trainingpipeline related hyperparameters, which include preprocessing steps for images and labels, batch size, numberof epochs, learning rate, scheduler, and optimizer were kept consistent for training all the models.",
  "Results and Discussion": "Ablation Study:First, we train FactorizePhys on UBFC-rPPG dataset and test on PURE and iBVP datasets, to compare different transformations of R with temporal, channel and spatialfeatures to factorization matrix V st RMN as tabulated in table 1. Superior cross-dataset generalization canbe observed when the temporal dimension, is mapped to M, as described in 3.2. We assess contribution ofFSAM over base FactorizePhys model and observe consistent performance gains with FSAM, as reported intable 3 in appendix A. We then investigate residual connection in table 3, and observe it to contribute positively.We also observed that the base FactorizePhys model trained with FSAM retains the performance gains in-spitewhen FSAM is skipped during the inference. As this eliminates the computational overhead during inference,we report our main results of FactorizePhys trained with FSAM, by running inference without the FSAM. On",
  "0.394.38 1.062.40 0.570.90 0.046.61 0.580.56 0.01": "contrary, in case of TSM based EfficientPhys model trained with FSAM, we observed performancedrop when FSAM was skipped during inference, and therefore for EfficientPhys with FSAM, we do notdrop FSAM during inference. Evaluation for factorization ranks and optimization steps to solve NMF showsconsistent superiority of rank-1 factorization in table 4 in appendix A. FactorizePhys vs. State-of-the-Art:We use heart rate (HR) along with BVP metrics that includesignal-to-noise ratio (SNR) and maximum amplitude of cross-correlation (MACC) for evaluation.SNR and MACC are direct measures to compare estimated rPPG signals with ground-truth BVP signals. TheHR metrics reported are the mean absolute error (MAE), the square root of the mean square error (RMSE), themean absolute percentage error (MAPE), and Pearsons correlation coefficient (Corr) of the estimatedHR. Uncertainty estimates quantifying the variability associated with signal estimation have been shown to bestrongly correlated with the absolute error of the estimated HR . In addition, for each metrics, we reportthe standard error to estimate the variability of each model. As most SOTA end-to-end models show robustwithin-dataset performance, we present cross-dataset performance in table 2, while reporting within-datasetperformance in table 6 in appendix A.First, we observe that for all the evaluation metrics reported, the proposed FactorizePhys with FSAMoutperforms the SOTA methods on PURE and iBVP datasets, across all training datasets. Thissuggests a consistent and superior generalization achieved by the proposed method. Cross-dataset evaluationon the UBFC-rPPG dataset further highlights the performance gains of the proposed FactorizePhys modelwhen trained with the iBVP and the SCAMPS datasets, and at-par performance when trainedwith the PURE dataset. When models are trained with SCAMPS (synthesized dataset), FactorizePhysuniquely outperforms the SOTA methods on all testing datasets further indicating the superior cross-datasetgeneralization. The performance of EfficientPhys with FSAM exceeds in most cases and remains at parin the rest, compared to the EfficientPhys model with SASN , suggesting the versatility of FSAM as anattention module. As 3D CNN kernels in FactorizePhys can learn spatial-temporal patterns better than theTSM based 2D-CNN model (EfficientPhys ), FactorizePhys with FSAM outperforms EfficientPhys with FSAM across all datasets. Lastly, the proposed method consistently achieves superior SNR andMACC for the estimated rPPG signals, highlighting the enhanced reliability of the extracted signals. Computation Cost and Latency:We compare computational complexity and latency for all the modelsin fig. 4[A], and provide further details in table 9 in appendix A. The cumulative MAE is computed byaveraging cross-dataset performance for respective models across all combinations of training and testingdatasets reported in table 2. The proposed FactorizePhys with FSAM not only shows the best performance, ithas significantly less number of model parameters and performs at par in terms of latency as the 2D-CNNSOTA rPPG method, EfficientPhys . Specifically, dropping FSAM during inference does not resultin loss of performance for FactorizePhys, while reducing latency considerably, making it highly suitablefor real-time and resource-constrained deployment. In contrast, when FSAM was dropped after trainingEfficientPhys with FSAM, it did not retain the performance (results not shown). We interpret that whileFSAM effectively influences the 3D convolutional kernels in FactorizePhys to increase the saliency of relevantspatial-temporal features, 2D convolutional kernels cannot benefit adequately due to the limited ability tomodel spatial-temporal features. It should also be noted that the higher latency of FactorizePhys compared toEfficientPhys , although it has fewer model parameters, can be attributed to the difference in floating-pointoperations (FLOPS) between the 3D-CNN and 2D-CNN architectures.",
  "All metrics are shown with two decimal places, except those correlation measures that range between 0.99 and 1.0": ": (A) Cumulative cross-dataset performance (MAE) v/s latency plot. The size of the spherecorresponds to the number of model parameters; (B) Visualization of learned spatial-temporal features fromthe base 3D-CNN model trained without and with FSAM; System specs: Ubuntu 22.04 OS, NVIDIA GeForce RTX 3070Laptop GPU, Intel Core i7-10870H CPU @ 2.20GHz, 16 GB RAM. Visualization of Learned Attention:We compute absolute cosine similarity between the temporal di-mension of 4D embeddings (with temporal, spatial, and channel dimensions) and the ground-truth signal tovisualize the learned attention for FactorizePhys trained without and with FSAM in fig. 4[B], where each tilerepresents a channel of the embedding layer. A higher cosine similarity score between the temporal dimensionof the embeddings and the ground-truth PPG signal, which is observed for FactorizePhys trained with FSAM,indicates a higher saliency of temporal features. The spatial spread of high cosine similarity scores in differentchannels for FactorizePhys trained with FSAM, highlights selectivity of the learned attention, providing clearerevidence that the FactorizePhys model trained with FSAM can effectively pick the spatial features havingthe strong presence of the rPPG signal (i.e., facial regions with visible skin surface). [B] not onlysuggests the effectiveness of the joint computation of multidimensional attention, but also offers more intuitivevisualization of learned spatial-temporal features than existing visualization approaches .",
  "Conclusion": "We present FactorizePhys, a 3D-CNN model utilizing the Factorized Self-Attention Module, FSAM, toconcurrently extract multidimensional (spatial, temporal, and channel) attention for the downstream task ofrPPG estimation from video frames. The assessment performed utilizing various rPPG datasets demonstratesthat our proposed method possesses superior generalization capabilities across different datasets, comparedto current state-of-the-art methods. Moreover, when adjusted to the 2D-CNN architecture, FSAM achievesperformance on par with the established SASN attention, underscoring its adaptability across diversenetwork architectures.Broader Impacts and Limitations: The superior performance of FactorizePhys equipped with FSAM toestimate rPPG indicates its potential utility in various healthcare applications that require the estimation ofphysiological signals through noncontact imaging. Although FSAM has shown efficacy as a multidimensionalattention mechanism specifically for the extraction of rPPG signals, more research is needed to determinethe efficacy of the proposed method in extracting heart rate variability metrics as well as other physiologicalsignals. Despite the state-of-the-art performance of the proposed rPPG method, signal peaks can still besusceptible to challenging real-world scenarios, such as active head movements, occlusions, and dynamicchanges in ambient lighting conditions, an issue that is qualitatively illustrated in the waveforms depictedin appendix A.11. Moreover, it is imperative to conduct additional research to evaluate the effectiveness ofFSAM across other spatial-temporal domains, including video understanding, video object tracking, and videosegmentation, along with several other downstream tasks that depend on multi-dimensional input data. Inthe context of signal estimation tasks, the utilization of NMF variants that integrate temporal or frequencyconstraints on time series vectors may offer enhanced attention capabilities. These constraints are congruentwith the characteristics of the ground truth and present avenues for future investigation.",
  "All datasets provide video recordings with a resolution of 640 480, and frame rate of 30 FPS. Below weprovide data-specific details": "iBVP :The iBVP dataset consists of 124 synchronized RGB and thermal infrared videos from 31subjects, acquired under controlled conditions. Each video is 3 minutes in duration, and the ground truth BVPsignals were acquired from the ear using PhysioKit . Data were acquired under 4 different conditionsthat include controlled breathing, math tasks, and head movements. BVP signals are marked with the signalquality, enabling the use of the video frames only where the quality of ground-truth BVP signal is high. In thiswork, we use only RGB frames to train the models. PURE :This data set comprises video recordings from 10 subjects, with the ground-truth BVP andSpO2 signals acquired from the subjects finger. For each participant, six recordings are acquired under variedmotion conditions, offering a range of data reflecting different physical states.",
  "A.2Implementation Overview": "The preprocessing steps for video frames include face detection using the YOLO5Face face detector atan interval of 30 frames and using the detected facial bounding box to crop 30 subsequent frames, prior toperforming the next face detection. The cropped facial frames are resized to a resolution of 72 72, which hasbeen shown to be sufficient to estimate the rPPG. Additionally, to ensure uniform input data for all models, weadd Diff layer to the PhysNet and PhysFormer architectures, as implemented by EfficientPhys and the proposed FactorizePhys models, and train all the models from scratch using uniformly preprocessedvideo frames.The number of frames in a video chunk is maintained as 161, which after the Diff layer becomes 160,making the spatial-temporal input data size 160 72 72. Ground-truth BVP signals are also uniformlystandardized for training all models. This is different from some of the recent work that applies Diff inaddition to standardization. We empirically found that all models perform significantly better when trainedwith the standardized BVP signals, although when the Diff is applied to the video frames.All models were trained with 10 epochs on, following a recent work , as a higher number of epochs,e.g. 30 epochs as used in rPPG-Toolbox resulted in poor generalization for all models. However, weused only one epoch for all models to train on the SCAMPS dataset, since this dataset is a synthesizeddataset with generated BVP signals that are easier for models to learn, unlike real-world datasets. Trainingbeyond one epoch resulted in poorer cross-dataset performance for all the models. The batch size of 4 wasused consistently throughout the training and the maximum learning rate was set to 1 103 with 1 cyclelearning rate scheduler for all CNN models.In addition, CNN models were optimized using negative Pearson correlation as a loss function. Thelearning rate for PhysFormer was set to 1 104 and it was optimized using a dynamic loss composedof several hyperparameters, a negative Pearson loss, a frequency cross-entropy loss and a label distribution lossas used by the authors and implemented in the rPPG-Toolbox . Before computing HR for performanceevaluation, both ground truth and estimated BVP signals were filtered using a bandpass filter (low cutoff =0.60 Hz, high cutoff = 3.30 Hz) to accommodate HR ranges of 36 to 198 BPM. HR was then computed usingthe FFT-peaks-based approach as implemented in rPPG-Toolbox .",
  "Base1.74 0.394.39 1.062.42 0.570.90 0.046.59 0.570.56 0.01": "Retention of performance gains despite FSAM being skipped during inference, for FactorizePhys trainedwith FSAM offers insight into the mechanics of how FSAM functions. This can be interpreted as follows:Optimization of a network having FSAM implemented as an attention mechanism influences the network toincrease the saliency of the most relevant features, so that a factorized approximation of embeddings retainsthese features, while discarding the less important features. Due to the increased saliency of relevant featuresand the presence of residual connection, FSAM can be skipped during inference, significantly reducingcomputational overhead.In table 4, we present results to compare the performance obtained for different ranks L, as well asthe optimization steps used to solve factorization. For all experiments, FactorizePhys is trained with theUBFC-rPPG dataset and the performance is presented for the PURE dataset . We can observe thatthe best performance was achieved for rank L = 1 for the different steps used to solve the factorization. Forhigher ranks, performance remains on par with that of the network without the FSAM, indicating that for therPPG estimation task, the rank-1 factorization offers the optimal spatial-temporal attention. These results alignwith the expected single source of the underlying BVP signals in different facial regions.",
  "A.4Statistical Significance of the Main Results": "We performed repeated experiments with 10 different random seed values between 1 and 1000 to comparethe proposed FactorizePhys trained with FSAM with the best performing SOTA rPPG method. For thecross-dataset generalization results reported in table 2, EfficientPhys with SASN was found to performthe best among the existing SOTA methods.For each random seed value, we trained the proposed FactorizePhys with FSAM and EfficientPhys withSASN on the UBFC-rPPG dataset and evaluated them on the PURE dataset . Paired T tests foreach reported evaluation metrics suggest that the performance gains achieved with the proposed method arestatistically significant compared against the best performing SOTA rPPG method, highlighting its effectivenessand thereby highlighting contributions of this work in the research field of end-to-end rPPG estimation fromvideo frames.",
  "A.5Within Dataset Performance": "In this work, we primarily focus on comparing rPPG methods for their cross-dataset generalization, whichoffers more critical evaluation and reliable estimates of how models perform on unseen or out-of-distributiondata. Within-dataset performance signifies an representation ability of model to fit the data, derived from thesame distribution, serving as an essential criteria. Therefore, for completeness, in table 6, we report within-",
  "A.6Scalability Assessment of FSAM": "We further investigate FSAM for its scalability to higher spatial-temporal resolution. For this, we performwithin-dataset evaluation on the UBFC-rPPG dataset , which is pre-processed with the regular inputdimension of 160 72 72 as well as with a higher spatial and temporal dimension of 240 128 128.Repeatable experiments are conducted with 10 different random seeds between 1 and 1000 to compare theperformance of FactorizePhys with FSAM for each spatial-temporal input dimension.Comparable performance, as observed in table 7, for both spatial-temporal input dimensions, suggests thatFSAM can be easily deployed for different spatial-temporal scales. It should also be noted that the higherspatial dimension of video frames (i.e., 128 128) does not produce improved performance, indicating thatthe spatial dimension of 72 72 is sufficient to extract rPPG signals with end-to-end methods.",
  "A.7Multimodal rPPG Extraction": "As iBVP dataset offers synchronized RGB and thermal infrared video frames, we conducted a brief experimentusing FactorizePhys with FSAM to investigate whether combining both modalities can result in performancegains for the estimation of rPPG. For this, we also individually trained FactorizePhys on RGB and thermalframes keeping the identical data split of 70%-30%. Results in table 8 suggest weaker presence of rPPG signal",
  "A.9Computational Cost and Latency": "compares model parameters, latency on GPU and CPU, and model size of the proposed FactorizePhyswith FSAM with that of the existing SOTA rPPG methods. Considering the identical inference time perfor-mance of the base FactorizePhys, when trained using the proposed FSAM, the proposed method uses an orderof magnitude fewer parameters and achieves a par latency on both CPU and GPU systems. Relatively higherlatency compared to the EfficientPhys model, despite the fewer model parameters, is due to the differencein the number of floating point operations (FLOPS). FactorizePhys, being a 3D-CNN architecture, requiresmore FLOPS to compute 3D features at each layer compared to the fewer FLOPS for EfficientPhys which implements the 2D-CNN architecture. It should be noted that the FLOPS are also dependent on theinput dimension, which is kept consistent for all the models. For resource critical deployment, FLOPS can besignificantly reduced by decreasing the spatial dimension of input from 72 72 to 8 8 as found optimal forRTrPPG or to 9 9 as used in the small branch of the Bigsmall model for rPPG estimation.",
  "A.12Safeguards": "We intend to release our rPPG estimation code only for academic purposes, with Responsible AI license(RAIL). Research areas that will benefit directly from this work include human-computer interaction andcontactless health tracking or vital signs monitoring. Although the methods presented in this work maypotentially benefit certain clinical scenarios, thorough validation studies, with appropriate ethics approval, arerequired to critically assess performance in such settings.In addition, in some recent work, rPPG methods have been indicated as effective in detecting deep-fakevideos. In this context, we would like to caution such a use, considering the main results presented for themodels trained using the SCAMPS dataset, consisting of synthesized avatars. We argue that the rPPGsignal can be embedded in the synthesized (or deep-fake) videos, with a similar approach as used for generatingthe SCAMPS dataset. In such scenarios, in spite of high accuracy in estimating rPPG signals, suchmethods can be fooled by the synthesized videos that embed BVP signals. Therefore, we highlight that it isnecessary to use the rPPG signal estimation methods in this context with great caution.",
  "Frdric Bousefsaf, Alain Pruski, and Choubeila Maaoui. 3d convolutional neural networks for remote pulse ratemeasurement and mapping from facial video. Applied Sciences, 9(20), 2019": "Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko.End-to-end object detection with transformers. In European conference on computer vision, pages 213229. Springer,2020. Weixuan Chen and Daniel McDuff. Deepphys: Video-based physiological measurement using convolutional attentionnetworks. In Proceedings of the European Conference on Computer Vision (ECCV), September 2018. Youngjun Cho. Rethinking eye-blink: Assessing task difficulty through physiological representation of spontaneousblinking. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, CHI 21, New York,NY, USA, 2021. Association for Computing Machinery. Youngjun Cho, Nadia Bianchi-Berthouze, and Simon J. Julier. Deepbreath: Deep learning of breathing patterns forautomatic stress recognition using low-cost thermal imaging in unconstrained settings. In 2017 Seventh InternationalConference on Affective Computing and Intelligent Interaction (ACII), pages 456463, 2017. Youngjun Cho, Simon J Julier, and Nadia Bianchi-Berthouze. Instant stress: detection of perceived mental stressthrough smartphone photoplethysmography and thermal imaging. JMIR mental health, 6(4):e10140, 2019. Youngjun Cho, Simon J. Julier, Nicolai Marquardt, and Nadia Bianchi-Berthouze. Robust tracking of respiratory ratein high-dynamic range scenes using mobile thermal imaging. Biomed. Opt. Express, 8(10):44804503, Oct 2017.",
  "Inderjit S Dhillon and Dharmendra S Modha. Concept decompositions for large sparse text data using clustering.Machine learning, 42:143175, 2001": "Shuai Ding, Zhen Ke, Zijie Yue, Cheng Song, and Lu Lu. Noncontact multiphysiological signals estimation viavisible and infrared facial features fusion. IEEE Transactions on Instrumentation and Measurement, 71:113, 2022. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. Animage is worth 16x16 words: Transformers for image recognition at scale. In International Conference on LearningRepresentations, 2021.",
  "Wenbin Du, Yali Wang, and Yu Qiao. Recurrent spatial-temporal attention network for action recognition in videos.IEEE Transactions on Image Processing, 27(3):13471360, 2018": "Jun Fu, Jing Liu, Haijie Tian, Yong Li, Yongjun Bao, Zhiwei Fang, and Hanqing Lu. Dual attention network forscene segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR), June 2019. Xiao Fu, Kejun Huang, Nicholas D. Sidiropoulos, and Wing-Kin Ma. Nonnegative matrix factorization for signal anddata analytics: Identifiability, algorithms, and applications. IEEE Signal Processing Magazine, 36(2):5980, 2019.",
  "R.M. Gray and D.L. Neuhoff. Quantization. IEEE Transactions on Information Theory, 44(6):23252383, 1998": "Meng-Hao Guo, Tian-Xing Xu, Jiang-Jiang Liu, Zheng-Ning Liu, Peng-Tao Jiang, Tai-Jiang Mu, Song-Hai Zhang,Ralph R Martin, Ming-Ming Cheng, and Shi-Min Hu. Attention mechanisms in computer vision: A survey.Computational visual media, 8(3):331368, 2022. Xudong Guo, Xun Guo, and Yan Lu. Ssan: Separable self-attention network for video representation learning. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1261812627,June 2021. Anup Kumar Gupta, Rupesh Kumar, Lokendra Birla, and Puneet Gupta. Radiant: Better rppg estimation using signalembeddings and transformer. In Proceedings of the IEEE/CVF Winter Conference on Applications of ComputerVision (WACV), pages 49764986, January 2023.",
  "Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition (CVPR), June 2018": "Min Hu, Fei Qian, Xiaohua Wang, Lei He, Dong Guo, and Fuji Ren. Robust heart rate estimation with spa-tialtemporal attention network from facial videos. IEEE Transactions on Cognitive and Developmental Systems,14(2):639647, 2022. Bin Huang, Shen Hu, Zimeng Liu, Chun-Liang Lin, Junfeng Su, Changchen Zhao, Li Wang, and Wenjin Wang.Challenges and prospects of visual contactless physiological monitoring in clinical study. NPJ Digital Medicine,6(1):231, 2023. Jitesh Joshi, Nadia Berthouze, and Youngjun Cho. Self-adversarial multi-scale contrastive learning for semanticsegmentation of thermal facial images. In 33rd British Machine Vision Conference 2022, BMVC 2022, London, UK,November 21-24, 2022. BMVA Press, 2022.",
  "Tonglai Liu, Ronghai Luo, Longqin Xu, Dachun Feng, Liang Cao, Shuangyin Liu, and Jianjun Guo. Spatial channelattention for deep convolutional neural networks. Mathematics, 10(10):1750, 2022": "Xin Liu, Josh Fromm, Shwetak Patel, and Daniel McDuff. Multi-task temporal shift attention networks for on-devicecontactless vitals measurement. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advancesin Neural Information Processing Systems, volume 33, pages 1940019411. Curran Associates, Inc., 2020. Xin Liu, Brian Hill, Ziheng Jiang, Shwetak Patel, and Daniel McDuff. Efficientphys: Enabling simple, fast andaccurate camera-based cardiac measurement. In Proceedings of the IEEE/CVF Winter Conference on Applications ofComputer Vision (WACV), pages 50085017, January 2023. Xin Liu, Girish Narayanswamy, Akshay Paruchuri, Xiaoyu Zhang, Jiankai Tang, Yuzhe Zhang, Roni Sengupta,Shwetak Patel, Yuntao Wang, and Daniel McDuff. rppg-toolbox: Deep remote ppg toolbox. In A. Oh, T. Naumann,A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems,volume 36, pages 6848568510. Curran Associates, Inc., 2023. Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer:Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conferenceon Computer Vision (ICCV), pages 1001210022, October 2021. Hao Lu and Hu Han. Nas-hr: Neural architecture search for heart rate estimation from face videos. Virtual Reality &Intelligent Hardware, 3(1):3342, 2021. Emotion recognition for human-computer interaction. Hao Lu, Hu Han, and S. Kevin Zhou. Dual-gan: Joint bvp and noise modeling for remote physiological measurement.In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1240412413, June 2021. Daniel McDuff, Miah Wander, Xin Liu, Brian Hill, Javier Hernandez, Jonathan Lester, and Tadas Baltrusaitis.Scamps: Synthetics for camera measurement of physiological signals. Advances in Neural Information ProcessingSystems, 35:37443757, 2022. Clara Moge, Katherine Wang, and Youngjun Cho. Shared user interfaces of physiological data: Systematic review ofsocial biofeedback systems and contexts in hci. In Proceedings of the 2022 CHI Conference on Human Factors inComputing Systems, pages 116, 2022. Girish Narayanswamy, Yujia Liu, Yuzhe Yang, Chengqian Ma, Xin Liu, Daniel McDuff, and Shwetak Patel. Bigsmall:Efficient multi-task learning for disparate spatial and temporal physiological measurements. In Proceedings of theIEEE/CVF Winter Conference on Applications of Computer Vision, pages 79147924, 2024.",
  "Xuesong Niu, Shiguang Shan, Hu Han, and Xilin Chen. Rhythmnet: End-to-end heart rate estimation from face viaspatial-temporal representation. IEEE Transactions on Image Processing, 29:24092423, 2020": "Xuesong Niu, Xingyuan Zhao, Hu Han, Abhijit Das, Antitza Dantcheva, Shiguang Shan, and Xilin Chen. Robustremote heart rate estimation from face utilizing spatial-temporal attention. In 2019 14th IEEE InternationalConference on Automatic Face & Gesture Recognition (FG 2019), pages 18, 2019. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, ZemingLin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library.Advances in neural information processing systems, 32, 2019.",
  "Delong Qi, Weijun Tan, Qi Yao, and Jingfeng Liu. Yolo5face: Why reinventing a face detector. In EuropeanConference on Computer Vision, pages 228244. Springer, 2022": "Leslie N Smith and Nicholay Topin. Super-convergence: Very fast training of neural networks using large learningrates. In Artificial intelligence and machine learning for multi-domain operations applications, volume 11006, pages369386. SPIE, 2019. Rencheng Song, Huan Chen, Juan Cheng, Chang Li, Yu Liu, and Xun Chen. Pulsegan: Learning to generaterealistic pulse waveforms in remote photoplethysmography. IEEE Journal of Biomedical and Health Informatics,25(5):13731384, 2021. Rencheng Song, Han Wang, Haojie Xia, Juan Cheng, Chang Li, and Xun Chen. Uncertainty quantification for deeplearning-based remote photoplethysmography. IEEE Transactions on Instrumentation and Measurement, 2023. Sijie Song, Cuiling Lan, Junliang Xing, Wenjun Zeng, and Jiaying Liu. An end-to-end spatio-temporal attentionmodel for human action recognition from skeleton data. Proceedings of the AAAI Conference on Artificial Intelligence,31(1), Feb. 2017.",
  "Radim petlk, Vojtech Franc, and Jir Matas. Visual heart rate estimation with convolutional neural network. InProceedings of the british machine vision conference, Newcastle, UK, pages 36, 2018": "Ronny Stricker, Steffen Mller, and Horst-Michael Gross. Non-contact video-based pulse rate measurement on amobile service robot. In The 23rd IEEE International Symposium on Robot and Human Interactive Communication,pages 10561062. IEEE, 2014. Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai. Vl-bert: Pre-training of genericvisual-linguistic representations. In International Conference on Learning Representations, 2020.",
  "Snigdha Tariyal, Angshul Majumdar, Richa Singh, and Mayank Vatsa. Deep dictionary learning. IEEE Access,4:1009610109, 2016": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ukasz Kaiser,and Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus,S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. CurranAssociates, Inc., 2017.",
  "Wim Verkruysse, Lars O Svaasand, and J Stuart Nelson. Remote plethysmographic imaging using ambient light.Opt. Express, 16(26):2143421445, Dec 2008": "Jing Wang and Lei Liu. A multi-attention deep neural network model base on embedding and matrix factorizationfor recommendation. International Journal of Cognitive Computing in Engineering, 1:7077, 2020. Rui-Xuan Wang, Hong-Mei Sun, Rong-Rong Hao, Ang Pan, and Rui-Sheng Jia. Transphys: Transformer-basedunsupervised contrastive learning for remote heart rate measurement. Biomedical Signal Processing and Control,86:105058, 2023.",
  "Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In So Kweon. Cbam: Convolutional block attention module.In Proceedings of the European Conference on Computer Vision (ECCV), September 2018": "Hanguang Xiao, Tianqi Liu, Yisha Sun, Yulin Li, Shiyi Zhao, and Alberto Avolio. Remote photoplethysmographyfor heart rate measurement: A review. Biomedical Signal Processing and Control, 88:105608, 2024. Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M. Alvarez, and Ping Luo. Segformer: Simpleand efficient design for semantic segmentation with transformers. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S.Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, volume 34, pages1207712090. Curran Associates, Inc., 2021. Qian Xie, Yu-Kun Lai, Jing Wu, Zhoutao Wang, Yiming Zhang, Kai Xu, and Jun Wang. Mlcvnet: Multi-levelcontext votenet for 3d object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR), June 2020. Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, andYoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention. In Francis Bachand David Blei, editors, Proceedings of the 32nd International Conference on Machine Learning, volume 37 ofProceedings of Machine Learning Research, pages 20482057, Lille, France, 0709 Jul 2015. PMLR. Ming Xu, Guang Zeng, Yongjun Song, Yue Cao, Zeyi Liu, and Xiao He. Ivrr-ppg: An illumination variation robustremote-ppg algorithm for monitoring heart rate of drivers. IEEE Transactions on Instrumentation and Measurement,72:110, 2023. Shuangjie Xu, Yu Cheng, Kang Gu, Yang Yang, Shiyu Chang, and Pan Zhou. Jointly attentive spatial-temporalpooling networks for video-based person re-identification. In Proceedings of the IEEE international conference oncomputer vision, pages 47334742, 2017. Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong He. Attngan:Fine-grained text to image generation with attentional generative adversarial networks. In Proceedings of the IEEEConference on Computer Vision and Pattern Recognition (CVPR), June 2018. Chenggang Yan, Yunbin Tu, Xingzheng Wang, Yongbing Zhang, Xinhong Hao, Yongdong Zhang, and Qionghai Dai.Stat: Spatial-temporal attention mechanism for video captioning. IEEE Transactions on Multimedia, 22(1):229241,2020. Zongxin Yang, Linchao Zhu, Yu Wu, and Yi Yang. Gated channel transformation for visual recognition. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020. Zitong Yu, Yuming Shen, Jingang Shi, Hengshuang Zhao, Yawen Cui, Jiehua Zhang, Philip Torr, and GuoyingZhao. Physformer++: Facial video-based physiological measurement with slowfast temporal difference transformer.International Journal of Computer Vision, 131(6):13071330, February 2023. Zitong Yu, Yuming Shen, Jingang Shi, Hengshuang Zhao, Philip H.S. Torr, and Guoying Zhao. Physformer: Facialvideo-based physiological measurement with temporal difference transformer. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition (CVPR), pages 41864196, June 2022.",
  "Yuhui Yuan, Lang Huang, Jianyuan Guo, Chao Zhang, Xilin Chen, and Jingdong Wang. Ocnet: Object context forsemantic segmentation. International Journal of Computer Vision, 129(8):23752398, 2021": "Changchen Zhao, Hongsheng Wang, Huiling Chen, Weiwei Shi, and Yuanjing Feng. Jamsnet: A remote pulseextraction network based on joint attention and multi-scale fusion. IEEE Transactions on Circuits and Systems forVideo Technology, 33(6):27832797, 2023. Changchen Zhao, Menghao Zhou, Zheng Zhao, Bin Huang, and Bing Rao.Learning spatio-temporal pulserepresentation with global-local interaction and supervision for remote prediction of heart rate. IEEE Journal ofBiomedical and Health Informatics, 28(2):609620, 2024. Hongyi Zheng, Hongwei Yong, and Lei Zhang. Deep convolutional dictionary learning for image denoising. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 630641,June 2021. Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable {detr}: Deformabletransformers for end-to-end object detection. In International Conference on Learning Representations, 2021. Yu Zitong, Li Xiaobai, and Guoying Zhao. Remote photoplethysmograph signal measurement from facial videosusing spatio-temporal networks. In 30th British Machine Vision Conference (BMVC), 9th-12th September 2019,Cardiff, UK, September 2019."
}