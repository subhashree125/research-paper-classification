{
  "REMI BARDENET, SUBHROSHEKHAR GHOSH, HUGO SIMON-ONFROY, AND HOANG-SON TRAN": "ABSTRACT. Determinantal point processes (DPPs) are random configurations of pointswith tunable negative dependence. Because sampling is tractable, DPPs are natural candi-dates for subsampling tasks, such as minibatch selection or coreset construction. A coresetis a subset of a (large) training set, such that minimizing an empirical loss averaged overthe coreset is a controlled replacement for the intractable minimization of the originalempirical loss. Typically, the control takes the form of a guarantee that the average lossover the coreset approximates the total loss uniformly across the parameter space. Recentwork has provided significant empirical support in favor of using DPPs to build random-ized coresets, coupled with interesting theoretical results that are suggestive but leavesome key questions unanswered. In particular, the central question of whether the car-dinality of a DPP-based coreset is fundamentally smaller than one based on independentsampling remained open. In this paper, we answer this question in the affirmative, demon-strating that DPPs can provably outperform independently drawn coresets. In this vein, wecontribute a conceptual understanding of coreset loss as a linear statistic of the (random)coreset.We leverage this structural observation to connect the coresets problem to amore general problem of concentration phenomena for linear statistics of DPPs, whereinwe obtain effective concentration inequalities that extend well-beyond the state-of-the-art,encompassing general non-projection, even non-symmetric kernels. The latter have beenrecently shown to be of interest in machine learning beyond coresets, but come with a lim-ited theoretical toolbox, to the extension of which our result contributes. Finally, we arealso able to address the coresets problem for vector-valued objective functions, a noveltyin the coresets literature.",
  ". INTRODUCTION": "Let X =xi | i [1, n]be a set of n points in a Euclidean space, called the data set.Let F be a set of nonnegative functions on X, called queries. Many classical learningproblems, supervised or unsupervised, are formulated as finding a query f in F thatminimizes an additive loss function of the form",
  "Penalty terms can be added to each function, to cover e.g. ridge or lasso regression": "In many machine learning applications, the complexity of the corresponding optimiza-tion problem grows with the cardinality n of the dataset. When n 1 makes optimiza-tion intractable, one is tempted to reduce the amount of data, using only a tractablenumber of representative samples. This is the idea formalized by coresets; we refer to for a survey, and to for specific coreset constructions for k-means and Euclideanclustering. An -coreset is a subset S X, possibly with corresponding weights (x),x S, such that",
  "xS(x)f(x)": "is within of L(f), uniformly in f F. If the cardinality m of S is significantly smallerthan the intractable size n of the original data set, one has reduced the complexity of thealgorithm at a little cost in accuracy.Many randomized coreset constructions, where such guarantees are shown to holdwith large probability, are built by drawing elements independently from the data set X[1, Chapter 3]. Because a representative coreset should intuitively be made of diversedata points, negative dependence between the coreset elements has been proposed as aneffective possibility to improve their performance . In particular, the authors advo-cate the use of Determinantal Point Processes (DPPs), a family of probability distributionsover subsets of X parametrized by an n n kernel matrix K that enforces diversity, allof this while coming with a polynomial-time exact sampling algorithm. give extensive theoretical and empirical justification for the use of DPPs in ran-domized coreset construction. In one of their key results, using concentration results in, bound the cardinality of a DPP-based -coreset, and their bound is O(2).However, it is known that the best -coresets built with independent samples are also ofcardinality O(2). Thus, the crucial question of whether DPP-based coresets can pro-vide a strict improvement remained to be settled; given the computational simplicity ofindependent schemes, this would be fundamental to justify the deployment of DPP-basedmethods.In this paper, we settle this question in the affirmative, demonstrating that for carefullychosen kernels, DPP-based coresets provably yield significantly better accuracy guaran-tees than independent schemes; equivalently, to achieve similar accuracy it suffices touse significantly smaller coresets via DPPs. In particular, we will show that DPP-basedcoresets actually can achieve cardinality m = O(2/(1+)). The quantity depends on thevariance of the subsampled loss under the considered DPP, and some DPPs yields > 0.A cornerstone of our approach is a structural understanding of the coreset loss (1.2) as aso-called linear statistic of the random point set S, which enables us to go beyond earlierresults that were based on concentration properties of general Lipschitz functions of aDPP .In this endeavour, we obtain very widely-applicable concentration inequalities for lin-ear statistics of DPPs compared to the state of the art; cf. that mostly focuses onscalar-valued statistics for finite rank ensembles on R. In particular, we are able to ad-dress all DPPs that have appeared so far in the ML literature. Specifically, our results areable to handle non-symmetric kernels and vector-valued linear statistics.DPPs with non-symmetric kernels have recently been shown to be of significant inter-est in machine learning, such as recommendation systems , but they comewith a limited theoretical toolbox, to which this paper makes a contribution. On theother hand, vector-valued statistics arise naturally in many learning problems, includingcoreset settings such as the gradient estimator in Stochastic Gradient Descent . How-ever, the literature on coresets for vector-valued statistics is scarce, and in this paper weinaugurate their study with effective approximation guarantees via DPPs.The rest of the paper is organized as follows. contains background on DPPsand coresets. contains our contributions. provides numerical illus-trations. contains a discussion on limitations and future work.",
  "We introduce here the two key notions of determinantal point process and coreset, andobserve that a coreset guarantee is a uniform control over specific linear statistics of apoint process": "2.1. Determinantal point processes. A point process S on a Polish space X is a randomlocally finite subset of X. Given a reference measure on X (e.g., the Lebesgue measureif X = Rd or the counting measure if X is discrete), a point process S is called a DPP(w.r.t. ) if there exists a measurable function K : X X C such that",
  "X k f(x1, . . . , xk) det[K(xi, xj)]kk dk(x1, . . . , xk),": "where the sum in the LHS ranges over all pairwise distinct k-tuples of the random locallyfinite subset S, for all bounded measurable f : X k R and for all k N. Such a functionK is called a kernel for the DPP S, and is called the background measure.When the ground set X is of finite cardinality n, an equivalent but more intuitive wayto define DPPs is as follows: a random subset S of X is called a DPP if there exists ann n-matrix K such that",
  "P(T S) = det[KT], T X,": "where KT denotes the submatrix of K with rows and columns indexed by T.In a similar vein to Gaussian processes, all the statistical properties of a DPP are en-coded in this kernel function K and background measure . A feature of DPPs withfar-reaching implications for machine learning is that sampling and inference with DPPsare tractable. We refer the reader to for general references. Originally intro-duced in electronic optics , they have been turned into generic statistical models forrepulsion in spatial statistics and machine learning . Example 3 (L-ensemble and m-DPP). Let X be a finite set of cardinality n, be thecounting measure, and L be a positive semi-definite n n-matrix. The L-ensemble withparameter L is the point process S on X such that, for all T X, P(S = T) det[LT],where LT is the square submatrix of L corresponding to the rows and columns indexedby the subset T. It can be shown that S is a DPP on X with kernel K := L(I + L)1.In general, the cardinality of S is a random variable.By conditioning on the event{|S| = m}, we obtain the so-called m-DPPs . Example 4 (Multivariate OPE; ). Let X = Rd and be a measure on Rd having allmoments finite, let (pk)kNd be the orthonormal sequence resulting from applying theGram-Schmidt procedure to the monomials xk11 . . . xkdd , taken in the graded lexical order.The kernel K(m)(x, y) := m1k=0 pk(x)pk(y) then defines a projection DPP on Rd, called themultivariate Orthogonal Polynomial Ensemble (OPE) of rank m and reference measure. Multivariate OPEs were used in as nodes for numerical integration, leading to aMonte Carlo estimator with mean squared error decaying in m11/d, faster than underindependent sampling. In , the authors investigated the problem of DPP-based mini-batch sampling for Stochastic Gradient Descent (SGD), and exploited a delicate interplaybetween a finite dataset and its ambient data distribution to leverage this fast decay for",
  "improved approximation guarantees. In particular, they proposed the following DPP de-fined on a (large) finite ground set": "Example 5 (Discretized multivariate OPE; ). Let n N and X = {x1, . . . , xn} d. Let q(x)dx be a probability measure on d. Let K(m)qbe the multivariateOPE kernel of rank m with reference measure q(x)dx, as defined in Example 4. Let : d R+ be a function, assumed to be positive on X, and consider",
  "where L and LS are respectively defined in (1.1) and (1.2)": "An immediate and important consequence of (2) is that the ratio of the minimum valueof LS by that of L is within O() of 1 [1, Theorem 2.1].One way to satisfy (2) with high probability for a single f is through importance sam-pling, taking S to be formed of m > 0 i.i.d. samples from some instrumental densityq on X, and taking = /q in (1.2). showed that a suitable choice of q actuallyyields the uniform guarantee (2). It suffices to take for instrumental pdf q(x) (x)s(x),where s upper-bounds the so-called sensitivity",
  "*Note that we defined a coreset as a subset and not a sub-multiset of X, thus ignoring multiplicity. Thisis because we allow weights in (1.2), so that repeated items are unnecessary in a coreset": "Note the arbitrary scaling factor 1/n in (2.4) compared to (2), which we adopt tosimplify comparisons between the two coreset definitions. With an additive coreset, theminimal value of LS is guaranteed to be within n of the minimal value of L: Similarlyto a multiplicative coreset, with suitably small one should be happy to train onesalgorithm only on S.",
  ". Coreset guarantee and linear statistics. Let S be a point process on a finite setX = {x1, . . . , xn}. For a test function : X R, we denote by () :=": "xS (x) theso-called linear statistic of . In a coreset problem, for a query f F, the estimated lossLS(f) in (1.2) is the linear statistic (f).When S is a DPP with a kernel K on X (w.r.t. the counting measure), we will choosethe weight (x) = K(x, x)1, where for x = xi X, we define K(x, x) to be Kii. By (2.1),this choice makes LS(f) an unbiased estimator for L(f). Guaranteeing a coreset guaran-tee such as (2.4) with high probability thus corresponds to a uniform-in-f concentrationinequality for the linear statistic (f). This motivates studying the concentration oflinear statistics under a DPP, to which we now turn.",
  ". THEORETICAL RESULTS": "We first give new results on the concentration of linear statistics under very generalDPPs. These results are of interest in their own right, and should find applications inML beyond coresets. Next we examine the implications of the concentration of linearstatistics for coresets, showing that a suitable DPP does yield a coreset size of size o(2),thus beating independent sampling.",
  "where A > 0 is a universal constant": "Our Theorem 1 is similar in spirit to a seminal concentration inequality by . How-ever, their result only applies to DPPs with Hermitian projection kernels of finite rank.We emphasize that our Theorem 1 is applicable to all Hermitian kernels on general Polishspaces.In view of recent interest in machine learning on DPPs with non-symmetric kernels,we present here a concentration inequality for such DPPs. We propose a novel approachto control the Laplace transform in the non-symmetric case (which can also be appliedto the symmetric setting). As a trade-off, the range for becomes a bit smaller. For sim-plicity, we present the result for a finite ground set, but the proof applies more generally.",
  "We conclude with a concentration inequality for linear statistics of vector-valued func-tions": "Theorem 3 (Vector-valued statistics). Let S be a DPP on a Polish space X with referencemeasure and Hermitian kernel K. Let = (1, . . . , p) : X Rp be a vector-valued testfunction, and we denote by (), V() the vectors ((i))pi=1 and (Var [(i)]1/2)pi=1, respec-tively. Let x2 := pi=1 2i |xi|2 be a weighted norm on Rp for some weights 1, . . . p 0.Then, for some universal constant A > 0, we have",
  "Var[(i)]i": "3.2. DPPs for coresets. We demonstrate the effectiveness of concentration inequalitiesfor linear statistics of DPPs in the coresets problem, achieving uniform approximationguarantees over function classes. To accommodate as many ML settings as possible, weshall consider two natural types of function classes: vector spaces of functions (A.1) andparametrized function spaces (A.2).For vector spaces of functions, we assume that",
  "(A.1)dim spanR(F) = D < for some D": "This assumption covers common situations like linear regression in Example 2, where weobserve that each f F is a quadratic function in (d + 1) variables. Thus the dimensionof the linear span of F is at most (d + 1)2 + (d + 1) + 1.Another popular class ofqueries, originating in signal processing problems, is the class of band-limited functions.A function f : Td R (where Td denotes the d-dimensional torus) is said to be band-limited if there exists B N such that its Fourier coefficients f(k1, . . . , kd) = 0 wheneverthere is a kj such that |kj| > B. It is easy to see that the space F of B-bandlimitedfunctions satisfies dim F (2B + 1)d.Another common scenario is when F is parametrized by a finite-dimensional parame-ter space:",
  "(A.3)f f for some > 0, uniformly on": "Conditions (A.2) and (A.3) cover e.g. the k-means problem of Example 1, as well as (non-)linear regression settings. For k-means, for instance, each query is parametrized by itscluster centers C = {q1, . . . , qk}, which can be viewed as a parameter (q1, . . . , qk) Rkd.Finally, with the idea in mind to derive multiplicative coresets from additive ones, wenote that since L(f) is typically of order n (for any f whose effective support covers apositive fraction of the ground set), it is natural to assume that",
  "(A.4)1n|L(f)| c, for some c > 0, uniformly on F": "Theorem 4. Let S be a DPP with a Hermitian kernel K on a finite set X = {x1, . . . , xn} andm = E[|S|]. Assume that for all i {1, . . . , n}, Kii m/n for some > 0 not dependingon m, n. Let V supfF Var [n1LS(f)]. Under (A.1) and (A.4),",
  "Here A > 0 is a universal constant and C = C(, B, , , c) > 0 is some constant": "Remark 4.1. For a bounded query f, Var [n1LS(f)] = O(m1) for i.i.d. sampling. In com-parison, sampling with DPPs often yields smaller variance for linear statistics, in O(m(1+))for some > 0; see .3 for an example. Thus, the upper bound for the range of for which we could use our concentration result is O(m). Plugging in = m for gives the upper bounds 2 exp6D Cm1+2and 2 expCD + D log m Cm1+2 respectively (C and C are some positive constants independent of m and n), which bothconverge to 0 as m as long as < (1 + )/2. In other words, the accuracy rate canbe chosen to be as small as m1/2/2, for any 0 < < , which is strictly smaller than thebest accuracy rate m1/2 of i.i.d. sampling. Remark 4.2. For i.i.d. sampling S with expected size m, P(x S) = m/n for all x X. Fora DPP S with kernel K, one has P(xi S) = Kii. Thus, assuming that for all i, Kii m/nfor some > 0 means that every point in the dataset X should have a reasonable chance tobe sampled. This also guarantees that the estimated loss LS(f) =",
  "xS K(x, x)1 to be bounded with high probability, which followsfrom the condition K(x, x) m/n and the fact that |S| is highly concentrated around itsmean m": "Remark 4.4. However, we remark that the assumption |S| B m a.s. holds for mostkernels of interest; DPPs with projection kernels being typical and significant examples. Inmachine learning terms, it entails that the coresets are not much bigger than their expectedsize m; whereas in practice, sampling schemes typically produce coresets of a fixed size (suchas with projection DPPs).",
  "3c (supfF maxi=1,...,p fi)1": "3.3. Application: Discretized multivariate OPE.. We revisit Example 5. It has beenshown in that sampling with the DPP S constructed in this example yields significantvariance reduction for a wide class of linear statistics on X. To be more precise, in theirsetting, X = {x1, . . . , xn} is a random data set, where xis are i.i.d. samples from adistribution with support inside a d-dimensional hypercube; is a density estimatorfor and q(x)dx is a reference measure on that hypercube. The DPP S is then defined bythe kernel K in 5 w.r.t. the empirical measure n1 xX x. We normalize the kernel bysetting K := n1K, so that S is a DPP with kernel K w.r.t. the counting measure on X.Then, under some mild assumptions on and q(x)dx, with high probability in the dataset X, we have Var [n1LS(f)] = O(m(1+1/d)) for any test function f satisfying somemild regularity conditions. For more details, we refer the reader to .The significant reduction on the variance of linear statistics motivates us to apply The-orem 4 to this setting. We also remark that all assumptions on the kernel in Theorem 4are satisfied for K with high probability in the data set X (see Appendix 6.6). Let F bea family of test functions on X satisfying regularity conditions as in . Then we canstate:",
  "2d. Meanwhile, for i.i.d. sampling, the accuracy rate isat best m1/2": "Remark 6.2. It may be noted that, DPPs being Hilbert space-based models, they interact wellwith linear projection based methods. As such, our method can be applied on dimensionallyreduced data, wherein the d in Remark 6.1 can be taken to be the reduced dimension,which is usually quite small. As such, the improvement in the approximation guarantees issubstantial, especially for large scale problems entailing large m.",
  ". EXPERIMENTS": "In this section, we compare randomized coresets for the k-means problem of Exam-ple 1, on different datasets. One virtue of k-means as a benchmark is that asymptoticallytight upper-bound on sensitivity (2.3) can easily be computed [1, Lemma 2.2]. 4.1. Competing approaches. We compare 6 different coreset samplers.* Each samplertakes as input a finite dataset X, an integer m, and sampler-specific parameters.Itreturns a random subset S X of cardinality m. For the associated weight function in (1.2), we always take the inverse of the marginal probability of inclusion, i.e. (x) =1/P(x S).The first two baselines use independent sampling. The uniform method returns msamples from X, uniformly and without replacement, and runs in O(m). The secondmethod, sensitivity, is specific to the k-means problem. It corresponds to the classicalsensitivity-based importance sampling coreset of described in . It runs inO(nk + nm).The rest of the methods use negative dependence. The third method, termed G-mDPP,uses an m-DPP sampler where the likelihood kernel is a Gaussian kernel, with adjustablebandwidth denoted by h. It is basically Algorithm 1 of , except we do not approxi-mate the likelihood kernel using random features. We prefer avoiding approximations inthis paper to isolatedly probe the benefit of negative dependence, but our choice comesat the cost O(n3) of performing SVD as a preprocessing, in addition to the usual O(nm2)sampling time. Similarly, we compute the marginal probabilities of inclusion of m-DPPsexactly, via Equation (205) and Algorithm 7 of . These costly steps will likely be ap-proximated in real data applications; see the discussion of complexity to . Thefourth method, OPE, is the discretized OPE of 5. We take q to be a product of univariatebeta pdfs, with parameters tuned to match the marginal moments of the dataset, as in. We take to be a kernel density estimator (KDE) built on X, using the Epanechnikovkernel, with Scotts bandwidth selection method, as implemented in the scikit-learnpackage . When KDE estimation is precomputed as in our experiments, the methodruns in O(nm2), and O(n2 + nm2) otherwise. Note that there is no cubic power of n, asone can perform the eigenvalue thresholding in 1 by a reduced SVD of the m n featurematrix (pk(xi)). The fifth method, termed Vdm-DPP, is Algorithm 2 of , which runsin O(nm2). It is an OPE in the sense of 4, but where the reference measure is thediscrete empirical measure of the dataset. Although we have no result on how its linearstatistics scale, its similarity with the discretized OPE, as well as its numerical perfor-mance in the experiments of , make us expect Vdm-DPP to behave similarly to OPE.The sixth method, stratified, is a stratified sampling baseline limited to the case whereX d and X is well-spread. It partitions d into a grid of m bins, and thenindependently draws one element uniformly in the intersection of X with each bin. Itis a special case of projection DPP, which runs in O(nm) and has obvious pitfalls, likerequiring that X has a non-empty intersection with each bin, which is unlikely to be thecase for non-uniformly spread datasets and high dimensions. Yet, this is a simple solutionthat one would likely implement to probe the benefits of negative dependence. 4.2. The performance metric. To investigate the cardinality of a coreset for a givenerror, we let QS denote the quantile function of sup |LS(f) L(f)|/L(f), the supremumover all queries of the relative error. Intuitively, QS(0.9) = 102 means that 90% of thesampled coresets have a worst case relative error below 102. We shall look at how anestimated QS(0.9) varies with m, especially its slope in log-log plots with respect to m.Now, the set F of all queries for k-means in combinatorially large, even for small values of",
  "*The link to a GitHub repository is temporarily hidden for anonymity": "k. Therefore, each time we need to evaluate the supremum of the relative error, we ratheruniformly sample without replacement k elements of X, 100 times and independently,and we take the maximum value of the relative error among these 100 values. Moreover,for each method and each coreset size m, the quantile function QS(0.9) is estimated byan empirical quantile over 100 independent coresets sampled for each value of m. 4.3. Results. We first consider a synthetic dataset of n = 1024 data points, sampled uni-formly and independently in d; see a. We consider d = 2 for demonstrationpurposes, but we have observed similar results for other small dimensions. b de-picts our estimate of QS(0.9) as a function of the coreset size m, in log-log format. Thetwo i.i.d. baselines decrease as m1/2, as expected. The stratified baseline, intuitivelywell-suited to uniformly-spread datasets, outperforms all other methods with a m1 rate,consistent with its known optimal variance reduction . Finally, the m-DPP and thetwo DPPs also yield a faster decay, eventually outperforming the i.i.d. baselines as mgrows. This is expected for the discretized OPE, as it follows from the theoretical resultsfrom ; but it is interesting to see that the Gaussian m-DPP and the Vdm-DPP seemto reach a similar m3/4 fast rate. For the Gaussian m-DPP, however, the performancedepends on the value of the bandwidth of the Gaussian kernel: in c, we see thatthe rate of decay can go from i.i.d.-like to OPE-like as the bandwidth increases; this isexpected from results like . Note that the color code of c differs from otherfigures.",
  "FIGURE 1. Results for the uniform dataset": "In the uniform dataset of 1a, the sensitivity function is almost flat, which makessensitivity behave like uniform. To give an edge to sensitivity, we now considerthe trimodal dataset shown in 2a, with an OPE sample superimposed. The performanceof sensitivity improves; see b, while the determinantal samplers still outper-form the independent ones thanks to a faster decay. For this dataset, it is not easy tostratify, and we thus do not show results for stratified. We note that the size of a marker placed at x is proportional to the corresponding weight 1/K(x, x) in the esti-mator of the average loss. Equivalently, the marker size is inversely proportional to themarginal probability of x being included in the DPP sample.Finally, we consider the classical MNIST dataset, after a PCA of dimension 4. Fig-ure 2c shows again the faster decay of the performance metric for the two DPPs (OPEand Vdm-DPP), compared to the two independent methods. However, the advantage pro-gressively disappears as the dimension increases beyond 4 (unshown), as expected fromthe gain in variance of the discretized multivariate OPE, which becomes negligible whend 1; see 5 for suggestions on how to prove a dimension-independent decay. The sourcecode used in this work is available at github.com/hsimonfroy/DPPcoresets, where DPPsamplers are built upon the Python package DPPy .",
  ". DISCUSSION": "5.1. Limitations. Our paper is a theoretical contribution, and our approach has severallimitations before it can be a practical addition to the coreset toolbox. The improvementover independent sampling relies on a variance scaling for linear statistics of a particularDPP, which itself relies on both 1) an Ansatz that the dataset was generated i.i.d. fromsome pdf with a large support, and 2) the availability of a good approximation to ;see 3 and . While Item 1) is usually deemed to be reasonable in a wide range ofsituations, we solve Item 2) by relying on a kernel density estimator, which is costly tomanipulate. Another limitation is that the improvement over independent sampling isin 1/d and thus progressively vanishes as the dimension increases. Finally, a classicalcaveat is that although tractable, sampling a DPP still costs O(nm2), provided the kernelis available in diagonalized form. 5.2. Future work. The limitations above set up a research program. In particular, anintriguing observation in our empirical studies is the comparative performance of vari-ous DPP-based coreset samplers; several of them exhibit effective performance. Whilewe have sharp theoretical guarantees for the discretized OPE-based scheme, obtaining similar guarantees and parameter-tuning protocols for other samplers, like m-DPPs, willbe of great practical interest as they would bypass the need, e.g., for an approximationto the data-generating mechanism . The DPP called Vdm-DPP in 4, which is itself anOPE for a discrete measure, might be a bridge between OPEs and m-DPPs, as Vdm-DPPcan be seen as a limit of Gaussian m-DPPs . On a more general note, improvingthe computational complexity of sampling DPPs remains an active topic, and we shouldexamine which techniques, e.g. by leveraging low-rank structures, preserve the smallcoreset property. Any breakthrough in the complexity of DPP sampling would also havesalutary consequences for the broader program of negative dependence as a toolbox formachine learning. On the dependence of the rate to the dimension, we propose to inves-tigate the impact of smoothness of the test functions on the rate: in numerical integrationwith mixtures of DPPs, smoothness does bring dimension-independent rates . Finally,in a more theoretical direction, extending concentration inequalities for linear statisticsbeyond the restricted range of appearing e.g. in Theorem 1 is a mathematically chal-lenging problem, with potential learning-theoretic consequences.",
  "3cf": "Proof of Theorem 4: Assuming (A.1). We let Fsym := {f : || 1, f F}, and let B bethe convex hull of Fsym. Since B is a symmetric convex body in F, there exists a norm F in F such that B is the unit ball in (F, F).Define",
  ". Proof of Theorem 6": "Proof of Theorem 6. We remark that Var [n1LS(f)] = O(m(1+1/d)) uniformly for all f F, w.h.p. in the data set X. Hence, Theorem 6 is a direct application of Theorem 4 withV = Cm(1+1/d) for some constant C > 0. As we discussed in the Remark 4.1, the rangefor is O(m1/d). Thus, it suffices to check the conditions on K. Since K is a projectionof rank m, |S| = m a.s. Moreover, we have n K(x, x) = K(x, x), which is typically oforder m, where we used an uniform CLT result and an asymptotic for multivariate OPEkernels (see for more details). and Disclosure of Funding.RB and HSO acknowledge supportfrom ERC grant Blackjack (ERC-2019-STG-851866) and ANR AI chair Baccarat (ANR-20-CHIA-0002). SG was supported in part by the MOE grants R-146-000-250-133, R-146-000-312-114, A-8002014-00-00 and MOE-T2EP20121-0013. HST was supported bythe NUS Research Scholarship.",
  "Victor-Emmanuel Brunel. Learning signed determinantal point processes through the principal minorassignment problem. Advances in Neural Information Processing Systems, 31, 2018": "Vincent Cohen-Addad, Kasper Green Larsen, David Saulpic, Chris Schwiegelshohn, and Omar AliSheikh-Omar. Improved coresets for euclidean k-means. In S. Koyejo, S. Mohamed, A. Agarwal,D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35 - 36thConference on Neural Information Processing Systems, NeurIPS 2022, Advances in Neural InformationProcessing Systems. Neural Information Processing Systems Foundation, 2022. Publisher Copyright: 2022 Neural information processing systems foundation. All rights reserved.; 36th Conferenceon Neural Information Processing Systems, NeurIPS 2022 ; Conference date: 28-11-2022 Through09-12-2022. Michal Derezinski, Feynman Liang, and Michael Mahoney. Bayesian experimental design using regu-larized determinantal point processes. In International Conference on Artificial Intelligence and Statis-tics, pages 31973207. PMLR, 2020. Michal Derezinski and Michael Mahoney. Distributed estimation of the inverse hessian by determi-nantal averaging. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d Alche-Buc, E. Fox, and R. Gar-nett, editors, Advances in Neural Information Processing Systems 32, pages 1140111411. CurranAssociates, Inc., 2019.",
  "Michael Langberg and Leonard J. Schulman. Universal epsilon-approximators for integrals, pages 598607. 2010": "Frederic Lavancier, Jesper Mller, and Ege Rubak. Determinantal point process models and statisticalinference. Journal of the Royal Statistical Society Series B: Statistical Methodology, 77(4):853877,December 2014. Odile Macchi. Processus ponctuels et coincidences Contributions `a letude theorique des processusponctuels, avec applications `a loptique statistique et aux communications optiques. PhD thesis, Uni-versite Paris-Sud, 1972."
}