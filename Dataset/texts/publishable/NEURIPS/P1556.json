{
  "Abstract": "Deep neural networks (DNNs) often suffer from the overconfidence issue, whereincorrect predictions are made with high confidence scores, hindering the appli-cations in critical systems. In this paper, we propose a novel approach calledTypicalness-Aware Learning (TAL) to address this issue and improve failure detec-tion performance. We observe that, with the cross-entropy loss, model predictionsare optimized to align with the corresponding labels via increasing logit magnitudeor refining logit direction. However, regarding atypical samples, the image contentand their labels may exhibit disparities. This discrepancy can lead to overfittingon atypical samples, ultimately resulting in the overconfidence issue that we aimto address. To tackle the problem, we have devised a metric that quantifies thetypicalness of each sample, enabling the dynamic adjustment of the logit magnitudeduring the training process. By allowing atypical samples to be adequately fittedwhile preserving reliable logit direction, the problem of overconfidence can be mit-igated. TAL has been extensively evaluated on benchmark datasets, and the resultsdemonstrate its superiority over existing failure detection methods. Specifically,TAL achieves a more than 5% improvement on CIFAR100 in terms of the AreaUnder the Risk-Coverage Curve (AURC) compared to the state-of-the-art. Code isavailable at",
  "Introduction": "Failure detection plays a vital role in machine learning applications, particularly in high-risk domainswhere the reliability and trustworthiness of predictions are crucial. Applications such as medicaldiagnosis Esteva et al. , autonomous driving Janai et al. , Yang et al. , andother visual perception tasks Lai et al. , Tian et al. , Tang et al. [2024b], Luo et al. ,Tian et al. , Peng et al. , Tian et al. , Peng et al. , Tian et al. [2022b] requireaccurate assessments of prediction confidence before making critical decisions. The goal of failuredetection is to enhance the reliability and trustworthiness of predictions, ensuring that high-confidencepredictions are relied upon while low-confidence predictions are appropriately rejected Moon et al., Zhang et al. . This is essential for maintaining the safety and effectiveness of theseapplications. Indeed, deep neural networks (DNNs) trained using the cross-entropy loss often suffer from the issueof overconfidence. This leads to unreliable confidence scores, which in turn hinder the effectivenessof failure detection methods. It is common for models to make incorrect predictions with highconfidence scores, sometimes even close to 1.0. A recent study called LogitNorm Wei et al. has shed light on this problem. It reveals that the softmax cross-entropy loss can cause the magnitudeof the logit vector to continue increasing, even when most training examples are correctly classified.This phenomenon contributes to the models overconfidence.",
  "Direction towards the Target": ": Illustration of the motivation. We observe that directly aligning the predictions of atypicalsamples to the target label is not appropriate, causing overconfidence (horse with 95% confidence).Instead, the confidence should be aligned with the human perception. During training, the cross-entropy loss increases the magnitude f and adjusts their direction towards the target (represented bythe angle ). Consider this example where an image of a human body with a horse head is presented,the loss may optimize towards f 2 in the blue box, which is not the ideal outcome direction. Instead, itwould be better to optimize towards f 1, rather than being biased towards either one, ensuring a morebalanced and unbiased representation and allowing for a more accurate estimation of confidence. To alleviate the overconfidence problem, LogitNorm Wei et al. proposes assigning a constantmagnitude to decouple the influence of the magnitude during optimization. The cross-entropy losswith the decoupled logit vector f = f(x; ) is defined as what follows:",
  "Ci=1 e||f|| f i ,(1)": "where is the parameters of the DNN model, x is the input image with label y, the logit vector fis decoupled into the magnitude f and the directions f. Based on the decomposition of the logitvector in Eq. (1), we can observe that the overconfidence issue could stem from either increasing for decreasing (the angle between the directions of prediction and label) during training. Key observation & Motivation.Following the exclusion of the logit magnitudes impact byLogitNorm Wei et al. , we posit that the risk of the overconfidence issue still arises from logitdirections. Typical samples, which have clear contextual information, help models generalize well.However, optimizing the direction for ambiguous atypical samples can still cause overconfidence. Inthese cases, the labels do not match the image context well. Aligning the logit direction of atypicalsamples may still lead to high softmax scores near 1.0 which worsens the overconfidence problem. According to previous work Zhu et al. [2023b], Yuksekgonul et al. , the definitions of typicaland atypical samples are based on their semantic similarity to most samples and the ease with whichthe model learns them. Specifically: Typical samples are those that exhibit similarity to a majority of other samples at the semanticlevel. These samples possess typical features that are easier for deep neural networks to learnand generalize. Atypical samples, on the other hand, differ significantly from other samples at the semanticlevel. They pose a challenge for the model to generalize due to their uniqueness. These samplesare often located near the decision boundary, causing the model to have higher uncertainty inmaking predictions for them.In , we present an atypical example to illustrate the issue at hand. Despite the ground-truth labelbeing a horse, the image depicts a horse with a human body, which could reasonably be predicted aseither a human or a horse with a confidence score of around 50%. However, the model incorrectlypredicts the image as a horse with an excessively high confidence score of 95%. Upon examiningEq. (1), we observe that the confidence score is determined by two crucial factors: magnitude anddirection. This prompts an important question regarding the decoupling of these factors to determinewhich one is more reliable in accurately approximating real confidence. Addressing this inquiry isessential for effective failure detection. Our approach.Based on the aforementioned observations, we propose a novel approach calledTypicalness-Aware Learning (TAL). TAL dynamically adjusts the magnitudes of logits based on thetypicalness of the samples, allowing for differentiated treatments of typical and atypical samples. Bydoing so, TAL aims to mitigate the adverse effects caused by atypical samples and emphasizes that the direction of logits serves as a more reliable indicator of model confidence. In the blue dashed boxof , we provide an example that illustrates the impact of TAL on an atypical sample. The logitvector could be changed from f 2 to f 1, indicating that the scores obtained with f for both \"horse\"and \"human\" become nearly equal. This alignment better aligns with human perception, highlightingthe effectiveness of TAL in improving model confidence estimation. The proposed TAL approach is model-agnostic, making it easily applicable to models with variousarchitectures, such as CNN He et al. [2016b] and ViT Dosovitskiy et al. . Experimental resultson benchmark datasets, including CIFAR10, CIFAR100, and ImageNet, demonstrate the superiorityof TAL over existing failure detection methods. Specifically, on CIFAR100, our method achievesa significant improvement of more than 5% in terms of the Area Under the Risk-Coverage Curve(AURC) compared to the state-of-the-art method Zhu et al. .",
  "In summary, the main contributions of this paper are as follows:": "We propose a new insight that the overconfidence might stem from the presence of atypicalsamples, whose labels fail to accurately describe the images. This forces the models to conformto these imperfect labels during training, resulting in unreliable confidence scores. In order to mitigate the issue of overfitting on atypical samples, we introduce the Typicalness-Aware Learning (TAL), which enables the identification and separate optimization of typicaland atypical samples, thereby alleviating the problem of overconfidence. Extensive experiments demonstrate the effectiveness and robustness of TAL. Besides, TALhas no structural constraints to the target model and is complementary to other existing failuredetection methods.",
  "Background and Preliminary": "Prior to introducing our method, we present the background of Failure Detection (FD). Additionally,we highlight the distinctions between failure detection and two closely related concepts: ConfidenceCalibration (CC) and Out-of-Distribution detection (OoD-D). Failure Detection. Failure detection (FD) Jaeger et al. , Moon et al. , Zhu et al. [2022,2023a] aims to differentiate between correct and incorrect predictions by utilizing the ranking of theirconfidence levels. In particular, a confidence-rate function () is employed to assess the confidencelevel of each prediction. High-confidence predictions are accepted, while low-confidence predictionsare rejected. By using a predetermined threshold R+, users can make informed decisions basedon the following function g:",
  "where () denotes a confidence-rate function, such as the maximum softmax probability": "Failure Detection vs. Confidence Calibration. Confidence calibration (CC) Thulasidasan et al., Lin et al. , Mller et al. , Yun et al. primarily emphasizes the alignmentof predicted probabilities with the actual likelihood of correctness, rather than explicitly detectingfailed predictions as in FD. The goal of CC is to ensure that the predictive confidence is indicative ofthe true probability of correctness:",
  "This implies that when a model predicts a set of inputs x to belong to class y with a probability p,we would expect approximately p of those inputs to truly belong to class y": "However, as observed by Zhu et al. , models calibrated with CC algorithms do not perform wellin FD. Traditional metrics used to evaluate CC, such as the Expected Calibration Error (ECE Naeiniet al. ), do not accurately reflect performance in FD scenarios. Instead, alternative metrics likethe Area Under the Risk-Coverage Curve (AURC El-Yaniv et al. ) and the Area Under theReceiver Operating Characteristic Curve (AUROC Bradley ) are recommended for assessingFD performance. Failure Detection vs. Out-of-Distribution Detection. While both Out-of-Distribution Detection(OoD-D) and failure detection tasks aim to enhance confidence reliability, they have distinct objectives,as depicted in . OoD-D focuses on rejecting predictions of semantic shift while accepting",
  "class": ": The differences between closely related tasks. The blue curve represents the decisionboundary, and the shaded area in the figure indicates incorrect predictions. (a) illustrates the objectiveof OoD-D tasks to reject predictions with semantic shifts and accept in-distribution predictions,without concern for predictions with covariate shifts. (b) shows the old setting of FD tasks, acceptingcorrect in-distribution predictions and rejecting incorrect out-of-distribution predictions. (c) displaysthe new setting of FD tasks, accepting correct in-distribution predictions and correct predictionswith covariate shifts, while rejecting incorrect in-distribution predictions, incorrect predictions withcovariate shifts, and predictions with semantic shifts. (d) illustrates examples of OoD-D, Old FD,and New FD tasks. A classifier trained on CIFAR10 Krizhevsky and Hinton is evaluated on 6images under a whole range of relevant distribution shifts: For instance, the 3rd and the 4th images ingrayscale depict an airplane and a horse which encounter covariate shifts from that in the originalCIFAR10. The 5th and the 6th images depict samples belonging to unseen categories with semanticshifts. in-distribution predictions. However, it does not explicitly address the rejection of cases affected bycovariate shifts. Additionally, through empirical observations in Sec. 4, we find that OoD-D methodsare not well-suited for the Failure Detection task. Traditional Failure Detection (Old FD) vs. New Failure Detection (New FD). Traditional failuredetection methods Zhu et al. [2022, 2023a], Moon et al. primarily focus on assessing theaccuracy of predictions for in-distribution data. They also evaluate the discriminative performance ofdistinguishing correct and incorrect predictions for covariate shift data based on confidence scores.While these approaches address certain aspects of distribution shifts, they overlook the semanticshifts. To address the limitations of Out-of-Distribution Detection (OoD-D) and traditional failure detection(Old FD) methods, Jaeger et al. proposes a new setting called New FD. The objective of NewFD is to accept correct predictions for both in-distribution and covariate shift samples, while rejectingincorrect predictions for all possible failures, including in-distribution, covariate shift, and semanticshift samples. Compared to OoD-D and Old FD, it enables more effective decision-making in realworld.",
  "Method": "In this section, we present our proposed strategy called Typicalness-Aware Learning (TAL), as shownin . First, in Sec. 3.1, we address the shortcomings of existing training objectives and identifyoverfitting of atypical samples as a potential cause of overconfidence. Next, in Sec. 3.2, we outlinethe methodology used to calculate the \"typicalness\" of samples, enabling selective optimization andmitigating the negative impact of atypical samples. Finally, in Sec. 3.3, we introduce the TAL strategy,",
  "Learning": ": The framework of TAL. During training, statistical information (mean j and variancej) of features from correct predictions updates the Historical Features Queue (HFQ) at time-step t.The typicalness measure is calculated by comparing these statistics between the current batch andthe HFQ. This influences the overall loss calculation, guiding the model to differentiate betweenatypical and typical samples. In the inference phase, TAL operates similarly to a model trainedwith conventional cross-entropy. Confidence is derived from the cosine similarity of the predictedlogit direction, emphasizing our approach of using direction as a more reliable confidence metric.The framework distinguishes between typical (high ) and atypical (low ) samples, influencing theoptimization process accordingly.",
  "Revisit the Cross-entropy Loss": "In Eq. (1), the optimization of the cross-entropy loss involves either increasing the magnitude of thelogits or aligning them better with the labels. However, LogitNorm Wei et al. researchers haveobserved that as the training progresses and the model becomes more accurate in classifying samples,it tends to generate significantly larger logit magnitudes, leading to overconfidence. To address thisissue, LogitNorm introduces a constant magnitude T in Eq. (4) to mitigate the problem:",
  "ki=1 e f iT .(4)": "By keeping the magnitude constant in Eq. (4), the model places greater emphasis on producingfeatures that align more closely with the target label in terms of direction, in order to minimize thetraining loss. However, as shown in , the presence of atypical samples with ambiguous contentand labels may still cause overconfidence. Hence, it is imperative to devise a method that allows forthe differentiation and separate optimization of typical and atypical samples. Drawing inspiration from the cognitive process of human decision-making, it is reasonable todistinguish between typical and atypical samples by leveraging the knowledge acquired during thetraining phase. This approach allows us to effectively mitigate the negative effects of atypical samples,thereby preventing the occurrence of erroneous overconfidence.",
  "Distinguish Typical and Atypical Samples": "To differentiate typical samples from atypical ones, we introduce a method for evaluating typicalnessand implement typicalness-aware learning (TAL). This approach entails calculating the mean andvariance of the feature representations. Specifically, we calculate the mean and variance of eachsamples feature channels based on insights from CORES Tang et al. [2024a]. The insight stems fromthe observation that in-distribution samples show larger magnitudes (mean) and variations (variance)in convolutional responses across channels compared to OoD samples, which are a type of atypicalsample. The mean response of OOD samples is smaller than correct ID samples, as shown in (a).These statistical characteristics are subsequently compared to a set of historical data, representingtypical samples, stored in a structured queue known as the \"Historical Feature Queue\" (HFQ). By",
  "comparing the statistical features of the input samples to those in the HFQ, we can quantify theirtypicalness": "Initialize and update HFQ. We commence the process by initializing the HFQ, denoted as Q, witha predetermined size equivalent to the number of samples in the training dataset. This structuredqueue is responsible for retaining the mean and variance of feature representations for typical samplesidentified throughout the training phase. To establish the initial state of the queue, we do not adopt the model trained from scratch. Instead,we employ a model that has been trained for a few epochs, corresponding to a small portion () ofthe total training epochs. This approach ensures the quality of the queue during the early stagesof training. In this study, we set = 0.05, which corresponds to 5% of the total training duration.During this initialization phase, for each batch of data, we calculate the mean () and variance (2) ofthe feature vectors for each correctly predicted sample. Each sample in the queue stores its statisticalfeatures, denoted as follows, given a prediction y and the ground truth label y:",
  "where i and 2i represent the mean and variance of the feature vectors of the i-th sample, respectively": "Once initialized, the statistics (mean and variance) of accurately predicted samples in each batchare directly added to the queue, as shown in . The queue has a fixed length of 20,000, andthe ablation study is provided in Sec 4.3. The queue is updated using a First-In-First-Out (FIFO)approach, guaranteeing that it preserves a representative assortment of typical samples observedthroughout the training process, while also adapting to the evolving data distributions. We empiricallyfind this simple strategy works well in our experiments. Typicalness assessment. To evaluate the typicalness of a new sample, we first calculate the mean(new) and variance (2new) of its features f. Subsequently, we compute the L2 distance d betweenthe feature distribution of the new sample, represented by new and 2new, and the distributions ofthe features stored in the HFQ, denoted as (j, 2j ) Q. Finally, we normalize the resulting distanceusing min-max normalization to obtain the typicalness .",
  "dmax dmin.(7)": "Where dmin and dmax represent the minimum and maximum distances of samples in the batch.Eq. (7) normalizes the value of within the range of , then can serve as a indicator of sampletypicalness. A high value suggests that the sample is highly typical compared to the historical data.Conversely, a low value indicates an atypical or anomalous sample.",
  "Typicalness-Aware Learning": "Sec. 3.1 highlights the potential negative impact of atypical samples on the training process. Buildingupon the insights provided in Sec. 3.2, we now introduce Typicalness-Aware Learning (TAL) in thissection. TAL leverages the typicalness to distinguish between typical and atypical samples duringthe optimization process. This approach aims to mitigate the issue of overconfidence that arises fromthe presence of atypical samples. The training objective of TAL. The training objective of TAL is defined by incorporating anadditional loss term LTAL. This is achieved by modifying the LogitNorm equation, denoted as Eq. (4),to Eq. (8) where the samples x are assigned with dynamic magnitudes T() based on typicalness .",
  "where we empirically set Tmax and Tmin to 10 and 100, and they perform well on different benchmarks.The ablation study on different values is shown in Sec. 4.3": "Specifically, in Eq. (9), a smaller magnitude T() will be assigned to typical samples with large, and a larger magnitude T() will be assigned to less typical samples with smaller , enablingdifferent treatments for typical/atypical samples. In this manner, for atypical samples, a higher valueof T() reduces the influence that pulls them towards the label direction. This helps prevent theirlogit directions from being excessively optimized. In other words, the inverse proportionality between T() and encourages the model to yielddirections of f that are well-aligned with the labels for the typical samples with large by assigning asmall magnitude T(). Conversely, for atypical samples with small , the directions are not requiredto be as precise as the typical ones as the current T() is large, to mitigate the adverse impactsbrought by the ambiguous label. To this end, the direction f can serve as a more reliable indicator ofthe model confidence.",
  "The TAL loss, denoted as LTAL, is utilized to optimize the directions of reliable typical samples withlarge typicalness , as well as potentially some atypical samples with small": "The CE loss (not only relying on the CE loss, as it is regulated by ) for atypical samples enables theoptimization of both direction and magnitude. This may help reduce the adverse effects of atypicalsamples on the direction, enhancing the reliability of direction as a confidence indicator. This ensuresthat the optimization force on the logit directions of atypical samples is weaker compared to that oftypical samples. The inference process does not involve the calculation of typicalness, and the onlydifference from the normal inference process is that our method uses Cosine as the confidence score. To summarize, our proposed approach, as indicated in Eq. (10), enables models to selectively andadaptively optimize typical and atypical samples according to their typicalness values. This strategyenhances the reliability of feature directions as indicators of model confidence, ultimately improvingthe performance on failure detection task.",
  "Experiments": "To evaluate the effectiveness of the proposed Typicalness-Aware Learning (TAL) strategy, we conductextensive experiments on various datasets, network architectures, and failure detection (FD) settings.More details such as the training configuration can be found in Appendix A. Datasets and models. We first evaluate on the small-scale CIFAR-100 Krizhevsky and Hinton dataset with SVHN Goodfellow et al. as its out-of-distribution (OOD) test set. Todemonstrate scalability, we further conduct experiments on large-scale ImageNet Deng et al. using ResNet-50, with Textures Cimpoi et al. and WILDS Koh et al. serving as OODdata. For CIFAR-100, we verify TALs effectiveness across diverse architectures including ResNet Heet al. [2016a], WRNet Zagoruyko and Komodakis , DenseNet Huang et al. , and thetransformer-based DeiT-Small Touvron et al. . Detailed experimental results are provided inAppendix C. Three settings. We evaluate TAL under three different settings: Old FD setting, OOD detectionsetting, and New FD setting (detailed in ). While Old FD distinguishes between correctand incorrect in-distribution predictions, and OOD detection identifies out-of-distribution samples,our New FD setting aims to separate correctly predicted in-distribution samples from both mis-classified and out-of-distribution samples. We maintain a 1:1 ratio between in-distribution andout-of-distribution samples in testing, and also report results for Old FD and OOD detection settingsfor completeness.",
  "Baselines. We compare our proposed TAL method against classical Maximum Softmax Probability(MSP), MaxLogitHendrycks and Gimpel , CosineZhang and Xiang , Energy Liu et al": ", Entropy Tian et al. [2022a], Mahalanobis De Maesschalck et al. , Gradnorm Huanget al. , SIRC Xia and Bouganis and recent LogitNorm Wei et al. , OpenMix Zhuet al. [2023a] and (FMFP) Zhu et al. . It is worth noting that FMFP focuses on improvingaccuracy for failure detection. Evaluation metrics. To comprehensively assess the performance of TAL in failure detection, weadopt three widely recognized evaluation metrics Jaeger et al. , Zhu et al. , Galil et al., including Area Under the Risk-Coverage Curve (AURC), Area Under the Receiver OperatingCharacteristic Curve (AUROC), False Positive Rate at 95% True Positive Rate (FPR95).",
  "Comparisions with the State-of-the-art on CIFAR": "Evaluation with CNN-based architectures. As shown in Tab. 1, our TAL strategy outperformsexisting methods in New FD settings. Here are the key observations: 1) OoD methods like Energyand LogitNorm do not achieve satisfactory performance in the FD task. Please refer to Appendix Bfor explanation. 2) TAL consistently surpasses the baseline MSP and MaxLogit across variousnetwork architectures by a large margin. 3) In terms of comparison with FMFP, the prior SOTAmethod in the field of FD, our analysis reveals that TAL is complementary to the FMFP method andexhibits superior performance when combined with it. 4) Regarding Openmix, the metrics presenteddemonstrate that it falls short when compared to TAL.",
  "Comparisions with the Baseline on ImageNet": "To showcase the scalability of our approach, we present the results on ImageNet in . It isobvious that our TAL strategy consistently enhances the failure detection performance of the baselinemethod, significantly improving the reliability of confidence. Notably, TAL reduces the AURC by3.7 and 11.6 points, indicating a better overall performance in distinguishing between correct and",
  "incorrect predictions. It is worth noting that TAL achieves these impressive improvements whilemaintaining a comparable accuracy to the MSP baseline": "Additionally, we present the Risk-Coverage (RC) curves ((c)) for both old and new FD tasksettings on ImageNet. The comparison between TAL and baseline RC curves demonstrates theeffectiveness of our method. (d) further visualizes typical and atypical data examples. For thefish category in ImageNet, typical data includes common fish images, while atypical data comprisesboth rare fish images from ImageNet and out-of-distribution samples.",
  "Ablation Study": "The ablation study of key components. We conduct experimental ablations on the componentsof our TAL loss with CIFAR100. The results are summarized in . With the dynamicmagnitude T() strategy, we achieve substantial enhancements in Failure Detection performance,which manifests the effectiveness of integrating typicalness-aware strategies into training approaches. : Ablation of the key components. Best are bolded and second best are underlined. AURCand EAURC are multiplied by 103, the remaining metrics are percentages except ACC. \"Fixed T\"means the dynamic magnitude T() in TAL is not adopted.",
  "TAL loss (Dynamic T) + Cross entropyOld FD94.3349.4385.5861.2438.6993.5668.700.72New FD351.4972.6988.9247.4452.4683.0392.860.72": "The impacts of Tmin and Tmax. We perform an ablation study using ResNet110 on the CIFAR10and CIFAR100 datasets to examine the impact of Tmin and Tmax on failure detection performance. (a) and (b) present the experimental results of the failure detection metric EAURC forCIFAR10 and CIFAR100, respectively. Darker regions in the figures correspond to lower values ofthe metric, indicating superior failure detection performance. The findings suggest that while Tminshould not be set too small, a moderate increase in Tmax can enhance failure detection capabilities. The effects of the length of Historical Feature Queue. We conduct an ablation study on theCIFAR100 dataset to examine the impact of queue length on failure detection performance. Theoriginal CIFAR100 dataset consists of 50,000 training images, with 5,000 images reserved forvalidation and the remaining 45,000 images used for training. The results, depicted in (c), demonstrate that queue lengths ranging from 10,000 to 50,000 yield similar failure detection",
  "(d) Typical and atypical examples": ": (a) Comparison of the Mean of Features between ID and OOD; (b) Comparison of differentmethods for measuring typicality; (c) The Risk-Coverage curves on old and new setting FD tasks; (d)Examples of typical and atypical examples. Ablation of Typicality Measures. As depicted in (b), we have conduced extra ablationexperiments with K-nearest neighbor (KNN) distance and Gaussian Mixture Models (GMM) toassess typicality. These alternative measures did not enhance performance (lower AURC is preferable),thereby reinforcing the validity of our selection of mean/variance criteria.",
  "Concluding Remarks": "Summary. This paper introduces Typicalness-Aware Learning (TAL), a novel approach for mitigatingoverconfidence in DNNs and improving failure detection performance. The effectiveness of TALcan be attributed to a crucial insight: overconfidence in deep neural networks (DNNs) may arisewhen models are compelled to conform to labels that inadequately describe the image content ofatypical samples. To address this issue, TAL leverages the concept of typicalness to differentiate theoptimization of typical and atypical samples, thereby enhancing the reliability of confidence scores.Extensive experiments have been conducted to validate the effectiveness and robustness of TAL. Wehope TAL can inspire new ideas for further enhancing the trustworthiness of deep learning models. Limitations. The main contribution of TAL lies in recognizing the issue of overfitting atypicalsamples as a cause of overconfidence and proposing a comprehensive framework to tackle thisproblem. Given that the methods adopted in this work are simple yet effective, there is still potentialfor further improvement by incorporating more advanced designs, such as the methods for typicalnesscalculation and the dynamic magnitude generation. These are left as future work to be explored. Broader impacts. As deep learning models become increasingly integrated into critical systems,from autonomous vehicles to medical diagnostics, the need for accurate and reliable confidencescores is paramount. TALs ability to improve failure detection performance directly addresses thisneed, potentially leading to safer and more dependable AI systems. This work was supported by National Natural Science Foundation of China (grant No. 62376068,grant No. 62350710797), by Guangdong Basic and Applied Basic Research Foundation (grantNo. 2023B1515120065), by Shenzhen Science and Technology Innovation Program (grant No.JCYJ20220818102414031). Kendrick Boyd, Kevin H Eng, and C David Page. Area under the precision-recall curve: pointestimates and confidence intervals. In Proceedings of the European Conference on MachineLearning and Principles and Practice of Knowledge Discovery in Databases (ECML-PKDD),pages 451466. Springer, 2013.",
  "Roy De Maesschalck, Delphine Jouan-Rimbaud, and Dsir L Massart. The mahalanobis distance.Chemometrics and intelligent laboratory systems, 50(1):118, 2000": "Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scalehierarchical image database. In Proceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition (CVPR), pages 248255. Ieee, 2009. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, ThomasUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. Animage is worth 16x16 words: Transformers for image recognition at scale.arXiv preprintarXiv:2010.11929, 2020.",
  "Ran El-Yaniv et al. On the foundations of noise-free selective classification. Journal of MachineLearning Research (JMLR), 11(5), 2010": "Andre Esteva, Brett Kuprel, Roberto A Novoa, Justin Ko, Susan M Swetter, Helen M Blau, andSebastian Thrun. Dermatologist-level classification of skin cancer with deep neural networks.Nature, 542(7639):115118, 2017. Ido Galil, Mohammed Dabbah, and Ran El-Yaniv. What can we learn from the selective predictionand uncertainty estimation performance of 523 imagenet classifiers?In Proceedings of theInternational Conference on Learning Representations (ICLR), 2023.",
  "Yonatan Geifman, Guy Uziel, and Ran El-Yaniv. Bias-reduced uncertainty estimation for deep neuralclassifiers. arXiv preprint arXiv:1805.08206, 2018": "Ian J Goodfellow, Yaroslav Bulatov, Julian Ibarz, Sacha Arnoud, and Vinay Shet. Multi-digit numberrecognition from street view imagery using deep convolutional neural networks. arXiv preprintarXiv:1312.6082, 2013. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity Mappings in Deep ResidualNetworks. In Proceedings of the Europeon Conference on Computer Vision (ECCV), volume9908, pages 630645. Springer International Publishing, Cham, 2016a. ISBN 978-3-319-46492-3978-3-319-46493-0. doi: 10.1007/978-3-319-46493-0_38. URL Series Title: Lecture Notes in Computer Science. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for imagerecognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR), June 2016b.",
  "Rui Huang, Andrew Geng, and Yixuan Li. On the importance of gradients for detecting distributionalshifts in the wild. Advances in Neural Information Processing Systems, 34:677689, 2021": "Paul F Jaeger, Carsten Tim Lth, Lukas Klein, and Till J. Bungert. A call to reflect on evaluation prac-tices for failure detection in image classification. In Proceedings of the International Conferenceon Learning Representations (ICLR), 2023. Joel Janai, Fatma Gney, Aseem Behl, Andreas Geiger, et al. Computer vision for autonomousvehicles: Problems, datasets and state of the art. Foundations and Trends in Computer Graphicsand Vision (FOUND TRENDS COMPUT), 12(13):1308, 2020. Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Bal-subramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, et al. Wilds: Abenchmark of in-the-wild distribution shifts. In International conference on machine learning,pages 56375664. PMLR, 2021.",
  "Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoningsegmentation via large language model. arXiv preprint arXiv:2308.00692, 2023": "Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollr. Focal loss for dense objectdetection. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages29802988, 2017. Weitang Liu, Xiaoyun Wang, John Owens, and Yixuan Li. Energy-based Out-of-distribution De-tection. In Proceedings of the Advances in Neural Information Processing Systems (NeurIPS),volume 33, pages 2146421475. Curran Associates, Inc., 2020. URL Xiaoliu Luo, Zhuotao Tian, Taiping Zhang, Bei Yu, Yuan Yan Tang, and Jiaya Jia. Pfenet++:Boosting few-shot semantic segmentation with the noise-filtered context-aware prior mask. IEEETransactions on Pattern Analysis and Machine Intelligence, 46(2):12731289, 2024. doi: 10.1109/TPAMI.2023.3329725. Jooyoung Moon, Jihyo Kim, Younghak Shin, and Sangheum Hwang. Confidence-Aware Learningfor Deep Neural Networks. In Proceedings of the International Conference on Machine Learning(ICML), pages 70347044. PMLR, November 2020. ISSN: 2640-3498.",
  "Rafael Mller, Simon Kornblith, and Geoffrey E Hinton. When does label smoothing help? Proceed-ings of the Advances in Neural Information Processing Systems (NeurIPS), 32, 2019": "Mahdi Pakdaman Naeini, Gregory Cooper, and Milos Hauskrecht. Obtaining well calibrated proba-bilities using bayesian binning. In Proceedings of the AAAI conference on artificial intelligence(AAAI), volume 29, 2015. Bohao Peng, Zhuotao Tian, Xiaoyang Wu, Chenyao Wang, Shu Liu, Jingyong Su, and Jiaya Jia.Hierarchical dense correlation distillation for few-shot segmentation. In Proceedings of theIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023.",
  "Bohao Peng, Xiaoyang Wu, Li Jiang, Yukang Chen, Hengshuang Zhao, Zhuotao Tian, and Jiaya Jia.Oa-cnns: Omni-adaptive sparse cnns for 3d semantic segmentation. In CVPR, 2024": "Keke Tang, Chao Hou, Weilong Peng, Runnan Chen, Peican Zhu, Wenping Wang, and Zhihong Tian.Cores: Convolutional response-based score for out-of-distribution detection. In Proceedings of theIEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1091610925, 2024a. Longxiang Tang, Zhuotao Tian, Kai Li, Chunming He, Hantao Zhou, Hengshuang Zhao, Xiu Li, andJiaya Jia. Mind the interference: Retaining pre-trained knowledge in parameter efficient continuallearning of vision-language models. arXiv preprint arXiv:2407.05342, 2024b. Sunil Thulasidasan, Gopinath Chennupati, Jeff A Bilmes, Tanmoy Bhattacharya, and Sarah Michalak.On mixup training: Improved calibration and predictive uncertainty for deep neural networks.Proceedings of the Advances in Neural Information Processing Systems (NeurIPS), 32, 2019. Yu Tian, Yuyuan Liu, Guansong Pang, Fengbei Liu, Yuanhong Chen, and Gustavo Carneiro. Pixel-wise energy-biased abstention learning for anomaly segmentation on complex urban driving scenes.In Proceedings of the Europeon Conference on Computer Vision (ECCV), pages 246263. Springer,2022a. Zhuotao Tian, Michelle Shu, Pengyuan Lyu, Ruiyu Li, Chao Zhou, Xiaoyong Shen, and Jiaya Jia.Learning shape-aware embedding for scene text detection. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition (CVPR), June 2019.",
  "Zhuotao Tian, Hengshuang Zhao, Michelle Shu, Zhicheng Yang, Ruiyu Li, and Jiaya Jia. Priorguided feature enrichment network for few-shot segmentation. TPAMI, 2020": "Zhuotao Tian, Xin Lai, Li Jiang, Shu Liu, Michelle Shu, Hengshuang Zhao, and Jiaya Jia. Generalizedfew-shot semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition (CVPR), 2022b. Zhuotao Tian, Jiequan Cui, Li Jiang, Xiaojuan Qi, Xin Lai, Yixin Chen, Shu Liu, and Jiaya Jia.Learning context-aware classifier for semantic segmentation. In Proceedings of the Thirty-SeventhAAAI Conference on Artificial Intelligence, 2023. Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and HervJgou. Training data-efficient image transformers & distillation through attention, January 2021.URL arXiv:2012.12877 [cs]. Hongxin Wei, Renchunzi Xie, Hao Cheng, Lei Feng, Bo An, and Yixuan Li. Mitigating NeuralNetwork Overconfidence with Logit Normalization. In Proceedings of the International Conferenceon Machine Learning (ICML), pages 2363123644. PMLR, June 2022. ISSN: 2640-3498. Guoxuan Xia and Christos-Savvas Bouganis. Augmenting softmax information for selective classifi-cation with out-of-distribution data. In Proceedings of the Asian Conference on Computer Vision,pages 19952012, 2022. Senqiao Yang, Jiaming Liu, Ray Zhang, Mingjie Pan, Zoey Guo, Xiaoqi Li, Zehui Chen, Peng Gao,Yandong Guo, and Shanghang Zhang. Lidar-llm: Exploring the potential of large language modelsfor 3d lidar understanding. arXiv preprint arXiv:2312.14074, 2023. Senqiao Yang, Zhuotao Tian, Li Jiang, and Jiaya Jia. Unified language-driven zero-shot domain adap-tation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,pages 2340723415, 2024.",
  "Mert Yuksekgonul, Linjun Zhang, James Y Zou, and Carlos Guestrin. Beyond confidence: Reliablemodels should also consider atypicality. Advances in Neural Information Processing Systems, 36,2024": "Sukmin Yun, Jongjin Park, Kimin Lee, and Jinwoo Shin. Regularizing class-wise predictions viaself-knowledge distillation. In Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition (CVPR), pages 1387613885, 2020. Sergey Zagoruyko and Nikos Komodakis. Wide Residual Networks. In Proceedings of the British Ma-chine Vision Conference (BMVC), York, France, January 2016. British Machine Vision Association.URL arXiv:1605.07146 [cs].",
  "Zihan Zhang and Xiang Xiang. Decoupling maxlogit for out-of-distribution detection. In Proceedingsof the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 33883397, 2023": "Fei Zhu, Zhen Cheng, Xu-Yao Zhang, and Cheng-Lin Liu. Rethinking confidence calibration forfailure prediction. In Proceedings of the Europeon Conference on Computer Vision (ECCV), pages518536. Springer, 2022. Fei Zhu, Zhen Cheng, Xu-Yao Zhang, and Cheng-Lin Liu. Openmix: Exploring outlier samples formisclassification detection. In Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition (CVPR), pages 1207412083, 2023a. Jianing Zhu, Hengzhuang Li, Jiangchao Yao, Tongliang Liu, Jianliang Xu, and Bo Han. Unleash-ing mask: Explore the intrinsic out-of-distribution detection capability. In Proceedings of theInternational Conference on Machine Learning (ICML), pages 4306843104. PMLR, 2023b.",
  "A.1Baselines": "We compare our proposed TAL method against seven baseline approaches for failure detection.1 Maximum Softmax Probability (MSP), 2 MaxLogitHendrycks and Gimpel and 3 Co-sineZhang and Xiang utilize the maximum softmax probability of f, the maximum logit (f)value and the cosine similarity between the f and the corresponding labels one-hot vector as theconfidence score, respectively. 4 Energy Score Liu et al. uses the negative energy of thesoftmax output as the confidence score. 5 LogitNorm Wei et al. normalizes the logits to afixed magnitude to improve confidence score reliability. 6 OpenMix Zhu et al. [2023a], a SOTAOOD detection method, leverages data augmentation techniques to enhance confidence score separa-tion between in-distribution and out-of-distribution samples. 7 Failure Misclassification FeaturePropagation (FMFP) Zhu et al. , a SOTA failure detection method, focuses on improvingmodel accuracy and confidence score reliability through stochastic weight averaging (SWA) andsharpness-aware minimization (SAM). We also explore combining the proposed TAL with FMFP( 8) to investigate their complementary effects.",
  "A.2Evaluation Metrics": "To comprehensively assess the performance of TAL in failure detection, we adopt nine widelyrecognized evaluation metrics Jaeger et al. , Zhu et al. , Galil et al. , including AreaUnder the Risk-Coverage Curve (AURC), Excess Area Under the Risk-Coverage Curve (EAURC),Area Under the Receiver Operating Characteristic Curve (AUROC), False Positive Rate at 95%True Positive Rate (FPR95), True Negative Rate at 95% True Positive Rate (TNR95), Area Underthe Precision-Recall curve of Suceess and Error (AUPR_Success and AUPR_Error). 1 AURC El-Yaniv et al. : Area Under the Risk-Coverage Curve, depicting the error rate as a function ofconfidence thresholds. 2 EAURC Geifman et al. : Excess Area Under the Risk-CoverageCurve, evaluating the ranking ability of confidence scores. 3 AUROC Bradley : Area Underthe Receiver Operating Characteristic Curve, illustrating the trade-off between true positive rate (TPR)and false positive rate (FPR). 4: False Positive Rate at 95% True Positive Rate. 5: True NegativeRate at 95% True Positive Rate.7 AUPR_SuccessBoyd et al. and 8 AUPR_Successrepresent two approximations for estimating the Area Under the Precision-Recall curve (AUPR), withAUPR_Success sampling thresholds from positive scores while AUPR_Error utilizes only scores ofobserved positive and negative instances as thresholds. 9: Test accuracy, providing a reference foroverall model performance.",
  "A.3Training Configuration": "For experiments on the CIFAR Krizhevsky and Hinton , we employ an SGD optimizer withan initial learning rate of 0.1, a momentum of 0.9, and a weight decay of 0.0005. The models aretrained for 200 epochs with a batch size of 256 on a single NVIDIA GeForce RTX 3090 GPU.Furthermore, we adopt a CosineAnnealingLR scheduler to adjust the learning rate during training.On ImageNet Deng et al. , we use the ResNet-50 architecture as our backbone. The models are",
  "BThe Reasons Why OoD method Performs Poor in FD": "It is interesting to note that in the Old Few-Shot Detection (FD) setting, most Out-of-Distribution(OoD) methods, such as Energy Score and LogitNorm, exhibit poor performance compared to baselinemethods like Maximum Softmax Probability (MSP) and MaxLogit. This decline in performance canbe attributed to the fact that while OoD methods effectively increase the confidence score gap betweenin-distribution and out-of-distribution samples, they inadvertently disrupt the natural confidence scorehierarchy within the in-distribution samples. As a result, there is a greater overlap in the confidencedistributions of correctly and incorrectly predicted in-distribution samples, as illustrated in .This observation highlights the importance of developing dedicated FD methods that can effectivelydistinguish between correct and incorrect predictions within the in-distribution data.",
  "Evaluation on Transformer-based architectures": "Considering the remarkable success of vision transformers as network architectures, it is crucial toincorporate a transformer-based network in our analysis and evaluate the effectiveness of our proposedmethod. It is worth noting that the substantial disparities between ViT Dosovitskiy et al. andCNN architectures in terms of their design, feature representation, and learning mechanisms maypose challenges in directly applying methods that have proven effective on CNNs to ViT models. Implementation Details of ViT. TAL can also be applied to Transformer architectures. However, it isimportant to note that certain CNN-based methods may not perform well on Transformer-based tasks.For example, the accuracy of OpenMix with default settings is significantly lower (approximately0.20) compared to the baseline. This indicates that OpenMix, originally designed for CNNs, doesnot effectively detect failures when applied to ViT models without appropriate modifications andadjustments that consider the unique architectural characteristics of ViT. Specifically, ViT learns the relationships between image patches through the Self-Attention mech-anism, resulting in distinct feature representations and gradient flow patterns compared to CNNs.Moreover, ViT typically employs different normalization techniques, such as Layer Normalization,instead of the commonly used Batch Normalization in CNNs. These differences can impact theeffectiveness of certain methods when applied to ViT. To successfully adapt these methods to ViT,appropriate modifications and adjustments may be necessary to account for the unique architecturalcharacteristics of ViT. To explore the performance of current popular failure detection methods on ViT models, we conductedexperiments using the pre-trained DeiT-Small model (\"deit_small_patch16_224\") from the timmlibrary. We employed the SGD optimizer with a base learning rate of 0.01, a weight decay of 0.0005,and a momentum of 0.9. The learning rate scheduler was set to CosineAnnealingLR, with the T_maxparameter determined by total training epochs. The experiments were conducted on the CIFAR100dataset, with a total of 25 training epochs and a batch size of 256. Experimental Results on ViT. The experimental results shown in Tab. 4 and Tab. 5 validate theapplicability of TAL to vision transformers. By prioritizing optimization with typical samples andconcurrently relaxing the requirements for atypical samples, TAL effectively mitigates overconfidenceand enhances failure detection performance. : New FD Setting evaluation on CIFAR100 with ViT. Mean and standard deviations of FailureDetection performance on CIFAR benchmarks. The experimental results are reported over fiveepochs. Best are bolded and second best are underlined. AURC and EAURC are multiplied by 103,the remaining metrics are percentages except ACC.",
  "The answer NA means that the paper has no limitation while the answer No means thatthe paper has limitations, but those are not discussed in the paper": "The authors are encouraged to create a separate \"Limitations\" section in their paper. The paper should point out any strong assumptions and how robust the results are toviolations of these assumptions (e.g., independence assumptions, noiseless settings,model well-specification, asymptotic approximations only holding locally). The authorsshould reflect on how these assumptions might be violated in practice and what theimplications would be. The authors should reflect on the scope of the claims made, e.g., if the approach wasonly tested on a few datasets or with a few runs. In general, empirical results oftendepend on implicit assumptions, which should be articulated. The authors should reflect on the factors that influence the performance of the approach.For example, a facial recognition algorithm may perform poorly when image resolutionis low or images are taken in low lighting. Or a speech-to-text system might not beused reliably to provide closed captions for online lectures because it fails to handletechnical jargon.",
  "If applicable, the authors should discuss possible limitations of their approach toaddress problems of privacy and fairness": "While the authors might fear that complete honesty about limitations might be used byreviewers as grounds for rejection, a worse outcome might be that reviewers discoverlimitations that arent acknowledged in the paper. The authors should use their bestjudgment and recognize that individual actions in favor of transparency play an impor-tant role in developing norms that preserve the integrity of the community. Reviewerswill be specifically instructed to not penalize honesty concerning limitations.",
  "If the contribution is a dataset and/or model, the authors should describe the steps takento make their results reproducible or verifiable": "Depending on the contribution, reproducibility can be accomplished in various ways.For example, if the contribution is a novel architecture, describing the architecture fullymight suffice, or if the contribution is a specific model and empirical evaluation, it maybe necessary to either make it possible for others to replicate the model with the samedataset, or provide access to the model. In general. releasing code and data is oftenone good way to accomplish this, but reproducibility can also be provided via detailedinstructions for how to replicate the results, access to a hosted model (e.g., in the caseof a large language model), releasing of a model checkpoint, or other means that areappropriate to the research performed. While NeurIPS does not require releasing code, the conference does require all submis-sions to provide some reasonable avenue for reproducibility, which may depend on thenature of the contribution. For example(a) If the contribution is primarily a new algorithm, the paper should make it clear howto reproduce that algorithm.",
  "(b) If the contribution is primarily a new model architecture, the paper should describethe architecture clearly and fully": "(c) If the contribution is a new model (e.g., a large language model), then there shouldeither be a way to access this model for reproducing the results or a way to reproducethe model (e.g., with an open-source dataset or instructions for how to constructthe dataset). (d) We recognize that reproducibility may be tricky in some cases, in which caseauthors are welcome to describe the particular way they provide for reproducibility.In the case of closed-source models, it may be that access to the model is limited insome way (e.g., to registered users), but it should be possible for other researchersto have some path to reproducing or verifying the results.",
  "Guidelines:": "The answer NA means that there is no societal impact of the work performed. If the authors answer NA or No, they should explain why their work has no societalimpact or why the paper does not address societal impact. Examples of negative societal impacts include potential malicious or unintended uses(e.g., disinformation, generating fake profiles, surveillance), fairness considerations(e.g., deployment of technologies that could make decisions that unfairly impact specificgroups), privacy considerations, and security considerations. The conference expects that many papers will be foundational research and not tiedto particular applications, let alone deployments. However, if there is a direct path toany negative applications, the authors should point it out. For example, it is legitimateto point out that an improvement in the quality of generative models could be used togenerate deepfakes for disinformation. On the other hand, it is not needed to point outthat a generic algorithm for optimizing neural networks could enable people to trainmodels that generate Deepfakes faster. The authors should consider possible harms that could arise when the technology isbeing used as intended and functioning correctly, harms that could arise when thetechnology is being used as intended but gives incorrect results, and harms followingfrom (intentional or unintentional) misuse of the technology. If there are negative societal impacts, the authors could also discuss possible mitigationstrategies (e.g., gated release of models, providing defenses in addition to attacks,mechanisms for monitoring misuse, mechanisms to monitor how a system learns fromfeedback over time, improving the efficiency and accessibility of ML).",
  ". Safeguards": "Question: Does the paper describe safeguards that have been put in place for responsiblerelease of data or models that have a high risk for misuse (e.g., pretrained language models,image generators, or scraped datasets)?Answer: [NA]Justification: The proposed TAL is focused on improving failure detection for generalclassification tasks without requiring the release of potentially unsafe data or models.Guidelines: The answer NA means that the paper poses no such risks. Released models that have a high risk for misuse or dual-use should be released withnecessary safeguards to allow for controlled use of the model, for example by requiringthat users adhere to usage guidelines or restrictions to access the model or implementingsafety filters.",
  ". Licenses for existing assets": "Question: Are the creators or original owners of assets (e.g., code, data, models), used inthe paper, properly credited and are the license and terms of use explicitly mentioned andproperly respected?Answer: [Yes]Justification: The paper properly credits and mentions the licenses for existing assets used.Guidelines: The answer NA means that the paper does not use existing assets. The authors should cite the original paper that produced the code package or dataset. The authors should state which version of the asset is used and, if possible, include aURL. The name of the license (e.g., CC-BY 4.0) should be included for each asset. For scraped data from a particular source (e.g., website), the copyright and terms ofservice of that source should be provided. If assets are released, the license, copyright information, and terms of use in thepackage should be provided. For popular datasets, paperswithcode.com/datasetshas curated licenses for some datasets. Their licensing guide can help determine thelicense of a dataset.",
  ". New Assets": "Question: Are new assets introduced in the paper well documented and is the documentationprovided alongside the assets?Answer: [Yes]Justification: The paper introduces new codes, which will be documented in the github link.Guidelines: The answer NA means that the paper does not release new assets. Researchers should communicate the details of the dataset/code/model as part of theirsubmissions via structured templates. This includes details about training, license,limitations, etc.",
  "According to the NeurIPS Code of Ethics, workers involved in data collection, curation,or other labor should be paid at least the minimum wage in the country of the datacollector": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with HumanSubjectsQuestion: Does the paper describe potential risks incurred by study participants, whethersuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)approvals (or an equivalent approval/review based on the requirements of your country orinstitution) were obtained?Answer: [NA]Justification: The paper does not involve research with human subjects.Guidelines:",
  "The answer NA means that the paper does not involve crowdsourcing nor research withhuman subjects": "Depending on the country in which research is conducted, IRB approval (or equivalent)may be required for any human subjects research. If you obtained IRB approval, youshould clearly state this in the paper. We recognize that the procedures for this may vary significantly between institutionsand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and theguidelines for their institution."
}