{
  "Abstract": "We introduce Constrained Human-AI Cooperation (CHAIC), an inclusive embodiedsocial intelligence challenge designed to test social perception and cooperationin embodied agents. In CHAIC, the goal is for an embodied agent equipped withegocentric observations to assist a human who may be operating under physicalconstraintse.g., unable to reach high places or confined to a wheelchairinperforming common household or outdoor tasks as efficiently as possible. Toachieve this, a successful helper must: (1) infer the humans intents and constraintsby following the human and observing their behaviors (social perception), and (2)make a cooperative plan tailored to the human partner to solve the task as quicklyas possible, working together as a team (cooperative planning). To benchmark thischallenge, we create four new agents with real physical constraints and eight long-horizon tasks featuring both indoor and outdoor scenes with various constraints,emergency events, and potential risks. We benchmark planning- and learning-based baselines on the challenge and introduce a new method that leverages largelanguage models and behavior modeling. Empirical evaluations demonstrate theeffectiveness of our benchmark in enabling systematic assessment of key aspectsof machine social intelligence. Our benchmark and code are publicly available at",
  "Introduction": "Humans possess a remarkable ability to observe, infer, and help others, even when others havedifferent mental models and physical constraints in the world from themselves (Warneken andTomasello, 2006). From a young age, humans are able to watch other people attempt to perform atask, and if other people fail, they can develop plans of action that best assist them. In contrast, AIagents struggle to exhibit such basic social skills and fail to adjust their plans for the specific humansthey wish to aid (Valmeekam et al., 2022; Ngo et al., 2022), rendering them poor personalized helpers. For AI agents to best assist human partners in performing tasks in the real world, they must possesstwo fundamental capabilities: (1) contextual perception, i.e., the ability to follow and observe humanbehavior and identify the specific goals and constraints faced by each human; and (2) cooperativeplanning, i.e., the ability to plan actions that are best tailored to helping each human with differentgoals and constraints. While there have been some embodied benchmarks and environments designedto test general multi-agent intelligence (Puig et al., 2021, 2023b; Gan et al., 2021), such efforts havelargely excluded the unique accessibility challenges that real humans may possess in the world andneglect the differences among individuals. Moreover, outdoor scenarios and emergencies are also",
  "arXiv:2411.01796v2 [cs.AI] 5 Nov 2024": ": Constrained Human-AI Cooperation (CHAIC) for benchmarking embodied agentsthat socially perceive and assist human partners with physical constraints. Left: A human partneris confined to a wheelchair and struggles to move past an obstacle. The helper agent infers the humanpartners constraints and intents and assists him by removing the obstacle. Right: In a moving housescenario, after observing a human partner fail to lift heavy furniture, the helper agent understands herintents and constraints and assists her in carrying the furniture together.",
  "prevalent in human life, but receive little attention in the embodied intelligence community (Deitkeet al., 2022)": "This paper introduces the first large-scale embodied social intelligence challenge with accessibilityexplicitly in mind: Constrained Human-AI Cooperation (CHAIC). In this challenge, an embodiedagent with egocentric visual observation must actively perceive and cooperate with a human partnerpossibly with physical constraints in a near photo- and physically realistic virtual environment tocomplete common household and outdoor tasks as efficiently as possible. This is motivated by theidea that people who need the most help from autonomous agents are those who are currently notexplicitly accounted for in embodied intelligence frameworks. In CHAIC, a helper agent needs tofollow and observe the human partner to infer their goals and constraints; then, the agent plansa user-tailored strategy for aiding the human in efficiently performing tasks together; moreover,with the existence of unexpected emergencies, the agent needs to be reactive and adjust its strategyaccordingly. To create the challenge with accessibility in mind, we design and implement four new agents with realphysical constraints that reflect the rich diversity of human partners in the real world. For example,a human partner confined to a wheelchair struggles to move past obstacles or a human partnerstruggles with heavy furniture when moving house in an outdoor scene, shown in , and eightlong-horizon tasks featuring both indoor and outdoor scenes on top of the ThreeDWorld (Gan et al.,2021), explicitly motivating the development of embodied agents that prioritize accessibility effortswhen learning and planning and can thrive in rich scenarios. We benchmark several baseline models, including planning- and learning-based agents, especiallythose powered by foundation models. We also introduce a new method for building agents thatcombines the behavior modeling capabilities of video models with the reasoning ability of largelanguage models. Our benchmark results suggest that current baselines have difficulty modelingpartner behaviors from raw RGB images, and LLM-driven agents are competitive agents in decision-making. We hope this new challenge will advance the study of social intelligence in embodied agentsin complex scenarios including diverse human partners with constraints and rich indoor and outdoorscenes. This initiative calls on the community to develop and evaluate embodied agents with a strongemphasis on accessibility and inclusivity.",
  "Related Work": "Embodied Multi-Agent Cooperation ChallengesOur benchmark and environment build on arich history of realistic 3D simulated environments (Zhou et al., 2024; Li et al., 2023; Padmakumaret al., 2022; Kolve et al., 2017; Shridhar et al., 2020; Misra et al., 2018; Zhu et al., 2017; Xia et al.,2018; Savva et al., 2019; Xiang et al., 2020). Various tasks and methods have been introduced formulti-agent cooperation (Lowe et al., 2017; Samvelyan et al., 2019; Carroll et al., 2019; Suarez et al.,2019; Jaderberg et al., 2019; Amato et al., 2019; Baker et al., 2020; Bard et al., 2020; Jain et al.,2020; Puig et al., 2023b; Wen et al., 2022; Szot et al., 2023; Zhang et al., 2023, 2024). Specifically,Puig et al. (2021, 2023a) explored the inter-agent perception of isomorphic agents during householdtasks. However, these works did not address the explicit challenge of actively perceiving diversehuman partners with physical constraints from visual observations and adapting the cooperationstrategy accordingly. In contrast, our challenge is designed explicitly not only to study the socialperception of the partners goals and constraints from visual observations but also to capture thenuances of human physical mobility constraints that might impair the successful completion of suchtasks. A contemporary work (Cao et al., 2024) also studies assistive agents for vulnerable groupsbut focuses only on indoor scenarios with oracle symbolic observations. In contrast, our proposedCHAIC Challenge features both indoor and outdoor scenarios, an egocentric visual observation,newly created physically constrained agents, and unexpected events, enabling rich, physics-driveninteractions on real-world assistive tasks. Accessibility in AI DesignPeople with disabilities or physical impairments are a central focus areaof study in robotics, including care for wheelchair users, elderly users, and users with aging-relatedailments like dementia (de Saille et al., 2022; Sundaresan et al., 2022; Broadbent et al., 2009; Bendaet al., 2020; Cooper et al., 2016; Lee et al., 2017). These works often study the best ways to design forinclusivity; in other words, how to best build assistive robots to handle the explicit physical needs ofthe users in question (Benda et al., 2020). We build on these design principles to create the first-everlarge-scale embodied intelligence environment that explicitly models such impairments.",
  "The Constrained Human-AI Cooperation (CHAIC) Challenge": "The Constrained Human-AI Cooperation (CHAIC) Challenge seeks to study how embodiedagents perform in terms of social perceptions of human partners with diverse physical constraintsand cooperative planning abilities within rich scenarios. Built on top of ThreeDWorld, a realistic3D embodied AI platform, we design and implement four new agents with real physical constraints(.1) and eight tasks featuring both indoor and outdoor scenes, including emergencies(.2). For each task, there is a constrained agent mimicking a human partner with capabilityconstraints, trying to find and transport some target objects to a specific goal location, and a helperagent trying to infer the constrained agents goal and capability constraints through active perceptionof its behaviors to assist the constrained agent better. The success of the helper agent is measured bythe ratio of target objects successfully transported by both of them. provides an overview ofthe challenge, with further details in .3.",
  "Each agent possesses two properties: reaching range and strength. An agent can successfully interactwith objects whose heights are within its reaching range and whose weights are lighter than its": ": Overview of CHAIC challenge: We present four agents with diverse capability constraintsand eight tasks built around these constraints, featuring both indoor and outdoor scenarios. Thetasks are named no constraint, high container, high goal location, high target object, obstacle, lowtarget object, shopping, and moving house (four of them are shown on the left). In each task, thereare objects, containers, and a goal location. A helper agent needs to infer the partners intents andconstraints from its egocentric observations (shown on the right) and make a tailored plan to assistthe partner in transporting the intended target objects to the goal location using containers as tools. strength limit. When an agent attempts an action that exceeds its capabilities, the action does not failimmediately but instead has a success rate. This rate is calculated using the formula exp(/)/,where represents the excess amount, and and are constants. If an action exceeds multiplecapability thresholds, the probabilities of success are multiplied. We have developed the followingconstrained agents:",
  "Challenge Details": "In CHAIC, an embodied helper agent Ah is tasked to infer the goal G and the constraints of aconstrained agent Am and assist Am in finding and transporting a set of target objects Ot fromrandom locations to a goal location Lg. There are containers scattered in the environment, which theagents can use to transport more objects simultaneously. An agent could take two objects at a timewithout a container, and the capacity of a container is set to three. Formally, a task in the challenge is defined by the goal G of the constrained agent Am (i.e., a setof goal predicates describing the final desired state) and an initial environment E where the helperagent Ah is placed alongside the constrained agent Am to complete the task. The ground truth goalsand constraints of the constrained agent are hidden from the helper agent Ah, thereby explicitlymotivating the need for active perception for the agent to infer intents and constraints.",
  "Task Generation": "Indoor TaskTo generate an indoor task, a floorplan configuration with six to eight interconnectedrooms and a target task is initially sampled from predefined sets. For each scene, objects related togoals in the predefined set are placed on low surfaces such as tables, chairs, sofas, and floors for lowobjects, and higher surfaces like cabinets or refrigerators for high objects. However, only a subset ofthe objects is the target object set. The target object set is a set that includes all objects related to aspecific type like food or fruit, one object randomly selected from a non-target set, and two additionalfragile vases if the task is a high-target task. The number of targets is around ten. Then, a goallocation and up to six containers are added to the scene based on available space and task constraints. We randomly initialize two agents (one constrained agent Am and one helper Ah), and each agent isplaced in a free space at least 0.5 meters away from the nearest wall. This setup ensures sufficientinitial distance between the agents and the walls, allowing unrestricted movement at the beginning ofthe task. Outdoor TaskThe generation of outdoor tasks is largely the same as the indoor task generation.For the shopping task, six shops are generated and spread out on both sides of the road, and each shopsells one specific category of items. The goal location of the shopping task is a fixed, predeterminedplace in front of the bicycle agents house. In the moving house task, the target objects include fivepieces of furniture on the road in front of a house. The goal location is a truck parked nearby. Thedetails of outdoor task generation can be found in Appendix G. : LLM+BM Helper Implementation Pipeline: An overview of the LLM+BM Helper withspecific modules for Perception, Behavior Modeling, Decision, and Execution. (1) The perceptionmodule detects objects from raw RGB images; (2) the memory module builds the semantic mapof the environment using depth images and records behaviors; (3) the behavior modeling modulerecognizes the action of the partner and localizes the object corresponding to the action; (4) thedecision module decides plans for the next steps using foundation models; and (5) the executionmodule generates low-level actions.",
  "Dataset Construction": "For each of the eight tasks, we create 12 episodes for training and 12 episodes for testing, resultingin approximately 200 episodes in total. We ensure that the environments of the test set are differentfrom those of the training set. We randomly sample the initial starting states for each task. Anepisode terminates when all goal predicates of the task are satisfied or when the maximum time stephorizon T = 3000 frames is reached (for the moving furniture task, the maximum time step horizonis T = 1500).",
  "Language Agent Augmented with Behavior Modeling Module": "We also introduce a new agent framework combining the prowess of action recognition models andthe reasoning ability of large language models (LLMs). Due to their simplicity and generalizationability, LLMs can also be implemented in other environments or the real world. We built a behaviormodeling module, which models the behaviors of the constrained agent via an action recognitionmodel and incorporated it into the CoELA framework (Zhang et al., 2023) with four other modules:(1) the perception module, which transforms the raw RGB-D observations into structured semanticmaps via an object detection model; (2) the memory module, which saves all the history informationin a structured manner; (3) the decision module, which generates high-level plans and is driven bylarge language models; and (4) the execution module, which turns the generated plans into low-levelactions. More details regarding these modules can be found in Appendix B.1. shows anoverview of the framework.",
  "Behavior Modeling Module": "To infer the intents and inabilities of constrained agents, the behavior modeling module extractsconstrained agents actions and status from a sequence of egocentric images (i.e., a video). Thebehavior modeling module contains two parts: action recognition and action grounding. Action RecognitionWe adopt an action recognition model to enable the helper agent to recognizethe actions of the constrained agent. We select the TSN model (Wang et al., 2016) pretrained onKinetics-400 (Kay et al., 2017) as the base video action recognition model. There are four typesof actions: pick up, put on, put in, and walking (including move forward, turn left, and turn right).Each action may be successfully executed or fail (except for put in and walking, which are always successful), so there are six classes in total. We collect data by having an agent follow the constrainedagent to observe its behaviors while executing a task and store the action video clips in the trainingset. During testing, the helper agent utilizes this model to recognize the actions of the constrainedagent when it is within observation. The training details can be found in Appendix D.2. Action GroundingAfter the helper recognizes the action of the constrained agent, it looks up thesemantic map in the memory module for the predicate of the action. For example, when the actionis pick up, the predicate will be the nearest object to the constrained agent. Finally, the behaviormodeling module identifies the action, the predicate of the action, and the status of the action of theconstrained agent.",
  "Constrained Agent Policy": "The constrained agent takes ground truth object segmentation as observation to mitigate the impact ofimperfect visual perception on performance and chooses actions based on a rule-based high-levelplanner designed with handwritten rules by human experts. At the beginning of an episode, the constrained agent will explore the environment to find more targetobjects, containers, and the goal location. Whenever the agent finds target objects or containers, itwill pick them up if it has free hands. If more than 50% of the time steps are left and it does not havea container in hand, its priority will be to pick up a container; otherwise, it will pick up a target. Ifthe agent cannot carry more objects, it will put the object on the goal location. If less than 25% ofthe time steps is left (37.5% if it has not found the goal location yet) and the agent is carrying a targetobject, it will put the object on the goal location immediately since it is often a long walk to the goallocation. When selecting possible targets, the agent will opt for the closest one if multiple options areavailable. Moreover, at any time, if the agent can put an object in a container, it will do so.",
  "To evaluate the success of helper agents, we measure the following three metrics:": "Transport rate (TR): The percentage of target objects that the agents successfully transported.We also calculate the Efficiency Improvement (EI) of having the helper as M/M0, where Mdenotes the increase in the transport rate after adding the helper, and M0 denotes the larger of thetransport rates of the team or the constrained agent alone, for numerical stability.",
  "Baselines": "We test four types of planning-based helpers: Random Helper, Rule-Based Hierarchical Plan Helper(RHP), LLM+BM Helper, and VLM Helper. All the helpers share the same Perception Module,Memory Module, and Execution Module as the language agent introduced in , but the criticaldifferences lie in the high-level planner. Meanwhile, an Oracle Helper is tested to demonstrate theupper-bound performance. Below is the description of each type of helper:",
  "Random Helper: A naive helper randomly selects a plan from a list of valid plans": "Rule-Based Hierarchical Plan Helper (RHP): This helper uses prior knowledge of the taskand relies on handcrafted rules by human experts to make plans to assist the constrained agent incompleting the task. Further details on the rules can be found in Appendix B.2. : Quantitative results on CHAIC benchmark. We report the average Transport Rate (TR),Efficiency Improvement (EI) and Goal Inference Accuracy (IA) here. w/o means the main agent doesthe task solely without a helper. The Emergency Rate (ER) metric is also reported for the shoppingtask.",
  "LLM+BM Helper: A language agent augmented with a Behavior Modeling module introduced in. Example prompts can be found in Appendix C.1. We use GPT-4 as our decision-makingLLM": "VLM Helper: A vision-language agent similar to the LLM+BM Helper. The last 10 frames ofegocentric RGB-D observation are added as visual inputs to perceive the constrained agent. Weuse GPT-4o as our decision-making VLM.2 Oracle Helper: An oracle helper that knows the ground truth goal, as the ground truth objectsegmentation, and the task progress. It behaves the same way as the RHP and is close to theupper-bound performance a helper could achieve.",
  "Main Results": "We conducted an extensive evaluation by deploying four baseline models across eight distinctconstraint settings and measured four specific metrics, as outlined in .1.2. The results arepresented in . Overall, the LLM+BM Helper emerges as a strong baseline, achieving thehighest transport rate (TR) in 6 out of 8 tasks, the most significant efficiency improvement (EI) in 7out of 8 tasks, and the best goal inference accuracy (IA) in 4 out of 8 tasks. Behavior Modeling AnalysisOur LLM+BM Helper achieves a reasonable IA metric comparedwith other helpers, which shows our behavior model successfully models the partners behaviorsto some extent. However, compared with the Oracle Helper, all the other baseline agents performpoorly on the IA metric. The IA metric reflects whether the helper successfully determines theneeds of the constrained agent, so the gap shows all our baselines do not work well in inferring thebehavior of the constrained agent from the raw RGB-D image sequence. Nevertheless, our fine-tunedaction recognition model achieves 86% accuracy on the validation set (See Appendix D.2 for theaction recognition model details). Two reasons contribute to the discrepancy: (1) Due to blocking ordistance, the action clip received by the helper may be incomplete or out-of-distribution from trainingdata. (2) The current LLM-based decision module is insufficient to balance observing the partnersbehavior and acting independently.",
  "The main experiments are carried out between May 28 to Jun.5, 2024": "LLM Can Infer Goals Correctly and Perform Actions ProperlyIn analyzing some of thechain-of-thought outputs of LLM, we observe that the LLM-based helper can accurately infer thetarget objects desired by the constrained agent and formulate appropriate plans to collect them. Forinstance, in an outdoor shopping scene, the bike agent named David seeks some fruit. Initially,the LLM helper assesses, Since David hasnt picked any object yet, its challenging to preciselydetermine his target objects. It then realizes, No matter what object David wants, the best firststep would be to maximize the efficiency of carrying objects by using a container, and subsequentlyproceeds to pick up a container. Upon observing the bike agent picking an apple, the LLM helperdeduces, Considering the constraints and the objects David has shown interest in (i.e., an apple),the best course of action from the provided list would be to goto and pick up target <apple>. Witha container and a target object in both hands, the LLM helper notes, Considering I am currentlyholding two target objects (one directly and one in a container), the optimal next action is to put theobject in your one hand to the container in your other hand. This action will free up one of my hands,allowing me to pick up more target objects and transport them efficiently to the goal. Meanwhile, the LLM helper is capable of picking other fruits besides apples, demonstrating itsaccuracy in inferring the object category. After freeing up one hand, the LLM helper states, Basedon the observed actions and status of David, its clear that his target objects are fruits, specificallyapples...so picking up more grapes aligns with the goal. Finally, after collecting several fruits andhaving both hands full, the LLM helper concludes, the best action to take next is to transport objectin hand to goal space. This action involves taking the container filled with target objects, along withthe additional grape in the other hand, to the specified goal location. The detailed analysis of thesechain-of-thought outputs is shown in Appendix F.1. Dealing with EmergenciesIn outdoor shopping tasks, the helper needs to handle unpredictedemergencies, requiring swift responses. The Emergency Rate (ER) metric shows that even if LLM-and VLM-based helpers can achieve high scores in normal tasks, they cannot handle emergencies asefficiently as RHP. To improve, some rule-based control may be required in LLM- and VLM-basedhelpers to help them prioritize and respond more effectively in urgent situations.",
  "Failure Case AnalysisDuring the experiment, we discovered some common failure situationsleading to poor performance, which might be helpful for further helper design": "Spatial Information Analysis: The LLM-based agents do not understand spatial information verywell when provided with text inputs of object locations. They often choose a distant object ratherthan a nearby one, even if they share the same name. Additionally, they often underestimate thecost of reaching the goal location and fail to transport due to time limits. Acting without Cooperation: In the obstacle task, a reasonable solution for the helper is toremove obstacles first to free the constrained agent. However, LLM- and VLM-based helpersoften transport objects alone without assisting the constrained agent, leading to relatively badperformance in this task. VLM is Unable to Infer the Targets Needed by Constrained Agents: In certain tasks, the VLMHelper baseline performs worse than both the random baseline and the No Helper baseline. This isprimarily because VLM cannot accurately infer the preferred objects of constrained agents whenthey observe them picking up items, leading it to consistently follow. Consequently, the VLMHelper fails to transport any objects, making it less effective than randomly transporting someobjects, as done by the random helper. Additionally, frequently following the constrained agent caninterfere with their actionssuch as blocking their pathresulting in the VLM Helper baselinesometimes performing worse than having no helper. However, the LLM+BM Helper transportssome objects even if it does not infer the goal correctly from the BM model, achieving a relativelyhigher score than the VLM Helper.",
  "Conclusion": "In this work, we proposed an accessibility-centered embodied social intelligence challenge: the Con-strained Human-AI Cooperation (CHAIC) Challenge. This challenge includes four new agents withphysical constraints and eight long-horizon tasks featuring both indoor and outdoor scenes, designedto test the critical skills of social perception and cooperation in embodied agents. Our experimental results benchmarking both planning- and learning-based baselines illustrate the systematic evaluationthat such a benchmark can provide for future efforts. We further perform an in-depth analysis offailure cases and provide insights for the future development of embodied social intelligence. LimitationsWhile we aimed to preserve as much realism as possible, there are undoubtedlyaspects of human behavior, particularly in how physical constraints manifest in the world, thatare challenging to simulate. Meanwhile, the rule-based control of constrained agents makes theirbehavior lack diversity. This may be solved by leveraging LLMs to control constrained agents.Moreover, while we believe our challenge takes a good first step forward in introducing accessibilitychallenges to embodied social intelligence benchmarking efforts, we emphasize that our challenge isnot representative of all possible constraints that such users may face.",
  "Acknowledgement": "We thank Qinhong Zhou for his insightful feedback and help with paper writing, and Jeremy Schwartzand Esther Alter for setting up and updating the ThreeDWorld environments. This project is supportedby the Honda Research Institute. Christopher Amato, George Konidaris, Leslie P Kaelbling, and Jonathan P How. Modeling andplanning with macro-actions in decentralized pomdps. Journal of Artificial Intelligence Research,64:817859, 2019. Bowen Baker, Ingmar Kanitscheider, Todor Markov, Yi Wu, Glenn Powell, Bob McGrew, and IgorMordatch. Emergent tool use from multi-agent autocurricula. In International Conference onLearning Representations, 2020. Nolan Bard, Jakob N Foerster, Sarath Chandar, Neil Burch, Marc Lanctot, H Francis Song, EmilioParisotto, Vincent Dumoulin, Subhodeep Moitra, Edward Hughes, et al. The hanabi challenge: Anew frontier for ai research. Artificial Intelligence, 280:103216, 2020.",
  "Zhihao Cao, Zidong Wang, Siwen Xie, Anji Liu, and Lifeng Fan. Smart help: Strategic opponent mod-eling for proactive and adaptive robot assistance in households. arXiv preprint arXiv:2404.09001,2024": "Micah Carroll, Rohin Shah, Mark K Ho, Tom Griffiths, Sanjit Seshia, Pieter Abbeel, and AncaDragan. On the utility of learning about humans for human-ai coordination. Advances in neuralinformation processing systems, 32, 2019. Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu Xiong, Xiaoxiao Li, Shuyang Sun, WansenFeng, Ziwei Liu, Jiarui Xu, Zheng Zhang, Dazhi Cheng, Chenchen Zhu, Tianheng Cheng, QijieZhao, Buyu Li, Xin Lu, Rui Zhu, Yue Wu, Jifeng Dai, Jingdong Wang, Jianping Shi, Wanli Ouyang,Chen Change Loy, and Dahua Lin. MMDetection: Open mmlab detection toolbox and benchmark.arXiv preprint arXiv:1906.07155, 2019.",
  "Kaiming He, Georgia Gkioxari, Piotr Dollr, and Ross Girshick. Mask r-cnn. In Proceedings of theIEEE international conference on computer vision, pages 29612969, 2017": "Max Jaderberg, Wojciech M Czarnecki, Iain Dunning, Luke Marris, Guy Lever, Antonio GarciaCastaneda, Charles Beattie, Neil C Rabinowitz, Ari S Morcos, Avraham Ruderman, et al. Human-level performance in 3d multiplayer games with population-based reinforcement learning. Science,364(6443):859865, 2019. Unnat Jain, Luca Weihs, Eric Kolve, Ali Farhadi, Svetlana Lazebnik, Aniruddha Kembhavi, andAlexander Schwing. A cordial sync: Going beyond marginal policies for multi-agent embodiedtasks. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328,2020, Proceedings, Part V 16, pages 471490. Springer, 2020. Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan,Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset.arXiv preprint arXiv:1705.06950, 2017. Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt, Luca Weihs, Alvaro Herrasti, MattDeitke, Kiana Ehsani, Daniel Gordon, Yuke Zhu, et al. Ai2-thor: An interactive 3d environmentfor visual ai. arXiv preprint arXiv:1712.05474, 2017. Hee Rin Lee, Selma abanovic, Wan-Ling Chang, Shinichi Nagata, Jennifer Piatt, Casey Bennett,and David Hakken. Steps toward participatory design of social robots: mutual learning witholder adults with depression. In Proceedings of the 2017 ACM/IEEE international conference onhuman-robot interaction, pages 244253, 2017. Chengshu Li, Ruohan Zhang, Josiah Wong, Cem Gokmen, Sanjana Srivastava, Roberto Martn-Martn, Chen Wang, Gabrael Levine, Michael Lingelbach, Jiankai Sun, et al. Behavior-1k: Abenchmark for embodied ai with 1,000 everyday activities and realistic simulation. In Conferenceon Robot Learning, pages 8093. PMLR, 2023. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, PiotrDollr, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings,Part V 13, pages 740755. Springer, 2014. Ryan Lowe, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multi-agent actor-critic for mixed cooperative-competitive environments. Advances in neural information processingsystems, 30, 2017. Dipendra Misra, Andrew Bennett, Valts Blukis, Eyvind Niklasson, Max Shatkhin, and Yoav Artzi.Mapping instructions to actions in 3d environments with visual goal prediction. arXiv preprintarXiv:1809.00786, 2018.",
  "Richard Ngo, Lawrence Chan, and Sren Mindermann. The alignment problem from a deep learningperspective. arXiv preprint arXiv:2209.00626, 2022": "Aishwarya Padmakumar, Jesse Thomason, Ayush Shrivastava, Patrick Lange, Anjali Narayan-Chen,Spandana Gella, Robinson Piramuthu, Gokhan Tur, and Dilek Hakkani-Tur. Teach: Task-drivenembodied agents that chat. In Proceedings of the AAAI Conference on Artificial Intelligence, pages20172025, 2022. Xavier Puig, Tianmin Shu, Shuang Li, Zilin Wang, Yuan-Hong Liao, Joshua B Tenenbaum, SanjaFidler, and Antonio Torralba. Watch-and-help: A challenge for social perception and human-aicollaboration. In International Conference on Learning Representations, 2021. Xavier Puig, Tianmin Shu, Joshua B Tenenbaum, and Antonio Torralba. Nopa: Neurally-guidedonline probabilistic assistance for building socially intelligent home assistants. arXiv preprintarXiv:2301.05223, 2023a. Xavier Puig, Eric Undersander, Andrew Szot, Mikael Dallaire Cote, Tsung-Yen Yang, Ruslan Partsey,Ruta Desai, Alexander William Clegg, Michal Hlavac, So Yeon Min, et al. Habitat 3.0: A co-habitatfor humans, avatars and robots. arXiv preprint arXiv:2310.13724, 2023b. Antonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, and NoahDormann. Stable-baselines3: Reliable reinforcement learning implementations. Journal ofMachine Learning Research, 22(268):18, 2021. Mikayel Samvelyan, Tabish Rashid, Christian Schroeder de Witt, Gregory Farquhar, Nantas Nardelli,Tim GJ Rudner, Chia-Man Hung, Philip HS Torr, Jakob Foerster, and Shimon Whiteson. Thestarcraft multi-agent challenge. In Proceedings of the 18th International Conference on AutonomousAgents and MultiAgent Systems, pages 21862188, 2019. Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain,Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, et al. Habitat: A platform for embodied airesearch. In Proceedings of the IEEE/CVF international conference on computer vision, pages93399347, 2019.",
  "Priya Sundaresan, Suneel Belkhale, and Dorsa Sadigh. Learning visuo-haptic skewering strategiesfor robot-assisted feeding. In 6th Annual Conference on Robot Learning, 2022": "Andrew Szot, Unnat Jain, Dhruv Batra, Zsolt Kira, Ruta Desai, and Akshara Rai. Adaptive coordina-tion in social embodied rearrangement. In International Conference on Machine Learning, pages3336533380. PMLR, 2023. Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan, and Subbarao Kambhampati. Large languagemodels still cant plan (a benchmark for llms on planning and reasoning about change). arXivpreprint arXiv:2206.10498, 2022. Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang, and Luc Van Gool.Temporal segment networks: Towards good practices for deep action recognition. In Europeanconference on computer vision, pages 2036. Springer, 2016.",
  "Felix Warneken and Michael Tomasello. Altruistic helping in human infants and young chimpanzees.science, 311(5765):13011303, 2006": "Muning Wen, Jakub Kuba, Runji Lin, Weinan Zhang, Ying Wen, Jun Wang, and Yaodong Yang. Multi-agent reinforcement learning is a sequence modeling problem. Advances in Neural InformationProcessing Systems, 35:1650916521, 2022. Fei Xia, Amir R Zamir, Zhiyang He, Alexander Sax, Jitendra Malik, and Silvio Savarese. Gibson env:Real-world perception for embodied agents. In Proceedings of the IEEE conference on computervision and pattern recognition, pages 90689079, 2018. Fanbo Xiang, Yuzhe Qin, Kaichun Mo, Yikuan Xia, Hao Zhu, Fangchen Liu, Minghua Liu, HanxiaoJiang, Yifu Yuan, He Wang, et al. Sapien: A simulated part-based interactive environment. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages1109711107, 2020. Hongxin Zhang, Weihua Du, Jiaming Shan, Qinhong Zhou, Yilun Du, Joshua B Tenenbaum, TianminShu, and Chuang Gan. Building cooperative embodied agents modularly with large languagemodels. arXiv preprint arXiv:2307.02485, 2023. Hongxin Zhang, Zeyuan Wang, Qiushi Lyu, Zheyuan Zhang, Sunli Chen, Tianmin Shu, Yilun Du,and Chuang Gan. Combo: Compositional world models for embodied multi-agent cooperation.arXiv preprint arXiv:2404.10775, 2024. Qinhong Zhou, Sunli Chen, Yisong Wang, Haozhe Xu, Weihua Du, Hongxin Zhang, Yilun Du,Joshua B. Tenenbaum, and Chuang Gan. Hazard challenge: Embodied decision making indynamically changing environments, 2024. Yuke Zhu, Daniel Gordon, Eric Kolve, Dieter Fox, Li Fei-Fei, Abhinav Gupta, Roozbeh Mottaghi,and Ali Farhadi. Visual semantic planning using deep successor representations. In Proceedings ofthe IEEE international conference on computer vision, pages 483492, 2017.",
  "A.2Comparison with Other Embodied Challenges": "We compare the differences between our proposed challenge and others in . Our CHAIC Chal-lenge represents the first large-scale embodied social intelligence challenge focused on accessibility,incorporating outdoor scenes with emergent events, and requires goal inference from observations. : Comparison between Various Embodied Challenges. *Social rearrangement assumesoracle position information of both agents and the target objects. **Smart Help assumes perfectperceiving of the other agents actions and statuses.",
  "B.1More Details on LLM+BM Helper": "We have tested several baselines in the benchmark, and the LLM+BM helper has shown the bestperformance. The LLM+BM helper consists of multiple modules. In addition to the behaviormodeling module discussed in the main paper, the helper has several other modules, which weintroduce here: Perception ModuleThe perception module is used to extract useful information from raw RGB-Dimages. Following (Zhang et al., 2023), we have fine-tuned a Mask R-CNN (He et al., 2017) modelon the images collected in the training dataset to obtain object-wise segmentation masks. The trainingset contains the same kinds of objects but with different layouts and scene backgrounds compared tothe test set. The training details are in Appendix D.1. Memory ModuleThe memory module is designed to understand the environments layout andthe positions of objects by an occupancy map and a semantic map. Firstly, the agent continuouslyupdates a 2D grid-based top-down occupancy map when executing actions. Initially, all areas onthe occupancy map are marked as unknown, and the map is updated using depth images. Utilizingdepth images and camera intrinsics, the memory module first maps each pixel from the depth imageinto 3D space and then projects it onto the occupancy map. The semantic map, which also adopts atop-down view and maintains the same grid size as the occupancy map, records the locations of alldetected objects within the grid. Decision ModuleAn LLM-based decision module is used to generate a subgoal without anyprior knowledge or specific design. The prompt encompasses six components: task description,self-information, information about other agents, task progress, semantic map information, andavailable plans. The LLMs output should be a plan from the list of valid plans, with specific objectIDs included if the plan involves actions like \"pick up\" or \"put on\". Detailed information about theprompt is available in Appendix C.1. Execution ModuleThe execution module is a low-level executor, which serves as a low-levelexecutor, bridging the gap between high-level plans and low-level actions, including navigation andexploration. When the high-level plan directs the picking up of a previously seen object or traveling to a goal space, the navigation module can recursively generate a path with low-level navigationcommands using the occupancy map generated from the memory module. This map distinguishesbetween free, unknown, occupied, and wall spaces, assigning increasingly higher costs respectively.The navigation module employs the A* algorithm (Hart et al., 1968) to determine the most efficientroute from the agents current location to the target object and generates the necessary actions tofollow this path. The path is recalculated whenever the occupancy map is updated. Additionally,for exploration tasks, the navigation module utilizes the frontier exploration method to enhance theefficiency of discovering new areas by repeatedly moving toward unknown spaces adjacent to knownones.",
  "B.2Detailed Rules for Rule-Based Hierarchical Plan Helper (RHP)": "The rule for Rule-Based Hierarchical Plan Helper (RHP) is similar to that of the constrained agentdescribed in .1.1. The only difference is that since the helper does not know the exact goalof the constrained agent, the ruled-based agent will choose the target object randomly among allavailable objects. In detail, at the start of an episode, the rule-based agent explores the environment to locate objects,containers, and the goal location. It picks up objects or containers if its hands are free. If over 50% ofthe time steps remain and the agent does not obtain a container, it prioritizes acquiring one; otherwise,it focuses on objects. When unable to carry more, it deposits objects at the goal location. If lessthan 25% of the time steps is left (or 37.5% without a goal location identified), it immediately placesobjects in hands at the goal. The rule-based agent chooses the nearest object when multiple areavailable and puts objects in containers whenever possible.",
  "C.1Detailed Prompt of Decision Module of LLM+BM Helper": "The LLM+BM Helper uses an LLM-based decision module to determine the plan for the next step.The prompt of the LLM-based decision module contains six parts: task description, self-information,information about other agents, task progress, semantic map information, and available plans. Thedecision module needs to select a plan and fill in the plan with proper parameters, and then theexecution module will execute the plan. Following are prompt descriptions and examples of eachpart:",
  "C.1.3Information about Other Agents": "Information about Other Agents contains information about other agents, including the actions ofthe constrained agent that the helper has seen, together with their statuses, the objects that the helperhas seen the constrained agent holding, the position of the constrained agent when the helper last sawhim/her. For shopping tasks, it also includes the position of the child when the helper last saw her. Aprompt example is listed in .",
  "C.2Detailed Prompt of Decision Module of VLM Helper": "The prompt of the decision module of VLM Helper is similar to that of LLM+BM Helper. The onlydifference is the VLM-based decision module perceives the behavior of the partner through raw RGBimages as an additional observation. In detail, an image sequence with the last ten images is added tothe input of the VLM-based decision module.",
  "D.1Detection Model for Object Detection": "Since the helper receives raw RGB-D images from the environment, an object detection model isnecessary to identify objects within these images. We fine-tuned an object detection model using ourdataset collected from training scenes. Data CollectionTo collect training data in the environment, a helper roams randomly and solelywithin the scenes, collecting egocentric images combined with ground truth segmentation. Theenvironment is split into training and validation, and we collected 61K images at a resolution of512 512 in total. There are 53 types of objects related to the benchmark, so the detection modelhas the same number of labels. Training DetailsWe utilized the open-source code provided by MMDetection (Chen et al., 2019) asour training framework and selected a Mask R-CNN (He et al., 2017) model pre-trained on the COCOdataset (Lin et al., 2014) with a ResNet50 (He et al., 2016) backbone. The model was fine-tunedfor four epochs, incorporating a warm-up stage of 500 steps and a batch size of 16. The optimizeremployed was SGD with lr = 0.01, momentum = 0.9, and weight_decay = 0.0001. This fine-tuning process was finished on an NVIDIA A10G GPU in approximately six hours. The fine-tunedmodel achieved a 94.4% mAP@50 (Segmentation Mean Average Precision at 50% intersection overunion) on the validation set.",
  "D.2Action Recognition Model for Behavior Modeling": "Recognizing the actions of the partner is a crucial ability for understanding its intentions, whilecurrent foundation models cannot directly discern actions in the wild. Therefore, an auxiliary actionrecognition model is necessary for our baseline agents. Similarly to the detection model, we fine-tunedan action recognition model. Data CollectionCollecting behavior data is more challenging than gathering object detection data.To simulate the real situation, we created a follower whose sole action is to track the constrainedagent. This follower has access to the action history, which when indicating that the constrained agentis acting, triggers the extraction of an RGB image sequence from the observers viewpoint. Theseimages are then concatenated into a video clip, each containing 50 to 100 frames at a resolution of512 512. Sometimes the constrained agent is obscured, preventing full visibility throughout someactions. Consequently, we discarded any action clip where the visibility of the constrained agent wasless than 20%. The dataset comprises six behaviors for the constrained agent: successful pick-up, failpick-up, put-in, successful put-on, fail put-on and moving. In total, the dataset contains 3,000 videoclips. Training DetailsWe utilized the open-source tools provided by MMDetection (Contributors, 2020)for training and employed the Temporal Segment Network (TSN) (Wang et al., 2016), pre-trainedon the Kinetics-400 dataset (Kay et al., 2017), as our base model with a ResNet50 backbone (Heet al., 2016). The sampling strategy was set to 16 1 1 (number of clips, clip length, clip interval).We fine-tuned the model 100 epochs using the same optimizer as object detection and selected thebest checkpoint from the validation set. The fine-tuned model achieved 86.1% top-1 accuracy on thevalidation set.",
  "E.3Comparison with Learning-Based Baselines": "Although the baselines in the main paper are all non-training baselines, we also tested some learning-based baselines. The results are shown in . However, we observe that in most tasks, ourlearning-based baselines perform similarly to the main agent operating without a helper (exceptfor the outdoor furniture task where two agents make a crucial difference because of their differentstrength capacities and the relatively easy task setting). This is primarily due to the following reasons: The inherent difficulty of vision-based reinforcement learning. Even with depth and seg-mentation information which we encoded as a semantic map, the agent is still hard to learnnon-trivial features from the observations. The slow data collection process and the inability to parallelize in ThreeDWorld make ittime-consuming to gather large-scale online data for training. In our experiments, collectingrollouts of size 104 for each task requires one day on an NVIDIA A10G GPU. RL BaselineAn end-to-end reinforcement learning helper trained on each task separately. Weuse the Stable-Baselines3 (Raffin et al., 2021) codebase and PPO (Schulman et al., 2017) algorithmto wrap and train our RL helpers. We made minor modifications to the observation space to fit ourtraining needs. Namely, we pack the RGBD image, semantic map, agent position/direction, andstatus of agent-holding objects as a customized RL observation. The reward is designed as a linearcombination of transported objects and distance to the nearest target object. A penalty is also appliedfor each invalid action. The policy network extracts features from each observation class with either CNN for images orMLP for scalar information and concatenates them. The concatenated features then pass througha two-layer MLP to produce a vector of length 64. This part is shared by both the actor and thecritic in PPO. During training, we use a batch size of 2 and update the policy every 2 rollout stepdue to slow data collection. We use default training parameters in Stable-Baselines3, including = 0.99, = 0.95, lr = 3 104, etc. For each task, we train the model for 104 steps. SmartHelp BaselineA helping method proposed by Cao et al. (2024). For the opponent modelingmodule, we set the window size w of the state feature to 5. The input of the opponent modeling is theobservation of the helper agent, which contains information on each object in the helpers view, aswell as the constrained agents information, if visible to the helper. The information on each objectincludes the type, weight, position, and height. The information on the constrained agent includesposition and orientation, the last action with its status, and the objects currently held by the agent.The helper receives the ground truth actions and statuses of the constrained agent. We run simulations with a random helper and an oracle constrained agent for 12 episodes eachto collect the dataset of constrained agent trajectories used for training the opponent model. Wecollect 8355 trajectories after balancing, each containing observations at five discrete time points,together with the constrained agents goal and constraint. The constraint of the constrained agent isa 5-dimension vector, which includes the maximum and minimum heights the agent can reach, themaximum weight the agent can hold, whether the agent is holding a bike and whether the agent isconfined to a wheelchair, with each dimension scaled to . The goal contains the type of the goaland possible target index. There are six types of goal: explore, wait, pick, puton, putin,unknown. If the helper agent doesnt see the constrained agent for all five observations, the groundtruth goal is set to unknown. We balance the number of data for each goal in the trajectory dataset.For some goals like pick, there is a target index for the goal indicating the type of the object it picks,and there are 53 types of objects in total. We trained the opponent model using the cross-entropy lossfor goals and the mean squared error loss for constraints. We use the Adam optimizer with a learningrate of 1 106 and a batch size of 32. Finally, we achieved the goal prediction accuracy of 93%,and the mean squared error loss for the constraint is 0.02.",
  "We analyze the chain-of-thought outputs of LLM and the actions of LLM+BM Helper, and is an example of the shopping task": "LLM+BM helper knows to use containers to increase transport efficiencyAs shown in Fig-ure 11a, at the beginning of the task, the helper did not see David, the constrained agent, pick anythingyet. But he could reason to pick up the container to improve transportation efficiency. He saw twocontainers when he was planning and chose the container closer to his current position. LLM+BM helper can infer objects that constrained agent needs correctlyAs shown in Fig-ure 11b, after seeing David picking up an apple, the helper could reason out that David wanted fruits,so he picked up an apple, and he could also pick up other fruits like grapes afterward. LLM+BM helper knows to put things in container continuouslyAs shown in c, afterpicking up the apple, the helper reasoned to put the apple into the container, freeing up one hand topick up more target objects. LLM+BM helper knows when to transport the objects into goal spaceAs shown in d,after putting three objects into the container and picking an object in the other hand, the helperdecided to transport them to goal space, the fire hydrant in front of the house of Davids. The helperanalyzed that he could not get objects anymore while transporting these objects with maximumefficiency, and David needed the objects he transported.",
  "Helper": "Given that David that he moves slowly, it would be efficient to collect objects that are of the same kind that David might choose based on his initial actions and status. Since David hasnt picked any object yet, its challenging to precisely determine his target objects The best first step would be to maximize the efficiency of carrying objects by using a container. Among the visible containers, the <wood_basket> (7157967) is closer to my current position. Therefore, the best action to take first would be to 'goto and pick up container <wood_basket> (7157967)'. Given the goal is to help David transport as many wanted target objects as possible to the <b03_fire_hydrant> (8882855) and considering the constraints and the objects David has shown interest in (i.e., an apple), the best course of action from the provided list would be to 'goto and pick up target <apple> (4455088)'. This aligns with the objective of collecting similar kinds of things that David has picked, which are fruits in this case. I currently hold an apple in one hand and a wood basket in the other hand. The optimal next action from the given options to achieve the goal as soon as possible, considering I am currently holding two target objects (one directly and one in a container), is to: put the object in my one hand to the container in my other hand. This action will free up one of my hands, allowing me to pick up more target objects and transport them efficiently to the goal. the best action to take next is to 'transport object in hand to goal space'. This choice is optimal because: 1. The container is full, meaning Im efficiently utilizing the container's capacity to transport as many target objects as possible in one trip. 2. Transporting the objects to the goal space now will minimize unnecessary steps and back-and-forth movement, adhering to the constraint of a maximum of 3000 steps. 3. Since David is collecting fruits (apples and oranges noted in his bike basket), and I have collected similar kinds (grapes and an apple), transporting these items aligns with the goal of assisting David by handling analogous target objects.",
  ": LLM+BM Helpers behaviors in one episode of the shopping task, together with thechain-of-thought outputs of the LLM-based decision module (Some of the outputs are omitted forspace reasons)": "VLM Helper knows to explore at firstAs shown in a, at the beginning of the task,the helper could not see the constrained agent. Therefore, the helper chose to explore to find theconstrained agent and objects. VLM Helper knows to follow the constrained agentAs shown in b, after seeing theconstrained agent, the helper chose to follow her to get the information about the things she needed.Then, he saw the constrained agent pick up bread. However, sometimes the VLM helper is unable toinfer the objects needed by the constrained agent, causing him to consistently follow the constrainedagent. VLM Helper sometimes infer objects that constrained agent needs correctlyAs shown inc and d, after seeing the constrained agent picking bread, the helper could collectother objects of this kind, like loaf bread and hamburger. VLM Helper cannot transport objects efficientlyIn this episode, the VLM Helper only trans-ported one thing at a time to the goal space, which is inefficient. First, the helper didnt use thecontainer. Second, the helper didnt use both hands to carry the target objects. The most efficient wayis to hold one container with three objects in one hand and hold one object in the other hand, thustransporting four objects at a time.",
  "G.1Details of Shopping Task Generation": "For the shopping task, six shops are generated and spread out on both sides of the road. Each shopsells one specific category of items. There are three categories of items, each of which is sold inexactly two stores. The item categories and specific items for each category are listed below:",
  "Fruit: apple, orange, grape, and banana. Baked food: loaf bread, croissant, burger, and donut. Drink: cola, pepsi, sprite, and fanta": "For each episode, we first randomly select one category of items. Then randomly select severalobjects in this category, together with another object randomly selected in the other two categories asthe target objects. The number of target objects is between 11 and 13. The goal location is a fixed,pre-determined place in front of the bicycle agents house. Then we randomly select three shops andput a container on each of them. Finally, we randomly initialize the two agents. Their initial positionsare guaranteed at least 0.5 meters away from the nearest shop.",
  "G.2Details of Moving House Task Generation": "We choose twelve common pieces of furniture for the moving house task. For each episode, werandomly select five pieces of furniture and randomly put them in the area in front of the house. Weensure that the furniture does not overlap each other. Then we set the initial positions of the agentsnear the place area. The goal location is a truck parked around 10 meters away from the place area.The frail agents strength is 100, while the helper agents strength is 600. The weights of furniturerange from 50 to 900.",
  "H.1Broader Impacts": "By building the CHAIC benchmark, our work tries to simulate the disability of human beings in asimulated environment. The benchmark proposes several tasks and scenes that are common in thereal world. After setting up the benchmark, we built a few helper agents to help the constrainedagents fulfill their tasks with visual observation only. This kind of helper agent has a wide range ofpotential usage in real life but there is not much research on this in academia. Through this research,we hope to pave the way to more friendly and helpful AI agents that can be implemented in bothsimulated environments and the real world.",
  "The paper is withdrawn from CVPR2024, where the reviewers posted two main points:": "The reviewers thought it was not natural that the helper could know the whole action historyof the constrained agent. We agree with it, and currently, the helper cannot get any textinformation about the action history of the constrained agent and needs to infer it from rawRGB-D observation. The reviewers thought the objects and tasks were not rich enough. Therefore, we createboth indoor and outdoor scenes with various tasks in this submission, and the number oftask-relevant objects increases from about 20 to over 50.",
  "I.2Composition": "What do the instances that comprise the dataset represent (e.g., documents, photos,people, countries)? Are there multiple types of instances (e.g., movies, users, and ratings;people and interactions between them; nodes and edges)? Please provide a description.One instance is a sequence of commands and the ThreeDWorld Platform can read thecommands in the instance and then create the scenes and tasks.",
  "How many instances are there in total (of each type, if appropriate)?There are 192 instances in total. There are 8 tasks, and each task contains 12 traininginstances and 12 testing instances": "Does the dataset contain all possible instances or is it a sample (not necessarily random)of instances from a larger set? If the dataset is a sample, then what is the larger set? Is thesample representative of the larger set (e.g., geographic coverage)? If so, please describe how this representativeness was validated/verified. If it is not representative of the larger set,please describe why not (e.g., to cover a more diverse range of instances, because instanceswere withheld or unavailable).The dataset is a sample. We have an instance generation pipeline that can generate infiniteinstances for each task. What data does each instance consist of? Raw data (e.g., unprocessed text or images)or features? In either case, please provide a description.Each instance consists of a command sequence used for task initialization in the ThreeD-World Platform.",
  "Are there any errors, sources of noise, or redundancies in the dataset? If so, pleaseprovide a description.No": "Is the dataset self-contained, or does it link to or otherwise rely on external resources(e.g., websites, tweets, other datasets)? If it links to or relies on external resources, a) arethere guarantees that they will exist, and remain constant, over time; b) are there officialarchival versions of the complete dataset (i.e., including the external resources as theyexisted at the time the dataset was created); c) are there any restrictions (e.g., licenses, fees)associated with any of the external resources that might apply to a future user? Pleaseprovide descriptions of all external resources and any restrictions associated with them, aswell as links or other access points, as appropriate.The dataset is linked to the ThreeDWorld Platform, a) which has been maintained and willbe maintained for a long time at TDW GitHub, and we use its stable version 1.12.27. b) thecomplete dataset can be found at CHAIC GitHub. c) The 3D models used in the dataset canbe publicly downloaded via Google Drive. The dataset is subjected to an MIT License. Does the dataset contain data that might be considered confidential (e.g., data that isprotected by legal privilege or by doctor-patient confidentiality, data that includes thecontent of individuals non-public communications)? If so, please provide a description.No.",
  "How was the data associated with each instance acquired? Was the data directly ob-servable (e.g., raw text, movie ratings), reported by subjects (e.g., survey responses), or": "indirectly inferred/derived from other data (e.g., part-of-speech tags, model-based guessesfor age or language)? If data was reported by subjects or indirectly inferred/derived fromother data, was the data validated/verified? If so, please describe how.The data is not directly observable or reported. Data is created from an automatic scenegeneration pipeline and visualized via the ThreeDWorld Platform. What mechanisms or procedures were used to collect the data (e.g., hardware apparatusor sensor, manual human curation, software program, software API)? How were thesemechanisms or procedures validated?We use an automatic scene generation pipeline for each task to generate data.",
  "If the dataset is a sample from a larger set, what was the sampling strategy (e.g.,deterministic, probabilistic with specific sampling probabilities)?Random Sampling": "Who was involved in the data collection process (e.g., students, crowdworkers, contrac-tors) and how were they compensated (e.g., how much were crowdworkers paid)?Since we automatically generated data, only authors were involved in the data collectionprocess. Over what timeframe was the data collected? Does this timeframe match the creationtimeframe of the data associated with the instances (e.g., recent crawl of old news articles)?If not, please describe the timeframe in which the data associated with the instances wascreated.The data was collected between 15 May and 10 June 2024. Were any ethical review processes conducted (e.g., by an institutional review board)? Ifso, please provide a description of these review processes, including the outcomes, as wellas a link or other access point to any supporting documentation.No.",
  "What (other) tasks could the dataset be used for?Embodied behavior recognition, embodied object detection": "Is there anything about the composition of the dataset or the way it was collectedand preprocessed/cleaned/labeled that might impact future uses? For example, is thereanything that a future user might need to know to avoid uses that could result in unfair treatment of individuals or groups (e.g., stereotyping, quality of service issues) or otherundesirable harms (e.g., financial harms, legal risks) If so, please provide a description. Isthere anything a future user could do to mitigate these undesirable harms?No.",
  "I.6Distribution": "Will the dataset be distributed to third parties outside of the entity (e.g., company,institution, organization) on behalf of which the dataset was created? If so, pleaseprovide a description.Yes, the dataset is public and everyone is welcome to use it. How will the dataset will be distributed (e.g., tarball on website, API, GitHub)? Doesthe dataset have a digital object identifier (DOI)?The dataset will be distributed via GitHub. Currently, the dataset does not have a DOI.",
  "When will the dataset be distributed?Before NeurIPS 2024": "Will the dataset be distributed under a copyright or other intellectual property (IP)license, and/or under applicable terms of use (ToU)? If so, please describe this licenseand/or ToU, and provide a link or other access point to, or otherwise reproduce, any relevantlicensing terms or ToU, as well as any fees associated with these restrictions.This dataset is licensed under the MIT License. Have any third parties imposed IP-based or other restrictions on the data associatedwith the instances? If so, please describe these restrictions, and provide a link or otheraccess point to, or otherwise reproduce, any relevant licensing terms, as well as any feesassociated with these restrictions.No. Do any export controls or other regulatory restrictions apply to the dataset or toindividual instances? If so, please describe these restrictions, and provide a link or otheraccess point to, or otherwise reproduce, any supporting documentation.No.",
  "would be retained for a fixed period of time and then deleted)? If so, please describethese limits and explain how they will be enforced.N/A": "Will older versions of the dataset continue to be supported/hosted/maintained? If so,please describe how. If not, please describe how its obsolescence will be communicated todataset consumers.Yes, the old version will be kept in GitHub. If others want to extend/augment/build on/contribute to the dataset, is there a mech-anism for them to do so? If so, please provide a description. Will these contributionsbe validated/verified? If so, please describe how. If not, why not? Is there a process forcommunicating/distributing these contributions to dataset consumers? If so, please providea description.Others can contact paper authors when they want to contribute."
}