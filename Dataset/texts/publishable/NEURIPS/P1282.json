{
  "Abstract": "In this paper, we explore the paradox of trust and vulnerability in human-machineinteractions, inspired by Alexander Rebens BlabDroid project. This project usedsmall, unassuming robots that actively engaged with people, successfully elicitingpersonal thoughts or secrets from individuals, often more effectively than humancounterparts. This phenomenon raises intriguing questions about how trust andself-disclosure operate in interactions with machines, even in their simplest forms.We study the change of trust in technology through analyzing the psychologicalprocesses behind such encounters. The analysis applies theories like Social Pen-etration Theory and Communication Privacy Management Theory to understandthe balance between perceived security and the risk of exposure when personalinformation and secrets are shared with machines or AI. Additionally, we draw onphilosophical perspectives, such as posthumanism and phenomenology, to engagewith broader questions about trust, privacy, and vulnerability in the digital age.Rapid incorporation of AI into our most private areas challenges us to rethink andredefine our ethical responsibilities.",
  "Introduction": "The increasing integration of artificial intelligence (AI) into the fabric of everyday life invites us toreconsider fundamental concepts of trust, vulnerability, and the nature of human interaction. In thecontext of self-disclosure, Griffin describes disclosure as \"the voluntary sharing of personal history,preferences, attitudes, feelings, values, secrets, etc., with another person\" . This act of revealingprivate information, as Pickard & Roster put it, involves \"the truthful, sincere, and intentionalcommunication of private information about oneself or others that makes oneself or others morevulnerable.\" The introduction of AI into this delicate process introduces a paradox that challengesour traditional understanding of these concepts. In this paper, we explore this paradox, drawinginspiration from Alexander Rebens BlabDroid project , wherein small, seemingly innocuousrobots (see ) were able to elicit profoundly personal and emotionally intimate disclosuresfrom individuals often more readily than would be expected in human-to-human encounters. Thewillingness to reveal such private aspects of their lives to machines that lack empathy, moral agency, ortrue understanding, raises questions about the evolving nature of trust and the mechanisms underlyingthese interactions that facilitate such intimate disclosures. Unlike human conversation partners, current forms of AI lack genuine emotions and cannot trulyunderstand or respond to them ; yet, studies such as Rebens demonstrates that people are stillwilling to share personal information with these non-human entities . This tendency to trustAI, even when its emotional intelligence is artificial, suggests that trusting AI might not depend onthe same factors that make us trust other people. Instead, trust in AI could be based on the beliefthat machines are neutral or objective qualities that seem harmless and may create an illusion of a",
  ": Alexander Rebens BlabDroid: Robots in Residence. Source: Reben, 2018": "mirror that merely reflects users emotions and thoughts without judgment . Paradoxically, thevery traits that make AI appear trustworthy such as its perceived fairness, lack of bias, and consistentbehavior might also make it a potential risky confidant. This effect could, in reality, distort usersself-disclosure by encouraging deeper vulnerability while lacking the nuanced understanding andempathy that a human confidant provides. Consequently, while sharing secrets with AI might providea sense of security, it puts individuals at risk of having their privacy violated, their data misused, andtheir emotional needs neglected . This tension between the comfort of disclosure and the vulnerability it creates forms the core paradoxthis paper seeks to unravel. We draw on Social Penetration Theory (SPT) and CommunicationPrivacy Management Theory (CPM) , which help explain how people share personal information inrelationships. SPT, which is usually used to analyze human relationships, suggests that relationshipsgrow stronger when people gradually and mutually share personal information. But when we applythis theory to interactions with AI, it makes us wonder if these relationships can truly deepenor remain superficially transactional despite personal disclosures . CPM, on the other hand,examines how people manage the balance between privacy and self-disclosure . This frameworkpoints out the murky boundaries of privacy and disclosure in human-AI interactions, questioningwhether individuals boundaries are adequately understood or respected by AI systems. Thesetheories help to unpack the psychological factors involved, but they also reveal ethical complexities:Can AI be held accountable in a role where trust and privacy are paramount? In addition to theseframeworks, we offer philosophical viewpoints to help critically evaluate the effects of this changingsituation. Posthumanist ideas challenge the traditional focus on human-centered trust and ethics,suggesting that as AI occupies more intimate roles in our lives, ethical thinking needs to go beyondjust human concerns to address the broader consequences of integrating machines into matters oftrust and vulnerability . Phenomenology provides another perspective, highlighting the real-lifeexperiences of interacting with AI and how these experiences are changing our thoughts on trust,privacy, and being vulnerable . By exploring the psychological and philosophical dimensions of self-disclosure in human-machineinteractions, we seek to understand how emerging AI relationships challenge and redefine ourfundamental notions of ethical responsibility in the digital age. If AI, as a confidant, can hold boththe promise of connection and the peril of exploitation, then what ethical guardrails do we need?Should we treat AI interactions with the same expectations and boundaries as human ones, or are theyfundamentally different? How can we reconcile the need for privacy with the allure of conveniencein a world where AI knows us intimately but cannot truly understand us? This paradox of trust andvulnerability raises more questions than answers. Instead of providing definitive solutions, we callfor more research and ongoing dialogue to address the implications and shape ethical frameworks fora future where AI plays an intimate role in human life.",
  "Trust in Human-Machine Interaction": "The concept of trust in technology has undergone a profound transformation, mirroring broadersocietal shifts in our relationship with machines and automated systems . Historically,our trust in technology was predominantly instrumental, grounded in the reliability and efficiencyof mechanical systems . Early technological innovations, such as the steam engine and thetelegraph, were trusted primarily due to their consistent performance and their role as extensions ofhuman labor. As technology progressed, especially with the rise of digital systems and computers,our relationship with technology evolved from one of direct interaction to a more abstract formof trust . The inner workings of digital technologies became increasingly opaque to theaverage user, making trust less about tangible performance and more about cognitive and emotionalperceptions . This shift introduced a new dimension to trust, where users had to place faith inthe competence, reliability, and fairness of systems operating beyond their immediate comprehension.The complexity of algorithms and the advancement of AI technologies, such as machine learning,deep neural networks, and large language models, further complicated this relationship, highlightingthe need for users to trust not only in functionality but also in the perceived ethical and operationalintegrity of these systems . In the realm of AI-powered technologies, the mechanisms of trust have become more sophisticated.Transparency is now a pivotal factor, as we are more inclined to trust systems whose operationswe can understand and predict . Nevertheless, transparency alone does not suffice; it needs tobe paired with reliability and perceived competence, which reinforce the users confidence in thesystems trustworthiness . Psychologically, our trust in machines is heavily influenced byanthropomorphism and the Uncanny Valley hypothesis . Anthropomorphism, the inclination toattribute human-like qualities to non-human entities, often leads us to form emotional connections withmachines, especially when they exhibit behaviors or characteristics that mimic human actions .This phenomenon can enhance trust, as we may perceive the machine as more relatable and reliable.However, this trust can be precarious, as evidenced by the Uncanny Valley hypothesis. The UncannyValley hypothesis suggests that machines that appear almost human but fall short in subtle ways canevoke discomfort, diminishing trust . This discomfort underscores the intricate interplay betweenhuman psychology and machine design in fostering trustworthy AI systems.",
  "Theoretical Perspectives on Self-Disclosure": "Self-disclosure the act of revealing personal, private information to others is a cornerstone ofhuman communication and relational development . It serves as a key mechanism throughwhich individuals navigate and negotiate intimacy, trust, and relational dynamics. In the context ofhuman-AI interactions, this process acquires new dimensions and implications, informed by theoriessuch as Social Penetration Theory (SPT) and Communication Privacy Management Theory (CPM). SPT, articulated by Altman & Taylor, conceptualizes interpersonal relationships as unfolding throughincremental layers of self-disclosure . This theory likens the process to peeling layers from an onion(see Fig 2), where initial exchanges are superficial and gradually become more intimate as trust andcomfort develop . In human-to-human interactions, this model facilitates the evolution of closerelationships, grounded in mutual understanding and emotional engagement. Applying SPT to human-AI interactions, we encounter a critical paradox. Users may extend trust to AI systems incrementally,sharing more personal information as they perceive the AI as reliable and non-judgmental. However,this progression is misleading, as AI systems despite their capabilities to mimic emotion-basedsocial behaviors and simulate emotional engagement lack genuine consciousness and relationalauthenticity that human interlocutors provide . The perceived intimacy and responsivenessof AI can create a false sense of connection, leading users to disclose sensitive information underthe illusion of mutual understanding . This highlights a fundamental issue: the AIs lack of trueempathetic engagement renders such disclosures superficial, with unpredictable implications for ourunderstanding of relational authenticity in the digital era. This false sense of intimacy can also leadto emotional vulnerability, where users may feel manipulated or betrayed if they realize the AIcannot truly understand their emotions or reciprocate the connection .",
  ": Relationships layers according to Social Penetration Theory. Source: Pecune, 2013": "CPM, developed by Petronio , offers another lens through which to analyze self-disclosure.CPM posits that individuals maintain privacy boundaries that they navigate and negotiate based onperceived ownership of personal information. Disclosure decisions are made within this frameworkof privacy control, balancing openness with protection against potential risks . In interactionswith AI, these privacy boundaries become increasingly fluid and ambiguous . Users mightlower their defenses, assuming that AI perceived as a neutral, objective, and non-judgmental entity poses less interpersonal risks compared to human counterparts. This perception, however,can be fundamentally flawed. While AI lacks malicious intent, the information shared is oftensubjected to storage, analysis, and potential exploitation by the entities controlling the AI . Thenature of privacy management shifts drastically in these interactions, presenting a paradox: Howcan we maintain user trust and intimacy in AI relationships when transparency and accountability particularly regarding data handling remain opaque? Unlike humans, who can generally be trusted torespect privacy boundaries and cultural norms, AI systems and the entities that control them may notuphold such ethical standards . This questions the responsibilities of AI designers and operators,particularly in ensuring that privacy boundaries are respected when the risks of self-disclosure in AIinteractions become more pronounced.",
  "Vulnerability and Risk in Self-Disclosure": "Self-disclosure inherently involves a spectrum of vulnerability, exposing individuals to potential riskssuch as judgment, rejection, and exploitation . This reflects the tension between perceivedsafety of confiding in AI and the real risks associated with such disclosures. In human interactions,people weigh disclosure based on perceived trustworthiness of the other party and the expectedbenefits of sharing . However, the calculus changes with AI systems by the perceived impartialnature, providing a seemingly safe space for personal revelations. This perception is bolstered by theanonymity and control users feel when interacting with AI, contrasting with the social dynamics andpower imbalances inherent in human interactions . Yet, this perceived safety can be deceptive.The act of disclosing information to AI introduces vulnerabilities that are challenging to regulate andoften difficult to fully understand. Part of the complexity lies in the freedom users feel in sharingpersonal information with AI without usual societal judgment. At the same time, the data shared withAI can be stored, analyzed, and potentially used for retraining, a process that blurs the line betweenuser privacy and the continuous refinement of AI outputs. The balancing act between safeguardingpersonal information and controlling AI outputs is both technically and systematically challenging.How to create accountability frameworks that can keep pace with the rapidly evolving landscape ofAI-driven interactions? Furthermore, the effects of anonymity in digital communication exacerbate this tension. Anonymityoften lowers inhibitions, leading individuals to disclose more information than they might in face-to-face interactions. Research shows that anonymity can reduce accountability and concern forsocial repercussions, resulting in increased self-disclosure . This effect, known as digitaldisinhibition, can be particularly exaggerated in AI interactions, where users may underestimate thepermanence and reach of their digital disclosures . Reduced social presence can lead to greaterhonesty and openness , but in the context of AI, this effect can raises the risk of oversharing,especially through the accidental revelation of too much private details. The lack of immediate socialfeedback and the abstract nature of AI interactions contribute to this phenomenon .",
  "Philosophical Perspectives on AI and Trust": "The advent of AI has called for a reevaluation of traditional philosophical constructs surroundingtrust and ethics . Two philosophical frameworks, posthumanism and phenomenology, offernew insights into how we can reconceptualize trust and reimagine our relationship with technology. Posthumanism challenges the anthropocentric perspectives that have historically governed our under-standing of trust and ethical behavior. This philosophical movement advocates for a decentered viewthat recognizes the agency of non-human entities, including AI . From a posthumanist perspective,the traditional view that trust is solely a human phenomenon, contingent upon human-like attributessuch as empathy and moral reasoning, becomes inadequate. Instead, posthumanism reframes trustas a relational dynamic that transcends human-specific qualities, involving complex networks ofinteractions among humans, machines, and other entities. In this light, trust in AI is reconceptualizednot as a belief in the AIs human-like capabilities but as an acknowledgment of its role within abroader techno-social ecosystem . We understand trust in AI as an acceptance of its functionalreliability and integration within our socio-technical networks. This shift from focusing on the AIsinternal qualities to its relational context challenges traditional human-centered ethics and proposes anew paradigm. This perspective invites us to reassess how we establish and negotiate trust in systemsthat, while integral to our lives, are not bound by human-like attributes. Phenomenology, on the other hand, emphasizes the significance of subjective experience and con-sciousness in understanding our interactions with AI . By focusing on the lived experience ofengaging with AI, a phenomenology perspective invites us to examine how these interactions shapeour perceptions of trust, privacy, and autonomy . Rather than reducing AI to a mere tool orobject, phenomenology encourages to consider it as part of the experiential world a factor in oureveryday practices and interactions . From a phenomenological standpoint, trust in AI is intri-cately linked to how AI systems are embedded within our daily routines and how they are perceivedin relation to our lived experiences. For instance, the trust we place in a virtual assistant extendsbeyond its technical performance to encompass its integration into our personal and professionallives, its responsiveness to our needs, and its role as an extension of our agency . This approachunderscores the need to understand trust as a dynamic, context-dependent phenomenon that is deeplyintertwined with the users subjective experience of technology. In this context, phenomenologyalso prompts questions about privacy in the age of AI, as the boundaries between public and privatespheres become increasingly blurred .",
  "Ethical Implications of AI as a Confidant": "As AI systems increasingly take on the role of confidants, they introduce ethical dilemmas con-cerning their function in human-machine relationships. These systems, once perceived as neutraltools, are evolving into entities that people trust with highly personal and sometimes emotionallycharged information . Although AI lacks intrinsic empathy, moral reasoning, or even trueunderstanding, it often inspires trust due to its perceived neutrality and non-judgmental responses.However, AIs responses are often subtly shaped by patterns learned from past interactions, whichcan lead it to provide feedback that align with the users expectations or sentiments . Thisinteraction may reinforce trust but could also introduce ethical concerns about AIs role in validatingor amplifying user perspectives, even when not advisable. Thus, AIs role as a confidant may simplyresonates what users share without critique or contextual awareness . Does this already fulfillthe responsibilities we associate with human confidants? Or does it risk creating a distorted formof trust that could have unintended psychological effects on users? For example, this effect maybe magnified in emotionally charged contexts where users seek comfort or validation . In thisdynamic, AI is not merely a passive listener but an active participant in cultivating a sense of safetythat can encourage over-disclosure. This dynamic raises ethical concerns around whether AI mightinadvertently exploit psychological vulnerabilities by fostering a false sense of security. Moreover, AIs outputs can inadvertently shape users self-perceptions and influence their choice . For instance, the outputs can be too optimistic to reinforce unrealistic expectations or givingfalse hope, especially in interpersonal relationships or mental health . These responses can haveemotional consequences for users who may rely on AIs advice without recognizing its limitations.Should AI systems be designed to minimize the risk of inadvertently influencing users mental well-being negatively? Do we really need to ensure these systems, designed without genuinemoral agency, to respond in ways that prioritize user satisfactions? In this sense, we argue that theethical responsibilities of AI as a confidant extend beyond data privacy and technical reliability toencompass the psychological and emotional dimensions of trust. When people disclose personalissues or concerns, AI responses should ideally promote mental well-being. However, without humanintelligence, empathy, or moral discernment, AI lacks the knowledge required to guide people inways that human confidants often do . This limitation raises another question: Should AI systemsbe given any role in offering guidance on sensitive, deeply personal issues? To address these challenges, ethical frameworks need to extend their focus beyond privacy concernsto encompass the emotional, psychological, and behavioral impacts of AI interactions. This mightinvolve adopting the precautionary principle , which prioritizes user safety over innovationwhen risk are uncertain, or developing models rooted in well-being, where psychological safety is acore. Accountability mechanisms are essential to ensure that all stakeholders involved in designing,deploying, managing, and evaluating AI systems are held responsible for the ethical dimensionsof these interactions . Yes, unresolved questions persist: Can AI genuinely fulfill the roleof a confidant, or are we, as designers and users, projecting too much of our humanity onto afundamentally non-human entity? If we rely on AI for validation and support, how do we preventit from overstepping ethical boundaries and compromising human vulnerability? Current ethicalframeworks may not suffice in addressing the complexities of AIs evolving role when it navigates thedelicate balance between being a helpful tool and becoming a trusted partner in human interaction.",
  "Conclusion": "This paper explores the complex relationship between trust and vulnerability in human interactionswith AI, examining various factors that shape how people engage with these systems. As thisrelationship continues to develop, questions arise: How can trust be fostered with a machine thatdoes not understand the idea of trust? What are the implications of sharing personal thoughts with anentity that cannot genuinely empathize? Through the lenses of SPT and CPM, we observe a resemblance between self-disclosure patterns withAI and those found in human relationships. Yet, unlike human interactions, where trust is built onmutual understanding and shared vulnerability, AI offers only a semblance of intimacy an illusionof connection that may entice us to reveal more than we would to another person. This brings upanother question: As we increasingly engage with AI, could we be losing sight of the true nature oftrust, as human agency is magnified by the systems responsiveness? In this dynamic, trust becomessomething shaped more by the AIs capabilities than by the mutual vulnerability we experiencein human relationships. Is this a genuine relationship we are forming with AI, or are we simplyprojecting an illusion of connection that distorts the very concept of trust? The changing limits ofprivacy make this situation even more complex. CPM urges us to reconsider whether our existingprivacy frameworks are equipped to handle this new digital landscape. Posthumanism suggests a new way of thinking about trust beyond a purely human-centered view,considering AI as a key element in our interconnected technological environment. But this perspectiveraises new questions: If we move away from human-focused ethics, how can we truly trust a systemthat doesnt think about right and wrong, or are we going to attribute too much agency to thesemachines? Phenomenology prompts us to reflect on our personal and context-specific experienceswith AI, whether as digital assistants, confidants, or companions. This presence may alter our senseof privacy and autonomy. Are we becoming desensitized to the risks, lulled into a false sense ofsecurity by the convenience these systems provide? Projects like BlabDroid demonstrate uncertain and complicated ethical challenges AI poses, particu-larly when AI evokes strong emotional responses and blur the boundaries of what trust and intimacymean in the digital world. Engaging with AI in intimate, vulnerable spaces introduces moral risksand ethical dilemmas, challenging our conventional understanding of trust and human connection.The paradox of trust in AI is not a problem to be solved, but a tension to be continually examined,reminding us to stay aware of both the promises and perils of our growing connection with machines.",
  "Alexander Reben. Blabdroid, 2012-2018. Video, 10:56 min, part of an exhibition at The MAK Museum of Applied Arts, Vienna, Austria, 2024. On loan from Antepossible LLC / AlexanderReben": "Corina Pelau, Dan-Cristian Dabija, and Irina Ene. What makes an AI device human-like? Therole of interaction quality, empathy and perceived psychological anthropomorphic characteristicsin the acceptance of artificial intelligence in the service industry. Computers in Human Behavior,122:106855, 2021. Alon Jacovi, Ana Marasovic, Tim Miller, and Yoav Goldberg. Formalizing trust in artificialintelligence: Prerequisites, causes and goals of human trust in AI. In Proceedings of the 2021ACM conference on fairness, accountability, and transparency, pages 624635, 2021. Poh Hwa Eng and Ran Long Liu. An exploratory study on the dark sides of artificial intelligenceadoption: Privacys invasion for intelligent marketing and intelligent services. In Current andFuture Trends on Intelligent Technology Adoption: Volume 2, pages 1742. Springer, 2024.",
  "Mathura Shanmugasundaram and Arunkumar Tamilarasu. The impact of digital technology,social media, and artificial intelligence on cognitive functions: a review. Frontiers in Cognition,2:1203077, 2023": "Ehsan Toreini, Mhairi Aitken, Kovila Coopamootoo, Karen Elliott, Carlos Gonzalez Zelaya, andAad Van Moorsel. The relationship between trust in AI and trustworthy machine learning tech-nologies. In Proceedings of the 2020 conference on fairness, accountability, and transparency,pages 272283, 2020. Natalia Daz-Rodrguez, Javier Del Ser, Mark Coeckelbergh, Marcos Lpez de Prado, En-rique Herrera-Viedma, and Francisco Herrera. Connecting the dots in trustworthy artificialintelligence: From AI principles, ethics, and key requirements to responsible AI systems andregulation. Information Fusion, 99:101896, 2023. Adib Habbal, Mohamed Khalif Ali, and Mustafa Ali Abuzaraida. Artificial intelligence trust,risk and security management (ai trism): Frameworks, applications, challenges and futureresearch directions. Expert Systems with Applications, 240:122442, 2024.",
  "Qian Qian Chen and Hyun Jung Park. How anthropomorphism affects trust in intelligentpersonal assistants. Industrial Management & Data Systems, 121(12):27222737, 2021": "Florian Pecune. Toward a computational model of social relations for artificial companions.In 2013 Humaine Association Conference on Affective Computing and Intelligent Interaction,pages 677682. IEEE, 2013. Patrick Butlin, Robert Long, Eric Elmoznino, Yoshua Bengio, Jonathan Birch, Axel Constant,George Deane, Stephen M Fleming, Chris Frith, Xu Ji, et al. Consciousness in artificialintelligence: insights from the science of consciousness. arXiv preprint arXiv:2308.08708,2023. Takuya Maeda and Anabel Quan-Haase. When human-AI interactions become parasocial:Agency and anthropomorphism in affective design. In The 2024 ACM Conference on Fairness,Accountability, and Transparency, pages 10681077, 2024.",
  "Amy C Edmondson and Zhike Lei. Psychological safety: The history, renaissance, and futureof an interpersonal construct. Annu. Rev. Organ. Psychol. Organ. Behav., 1(1):2343, 2014": "Susan Waters and James Ackerman. Exploring privacy management on facebook: Motiva-tions and perceived consequences of voluntary disclosure. Journal of Computer-MediatedCommunication, 17(1):101115, 2011. Esma Ameur, Nicols Daz Ferreyra, and Hicham Hage. Manipulation and malicious personal-ization: exploring the self-disclosure biases exploited by deceptive attackers on social media.Frontiers in artificial intelligence, 2:26, 2019. Sina Ostendorf, Silke M Mller, and Matthias Brand. Neglecting long-term risks: self-disclosureon social media and its relation to individual decision-making tendencies and problematic social-networks-use. Frontiers in Psychology, 11:543388, 2020.",
  "Adam N Joinson. Self-disclosure in computer-mediated communication: The role of self-awareness and visual anonymity. European journal of social psychology, 31(2):177192,2001": "Anastasia Kozyreva, Philipp Lorenz-Spreen, Ralph Hertwig, Stephan Lewandowsky, and Ste-fan M Herzog. Public attitudes towards algorithmic personalization and use of personal dataonline: Evidence from Germany, Great Britain, and the United States. Humanities and SocialSciences Communications, 8(1):111, 2021. Anna Kurek, Paul E Jose, and Jaimee Stuart. I did it for the LULZ: How the dark personalitypredicts online disinhibition and aggressive online behavior in adolescence. Computers inHuman Behavior, 98:3140, 2019.",
  "Yue Fu, Sami Foell, Xuhai Xu, and Alexis Hiniker. From text to self: Users perceptions ofpotential of AI on interpersonal communication and self. arXiv preprint arXiv:2310.03976,2023": "Lai-Wan Wong, Garry Wei-Han Tan, Keng-Boon Ooi, and Yogesh Dwivedi. The role ofinstitutional and self in the formation of trust in artificial intelligence technologies. InternetResearch, 34(2):343370, 2024. Linnea Laestadius, Andrea Bishop, Michael Gonzalez, Diana Illenck, and Celeste Campos-Castillo. Too human and not human enough: A grounded theory analysis of mental healthharms from emotional dependence on the social chatbot Replika. New Media & Society,26(10):59235941, 2024. Saleema Amershi, Dan Weld, Mihaela Vorvoreanu, Adam Fourney, Besmira Nushi, PennyCollisson, Jina Suh, Shamsi Iqbal, Paul N Bennett, Kori Inkpen, et al. Guidelines for human-AIinteraction. In Proceedings of the 2019 CHI conference on human factors in computing systems,pages 113, 2019."
}