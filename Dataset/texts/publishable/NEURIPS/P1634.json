{
  "Abstract": "Multi-View Representation Learning (MVRL) aims to learn a unified represen-tation of an object from multi-view data. Deep Canonical Correlation Analysis(DCCA) and its variants share simple formulations and demonstrate state-of-the-art performance. However, with extensive experiments, we observe the issue ofmodel collapse, i.e., the performance of DCCA-based methods will drop drasti-cally when training proceeds. The model collapse issue could significantly hinderthe wide adoption of DCCA-based methods because it is challenging to decidewhen to early stop. To this end, we develop NR-DCCA, which is equipped with anovel noise regularization approach to prevent model collapse. Theoretical anal-ysis shows that the Correlation Invariant Property is the key to preventing modelcollapse, and our noise regularization forces the neural network to possess sucha property. A framework to construct synthetic data with different common andcomplementary information is also developed to compare MVRL methods com-prehensively. The developed NR-DCCA outperforms baselines stably and con-sistently in both synthetic and real-world datasets, and the proposed noise regu-larization approach can also be generalized to other DCCA-based methods suchas DGCCA. Our code will be released at",
  "Introduction": "In recent years, multi-view representation learning (MVRL) has emerged as a core technologyfor learning from multi-source data and providing readily useful representations to downstreamtasks (Sun et al. 2023, Yan et al. 2021), and it has achieved tremendous success in various ap-plications, such as video surveillance (Guo et al. 2015, Feichtenhofer et al. 2016, Deepak, K. et al.2021), medical diagnosis (Wei et al. 2019, Xu et al. 2020) and social media (Srivastava & Salakhut-dinov 2012, Karpathy & Fei-Fei 2015, Mao et al. 2014, Fan et al. 2020). Specifically, multi-sourcedata can be collected from the same object, and each data source can be regarded as one view ofthe object. For instance, an object can be described simultaneously through texts, videos, and au-dio, which contain both common and complementary information of the object (Yan et al. 2021,Zhang, Liu & Fu 2019, Hwang et al. 2021, Geng et al. 2021), and the MVRL aims to learn a unifiedrepresentation of the object from the multi-view data.",
  "arXiv:2411.00383v1 [cs.LG] 1 Nov 2024": "The key challenge of MVRL is to learn the intricate relationships of different views. The Canoni-cal Correlation Analysis (CCA), which is one of the early and representative methods for MVRL,transforms all the views into a unified space by maximizing their correlations (Hotelling 1992, Horst1961, Hardoon et al. 2004, Lahat et al. 2015, Yan et al. 2023, Sun et al. 2023). Through correlationmaximization, CCA can identify the common information between different views and extract themto form the representation of the object. On top of CCA, Linear CCA, and DCCA maximize thecorrelation defined by CCA through gradient descent, while the former uses an affine transforma-tion and the latter uses Deep Neural Networks (DNNs). (Andrew et al. 2013). Indeed, there arequite a few variants of DCCA, such as DGCCA (Benton et al. 2017), DCCAE (Wang et al. 2015),DVCCA (Wang et al. 2016), DTCCA (Wong et al. 2021) and DCCA GHA (Chapman et al. 2022). However, extensive experimentation reveals that DCCA-based methods typically excel during theinitial stages of training but suffer a significant decline in performance as training progresses.This phenomenon is defined as model collapse within the context of DCCA. Notably, our definitionis grounded in the performance of the learned representations on downstream tasks. Previous studiesfound that the representations (i.e., final output) of both Linear CCA and DCCA are full-rank (An-drew et al. 2013, De Bie & De Moor 2003). Nevertheless, they did not further explore whethermerely guaranteeing that the full-rank representations can guarantee that the weight matrices arefull-rank. Though early stopping could be adopted to prevent model collapse (Prechelt 1998, Yao et al. 2007),it remains challenging when to stop. The model collapse issue of DCCA-based methods prevents theadoption in large models, and currently, many applications still use simple concatenation to combinedifferent views (Yan et al. 2021, Zheng et al. 2020, Nie et al. 2017). Therefore, how to develop aDCCA-based MVRL method free of model collapse remains an interesting and open question. In this work, we demonstrate that both representations and weight matrices of Linear CCA arefull-rank whereas DCCA only guarantees that representations are full-rank but not for the weightmatrices. Considering that Linear CCA does not show the model collapse while DCCA does, weconjecture that the root cause of the model collapse in DCCA is that the weight matrices in DNNstend to be low-rank. A wealth of research supports this assertion, both theoretically and empiri-cally, demonstrating that over-parameterized DNNs are predisposed to discovering low-rank solu-tions (Jing et al. 2021, Saxe et al. 2019, Soudry et al. 2018, Dwibedi et al. 2021). If the weightmatrices in DNNs tend to be low-rank, it means that the weight matrices are highly self-related andredundant, which limits the expressiveness of DNNs and thus affects the quality of representations. Therefore, this paper develops NR-DCCA, a DCCA-based method equipped with a generalizednoise regularization (NR) approach. The NR approach ensures that the correlation with random datais invariant before and after the transformation, which we define as the Correlation Invariant Prop-erty (CIP). It is also verified that the NR approach can be applied to other DCCA-based methods.Comprehensive experiments using both synthetic datasets and real-world datasets demonstrate theconsistent outperformance and stability of the developed NR-DCCA method. From a theoretical perspective, we derive the equivalent conditions between the full-rank propertyand CIP of the weight matrix. By forcing DNNs to possess CIP and thus mimicking the behaviorof Linear CCA, we introduce random data to constrain the weight matrices in DNNs and expect toavoid them being redundant and thus prevent model collapse.",
  "Multi-view representation learning": "MVRL aims to uncover relationships among multi-view data in an unsupervised manner, therebyobtaining semantically rich representations that can be utilized for various downstream tasks (Sunet al. 2023, Yan et al. 2021). Several works have been proposed to deal with MVRL from differ-ent aspects. DMF-MVC (Zhao et al. 2017) utilizes deep matrix factorization to extract a sharedrepresentation from multiple views. MDcR (Zhang, Fu, Hu, Zhu & Cao 2016) maps each view toa lower-dimensional space and applies kernel matching to enforce dependencies across the views.CPM-Nets (Zhang, Han, Fu, Zhou, Hu et al. 2019) formalizes the concept of partial MVRL andmany works have been proposed for such issue (Zhang et al. 2020, Tao et al. 2019, Li et al. 2022,Yin & Sun 2021). AE2-Nets (Zhang, Liu & Fu 2019) utilizes a two-level autoencoder frameworkto obtain a comprehensive representation of multi-view data. DUA-Nets (Geng et al. 2021) takes agenerative modeling perspective and dynamically estimates the weights for different views. MVT-CAE (Hwang et al. 2021) explores MVRL from an information-theoretic perspective, which cancapture the shared and view-specific factors of variation by maximizing or minimizing specific totalcorrelation. Our work focuses on CCA as a simple, classic, and theoretically sound approach as itcan still achieve state-of-the-art performance consistently.",
  "CCA and its variants": "Canonical Correlation Analysis (CCA) projects the multi-view data into a unified space by maxi-mizing their correlations (Hotelling 1992, Horst 1961, Hardoon et al. 2004, Lahat et al. 2015, Yanet al. 2023, Sun et al. 2023). It has been widely applied in various scenarios that involve multi-view data, including dimension reduction (Zhang, Zhang, Pan & Zhang 2016, Sun, Ceran & Ye2010, Avron et al. 2013), classification (Kim et al. 2007, Sun, Ji & Ye 2010), and clustering (Fernet al. 2005, Chang & Lin 2011). To further enhance the nonlinear transformability of CCA, KernelCCA (KCCA) uses kernel methods, while Deep CCA (DCCA) employs DNNs. Since DNNs isparametric and can take advantage of large amounts of data for training, numerous DCCA-basedmethods have been proposed. Benton et al. (2017) utilizes DNNs to optimize the objective of Gen-eralized CCA, to reveal connections between multiple views more effectively. To better preserveview-specific information, Wang et al. (2015) introduces the reconstruction errors of autoencodersto DCCA. Going a step further, Wang et al. (2016) proposes Variational CCA and utilizes dropoutand private autoencoders to project common and view-specific information into two distinct spaces.Furthermore, many studies are exploring efficient methods for computing the correlations betweenmulti-view data when dealing with more than two views such as MCCA, GCCA, and TCCA (Horst1961, Nielsen 2002, Kettenring 1971, Hwang et al. 2021). Some research focuses on improving theefficiency of computing CCA by avoiding the need for singular value decomposition (SVD) (Changet al. 2018, Chapman et al. 2022). However, the model collapse issue of DCCA-based methods hasnot been explored and addressed.",
  "Noise Regularization": "Noise regularization is a pluggable approach to regularize the neural networks during train-ing (Bishop 1995, An 1996, Sietsma & Dow 1991, Gong et al. 2020). In supervised tasks, Sietsma &Dow (1991) might be the first to propose that, by adding noise to the train data, the model will gener-alize well on new unseen data. Moreover, Bishop (1995), Gong et al. (2020) analyze the mechanismof the noise regularization, and He et al. (2019), Gong et al. (2020) indicate that noise regularizationcan also be used for adversarial training to improve the generalization of the network. In unsuper-vised tasks, Poole et al. (2014) systematically explores the role of noise injection at different layersin autoencoders, and distinct positions of noise perform specific regularization tasks. However,how to make use of noise regularization for DCCA-based methods, especially for preventing modelcollapse, has not been studied.",
  "Settings for MVRL": "Suppose the set of datasets from K different sources that describe the same object is representedby X, and we define X = {X1, , Xk, , XK}, Xk Rdkn, where xk represents the k-thview (k-th data source), n is the sample size, and dk represents the feature dimension for the k-thview. And we use Xk to denote the transpose of Xk. We take the Caltech101 dataset as an exampleand the training set has 6400 images. One image has been fed to three different feature extractorsproducing three features: a 1984-d HOG feature, a 512-d GIST feature, and a 928-d SIFT feature.Then for this dataset, we have X1 R19846400, X2 R5126400 , X3 R9286400. The objective of MVRL is to learn a transformation function that projects the multi-view dataX to a unified representation Z Rmn, where m represents the dimension of the representationspace, as shown below:Z = (X) = (X1, , Xk, , XK).(1)After applying for representation learning, we expect that the performance of using Z would bebetter than directly using X for various downstream tasks.",
  "Corr(W1X1, W2X2) = tr((1/211121/222)1/211121/222)1/2(2)": "where tr denotes the matrix trace, 11, 22 represent the self-covariance matrices of the projectedviews, and 12 is the cross-covariance matrix between the projected views (DAgostini 1994, An-drew et al. 2013). The correlation between the two projected views can be regarded as the sum ofall singular values of the normalized cross-covariance (Hotelling 1992, Anderson et al. 1958).",
  "{W k }k = arg max{Wk}k Corr(W1X1, , WkXk, , WKXK).(4)": "Once W k is obtained by backpropagation, the multi-view data are projected into a unified space.Lastly, all projected data are concatenated to obtain Z = [W 1 X1; ; W k Xk; ; W KXK] fordownstream tasks. As an extension of linear CCA, DCCA employs neural networks to capture the nonlinear relationshipamong multi-view data. The only difference between DCCA and Linear CCA is that the lineartransformation matrix Wk is replaced by multi-layer perceptrons (MLP). Specifically, each Wk isreplaced by a neural network fk, which can be viewed as a nonlinear transformation. Similar toLinear CCA, the goal of DCCA is to solve the following optimization problem:",
  "Despite exhibiting promising performance, DCCA shows a significant decline in performance asthe training proceeds. We define this decline-in-performance phenomenon as the model collapse ofDCCA": "Previous studies found that the representations (i.e., final output) of both Linear CCA and DCCAare full-rank (Andrew et al. 2013, De Bie & De Moor 2003). However, we further demonstratethat both representations and weight matrices of Linear CCA are full-rank whereas DCCA onlyguarantees that representations are full-rank but not for the weight matrices. Given that Linear CCAhas only a single layer of linear transformation Wk and the representations WkXk are constrained tobe full-rank by the loss function, Wk in Linear CCA is full-rank (referred to Lemma 4 and assumethat Wk is a square matrix and Xk is full-rank). As for DCCA, we consider a simple case whenfk(Xk) = Relu(WkXk), and fk is a single-layer network and uses an element-wise Relu activationfunction. Only the representations Relu(WkXk) are constrained to be full-rank, and hence wecannot guarantee that WkXk is full-rank. For example, when Relu(WkXk) = 1, 00, 1, it is clearthat this is a matrix of rank 2, but in fact WkXk can be 1,11,1, and this is not full-rank. Thisreveals that the neural network fk is overfitted on Xk, i.e., making representations Relu(WkXk) tobe full-rank with the constraint of its loss function, rather than Wk itself being full-rank (verified inAppendix A.5.1). Thus, we hypothesize that model collapse in DCCA arises primarily due to the low-rank natureof the DNN weight matrices. To investigate this, we analyze the eigenvalue distributions of thefirst linear layers weight matrices in both DCCA and NR-DCCA across various training epochson synthetic datasets. illustrates that during the initial training phase (100th epoch), theeigenvalues decay slowly for both DCCA and NR-DCCA. However, by the 1200th epoch, DCCAexhibits a markedly faster decay in eigenvalues compared to NR-DCCA. This observation suggests asynchronization between model collapse in DCCA and increased redundancy of the weight matrices.For more details on the experimental setup and results, please refer to .2.",
  "Method": "Based on the discussions in previous sections, we present NR-DCCA, which makes use of the noiseregularization approach to prevent model collapse in DCCA. Indeed, the developed noise regular-ization approach can be applied to variants of DCCA methods, such as Deep Generalized CCA(DGCCA) (Benton et al. 2017). An overview of the NR-DCCA framework is presented in . : Illustration of NR-DCCA. We take the CUB dataset as an example: similar to DCCA,the k-th view Xk is transformed using fk to obtain new representation fk(Xk) and then maximizethe correlation between new representations. Additionally, for the k-th view, we incorporate theproposed NR loss to regularize fk. The key idea in NR-DCCA is to generate a set of i.i.d Gaussian white noise, denoted as A ={A1, , Ak, , AK}, Ak Rdkn, with the same shape as the multi-view data Xk. In Lin-ear CCA, the correlation with noise is invariant to the linear transformation Wk: Corr(Xk, Ak) =Corr(WkXk, WkAk) (rigorous proof provided in Theorem 1). However, for DCCA, Corr(Xk, Ak)might not equal Corr(fk(Xk), fk(Ak)) because the powerful neural networks fk have overfittedto the maximization program in DCCA and the weight matrices have been highly self-related.Therefore, we enforce the DCCA to mimic the behavior of Linear CCA by adding an NR lossk = |Corr(fk(Xk), fk(Ak)) Corr(Xk, Ak)|, and hence the formulation of NR-DCCA is:",
  "Theoretical Analysis": "In this section, we provide the rationale for why the developed noise regularization can help toprevent the weight matrices from being low-rank and thus model collapse. Moreover, we prove theeffect of full-rank weight matrices on the representations, which provides a tool to empirically verifythe full-rank property of weight matrices by the quality of representations. Utilizing a new Moore-Penrose Inverse (MPI)-based (Petersen et al. 2008) form of Corr in CCA,we discover that the full-rank property of Wk is equal to CIP:Theorem 1 (Correlation Invariant Property (CIP) of Wk) Given Wk is a square matrix for anyk and k = |Corr(WkXk, WkAk) Corr(Xk, Ak)|, we have k = 0 (i.e. CIP) Wk isfull-rank. Similarly, we say fk possess CIP if k = 0. Under Linear CCA, it is redundant to introduce the NRapproach and force Wk to possess CIP, since forcing WkXk to be full-rank is sufficient to ensure thatWk is full-rank. However, in DCCA, fk is overfitted on Xk, i.e., making representations fk(Xk)to be full-rank, rather than weight matrices in fk being full-rank. By forcing fk to possess CIP andthus mimicking the behavior of Linear CCA, the NR approach constrains the weight matrices to befull-rank and less redundant and thus prevents model collapse.",
  "minQk QkWk(Xk + Ak) WkXkF nWkAkF , E(WKAk2F ) = Wk2F(8)": "where F denotes the Frobenius norm and is a small positive threshold. Pk and Qk are searchedweight matrices of k-th view to recover the input and discard noise, respectively. And we referPkWkXk XkF and QkWk(Xk + Ak) WkXkF as reconstruction loss and denosing loss. Theorem 2 suggests that the obtained representation is of low reconstruction loss and denoisingloss. Low reconstruction loss suggests that the representations can be linearly reconstructed to theinputs. This implies that Wk preserves distinct and essential features of the input data, which is adesirable property to avoid model collapse since it ensures that the model captures and retains thewhole modality of data (Zhang, Liu & Fu 2019, Tschannen et al. 2018, Tian & Zhang 2022). Lowdenoising loss implies that the models representation is robust to noise, which means that smallperturbations in the input do not lead to significant changes in the output. This condition can beseen as a form of regularization that prevents overfitting the noise in the data (Zhou & Paffenroth2017, Yan et al. 2023, Staerman et al. 2023). Additionally, the theorem also suggests that the rankof weight matrices is a good indicator to assess the quality of representations, which coincides withexisting literature (Kornblith et al. 2019, Raghu et al. 2021, Garrido et al. 2023, Nguyen et al. 2020,Agrawal et al. 2022).",
  "RQ3: Does NR-DCCA perform consistently in real-world datasets?": "We follow the protocol described in Hwang et al. (2021) for evaluating the MVRL methods. For eachdataset, we construct a training dataset and a test dataset. The encoders of all MVRL methods aretrained on the training dataset. Subsequently, we encode the test dataset to obtain the representation,which will be evaluated in downstream tasks. We employ Ridge Regression (Hoerl & Kennard1970) for the regression task and use R2 as the evaluation metric. For the classification task, we usea Support Vector Classifier (SVC) (Chang & Lin 2011) and report the average F1 scores. All tasksare evaluated using 5-fold cross-validation, and the reported results correspond to the average valuesof the respective metrics. For a fair comparison, we use the same architectures of MLPs for all D(G)CCA methods. To be spe-cific, for the synthetic dataset, which is simple, we employ only one hidden layer with a dimensionof 256. For the real-world dataset, we use MLPs with three hidden layers, and the dimension of themiddle hidden layer is 1024. We further demonstrate that increasing the depth of MLPs further ac-celerates the mod collapse of DCCA, while NR-DCCA maintains a stable performance in AppendixA.7. Baseline methods include CONCAT, PRCCA (Tuzhilina et al. 2023), KCCA (Akaho 2006),Linear CCA Wang et al. (2015),Linear GCCA,DCCA (Andrew et al. 2013),DCCA EY,DCCA GHA (Chapman et al. 2022), DGCCA (Benton et al. 2017), DCCAE/DGCCAE (Wanget al. 2015), DCCA PRIVATE/DGCCA PRIVATE (Wang et al. 2016), and MVTCAE (Hwanget al. 2021). It is important to note that our proposed NR approach requires the noise matrix employed to befull-rank, which is compatible with several common continuous noise distributions. In our primaryexperiments, we utilize Gaussian white noise. Additionally, as demonstrated in Appendix A.6,uniformly distributed noise is also effective in our NR approach. Details of the experiment settings including datasets and baselines are presented in Appendix A.3.Hyper-parameter settings, including ridge regularization of DCCA, of NR, are discussed in Ap-pendix A.5. We also analyze the computational complexity of different DCCA-based methods inAppendix A.12 and the learned representations are visualized in Appendix A.9. In the main paper,we mainly compare Linear CCA, DCCA-based methods, and NR-DCCA while other MVRL meth-ods are discussed in Appendix A.11. The results related to DGCCA and are similar and presentedin Appendix A.10.",
  "Construction of synthetic datasets (RQ1)": "We construct synthetic datasets to assess the performance of MVRL methods, and the frameworkis illustrated in . We believe that the multi-view data describes the same object, whichis represented by a high-dimensional embedding Gdn, where d is the feature dimension and nis the size of the data, and we call it God Embedding. Each view of data is regarded as a non-linear transformation of part (or all) of G. For example, we choose K = 2, d = 100, and thenX1 = 1(G[0 : 50 + CR/2, :]), X2 = x(G[50 CR/2 : 100], :), where 1 and 2 are non-lineartransformations, and CR is referred to as common rate. The common rate is defined as follows:Definition 1 (Common Rate) For two view data X = {X1, X2}, common rate is defined as thepercentage overlap of the features in X1 and X2 that originate from G. One can see that the common rate ranges from 0% to 100%. The larger the value, the greater thecorrelation between the two views, and a value of 0 indicates that the two views do not share anycommon dimensions in G. Additionally, we construct the downstream tasks by directly transformingthe God Embedding G. Each task Tj = j(G), where j is a transformation, and Tj represents thej-th task. By setting different G, common rates, k, and j, we can create various synthetic datasetsto evaluate the MVRL methods. Finally, Xk are observable to the MVRL methods for learningthe representation, and the learned representation will be used to classify/regress Tj to examine theperformance of each method. Detailed implementation is given in Appendix A.4.",
  "(e) Denoisng": ": (a) Mean and standard deviation of the (D)GCCA-based method performance acrosssynthetic datasets in different training epochs.(b) The mean correlation between noise and real dataafter transformation varies with epochs. (c) Average NESum across all weights within the trainedencoders. (d,e) The mean of Reconstruction and Denoising Loss on the test set. : Performance of DGCCA-based methods in real-world datasets. Each column representsthe performance on a specific dataset. The number of views in the dataset is denoted in the paren-theses next to the dataset name.",
  "Consistent Performance on Real-world Datasets (RQ3)": "We further conduct experiments on three real-world datasets: PolyMnist (Sutter et al. 2021),CUB (Wah et al. 2011), Caltech (Deng et al. 2018). Additionally, we use a different number ofviews in PolyMnist. The results are presented in , and the performance of the final epoch inthe figure is presented in in the Appendix A.11. Generally, the proposed NR-DCCA demon-strates a competitive and stable performance. Different from the synthetic data, the DCCA-basedmethods exhibit varying degrees of collapse on various datasets, which might be due to the complexnature of the real-world views. : Performance of different methods in real-world datasets. Each column represents theperformance on a specific dataset. The number of views in the dataset is denoted in the parenthesesnext to the dataset name.",
  "Conclusions": "We propose a novel noise regularization approach for DCCA in the context of MVRL, and it canprevent model collapse during the training, which is an issue observed and analyzed in this paper forthe first time. Specifically, we theoretically analyze the CIP in Linear CCA and demonstrate that it isthe key to preventing model collapse. To this end, we develop a novel NR approach to equip DCCAwith such a property (NR-DCCA). Additionally, synthetic datasets with different common ratesare generated and tested, which provide a benchmark for fair and comprehensive comparisons ofdifferent MVRL methods. The NR-DCCA developed in the paper inherits the merits of both LinearCCA and DCCA to achieve stable and consistent outperformance in both synthetic and real-worlddatasets. More importantly, the proposed noise regularization approach can also be generalized toother DCCA-based methods (e.g., DGCCA). In future studies, we wish to explore the potential of noise regularization in other representationlearning tasks, such as contrastive learning and generative models. It is also interesting to furtherinvestigate the difference between our developed NR and other neural network regularization ap-proaches, such as orthogonality regularization (Bansal et al. 2018, Huang et al. 2020) and weightdecay (Loshchilov & Hutter 2017, Zhang et al. 2018, Krogh & Hertz 1991). Our ultimate goalis to make the developed noise regularization a pluggable and useful module for neural networkregularization. The work described in this paper was supported by the Research Grants Council of the Hong KongSpecial Administrative Region, China (Project No. PolyU/25209221 and PolyU/15206322), andgrants from the Otto Poon Charitable Foundation Smart Cities Research Institute (SCRI) at theHong Kong Polytechnic University (Project No. P0043552). The contents of this article reflect theviews of the authors, who are responsible for the facts and accuracy of the information presentedherein. Agrawal, K. K., Mondal, A. K., Ghosh, A. & Richards, B. (2022), a-req: Assessing representa-tion quality in self-supervised learning by measuring eigenspectrum decay, Advances in NeuralInformation Processing Systems 35, 1762617638.",
  "Chapman, J., Aguila, A. L. & Wells, L. (2022), A generalized eigengame with extensions to multi-view representation learning, arXiv preprint arXiv:2211.11323": "Chapman, J. & Wang, H.-T. (2021), Cca-zoo: A collection of regularized, deep learning based,kernel, and probabilistic cca methods in a scikit-learn style framework, Journal of Open SourceSoftware 6(68), 3823. DAgostini, G. (1994), On the use of the covariance matrix to fit correlated data, Nuclear Instru-ments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors andAssociated Equipment 346(1-2), 306311.",
  "Deng, C., Chen, Z., Liu, X., Gao, X. & Tao, D. (2018), Triplet-based deep hashing network forcross-modal retrieval, IEEE Transactions on Image Processing 27(8), 38933903": "Dwibedi, D., Aytar, Y., Tompson, J., Sermanet, P. & Zisserman, A. (2021), With a little help frommy friends: Nearest-neighbor contrastive learning of visual representations, in Proceedings ofthe IEEE/CVF International Conference on Computer Vision, pp. 95889597. Fan, W., Ma, Y., Xu, H., Liu, X., Wang, J., Li, Q. & Tang, J. (2020), Deep adversarial canonical cor-relation analysis, in Proceedings of the 2020 SIAM International Conference on Data Mining,SIAM, pp. 352360. Feichtenhofer, C., Pinz, A. & Zisserman, A. (2016), Convolutional two-stream network fusion forvideo action recognition, in Proceedings of the IEEE conference on computer vision and patternrecognition, pp. 19331941. Fern, X. Z., Brodley, C. E. & Friedl, M. A. (2005), Correlation clustering for learning mixturesof canonical correlation models, in Proceedings of the 2005 SIAM International Conference onData Mining, SIAM, pp. 439448. Garrido, Q., Balestriero, R., Najman, L. & Lecun, Y. (2023), Rankme: Assessing the downstreamperformance of pretrained self-supervised representations by their rank, in International Confer-ence on Machine Learning, PMLR, pp. 1092910974.",
  "Sun, J., Xiu, X., Luo, Z. & Liu, W. (2023), Learning high-order multi-view representation bynew tensor canonical correlation analysis, IEEE Transactions on Circuits and Systems for VideoTechnology": "Sun, L., Ceran, B. & Ye, J. (2010), A scalable two-stage approach for a class of dimensionalityreduction techniques, in Proceedings of the 16th ACM SIGKDD international conference onKnowledge discovery and data mining, pp. 313322. Sun, L., Ji, S. & Ye, J. (2010), Canonical correlation analysis for multilabel classification: A least-squares formulation, extensions, and analysis, IEEE Transactions on Pattern Analysis and Ma-chine Intelligence 33(1), 194200.",
  "Definition 2 Given a specific matrix Y , its Moore-Penrose Inverse (MPI) is denoted as Y +. Y +satisfies: Y Y +Y = Y , Y +Y Y + = Y +, Y Y + is symmetric, and Y +Y is symmetric": "The MPI Y + is unique and always exists for any Y . Furthermore, when matrix Y is invertible, itsinverse matrix Y is exactly Y +. Using the definition of MPI, we can rewrite the formulation ofCCA. In particular, Corr(, ) can be derived by replacing the inverse with MPI. Using Corr(Xk, Ak)as an example, the following Lemma holds:",
  "(11)": "The first row is based on the definition of Corr, the second row is because the trace is invariant undercyclic permutation, the fifth row is to replace matrix inverse by MPI and the ninth row is due toY + = Y (Y Y +) (Petersen et al. 2008).Lemma 3 Given a specific matrix Y and its MPI Y +, let Rank(Y ) and Rank(Y +Y ) be the ranksof Y and Y +Y , respectively. It is true that:",
  "Proof of Lemma 3:": "Firstly, the column space of Y +Y is a subspace of the column space of Y .Therefore,Rank(Y +Y ) Rank(Y ). On the other hand, according to the definition of MPI (Petersen et al.2008), we know that Y = Y (Y +Y ). Since the rank of a product of matrices is at most the min-imum of the ranks of the individual matrices, we have Rank(Y ) Rank(Y +Y ). Combining thetwo inequalities, we have Rank(Y ) = Rank(Y +Y ). Furthermore, since (Y +Y )(Y +Y ) = Y +Y(it holds that Y + = Y +Y Y + according to the definition of MPI (Petersen et al. 2008)), Y +Y isan idempotent and symmetric matrix, and thus its eigenvalues must be 0 or 1. So the sum of itseigenvalues is exactly its rank. Considering matrix trace is the sum of eigenvalues of matrices, wehave Rank(Y +Y ) = tr(Y +Y ).Lemma 4 Rank(WkXk) < Rank(Xk) , when Wk is not a full-rank matrix and Xk is a full-rankmatrix. Proof of Lemma 4: Since the rank of a product of matrices is at most the minimum of the ranks ofthe individual matrices, we have Rank(WkXk) min(Rank(Wk), Rank(Xk)). Considering Xkis full-rank, Rank(Xk) = min(dk, n) and then Rank(WkXk) min(Rank(Wk), Rank(Xk)) =min(Rank(Wk), min(dk, n)). Since Wk is not full-rank, we have Rank(Wk) < dk. In conclusion,Rank(WkXk) < min(dk, min(dk, n)) and then Rank(WkXk) < dk Rank(Xk).",
  "A.1.2Main proofs of Theorem 1": "We prove the two directions of Theorem 1 in the following two Lemmas. First, we prove CIP holdsif Wk is a square and full-rank matrix.Lemma 5 For any k, if Wk is a square and full-rank matrix, the correlation between Xk and Akremains unchanged before and after the transformation by Wk (i.e. CIP holds for Wk). Mathemati-cally, we have Corr(Xk, Ak) = Corr(WkXk, WkAk).",
  "Firstly, we have the k-th view data Xk to be full-rank, as we can always delete the redundant data,and the random noise Ak is full-rank as each column is generated independently. Without loss": "of generality, we assume that all the datasets Xk are zero-centered with respect to row (Hotelling1992), which implies that WkAk and WkXk are both zero-centered matrices 1. When computing thecovariance matrix, there is no need for an additional subtraction of the mean of row, which simplifiesour subsequent derivations. And Wk is always full-rank since Linear CCA seeks full-rank WkXk.Then by utilizing Lemma 2, we derive that the correlation between Xk and Ak remains unchangedbefore and after the transformation:",
  "= Corr(Xk, Ak)": "(12)The first row is based on Lemma 2, the second row is because given two matrices B and C,(BC)+ = (B+BC)+(BC+C)+ always holds (Petersen et al. 2008), and the third row utilizesthe properties of full-rank and square matrix Wk: W +k = W k , which means W +k Wk = WkW +k =Idk (Petersen et al. 2008).",
  "Proof of Lemma 6:": "This Lemma is equivalent to its contra-positive proposition: if Wk is not a full-rank matrix, thereexists random noise data Ak such that k = |Corr(WkXk, Wk(Ak)) Corr(Xk, Ak)| is not 0.And we find that when Wk is not full-rank, there exists Ak = Xk such that k = 0. We have thefollowing derivation:",
  "(n 1)2 tr(A+k AX+k Xk)1/2": "(13)The first row is the definition of NR loss with respect to Wk, the second row is based on the newform of CCA, the third row is because given two specific matrices B and C, it holds the equality(BC)+ = (B+BC)+(BC+C)+ (Petersen et al. 2008), and the fourth row utilizes the properties offull-rank matrix: for full-rank matrices Xk and Ak, whose sample size is larger than dimension size,they fulfill: XkXk+ = Idk, AkAk+ = Idk (given a specific full-rank matrix Y , if its number of rowsis smaller than that of cols, it holds that Y + = Y (Y Y ), which means that Y Y + = I) (Petersenet al. 2008).",
  "(n 1)2 tr(X+k Xk)1/2": "(14)The first row is to replace Ak with Xk, the second row is because X+k XkX+k= X+k and(W +k WkXk)+W +k WkXk(W +k WkXk)+ = W +k WkXk, which are based on the definition of MPIthat given a specific matrix Y , Y +Y Y + = Y + (Petersen et al. 2008).",
  "(17)": "Equation a is due to Equation 16 (Petersen et al. 2008). Equation b holds because given two matricesB and C, (BC)+ = (B+BC)+(BC+C)+ always holds and Equation c is because for full-rankmatrix Xk Rdkn(dk < n), XkX+k = Idk. Equation c utilizes the properties of full-rank andsquare matrix Wk: W +k= W k , which means W +k Wk = WkW +k= Idk (Petersen et al. 2008).Equation d is based on the definition of MPI: given a specific matrix Y and its Y +, it holds thatY Y +Y = Y . We show the first property in Theorem 2.",
  "nWkAkF": "(18)Equation a is because of Equation 16. Equation b holds because given two matrices B and C,(BC)+ = (B+BC)+(BC+C)+ always holds and Equation c is because we assume Xk + Ak is afull-rank matrix. Equation e utilizes the properties of full-rank and square matrix Wk: W +k Wk =WkW +k = Idk. Equation g is based on the definition of MPI: (Xk + Ak)(Xk + Ak)+(Xk + Ak) =(Xk + Ak). Equation j holds because given two specific matrices B and C, BCF BF CF (Belitskii et al. 2013). Equation k and l is because given a specific matrix B, I B+B is anidempotent matrix and I B+BF =",
  "Synthetic datasets:": "All the datasets used in the paper are either provided or open datasets. Detailed proofs of all theTheorems in the main paper can be found in the Appendix. Both source codes and appendix can bedownloaded from the supplementary material. We make 6 groups of multi-view data originating from the same G Rdn (we set n =4000, d = 100).Each group consists of tuples with 2 views (2000 tuples for training and2000 tuples for testing) and a distinct common rate.Common rates of these sets are from{0%, 20%, 40%, 60%, 80%, 100%} and there are 50 downstream regression tasks. We report themean and standard deviation of R2 score across all the tasks.",
  "Real-world datasets:": "PolyMnist (Sutter et al. 2021): A dataset consists of tuples with 5 different MNIST images (60, 000tuples for training and 10, 000 tuples for testing). Each image within a tuple possesses distinctbackgrounds and writing styles, yet they share the same digit label. The background of each view israndomly cropped from an image and is not used in other views. Thus, the digit identity representsthe common information, while the background and writing style serve as view-specific factors. Thedownstream task is the digit classification task. CUB (Wah et al. 2011): A dataset consists of tupleswith deep visual features (1024-d) extracted by GOOGLENET and text features (300-d) obtainedthrough DOC2VEC (Le & Mikolov 2014) (480 tuples for training and 600 tuples for testing). ThisMVRL task utilizes the first 10 categories of birds in the original dataset and the downstream task isthe bird classification task. Caltech (Deng et al. 2018): A dataset consists of tuples with traditionalvisual features extracted from images that belong to 101 object categories, including an additionalbackground category (6400 tuples for training and 9144 tuples for testing). Following Yang et al.(2021), three features are used as views: a 1, 984-d HOG feature, a 512-d GIST feature, and a 928-dSIFT feature. Baselines: All of our experiments are conducted with fixed random seeds and all the performanceof downstream tasks is the average value of a 5-fold cross-validation. We use one single 3090 GPU.The CCA-zoo package is adopted as the implementation of various CCA/GCCA-based methods,and the original implementation of MVTCAE is employed. Both baselines and our developed NR-DCCA/NR-DGCCA are implemented in the same PyTorch environment (see requirements.txt in thesource codes).",
  "A.4Implementation Details of Synthetic Datasets": "We draw n random samples with dim d from a Gaussian distribution as G Rdn to representcomplete representations of n objects. We define the non-linear transformation k as the additionof noise to the data, followed by passing it through a randomly generated MLP. To generate the datafor the k-th view, we select specific feature dimensions from G based on a given common rate 1and then apply k to those selected dimensions. And we define j as a linear layer, and task Tj isgenerated by directly passing G through j.",
  "A.5Hyper-parameter Settings": "To ensure a fair comparison, we tune the hyper-parameters of all baselines within the ranges sug-gested in the original papers, including hyper-parameter r of ridge regularization, except for thefollowing fixed settings: The embedding size for the real-world datasets is set as 200, while the size for synthetic datasets isset as 100. Batch size is min(2000, full-size). The same MLP architectures are used for D(G)CCA-based methods. The hyper-parameter r of ridge regularization is set as 0 in our NR-DCCA andNR-DGCCA. And the best r for Linear (G)CCA and D(G)CCA-based methods is tuned on thevalidation data (synthetic datasets and PolyMnist: 1e-3, CUB and Caltech101 : 0). In the synthetic datasets, Linear CCA and Linear GCCA use a minimum learning rate of 1e 4,DCCA, DGCCA, DCCAE, and DGCCAE methods utilize a bigger learning rate of 5e 3.DCCA PRIVATE/DGCCA PRIVATE employ a slightly higher learning rate of 1e 2. In contrast,our proposed methods, NR-DCCA/NR-DGCCA, utilize the maximum learning rate of 1.5e 2.And the regularization weight is set as 200. In the real-world datasets, the learning rates for all deep methods are set to 1e4 while that of LinearCCA and Linear GCCA are 1e 5. To expedite the computation of Corr(Xk, Ak), in the real-worlddatasets, we simply employ Xk[: outdim, :] and Ak[: outdim, :] to compute of Corr(Xk, Ak). Theoptimal values of NR-DCCA for the CUB, PolyMnist, and Caltech datasets are found to be 1.5,2, and 10, respectively.",
  "A.5.1Hyper-parameter r in Ridge Regularization": "In this section, we discuss the effects of hyper-parameter r in ridge regularization. Ridge regulariza-tion is commonly used across almost all (D)CCA methods, which improves numerical stability. Itworks by adding an identity matrix I to the estimated covariance matrix. However, ridge regulariza-tion mainly regularizes the features, rather than the transformation (i.e., Wk in Linear CCA and fkin DCCA) and it cannot prevent the weight matrices in DNNs from being low-rank or redundant. Tofurther support our arguments, we provide the experimental results with different ridge parameterson a real-world dataset CUB as shown in . One can see that the ridge regularization evendamages the performance of DCCA and also leads to an increase in the internal correlation withinthe feature and the correlation between the feature and noise. In our NR-DCCA, we set the ridgeparameter to zero. We conjecture the reason is that the large ridge parameter could make the neuralnetwork even lazier to actively project the data into a better feature space, as the full-rank propertyof features and covariance matrix are already guaranteed.",
  "A.5.2Hyper-parameter of NR-DCCA": "The choice of the hyper-parameter is essential in NR-DCCA. Different from the conventionalhyper-parameter tuning procedures, the determination of is simpler, as we can search for thesmallest that can prevent the model collapse, and the model collapse can be directly observedon the validation data. Specifically, we increase the adaptively until the model collapse issue istackled, i.e., the correlation with noise will not increase or the performance of DCCA will not dropwith increasing training epochs, then the optimal is found. To further illustrate the influence of in NR-DCCA, we present performance curves of NR-DCCA in CUB under different . As shownin , if is too large, the convergence of the training becomes slow; if is too small, modelcollapse remains. Additionally, one can see the NR-DCCA outperforms DCCA robustly with a widerange of .",
  "A.6Effects of the Distribution of Noise": "From our theoretical analysis, the most important feature of noise in NR is that the sampled noisematrix is a full-rank matrix. Therefore, continuous distributions such as the uniform distribution canalso be applied to NR, which demonstrates the robustness of the proposed NR method. We compareNR-DCCA with different noise distributions on synthetic datasets, and both noises are effective insuppressing model collapse as shown in .",
  "A.7Effects of depths of Encoders": "In this section, we test the effects of depths of encoders (i.e. MLPs) on model collapse and NR.Specifically, we increase the depth of MLPs to observe the variation in the performance of DCCAand NR-DCCA on synthetic datasets. As shown in , The increase in network depth resultsin a faster decline in DCCA performance, while NR-DCCA still maintains a stable performance.",
  "A.8Effects of NR Loss on Filter Redundancy": "Extensive research has established a significant correlation between the redundancy of neurons orfilters and the compromised generalization capabilities of neural networks, indicating a propensityfor overfitting (Wang et al. 2020, Morcos et al. 2018, Zhu et al. 2018). Considering the fully con-nected layer with 1024 units in a MLP network as a paradigm, the initial layers weights, denotedby W R102432828, can be interpreted as 1024 discriminative filters. These filters operate onimages with 3 channels, each of size 28 28, with every filter representing a vector in a 3 28 28dimensional space. Subsequently, a similarity matrix S is constructed, wherein each element Sijquantifies the cosine similarity between the ith and jth filters, with higher values indicating greaterredundancy. To further assess filter redundancy in W, we employ NESum, a metric designed forevaluating redundancy and whiten degrees of features (Zhang et al. 2023).Definition 3 (NESum of Weight) Given a weight matrix W Routputinput with an accompa-nying output-wise similarity matrix S Routputoutput and eigenvalues {i}outputi=1sorted in de-scending order, the normalized eigenvalue sum is defined as follows:",
  "A.9Visualization of Learned Representations": "To further demonstrate the effectiveness of our method, we employ 2D-tSNE visualization to depictthe learned representations of the CUB dataset (test set) under different methods. Each data pointis colored based on its corresponding class, as illustrated in . There are a total of 10 cat-egories, with 60 data points in each category. A reasonable distribution of learned representationsentails that data points belonging to the same class are grouped in the same cluster, which is distin-guishable from clusters representing other classes. Additionally, within each cluster, the data points should exhibit an appropriate level of dispersion, indicating that the data points within the sameclass can be differentiated rather than collapsing into a single point. This dispersion is indicative ofthe preservation of as many distinctive features of the data as possible. From Figure. 9, we can observe that CCA, DCCA / DGCCA have all confused the data from dif-ferent categories. Specifically, CCA completely scatters the data points as it cannot handle non-linear relationships. By incorporating autoencoders, DCCAE / DGCCAE and DCCA PRIVATE /DGCCA PRIVATE have partially separated the data; however, they have not fully separated thegreen and orange categories. NR-DCCA / NR-DGCCA is the only method that successfully sepa-rates all categories. It is worth noting that our approach not only separates the data into different clusters but also main-tains dispersion within each cluster. Unlike DCCA PRIVATE / DGCCA PRIVATE, where the datapoints within a cluster form a strip-like distribution, our method ensures that the data points withineach cluster remain appropriately scattered.",
  "A.10DGCCA and NR-DGCCA": "This section presents the experimental results for DGCCA and NR-DGCCA, which supplement theresults of GCCA and NR-DCCA presented in the main paper. In general, DGCCA is a variant ofDCCA, and hence the proposed noise regularization approach can also be applied. We repeat theexperiments in Figures 4,and 5, and hence we have the results for DGCCA in , and 11.One can see that the proposed noise regularization approach can also help DGCCA prevent modelcollapse, proving its generalizability.",
  "A.11Additional Experimental Results": "and 4 present the model performance of various MVRL methods in synthetic and real-worlddatasets, and both tables correspond to the final epoch of the results presented in and 5. Itshould be noted that the values in represent the mean and standard deviation of the methodsacross different tasks, indicating their performance and variability.",
  "DCCADCCAEDCCA PRIVATENR-DCCA": "Generation of Noise---O(K N D)MLP EncoderO(K N L H2)O(K N L H2)O(2 K N L H2)O(2 K N L H2)MLP Decoder-O(K N L H2)O(K N L H2)-Reconstruction Loss-O(K N D)O(K N D)-Correlation MaximizationO((M K)3)O((M K)3)O((M K)3)O((M K)3)Noise Regularization---O(2 K (M K)3) Complexity of MLP: We will use neural networks with the same MLP structure, consistingof L hidden layers, each with H neurons. Therefore, the computational complexity of onepass of the data through the neural networks can be expressed as O(N (D H +D M +L H2)). To simplify, we use O(N L H2). Complexity of Corr: During the process of calculating Cor among K views, three maincomputations are involved. The calculation complexity of the covariance is O(N (M K)2. Second, the complexity of the inverse matrix and the eigenvalues are O((M K)3.As a result, the computational complexity of calculating Cor can be considered as O((M K)3)."
}