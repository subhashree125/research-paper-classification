{
  "Abstract": "Real-world offline datasets are often subject to data corruptions (such as noise oradversarial attacks) due to sensor failures or malicious attacks. Despite advances inrobust offline reinforcement learning (RL), existing methods struggle to learn robustagents under high uncertainty caused by the diverse corrupted data (i.e., corruptedstates, actions, rewards, and dynamics), leading to performance degradation inclean environments. To tackle this problem, we propose a novel robust variationalBayesian inference for offline RL (TRACER). It introduces Bayesian inferencefor the first time to capture the uncertainty via offline data for robustness againstall types of data corruptions. Specifically, TRACER first models all corruptions asthe uncertainty in the action-value function. Then, to capture such uncertainty, ituses all offline data as the observations to approximate the posterior distributionof the action-value function under a Bayesian inference framework. An appealingfeature of TRACER is that it can distinguish corrupted data from clean data usingan entropy-based uncertainty measure, since corrupted data often induces higheruncertainty and entropy. Based on the aforementioned measure, TRACER canregulate the loss associated with corrupted data to reduce its influence, therebyenhancing robustness and performance in clean environments. Experiments demon-strate that TRACER significantly outperforms several state-of-the-art approachesacross both individual and simultaneous data corruptions.",
  "Introduction": "Offline reinforcement learning (RL) aims to learn an effective policy from a fixed dataset withoutdirect interaction with the environment . This paradigm has recently attracted much attention inscenarios where real-time data collection is expensive, risky, or impractical, such as in healthcare ,autonomous driving , and industrial automation . Due to the restriction of the dataset, offlineRL confronts the challenge of distribution shift between the policy represented in the offline datasetand the policy being learned, which often leads to the overestimation for out-of-distribution (OOD)actions . To address this challenge, one of the promising approaches introduce uncertaintyestimation techniques, such as using the ensemble of action-value functions or Bayesian inference tomeasure the uncertainty of the dynamics model or the action-value function regardingthe rewards and transition dynamics. Therefore, they can constrain the learned policy to remain closeto the policy represented in the dataset, guiding the policy to be robust against OOD actions.",
  "+3": ": Graphical model of decision-making process. Nodes connected by solid lines denote datapoints in the offline dataset, while the Q values (i.e., action values) connected by dashed lines are notpart of the dataset. These Q values are often objectives that offline algorithms aim to approximate. Nevertheless, in the real world, the dataset collected by sensors or humans may be subject to extensiveand diverse corruptions , e.g., random noise from sensor failures or adversarial attacks duringRLHF data collection. Offline RL methods often assume that the dataset is clean and representative ofthe environment. Thus, when the data is corrupted, the methods experience performance degradationin the clean environment, as they often constrain policies close to the corrupted data distribution. Despite advances in robust offline RL , these approaches struggle to address the challenges posedby diverse data corruptions . Specifically, many previous methods on robust offline RL aimto enhance the testing-time robustness, learning from clean datasets and defending against attacksduring testing . However, they cannot exhibit robust performance using offline datasetwith perturbations while evaluating the agent in a clean environment. Some related works for datacorruptions (also known as corruption-robust offline RL methods) introduce statistical robustnessand stability certification to improve performance, but they primarily focus on enhancing robustnessagainst adversarial attacks . Other approaches focus on the robustness against both randomnoise and adversarial attacks, but they often aim to address only corruptions in states, rewards, ortransition dynamics . Based on these methods, recent work extends the data corruptionsto all four elements in the dataset, including states, actions, rewards, and dynamics. This workdemonstrates the superiority of the supervised policy learning scheme for the data corruptionof each element in the dataset. However, as it does not take into account the uncertainty in decision-making caused by the simultaneous presence of diverse corrupted data, this work still encountersdifficulties in learning robust agents, limiting its applications in real-world scenarios. In this paper, we propose to use offline data as the observations, thus leveraging their correlationsto capture the uncertainty induced by all corrupted data. Considering that (1) diverse corruptionsmay introduce uncertainties into all elements in the offline dataset, and (2) each element is correlatedwith the action values (see dashed lines in ), there is high uncertainty in approximating theaction-value function by using various corrupted data. To address this high uncertainty, we proposeto leverage all elements in the dataset as observations, based on the graphical model in . Byusing the high correlations between these observations and the action values , we can accuratelyidentify the uncertainty of the action-value function. Motivated by this idea, we propose a robust variational Bayesian inference for offline RL (TRACER)to capture the uncertainty via offline data against all types of data corruptions. Specifically, TRACERfirst models all data corruptions as uncertainty in the action-value function. Then, to capturesuch uncertainty, it introduces variational Bayesian inference , which uses all offline data asobservations to approximate the posterior distribution of the action-value function. Moreover, thecorrupted observed data often induce higher uncertainty than clean data, resulting in higher entropyin the distribution of action-value function. Thus, TRACER can use the entropy as an uncertaintymeasure to effectively distinguish corrupted data from clean data. Based on the entropy-baseduncertainty measure, it can regulate the loss associated with corrupted data in approximating theaction-value distribution. This approach effectively reduces the influence of corrupted samples,enhancing robustness and performance in clean environments. This study introduces Bayesian inference into offline RL for data corruptions. It significantly capturesthe uncertainty caused by diverse corrupted data, thereby improving both robustness and performancein offline RL. Moreover, it is important to note that, unlike traditional Bayesian online and offline RLmethods that only model uncertainty from rewards and dynamics , our approach identifies the",
  "uncertainty of the action-value function regarding states, actions, rewards, and dynamics under datacorruptions. We summarize our contributions as follows": "To the best of our knowledge, this study introduces Bayesian inference into corruption-robustoffline RL for the first time. By leveraging all offline data as observations, it can captureuncertainty in the action-value function caused by diverse corrupted data. By introducing an entropy-based uncertainty measure, TRACER can distinguish corruptedfrom clean data, thereby regulating the loss associated with corrupted samples to reduce itsinfluence for robustness.",
  "Preliminaries": "Bayesian RL.We consider a Markov decision process (MDP), denoted by a tuple M =(S, A, R, P, P0, ), where S is the state space, A is the action space, R is the reward space,P(|s, a) P(S) is the transition probability distribution over next states conditioned on a state-action pair (s, a), P0() P(S) is the probability distribution of initial states, and [0, 1) is thediscount factor. Note that P(S) and P(A) denote the sets of probability distributions on subsetsof S and A, respectively. For simplicity, throughout the paper, we use uppercase letters to refer torandom variables and lowercase letters to denote values taken by the random variables. Specifically,R(s, a) denotes the random variable of one-step reward following the distribution (r|s, a), andr(s, a) represents a value of this random variable. We assume that the random variable of one-steprewards and their expectations are bounded by Rmax and rmax for any (s, a) S A, respectively.",
  "Note that V (s) = Ea [Q(s, a)] = Ea, [D(s, a)]": "Variational Inference.Variational inference is a powerful method for approximating complexposterior distributions, which is effective for RL to handle the parameter uncertainty and deal withmodelling errors . Given an observation X and latent variables Z, Bayesian inference aims tocompute the posterior distribution p(Z|X). Direct computation of this posterior is often intractabledue to the high-dimensional integrals involved. To approximate the true posterior, Bayesian inferenceintroduces a parameterized distribution q(Z; ) and minimizes the Kullback-Leibler (KL) divergenceDKL(q(Z; )p(Z|X)). Note that minimizing the KL divergence is equivalent to maximizing theevidence lower bound (ELBO) : ELBO() = Eq(Z;)[log p(X, Z) log q(Z; )]. Offline RL under Diverse Data Corruptions.In the real world, the data collected by sensors orhumans may be subject to diverse corruption due to sensor failures or malicious attacks. Let b andB denotes the uncorrupted and corrupted dataset with samples {(sit, ait, rit, sit+1)}Ni=1, respectively.Each data in B may be corrupted. We assume that an uncorrupted state follows a state distributionpb(), a corrupted state follows pB(), an uncorrupted action follows a behavior policy b(|sit), acorrupted action is sampled from B(|sit), a corrupted reward is sampled from B(|sit, ait), anda corrupted next state is drawn from PB(|sit, ait). We also denote the uncorrupted and corruptedempirical state-action distributions as pb(sit, ait) and pB(sit, ait), respectively. Moreover, we introducethe notations as follows.T Q(s, a) = r(s, a) + EsPB(|s,a) [V (s)] ,r(s, a) = ErB(|s,a)[r],(2)",
  "Algorithm": "We first introduce the Bayesian inference for capturing the uncertainty caused by diverse corrupteddata in .1. Then, we provide our algorithm TRACER with the entropy-based uncertaintymeasure in .2. Moreover, we provide the theoretical analysis for robustness, the architecture,and the detailed implementation of TRACER in Appendices A.1, B.1, and B.2, respectively.",
  "Variational Inference for Uncertainty induced by Corrupted Data": "We focus on corruption-robust offline RL to learn an agent under diverse data corruptions, i.e., randomor adversarial attacks on four elements of the dataset. We propose to use all elements as observations,leveraging the data correlations to simultaneously address the uncertainty. By introducing Bayesianinference framework, our aim is to approximate the posterior distribution of the action-value function. At the beginning, based on the relationships between the action values and the four elements(i.e., states, actions, rewards, next states) in the offline dataset as shown in , we defineD = D(S, A, R) p(|S, A, R), parameterized by . Building on the action-value distribution,we can explore how to estimate the posterior of D using the elements available in the offline data. Firstly, we start from the actions {ait}Ni=1 following the corrupted distribution B and use them asobservations to approximate the posterior of the action-value distribution under a variational inference.As the actions are correlated with the action values and all other elements in the dataset, the likelihoodis pa(A|D, S, R, S), parameterized by a. Then, under the variational inference framework, wemaximize the posterior and derive to minimize the loss function based on ELBO:",
  "The goal of first terms in Equations (7), (8), and (9) is to estimate B(A|S), B(R|S, A), and pB(S)using pa(A|D, S, R, S), pr(R|D, S, A), and ps(S|D, A, R), respectively. As we do not": "have the explicit expression of distributions B, B, and pB, we cannot directly compute the KLdivergence in these first terms. To address this issue, based on the generalized Bayesian inference ,we can exchange two distributions in the KL divergence. Then, we model all the aformentioneddistributions as Gaussian distributions, and use the mean and standard deviation to representthe corresponding p. For implementation, we directly employ MLPs to output each (, ) usingthe corresponding conditions of p. Then, based on the KL divergence between two Gaussiandistributions, we can derive the loss function as follows.",
  "+(s s)T 1s (s s) + log |a| |r| |s|,(10)": "Moreover, the goal of second terms in Equations (7), (8), and (9) is to maximize the likelihoods ofD given samples s ps, a pa, or r pr. Thus, with (s, a, r) B, we propose minimizingthe distance between D(s, a, r) and D(s, a, r), D(s, a, r) and D(s, a, r), and D(s, a, r) andD(s, a, r), where s ps, a pa, and r pr. Then, based on , we can derive the followingloss with any metric to maximize the log probabilities:",
  "where () = | I { < 0}| lH () with the threshold , Z denotes the value distribution, ,": "iis the sampled TD error based on the parameters i, and are two samples drawn from a uniformdistribution U(), D(s, a, r) := F 1D(s,a,r)() is the sample drawn from p(|s, a, r), Z(s) :=F 1Z(s)() is sampled from p(|s), F 1X () is the inverse cumulative distribution function (also knownas quantile function) at for the random variable X, and N and N represent the respectivenumber of iid samples and . Notably, based on , we have Qi(s, a) = Nn=1 Dni (s, a, r). In addition, if we learn the value distribution Z, the action-value distribution can extract the informa-tion from the next states based on Equation (12), which is effective for capturing the uncertainty. Onthe contrary, if we directly use the next states in the offline dataset as the observations, in practice,the parameterized model of the action-value distribution needs to take (s, a, r, s, a, r, s) as theinput data. Thus, the model can compute the action values and values for the sampled TD errorin Equation (12). To avoid the changes in the input data caused by directly using next states asobservations in Bayesian inference, we draw inspiration from IQL and RIQL to learn a parameterizedvalue distribution. Based on Equations (5) and (12), we derive a new objective as:",
  ". (14)": "Thus, we have the whole loss function LD|S,A,R = Lfirst(i, s, a, r) + Lsecond(i, s, a, r) inthe generalized variational inference framework. Moreover, based on the assumption of heavy-tailednoise in , we have a upper bound of action-value distribution by using the Huber regression loss. Entropy-based Uncertainty Measure for Regulating the Loss associated with Corrupted Data. Tofurther address the challenge posed by diverse data corruptions, we consider the problem: how toexploit uncertainty to further enhance robustness? Considering that our goal is to improve performance in clean environments, we propose to reduce theinfluence of corrupted data, focusing on using clean data to learn agents. Therefore, we provide atwo-step plan: (1) distinguishing corrupted data from clean data; (2) regulating the loss associatedwith corrupted data to reduce its influence, thus enhancing the performance in clean environments. For (1), as the Shannon entropy for the measures of aleatoric and epistemic uncertainties providesimportant insight , and the corrupted data often results in higher uncertainty and entropy ofthe action-value distribution than the clean data, we use entropy to quantify uncertainties ofcorrupted and clean data. Furthermore, by considering that the exponential function can amplify thenumerical difference in entropy between corrupted and clean data, we propose the use of exponentialentropy a metric of extent of a distributionto design our uncertainty measure. Specifically, based on Equation 12, we can use the quantile points {n}Nn=1 to learn the correspondingquantile function values {Dn}Nn=1 drawn from the action-value distribution p. We sort the quantilepoints and their corresponding function values in ascending order based on the values. Thus, wehave the sorted sets {n}Nn=1, {Dn}Nn=1, and the estimated PDF values {n}Nn=1, where 1 = 1 and",
  "where n denotes (n1 + n)/2 for 1 < n N, and Dn denotes Dn Dn1 for 1 < n N": "For (2), TRACER employs the reciprocal value of exponential entropy 1/ exp(H(pi)) to weight thecorresponding loss of i in our proposed whole loss function LD|S,A,R. Therefore, during the learningprocess, TRACER can regulate the loss associated with corrupted data and focus on minimizing theloss associated with clean data, enhancing robustness and performance in clean environments. Notethat we normalize entropy values by dividing the mean of samples (i.e., quantile function values)drawn from action-value distributions for each batch. In , we show the relationship of entropyvalues of corrupted and clean data estimated by Equation (15) during the learning process. The resultsillustrate the effectiveness of the entropy-weighted technique for data corruptions.",
  "Experiments": "In this section, we show the effectiveness of TRACER across various simulation tasks using diversecorrupted offline datasets. Firstly, we provide our experiment setting, focusing on the corruptionsettings for offline datasets. Then, we illustrate how TRACER significantly outperforms previous state-of-the-art approaches under a range of both individual and simultaneous data corruptions. Finally, weconduct validation experiments and ablation studies to show the effectiveness of TRACER.",
  "Experiment Setting": "Building upon RIQL , we use two hyperparameters, i.e., corruption rate c and corruptionscale , to control the corruption level. Then, we introduce the random corruption and adversarialcorruption in four elements (i.e., states, actions, rewards, next states) of offline datasets. Theimplementation of random corruption is to add random noise to elements of a c portion of the offlinedatasets, and the implementation of adversarial corruption follows the Projected Gradient Descentattack using pretrained value functions. Note that unlike other adversarial corruptions, theadversarial reward corruption multiplies to the clean rewards instead of using gradient optimization.We also introduce the random or adversarial simultaneous corruption, which refers to random oradversarial corruption simultaneously present in four elements of the offline datasets. We apply thecorruption rate c = 0.3 and corruption scale = 1.0 in our experiments.",
  "IQLRIQLTRACER": ": In the left, we report the means and standard deviations on CARLA under randomsimultaneous corruptions. In the right, we report the results with random simultaneous corruptionsagainst different corruption levels. We conduct experiments on D4RL benchmark . Referring to RIQL, we train all agents for3000 epochs on the medium-replay-v2 dataset, which closely mirrors real-world applications asit is collected during the training of a SAC agent. Then, we evaluate agents in clean environments,reporting the average normalized performance over four random seeds. See Appendix C for detailedinformation. The algorithms we compare include: (1) CQL and IQL , offline RL algorithmsusing a twin Q networks. (2) EDAC and MSG , offline RL algorithms using ensemble Q net-works (number of ensembles > 2). (3) UWMSG and RIQL, state-of-the-arts in corruption-robustoffline RL. Note that EDAC, MSG, and UWMSG are all uncertainty-based offline RL algorithms.",
  "Main results under Diverse Data Corruptions": "We conduct experiments on MuJoCo (see Tables 1, 2, and 3, which highlight the highest results)and CARLA (see the left of ) tasks from D4RL under diverse corruptions to show thesuperiority of TRACER. In , we report all results under random or adversarial simultaneousdata corruptions. These results show that TRACER significantly outperforms other algorithms inall tasks, achieving an average score improvement of +21.1%. In the left of , results onCARLA-lane_v0 under random simultaneous corruptions also illustrate the superiority of TRACER.See Appendix C.2 for details. Random Corruptions.We report the results under random simultaneous data corruptions of allalgorithms in . Such results demonstrate that TRACER achieves an average score gain of+22.4% under the setting of random simultaneous corruptions. Based on the results, it is clear thatmany offline RL algorithms, such as EDAC, MSG, and CQL, suffer the performance degradationunder data corruptions. Since UWMSG is designed to defend the corruptions in rewards and dynamics,its performance degrades when faced with the stronger random simultaneous corruption. Moreover,we report results across a range of individual random data corruptions in , where TRACERoutperforms previous algorithms in 7 out of 12 settings. We then explore hyperparameter tuning onHopper task and further improve TRACERs results, demonstrating its potential for performancegains. We provide details in Appendix C.3.1. Adversarial Corruptions.We construct experiments under adversarial simultaneous corruptionsto evaluate the robustness of TRACER. The results in show that TRACER surpasses othersby a significant margin, achieving an average score improvement of +19.3%. In these simultaneouscorruption, many algorithms experience more severe performance degradation compared to therandom simultaneous corruption, which indicates that adversarial attacks are more damaging to thereliability of algorithms than random noise. Despite these challenges, TRACER consistently achievessignificant performance gains over other methods. Moreover, we provide the results across a range ofindividual adversarial data corruptions in , where TRACER outperforms previous algorithmsin 7 out of 12 settings. We also explore hyperparameter tuning on Hopper task and further improveTRACERs results, demonstrating its potential for performance gains. See Appendix C.3.1 for details. AdversarialRandomMixed Corruptions",
  "Random Simultaneous Corruptions": "TRACER Using EntroTRACER NOT using Entro : In the first column, we report the mean and standard deviation to show the superiority ofusing the entropy-based uncertainty measure. In the second and third columns, we report the resultsover three seeds to show the higher entropy of corrupted data compared to clean data during training.",
  "Evaluation of TRACER under Various Corruption Levels": "Building upon RIQL, we further extend our experiments to include Mujoco datasets with variouscorruption levels, using different corruption rates c and scales . We report the average scores andstandard deviations over four random seeds in the right of , using batch sizes of 256. Results in the right of demonstrate that TRACER significantly outperforms baseline algo-rithms in all tasks under random simultaneous corruptions with various corruption levels. It achievesan average score improvement of +33.6%. Moreover, as the corruption levels increase, the slightdecrease in TRACERs results indicates that while TRACER is robust to simultaneous corruptions,its performance depends on the extent of corrupted data it encounters. We also evaluate TRACER indifferent scales of corrupted data and provide the results in Appendix C.4.1.",
  "Evaluation of the Entropy-based Uncertainty Measure": "We evaluate the entropy-based uncertainty measure in action-value distributions to show: (1) isthe uncertainty caused by corrupted data higher than that of clean data? (2) is regulating the lossassociated with corrupted data effective in improving performance? For (1), we first introduce labels indicating whether the data is corrupted. Importantly, these labelsare not used by agents during the training process. Then, we estimate entropy values of labelledcorrupted and clean data in each batch based on Equation (15). Thus, we can compare entropy valuesto compute results, showing how many times the entropy of the corrupted data is higher than thatof clean data. Specifically, we evaluate the accuracy every 50 epochs over 3000 epochs. For eachevaluation, we sample 500 batches to compute the average entropy of corrupted and clean data. Eachbatch consists of 32 clean and 32 corrupted data. We illustrate the curves over three seeds in thesecond and third columns of , where each point shows how many of the 500 batches havehigher entropy for corrupted data than that of clean data. indicates an oscillating upward trend of TRACERs measurement accuracy using entropy(TRACER Using Entro) under simultaneous corruptions, demonstrating that using the entropy-baseduncertainty measure can effectively distinguish corrupted data from clean data. These curves alsoreveal that even in the absence of any constraints on entropy (TRACER NOT using Entro), the entropyassociated with corrupted data tends to exceed that of clean data. For (2), in the first column of , these results demonstrate that TRACER using the entropy-based uncertainty measure can effectively reduce the influence of corrupted data, thereby enhancingrobustness and performance against all corruptions. We provide detailed information for this evalua-tion in Appendix C.4.2.",
  "Related Work": "Robust RL.Robust RL can be categorized into two types: testing-time robust RL and training-timerobust RL. Testing-time robust RL refers to training a policy on clean data and ensuringits robustness by testing in an environment with random noise or adversarial attacks. Training-timerobust RL aims to learn a robust policy in the presence of random noise or adversarialattacks during training and evaluate the policy in a clean environment. In this paper, we focus ontraining-time robust RL under the offline setting, where the offline training data is subject to variousdata corruptions, also known as corruption-robust offline RL. Corruption-Robust RL.Some theoretical work on corruption-robust online RL aims toanalyze the sub-optimal bounds of learned policies under data corruptions. However, these studiesprimarily address simple bandits or tabular MDPs and focus on the reward corruption. Some furtherwork extends the modeling problem to more general MDPs and begins to investigate thecorruption in transition dynamics. It is worth noting that corruption-robust offline RL has not been widely studied. UWMSG designs a value-based uncertainty-weighting technique, thus using the weight to mitigate the impactof corrupted data. RIQL further extends the data corruptions to all four elements in the offlinedataset, including states, actions, rewards, and next states (dynamics). It then introduces quantileestimators with an ensemble of action-value functions and employs a Huber regression based onIQL , alleviating the performance degradation caused by corrupted data. Bayesian RL.Bayesian RL integrates the Bayesian inference with RL to create a framework fordecision-making under uncertainty . It is important to highlight that Bayesian RL is divided intotwo categories for different uncertainties: the parameter uncertainty in the learning of models and the inherent uncertainty from the data/environment in the distribution over returns . Inthis paper, we focus on capturing the latter. For the latter uncertainty, in model-based Bayesian RL, many approaches explicitlymodel the transition dynamics and using Bayesian inference to update the model. It is useful whendealing with complex environments for sample efficiency. In model-free Bayesian RL, value-basedmethods use the reward information to construct the posterior distribution of the action-valuefunction. Besides, policy gradient methods use information of the return to construct theposterior distribution of the policy. They directly apply Bayesian inference to the value function orpolicy without explicitly modeling transition dynamics. Offline Bayesian RL.offline Bayesian RL integrates Bayesian inference with offline RL to tacklethe challenges of learning robust policies from static datasets without further interactions with theenvironment. Many approaches use Bayesian inference to model the transition dynamics orguide action selection for adaptive policy updates, thereby avoiding overly conservative estimates inthe offline setting. Furthermore, recent work applies variational Bayesian inference to learn themodel of transition dynamics, mitigating the distribution shift in offline RL.",
  "Conclusion": "In this paper, we investigate and demonstrate the robustness and effectiveness of introducing Bayesianinference into offline RL to address the challenges posed by data corruptions. By leveraging Bayesiantechniques, our proposed approach TRACER captures the uncertainty caused by diverse corrupteddata. Moreover, the use of entropy-based uncertainty measure in TRACER can distinguish corrupteddata from clean data. Thus, TRACER can regulate the loss associated with corrupted data to reduceits influence, improving performance in clean environments. Our extensive experiments demonstratethe potential of Bayesian methods in developing reliable decision-making. Regarding the limitations of TRACER, although it achieves significant performance improvementunder diverse data corruptions, future work could explore more complex and realistic data corruptionscenarios and related challenges, such as the noise in the preference data for RLHF and adversarialattacks on safety-critical driving decisions. Moreover, we look forward to the continued developmentand optimization of uncertainty-based corrupted-robust offline RL, which could further enhance theeffectiveness of TRACER and similar approaches for increasingly complex real-world scenarios. We would like to thank all the anonymous reviewers for their insightful comments. This workwas supported in part by National Key R&D Program of China under contract 2022ZD0119801,National Nature Science Foundations of China grants U23A20388 and 62021001, and DiDi GAIACollaborative Research Funds. Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without explo-ration. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th InternationalConference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages20522062. PMLR, 2019.",
  "Rafael Figueiredo Prudencio, Marcos R. O. A. Mximo, and Esther Luna Colombini. A survey on offlinereinforcement learning: Taxonomy, review, and open problems. CoRR, abs/2203.01387, 2022": "Shengpu Tang and Jenna Wiens. Model selection for offline reinforcement learning: Practical considerationsfor healthcare settings. In Ken Jung, Serena Yeung, Mark P. Sendak, Michael W. Sjoding, and RajeshRanganath, editors, Proceedings of the Machine Learning for Healthcare Conference, volume 149 ofProceedings of Machine Learning Research, pages 235. PMLR, 2021. Christopher Diehl, Timo Sievernich, Martin Krger, Frank Hoffmann, and Torsten Bertram. Uncertainty-aware model-based offline reinforcement learning for automated driving. IEEE Robotics Autom. Lett.,8(2):11671174, 2023. Tony Z. Zhao, Jianlan Luo, Oleg Sushkov, Rugile Pevceviciute, Nicolas Heess, Jon Scholz, Stefan Schaal,and Sergey Levine. Offline meta-reinforcement learning for industrial insertion. In 2022 InternationalConference on Robotics and Automation, pages 63866393. IEEE, 2022. Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy q-learningvia bootstrapping error reduction. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, FlorencedAlch-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information ProcessingSystems 32, pages 1176111771, 2019. Jiafei Lyu, Xiaoteng Ma, Xiu Li, and Zongqing Lu. Mildly conservative q-learning for offline reinforcementlearning. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors,Advances in Neural Information Processing Systems 35, 2022. Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y. Zou, Sergey Levine, Chelsea Finn,and Tengyu Ma. MOPO: model-based offline policy optimization. In Hugo Larochelle, MarcAurelioRanzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural InformationProcessing Systems 33, 2020. Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel: Model-basedoffline reinforcement learning. In Hugo Larochelle, MarcAurelio Ranzato, Raia Hadsell, Maria-FlorinaBalcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33, 2020. Porter Jenkins, Hua Wei, J. Stockton Jenkins, and Zhenhui Li. Bayesian model-based offline reinforcementlearning for product allocation. In Thirty-Sixth AAAI Conference on Artificial Intelligence, pages 1253112537. AAAI Press, 2022. Sen Lin, Jialin Wan, Tengyu Xu, Yingbin Liang, and Junshan Zhang.Model-based offline meta-reinforcement learning with regularization. In The Tenth International Conference on Learning Rep-resentations. OpenReview.net, 2022. Gaon An, Seungyong Moon, Jang-Hyun Kim, and Hyun Oh Song. Uncertainty-based offline reinforcementlearning with diversified q-ensemble. In MarcAurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin,Percy Liang, and Jennifer Wortman Vaughan, editors, Advances in Neural Information Processing Systems34, pages 74367447, 2021. Yue Wu, Shuangfei Zhai, Nitish Srivastava, Joshua M. Susskind, Jian Zhang, Ruslan Salakhutdinov, andHanlin Goh. Uncertainty weighted actor-critic for offline reinforcement learning. In Marina Meila andTong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139of Proceedings of Machine Learning Research, pages 1131911328. PMLR, 2021. Chenjia Bai, Lingxiao Wang, Zhuoran Yang, Zhi-Hong Deng, Animesh Garg, Peng Liu, and Zhaoran Wang.Pessimistic bootstrapping for uncertainty-driven offline reinforcement learning. In The Tenth InternationalConference on Learning Representations. OpenReview.net, 2022. Filippo Valdettaro and A. Aldo Faisal. Towards offline reinforcement learning with pessimistic value priors.In Fabio Cuzzolin and Maryam Sultana, editors, Epistemic Uncertainty in Artificial Intelligence - FirstInternational Workshop, volume 14523 of Lecture Notes in Computer Science, pages 89100. Springer,2024. Xuezhou Zhang, Yiding Chen, Xiaojin Zhu, and Wen Sun. Corruption-robust offline reinforcement learning.In Gustau Camps-Valls, Francisco J. R. Ruiz, and Isabel Valera, editors, International Conference onArtificial Intelligence and Statistics, volume 151 of Proceedings of Machine Learning Research, pages57575773. PMLR, 2022. Chenlu Ye, Rui Yang, Quanquan Gu, and Tong Zhang. Corruption-robust offline reinforcement learningwith general function approximation. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko,Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36, 2023. Rui Yang, Han Zhong, Jiawei Xu, Amy Zhang, Chongjie Zhang, Lei Han, and Tong Zhang. Towards robustoffline reinforcement learning under diverse data corruption. In The Eleventh International Conference onLearning Representations, 2023. Kishan Panaganti, Zaiyan Xu, Dileep Kalathil, and Mohammad Ghavamzadeh. Robust reinforcementlearning using offline data. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, andA. Oh, editors, Advances in Neural Information Processing Systems 35, 2022. Rui Yang, Chenjia Bai, Xiaoteng Ma, Zhaoran Wang, Chongjie Zhang, and Lei Han. RORL: robust offlinereinforcement learning via conservative smoothing. In Sanmi Koyejo, S. Mohamed, A. Agarwal, DanielleBelgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35, 2022. Jose H. Blanchet, Miao Lu, Tong Zhang, and Han Zhong. Double pessimism is provably efficient fordistributionally robust offline reinforcement learning: Generic algorithm and robust partial coverage. InAlice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors,Advances in Neural Information Processing Systems 36, 2023. Yuzhe Ma, Xuezhou Zhang, Wen Sun, and Jerry Zhu. Policy poisoning in batch reinforcement learning andcontrol. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence dAlch-Buc, Emily B. Fox,and Roman Garnett, editors, Advances in Neural Information Processing Systems 32, pages 1454314553,2019. Fan Wu, Linyi Li, Huan Zhang, Bhavya Kailkhura, Krishnaram Kenthapadi, Ding Zhao, and Bo Li.COPA: certifying robust policies for offline reinforcement learning against poisoning attacks. In The TenthInternational Conference on Learning Representations. OpenReview.net, 2022. Zhihe Yang and Yunjian Xu. Dmbp: Diffusion model based predictor for robust offline reinforcementlearning against state observation perturbations. In The Twelfth International Conference on LearningRepresentations, 2023.",
  "Box George EP and Tiao George C. Bayesian inference in statistical analysis. John Wiley & Sons, 2011": "Esther Derman, Daniel J. Mankowitz, Timothy A. Mann, and Shie Mannor. A bayesian approach to robustreinforcement learning. In Amir Globerson and Ricardo Silva, editors, Proceedings of the Thirty-FifthConference on Uncertainty in Artificial Intelligence, volume 115 of Proceedings of Machine LearningResearch, pages 648658. AUAI Press, 2019. Matthew Fellows, Anuj Mahajan, Tim G. J. Rudner, and Shimon Whiteson. VIREL: A variational inferenceframework for reinforcement learning. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer,Florence dAlch-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural InformationProcessing Systems 32, pages 71207134, 2019. Mattie Fellows, Kristian Hartikainen, and Shimon Whiteson. Bayesian bellman operators. In MarcAurelioRanzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors,Advances in Neural Information Processing Systems 34, pages 1364113656, 2021. Brendan ODonoghue. Variational bayesian reinforcement learning with regret bounds. In MarcAurelioRanzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors,Advances in Neural Information Processing Systems 34, pages 2820828221, 2021. Ron Dorfman, Idan Shenfeld, and Aviv Tamar. Offline meta reinforcement learning - identifiabilitychallenges and effective data collection strategies. In MarcAurelio Ranzato, Alina Beygelzimer, Yann N.Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, Advances in Neural Information ProcessingSystems 34, pages 46074618, 2021. Dibya Ghosh, Anurag Ajay, Pulkit Agrawal, and Sergey Levine. Offline RL policies should be trained tobe adaptive. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvri, Gang Niu, and SivanSabato, editors, International Conference on Machine Learning, volume 162 of Proceedings of MachineLearning Research, pages 75137530. PMLR, 2022. Yuhao Wang and Enlu Zhou. Bayesian risk-averse q-learning with streaming observations. In Alice Oh,Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances inNeural Information Processing Systems 36, 2023.",
  "Jeremias Knoblauch, Jack Jewson, and Theodoros Damoulas. Generalized variational inference. CoRR,abs/1904.02063, 2019": "Will Dabney, Mark Rowland, Marc G. Bellemare, and Rmi Munos. Distributional reinforcement learningwith quantile regression. In Sheila A. McIlraith and Kilian Q. Weinberger, editors, Proceedings of theThirty-Second AAAI Conference on Artificial Intelligence, pages 28922901. AAAI Press, 2018. Will Dabney, Georg Ostrovski, David Silver, and Rmi Munos. Implicit quantile networks for distribu-tional reinforcement learning. In Jennifer G. Dy and Andreas Krause, editors, Proceedings of the 35thInternational Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research,pages 11041113. PMLR, 2018.",
  "Michele Caprio, Souradeep Dutta, Kuk Jin Jang, Vivian Lin, Radoslav Ivanov, Oleg Sokolsky, and InsupLee. Credal bayesian deep learning. arXiv e-prints, pages arXiv2302, 2023": "Michele Caprio, Yusuf Sale, Eyke Hllermeier, and Insup Lee. A novel bayes theorem for upper probabili-ties. In Fabio Cuzzolin and Maryam Sultana, editors, Epistemic Uncertainty in Artificial Intelligence - FirstInternational Workshop, volume 14523 of Lecture Notes in Computer Science, pages 112. Springer, 2024. Souradeep Dutta, Michele Caprio, Vivian Lin, Matthew Cleaveland, Kuk Jin Jang, Ivan Ruchkin, OlegSokolsky, and Insup Lee. Distributionally robust statistical verification with imprecise neural networks.CoRR, abs/2308.14815, 2023.",
  "Sheri Edwards. Thomas m. cover and joy a. thomas, elements of information theory (2nd ed.), john wiley& sons, inc. (2006). Inf. Process. Manag., 44(1):400401, 2008": "Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towardsdeep learning models resistant to adversarial attacks. In 6th International Conference on LearningRepresentations, 2018. Huan Zhang, Hongge Chen, Chaowei Xiao, Bo Li, Mingyan Liu, Duane S. Boning, and Cho-Jui Hsieh.Robust deep reinforcement learning against adversarial perturbations on state observations. In HugoLarochelle, MarcAurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors,Advances in Neural Information Processing Systems 33, 2020.",
  "Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4RL: datasets for deepdata-driven reinforcement learning. CoRR, 2020": "Gaon An, Seungyong Moon, Jang-Hyun Kim, and Hyun Oh Song. Uncertainty-based offline reinforcementlearning with diversified q-ensemble. In MarcAurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin,Percy Liang, and Jennifer Wortman Vaughan, editors, Advances in Neural Information Processing Systems34, 2021. Seyed Kamyar Seyed Ghasemipour, Shixiang Shane Gu, and Ofir Nachum. Why so pessimistic? estimatinguncertainties for offline RL through ensembles, and why their independence matters. In Sanmi Koyejo,S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, Advances in Neural InformationProcessing Systems 35, 2022. Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In 2012IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS 2012, Vilamoura, Algarve,Portugal, October 7-12, 2012, pages 50265033. IEEE, 2012. Alexey Dosovitskiy, Germn Ros, Felipe Codevilla, Antonio M. Lpez, and Vladlen Koltun. CARLA: anopen urban driving simulator. In 1st Annual Conference on Robot Learning, volume 78 of Proceedings ofMachine Learning Research, pages 116. PMLR, 2017. Chi Jin, Tiancheng Jin, Haipeng Luo, Suvrit Sra, and Tiancheng Yu. Learning adversarial markovdecision processes with bandit feedback and unknown transition. In Proceedings of the 37th InternationalConference on Machine Learning, 2020. Thodoris Lykouris, Vahab S. Mirrokni, and Renato Paes Leme. Stochastic bandits robust to adversarialcorruptions. In Ilias Diakonikolas, David Kempe, and Monika Henzinger, editors, Proceedings of the 50thAnnual ACM SIGACT Symposium on Theory of Computing, 2018. Aviv Rosenberg and Yishay Mansour. Online convex optimization in adversarial markov decision pro-cesses. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th InternationalConference on Machine Learning, 2019.",
  "Anupam Gupta, Tomer Koren, and Kunal Talwar. Better algorithms for stochastic bandits with adversarialcorruptions. In Alina Beygelzimer and Daniel Hsu, editors, Conference on Learning Theory, 2019": "Chenlu Ye, Wei Xiong, Quanquan Gu, and Tong Zhang. Corruption-robust algorithms with uncertaintyweighting for nonlinear contextual bandits and markov decision processes. In Andreas Krause, EmmaBrunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, InternationalConference on Machine Learning, 2023. Thodoris Lykouris, Max Simchowitz, Alex Slivkins, and Wen Sun. Corruption-robust exploration inepisodic reinforcement learning. In Mikhail Belkin and Samory Kpotufe, editors, Conference on LearningTheory, 2021.",
  "Matthieu Geist and Olivier Pietquin. Kalman temporal differences. J. Artif. Intell. Res., 39:483532, 2010": "Ian Osband, Daniel Russo, and Benjamin Van Roy. (more) efficient reinforcement learning via posteriorsampling. In Christopher J. C. Burges, Lon Bottou, Zoubin Ghahramani, and Kilian Q. Weinberger,editors, Advances in Neural Information Processing Systems 26, pages 30033011, 2013. Marc Peter Deisenroth and Carl Edward Rasmussen. PILCO: A model-based and data-efficient approachto policy search. In Lise Getoor and Tobias Scheffer, editors, Proceedings of the 28th InternationalConference on Machine Learning, 2011. Richard Dearden, Nir Friedman, and Stuart Russell. Bayesian q-learning. In Jack Mostow and Chuck Rich,editors, Proceedings of the Fifteenth National Conference on Artificial Intelligence and Tenth InnovativeApplications of Artificial Intelligence Conference, 1998.",
  "Gal Yarin, McAllister Rowan, and Rasmussen Carl Edward. Improving pilco with bayesian neural networkdynamics models. In Data-efficient machine learning workshop, ICML, volume 4, page 25, 2016": "Zhihai Wang, Jie Wang, Qi Zhou, Bin Li, and Houqiang Li. Sample-efficient reinforcement learning viaconservative model-based actor-critic. In Thirty-Sixth AAAI Conference on Artificial Intelligence, pages86128620. AAAI Press, 2022. Yaakov Engel, Shie Mannor, and Ron Meir. Reinforcement learning with gaussian processes. In Luc DeRaedt and Stefan Wrobel, editors, Machine Learning, Proceedings of the Twenty-Second InternationalConference (ICML 2005), 2005.",
  "Vallender SS. Calculation of the wasserstein distance between probability distributions on the line. Theoryof Probability & Its Applications, 18:784786, 1974": "Anqi Li, Dipendra Misra, Andrey Kolobov, and Ching-An Cheng. Survival instinct in offline reinforcementlearning. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine,editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural InformationProcessing Systems 2023, 2023. Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter Abbeel,Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling.In Advances in Neural Information Processing Systems 34, pages 1508415097, 2021. Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximumentropy deep reinforcement learning with a stochastic actor. In International conference on machinelearning, pages 18611870. PMLR, 2018. Arsenii Kuznetsov, Pavel Shvechikov, Alexander Grishin, and Dmitry Vetrov. Controlling overestimationbias with truncated mixture of continuous distributional quantile critics. In International Conference onMachine Learning, pages 55565566. PMLR, 2020. Zhihai Wang, Taoxing Pan, Qi Zhou, and Jie Wang. Efficient exploration in resource-restricted rein-forcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages1027910287, 2023.",
  "Chao Yu, Jiming Liu, Shamim Nemati, and Guosheng Yin. Reinforcement learning in healthcare: A survey.ACM Computing Surveys (CSUR), 55(1):136, 2021": "Zhihai Wang, Jie Wang, Dongsheng Zuo, Yunjie Ji, Xinli Xia, Yuzhe Ma, Jianye Hao, Mingxuan Yuan,Yongdong Zhang, and Feng Wu. A hierarchical adaptive multi-task reinforcement learning framework formultiplier circuit design. In Forty-first International Conference on Machine Learning. PMLR, 2024. Zhihai Wang, Xijun Li, Jie Wang, Yufei Kuang, Mingxuan Yuan, Jia Zeng, Yongdong Zhang, and FengWu. Learning cut selection for mixed-integer linear programming via hierarchical sequence model. In TheEleventh International Conference on Learning Representations, 2023. Jie Wang, Zhihai Wang, Xijun Li, Yufei Kuang, Zhihao Shi, Fangzhou Zhu, Mingxuan Yuan, Jia Zeng,Yongdong Zhang, and Feng Wu. Learning to cut via hierarchical sequence/set model for efficient mixed-integer programming. IEEE Transactions on Pattern Analysis and Machine Intelligence, pages 117,2024. Zhihai Wang, Zijie Geng, Zhaojie Tu, Jie Wang, Yuxi Qian, Zhexuan Xu, Ziyan Liu, Siyuan Xu, ZhentaoTang, Shixiong Kai, et al. Benchmarking end-to-end performance of ai-based chip placement algorithms.arXiv preprint arXiv:2407.15026, 2024.",
  "dE(s) } M,(s, a) S A,": "where E(a | s) b(a | s) exp ( [T Q V ] (s, a)) is the policy of clean data, IQL inEquation (17) is the policy following IQLs supervised policy learning scheme under clean data,IQL = arg minEsB [KL (E( | s), ( | s))]is the policy under corrupted data, and E(a | s)",
  "= arg minEsb [DKL (E( | s), ( | s))] .(18)": "As TRACER directly applies the weighted imitation learning technique from IQL to learn thepolicy, we can use IQL as the policy learned by TRACER under data corruptions, akin to RIQL.Assumption A.2 requires that each pair, including the policy E and the clean data b, the policyE and the corrupted dataset B, E and E, E and IQL, and E and IQL, has good coverage. Itis similar to the coverage condition in . Based on Assumptions A.1 and A.2, we can derivethe following theorem to show the robustness of our approach using the supervised policy learningscheme and learning the action-value distribution.",
  "The value distribution q, also denoted by p(|s) in .2 of the main text, is an expactation ofaction value distribution Ea,r[p(|s, a, r)]": "Note that as TRACER directly applies the weighted imitation learning technique from IQL to learnthe policy (see .2), we can use IQL as the policy learned by TRACER under data corruptions,akin to RIQL. Let qIQL and qIQL be the value distributions of learned policies following IQLs supervised policylearning scheme under corrupted and clean data, respectively. Z IQL qIQL and ZIQL qIQL.Let pE be the value distribution of the policy of clean data, pE be the value distribution of thepolicy of corrupted data, ZE pE, and Z E pE. We have",
  "e2).(46)": "Note that in Theorem A.4, the major difference between TRACER and IQL/RIQL is that TRACERuses the action-value and value distributions rather than the action-value and value functions inIQL/RIQL. Therefore, we provide this theorem to prove an upper bound on the difference in valuedistributions of TRACER to show its robustness, which also provides a guarantee of how TRACERsperformance degrades with increased data corruptions.",
  "NotationsDescriptions": "Cumulative corruption level.iMetric quantifying the corruption level of rewards and next states (transition dynamics).iMetric quantifying the corruption level of states and actions.b(|s)The behavior policy that is used to collect clean data.B(|s)The behavior policy that is used to collect corrupted data.E(|s)The policy that we want to learn under clean data.E(|s)The policy that we are learning under corrupted data.IQL(|s)The learned policy using IQLs weighted imitation learning under clean data.IQL(|s)The learned policy using IQLs weighted imitation learning under corrupted data.d(s, a)The probability density function associated with policy at state s and action a.W1(p, q)The Wasserstein-1 distance that measures the difference between distributions p and q.qThe value distribution of the policy .Z(s)The random variable of the value distribution q(|s).1KL divergence between E and IQL, i.e., the standard imitation error under clean data.2KL divergence between E and IQL, i.e., the standard imitation error under corrupted data.nThe midpoint of the probability density p.",
  "B.1Architecture of TRACER": "On the actor-critic architecture based on IQL, our approach TRACER adds just one ensemble model(pa, pr, ps) to reconstruct the data distribution and replace the function approximation in the criticwith a distribution estimation (see ). Based on our experiments, this structure significantlyimproves the ability to handle both simultaneous and individual corruptions.",
  "B.2Implementation Details for TRACER": "TRACER consists of four components: the actor network, the critic network, the value network, andthe observation model. We first implement the actor network for decision-making with a 3-layer MLP,where the dimension of the hidden layers are 256 and the activation functions are ReLUs. Then, weutilize quantile regression to design the critic networks and the value network, approximatingthe distributions of the action-value function D and value function Z, respectively. Specifically, for the action-value distribution, we use a function h : Rd to computean embedding for the sampling point . Then, we can obtain the corresponding approximationby D(s, a) f(g(s) h())a. Here, g : S Rd is the function computed by MLPs andf : Rd R|A| is the subsequent fully-connected layers mapping g(s) to the estimated action-valuesD(s, a) f(g(s))a. For the approximation of the value distribution Z(s), we leverage the samearchitecture. Note that the function h is implemented by a fully-connected layer, and the outputdimension is set to 256. We then use an ensemble model with K networks. Each network is a 3-layerMLP, consisting of 256 units and ReLU activations. By using the ensemble model, we can constructthe critic network, using the quantile regression for optimization. Moreover, the value networkis constructed by a 3-layer MLP. The dimension of the hidden layers are also 256 and activationfunctions are ReLUs. The observation model uses an ensemble model with 3 neural networks. Each network is used toreconstruct different observations in the offline dataset. It consists of two fully connected layers,and the activation functions are ReLUs. We apply a masked matrix during the learning process forupdating the observation model. Thus, each network in the observation model can use different inputdata to compute Equations (7), (8), and (9), respectively. Following the setting of , we list the hyper-parameters of N, K, and for the approxmiation ofaction-value functions, and in the Huber loss in TRACER under random and adversarial corruptionin and , respectively. Here, N is the number of samples , and K is the number ofensemble models. Moreover, we apply Adam optimizer using a learning rate 1 103, = 0.99, target smoothingcoefficient = 0.05, batch size 256 and the update frequency for the target network is 2. Thecorruption rate is c = 0.3 and the corruption scale is = 1.0 for all experiments. For the loss functionof observation models, we use a linear decay parameter from 0.0001 to 0.01 to trade off the lossLfirst and Lsecond. We update each algorithm for 3000 epochs. Each epoch uses 1000 update timesfollowing . Then, we evaluate each algorithm in clean environments for 100 epochs and reportthe average normalized return (calculated by 100 scorerandom score",
  "C.1Details of Data Corruptions": "We follow the corruption settings proposed by , applying either random or adversarial corruptionsto each element of the offline data, namely state, action, reward, and dynamics (or next-state),to simulate potential attacks that may occur during the offline data collection and usage process inreal-world scenarios. We begin with the details of random corruption below. Random observation corruption. We randomly sample c% transitions (s, a, r, s) from theoffline dataset, and for each of these selected states s, we add noise to form s = s+std(s),where Uniformds, c is the corruption rate, is the corruption scale, ds refers tothe dimension of states and std(s) represents the ds-dimensional standard deviation of allstates in the offline dataset. Random action corruption. We randomly sample c% transitions (s, a, r, s) from theoffline dataset, and for each of these selected actions a, we add noise to form a = a + std(a), where Uniformda, da refers to the dimension of actions and std(a)represents the da-dimensional standard deviation of all actions in the offline dataset. Random reward corruption. We randomly sample c% transitions (s, a, r, s) from theoffline dataset, and for each of these selected rewards r, we modify it to r = Uniform. We adopt this harder reward corruption setting since has found that offline RLalgorithms are often insensitive to small perturbations of rewards. Random dynamics corruption. We randomly sample c% transitions (s, a, r, s) fromthe offline dataset, and for each of these selected next-step states s, we add noise to forms = s+std(s), where Uniformds , ds refers to the dimension of next-stepstates and std(s) represents the ds-dimensional standard deviation of all next states in theoffline dataset.",
  "The harder adversarial corruption settings are detailed as follows:": "Adversarial observation corruption. To impose adversarial attack on the offline dataset, wefirst need to pretrain an EDAC agent with a set of Qp functions and a policy function p usingclean dataset. Then, we randomly sample c% transitions (s, a, r, s) from the offline dataset,and for each of these selected states s, we attack it to form s = minsBd(s,)Qp(s, a). Here,Bd(s, ) = {s||s s| std(s)} regularizes the maximum difference for each statedimension. Adversarial action corruption. We randomly sample c% transitions (s, a, r, s) from theoffline dataset, and for each of these selected actions a, we use the pretrained EDAC agentto attack it to form a = minaBd(a,)Qp(s, a). Here, Bd(a, ) = {a||a a| std(a)}regularizes the maximum difference for each action dimension. Adversarial reward corruption. We randomly sample c% transitions (s, a, r, s) fromthe offline dataset, and for each of these selected rewards r, we directly attack it to formr = r without any adversarial model. This is because that the objective of adversarialreward corruption is r = minrB(r,) r + E[Q(s, a)]. Here B(r, ) = {r | |r r| (1+)rmax} regularizes the maximum distance for rewards. Therefore, we have r = r. Adversarial dynamics corruption. We randomly sample c% transitions (s, a, r, s) fromthe offline dataset, and for each of these selected next-step states s, we use the pretrainedEDAC agent to attack it to form s = min sBd(s,)Qp(s, p(s)). Here, Bd(s, ) =",
  "{s||s s| std(s)} regularizes the maximum difference for each dimension of thedynamics": "The optimization of the above adversarial corruptions are implemented through Projected GradientDescent . Taking adversarial observation corruption for example, we first initialize a learnablevector z ds, and then conduct a 100-step gradient descent with a step size of 0.01 fors = s + z std(s), and clip each dimension of z within the range after each update. For actionand dynamics corruption, we conduct similar operation. Building upon the corruption settings for individual elements as previously discussed, we furtherintensify the corruption to simulate the challenging conditions that might be encountered in real-worldscenarios. We present the details of the simultaneous corruption below: Random simultaneous corruptions. We sequentially conduct random observation corrup-tion, random action corruption, random reward corruption, and random dynamics corruptionto the offline dataset in order. That is to say, we randomly select c% of the transitions eachtime and corrupt one element among them, repeating four times until states, actions, rewards,and dynamics of the offline dataset are all corrupted. Adversarial simultaneous corruptions. We sequentially conduct adversarial observationcorruption, adversarial action corruption, adversarial reward corruption, and adversarialdynamics corruption to the offline dataset in order. That is to say, we randomly select c%of the transitions each time and attack one element among them, repeating four times untilstates, actions, rewards, and dynamics of the offline dataset are all corrupted.",
  "C.2Details for CARLA": "We conduct experiments in CARLA from D4RL benchmark. We use the clean environment CARLA-Lane-v0 to evaluate IQL, RIQL, and TRACER (Ours). We report each mean result with the standarddeviation in the left of over four random seeds for 3000 epochs. We apply the randomsimultaneous data corruptions, where each element in the offline dataset (including states, actions,rewards, and next states) may involve random noise. We follow the setting in .2, using thecorruption rate c = 0.3 and scale = 1.0 in the CARLA task. The results in the left of showthe superiority of TRACER in the random simultaneous corruptions. We provide the hyperparametersof TRACER in .",
  "C.3.1Results Comparison between TRACER and RIQL under Individual Corruptions": "In Tables 2 and 3 of the main text, we adhered to commonly used settings for individual corruptionsin corruption-robust offline RL. We directly followed hyperparameters from RIQL (i.e., for huberloss, the ensemble number K, and in action-value functions, see Tables 5 and 6). Results showthat TRACER outperforms RIQL in 18 out of 24 settings, demonstrating its robustness even whenaligned with RIQLs hyperparameters.",
  "We further conduct experiments on two AntMaze datasets and two additional Mujoco datasets,presenting results under random simultaneous corruptions in": "Each result represents the mean and standard error over four random seeds and 100 episodes in cleanenvironments. For each experiment, the methods train agents using batch sizes of 64 for 3000 epochs.Building upon RIQL, we apply the experiment settings as follows. (1) For the two Mujoco datasets,we use a corruption rate of c = 0.3 and scale of = 1.0. Note that simultaneous corruptions withc = 0.3 implies that approximately 76.0% of the data is corrupted. (2) For the two AntMaze datasets,we use the corruption rate of 0.2, corruption scales for observation (0.3), action (1.0), reward (30.0),and dynamics (0.3).",
  "C.4.1Evaluation for Robustness of TRACER across Different Scales of Corrupted Data": "Theorem A.4 shows that the higher the scale of corrupted data, the greater the difference in action-value distributions and the lower the TRACERs performance. Thus, we further conduct experimentsto evaluate TRACER across various corruption levels. Specifically, we apply different corruption ratec in all four elements of the offline dataset. We randomly select c% of transitions from the datasetand corrupt one element in each selected transition. Then, we repeat this step four times until allelements are corrupted. Therefore, approximately 100 (1 (1 c)4)% of data in the offline datasetis corrupted. In , we evaluate TRACER using different c%, including 10%, 20%, 30%, 40%, and 50%.These rates correspond to approximately 34.4%, 59.0%, 76.0%, 87.0%, and 93.8% of the data beingcorrupted. The results in demonstrate that while TRACER is robust to simultaneouscorruptions, its scores depend on the extent of corrupted data it encounters.",
  ": Results and standard errors underrandom simultaneous corruptions": "In , we additionally report the entropy valuesof TRACER with and without using entropy-baseduncertainty measure, corresponding to . Thecurves illustrate that TRACER using entropy-baseduncertainty measure can effectively regulate the lossassociated with corrupted data, reducing the influ-ence of corrupted samples. Therefore, the estimatedentropy values of corrupted data can be higher thanthose of clean data.",
  "EMore Related Work": "Online RL. In general, standard online RL fall into two categories: model-free RL andmodel-based RL . In recent years, RL has achieved great success in many important real-world decision-making tasks . However, the online RL methods still typically rely on activedata collection to succeed, hindering their application in scenarios where real-time data collection isexpensive, risky, and/or impractical. Thus, we focus on the offline RL paradigm in this paper.",
  "GLimitations and Negative Societal Impacts": "TRACERs performance is related to the extent of corrupted data within the offline dataset. AlthoughTRACER consistently outperforms RIQL even under conditions of extensive corrupted data (see), its performance does degrade as the corruption rate (i.e., the extent/scale of corrupted data)increases. To tackle this problem, we look forward to the continued development and optimization ofcorruption-robust offline RL with large language models, introducing the prior knowledge of cleandata against large-scale or near-total corrupted data. Thus, this corruption-robust offline RL canperform well even when faced with large-scale or near-total corrupted data in the offline dataset. This paper proposes a novel approach called TRACER to advance the field of robustness in offline RLfor diverse data corruptions, enhancing the potential of agents in real-world applications. Althoughour primary focus is on technical innovation, we recognize the potential societal consequences of ourwork, as robustness in offline RL for data corruptions has the potential to influence various domainssuch as autonomous driving and the large language model field. We are committed to ethical researchpractices and attach great importance to the social implications and ethical considerations in thedevelopment of robustness research in offline RL."
}