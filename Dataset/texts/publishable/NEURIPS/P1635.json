{
  "Benjamin M. MarlinUniversity of Massachusetts Amherst,": "The use of reinforcement learning (RL) to learn policies for just-in-time adaptive interventions(JITAIs) is of significant interest in many behavioral intervention domains including improving levelsof physical activity. In a messaging-based physical activity JITAI, a mobile health app is typicallyused to send messages to a participant to encourage engagement in physical activity. In this setting,RL methods can be used to learn what intervention options to provide to a participant in differentcontexts. However, deploying RL methods in real physical activity adaptive interventions comes withchallenges: the cost and time constraints of real intervention studies result in limited data to learnadaptive intervention policies. Further, commonly used RL simulation environments have dynamicsthat are of limited relevance to physical activity adaptive interventions and thus shed little light onwhat RL methods may be optimal for this challenging application domain. In this paper, we introduceStepCountJITAI, an RL environment designed to foster research on RL methods that address thesignificant challenges of policy learning for adaptive behavioral interventions.",
  "Introduction": "Reinforcement learning (RL) is increasingly being considered for the development of just-in-timeadaptive interventions (JITAIs) that aim to increase physical activity [Coronato et al., 2020, Yu et al.,2021, Gnl et al., 2021, Liao et al., 2022]. In a physical activity adaptive intervention, participantstypically use a wearable device (e.g., Fitbit) to log aspects of physical activity such as step counts[Nahum-Shani et al., 2018]. In an adaptive messaging-based intervention, a mobile health app isused to send messages to each participant to encourage increased physical activity. In an adaptiveintervention, the selection of which messages to send at what times is personalized using context (ortailoring) variables. Context variables can include external factors such as time of day and location, aswell as behavioral variables such as whether the participant is experiencing significant stress. Somecontext variables can be inferred from wearable sensor or other real time data, while others may beprovided by participants via self-report mechanisms. In this setting, RL methods can be used to learn what intervention options to provide to a participantin different contexts with the goal of maximizing a measure of cumulative physical activity, such astotal step count over the intervention duration. The state variables used by an RL method correspondto the context variables (observed, inferred, or self-reported) relevant to selecting intervention options.The immediate reward is typically taken to be the step count in a window of time following anintervention decision point. However, deploying RL methods in real physical activity adaptive interventions comes with chal-lenges: the cost and time constraints of real intervention studies result in limited data to learn adaptiveintervention policies. Real behavioral studies are difficult to conduct because they involve followingmany participants and can run for weeks or months while only allowing a handful of interactionswith the participant per day [Hardeman et al., 2019]. This problem is particularly acute given theneed to personalize intervention policies to individual participants. The general problem of data scarcity in real adaptive intervention trials means that RL methods thatrequire a large number of episodes to achieve high performance [Sutton and Barto, 1998, Mnih et al.,2013, Coronato et al., 2020] cannot typically be used in real adaptive intervention studies. Thus, there",
  "arXiv:2411.00336v1 [cs.LG] 1 Nov 2024": "is a need to create new simulation environments that reflect the specific challenges of the adaptiveintervention domain to support the exploration of RL methods that are better tailored to meet thesechallenges. Indeed, commonly used benchmark simulation environments have dynamics that are notparticularly relevant to the adaptive intervention domain where there can be significant variation indynamics within individuals over time as well as between individuals. We leverage insights from the behavioral domain to construct the proposed simulation environment.In real behavioral studies, there can be missingness and uncertainty among the context variables.For example, participants may not supply requested self-reports or use study devices as expected.Modeling the context uncertainty can provide useful information for decision-making: if the contextuncertainty is too high, then a better RL policy might be to send a non-contextualized message,instead of sending an incorrectly contextualized messages that may cause a participant to lose trust inthe intervention system or attend less to messages in the future. The proposed simulation environment reflects these considerations via two primary behavioralvariables: habituation level, which measures how much the participant becomes accustomed toreceiving messages, and disengagement risk, which measures how likely the participant is to abandonthe study. The simulation dynamics relate message contextualization accuracy and habituation level toimmediate reward in terms of step count while excess disengagement risk results in early terminationof the simulation. The simulation environment also includes stochasticity to represent between- andwithin-participant variability. In this work, we extend the simulation environment introduced in[Karine et al., 2023] to create a stochastic version of this simulator and introduce new parameters tocontrol the level of stochasticity and between person variability.",
  "Our contributions are:": "1. We introduce StepCountJITAI, a messaging-based physical activity JITAI simulation environmentthat models stochastic behavioral dynamics and context uncertainty, with parameters to controlstochasticity. StepCountJITAI can help to accelerate research on RL algorithms for data scarceadaptive intervention optimization by offering a challenging new simulation environment basedon key aspects of behavioral dynamics. 2. We provide an open source implementation of StepCountJITAI using a standard API for RL(i.e., gymnasium) to maximize compatibility with existing RL research workflows. We providequickstart code samples in Appendix E.2 and detail the StepCountJITAI interface in AppendixE.1. StepCountJITAI is available here:",
  "Methods": "We provide an overview of the use of StepCountJITAI in an RL loop in , and providedetails below. We first describe the specifications, then introduce our new method. We use the samespecifications and deterministic dynamics as in the base simulator [Karine et al., 2023]. For thenotation, we use the following: short variable names are in upper case, and variable values are inlower case with subscript t indicating the time index, for example: we use C for the true contextvariable name, and ct for the true context value at time t.",
  "True Context (C). We include an abstract binary context ct {0, 1}. This context can represent abinary state such as stressed/not stressed, fatigued/not fatigued, or home/not home": "Context probability (P ). This variable represents an inferred probability that the true contexttakes value 1, where pt . It models the fact that in real-world studies, we typically donot have access to the true context, but can make inferences about the context using probabilisticmachine learning models. Most likely context (L). The most likely context lt {0, 1} is defined as the context value withthe highest inferred probability according to pt. It can be used to model situations where the contextuncertainty is discarded when learning intervention policies.",
  "(b) Example of average returns using StepCountJITAI with RL methods:DQN, REINFORCE, PPO and TS": ": Overview of StepCountJITAI in an RL loop. StepCountJITAI is a simulation environment for physicalactivity adaptive interventions. StepCountJITAI models stochastic behavioral dynamics and context uncertainty,with parameters to control stochasticity. Step count is used as the reward. The actions correspond to physicalactivity motivational messages with different contextualization levels. The messages can be non-contextualized,or customized to a binary context. The behavioral variables are: habitation and disengagement risk. Habituation level (H). As the participant receives more messages, the participant becomes moreaccustomed to the messages, thus the habituation level ht will increase, with ht . Anincrease in ht also reduces the step count st because the messages become less effective. Disengagement risk (D). If the participant keeps receiving messages that are not useful (e.g., thecontext for the customized messages do not match the true context), then the disengagement risk dtwill increase, with dt . If dt exceeds a preset threshold Dthreshold the episode ends and allfuture rewards are 0.",
  "StepCountJITAI dynamics": "The environment dynamics depend on four actions: a = 0 indicates no message is sent. a = 1indicates a non-contextualized message is sent. a = 2 indicates a message customized to context 0 issent. a = 3 indicates a message customized to context 1 is sent to the participant. The environment dynamics can be summarized as follows: Sending a message causes the habituationlevel to increase. Not sending a message causes the habituation level to decrease. An incorrectlytailored message causes the disengagement risk to increase. A correctly tailored message causesthe disengagement risk to decrease. When the disengagement risk exceeds the given threshold, theepisode ends. The reward is the surplus step count, beyond a baseline count, attenuated by thehabituation level. The base simulator implements deterministic dynamics, which we summarize in Appendix C. In thiswork, we extend the base simulator to create a simulation environment with additional stochasticityby introducing noise into the existing deterministic dynamics. We let ht be the habituation level,dt be the disengagement risk level, st be the step count at time t. The dynamics of habituation anddisengagement are governed by increment and decay parameters including the habituation decay h,the habituation increment h, the disengagement risk decay d, and the disengagement risk incrementd. In the base simulator the dynamics parameters h, h, d, and d are fixed. We make themstochastic at the episode level to model between person variation in the dynamics of habituation anddisengagement risk. We also make the state variables ht, dt and st stochastic. We construct two different versions of the stochastic dynamics based on the uniform and betadistributions. The uniform uncertainty-based dynamics are summarized below where the a parameterscontrol the width of a uniform distribution about the mean values. The step counts themselves arepositive reals and are sampled from a Gamma distribution parameterized by its mean and standarddeviation s. The alternative beta distribution-based stochastic dynamics sample the values in",
  "st+1": "where ct is the true context, xt is the context feature, is the context uncertainty, pt is the probabilityof context 1, lt is the inferred context, ht is the habituation level, dt is the disengagement risk, st isthe step count (st is the participants number of walking steps), and at is the action value at time t., 1, 2, ms are fixed parameters. The default parameters are provided in Appendix C. The spreadsof the distributions are controlled by the parameters ade, ahd and s. We describe where the environmental dynamics occur in a typical RL loop: at each time t, the agentobserves the current state (e.g., [ct, ht, dt]), and selects an action at based on the observed statevariables. Then, the environment responds by transitioning to a new state (e.g., [ct+1, ht+1, dt+1]),and providing a reward (e.g., the participant step count st+1).",
  "Experiments": "We perform RL experiments using StepCountJITAI including learning action selection policies withvarious RL methods: REINFORCE and PPO as examples of policy gradient methods, and DQNas an example of a value function method [Williams, 1987, Schulman et al., 2017, Mnih et al.,2013]. We also consider a standard Thompson sampling (TS) [Thompson, 1933]. We provide the RLimplementation settings in Appendix F.4, and code samples in Appendix E.2.3. In (b), weshow the mean and standard deviation of the average return over 10 trials, with 1500 episodes pertrial, when using StepCountJITAI, with observed data [C, H, D], and using the stochastic parametersfor Uniform distributions: ahd = 0.2, ade = 0.5, s = 20, and context uncertainty = 2. In thissetting, we show that the RL and TS agents are able to learn, with a maximum average return ofaround 3000 for RL and 1500 for TS. As expected, TS shows a lower average return than RL whenusing a complex environment such as StepCountJITAI. We show additional results including generating traces in Appendix F.3 and stochastic variableshistograms in Appendix F.2. We perform additional RL experiments using various parameter settingsto control stochasticity in Appendix F.5.",
  "Conclusion": "We introduce StepCountJITAI, a simulation environment for physical activity adaptive interventions.StepCountJITAI is implemented using a standard RL API to maximize compatibility with existingRL research workflows. StepCountJITAI models key aspects of behavioral dynamics includinghabituation and disengagement risk, as well as context uncertainty and between person variability indynamics. We hope that StepCountJITAI will help to accelerate research on new RL algorithms forthe challenging problem of data scarce adaptive intervention optimization.",
  "Acknowledgements": "This work was supported by National Institutes of Health National Cancer Institute, Office of Behaviorand Social Sciences, and National Institute of Biomedical Imaging and Bioengineering through grantsU01CA229445 and 1P41EB028242. Antonio Coronato, Muddasar Naeem, Giuseppe De Pietro, and Giovanni Paragliola. Reinforcementlearning for intelligent healthcare applications: A survey. Artificial Intelligence in Medicine, 109:101964, 2020. Suat Gnl, Tuncay Naml, Ahmet Cosar, and Ismail Hakk Toroslu. A reinforcement learning basedalgorithm for personalization of digital, just-in-time, adaptive interventions. Artificial Intelligencein Medicine, 115:102062, 2021. Wendy Hardeman, Julie Houghton, Kathleen Lane, Andy Jones, and Felix Naughton. A systematicreview of just-in-time adaptive interventions (jitais) to promote physical activity. InternationalJournal of Behavioral Nutrition and Physical Activity, 16(1):121, 2019. Karine Karine, Predrag Klasnja, Susan A. Murphy, and Benjamin M. Marlin. Assessing the impactof context inference error and partial observability on rl methods for just-in-time adaptive inter-ventions. In Proceedings of the Thirty-Ninth Conference on Uncertainty in Artificial Intelligence,volume 216, pages 10471057, 2023.",
  "Peng Liao, Zhengling Qi, Runzhe Wan, Predrag Klasnja, and Susan A. Murphy. Batch policy learningin average reward Markov decision processes. The Annals of Statistics, 50(6):3364 3387, 2022": "Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, DaanWierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. In NeurIPS DeepLearning Workshop, 2013. Inbal Nahum-Shani, Shawna N Smith, Bonnie J Spring, Linda M Collins, Katie Witkiewitz, AmbujTewari, and Susan A Murphy. Just-in-time adaptive interventions (jitais) in mobile health: keycomponents and design principles for ongoing health behavior support. Annals of BehavioralMedicine, 52(6):446462, 2018.",
  "ParameterDescription": "Context uncertainty. The default value is = 0.4dDisengagement risk decay. The default value is d = 0.1.hHabituation decay. The default value is h = 0.1.dDisengagement risk increment. The default value is d = 0.4.hHabituation increment. The default value is h = 0.05.1at = 1 base step count. The default value is 1 = 50.2at = ct + 2 base step count. The default value 2 = 200.msBase step count. The default value is ms = 0.1.",
  "ahdParameter to control the spread of the Uniform distributions for ht and dt.adeParameter to control the spread of the Uniform distributions for d, d, h and h": "dParameter to control the spread of the Beta distribution for dt.hParameter to control the spread of the Beta distribution for ht.dParameter to control the spread of the Beta distribution for d.hParameter to control the spread of the Beta distribution for h.dParameter to control the spread of the Beta distribution for d.hParameter to control the spread of the Beta distribution for h.",
  "ms + (1 ht+1) 1if at = 1ms + (1 ht+1) 2if at = ct + 2msotherwise": "where ct is the true context, xt is the context feature, is the context uncertainty, pt is the probabilityof context 1, lt is the inferred context, ht is the habituation level, dt is the disengagement risk, st isthe step count (st is the participants number of walking steps), at is the action value at time t.",
  "The behavioral dynamics can be tuned using the parameters: disengagement risk decay d, disen-gagement risk increment d, habituation decay h, and habituation increment h": "The default parameters values for the base simulator are: = 0.4, h = 0.1, h = 0.05, d = 0.1,d = 0.4, 1 = 50, 2 = 200, ms = 0.1, disengagement threshold Dthreshold = 0.99 (the studyends if dt exceeds Dthreshold). The maximum study length is 50 days with one intervention per day,thus the maximum episode length is 50 days.",
  "The context uncertainty is a parameter that was introduced in the base simulator [Karine et al.,2023]. The user can use to control the desired context error": "We perform some experiments to show the relationship between the context uncertainty and thecontext error. In our experiment, we generate the true context ct and the inferred context lt, using thedeterministic dynamics equations in Section C, for various fixed values of . Then we compute thecontext error (percentage of true context values that match the inferred context values), for N = 5000.",
  "F.3Experiments: Generating traces using StepCountJITAI with fixed actions or randomactions": "We provide examples of traces using deterministic StepCountJITAI, stochastic StepCountJITAI withUniform distributions, and stochastic StepCountJITAI with Beta distributions, when using one of thefollowing policies. We implement two policies: policy always a = 3 where at each time t, theselected action is fixed, with value a = 3, and policy random action where at each time t, theselected action has a random value a .",
  "We provide the code sample for policy random action in Section E.2.2. The code sample for policyalways a = 3 is the same except that the action is fixed to the value 3": "In our experiments, for each version of StepCountJITAI, we generate observed data [C, P, L, H, D]in a loop, using one of the two policies described above, for 30 time steps. We plot the traces of[C, P, L, H, D], the actions and the cumulative rewards at each time step t.",
  "For deterministic StepCountJITAI, we use: context uncertainty = 0.01 (i.e., nearly 0 context error)and the same default parameters as in the base simulator, as described in Appendix C": "For stochastic StepCountJITAI with Uniform distributions, we run experiments for various com-binations of parameters to control the stochasticity: [, ahd, s, ade] values: [.1, .05, 2.5, .05],[.8, .05, 2.5, .05], [1., .05, 2.5, .05], [2., .05, 2.5, .05], [.1, .2, 10., .2], [.8, .2, 10., .2], [1., .2, 10., .2],[2., .2, 10., .2], [.1, .2, 20., .5], [.8, .2, 20., .5], [1., .2, 20., .5], [2., .2, 20., .5]. We show the resultsfor: = 2, ahd = 0.2, s = 20 and ade = 0.5.",
  "For stochastic StepCountJITAI with Beta distributions, we run experiments with values in{1, 20, 100}, s in {2.5, 10, 20} and in {0.1, 0.8, 2}. We show the results for: = 2, all = 100and s = 20": "The traces are shown in . We can see that when using deterministic StepCountJITAI withnearly 0 context uncertainty, the true context ct, the probability of context=1 pt and the inferredcontext lt match as expected. When using the stochastic versions of StepCountJITAI, we note thatthe true context ct and inferred context lt do not always overlap, due to the context uncertainty. We note that these two policies are ineffective as expected. We can see that the cumulative rewarddecreases over time as per the environment dynamics. In the main paper, in , we describethe experiments with the RL methods, which have better policies. value ctptlt value htdt 0.0 2.5 action action t cumulative reward cumulativereward",
  "F.4RL Experiment Details": "In , we describe the experiment and results when using StepCountJITAI with RL. Belowwe provide the experiment details. For each RL method, we select the best hyperparameters thatmaximize the performance, with the lowest number of episodes: the average return is around 3000for the RL methods, and around 1500 for basic TS. All experiments can be run on CPU, using GoogleColab within 2GB of RAM.",
  "The RL implementation details are as follows": "REINFORCE. We use a one-layer policy network. We perform a hyperparameter search over hiddenlayer sizes , and Adam optimizer learning rates from 1e-6 to 1e-2. We report theresults for 128 neurons, batch size b = 64, and Adam optimizer learning rate lr = 6e-4. DQN. We use a two-layer policy network. We perform a hyperparameter search over hidden layerssizes , batch sizes , Adam optimizer learning rates from 1e-6 to 1e-2, andepsilon greedy exploration rate decrements from 1e-6 to 1e-3. We report the results for 128 neuronsin each hidden layer, batch size b = 64, Adam optimizer learning rate lr = 5e-4, epsilon lineardecrement = 0.001, decaying from 1 to 0.01. The target Q network parameters are replacedevery K = 1000 steps. PPO. We use a two-layer policy network, and a three layers critic network. We perform a hyperpa-rameter search over hidden layers sizes , batch sizes , Adam optimizerlearning rates from 1e-6 to 1e-2, horizons from 10 to 40, policy clips from 0.1 to 0.5, and the otherfactors from .9 to 1.0. We report the results for 256 neurons in each hidden layer, batch size b = 64,Adam optimizer learning rate lr = 5e-3, horizon H = 20, policy clip c = 0.08, discounted factor = 0.99 and Generalized Advantage Estimator (GAE) factor = 0.95.",
  "F.5Additional RL Results for StepCountJITAI": "In , we show an example where the RL methods achieve a high average return of around3000. Below we perform additional experiments for StepCountJITAI with RL, to show when astandard RL method can or cannot work. We use different observed data, and different sets ofparameters to control the stochasticity in the environment dynamics and context uncertainty. We show an example of a case study where we do not have access to the true context C, but onlyto the inferred context L, and we have access to the behavioral variables H and D. Thus, we useStepCountJITAI with observed data [L, H, D]. We use the version with stochasticity using Uniformdistributions, as described in .2. We use the same RL settings as described in Appendix F.4. For the StepCountJITAI parameter settings, we use two settings of context uncertainty: lower contextuncertainty = 0.1 and higher context uncertainty = 0.8, and two settings of parameters to controlthe stochasticity in the environment dynamics: lower stochasticity [ahd, s, ade] = [0.05, 2.5, 0.05]and higher stochasticity [ahd, s, ade] = [0.2, 20.0, 0.5]. We show the mean and standard deviationof the average return over 10 trials, with 1500 episodes per trial. We can see that when using thesettings for lower context uncertainty and lower stochasticity in the environment dynamics, all theRL methods are able to learn, and reach a high average return of around 3000. When using thesettings for lower context uncertainty but with higher stochasticity in the environment dynamics, thevariability in the average returns is also higher. Using the setting for higher context uncertainty, allthe RL methods average returns drop to below 2000. As expected, TS shows a lower average returnthan the RL methods in all the experiments. episode"
}