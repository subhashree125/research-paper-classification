{
  "Abstract": "In the adversarial streaming model, the input is a sequence of adaptive updates that definesan underlying dataset and the goal is to approximate, collect, or compute some statistic whileusing space sublinear in the size of the dataset. In 2022, Ben-Eliezer, Eden, and Onak showed adense-sparse trade-off technique that elegantly combined sparse recovery with known techniquesusing differential privacy and sketch switching to achieve adversarially robust algorithms for Lpestimation and other algorithms on turnstile streams. In this work, we first give an improvedalgorithm for adversarially robust Lp-heavy hitters, utilizing deterministic turnstile heavy-hitteralgorithms with better tradeoffs. We then utilize our heavy-hitter algorithm to reduce theproblem to estimating the frequency moment of the tail vector. We give a new algorithm forthis problem in the classical streaming setting, which achieves additive error and uses spaceindependent in the size of the tail. We then leverage these ingredients to give an improvedalgorithm for adversarially robust Lp estimation on turnstile streams.",
  "Introduction": "Adversarial robustness for big data models is increasingly important not only for ensuring thereliability and security of algorithmic design against malicious inputs and manipulations, but also toretain guarantees for honest inputs that are nonetheless co-dependent with previous outputs of thealgorithm. One such big data model is the streaming model of computation, which has emerged as acentral paradigm for studying statistics of datasets that are too large to store. Common examples ofdatasets that are well-represented by data streams include database logs generated from e-commercetransactions, Internet of Things sensors, scientific observations, social network traffic, or stockmarkets. To capture these applications, the one-pass streaming model defines an underlying datasetthat evolves over time through a number of sequential updates that are discarded irrevocably afterprocessing, and the goal is to compute or approximate some fixed function of the dataset whileusing space sublinear in both the length m of the data stream and the dimension n of the dataset.",
  "arXiv:2412.05807v1 [cs.DS] 8 Dec 2024": "sequence of updates may be adversarial. In other words, the dataset is oblivious to any algorithmicdesign choices, such as instantiations of internal random variables. This is vital for many streamingalgorithms, which crucially leverage randomness to achieve meaningful guarantees in sublinear space.For example, the celebrated AMS sketch [AMS99] initializes a random sign vector s and outputss, f2 as the estimate for the squared L2 norm of the underlying frequency vector f defined bythe stream. To show correctness of the sketch, we require s to be chosen uniformly at random,independent of the value of f. Similar assumptions are standard across many fundamental sublinearalgorithms for machine learning, such as linear regression, low-rank approximation, or column subsetselection.Unfortunately, such an assumption can be unreasonable [MNS11, GHS+12, BMSC17, NY19, CN20], as an honest user may need to repeatedly interact with an algorithm, choosing their futureactions based on responses to previous questions. For example, in recommendation systems, it isadvisable to produce suggestions so that when a user later decides to dismiss some of the itemspreviously recommended by the algorithm, a new high-quality list of suggestions can be quicklycomputed without solving the entire problem from scratch [KMGG07, MBN+17, KZK18, OSU18,AMYZ19]. Another example is in stochastic gradient descent or linear programming, where eachtime step can update the eventual output by an amount based on a previous query. For taskssuch as linear regression, actions as simple as sorting a dataset have been shown to cause popularmachine learning libraries to fail [BHM+21].",
  "Adversarially robust streaming model.In the adversarial streaming model [BY20, HKM+20,": "ABD+21, BHM+21, KMNS21, WZ21b, BKM+22, BEO22, BJWY22, CGS22, ACGS23, ACSS23,DSWZ23, GLW+24], a sequence of adaptively chosen updates u1, . . . , um is given as an input datastream to an algorithm. The adversary may choose to generate future updates based on previousoutputs of the algorithm, while the goal of the algorithm is to correctly approximate or compute afixed function at all times in the stream. Formally, the black-box adversarial streaming model canbe modeled as a two-player game between a streaming algorithm A and a source E that creates astream of adaptive and possibly adversarial inputs to A. Prior to the game, a fixed statistic Q isdetermined, so that the goal of the algorithm is to approximate Q on the sequence of inputs seen ateach time. The game then proceeds across m rounds, where for t [m], in the t-th round:",
  "(3) E observes and records the response Zt": "The goal of E is to induce from A an incorrect response Zt to the query Q at some time t [m]throughout the stream using its control over the sequence u1, . . . , um. By the nature of the game,only a single pass over the stream is permitted. In the context of our paper, each update ut hasthe form (it, t), where it [n] and t {1}. The updates implicitly define a frequency vectorf Rn, so that ut changes the value of the (it)-th coordinate of f by t.",
  "Turnstile streams and flip number.In the turnstile model of streaming, updates are allowedto either increase or decrease the weight of elements in the underlying dataset, as compared to": "insertion-only streams, where updates are only allowed to increase the weight. Whereas varioustechniques are known for the adversarial robustness on insertion-only streams, significantly lessis known for turnstile streams. While near-optimal adversarially robust streaming algorithms forfundamental problems such as Lp estimation have been achieved in polylogarithmic space for p 2by [WZ21b] in the insertion-only model, it is a well-known open question whether there exists aconstant C = (1) such that the same problems require space (nC) in the turnstile model, wheren is the dimension of the underlying frequency vector. Indeed, [HW13] showed that the existence ofa constant C = (1) such that no linear sketch with sketching dimension o(nC) can approximatethe L2 norm of an underlying frequency vector within even a polynomial multiplicative factor, whenthe adversarial input stream is turnstile and real-valued.Given an accuracy parameter (1+), the flip number is the number of times the target functionQ changes by a factor of (1+O ()). It is known that for polynomially-bounded monotone functions Qon insertion-only streams, we generally have = O1 log m, but for turnstile streams that toggle theunderlying frequency vector between the all-zeros vector and a nonzero vector with each update, wemay have = (m). There are various techniques that then implement [BJWY22] or even roughly",
  "results show that Omp/(2p+1)space suffices for the goal of Lp norm estimation, where the objective": "is to estimate (fp1 + . . . + fpn)1/p for an input vector f Rn, which is an important problem that hasa number of applications, such as network traffic monitoring [FKSV02, KSZC03, TZ04], clusteringand other high-dimensional geometry problems [BIRW16, CJLW22, CCJ+23, CWZ23], low-rankapproximation and linear regression [CW09, FMSW10, BDM+20, VVWZ23, WY23], earth-moverestimation [Ind04, AIK08, ABIW09], cascaded norm estimation [JW09, MRWZ20], and entropyestimation [HNO08]. Unfortunately, there has been no progress for Lp estimation on turnstilestreams since the work of [BEO22], either in terms of achievability or impossibility. Thus we ask:",
  "fp": "Generally, heavy-hitter algorithms actually solve the harder problem of outputting an estimatedfrequency fi such that | fi fi| C fp, for each i [n], where C < 1 is some constant such as16. Observe that such a guarantee solves the -Lp-heavy hitters problem because each i such thatfi fp must have fi > 3",
  "4p3. It can be shown that =2p": "(4p3)(2p+1),which is positive for all p [1, 2). Thus our result shows that the true nature of the heavy-hitterproblem lies beyond the techniques of [BEO22].A particular regime of interest is p = 1, where the previous dense-sparse framework of [BEO22]achieves Om1/3bits of space, but our result in Theorem 1.2 only requires polylogarithmic space. Moment estimation.Along the way to our main result, we also give a new algorithm forestimating the residual of a frequency vector up to some tail error. More precisely, given a frequencyvector f that is defined implicitly through a data stream and a parameter k > 0, let g be a tailvector of f, which omits the k entries of f largest in magnitude, breaking ties arbitrarily. Similarly,let h be a tail vector of f that omits the (1 )k entries of f largest in magnitude, where (0, 1)serves as an error parameter. Then we give a one-pass streaming algorithm that outputs an estimatefor gpp up to additive hpp, using space poly1, log n. In particular, our space is independentof the tail parameter k. Our algorithm uses a standard approach of subsampling coordinates intosubstreams and estimating the heavy-hitters in each substream. The main point is that if there arelarge items in the top k coordinates, they can be estimated to good accuracy and there can only bea small number of them. On the other hand, if there is a large number of small coordinates in thetop k coordinates, we can roughly estimate the total number of these coordinates and the error willbe absorbed by the (1 )k relaxation. We defer a more formal discussion to and the fullguarantees specifically to Theorem 3.6. We then give our main result:",
  "O (mc) poly1, log(nm)bits of space and outputs a (1 + )-approximation to the Lp norm of theunderlying vector at all times of an adversarial stream of length m": "It can again be shown that our result in Theorem 1.3 again improves on the dense-sparseframework of [BEO22] across all p (1, 2). For example, for p = 1.5, the previous result uses spaceOm3/8= Om0.375, while our algorithm uses space Om47/126 Om0.373. Although ourquantitative improvement is mild, it nevertheless illustrates that the dense-sparse technique doesnot serve as an impossibility barrier.",
  "Technical Overview": "Recall that the flip number is the number of times the Fp moment changes by a factor of(1 + O ()), given a target accuracy (1 + ). Given a stream with flip number , the standardsketch-switching technique [BJWY22] for adversarial robustness is to implement independentinstances of an oblivious streaming algorithm for Fp estimation, iteratively using the output of eachalgorithm only when it differs from the output of the previous algorithm by a (1 + )-multiplicativefactor. The computation paths technique of [BJWY22] similarly suffers from an overhead of roughly. Subsequently, [HKM+20, ACSS23] showed that by using differential privacy, it suffices to useroughly independent instances of an oblivious streaming algorithm for Fp estimation to achievecorrectness at all times for an adaptive input stream. Unfortunately, the flip number for a streamof length m can be as large as (m), such as in the case where the underlying frequency vectoralternates between the all zeros vector and a nonzero vector.The dense-sparse framework of [BEO22] observes that the only case where the flip number canbe large is when there are a large number of times in the stream where the corresponding frequencyvector is somewhat sparse. For example, in the above scenario where the underlying frequencyvector alternates between the all zeros vector and a nonzero vector, all input vectors are 1-sparse.In fact, they notice that for Fp estimation, that once the frequency vector has at least mC nonzeroentries for any fixed constant C (0, 1), then since all entries must be integral and all updatesonly change each entry by 1, at least (mC/p) updates are necessary before the p-th moment ofthe resulting frequency vector can differ by at least a (1 + )-multiplicative factor. Hence in thestream updates where the frequency vector has at least mC nonzero entries, the flip number can beat most Om1C/p, for = (1). Thus it suffices to run Om1/2C/2pindependent instances ofthe oblivious algorithm, using the differential privacy technique of [HKM+20, ACSS23]. Moreover,in the case where the vector is mC-sparse, there are sparse recovery techniques that can exactlyrecover all the nonzero coordinates using OmCspace, even if the input is adaptive. Hence by",
  "balancing OmC= Om1/2C/2pat C = 1": "3, [BEO22] achieves Omp/(2p+1)overall space forFp estimation for adaptive turnstile streams.Our key observation is that for p (1, 2), if the frequency vector has at least mC nonzero entries,a sequence of O(mC/p) updates may not always change the p-th moment of the underlying vector.For example, if the updates are all to separate coordinates, then the p-th moment may actuallychange very little. In fact, a sequence of O(mC/p) updates may only change the p-th momentof the underlying vector by a multiplicative (1 + ) factor if most of the updates are to a smallnumber of coordinates. As a result, most of the updates are to some coordinate that was eitherinitially a heavy-hitter or subsequently a heavy-hitter. Then by tracking the heavy-hitters of theunderlying frequency vector, we can handle the hard input for [BEO22], thus demanding a largernumber of stream updates before the p-th moment of the vector can change by a multiplicative(1 + ) factor. Consequently, the number of independent instances decreases, which facilitates abetter balancing and allows us to achieve better space bounds. Unfortunately, there are multiplechallenges to realizing this intuition. Heavy-hitters.First, we need a streaming algorithm for accurately reporting the frequenciesof the Lp-heavy hitters at all times in the adaptive turnstile stream. However, such a subroutineis not known and navely, one might expect an estimate of the Lp norm might be necessary toidentify the Lp heavy-hitters. Moreover, algorithms for finding Lp heavy-hitters are often used to estimate the Lp norm of the underlying frequency, e.g., [IW05, WZ12, BBC+17, LSW18, BWZ21,WZ21a, MWZ22, BMWZ23, JWZ24]. Instead, we use a turnstile streaming algorithm DetHH forLp heavy-hitters [GM07] that uses sub-optimal space O12 n22/pbits of space for p (1, 2], rather than the optimal CountSketch, which uses O12 log2 nbits of space. However, theadvantage of DetHH is that the algorithm is deterministic, so we can utilize the previous intuitionfrom the dense-sparse framework of [BEO22]. In particular, if the universe size is small, then wecan run DetHH, and if the universe size is large, then we collectively handle these cases using anensemble of CountSketch algorithms via differential privacy. We provide the full details of therobust Lp-heavy hitter algorithm in , ultimately achieving Theorem 1.2. Residual estimation.The remaining step for the subroutine that will be ultimately incorporatedinto the differential privacy technique of [HKM+20, ACSS23] is to estimate the contribution of theelements that are not Lp heavy-hitters, i.e., the residual vector, toward the overall p-th moment.More generally, given a tail parameter k > 0 and an error parameter (0, 1), let g be a tail vectorof f that omits the k entries of f largest in magnitude, breaking ties arbitrarily and let h be atail vector of f that omits the (1 )k entries of f largest in magnitude. We define the level setsof the p-th moment so that level set roughly consists of the coordinates of g with magnitude[(1 + ), (1 + )+1). We then estimate the contribution of each level set to the p-th moment of theresidual vector using the subsampling framework introduced by [IW05].Namely, we note that any significant level set has either a small number of items with largemagnitude, or a large number of items that collectively have significant contribution to the p-thmoment. In the former case, we can use CountSketch to identify the items with large magnitude,while in the latter case, it can be shown that after subsampling the universe, there will be a largenumber of items in the level set that remain. Moreover, these items will now be heavy with respectto the p-th moment of the resulting frequency vector after subsampling with high probability. Thus,these items can be identified by CountSketch on the subsampled universe. Furthermore, afterrescaling inversely by the sampling probability, the total number of such items in the level set canbe estimated accurately by rescaling the number of the heavy-hitters in the subsampled universe.Hence in both cases, we can estimate the number of items in the significant level sets and subtractoff the largest k such items. We provide the full details of the residual estimation algorithm in, culminating in Theorem 3.6.",
  "Preliminaries": "For a positive integer n > 0, we use [n] to denote the set of integers {1, . . . , n}. We use poly(n) todenote a fixed polynomial in n whose degree can be set by adjust constants in the algorithm basedon various desiderata, e.g., in the failure probability. We use polylog(n) to denote poly(log n). Whenthere exist constants to facilitate an event to occur with probability 1 1 poly(n), we say that theevent occurs with high probability. For a random variable X, we use E [X] to denote its expectationand Var(X) to denote its variance.Recall that for p > 0, the Lp norm of a vector v Rn is vp = (vp1 + . . . + vpn)1/p. The p-thmoment of v is defined as Fp(v) = vpp. Note that for a constant p 1, a (1 + )-approximationto the Fp(v) implies a (1 + )-approximation to vp. Similarly, for a sufficiently small constant (0, 1), a (1 + O ())-approximation to vp implies a (1 + O ())p = (1 + )-approximation to Fp(v). We thus use the problems of Lp norm estimation and Fp moment estimation interchangeablyin discussion.We use Fp,Res(k)(f) to denote the p-th moment of a vector g obtained by setting to zero thek coordinates of f largest in magnitude, breaking ties arbitrarily. We also define v0 to be thenumber of nonzero coordinates of v, so that v0 = |{i [n] | vi = 0}|.We recall the following notions regarding differential privacy. Definition 1.4 (Differential privacy). [DMNS06] Given > 0 and (0, 1), a randomized algorithmA : D R with domain D and range R is (, )-differentially private if, for every neighboringdatasets S and S and for all E R,",
  "f(t)2,where f(t) is the induced frequency vector at time t": "To achieve the guarantees of Theorem 2.3, a natural approach would be to apply Theorem 1.8 tothe guarantees of CountSketch in Theorem 2.2. However, this does not achieve the optimal boundsbecause each round of adaptive queries can require multiple answers, i.e., estimated frequencies foreach of the heavy-hitters at that time. Thus, [CLN+22] proposed a slight variation of the algorithmalong with intricate analysis to achieve the guarantees of Theorem 2.3.While RobustCS has better space guarantees than DetHH, determinism nevertheless serves animportant purpose for us. Namely, adversarial input can induce failures on randomized algorithmsbut cannot induce failures on deterministic algorithms. On the other hand, the space usage ofDetHH grows with the size of the universe. Thus, we now use insight from the dense-sparseframework of [BEO22]. If the universe size is small, then we shall use DetHH. On the other hand, ifthe universe size is large, then we shall use the following robust version of CountSketch, requiringroughly number of independent instances, where is the flip number. The key observation isthat because the universe size is large, then the flip number will be much smaller than in the worstpossible case. Moreover, we can determine which case we are in, i.e., the large universe case or thesmall universe case, by using the following L0 estimation algorithm:",
  "+ log log mbits of space, and with probability at least 1 , outputs a(1 + )-approximation to L0": "We give our algorithm in full in Algorithm 2. Because DetHH is a deterministic algorithm, itwill always be correct in the case where the universe size is small. Thus, we first prove that in thecase where the universe size is large, then RobustCS ensures correctness within each sequence of updates.",
  ":STATE SPARSE": "Lemma 2.5. Suppose the number of distinct elements at the beginning of a block is at least 50t.Let S be the output of RobustCS at the beginning of a block. Then conditioned on the correctnessof RobustCS, S solves the Lp-heavy hitter problem on the entire block. Proof. Suppose the number of distinct elements at the beginning of a block is at least 50t. Letf be the frequency vector at the beginning of the block and let g be the frequency vector at anyintermediate step in the block. Conditioned on the correctness of RobustCS, we have that theestimated frequency fi of each item i satisfies",
  "Oblivious Residual Estimation Algorithm": "In this section, we consider norm and moment estimation of a residual vector, permitting bicriteriaerror by allowing some slack in the size of the tail. Specifically, suppose the input vector f arrivesin the streaming model. Given a tail parameter k > 0 and an error parameter (0, 1), let g be atail vector of f that omits the k entries of f largest in magnitude, breaking ties arbitrarily and let hbe a tail vector of f that omits the (1 )k entries of f largest in magnitude. We give an algorithmthat estimates gpp up to additive hpp, using space poly1, log n, which is independent of thetail parameter k. It should be noted that our algorithm is imprecise on gpp in two ways. Firstly,it incurs additive error proportional to . Secondly, the additive error has error with respect to h,which is missing the top (1 )k entries of f in magnitude, rather than the top k. Nevertheless, thespace bounds that are independent of k are sufficiently useful for our subsequent application of Lpestimation. We first define the level sets of the p-th moment and the contribution of each level set. Definition 3.1 (Level sets and contribution). Let > 0 be a parameter and let m be the length ofthe stream. Let M be the power of two such that mp M < (1 + )mp and let . Then foreach integer 1, we define the level set :=i [n] | fi M",
  "(1+)1 ,M": "(1+), so it is possible that j could be classified intocontributing to even if j / . Hence, we first analyze an idealized setting, where each index jis correctly classified across all level sets [L]. We that we achieve a (1 + O ())-approximation toFp in the idealized setting and then argue that because we choose uniformly at random, then onlyapproximation guarantee will worsen only slightly but still remain a (1 + )-approximation to Fp,since only a small number of coordinates will be misclassified and so our approximation guaranteewill only slightly degrade.",
  "Observe that to provide the guarantees of Theorem 3.2, CountSketch would require spacepoly1, log n, rather than quadratic dependency 1": ".We now describe our residual estimation algorithm. Our algorithm attempts to estimate thecontribution of each level set. Some of these level sets contribute a significant amount to thep-th moment of f, whereas other level sets do not. It can be seen that the number of items in eachlevel set that is contributing can be estimated up to a (1 + O ())-approximation. In particular,either a contributing level set has a small number of items with large mass, or a large number ofitems that collectively have significant mass. We use the heavy-hitter algorithm CountSketch todetect the level sets with a small number of items with large mass, and count the number of itemsin these level sets. For the large number of items that collectively have significant mass, it can beshown that after subsampling the universe, there will be a large number of these items remaining,and those items will be identified by CountSketch on the subsampled universe. Moreover, thetotal number of such items in the level set can be estimated accurately by rescaling the number ofthe heavy-hitters in the subsampled universe inversely by the sampling probability. We can thuscarefully count the number of items in the contributing level sets and subtract off the largest k suchitems. Because we only have (1 + )-approximations to the number of such items, it may be possiblethat we subtract off too many, hence the bicriteria approximation.Finally, we note that for the insignificant level sets, we can no longer estimate the number ofitems in these level set up to (1 + )-factor. However, we note that the number of such items is onlyan fraction of the number of items in the lower level sets that are contributing. Therefore, we canshow that it suffices to set the contribution of these level sets to zero. Our algorithm appears in full in Algorithm 3.We now show that the number of items (as well as their contribution) in each contributing levelset with a small number of items with large mass will be estimated within a (1 + )-approximation.",
  "> 1": "Informally, the casework corresponds to whether the frequenciesfjp in a significant level setare large or not large, i.e., whether they are above the heavy-hitter threshold before subsamplingthe universe. Thus if the frequencies are large, then the heavy-hitter algorithm will estimate theirfrequencies, but if the frequencies are not large, then we must perform subsampling before the itemssurpass the heavy-hitter threshold.Supposelog(1 + ) log 2 log(nm)",
  "log(nm)": "-approximation to (fj)p. As a result, a misclassifiedindex induces at most (fj)p additive error to the contribution of level set and hence at most(fj)p additive error to the contribution of level set in the residual vector. Therefore, the totaladditive error across all j [n] due to misclassification is at most Fp in expectation. By Markovsinequality, the total additive error due to misclassification is at most",
  ". By Theorem 3.2, each instance of CountSketch": "with threshold 3 uses O16 log2(nm)bits of space. Finally, we remark that in the analysis forsubsampling, the concentration inequalities still hold with O (log n)-wise independence. Thus wecan use O (log n)-wise independence hash functions to encode the subsampling process. Therefore,the total space usage of Algorithm 2 is O16 log3(nm)bits.",
  "Adversarially Robust Lp Estimation": "In this section, we give an adversarially robust algorithm for Fp moment estimation on turnstilestreams. Due to the relationship between the Fp moment and the Lp norm, our result similarlytranslates to a robust algorithm for Lp norm estimation. We first require an algorithm to recoverall the coordinates of the underlying frequency vector if it is sparse. Theorem 4.1. [GSTV07] There exists a deterministic algorithm SparseRecover that recovers ak-sparse frequency vector defined by an insertion-deletion stream of length n. The algorithm usesk polylog(n) bits of space.",
  "We remark that SparseRecover is deterministic and guarantees correctness on a turnstilestream, even if the frequency vector is not sparse at some intermediate step of the stream. On": "the other hand, if the frequency vector is not sparse, then a query to SparseRecover could beerroneous. Hence, our algorithm thus utilizes robust LZeroEst to detect whether the underlyingfrequency vector is dense or sparse. Similar to [BEO22], the intuition is that due to the sparsecase always succeeding, the adversary can only induce failure if the vector is dense, which in turndecreases the flip number. However, because we also accurately track the heavy-hitters, then theadversary must spread the updates across a multiple number of coordinates, resulting in a largernumber of updates necessary to double the residual vector. Since the number of updates is larger,then the flip number is smaller, and so our algorithm can use less space. Unfortunately, even thoughthe residual vector may not double in its p-th moment, the p-th moment of entire frequency vectorf may change drastically. This is a nuance for the analysis because our error guarantee can nolonger be relative to the fpp. Indeed, fpp additive error may induce (1 + )-multiplicative errorat one point, but at some later point we could have fpp fpp, so that the same additive errorcould even be polynomial multiplicative error. Hence, we require the ResidualEst subroutinefrom , whose guarantees are in terms of the residual vector. We give our algorithm in fullin Algorithm 4.We first show that the p-th moment of f can be essentially split by looking at the p-th momentof the vector consisting of the largest k coordinates and the remaining tail vector. Lemma 4.2. Let (0, 1) be a fixed accuracy parameter and let p > 0 be fixed. Let f Rn be anyfixed vector and let k 0 be any fixed parameter. Let g be the vector consisting of the k coordinatesof f largest in magnitude and let h be the residual vector, so that f = g + h. Suppose G and Hsatisfy",
  "(1 )fpp F (1 + )fpp": "Proof. Consider the first time t in a block of updates and let f be the frequency vector inducedby the stream up to that point. We first observe that RobustHH with threshold will returnany coordinates i [n] such that fi pp fpp up to (1 + )-approximation. For the remainingcoordinates in the k-sparse vector returned by RobustHH, any k of them can contribute at mostp fpp. Therefore, we have by Lemma 4.2 that conditioned on the correctness of RobustHH andResidualEst, we have G + H is a (1 + O ())-approximation to fpp. For the purposes of notation,let h denote the residual vector of f at time t, omitting the k coordinates of f largest in magnitude.Now, consider some later time t in the same block of updates and let v be the frequency vectorinduced by the updates in the block, i.e., the updates from t to t. Let u be the residual vectoromitting the k coordinates of f + v largest in magnitude. Since v1 for = O mc/pk11/p,",
  "hpp. Thus provided that H is a (1 + O ())-approximation to hpp, then it remains a1 +": "4-approximation to upp. Hence conditioned on thecorrectness again of RobustHH at time t, we have that H + H remains a (1 + )-approximationto fpp at time t.As correctness of RobustHH follows from Theorem 1.2, it remains to show correctness ofResidualEst on an adaptive stream. Because each block has size , then the stream has at mostm such blocks. Hence by the adversarial robustness of differential privacy, i.e., Theorem 1.8, itsuffices to run O m",
  "Empirical Evaluations": "In this section, we describe our empirical evaluations for comparing the flip number of the entirevector and the flip number of the residual vector on real-world datasets. Note that these quantitiesparameterize the space used by the algorithm of [BEO22] and by our algorithm, respectively. CAIDA traffic monitoring dataset.We used the CAIDA dataset [CAI16] of anonymizedpassive traffic traces from the equinix-nyc data centers high-speed monitor. The dataset iscommonly used for empirical evaluations on frequency moments and heavy-hitters. We extractedthe sender IP addresses from 12 minutes of the internet flow data, which contained roughly 3 milliontotal events. Experimental setup.Our empirical evaluations were performed Python 3.10 on a 64-bit operatingsystem on an AMD Ryzen 7 5700U CPU, with 8GB RAM and 8 cores with base clock 1.80 GHz. Ourcode is available at We compare the flip number of theentire data stream versus the flip number of the residual vector across various values of the algorithmerror {101, 102, . . . , 105}, values of the heavy-hitter threshold {41, 42, . . . , 410}, andthe frequency moment parameter p {1.1, 1.2, . . . , 1.9}. We describe the results in .",
  "(c) Flip number across p": ": Empirical evaluations on the CAIDA dataset, comparing flip number of the p-th frequencymoment and the residual, for = = 0.001 and p = 1.5 when not variable. Smaller flip numbersindicate less space needed by the algorithm. Results and discussion.Our empirical evaluations serve as a simple proof-of-concept demon-strating that adversarially robust algorithm can use significantly less space than existing algorithms.In particular, existing algorithms use space that is an increasing function of the flip number of thep-th frequency moment, while our algorithms use space that is an increasing function of the flipnumber of the residual, which is significantly less across all settings in . While the ratiodoes increase as the exponent p increases in c, there is not a substantial increase, i.e., 1.24to 1.31 from p = 1.1 to p = 1.9. On the other hand, as decreases in b, the ratio increasesfrom 1.002 for = 41 to 1.6 for = 410. Similarly, in a, the ratio of these quantitiesbegins at 1.17 for = 101 and increases to as large as 1.75 for = 105. Therefore, even in thecase where the input is not adaptive, our empirical evaluations demonstrate that these flip numberquantities can be quite different, and consequently, our algorithm can use significantly less spacethan previous existing algorithms. [ABD+21]Noga Alon, Omri Ben-Eliezer, Yuval Dagan, Shay Moran, Moni Naor, and Eylon Yogev.Adversarial laws of large numbers and optimal regret in online classification. In STOC:53rd Annual ACM SIGACT Symposium on Theory of Computing, pages 447455, 2021.2 [ABIW09]Alexandr Andoni, Khanh Do Ba, Piotr Indyk, and David P. Woodruff. Efficient sketchesfor earth-mover distance, with applications. In 50th Annual IEEE Symposium onFoundations of Computer Science, FOCS, pages 324330, 2009. 3",
  "of the 42nd ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of DatabaseSystems, PODS, pages 141153, 2023. 2": "[ACSS23]Idan Attias, Edith Cohen, Moshe Shechner, and Uri Stemmer. A framework for adver-sarial streaming via differential privacy and difference estimators. In 14th Innovationsin Theoretical Computer Science Conference, ITCS, pages 8:18:19, 2023. 2, 3, 5, 6, 7 [AIK08]Alexandr Andoni, Piotr Indyk, and Robert Krauthgamer.Earth mover distanceover high-dimensional spaces. In Proceedings of the Nineteenth Annual ACM-SIAMSymposium on Discrete Algorithms, SODA, pages 343352, 2008. 3",
  "[AMS99]Noga Alon, Yossi Matias, and Mario Szegedy. The space complexity of approximatingthe frequency moments. J. Comput. Syst. Sci., 58(1):137147, 1999. 2": "[AMYZ19] Dmitrii Avdiukhin, Slobodan Mitrovic, Grigory Yaroslavtsev, and Samson Zhou. Adver-sarially robust submodular maximization under knapsack constraints. In Proceedingsof the 25th ACM SIGKDD International Conference on Knowledge Discovery & DataMining, KDD, pages 148156, 2019. 2 [BBC+17]Jaroslaw Blasiok, Vladimir Braverman, Stephen R. Chestnut, Robert Krauthgamer, andLin F. Yang. Streaming symmetric norms via measure concentration. In Proceedingsof the 49th Annual ACM SIGACT Symposium on Theory of Computing, STOC, pages716729, 2017. 6 [BDM+20] Vladimir Braverman, Petros Drineas, Cameron Musco, Christopher Musco, Jalaj Upad-hyay, David P. Woodruff, and Samson Zhou. Near optimal linear algebra in the onlineand sliding window models.In 61st IEEE Annual Symposium on Foundations ofComputer Science, FOCS, pages 517528, 2020. 3 [BEO22]Omri Ben-Eliezer, Talya Eden, and Krzysztof Onak. Adversarially robust streaming viadense-sparse trade-offs. In 5th Symposium on Simplicity in Algorithms, SOSA, 2022.(to appear). 2, 3, 4, 5, 6, 8, 17, 19 [BHM+21] Vladimir Braverman, Avinatan Hassidim, Yossi Matias, Mariano Schain, Sandeep Silwal,and Samson Zhou. Adversarial robustness of streaming algorithms through importancesampling. In Advances in Neural Information Processing Systems 34: Annual Conferenceon Neural Information Processing, NeurIPS, pages 35443557, 2021. 2 [BIRW16]Arturs Backurs, Piotr Indyk, Ilya P. Razenshteyn, and David P. Woodruff. Nearly-optimal bounds for sparse recovery in generic norms, with applications to k-mediansketching. In Proceedings of the Twenty-Seventh Annual ACM-SIAM Symposium onDiscrete Algorithms, SODA, pages 318337, 2016. 3",
  "[BJWY22] Omri Ben-Eliezer, Rajesh Jayaram, David P. Woodruff, and Eylon Yogev. A frameworkfor adversarially robust streaming algorithms. J. ACM, 69(2):17:117:33, 2022. 2, 3, 5": "[BKM+22] Amos Beimel, Haim Kaplan, Yishay Mansour, Kobbi Nissim, Thatchaphol Saranurak,and Uri Stemmer. Dynamic algorithms against an adaptive adversary: generic con-structions and lower bounds. In STOC 22: 54th Annual ACM SIGACT Symposium onTheory of Computing, pages 16711684, 2022. 2, 7 [BMSC17] Ilija Bogunovic, Slobodan Mitrovic, Jonathan Scarlett, and Volkan Cevher. Robustsubmodular maximization: A non-uniform partitioning approach. In Proceedings of the34th International Conference on Machine Learning, ICML, pages 508516, 2017. 2 [BMWZ23] Vladimir Braverman, Joel Manning, Zhiwei Steven Wu, and Samson Zhou. Privatedata stream analysis for universal symmetric norm estimation. In Approximation,Randomization, and Combinatorial Optimization. Algorithms and Techniques, AP-PROX/RANDOM, pages 45:145:24, 2023. 6",
  "[CCF04]Moses Charikar, Kevin C. Chen, and Martin Farach-Colton. Finding frequent items indata streams. Theor. Comput. Sci., 312(1):315, 2004. 8, 10": "[CCJ+23]Xi Chen, Vincent Cohen-Addad, Rajesh Jayaram, Amit Levi, and Erik Waingarten.Streaming euclidean MST to a constant factor. In Proceedings of the 55th Annual ACMSymposium on Theory of Computing, STOC, pages 156169, 2023. 3 [CGS22]Amit Chakrabarti, Prantar Ghosh, and Manuel Stoeckl. Adversarially robust coloringfor graph streams. In 13th Innovations in Theoretical Computer Science Conference,ITCS, pages 37:137:23, 2022. 2 [CJLW22]Xi Chen, Rajesh Jayaram, Amit Levi, and Erik Waingarten. New streaming algorithmsfor high dimensional EMD and MST. In STOC 22: 54th Annual ACM SIGACTSymposium on Theory of Computing, pages 222233, 2022. 3 [CLN+22]Edith Cohen, Xin Lyu, Jelani Nelson, Tams Sarls, Moshe Shechner, and Uri Stemmer.On the robustness of countsketch to adaptive inputs. In International Conference onMachine Learning, ICML, pages 41124140, 2022. 8 [CN20]Yeshwanth Cherapanamjeri and Jelani Nelson. On adaptive distance estimation. InAdvances in Neural Information Processing Systems 33: Annual Conference on NeuralInformation Processing Systems 2020, NeurIPS, 2020. 2 [CSW+23] Yeshwanth Cherapanamjeri, Sandeep Silwal, David P. Woodruff, Fred Zhang, QiuyiZhang, and Samson Zhou. Robust algorithms on adaptive inputs from bounded adver-saries. In The Eleventh International Conference on Learning Representations, ICLR,2023. 7",
  "[CW09]Kenneth L. Clarkson and David P. Woodruff. Numerical linear algebra in the streamingmodel. In Proceedings of the 41st Annual ACM Symposium on Theory of Computing,STOC, pages 205214, 2009. 3": "[CWZ23]Vincent Cohen-Addad, David P. Woodruff, and Samson Zhou. Streaming euclideank-median and k-means with o(log n) space. In 64th IEEE Annual Symposium onFoundations of Computer Science, FOCS, pages 883908, 2023. 3 [DFH+15]Cynthia Dwork, Vitaly Feldman, Moritz Hardt, Toniann Pitassi, Omer Reingold, andAaron Leon Roth. Preserving statistical validity in adaptive data analysis. In Proceedingsof the Forty-Seventh Annual ACM on Symposium on Theory of Computing, STOC,pages 117126. ACM, 2015. 7 [DMNS06] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam D. Smith. Calibrating noiseto sensitivity in private data analysis. In Theory of Cryptography, Third Theory ofCryptography Conference, TCC, Proceedings, pages 265284, 2006. 7",
  "[FKSV02]Joan Feigenbaum, Sampath Kannan, Martin Strauss, and Mahesh Viswanathan. Anapproximate l1-difference algorithm for massive data streams.SIAM J. Comput.,32(1):131151, 2002. 3": "[FMSW10] Dan Feldman, Morteza Monemizadeh, Christian Sohler, and David P. Woodruff. Coresetsand sketches for high dimensional subspace approximation problems. In Proceedingsof the Twenty-First Annual ACM-SIAM Symposium on Discrete Algorithms, SODA,pages 630649, 2010. 3 [GHS+12]Anna C. Gilbert, Brett Hemenway, Martin J. Strauss, David P. Woodruff, and MaryWootters. Reusable low-error compressive sampling schemes through privacy. In IEEEStatistical Signal Processing Workshop, SSP, pages 536539, 2012. 2",
  "[GLW+24] Elena Gribelyuk, Honghao Lin, David P. Woodruff, Huacheng Yu, and Samson Zhou.A strong separation for adversarially robust l0 estimation for linear sketches. CoRR,abs/2409.16153, 2024. 2": "[GM07]Sumit Ganguly and Anirban Majumder. Cr-precis: A deterministic summary structurefor update data streams. In Combinatorics, Algorithms, Probabilistic and ExperimentalMethodologies, First International Symposium, ESCAPE, pages 4859, 2007. 6, 8 [GSTV07]Anna C. Gilbert, Martin J. Strauss, Joel A. Tropp, and Roman Vershynin. One sketchfor all: fast algorithms for compressed sensing. In Proceedings of the 39th Annual ACMSymposium on Theory of Computing, pages 237246, 2007. 16 [HKM+20] Avinatan Hassidim, Haim Kaplan, Yishay Mansour, Yossi Matias, and Uri Stemmer.Adversarially robust streaming algorithms via differential privacy. In Advances inNeural Information Processing Systems 33: Annual Conference on Neural InformationProcessing Systems, NeurIPS, 2020. 2, 3, 5, 6, 7 [HNO08]Nicholas J. A. Harvey, Jelani Nelson, and Krzysztof Onak. Sketching and streamingentropy via approximation theory. In 49th Annual IEEE Symposium on Foundations ofComputer Science, FOCS, pages 489498, 2008. 3",
  "[JW09]T. S. Jayram and David P. Woodruff. The data stream space complexity of cascadednorms. In 50th Annual IEEE Symposium on Foundations of Computer Science, FOCS,pages 765774, 2009. 3": "[JWZ24]Rajesh Jayaram, David P. Woodruff, and Samson Zhou. Streaming algorithms with fewstate changes. In Proceedings of the 43rd ACM SIGMOD-SIGACT-SIGAI Symposiumon Principles of Database Systems, PODS, 2024. 6 [KMGG07] Andreas Krause, H. Brendan McMahan, Carlos Guestrin, and Anupam Gupta. Se-lecting observations against adversarial objectives. In Advances in Neural InformationProcessing Systems 20, Proceedings of the Twenty-First Annual Conference on NeuralInformation Processing Systems, pages 777784, 2007. 2 [KMNS21] Haim Kaplan, Yishay Mansour, Kobbi Nissim, and Uri Stemmer. Separating adaptivestreaming from oblivious streaming using the bounded storage model. In Advances inCryptology - CRYPTO - 41st Annual International Cryptology Conference, CRYPTOProceedings, Part III, pages 94121, 2021. 2 [KNW10]Daniel M. Kane, Jelani Nelson, and David P. Woodruff. An optimal algorithm forthe distinct elements problem. In Proceedings of the Twenty-Ninth ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems, PODS, pages 4152,2010. 8 [KSZC03]Balachander Krishnamurthy, Subhabrata Sen, Yin Zhang, and Yan Chen. Sketch-basedchange detection: methods, evaluation, and applications. In Proceedings of the 3rdACM SIGCOMM Internet Measurement Conference, IMC, pages 234247, 2003. 3",
  "In Proceedings of the 35th International Conference on Machine Learning, ICML, 2018.2": "[LSW18]Roie Levin, Anish Prasad Sevekari, and David P. Woodruff. Robust subspace approxi-mation in a stream. In Advances in Neural Information Processing Systems 31: AnnualConference on Neural Information Processing Systems, NeurIPS, 2018. 6 [MBN+17] Slobodan Mitrovic, Ilija Bogunovic, Ashkan Norouzi-Fard, Jakub Tarnawski, andVolkan Cevher. Streaming robust submodular maximization: A partitioned thresholdingapproach. In Advances in Neural Information Processing Systems 30: Annual Conferenceon Neural Information Processing Systems, pages 45574566, 2017. 2",
  "[MNS11]Ilya Mironov, Moni Naor, and Gil Segev. Sketching in adversarial environments. SIAMJ. Comput., 40(6):18451870, 2011. 2": "[MRWZ20] Sepideh Mahabadi, Ilya P. Razenshteyn, David P. Woodruff, and Samson Zhou. Non-adaptive adaptive sampling on turnstile streams. In Proceedings of the 52nd AnnualACM SIGACT Symposium on Theory of Computing, STOC, pages 12511264, 2020. 3 [MWZ22]Sepideh Mahabadi, David P. Woodruff, and Samson Zhou. Adaptive sketches for robustregression with importance sampling. In Amit Chakrabarti and Chaitanya Swamy,editors, Approximation, Randomization, and Combinatorial Optimization. Algorithmsand Techniques, APPROX/RANDOM), pages 31:131:21, 2022. 6",
  "[OSU18]James B. Orlin, Andreas S. Schulz, and Rajan Udwani. Robust monotone submodularfunction maximization. Math. Program., 172(1-2):505537, 2018. 2": "[TZ04]Mikkel Thorup and Yin Zhang. Tabulation based 4-universal hashing with applicationsto second moment estimation. In Proceedings of the Fifteenth Annual ACM-SIAMSymposium on Discrete Algorithms, SODA, pages 615624, 2004. 3 [VVWZ23] Ameya Velingker, Maximilian Vtsch, David P. Woodruff, and Samson Zhou. Fast (1+)-approximation algorithms for binary matrix factorization. In International Conferenceon Machine Learning, ICML, pages 3495234977, 2023. 3",
  "[WZ12]David P. Woodruff and Qin Zhang. Tight bounds for distributed functional monitoring.In Proceedings of the 44th Symposium on Theory of Computing Conference, STOC,pages 941960, 2012. 6": "[WZ21a]David P. Woodruff and Samson Zhou.Separations for estimating large frequencymoments on data streams. In 48th International Colloquium on Automata, Languages,and Programming, ICALP, pages 112:1112:21, 2021. 6 [WZ21b]David P. Woodruff and Samson Zhou. Tight bounds for adversarially robust streamsand sliding windows via difference estimators. In 62nd IEEE Annual Symposium onFoundations of Computer Science, FOCS, pages 11831196, 2021. 2, 3"
}