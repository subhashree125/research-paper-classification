{
  "Abstract": "Accurate prediction over long time horizons is crucial for modeling complexphysical processes such as wave propagation. Although deep neural networksshow promise for real-time forecasting, they often struggle with accumulatingphase and amplitude errors as predictions extend over a long period. To addressthis issue, we propose a novel loss decomposition strategy that breaks down theloss into separate phase and amplitude components. This technique improves thelong-term prediction accuracy of neural networks in wave propagation tasks byexplicitly accounting for numerical errors, improving stability, and reducing erroraccumulation over extended forecasts.",
  "Introduction": "Accurate long-horizon predictions are essential for understanding and modeling complex physicalphenomena, particularly in the context of wave propagation governed by partial differential equations(PDEs) . Traditional numerical methods for solving PDEs, such as finite difference, finite element,and spectral methods, have been extensively studied and applied to solve the wave equation.Although these methods are effective, they often require substantial computational resources, makingthem unsuitable for real-time predictions and control related tasks. Machine learning techniques, particularly deep neural networks, have emerged as powerful tools formodeling complex physical phenomena, including wave propagation and underwater noise prediction. Despite their success in numerous domains, neural networks often struggle withaccurate long-term predictions, primarily due to accumulated phase and amplitude errors .This issue is particularly pronounced in autoregressive models, where errors can compound over time,leading to significant deviations from the true dynamics . Recent studies have highlighted the importance of loss decomposition in addressing these challenges.Traditional mean squared error (MSE) loss functions do not distinguish between different typesof prediction errors, such as phase and amplitude errors, which are critical to accurate long-termforecasting . To mitigate this, we propose decomposing the MSE into dissipation anddispersion errors , which separately account for the amplitude and phase errors, respectively. Byexplicitly considering these error types, our approach aims to enhance the neural networks ability tomaintain accurate predictions over extended horizons. The concept of loss decomposition is not entirely new; it has been previously explored in numericalanalysis and signal processing. For example, Kreiss and Oliger introduced the idea of dissipationand dispersion errors in the context of finite difference methods. More recently, Guen et al. applied similar ideas to machine learning, suggesting that decomposed loss functions can guideneural networks to learn more robust representations of time series forecasting. Our work builds on",
  "Linear advection equation": "In this study, we focus on data generated from the linear convection equation as a model problemfor wave propagation. Although the approach has been applied to the inviscid Burgers equation toinvestigate nonlinear wave propagation, we limit our discussion to the linear convection equation forbrevity. The solution U in the domain satisfies the following parametric partial differentialequation:Ut + U",
  "U(x, 0) = U0(x) f(x),(2)where [0.775, 1.25] represents the wave speed.The initial profile is given by f(x) =1": "ex2/2 with = 5 103. A one-dimensional spatial discretization of 256 grid pointsand 200 time steps is applied. The training set consists of N = 20 parameter instances, uniformlydistributed over the range of , while the test set includes Ntest = 19 parameter instances. The testparameters are selected such that test,i = train,i+train,i+1",
  "Attention-based Convolution Recurrent Autoencoder Network": "We employ a deep learning architecture to model wave propagation, specifically utilizing a 15-layerAttention-Based Convolutional Recurrent Autoencoder Network . The encoder includes aconvolutional layer, followed by max pooling and fully connected layers. The output layer of theencoder contains r neurons, where r represents the dimensionality of the reduced manifold. Thepropagator is composed of attention-based sequence-to-sequence RNN-LSTM layers that process thereduced manifold data, advancing it along the time dimension. The decoder, designed as the reverseof the encoder, reconstructs the data from the latent space back to the original physical dimension.The model architecture and hyperparameters were adopted from the work of Deo et al. . Themodel is trained from scratch using TensorFlow on a single NVIDIA P2200 Pascal GPU, withtraining converging in 285 epochs within 20 minutes of wall-clock time. shows the visualrepresentation of AB-CRAN architecture.",
  ": Illustration of attention-based convolutional recurrent autoencoder architecture": "address this limitation, MSE can be decomposed into two components: one representing dissipationerror (amplitude) and the other capturing dispersion error (phase). The total mean square error can berewritten as: = 2 (ua) + 2 (ud) 2 (ua) (ud) + (ua ud)2 ,(3)where ua is the ground-truth solution and ud the predicted solution. 2 (ua) , 2 (ud) are the variancesof ua and ud, respectively. cov (ua, ud) is the covariance between ua and ud and is the correlationcoefficient. Eq. 3 can be rewritten as",
  "We define the first two terms of the RHS of Eq. 4 as the dissipation error and the third term as thedispersion error:DISS = [ (ua) (ud)]2 + (ua ud)2 ,DISP = 2(1 ) (ua) (ud) .(5)": "When two wave patterns differ solely in amplitude but share the same phase, their correlationcoefficient, , should be 1, with DISP = 0 as indicated by equation (5) . This outcome aligns with ourexpectations and is a reasonable result. We employ this decomposition to formulate the loss functionas follows:Lfunction = (1 )LDecoder + Lpropagator ,Lpropagator = (1 )DISP + DISS ,(6) where and are hyperparameters. To determine the values of and , we utilize hyperparametertuning with the Ray Tune ASHA algorithm . Our experiments reveal that setting and greaterthan 0.5 prioritizing the propagator, with a stronger emphasis on reducing dispersion error within thepropagator works best. This work presents the first introduction of phase and amplitude componentsinto the loss function for training deep neural networks for wave propagation.",
  ": Linear convection problem: Exact solution (left), AB-CRAN solution with n = 2 (center)and error e = |u u| (right) for the testing parameter test = 1.0125 in the space-time domain": "pass, noise is added to the input data before passing it through the encoder-decoder pipeline. Thedecoder is then tasked with reconstructing the original noise-free data, optimizing the mean squarederror between the noisy inputs and the clean data, resulting in a denoising decoder loss. In thesecond stage, the projected data from the encoder are processed through the propagator to generate atime-evolved output, and the corresponding loss is decomposed into dissipation and dispersion errors,as explained in the above section. The entire model is optimized using the AdamW optimizer and a cosine annealing warm-restart scheduler . We also implemented an early stopping strategywith a patience threshold of 50 epochs.",
  "Results": "We randomly selected a parameter from the test set for evaluation. The AB-CRAN architecture wasprovided with data from the linear convection equation, using = 1.0125 as input. illustratesthe exact solution alongside the AB-CRAN approximation for this test case. The results demonstratethat the AB-CRAN model, trained with the proposed loss decomposition strategy, captures wavedynamics with high fidelity, accurately preserving key features of the solution. Our loss decomposition strategy significantly improves phase accuracy compared to the conventionalmean squared error (MSE) loss. As shown in a, predictions based on MSE exhibit noticeablephase lag over time, highlighting the inability of traditional loss functions to maintain phase alignment.In contrast, our method preserves phase throughout the prediction horizon. Furthermore, bdemonstrates that the proposed approach extends the prediction horizon by effectively mitigatingerror accumulation. By separately addressing phase and amplitude errors, the model achieves greaterstability and accuracy, particularly for long-term forecasts. This decomposition-based approach provides a robust and generalizable framework for long-termforecasting of physical systems. By isolating and addressing phase and amplitude errors, it overcomeslimitations inherent in traditional MSE-based methods. The strategy reduces error accumulation,ensuring phase alignment and amplitude stability over extended time horizons. These improvementsmake the method broadly applicable to various physical forecasting tasks, such as wave propagation,fluid dynamics, and climate modeling, establishing a new standard for accuracy and reliability inlong-horizon predictions.",
  "Conclusion": "In this work, we presented a novel loss decomposition strategy that addresses the limitations oftraditional mean squared error (MSE)-based loss functions for long-term predictions in physicalsystems. By separating the loss into phase and amplitude components, our approach independentlytackles dispersion errors (phase misalignment) and dissipation errors (amplitude inaccuracies). Thistargeted decomposition significantly enhances both the accuracy and stability of predictions, extending",
  "(b)": ": Accuracy assessment for wave prediction: (a) Comparison of MSE-based loss and Loss-decomposition-based training prediction for t= 0.036, 0.392 (b) Error vs. time-horizon plot com-paring MSE-based and loss-decomposition-based training predictions, with a time-horizon of 10time-steps the prediction horizon while maintaining physical consistency. Our results demonstrate that theproposed method outperforms standard MSE-based approaches, offering substantial improvementsfor tasks such as wave propagation modeling. The flexibility of this framework makes it broadly applicable to various forecasting tasks in physicaldomains, including flow dynamics, ocean acoustics, and climate modeling. By addressing phaseand amplitude errors independently, our method ensures reliable long-term predictions for complexsystems. This study underscores the potential of tailored loss functions to advance the modeling andsimulation of complex dynamical systems, providing a promising foundation for future research indata-driven physical modeling and scientific computing.",
  "Indu Kant Deo, Rui Gao, and Rajeev Jaiman. Combined spacetime reduced-order model withthree-dimensional deep convolution for extrapolating fluid dynamics. Physics of Fluids, 35(4),2023": "Indu Kant Deo, Akash Vankateshwaran, and Rajeev Jaiman. Predicting wave propagation forvarying bathymetry using conditional convolutional autoencoder network. In InternationalConference on Offshore Mechanics and Arctic Engineering, volume 87844, page V006T08A038.American Society of Mechanical Engineers, 2024. Indu Kant Deo, Akash Venkateshwaran, and Rajeev K Jaiman. Continual learning of range-dependent transmission loss for underwater acoustic using conditional convolutional neural net.arXiv preprint arXiv:2404.08091, 2024."
}