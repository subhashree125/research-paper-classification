{
  "Abstract": "Are generative pre-trained transformer (GPT) models only trained to predict thenext token, or do they implicitly learn a world model from which a sequence isgenerated one token at a time? We examine this question by deriving a causalinterpretation of the attention mechanism in GPT, and suggesting a causal worldmodel that arises from this interpretation. Furthermore, we propose that GPT-models, at inference time, can be utilized for zero-shot causal structure learningfor in-distribution sequences. Empirical evaluation is conducted in a controlledsynthetic environment using the setup and rules of the Othello board game. AGPT, pre-trained on real-world games played with the intention of winning, istested on synthetic data that only adheres to the game rules. We find that the GPTmodel tends to generate next moves that adhere to the game rules for sequences forwhich the attention mechanism encodes a causal structure with high confidence. Ingeneral, in cases for which the GPT model generates moves that do not adhere tothe game rules, it also fails to capture any causal structure.",
  "Introduction": "In recent years, the generative pre-trained transformer (GPT) model [Radford et al., 2018] hasdemonstrated high-quality generative capabilities, as perceived by humans. Although this model istrained to generate one token at a time, it have been demonstrated to perform a range of tasks beyondnext-token prediction, such as visual understanding and even symbolic reasoning [Liu et al., 2024,Team et al., 2023, Chowdhery et al., 2023]. Are these emergent abilities [Li et al., 2023] or merely amirage resulting from the choice of metric and task [Schaeffer et al., 2024]? In this paper we suggest that there is no restriction in the GPT architecture for learning conditionalindependence (CI) relations between tokens in a sequence. Moreover, under certain assumptions, acausal structure is directly entailed from these CI relations. One may ask whether this absence ofrestriction results in implicitly learning a causal model of the world during the pre-training procedureof GPT. Assuming that both a causal world model and a model based on surface statistics are sufficientsolutions, one possibility is that a causal world model is a more compact representation, making itmore likely to be learned during pre-training (Occams razor). While surface statistics models maydistribute weights uniformly across a wide range, a causal structure imposes constraints that narrowthe range of possible weight. If so, what are the assumptions underlying this causal world model? Recently, Rohekar et al. derived a causal interpretation for unmasked self-attention in BERTmodels [Devlin et al., 2019]. In this paper we follow a similar approach, with several differences, andpropose a causal interpretation of the masked attention mechanism in GPT. We additionally define acorresponding causal world model. The ABCD method Rohekar et al. is adapted and usedto learn causal structures of which the induced dependency-relations are encoded in the attentionmatrices in GPT. We then ask whether errors generated by GPT are correlated with the uncertainty inrepresenting the causal structure by the attention matrices. To this end, we define a metric based onthe entropy of p-values of CI tests that are used for inferring the causal structures.",
  "arXiv:2412.07446v1 [cs.AI] 10 Dec 2024": "Recent work examined the internal process of large language models and examined whether a worldmodel is implicitly learned using a well-defined and constrained setting, such as in the Chess gamesetting [Toshniwal et al., 2022] and Othello board game setting [Li et al., 2023]. For the Othelloboard game setting, Li et al. demonstrated that the board state can be inferred from attentionmatrices in GPT, and Nanda et al. showed that even a linear classifier suffices to reconstructthe state of the board game from the attention matrices. They claim an emergent world model inGPT. Nevertheless, they do not provide explanation on how the board game is encoded within theattention matrices and why the attention mechanism can represent the board state. In essence, theydo not provide an explanation to the apparent emergence of the world model. In addition, the worldmodel they reconstruct (board game state) is specific only to the domain for which the GPT modelwas trained and lacks recovering the generative mechanism of the token-sequences. In this paper, we consider the structural causal model as the world model, which describes thegenerative process and is generally applicable for a range of applications (not domain specific, suchas Othello board state, used by Li et al. ). We explore whether GPT is able to capture propertiesof this world model, which may explain it apparent emergence.",
  "Attention in GPT": "Attention is a mechanism that estimates network weights with respect to the context in an inputsequence of tokens [Schmidhuber, 1992]. In a GPT model, which is based on the decoder part of theTransformer architecture [Vaswani et al., 2017], an attention layer estimates an nn lower-triangular(masked) attention matrix A given an input sequence of n tokens. The input sequence is in the formof an n d matrix Y, where the i-th row vector is Y(i, ), is an embedding (representation) of thei-th token in d dimensions. The attention matrix is estimated by A = softmax(YWQKY), suchthat A is lower triangular and the rows sum to 12. In addition to the attention weights, the attentionlayer calculates a values matrix, V = YWV , where row V(i, ) is the value vector of the i-th token.Then, the output embeddings areZ = AV,(1) where the i-th row, Zi, is the embedding of the i-th output token. In a GPT, several attention layers arestacked, and pre-trained such that the i-th output embedding in the last layer predeicts the (i + 1)-thinput token. That is, predicts the next input token. It is important to note that in the GPT architecture, the embedding of one token is influenced byanother token only by the attention matrix, A. In addition, note that an attention matrix A is estimateduniquely for each input sequence of tokens, using weight matrices {WQK, WV } learned commonlyfor all in-distribution input sequences.",
  "Structural Causal Model": "A structural causal model (SCM) is a model that can encode causal mechanisms in a domain[Pearl, 2009, Spirtes et al., 2000, Peters et al., 2017] and explain data samples generated from thesecausal mechanisms [Pearl and Mackenzie, 2018]. An SCM is a tuple {U, X, F, P(U)}, whereU = {U1, . . . , Um} is a set of latent exogenous random variables, X = {X1, . . . , Xn} is a set ofendogenous random variables, F = {f1, . . . , fn} is a set of deterministic functions describing thevalues X given their direct causes, and P(U) is the distribution over U. Moreover, each endogenousvariable Xi has exactly one unique exogenous cause Ui (m = n). The value of an endogenousvariable Xi, i [1, . . . , n] is determined by",
  "dK, where generally the weight matrices WQ and WK arelearned explicitly and dK is the number of columns in these matrices [Vaswani et al., 2017]": "where P ai is the set of direct causes (parents in the causal graph) of Xi, and left-arrow indicatesassignment resulting from the cause-effect relation. A graph G corresponding to an SCM consists ofa node per variable, and directed edges for direct cause-and-effect relations that are evident from F. In this paper we employ a linear-Gaussian SCM having a directed acyclic graph (DAG). In thesemodels each variable is determined by a linear combination of its direct causes and an independentlydistributed additive noise determined by a corresponding normally distributed exogenous variable. For a linear-Gaussian SCM let G be a weight matrix, where G(i, j) is the weight of parent (directcause) node Xj linearly determining the child (direct effect) node Xi. Node Xk is not a parent of Xiif and only if G(i, k) = 0. In addition, U N(U, CU), where in this paper we assume CU is adiagonal matrix. The set of functions F is defined such that i [1, . . . , n],",
  "k=0Gk.(5)": "It can be seen that element (i, j) represents the cumulative effect of Xj on Xi via all directed pathshaving length up to n 1. The equivalent weight of a directed path from Xj to Xi is the productof the weights of all edges on that path, and the cumulative effect via all the paths is the sum overequivalent weights of distinct directed paths from Xj to Xi. Note that even if some of the nodes arelatent confounders is still (I G)1 triangular because, by definition, latent confounders do not haveancestors and are first in a topological ordering. Equation 4 represents a system with input U, outputX and weights (I G)1. The covariance matrix of the output is (see Appendix A, Equation 10)",
  "A Relation between GPT and SCM-based World Model": "Rohekar et al. derived a causal interpretation of models [Devlin et al., 2019] (also demostratedfor recommender systems [Nisimov et al., 2022]). We follow a similar approach, with severalimportant modifications and extensions, to derive a causal interpretation to GPT. First, unlike BERT,which is pre-trained to predict the input sequence, GPT is pre-trained to predict the next tokens inthe sequence. Given an input sequence of tokens, {t0, . . . , tn1}, GPT predicts tokens {t1, . . . , tn}.An attention matrix A and the corresponding values matrix V have n rows corresponding to inputtokens {t0, . . . , tn1} and the output embeddings of of these tokens are the rows of matrix Z = AV.Note that V = YWV , where WV is a weight matrix fixed for all input sequences, and Y is inputembedding of a specific sequence tokens. Each column of WV can be viewed as an independentvector onto which the input embeddings are projected. That is V(i, j) is the projection of token tiinput embedding Y(i, ) on, common to all in-distribution sequences, vector WV (, j). At inference,each attention matrix of the last attention layer, A, is extracted and a lower uni-triangular matrix iscalculated, D1A, where D diag(A). Then estimate the covariance matrix",
  "C =D1AD1A.(7)": "Note that unlike Rohekar et al. , which proposed C = AA for unmasked self-attention,we utilize the triangular form of the masked attention in GPT to revert the attention normalizationperformed by the softmax and obtain a uni-triangular form. Thus, this covariance matrix allowsus to treat properties calculated from different attention matrices in a similar manner. In this paper(.2 and .3), properties we calculate are based on p-values when testing conditionalindependence relations between tokens. Next, following Rohekar et al. we relate each token toan endogenous node in an SCM, and CU = I from the central limit theorem [Rohekar et al., 2024].Thus, equate covariance C = CUD1AD1A =(I G)1 (I G)1,(8) where both D1A and (I G)1 are lower uni-triangular matrices, and the (i, j) elements, i > j,of these matrices have the same meaning, influence of token/node j on token/node i. Finally, sinceGPT is pre-trained to predict tokens {t1, . . . , tn} given input tokens {t0, . . . , tn1}, and since theonly cross-token influence on embeddings is in the attention layers, the last attention layer capturesthe causal structure underlying the output tokens. Earlier attention layers transform embeddings of{t0, . . . , tn1} to values, V, which are equivalent to instantiations of exogenous variables, U inSCM. This follows from equating Equation 1 and Equation 4, where D1A = (I G)1. In light of the causal interpretation of GPT, one important question is what is the causal world modelthat is supported by the GPT architecture. Often, a single causal structure is assumed to governa domain. In contrast, the causal world model that is entailed from the causal interpretation ofGPT assumes a distinct structural causal model for each in-distribution sequence. Specifically, in acausal world model supported by a GPT with k-heads in the last attention layer, each in-distributionsequence is assumed to be generated by an ensemble of k distinct SCMs. In addition, for a given head, the causal structure over a sequence of tokens {t1, . . . , tn} is equal tothe sub-graph over these tokens for all in-distribution extensions of the sequence. That is, given asequence of tokens {t1, . . . , tn} and a corresponding graph structure Gn, observing any next token,tn+1, such that {t1, . . . , tn, tn+1} is in-distribution, should not violate causal relations in Gn andmay only reveal relations between tokens {t1, . . . , tn} and token tn+1.",
  "GPT for Zero-Shot Causal Structure Learning": "The causal interpretation presented in this paper leads to a view in which each attention modulerepresents associations (correlations) between input tokens that are induced by the underlying causalstructure. Although this allows only rung-1 inference in the ladder of causation [Pearl and Mackenzie,2018], under certain assumptions, many of the underlying causal relations can be extracted, evenin the presence of latent confounders and selection bias [Spirtes et al., 2000]. These relations aregenerally represented in a type of causal structure called partial ancestral graph (PAG) [Richardsonand Spirtes, 2002]. We follow a procedure called ABCD, proposed by Rohekar et al. withseveral modifications. First, since the causal (topological) order is given (restricted by the maskedattention in GPT) we can apply causal discovery recursively to efficiently learn the causal structure.To this end we call the ICD algorithm [Rohekar et al., 2021] (other algorithms can be used [Spirteset al., 2000, Colombo et al., 2012, Claassen et al., 2013, Yehezkel and Lerner, 2009, Rohekar et al.,2018, Nisimov et al., 2021]) to reconstruct a causal structure in each recursive iteration (Algorithm 1).The result is a PAG structure. Thus, a causal structure for a specific output sequence can be learnedin a zero-shot manner directly from the attention matrix.",
  "Causal Structure Confidence": "In this section we derive a metric that describe how compatible a sequence is with the causal worldmodel implicitly encoded by GPT. Given an output sequence of tokens, S, and a causal structureG recovered from the last attention layer A, can we score the confidence in this causal structure?Recall that in the proposed world model each sequence has its own causal structure, and each causalstructure may have latent variables. It is not clear how to calculate likelihood P(S|G). We propose the following approach. A causal structure-learning algorithm performs multiple statisti-cal tests of conditional independence (CI) using the covariance matrix estimated from the attentionmatrix. These CI tests calculate p-values and compare them against a predetermined threshold ofsignificance level (). It is important to note that there is a one-to-one correspondence between the results of these CI test and the entailed causal structure. That is, a causal structure can be representeduniquely by a set of CI tests and their results. Hence, we propose a scoring function based on thedistribution of these p-values to evaluate the confidence in a structure learned from a given attentionmatrix. A complete undirected graph corresponds to lack of knowledge about causal relations. Gener-ally, causal structure-learning algorithms prune edges from this graph based on statistical CI testsbetween pairs of variables (tokens in our case). The removal of edges between independent variablesthen may entail causal relations between other variables [Zhang, 2008]. Let p = {p1, . . . , p} a setof all p-values computed as part of causal structure learning. The null-hypothesis is independence,where p-values greater than the significance threshold, , correspond to edges removed from thecomplete graph. We denote pind = {p : p p and p }, and pdep = {p : p p and p < }.Since p-values are uniformly distributed under the null hypothesis, we expect the entropy of p-valuescorresponding to independence, redundant relations (spurious correlations), Hind to be higher formatrices that correspond to a structure compared to those that do not. In addition, we expect thedistribution of p-values smaller than the significance level to be weighted towards zero. Hence,entropy of p-values corresponding to dependence relations, Hdep is expected to be lower for matricesthat correspond to a structure compared to ones that do not. We therefore define the followingconfidence score given an attention matrix A,R(A) = Hind Hdep,(9)where Hind =",
  "Experiments and Results": "We use an experiment setup in which the world layout and rules are well defined and known but werenot used during training with samples from this world (legal samples). We measure how well attentionin the learned GPT model represents a causal model using Equation 9, and whether it is correlatedwith the ability of the model to generate tokens that adhere to the world rules (legal sequences).",
  "Setup": "We examine a GPT model trained by Li et al. , for predicting the next move given a sequenceof consecutive moves in the Othello strategy board game. They trained the model on approximately132,000 real-world sequences, where it is assumed the players played with the intention of winning.No information about the game board layout or game rules was used in their training process.For example, positional encoding was not used. In our experiments we use a test set that is notin-distribution with respect to the training set, but in-distribution with respect to the game rules.As a test set we use 1,000 random sequences of legal moves. That is, each sequence consists ofmoves that adhere to the Othello game rules but without considering any strategy of winning thegame as in the training set. In other words, the support of the test distribution is not a subset of thesupport of the training distribution, supp(Ptrain) supp(Ptest). This enables evaluating whetherthe model implicitly encoded the game rules. In we plot the accuracy of the model ingenerating a legal next move (vertical axis) as a function of test sequence size (horizontal axis).Although the average accuracy of the model is 95% (dashed red line), it is not uniformly distributedacross different sequence lengths. For example, given a sequence of 15 moves, GPT generates alegal 16-th move in 88% of the times (adheres to the game board state and rules). It is evident thatthe accuracy is significantly lower for input sequence lengths in the range (lower than theaverage 95%). By definition of the Othello game rules, at the beginning of a game there are onlyfour legal moves, and as this game unfolds, the number of possible legal moves increases beforefinally decreasing again as the number of vacant spaces on the board decreases. It might be thatmemorization of surface statistics can take place at the beginning and end of the game. We thereforereport experiment results for input sequences with sizes in the range (gray area) wherethe accuracy is lower than the average. For implementation and empirical evaluation we used theCausality Lab: github.com/IntelLabs/causality-lab package.",
  "sequence length": ": Average difference of structural confidence score, R, between legal and illegal move genera-tion (vertical axis) for different input-sequence length (horizontal axis). Error bars are 95% confidenceinterval. Confidence score are calculated from p-values of: a) all unconditional independence testscalculated directly from raw attention values, b) all CI tests having exactly one conditioning node,c) only tests from cases a) and b), d) CI-tests, without limiting the conditioning set sizes, needed toreconstruct a causal structure. Sequence lengths with statistically significant differences in confidencescores are denoted by green markers.",
  "Ablation Study": "Next we examine if conditional independence tests from which the causal structure is entailed providean advantage over pair-wise correlations directly represented by elements in the attention matrix.To this end we calculate the confidence score using p-values of a) all pair-wise correlations (fromraw attention-matrix elements)empty conditioning set, b) CI-tests having exactly one node in theconditioning set, c) all CI-tests having empty or exactly one node in the conditioning set, and d)CI-tests used to reconstruct the causal structure without limiting conditioning set sizes. The results aredepicted in , with corresponding sub-figures. Vertical axis describe the difference betweenstructural confidence averaged over legal and illegal predictions. Error bars indicate 95% confidenceintervals. Horizontal axis indicate sequence length. It is evident that relying solely on raw attentionvalues, case a), the difference between legal and illegal generated tokens in not statistically significant,except for sequence length 20. Relying solely on CI-test with exactly one node in the conditioningset, case b), the difference between the structural confidence is positive for all tested sequence lengthsbut statistically significant only for sequences lengths 17. When employing pair-wise correlationsand CI tests with exactly one node in the conditioning tests, the result is statistically significant forboth sequence lengths 17 and 20, implying that these two types of tests are complementary. Finally,it is evident that using CI-tests needed to learn the causal graph, without limiting the conditioning setsizes, case d), provide the best results where sequence lengths in are statistically significantand the difference between legal and illegal is positive in all sequence lengths.",
  "Conclusions": "We presented a causal interpretation of GPT that may explain apparent emergence of world model inrecent studies. Following this interpretation, we described a method that utilizes the the triangularform of the attention matrices in GPT to efficiently recover the causal structures for input sequences(zero-shot causal-discovery). Finally, using experiments in a controlled environment of the Othelloboard game we demonstrated that GPT implicitly learns to represent causal structures in attentionheads. Specifically, in cases where the confidence in recovering any structure from the attentionmatrices is low, GPT generally fails to predict a token that adheres to the Othello board game rules. Infuture work, these results may provide insights on the sources of hallucination in GPT-based modelsand may lead to deriving a method to detect them.",
  "Sequences Lengths 30": ": Legal move generation accuracy (vertical axis) as a function of structural confidence scoreR (horizontal axis). Horizontal limits for each point indicates interval of R in which accuracy wasaveraged. Horizontal dotted red line indicates average accuracy. For sequences of lengths greaterthan 15 the accuracy monotonically increases with the structural confidence score, whereas this trendis not evident for sequences having length 10.",
  "Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances inneural information processing systems, 36, 2024": "Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, RaduSoricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capablemultimodal models. arXiv preprint arXiv:2312.11805, 2023. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, AdamRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1113,2023. Kenneth Li, Aspen K Hopkins, David Bau, Fernanda Vigas, Hanspeter Pfister, and Martin Watten-berg. Emergent world representations: Exploring a sequence model trained on a synthetic task. InThe Eleventh International Conference on Learning Representations, 2023.",
  "Thomas Richardson and Peter Spirtes. Ancestral graph markov models. The Annals of Statistics, 30(4):9621030, 2002": "Raanan Y Rohekar, Shami Nisimov, Yaniv Gurwicz, and Gal Novik. Iterative causal discovery inthe possible presence of latent confounders and selection bias. Advances in Neural InformationProcessing Systems, 34:24542465, 2021. Diego Colombo, Marloes H Maathuis, Markus Kalisch, and Thomas S Richardson. Learning high-dimensional directed acyclic graphs with latent and selection variables. The Annals of Statistics,pages 294321, 2012.",
  "BRecursive Causal Discovery for GPT": "In Algorithm 1 we describe an efficient causal discovery algorithm that utilizes the causal orderrestricted by GPT by masking attention matrices, forcing them to a lower-triangular form. That is,in a sequence of tokens, {t1, . . . , tn}, token t is not an ancestor of token t1 for all > 1. In line2, the last token is popped from the sequence and placed in tn resulting in a shorter sequence S.Then, a recursive call is made in in line 3 to learn the structure over tokens in S. Note that since it isensured that tn is not an ancestor of any token in S the skeleton and v-structure relations of G isensured not to be change when adding tn to the graph [Spirtes et al., 2000]. In lines 46 token tn isconnected to every node in G. Finally, using the ICD algorithm [Rohekar et al., 2021] edges betweentn and the rest of the graph are learned (removed if conditional independence is found) and the graphis oriented [Zhang, 2008]."
}