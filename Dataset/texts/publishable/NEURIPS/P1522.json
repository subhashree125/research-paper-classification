{
  "Abstract": "Recent work in database query optimization has used complex machine learningstrategies, such as customized reinforcement learning schemes. Surprisingly, weshow that LLM embeddings of query text contain useful semantic informationfor query optimization. Specifically, we show that a simple binary classifierdeciding between alternative query plans, trained only on a small number of labeledembedded query vectors, can outperform existing heuristic systems. Although weonly present some preliminary results, an LLM-powered query optimizer couldprovide significant benefits, both in terms of performance and simplicity.",
  "Introduction": "Query optimization is the task of transforming complex SQL queries into efficient programs (Selingeret al. ), referred to as query plans. Optimizers represent substantial engineering efforts (Giak-oumakis and Galindo-Legaria ), often spanning hundreds of thousands of lines of code (Graefeand McKenna ). Most query optimizers today are driven by complex, manually-written heuris-tics. Despite significant advancements, query optimizers (QOs) are far from perfect, frequentlymaking costly mistakes (Leis et al. ). Recent work has shown that machine learning techniques can be used to steer query optimizers in theright direction, helping the optimizer determine which plan to select for query execution. Researchershave used supervised learning (e.g., Woltmann et al. ), reinforcement learning (e.g., Marcuset al. ), and hybrid approaches (e.g., Anneser et al. ) to effectively steer optimizers.However, each approach performs sophisticated feature engineering on statistics kept internally by thedatabase, and, as a result, requires complex and deep integration with the underlying query optimizer.This has a number of downsides that have hindered practical adoption (Zhu et al. ). In this extended abstract, we present initial results for LLMSTEER, a simpler approach to steeringQOs. Instead of manually engineering complex features from plans or data statistics, we use a largelanguage model (LLM) to embed raw SQL submitted by the database user. We then train a supervisedlearning model on a small labeled set of queries to predict the optimal direction in which to steer theQO. This places the entire steering component outside of the database, simplifying integration.",
  "arXiv:2411.02862v1 [cs.DB] 5 Nov 2024": "As database experts, we did not expect this simple approach to work. Common wisdom withinthe database community is that complex features such as cardinality estimates (Kipf et al. )or operator models (Heinrich et al. ) are required for the task. Experimentally, we showthat LLMs are capable of making these decisions without any such information. We found no simpleexplanation for LLMs apparent success; the LLM-based approach was insensitive to at least somesyntax changes, and worked across two different workloads. In , we describe LLMSTEERand its simple, yet powerful, design. In , we present results from initial experiments. Weconclude in with a discussion of future directions and questions left unanswered.",
  "LLMSTEER: A Surprisingly Simple Approach to Query Steering": "Query hintsGiven a SQL query, an optimizer can generate several plan variants, each of whichmay use different operators or data access patterns. Hints are optional keywords or clauses that canbe inserted into a query to guide the optimizer into generating plans with specific characteristics,providing a coarse-grained way to influence a querys execution plan. For example, a hint mayindicate to the optimizer that it should only consider plans with hash joins, make use of a helpfulindex, or limit parallelism. Optimizer steeringOf course, determining the correct hint for a query requires a priori knowledgeof the data and workload. Steering an optimizer is the task of selecting a hint or set of hints (\"hint set\")for a particular query such that the selected plan results in reduced or minimal latency. Although hintscan be effective in fine-tuning database performance, selecting hints can be extremely complicated,and providing the optimizer with incorrect hints can severely degrade query latency. As a result, thepractice of manually issuing a hint is used sparingly, and is commonly restricted to experts mostfamiliar with the underlying data (Marcus et al. ). Problem definitionLLMSTEER attempts to automatically determine an appropriate hint for aquery once that query is submitted to the system. Given a small set of labeled embedded queryvectors, LLMSTEER trains a classification model to map unseen queries to an appropriate hint. Weevaluate the quality of LLMSTEERs decisions using two common metrics (van Renen et al. ):the change in total and P90 tail latency. When executing a query workload, total latency is thecumulative execution time of all queries. P90 tail latency is the 90th percentile query latency. LLMSTEERAn overview of LLMSTEER is depicted in . When a query is first submitted toLLMSTEER ( 1 ), the raw SQL is embedded using a large language model, producing an embeddingvector ( 2 ). Since our goal is to train a supervised learning model using a small number of examples k,and since LLMs often use high d-dimensional embeddings (i.e., d > k), we next apply dimensionalityreduction ( 3 ). The final feature vector is passed through a classifier to determine the choice ofoptimal hint for the given query ( 4 ). The hint is then combined with the original SQL query ( 5 )and submitted to the database management system (DBMS) where a query plan is generated andexecuted ( 6 ).",
  "Initial Results": "We set out to answer two preliminary questions: first, can LLMSTEER find hints for queries thatoutperform existing query optimizers? Second, since LLMSTEER operates on SQL syntax directly,how robust or sensitive is LLMSTEER to non-semantic syntactic changes in SQL queries (e.g.,changes in indentation or whitespace)? Experimental setupData used in this work contains 3246 SQL queries, 113 originating from Leiset al. s Join Order Benchmark (JOB) and the remaining 3133 are the \"core\" subset of Negiet al. s Cardinality Estimation Benchmark (CEB).1 To obtain reliable estimates of latencies,each query is executed under a given hint set 5 times and latencies are averaged over the runs to yielda final latency that is used (Yi et al. ). All queries are executed using PostgreSQL version16.1. A collection of 48 hint sets is considered, the same as those used by Marcus et al. and Heinrich et al. . To generate embeddings, we use OpenAIs text-embedding-3-largemodel. Given the latencies of queries under each hint, we determine the hint with the highest potentialfor improvement if applied perfectly, and we use this hint as our alternative plan (i.e., LLMSTEERdecides between the default plan and the alternative plan produced by giving the selected hint tothe DBMS). Afterward, a binary label is generated for each query by determining which of the twopossibilities (the default plan or the alternative plan) produces the query plan with lower latency.Approximately 30% of queries in the data perform better with the alternative hint. We considered a number of models and found support vector machines with the RBF kernel trained on120 principal components most performant; models trained on 5, 50, and 120 principal componentswere evaluated, preserving approximately 50%, 80%, and 90% of variance in the original embeddings.Class weights were used, defined as the ratio of class frequencies, otherwise no hyperparametertuning was performed and default values were used for all models the final SVM model usedregularization strength C = 1.0 and a kernel coefficient of = 1/1202X. To train models, weemployed a 10-fold cross-validation procedure with stratified random sampling. Our code, alongwith the data used in the analysis and produced embeddings, are available on GitHub.2 OptimalLLMSteer Alternative PostgreSQL Steering strategy Total Latency (s)",
  ": Mean LLMSTEER performance on 10-fold cross-validation testing workloads": "Experiment 1: LLMSTEER vs. PostgreSQL Optimizer shows the distribution ofquery latencies using four different strategies: the optimal (unknowable in practice), the defaultplan, the alternative plan, and LLMSTEER. LLMSTEER falls short of the optimal steering strategy,but effectively combines the benefits of PostgreSQL and the alternative hint. PostgreSQLs defaultplan outperforms the alternative plan at the lower end of the latency distribution, highlighted byPostgreSQL having a lower median latency and reaching 50% of total workload latency earlierin the empirical CDF (IV). These dynamics invert at the higher end of the distribution, where thealternative has a significantly lower P90 (II). LLMSTEER tracks the performance of the optimalstrategy in the latter half of the latency distribution, saturating faster than both PostgreSQL and thealternative (I). The system also has a smaller performance gap to PostgreSQL and the optimal relativeto the alternative plan earlier in the latency distribution, achieving a lower median latency on testingworkloads (III). Thus, LLMSTEER can be seen as trading a small increase in median latency for",
  "a large reduction in P90 and total latency, a tradeoff many practitioners would eagerly accept (vanRenen et al. )": "We evaluate the performance of LLMSTEER against the native PostgreSQL optimizer on P90 and totallatency in as well. LLMSTEER represents a significant improvement on the PostgreSQLdefault, reducing total and P90 latency by 72% on average across testing cross-validation folds.LLMSTEER performs near optimal relative to the steering strategy that selects the correct hint setfor every query, achieving a total and P90 latency that is only 30% and 12% higher. Additionally,LLMSTEER shows stability, with performance gains consistent across testing workloads with minimaldeviation. PostgreSQLAlternative Predicted hint PostgreSQL",
  ": Aggregate confusion matrix ofLLMSTEER across cross-validation testingworkloads": "Total and P90 latency are generally accepted mea-sures of performance within the database community,and are tied directly to the performance and optimiza-tion outcomes of the system, so we place a largeremphasis on the relative reduction in these metrics.However, we also assess the performance of LLM-STEER on traditional classification metrics. The sys-tem achieves a mean recall of 0.9177 ( = 0.0202),precision of 0.5174 ( = 0.0183), and AUROC of0.8515 ( = 0.0143). The mean accuracy of LLM-STEER is 0.7113 ( = 0.0205), surpassing the perfor-mance of the naive classifier (i.e., the classifier thatalways selects the prior) marginally, which would havean accuracy of 0.6929. shows the aggregateconfusion matrix of LLMSTEER; true-positives oc-cur where the true hint and predicted hint are boththe alternative, while true-negatives occur where thetrue hint and predicted hint are both PostgreSQL. Thesystem prioritizes minimizing false-negatives at the expense of a higher false-positive rate, resultingin improved performance due to the asymmetric cost associated with steering the QO incorrectly. Experiment 2: Robustness to Syntactic ChangesSQL queries in original training and testingworkloads are structured as single-line declarative statements. In practice, database users willrarely structure queries like this, as it impedes the ability to create complex queries and debug SQLstatements. There are many ways to alter a query without changing its semantic meaning (Listings 1& 2), and LLMs are likely to produce different embeddings for queries based on their syntax. Toassess robustness to such syntactic changes, we modified each query in various ways. We refer to\"Syntax A\" as the original phrasing of each query, and introduce \"Syntax B\" and \"Syntax C,\" whichuse newline characters at the end of keyword blocks (i.e., SELECT, FROM, WHERE) and use eitherspaces or tabs respectively for indentation. shows that LLMSTEER exhibits robustness to atleast these classes of syntax changes. Notably, when LLMSTEER trained on original queries (SyntaxA), it was still effective on workloads with Syntax B and C; despite a 28% increase in total latencywhen tested on syntax B and 27% when tested on Syntax C, this still represented a reduction of 64%relative to PostgreSQL. LLMSTEER performed best when tested on a workload with the same syntaxas it was trained on, but when trained on queries with Syntax B and C in particular, we observedminimal decrease in performance regardless of the syntax used in the testing workload. Syntax ASyntax BSyntax C Testing workload syntax Total Latency (s) PostgreSQLAlternativeOptimal",
  "(c) Trained on syntax C": ": Mean total latency of LLMSTEER trained on augmented syntaxes across 10-fold cross-validation testing workloads. Syntax A represents original queries, Syntax B represents formattedqueries with spaced indention, Syntax C represents formatted queries with tabbed indentation. Challenge: Scaling to more hintsUnfortunately, our simplified approach did not scale. Con-sidering PostgreSQLs 48 hint sets, there are too few queries associated with each class, makingit challenging for a classifier to learn the complex relationship between queries and hints. Thedistribution of queries across the collection of hints is also skewed, with the most frequently optimalhint set occurring 525x more often than the least frequently optimal. Despite this, even in the absenceof a more complex strategy, the ability to steer the optimizer between just two alternatives leads tosignificantly improved performance.",
  "Conclusion and Future Work": "In this extended abstract, we present LLMSTEER, demonstrating its usage in effectively steeringquery optimizers. Benchmarked against PostgreSQLs default query optimizer, results from initialexperimentation show that LLMSTEER is capable of reducing total and tail latency by 72% onaverage. We were surprised to discover that LLMSTEER worked, since established wisdom of thedatabase community indicates that the system should not have been successful. With this, we havefar more questions than answers.",
  "We believe that LLMSTEER opens up a number of future research directions": "The right embedding model?The quality of embeddings are often highly dependent on the down-stream task and the LLM used. At the time of this work, OpenAIs text-embedding-3-large didnot rank within the top 30 models on the overall massive text embedding benchmark (MTEB) (Muen-nighoff et al. ). There may be models which can create richer representations of SQL queries,containing additional semantic information that may be helpful in steering optimizers. It is unclearwhether open source embedding models with fewer parameters can be just as effective as their largercounterparts, and quantization techniques also present a promising alternative to using models withstrictly fewer parameters. The effects of quantization on embedding quality are yet to be explored inthe context of query optimization, but methods like Frantar et al. s GPTQ, Xiao et al. sSmoothQuant, and Lin et al. s AWQ may be essential to preserving performance gains inproduction systems. Fine-tune an LLM?Given these preliminary results, it seems plausible that attaching a classifica-tion head to a language model and allowing the model to use its own activations in the selection ofhints could be effective. More broadly, we question whether LLMs can be fine-tuned to perform thetask of steering query optimizers? That is, can we teach an LLM to select the optimal hint given aquery in a few-shot setting, or by fine-tuning an LLM on SQL directly, and would this prove to bemore effective than LLMSTEER? Further, what other features can we provide to an LLM to enhanceperformance on this task for example, could an LLM outperform existing methods if surfacedwith a text representation of query plans?",
  "While we are cautiously optimistic about using LLMs for query optimization, we plan to investigateseveral additional possible explanations for LLMSTEERs surprising behavior": "SyntaxAdditional investigation into how syntax impacts performance is necessary. For example,assessing the effects of comma-first notation, inclusion of comments, formatting of keywords andidentifiers (e.g., lowercase, uppercase, title case), and any combination of these modifications onthe ability for models to learn and generalize is of practical importance. Obviously, being robust tosimple semantic-preserving reformulations of queries is critical for any real-world deployment. Is the benchmark in the LLMs training set?There are still multiple dimensions on whichLLMSTEER must be evaluated, providing sufficient cause for caution. It is unclear if the LLMhas been exposed to the query benchmarks used in this work; to establish that LLMSTEER has theability to generalize, stronger evidence is needed to determine whether the LLM has trained on, andoverfit to, these datasets. As a result, we question how LLMSTEER might perform on novel SQLqueries that are significantly different from existing datasets? Current large language models areinternet scale, making evaluation increasingly difficult the creation of new query benchmarks isnontrivial, and although beneficial to the database community, once a new benchmark is released, thenext generation of LLMs may be trained on the data, confounding the results of future studies. Thiscycle presents a unique challenge with no clear solution. However, ablation studies and perturbationanalysis may yield compelling results, providing key evidence that further validates the performanceand generalizability of the system. Taking a hint (or three)Developing an understanding of any limitations related to scaling thesystem beyond two hints will be helpful in extending the utility of this system. In future work,performance gains must also be compared to a variety of query optimizers and current SOTA methodssuch as those using reinforcement learning. Latency of query processing is not captured in this work evaluating overhead and the overall latency impact in the critical path for queries is necessary tojustify the use of embeddings.",
  "L. Giakoumakis and C. A. Galindo-Legaria. Testing SQL Servers Query Optimizer: Challenges,Techniques and Experiences. IEEE Data Eng. Bull., 31:3643, 2008": "G. Graefe and W. J. McKenna. The Volcano Optimizer Generator: Extensibility and Efficient Search.In Proceedings of the Ninth International Conference on Data Engineering, ICDE 93, pages209218, Washington, DC, USA, 1993. IEEE Computer Society. ISBN 978-0-8186-3570-0. URL R. Heinrich, M. Luthra, H. Kornmayer, and C. Binnig. Zero-shot cost models for distributedstream processing. In Proceedings of the 16th ACM International Conference on Distributedand Event-Based Systems, DEBS 22. ACM, June 2022. doi: 10.1145/3524860.3539639. URL",
  "V. Leis, A. Gubichev, A. Mirchev, P. Boncz, A. Kemper, and T. Neumann. How good are queryoptimizers, really? Proceedings of the VLDB Endowment, 9(3):204215, 2015": "J. Lin, J. Tang, H. Tang, S. Yang, W.-M. Chen, W.-C. Wang, G. Xiao, X. Dang, C. Gan, and S. Han.Awq: Activation-aware weight quantization for on-device llm compression and acceleration.Proceedings of Machine Learning and Systems, 6:87100, 2024. R. Marcus, P. Negi, H. Mao, N. Tatbul, M. Alizadeh, and T. Kraska. Bao: Making learned queryoptimization practical. In Proceedings of the 2021 International Conference on Management ofData, SIGMOD 21, page 12751288, New York, NY, USA, 2021. Association for ComputingMachinery. ISBN 9781450383431. doi: 10.1145/3448016.3452838. URL",
  "P. Negi, R. Marcus, A. Kipf, H. Mao, N. Tatbul, T. Kraska, and M. Alizadeh. Flow-loss: Learningcardinality estimates that matter. arXiv preprint arXiv:2101.04964, 2021": "P. G. Selinger, M. M. Astrahan, D. D. Chamberlin, R. A. Lorie, and T. G. Price. Access PathSelection in a Relational Database Management System. In J. Mylopolous and M. Brodie, editors,SIGMOD 79, SIGMOD 79, pages 511522, San Francisco (CA), 1979. Morgan Kaufmann.ISBN 978-0-934613-53-8. doi: 10.1016/B978-0-934613-53-8.50038-8. URL A. van Renen, D. Horn, P. Pfeil, K. E. Vaidya, W. Dong, M. Narayanaswamy, Z. Liu, G. Saxena,A. Kipf, and T. Kraska. Why TPC is not enough: An analysis of the Amazon Redshift fleet. Proceed-ings of the VLDB Endowment, 2024. URL L. Woltmann, J. Thiessat, C. Hartmann, D. Habich, and W. Lehner. FASTgres: Making LearnedQuery Optimizer Hinting Effective. Proceedings of the VLDB Endowment, 16(11):33103322,Aug. 2023. ISSN 2150-8097. doi: 10.14778/3611479.3611528. URL G. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth, and S. Han. Smoothquant: Accurate and efficientpost-training quantization for large language models. In International Conference on MachineLearning, pages 3808738099. PMLR, 2023. Z. Yi, Y. Tian, Z. G. Ives, and R. Marcus. Low rank approximation for learned query optimiza-tion.In Proceedings of the Seventh International Workshop on Exploiting Artificial Intelli-gence Techniques for Data Management, aiDM 24, New York, NY, USA, 2024. Associationfor Computing Machinery.ISBN 9798400706806.doi: 10.1145/3663742.3663974.URL R. Zhu, L. Weng, W. Wei, D. Wu, J. Peng, Y. Wang, B. Ding, D. Lian, B. Zheng, and J. Zhou.PilotScope: Steering Databases with Machine Learning Drivers. PVLDB, 17(5):980993, 2024.doi: 10.14778/3641204.3641209."
}