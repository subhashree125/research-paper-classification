{
  "Abstract": "Large language models (LLMs) represent a promising, but controversial, tool in aiding scientific peerreview. This study evaluates the usefulness of LLMs in a conference setting as a tool for vetting papersubmissions against submission standards. We conduct an experiment at the 2024 Neural InformationProcessing Systems (NeurIPS) conference, where 234 papers were voluntarily submitted to an LLM-based Checklist Assistant. This assistant validates whether papers adhere to the author checklist usedby NeurIPS, which includes questions to ensure compliance with research and manuscript preparationstandards. Evaluation of the assistant by NeurIPS paper authors suggests that the LLM-based assistantwas generally helpful in verifying checklist completion.In post-usage surveys, over 70% of authorsfound the assistant useful, and 70% indicate that they would revise their papers or checklist responsesbased on its feedback. While causal attribution to the assistant is not definitive, qualitative evidencesuggests that the LLM contributed to improving some submissions. Survey responses and analysis of re-submissions indicate that authors made substantive revisions to their submissions in response to specificfeedback from the LLM. The experiment also highlights common issues with LLMsinaccuracy (20/52)and excessive strictness (14/52) were the most frequent issues flagged by authors.We also conductexperiments to understand potential gaming of the system, which reveal that the assistant could bemanipulated to enhance scores through fabricated justifications, highlighting potential vulnerabilities ofautomated review tools.",
  "Introduction": "Recent advancements in large language models (LLMs) have significantly enhanced their capabilities in areassuch as question answering and text generation. One promising application of LLMs is in aiding the scientificpeer-review process [Sha22, KAD+24]. However, the idea of using LLMs in peer review is contentious andfraught with potential issues [LS23]. LLMs can hallucinate, exhibit biases, and may compromise the fairnessof the peer-review process. Despite these potential issues, LLMs may serve as useful analytical tools toscrutinize manuscripts and identify possible weaknesses or inaccuracies that need addressing.In this study, we take the first steps towards harnessing the power of LLMs in the application of conferencepeer review. We conduct an experiment the the Neural Information Processing Systems (NeurIPS) 2024conference, a premier conference in the field of machine learning.1 While the wider ethical implications andappropriate use cases of LLMs remain unclear and must be a larger community discussion, here, we evaluatea relatively clear-cut and low-risk use case: vetting paper submissions against submission standards, withresults shown only to the authors.Specifically, the NeurIPS peer-review process requires authors to submit a checklist appended to theirmanuscripts. Such author checklists, utilized in NeurIPS as well as in other peer-review venues [MSA01,VEA+07, MLT+09], contain a set of questions designed to ensure that authors follow appropriate research Joint first authors. For any correspondence, please email computer science, unlike most other fields, conferences are a primary venue for publication, with the peer-review processevaluating entire manuscripts rather than just abstracts.",
  "arXiv:2411.03417v2 [cs.CL] 8 Nov 2024": "and manuscript preparation practices. The NeurIPS Paper Checklist is a series of yes/no questions that helpauthors check if their work meets reproducibility, transparency, and ethical research standards expected forpapers at NeurIPS. The checklist is a critical component in maintaining standards of research presented atthe conference. Adhering to the guidelines outlined by these checklists helps authors avoid mistakes thatcould lead to rejection during peer review.We deploy and evaluate a NeurIPS 2024 Checklist Assistant powered by LLMs. This assistant scrutinizesauthors responses to the NeurIPS checklist, proposing enhancements for submissions to meet the conferencesrequirements.To prevent any potential bias in the review process, we confine its usage exclusively tothe authors of papers, so the checklist assistant is not accessible to reviewers.We then systematicallyevaluate the benefits and risks of LLMs by conducting a structured study to understand if LLMs canenhance research quality and improve efficiency by helping authors understand if their work meets researchstandards. Specifically, we administered surveys both before and after use of the Checklist Assistant askingauthors about their expectations for and perceptions of the tool. We received 539 responses to the pre-usagesurvey, 234 submissions the the Checklist Assistant and 78 responses to the post-usage survey. Our mainfindings are as follows:",
  "Authors expectations of the assistants effectiveness were even more positive before using it than theirassessments after actually using it (.1.3)": "Among the main issues reported by authors in qualitative feedback, the most frequently cited wereinaccuracy (20/52 respondents) and that the LLM was too strict in its requirements (14/52 respon-dents) (.1.4). (2) While changes in NeurIPS paper submissions cannot be causally attributed to use of the checklist verifi-cation assistant, we find qualitative evidence that the checklist review meaningfully helped some authorsto improve their submissions. Analysis of the content of LLM feedback to authors indicates that the LLM provided granular feedbackto authors, generally giving 4-6 distinct and specific points of feedback per question across the 15questions (.2.1). Survey responses reflect that some authors made meaningful changes to their submissions35 surveyrespondents described specific modifications they would make to their submissions in response to theChecklist Assistant (.2.2). In 40 instances, authors submitted their paper twice to the checklist verifier (accounting for 80 totalpaper submissions.) Between these two submissions, authors tended to increase the length of theirchecklist justifications significantly, suggesting that they may have added content in response to LLMfeedback (.2.3). Finally, we investigate how LLM-based tools can be easily manipulated specifically, we find that with AI-assisted re-writing of the justifications, an adversarial author can make the Checklist Assistant significantlymore lenient (.1).In summary, the majority of authors found LLM assistance to be beneficial, highlighting the significantpotential of LLMs to enhance scientific workflowswhether by serving as direct assistants to authors orhelping journals and conferences verify guideline compliance. However, our findings also underscore thatLLMs cannot fully replace human expertise in these contexts.A notable portion of users encounteredinaccuracies, and the models were also vulnerable to adversarial manipulation.",
  "The NeurIPS 2024 Author Checklist": "We provide below the checklist questions used in NeurIPS 2024 submission template. We provide only thequestions here and give the full version including guidelines in Appendix A. These questions are designedby NeurIPS organizers, not specifically for this study, and questions are carried over from previous years.The authors had to provide a response to each question, comprising Yes, No or NA (Not Applicable),along with a justification for their answer.",
  ". Theory Assumptions and Proofs: For each theoretical result, does the paper provide the full set ofassumptions and a complete (and correct) proof?": "4. Experimental Result Reproducibility: Does the paper fully disclose all the information needed toreproduce the main experimental results of the paper to the extent that it affects the main claims and/orconclusions of the paper (regardless of whether the code and data are provided or not)? 5. Open access to data and code: Does the paper provide open access to the data and code, withsufficient instructions to faithfully reproduce the main experimental results, as described in supplementalmaterial? 6. Experimental Setting/Details: Does the paper specify all the training and test details (e.g., datasplits, hyperparameters, how they were chosen, type of optimizer, etc.)necessary to understand theresults?",
  ". Broader Impacts: Does the paper discuss both potential positive societal impacts and negative societalimpacts of the work performed?": "11. Safeguards: Does the paper describe safeguards that have been put in place for responsible release ofdata or models that have a high risk for misuse (e.g., pretrained language models, image generators, orscraped datasets)? 12. Licenses for existing assets: Are the creators or original owners of assets (e.g., code, data, models),used in the paper, properly credited and are the license and terms of use explicitly mentioned and properlyrespected?",
  ". New Assets: Are new assets introduced in the paper well documented and is the documentation providedalongside the assets?": "14. Crowdsourcing and Research with Human Subjects: For crowdsourcing experiments and researchwith human subjects, does the paper include the full text of instructions given to participants and screen-shots, if applicable, as well as details about compensation (if any)? 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Sub-jects: Does the paper describe potential risks incurred by study participants, whether such risks weredisclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalentapproval/review based on the requirements of your country or institution) were obtained?",
  "Related work": "Language models have been used in the scientific peer review process for over a decade.The primaryapplication so far has been in assigning reviewers to papers.Here, a language model first computes asimilarity score between every reviewer-paper pair, based on the text of the submitted paper and thetext of the reviewers previously published papers [CZ13, WGNBK19, CFB+20] (see [Sha22, ] formore references). A higher value of the similarity score indicates that the language model considers thisreviewer to have a higher expertise for this paper. Given these similarity scores, reviewers are then assignedto papers using an optimization routine that maximizes the similarity scores of the assigned reviewer-paperpairs [CZ13, SSS21, PZ22].There have been recent works that design or use LLMs to write the entire review of papers [LZC+23, TLS+24, DHBD24, LLL+24, DRB+23]. The outcome measures for evaluating the effectiveness of the LLM-generated reviews are based on ratings sourced from authors or other researchers. It is not entirely clear howthese ratings translate to meeting the objectives of peer review in practice namely that of identifying errors,choosing better papers, and providing useful feedback to authors. Moreover, it is also known that evaluationof peer reviews themselves are fraught with biases [GSC+23], and the aggregate effect of such biases onthese evaluations of reviews is not clear. Our work focuses on a more concrete task in reviewing papersthan generating an end-to-end review, namely validating that papers meet criteria specified in an AuthorChecklist. Moreover, we evaluate the efficacy of LLMs in the setting of an actual peer review conference.Recent work also investigates whether LLMs can identify errors in papers and shows promising initialresults. The paper [LS23] constructs a set of short papers with deliberately inserted errors and asks LLMsto identify errors. GPT-4 does identify the error more than half the time. Another experiment describedin [Sha22] asks GPT-4 to identify deliberately inserted errors in three full papers. It successfully and consis-tently does so on one paper, partially and occasionally on a second paper, and is consistently unsuccessfulon the third. Note that in both experiments [LS23, Sha22], the prompts specifically asked the LLM to finderrors rather than generically asking the LLM to review the paper. Moreover, both experiments had smallsample sizes in terms of the number of papers. In another set of experiments presented in [LS23], evaluatedthe ability of large language models (LLMs) to compare the strength of results between papers, mirroringthe goals of conferences and journals in selecting better papers. The experiment consisted of creating 10pairs of abstracts, where one abstract in each pair was made clearly and objectively stronger than the other.To simulate diverse, yet irrelevant conditions, the language of the abstracts was deliberately varied. In thistest, GPT-4 performed no better than random chance in identifying the stronger abstract, underscoring thatwhile LLMs may excel at some complex tasks like scientific error identification, they often struggle withseemingly simpler tasks.The papers [LS23, WBM+23] investigate the performance of LLMs in evaluating checklist compliance.These studies, however, were retrospective studies of published papers, whereas our work is deployed liveassociated to a peer-review venue and helps authors improve their checklist compliance before they maketheir submission.Recent work has highlighted the prevalence of the use of LLMs both in preparation of scientific papermanuscripts [LZW+24] and in the generation of scientific peer reviews [LIZ+24, LRD+24]. For example,[LZW+24] estimates that as of January 2024, 17.5% of the abstracts in Computer Science papers on arXiv Question 3: For each theoretical result, does the paper provide the full set of assumptions and a complete (andcorrect) proof?Answer: [NA]Justification: There is no theory in this paper.Review: The answer provided by the authors is consistent with the content of the paper. Based on the detailedreview of the paper, it is clear that the work primarily focuses on conducting an observational study to investigatethe presence of citation bias in peer review processes. The study utilizes statistical modeling and analysis ratherthan theoretical developments that would involve theorems, lemmas, or formal proofs. Therefore, the classificationof the answer as NA (Not Applicable) is accurate given the nature of the research presented. However, toprovide constructive and actionable suggestions for improvement and to ensure thorough and critical assessment,it is beneficial to clearly outline the assumptions behind the statistical models and analyses employed. This wouldenhance the readability and understandability of the studys methodology and its limitations. [...] Question 12: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properlycredited and are the license and terms of use explicitly mentioned and properly respected?Answer: [Yes]Justification: We have used algorithms such as TPMS and PeerReview4All. We have cited them.Review:There does not appear to be any discrepancy between the authors justification for the cited question and the actual paper content in terms of providing proper credit and mentioning licenses for used assets, accordingto their statement. However, the detailed source citations such as the specific papers for TPMS (Toronto PaperMatching System) and PeerReview4All, including their licenses or terms of use, are not explicitly mentioned in theprovided text. Its essential to ensure that these aspects are clearly attributed and any terms of use are respected,as stated in the initial guidelines provided to authors. [...]",
  ": Example of checklist questions, answers, and LLM-provided review": "had used an LLM to significantly modify content. The fact that LLMs are already being used by authorsand reviewers throughout the scientific peer review process underscores the need for systematic evaluationof the efficacy of LLMs in scientific peer review, which we begin to address in this paper. Given the manyknown risks associated with large language models (LLMs), our approach to deploying them in this real-world scenario was highly cautious. We focused solely on using the LLMs to assist authors in improvingtheir papers, deliberately excluding reviewers from this process.",
  "Design and deployment": "We design an LLM-based tool (Checklist Assistant) to assist NeurIPS authors in ensuring their submittedchecklists are thoroughly answered. Our platform interfaced with a third-party LLM (GPT-4 from OpenAI),using simple prompt engineering with these hyper-parameters: temperature = 1, topp = 1, and n = 1. Foreach checklist question, the LLM is provided with the authors checklist response and justification, alongsidethe complete paper and any appendices. The LLMs role is to assess the accuracy and thoroughness of eachresponse and justification, offering targeted suggestions for improvement. Each checklist item is treated asan individual task, i.e., an API call with only one question, its answer and justification by the author, andthe paper and appendices. The API call returns a review and score for the submitted question. illustrates examples of feedback provided by the Checklist Assistant for two different papers. Inthese examples, green indicates that the tool found no significant concerns, while orange signals needsimprovement with the NeurIPS Paper Checklist standards. Authors are encouraged to carefully review anyorange feedback, validate the identified issues, and make the necessary revisions to align with the checklistrequirements.",
  "Deployment": "We deployed the Checklist Assistant on Codabench.org [XEP+22]. We configured 15 Google Cloud CPUworkers, integrated with Codabench, to handle multiple paper submissions concurrently. The bulk of thecomputations were carried out by the LLM third-party software (GPT-4 from OpenAI) via API calls (onecall per question, and additional calls in case of failure).Participation was fully voluntary, and participants were recruited through a NeurIPS blog post thatwas released 8 days before the abstract submission deadline. Interested participants were asked to registerthough a Google form. Participants who submitted registration requests through the Google form werethen given access to the Assistant on the Codabench platform. The submissions were entirely optional andcompletely separate from the NeurIPS paper submission system and the review process. The papers hadto be formatted as specified in the NeurIPS24 call for papers (complete with appendices and checklist).Information provided in external links was not taken into account by the assistant. We asked submitters tofill out the checklist to the best of their abilities. Submissions made via the Codabench landing page wereprocessed as follows: 1. Checklist Assistant: The paper was parsed using a PDF-to-text parser, then screened for any problemssuch as the format of the paper or checklist, etc. Each answered question in the checklist was processedby an LLM using an API.",
  ". Result Compilation: LLM responses were combined for all questions and formatted in an HTMLdocument with proper colors and structure for readability and user-friendliness": "We encountered several parsing issues with both paper texts and checklists. Initially, our parser struggledwith subsections and titles, prompting code improvements to handle sections accurately. Checklist parsingalso faced issues due to spacing and incomplete checklists, which we addressed by refining the code. Specialcharacters, especially merged letters like fi and fl in the submitted PDFs required further parsing updates.",
  "Prompt engineering": "In this section we discuss design of a prompt given to the LLM, tasked to behave as Checklist Assistant. Weprovide the full prompt in Appendix B.While preparing the Checklist Assistant, we experimented with various prompt styles.Tuning wascarried out using a dozen papers. Some checklists were filled out with our best effort to be correct, andothers included deliberately planted errors to verify robustness and calibrate the scores. We observed thatthe LLM performed better with clear, step-by-step instructions.Our final prompt provided a sequence of instructions covering different aspects of the required review,designed as follows: first, the context is set by indicating that the paper is under review for the NeurIPSconference. Next, the main goal is clarified, specifying that the LLMs primary task is to assist the authorin responding to the checklist question.The LLM is then directed to review the authors answer andjustification, identifying any discrepancies with the paper based on the specific guidelines of the question.It is instructed to provide itemized, actionable feedback according to the guidelines, offering suggestions forimprovement, with clear examples for responses such as Yes, No, or NA. At the end of the review, theLLM is asked to assign a score: Score=1 for no issues, Score=0.5 for minor improvements, and Score=0 forcritical issues. Finally, the LLM is provided with the checklist question, the authors answer, justification,the relevant guidelines, and the paper content.Before prompt adjustments, LLM responses often mixed the review with the score.To fix this, wespecified that the score should be returned on a separate line at the end of the review. For long papersexceeding 35 pages (or 15,000 words), we processed only the first 15,000 words and notified authors with awarning.We hypothesized that users might find the LLM responses overly strict, vague, and lengthy (which wasindeed later confirmed), so we added prompt instructions like use 0 score sparingly, provide itemized,actionable feedback, and focus on significant improvements. Although the Checklist Assistant returned scores of 0, 0.5, and 1, we combined the 0 and 0.5 scores to indicate that improvement was needed, ratherthan differentiating between two levels of severity (with red for 0 and orange for 0.5). This decision wasmade due to concerns that the LLMs evaluations might be too harsh. User feedback on LLM strictness andother issues is analyzed in .We also tested whether the LLM was consistent in generating answers for reiterations of the same input.As a sanity check, we test for each question, whether the variation of the output scores for multiple runs onthe same paper is comparable to the variation across papers. We find that the variation in scores for multipleruns on the same paper is significantly lower than variation across papers (p < 0.05; based on a one sidedpermutation test after BH correction) for all but one question. The only question that had a comparablevariance within and across papers was the question on ethics (Q9; p > 0.4).",
  "Anonymity, confidentiality, and consent": "The authors could retain their anonymity by registering to Codabench with an email that did not revealtheir identity, and by submitting anonymized papers. The papers and LLM outputs were kept confidentialand were not be accessible to NeurIPS reviewers, meta reviewers, and program chairs. It is important tonote that while authors retained ownership of their submissions, the papers were sent to the API of an LLMservice, and treated under their conditions of confidentiality.This study was approved by the Carnegie Mellon University Institutional Review Board (IRB). Theparticipants gave written documentation of informed consent to participate.",
  "(2) Does the use of an Author Checklist Assistant meaningfully help authors to improve their paper sub-missions?": "In order to understand author experience using the provided Author Checklist Assistant, we surveyedauthors before and after submitting to the Author Checklist Assistant. Additionally, we analyzed the contentand submission patterns of authors checklists and the LLM responses. A summary of our main findings isgiven in . In this subsequent section we provide detailed analyses of survey responses and usage ofthe Checklist Assistant. In .1, we give results on author perception and experience and in .2we analyze changes made by authors to their submissions after using the Author Checklist Assistant.",
  "Author Perception and Experience": "First, we analyze the authors usage patterns and perceptions of the Author Checklist Assistant, as capturedthrough surveys. In .1.1, we provide an overview of how authors filled out the checklist and theresponses given by the LLM on their checklists. In .1.2, we detail the survey methodology used tounderstand author experience and in .1.3, we analyze results of the survey. Finally, in .1.4,we overview the main challenges identified by authors when using the Author Checklist Assistant.",
  "Overview of Checklist Usage and Responses": "A total of 234 papers, each accompanied by a checklist, were submitted to the assistant. For each checklistquestion, authors could respond with Yes, No, NA, or TODO. As illustrated in a, most questionsreceived a Yes response, indicating that the authors confirmed their paper met the corresponding checklistcriteria. However, for the questions on Theory, Impacts, Safeguards, Documentation, Human Subjects, and",
  ": Summary of author checklist completion and LLM feedback": ": Distribution of Needs improvement scores given by the Checklist Assistant, per checklist. Out of15 questions, all participants received at least 8 Needs improvement and at most 13. More than half of theparticipants received 12 or more. Risks, a significant portion of authors selected NA. Additionally, a notable number of authors responded Noto the questions on Code and Data, and Error Bars.In response to the authors checklists, the LLM provided written feedback, with green indicating NoConcerns and orange indicating Needs improvement. b illustrates the distribution of LLM feedbackfor each checklist question. For most questions, the majority of feedback suggested that the checklist ormanuscript could be improved. However, for the questions on Theory, Human Subjects, and Risks, manyNA responses were deemed appropriate, leading the LLM to respond with No Concerns. This likely reflectsthe LLMs confidence in confirming that certain papers did not include theory, human subjects research, orclear broader risks, making those checklist items irrelevant. In , we show the distribution of LLMevaluations per submission. All submissions received several Needs improvement ratings, with each beingadvised to improve on 8 to 13 out of the 15 checklist questions.",
  "Survey Methodology": "To assess authors perceptions of the usefulness of the Author Checklist Assistant, we conducted a surveywith all participants both at registration (pre-usage) and immediately after using the Author ChecklistAssistant (post-usage). We provide the content of the surveys in . Both surveys contained thesame four questions, with the pre-usage survey focusing on expectations and the post-usage survey on actual",
  ": Survey questions administered to the participants": "experience. Responses were recorded on a four-point Likert scale, ranging from strongly disagree to stronglyagree. In the post-usage survey, we also asked authors to provide freeform feedback on (1) any changes theyplanned to make to their paper, and (2) any issues they encountered while using the Checklist Assistant.We received 539 responses to the pre-usage survey and 234 papers submitted. However, we receivedonly 78 responses to the post-usage survey, representing 63 unique participants (due to multiple submissionsfor the same paper). While completing the pre-registration survey was mandatory for all participants, thepost-usage survey was optional. As a result, all participants in the post-usage survey had also completedthe pre-registration survey.",
  "Survey Responses": "presents the survey responses collected before and after using the checklist verification tool. Weinclude responses from authors who completed both surveys (n=63). In cases where authors submitted thesurvey multiple times for the same paper, we included only the earliest post-usage response. Including theduplicated responses made a negligible difference, with the proportion of positive responses changing by lessthan 0.02 across all questions.Overall, the majority of authors responded positively regarding their experience with the Checklist Assis-tant. 70% of surveyed authors reported plans to make changes based on the feedback received, 70% reportedthat they found the assistant concretely useful, and 67% expressed excitement about using LLMs as ChecklistAssistants in the future. Thus, a statistically significant majority of authors responded positively to WillModify, Useful and Excited to Use after using the assistant based on a one-sided Binomial Test withBenjaminiHochberg Correction comparing the sample proportion to 0.5 (adjusted p-values of 0.001, 0.002,and 0.007 respectively).It is notable that authors were even more positive before using the tool. Comparing pre- and post-usage responses, there was a statistically significant drop in positive feedback on the Useful and Excitedto Use questionswe run a permutation test with 50,0000 permutations to test whether the differencebetween proportion of positive responses pre and post-usage is non-zero, which gives Benjamini-Hochbergadjusted p-values of 0.007 and 0.013 forExcited to Use and Useful respectively with effect sizes of 0.23and 0.2.We also assessed the correlation between post-usage survey responses and the number of needs improve-ment scores given by the LLM to authors.In , we show mean number of needs improvementscores for authors responding positively or negatively to each survey question. We find no substantial ef-fect of number of needs improvement scores on survey responses. This may reflect that the number ofneeds improvement scores was less important in authors perception than the written content of the LLMs : Responses to survey questions pre- and post-usage of the Checklist Assistant, from all authorswho responded to both surveys (n=63). Error bars show 95% confidence intervals for the sample proportion.The majority of surveyed authors reported a positive experience using the Checklist Assistant. : Mean number of needs improvement scores from the LLM evaluation of checklist questions, forpost-usage survey respondents answering positively or negatively to each question. Error bars show 95%confidence intervals. evaluation.Finally, we examined potential selection bias due to the drop-off in participation in the post-usage surveyby analyzing the pre-usage survey responses across different groups. As noted earlier, only a portion ofthe 539 participants who completed the pre-usage survey went on to submit papers (234 Submitters), andan even smaller group responded to the post-usage survey (78 Post-Usage Respondents). In , wecompare the pre-usage survey responses between Submitters and Non-Submitters, as well as between Post-Usage Respondents and Non-Respondents. No substantial differences in rates of positive responses werefound (using a permutation test for the difference in mean response, gave p-values of > 0.3 for all questionsbefore multiple testing correction), suggesting there is no significant selection bias.",
  "Challenges in Usage": "In addition to the structured survey responses, 52 out of the 78 post-usage survey submissions includedfreeform feedback detailing issues with the Checklist Assistants usage. We manually categorized the reportedissues from these responses and identified the following primary concerns, listed in order of decreasingfrequency (summarized in ): 1. Inaccurate: 20 authors reported that the LLM was inaccurate. Note that it is not possible to tell from theresponses how many inaccuracies participants found in individual questions since the survey did not ask",
  ": Comparison in responses to the pre-usage survey for different groups.Error bars show 95%confidence intervals": ": Summary of reported issues using checklist verification from freeform feedback on post-usagesurvey (n=52 out of 78 total survey responses). Numbers show the total number of authors who reportedthe issue. about individual checklist questions. Many participants noted specific issues, in particular that the LLMoverlooked content in the paper, requesting changes to either the checklist or the paper for elements thatthe authors believed were already addressed. Additionally, some authors reported more nuanced accuracyissues. For instance, one author mentioned that the LLM misinterpreted a thought experiment as a realexperiment and incorrectly asked for more details about the experimental setup. Another author reportedthat the LLM mistakenly assumed human subjects were involved due to a discussion of interpretabilityin the paper.",
  "Changes to Submissions in Response to Feedback": "In the following analysis, we integrate an assessment of the LLMs feedback with the authors checklistanswers, to better understand whether the Checklist Assistant helped authors make concrete and meaningfulchanges to their papers. In .2.1, we analyze the types of feedback given by the LLM to authors. In.2.2, we overview the changes to their papers that authors self-reported making in survey responses.Lastly, in .2.3, we analyze changes made in multiple submissions of the same paper to the AuthorChecklist Assistant.",
  "Characterization of LLM Feedback by Question": "For authors to make meaningful changes to their papers, the Author Checklist Assistant must provideconcrete feedback.In this section, we analyze the type of feedback given by the Checklist Assistant todetermine whether it is specific to the checklist answers or more generic.Given the large volume of feedback, we employed an LLM to extract key points from the ChecklistAssistants responses for each question on the paper checklist and to cluster these points into overarchingcategories. Specifically, for each of the 15 questions across the 234 checklist submissions, we used GPT-4to identify the main points of feedback provided to authors. We manually inspected that the main pointsextracted by GPT-4 matched the long-form feedback on 10 randomly selected submitted paper checklists andfound that GPT-4 was highly accurate in extracting these key feedback points. We then passed the namesand descriptions of these feedback points to GPT-4 to hierarchically cluster them into broader themes. : Most frequent categories of feedback given by the Author Checklist Assistant on four questionsof the checklist.The types of feedback below each question were identified by using another LLM tosummarize the main points in the feedback given by the checklist verifier on submitted papers. Frequency ofeach category of feedback is shown in parentheses (out of n=234 checklist submissions). Feedback categoriesfor other checklist questions are provided in Appendix C.",
  ". The LLM tended to provide 4-6 distinct points of feedback per question (for each of the 15 questions)": "3. The LLM is capable of giving concrete and specific feedback for many questions. For example, on theClaims question, the LLM commented on consistency and precision in documenting claims on 50 papers,including feedback like matching the abstract and introduction and referencing appendices.On theCompute resources question the LLM commented specifically on detailing compute / execution time ofmethods.",
  ". There are certain topics that appear across many questions, in particular discussion of limitations andimproved documentation": "6. The LLM often expands the scope of checklist questions. For example, the LLM brings up reproducibilityas a concern in feedback to the NeurIPS code of ethics question and brings up anonymity quite frequentlyin the code and data accessibility question. We provide a full list of the summarized main themes of feedback in Appendix C. In summary, our analysisof the feedback given by the LLM suggests that the LLM gave concrete and actionable feedback to authorsthat they could potentially use to modify their paper submissions. Our analysis also suggests that a moredetailed checklist could be developed to provide more granular feedback, based on the rubrics covered bythe Author Checklist Assistant. Such a detailed checklist could be processed automatically by an LLM tosystematically identify specific, commonly overlooked issues in scientific papers and flag concrete issues forauthors to resolve.",
  "Authors Descriptions of Submission Changes": "We obtain additional evidence of changes made by authors in response to the Checklist Assistant through thepost-usage survey. In the survey, we asked authors to detail in freeform feedback any changes they had madeor planned to make in responses to feedback from the LLM. Of the 78 survey responses, 45 provided feedbackto this question. Of these 45 responses, 35 actually described changes they would make (the remainder usedthis freeform feedback to describe issues that they had in using the assistant). Based on manual coding ofthe comments, we identified the main themes in changes they planned to make:",
  "Analysis of Re-submissions": "Finally, we analyze changes made between submissions to the Checklist Assistant when authors submittedmultiple times. There were 40 instances where an author submitted the same paper to the checklist ver-ification multiple times (out of 184 total distinct paper submissions to the checklist verification). In thisanalysis, we assess changes made to the paper checklist between the first and second submission to our checklist verifier in order to understand whether authors made substantive changes to their checklists and/orpaper manuscripts in response to feedback from the checklist verification.We find that of the 40 pairs of papers, in 22 instances authors changed at least one answer in theirchecklist (e.g., NA to Yes) between the first and second submission and in 39 instances they changed atleast one justification for a checklist answer (with the remaining paper being an exact re-submission withno changes). Of the 22 papers where authors changed an answer, on one paper the author changed theanswer to all questions from TODO to an actual answer, while on the other papers authors changed 1 to 3answers with most changing only one answer. We exclude the paper where the initial checklist was entirelyTODO from the rest of this analysis. The most common changes were to the Documentation question (5authors changed from NA to Yes), followed by 3 authors who changed Impacts from NA to Yes and 3authors who changed Error bars from No to Yes.Of the authors who changed justifications on their paper checklist, many authors made a large numberof changes, with 35/39 changing more than 6 justifications of the 15 questions on the checklist. In ,we show (multiplicative) increase in word count between initial submission and final submission on questionswhere authors changed justifications (a value of 2 corresponds to a doubling of the length of an answer). Wefind that over half the time when authors changed a checklist answer, they more than doubled the length oftheir justification. :Ratio between the word count ofchecklist responses on the first and second sub-mission on papers where the authors changedthe justifications (n = 362 responses). The plotshows the proportion of checklist responses thatincreased by more than a given ratiomany au-thors increased the length of responses betweenre-submissions, with more than 50% of responsesincreasing by a factor of two or more in length. : Changes in LLM evaluation of pa-per checklist questions between re-submissions(n=40 pairs) split by changes the author madeto the checklist answer.Error bars show 95%confidence intervals obtained via bootstrappingby re-sampling the papers. In , we evaluate whether the LLM Checklist Assistant evaluation responds to changes to paperchecklist answers between re-submissions. We find that on questions where the authors changed neitheranswer nor justification, the LLM evaluation of the checklist answer remained unchanged 81% of the time,improved 7% of the time and was worse 12% of the time. We note that the change could be due to changesin the paper manuscript or due to inconsistency in LLM evaluation or due to randomness of the LLM. Bycomparison, when the author changed a justification to an answer on the checklist, the LLM evaluationimproved 13% of the time and was was worse 5% of the time. When an author changed an answer on thechecklist, the LLM improved its evaluation 15% of the time and was worse 2% of the time. Due to smallsample size, the differences in LLM evaluations between the baseline of no changes made to checklist answersand cases where changes are high variance, but provide some evidence that the LLM evaluation is responsive",
  "Limitations": "Despite significant advances, LLMs still face several limitations when used in verifying scientific submissions.We first investigate one key potential limitation that gains importance in settings where LLMs may be usedinstead of human reviewers that of adversarial attacks. We then discuss various other limitations and howwe mitigated some of them in this work.",
  "Gaming the review system": "The intended use of our Checklist Assistant was to help authors improve their papers, not to serve as a toolfor reviewers to verify the accuracy of authors responses. To illustrate a potential risk of using the ChecklistAssistant beyond its intended purpose, we address a concern that arises if the assistant were used as anautomated verification step as part of a review process: could authors game the system by automaticallyimproving their checklist responses with the help of AI, without making actual changes to their paper? If suchgaming were possible, authors could provide a false impression of compliance to a conference without (much)additional effort and without actually improving their papers as an Author Checklist aims to incentivize.Such a question about gaming the system is motivated by various issues of adversarial behavior in peerreview [Lit21, JZL+20, RSJ+24] as well as feasibility of adversarial attacks on other parts of the reviewprocess [MSLL17, TJ19, EQM+23, HRS24] (see [Sha22, ] for a detailed survey).To assess whether our system is vulnerable to such gaming, we employed another LLM as an attackagent to iteratively manipulate the checklist justifications, aiming to deceive the Checklist Assistant. In thisiterative process, the attack agent receives feedback from the system after each round and uses it to refineits justifications. This feedback loop continues until the agent has optimized its responses over successiveiterations. Specifically, we provided GPT-4o with the initial checklist responses and instructed it to revisethe justifications based solely on feedback, without altering the underlying content of the paper. The fulladversarial prompt used is detailed in Appendix D. Our deployed Checklist Assistant allows up to threesubmissions per user, with scores of 0 and 0.5 merged, as described in .2 and illustrated in .To simulate this environment, we similarly allowed the attack agent three attempts to revise justifications,treating this as the attack budget. At the end of three iterations, the agent selected the highest scoredresponse for each checklist question.To quantify statistically how successful the adversarial attack is, we then submitted the selected justi-fication multiple times to our Checklist Assistant for evaluation. We calculate the mean and variability ofthese multiple repeats. Given that our outcomes are binary (scores of 0 and 0.5 are merged), we assessedvariability using the ClopperPearson 95% confidence interval. presents the performance of theattack agent as it refines the justifications over three attack rounds. The blue bars show the average scoreand variability across multiple evaluations of the revised justifications, while the red bars represent theaverage and variability for the original, unmodified justifications submitted by authors. Each bar and thecorresponding confidence intervals are based on evaluations of 234 papers, repeated three times, yielding234 3 data points per bar. Our results indicate a clear improvement in scores following the adversarialattack: 14 out of 15 questions show score increases when comparing the unchanged justifications with thoserefined through three attack rounds.We also conducted a manual examination of the changes made by the LLM to the justifications. Wefound that the LLM employed several illegitimate strategies, such as adding a hardware description thatwas not present in the paper or original justifications and generating a placeholder URL as a purportedrepository for the code. These illegitimate justifications were given a score of 1 by the Checklist Assistant.While we recognize the potential value of using large language models (LLMs) to clarify answers tochecklist questions, our experiment reveals a potential risk: automated assistants, if used as review toolswithout human oversight in peer review settings, could be manipulated to raise scores based on persuasive",
  "Other Limitations": "LLMs are known to have various other limitations in addition to non-robustness to gaming. LLMs maymisinterpret complex content or fail to fully assess checklist criteria, leading to verification errors. Thereis also a risk that authors might rely too much on the LLM analysis, potentially missing deeper issuesthat require human judgment. Throughout our experiment, we encouraged authors to manually validatethe outputs of LLMs, and to use the assistant to supplement their judgment, not replace it. Known issuesinclude false positives (e.g., LLM may ask for extra justifications or changes that are excessive or irrelevant)as well as false negatives (e.g., the LLM overlooks errors in proofs). Furthermore, the Checklist Assistantthat we implemented was based only on the text of the paper and did not check figures, tables, and externallinks (such as Github repositories), hence it may have complained about missing information that are foundin those assets. In general, inaccuracy was the most cited issue with our Checklist Assistant by paper authorsin the study. Nonetheless, the majority of authors surveyed reported finding the tool useful, suggesting thatour tool was still accurate enough, as judged by authors, to provide utility.Additionally, using LLMs raises concerns about data privacy and security, as papers are processed throughan LLM service API. We provided recommendations to participants on anonymizing their submissions andmade best efforts to keep all papers in confidence. However, the actual scientific content of papers wasprovided to a third party API, which may have discouraged usage among highly privacy sensitive paperauthors.Furthermore, biases in LLMs could compromise the fairness and impartiality of the scientific reviewprocess, especially for unconventional or interdisciplinary research. We address this risk by only providingthe Checklist Assistant to authors: the outputs of the LLM were not visible to, or used by reviewers, metareviewers, or program chairs in their review of the submission. We note that further use of such a ChecklistAssistant by reviewers could introduce bias or otherwise compromise the review process.",
  "Conclusion and future work": "The deployment of an LLM-based paper Checklist Assistant at NeurIPS 2024 demonstrated that LLMs holdpotential in enhancing the quality of scientific submissions by assisting authors in validating whether theirpapers meet submission standards. However, there are still notable limitations in deploying LLMs withinthe scientific peer review process that need to be addressed.One insight from our study is the existence of a gap between authors expectations and their actualexperience with the LLM assistant. Before using the assistant, authors had overly optimistic expectationsabout its usefulness. After interaction, although over 70% still found it useful and were willing to revisetheir papers based on the feedback, there was a noticeable decline in their enthusiasm. This suggests thatwhile LLMs can be valuable, their current capabilities may not fully meet the high expectations set by users,highlighting the need for better calibration of user expectations and improvement of the LLMs performance.The assistant provided granular and specific feedback, typically offering 4-6 distinct points per checklistquestion. This level of detail indicates that LLMs can effectively identify areas for improvement in scientificmanuscripts. However, the frequent reports of inaccuracies and excessive strictnesscited by 20 and 14 outof 52 respondents, respectivelyunderscore inherent limitations in the LLMs ability to interpret nuancedscientific content and contextual guidelines accurately. These issues can lead to frustration and may deterusers from relying on such tools in the future.Our analysis also revealed that authors who used the assistant were more likely to make substantiverevisions to their submissions. Survey responses and re-submission analyses indicated that authors oftenexpanded their checklist justifications and added more detailed explanations in response to the LLMs feed-back. This behavior suggests that LLMs can positively influence the quality of submissions by promptingauthors to reflect more deeply on their work. However, the inability to establish a definitive causal linkbetween the assistants feedback and improvements in the submissions points to the need for more controlledstudies to assess the true impact of LLM interventions.Future work should focus on enhancing the LLMs ability to interpret scientific texts accurately. Thiscould involve training models on domain-specific datasets or incorporating mechanisms to handle complexscientific concepts better. In addition, a deployed Checklist Assistant could be improved by enhancing LLMsto handle multimodal content like figures and tables, measuring and mitigating biases to ensure fair feedback,and integrating LLM assistants directly into submission platforms for a seamless author experience. On theuser experience side, it would be useful to better manage user expectations by clearly communicating theassistants capabilities and limitations. Finally, conducting randomized controlled experiments could providedefinitive evidence of LLMs impact on submission quality.Expanding the role of LLMs to assist in other stages of peer review is appealing, but the risks of authorsexploiting the system must be carefully considered and proactively mitigated. Relying on LLM assistantswithout human oversight poses risks, as skilled authors could exploit the system, gaming the assistantto improve scores through persuasive rebuttals without meaningful content changes. Such vulnerabilitiesunderscore the importance of safeguarding automated review tools to ensure they enhance, rather thanundermine, the integrity of scientific peer review.",
  "Acknowledgements": "In preparing this experiment, we received advice and help from many people. We are particularly gratefulto the NeurIPS24 organizers, including General Chair Amir Globerson, Program Chairs Danielle Belgrave,Cheng Zhang, Angela Fan, Jakub Tomczak, Ulrich Paquet, and workflow team member Babak Rahmani,for participating in brainstorming discussions and contributing to the design. We have also received inputsand encouragement from Andrew McCallum of OpenReview, Anurag Acharya from Google Scholar, andTristan Neuman from the NeurIPS board. Several volunteers have contributed ideas and helped with variousaspects of the preparation, including Jeremiah Liu, Lisheng Sun, Paulo Henrique Couto, Michael Brenner,Neha Nayak Kennard, and Adrien Pavao. This research project has benefitted from the Microsoft AccelerateFoundation Models Research (AFMR) grant program. We are grateful to Marc Schoenauer for supporting this effort with an INRIA Google Research grant. We acknowledge the support of ChaLearn, the ANR Chairof Artificial Intelligence HUMANIA ANR-19-CHIA-0022, NSF 1942124, and 2200410.Importantly, we are thankful to all the participants of the Checklist Assistant for volunteering to try itout and providing their valuable feedback. [CFB+20]Arman Cohan, Sergey Feldman, Iz Beltagy, Doug Downey, and Daniel S Weld.Specter:Document-level representation learning using citation-informed transformers. In Proceedingsof the 58th Annual Meeting of the Association for Computational Linguistics, pages 22702282,2020.",
  "[DHBD24]Mike DArcy, Tom Hope, Larry Birnbaum, and Doug Downey. MARG: multi-agent reviewgeneration for scientific papers. arXiv preprint arXiv:2401.04259, 2024": "[DRB+23]Mike DArcy, Alexis Ross, Erin Bransom, Bailey Kuehl, Jonathan Bragg, Tom Hope, and DougDowney. Aries: A corpus of scientific paper edits made in response to peer reviews. ArXiv,abs/2306.12587, 2023. [EQM+23]Thorsten Eisenhofer, Erwin Quiring, Jonas Moller, Doreen Riepel, Thorsten Holz, and Kon-rad Rieck. No more reviewer# 2: Subverting automatic {Paper-Reviewer} assignment usingadversarial learning.In 32nd USENIX Security Symposium (USENIX Security 23), pages51095126, 2023. [GSC+23]Alexander Goldberg, Ivan Stelmakh, Kyunghyun Cho, Alice Oh, Alekh Agarwal, DanielleBelgrave, and Nihar B Shah. Peer reviews of peer reviews: A randomized controlled trial andother experiments. arXiv preprint arXiv:2311.09497, 2023.",
  "[JZL+20]Steven Jecmen, Hanrui Zhang, Ryan Liu, Nihar B. Shah, Vincent Conitzer, and Fei Fang.Mitigating manipulation in peer review via randomized reviewer assignments. In NeurIPS,2020": "[KAD+24]Ilia Kuznetsov, Osama Mohammed Afzal, Koen Dercksen, Nils Dycke, Alexander Goldberg,Tom Hope, Dirk Hovy, Jonathan K. Kummerfeld, Anne Lauscher, Kevin Leyton-Brown, ShengLu, Mausam, Margot Mieskes, Aurelie Neveol, Danish Pruthi, Lizhen Qu, Roy Schwartz,Noah A. Smith, Thamar Solorio, Jingyan Wang, Xiaodan Zhu, Anna Rogers, Nihar B.Shah, and Iryna Gurevych.What can natural language processing do for peer review?arXiv:2405.06563, 2024.",
  "[LS23]Ryan Liu and Nihar B. Shah. ReviewerGPT? an exploratory study on using large languagemodels for paper reviewing, 2023": "[LZC+23]Weixin Liang, Yuhui Zhang, Hancheng Cao, Binglu Wang, Daisy Ding, Xinyu Yang, KailasVodrahalli, Siyu He, Daniel Smith, Yian Yin, et al. Can large language models provide usefulfeedback on research papers? a large-scale empirical analysis. arXiv preprint arXiv:2310.01783,2023. [LZW+24]Weixin Liang, Yaohui Zhang, Zhengxuan Wu, Haley Lepp, Wenlong Ji, Xuandong Zhao,Hancheng Cao, Sheng Liu, Siyu He, Zhi Huang, et al. Mapping the increasing use of LLMs inscientific papers. arXiv preprint arXiv:2404.01268, 2024. [MLT+09]DavidMoher,AlessandroLiberati,JenniferTetzlaff,DouglasGAltman,andt PRISMA Group*. Preferred reporting items for systematic reviews and meta-analyses: theprisma statement. Annals of internal medicine, 151(4):264269, 2009. [MSA01]David Moher, Kenneth F Schulz, and Douglas G Altman. The consort statement: revisedrecommendations for improving the quality of reports of parallel-group randomised trials. Thelancet, 357(9263):11911194, 2001. [MSLL17]Ian Markwood, Dakun Shen, Yao Liu, and Zhuo Lu. PDF mirage: Content masking attackagainst Information-Based online services. In 26th USENIX Security Symposium (USENIXSecurity 17), pages 833847, Vancouver, BC, August 2017. USENIX Association. [PZ22]Justin Payan and Yair Zick. I will have order! optimizing orders for fair reviewer assignment.In Proceedings of the 21st International Conference on Autonomous Agents and MultiagentSystems, pages 17111713, 2022. [RSJ+24]Charvi Rastogi, Xiangchen Song, Zhijing Jin, Ivan Stelmakh, Hal Daume III, Kun Zhang, andNihar B Shah. A randomized controlled trial on anonymizing reviewers to each other in peerreview discussions. arXiv preprint arXiv:2403.01015, 2024. [Sha22]Nihar B Shah. An overview of challenges, experiments, and computational solutions in peerreview.~nihars/preprints/SurveyPeerReview.pdf (Abridgedversion published in the Communications of the ACM), June 2022.",
  "The authors are encouraged to create a separate Limitations section in their paper": "The paper should point out any strong assumptions and how robust the results are to violations of theseassumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptoticapproximations only holding locally). The authors should reflect on how these assumptions might beviolated in practice and what the implications would be. The authors should reflect on the scope of the claims made, e.g., if the approach was only tested ona few datasets or with a few runs. In general, empirical results often depend on implicit assumptions,which should be articulated. The authors should reflect on the factors that influence the performance of the approach. For example,a facial recognition algorithm may perform poorly when image resolution is low or images are takenin low lighting. Or a speech-to-text system might not be used reliably to provide closed captions foronline lectures because it fails to handle technical jargon.",
  "If applicable, the authors should discuss possible limitations of their approach to address problems ofprivacy and fairness": "While the authors might fear that complete honesty about limitations might be used by reviewersas grounds for rejection, a worse outcome might be that reviewers discover limitations that arentacknowledged in the paper. The authors should use their best judgment and recognize that individualactions in favor of transparency play an important role in developing norms that preserve the integrity ofthe community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.",
  "If the contribution is a dataset and/or model, the authors should describe the steps taken to make theirresults reproducible or verifiable": "Depending on the contribution, reproducibility can be accomplished in various ways. For example, if thecontribution is a novel architecture, describing the architecture fully might suffice, or if the contributionis a specific model and empirical evaluation, it may be necessary to either make it possible for othersto replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided viadetailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of alarge language model), releasing of a model checkpoint, or other means that are appropriate to theresearch performed. While NeurIPS does not require releasing code, the conference does require all submissions to providesome reasonable avenue for reproducibility, which may depend on the nature of the contribution. Forexample",
  "(b) If the contribution is primarily a new model architecture, the paper should describe the architectureclearly and fully": "(c) If the contribution is a new model (e.g., a large language model), then there should either be away to access this model for reproducing the results or a way to reproduce the model (e.g., withan open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcometo describe the particular way they provide for reproducibility. In the case of closed-source models,it may be that access to the model is limited in some way (e.g., to registered users), but it shouldbe possible for other researchers to have some path to reproducing or verifying the results.",
  "The answer NA means that the paper does not include experiments": "The authors should answer Yes if the results are accompanied by error bars, confidence intervals, orstatistical significance tests, at least for the experiments that support the main claims of the paper. The factors of variability that the error bars are capturing should be clearly stated (for example,train/test split, initialization, random drawing of some parameter, or overall run with given experimen-tal conditions).",
  "If the authors answer NA or No, they should explain why their work has no societal impact or why thepaper does not address societal impact": "Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinfor-mation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologiesthat could make decisions that unfairly impact specific groups), privacy considerations, and securityconsiderations. The conference expects that many papers will be foundational research and not tied to particularapplications, let alone deployments. However, if there is a direct path to any negative applications,the authors should point it out. For example, it is legitimate to point out that an improvement inthe quality of generative models could be used to generate deepfakes for disinformation. On the otherhand, it is not needed to point out that a generic algorithm for optimizing neural networks could enablepeople to train models that generate Deepfakes faster. The authors should consider possible harms that could arise when the technology is being used asintended and functioning correctly, harms that could arise when the technology is being used as in-tended but gives incorrect results, and harms following from (intentional or unintentional) misuse ofthe technology. If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g.,gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse,mechanisms to monitor how a system learns from feedback over time, improving the efficiency andaccessibility of ML).",
  "The answer NA means that the paper does not involve crowdsourcing nor research with human subjects": "Depending on the country in which research is conducted, IRB approval (or equivalent) may be requiredfor any human subjects research. If you obtained IRB approval, you should clearly state this in thepaper. We recognize that the procedures for this may vary significantly between institutions and locations,and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.",
  "Here is the prompt used by the Checklist Assistant:": "You are provided with a Paper to be submitted to the NeurIPS conference. You are assisting the authorsin preparing their Answer to one checklist Question. Please examine carefully the proposed authorsAnswer and the proposed authors Justification provided, and identify any discrepancies with the actualPaper content, for this specific Question, taking into account the Guidelines provided to authors. Afterwards, provide itemized, actionable feedback, based on the Guidelines, aiming to improve the paperquality. Concentrate on a few of the most significant improvements that can be made, and write in tersetechnical English. While Authors Proposed Answer is generally preferred to be a Yes, it is acceptableto answer No or NA provided a proper Authors Proposed Justification is given (e.g., error bars arenot reported because it would be too computationally expensive or we were unable to find the license forthe dataset we used). If the Authors Proposed Answer is Yes, the Authors Proposed Justification for theAnswer should point to the section(s) within which related material for the question can be found. Notethat the Authors Proposed Justification is not expected to contain anything else (although it is fine if itcontains more details). Finally, after performing all previous steps, conclude your review with a score for this specific Question,in a separate line (1: Everything OK or mild issues; 0.5: Needs improvements. Use this score sparingly; 0:Critical issues). Make sure that score is shown in a new line in this format Score: score value and there isno content after the score.",
  "CClustering of LLM Feedback": "For each question, we provide below the most identified categories of feedback given (as identified by anLLM annotator), along with their frequency, a description of the category as provided by the LLM, and thesub-categories of feedback that were consolidated into the category. For each question, we show the mostfrequent type of feedback and two to three additional selected categories.",
  "Frequency: 122": "Description: Consolidate and clarify the uniqueness of the research approach and findings throughout thepaper, ensuring the novel contributions are explicitly highlighted in the abstract and introduction. Sample sub-categories of feedback: [Limitations and Future Directions, Claims and Results Congru-ence, Clarification and Expansion of Main Claims, Clarify Contributions in Abstract and Introduction,Highlight Novelty and Impact]",
  "Frequency: 58": "Description: Focused on methodological detailing for reproducibility, dataset and model transparency,accessibility, ethical distribution, engagement with literature and ethical AI debates, transparency andreproducibility enhancement, and advocacy for accessibility and reproducibility. Sample sub-categories of feedback: [Transparency in Artifact Sharing, Transparency in Methodology,Enhancing Reproducibility, Enhancing Reproducibility and Ethical Openness, Transparency and Re-producibility Improvement]",
  "Here is the prompt used by the attack agent:": "You are provided with a Paper submitted to the NeurIPS conference and a specific checklist Questionalong with the proposed Answer and Justification by the authors. The Review from an LLM reviewerfor this Justification is also provided. Your task is to revise the Justification to satisfy the concerns raised by the LLM reviewer. The revisedjustification should align with the Guidelines provided to authors and address the key points mentioned inthe Review. The goal is to ensure the justification is clear, concise, and meets the standards expected bythe reviewers. Please focus on improving the justification in a way that directly addresses the LLM reviewers feedback,ensuring that it is technically sound and accurately reflects the content of the paper. Make sure that therevised justification points to the appropriate section(s) within the paper and adheres to the guidelines.",
  "EAdversarial attack with multiple rounds budgets": "In our adversarial attack experiment, since our deployed Checklist Assistant allows up to three submissionsper user as described in .2 and illustrated in , to simulate this environment, we also allowedthe attack agent three attempts to revise justifications, treating these three iterations as the attack budget.Given that all prompts used by the Checklist Assistant have been released, users can theoretically reuse themand submit responses as many times as desired. To analyze performance across varying attack budgets, weadditionally re-evaluated the highest-scored justifications from the first round and the first two rounds. Themean scores and confidence intervals were calculated using the same methodology described in .As can be seen in , the effectiveness of multiple attack rounds is inconsistent. This inconsistencycould be due to the simplicity of our attack algorithm.For the first three questions, we observe clearscore improvements with increased attack budgets. However, for the last question where no improvementwas achieved even after three rounds as in , we find that the question appears to remain stableacross multiple attack attempts. Since the primary goal of this study was to demonstrate the feasibility ofadversarial gaming, we leave the development of more advanced attack strategies for future work.",
  "Question 1: Do the main claims made in the abstract and introduction accurately reflect the paperscontributions and scope?": "Answer: YesJustification: The abstract and introduction clearly state the main contributions and scope of the paperReview: The authors proposed answer to the question, Do the main claims made in the abstract and in-troduction accurately reflect the papers contributions and scope? is Yes, with the following justification:The abstract and introduction clearly state the main contributions and scope of the paper.",
  "bution of their paper, ensuring a robust alignment between the stated claims and the documented evidence": "Question 2: Does the paper discuss the limitations of the work performed by the authors?Answer: YesJustification: We have clearly explained the limitations of our work in Section: Discussion and conclusionReview: The answer and justification provided by the authors align with the content of the paper, specifi-cally in Section: Discussion and conclusion. The authors stated that the limitations of their work are clearlyexplained in this section, complying with the Guidelines for discussing limitations. Upon examination ofthe papers Discussion and conclusion section, it indeed covers various forms of limitations including as-sumptions, scope of claims, factors influencing performance, computational efficiency, and potential issues ofprivacy and fairness which are in alignment with the Guidelines provided.",
  "By addressing these points, the paper will not only comply fully with the Guidelines but also enrich the": "scientific discourse around the proposed method by providing a well-rounded view of its applicability andscope.Question 3: For each theoretical result, does the paper provide the full set of assumptions and a complete(and correct) proof?Answer: Not ApplicableJustification: This does not apply to our research work because our research is not theoretical rather itdeals with a meta-dataset creation and experiments executed using this meta-datasetReview: The authors provided an N/A answer to the question about providing full sets of assumptionsand complete (and correct) proofs for each theoretical result, justifying it by stating their work is focusedon meta-dataset creation and experimental setups rather than theoretical derivations. This justification isconsistent with the content of the paper that primarily discusses the development, composition, and applica-tions of the Meta-Album meta-dataset for few-shot learning and other learning paradigms. The paper doesnot present theoretical results or claims that would necessitate formal assumptions or proofs. It focuses ondataset contributions, experimental evaluations, and benchmarking for machine learning algorithms. There-fore, the N/A answer is appropriate and correctly justifies the nature of the work as being empirical andexperimental, without theoretical claims that require the provision of assumptions or proofs.",
  ". Ensure that any future versions of the paper or related works that might include theoretical aspects": "clearly outline assumptions and provide complete proofs or references to where these can be found, adheringto the guidelines for papers with theoretical content.Question 4: Does the paper fully disclose all the information needed to reproduce the main experimentalresults of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardlessof whether the code and data are provided or not)?Answer: YesJustification: We have disclosed all the details of our experiments and provide code and instructionspublicly on the Meta-Album GitHub repository ( for repro-ducibility and results verificationReview: The proposed authors Answer and Justification suggest that all necessary information forreproducing the main experimental results is disclosed and made publicly accessible via their GitHub repos-itory, which ostensibly contains the code and instructions. According to the guidelines, this should indeedinclude detailed descriptions of the experiments, code, architecture, datasets used, preprocessing steps, and other relevant details for reproducibility. However, the given paper extract does not directly confirm orprovide specific sections where this information can be found within the paper itself, as required by theguidelines for a Yes answer.",
  "By addressing these points, the paper would not only comply with the NeurIPS reproducibility criteria": "but also provide the scientific community with a comprehensive resource for validating and building uponthe presented work.Question 5: Does the paper provide open access to the data and code, with sufficient instructions tofaithfully reproduce the main experimental results, as described in supplemental material?Answer: YesJustification: We have publicly provided the code on our github repository and details about how to accessthe datasets on our website.Review: Feedback and Actionable Points for Improvement:",
  ". Clarify Exact Location of Reproducibility Materials: While the Authors Proposed Justification": "mentions that code and data access details are provided on GitHub and the website, it lacks specific refer-ences to sections in the paper that outline these aspects. Ensure that exact locations within the paper (e.g.,section numbers or Appendix references) are given to direct readers efficiently to supplementary materialsfor reproducing main experimental results.",
  "Conclusion:The response to the checklist question attempts to affirm the open accessibility and reproducibility of": "the experimental results but lacks specificity in its justification. Directing to a general GitHub repositoryand a website does not fully assure the checklists criteria unless explicit, easy-to-follow instructions and ref-erences to these in the main paper are provided. Adaptations based on the feedback above can enhance thepapers adherence to NeurIPS guidelines on reproducibility and open-access practices, making it a strongersubmission.Question 6: Does the paper specify all the training and test details (e.g., data splits, hyperparameters,how they were chosen, type of optimizer, etc.) necessary to understand the results?Answer: YesJustification: We have explained the experimental settings or details in .2: Experiments. Addi-tional details can be found in the Meta-Album GitHub repository.Review: The papers Question section focuses on whether the paper specifies all necessary details forunderstanding the results, including training and test details like data splits, hyperparameters, type ofoptimizer, etc. The authors proposed Answer is Yes, and the Justification points to .2(Experiments) and the Meta-Album GitHub repository for experimental settings or details.",
  "Conclusion:The paper needs to incorporate improvements to fully meet the NeurIPS checklist criteria regarding the": "specification of training and test details. Adhering to the actionable feedback provided will significantly en-hance the papers quality and compliance with NeurIPS guidelines.Question 7: Does the paper report error bars suitably and correctly defined or other appropriate informationabout the statistical significance of the experiments?Answer: YesJustification: In figure-2, figure-3, Appendix E: Within Domain Few-shot learning, and Appendix F: Cross-Domain Few-shot learning, we have reported error barsReview: The proposed authors answer indicates that error bars have been reported in figures and ap-pendices specified, which suggests adherence to the guidelines that require such statistical measures to beincluded for the experiments. However, the feedback and improvements depend on verification against theactual paper content as provided, which is crucial for assessing adherence to the guidelines fully. Based on theavailable text, the following feedback is provided to further align with the NeurIPS conference requirements:",
  ". Clarity on Variability Factors and Error Bar Calculation: The paper should clearly explain": "what variability the error bars are capturing. This detail is critical to understand the reliability of the re-ported results. If this is currently not mentioned, it should be added to the relevant sections where errorbars are reported. The authors could consider specifying whether the variability is due to initial conditions,training/test splits, or any other experimental stochasticity.",
  ". Method of Calculating Error Bars: It is unclear from the given text how the error bars were": "calculated. The paper should explicitly state whether a standard deviation, standard error of the mean, oranother method was used to calculate the error bars. Moreover, whether any assumptions were made (e.g.,assuming normal distribution of errors) should be clarified. If this is not included in the current version,adding a brief methodological explanation either in the figure captions where error bars are reported or inthe methodology section would improve the paper.",
  "By addressing these points, the paper can offer a more comprehensive statistical account of the experi-": "mental results, in line with the NeurIPS guidelines. This will not only improve readability and interpretabilitybut also enhance the papers scientific rigor.Question 8: For each experiment, does the paper provide sufficient information on the computer resources(type of compute workers, memory, time of execution) needed to reproduce the experiments?Answer: YesJustification: We have provided details about compute resources used in .2: Experiments, Ap-pendix E: Within Domain Few-shot learning, and Appendix F: Cross-Domain Few-shot learningReview: The authors proposed answer and justification appear seem consistent with the guidelines pro-vided and the content outlined in the paper. The paper describes the Meta-Album dataset and its applicationfor few-shot learning in section 3 of the paper, and details related to computational resources used for ex-periments can be found in Appendices E and F, as mentioned in their justification. However, to provideconcrete and actionable feedback for improvement based on the guidelines provided and the detailed contentpresented in the paper, the following items should be addressed:",
  "- Revisit Appendices E and F to ensure they are directly and clearly addressing the guidelines requests": "on reporting computational resources.Question 9: Does the research conducted in the paper conform, in every respect, with the NeurIPS Codeof Ethics YesJustification: We completely comply with the NeurIPS Code of EthicsReview: Based on the guidelines provided for answering the checklist question related to the NeurIPS Codeof Ethics compliance and the content available in the proposed paper, several discrepancies and areas ofimprovement have been identified. The primary issue is the lack of specific references to sections in the paperthat demonstrate compliance with the NeurIPS Code of Ethics.",
  "Feedback and Actionable Recommendations:1. Explicitly Mention Specific Sections: The authors justification for answering Yes to the ques-": "tion Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics?is too general. It is recommended that the authors explicitly mention the specific sections of the paper whereethical considerations, data usage permissions, and any potential ethical implications of their research arediscussed. This level of detail is necessary for adequately demonstrating compliance with the NeurIPS Codeof Ethics.",
  ". Data Usage Permissions and Licenses: While the paper briefly mentions verified licenses for": "datasets used in the research, it is recommended to elaborate on these aspects in the paper. Specifically, theauthors should:- Provide a detailed account of how data usage complies with the respective licenses and any ethical consid-erations taken into account while using these datasets.- Discuss the process of data acquisition, processing, and usage in compliance with ethical standards, par-ticularly if data involves human subjects or potentially sensitive information.",
  "By addressing these recommendations, the authors will significantly strengthen the ethical justification": "of their work, ensuring it strictly conforms to the NeurIPS Code of Ethics. More detailed and specific dis-cussions related to ethics will not only improve the papers quality but also enhance its contribution to thefield responsibly.Question 10: Does the paper discuss both potential positive societal impacts and negative societal impactsof the work performed?Answer: YesJustification: There are no negative societal impacts. Rather, this meta-dataset can foster progress in thefields of few-shot learning and meta-learning. We have added recommended use in .3.Review: The authors proposed answer is affirmative, stating that the paper discusses the societal impactsof their work. However, their justification only mentions the positive impacts and specifically denies theexistence of any negative impacts, which contradicts the requirement to discuss both potential positive andnegative societal impacts.",
  ". Engage with Broader Community: Encourage feedback from the broader machine learning and": "societal impact research communities on potential negative impacts that the authors might have overlooked.This can be achieved through workshops, preprints for community feedback, or collaboration with researchersin ethics and policy.Question 11: Does the paper describe safeguards that have been put in place for responsible release of dataor models that have a high risk for misuse (e.g., pretrained language models, image generators, or scrapeddatasets)?Answer: YesJustification: Indeed, we are releasing our datasets with proper license and terms of use. We have alsoprovided recommended usage of these datasets.Review: The answer provided by the authors indicates that they have put safeguards in place for theresponsible release of data by releasing datasets with proper licensing and terms of use. However, they didnot point to the specific sections in the paper where these details are discussed, which is a requirementaccording to the guidelines for answering the checklist question. Furthermore, the paper discusses the releaseof a multi-domain meta-dataset designed to facilitate few-shot learning among other tasks, and details aboutlicense information are provided in Appendix B, but there is no explicit discussion on the safeguards forresponsible release beyond licensing, such as control over access or usage restrictions for highly sensitivedata.",
  "also contribute to setting a standard for the release of similar datasets in the future.Question 12: Are the creators or original owners of assets (e.g., code, data, models), used in the paper,": "properly credited and are the license and terms of use explicitly mentioned and properly respected?Answer: YesJustification: We are releasing our datasets with proper license, we are also mentioning the original licensesfor the original datasets in Appendix B: License information of Meta-Album datasetsReview: The authors proposed answer and justification indicate that they have credited the original cre-ators of assets used in their work and have mentioned the licenses and terms of use for these assets inAppendix B of their paper, as per the guidelines. However, theres no explicit mention of whether all code,data, models, or other assets not created as part of the Meta-Album project have been similarly acknowledgedand whether their licenses have been respected as per the requirement.",
  ". Third-Party Asset Acknowledgment: For any third-party assets (e.g., pre-trained models, software": "libraries) used that were not explicitly created by the paper authors, include a section acknowledging theseassets and their original creators. Provide a citation to the original works and mention the assets license ifapplicable. This section can be included in the appendix or as part of the main text where these assets arefirst mentioned.",
  "By addressing these points, the paper will offer clarity on the intellectual property considerations and": "ensure that it respects and acknowledges the contributions of all asset creators correctly.Question 13: Are new assets introduced in the paper well documented and is the documentation providedalongside the assets?Answer: YesJustification: We have properly documented our new datasets using meta-data files and datasheets fordatasets (provided in the appendices).We have also designed a website for users to easily get all thedatasets and related informationReview: Based on the provided information, the key issues identified in relation to the Question regardingdocumentation and availability of new assets are as follows:",
  ". Anonymization During Review Process: Verify and ensure that any shared URLs or references to": "datasets and assets are appropriately anonymized if required by the submission guidelines, particularly dur-ing the review phase to comply with double-blind review requirements.Question 14: For crowdsourcing experiments and research with human subjects, does the paper includethe full text of instructions given to participants and screenshots, if applicable, as well as details aboutcompensation (if any)?Answer: Not ApplicableJustification: This does not apply on our research as we are not doing any crowdsourcing experimentsReview: The provided Paper content accurately supports the authors answer that crowdsourcing ex-periments and research with human subjects are not applicable to their work. The paper introduces Meta-Album, a meta-dataset for few-shot image classification, which is compiled from various open datasets forfacilitating research in meta-learning and does not involve any crowdsourcing or direct research with humansubjects. The justification aligns with the guidelines provided, which state that if the research does notinvolve crowdsourcing nor human subjects, answering N/A is appropriate.",
  "Conclusion: The authors response and justification regarding crowdsourcing and human subjects research": "are valid based on the provided paper content. However, enriching the document with explicit clarificationsand enhancements as suggested would align the research closer to best practices in ethical AI research anddataset compilation.Question 15: Does the paper describe potential risks incurred by study participants, whether such riskswere disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalentapproval/review based on the requirements of your country or institution) were obtained?Answer: Not ApplicableJustification: This does not concerns us as we are not dealing with human subjects in our research.Review: Given the provided context and guidelines, the papers content and the authors response to theNeurIPS checklist question regarding the study participants and potential risks do seem aligned, consider-ing the focus on image classification without involving human subjects directly. The nature of the paper,Meta-Album: Multi-domain Meta-Dataset for Few-Shot Image Classification, does not inherently requirethe involvement of human subjects in a capacity that would usually pose ethical concerns or necessitateIRB oversight. The datasets compiled and utilized appear to be primarily sourced from publicly availableor licensed image datasets, thus falling outside the scope of research requiring such review or disclosureconcerning human subject participation or potential risks."
}