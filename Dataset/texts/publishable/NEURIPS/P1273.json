{
  "Abstract": "In this paper, we present an intuitive analysis of the optimization technique based on the quan-tization of an objective function. Quantization of an objective function is an effective optimiza-tion methodology that decreases the measure of a level set containing several saddle points andlocal minima and finds the optimal point at the limit level set. To investigate the dynamics ofquantization-based optimization, we derive an overdamped Langevin dynamics model from an in-tuitive analysis to minimize the level set by iterative quantization. We claim that quantization-basedoptimization involves the quantities of thermodynamical and quantum mechanical optimization asthe core methodologies of global optimization. Furthermore, on the basis of the proposed SDE, weprovide thermodynamic and quantum mechanical analysis with Witten-Laplacian. The simulationresults with the benchmark functions, which compare the performance of the nonlinear optimiza-tion, demonstrate the validity of the quantization-based optimization.",
  ". Introduction": "From a conventional engineering perspective, quantization is one of the significant signal process-ing techniques, such as effectively compressing original data. For a long time, the goalof quantization research has been to reduce the quantization error and restore an original signalfaithfully from compressed data by quantization. Instead of researching quantization as a branch ofsignal processing, we could find an effective non-convex optimization algorithm that quantizes thelevel set of an objective function and appropriately decreases the quantization step to time index.Furthermore, conventional research on quantization demonstrates that if the quantization error doesnot depend on the original signal when we quantize the signal uniformly for a large amount of data,the quantization error follows an independent increment distribution(i.i.d.). Therefore,we can design an effective stochastic optimization algorithm using a suitable quantization processfrom the i.i.d. quantity of the quantization error. For this purpose, we present an intuitive stochas-tic analysis of an optimization algorithm based on quantization applied to a random search. Theproposed analysis presents the stochastic differential equation (SDE) to describe the dynamics ofquantization-based optimization. We can establish the Witten-Laplacian to demonstratethat the quantization to an objective function provides the escape property from local minima. Fi-nally, we verify the validity of the proposed analysis by comparing the optimization performanceof the quantization-based algorithm with that of other conventional global optimization algorithmssuch as simulated annealing(SA) and quantum annealing(QA).",
  "The conventional research relevant to signal processing defines a quantization such that xQ x": "+12 for x R, where R+ denotes a fixed-value quantization step. We provide amore detailed definition of quantization to explore the impact of the quantization error, using thequantization parameter as a reciprocal of the quantization step such that Qp 1, so we denotethe quantization parameter as Qp and the quantization step as Q1p , respectively.",
  "EqQ1p q = 0,EqQ2p q = Q2p Eqq2 = 1/(12 Q2p)(3)": "Furthermore, we can establish an independent stochastic process with a fraction for quantizationsuch as (qt)t0. Based on Assumption 1, such a stochastic process is an i.i.d. process, and wecan regard the stochastic process (qt)t0 as a white noise, which is known as the White NoiseHypothesis (WNH), from conventional researches such as . In order to proceed withthe main discussion, we consider the optimization problem for an objective function f C suchthatminimize f : Rd R+.(4) For a combinatorial optimization problem such as the Traveling Salesman Problem(TSP), we dealwith an actual input represented as x m. In such a case, we assume that there exists a propertransformation from a binary input to a real vector space such that T : m X Rd, where Xrepresents the virtual domain of the objective function f. Consequently, we consider the objectivefunction as (4) regardless of whether the domain is related to the problem.Finally, we provide the following assumption for the virtual objective function.",
  ". Primitive Analysis of Quantization-based Optimization Algorithm": "We present a fundamental optimization algorithm as Algorithm 1 for combinatorial optimizationwith a binary domain and a general optimization problem with a continuous domain. The presentedalgorithm is similar to the elementary MCMC algorithm except for the procedure under the if clause,as it is based on a random search employed in SA and QA. The most crucial difference is that thepresented algorithm quantizes the objective function regarding a randomly selected candidate xt,and the quantization error induced by the quantization adds i.i.d. noise to the original objectivefunction, such that fQ(xt) = f(xt) + Qtt, where the time index t is equal to in Algorithm 1.From the perspective of updating the parameter, this operation is similar to an annealing operationdeduced by the acceptance probability in SA and QA.The other difference is that the algorithm compares the quantized temporary optimal objectivefunction denoted as fQopt with a quantized objective function to the candidate fQ(xt). Naturally, thequantized function is not a real objective function value, so that we can model a virtual objectivefunction for the quantized objective function. For instance, when we represent the objective functionwith a power series based on the given base value b denoted in (2) such that f(xt) = f0k=0 ckbk,we can write the quantized objective function as follows:",
  "j=n+1cjbj = fQ(xt) Q1p (t)t.(7)": "In (7), we can set the quantized objective function such as fQ(xt) = f0 + ni=1 cibi, and thequantization error as Q1p (t)t = j=n+1 cjbj. Therefore, since the quantization step Q1p (t)decreases with the power of b as represented in Algorithm 1 and Definition 2, the equation (7) issimilar to the Hamiltonian approximation for a tunneling effect in QA.Moreover, comparing the quantized value of the objective function in the algorithm does notrequire accurate modeling of the quantization error, so we can design a virtual objective functionfor the part of the quantization error. Notably, instead of using the objective functions accurateHessian, we can design the virtual function using a simple Hessian for convenient analysis. Thismethod helps to analyze and establish a learning equation for machine learning with the proposedquantization. In the following chapters, we will discuss the design of the virtual objective functionin more detail.",
  ". Association to the Stochastic Differential Equation(SDE)": "As shown in Algorithm 1, the update condition for the temporary optimal point is that fQ(xt+1)is less than or equal to fQ(xt), that is, fQ(xt+1) fQ(xt). Since fQ(xt+1) < fQ(xt) is anevident update condition, we do not consider this case. Instead, we investigate the condition whenthe quantized objective function is equal, i.e. fQ(xt+1) = fQ(xt).First, we establish a discrete search equation to describe the dynamics of the proposed algorithmregarding the condition of equal quantized objective function. Since the algorithm minimizes theobjective function, the search equation requires a negative gradient as a primary directional deriva-tive. However, as is well known in convex optimization, a positive definite Hessian to the objectivefunction gives information about local minima from the Taylor expansion with the search equation.On the other hand, several local minima and saddle points can exist in the sufficiently large domainof the equal quantized objective function. Accordingly, the search equation with a negative gradi-ent needs additional components to elaborate the algorithms dynamics due to the limitation of theTaylor expansion based on a quadratic approximation. To compensate for such a limitation, we adda random vector rt Rd, which we assume that the expectation is zero, to the candidate of thesearch equation as follows:",
  "the assumption of the quantized objective functions and = 10": "We suppose that the algorithm success to update when fQ(xt+1) fQ(xt) and fail to update whenfQ(xt+1) > fQ(xt). Although we cannot estimate the probability of success correctly, we cansuppose that the distribution of the norm of rt is symmetric to the condition of fQ(xt+1) = fQ(xt).Therefore, we consider that the variance of rt is equal to 20Q1p (t)Id. Furthermore, rt affectsprimarily the real value of the objective function f(xt) and not significantly on fQ(xt) affectedby the quantization step Q1p (t). Accordingly, since rt is an independent random vector for thequantization error, we can regard rt as Gaussian white noise without loss of generality. Therefore,we obtain the following theorem regarding SDE to describe the proposed search algorithm. Theorem 2Based on the given candidate (8) of the search equation and the variance of rt, i.e.,Ertrt = 20Q1p (t)Id, we can obtain the approximated SDE form for the proposed quantization-based search algorithm as follows:",
  ". Quantum Mechanical Quantities of the Quantization-Based Optimization": "In the previous section, we present the search equation formed as the stochastic difference equation(8) and the approximated SDE (9) for the quantization-based optimization. Comparing both equa-tions, when we design the noise model that directly affects the directional derivative, the SDE forthe search equation denotes the standard SDE employed with a damped variance term representedas a simulated temperature or a quantization step, as we propose.An additional advantage of the SDE (9) is that we can analyze the dynamics of the algorithmwith quantum mechanics. For this purpose, we introduce the Fokker-Plank equation (FPE) for theSDE (9) with a differential operator L as follows :",
  "t = L,L = x (xf(xt)) + Q1p (t),(10)": "where denotes the standard Laplacian, (x, t) : Rd R+ R is the density of therandom vector Xt defined in (9). The stationary solution of (10) is the well-known Boltzmann-Gibbs distribution Z1 exp(Qp(t)f(xt)), where Z = Rd exp(Qp(t)f(x))dx < .For the analysis based on quantum dynamics under the regime of a small quantization step(Q1p (t) 0), we derive a following Witten-Laplacian on 0-forms (0)f,h associated with f and thesmall parameter h(t) 2Q1p (t) from the FPE (10):",
  "where V denotes a potential energy defined as V = m": "2 (xf2 hxf), m denotes a mass ofa particle, and denotes the reduced Plank constant.To verify the claims that the search algorithm based on the quantized objective function inChapter 2 involves a quantum mechanical effect, we investigate the potential energy V when thesearch parameter xt is trapped in a local minimum. Under the assumption, we note that the normof the gradient xf is zero at the local minimum. In spite of the case, the Laplacian term2Q1p (t)xf still remains non-zero in V ; thus, for small Q1p (t) = h",
  "2xu exp(f/h) > 0(13)": "Equation (13) reveals that the proposed algorithm can increase the probability density (x, t) tomove the searching point through a potential barrier even if xt falls in a local minimum. Addition-ally, the stop condition of the algorithm depends on the quantization step Q1p (t), which decreasesto zero. According to the tunneling effect provided by the Schrodinger equation, the height of theenergy barrier that the algorithm can penetrate is equal to the quantization step Q1p (t) since theproposed algorithm quantizes the objective function.",
  ". Experimental Result": "We conducted experiments on well-known continuous benchmark functions such as Xin-She YangN4, Salomon, Drop-Wave, and Shaffer N2 to compare the optimization performancewith thermodynamic-based (SA), quantum mechanical (QA) and quantization-based algorithms.Since all benchmark test functions are continuous, simulated annealing, quantum annealing, andthe proposed quantization-based optimization are enabled to find the global minima within finite it-erations. Furthermore, the quantization-based optimization finds the global minimum within feweriterations than the SA and QA algorithms. We predict that the energy barrier induced virtually bythe quantization is relatively lower and more easily penetrated than the natural barrier imposed bythe object function. Such a quantity of the quantization-based search algorithm can improve theoptimization performance. As for the experiments on the Xin-She-Yang N4 function, quantum an-nealing fails to find the global minimum, whereas SA and the proposed algorithm successfully findit. The simulation result demonstrates that the presented analysis based on the FPE to thermody-namical and the proposed quantization-based optimization is valid.",
  ". Conclusion": "We present an intuitive analysis of quantization-based optimization based on stochastic analysisand quantum mechanics. The proposed SDE for the algorithm is a standard overdamped Langevindynamics appropriate to the algorithms dynamics. Based on the presented SDE, we provide an FPEand Witten-Laplacian, including the quantization parameter, to analyze the algorithms dynamics.However, despite the significant performance difference, the analysis formulas presented in thispaper are similar to those of SA. We suspect that the tunneling effect, which we have not investigatedsufficiently, is the primary cause. In future work, we will analyze the algorithms dynamics inmore detail using a quantum mechanical perspective and combine the quantization of the objectivefunction with the learning equation in machine learning.",
  "S. Kirkpatrick, C. D. Gelatt, and M. P. Vecchi. Optimization by simulated annealing. SCI-ENCE, 220(4598):671680, 1983": "Mohamed Lalaoui, Abdellatif El Afia, and Raddouane Chiheb. Simulated annealing with adap-tive neighborhood using fuzzy logic controller. In Proceedings of the International Conferenceon Learning and Optimization Algorithms: Theory and Applications, LOPAL 18, New York,NY, USA, 2018. Association for Computing Machinery. ISBN 9781450353045. Dorian Le Peutrec and Boris Nectoux. Small eigenvalues of the witten laplacian with dirichletboundary conditions: the case with critical points on the boundary. Analysis & PDE, 14(8):25952651, 2021. doi: 10.2140/apde.2021.14.2595.",
  "D. Marco and D.L. Neuhoff.The validity of the additive noise model for uniform scalarquantizers. IEEE Transactions on Information Theory, 51(5):17391755, 2005. doi: 10.1109/TIT.2005.846397": "Luong Trung Nguyen and Byonghyo Shim. Gradual federated learning using simulated an-nealing. In ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech andSignal Processing (ICASSP), pages 30903094, 2021. L.M. Rasdi Rere, Mohamad Ivan Fanany, and Aniati Murni Arymurthy. Simulated annealingalgorithm for deep learning. Procedia Computer Science, 72:137144, 2015. ISSN 1877-0509. The Third Information Systems International Conference 2015.",
  "(1s)ht2xf(xt+sht)htds+Q1p (t)(t+1t) (14)": "To investigate the condition for no imposed random vector such that rt = 0, we let the directionalderivative as ht = xf(xt), for R(0, 1). Additionally, according to Assumption 2, weemploy a virtual function f(xt) that satisfies f(xt) = f(xt) and fQ(xt+1)fQ(xt) = fQ(xt+1)fQ(xt). Hence, we can obtain the following equation:",
  "INTUITIVE ANALYSIS OF THE QUANTIZATION-BASED OPTIMIZATION: FROM STOCHASTIC AND QUANTUM MECHANICAL PER": "Suppose that the parameter xt remains a local minima x, and the assumption implies thatxf(x) = 0 at t R+. Particularly, from the fact that h = 2Q1p , we assume that h is sufficientlysmall to neglect the effect of hxu.Holding such the assumption for paralysis of searching caused by local minima, we note thatthere exists a non-zero term xf in the RHS of the Witten-Laplacian (46). Since the trappingpoint is a local minimum, the eigenvalue of the Laplacian xf is positive, and it implies that theprobability density represents increasing as follows:",
  "D.5. Specification of the Benchmark Functions in Simulation Results": "The benchmark functions used for optimization performance tests are widely known as the com-plication of finding the global minimum. represents the 3D plot of each benchmark. Allfunctions involve several local minima and the global minimum at x = 0. shows how thequantization-based optimizer searches for the global minimum of the benchmark functions, with the1D plotted on the Y-zero axis. Those limited searches on a one-dimensional plane cause the conver-gence point to be slightly different from the global minimum due to the limited gradient. However,as shown in the figures, even if the algorithm is trapped at a local minimum point, the algorithm canescape the local minimum, proceed with searching, and finally converge to the global minimum."
}