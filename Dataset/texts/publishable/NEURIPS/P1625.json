{
  "Abstract": "Subspace learning is a critical endeavor in contemporary machine learning, par-ticularly given the vast dimensions of modern datasets. In this study, we delveinto the training dynamics of a single-layer GAN model from the perspective ofsubspace learning, framing these GANs as a novel approach to this fundamentaltask. Through a rigorous scaling limit analysis, we offer insights into the behaviorof this model. Extending beyond prior research that primarily focused on se-quential feature learning, we investigate the non-sequential scenario, emphasizingthe pivotal role of inter-feature interactions in expediting training and enhancingperformance, particularly with an uninformed initialization strategy. Our investi-gation encompasses both synthetic and real-world datasets, such as MNIST andOlivetti Faces, demonstrating the robustness and applicability of our findings topractical scenarios. By bridging our analysis to the realm of subspace learning,we systematically compare the efficacy of GAN-based methods against conven-tional approaches, both theoretically and empirically. Notably, our results unveilthat while all methodologies successfully capture the underlying subspace, GANsexhibit a remarkable capability to acquire a more informative basis, owing to theirintrinsic ability to generate new data samples. This elucidates the unique advantageof GAN-based approaches in subspace learning tasks.",
  "Introduction": "Subspace learning is a widely explored task, especially with the growth of dimensionality in moderndatasets. It is important to identify meaningful subspaces within the data, such as those determinedby principal component analysis (PCA). However, due to the high dimensionality of the data, itis common to employ online methods such as Ojas method and GROUSE . Meanwhile,Generative Adversarial Networks (GANs) , primarily used as generative models, have alsodemonstrated the ability to learn meaningful representations of data . Inspired by this, weexplore how single-layer GAN models can be viewed as a form of subspace learning. We seek to improve the understanding of GAN training by relaxing some common assumptionsmade in previous analysis of GANs . Specifically, we focus on the training dynamics of thegradient-based learning algorithms, which can be converted into a continuous-time stochastic processcharacterized by an ordinary differential equation (ODE). Furthermore, the dynamics of the modelweights form a stochastic process modeled by a stochastic differential equation (SDE). Understandingthese two equations provides the relevant information to understand convergence behaviour of the",
  "training. We extend previous work to discriminators with same dimensionality as the generator.Finally, we discuss the new training outcomes that can arise in the higher-dimensionality case": "Our work explores what happens when the training switches from sequential learning in the single-feature discriminator case , to the non-sequential (multi-feature) learning of our discriminator. Weshow that the non-sequential learning of features not only allows for faster learning and convergence,but also a higher maximum similarity with the true subspace compared to the sequential case, wheneverything else is kept the same. This shows that contrary to the general approach of makingdiscriminators much weaker than generators, it is still possible to use a powerful discriminator. Infact, doing so can lead to much faster training with better performance, through careful choice oflearning rates. We further show that our new framework can be used to analyze the cases where we assume differentdimensionalities between the true subspace, fake subspace, and discriminator. Through the use of asimple uplifting trick on the relevant Grassmannians, we are able to extend our analysis to arbitrarydimensionalities. To understand how GANs compare with existing subspace learning algorithms, weprovide both theoretical and empirical comparisons with existing such algorithms. We see that thefeatures learned by a GAN model are more meaningful and represent the data better as compared toOjas method, due to the requirement of being able to generate new data from the underlying datadistribution. Finally, we test our approach using two prominent real-world datasets: MNIST and Olivetti Faces,comparing our method against a sequential discriminator, and show that all of the key insightsgained through the theoretical analysis are visible in training on this dataset as well. This showsthat our analysis has very practical applications, and can lead to interesting research directionsexploring these ideas in more powerful GAN architectures. This testing is additionally done onthe case where we assume different dimensionalities for each component of the model, showingthat the results are as expected. We release all our code at",
  "Training dynamics for GANs": "Much of this work is inspired by Wang et al. , which was one of the first to undertake thistask. However, in this case, they use a single-feature discriminator, and show that the choice oflearning rates relative to the strength of noise is what determines the outcome of the training process:convergence, oscillations, or mode-collapse. There have been further attempts to understand the convergence of GANs through other approaches,not just the dynamics of the gradient-based optimization. Heusel et al. showed that underreasonable assumptions on the training process and hyperparameters, using a specific update rule willguarantee that the GAN converges to a local Nash equilibrium. Mazumdar et al. introduced a newlearning algorithm under which the local Nash equilibria are the only attracting fixed points, so thatthe training algorithm tends to move towards these points. This type of analysis is very similar to ourapproach, focused on understanding fixed points. Other types of models have also been analyzed inthe high-dimensional regime, such as linear VAEs , two-layer neural networks solving a regressiontask , and two-layer autoencoders . A review of these methods can be found in .",
  "Subspace learning": "Subspace learning is a heavily explored field, with many algorithms. However, when the noise ofthe data has a non-zero variance, most approaches fail, and the general technique used to solve theproblem is some type of online PCA-based method. In Wang et al. , a similar analysis of dynamicsthrough ODEs is performed for multiple algorithms which learn subspaces with non-zero variance ofnoise. This analysis allows for steady-state and phase transition analysis of these algorithms. Balzanoet al. presents a survey of different online PCA algorithms used for subspace learning, in thecase where only some of the data is visible at each timestep, and discuss how it is possible to find aunique subspace of a given rank which matches all the provided data.",
  "Our data yk is drawn from the following generative model, known as a spiked covariance model :yk = Uck + T ak(1)": "Here, U Rnd is the true subspace we wish to learn represented as an orthonormal basis, ck Rdis a zero-mean random vector with covariance matrix , ak Rn is a standard Gaussian vector, andT represents the noise level. The spiked covariance model is very widely studied, due to the non-triviality of learning U wheneverT > 0. The key property of this model is that the top d eigenvectors of the data covariance E[ykyTk ]are given by the columns of U. If there exists a strict eigengap between the top d correspondingeigenvalues and the other eigenvalues, then the reconstruction loss function is proven to have U as aglobal minima .",
  "Online subspace learning algorithms": "Subspace learning is a very important task in machine learning, most commonly performed by algo-rithms such as PCA or ICA. However, these approaches involve costly operations such as calculatingcovariance matrices or calculating matrix inverses, infeasible in high dimensions. Therefore, it isvery common to use an online version of these algorithms, processing samples one at a time. Online subspace learning algorithms typically fall into two categories: algebraic methods andgeometric methods. Algebraic methods are based on computing the top eigenvectors of somerepresentation of a sample covariance matrix. Assuming a strict eigengap, the top eigenvectorswill yield the true subspace. Meanwhile, geometric methods optimize a certain loss function oversome geometric space (Euclidean space or a Grassmannian manifold). We review two subspacelearning algorithms here, Ojas Method and GROUSE . While GROUSE was introduced forthe missing data case, it can be used for full data too. For further details about these algorithms andtheir categorization, we direct the reader to . However, we suggest a third category of online subspace learning algorithms, which we call thegenerative methods. Such methods, including single-layer GANs, do not have information about thespecific task, and instead aim to learn the data simply by seeing the data and attempting to generatedata from the same distribution.",
  "Generative Models": "Here, we focus specifically on GANs. A GAN model seeks to learn a representation of the underlyingsubspace through the use of two components: a generator and a discriminator. The generator learnsthe subspace by trying to generate new samples from the subspace, while the discriminator acts as aclassifier, attempting to distinguish data from the true subspace from data produced by the generator. Note that measuring performance through cosine similarity can actually be viewed as a way tomeasure the generalization performance of the generator, as it doesnt depend on any specific instanceof generated data and instead provides a concrete measure of how similar the generated data will be.",
  "The learning in the GAN model critically depends on the choice of discriminator, which aims toseparate the data from the true and generated subspaces": "The most common approach when training GANs is to use a discriminator that is weaker than thegenerator. If the discriminator is too strong, then it will easily learn to distinguish between true andgenerated samples, leading to vanishing gradients for the generator and thus preventing learning.However, a weak discriminator results in sequential learning, where the generator is only able tolearn a subset of the features at a time. In multi-feature cases, this will lead to very slow learning.",
  "D(y; W) = D(yT W)(5)": "where D : Rn R is some function (see the assumptions below). Since this discriminator is able tofocus on all the features at once, this means the generator is also able to learn every feature at once.This is in contrast to the single-feature case (where W Rn) analyzed previously. While this is astrong assumption on the discriminator, we show below how this assumption can be relaxed.",
  "Training procedure": "GAN training is modeled as a two-player minimax game, where the discriminator attempts tomaximize some loss function and the generator attempts to minimize it. This is used as a way to learna \"surrogate\" subspace which represents the true subspace. Therefore, the GAN model can be seen asa form of subspace learning, except that the focus is on generating new samples from the subspace. Specifically, let L(y, y; W) be a loss function depending on the discriminator weights, and true andfake samples. If G denotes the true distribution and G denotes the generator distribution, the minimaxgame can be represented asminV maxW EyGEy GL(y, y; W).",
  "tr(H(VT V))(6)": "Here, F, F are functions affecting the outputs of the discriminator, H is an element-wise functionused for regularizing the weights of the generator and discriminator, and > 0 controls the strengthof the regularization. As , the matrices V, W will become orthonormal. The standard approach to solve this minimax game is using stochastic gradient descent (SGD). Attimestep k, given a sample yk from the true subspace and a sample yk from the generator subspace,we perform the following updates:",
  "Development of ODE": "Similar to , we make the following definitions:Definition 4.1. Xk := [U, Vk, Wk] Rn3d is called the microscopic state of the training processat time k.Definition 4.2. The tuple {Pk, Qk, Rk, Sk, Zk} is called the macroscopic state of Xk at time k, wherePk := UTk Vk, Qk := UT Wk, Rk := VTk Wk, Sk := VTk Vk, and Zk := WTk Wk. The macroscopicstate can be written in matrix notation as Mk = XTk Xk, in which we get",
  "(A.6) The columns of the discriminator matrix W are orthonormal, so that WT W = Id": "Assumptions (A1) and (A2) are the usual i.i.d assumptions common in machine learning. (A3) isimportant for deriving the update equations. (A4) and (A5) are used to guarantee that the macroscopicstate can converge. Our assumption (A6) of orthonormal discriminator matrix allows us to simplifythe equations since the Z matrix of the macroscopic state is always just Id. Under these assumptions, as well as letting , we obtain a modified Theorem 1 from Wanget al. , specifically considering the reduced case of equation (13). Note that our choice of F, F, Dmeans that our equations become an arbitrary-dimensional version of the original equations.",
  "Simulations": "In order to demonstrate that the ODE properly represents the training dynamics of the GAN model,we first perform simulations and show that the empirical results match the ODE, seen in .To understand how the training dynamics change based on the generator learning rate, we fix thediscriminator learning rate as = 0.2 and fix the generator learning rate = 0.04. We show theresults on 4 different noise levels. In all cases, we let = = diag([",
  "])": "We set P0 = Q0 = 0.1 I, and we ensure that the empirical setup is initialized with exactly matchingP and Q values. We note that the ODE will never learn when the initialization is exactly 0, and sowe must provide some level of similarity to start training. However, this is not very restrictive, as ourexperiments show that even random matrices will have approximately 0.001 I for both P and Q,which is sufficient to escape the fixed point around 0. : ODE results for learning rate = 0.04, = 0.2 and four different noise levels, with d = 2.The columns represent G = T = 2, 1, 3, 4 respectively. At = 5 or higher, the generator is unableto learn anything. In all cases, the green and red represent the two diagonals of P, and the blue andyellow represent the two diagonals of Q. We see that the simulations do match the predicted ODEresults.",
  "Off-diagonal simulations": "A key insight found from the multi-feature discriminator is that the interaction between differentfeatures can help learning. When the macroscopic states are initialized to non-diagonal matrices, wesee that the dimension with smaller covariance is actually able to attain better results and reach asimilar cosine similarity to the dimension with higher covariance. Such an outcome is not possible inthe sequential learning regime, due to the lack of interaction between features. In sequential learning,features are learned one at a time, and once a feature has been learned, the training will focus on adifferent feature instead. This phenomenon can be seen in , showing that the off-diagonalinitialization allows for not only faster training (which also happens in the diagonal initializationcase), but also higher steady-state values compared to the sequential learning case. We are unable toprovide a detailed characterization of these fixed points, as a neat closed-form solution cannot beobtained. : ODE results when initialized with off-diagonal entries. We focus on the case G = T = 2,as that noise level is seen above to be ideal for learning. Additionally, in all cases, = 0.04, = 0.2.The solid lines are with our approach, while the dashed lines are using the discriminator in Wang et al.. From left to right, we use an initialization of 0.1, 0.01, 0.001, 0.0001 for each component of themacroscopic states. It can be seen that our approach outperforms the single-feature discriminator inevery case, with the gap becoming larger as the initialization approaches 0.",
  "Unknown number of features": "While this type of analysis can provide interesting insights, it has a very restrictive assumption thatwe know the number of features d. This is done so that the macroscopic states are well-defined.However, we now seek to extend this analysis to the case where the true subspace has d features, thegenerator subspace has p features, and the discriminator learns q features, where we do not assumethat d = p = q. While this analysis can be performed under any assumptions on the relative size of d,p, and q, we focus on the single case d q p n.",
  "contains the first d standard basis vectors. We introduce the idea of uplifting (inspired by the work in) the matrices U, W to the dimensionality of V": ": The graph shows the Grassmann distance over time on the Olivetti Faces dataset, for Ojasmethod (Blue) and the GAN model (Orange), as well as the single-feature GAN model (Green).We use the same hyperparameters as all previous experiments, measured with respect to a full PCAdecomposition which acts as a surrogate for the true subspace. First, since U is an orthonormal matrix, it lives in the Grassmannian Gr(d, n) of d-dimensionalsubspaces of Rn. Similarly, W Gr(q, n). Our goal is to embed U and W into Gr(p, n). Once wedo this, we can again calculate the macroscopic states we are interested in. To do this, we use thefollowing map:",
  ".(15)": "This produces a new matrix U Gr(p, n). We can perform a similar trick with W to obtain a matrixW. The important details about this uplifting trick are the following: (1) Due to the construction,we preserve orthonormality of all the matrices, (2) the subspaces of interest are found as the first dcolumns of the matrix U and the first q columns of the matrix W, and (3) the analysis of the diagonalcase is unchanged under this uplifting (In the diagonal case, there is no interaction between thedifferent dimensions, so we ignore the other dimensions. In the non-diagonal case, these additionaldimensions only provide minor noise, and so dont affect the training at all).",
  "Real image subspace learning": "In order to demonstrate the practicality of this analysis, we test our approach on the MNIST andOlivetti Faces dataset, and compare our approach with the single-feature discriminator fromWang et al. . Here, we include some qualitative results regarding the learned features, and providea quantitative analysis on the performance differences between the multi-feature and single-featurediscriminators. We include the Olivetti Faces results in , and the MNIST results can be foundin Appendix A. To perform these visualizations and measure performance, we first perform PCA on the entire datasetand extract the top K (16 or 36) features. We then use this as an approximation of the true subspaceU, which allows us to compare the distances. We then track the Grassmann distance between thetrue and learned subspaces for both the multi-feature and single-feature approaches. The Grassmanndistance between two d-dimensional subspaces of an n-dimensional space is given by",
  ",(16)": "where the i are the principal angles between the subspaces. Here, a lower distance means abetter similarity between the subspaces. If the two matrices are orthonormal, the principal anglesare the singular values of the cosine similarity matrix, explicitly connected with the macroscopicstates. shows the Grassmann distances for the sequential and multi-feature learning cases on",
  "Final Epoch": ": We provide results on the Olivetti Faces dataset, a well-known dataset. We show the top 16learned features for all approaches at 3 stages of training: after the 1st epoch, the 200th epoch, andthe end of training. We train all approaches for 500 epochs, equivalent to approximately 50 timestepsof simulated training. It can be clearly seen that while Ojas method learns quicker than the GANmodel, eventually the GAN model outperforms it. Additionally, we see that the features learned bythe GAN model are much more diverse and meaningful than those learned by Ojas method (whoselearned features are more similar). For the single-feature GAN model, we can see that the learning issignificantly slower, and never approaches anywhere close to the other two results. the Olivetti Faces dataset. This provides empirical justification on a real dataset, showing first thatthe phenomenon of faster training identified by the ODE in applies to practical settings aswell. Furthermore, due to having no restrictions on off-diagonal entries of the macroscopic states, wesee that the results in also apply to practical datasets, since our multi-feature discriminatorattains better performance even in less time.",
  "J(U) = Exx UUT x(17)": "known as the reconstruction error. This is because the global optima of this loss function is the truesubspace itself, and so, we can view this as a prior included in the subspace learning algorithms.GANs do not have such information, and instead seeks to learn the subspace simply through seeingthe datapoints. Therefore, we can consider GANs to be a third type of subspace learning algorithm,which we call the generative algorithms. We seek to understand how well the GAN model is able to learn a subspace compared to the existing subspace learning algorithms. We compare both analyticallyusing the derived ODEs, as well as empirically on synthetic and the MNIST dataset, in order to seeunder what circumstances GANs learn a subspace at a comparable rate.",
  "Learned features": "in the Appendix compares the features learned by the GAN model to the features learned byOjas method. Both models are initialized to exactly the same weights, and trained on the same dataat the same time, for a single epoch. For the GAN model, we use the same hyperparameters as theprevious experiments above. For Ojas method, we used a learning rate of 0.1, which experimentallywe found to produce the best results. We can clearly see that the features learned by the GAN modelare more meaningful and more clearly resemble the true data, while most of the features that Ojasmethod learns arent very interpretable. This suggests that because the GAN needs to be able togenerate the images, this acts as a form of regularization on what types of features are learned.",
  "Conclusion": "Our investigation into single-layer GAN models through the lenses of online subspace learning andscaling limit analysis has provided valuable insights into their data subspace learning dynamics. Byextending our analysis to include multi-feature discriminators, weve unearthed novel phenomenapertaining to the interactions among different features, significantly enhancing learning efficiency.This advantage is particularly pronounced in scenarios of near-zero initialization, where the generatorachieves higher maximum and steady-state performances compared to the sequential discriminator.Moreover, the interaction between dimensions enables the generator to closely match variances acrossdimensions, a feat unattainable in the sequential scenario. In the context of subspace learning, we seethat in higher noise levels, the GAN is able to more consistently outperform Ojas method on a widerange of generator, discriminator, and Oja learning rates. Introducing an uplifting method for analysis in arbitrary dimensionalities enables us to better modeluncertainties inherent in real-world subspace modeling. Practical validation on the MNIST andOlivetti Faces datasets reaffirms the applicability of our theoretical findings, underscoring the supe-riority of overparametrization in single-layer GANs over data availability. This prompts intriguingavenues for research in multi-layer GANs, probing whether similar phenomena persist in morecomplex architectures. Exploring these directions holds promise for further advancements in the field.Finally, we observe that GAN models excel in acquiring a more meaningful feature basis compared toOjas method when applied to the real-world datasets, which we attribute to their ability to generatenew data samples.",
  "Erkki Oja. Simplified neuron model as a principal component analyzer. Journal of MathematicalBiology, 15:267273, 1982": "Laura Balzano, Robert D. Nowak, and Benjamin Recht. Online identification and trackingof subspaces from highly incomplete information. 2010 48th Annual Allerton Conference onCommunication, Control, and Computing (Allerton), pages 704711, 2010. Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, SherjilOzair, Aaron C. Courville, and Yoshua Bengio. Generative adversarial networks. Communica-tions of the ACM, 63:139 144, 2014. Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generativeadversarial networks. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR), pages 43964405, 2018. Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, and Dani Lischinski. Styleclip:Text-driven manipulation of stylegan imagery. 2021 IEEE/CVF International Conference onComputer Vision (ICCV), pages 20652074, 2021.",
  "Eric V. Mazumdar, Michael I. Jordan, and S. Shankar Sastry. On finding local nash equilibria(and only local nash equilibria) in zero-sum games. 2019": "Yuma Ichikawa and Koji Hukushima. Learning dynamics in linear vae: Posterior collapsethreshold, superfluous latent space pitfalls, and speedup with kl annealing. In InternationalConference on Artificial Intelligence and Statistics, 2023. Rodrigo Veiga, Ludovic Stephan, Bruno Loureiro, Florent Krzakala, and Lenka Zdeborova.Phase diagram of stochastic gradient descent in high-dimensional two-layer neural networks.Journal of Statistical Mechanics: Theory and Experiment, 2023, 2022.",
  "A.1Comparisons with Single-Feature Discriminator and Ojas Method": "In order to demonstrate that our approach works in more practical settings, we train our model onthe MNIST dataset. Then, in order to understand what the model has learned about the dataset, wecompute the SVD of the generator weights V, and plot the left singular vectors. Each of these vectorscorrespond to a single feature learned by the model, and so viewing these will help understand themodel performance. Finally, we perform the same tests using the single-feature discriminator, todemonstrate the effects of sequential vs non-sequential learning of features. Our theory and development in the paper operated under the assumption that we knew ev-erything about the true subspace. While this is not possible for these image datasets (since we cannotdetermine the true subspace U or the distribution of c), we can still use the same assumptions andmodel structure. Therefore, the generator still samples a c from a standard Gaussian distribution, andthe choice of covariance and noise levels are determined through testing. The dataset is flattened into a 1 784 vector, so our ambient dimension n = 784. For ourmulti-feature model, we train for a single epoch. For the sequential discriminator, we train for 5epochs. We focus on the d = 36 case, although it can be further scaled up as necessary. Through test-ing, we fix the covariance matrix = 5Id and G = 1. We use a generator learning rate of = 0.04and a discriminator learning rate of = 0.2. While the multi-feature discriminator is able to learngood representations of all 36 basis elements as seen in , the sequential discriminator is un-able to learn even half of them in the 5 epochs. As can be seen, the last 18 basis elements are just noise. This scaling becomes very problematic as the number of features increases.Even with just36 features, a small amount given modern datasets, such a model requires significantly more trainingand is still unable to perform as well as the multi-feature model. Finally, we provide a comparison of the GAN learned features with the Ojas learned features in. It can be seen that most of the GAN features are more visually representative of the datasetcompared to Ojas method. : Comparison between the generator basis vectors learned by the multi-feature and single-feature discriminators on 36 features. The multi-feature model is trained for 1 epoch, while thesingle-feature model is trained for 5 epochs.",
  "A.2Grassmann distances": "contains a comparison of the Grassmann distances for the multi-feature and single-featurecases. Even after 5 epochs, the sequential discriminator still has a much higher Grassmann distancethan the multi-feature model, even though it has seen 5 times as much data. Specifically, after oneepoch of training, the multifeature discriminator has a distance of 2.46, while the single-feature : After training both GAN and Ojas method on the MNIST dataset with 16 features, we useSVD to extract their learned features, and compare them here. We can see that despite Ojas methodlearning quicker than the GAN model (seen in previous analysis), the GAN features learned are abetter representation of the data, while most of Ojas features do not resemble the data. Note that Ojalearning an 8 as the first feature is due to the order of training samples seen.",
  "discriminator finishes with a distance of 3.17, showing a significant gap. We tested with up to 20epochs, but saw no improvements for the sequential discriminator past 5 epochs": "We also see an example of the training outcomes predicted by the ODE in . Specifically,our choice of learning rates = 0.04, = 0.2 and noise level = 1 is seen in Row 1, Column 2 of, and we see the expected result of the generator oscillating around its steady state. : In the first figure, using PCA on the MNIST dataset, we obtain the top 16 features ofthe data, and use these features as an approximation for the true subspace U. We then track theGrassmann distance between the learned subspace and this approximation, for both our model trainedfor 1 epoch, and the sequential discriminator trained for 1 and 5 epochs. It is clearly seen that ourdiscriminator learns much faster than the sequential discriminator, while at the same time obtaining amuch lower distance, even when the sequential discriminator has sees 5 times as much data.",
  "Once we have Lemmas B.2 and B.3, we can show that condition (C.4) is satisfied.Lemma B.4. Condition (C.4) is satisfied for our macroscopic state stochastic process": "Proof. We show that the expected norm squared of each macroscopic state is less than some C(T).The cases of Pk and Sk are proven in Lemma 3 of , and require no changes. Additionally, byour assumption (A.6) that the matrix Wk is orthonormalized, we know that Zk = Id, and so therequirement is trivially satisfied for Zk. Thus, it remains to show this for Qk and Rk. We show thisfor Qk, and Rk follows similarly.",
  "(43)": "where in the last line, we used the previously calculated values for E||Zk||2 and E||Sk||2. The valuesfk and f2k are the values of f = F and f = F evaluated on the corresponding inputs.The conditions for the rest of the macroscopic states can be shown in the same way."
}