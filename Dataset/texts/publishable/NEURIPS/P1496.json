{
  "Abstract": "Despite the substantial progress of novel view synthesis, existing methods, ei-ther based on the Neural Radiance Fields (NeRF) or more recently 3D GaussianSplatting (3DGS), suffer significant degradation when the input becomes sparse.Numerous efforts have been introduced to alleviate this problem, but they stillstruggle to synthesize satisfactory results efficiently, especially in the large scene.In this paper, we propose SCGaussian, a Structure Consistent Gaussian Splattingmethod using matching priors to learn 3D consistent scene structure. Consideringthe high interdependence of Gaussian attributes, we optimize the scene structurein two folds: rendering geometry and, more importantly, the position of Gaussianprimitives, which is hard to be directly constrained in the vanilla 3DGS due tothe non-structure property. To achieve this, we present a hybrid Gaussian repre-sentation. Besides the ordinary non-structure Gaussian primitives, our model alsoconsists of ray-based Gaussian primitives that are bound to matching rays andwhose optimization of their positions is restricted along the ray. Thus, we canutilize the matching correspondence to directly enforce the position of these Gaus-sian primitives to converge to the surface points where rays intersect. Extensiveexperiments on forward-facing, surrounding, and complex large scenes show theeffectiveness of our approach with state-of-the-art performance and high efficiency.Code is available at",
  "Introduction": "Few-shot novel view synthesis (NVS) aims to reconstruct the scene given only a sparse collectionof views, which has always been a cornerstone and challenging task in computer vision. Neuralradiance field (NeRF) , emerged as an excelled 3D representation, has shown great success inrendering photo-realistic novel views. However, such impressive results require an expensive andtime-consuming collection of dense images which impedes many practical applications, e.g., theinput is typically much sparser in autonomous driving, robotics, and virtual reality. Although manyattempts have been proposed to solve this challenging few-shot rendering problem from the aspect ofpre-training , regularization terms , external priors ,etc., these NeRF-based methods still suffer from low rendering speed and high computational cost,i.e., each scene requires hours or even days of training time. Recently, an efficient representation 3D Gaussian Splatting (3DGS) is proposed to leverage aset of Gaussian primitives (initialized from the Structure-from-motion (SFM) points) alongwith some attributes to explicitly model the 3D scene. Through replacing the cumbersome volumerendering in NeRF methods with the efficient differentiable splatting, which directly projects theGaussian primitives onto the 2D image plane, 3DGS has expressed remarkable improvement in both",
  "PSNR: 36.68 PSNR: 18.80 PSNR: 20.32 PSNR: 25.91": ": Comparisons in view synthesis and geometry rendering. 3DGS can synthesizehigh-quality novel views and plausible geometry with excessive inputs, but suffers from significantdegradation in the sparse scenario. Even using the monocular depth prior, DNGaussian stillstruggles to generate accurate geometry and novel views. In contrast, our method can learn the moreconsistent scene structure and render the more realistic images. rendering quality and speed, i.e., high-resolution images can be rendered in real-time. Even with thisunprecedented performance, 3DGS still relies on dense image captures and faces the same problemof novel view degeneration with NeRF methods, when only a few inputs are available. In this paper, we aim to address this issue by establishing a few-shot 3DGS model with a consis-tent structure to pursue high-quality and efficient novel view synthesis. Compared with the densecounterpart, this few-shot system introduces more challenging problems, e.g., the trivial multi-viewconstraints that the model can only be supervised from sparse viewpoints, and the high interdepen-dence between Gaussian attributes make their optimization ambiguity, i.e., optimizing the positionvs optimizing the shape. Although some recent efforts have attempted to use monoculardepth priors to stabilize the optimization, e.g., the monocular depth consistency of sampledvirtual viewpoints and the hard-soft monocular depth regularization , as shown in ,the inherent scale and multi-view inconsistency of monocular depth make it hard to guarantee aconsistent scene structure and lead to unsatisfactory rendering results, especially in complex scenes. To this end, we are motivated to exploit the matching prior, which exhibits worthwhile characteristicsindicating the ray/pixel correspondence between views and the multi-view visible region. Basedon this, we propose SCGaussian, a framework that leverages matching priors to explicitly enforcethe optimization of scene structure to be 3D consistent. A straightforward idea for this purpose isto use the ray correspondence to supervise the reprojection error of the rendering depth. However,we observe that the rendering geometry is not always consistent with the scene structure due to theinterdependence of Gaussian attributes. In this paper, we argue that in addition to the renderinggeometry, the more important aspect to ensure the consistency of the scene structure is to optimizethe position of Gaussian primitives. To achieve this, we design a hybrid representation, whichconsists of ray-based Gaussian primitives besides the ordinary non-structure Gaussian primitives. Forthese rays-based ones, we bind them to matching rays and restrict the optimization of their positionalong the ray, thus we can utilize the matching correspondence to optimize the position of Gaussianprimitives to converge to the consistent surface position where rays intersect. In this dual optimizationsolution, both the position and shape of the Gaussian primitives can be constrained properly. Extensive experiments on LLFF , IBRNet , DTU , Tanks and Temples and Blender datasets show the effectiveness of our SCGaussian, which is capable of synthesizing detail andaccurate novel views in these forward-facing, surrounding, and complex large scenes, achievingnew state-of-the-art performance in both rendering quality (3 5 dB improvement on challengingcomplex scenes ) and efficiency (200 FPS rendering and 1-minute convergence speed).",
  "Related Works": "Novel view synthesis. Novel view synthesis is a task to render realistic images of unseen viewsgiven a set of training images. Many methods are proposed to address this problem in both traditional and deep-learning based manners. In particular, NeRF achievesphoto-realistic rendering and has become one of the most popular methods in recent years, which",
  "depth 1depth 2": ": Framework of SCGaussian. We first extract the matching prior from the sparse input, andrandomly initialize the hybrid Gaussian representation. The ray-based Gaussian primitives are boundto matching rays, and are explicitly optimized using the matching correspondence. The renderinggeometry optimization is further conducted to optimize the shape of all types of Gaussian primitives.Combined with the ordinary photometric loss, SCGaussian can learn the consistent scene structure. successfully combines multi-layer perceptrons (MLP) and volume rendering. The following workstry to improve NeRF in many aspects, e.g., quality , pose-free , dynamic viewsynthesis , training and rendering efficiency . And morerecently, a point-based method 3D Gaussian Splatting represents the scene as 3D Gaussians andsignificantly improve the rendering speed to a real-time level. And it has shown an advantage in manyaspects compared with NeRF-based methods. However, these methodsneed dense input views, which makes them unsuitable for many practical applications. Few-shot novel view synthesis. Compared with the ordinary NVS, the few-shot NVS is a morepractical task but also more challenging. The original rendering methods always suffer from dramaticdegradation when applied directly to the sparse scenario. Many works have attempted to solve thisproblem. Specifically, one thread of works attempt to pre-train a generalizable modelon the large-scale datasets first and apply it to the target scene with sparse inputs. Another alternateapproach is to optimize the model from scratch for each scene. try to add the depth supervisionfrom the SFM points or depth completion model, and adopt the more practical monoculardepth prior. To exploit smoothness and semantic priors, works choose to render some patchesfirst and introduce the geometry and appearance regularization. These methods are all based on theNeRF and rely on volume rendering to synthesize novel views, which is always time-consuming.Some recent methods combine the efficient 3DGS representation with monocular depth and multi-view stereo prior to improve the efficiency of the few-shot NVS task. However,since 3DGS relies on the initialization of sparse SFM points and adequate multi-view constraints,which are hard to observe in the sparse scenario, how to learn the globally consistent structure is thecrucial bottleneck.",
  "Methodology": "In this section, we introduce the proposed new few-shot approach, SCGaussian, which can learnconsistent 3D scene structure using matching priors. The overall framework of our model is illustratedin . In Sec. 3.1, we first review the 3DGS. Then we elaborate on the challenge of few-shot3DGS and the motivation of using matching priors in Sec. 3.2, and the design of our StructureConsistent Gaussian Splatting will be introduced in Sec. 3.3. The full loss function and training detailwill be described in Sec. 3.4.",
  "G = arg minGLphoto(G).(7)": "With adequate training views, the optimized model is capable of generating great novel view renderingresults. However, as shown in , when the input becomes sparse, the 3DGS model alwaysoverfits training views and suffers from a significant degradation in test poses. We observe that the challenge of the few-shot 3DGS mainly comes from the failure of learningthe 3D consistent scene structure, e.g., the learned Gaussian primitives cannot distribute over theaccurate surface region and the rendering geometry is multi-view inconsistent. In the sparse scenario,the supervision signal only comes from a few training poses, and this trivial multi-view constraintmakes it hard to bias the model towards learning a 3D consistent solution. Conversely, as shownin (a), 3DGS model tends to learn the inconsistent Gaussians for each view separately, e.g.,",
  "(a) Inconsistent geometry with sparse inputs(b) Optimization ambiguity of Gaussian": ": Visualization of some challenges faced by few-shot 3DGS. (a) The expected Gaussianin the surface region cannot be learned, and the model tends to learn the inconsistent Gaussian andoverfit the training views. While the training loss is small enough, the testing error is pretty bad. (b)The attributes of Gaussian primitives are interdependent and the model tends to increase the size tocover the pixels rather than correct the position. the model learns a wall extremely close to each camera, in which case the training loss is stillsmall. Furthermore, we find that 3DGS has an obvious optimization ambiguity due to the highinterdependence of attributes, e.g., shape versus position. Theoretically, the model needs to learnmore small-sized Gaussian primitives over the texture region to recover high-frequency details, but inpractice, it prefers to increase the size of Gaussian primitives to cover these pixels as shown in (b), resulting in an overly smooth view synthesis. To ensure that the learned structure is consistent,a heuristic strategy in the vanilla 3DGS is to use sparse SFM points as initialization and guide themodels optimization, which is especially crucial for complex scenes. However, in the sparse scenario,its pretty hard to stably extract enough SFM points, and usually, only random initialization can beused like . This amplifies the challenge of learning the consistent structure. Although some methods attempt to use the monocular depth to regularize the geometry,the inherent scale and multi-view ambiguity of monocular depth make it difficult to solve theaforementioned problems. Thus, we are interested in the question: how can we make 3DGS withoutSFM point initialization to learn 3D consistent scene structure under sparse input? In this paper, weconsider exploiting matching priors using the pre-trained matching model , which doesnt facethe ambiguity problem like monocular depth. Matching priors have two important characteristics: raycorrespondence and ray position. Ray correspondence. The pair of matching rays represent the corresponding 2D position of aconsistent 3D point in different views, which can serve as the prominent multi-view constraint, i.e.,the matching rays should theoretically intersect at the same surface position. Given a pair of matchingrays {ri, rj} at image Ii and Ij, and the corresponding pixel coordinates are {pi, pj}, supposing wehave computed the position of the surface point intersect with each ray as Xi and Xj, we can get thefollowing equation:Xi = Xj.(8) Meanwhile, given the camera intrinsics {Ki, Kj} and extrinsics {[Ri, ti], [Rj, tj]}, we can furtherproject the surface point to another 2D image plane and get the projected pixel coordinate, e.g., theprojection from i to j can be modeled as:",
  "where is the projection operator ([x, y, z]T ) = [x/z, y/z]T . Thus we have the equation in pixelcoordinate: pj = pij, and similarly, we have pi = pji": "Ray position. In the matching prior, the position of matching rays exactly indicates the region thatis commonly visible to at least two views. This multi-view visible region plays a crucial role in thereconstruction model, as its meaningless when there is no overlapping region between views. In thesparse scenario, the stereo correspondence is insignificant and the non-overlapping region can evenharm the model training, while the importance of the multi-view visible region is magnified.",
  "Structure Consistent Gaussian Splatting": "To fully exploit the characteristics of matching prior, our SCGaussian explicitly optimizes the scenestructure in two folds: the position of Gaussian primitive and the rendering geometry. Optimizingthe position of Gaussian primitive is non-trivial due to the non-structural properties of Gaussianprimitives. To address this, we present a hybrid Gaussian representation. Besides ordinary non-structure Gaussian primitives used to recover the background region visible in a single view, ourmodel also consists of ray-based Gaussian primitives which are bound to matching rays, in whichcase their positions are restricted to be optimized along the ray. Initialization and densification. Different from existing methods that initialize with either SFMpoints or random points , we initialize with ray-based Gaussian primitives and bind themto matching rays. For convenience, here we discuss two input images Ii and Ij. Suppose we haveN pairs of matching rays {rki , rkj }Nk=1, we can initialize N pairs of ray-based Gaussian primitives{Gki , Gkj }Nk=1. Similar to 3DGS, each primitive is equipped with a set of learnable attributes but witha different position representation. The position of the ray-based Gaussian primitive is defined as: = o + zd,(10)",
  "where o and d refer to the camera center and ray direction respectively, and z is a learnable distancefactor, which is randomly initialized": "For densification, we follow the same strategy in to determine the under-reconstructioncandidates using the average magnitude of view-space position gradients, and generate the non-structure Gaussian primitives, whose positions can be optimized in arbitrary directions. Optimize the position of Gaussian primitives. As analyzed in Sec. 3.2, the accurate position ofGaussian primitives plays a fundamental role in the learned scene structure. Since the matchingcorrespondence between ray-based Gaussian primitives can be constructed using the binding strategy,we can conveniently optimize their positions. For a pair of matching rays {ri, rj} in image Ii and Ij, thanks to our binding strategy, we can geta pair of binding Gaussian primitives {Gi, Gj}, and their positions in 3D space are i = oi + zidiand j = oj + zjdj respectively. According to Eq. (8) and Eq. (9), we can get the projected 2Dcoordinate from i to j: pij(i) and from j to i: pji(j). Thus we can get the projection error ofthis pair of Gaussian primitives as:Lijgp= pj pij(i)",
  "L = Lphoto + Lgp + Lrg.(14)": "Training details. During training, we set = 1.0. To avoid the model falling into sub-optimizationin the early stage of training, we set = 0 and then increase it to = 0.3 after 1k iterations. Toensure that the Gaussian primitive converges to the optimal position, we use a caching strategy inthe first 1k iterations, i.e., cache the position with the minimum Gaussian position loss Lgp at eachiteration. Meanwhile, considering there are some mismatched ray pairs in the matching prior, wefurther filter out those primitives with large Gaussian position loss Lgp > . During optimization,the ray-based primitive will not be pruned. We build our model based on the official 3DGS codebase,and train the model for 3k iterations with the same setting as 3DGS but set the learning rate of thelearnable distance factor z to 0.1 at the beginning and decrease to 1.6 106.",
  "Experiments": "In this section, we demonstrate the performance of our model in popular datasets and conduct ablationstudies to verify the effectiveness of our designs. Next, we first describe the common datasets and theselected baselines for comparison, then analyze the results. Datasets & metrics. We evaluate our model on forward-facing, complex large-scale and surroundingdatasets under the sparse setting: LLFF , IBRNet , Tanks and Temples (T&T) , DTU and NeRF Blender Synthetic dataset (Blender) . LLFF dataset contains 8 real scenes, andfollowing previous methods , every 8-th images are held out for testing, and sparse viewsare evenly sampled from the remaining images for training. IBRNet dataset is also a real forward-facing dataset, and we select 9 scenes for evaluation and adopt the same split as in LLFF. T&T is alarge-scale dataset collected from more complex realistic environments containing both indoor andoutdoor scenes, and we use 8 scenes for evaluation and also apply the same split as in LLFF. DTUis an object-centric dataset, which contains more texture-poor scenes. We use the same evaluationstrategy as on DTU. For Blender, containing 8 object-centric synthetic scenes, we follow totrain with 8 images and test on 25 images. We report PSNR, SSIM, and LPIPS scores to measureour reconstruction quality and also report the geometric average (AVG) of MSE = 10PSNR/10,",
  "SSIM and LPIPS as in": "Baselines. We compare our model against both NeRF-based and 3DGS-based few-shot NVS methods.For NeRF-based methods, we compare with methods with relatively high performance, includingMipNeRF , DietNeRF , RegNeRF , FreeNeRF and SparseNeRF . For 3DGS-based methods, we compare with the vanilla 3DGS and its recent few-shot follow-ups like FSGS and DNGaussian .",
  ": Qualitative comparisons on Tanks and Temples dataset with 3 training views": "discrete properties as discussed in , our SCGaussian still achieves the best performance in allmetrics. Note that FSGS uses the sparse SFM points for initialization, even though, our modelholds remarkable superiority. Moreover, our advantage against previous methods is amplified in theIBRNet dataset, which has more low-texture scenes. Some qualitative comparisons are shown in , from which we can see that our method can recover more accurate high-frequency details. Results on T&T. To evaluate the performance of our model on complex large scenes, we conductfurther comparisons on the T&T dataset. Using the same split strategy as LLFF, we quantitativelycompare with existing methods with 3 training views in Tab. 2. With the large difference in camera",
  "DGS 17.140.4930.397FSGS 20.010.6520.323DNGaussian 18.590.5730.437SCGaussian (Ours)22.170.7520.257": "poses and the unbounded scene range, previousNeRF-based methods , mostly de-signed for the bounded scenes, are hard to recon-struct plausible results. Among them, methods using geometric regularization performbetter. Even combined with explicit point repre-sentation, the recent 3DGS still struggles on thislarge scene with sparse inputs. Although somerecent efforts apply the monocular depth prior or the initialization of sparse SFM point to 3DGS, they have limited reconstructionquality. From the qualitative comparisons shownin , we can see that our method can syn-thesize the novel view with more accurate and complete details. Benefiting from our novel design in",
  "Ours20.770.7050.2180.105": "hybrid representation and explicit optimization of rendering geometry and position of Gaussian primi-tives, our model can learn more consistent structure as shown in , and show great generalizationability on these large scenes. Results on DTU. We further conduct more experiments on DTU dataset to prove the robustness onmore texture-poor scenes. The quantitative results in Tab. 4 indicate that our method achieves the bestperformance on all metrics, which proves that our model is still robust on those texture-poor scenes.The qualitative results in also demonstrate that our method can recover more accurate details. Results on Blender. We test on the Blender dataset to verify our performance in the surroundingscenario. While uses its uppooling strategy to clone more Gaussian primitives and gets thebest SSIM score, our method achieves the best PSNR and LPIPS scores, as the quantitative resultsreported in Tab. 3. We visualize more qualitative comparisons in to demonstrate our superiority,and we can see that our method has a clear advantage in recovering fine details and reconstructingcomplete structures. This further demonstrates our generalization ability in different scenes.",
  ": PSNR vs view number on T&T": "Ablation studies. We conduct a few ablation stud-ies on LLFF and T&T datasets to understand howour model performs with different settings. Ourbaseline is the vanilla 3DGS. From the resultsshown in Tab. 6, we can see that using only thehybrid representation (Hybrid Rep.), our modelcan already bring more than 2dB improvementto the baseline, which verifies that our modelcan indeed mitigate the risk of overfitting. Com-bined with the optimization of rendering geometry(Rend. Geo.), the performance can be further im-proved. When we optimize both the renderinggeometry and the position of Gaussian primitives(Dual optim.), the model can learn the more con-sistent scene structure and render more convincingnovel views. These results prove our motivationfor learning the 3D consistent structure. And theadopted cache & filter strategy further mitigatesthe impact of wrong matching priors. To assessthe performance of the model with different num-bers of views, we conduct the comparison in T&T dataset, as shown in . Our model can",
  "consistently outperform the SOTA method , and the advantage becomes more significant as thenumber of views increases": "Triangulation initialization. To prove the effectiveness of our optimization strategy, we performsome comparisons with methods that directly use the triangulation points of matched pixels forinitialization. The results are shown in Tab. 5, which indicate that using the triangulation initializationcan improve the performance of the baseline especially equipped with more structured ScaffoldGS or OctreeGS models. Even though, our model still achieves the best performance anddemonstrates the effectiveness of our model.",
  "Ours + GIM20.770.7050.2180.105Ours + DKM20.920.7320.1890.099Ours + LoFTR20.940.7370.1820.097Ours + SuperGlue20.250.6890.2210.110": "Robustness to matching models. We performmore experiments in Tab. 7 to verify the robust-ness of our model to different pre-trained match-ing models. Concretely, we use the same opti-mization and testing configuration for all mod-els, and additionally use the DKM , LoFTR and SuperGlue models to extract thematching prior. The results in Tab. 7 show thatall these matching models can bring a satisfactory improvement to the baseline, and our method caneven achieve better performance when using weaker matching models (e.g., GIM vs. LoFTR). Theseresults prove that our strategy is robust to different matching models. Efficiency. With a single NVIDIA RTX 3090 GPU, the training of our method consumes about 3GBmemories and converges within 1 minute on LLFF 3-view setting, which is much faster than existingmethods, e.g., need about 10 hours and needs about 10 minutes. Our method alsoachieves a real-time inference speed of over 200FPS at 504 378 resolution, superior to NeRF-basedmethods (e.g., at 0.04FPS) and comparable to 3DGS-based methods (e.g., at 181FPS). Limitation. Following the common pipeline in the research field of few-shot NVS, our modelrequires an accurate camera pose, which may not always be available. Thus liberating this limitationcould further improve our work to be more practical, and we will investigate this in future work.",
  "Conclusion": "In this paper, we observed the main challenge of few-shot 3DGS is learning the 3D consistent scenestructure, and we exploited the matching prior to construct a Structure Consistent Gaussian Splattingmethod named SCGaussian. Due to the optimization ambiguity of Gaussian attributes between theposition and shape, we presented two approaches to optimize the scene structure: explicitly optimizethe rendering geometry and the position of Gaussian primitives. While directly constraining theposition is non-trivial in the vanilla 3DGS, we introduced a hybrid Gaussian representation, consistingof ordinary non-structure Gaussian primitives and ray-based Gaussian primitives. In this way, boththe position and shape of Gaussian primitives can be optimized to be 3D consistent. To evaluateour method as comprehensively as possible, we conducted experiments on forward-facing, complexlarge-scale, and surrounding datasets. The results consistently demonstrate that our method achievesnew state-of-the-art performance while being highly efficient.",
  "and Disclosure of Funding": "This work is financially supported by the Outstanding Talents Training Fund in Shenzhen, this workis also supported by the National Natural Science Foundation of China U21B2012, Shenzhen Scienceand Technology Program-Shenzhen Cultivation of Excellent Scientific and Technological InnovationTalents project(Grant No. RCJC20200714114435057). J. Jiao is supported by the Royal Society ShortIndustry Fellowship (SIF\\R1\\231009) and the Amazon Research Award. In addition, we sincerelythank all assigned anonymous reviewers, whose comments were constructive and very helpful to ourwriting and experiments. Jonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla,and Pratul P Srinivasan. Mip-nerf: A multiscale representation for anti-aliasing neural radiancefields. In ICCV, pages 58555864, 2021.",
  "Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao Su. Tensorf: Tensorial radiancefields. In ECCV, pages 333350. Springer, 2022": "Anpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang, Fanbo Xiang, Jingyi Yu, and HaoSu. Mvsnerf: Fast generalizable radiance field reconstruction from multi-view stereo. In ICCV,pages 1412414133, 2021. Paul E Debevec, Camillo J Taylor, and Jitendra Malik. Modeling and rendering architecturefrom photographs: A hybrid geometry-and image-based approach. In Seminal Graphics Papers:Pushing the Boundaries, Volume 2, pages 465474. 2023.",
  "Ren Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction.In ICCV, pages 1217912188, 2021": "Ren Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towardsrobust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. IEEETPAMI, 44(3):16231637, 2020. Kerui Ren, Lihan Jiang, Tao Lu, Mulin Yu, Linning Xu, Zhangkai Ni, and Bo Dai. Octree-gs: Towards consistent real-time rendering with lod-structured 3d gaussians. arXiv preprintarXiv:2403.17898, 2024.",
  "A.1More experimental details": "Our experiments are performed following the common solution of existing methods. LLFF datasetcontains eight different scenes, and we perform the training and inference at 8 downsamplingscale with a resolution of 504 378. IBRNet is another forward-facing dataset collected by that contains larger camera motion and more scenes. We select nine scenes for evaluation, whichinclude giraffe_plush, yamaha_piano, sony_camera, Japanese_camilia, scaled_model, dumb-bell_jumprope, hat_on_fur, roses and plush_toys. We use the same setting as LLFF. Tanksand Temples is a large-scale complex dataset, which has large camera motion. For comparison,we use eight scenes, namely Ballroom, Barn, Church, Family, Francis, Horse, Ignatius,and Museum, both indoors and outdoors. We train and infer at the resolution of 960 540. Forthe Blender dataset, we use the common solution in existing methods and run at the resolution of400 400 (2 downsampling).",
  "A.2More results of consistent structures": "The consistent scene structure is important for reconstruction models, including the NeRF-based andthe 3DGS-based, and the inaccurate structure, e.g., floaters or walls, can lead to extremely poor novelview synthesis results. Thus in this paper, we propose SCGaussian to solve the challenge of learningconsistent structure in few-shot 3DGS models. From the quantitative and qualitative results shown inour main paper, we can see that our method can synthesize more complete novel views, especially inthe high-frequency regions, and these results just demonstrate our learned scene structure is more 3Dconsistent. We show some comparisons of rendering geometry in , and we can see that when the inputbecomes sparse, existing methods fail to learn the plausible geometry while our method can stillrender the accurate geometry. Here, we show more comparisons in to understand the resultsof the position of Gaussian primitives. To visualize these positions, we first fix the opacity of allprimitives to a large value (1.0 in our setting) and then we render the distance of Gaussian primitivesusing the Gaussian rasterization. In this way, the rendering results can indicate the position of thenearest Gaussian primitives (third row in ). We can see that both the rendering geometry andthe position of Gaussian primitives of our method are more accurate and 3D consistent. Meanwhile, we find that our learned structure in texture-less regions (e.g., the wall region shown in 3rd row) is even better (smoother) than the dense version (with way more views of inputs) of3DGS. We suspect the main reason is that the proposed method has better control over the number ofGaussian primitives, i.e., adaptively allocates more primitives in high-textured regions while fewerprimitives in the texture-less regions.",
  "A.3Effectiveness of dual optimization based on the hybrid representation": "In this paper, we propose a dual optimization strategy to separately optimize the rendering geometryand position of Gaussian primitives based on our hybrid Gaussian representation. While we showsome quantitative results in Tab. 6, here we show more visual results in . The model w/Matching prior in Tab. 6 refers to the straightforward combination of matching priors and the vanilla3DGS, and the model w/ Dual optim corresponds to the model optimizes both rendering geometryand position of Gaussian primitives based on the hybrid representation. We can see that the straightforward solution is still hard to synthesize accurate novel views and stillsuffers from obvious inconsistencies in its rendering geometry. And our solution, optimizing boththe rendering geometry and position of Gaussian primitives based on our hybrid representation, cansynthesize more accurate novel views and render more consistent depth.",
  "A.4Results at different resolutions": "While 3DGS model can synthesize high-resolution images efficiently, we here conduct more compar-isons with existing methods at a higher resolution (1008 756) on LLFF dataset. As the quantitativeresults shown in Tab. 8, our method still achieves the best in all metrics. Compared with the previousfew-shot 3DGS method , our advantage is amplified at the higher resolution. We further show some visual comparisons in . We can see that our method can recover morehigh-frequency details with the best accuracy, while previous 3DGS-based method even losessome structures. Compared with the NeRF-based methods , which have a slow renderingspeed and smooth reconstruction, our advantage is more significant.",
  "A.5Results for different view numbers": "As shown in , our method can consistently outperform existing methods with different numbersof inputs. Here, we show some visual comparisons in to qualitatively evaluate our advantage.We can see that our method can recover more details with both 3 and 6 training views. Furthermore,we report some quantitative comparisons in Tab. 9. The results show that the 3DGS-based methodsperform better than the NeRF-based method on the complex scene. FreeNeRF propose efficientfrequency regularization terms to improve the few-shot performance in the bounded scene, but wecan see that this simple strategy does not work well in the unbound scene, while SparseNeRF uses the monocular depth prior to achieve better performance than FreeNeRF. This suggests thatusing external priors may be a better option in complex scenarios. Meanwhile, we find a situation thatthe vanilla 3DGS performs better than DNGaussian, which adopts the monocular depth to regularizethe geometry. We analyze that the main reason is the inherent scale and multi-view inconsistency ofmonocular depth. This situation further demonstrates the superiority of the matching prior that weadopt.",
  "A.6More discussion on the hybrid representation": "Optimizing the position of Gaussian primitives to the 3D consistent surface position is fundamentalfor the novel view synthesis task. However the Gaussian primitive in the vanilla 3DGS is non-structureand is hard to be controlled, whose position can be moved to arbitrary directions. While the densecounterpart can leverage the initialization of SFM points to guide the optimization of the Gaussianprimitive, our few-shot model with sparse inputs can only start from the random initialization, whichobviously makes the optimization of the position of Gaussian primitives become more difficult. Thus,it would be an ideal solution if there was a method that could directly control the position of theGaussian primitive.",
  "only non-structure19.400.6340.259only ray-based20.500.6840.231hybrid rep.20.770.7050.218": "With the ray correspondence in the matchingprior, we can assume that there is a surface pointin the matching ray. Therefore, we propose tobind Gaussian primitives to matching rays, re-strict the optimization of their positions alongthe ray and enforce them to converge to the sur-face position. This approach makes the opti-mization of Gaussian primitives more control-lable. Meanwhile, we notice that there are stillregions that are not multi-view visible, only using these ray-based Gaussian primitives makes it hardto cover the complete scene, as shown in . Here, we treat these regions only visible to a singleview as the background, and we use the ordinary non-structure Gaussian primitives to recover themand propose the hybrid representation. We report the ablation results of the hybrid representation inTab. 8.",
  "A.7Error bars": "Although most previous dont provide the error bar, here, to enhance the experimental significance,we run all methods 5 times and report error bars of SparseNeRF , 3DGS , DNGaussian and our method in . We can see that the results of the baseline model 3DGS have thelargest fluctuations in all metrics. Our method gets the best score on all metrics and has relativelysatisfactory stability."
}