{
  "Abstract": "Machine learning (ML) algorithms, particularly attention-based transformer mod-els, have become indispensable for analyzing the vast data generated by particlephysics experiments like ATLAS and CMS at the CERN LHC. Particle Transformer(ParT), a state-of-the-art model, leverages particle-level attention to improve jet-tagging tasks, which are critical for identifying particles resulting from protoncollisions. This study focuses on interpreting ParT by analyzing attention heatmaps and particle-pair correlations on the - plane, revealing a binary attentionpattern where each particle attends to at most one other particle. At the sametime, we observe that ParT shows varying focus on important particles and subjetsdepending on decay, indicating that the model learns traditional jet substructureobservables. These insights enhance our understanding of the models internalworkings and learning process, offering potential avenues for improving the effi-ciency of transformer architectures in future high-energy physics applications.",
  "Introduction": "Machine learning (ML) algorithms are becoming crucial for effectively analyzing the enormous dataproduced by particle physics experiments like ATLAS and CMS at the CERN LHC . Attention-based transformer models , with their ability to effectively capture and weigh the relative impor-tance of different elements in input data, have revolutionized various domains, including naturallanguage processing and computer vision . The ability of the attention mechanism to discernintricate correlations in the collimated spray of particles jets initiated from massive particle decayshas proven invaluable in searches for new physics and standard model measurements . High-energy collisions of protons at LHC can create new unstable particles which then decay andproduce sprays of outgoing particles, called jets, which are observed by these experiments. Jettagging, or the process of identifying the particle that initiates this spray, is a critical step in dataanalysis at the LHC . One such state-of-the-art model for jet tagging is Particle Transformer(ParT) , which is a transformer variant that leverages physics-inspired particle-particle pairwisefeatures (or interactions) to augment the attention mechanism. Although this is one of the bestperforming jet-tagging models, there is effort required to interpret the underlying workflow and whythe model performs so well.",
  "arXiv:2412.03673v2 [hep-ph] 9 Dec 2024": "to-particle correlations potentially making these ML models more interpretable. These attentionscores between particles highlight the most important connections between particles necessary forclassification. This helps us to understand whether the neural network is learning the physics we know.In addition, looking at which parts of the input the neural network focuses on can help us improve thecomputational performance by pruning unnecessary parts of the neural network architecture, resultingin a more efficient jet tagging model. Related WorkThis paper builds on prior research that has applied machine learning, specificallygraph neural networks (GNNs), to particle physics problems at the CERN LHC . Refer-ence attempts to explain how ParticleNet , a GNN designed for jet tagging learns to classifythree-prong hadronic top quark decays using edge relevancy (R) graphs based on layerwise rele-vance propagation. Similarly, we visualize the attention scores between pairs of particles in order toinvestigate how ParT classifies jets.",
  "Methods": "This study uses the pre-trained ParT model, trained using the JETCLASS dataset , whichconsists of 100 million examples of jets for training, 5 million for validation, and 20 million for testing,containing 10 classes of jets with different decay modes of quarks, gluons, W and Z bosons, Higgsbosons, and top quarks. The classes are q/g, H bb, H cc, H gg, H 4q, H qq,t bqq, t b, W qq, and Z qq, indicating the decays to combination of light quarks(q), charm quark (c), bottom quark (b), and lepton (). There are 17 input features divided into 3categories: kinematics, particle ID, and trajectory displacement. Each jet is composed of a maximumof 128 particles, ordered by pT, with an average of 3050 particles per jet. ParT introduces a modified attention mechanism, called particle multihead attention (P-MHA). Givena particle-level representation of a jet with N constituents with each constituent having d features,x RNd, the attention is computed as",
  "where Ui RNN is a single feature for each pair of particles, which can be concatenated into alarger structure, known as the pairwise interaction matrix U RNNd": "In ParT, the number of pairwise features d must be equal to the number of heads h, which is chosento be 8. In addition, dk = 16 so that the overall output dimension is the same as the input dimensiond = hdk = 128. The pairwise interaction matrix U is learned by applying a 4-layer pointwise MLPwith (64, 64, 64, 8) channels and GELU nonlinearity from the 4 features motivated by Ref. ,(ln , ln kT, ln z, ln m2), where",
  "Results": "Distribution of Attention ScoresWe visualize the attention scores across all heads in a heat mapand observe that the ParT models attention scores exhibit a nearly binary (on/off) pattern. As shownin , the distribution of attention scores reveals that most values are either close to one or zero.This binary characteristic indicates that particles generally attend to, at most one other particle. Thisbehavior raises the intriguing question of whether the model is capable of capturing underlyingphysical laws and how we might interpret the specific physics the model is learning from the data.The binary nature of the attention weights also suggests potential for more efficient transformer 0.00.20.40.60.81.0",
  "models by reducing the computational complexity of attention mechanisms, focusing only on keyinteractions between particles": "Particle Attention GraphsWe decluster each jet into a specific number of subjets using the kTalgorithm implemented in the FastJet package with Python bindings . We cluster intotwo subjets for leptonic top quark decays (t b), three subjets for hadronic top decays (t bqq),and four subjets for Higgs decays into four quarks (H 4q). For each process, we visualize theattention scores by representing each particle as a point in - space in . We compare theseresults with those obtained with randomly initialized attention weights. We focus on the heads in thefinal layer of P-MHA. In , we observe that compared to an untrained, randomly initialized model, ParT demonstrates asignificantly stronger tendency to attend to the lepton in the t b process. This behavior indicatesthat ParT recognizes the importance of the lepton in classifying the underlying resonance and decaymode. Additionally, in t bqq and H 4q P-MHA captures both inter-subjet and intra-subjetinteractions. Specifically, ParT exhibits attention between different subjets and also within individualsubjets. For example, in , P-MHA connects all different subjets but does not form connectionswithin the subjets. In contrast, P-MHA focuses its attention on interactions between subjets 0 and 1while neglecting connections with subjet 2, instead of forming connections within subjet 2. In order to quantify how often ParT exhibits these behaviors, we display the distribution of attentionvalues for different jet classes. In particular, we find the sum of all attention values to a lepton int b jets, and divide it by the sum of all attention values in the head. Since the sum of all attentionvalues in a head should be equal to the number of particles, this effectively is the proportion ofattention weights that attend to the lepton. Similarly, for t bqq and H 4q jets, we display thesum of attention weights between particles in different subjets divided by the sum of all attentionweights. In , we observe that the trained ParT model is more likely to have a head that consists ofonly inter-subjet connections or only intra-subjet connections than an untrained, randomly initializedmodel. ParT attending to only leptons or to no leptons for t b jets, and only intra-subjet or onlyinter-subjet for t bqq and H 4q jets is some evidence that ParT is learning relevant physicalproperties of the jet data. LimitationsOur analysis is constrained to the final attention layer, limiting insights into how earlierlayers contribute to the representations. Additionally, the observed attention patterns may dependon the specific subjet clustering algorithm used, which could influence the models interpretability.Exploring intermediate layers and varying subjet algorithms in future studies could provide a morecomprehensive understanding of ParTs attention mechanisms.",
  "Optimizing Attention Mechanism for Efficiency": "Leveraging the binary nature of P-MHA, we constrain the total number of particles used in theattention mechanism, and explore the impact of the number of particles attended to in each attentionhead on the overall performance. For only one particle, we directly replace attention with softmax.For other numbers of particles, we keep the top-k highest attention particles, setting the rest of the",
  "Attention Weight": ": Visualization of attention values in the - plane for pairs of particles (: muon, :charged hadron, : neutral hadron, : photon, and : electron) within jets. The transparency of eachpoint is proportional to the transverse momentum of the particle (pT). The intensity of the connectinglines represents the magnitude of the attention scores. Solid lines depict intra-subjet connections,while dotted lines represent inter-subjet connections. The plots in the first column correspond tot b, the second column to t bqq, and the third column to H 4q processes.",
  "H bbH ccH ggH 4qH qqt bqqt bW qqZ qqMax particlesAccuracyAUCRej50%Rej50%Rej50%Rej50%Rej99%Rej50%Rej99.5%Rej50%Rej50%": "10.7700.9754439613255824619442384660121717520.7980.97955780186267390289442191005028623130.8140.98176757226873515346059171111134127240.8250.98317326255879661388478131242237929960.8380.9849862130638993844741282113889438339100.8510.986598523670103135251022061915748502375200.8590.9875111114237118180252632941216000539399300.8600.98771098941931221859536233898161295434021280.8610.9877106384149123186454793278715873543402 particles to zero, and then applying softmax over those k particles (out of 128 maximum). We findthat with just 1 particle, some performance is maintained and with 30 particles, the performance ofParT is recovered as shown in . By filtering the particles that P-MHA can consider, we canpotentially reduce the number of calculations and increase computational efficiency.",
  "Summary and Outlook": "The ParT model demonstrates a unique binary attention pattern, in which each particle typicallyattends to at most one other particle. This focused attention mechanism contrasts with other trans-former models, such as vision transformers, which spread attention across many elements. The 0.00.20.40.60.81.0 Sum of Attention to Lepton / Sum of Attention 10 5 10 4 10 3 10 2 10 1",
  "sparse and selective attention in ParT prioritizes key relationships, potentially uncovering importantsubstructures in jet tagging, and enhancing model interpretability in high-energy physics": "This binary pattern also opens up opportunities for optimization. This could lead to more efficientarchitectures without compromising performance, particularly in tasks such as jet tagging, wherethe identification of essential interactions is vital. Future work could explore these optimizations tofurther improve both the interpretability and efficiency of transformers in physics applications. Thiswork will be helpful in designing a more efficient physics-based attention mechanisims. In addition, the importance of the interaction matrix for the performance of ParT has not beenexplored fully. In Ref. , many P-MHA layers are replaced with the interaction matrix and findimproved performance and efficiency. Understanding what the interaction matrix is learning toaugment P-MHA could lead to a more efficient architecture. Broader ImpactStudies that interpret machine learning algorithms are important to increaseconfidence in model predictions and also to improve the performance of future machine learningmodels. This study uses an xAI method in order to explain the performance of the state-of-the-arttransformer model for the LHC, by demonstrating that the model is indeed learning physics. Thisstudy opens doors for future studies to extract and discover physics information in machine learningmodels that are currently not being used for prediction.",
  "and Disclosure of Funding": "This work is supported by the DOE Office of Science, Award No. DE-SC0023524, Fermi ResearchAlliance, LLC under Contract No. DE-AC02-07CH11359 with the DOE, Office of Science, Office ofHigh Energy Physics, LDRD L2024-066-1, DOE Office of Science, Office of High Energy PhysicsDesigning efficient edge AI with physics phenomena Project (DE-FOA-0002705), DOE Office ofScience, Office of Advanced Scientific Computing Research under the Real-time Data ReductionCodesign at the Extreme Edge for Science Project (DE-FOA-0002501),a nd DOE Early CareerResearch Program Award No. DE-SC0021187. The work is also supported in part by RCSA Grant#CS-CSA-2023-109, Sloan Foundation Grant #FG-2023-20452, NSF awards CNS-1730158, ACI-1540112, ACI-1541349, OAC-1826967, OAC-2112167, CNS-2100237, CNS-2120019, the Universityof California Office of the President, and the University of California San Diegos California Institutefor Telecommunications and Information Technology/Qualcomm Institute, AI2050 program atSchmidt Futures (Grant G-23-64934), and the NSF HDR Institute A3D3 (PHY-2117997).",
  "J. Shlomi, P. Battaglia, and J.-R. Vlimant, Graph Neural Networks in Particle Physics, Mach.Learn.: Sci Technol. 2 (2021) 021001, doi:10.1088/2632-2153/abbf9a,arXiv:2007.13681": "J. Duarte and J.-R. Vlimant, Graph neural networks for particle tracking and reconstruction,in Artificial Intelligence for High Energy Physics, P. Calafiura, D. Rousseau, and K. Terao, eds.,p. 387. World Scientific, 2022. arXiv:2012.01249. doi:10.1142/9789811234033_0012. F. Mokhtar, R. Kansal, and J. Duarte, Do graph neural networks learn traditional jetsubstructure?, in Machine Learning and the Physical Sciences Workshop at the 36thConference on Neural Information Processing Systems. 2022. arXiv:2211.09912."
}