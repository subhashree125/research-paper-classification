{
  "Abstract": "We present ConceptFactory, a novel scope to facilitate more efficient annotation of3D object knowledge by recognizing 3D objects through generalized concepts (i.e.object conceptualization), aiming at promoting machine intelligence to learn com-prehensive object knowledge from both vision and robotics aspects. This idea origi-nates from the findings in human cognition research that the perceptual recognitionof objects can be explained as a process of arranging generalized geometric compo-nents (e.g. cuboids and cylinders). ConceptFactory consists of two critical parts: i)ConceptFactory Suite, a unified toolbox that adopts Standard Concept TemplateLibrary (STL-C) to drive a web-based platform for object conceptualization, and ii)ConceptFactory Asset, a large collection of conceptualized objects acquired usingConceptFactory suite. Our approach enables researchers to effortlessly acquire orcustomize extensive varieties of object knowledge to comprehensively study differ-ent object understanding tasks. We validate our idea on a wide range of benchmarktasks from both vision and robotics aspects with state-of-the-art algorithms, demon-strating the high quality and versatility of annotations provided by our approach.Our website is available at",
  "Introduction": "In the current data-driven era, the availability of a large amount of training data with dense annotationshas become an indispensable factor for the successful implementation of deep neural networks ina wide range of 3D object understanding tasks. Particularly, for tasks like segmentation, poseestimation and more sophisticated robot manipulation, current approaches require a substantial volume of annotations of semantic, pose and affordance knowledge to fullydemonstrate their power. However, there are two primary issues demanding attention in 3D object knowledge annotation. Onone hand, some types of knowledge such as affordance for manipulation are highly complicated tomanually annotate , resulting in few existing datasets being available for such labels. On the otherhand, common practices of acquiring these knowledge annotations follow the conventionalparadigm that only a single type of knowledge is labeled on one object at a time, for which researchersdevelop different annotation platforms to adapt to various knowledge types and let annotators engagein multiple rounds of annotations, taking significant time and human effort. In this paper, We present ConceptFactory as a novel annotation paradigm that addresses theseexisting issues and facilitates more efficient annotation of 3D object knowledge. The idea behindConceptFactory originates from the well-known Recognition-by-Components theory in humancognition research, which finds that the perceptual recognition of objects can be explained as aprocess of arranging generalized geometric components. Inspired by this theory, we devise anefficient knowledge annotation paradigm performing in two steps. i) Describe the shape of an object",
  "arXiv:2411.00448v1 [cs.CV] 1 Nov 2024": "with generalized geometric concepts, or in other words, object conceptualization. ii) Procedurallydefine (different types of) knowledge on these generalized concepts. In this manner, all types ofknowledge defined on the concepts can be automatically propagated to the object as various types ofannotations, taking advantage of correspondence between the concepts and the object shape. ConceptFactory provides a favorable solution to both aforementioned issues. First, manual knowledgeannotation on 3D objects, which can be very complicated in some cases, is no longer required. Instead,researchers only need to procedurally define a type of knowledge with mathematical rules on certainconcepts, and these knowledge will be automatically propagated to all target objects consisting ofsuch concepts. Second, intensive human effort is required only once during object conceptualization,compared to the conventional annotation paradigm where significant labor and time resources arerepeatedly expended for annotating each type of knowledge. ConceptFactory comes with two critical components. The first one is ConceptFactory Suite, a unifiedtoolbox that adopts Standard Concept Template Library (STL-C) to drive a web-based platformfor object conceptualization. The STL-C consists of 263 concept templates that comprehensivelycovers the essential structure of daily objects, and the conceptualization platform guides usersto select and parameterize concept templates in STL-C to describe a given object and therebyobtains the conceptualization result. Then, a wide range of knowledge, which is procedurallydefined on the templates, can be automatically propagated to the object as annotations. The othercomponent is ConceptFactory Asset, a large collection of conceptualized objects acquired usingConceptFactory suite, containing 4380 objects from 39 categories involving 39k template instancesand 295k parameters. We present such asset considering that the object conceptualization processstill requires certain human effort, thereby offering already conceptualized objects to the communitywould make it convenient for researchers to use and study on, e.g. customizing their own knowledgeand conduct experiments with them. The knowledge annotations offered by our approach are mathematically grounded and functionallyaligned, serving as a catalyst for machine intelligence to recognize and interact with objects. Wedemonstrate the effectiveness of our idea from both vision and robotic aspects on a wide range ofbenchmark tasks including segmentation, pose estimation and robot manipulation through state-of-the-art algorithms, figuring out that our approach can easily gather various types of annotations, withquality comparable or even better than those acquired through conventional annotation paradigms.",
  "Object Recognition in Human Cognition": "Over the last few decades, numerous studies on cognitive science have placedtheir focus on the inner mechanisms within human perception of objects, and consider conceptualknowledge having major influences during such process . Biederman foundthat the perceptual recognition of objects is conceptualized to be a process in which an object issegmented into an arrangement of simple geometric components, such as blocks, cylinders, wedges,and cones. Meanwhile, other studies also indicates a strong connection between humanperception and conceptual knowledge. Such connection is even stronger for infants , sincethey are way less susceptible to empirical influences. These findings reveal a plausible path for humanunderstanding of objects, and also inspire us with a novel methodology to label abundant humanknowledge on objects, thereby helping intelligent agents to better understand the physical world.",
  "D Object Understanding Tasks": "As the basic elements that constitute our daily life, 3D objects usually carry abundant informationwithin their physical shapes assigned by humans. Given such crucial status of 3D objects, it is ofgreat importance to teach machine intelligence to understand them and thereby enable it to perceiveand interact with the objects. This involves both vision and robotics aspects. For vision tasks, oneof the frequently studied task is part segmentation , which aims at assigningvarious types of pre-defined labels to points on the object. Additionally, some recent studies alsofocus on part pose estimation , which queries the 6-dimensional transformation of detectedparts on the object, inferring their scales, rotations and positions. For robotics tasks, many studies",
  "D Object Datasets": "Throughout the years, datasets paved the way for machine learning across various modalities , empowering neural networks to carry out numerous sophisticated tasks . However, regarding the perception and interaction with 3D objects which play an important rolein daily life, most of the related large-scale object datasets throughout the years only provide class labels for each model, making them suitable for a very limited variety of taskslike classification. In order to mitigate this issue, many researchers have placed their efforts onannotating more detailed knowledge onto 3D models in these object datasets. For example, severalexisting popular datasets are derived from ShapeNet . ShapeNetPart offers part-levelsemantic segmentations of the models across 16 categories, and PartNet goes one step furtherand provides fine-grained segmentations across 24 categories. More recently, PartNet-Mobility was proposed with URDF styled annotations, which adds joint information for articulated objects.And GAPartNet offers annotations on generalizable and actionable parts (GAParts) which sharesimilar functionalities across different object categories. These valuable contributions significantlyfacilitate a wide range of both vision and robotics tasks.",
  "Knowledge Acquisition on 3D Objects": "Apart from gathering 3D object assets, it is also crucial to align various types of knowledge ontothese objects to enable training for modern-day networks. However, such knowledge are typicallyacquired under type-specific paradigms. For example, knowledge like segmentations are acquiredby either painting 2D projections or splitting/merging meshes , whereas pose-relatedknowledge are generally acquired via oriented bounding boxes . Affordance knowledge, beingmuch less definitive and task-specific, is very difficult to collect human annotations . Instead, oneline of work use repetitive random agent trials in simulations to acquire actionability scoresover pixels on object surfaces by observing the state changes of the object after each trial. As thediverse array of knowledge types requires different annotation manners, it can be labor-intensive andtime-consuming to acquire a full set of knowledge for an object. In this paper, we resolve this issueby proposing an efficient universal knowledge acquisition paradigm based on conceptualization.",
  "ConceptFactory Suite": "We develop the ConceptFactory suite as a general toolbox for object conceptualization. It consists oftwo major components, namely the Standard Concept Template Library (STL-C) and the correspond-ing conceptualization platform. We will introduce the construction of STL-C in Sec. 3.2. Then inSec. 3.3 we delve into the conceptualization platform and demonstrate how to use STL-C to describean object. Finally, we show how these conceptual descriptions facilitate efficient object knowledgeannotation in Sec. 3.4. But first, we discuss the motivations of developing ConceptFactory in Sec. 3.1.",
  "Why Concepts?": "The entire ConceptFactory suite takes inspiration from the advancements of researches on humancognition and brain science , where it is discovered that we humans learnabout the physical world by perceiving geometry patterns from objects and inducing them along withrelated knowledge as commonsense for future reference. Based on such findings, we establish a novelknowledge annotation paradigm for object understanding tasks by explicitly modelling such abstractcommonsense information as concepts for regular geometry patterns and reversing the inductionprocess. Specifically, by generalizing the concepts towards certain objects, various knowledgeassociated with the concepts can be automatically propagated to all these objects. An illustration ofsuch process is shown in -Left. Compared with conventional annotation process where only oneobject is labeled with a single type of knowledge at a time, such evolution in annotation paradigmwill greatly speed up the knowledge annotation process as well as diversify the types of knowledgethat can be annotated to the objects, empowering more sophisticated tasks in the data-driven era.",
  "lever_position = [(+)/2, , 0]": "self.axis = Cylinder(axis_size, axis_position) self.lever = Cuboid(lever_size, lever_position) : [Left] Illustration of the relationship between human cognition (a-b) and our approach(c-e), exemplified by handle as object and affordable interaction as knowledge. (a) Human recognizesobjects as an arrangement of geometric components. (b) Abstract commonsense information areinduced from the geometries in human mind. (c) Explicitly model the abstract information as aregular geometry concept with specific knowledge. (d) Generalize the concept towards differentobjects. (e) Propagate the knowledge from the concept to objects as annotations. [Right] Example ofparameters and the constructor of a concept template. Please refer to the codes in our website forconcept template implementations. : Shape instances of geometry (Top) and concept (Bottom) templates with specific parameters.[Bottom] The figures on the left side of the arrows display each geometry component of a concepttemplate individually, whereas those on the right side are example instances of concept templateswith various parameters. The instance at bottom-right is the result of modifying discrete parameters.",
  "Standard Concept Template Library": "Named after STL in C++ which provides commonly used program templates, our Standard ConceptTemplate Library, a.k.a. STL-C, consists of templates for numerous generalized geometry patternsthat are frequently notable in daily life. Each of the concept templates is implemented as a Pythonclass template, and can be instantiated as a concept instance that describes a 3D shape when giventhe template parameters. A brief illustration of the template architecture is in -Right. Geometry Templates.To avoid the tedious effort of repeatedly defining frequently used geometriesfrom scratch when developing concept templates, a good solution is to first build templates of thesegeometries aiming at facilitating the construction of concept templates through inheritance. Toachieve this goal, we parameterize the geometries by introducing geometry templates, which can beinstantiated into various geometry instances given different parameters. In practice, the constructionof STL-C has involved ten geometry templates, with some frequently used ones shown in -Top. Concept Templates.Based on geometry templates, we can easily construct concept templatesas a descriptor of geometry patterns. Specifically, each of the concept templates explicitly depictsa geometry pattern by assembling various templates* under specific constraints embedded in thepattern. Such constraints will manifest during parameterization of the concept template. That is,the parameters for the concept template will be processed under constraint-defined rules to generateparameters for member geometry templates, which then instantiate accordingly as parts of the conceptinstance. Particularly, when describing periodic patterns, we introduce additional discrete parameters",
  "to concept templates to specify the number of repetitions in geometry instances. -Bottom showsexamples of some concept templates": "Discussion.Currently, we have included a total of 263 distinct concept templates into STL-Cand found their capability to comprehensively cover a total of 4380 objects across 39 categories.In addition to the concept templates available in STL-C, it is also worth noting that thanks to theinheritable nature of a template representation, users can easily customize new templates usingexisting ones to cover novel shapes in specific applications. Such property of STL-C will significantlyimprove its representational power.",
  "Conceptualization Platform": "Web-based Interface.To efficiently perform the conceptualization process, we devise a web-basedinterface which divides the whole process into specific user tasks. Through such system, userscan easily choose concept templates for the parts of a given object, and adjust their parameters foroptimal approximation. Both the target object and the parameterized concept instances are renderedin real-time as reference. gives an overview to our conceptualization interface and workflow. Concept Parameter Optimizer.To further speed up the conceptualization process for each ofthe objects parts, we introduce a template parameter optimizer to the platform that is capable ofautomatically adjusting the concept parameters with a single click. The optimizer is made possiblethanks to the compatibility of concept templates to differentiable rendering. Specifically, a conceptinstance can be rendered through 1) differentiable calculations for parameters of member geometrytemplates, and 2) differentiable deformations on the geometry templates respective default template",
  "Principles for Object Conceptualization with STL-C": ": [Left] Minor gaps in geometric de-tails between the original object (bottom) andits conceptualization (top). [Right] Restoringthe geometric details via deformation basedon point-wise correspondences. Compared with the complexity of an object as awhole, each of the objects parts typically enjoysa much simpler structure as well as less variations,making them more suitable as units for conceptualiza-tion. Therefore, We divide objects of each categoryinto a group of parts according to their structural hi-erarchy for better guidance to the conceptualizationprocess. In practice, for each of the objects parts, wefirst choose a template from STL-C whose embeddedconcept best matches the parts geometric structure,and then we carefully parameterize the template sothat the resulting concept instance serves as an effec-tive approximation of the part. These parameterizedtemplate instances are then spatially arranged throughparameterized spatial transformations so that they arealigned with the objects respective parts, forming anconceptualization of the object. As the conceptualization is capable of effectively representing the objects structure in general, thereremains a gap between the objects actual shape and the structure in terms of geometric details, see-Left. Such gaps are typically the consequences of objects having local shape irregularities. Wedraw inspiration from BPS and address this issue by establishing a point-wise correspondencebetween the concepts and the actual shape of the object. Specifically, for each point x on the objectsurface, we find a corresponding point y on the concept instance that minimizes L2(x, y). Byestablishing such correspondence, we can restore the geometric details for the conceptual descriptionof an object by applying deformation (x y) to y, as is illustrated in -Right.",
  "& Optimize": ": An overview to our conceptualization interface and the workflow (blue arrow). The interfaceis divided into four components: work space, target view, template rendering, and mixed view. Inwork space, users first select best-match templates for each part of the target object, then parameterizeeach concept template with the help of the optimizer, and finally save the conceptualization result.Target view illustrates the shape of the target object, template rendering displays instances of concepttemplates with current parameters, while mixed view visualizes the integration between target view(gray) and template rendering (blue), helping users perform the conceptualization efficiently. Zoomin for a clear view.",
  "= ReLU 0.5 + 0.5,": ": Examples of differentiable deformations on template instances with default parameters.[Left] A default quadrangular prism instance in mesh with all edge lengths set to 1, bearing the sameshape as a cuboid. Its eight vertices are labelled from v1 to v8. The upper case show a translation isapplied to four top vertices for a 3d parallelogram, and the lower case show a scaling is applied to fourbottom vertices for a frustum of a pyramid. [Right] A sphere mesh with radius one and set of verticesV. The upper case show a scaling is applied to y-axis of all vertices for an ellipsoid, and the lower caseshow we use ReLU operation to truncate a sphere. Through differentiable transformations (addition,multiplication, etc.) on their respective vertices, the shapes can be deformed in a differentiablemanner. instances (a template instance with default parameters, e.g. a sphere with radius 1) parameterizedby the instances parameters. provides more details. By calculating and minimizing the gapwith loss functions between the template instance and the corresponding object part, the gradientscan be directly back-propagated to the templates parameters for updates. We adopt Point2Meshloss in our implementation. Benefiting from the optimizer, a large number of tedious parameteradjustments can be automatically achieved, and users just need to refine parameters that are notproperly optimized. The incorporation of parameter optimizer greatly reduces the workload duringconceptualization from about 10 min to 7 min per object on average.",
  "Procedural Knowledge Annotation": "After obtaining the conceptual description of an object, we explain how different types of knowledgeare annotated on the object. A significant benefit of our concept templates is its compatibility toprocedural definitions of knowledge, facilitating a fully automatic knowledge annotation scheme. Particularly, by implementing mathematically defined knowledge as attributes of STL-C templates,the knowledge on template instances can be propagated to the object, according to correspondencebetween the concepts and the object shape built during conceptualization. Considering the commonknowledge types on objects can be broadly classified into two distinct categories, namely region-basedknowledge and pose-based knowledge, we introduce their annotation mechanisms respectively asfollows. A brief illustration is in .",
  "lid.is_handle; lid.is_body;": "transformation matrix for translation : [Left] Conceptualization results of a KitchenPot, geometric details and certain parametersare omitted for simplicity. [Right] Procedural annotation for different types of knowledge. (a-b) Region-based knowledge like semantics and affordable area is implemented through regiondiscrimination function. (c-d) Pose-based knowledge like part pose and grasp pose is implementedwith transformations from local to world coordinates. Please refer to the codes in our website fordetailed implementations. Region-Based Knowledge Annotation.We consider region-based knowledge as collections oflabeled regions on an object, with one most common example being semantic segmentation. Toenable region-based knowledge annotation, users can implement region descriptions as a regiondiscrimination function which decides whether a given point is located within the designated regionof the concept instance. Then the set of points within this region can be considered as conforming tosuch knowledge. In this manner, numerous region-based knowledge can be easily defined onto thetemplates. And further through the point-wise correspondence (Sec. 3.3.1), the annotation of eachpoint on the object can be obtained at a fine-grained level, by propagating the knowledge from itscorresponding point on the concept. Pose-Based Knowledge Annotation.For pose-based knowledge such as part pose, grasp pose,etc., they can be initially defined in a concepts local coordinates, and then gradually transformed toworld coordinates as the concept finds its place in the overall object description. Visualization of Knowledge Annotation.We present visualizations of various knowledge types(affordance, semantic segmentation, and part pose) across object categories in Fig 7, showingcomparisons between original annotations (Left) and those from ConceptFactory Suite (Right). Objectaffordances are highlighted in red regions. Compared to the conventional annotation method , ourapproach enjoys two important benefits. First, our approach provides more accurate and consistentlabels with less noise, which greatly facilitates manipulation frameworks to better learn objectaffordance knowledge and thereby enhances its power. Second, the affordance labels are assignedby human experts instead of being acquired by trials in simulation environments . This ensurescontrollable manipulation as robots just learn those kind of affordance that provided by humanpreferences. Semantic segmentation annotations are denoted by distinct colors, and our approachproduces annotations nearly identical to the original ones, or even better in some cases. e.g. for chairin Row 1, Col 3-4, the original annotations confuse with the definition of crossbars between legswhile our approach reasonably and consistently labels these regions to part of legs. Part poses aredenoted by oriented bounding boxes, and our method accurately matches the precision of the originalannotations. Discussion.Compared with the conventional knowledge annotation paradigm where the annotationprocess is performed on one object at a time for every type of knowledge (e.g. about 8 min/objfor part semantics and 10 min/obj for part pose ), the concept based object descriptionoffers a once-and-for-all alternative at a much lower cost. Specifically, by defining knowledge onrelevant concept templates, the corresponding knowledge annotation is automatically completed onceconceptualization is performed on the object. The human time cost for our approach only involvesobject conceptualization, which is about 7 min/obj. The comparison demonstrates that, as the number",
  "ConceptFactory Asset": "With the help of ConceptFactory suite, we further present a large-scale ConceptFactory asset, provid-ing fine-grained conceptualization results for 4380 objects from 39 categories with 39k geometryinstances and 295k parameters, including an average of 8.8 geometry instances and 67.4 parametersin each conceptualization result. We select objects from popular sources that i) are widely used in vision and manipulation tasks, ii) include both CAD and scanned models, andiii) yield rich and diverse conceptualization results. Tab. 1 shows the statistics. These assets enablethe community to avoid certain human efforts involved in object conceptualization. Our purpose ofreleasing these assets is to help researchers easily acquire a large amount of well-annotated data tomeet their research needs in specific applications, by customizing knowledge according to Sec. 3.4.",
  "Ittl476 499 429 747 198 556 45245 1.4k 228 27237 12k 327 1.9k 528 394 140 341 39kImed6522611831113536.51531146.575-Imax16836101324317191031382254516141413-": "Pttl4.2k 3.1k 3.3k 4.4k 1.2k 3.7k 318 1.6k 7.6k 1.4k 243 1.4k 120k2.4k 24k 3.6k 1.8k 1.3k 3.2k 295kPmed5828188 3460552068.5 76292742180 27127 31357763-Pmax123 39259 6170142 23131 83432746248 48521 48387792- : ConceptFactory asset statistics per object category. Each category is denoted by a 3-charactercode (TTL for Total) and see the supplementary material for the cross reference table. For eachcategory, we count the number of objects N, as well as the number of geometry instances I andparameters P in the conceptualization results. For I and P, we report the total (ttl), median (med)and maximum (max) value. More visualizations and discussions are in the supplemental material.",
  "Experiments": "We conduct exhaustive experiments on ConceptFactory assets across various vision and manipulationtasks to validate the effectiveness of our knowledge annotation scheme. Specifically, we train baselinenetworks twice to get two separate models using either the original annotations, which are obtainedaccording to conventional knowledge acquisition paradigms, or annotations provided by our schemeon the same set of objects. We regard the performance gap between the two models as a measurementfor our annotations quality. See supplementary material for detailed experiment settings and results.",
  "Vision Tasks": "Overview.We conduct experiments on three vision tasks, i.e. semantic segmentation, cross categorypart segmentation and part pose estimation, to demonstrate the versatility of our concept-basedannotation scheme. We choose PointTransformer and PointNet++ as baseline networks forsemantic segmentation task, and GAPartNet for the other two tasks. The performance is evaluatedon test objects with the original annotations as ground truth. The experiments involve 2316 objects in29 categories from ConceptFactory assets which possess original annotations for these tasks. Main Results.Tab. 2 reports the performance of two separate baseline models using the originaland our annotations for training on three different tasks, showing only minor discrepancies betweenthem. This concretely proves that annotations provided by our scheme possess high quality on parwith original ones. Specifically, segmentation and pose estimation task respectively demonstrate the effectiveness of our approach in providing region-based and pose-based knowledge. We attributea slight performance decline in some cases to the cognitive variance between our annotators forobject conceptualization and the annotators involved in obtaining the original annotations, as weuse the original annotations as ground truth to evaluate the performance of both models. For certainperformance improvements, we consider the fact that our mathematically grounded annotations canoffer better consistency and lower noise, which supervise networks for better convergence.",
  "Manipulation Tasks": "Overview.We proceed to analyze the superiority of our approach in providing annotations formanipulation tasks. We introduce Where2Act , Where2Explore and GAPartNet as baselineapproaches, where the former two rely on affordance annotations for training and the latter onerelies on pose annotations. Unlike vision tasks where annotations for ground truths are still labeledby human based on ones subjective cognition, performance on manipulation tasks can be fairlyevaluated via success rate, a fully objective metric. Therefore, manipulation tasks can serve as aconcrete benchmark to more accurately validate the quality of our annotations. Particularly, weselect 989 objects in 15 categories from ConceptFactory assets, which are suitable for single-gripper-manipulation. These objects effectively cover a wide range of 26 manipulation tasks. The experimentsare conducted in SAPIEN simulator . Main Results.For Where2Act and Where2Explore, they require affordance annotations for training.However, it is extremely difficult to collect human annotations with previous annotation scheme ,and instead they acquire the affordance by interacting with objects in simulation. As our approachallows for human assignment of affordance annotations, we compare the performance of baselinemethods trained with the original and our affordance annotations in Tab. 3. Considering affordanceforms can vary significantly across different manipulation tasks, the remarkable improvements, up to26.6% on Where2Act-push, suggest the superiority of our affordance annotations in terms of bothversatility and quality. Particularly, we attribute the prominent improvements to three benefits of ourannotations compared with original ones acquired in simulation: i) better annotations consistency, ii)less noise, and iii) avoiding inaccurate labels caused by imperfections in simulation. We also regardthis capability of providing affordance knowledge as the major indication of the advantages of ourannotation scheme.",
  "Conclusion": "In this paper, we introduce ConceptFactory, a novel scope to facilitate more efficient annotation of3D object knowledge by recognizing 3D objects through generalized concepts, in order to promotemachine intelligence to learn comprehensive knowledge for various 3D object understanding tasksof both vision and robotics aspects. ConceptFactory suite is first proposed as a unified toolboxthat adopts Standard Concept Template Library (STL-C) to drive a web-based platform for objectconceptualization. Taking advantage of the concept parameter optimizer, users can easily concep-tualize an object with the platform and then automatically annotate various types of knowledge onthe object in a procedural process. We further present ConceptFactory assets as a large collection ofconceptualized objects acquired using ConceptFactory suite. With these assets, researchers can avoidcertain human efforts for object conceptualization and expeditiously acquire huge rich-annotated datafor their studies. We comprehensively evaluate the effectiveness of our annotation paradigm on fourrepresentative object understanding tasks from both vision and robotic aspects, and the experimentssuggest the high quality and versatility of annotations provided by our approach. Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet++: Deep hierarchical featurelearning on point sets in a metric space. Advances in neural information processing systems, 30, 2017. 1,2, 3",
  "Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip HS Torr, and Vladlen Koltun. Point transformer. InProceedings of the IEEE/CVF international conference on computer vision, pages 1625916268, 2021. 1,2, 9": "Jianhua Sun, Hao-Shu Fang, Xianghui Zhu, Jiefeng Li, and Cewu Lu. Correlation field for boosting 3dobject detection in structured scenes. In Proceedings of the AAAI conference on artificial intelligence,volume 36, pages 22982306, 2022. 1 Jianhua Sun, Hao-Shu Fang, Yuxuan Li, Runzhong Wang, Minghao Gou, and Cewu Lu. Instaboost++:Visual coherence principles for unified 2d/3d instance level data augmentation. International Journal ofComputer Vision, 131(10):26652681, 2023. 1 He Wang, Srinath Sridhar, Jingwei Huang, Julien Valentin, Shuran Song, and Leonidas J Guibas. Normal-ized object coordinate space for category-level 6d object pose and size estimation. In Proceedings of theIEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 26422651, 2019. 1 Haoran Geng, Helin Xu, Chengyang Zhao, Chao Xu, Li Yi, Siyuan Huang, and He Wang. Gapartnet:Cross-category domain-generalizable object perception and manipulation via generalizable and actionableparts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),pages 70817091, June 2023. 1, 2, 3, 7, 8, 9, 10 Kaichun Mo, Leonidas J Guibas, Mustafa Mukadam, Abhinav Gupta, and Shubham Tulsiani. Where2act:From pixels to actions for articulated 3d objects. In Proceedings of the IEEE/CVF International Conferenceon Computer Vision, pages 68136823, 2021. 1, 3, 7, 8, 10 Chuanruo Ning, Ruihai Wu, Haoran Lu, Kaichun Mo, and Hao Dong. Where2explore: Few-shot affordancelearning for unseen novel categories of articulated objects. Advances in Neural Information ProcessingSystems, 36, 2024. 1, 3, 10 Kaichun Mo, Shilin Zhu, Angel X. Chang, Li Yi, Subarna Tripathi, Leonidas J. Guibas, and Hao Su.PartNet: A large-scale benchmark for fine-grained and hierarchical part-level 3D object understanding. InThe IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019. 1, 3, 7, 8 Fanbo Xiang, Yuzhe Qin, Kaichun Mo, Yikuan Xia, Hao Zhu, Fangchen Liu, Minghua Liu, Hanxiao Jiang,Yifu Yuan, He Wang, Li Yi, Angel X. Chang, Leonidas J. Guibas, and Hao Su. SAPIEN: A simulatedpart-based interactive environment. In The IEEE Conference on Computer Vision and Pattern Recognition(CVPR), June 2020. 1, 2, 3, 8, 10",
  "Meng-Hao Guo, Jun-Xiong Cai, Zheng-Ning Liu, Tai-Jiang Mu, Ralph R Martin, and Shi-Min Hu. Pct:Point cloud transformer. Computational Visual Media, 7:187199, 2021. 2": "Jiayi Liu, Ali Mahdavi-Amiri, and Manolis Savva. Paris: Part-level reconstruction and motion analysis forarticulated objects. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages352363, 2023. 2 Yian Wang, Ruihai Wu, Kaichun Mo, Jiaqi Ke, Qingnan Fan, Leonidas Guibas, and Hao Dong. AdaAfford:Learning to adapt manipulation affordance for 3d articulated objects via few-shot interactions. Europeanconference on computer vision (ECCV 2022), 2022. 3",
  "George A Miller. Wordnet: a lexical database for english. Communications of the ACM, 38(11):3941,1995. 3": "Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchicalimage database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248255.Ieee, 2009. 3 Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollr,and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer VisionECCV 2014:13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages740755. Springer, 2014. 3 Angel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, SilvioSavarese, Manolis Savva, Shuran Song, Hao Su, et al. Shapenet: An information-rich 3d model repository.arXiv preprint arXiv:1512.03012, 2015. 3, 8 Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervisedlearning using nonequilibrium thermodynamics. In International conference on machine learning, pages22562265. PMLR, 2015. 3 Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770778, 2016.3 Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified, real-timeobject detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages779788, 2016. 3",
  "Philip Shilane, Patrick Min, Michael Kazhdan, and Thomas Funkhouser. The princeton shape benchmark.In Proceedings Shape Modeling Applications, 2004., pages 167178. IEEE, 2004. 3": "Vanamali ThiruvadandamPorethi, Afzal Godil, Helin Dutagaci, Zhouhui Lian, Ryutarou Ohbuchi, andTakahiko Furuya.Shrec 10 track: Generic 3d warehouse.Eurographics Workshop on 3D ObjectRetrieval(2010), Norrkping, SE, 2010-05-02 00:05:00 2010.URL 3 Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and Jianxiong Xiao.3d shapenets: A deep representation for volumetric shapes. In Proceedings of the IEEE conference oncomputer vision and pattern recognition, pages 19121920, 2015. 3 Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, AlanFan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, Eli VanderBilt, Aniruddha Kembhavi, CarlVondrick, Georgia Gkioxari, Kiana Ehsani, Ludwig Schmidt, and Ali Farhadi. Objaverse-xl: A universe of10m+ 3d objects. arXiv preprint arXiv:2307.05663, 2023. 3 Li Yi, Vladimir G Kim, Duygu Ceylan, I-Chao Shen, Mengyan Yan, Hao Su, Cewu Lu, Qixing Huang, AllaSheffer, and Leonidas Guibas. A scalable active framework for region annotation in 3d shape collections.ACM Transactions on Graphics (ToG), 35(6):112, 2016. 3, 8 Jianhua Sun, Yuxuan Li, Longfei Xu, Jiude Wei, Liang Chai, and Cewu Lu. Discovering conceptualknowledge with analytic ontology templates for articulated objects. arXiv preprint arXiv:2409.11702,2024. 3 Sergey Prokudin, Christoph Lassner, and Javier Romero. Efficient learning on point clouds with basis pointsets. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), October2019. 5"
}