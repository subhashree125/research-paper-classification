{
  "Abstract": "This paper presents an approach for designing neural networks, along with othermachine learning models, which adhere to a collection of input-output specifica-tions. Our method involves the construction of a constrained predictor for each setof compatible constraints, and combining these predictors in a safe manner using aconvex combination of their predictions. We demonstrate the applicability of thismethod with synthetic datasets and on an aircraft collision avoidance problem.",
  "Introduction": "The increasing adoption of machine learning models, such as neural networks, in safety-criticalapplications, such as autonomous vehicles and aircraft collision avoidance, highlights an urgentneed for the development of guarantees on safety and robustness. These models may be requiredto satisfy specific input-output specifications to ensure the algorithms comply with physical laws,can be executed safely, and are consistent with prior domain knowledge. Furthermore, these modelsshould demonstrate adversarial robustness, meaning their outputs should not change abruptly withinsmall input regions a property that neural networks often fail to satisfy. Recent studies have shown the capacity to verify formally input-output specifications and adversarialrobustness properties of neural networks. For instance, the Satisability Modulo Theory (SMT) solverReluplex was employed to verify properties of networks being used in the Next-Generation AircraftCollision Avoidance System for Unmanned aircraft (ACAS Xu). Reluplex has also been used toverify adversarial robustness. While Reluplex and other similar techniques can effectively determineif a network satisfies a given specification, they do not offer a way to guarantee that the network willmeet those specifications. Therefore, additional methods are needed to adjust networks if it is foundthat they are not meeting the desired properties. There has been an increase in techniques for designing networks with certified adversarial robustness,but enforcing more general safety properties in neural networks is still largely unexplored. One ap-proach to achieving provably correct neural networks is through abstraction-refinement optimization.This approach has been applied to the ACAS-Xu dataset, but the network was not guaranteed to meetthe specifications until after training. Our work seeks to design networks with enforced input-outputconstraints even before training has been completed. This will allow for online learning scenarioswhere a system has to guarantee safety throughout its operation. This paper presents an approach for designing a safe predictor (a neural network or any othermachine learning model) that will always meet a set of constraints on the input-output relationship.This assumes that the constrained output regions can be formulated to be convex. Our correct-by-construction safe predictor is guaranteed to satisfy the constraints, even before training, and atevery training step. We describe our approach in , and show its use in an aircraft collisionavoidance problem in . Results on synthetic datasets can be found in Appendix B.",
  "Method": "Considering two normed vector spaces, an input space X and an output space Y , and a collectionof c different pairs of input-output constraints, (Ai, Bi), where Ai X and Bi is a convex subsetof Y for each constraint i, the goal is to design a safe predictor, F : X Y , that guaranteesx Ai F(x) Bi. Let b be a bit-string of length c. Define Ob as the set of points z such that, for all i, bi = 1 impliesz Ai, and bi = 0 implies z / Ai. Ob thus represents the overlap regions for each combination ofinput constraints. For example, O101 is the set of points in A1 and A3, but not in A2, and O0...0 isthe set where no input constraints apply. We also define O as the set of bit strings, b, such that Obis non-empty, and define k = |O|. The sets {Ob : b O} create a partition of X according to thecombination of input constraints that apply.",
  "Theorem 2.1. For all i, if x Ai, then F(x) Bi": "A formal proof of Theorem 2.1 is presented in Appendix A and can be summarized as: if an input isin Ai, then by construction of the proximity and weighting functions, all of the constrained predictors,Gb, that do not map to Bi will be given zero weight. Only the constrained predictors that map toBi will be given non-zero weight, and because of the convexity of Bi, the weighted average of thepredictions will remain in Bi. If all Gb are continuous and if there are no two input sets, Ai and Aj, for which (Ai Aj) (AiAj), then F will be continuous. In the worst case, as the number of constraints grows linearly,the number of constrained predictors needed to describe our safe predictor grows exponentially. Inpractice, however, we expect many of the constraint overlap sets, Ob, to be empty. Consequently, anypredictors corresponding to an empty set can be ignored. This significantly reduces the number ofconstrained predictors needed for many applications.",
  "Proximity Functions": "The proximity functions, i, describe how close an input, x, is to a particular input constraint region,Ai. These functions are used to compute the weights of the constrained predictors. A desirableproperty for i is for i(x) 1 as d(x, Ai) , for some distance function. This ensures thatwhen an input is far from a constraint region, that constraint has little influence on the prediction forthat input. A natural choice for such a function is:",
  "Learning": "If we have families of differentiable functions Gb(x; b), continuously parameterized by b, andfamilies of i(x; i), differentiable and continuously parameterized by i, then F(x; , X), where = {b : b O} and X = {i : i = 1, ..., c}, is also continuously parameterized and differentiable.We can thus apply standard optimization techniques (e.g., gradient descent) to find parameters of Fthat minimize a loss function on some dataset, while also preserving the desired safety properties.Note that the safety guarantee holds regardless of the parameters. To create each Gb(x; b) weconsider choosing:",
  "and then defining Gb(x; b) = hb(gb(x; b))": "The framework proposed here does not require an entirely separate network for each b. In manyapplications, it may be advantageous for the constrained predictors to share earlier layers, thuscreating a shared representation of the input space. In addition, our definition of the safe predictor isgeneral and is not limited to neural networks. In Appendix B, we show examples of applying our approach to synthetic datasets in 2-D and 3-Dwith simple neural networks. These examples show that our safe predictor can enforce arbitraryinput-output specifications using convex output constraints on neural networks, and that the learnedfunction is smooth.",
  "Application to Aircraft Collision Avoidance": "Aircraft collision avoidance requires robust safety guarantees. The Next-Generation CollisionAvoidance System (ACAS X), which issues advisories to prevent near mid-air collisions, has bothmanned (ACAS Xa) and unmanned (ACAS Xu) variants. The system was originally designed tochoose optimal advisories while minimizing disruptive alerts by solving a partially observable Markovdecision process. The solution took the form of a large look-up table, mapping each possible inputcombination to scores for all possible advisories. The advisory with the highest score would then beissued. By using a deep neural network (DNN) to compress the policy tables, it has been necessary toverify that the DNNs meet certain safety specifications. A desirable 201csafeability201d property for ACAS X was defined in a previous work. This propertyspeci01ed that for any given input state within the 201csafeable region,201d an advisory would neverbe issued that could put the aircraft into a state where a safe advisory would no longer exist. Thisconcept is similar to control invariance. A simplified model of the ACAS Xa system was created,named VerticalCAS. DNNs were then generated to approximate the learned policy, and Reluplex wasused to verify whether the DNNs satisfied the safeability property. This work found thousands ofcounterexamples where the DNNs did not meet the criteria. Our approach for designing a safe predictor ensures any collision avoidance system will meet thesafeability property by construction. Appendix C describes in detail how we apply our approach toa subset of the VerticalCAS datasets using a conservative, convex approximation of the safeabilityconstraints. These constraints are defined such that if an aircraft state is in the \"unsafeable region\",Aunsafeable,i, for the ith advisory, the score for that advisory must not be the highest, i.e., x Aunsafeable,i Fi(x) < maxj Fj(x), where Fj(x) is the output score for the jth advisory. shows the performance of a standard, unconstrained network and our safe predictor. For bothnetworks, we present the percentage accuracy (ACC) and violations (percentage of inputs for whichthe network outputs an unsafe advisory). We train and test using PyTorch with two separate datasets,based on the previous advisory being Clear of Conflict (COC) and Climb at 1500 ft/min (CL1500).As shown in the table, our safe predictor adheres to the required safeability property. Furthermore,the accuracy of our predictor remains the same as the unconstrained network, demonstrating we arenot losing accuracy to achieve safety guarantees.",
  "Discussion and Future Work": "We propose an approach for designing a safe predictor that adheres to input-output specifications foruse in safety-critical machine learning systems, demonstrating it on an aircraft collision avoidanceproblem. The novelty of our approach is its simplicity and guaranteed enforcement of specificationsthrough combinations of convex output constraints during all stages of training. Future work includesadapting and using techniques from optimization and control barrier functions, as well as incorporatingnotions of adversarial robustness into our design, such as extending the work to bound the Lipschitzconstant of our networks.",
  "Example on Synthetic Datasets": "depicts an example of applying our safe predictor to a notional regression problem with 1-Dinput and outputs, and one input-output constraint. The unconstrained network has a single hiddenlayer of dimension 10 with ReLU activations, followed by a fully connected layer. The safe predictorshares this structure with constrained predictors, G0 and G1, but each predictor has its own fullyconnected layer. The training uses a sampled subset of points from the input space and the learnedpredictors are shown for the continuous input space. shows an example of applying the safe predictor to a notional regression problem with a 2-Dinput and 1-D output and two overlapping constraints. The unconstrained network has two hiddenlayers of dimension 20 with ReLU activations, followed by a fully connected layer. The constrainedpredictors G00, G10, G01 and G11 share the hidden layers and have an additional hidden layer of size20 with ReLU followed by a fully connected layer. Again, training uses a sampled subset of pointsfrom the input space and the learned predictors are shown for the continuous input space.",
  "C.1 Safeability Constraints": "The safeability property from prior work can be encoded into a set of input-output constraints. Thesafeable region for a given advisory is the set of input space locations where that advisory can bechosen, for which future advisories exist that will prevent an NMAC. If no future advisories exist forpreventing an NMAC, the advisory is deemed unsafeable, and the corresponding input region is theunsafeable region. shows an example of these regions for the CL1500 advisory.",
  "C.2 Proximity Functions": "We start by generating the bounds on the unsafeable regions. Then, a distance function is computedbetween points in the input space (vO vI, h, ), and the unsafeable region for each advisory. Whilethese are not true distances, their values are 0 if and only if the data point is inside the unsafeable set.When used to produce proximity functions as given in Equation 1, these values help ensure safety. shows examples of the unsafeable region, distance function, and proximity function for theCL1500 advisory.",
  "C.3 Structure of Predictors": "The compressed versions of the policy tables from prior work are neural networks with six hiddenlayers, 45 dimensions in each layer, and ReLU activation functions. We use the same architecturefor our standard, unconstrained network. For constrained predictors, we use a similar architecture.However, the first four hidden layers are shared between all of the predictors. This learns a single,shared input space representation, and also allows each predictor to adapt to its constraints. Eachconstrained predictor has two additional hidden layers and their outputs are projected onto our convexapproximation of the safe output region. We accomplish this by setting the score for any unsafeableadvisory i to Gi(x) = minj Gj(x) . In our experiments, we used = 0.0001. To enforce the VerticalCAS safeability constraints, we need 30 separate predictors. This increasesthe size of the network from 270 to 2880 nodes for the unconstrained and safe implementationsrespectively. However, our safe predictor remains smaller than the original look-up tables by severalorders of magnitude.",
  "C.4 Parameter Optimization": "We define our networks and perform parameter optimization using PyTorch. We optimize theparameters of both the unconstrained network and our safe predictor using the asymmetric lossfunction, guiding the network to select optimal advisories while accurately predicting scores fromthe look-up tables. Each dataset is split using an 80/20 train/test split, with a random seed of 0. Theoptimizer is ADAM, with a learning rate of 0.0003, a batch size of 216, and the number of trainingepochs is 500."
}