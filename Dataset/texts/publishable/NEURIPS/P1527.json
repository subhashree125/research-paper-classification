{
  "Abstract": "Multimodal Sentiment Analysis (MSA) is an important research area that aims tounderstand and recognize human sentiment through multiple modalities. The com-plementary information provided by multimodal fusion promotes better sentimentanalysis compared to utilizing only a single modality. Nevertheless, in real-worldapplications, many unavoidable factors may lead to situations of uncertain modalitymissing, thus hindering the effectiveness of multimodal modeling and degradingthe models performance. To this end, we propose a Hierarchical RepresentationLearning Framework (HRLF) for the MSA task under uncertain missing modali-ties. Specifically, we propose a fine-grained representation factorization modulethat sufficiently extracts valuable sentiment information by factorizing modalityinto sentiment-relevant and modality-specific representations through crossmodaltranslation and sentiment semantic reconstruction. Moreover, a hierarchical mutualinformation maximization mechanism is introduced to incrementally maximize themutual information between multi-scale representations to align and reconstruct thehigh-level semantics in the representations. Ultimately, we propose a hierarchicaladversarial learning mechanism that further aligns and adapts the latent distributionof sentiment-relevant representations to produce robust joint multimodal repre-sentations. Comprehensive experiments on three datasets demonstrate that HRLFsignificantly improves MSA performance under uncertain modality missing cases.",
  "Introduction": "Multimodal sentiment analysis (MSA) has attracted wide attention in recent years. Unlike unimodalemotion recognition tasks , MSA understands and recognizes human emotionsthrough multiple modalities, including language, audio, and visual . Previous studies haveshown that combining complementary information among different modalities facilitates valuablesemantic generation . MSA has been well studied so far under the assumptionthat all modalities are available in the training and inference phases . Nevertheless, in real-world applications, modalities may be missing due to security concerns,background noises, sensor limitations and so on. Ultimately, these incomplete multimodal datasignificantly hinder the performance of MSA. For instance, as shown in , the entire visual",
  "modality and some frame-level features in the language and audio modalities are missing, leading toan incorrect prediction": "In recent years, many studies attempt to address theproblem of missing modalities in MSA. For example, SMIL estimates the latent features ofthe missing modality data via Bayesian Meta-Learning. However, these methods are constrainedby the following factors: (i) Implementing complex feature interactions for incomplete modalitiesleads to a large amount of information redundancy and cumulative errors, resulting in ineffectiveextraction of sentiment semantics. (ii) Lacking consideration of semantic and distributional alignmentof representations, causing imprecise feature reconstruction and nonrobust joint representations.",
  ": A case of incorrect prediction by the tra-ditional model with missing modalities. The pinkand yellow areas indicate intra- and inter-modalitymissingness, respectively": "To address the above issues, we propose a Hi-erarchical Representation Learning Framework(HRLF) for the MSA task under uncertain miss-ing modalities. HRLF has three core contribu-tions: (i) We present a fine-grained representa-tion factorization module that sufficiently ex-tracts valuable sentiment information by fac-torizing modality into sentiment-relevant andmodality-specific representations through intra-and inter-modality translations and sentiment se-mantic reconstruction. (ii) Furthermore, a hierar-chical mutual information maximization mech-anism is introduced to incrementally align thehigh-level semantics by maximizing the mutualinformation of the multi-scale representationsof both networks in knowledge distillation. (iii)Eventually, we propose a hierarchical adversar-ial learning mechanism to progressively alignthe latent distributions of representations leveraging multi-scale adversarial learning. Based on thesecomponents, HRLF significantly improves MSA performance under uncertain modality missing caseson three multimodal benchmarks.",
  "Multimodal Sentiment Analysis": "Multimodal Sentiment Analysis (MSA) seeks to comprehend and analyze human sentiment byutilizing diverse modalities. Unlike conventional single-modality sentiment recognition, MSA posesgreater challenges owing to the intricate nature of processing and analyzing heterogeneous dataacross modalities. Mainstream studies in MSA focus on designingcomplex fusion paradigms and interaction mechanisms to improve MSA performance. For instance,CubeMLP employes three distinct multi-layer perceptron units for feature amalgamation alongthree axes. However, these methods rely on complete modalities and thus are impractical for real-world deployment. There are two primary approaches for addressing the missing modality problemin MSA: (1) Generative methods and (2) joint learning methods . Generative methods aim to regenerate missing features and semantics within modalities byleveraging the distributions of available modalities. For example, TFR-Net employes a featurereconstruction module to guide the extractor to reconstruct missing semantics. Joint learning methodsfocus on deriving cohesive joint multimodal representations based on inter-modality correlations.For instance, MMIN produces robust joint multimodal representations via cross-modalityimagination. However, these methods cannot extract rich sentiment information from incompletemodalities due to their inefficient interaction. In contrast, our learning paradigm achieves effectiveextraction and precise reconstruction of sentiment semantics through complete modality factorization.",
  ")": ": The structure of our HRLF, which consists of three core components: Fine-grainedRepresentation Factorization (FRF) module, Hierarchical Mutual Information (HMI) maximizationmechanism, and Hierarchical Adversarial Learning (HAL) mechanism. representation learning primarily rely on auto-encoders and generative adversarial networks .For example, FactorVA is introduced to achieve factorization by leveraging the characteristic thatrepresentations are both factorial and independent in dimension. Recently, factorization learning hasbeen progressively utilized in MSA tasks . For instance, FDMER utilizes consistencyand discreteness constraints between modalities to disentangle modalities into modality-invariantand modality-private features. DMD disentangles each modality into modality-independent andmodality-exclusive representations and then implements a knowledge distillation strategy amongthe representations with dynamic graphs. MFSA refines multimodal representations and learnscomplementary representations across modalities by learning modality-specific and modality-agnosticrepresentations. Despite the progress these studies have brought to MSA, certain limitations persist:(i) The supervision of the factorization process is coarse-grained and insufficient. (ii) Focusing solelyon factorizing distinct representations at the modality level, without taking into account sentimentallybeneficial and relevant representations. By contrast, the proposed method decomposes sentiment-relevant representations precisely through intra- and inter-modality translation and sentiment semanticreconstruction. Furthermore, hierarchical mutual information maximization and adversarial learningparadigms are employed to refine and optimize the representation of factorization at the semanticlevel and the distributional level, respectively, thus yielding robust joint multimodal representations.",
  "Knowledge Distillation": "Knowledge distillation leverages additional supervisory signals from a pre-trained teacher network toaid in training a student network . There are generally two categories of knowledge distillationmethods: distillation from intermediate features and distillationfrom logits . Many studies utilize knowledge distillationfor MSA tasks with missing modalities. These approaches aim to transfer dark knowledge fromteacher networks trained on complete modalities to student networks trained by missing modalities.The teacher network typically provides richer and more comprehensive feature representationsthan the student network. For instance, KD-Net utilizes a teacher network with completemodalities to supervise the unimodal student network at both the feature and logits levels. Despitetheir promising results, these methods neglect precise supervision of representations, resulting inlow-quality knowledge transfer. To this end, we implement hierarchical semantic and distributionalalignment of the multi-scale representations of both networks to transfer knowledge effectively.",
  "Given a multimodal video segment with three modalities as S = [XL, XA, XV ], where XL RTLdL, XA RTAdA, and XV RTV dV denote language, audio, and visual modalities,": "respectively. = {L, A, V } denotes the set of modality types. Tm() is the sequence length anddm() is the embedding dimension, where m . We define two missing modality cases to simulatethe most natural and holistic challenges in real-world scenarios: (1) intra-modality missingness,which indicates some frame-level features in the modality sequences are missing. (2) inter-modalitymissingness, which denotes some modalities are entirely missing. We aim to recognize the utterance-level sentiments using incomplete multimodal data.",
  "Overall Framework": "illustrates the main workflow of HRLF. The teacher and student networks adopt a consistentstructure but have different parameters. During the training phase, the workflow of our HRLF is asfollows: (i) We first train the teacher network with complete-modality samples and their sentimentlabels. (ii) Given a video segment sample S, we generate a missing-modality sample S withthe Modality Stochastic Missing (MSM) strategy. MSM simultaneously performs intra-modalitymissingness and inter-modality missingness. S and S are fed into the pre-trained teacher networkand the initialized student network, respectively. (iii) We input each sample into the FRF module,to factorize each modality into a sentiment-relevant representation Qm and a modality-specificrepresentation Um, where m . (iv) Sequences [CL, CA, CV ] and [CL, CA, CV ] are generatedby concatenating Qm and Um from all modalities in the teacher and student networks. Each elementof the sequences is concatenated to yield the joint multimodal representations Ht and Hs. (v)The multi-scale representations of both networks are obtained by passing Ht and Hs through thefully-connected layers. The proposed HMI and HAL are used to align the semantics and distributionbetween the multiscale representations. (vi) The outputs Ht and Hs of the fully-connected layersare fed into the task-specific classifier to get logits Lt and Ls. We constrain the consistency betweenlogits and utilize Ls to implement the sentiment prediction. In the inference phase, testing samplesare only fed into the student network for downstream tasks.",
  "Fine-grained Representation Factorization": "Modality missing leads to ambiguous sentiment cues in the modality and information redundancy inmultimodal fusion. It hinders the model from capturing valuable sentiment semantics and filteringsentiment irrelevant information. Although previous studies in MSA decompose the task-relevant semantics contained in the modality to some extent via simple auto-encoder networks withreconstruction constraints, their purification of sentiment semantics is inadequate, and they cannotbe applied to modality missing scenarios. Therefore, we propose a Fine-grained RepresentationFactorization (FRF) module to capture sentiment semantics in modalities. The core idea is to factorizeeach modality representation into two types of representations: (1) sentiment-relevant representation,which contains the holistic sentiment semantics of the sample. It is modality-independent, sharedacross all modalities of the same subject, and robust to modality missing situations. (2) modality-specific representation, which represents modality-specific task-independent information. As shown in , FRF receives the multimodal sequences [XL, XA, XV ] with modality numbern = 3. The modality X with passes through a 1D temporal convolutional layer with kernelsize 3 3 and adds the positional embedding to obtain the preliminary representations, denotedas R = W33(X) + PE(T, d) RTd. The R is fed into a Transformer encoderF(), and the last element of its output is denoted as Z = F(R) Rd. The Z Z is thelow-level modality representation of the modality . We aim to factorize modality representation Zinto a sentiment-relevant representation Q by a sentiment encoder Q = ES(Z) and a modality-specific representation U by a modality encoder U = EM (Z). ES() and EM () are composedof multi-layer perceptrons with the ReLU activation. The following two processes ensure adequatefactorization and semantic reinforcement of the above two representations. Intra- and Inter-modality Translation. The proposed FRF effectively decouples sentiment-relevantand modality-specific representations by simultaneously performing intra- and inter-modality transla-tions. Given a pair of representations Q and U factorized by Z and Z with , , the decoderDr() is supposed to translate and synthesize the representation Z, whose reconstructed domaincorresponds to the modality representation Z Z. The Dr() consists of feed-forward neurallayers. The modality translations include intra-modality translation (i.e., = ) and inter-modalitytranslation (i.e., = ), whose losses are respectively denoted as:",
  "Hierarchical Mutual Information Maximization": "The underlying assumption of knowledge distillation is that layers in the pre-trained teacher networkcan represent certain attributes of given inputs that exist in the task . For successful knowledgetransfer, the student network must learn to incorporate such attributes into its own learning. Neverthe-less, previous studies based on knowledge distillation simply constrain the consistencybetween the features of both networks and lack consideration of the intrinsic semantics and inherentproperties of the features, leading to semantic misalignment. From the perspective of informationtheory , semantic alignment and attribute mining of representations can be characterized as main-taining high mutual information among the layers of the teacher and student networks. We constructa Hierarchical Mutual Information (HMI) maximization mechanism to implement sufficient semanticalignment and maximize mutual information. The core idea is to progressively align the semantics ofrepresentations through a hierarchical learning paradigm. Specifically, the sentiment-relevant and modality-specific representations Qm and Um of all modal-ities for teacher and student networks are concatenated to obtain the sequences [CL, CA, CV ]and [CL, CA, CV ]. Each element of the sequences is concatenated to yield the joint multimodalrepresentations Ht and Hs. The fully-connected layers are utilized to refine the representationHw R3d with w {t, s}, yielding Hw R3d. Moreover, we obtain the intermediate multi-scalerepresentations of all layers, denoted as Iw1 R2d, Iw2 Rd, and Iw3 R2d. For the above fiverepresentations, we concatenate features of the same scale to obtain multi-scale representationsEw1 R3d, Ew2 R2d, and Ew3 Rd, which are utilized in the subsequent computation. To estimate and compute the mutual information between representations, we define two randomvariables X and Y . The P(X) and P(Y ) are the marginal probability density function of X and Y .The joint probability density function of X and Y is denoted as P(X, Y ). The mutual informationof the random variables X and Y is represented as:",
  ".(5)": "We only need to obtain the maximum value of the mutual information, without focusing on its exactvalue. Referring to Deep InfoMax , we estimate the mutual information between variables basedon the Jensen-Shannon Divergence (JSD). The mutual information maximization issue translates intominimizing the JSD between the joint distribution p(x, y) and the marginal distribution p(x)p(y).",
  "Hierarchical Adversarial Learning": "Considering that the teacher network has more robust and stable representation distributions, wealso need to encourage the alignment of representation distributions in the latent space. Traditionalmethods simply minimize the KL divergence between both networks, which easilydisturbs the underlying learning of the student network in the deep layers, leading to confoundeddistributions and unrobust joint multimodal representations. To this end, we propose a Hierarchical Adversarial Learning (HAL) mechanism for incrementallyaligning the latent distributions between representations of student and teacher networks. The centralprinciple is that the student network tries to generate representations to mislead the discriminatorDe(), while De() discriminates between the representations of the student and teacher networks.In practice, De() is the fully-connected layers. Specifically, given multi-scale representations ofEw1 R3d, Ew2 R2d, and Ew3 Rd with w {t, s}, we implement adversarial learning on thesame-scale representations of the teacher and student networks to hierarchically supervise consistency.The objective function of HAL is formatted as:",
  "Optimization Objectives": "The Ht and Hs of the teacher and student networks are fed into their task-specific classifiers toproduce logits Lt and Ls, respectively, and the consistency of both is constrained with KL divergenceloss, denoted as LKL = KL(Lt, Ls). The Ls is used for sentiment recognition and supervised withtask loss, represented as Ltask. For the classification and regression tasks, we use cross-entropyand MSE loss as the task losses, respectively. The overall training objective Ltotal is expressed asLtotal = Ltask + LF RF + LHMI + LHAL + LKL.",
  "Datasets and Evaluation Metrics": "We conduct our experiments on three MSA benchmarks, including MOSI , MOSEI , andIEMOCAP . The experiments are performed under the word-aligned setting. MOSI is a realisticdataset for MSA. It comprises 2,199 short monologue video clips taken from 93 YouTube moviereview videos. There are 1,284, 229, and 686 video clips in train, valid, and test data, respectively.MOSEI is a dataset consisting of 22,856 movie review video clips, which has 16,326, 1,871, and4,659 samples in train, valid, and test data. Each sample of MOSI and MOSEI is labelled by humanannotators with a sentiment score of -3 (strongly negative) to +3 (strongly positive). On the MOSIand MOSEI datasets, we utilize two evaluation metrics, including the Mean Absolute Error (MAE)and F1 score computed for positive/negative classification results. The IEMOCAP dataset consistsof 4,453 samples of video clips. Its predetermined data partition has 2,717, 798, and 938 samplesin train, valid, and test data. As recommended by , four emotions (i.e., happy, sad, angry, andneutral) are selected for emotion recognition. The F1 score is used as the metric.",
  ": Comparison results of intra-modality missingness on IEMOCAP. We report on the F1score metric for the happy, sad, angry, and neutral categories": "toolkit to extract 74-dimensional acoustic features, including 12 Mel-frequency cepstral coeffi-cients, voiced/unvoiced segmenting features, and glottal source parameters. For the visual modality,we utilize the Facet to indicate 35 facial action units that record facial movement. Experimental Setup. Regarding the MOSI and MOSEI datasets, we use the alignedmultimodal sequences therein (e.g., all sequences of modalities have length 300) as the original inputfor the HRLF. All models are built on the Pytorch toolbox with four NVIDIA Tesla V100 GPUs.The Adam optimizer is employed for network optimization. For MOSI, MOSEI, and IEMOCAP,the detailed hyper-parameter settings are as follows: the learning rates are {1e 3, 2e 3, 4e 3},the batch sizes are {128, 16, 32}, the epoch numbers are {50, 20, 30}, and the attention heads are{10, 8, 10}. The embedding dimension is 40 on all three datasets. The raw features at the modalitymissing positions are replaced by zero vectors. For a fair comparison, we re-implement the State-Of-The-Art (SOTA) methods and combine them with our experimental paradigms. All experimentalresults are averaged over multiple experiments using five different random seeds.",
  "Comparison with State-of-the-art Methods": "We conduct a comparison between HRLF and eight representative, reproducible state-of-the-art(SOTA) methods, including complete-modality methods: Self-MM , CubeMLP , and DMD, and missing-modality methods: 1) joint learning methods (i.e., MCTN , TransM ,and CorrKD ), and 2) generative methods (i.e., SMIL and GCNet ). The extensiveexperiments are designed to comprehensively assess the robustness and effectiveness of HRLF inscenarios involving both intra-modality and inter-modality missingness. Robustness to Intra-modality Missingness. We simulate intra-modality missingness by ran-domly discarding frame-level features in sequences with ratio p {0.1, 0.2, , 1.0}. To vi-sualize the robustness of all models, and 4 show the performance curves of the mod-els for different ratios p. We have the following important observations. (i) As the ratio p in-creases, the performance of all models declines. This phenomenon demonstrates that intra-modalitymissingness leads to significant sentiment semantic loss and fragile multimodal representations.",
  ": Comparison results of intra-modality miss-ingness on (a) MOSI and (b) MOSEI": "(ii) Compared to complete-modality meth-ods, our HRLF demonstrates notable per-formance advantages in missing-modalitytesting conditions and competitive perfor-mance in complete-modality testing condi-tions. This is because complete-modalitymethods rely on the assumption of datacompleteness, while training paradigms formissing modalities excel in capturing andreconstructing valuable sentiment seman-tics from incomplete multimodal data. (iii)In contrast to the missing-modality meth-ods, our HRLF demonstrates the highestlevel of robustness. Through the purification of sentiment semantics and the dual alignment of repre-sentations, the student network masters the core competencies of precisely reconstructing missingsemantics and generating robust multimodal representations. Robustness to Inter-modality Missingness. To simulate the case of inter-modality missingness, weremove certain entire modalities from the samples. Tables 1 and 2 contrast the models resilience tointer-modality missingness. The notation {l} signifies that only the language modality is available, : Comparison of performance under six possible testing conditions of inter-modality miss-ingness and the complete-modality testing condition on the MOSI and MOSEI datasets. T-test isconducted on Avg. column. indicates that p < 0.05 (compared with the SOTA CorrKD).",
  "MOSEI": "Self-MM 71.5343.5737.6175.9174.6249.5258.7983.69CubeMLP 67.5239.5432.5871.6970.0648.5454.9983.17DMD 70.2646.1839.8474.7872.4552.7059.3784.78MCTN 75.5062.7259.4676.6477.1364.8469.3881.75TransM 77.9863.6858.6780.4678.6162.2470.2781.48SMIL 76.5765.9660.5777.6876.2466.8770.6580.74GCNet 80.5266.5461.8381.9681.1569.2173.5482.35CorrKD 80.7666.0962.3081.7481.2871.9274.0282.16HRLF (Ours)82.0569.3264.9082.6281.0973.8075.6382.93 while the audio and visual modalities are missing. {l, a, v} denotes the complete-modality testingcondition where all modalities are available. Avg. indicates the average performance across sixmissing-modality testing conditions. We have the following key findings: (i) The inter-modality miss-ingness leads to a decline in performance for all models, indicating that integrating complementaryinformation from diverse modalities enhances the sentiment semantics within joint representations.(ii) Across all six testing conditions involving inter-modality missingness, our HRLF consistentlydemonstrates superior performance among the majority of metrics, affirming its robustness. Forexample, on the MOSI dataset, HRLFs average F1 score is improved by 2.05% compared to CorrKD,and in particular by 3.87% in the testing condition where only visual modality is available (i.e.,{v}). The advantage comes from its learning of fine-grained representation factorization and thehierarchical semantic alignment and distributional alignment. (iii) In unimodal testing scenarios,HRLFs performance using only the language modality significantly exceeds other configurations,showing performance similar to that of the complete-modality setup. In bimodal testing scenarios,configurations involving the language modality exhibit superior performance, even outperformingthe complete-modality setup in specific metrics. This phenomenon underscores the richness ofsentiment semantics within the language modality and its dominance in sentiment inference andmissing semantic reconstruction processes.",
  ": Ablation results of intra-modalitymissingness case on the MOSI dataset": "To affirm the effectiveness and indispensability of themodule and mechanisms and strategies proposed inHRLF, we perform ablation experiments under twomissing-modality scenarios on the MOSI dataset, asshown in and . We have the fol-lowing important observations: (i) First, when theFRF is removed, sentiment-relevant and modality-specific information in the modalities are confused,hindering sentiment recognition and leading to sig-nificant performance degradation. This phenomenondemonstrates the effectiveness of the proposed repre-sentation factorization paradigm for adequate captureof valuable sentiment semantics. (ii) When our HMI : Comparison of performance under six possible testing conditions of inter-modality missing-ness and the complete-modality testing condition on the IEMOCAP dataset. T-test is conducted onAvg. column. indicates that p < 0.05 (compared with the SOTA CorrKD).",
  "Happy84.971.869.786.485.672.378.588.1Sad83.771.169.085.383.973.677.886.4Angry83.469.167.284.583.570.976.486.7Neutral66.856.154.568.967.056.961.771.3": "is eliminated, the worse performance demonstrates that aligning the high-level semantics in therepresentation by maximizing mutual information can generate favorable joint representations for thestudent network. (iii) Finally, we remove HAL, and the declined results illustrate that multi-scaleadversarial learning can effectively align the representation distributions of student and teacher net-works, thus effectively constraining the consistency across representations. This paradigm facilitatesthe recovery of missing semantics.",
  "Qualitative Analysis": "To intuitively show the robustness of the proposed framework against modality missingness, werandomly select 100 samples in each emotion category on the IEMOCAP testing set to performthe visualization evaluation. The comparison models include CubeMLP (complete-modalitymethod), TransM (joint learning-based missing-modality method), and GCNet (generation-based missing-modality method). (i) As shown in , CubeMLP fails to cope with the missingmodality challenge because representations with different emotion categories are heavily confounded,",
  "(c) GCNet(a) CubeMLP(b) TransM(d) HRLF": ": Visualization of representations from different methods with four emotion categories onthe IEMOCAP testing set. The default testing conditions contain intra-modality missingness (i.e.,missing rate p = 0.5 ) and inter-modality missingness (i.e., only the language modality is available). leading to the worst results. (ii) Although TransM and GCNet mitigate the indistinguishable emotionsemantics to some extent, their performance is sub-optimal since the distribution boundaries of thedifferent emotion representations are generally ambiguous and coupled. (iii) In comparison, our HRLFenables representations belonging to the same emotion category to form compact clusters, whilerepresentations of different categories are well separated. The above phenomenon benefits from theeffective extraction of sentiment semantics and the precise filtering of task redundant information bythe proposed hierarchical representation learning framework, which results in better joint multimodalrepresentations. This further confirms the robustness and superiority of our framework.",
  "Conclusion and Discussion": "In this paper, we present a Hierarchical Representation Learning Framework (HRLF) to addressdiverse missing modality dilemmas in the MSA task. Specifically, we mine sentiment-relevant repre-sentations through a fine-grained representation factorization module. Additionally, the hierarchicalmutual information maximization mechanism and the hierarchical adversarial learning mechanism areproposed for semantic and distributional alignment of representations of student and teacher networksto accurately reconstruct missing semantics and produce robust joint multimodal representations.Comprehensive experiments validate the superiority of our framework. Discussion of Limitation and Future Work. The current method defines the modality missingcases as both inter-modality missingness and intra-modality missingness. Nevertheless, in real-worldapplications, modality missing cases may be very intricate and difficult to simulate. Consequently,the proposed method may suffer some minor performance loss when applied to real-world scenarios.In the future, we will explore more intricate modality missing cases and design suitable algorithms tocompensate for this deficiency. Discussion of Broad Impacts. The positive impact of our approach lies in the ability to significantlyimprove the robustness and stability of multimodal sentiment analysis systems against heterogeneousmodality missingness in real-world applications. Nevertheless, this technology may have a negativeimpact when it falls into the wrong hands, e.g., the proposed model is used for malicious purposes byinjecting biased priors to recognize the emotions of specific groups.",
  "This work was supported in part by National Key R&D Program of China 2021ZD0113502and in part by Shanghai Municipal Science and Technology Major Project 2021SHZDZX0103": "Sungsoo Ahn, Shell Xu Hu, Andreas Damianou, Neil D Lawrence, and Zhenwen Dai. Vari-ational information distillation for knowledge transfer. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition (CVPR), pages 91639171, 2019. 5 Tadas Baltruaitis, Peter Robinson, and Louis-Philippe Morency. Openface: an open sourcefacial behavior analysis toolkit. In 2016 IEEE Winter Conference on Applications of ComputerVision (WACV), pages 110. IEEE, 2016. 7",
  "Konstantinos Bousmalis, George Trigeorgis, Nathan Silberman, Dilip Krishnan, and DumitruErhan. Domain separation networks. Conference on Neural Information Processing Systems(NeurIPS), 29, 2016. 3": "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe Kazemzadeh, Emily Mower, Samuel Kim,Jeannette N Chang, Sungbok Lee, and Shrikanth S Narayanan. Iemocap: Interactive emotionaldyadic motion capture database. Language Resources and Evaluation, 42:335359, 2008. 6 Jae Won Cho, Dong-Jin Kim, Jinsoo Choi, Yunjae Jung, and In So Kweon. Dealing withmissing modalities in the visual question answer-difference prediction task through knowledgedistillation. In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR), pages 15921601, 2021. 3",
  "Jang Hyun Cho and Bharath Hariharan. On the efficacy of knowledge distillation. In Proceedingsof the IEEE/CVF International Conference on Computer Vision (ICCV), pages 47944802,2019. 3": "Gilles Degottex, John Kane, Thomas Drugman, Tuomo Raitio, and Stefan Scherer. Covarepacollaborative voice analysis repository for speech technologies. In 2014 Ieee InternationalConference on Acoustics, Speech and Signal Processing (ICASSP), pages 960964. IEEE, 2014.7 Changde Du, Changying Du, Hao Wang, Jinpeng Li, Wei-Long Zheng, Bao-Liang Lu, andHuiguang He.Semi-supervised deep generative modelling of incomplete multi-modalityemotional data. In Proceedings of the 26th ACM international conference on Multimedia (ACMMM), pages 108116, 2018. 2 Yangtao Du, Dingkang Yang, Peng Zhai, Mingchen Li, and Lihua Zhang. Learning associativerepresentation for facial expression recognition. In IEEE International Conference on ImageProcessing (ICIP), pages 889893, 2021. 1 Tommaso Furlanello, Zachary Lipton, Michael Tschannen, Laurent Itti, and Anima Anandkumar.Born again neural networks. In International Conference on Machine Learning (ICML), pages16071616. PMLR, 2018. 3",
  "Wei Han, Hui Chen, and Soujanya Poria. Improving multimodal fusion with hierarchical mutualinformation maximization for multimodal sentiment analysis. arXiv preprint arXiv:2109.00412,2021. 2": "Devamanyu Hazarika, Roger Zimmermann, and Soujanya Poria. Misa: Modality-invariantand-specific representations for multimodal sentiment analysis. In Proceedings of the 28th ACMInternational Conference on Multimedia (ACM MM), pages 11221131, 2020. 1, 2, 4 Byeongho Heo, Jeesoo Kim, Sangdoo Yun, Hyojin Park, Nojun Kwak, and Jin Young Choi. Acomprehensive overhaul of feature distillation. In Proceedings of the IEEE/CVF InternationalConference on Computer Vision (ICCV), pages 19211930, 2019. 3 Byeongho Heo, Minsik Lee, Sangdoo Yun, and Jin Young Choi. Knowledge transfer viadistillation of activation boundaries formed by hidden neurons. In Proceedings of the AAAIConference on Artificial Intelligence (AAAI), volume 33, pages 37793787, 2019. 3",
  "Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network.arXiv preprint arXiv:1503.02531, 2015. 3, 5": "R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman,Adam Trischler, and Yoshua Bengio. Learning deep representations by mutual informationestimation and maximization. arXiv preprint arXiv:1808.06670, 2018. 5 Minhao Hu, Matthis Maillard, Ya Zhang, Tommaso Ciceri, Giammarco La Barbera, IsabelleBloch, and Pietro Gori. Knowledge distillation from multi-modal to mono-modal segmentationnetworks. In Medical Image Computing and Computer Assisted InterventionMICCAI 2020:23rd International Conference, Lima, Peru, October 48, 2020, Proceedings, Part I 23, pages772781. Springer, 2020. 3, 5, 6",
  "Mingcheng Li, Dingkang Yang, and Lihua Zhang. Towards robust multimodal sentimentanalysis under uncertain signal missing. IEEE Signal Processing Letters, 30:14971501, 2023.2": "Mingcheng Li, Dingkang Yang, Xiao Zhao, Shuaibing Wang, Yan Wang, Kun Yang, MingyangSun, Dongliang Kou, Ziyun Qian, and Lihua Zhang. Correlation-decoupled knowledge distil-lation for multimodal sentiment analysis with incomplete modalities. In Proceedings of theIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1245812468, 2024. 7, 8, 9 Yong Li, Yuanzhi Wang, and Zhen Cui. Decoupled multimodal distilling for emotion recognition.In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR), pages 66316640, 2023. 1, 2, 3, 7, 8, 9 Zheng Lian, Lan Chen, Licai Sun, Bin Liu, and Jianhua Tao. Gcnet: graph completion networkfor incomplete multimodal learning in conversation. IEEE Transactions on Pattern Analysisand Machine Intelligence, 2023. 2, 7, 8, 9 Zhizhong Liu, Bin Zhou, Dianhui Chu, Yuhang Sun, and Lingqiang Meng. Modality translation-based multimodal sentiment analysis under uncertain missing modalities. Information Fusion,101:101973, 2024. 2 Wei Luo, Mengying Xu, and Hanjiang Lai. Multimodal reconstruct and align net for missingmodality problem in sentiment analysis. In International Conference on Multimedia Modeling,pages 411422. Springer, 2023. 2 Mengmeng Ma, Jian Ren, Long Zhao, Sergey Tulyakov, Cathy Wu, and Xi Peng. Smil:Multimodal learning with severely missing modality. In Proceedings of the AAAI Conferenceon Artificial Intelligence (AAAI), volume 35, pages 23022310, 2021. 2, 7, 8, 9 Seyed Iman Mirzadeh, Mehrdad Farajtabar, Ang Li, Nir Levine, Akihiro Matsukawa, andHassan Ghasemzadeh. Improved knowledge distillation via teacher assistant. In Proceedings ofthe AAAI Conference on Artificial Intelligence (AAAI), volume 34, pages 51915198, 2020. 3 Louis-Philippe Morency, Rada Mihalcea, and Payal Doshi. Towards multimodal sentimentanalysis: Harvesting opinions from the web. In Proceedings of the 13th International Conferenceon Multimodal Interfaces, pages 169176, 2011. 1",
  "Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation inpytorch. 2017. 7": "Baoyun Peng, Xiao Jin, Jiaheng Liu, Dongsheng Li, Yichao Wu, Yu Liu, Shunfeng Zhou, andZhaoning Zhang. Correlation congruence for knowledge distillation. In Proceedings of theIEEE/CVF International Conference on Computer Vision (ICCV), pages 50075016, 2019. 3 Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors forword representation. In Proceedings of the 2014 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP), pages 15321543, 2014. 6 Hai Pham, Paul Pu Liang, Thomas Manzini, Louis-Philippe Morency, and Barnabs Pczos.Found in translation: Learning robust joint representations by cyclic translations betweenmodalities. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), volume 33,pages 68926899, 2019. 2, 7, 8, 9 Masoomeh Rahimpour, Jeroen Bertels, Ahmed Radwan, Henri Vandermeulen, Stefan Sunaert,Dirk Vandermeulen, Frederik Maes, Karolien Goffin, and Michel Koole. Cross-modal distillationto improve mri-based brain tumor segmentation with missing mri sequences. IEEE Transactionson Biomedical Engineering, 69(7):21532164, 2021. 3, 5, 6",
  "Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, andYoshua Bengio. Fitnets: Hints for thin deep nets. arXiv preprint arXiv:1412.6550, 2014. 3": "Roee Shraga, Haggai Roitman, Guy Feigenblat, and Mustafa Cannim. Web table retrieval usingmultimodal deep learning. In Proceedings of the 43rd International ACM SIGIR Conference onResearch and Development in Information Retrieval, pages 13991408, 2020. 1 Matthias Springstein, Eric Mller-Budack, and Ralph Ewerth. Quti! quantifying text-imageconsistency in multimodal documents. In Proceedings of the 44th International ACM SIGIRConference on Research and Development in Information Retrieval, pages 25752579, 2021. 1 Hao Sun, Hongyi Wang, Jiaqing Liu, Yen-Wei Chen, and Lanfen Lin. Cubemlp: An mlp-basedmodel for multimodal sentiment analysis and depression estimation. In Proceedings of the 30thACM International Conference on Multimedia (ACM MM), pages 37223729, 2022. 2, 7, 8, 9",
  "Frederick Tung and Greg Mori. Similarity-preserving knowledge distillation. In Proceedings ofthe IEEE/CVF International Conference on Computer Vision (ICCV), pages 13651374, 2019.3": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in Neural InformationProcessing Systems (NeurIPS), 30, 2017. 4 Hu Wang, Congbo Ma, Jianpeng Zhang, Yuan Zhang, Jodie Avery, Louise Hull, and GustavoCarneiro. Learnable cross-modal knowledge distillation for multi-modal learning with missingmodality. In International Conference on Medical Image Computing and Computer-AssistedIntervention, pages 216226. Springer, 2023. 3 Yansen Wang, Ying Shen, Zhun Liu, Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency.Words can shift: Dynamically adjusting word representations using nonverbal behaviors. InProceedings of the AAAI Conference on Artificial Intelligence (AAAI), volume 33, pages 72167223, 2019. 6 Yuanzhi Wang, Zhen Cui, and Yong Li. Distribution-consistent modal recovering for incompletemultimodal learning. In Proceedings of the IEEE/CVF International Conference on ComputerVision (ICCV), pages 2202522034, 2023. 2 Zilong Wang, Zhaohong Wan, and Xiaojun Wan. Transmodality: An end2end fusion methodwith transformer for multimodal sentiment analysis. In Proceedings of The Web Conference2020, pages 25142520, 2020. 2, 7, 8, 9",
  "Wenke Xia, Xingjian Li, Andong Deng, Haoyi Xiong, Dejing Dou, and Di Hu. Robust cross-modal knowledge distillation for unconstrained videos. arXiv preprint arXiv:2304.07775, 2023.3": "Chenglin Yang, Lingxi Xie, Chi Su, and Alan L Yuille. Snapshot distillation: Teacher-studentoptimization in one generation. In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition (CVPR), pages 28592868, 2019. 3 Dingkang Yang, Zhaoyu Chen, Yuzheng Wang, Shunli Wang, Mingcheng Li, Siao Liu, XiaoZhao, Shuai Huang, Zhiyan Dong, Peng Zhai, and Lihua Zhang. Context de-confoundedemotion recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition (CVPR), pages 1900519015, June 2023. 1 Dingkang Yang, Shuai Huang, Haopeng Kuang, Yangtao Du, and Lihua Zhang. Disentangledrepresentation learning for multimodal emotion recognition. In Proceedings of the 30th ACMInternational Conference on Multimedia (ACM MM), pages 16421651, 2022. 1, 3, 4",
  "Dingkang Yang, Shuai Huang, Yang Liu, and Lihua Zhang. Contextual and cross-modalinteraction for multi-modal speech emotion recognition. IEEE Signal Processing Letters,29:20932097, 2022. 1": "Dingkang Yang, Shuai Huang, Shunli Wang, Yang Liu, Peng Zhai, Liuzhen Su, Mingcheng Li,and Lihua Zhang. Emotion recognition for multiple context awareness. In Computer VisionECCV 2022: 17th European Conference, Tel Aviv, Israel, October 2327, 2022, Proceedings,Part XXXVII, pages 144162. Springer, 2022. 1 Dingkang Yang, Haopeng Kuang, Shuai Huang, and Lihua Zhang. Learning modality-specificand-agnostic representations for asynchronous multimodal language sequences. In Proceedingsof the 30th ACM International Conference on Multimedia (ACM MM), pages 17081717, 2022.1, 3",
  "Dingkang Yang, Haopeng Kuang, Kun Yang, Mingcheng Li, and Lihua Zhang. Towardsasynchronous multimodal signal interaction and fusion via tailored transformers. IEEE SignalProcessing Letters, 2024. 1": "Dingkang Yang, Mingcheng Li, Linhao Qu, Kun Yang, Peng Zhai, Song Wang, and LihuaZhang. Asynchronous multimodal video sequence fusion via learning modality-exclusiveand-agnostic representations. IEEE Transactions on Circuits and Systems for Video Technology,2024. 1 Dingkang Yang, Mingcheng Li, Dongling Xiao, Yang Liu, Kun Yang, Zhaoyu Chen, YuzhengWang, Peng Zhai, Ke Li, and Lihua Zhang. Towards multimodal sentiment analysis debiasingvia bias purification. In Proceedings of the European Conference on Computer Vision (ECCV),2024. 1 Dingkang Yang, Yang Liu, Can Huang, Mingcheng Li, Xiao Zhao, Yuzheng Wang, Kun Yang,Yan Wang, Peng Zhai, and Lihua Zhang. Target and source modality co-reinforcement foremotion understanding from asynchronous multimodal sequences. Knowledge-Based Systems,265:110370, 2023. 1 Dingkang Yang, Dongling Xiao, Ke Li, Yuzheng Wang, Zhaoyu Chen, Jinjie Wei, andLihua Zhang. Towards multimodal human intention understanding debiasing via subject-deconfounding. arXiv preprint arXiv:2403.05025, 2024. 1 Dingkang Yang, Kun Yang, Haopeng Kuang, Zhaoyu Chen, Yuzheng Wang, and Lihua Zhang.Towards context-aware emotion recognition debiasing from a causal demystification perspectivevia de-confounded training. IEEE Transactions on Pattern Analysis and Machine Intelligence,2024. 1 Dingkang Yang, Kun Yang, Mingcheng Li, Shunli Wang, Shuaibing Wang, and Lihua Zhang.Robust emotion recognition in context debiasing. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition (CVPR), pages 1244712457, 2024. 1 Junho Yim, Donggyu Joo, Jihoon Bae, and Junmo Kim. A gift from knowledge distillation:Fast optimization, network minimization and transfer learning. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition (CVPR), pages 41334141, 2017. 3 Wenmeng Yu, Hua Xu, Ziqi Yuan, and Jiele Wu. Learning modality-specific representationswith self-supervised multi-task learning for multimodal sentiment analysis. In Proceedings ofthe AAAI Conference on Artificial Intelligence (AAAI), volume 35, pages 1079010797, 2021.1, 7, 8, 9 Ziqi Yuan, Wei Li, Hua Xu, and Wenmeng Yu. Transformer-based feature reconstructionnetwork for robust multimodal sentiment analysis. In Proceedings of the 29th ACM InternationalConference on Multimedia (ACM MM), pages 44004407, 2021. 2",
  "Amir Zadeh, Minghai Chen, Soujanya Poria, Erik Cambria, and Louis-Philippe Morency. Tensorfusion network for multimodal sentiment analysis. arXiv preprint arXiv:1707.07250, 2017. 2": "Amir Zadeh, Paul Pu Liang, Navonil Mazumder, Soujanya Poria, Erik Cambria, and Louis-Philippe Morency. Memory fusion network for multi-view sequential learning. In Proceedingsof the AAAI Conference on Artificial Intelligence (AAAI), volume 32, 2018. 2 Amir Zadeh, Rowan Zellers, Eli Pincus, and Louis-Philippe Morency. Mosi: multimodalcorpus of sentiment intensity and subjectivity analysis in online opinion videos. arXiv preprintarXiv:1606.06259, 2016. 6, 7 AmirAli Bagher Zadeh, Paul Pu Liang, Soujanya Poria, Erik Cambria, and Louis-PhilippeMorency. Multimodal language analysis in the wild: Cmu-mosei dataset and interpretabledynamic fusion graph. In Proceedings of the 56th Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers), pages 22362246, 2018. 6, 7",
  "Sergey Zagoruyko and Nikos Komodakis. Paying more attention to attention: Improvingthe performance of convolutional neural networks via attention transfer.arXiv preprintarXiv:1612.03928, 2016. 3": "Jiandian Zeng, Tianyi Liu, and Jiantao Zhou. Tag-assisted multimodal sentiment analysis underuncertain missing modalities. In Proceedings of the 45th International ACM SIGIR Conferenceon Research and Development in Information Retrieval, pages 15451554, 2022. 2 Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid sceneparsing network. In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR), pages 28812890, 2017. 3 Jinming Zhao, Ruichen Li, and Qin Jin. Missing modality imagination network for emotionrecognition with uncertain missing modalities. In Proceedings of the 59th Annual Meeting ofthe Association for Computational Linguistics and the 11th International Joint Conference onNatural Language Processing (Volume 1: Long Papers), pages 26082618, 2021. 2",
  "The answer NA means that the paper has no limitation while the answer No means thatthe paper has limitations, but those are not discussed in the paper": "The authors are encouraged to create a separate \"Limitations\" section in their paper. The paper should point out any strong assumptions and how robust the results are toviolations of these assumptions (e.g., independence assumptions, noiseless settings,model well-specification, asymptotic approximations only holding locally). The authorsshould reflect on how these assumptions might be violated in practice and what theimplications would be. The authors should reflect on the scope of the claims made, e.g., if the approach wasonly tested on a few datasets or with a few runs. In general, empirical results oftendepend on implicit assumptions, which should be articulated. The authors should reflect on the factors that influence the performance of the approach.For example, a facial recognition algorithm may perform poorly when image resolutionis low or images are taken in low lighting. Or a speech-to-text system might not beused reliably to provide closed captions for online lectures because it fails to handletechnical jargon.",
  "If applicable, the authors should discuss possible limitations of their approach toaddress problems of privacy and fairness": "While the authors might fear that complete honesty about limitations might be used byreviewers as grounds for rejection, a worse outcome might be that reviewers discoverlimitations that arent acknowledged in the paper. The authors should use their bestjudgment and recognize that individual actions in favor of transparency play an impor-tant role in developing norms that preserve the integrity of the community. Reviewerswill be specifically instructed to not penalize honesty concerning limitations.",
  "If the contribution is a dataset and/or model, the authors should describe the steps takento make their results reproducible or verifiable": "Depending on the contribution, reproducibility can be accomplished in various ways.For example, if the contribution is a novel architecture, describing the architecture fullymight suffice, or if the contribution is a specific model and empirical evaluation, it maybe necessary to either make it possible for others to replicate the model with the samedataset, or provide access to the model. In general. releasing code and data is oftenone good way to accomplish this, but reproducibility can also be provided via detailedinstructions for how to replicate the results, access to a hosted model (e.g., in the caseof a large language model), releasing of a model checkpoint, or other means that areappropriate to the research performed. While NeurIPS does not require releasing code, the conference does require all submis-sions to provide some reasonable avenue for reproducibility, which may depend on thenature of the contribution. For example(a) If the contribution is primarily a new algorithm, the paper should make it clear howto reproduce that algorithm.",
  "(b) If the contribution is primarily a new model architecture, the paper should describethe architecture clearly and fully": "(c) If the contribution is a new model (e.g., a large language model), then there shouldeither be a way to access this model for reproducing the results or a way to reproducethe model (e.g., with an open-source dataset or instructions for how to constructthe dataset). (d) We recognize that reproducibility may be tricky in some cases, in which caseauthors are welcome to describe the particular way they provide for reproducibility.In the case of closed-source models, it may be that access to the model is limited insome way (e.g., to registered users), but it should be possible for other researchersto have some path to reproducing or verifying the results.",
  ". Experimental Setting/Details": "Question: Does the paper specify all the training and test details (e.g., data splits, hyper-parameters, how they were chosen, type of optimizer, etc.) necessary to understand theresults?Answer: [Yes]Justification: The 4.2 Implementation Details section of the paper specify all the trainingand testing details.Guidelines: The answer NA means that the paper does not include experiments. The experimental setting should be presented in the core of the paper to a level of detailthat is necessary to appreciate the results and make sense of them.",
  ". Experiment Statistical Significance": "Question: Does the paper report error bars suitably and correctly defined or other appropriateinformation about the statistical significance of the experiments?Answer: [Yes]Justification: In Tables 1 and 2 of the paper, we conducted significance tests on the experi-mental results to demonstrate the superior performance of the proposed framework.Guidelines: The answer NA means that the paper does not include experiments. The authors should answer \"Yes\" if the results are accompanied by error bars, confi-dence intervals, or statistical significance tests, at least for the experiments that supportthe main claims of the paper. The factors of variability that the error bars are capturing should be clearly stated (forexample, train/test split, initialization, random drawing of some parameter, or overallrun with given experimental conditions).",
  ". Code Of Ethics": "Question: Does the research conducted in the paper conform, in every respect, with theNeurIPS Code of Ethics [Yes]Justification: Our research conducted in the paper conform, in every respect, with theNeurIPS Code of Ethics.Guidelines: The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. If the authors answer No, they should explain the special circumstances that require adeviation from the Code of Ethics.",
  ". Broader Impacts": "Question: Does the paper discuss both potential positive societal impacts and negativesocietal impacts of the work performed?Answer: [Yes]Justification: Please refer to the 5 Conclusion and Discussion sections for the broaderimpacts of our workGuidelines: The answer NA means that there is no societal impact of the work performed. If the authors answer NA or No, they should explain why their work has no societalimpact or why the paper does not address societal impact. Examples of negative societal impacts include potential malicious or unintended uses(e.g., disinformation, generating fake profiles, surveillance), fairness considerations(e.g., deployment of technologies that could make decisions that unfairly impact specificgroups), privacy considerations, and security considerations. The conference expects that many papers will be foundational research and not tiedto particular applications, let alone deployments. However, if there is a direct path toany negative applications, the authors should point it out. For example, it is legitimateto point out that an improvement in the quality of generative models could be used togenerate deepfakes for disinformation. On the other hand, it is not needed to point outthat a generic algorithm for optimizing neural networks could enable people to trainmodels that generate Deepfakes faster. The authors should consider possible harms that could arise when the technology isbeing used as intended and functioning correctly, harms that could arise when thetechnology is being used as intended but gives incorrect results, and harms followingfrom (intentional or unintentional) misuse of the technology. If there are negative societal impacts, the authors could also discuss possible mitigationstrategies (e.g., gated release of models, providing defenses in addition to attacks,mechanisms for monitoring misuse, mechanisms to monitor how a system learns fromfeedback over time, improving the efficiency and accessibility of ML).",
  "Guidelines:": "The answer NA means that the paper does not use existing assets. The authors should cite the original paper that produced the code package or dataset. The authors should state which version of the asset is used and, if possible, include aURL. The name of the license (e.g., CC-BY 4.0) should be included for each asset. For scraped data from a particular source (e.g., website), the copyright and terms ofservice of that source should be provided. If assets are released, the license, copyright information, and terms of use in thepackage should be provided. For popular datasets, paperswithcode.com/datasetshas curated licenses for some datasets. Their licensing guide can help determine thelicense of a dataset.",
  "The answer NA means that the paper does not involve crowdsourcing nor research withhuman subjects": "Depending on the country in which research is conducted, IRB approval (or equivalent)may be required for any human subjects research. If you obtained IRB approval, youshould clearly state this in the paper. We recognize that the procedures for this may vary significantly between institutionsand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and theguidelines for their institution."
}