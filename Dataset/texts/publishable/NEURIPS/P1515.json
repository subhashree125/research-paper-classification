{
  "Abstract": "Residual networks, as discrete approximations of Ordinary Differential Equations(ODEs), have inspired significant advancements in neural network design, includingmultistep methods, high-order methods, and multi-particle dynamical systems. Theprecision of the solution to ODEs significantly affects parameter optimization,thereby impacting model performance. In this work, we present a series of advancedexplorations of Transformer architecture design to minimize the error compared tothe true solution. First, we introduce a predictor-corrector learning framework tominimize truncation errors, which consists of a high-order predictor and a multistepcorrector. Second, we propose an exponential moving average-based coefficientlearning method to strengthen our higher-order predictor. Extensive experimentson large-scale machine translation, abstractive summarization, language modeling,and natural language understanding benchmarks demonstrate the superiority ofour approach. On the WMT14 English-German and English-French tasks, ourmodel achieved BLEU scores of 30.95 and 44.27, respectively. Furthermore, onthe OPUS multilingual machine translation task, our model surpasses a robust 3.8BDeepNet by an average of 2.9 SacreBLEU, using only 1/3 parameters. Notably, italso beats LLama models by 5.7 accuracy points on the LM Harness Evaluation.",
  "Introduction": "Residual networks , formally yt+1 = yt + F(yt, t), represent a cornerstone in the developmentof deep neural networks , primarily due to their capacity to facilitate the flow of informationacross multiple layers. Beyond their pivotal role in convolutional networks, residual connections havebecome an essential element in the architecture of more complex models, including the Transformer and its various derivatives. This concept can be likened to the discretization process in the Eulermethod , which serves as a first-order solver for ordinary differential equations(ODEs), where dy(t) dt= F(y(t), (t)). In both cases, the new state (be it the next layers output inResNets or the solution at the next time step in the Euler method) is computed by taking the currentstate and adding an adjustment term. Given this analogy with ODEs, there has been a surge of interest in improving residual networkarchitectures by using more powerful numerical methods for ODEs. For instance, the linear multistepmethod has been employed to bolster the optimization of deep models. Other effortshave included redesigning the Transformer architecture from a multi-particle dynamical system",
  "arXiv:2411.03042v1 [cs.CL] 5 Nov 2024": "perspective and improving parameter learning efficiency through high-order methods .Additionally, ODEs have been extensively studied for their potential to accelerate diffusion processes,with multistep and high-order solvers offering more accurate predicted noise among each denoisingprocess, generating comparable images but consuming much fewer NFEs . In this work, we mainly focus on advancing the architecture design, specifically by minimizing thetruncation error across each timestep. Building upon the ODE Transformers , which replacethe first-order Euler method with a high-order method for more precise numerical solutions, ourfocus extends to addressing two key limitations. First, high-order solutions, such as those from theRunge-Kutta method or multistep methods, are found not to lead to significant improvements whenwe scale up training data and/or model size. Second, the gated fusion coefficient learning method,which is widely used in previous work, is not well suited for higher-order solutions. Our work draws inspiration from the predictor-corrector method , a well-established approach innumerical analysis known for its accuracy in solving differential equations. This method involves atwo-step process: a prediction step that estimates the solution based on known conditions, followedby a correction step that refines the prediction for a more accurate result. We introduce a novelfamily of PCformers that embrace this predictor-corrector paradigm. Our approach integrates thefinal solution using an exponential moving average (EMA) method, capitalizing on the insight thathigher-order intermediate approximations tend to be more accurate. This assertion is supported bythe truncation error analysis presented in .1.2. Our method is not only readily extensible toarbitrary higher orders but also consistently outperforms the gated fusion method.",
  "Our contributions are summarized below:": "We extend explicit ODE solutions to implicit ODE solutions via a predictor-corrector learningparadigm. This kind of iterative refinement can attain more accurate solutions than previous studiesboth theoretically and empirically. In particular, we choose the high-order method as the predictorand the multistep method as the corrector. To further strengthen the learning ability and training stability for high-order methods, we proposean exponential moving average coefficient learning method to replace the constant coefficients.This leads to a much stronger predictor. Our extensive experimental evaluation on several benchmarks, including WMT14 English-German,WMT14 English-French, WMT16 Romanian-English, and the OPUS multilingual machinetranslation benchmark, demonstrates the superior effectiveness of our PCformer models. Notably,our model surpasses the 3.8B DeepNet by an average BLEU score of 2.9 with only 1/3 of theparameters. Furthermore, our model can be extended to other domains. Results on abstractivesummarization, language modeling, and language understanding tasks demonstrate its generality.",
  "Background": "We build our method upon Transformer as it is one of the most popular models in NLP. Theencoder is a stack of identical layers. Each layer consists of a self-attention block and a feedforwardnetwork (FFN) block. Both of them are equipped with a residual connection and a layernormalization unit . The output of a block can be defined as",
  "yt+1 = yt + F(yt, t)(1)": "where F() is either the self-attention or FFN block. This equation illustrates that the layer outputyt+1 is determined by the layer input yt and a learnable derivative estimated by the current functionF. This approach aligns with the benefits of the Euler method, which we will discuss in the followingsections. In this work, we use Ft to denote the t-th layer representation and Fi to denote the i-thorder intermediate approximations. The Euler MethodThe Euler method is the most basic solution to solve ODEs given the initialvalue, involving a function y(t) of a variable t and its derivatives. The Euler method defines thefirst-order derivative of y(t)dy(t)",
  "(1 )3": ": Illustration of several advanced numerical methods and our proposed predictor-correctorparadigm. The right part plots a 4-order method as the predictor to obtain Pt+1; Ft+1 is then estimatedvia a function F(); A 4-step method as the corrector to obtain the yt+1. and a time variable t. In deep learning, we can use a trainable function F() to estimate f(y(t), t).In a nutshell, residual networks could be regarded as a 1st-order discretization of the Euler method. The advantage is obvious since residual networks deliver consistent performance gains inthe artificial intelligence, but the precision of yt+1 is limited. Fortunately, we can move forward alongthe numerical analysis perspective as more advanced numerical methods can alleviate this issue. The Linear Multistep MethodCompared with the Euler method, the linear multistep methoduses previously obtained solutions to estimate the current one, leading to more accurate results.Formally, a multistep method could be defined as: yt+1 = yt + ti=1 iFi, where Ft = F(yt, t). The High-Order ODE MethodAnother family of numerical methods is high-order ODE solversby repeatedly refining the solutions within a single step. Previous work employed the Runge-Kutta methods for a higher-order solution to ODEs, where Runge-Kutta is a classicfamily of iterative methods with different orders of precision. More formally, the explicit Runge-Kuttamethods of an n-order solution is defined to be: yt+1 = yt + ni=1 i Fi, where F1 = F(yt, t),Fi = F(yt + i1j=1 ij Fj, t). Note that Fi is the intermediate approximation to the solution at aninner step. and are coefficients to model the scale of the input and the output of Fi. This kind ofmethod can be adapted to Transformer blocks by reusing F() within a block.",
  "Predictor-Corrector Transformer": "In this section, we first show the core design of Predictor-Corrector paradigm to more accuratelysolve ODEs. Then we propose an alternative coefficient learning strategy that could be applied toarbitrary orders using the merit of the exponential moving average. At last, we show some additionaltraining techniques for stable and well-performance training.",
  "Predictor-Corrector Method": "A genuine problem-solving process involves the repeated use of available information toinitiate exploration, which discloses, in turn, more information until a way to attain thesolution is finally discovered. Newell et al. The Predictor-Corrector framework leverages an iterative process of using available informationto refine approximations continuously. Initially, the Predictor generates a rough estimate, which issubsequently refined by the Corrector using newly available data. This cyclical process mirrors the",
  "(9Ft+1 + 19Ft 5Ft1 + Ft2).(4)": "Formally, both Eq. 3 and Eq. 4 reused Ft, Ft1, Ft2 to improve the accuracy, but the correctornecessitates an approximate current solution Ft+1 to substitute Ft3, which is an implicit method.This is because that yt+1 is the value to be solved, thus we cannot compute Ft+1. To solve this, theAdams-Bashforth-Moulton methods utilize Eq. 3 to obtain the approximate value (Pt+1) for yt+1.Then Ft+1 could be approximated following Ft+1 = F(Pt+1, t). Concretely, the predictor providesa rough approximation, which is the combination of the preceding four layer representations. Andthe corrector then improves the approximation, offering a more precise sample derived from the data. However, applying the Adams-Bashforth-Moulton method directly to Transformer architecture designleads to unstable training and limited benefits due to the difficulty in optimizing constant coefficients.Similar issues have been observed in training a Runge-Kutta (RK4) network with numericallysuggested coefficients . To address these challenges, Wang et al. proposed a Dynamic LinearCombination of Layers (DLCL) method. This approach utilizes learnable coefficients and adjustssteps based on layer depth, effectively transforming it into a variable multistep method. Additionally,the Adams-Bashforth method is not the only choice for the predictor; other numerical methods, suchas high-order methods, are also considered strong alternatives as they often provide more accuratesolutions .",
  "F1, t), F3 = F(yt + 1": "2 F2, t), and F4 = F(yt + F3, t).To break the limit of constant coefficients, Li et al. employed a gated network to dynamicallycompute the coefficients of F1 and F2, however, this method cannot applied to higher-order methods,e.g., RK4. To facilitate higher-order optimization, we design a more flexible coefficient learningmethod via an exponential moving average strategy. Predictor with Exponential Moving Average Coefficient LearningThe Exponential MovingAverage (EMA) method is widely used for estimating time-series data by assigning variableweights to past observations, giving more importance to recent data compared to simple weightedaveraging methods. We hypothesize that high-order approximations at each step should have a larger impact on the final output, as they provide a more accurate initial state than previous ones. To supportthis claim, we replaced Eq. 6 by yt+1 = yt + Fi, where i [1, ..., 4]. We used a single-layer decoderto compute the perplexity (PPL) on the validation set to simulate truncation errors. Our expectation isthat the fewer truncation errors, the larger the coefficient it should own.",
  ": Truncation errors with differentintermediate approximations": "displays the perplexity comparisons. It showsthat a 4-th order approximation, used as a replacementfor the linear aggregation in Eq. 6, delivers comparableresults and outperforms other cases. This observationmotivates us to combine the benefits of EMA with thecoefficient learning method. EMA is more flexible to theorder of ODE solvers, that is could be easily extended to2 orders, 4 orders, or even larger. (b) illustratesthe design merit of our proposed PCformer with an EMAcoefficient predictor and a parameterization multistepcorrector. Here, we use the RK4-block as an example. Itis apparent that the original scales have been replaced by, (1 ), (1 )2, and (1 )3 from F4 to F1, where is learnable and the initializationis 0.5 empirically. In this way, our n-order predictor approximates Pt+1 as follows:",
  "i=1 (1 )ni Fi.(7)": "Corrector with ParameterizationLeveraging a robust predictor, our corrector is designed tobe computationally lightweight, striking an optimal balance between performance and efficiency.Utilizing the Adams-Moulton method, we parameterize the coefficients of previous states withlearnable parameters. These coefficients are initialized using an EMA value, where the newlyestimated Ft+1 is assigned a larger weight ( = 0.5), and the weights of previous states decrease in adescending order. In this way, we rewrite the Eq. 4 by",
  ": end procedure": "Step Normalization (RK-Norm)Webuilt our PCformer following pre-normarchitecture , by rewritten Eq. 1to yt+1 = yt + F(LN(yt), t), whereLN() denotes the normalization. Thisensures that the representation is nor-malized before computing the derivativeFi. To achieve this, we normalize theobtained intermediate approximationsFi at each inner step and then computethe offset, e.g., yt + 1 2LN( F1) to obtainthe F2 for the next timestep. Meanwhile,the Fi in Eq. 7 is rewritten by LN( Fi).If not, this oversight can cause insta-bility when computing the final ODEsolution, where we will make ablationsin the analysis section. The algorithm(right part) presents a more detailed computation flow of a single layer in our PCformer, where Hstores the previously obtained Ft+1.",
  "Transformer 6-6213M 100K 28.40-222M 300K 41.00-MacaronNet 6-6-- 30.20-----Transformer-DLCL 30-6137M50K 29.3028.6----": "Transformer-Base6-661M50K 27.8926.869M 100K 41.0539.1RK2-block (Gated) 6-661M50K 28.8927.769M 100K 42.3140.3RK2-block (EMA)6-661M50K 29.1128.169M 100K 42.4440.4RK4-block 6-661M50K 29.0327.969M 100K 42.5640.6RK4-block (EMA)6-661M50K 29.4328.469M 100K 42.7240.7 Transformer-Big6-6211M 100K 29.2128.1221M 100K 42.8940.9RK2-block (Gated) 6-6211M 100K 30.5329.4221M 100K 43.5941.6RK4-block 6-6211M 100K 30.3929.3221M 100K 43.5141.6PCformer (2-order)6-6211M 100K 30.9029.8221M 100K 43.8541.8Transformer-Big12-6286M 100K 29.9128.9297M 100K 43.2241.2RK2-block (Gated) 12-6286M 100K 30.7729.6297M 100K 43.9642.1RK4-block 12-6286M 100K 30.5529.4297M 100K 43.8141.8RK4-block (EMA)12-6286M 100K 30.6629.5297M 100K 44.1742.2PCformer (2-order)12-6286M 100K 30.9529.8297M 100K 44.2742.4",
  "B37.732.235.0": "Sublayer DroppingAdditionally, we observe that our models benefit from the rich informationbrought by high-order predictor and subsequent implicit multistep corrector. To prevent fromoverfitting (settling into sub-optimal solutions) as the learning ability is quite strong, we borrowed thesublayer dropping technique . The drop rate is empirically set as 0.1 which delivers robustresults in previous studies.",
  "Experimental Results": "We mainly evaluated the proposed method on machine translation, abstractive summarization, lan-guage modeling, and language understanding benchmarks. The details of datasets, and correspondinghyper-parameters please refer to Appendix C. For a clear comprehension, note that RK2-block (gated)is 2-order method with learnable coefficients in s work. And RK2-block (EMA) denotes ourEMA strategy. Results of En-De and En-Fr compares the proposed PCformer with state-of-the-art systemsin base and large configurations. As ODE Transformer is a strong baseline to ours, we implementedtheir results for a fair comparison. We can see that the proposed EMA coefficient learning methodcan further strengthen high-order methods, leading to better results than the gated fusion methodin Li et al. s work (comparisons in RK2-block). And EMA can facilitate RK4-block to delivera further gain of 0.40 BLEU points. The performance gains are more obvious for wider models,that PCformer sets or matches the new state-of-the-art with fewer parameters. Notably, a 6-layerPCformer (2-order) achieves a BLEU score of 30.90, surpassing the previous best of 30.77 by a12-layer RK2-block with gated fusion . For En-Fr, PCformer outperforms the standard Big model",
  "by 1.00 and 1.05 BLEU points with 2-order and 4-order configurations. This demonstrates that thepredictor-corrector paradigm is a more parameter-efficient option than pure high-order methods": "Results of En-Ro exhibits a similar phenomenon on the En-Ro task. Our predictor-correctorparadigm with EMA method achieves much better performance (35.49 v.s. 34.70) with DeLightwithin much less training cost. For a bigger model (line 7), it obtains a BLEU score of 35.80. Amuch higher performance (36.00) could be achieved by a carefully designed corrector which wouldbe discussed in the subsequent analyses. Results of OPUS provides the comparison of PCformer against existing state-of-the-art models on the OPUS-100 testset. The findings here are three aspects: 1) Across allconfigurations, PCformer delivers significant BLEU gains over vanilla baselines. 2) Our 12-layerEMA Pre-Cor model attains an average SacreBLEU score of 32.6, which not only outperformsthe 3.8B DeepNet but also beats its further optimized variant, BranchNorm, with only 1/8 modelparameters. 3) PCformer can benefit from the enlarging width and depth. Notably, our 1.2B modelshows an average SacreBLEU of 35.0, thereby setting a new state-of-the-art on the OPUS-100 testset. Abstractive Summarization presents the results of the abstractive summarization task. Asshown, our PCformer consistently improves upon pure the high-order method , in terms of threerouge scores. Notably, PCformer (2-order) even beats RK4-block which consumes less computationcost. Additionally, PCformer (4-order) sets a new state-of-the-art on the summarization task whichexcludes models based on pre-trained models. This result strongly supports our hypothesis that anappropriate coefficient learning schedule is essential for the effectiveness of higher-order methods,thereby enhancing the performance of our PCformer model. Language Modeling presents a comparative analysis of our PCformer against vanilla Trans-formers in Adaptive Input Representation and Shortformer settings. Our 2nd-order configu-ration achieves significant reductions in perplexity (PPL), outperforming Adaptive and Shortformerby 1.79 and 1.23 PPL, respectively, even within identical model capacity constraints. Remarkably,PCformer surpasses the high-order method (RK2-block) by a substantial margin in both settings onboth validation and test sets, demonstrating the superiority of PCformer.",
  "BERT60.691.386.6/-93.290.092.370.488.084.0PCformer65.992.087.3/-93.690.892.874.791.586.1": "LM Evaluation HarnessIn response to the increasing significance of attention mechanisms inLLMs, we conducted a comprehensive evaluation of PCformer using established benchmarks, LMEvaluation Harness , focusing on a diverse range of downstream tasks including common-sensereasoning and question-answering. presents our findings, where we utilized a llama-likemodel3, Transformer++, as the foundation for PCformer. We trained models with parameter sizesranging from 340M to 3B, using datasets comprising 6B to 100B tokens from Slimpajama. Theexperimental results indicate that PCformer consistently surpasses the performance of a well-tunedTransformer of equivalent capacity. Notably, PCformer achieves an average score improvement of 1.7points for the 340M model and 5.7 points for the 1B model across six challenging subtasks. Whenscaled to a 3B parameter size, PCformer demonstrates even greater gains, achieving an additional 3.5average score improvement compared to the 1B model, underscoring its scalability and potential withlarger model capacities and richer training datasets. Language UnderstandingWe also validate our method on the widely used natural languageunderstanding benchmarks, namely GLUE, which consists of 8 sub downstream tasks. The evaluationmetrics are as follows: The result for STS-B is the Pearson correlation; Matthews correlation isused for CoLA; Other tasks are measured by Accuracy. The results are presented in . Wecan see that PCformer achieves 2.1 points (on average) improvement over the BERT-large, whichdemonstrates the effectiveness of PCformer.",
  "Model1-Layer 2-Layer": "Residual-Block142.33136.07RK2-block131.80123.12RK2-block (gated) 128.48121.02RK2-block (EMA)124.01119.65PCformer (2-order)120.91118.37RK4-block126.89119.46RK4-block (EMA)121.82116.77PCformer (4-order)119.27114.32 Quantization of the Truncation ErrorFollowing thesuggestion in s work, we use the perplexity betweenthe single-layer Transformer decoder output and theground truth to approximate the truncation error. Theresults of were conducted on the Penn Treebankdataset. We see that the proposed EMA method achievesa lower perplexity than the learnable coefficient (gated)learning method, similar observation in 4-order (EMAv.s. Rk4-block). Additionally, the Predictor-Correctorparadigm can further reduce the truncation error, whichdemonstrates the effectiveness of our method.",
  "BLEU COMET BLEU COMET": "Transformer-big (6L)29.2151.8742.8971.21PCformer (RK2)30.9054.7443.8573.96PCformer (RK4)--44.1074.76Transformer-big (12L) 29.9152.9043.2272.33PCformer (RK2)30.9555.3844.2775.09PCformer (RK4)--44.2175.33 Evaluation by COMETOur PCformer con-sistently outperforms baselines, showing aneven larger gap in COMET than BLEU, asshown in . Both metrics exhibit sim-ilar performance trends, highlighting our ap-proachs effectiveness. Increasing model depthfrom 6 to 12 layers does not improve BLEU forthe En-De task but results in a 0.64 COMETgain. A similar pattern is observed in the En-Fr task, where PCformer (4-order) achievescomparable BLEU to its 2-order counterpart but gains 0.24 in COMET.",
  "(b) Other techniques: Figuring out severalkey components for high-order solutions": "Ablation Study on Predictor-Corrector FrameworkThe predictor-corrector framework is cru-cial in our work, with the choice of ODE solutions for each component significantly impactingperformance. We experimented with various combinations of predictors and correctors, includinghigh-order methods, linear multistep methods, and the Backward Euler method. summarizesthese results. We chose RK2-block with EMA as the default for high-order solutions due to itsperformance and efficiency. Key insights include: 1) The predictor must be highly accurate, as it setsthe performance lower bound. High-order predictors outperform the multistep method (DLCL) andthe Euler method. 2) A complex corrector isnt always optimal; a Backward Euler method sufficesfor small and medium datasets (e.g., En-Ro and En-De), while more complex methods may causeoverfitting. 3) Combining a multi-step method predictor with a high-order corrector performed worsethan other combinations, highlighting the importance of predictor choice. Ablation Study on Core Design Technique presents the performance of our PCformerwith various initial coefficients for the EMA method and stable training techniques. Our default, with set to 0.5, performs best because smaller values disrupt the numerical bound, and larger valuesoverly focus on recent approximations. As detailed in .2, RK-Norm is essential for trainingstability, as shown by the BLEU score drop without it. Testing different layer-wise coefficients andreplacing the learnable scalar with a learnable matrix vector showed no significant performancedifference, so these were excluded from our default settings.",
  "Transformer698.713.229.2Transformer1294.518.729.7Transformer2487.323.529.8ODE Transformer (RK2)693.515.130.7PCformer (RK2 predcitor)690.316.230.9ODE Transformer (RK4)687.117.330.5": "Inference Speed and Memory Con-sumption presents a detailedcomparison of inference speed and mem-ory consumption across various largemodel configurations, revealing that theproposed PCformer models achieve sat-isfactory inference performance. Thisis primarily because the computationaloverhead is concentrated on the decoderside rather than the encoder, as demon-strated in our experiments. Additionally,PCformer is memory-efficient, as shown by the memory usage comparison between the baseline andthe ODE Transformer. Despite the fact that our PCformer models are more than twice as slow as thevanilla baseline in encoder-only and decoder-only configurations, they deliver significantly superiorperformance, making the trade-off worthwhile. These performance gains are clearly illustrated in Ta-ble 6, where the substantial improvement in model effectiveness justifies the increased inference time.We acknowledge that further optimization to accelerate PCformers inference speed is a promisingdirection for future research. More AnalysesDue to the limited space in the main content, we summarized more detailedanalyses in Appendix D, including the parameter efficiency (), illustration of training andvalidation curves (), and visualization of coefficients during the learning procedure (). We anticipate that these analyses will offer a deeper and more comprehensive understanding ofour method.",
  "Related Work": "Ordinary Differential EquationsThe connection between ResNet and ODEs was first proposedby Weinan , while Neural ODENet introduced a new perspective on neural architecture design.Several architectures can be interpreted from the ODE perspective. Recentstudies leverage ODE benefits for Transformers. Lu et al. proposed MacaronNet using theStrang-Marchuk Splitting Scheme, and Zhang et al. introduced continuous self-attention models.Dutta et al. redesigned Transformer architecture for efficiency from a multi-particle dynamicsystem view. Li et al. showed that first-order ODE blocks could cause error accumulation,and high-order methods were suggested as solutions. In this work, we advance the Transformerdesign with a more accurate Predictor-Corrector paradigm and a general coefficient learning strategyinspired by the exponential moving average, showing significant performance improvements on NLPbenchmarks. ODE and Diffusion ModelsODEs and numerical methods are also popular in diffusion models,reducing prediction errors in denoising processes. Text-to-image generation typically uses a two-stagemodel, including a text-to-image diffusion model and super-resolution models. The standard diffusionmodel, DDPM , requires up to 1000 iterations to recover images from Gaussian noise. Subsequentwork accelerated DDPMs using denoising equations or scheduled variance , though often atthe cost of performance. Liu et al. proposed treating DDPMs as solving differential equations onmanifolds, introducing a pseudo linear multi-step method for efficiency and performance. Furtherdiffusion acceleration efforts were motivated by ODE benefits. More recently, Xu et al. presented a deep generative model solving the Poisson equation, opening new possibilities intext-to-image generation. ODEs also show promise in discrete diffusion models, as demonstrated byLezama et al. , who used a predictor-corrector paradigm to enhance the accuracy.",
  "Conclusions": "This paper advances the design of parameter-efficient neural network backbones through a numericalanalysis perspective. Previous work has utilized high-order ODE solutions for more accurate approxi-mations at each block, yielding promising results on various sequence generation tasks. However,challenges remain, such as the scalability of learnable coefficients to RK4-blocks and the lack ofexploration into implicit ODE methods. To address these issues, we introduce a predictor-correctorframework to improve estimation precision. Additionally, we proposed an EMA coefficients learningstrategy to promote coefficients learning for high-order methods with high flexibility. Experimentalresults across 8 benchmarks demonstrate the general ability and strong effectiveness of our approach.More concretely, Our PCformer achieves 30.95 and 44.27 BLEU scores on the WMT14 En-De andEn-Fr, setting a new state-of-the-art result on both testsets without considering data augmentationmethods. Also, it delivers an average BLEU of 35.0 on the OPUS multilingual dataset, beating Deep-Net and other variants with much fewer model parameters. Notably, PCformer demonstrates strongpotential in large language model scenarios, outperforming vanilla LLama models by a considerablemargin across various configurations on LM Harness Evaluation. Our codebase could be found at This work was supported in part by the National Science Foundation of China (No.62276056), theNatural Science Foundation of Liaoning Province of China (2022-KF-26-01), the Fundamental Re-search Funds for the Central Universities (Nos. N2216016 and N2316002), the Yunnan FundamentalResearch Projects (No. 202401BC070021), and the Program of Introducing Talents of Discipline toUniversities, Plan 111 (No.B16009). Jingang Wang is funded by Beijing Nova Program (Grant NO.20220484098).",
  "Subhabrata Dutta, Tanya Gautam, Soumen Chakrabarti, and Tanmoy Chakraborty. Redesigning thetransformer architecture with insights from multi-particle dynamical systems. pages 55315544, 2021": "Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster,Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, ChrisOciepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang,Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation,07 2024.",
  "Myle Ott, Sergey Edunov, David Grangier, and Michael Auli. Scaling neural machine translation. InProceedings of the Third Conference on Machine Translation: Research Papers, pages 19, 2018": "Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, andMichael Auli. fairseq: A fast, extensible toolkit for sequence modeling. In Proc. of NAACL, pages 4853,2019. Denis Paperno, Germn Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, SandroPezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernndez. The lambada dataset: Word predictionrequiring a broad discourse context. arXiv preprint arXiv:1606.06031, 2016.",
  "ABroader Impact": "We do not anticipate any specific negative impacts from our work. However, as with any machine learningmethod, we recommend exercising caution. Our primary contribution is advancing neural model design from anumerical perspective, aiming to facilitate models parameter learning and enhance the performance, which webelieve to be environmentally friendly. This approach encourages the community to enhance neural models byintegrating knowledge from various domains, such as physiology, physics, and mathematics, thereby furtherbenefiting AI research.",
  "BLimitations and Future Work": "The proposed PCformer has demonstrated significant performance gains across a variety of tasks, includingsequence generation, natural language understanding, and language modeling. While the encoder-decoderPCformer is computationally efficient, as it only employs the predictor-corrector paradigm on the encoder side,the primary computational overhead during inference remains in the decoder. However, when our method isapplied to encoder-only models (for NLU) or decoder-only models (for LM/LLM), the additional computationalcomplexity becomes non-negligible. Addressing how to further accelerate inference in these scenarios remainscritical, and we aim to explore this in future work.",
  "We plan to address this issue from two primary aspects:": "High-order Computation in Latent Space: When adopting high-order methods, the iterative computationsamong inner steps cannot be skipped. Therefore, we aim to investigate whether high-order computations canbe performed in a latent space with a reduced dimensionality, such as 64 or 128 hidden dimensions, comparedto the original 1024 dimensions. This approach presents a significant theoretical challenge: maintaining thestability of the ODE while computing higher-order intermediate approximations in the latent space. High-order Training and Inference in a First-order Manner: Another alternative is to achieve high-ordertraining and inference using a first-order (Euler-like) approach. This might involve employing distillationtechniques or treating high-order computations as a form of training regularization. We will continue to advance the design of PCformers to balance performance gains with inference efficiency,particularly in the context of large language models. This research could provide valuable insights to thecommunity on whether such modeling methods can further enhance the performance of LLMs.",
  "C.1Machine Translation": "DatasetsWe present experimental results across three WMT benchmarks, including WMT14 English-German (En-De), WMT14 English-French (En-Fr), and WMT16 English-Romanian (En-Ro), as well as on alarge-scale, challenging multilingual machine translation benchmark (OPUS). For the En-De task, the training data consisted of approximately 4.5M tokenized sentence pairs, as in . Allsentences were segmented into sequences of sub-word units with 32K merge operations using a sharedvocabulary. We selected newstest2013 as the validation data and newstest2014 as the test data.",
  "For the En-Ro task, we replicated the setup of , which used 600K/2K/2K sentence pairs for training,evaluation and inference, respectively": "Apart from the aforementioned three bilingual translation benchmarks, OPUS is a multilingual benchmark thatcontains more challenges for the model to serve. We choose OPUS , an English-centric corpus covering100 languages, which is randomly sampled from the OPUS collection. For a fair comparison with prior work, : Statistics of the datasets and hyperparameters for sequence generation tasks. For the dataset,we both report the vocabulary size, sentence numbers of training, validation and test sets. For thetraining, Lr denotes the peaking learning rate and Warmup denotes the warmup step of the Adamoptimizer. WD denotes whether we applied word dropout. For the inference, Beam and LP denotethe beam size and length penalty, respectively.",
  "TrainDevTestLr Warmup Batch Steps Beam LP": "WMT14 En-De340404.5M30003003 0.0021600080K50K4 0.6WMT14 En-Fr3728435.7M26,8223003 0.00216000320K100K4 0.6WMT16 En-Ro34976602K19991999 0.002800080K17K5 1.3OPUS209816 109.3M 371068 376000 0.00216000320K100K5 1.0CNN/DailyMail32584287K1336811490 0.0028000160K50K4 2.0PTB1000890873- 0.0022000160K3K4 2.0Wikitext-10332584180K376011490 0.0028000160K50K4 2.0 we used the already sampled subset4 and following the script provided by Zhang et al. to pre-processthe data, including the data filtering, sentence piece training and applying. After that, OPUS-100 containsapproximately 55M sentence pairs. Following the same training strategy with , we train a single modelfor both to English (XE) and from English (EX), thus the total training data is 110M. Note that 2000 sentencepairs for each language to serve as validation and test sets. EvalutationWe measured performance in terms of BLEU. Both tokenized BLEU and SacreBLEU5 scoreswere reported on the En-De and En-Fr tasks. For the OPUS task, we average the SacreBLEU scores among94 languages for both XEn and EnX. The beam size and length penalty of each task are summarized in. In order to attain results that are more compelling, it is imperative to acknowledge that BLEU may notbe a suitable metric for evaluating a models performance. To supplement our evaluation, we have included asummary of the COMET scores of the top-performing model, utilizing a reference-based model6. Training DetailsIn accordance with the recommendations provided by , we have incorporated relativepositional representation , namely RPR for short, into our model architecture to establish stronger baselines.To ensure stable learning during training, we have also borrowed the merit of dense connections among layers for stable optimization within FP16 training. Our models were trained on 8 GPUs with 4, 096 tokens perGPU. For the En-De, En-Fr and OPUS tasks, we have observed that larger batching schemas often result inbetter convergence . Therefore, we accumulated gradients every 2 and 8 steps for En-De and En-Fr/OPUS,respectively. Adam optimizer with (0.9, 0.997) for 1 and 2 is adopted. The hyperparameters includingthe learning rate, the warmup step and the total training steps of three tasks could be found in . Notethat we trained Base/Deep and Big models for 50K and 100K steps on the En-De task. We regarded mergingSAN and FFN as the default ODE block. In addition, main results were the average of three times running withdifferent random seeds (1, 42 and 2024), and we averaged the last several checkpoints towards the robustness.The detail of Base/Deep/Wide configurations is as follows: Base/Deep Model. The hidden size of self-attention was 512, and the dimension of the inner-layerin FFN was 2, 048. We used 8 heads for attention. For training, we set all dropout to 0.1 as default,including residual dropout, attention dropout, ReLU dropout. Label smoothing ls = 0.1 was appliedto enhance the generation ability of the model. For deep models, we only enlarged the encoder depthconsidering the inference speed. Wide (or Big) Model. We used the same architecture as Transformer-Base but with a larger hiddenlayer size 1, 024, more attention heads (16), and a larger feed forward inner-layer (4, 096 dimensions).The residual dropout was set to 0.3 for the En-De task and 0.1 for the En-Fr task.",
  "C.3Language Modeling": "DatasetsAs mentioned above, the truncation error analysis is conducted on the Penn Treebank , whichis a widely-used language model dataset. It contains 88K, 3, 370 and 3, 761 sentences for training, validationand test. The vocabulary size is 10K. We set the layer depth of the language model to 1 or 2 to make a faircomparison. Assuming the layer depth is 1, then the loss between the model output and the true label could beregarded as the truncation error. In this way, we alleviate the influence of the error accumulation across differentlayers. Apart from PTB, we also evaluate our approach on another widely acknowledged language modelingdataset, Wikitext-103 dataset , which is the largest available word-level language modeling benchmark withlong-term dependency. WikiText-103 consists of 103M training tokens from 28K articles on Wikipedia, and theaverage length of tokens per article is about 3.6K. The data is can be easily obtained and preprocessed followingBaevski et al. s work. SetupsFor the PTB dataset, we used the transformer_base configuration, whose hidden size is 512, andthe filter size of the FFN is 2, 048. All the dropout rates are 0.1, including the residual dropout, attention dropoutand ReLU dropout. Each model was trained up to 20 epochs, while most models arrived at convergence on thevalidation set when the epoch is 10. Then the validation PPL began to increase, though the training PPL is stilldeclining. This is due to the small model capacity. The warmup step was 2, 000 and the batch size was 4, 096.The max learning rate was set to 0.0007. For the Wikitext-103 dataset, all models were based on the originalopen-source Fairseq toolkit, and the corresponding configuration is transformer_lm_baevski_wiki103. Itinherits the configuration of transformer_big with 1, 024 hidden size and 4, 096 filter size. Besides thesecommon hyper-parameters, it adopts the adaptive input representation to reduce the embedding matrix bydecreasing the embedding size of low-frequency words or sub-words. This is also the most common choicewhen building large-scale language models.",
  "DMore Analyses": "Parameter Efficiency summaries the results of several efficient Transformer variants, includingLite Transformer , DeLight , a light version of the Evolved Transformer , and our PCformer. It isclear to see that PCformer is significantly more parameter efficient than others. We make detailed comparisons ofPCformer within different hyper-parameters, comprising of hidden size and model depth. Concretely, RK2-block",
  ": The comparison of training and validation PPL on base and wide models": "with gated fusion coefficient learning strategy delivers stronger performance than DeLight within the samemodel parameters. And it is on par with DeLight in terms of BLEU, but having 9M fewer parameters. Moreover,as expected, our newly proposed EMA coefficient learning method slightly improves the performance of gatedfusion in almost all scenarios. A notable bonus it brought is higher-order solutions, e.g., RK4-block is superiorto RK2-block with the help of EMA. It may offer a new choice for deploying NMT systems on edge devices. Training and Validation PerplexityApart from the BLEU scores, we also compare our methods withbaselines regarding perplexity on both training and validation sets. plots the training and validationPPL curves of the PCformer (RK2-block (EMA + Pre-Cor)) and the baseline on two representative translationtasks. All models were in big configurations for more convincing conclusions. The proposed RK2-block withEMA coefficient learning and Predictor-Corrector framework delivers much lower training and validation PPLswithin the same configurations. More specifically, our method still benefits from increasing model depth, as the12-layer model outperforms the 6-layer one. For both the En-De and En-Fr tasks, we observed that our 6-layermethod even shows lower PPLs than a 12-layer Transformer. This phenomenon is more evident in the WMTEn-Fr task, which again demonstrates the high parameter efficiency of our PCformer. Visualization of the Coefficient Learning ProcedureWe also collect the learning process of learnablecoefficients during training. As we discussed above that the inspiration of EMA coefficient learning methodcomes from the prior that the most current approximation is more precise. This assumption has already beenclarified by the comparisons of truncation errors. Here, we want to figure out how coefficients learned ifremoving the constraint. To achieve this goal, we set all coefficients to be independently initialized by themean of 1, e.g., a 2-order block with 1 = 0.5, 2 = 0.5. plots the learning curves of RK2-block andRK4-block within these two learning strategies. As we can see that, for the independent initialization scenarios,coefficients vary dramatically within the first several epochs, and then show convergence in a small range. Bothtwo figures show that the contribution of the most current coefficient is larger than others. To our surprise, 2 in RK2-block (Independent) converges to large than 1 and oppositely, 1 is even goes to anegative value. Meanwhile, 3 in the RK4-block (Independent) shows a negative impact to the final solution,but the other 3 coefficients vary within our expectation. We notice the order relation among these 4 coefficientsare consistent with the numerical suggested coefficients in the Adams-Bashforlth method (-9/24, 37/24, -59/24,",
  ": The coefficient learning curves of independent initialization and EMA oin both 2-order and4-order scenarios. The experiments are conducted on WMT En-De": "55/24). This indicates the underlying relationship between the numerical analysis and the neural networkoptimization. While, after employing our EMA method, coefficients are optimized along our expected direction,and these models are empirically better than those without constraints. Results on Time-Series ForecastingWe also followed the reviewers suggestion to evaluate PCformer ontime-series forecasting tasks. We selected 10 multivariate datasets from UEA Time Series Classification Archivefollowing the setting and the codebase provided by Flowformer . Thus we choose the Flowformer as thebaseline, which is also a strong model on these testsets. For the details, we build the PCformer upon Flowformerand report the 2-order predictor and Euler corrector as the training data is very small. Also, we use RK-Norm toavoid the model suffering from the overfitting problem as the authors of Flowformer trained their models uponto 100 epochs (or even 400 epoch on some tasks.). The results are evaluated by the best accuracy. We can seethat PCformer can beat the Flowformer by 2 average score on 10 testsets, which demonstrates the effectivenesson time-series forecasting tasks.",
  "The answer NA means that the paper has no limitation while the answer No means that the paperhas limitations, but those are not discussed in the paper": "The authors are encouraged to create a separate \"Limitations\" section in their paper. The paper should point out any strong assumptions and how robust the results are to violations ofthese assumptions (e.g., independence assumptions, noiseless settings, model well-specification,asymptotic approximations only holding locally). The authors should reflect on how theseassumptions might be violated in practice and what the implications would be. The authors should reflect on the scope of the claims made, e.g., if the approach was only testedon a few datasets or with a few runs. In general, empirical results often depend on implicitassumptions, which should be articulated. The authors should reflect on the factors that influence the performance of the approach. Forexample, a facial recognition algorithm may perform poorly when image resolution is low orimages are taken in low lighting. Or a speech-to-text system might not be used reliably to provideclosed captions for online lectures because it fails to handle technical jargon.",
  "If applicable, the authors should discuss possible limitations of their approach to address problemsof privacy and fairness": "While the authors might fear that complete honesty about limitations might be used by reviewersas grounds for rejection, a worse outcome might be that reviewers discover limitations thatarent acknowledged in the paper. The authors should use their best judgment and recognizethat individual actions in favor of transparency play an important role in developing norms thatpreserve the integrity of the community. Reviewers will be specifically instructed to not penalizehonesty concerning limitations.",
  "The answer NA means that the paper does not include theoretical results": "All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. All assumptions should be clearly stated or referenced in the statement of any theorems. The proofs can either appear in the main paper or the supplemental material, but if they appear inthe supplemental material, the authors are encouraged to provide a short proof sketch to provideintuition.",
  "Answer: [Yes]": "Justification: We have summarized all details including the datasets, the model configurations andtraining details in the appendix. Please refer to Section C of the Appendix. And we also make a clearalgorithm to illustrate the detailed computation flow of the proposed PCformer. And we will releasethe code after re-organizing the codebase, to provide detailed scripts for reproduce our results.",
  "If the contribution is a dataset and/or model, the authors should describe the steps taken to maketheir results reproducible or verifiable": "Depending on the contribution, reproducibility can be accomplished in various ways. Forexample, if the contribution is a novel architecture, describing the architecture fully might suffice,or if the contribution is a specific model and empirical evaluation, it may be necessary to eithermake it possible for others to replicate the model with the same dataset, or provide access tothe model. In general. releasing code and data is often one good way to accomplish this, butreproducibility can also be provided via detailed instructions for how to replicate the results,access to a hosted model (e.g., in the case of a large language model), releasing of a modelcheckpoint, or other means that are appropriate to the research performed. While NeurIPS does not require releasing code, the conference does require all submissionsto provide some reasonable avenue for reproducibility, which may depend on the nature of thecontribution. For example(a) If the contribution is primarily a new algorithm, the paper should make it clear how toreproduce that algorithm.",
  "(b) If the contribution is primarily a new model architecture, the paper should describe thearchitecture clearly and fully": "(c) If the contribution is a new model (e.g., a large language model), then there should either bea way to access this model for reproducing the results or a way to reproduce the model (e.g.,with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors arewelcome to describe the particular way they provide for reproducibility. In the case ofclosed-source models, it may be that access to the model is limited in some way (e.g.,to registered users), but it should be possible for other researchers to have some path toreproducing or verifying the results.",
  "Guidelines:": "The answer NA means that the paper does not use existing assets. The authors should cite the original paper that produced the code package or dataset. The authors should state which version of the asset is used and, if possible, include a URL. The name of the license (e.g., CC-BY 4.0) should be included for each asset. For scraped data from a particular source (e.g., website), the copyright and terms of service ofthat source should be provided. If assets are released, the license, copyright information, and terms of use in the package shouldbe provided. For popular datasets, paperswithcode.com/datasets has curated licenses forsome datasets. Their licensing guide can help determine the license of a dataset.",
  "The answer NA means that the paper does not involve crowdsourcing nor research with humansubjects": "Depending on the country in which research is conducted, IRB approval (or equivalent) may berequired for any human subjects research. If you obtained IRB approval, you should clearly statethis in the paper. We recognize that the procedures for this may vary significantly between institutions andlocations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines fortheir institution."
}