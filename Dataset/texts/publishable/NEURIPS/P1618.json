{
  "Abstract": "As a prominent category of imitation learning methods, adversarial imitation learn-ing (AIL) has garnered significant practical success powered by neural networkapproximation. However, existing theoretical studies on AIL are primarily lim-ited to simplified scenarios such as tabular and linear function approximationand involve complex algorithmic designs that hinder practical implementation,highlighting a gap between theory and practice. In this paper, we explore thetheoretical underpinnings of online AIL with general function approximation. Weintroduce a new method called optimization-based AIL (OPT-AIL), which centerson performing online optimization for reward functions and optimism-regularizedBellman error minimization for Q-value functions. Theoretically, we prove thatOPT-AIL achieves polynomial expert sample complexity and interaction complex-ity for learning near-expert policies. To our best knowledge, OPT-AIL is the firstprovably efficient AIL method with general function approximation. Practically,OPT-AIL only requires the approximate optimization of two objectives, therebyfacilitating practical implementation. Empirical studies demonstrate that OPT-AILoutperforms previous state-of-the-art deep AIL methods in several challengingtasks. 1",
  "Introduction": "Sequential decision-making tasks are prevalent in real-world applications, where agents seek policiesthat maximize long-term returns. Reinforcement learning (RL) provides a well-known frameworkfor developing effective policies through trial and error. However, RL often necessitates carefullydesigned reward functions and typically requires millions of interactions with the environment toachieve satisfactory performance . In contrast, imitation learning (IL) offers a more sample-efficient approach to learning effective policies by mimicking expert demonstrations, bypassing theneed for explicit reward functions. As a result, IL has gained popularity and demonstrated successin a wide range of real-world applications such as recommendation systems and generalistrobot learning .",
  "arXiv:2411.00610v1 [cs.LG] 1 Nov 2024": "data . In contrast, AIL utilizes an adversarial learning process to replicate the expertsstate-action distribution. This process involves the learner recovering an adversarial reward tomaximize the policy value gap and subsequently learning a policy that minimizes this gap under therecovered reward. Building on these foundational principles, numerous practical algorithms havebeen developed , achieving significant empirical advancements. From these empirical advances, a notable observation is that AIL often significantly outperformsBC . To better understand this phenomenon, recent research has focused on thetheoretical underpinnings of AIL , particularly in the online setting. Thisresearch examines both expert sample complexity (the number of expert trajectories required) andinteraction complexity (the number of trajectories needed when interacting with the environment),both of which are crucial for practical applications. In the tabular setting, the best-known complexityresult is achieved in . They developed the MB-TAIL algorithm, which leverages advanceddistribution estimation, achieving the expert sample complexity O(H3/2|S|/) and interactioncomplexity O(H3|S|2|A|/2), where |S| and |A| are the state space size and action space size,respectively, H is the horizon length and is the desired value gap. Furthermore, investigatedthe AIL theory in the linear function approximation setting. Notably, the BRIG approach proposedin uses linear regression for policy evaluation and achieves the expert sample complexityO(H2d/2) and interaction complexity O(H4d3/2), where d is the feature dimension. For acomplete summary of related results, please refer to . Despite substantial theoretical advances, there still exists a gap between theory and practice in AIL.First, prior theoretical analysis primarily focuses on restricted settings such as tabular or linear function approximation , which deviate from practice where AIL approaches oftenoperate with general function approximation (e.g., neural network approximation). Besides, mostprevious theoretical works involve algorithmic designs such as count-based or covariance-matrix-based bonuses, which are tailored to their respective settings. Implementing suchalgorithmic designs in practical settings, where neural network approximation is employed, presentssignificant challenges . : A summary of the expert sample complexity and interaction complexity. Here H is thehorizon length, is the desired imitation gap, |S| is the state space size, |A| is the action space size,|| is the cardinality of the finite policy class , d is the dimension of the feature space, dGEC is thegeneralized eluder coefficient, N(Rh) and N(Qh) are the covering numbers of the reward class Rhand Q-value class Qh, respectively. We use O to hide logarithmic factors.2",
  "O H4dGEC log(maxh[H] N (Qh)N (Rh))+H2": "Contribution. This paper aims to bridge the gap between theory and practice in AIL by develop-ing a provably efficient algorithm with general function approximation and providing a practicalimplementation equipped with neural networks. First, we introduce a new AIL approach called optimization-based adversarial imitation learning(OPT-AIL) and provide a comprehensive theoretical analysis for general function approximation.The core of OPT-AIL involves minimizing two key objectives. To recover the reward, OPT-AILsolves an online optimization problem using a no-regret approach. For policy learning, inspiredby , OPT-AIL infers the Q-value functions by minimizing the optimism-regularized Bellman 2We will not omit log(N(F)) for a function class F in the O notation since log(N(F)) could not be smallfor many function classes.3Here we present the result of BC in the worst case, which is consistent with this paper. error and then derives the corresponding greedy policies. Under mild assumptions, we prove thatOPT-AIL achieves the expert sample complexity O(H2 log(maxh[H] N(Rh))/2) and interactioncomplexity O((H4dGEC log(maxh[H] N(Qh)N(Rh)) + H2)/2). Here dGEC is the generalizedeluder coefficient, originally proposed in to measure the complexity of RL with functionapproximation, which we adapt to the AIL setting. N(Rh) and N(Qh) are the covering numbers ofthe reward class Rh and Q-value class Qh, respectively. To our best knowledge, OPT-AIL is the firstprovably efficient AIL approach with general function approximation. Furthermore, we offer a practical implementation of OPT-AIL, demonstrating its competitive per-formance on standard benchmarks. Notably, OPT-AIL only requires the approximate optimizationof two objectives, simplifying its practical implementation with deep neural networks. Leverag-ing this advantage, we implement OPT-AIL using neural network approximations and compare itsperformance against prior state-of-the-art (SOTA) deep AIL methods, which often lack theoreticalguarantees. Experimental results indicate that OPT-AIL outperforms SOTA deep AIL approaches onseveral challenging tasks within the DMControl benchmark.",
  "Related Works": "Adversarial Imitation Learning. The theoretical foundations of AIL have been extensively ex-plored in numerous studies . Early research focused on ideal settings where the transition function is knownor an exploratory data distribution is available, primarily addressing expert sample efficiency. No-tably, under mild conditions, proved that AIL can achieve a horizon-free imitation gap boundO(min{1, |S|/N}), where N denotes the number of expert trajectories. Recently, a new researchdirection has emerged that addresses more practical scenarios, specifically online AIL with unknowntransitions . This line of work investigates both expert sample complexity and in-teraction complexity. These recent advancements were discussed in the previous section and thuswill not be reiterated here. Most existing theoretical works focus on either tabular orlinear function approximation settings , and often lack practical implementations due toalgorithmic designs tailored to specific settings. In contrast, this work simultaneously providestheoretical guarantees for general function approximation and offers a practical implementation thatdemonstrates competitive performance. On the empirical side, there has been extensive research developing practicalAIL approaches that leverage general function (or neural network) approximation. A seminal methodin this field is generative adversarial imitation learning (GAIL) . In GAIL, a discriminator istrained to distinguish between samples from expert demonstrations and those generated by a policy,while the policy (or generator) learns to maximize the reward signal provided by the discriminator.More recently, proposed inverse Q-Learning (IQLearn), which achieves SOTA performanceacross a diverse set of tasks. However, these practical methods often lack rigorous theoreticalguarantees for general function approximation. General Function Approximation in Reinforcement Learning. Our work is closely related to abody of research focused on general function approximation in RL . Notably, proposed an algorithmic framework that incorporates a unified objective to balance exploration andexploitation in RL, demonstrating a sublinear regret bound. In this paper, we adapt this algorithmicdesign to address several RL sub-problems within the context of AIL. Unlike the RL setting, wherea fixed reward is provided in advance, AIL involves inferring the reward function from expertdemonstrations and environment interactions collected by the learning policies. Therefore, our workrequires developing a theoretical analysis for the joint learning process of both rewards and policies,highlighting a unique challenge in AIL compared to traditional RL.",
  "Preliminaries": "Markov Decision Process. In this paper, we consider episodic Markov Decision Processes (MDPs),represented by the tuple M = (S, A, P, rtrue, H, s1). Here, S and A denote the state and actionspaces, respectively. H signifies the planning horizon, while s1 stands for the fixed initial state. Theset P = {P1, . . . , PH} characterizes the non-stationary transition function of this MDP. Specifically,Ph(sh+1|sh, ah) determines the probability of transiting to state sh+1 given state sh and action ah attime step h, where h [H]. Similarly, rtrue = {rtrue1, . . . , rtrueH} outlines the reward function of thisMDP. Without loss of generality, we assume rtrueh: S A for h [H]. A non-stationary",
  "Assumption 1 (Realizability of R). The unknown true reward lies in the reward class, i.e., rtrue R": "Besides, the learner also has access to a Q-value function class Q = Q1 Q2 . . . QH withh [H], Qh (S A [0, H]), which is used for solving several RL sub-problems underdifferent inferred rewards in AIL. Since there is no reward in the H +1 step, we always set QH+1 0.Below, we present two standard assumptions about the function class Q that are commonly adoptedin the literature of RL with function approximation .",
  "Assumption 3 (Bellman Completeness of Q). For reward r R, T rh Qh+1 Qh, h [H], whereT rh denotes the Bellman operator under reward r and T rh Qh+1 = {T rh Qh+1 : Qh+1 Qh+1}": "In short, Assumption 2 states that the Q-value class Q should capture the optimal Q-value function,while Assumption 3 indicates the closeness of Q under Bellman update. It is easy to verify thatAssumptions 1, 2 and 3 are more general than the tabular MDP , linear mixture MDP and linear MDP assumptions used in previous works. When the function class contains a finite number of elements, its cardinality can be used to quantify itssize. However, for general function approximation, where the function class may contain an infinitenumber of elements, we utilize the standard -covering number to measure its complexity. Definition 1 (-covering number). For function class F (X R), the -covering number ofF, denoted as N(F), is defined as the minimum integer n such that there exists a finite subsetF F with |F| = n such that for any function f F, there exists f F satisfying thatmaxxX |f(x) f (x)| .",
  "Optimization-based Adversarial Imitation Learning": "In this section, we introduce a provably efficient method called Optimization-Based AdversarialImitation Learning (OPT-AIL). In .1, we delve into the core components of OPT-AIL,which involves online optimization for reward functions and optimism-regularized Bellman errorminimization for Q-value functions. We discuss the underlying principles and provide theoreticalguarantees with general function approximation. Thanks to its easy-to-implement merit, we providea practical implementation of OPT-AIL using stochastic-gradient-based methods in .2.",
  "Theoretical Analysis of OPT-AIL": "In this section, we present our provably efficient method OPT-AIL with general function approxima-tion; see Algorithm 1 for an overview. To start with, recall that our theoretical goal is to ensure thealgorithm can output a policy with -imitation gap by using finite expert samples and environmentinteractions. To obtain the final policy, we leverage the standard online-to-batch conversion technique. Specifically, during the learning process, the learner iteratively generates a sequence of rewards{rk}Kk=1 and policies {k}Kk=1, and outputs the policy that is uniformly sampled from {k}Kk=1.To analyze the imitation gap of , we leverage the following standard error decomposition lemma.",
  ".(1)": "Lemma 1 suggests that to achieve a small imitation gap, it is crucial to control both reward error andpolicy error. Specifically, reward error quantifies the distance between the true reward rtrue and thelearned reward rk through the imitation gap. Besides, policy error measures the value differencebetween the expert policy E and the learned policy k under the inferred reward rk. Notably, thispolicy error differs from the concept of regret in RL , where the reward is fixed. To theoretically minimize the reward error and policy error, we consider an iterative approach, inwhich each iteration first updates the reward and subsequently derives the policy. The subsequentparts detail the reward and policy updates, which involve solving two optimization problems.",
  ": end forOutput: sampled uniformly from {k}Kk=1": "Reward Update via Online Optimization (Line 3 in Algorithm 1). The goal of this step is tocontrol the reward error. More concretely, in iteration k, we aim to learn a reward rk such that the errorV krk V Erk (V krtrue V Ertrue) is small. We formulate this problem as an online optimization problem.In iteration k, using the previously observed loss functions {V ir V Er}k1i=0 , the reward learnerselects rk and then observes the current loss function V kr V Er. Moreover, since the previousexpected loss functions {V ir V Er}k1i=0 are not available, we instead minimize the estimated loss",
  "h=1rh((sh), (ah))": "Here ((sh), (ah)) is the state-action pair of trajectory visited in time step h and i ={si1, ai1, . . . , siH, aiH} is the trajectory collected by policy i. The ultimate goal of the rewardlearner is to minimize the cumulative losses Kk=1 V krk V Erk . To achieve this goal, we employa no-regret algorithm . In the following part, we formally define the reward optimization errorresulting from running the no-regret algorithm.Definition 2 (Reward Optimization Error). For any sequence of policies {k}Kk=1, the no-regretreward optimization algorithm sequentially outputs rewards r1, . . . , rK. The reward optimizationerror ropt is defined as ropt := (1/K) maxrRKk=1 V krk V Erk (V kr V Er). The reward optimization error, as defined above, aligns with the standard average regret in onlineoptimization , a concept not extensively explored in the context of AIL. When the loss functions{Lk(r)}Kk=0 are convex functions and the reward class R is a convex set, we can apply onlineprojected gradient descent as the no-regret algorithm, which ensures the reward optimizationerror ropt = O(1/",
  "K)": "Policy Update via Optimism-Regularized Bellman-error Minimization (Lines 4-5 in Algo-rithm 1). The target of policy updates is to control the policy error. In iteration k, the policy learneraims to recover a policy k such that the policy error V Erk V krk is small, where rk is the recentlyrecovered reward function. This is essentially an RL problem under reward function rk. Buildingupon , we leverage a model-free approach, based on Bellman error minimization, to solve this RLsub-problem. In particular, we first learn Q-value functions by solving the optimization problem of",
  "h=1Eh(Qh, Qh+1; Dk, rk) infQhQh Eh(Qh, Qh+1; Dk, rk),": "where Eh(Qh, Qh+1; Dk, rk) = k1i=0 (Qh(sih, aih) rkh maxaA Qh+1(sih+1, a))2, Dk ={ i}k1i=0 with i = {si1, ai1, . . . , siH, aiH} and > 0 is the regularization coefficient. As shownin , BEk(Q) is the estimated squared Bellman error of Q with respect to reward rk anddataset Dk. In this optimization problem, the main objective BEk(Q) ensures a small Bellman errorwhile the regularization term maxaA Q1(s1, a) tends to search an optimistic Q-value function forencouraging exploration. It is worth noting that Algorithm 1 only requires approximately solving theoptimization problem up to an error Qopt with Qopt = Lk(Qk) minQQ Lk(Q). After obtainingthe Q-value function Qk, we derive k as the greedy policy of Qk. Theoretical Guarantee of OPT-AIL. In the above part, we have explained the algorithmic mech-anisms of OPT-AIL. Now we present the theoretical guarantee. To ensure the sample efficiencyof solving RL sub-problems within AIL, we make a structural assumption on the underlying MDP.In particular, we assume that the MDP has a small generalized eluder coefficient. This coefficient,introduced in , quantifies the inherent difficulty of learning the MDP with function approximationin RL. We adapt this concept to AIL where the reward function is changing.Assumption 4 (Low generalized eluder coefficient ). We assume that given an > 0, thegeneralized eluder coefficient dGEC() is the smallest d (d 0) such that for any sequence of{rk}Kk=1 R, {Qk}Kk=1 Q and the corresponding greedy policies {k}Kk=1,",
  "where Qk1(s1, k) := Ea1k1 (|s1)[Qk1(s1, a1)]": "Intuitively, a low generalized eluder coefficient ensures that the prediction error Qk1(s1, k)V krk fork can be effectively controlled by the Bellman error on the dataset generated by historical policies{i}k1i=1 . As demonstrated in , the MDPs with low generalized eluder coefficient form a richclass of MDPs, which covers many well-known MDP instances such tabular MDPs, linear MDPs and MDPs with low Bellman eluder dimension . Now we are ready to present the theoreticalguarantee of OPT-AIL.Theorem 1. Under Assumptions 1, 2, 3 and 4. For any fixed (0, 1] and (0, 1], considerAlgorithm 1 with = c1 (KH3 log(4KHN(Q)N(R)/) + K2H3)/dGEC, where dGEC :=dGEC(/H), := c22/(H2dGEC + H), c1 and c2 are absolute constants. Then with probabilityat least 1 , we have that V E V + ropt + (Qopt/) if the expert sample complexity andinteraction complexity satisfy",
  ": end for": "Practical Reward Update. Here we detail a practical implementation of the reward update byapplying an online optimization approach. Recall that in line 3 of Algorithm 1, a no-regret algorithmis employed to solve the online optimization problem. To implement this mechanism, we choose theclassical online optimization approach Follow-the-Regularized-Leader (FTRL) as the no-regret",
  "(2)": "where ED[] denotes the empirical distribution of dataset D. Here (r) is the regularization term. Inpractice, we choose (r) as the gradient penalty of the reward model, which can help stabilizethe learning process . According to Eq. (2), the reward learner aims to maximize the value gapbetween the expert policy and all previous policies. Besides, as indicated in Eq. (2), all historical samples Dk are utilized for the reward update. Thislearning style is exactly off-policy reward learning . In particular, applying FTRL for thereward update and off-policy reward learning share the same main objective. Previous practicalworks found that this off-policy reward learning works well in practice, but could not givean explanation. In this work, we justify this off-policy learning style from an online optimizationperspective: performing off-policy learning, which aligns with the FTRL approach, can effectivelycontrol the reward optimization error. Practical Policy Update. To implement the policy update in practice, we adopt the actor-criticframework . In particular, we maintain a policy model and a Q-value model Q. Recallin line 4 of Algorithm 1, the Q-value function is learned by minimizing the optimism-regularizedBellman error. To implement this principle, following , we leverage the temporal differenceloss of the Q-value model and its delayed target to approximate the theoretical Bellman error.Then we arrive at the following objective.",
  "Experiment Set-up": "Environment. We conduct experiments on 8 tasks sourced from the feature-based DMControlbenchmark , a leading benchmark in IL that offers a diverse set of continuous control tasks. Foreach task, we adopt online DrQ-v2 to train an agent with sufficient environment interactions andregard the resultant policy as the expert policy. Then we roll out this expert policy to collect expertdemonstrations. Each algorithm is tested over five trials with different random seeds, and in each run,we evaluate the policy return using Monte Carlo approximation with 10 trajectories. Baselines. Existing theoretical AIL approaches like MB-TAIL and OGAIL include count-based or covariance-based bonuses, making it challenging to implement these designs when usingneural network approximations. Thus, we do not include these methods in our experiments. Instead,we compare OPT-AIL with prior deep IL methods, including BC , IQLearn , PPIL ,FILTER and HyPE , despite that most of them lack theoretical guarantees. Notably, IQLearn,FILTER and HyPE represent prior SOTA deep AIL approaches. To ensure a fair comparison, weimplement all these methods within the same codebase. For detailed implementations, please refer toAppendix C. # Expert Trajectories",
  "Hopper Stand": "OPT-AIL (Ours)IQLearnPPILFILTERHyPEBCExpert : Overall performance on 8 DMControl tasks over 5 random seeds following 500k interactionswith the environment. Here the x-axis is the number of expert trajectories and the y-axis is the return.The solid lines are the mean of results while the shaded region corresponds to the standard deviationover 5 random seeds. Same as the following figures. 0200k400k # Environment Interactions",
  "Experiment Results": "Expert Sample Efficiency. shows the performance of the learned policies after 500kenvironment interactions with varying numbers of expert trajectories. First, OPT-AIL significantlyoutperforms BC, verifying the theoretical claim that OPT-AIL can mitigate the compounding errorsissue inherent in BC for general function approximation. Moreover, OPT-AIL consistently matches orexceeds the performance of prior SOTA AIL methods on all tasks. Notably, OPT-AIL demonstratesoutstanding performance in scenarios with limited expert demonstrations, a common occurrence inreal-world applications. In particular, when there is only one expert trajectory, our method uniquelyachieves expert or near-expert performance on tasks like Finger Spin, Walker Run and HopperHop. Environment Interaction Efficiency. displays the learning curves of different algorithmswith 1 expert trajectory. Compared with prior SOTA AIL approaches, OPT-AIL achieves comparableor better performance regarding interaction efficiency on all 8 tasks. Notably, on Hopper Hop,Walker Run and Walker Run, OPT-AIL can achieve near-expert performance with substantiallyfewer environment interactions compared with prior approaches. We also demonstrate the superiorinteraction efficiency of OPT-AIL with other numbers of expert trajectories; please refer to AppendixD for additional results.",
  "Conclusions": "To narrow the gap between theory and practice in adversarial imitation learning, this paper investigatesAIL with general function approximation. We develop a new AIL approach termed OPT-AIL, whichcenters on performing online optimization for reward functions and optimism-regularized Bellmanerror minimization for Q-value functions. In theory, OPT-AIL achieves polynomial expert samplecomplexity and interaction complexity for general function approximation. In practice, OPT-AIL onlyrequires approximately solving two optimization problems, which enables an efficient implementation.Our experiments demonstrate that OPT-AIL outperforms prior SOTA methods in several challengingtasks, highlighting its potential to bridge theoretical soundness with practical efficiency. In tabular MDPs, the currently optimal expert sample complexity is O(H3/2/) , which isbetter than O(H2/2) attained in this paper. Therefore, a promising and valuable future directionwould be to develop more advanced AIL approaches that achieve this expert sample complexity inthe setting of general function approximation. Besides, established a horizon-free imitation gapbound for AIL in tabular MDPs. Thus it is interesting to explore horizon-free bounds for AIL withgeneral function approximation.",
  "Pieter Abbeel and Andrew Y. Ng. Apprenticeship learning via inverse reinforcement learning.In Proceedings of the 21st International Conference on Machine Learning, pages 18, 2004": "Jacob D Abernethy, Elad Hazan, and Alexander Rakhlin. Competing in the dark: An efficientalgorithm for bandit linear optimization. In Proceedings of the 21st Annual Conference onLearning Theory, pages 263274, 2008. Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert Schapire.Taming the monster: A fast and simple algorithm for contextual bandits. In Proceedings of the31st International Conference on Machine Learning, pages 16381646, 2014.",
  "Kiante Brantley, Wen Sun, and Mikael Henaff. Disagreement-regularized imitation learning. InProceedings of the 8th International Conference on Learning Representations, 2020": "Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choro-manski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. arXiv, 2307.15818, 2023. Xinshi Chen, Shuang Li, Hui Li, Shaohua Jiang, Yuan Qi, and Le Song. Generative adversarialuser model for reinforcement learning based recommendation system. In Proceedings of the36th International Conference on Machine Learning, pages 10521061, 2019. Ching-An Cheng, Tengyang Xie, Nan Jiang, and Alekh Agarwal. Adversarially trained actorcritic for offline reinforcement learning. In Proceedings of the 39th International Conferenceon Machine Learning, pages 38523878, 2022.",
  "Dylan J Foster, Adam Block, and Dipendra Misra. Is behavior cloning all you need? under-standing horizon in imitation learning. arXiv, 2407.15007, 2024": "Scott Fujimoto, Herke van Hoof, and David Meger. Addressing function approximation error inactor-critic methods. In Proceedings of the 35th International Conference on Machine Learning,pages 15821591, 2018. Divyansh Garg, Shuvam Chakraborty, Chris Cundy, Jiaming Song, and Stefano Ermon. Iq-learn:Inverse soft-q learning for imitation. In Advances in Neural Information Processing Systems 34,pages 40284039, 2021. Seyed Kamyar Seyed Ghasemipour, Richard S. Zemel, and Shixiang Gu. A divergence minimiza-tion perspective on imitation learning methods. In Proceedings of the 3rd Annual Conferenceon Robot Learning, pages 12591277, 2019. Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policymaximum entropy deep reinforcement learning with a stochastic actor. In Proceedings of the35th International Conference on Machine Learning, pages 18561865, 2018.",
  "Shengyi Jiang, Jingcheng Pang, and Yang Yu. Offline imitation learning with a misspecifiedsimulator. Advances in Neural Information Processing Systems 33, 2020": "Chi Jin, Qinghua Liu, and Sobhan Miryoosefi. Bellman eluder dimension: New rich classes ofrl problems, and sample-efficient algorithms. In Advances in Neural Information ProcessingSystems 34, pages 1340613418, 2021. Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I. Jordan. Provably efficient reinforcementlearning with linear function approximation. In Proceedings of the 33rd Annual Conference onLearning Theory, pages 21372143, 2020.",
  "Ziniu Li, Tian Xu, and Yang Yu. A note on target q-learning for solving finite mdps with agenerative oracle. arXiv preprint arXiv:2203.11489, 2022": "Zhihan Liu, Miao Lu, Wei Xiong, Han Zhong, Hao Hu, Shenao Zhang, Sirui Zheng, ZhuoranYang, and Zhaoran Wang. Maximize to explore: One objective function fusing estimation,planning, and exploration. Advances in Neural Information Processing Systems 36, 2024. Zhihan Liu, Yufeng Zhang, Zuyue Fu, Zhuoran Yang, and Zhaoran Wang. Provably efficientgenerative adversarial imitation learning for online and offline setting with linear functionapproximation. arXiv, 2108.08765, 2021. Oier Mees, Dibya Ghosh, Karl Pertsch, Kevin Black, Homer Rich Walke, Sudeep Dasari,Joey Hejna, Tobias Kreiman, Charles Xu, Jianlan Luo, You Liang Tan, Dorsa Sadigh, ChelseaFinn, and Sergey Levine. Octo: An open-source generalist robot policy. In First Workshop onVision-Language Models for Navigation and Manipulation at ICRA 2024, 2024. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc GBellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al.Human-level control through deep reinforcement learning. Nature, 518(7540):529533, 2015.",
  "Lior Shani, Tom Zahavy, and Shie Mannor. Online apprenticeship learning. arXiv, 2102.06924,2021": "Jing-Cheng Shi, Yang Yu, Qing Da, Shi-Yong Chen, and Anxiang Zeng. Virtual-taobao:virtualizing real-world online retail environment for reinforcement learning. In Proceedings ofthe 33rd AAAI Conference on Artificial Intelligence, pages 49024909, 2019. Arun Sai Suggala and Praneeth Netrapalli. Online non-convex learning: Following the perturbedleader is optimal. In Proceedings of the 31st International Conference on Algorithmic LearningTheory, pages 845861, 2020. Wen Sun, Anirudh Vemula, Byron Boots, and Drew Bagnell. Provably efficient imitationlearning from observation alone. In Proceedings of the 36th International Conference onMachine Learning, pages 60366045, 2019.",
  "Richard S Sutton and Andrew G Barto. Reinforcement Learning: An Introduction. MIT press,2018": "Gokul Swamy, Nived Rajaraman, Matt Peng, Sanjiban Choudhury, J Bagnell, Steven Z Wu,Jiantao Jiao, and Kannan Ramchandran. Minimax optimal online imitation learning via replayestimation. Advances in Neural Information Processing Systems 35, pages 70777088, 2022. Gokul Swamy, David Wu, Sanjiban Choudhury, Drew Bagnell, and Steven Wu. Inversereinforcement learning without reinforcement learning. In Proceedings of the 40th InternationalConference on Machine Learning, 2023.",
  "Umar Syed and Robert E. Schapire. A game-theoretic approach to apprenticeship learning. InAdvances in Neural Information Processing Systems 20, pages 14491456, 2007": "Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, DavidBudden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, et al. Deepmind control suite.arXiv preprint arXiv:1801.00690, 2018. Daniil Tiapkin, Denis Belomestny, Eric Moulines, Alexey Naumov, Sergey Samsonov, YunhaoTang, Michal Valko, and Pierre Menard. From dirichlet to rubin: Optimistic exploration in rlwithout bonuses. In Proceedings of the 39th International Conference on Machine Learning,pages 2138021431, 2022.",
  "Martin J Wainwright. High-dimensional statistics: A non-asymptotic viewpoint. CambridgeUniversity Press, 2019": "Ruosong Wang, Russ R Salakhutdinov, and Lin Yang. Reinforcement learning with generalvalue function approximation: Provably efficient approach via bounded eluder dimension.Advances in Neural Information Processing Systems 33, pages 61236135, 2020. Yizhou Wang, Tianyi Liu, Zhuoran Yang, Xingguo Li, Zhaoran Wang, and Tuo Zhao. Oncomputation and generalization of generative adversarial imitation learning. In Proceedings ofthe 8th International Conference on Learning Representations, 2020.",
  "Tianpei Yang, Hongyao Tang, Chenjia Bai, Jinyi Liu, Jianye Hao, Zhaopeng Meng, Peng Liu,and Zhen Wang. Exploration in deep reinforcement learning: a comprehensive survey. arXiv,2109.06668, 2021": "Denis Yarats, Rob Fergus, Alessandro Lazaric, and Lerrel Pinto. Mastering visual continuouscontrol: Improved data-augmented reinforcement learning. In International Conference onLearning Representations, 2021. Yufeng Zhang, Qi Cai, Zhuoran Yang, and Zhaoran Wang. Generative adversarial imitationlearning with neural network parameterization: Global optimality and convergence rate. InProceedings of the 37th International Conference on Machine Learning, pages 1104411054,2020.",
  "ABroader Impacts": "This study explores the theoretical foundations of adversarial imitation learning with general functionapproximation and demonstrates the efficiency of the proposed algorithm through standard bench-marks. Although the paper does not reveal any immediate social impacts, the potential practicalapplications of our research could drive positive change. By broadening the scope of adversarialimitation learning, our work may enable the creation of more efficient and effective solutions in fieldssuch as robotics and autonomous vehicles. Nonetheless, we must recognize the potential for negativeconsequences if this technology is misused. For example, imitation learning learns from humanexpert demonstrations and could raise privacy concerns. Therefore, it is essential to ensure that theadvancements in imitation learning are applied responsibly and ethically.",
  "N+ 2H.(5)": "Now we have obtained the upper bound on the estimation error |V Er V Er|. Then we proceed toupper bound the estimation error (1/K) Kk=1 V krtrue V krtrue and (1/K) Kk=1 V krk V krk . Withthe Hoeffdings inequality , with probability at least 1 , we obtain that",
  "K.(6)": "We proceed to analyze the term Kk=1 V krk V krk . Notice that rk are learned from historical trajecto-ries { 1, . . . , k1} and thus statistically depends on { 1, . . . , k1}. Therefore, V 1r1 , , V krk arenot independent and the standard Hoeffdings inequality is not applicable. To address this issue, weapply Azuma-Hoeffdings inequality for martingale. In particular, we define Fk as the filtrationinduced by { 1, , k} and can obtain that",
  "The last equation is obtained by choosing = 1/(16H2)": "We define (Qh) and (Rh) as the -cover of Qh and Rh, respectively. It is direct to have thatQ = (Q1) . . . (QH) and R = (R1) . . . (RH) are -covers of Q and R, respectively. Byunion bound, with probability at least 1 , for all (k, h, Q, r) [K] [H] Q R, we havethat",
  "i=0Xih(Q, r) + 82H2 log(2/)": "We define (Qh) and (Rh) as the -covers of Qh and Rh, respectively. It is direct to have thatQ = (Q1) . . . (QH) and R = (R1) . . . (RH) are -covers of Q and R, respectively. Byunion bound, with probability at least 1 , for all (k, h, Q, r) [K] [H] Q R,",
  "C.1Implementation Details of OPT-AIL": "Reward Update. As mentioned in .2, we choose (r) in Eq. (2) as the gradient penalty(GP) regularization of the reward model , which can help stabilize the online optimization processby enforcing 1-Lipschitz continuity of the reward model r. Here DI is linear interpolations betweenthe replay buffer Dk and expert demonstrations DE.",
  "h=1(rh(sh, ah) 1)2": "Policy Update. Here we present the implementation details of policy updates. Firstly, to stabilizethe training process, we refine the optimism regularization term by subtracting a baseline Q-valuefunction from random policy Unif(A), which has been utilized in . Furthermore,recognizing that initial state samples can be limited and lack diversity, we employ both the replaybuffer Dk and expert demonstrations DE to compute the Q-value loss, which is a common dataaugmentation approach and has been validated in many deep AIL methods . Incorporatingthese two enhancements, we reformulate the Q-value model training objective as follows.",
  "C.2Architecture and Training Details": "The experiments are conducted on a machine with 64 CPU cores and 4 RTX4090 GPU cores. Eachexperiment is replicated five times using different random seeds. For each task, we adopt onlineDrQ-v2 to train an agent with sufficient environment interactions1 and regard the resultantpolicy as the expert policy. Then we roll out this expert policy to collect expert demonstrations. Thearchitecture and training details of OPT-AIL and all baselines are listed below.",
  "M training steps for Cheetah Run, Hopper Hop, and Walker Run, and 1M training steps for other tasks": "OPT-AIL: Our codebase of OPT-AIL extends the open-sourced framework of IQLearn. We retain thestructure and parameter design of the actor and critic from the original framework while employingSAC with a fixed temperature for policy update. We also implement a discriminator with asimilar architecture to the critic network, and additionally incorporate layer normalization and tanhactivation before the output to improve training stability. A comprehensive enumeration of thehyperparameters of OPT-AIL is provided in .",
  "IQLearn: We use the authors codebase, which is available at": "DAC: We reproduce the DAC based on our codebase. Due to the difference in updating the discrim-inator compared to OPT-AIL, we refer to the official DAC implementation when reproducing thediscriminator. We remove the layer normalization and the tanh activation function before the output,and find that this resulted in better performance.",
  "HyPE: We use the authors codebase, which is available at": "We emphasize that for a fair comparison, all algorithms are implemented using the same codebase 2,with all hyperparameters kept consistent except for the gradient penalty coefficient. Specifically, inOPT-AIL, the gradient penalty coefficient is set to 1 for Cartpole Swingup, Walker Walk, andWalker Stand, and 10 for other tasks. For baselines, the gradient penalty coefficient is always set to10 as provided by the authors. We also attempt to adjust this parameter for the baselines but find thatthe default parameters provided by the authors work well.",
  "DAdditional Experimental Results": "In this section, we list the learning curves for 8 DMControl tasks with 4, 7, and 10 expert trajectoriesrespectively. The corresponding results are depicted in , , and . Here thex-axis is the number of environment interactions and the y-axis is the return. The solid lines are themean of results while the shaded region corresponds to the standard deviation over 5 random seeds.Our results demonstrate that OPT-AIL consistently achieves better interaction sample efficiency thanstate-of-the-art (SOTA) deep AIL methods, across varying numbers of expert trajectories. 0200k400k # Environment Interactions",
  "The answer NA means that the paper has no limitation while the answer No means thatthe paper has limitations, but those are not discussed in the paper": "The authors are encouraged to create a separate Limitations section in their paper. The paper should point out any strong assumptions and how robust the results are toviolations of these assumptions (e.g., independence assumptions, noiseless settings,model well-specification, asymptotic approximations only holding locally). The authorsshould reflect on how these assumptions might be violated in practice and what theimplications would be. The authors should reflect on the scope of the claims made, e.g., if the approach wasonly tested on a few datasets or with a few runs. In general, empirical results oftendepend on implicit assumptions, which should be articulated. The authors should reflect on the factors that influence the performance of the approach.For example, a facial recognition algorithm may perform poorly when image resolutionis low or images are taken in low lighting. Or a speech-to-text system might not beused reliably to provide closed captions for online lectures because it fails to handletechnical jargon.",
  "If applicable, the authors should discuss possible limitations of their approach toaddress problems of privacy and fairness": "While the authors might fear that complete honesty about limitations might be used byreviewers as grounds for rejection, a worse outcome might be that reviewers discoverlimitations that arent acknowledged in the paper. The authors should use their bestjudgment and recognize that individual actions in favor of transparency play an impor-tant role in developing norms that preserve the integrity of the community. Reviewerswill be specifically instructed to not penalize honesty concerning limitations.",
  "If the contribution is a dataset and/or model, the authors should describe the steps takento make their results reproducible or verifiable": "Depending on the contribution, reproducibility can be accomplished in various ways.For example, if the contribution is a novel architecture, describing the architecture fullymight suffice, or if the contribution is a specific model and empirical evaluation, it maybe necessary to either make it possible for others to replicate the model with the samedataset, or provide access to the model. In general. releasing code and data is oftenone good way to accomplish this, but reproducibility can also be provided via detailedinstructions for how to replicate the results, access to a hosted model (e.g., in the caseof a large language model), releasing of a model checkpoint, or other means that areappropriate to the research performed. While NeurIPS does not require releasing code, the conference does require all submis-sions to provide some reasonable avenue for reproducibility, which may depend on thenature of the contribution. For example(a) If the contribution is primarily a new algorithm, the paper should make it clear howto reproduce that algorithm.",
  "(b) If the contribution is primarily a new model architecture, the paper should describethe architecture clearly and fully": "(c) If the contribution is a new model (e.g., a large language model), then there shouldeither be a way to access this model for reproducing the results or a way to reproducethe model (e.g., with an open-source dataset or instructions for how to constructthe dataset). (d) We recognize that reproducibility may be tricky in some cases, in which caseauthors are welcome to describe the particular way they provide for reproducibility.In the case of closed-source models, it may be that access to the model is limited insome way (e.g., to registered users), but it should be possible for other researchersto have some path to reproducing or verifying the results.",
  ". Experimental Setting/Details": "Question: Does the paper specify all the training and test details (e.g., data splits, hyper-parameters, how they were chosen, type of optimizer, etc.) necessary to understand theresults?Answer: [Yes]Justification: All experimental details are described in and Appendix C.Guidelines: The answer NA means that the paper does not include experiments. The experimental setting should be presented in the core of the paper to a level of detailthat is necessary to appreciate the results and make sense of them.",
  ". Experiment Statistical Significance": "Question: Does the paper report error bars suitably and correctly defined or other appropriateinformation about the statistical significance of the experiments?Answer: [Yes]Justification: We report the standard deviation over 5 random seeds for all experiments inthis paper; see the detailed results in and Appendix D.Guidelines: The answer NA means that the paper does not include experiments. The authors should answer Yes if the results are accompanied by error bars, confi-dence intervals, or statistical significance tests, at least for the experiments that supportthe main claims of the paper. The factors of variability that the error bars are capturing should be clearly stated (forexample, train/test split, initialization, random drawing of some parameter, or overallrun with given experimental conditions).",
  ". Experiments Compute Resources": "Question: For each experiment, does the paper provide sufficient information on the com-puter resources (type of compute workers, memory, time of execution) needed to reproducethe experiments?Answer: [Yes]Justification: We describe the information on the computer resources for running theexperiments in Appendix C.Guidelines: The answer NA means that the paper does not include experiments. The paper should indicate the type of compute workers CPU or GPU, internal cluster,or cloud provider, including relevant memory and storage.",
  ". Broader Impacts": "Question: Does the paper discuss both potential positive societal impacts and negativesocietal impacts of the work performed?Answer: [Yes]Justification: We discuss both potential positive societal impacts and negative societalimpacts of this work in Appendix A.Guidelines: The answer NA means that there is no societal impact of the work performed. If the authors answer NA or No, they should explain why their work has no societalimpact or why the paper does not address societal impact. Examples of negative societal impacts include potential malicious or unintended uses(e.g., disinformation, generating fake profiles, surveillance), fairness considerations(e.g., deployment of technologies that could make decisions that unfairly impact specificgroups), privacy considerations, and security considerations. The conference expects that many papers will be foundational research and not tiedto particular applications, let alone deployments. However, if there is a direct path toany negative applications, the authors should point it out. For example, it is legitimateto point out that an improvement in the quality of generative models could be used togenerate deepfakes for disinformation. On the other hand, it is not needed to point outthat a generic algorithm for optimizing neural networks could enable people to trainmodels that generate Deepfakes faster. The authors should consider possible harms that could arise when the technology isbeing used as intended and functioning correctly, harms that could arise when thetechnology is being used as intended but gives incorrect results, and harms followingfrom (intentional or unintentional) misuse of the technology. If there are negative societal impacts, the authors could also discuss possible mitigationstrategies (e.g., gated release of models, providing defenses in addition to attacks,mechanisms for monitoring misuse, mechanisms to monitor how a system learns fromfeedback over time, improving the efficiency and accessibility of ML).",
  "Guidelines:": "The answer NA means that the paper does not use existing assets. The authors should cite the original paper that produced the code package or dataset. The authors should state which version of the asset is used and, if possible, include aURL. The name of the license (e.g., CC-BY 4.0) should be included for each asset. For scraped data from a particular source (e.g., website), the copyright and terms ofservice of that source should be provided. If assets are released, the license, copyright information, and terms of use in thepackage should be provided. For popular datasets, paperswithcode.com/datasetshas curated licenses for some datasets. Their licensing guide can help determine thelicense of a dataset.",
  "The answer NA means that the paper does not involve crowdsourcing nor research withhuman subjects": "Depending on the country in which research is conducted, IRB approval (or equivalent)may be required for any human subjects research. If you obtained IRB approval, youshould clearly state this in the paper. We recognize that the procedures for this may vary significantly between institutionsand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and theguidelines for their institution."
}