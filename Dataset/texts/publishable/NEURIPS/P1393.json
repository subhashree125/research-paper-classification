{
  "Abstract": "Vision Transformers (ViT) is known for its scalability. In this work, we targetto scale down a ViT to fit in an environment with dynamic-changing resourceconstraints. We observe that smaller ViTs are intrinsically the sub-networks of alarger ViT with different widths. Thus, we propose a general framework, namedScala, to enable a single network to represent multiple smaller ViTs with flexibleinference capability, which aligns with the inherent design of ViT to vary fromwidths. Concretely, Scala activates several subnets during training, introducesIsolated Activation to disentangle the smallest sub-network from other subnets,and leverages Scale Coordination to ensure each sub-network receives simplified,steady, and accurate learning objectives. Comprehensive empirical validations ondifferent tasks demonstrate that with only one-shot training, Scala learns slimmablerepresentation without modifying the original ViT structure and matches the per-formance of Separate Training. Compared with the prior art, Scala achieves anaverage improvement of 1.6% on ImageNet-1K with fewer parameters. Code isavailable at here.",
  "Introduction": "Vision Transformers (ViTs) are renowned for its scalability and various avenues havebeen explored to scale up ViT models. To tailor ViTs to run on devices with limited resources, somerecent progress utilize knowledge distillation to scale down ViT. Particularly, DeiT introduces two smaller variants of DeiT-B: DeiT-Ti and DeiT-S which have been widely used inresource-limited applications. Although these small ViTs exhibit enhanced efficiency, they lack theflexibility to implement customized adjustments that accommodate dynamically changing resourceconstraints in real-world scenarios, e.g., the computation budget of mobile phones depends on theenergy level (low-power mode) and number of running apps. Consequently, the standard SeparateTraining (ST) protocol trains models with different sizes separately to provide a spectrum of optionswith diversified performance and computation. ST requires repetitive training procedures to producemultiple model choices, and the challenge is amplified for foundation models . Fromusers perspective, they are only offered limited model choices, that might not cater to all scenarios. Analyzing the architectures of ViT-Ti/S/B, we observe that these ViTs are the same architecture withthe only difference in the number of embedding dimensions (we ignore the difference in the numberof heads as it does not impact the overall model size), indicating smaller ViTs are intrinsically thesub-networks of larger model with different widths (see ). This suggests that a large ViT can betransformed to represent small models by uniformly slicing the weight matrix at each layer. Givena width ratio r, we adjust the size of the network by this single hyperparameter, allowing a singleViT to represent multiple small variants with the weights of those sub-networks shared in a nestednature, e.g., ViT-B (r=0.25) equals ViT-Ti and ViT-B (r=0.5) corresponds to ViT-S. In this manner,we empower ViTs for flexible inference capability, and we aim to slice a ViT within a broad slicing",
  ": The available uniform slicingmethod US-Net lags behind Sepa-rate Training (ST) remarkably on ViTs.Performance gaps with ST are shown": "bound and fine-grained slicing granularity so that the diversity and number of sub-networks can beensured for higher flexibility. This problem is non-trivial as fully training all the sub-networks withina constrained budget is nearly infeasible. Consequently, it is quite challenging for these subnets tomatch the performance of separate training. Although various approaches have delved into slicing deep networks for flexible inference, theproblem we target to resolve, i.e., uniformly slicing ViTs within a large slicing bound and fine-grainedslicing granularity, is intrinsically different from others in three perspectives: (1) slicing strategy: asshown in (i), the supernet training techniques in NAS usually slice through multipledimensions with a small slicing bound, resulting in irregularities in model architectures and a minorcomputational adjustment space. (2) slicing granularity: recent width slicing approaches either slice specific portions of the network or utilize a considerably large slicing granularity, leadingto a limited number of models produced. (3) network architecture: US-Net shares a similarvision with us but it has only demonstrated success in the CNN architecture. It is crucial to note the fundamental differences between slicing CNN and ViT: (1) vanilla small ViTssuch as ViT-Ti/S/B, are inherently designed to vary based on widths, aligning with our approach.Conversely, many CNNs are structured to vary from depths, like ResNet-18/34 , and slicing themby width brings unconventional architectures. (2) slimmable CNN necessitates calibration for each sub-network pre-inference due to Batch Normalization , unlike slimmable ViTs that canbe directly utilized for evaluation. (3) the transformer architecture has wider applications thanCNN in this era, e.g., MAE , CLIP , DINOv2 , LLMs . Nevertheless, ViTshave much less image-specific inductive bias than CNN and their slimmable ability remains unclear.As shown in , we empirically implement US-Net on ViT-S and observe substantial performancegaps at most width ratios compared to ST, indicating that the available solution of uniform slicingdoes not work well on the transformer architecture. To investigate the underlying causes of this phenomenon, we conduct analyses in Sec. 3 which arebriefly summarized in two folds: (1) ViTs display minimal interpolation ability, indicating that theoptimization of intermediate subnets falls notably short compared to Separate Training (ST); (2)sustained activation of the smallest sub-network poses a negative effect on other subnets, whichaffects the overall performance as their weights are shared in a nested nature. To resolve these issues,we propose a general framework, named Scala, to enforce ViTs to learn slimmable representation.Specifically, we propose Isolated Activation to disentangle the representation of the smallest sub-network from other subnets while still preserving the lower bound performance. Besides, we presentScale Coordination to ensure each subnet receives simplified, steady, and accurate learning objectives.In this manner, the slimmable ViT can be transformed into multiple smaller variants during inferenceand match the performance of ST. Compared to ST which trains all the subnets individually, Scala reduces the storage and trainingcosts remarkably since the weights of smaller ViTs are shared with the full model and we only needone-shot training without extending the training duration. Further, Scala has a very large slicingbound and fine-grained slicing granularity, enabling diverse sub-network choices during evaluation.In this way, the delivered system can make tailored adjustments that accommodate dynamicallychanging resource constraints in real-world scenarios, promising the application on edge devices.Compared with the prior art SN-Net which supports flexible inference on ViTs, Scala clearlyoutperforms it under different computation with fewer parameters. Moreover, Scala matches the performance of ST on various tasks without modifying the network architecture, demonstratingits generalizability and potential to replace ST as a new training paradigm. The contributions aresummarized as follows: Although slicing ViTs exhibits multiple advantages, we provide detailed analysis andpractical insights into the slimmable ability between different architectures (Sec. 3 andTab. 2) and find slicing the ViT architecture to be the most challenging problem. We propose a general framework Scala to enable ViTs to learn slimmable representation forflexible inference. We present Isolated Activation to disentangle the representation of thesmallest subnet and Scale Coordination to ensure each subnet receives simplified, steady,and accurate signals. Comprehensive experiments on different tasks demonstrate that Scala, requiring only one-shot training, outperforms prior art and matches the performance of ST, substantially reducesthe memory requirements of storing multiple models.",
  "Related Work": "Scaling Up ViTs. Like Transfromer in NLP, scalability and performance improvements inViTs have been a central focus of recent research. Specifically, strategies have been explored toscale the depth of ViT and it is scaled to even larger sizes with almost 2 billion parametersand reaches new state-of-the-art results . Afterward, ViTs have been scaled up to 4 billion and 22 billion parameters with extraordinary performance and enormous costs. Scaling Down ViTs. The advent of ViTs has also sparked interest in scaling down these models.Techniques such as knowledge distillation have been explored to reduce the ViT model size . For example, DeiT presents smaller ViTs with 5M parameters. Additionally, researchershave explored quantization methods to further compress ViTs for deployment on edgedevices. Unfortunately, these static models cannot make customized adjustments for resource-changing environments in real scenarios. Slimmable Neural Network. The derivation of multiple smaller models from a single network hasbeen previously explored but most works focus on the CNN structure. Slimmable Networks and its variants train a shared network which adapts the width to accommodate the resourceconstraints during inference. Later, this idea is adapted into two-stage NAS methods forsupernet training. The supernet is scaled at multiple dimensions with a small computation change, incontrast to our work where we only scale the width dimension with a large slicing bound. SN-Net is a recently proposed method that constructs a supernet with several pre-trained models and insertslinear layers to build dynamic routes for flexible inference. Recently, several of these techniqueshave been extended to Transformer architecture , while they either scale part of the networkor the slicing granularity is large which means they could only deliver very few models in the end.Differing from the previous works, our method is the first work to scale the ViT structure with largeslicing bound and small slicing granularity which is intrinsically a more challenging problem.",
  "Revisiting Slicing in Vision Transformer": "Due to the excessive costs of constantly activating all the sub-networks during training, the sandwichrule is proposed in US-Net to train the slimmable network at the smallest width, largest width,and 2 random intermediate widths in each iteration so that the performance of the lower bound andupper bound are guaranteed. Although the intermediate sub-networks are optimized less frequentlycompared to Separate Training (ST), US-Net manages to achieve comparable performance with STon the CNN architecture. To have a better understanding of the distinction between CNN and ViT,we apply US-Net to MobileNetV2 , a CNN, and DeiT-S , a ViT, but constantly activatefour sub-networks with the width ratio of {0.25, 0.5, 0.75, 1.0} at each iteration. Subsequently, weevaluate the pre-trained models at both inbound [0.25, 1.0] and outbound (0, 0.25) unseen widthratios to evaluate their interpolation and extrapolation abilities, respectively. Shown in , CNNexhibits moderate interpolation and extrapolation capabilities by achieving acceptable performance atpreviously unobserved widths during training. In stark contrast, ViT fails entirely at unseen widths,suggesting that optimizing larger sub-networks does not directly benefit the performance of smallerViTs, even though their weights are shared in a nested nature. 0.20.30.40.5",
  ": We train US-Net on ViT-S without con-stantly activating the smallest subnet (denoted as*) and observe an average performance gain of1.8% at other width ratios on ImageNet-1K": "We analyze the results from the expected training epochs for each sub-network. Let X represent thetotal number of networks to be delivered, wherein X 2 intermediate sub-networks are included, andaccording to the sandwich rule, two of these are randomly sampled during each iteration. Formally,the expected training epochs for the intermediate networks can be expressed as:",
  "X 2 ,(1)": "where is the number of training epochs for the full model and it suggests that the optimization ofmost sub-networks falls notably short compared to ST. As ViT has demonstrated minimal interpolationability at unseen widths compared to CNN, each sub-network within the slimmable ViT requiresoptimal utilization of every training iteration to achieve satisfactory performance. Nevertheless,the smallest subnet is constantly activated during training according to the sandwich rule and wehypothesize that the over-emphasis of the smallest sub-network, often exhibits the worse performance,may increase the training difficulty of other subnets as their weights are shared in a nested nature.To validate it, we implement US-Net on DeiT-S without constantly activating the smallestsub-network. verifies our hypothesis showing an accuracy drop at the smallest subnet but asignificant performance improvement at other width ratios.",
  "Scala": "We first introduce the training and inference paradigms of Scala. Then, we describe Isolated Activationwhich disentangles the smallest subnet from other sub-networks while maintaining the lower boundperformance. Further, we present Scale Coordination to ensure each subnet receives simplified,accurate, and steady learning objectives. Without any modification to the architecture, we deliver ageneral framework Scala which could be easily built on existing methods.",
  "Framework": "Our goal is to build a general framework that makes a ViT F () slimmable, i.e., the deliverednetwork can be transformed into different small variants for flexible inference. First, we introducea hyperparameter r to denote the width ratio of the sub-network F r (). Based on our analysis in, ViTs have minimal interpolation ability, which suggests that all subnets have to be individuallyoptimized to achieve decent performance. Following the sandwich rule , we sample the smallests, largest l (l = 1), and 2 random intermediate width ratios m1, m2 at each iteration during training.The corresponding sub-networks are: F s (), F l (), F m1 () and F m2 (). and we accumulate thegradients of those subnets at each iteration. At the inference stage, the network F () is evaluated atan arbitrary width ratio that has been optimized during training by adjusting r.",
  "Isolated Activation": "Illustrated in Sec. 3, constant activation of the smallest sub-network F s () ensures its own accuracyat the cost of other subnets performance. This is a dilemma as there is a significant accuracy dropof F s () if we do not constantly activate it (see ), otherwise, the performance of other sub-networks are severely limited. To alleviate this issue, we propose Isolated Activation to disentangle the representation of F s () from other sub-networks while still constantly activating it. It not onlyensures the performance of the lower bound but facilitates the optimization of other subnets as well. Formally, given the learnable weight RCoCiHW of a random layer in ViT (Co, Ci stands forthe output, input channel number, H, W represents the height and width of the convolution kernel,H = W = 1 for fully connected layers), the weight of F r () where r = s is selected as:",
  "Scale Coordination": "We present the training strategy of Scala in this section. We follow the setting of DeiT to trainthe full model F l () with knowledge distillation and introduce a distillation token for knowledgetransfer between sub-networks. As our goal is to scale down a given network F () to multiple smallervariants, we simply choose the pre-trained model itself F () as the external teacher for the fullnetwork F l () to facilitate training. To optimize the sub-networks at different scales, we present theScale Coordination training strategy, which is composed of three techniques: Progressive KnowledgeTransfer, Stable Sampling, and Noisy Calibration, to ensure that each subnet receives simplified,accurate, and steady learning objectives. Progressive Knowledge Transfer. Given an input image v, the activated sub-network F r () producestwo predictions:prcls, prdis = F r (v; r) ,(4)where prcls and prdis denote the prediction generated by the classification and distillation head,respectively. As we activate multiple sub-networks: F s (), F l (), F m1 () and F m2 () at eachiteration during training, our idea is to utilize the predictions of the larger network to facilitate theoptimization of smaller subnets.",
  ",(5)": "where K represents the number of classes, r = R [ (r) + 1] and (r) denotes the index of r inR. Instead of using pldis as the optimization target for all smaller networks, we ensure each subnetreceives simplified learning objective as small subnet have large capacity gap compared to F l () (e.g.,F l () is almost 16 times larger than F s () if s = 0.25) and minimizing their KL loss complicatesthe optimization process and leads to inferior performance. With Progressive Knowledge Transfer,we simplify the optimization objective for small sub-networks by utilizing F m1 () and F m2 () asthe teacher assistants to fill the gap between F l () and F s () and train them in a one-shot manner. Stable Sampling. As the knowledge is gradually transferred from the larger network to the smallerone, the two intermediate networks serve as the bridge to connect F l () and F s (), as F m2 () is thestudent of F l () and F m1 () is the teacher of F s (). Therefore, we need to carefully control thewidth ratios m1 and m2 to prevent the obvious model capacity variation.",
  "Experiments": "We validate Scala with the plain ViT structure DeiT . We first analyze of the main property ofScala and compare our method with the state-of-the-art method SN-Net and Separate Training(ST) at a larger scale. Moreover, we examine the transferability of Scala and its application onSemantic Segmentation. Finally, we provide ablations to validate the efficacy of our designs.",
  "Experiment Settings": "All the object recognition experiments are carried out on ImageNet-1K . We follow the trainingrecipe of DeiT and conduct the experiments on 4 V100 GPUs. For Scala, we set s = 0.25,l = 1.0, and = 0.0625 so that we could enable a single ViT to represent 13 different networks(X = 13) with a large slicing bound (i.e., F l () is almost 16 times larger than F s ()).",
  "In this part, we conduct experiments over DeiT-S for 100-epoch training to prove the concept": "Comparison with scaling baselines. We compare Scala with multiple scaling baselines, including:(1) AutoFormer : we apply this ViT-based supernet training method into our setting to scalethrough width; (2) US-Net : the prior work that obtains similar performance with ST over theCNN structure; (3) Separate Training (ST): we repetitively train the model with different widths fromscratch and evaluate them individually. Tab. 1 shows that AutoFormer lags behind Scala remarkablyas we target to scale in a wider range. US-Net shows significantly worse performance comparedto ST which indicates that scaling down ViT is a more challenging problem compared to the CNNarchitecture. Nevertheless, Scala achieves better performance compared to ST at all width ratios withone-shot training, reducing the storage costs of saving multiple models observably. 0.00.51.01.52.02.53.03.54.04.5 GFLOPs Acc (%) STScala (X=25)Scala (X=13)Scala (X=7)Scala (X=4)",
  ": Comparisons of Scala and SeparateTraining (ST) over lightweight model Uniformer-XS with token pruning. Improvements overST are shown": "Slicing Granularity and Bound. shows the results of various slicing granularity . First,Scala outperforms ST with different and the advantage at small width ratios is more obvious, whichpromises its application on edge devices. Moreover, it is shown that less fine-grained granularity results in better overall performance with the same slicing bound as the expected training epochs for intermediate subnets increase correspondingly. We further conduct experiments with different swhile fixing l and to study the effect of the slicing bound. shows that smaller bounds lead tomarkedly better performance and it further verifies that slicing through a large bound is intrinsicallymore difficult, which distinguishes Scala from the supernet training methods in NAS. Application on Hybrid Structures. We experiment Scala on the CNN-ViT hybrid architectureUniformer-S . As Uniformer contains Batch Normalization (BN) which cannot be directlyevaluated after slicing because of normalization shifting , we calibrate the statistics of BN beforeinference following . Shown in , Uniformer-S is scaled down to 13 different variants withbetter performance compared to ST, demonstrating the generalization ability of Scala. However,performing BN calibration at each width ratio requires considerable extra effort. This highlights thebenefit of ViT, as Layer Normalization (LN) allows direct evaluation without additional operations. Application on Lightweight Structures.We further validate Scala on lightweight structureUniformer-XS which integrates the design of token pruning and train these methods for 150epochs. Shown in , Scala still matches the performance of ST and exhibits a significantadvantage at small width ratios, which promises its application on edge devices with a limited budget. Fast Interpolation of Slimmable Representation. Training models with different slicing granularity from scratch is time-consuming and here we show that the slimmable representation of certaingranularity can be scaled to others with a small amount of training epochs. Specifically, we train themodel with the original for 70 epochs and decrease the value of in the last 30 epochs to delivermore sub-networks for higher inference flexibility. shows the results of fast interpolation aresimilar to those trained from scratch and the newly appeared sub-networks are quickly interpolated toachieve decent performance. We further increase for sub-networks with higher performance and thephenomenon shown in is similar to down interpolation. Besides, we observe that the accuracyof abandoned sub-networks gradually decreases but they maintain the performance to a great extent.",
  "CNNMobileNet v258.860.460.761.661.864.364.4": "ratios to explore the interpolation ability. CNN exhibits very strong interpolation ability as theperformance at unseen widths lies in the range of trained width ratios. In contrast, CNN-ViT andViT suffer from remarkable performance decreases to different extents and ViT achieves almost zeroaccuracy which further validates that the problem we target to solve, i.e., slicing ViT, is the mostchallenging one.",
  "In this section, we perform training over DeiT-B for 300-epoch training following the standardprotocol on ImageNet-1K to compare with the state-of-the-art": "Comparisons with state-of-the-art. SN-Net is state-of-the-art work that supports flexibleinference on ViT. Specifically, it utilizes several pre-trained models (e.g., DeiT-Ti/S/B) to constructa supernet and inserts additional layers to build dynamic routes for flexible inference. Shown inTab. 3, we empirically compare Scala with SN-Net over DeiT-B following the standard 300-epochtraining protocol . Scala obtains similar performance with SN-Net at large width ratios and clearlyoutperforms it at small computational budgets. Besides, SN-Net has to preserve the parameters ofmultiple models and additional layers, while Scala only needs to keep the weights of the full network.When adopting the stronger teacher network as SN-Net does, Scala outperforms SN-Net with anaverage improvement of 1.6% across all width ratios. Comparisons with Separate Training. In Tab. 4, we compare with ST on DeiT-B with longertraining process, i.e., 300-epoch training, where r = 0.25, 0.50, 1.00 corresponds to DeiT-Ti, DeiT-Sand DeiT-B, respectively. Scala exhibits a clear advantage at r = 0.25 and matches the performanceof ST except r = 0.50 due to significantly less training time. When X = 7, we can achieve similarperformance at r = 0.50 with 40% training epochs of ST. Further reducing X to 4, resulting in theconstant activation of the two intermediate networks, allows us to consistently outperform ST at allwidth ratios. This substantiates the effectiveness of Scala and the slimmable representation.",
  "Transferability": "To assess the transferability of Scala, we employ DeiT-B as the backbone for a 300-epochpre-training on ImageNet-1K and leverage the foundation model DINOv2-B as the teachernetwork to inherit good behaviors. Our study aims to address two key questions: Whether the slimmable representation can be transferred to downstream tasks? As depictedin a, Scala consistently outperforms Separate Training (ST) across all width ratios, despitethe intermediate sub-networks being trained for approximately 55 epochs. After that, we conductlinear probing on video recognition dataset UCF101 with 8 evenly sampled frames and average theirfeatures for the final prediction. For the classification head added on Scala, we make it slimmable tofit the features with various dimensions and follow the same training protocol as in object recognition.In b, two notable observations emerge: (1) Scala consistently outperforms ST across differentwidth ratios on the UCF101 dataset, implying the great transferability of the slimmable representation;(2) Scala retains its slimmable ability when applied to a new task and exhibits promising performanceacross a wide slicing range (10141 GFLOPs), promising its application on other downstream tasks. GFLOPs Acc (%) STScala",
  "(b) Linear probing on UCF101 with 8 sampled frames": ": Transferability of Scala. We first conduct pre-training on ImageNet-1K with the help offoundation model DINOv2-B . Then we conduct linear probing on video recognition datasetUCF101. Improvements over ST are shown. Whether the generalization ability can be maintained in the slimmable representation? Inspiredby the work which replicates the success of vision foundation models on ImageNet-1K, weremove all the Cross-Entropy losses during training to alleviate the dataset bias issue and inheritthe strong generalization ability of the teacher network DINOv2. Then we conduct linear probingon 12 fine-grained classification datasets following the setup in DINOv2. Tab. 5 shows that Scalasignificantly outperforms DeiT variants on the average performance of fine-grained classificationwhich suggests that Scala indeed inherits the fruitful knowledge from DINOv2 with remarkableimprovement in its generalization ability. Moreover, the improvement over DeiT does not decreasewhen we scale down the width ratios during inference and it indicates that Scala maintains the flexibleinference capability very well even though it contains more knowledge than before. : Comparison of Scala and DeiT on 12 fine-grained classification datasets. Scala-B is distilledfrom DINOv2-B on ImageNet-1K for 300 epochs and then we conduct linear probing on fine-graineddatasets. The average accuracy of 12 datasets is shown in the last column.",
  "Scala58.763.468.371.373.374.476.1w/o IA57.361.566.469.872.073.475.8w/o PKT53.060.165.668.871.773.776.2w/o SS58.762.768.171.373.174.275.9w/o NC50.262.767.270.572.774.076.3": "semantic segmentation. We utilize the pre-trained model Uniformer-S drawn from , whichhas a hierarchical design and is obtained by 100-epoch training (our results lag behind official resultswhere the backbone is trained for 300 epochs), and equip it with Semantic FPN . To compare withSeparate Training (ST), we extract four subnets from Scala (Uniformer-S) and train them separately.Shown in Tab. 6, Scala outperforms ST at all widths which verifies the slimmable representationbenefits the downstream tasks. Note that we do not scale the decoder as it involves extra designsand is out of the scope of this work. However, we show that the slimmable representation can begeneralized to semantic segmentation as feature extractors because the feature maps are spatiallyintact, promising its application as an end-to-end slimmable framework on dense prediction tasks.",
  "Ablation Study": "We conduct ablation to examine the effectiveness of our designs in Tab. 7. First, we build Scalawithout Isolated Activation so that the smallest sub-network will entangle with others and it showsan obvious performance drop at all width ratios. Then, we remove Progressive Knowledge Transfer(PKT) and pass the knowledge from F l () to smaller subnets through classification token followingUS-Net . It shows much worse performance, especially at small ratios, which proves the strengthof PKT as it implicitly introduces some teacher assistants to simplify the optimization objective forsmall sub-networks. Further, we random sample the width ratios of m1 and m2 between (s, l) andcompare it with Stable Sampling (SS). The results are slightly inferior to SS which suggests SS ishelpful in securing the steady learning objective for each sub-network. Finally, we remove NoiseCalibration (NC) from Scala and only use the predictions from larger networks to guide the smallsubnets. It shows remarkable performance drops at small width ratios, where the noise from theteacher network is most obvious, demonstrating the effectiveness of NC in calibrating the noise andproviding accurate signals for sub-networks.",
  "Conclusion and Limitations": "In this paper, we observed that smaller ViTs are intrinsically the sub-networks of a large ViT withdifferent width ratios. However, slicing ViT is very challenging due to its poor interpolation ability. Toaddress this issue, we proposed Scala to enable a single network to represent multiple smaller variantswith flexible inference capability. Specifically, we proposed Isolated Activation to disentangle therepresentation of the smallest subnet from others and presented Scale Coordination to ensure thesub-network receives simplified, steady, and accurate learning objectives. Extensive experimentson different tasks prove that Scala, requiring only one-shot training, outperforms the state-of-the-art method under different computations and matches the performance of Separate Training withsignificantly fewer parameters, promising the potential as a new training paradigm. One limitation of Scala is the longer training time compared to conventional supervised learning ofa single model, attributable to the activation of multiple subnets during training. Nevertheless, ourtraining time is obviously less than separately training all the sub-networks. In the future, we aim toenhance the training efficiency of Scala.",
  "H. Touvron, M. Cord, A. Sablayrolles, G. Synnaeve, and H. Jgou. Going deeper with imagetransformers. In ICCV, 2021": "H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozire, N. Goyal,E. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. arXivpreprint arXiv:2302.13971, 2023. H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra,P. Bhargava, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXivpreprint arXiv:2307.09288, 2023.",
  "A.1Implementation Details": "For Separate Training (ST), we follow the exact training strategy of the official DeiT andUniformer setting. We use random horizontal flipping, random erasing , Mixup ,CutMix , and RandAugment for data augmentation. AdamW is utilized as the optimizerwith a momentum of 0.9 and a weight decay of 0.05. We set the learning rate to 1e-3 and decay witha cosine shape. The models are trained on 4 V100 and 8 A100 GPUs with a total batch size of 1024.We adopt Exponential Moving Average (EMA) following the official setting. While we utilize the pre-trained ST model (r = 1.00) as the teacher for F l () to facilitate trainingas mentioned in the Scale Coordination section in the main paper, we adopt a larger learning rate(2e-3) and mild data augmentation (reduce the magnitude for RandAugment to 1 and turn offrepeated augmentation) because Scale Coordination already regularizes the network training strongly.At every training iteration, we activate four sub-networks separately based on Stable Sampling andaccumulate their gradients for backpropagation. The rest hyperparameters are set as the same as thosein Separate Training.",
  "A.2Slimmable Ability of Vanilla Representation": "Tab. 1 shows that ViT is not slimmable if we directly evaluate the vanilla pre-trained model atother widths. Here, we further explore the slimmable ability of vanilla representation and fine-tunethe vanilla pre-trained model with Scala. Tab. 8 shows that fine-tuning obtains obviously worseperformance at small width ratios compared to training from scratch, which denotes that the vanillarepresentation is not slimmable and is essentially different from the slimmable representation.",
  "A.3Larger Slicing Bound": "As discussed in the main text, the slicing bound has a huge impact on the performance of Scala. Herewe further expand the slicing bound from [0.25, 1.00] to [0.125, 1.000]. As shown in , Scalasuffers from an obvious performance drop at r = 0.25 as it is not constantly activated in the newsetting. Nevertheless, our method still manages to outperform ST at all width ratios and shows asignificant advantage at the smallest ratio r = 0.125. 0.00.51.01.52.02.53.03.54.04.5 GFLOPs Acc (%) STScala (s=0.125)Scala (s=0.25)Scala (s=0.50)Scala (s=0.75)",
  "Scala (X=13)40075.7%79.5%7381.5%7382.3%Scala (X=7)40076.0%80.1%16081.7%16082.4%": "networks training time is much less. We further extend the training process to 400 epochs in thissection and the results are shown in Tab. 9. The overall performance at various width ratios isimproved with longer training and Scala (X = 7) outperforms ST at all widths even though theexpected training epochs for intermediate sub-networks are still much less than ST.",
  "Scala+(a)58.763.468.371.373.374.476.1Scala+(b)57.661.364.972.474.174.776.1Scala+(c)58.363.367.269.472.072.974.8": "We validated the effectiveness of Isolated Activation by removing this component and we furtherconduct more ablation studies on the activation methods where the designs are illustrated in .As shown in Tab. 10, choice (b) leads to slightly better performance at F m2 (), but the performancedrops at F m1 () significantly as it is entangled with F s () and the over-emphasize of F s () adverselyaffect its performance. On the other hand, choice (c) results in a similar performance at F m1 (),but the accuracy decreases significantly at F m2 () which further verifies our hypothesis that F s ()should be isolated to reduce its negative impact on other sub-networks.",
  "A.6Verification of Slimmable Ability": "In the main text, we found that ViT has the minimal interpolation ability compared to the CNNstructure. This suggests that optimizing larger sub-networks does not directly contribute to theperformance improvement of smaller variants, even though their weights are shared in a nested nature.A further question is, whether ViT can still maintain the slimmable ability for the unseen width ratiosduring training. To verify this point, we respectively fix the width ratios of m2, m1 to 0.8125 and 0.4375 duringtraining, so that only one sub-network is optimized at each range. shows that the accuracy ofunseen sub-networks is very low due to the lack of interpolation ability. Nevertheless, the performance 0.00.51.01.52.02.53.03.54.04.5 GFLOPs Acc (%) STScalaScala (fix m2) Scala (fix m1)",
  "Scala22M58.7%1000.468.3%181.373.3%182.776.1%1004.6": "While we have shown that Scala outperforms baseline methods US-Net and Separate Training(ST), we further compare Scala with much stronger baselines, by adding an external teacher totheir full network during training. Specifically, we adopt the pre-trained full model from ST as theteacher and conduct knowledge distillation for models with different widths separately. Tab. 11shows that ST+KD exhibits similar performance at larger width ratios with Scala, despite that Scalaclearly outperforms ST+KD at smaller widths, promising its application on edge devices. Althoughobtaining a better full model, US-Net+KD exhibits worse performance at smaller width ratios becauseit utilizes the full network as the teacher for all subnets and this phenomenon verifies our motivationof proposing Progressive Knowledge Transfer.",
  "A.8Comparisons in Training Time": "Assuming to deliver 13 models in the end, we compare the training time (100 Epoch) of Scala withUS-Net , Separate Training on 8 A100 GPUs. The difference between US-Net and Scala is notlarge as the transformer architecture has been well-optimized on GPU and we do observe a significanttime gap between Scala and Separate Training as they have to train 13 models iteratively. Moreover,Scala can be configured to deliver 25 models without an increase in training time as we sample 4networks at each iteration in all scenarios which further highlights our strengths.",
  "A.9Comparisons with MatFormer": "MatFormer only slices the FFN block in the transformer architecture so it offers a minorcomputational adjustment space and we adapt their method on DeiT-S to compare with Scala. shows that Scala achieves comparable performance with it (better in most cases) when s = 0.5 witha larger adjustment scope. : Comparison of Scala and MatFormer over DeiT-S. Scala offers a significantly broaderscope for computational adjustment compared to MatFormer as MatFormer only scales the FFNblock in ViT. The right figure provides a detailed magnification of the left figure.",
  "The answer NA means that the paper has no limitation while the answer No means thatthe paper has limitations, but those are not discussed in the paper": "The authors are encouraged to create a separate \"Limitations\" section in their paper. The paper should point out any strong assumptions and how robust the results are toviolations of these assumptions (e.g., independence assumptions, noiseless settings,model well-specification, asymptotic approximations only holding locally). The authorsshould reflect on how these assumptions might be violated in practice and what theimplications would be. The authors should reflect on the scope of the claims made, e.g., if the approach wasonly tested on a few datasets or with a few runs. In general, empirical results oftendepend on implicit assumptions, which should be articulated. The authors should reflect on the factors that influence the performance of the approach.For example, a facial recognition algorithm may perform poorly when image resolutionis low or images are taken in low lighting. Or a speech-to-text system might not beused reliably to provide closed captions for online lectures because it fails to handletechnical jargon.",
  "If applicable, the authors should discuss possible limitations of their approach toaddress problems of privacy and fairness": "While the authors might fear that complete honesty about limitations might be used byreviewers as grounds for rejection, a worse outcome might be that reviewers discoverlimitations that arent acknowledged in the paper. The authors should use their bestjudgment and recognize that individual actions in favor of transparency play an impor-tant role in developing norms that preserve the integrity of the community. Reviewerswill be specifically instructed to not penalize honesty concerning limitations.",
  ". Experimental Result Reproducibility": "Question: Does the paper fully disclose all the information needed to reproduce the main ex-perimental results of the paper to the extent that it affects the main claims and/or conclusionsof the paper (regardless of whether the code and data are provided or not)?Answer: [Yes]Justification: We have included the implementation details in the main text and appendix.Guidelines: The answer NA means that the paper does not include experiments. If the paper includes experiments, a No answer to this question will not be perceivedwell by the reviewers: Making the paper reproducible is important, regardless ofwhether the code and data are provided or not.",
  "If the contribution is a dataset and/or model, the authors should describe the steps takento make their results reproducible or verifiable": "Depending on the contribution, reproducibility can be accomplished in various ways.For example, if the contribution is a novel architecture, describing the architecture fullymight suffice, or if the contribution is a specific model and empirical evaluation, it maybe necessary to either make it possible for others to replicate the model with the samedataset, or provide access to the model. In general. releasing code and data is oftenone good way to accomplish this, but reproducibility can also be provided via detailedinstructions for how to replicate the results, access to a hosted model (e.g., in the caseof a large language model), releasing of a model checkpoint, or other means that areappropriate to the research performed. While NeurIPS does not require releasing code, the conference does require all submis-sions to provide some reasonable avenue for reproducibility, which may depend on thenature of the contribution. For example(a) If the contribution is primarily a new algorithm, the paper should make it clear howto reproduce that algorithm.",
  "(b) If the contribution is primarily a new model architecture, the paper should describethe architecture clearly and fully": "(c) If the contribution is a new model (e.g., a large language model), then there shouldeither be a way to access this model for reproducing the results or a way to reproducethe model (e.g., with an open-source dataset or instructions for how to constructthe dataset). (d) We recognize that reproducibility may be tricky in some cases, in which caseauthors are welcome to describe the particular way they provide for reproducibility.In the case of closed-source models, it may be that access to the model is limited insome way (e.g., to registered users), but it should be possible for other researchersto have some path to reproducing or verifying the results.",
  "Guidelines:": "The answer NA means that the paper does not use existing assets. The authors should cite the original paper that produced the code package or dataset. The authors should state which version of the asset is used and, if possible, include aURL. The name of the license (e.g., CC-BY 4.0) should be included for each asset. For scraped data from a particular source (e.g., website), the copyright and terms ofservice of that source should be provided. If assets are released, the license, copyright information, and terms of use in thepackage should be provided. For popular datasets, paperswithcode.com/datasetshas curated licenses for some datasets. Their licensing guide can help determine thelicense of a dataset.",
  "According to the NeurIPS Code of Ethics, workers involved in data collection, curation,or other labor should be paid at least the minimum wage in the country of the datacollector": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with HumanSubjectsQuestion: Does the paper describe potential risks incurred by study participants, whethersuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)approvals (or an equivalent approval/review based on the requirements of your country orinstitution) were obtained?Answer: [NA]Justification: Our paper does not involve crowdsourcing nor research with human subjects.Guidelines:",
  "The answer NA means that the paper does not involve crowdsourcing nor research withhuman subjects": "Depending on the country in which research is conducted, IRB approval (or equivalent)may be required for any human subjects research. If you obtained IRB approval, youshould clearly state this in the paper. We recognize that the procedures for this may vary significantly between institutionsand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and theguidelines for their institution."
}