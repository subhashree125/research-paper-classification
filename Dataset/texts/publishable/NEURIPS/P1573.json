{
  "KITTI": "nuS Waymo* S3DIS* OneDet3D (ours) : The high-level overview comparing the multi-domain joint training performance of 10existing 3D detectors and our OneDet3D. These models are jointly training on the indoor datasetsSUN RGB-D (SUN), ScanNet, and outdoor datasets KITTI, nuScenes (nuS). We also evaluate thecross-domain performance on the indoor S3DIS and outdoor Waymo datasets. The center of thecircle means that the corresponding metric is less than 10%, and the outermost means 90%. Existingindoor detectors are plotted in red, outdoor detectors are in green, and detectors that aim for differentscenes are in orange. Our model has the remarkable capacity to generalize across a wide range ofdiverse 3D scenes (a larger polygon area) with only one set of parameters and the same architecture.",
  "Abstract": "The current trend in computer vision is to utilize one universal model to address allvarious tasks. Achieving such a universal model inevitably requires incorporatingmulti-domain data for joint training to learn across multiple problem scenarios. Inpoint cloud based 3D object detection, however, such multi-domain joint training ishighly challenging, because large domain gaps among point clouds from differentdatasets lead to the severe domain-interference problem. In this paper, we pro-pose OneDet3D, a universal one-for-all model that addresses 3D detection acrossdifferent domains, including diverse indoor and outdoor scenes, within the sameframework and only one set of parameters. We propose the domain-aware parti-tioning in scatter and context, guided by a routing mechanism, to address the datainterference issue, and further incorporate the text modality for a language-guidedclassification to unify the multi-dataset label spaces and mitigate the category inter-ference issue. The fully sparse structure and anchor-free head further accommodatepoint clouds with significant scale disparities. Extensive experiments demonstratethe strong universal ability of OneDet3D to utilize only one trained model foraddressing almost all 3D object detection tasks ().",
  "(b)": ": Illustration of existing 3D detectors (a) and ours(b). Existing detectors can be divided into point-based (up)and voxel-based (down). Our model has the capacity for jointtraining on multi-domain point cloud data. 3D point cloud based object detec-tion aims to predict the oriented 3Dbounding boxes and the correspond-ing semantic category tags for thereal scenes given a point set. Unlikemature 2D detectors ,which once trained, can generally con-duct inference on different types ofimages in various scenes and envi-ronments, current 3D detectors stillfollow a single-dataset training-and-testing paradigm, i.e., point cloudsused during inference should be fromthe totally same domain as that usedduring training. Whether indoor or outdoor ,existing point cloud based 3D detectors can only be trained on datasets from one specific domain, thenbe tested on the same domain data. Such restriction of training and testing on a single dataset severelyhampers the generalization ability of 3D detectors, resulting in a significant lag in the progress of 3Ddetection compared to 2D in terms of universality. To address this issue, multi-domain joint training (i.e., multi-dataset joint training) should be intro-duced into point cloud based 3D object detection, to allow 3D detectors to learn from point clouds ofdifferent domains through large-scale joint training. In this way, a 3D detector, once trained, can wellgeneralize across various domains of point clouds. The ultimate goal is to obtain a 3D detector thatcan support unified 3D object detection across different domains with only one set of parameters,thereby achieving the target of universal 3D object detection. The motivation of multi-domain joint training is to learn universal 3D knowledge by leveraging pointclouds from different sources and domains, thereby establishing a general representation from 3Ddata to 3D spatial positions. Through this, a model independent of point cloud source, collection, anddomain can be achieved. With common 3D knowledge from diverse point clouds, it can effectivelyserve as a universal 3D detector and function as a 3D foundation model. However, achieving thisis highly challenging and difficult. As can be seen in , due to the significant domain gaps(e.g., point cloud ranges, scenes, object sizes, sparsity, etc.), existing 3D detectors fail to support this.Specifically, current 3D detectors can be generally divided into point-based and voxel-based ones.For point-based 3D detectors (the upper part of a) , it is difficult to apply the samesampling and grouping technique for different domain data. For voxel-based 3D detectors (the lowerpart of a) , which usually require converting from sparse to dense features for3D box prediction, the scale differences between indoor and outdoor point clouds make it difficult torepresent them using dense features of the same size. This consequently limits existing models tolearning domain-specific knowledge, restricting their ability to acquire generalized 3D knowledge. In this paper, we propose OneDet3D, a unified point cloud based 3D detector with only one set ofparameters through multi-domain joint training. As in b, we employ 3D sparse convolution forfeature extraction, which is more robust to domain gaps compared to point-based feature extractors , making it well-suited for adapting to point clouds from different domains. Subsequently, weutilize an anchor-free detection head, where objects are represented by center points ,enabling direct compatibility with sparse convolution and avoiding the constraints of fixed-size densefeatures. Such a fully sparse structure, together with the anchor-free detection head using center pointrepresentation, provides an effective architecture for multi-domain joint training. Based on the model architecture, during multi-domain joint training, the domain-interference issueshould be further addressed. This issue primarily comprises two aspects: data-level interferencecaused by differences in point clouds themselves, and category-level interference caused by labelconflict among categories across different domains. To mitigate the data-level interference, weemploy domain-aware partitioning, which partitions parameters where the interference problemmainly exists to be domain-specific and keeps the vast majority shared among different domains. Thedata-level interference can thus be effectively prevented without increasing the model complexity too much. Specifically, we partition re-scaling in normalization layers to maintain the consistency ofthe data scatter, and parameters about context learning for reducing the effect of range disparities.They are guided by a domain router implemented by a domain classifier. To alleviate category-levelinterference, we employ language-guided classification, leveraging the text modality to alleviateconflict issues. We utilize a combination of fully connected layers and sparse convolution forclass-specific and class-agnostic classification to ensure compatibility with the anchor-free head.",
  "Our main contributions can be summarized as follows:": "We propose OneDet3D, a multi-domain point cloud joint training model for universal 3D objectdetection. To the best of our knowledge, this is the first 3D detector that supports point cloudsfrom domains in both indoor and outdoor simultaneously with only one set of parameters. We propose the domain-aware partitioning in scatter and global context, guided by the domainrouting mechanism. In this way, the data-level interference issue caused by point cloud disparitiescan be alleviated during multi-dataset joint training. We integrate the text modality into the anchor-free head classification. Through employing bothfully connected layers and 3D sparse convolution for the dual-level of class-agnostic and class-specific classification, the issue of category-level interference can be mitigated. Extensive experiments demonstrate the one-for-all ability of our OneDet3D. OneDet3D possesses thestrong generalization ability in both category and scene, thus effectively achieving the goal of universal3D object detection. In the close-vocabulary setting, it achieves comparable performance using onlyone set of parameters. In the open-vocabulary setting, it obtains more than 7% performance.",
  "Related Work": "3D object detection aims to predict category tags and oriented 3D bounding boxes for the scene. Weprimarily discuss methods where point clouds serve as the input. Current 3D detectors can be generallycategorized into point-based and voxel-based methods. Point-based methods usuallyextract point-wise features, then perform clustering and classification for detection. Voxel-basedmethods usually extract voxel-wise features using 3D sparse convolution,then convert them into dense 3D features for 3D box prediction. Considering the disparities in pointclouds, existing 3D detection methods are also separated into indoor 3D detectors and outdoor ones , where totally different model architectures are utilizedfor each. Recently, proposes a unified model architecture for both indoor and outdoor 3Ddetection. However, these methods still follow a single-dataset training-and-testing paradigm, andcannot address 3D detection for point clouds from various domains with one set of parameters. Multi-dataset training aims to involve multiple datasets from various domains in training, so thatthe model can generalize in multi-domain data at the inference time. Since RGB images mainly differin content, while the structural differences in images themselves are not significant, multi-datasettraining has been widely studied in the field of 2D object detection . In comparison,substantial differences inherently exist in the point clouds themselves, making multi-dataset trainingmore challenging in the 3D object detection task. Some recent works have studiedthis problem. However, they only address multi-dataset training within either indoor or outdoorscenes and cannot handle multiple datasets simultaneously from both indoor and outdoor scenes.For example, and deal with multi-dataset training with RGB images, and focuson outdoor-only multi-dataset training, where the discrepancies between different datasets are farless than those between indoor and outdoor point clouds. OneDet3D demonstrates that despite thesesubstantial differences, 3D detection can still be addressed with a universal solution. This is a crucialadvancement for generalization in the 3D domain.",
  "Preliminary": "Given a point cloud x, 3D object detection aims to predict its label y, which consists of the categorytags and 3D bounding boxes. Multi-domain (i.e., multi-dataset) data are utilized during training.Denote the domain as D and the total number of domains as N, the total training data can thus bedenoted as D = {Dn = {(x(n), y(n))}}Nn=1. The purpose of multi-domain joint training is to train aunified model from all these domains, which can obtain the minimum prediction error on all differentdomains D. The obtained 3D detector should also generalize well on new domains.",
  "In 3D object detection, the following two-level interference exists among different domain pointclouds, making multi-domain joint training highly challenging:": "Data-level interference. As in Tab. 1, it can be observed that sensors for collecting indoor andoutdoor point clouds exhibit fundamental differences, resulting in significant disparities in the rangecovered by the point clouds, with differences exceeding 10 to nearly 20 times. This also leadsto substantial differences in object sizes and sparsity within the scenes. Because of such scaledifferences, it is challenging to utilize the same point-wise clustering technique or feature map withthe fixed size during joint training for point clouds from different scenes. Even among datasets thatbelong to the same category of indoor or outdoor point clouds, there are still slight differences in thesensors used for collection. For instance, SUN RGB-D points are from RGB-D camera captures,while ScanNet points are reconstructed from RGB images. Distinction in the number of LiDARbeams also leads to differences in point cloud sparsity. We thus propose domain-aware partitioning insection 4.2, guided by the routing technique, to alleviate such data-level interference. Category-level interference. Different datasets typically possess distinct label spaces. An objectclassified as background in one dataset might be considered as foreground in another. Even forthe same category, different datasets sometimes employ different classification and definition ways,such as the definition of the car category in outdoor datasets. Such dataset-specific taxonomy andannotation inconsistencies pose challenges in unifying multiple label spaces. The category-leveldifferences thus result in the interference problem among multiple datasets during training. Wepropose the language-guided classification in section 4.3 to mitigate such category-level interference.",
  "Method": "The overview of our OneDet3D is illustrated in . We utilize 3D sparse convolution for featureextraction and anchor-free detection head for 3D box prediction. Based on it, we propose the domain-aware partitioning during feature extraction to alleviate data-level interference, and propose thelanguage-guided classification in the anchor-free head to mitigate category-level interference.",
  "Multi-Domain Joint Training": "Architecture. We design the architecture of OneDet3D from the feature extractor and the detectionhead aspects. For the feature extractor, we utilize 3D sparse convolution to extract voxel-wise features.Compared to point-wise structures, voxel-wise features are more robust to domain gaps and lesssensitive to hyper-parameters, suitable for multi-domain training. Additionally, sparse convolution isnot only computationally efficient but also operates solely on points, thus not relying on fixed-sizefeature maps. This enables to extract domain-invariant 3D features for multi-domain joint training. For the detection head, we adopt the anchor-free way, where objects are represented by their centerpoints. It directly regards points from sparse convolutions as centers to represent objects, avoiding theneed for conversion from sparse to dense feature maps. We do not employ any pruning layers .Instead, we retain all points until the final stage for box prediction. This helps avoid the issue ofrequiring different pruning strategies due to variations in point clouds. Such a fully sparse architecturewell accommodates point clouds from multiple domains thus serves for multi-domain training. Joint training. During training, due to the disparities in object sizes across different point clouds,the localization accuracy requirements vary greatly among datasets. Considering this, besidesclassification, regression, and centerness prediction learning, we also introduce the 3D IoU predictionlearning to ensure that the box scores accurately represent their positional accuracy. dataset A dataset B dataset C multi-domain datadomain router scatter",
  "()()": ": The overview of OneDet3D. It utilizes multi-domain point clouds for training. Thedomain-aware partitioning in scatter and context avoids the data-level interference issue, and thelanguage-guided classification addresses the issue from category-level interference. Once trained,OneDet3D has the one-for-all ability to generalize to unseen domains, categories, and diverse scenes. For classification, we use soft focal loss as in . For easier optimization, we utilize the IoU in theBEV space, IoUBEV , as the soft target. Specifically, denote the binary target class label as c, thepredicted class probability as p, the classification loss is:",
  "Lcls = t |c IoUBEV p| log(|1 c p|)(1)": "where t = cIoUBEV +(1)(1cIoUBEV ). This classification loss can be viewed as usingthe soft target IoUBEV in focal loss . By employing IoUBEV , the network classification focusessolely on the position on the horizontal plane, which helps improve the calibration of classificationscores. Here, we discard positional information in the height direction to prevent optimization frombecoming overly complex, which makes the network easier to converge during joint training. For regression, we use the 3D IoU loss , optimizing with the usual 3D IoU. For both centernessand IoU prediction, we utilize the binary cross entropy loss. The IoU prediction branch is alsosupervised with the usual 3D IoU. Since different datasets vary in scale, we employ dataset-awaresampling during joint training: sampling datasets first and then randomly selecting samples. Denotenetwork parameters as , the objective of network training can thus be formulated as:",
  "Domain-Aware Partitioning": "During multi-domain joint training, we first aim to mitigate the data-level interference caused bydifferences in the inherent structure of point clouds. We identify two primary sources of interference.First, due to significant differences between data, interference mainly arises in the normalizationlayers, which adjust the scatter of data to maintain their consistency. Second, convolution mainlyfocuses on local information, leading to interference in context learning across different domain pointclouds, where scale difference mainly exists. Therefore, we partition parameters about these twoaspects into domain-specific ones. We design a domain router to guide such domain-aware training.In this way, the partitioned parameters are responsible for learning domain-equivalent knowledge,while the majority of the model can avoid interference and learn domain-invariant 3D knowledge.This allows multi-domain joint training to effectively acquire universal 3D representations. Domain router. Given the input point cloud x(n), the domain router aims to guide its path for thedomain-aware partitioning. We utilize a domain classifier for the routing mechanism by classifyingits correct domain label n. To achieve this, we employ 3D sparse convolutions with kernel sizes of 3and 1 for simple feature extraction, then utilize global average pooling (GAP) to obtain the feature ofthe whole scene. After applying softmax, we obtain the domain probability {p(n)d }Nn=1 and directlyuse cross entropy loss for classification. Due to the large domain differences, this classification task isrelatively simple thus the domain router can converge rapidly. During inference, when encounteringunseen domain data, such domain probability can indicate its similarity to seen domains and provideits data flowing path, enabling the model to generalize to unseen domains. Scatter partitioning. The normalization layers conduct regularization for input data, thus reducinghierarchical differences in data and making the network easier to train. Then normalization layersconduct re-scaling to adjust the scatter of data. Considering the significant differences in differentdomains, the same re-scaling operations will lead to disparities in the output data scatters. In thissituation, we partition scaling and shifting parameters after normalization for each domain data, sothat data scatter in different domains can be partitioned. All other convolution layers can be shared,only scaling and shifting parameters being domain-specific and differing. For unseen domains at theinference time, we introduce the domain probability from the domain router. Specifically, we keepN sets of scaling and shifting parameters {(n)}Nn=1, {(n)}Nn=1. Scatter partitioning can thus beformulated as:",
  "(3)": "where we utilize x to denote the output of normalization layers. Through this, the network can applyindividualized re-scaling operations from different domains, resulting in domain-specific scatter thuseffectively mitigating the data-level interference. Only introducing N sets of scaling and shiftingparameters almost negligibly increases the model size. Context partitioning. In addition, we separately learn global context information for differentdomain data to prevent interference between them in terms of global context. Specifically, for featuresf from the blocks in the feature extractor, we first apply a global average pooling layer to extractthe feature of the whole scene, then utilize a 3D sparse convolution to learn its context information.According to previous work , global information mainly matters in indoor scenes. We thusonly impose context learning for indoor domains. The process of context partitioning can thus beformulated as:f = f +",
  "Language-Guided Classification": "We then aim to alleviate category-level interference among domains caused by label conflicts.Different datasets inherently possess different label spaces, which leads to the problem of annotationinconsistency. Moreover, at the inference time, unseen domains may involve a label space thatis different from those seen during training. Such a category-level difference results in differentdefinitions of the same object, leading to the conflict and interference problems during training.To address this, we utilize language vocabulary embeddings from CLIP for classification.Specifically, we use the prompt \"a photo of {name}\" to extract language embeddings of the categorynames from different datasets using CLIP. These language embeddings are then used as parametersof the fully connected layer to perform the final classification, and are kept frozen during training.Each dataset utilizes its own language embeddings, effectively mitigating such interference. Due to the fully convolution architecture and the anchor-free head, the final classification is typicallyachieved through 3D sparse convolutions. To incorporate language embeddings, we convert the sparsefeatures of points into dense features, and then use language embeddings for classification throughfully connected layers. However, this conversion from sparse to dense features, together with thefrozen language embeddings, poses obstacles to gradient backpropagation, making it difficult for thenetwork to converge. To address this, we introduce a class-agnostic classification branch that performsonly foreground-background binary classification. This branch is shared across different datasetsand implemented using 3D sparse convolution. In this way, a part of classification can be addressedthrough 3D sparse convolution, making it easier to converge. The classification probabilities fromboth branches are multiplied finally. The utilization of such shared class-agnostic classification acrossdomains also facilitates the model in learning general category knowledge in the 3D domain. Open-vocabulary extension. Introducing language embeddings into classification enables ourOneDet3D to be easily extended to the open-vocabulary setting, benefiting from the generalizationability of text to unseen categories. To further ensure category scalability, we follow by firstperforming large-scale vocabulary inference on 2D images using a pre-trained 2D open-vocabularydetector , then projecting the obtained 2D boxes into 3D space to obtain 3D pseudo labels with anexpanded vocabulary. With such large-vocabulary pseudo labels for multi-domain joint training, the : The performance of OneDet3D for closed-vocabulary 3D object detection. OneDet3Dis joint training on these four datasets, and can conduct inference on them with the same modelarchitecture and only one set of parameters. APe, APm, APh denote the AP metric on easy, moderate,hard subsets separately, and APKIT denotes the AP metric computed the same as KITTI. Gray cellsindicate that the method original papers report results on this dataset. We re-implement previousmethods on other datasets and - indicates that the method fails to converge.",
  "OneDet3D (ours)65.051.370.956.292.884.282.381.081.8": "generalization ability to novel categories can be boosted. The multi-dataset training manner makes itpossible to comprehensively utilize different types of data from various domains, thus is quite suitablefor the open-vocabulary setting. Through such open-vocabulary extension, OneDet3D can generalizeto unseen categories. As a result, OneDet3D can generalize across various domains, categories, andscenes, thus can be considered to possess the capability of universal 3D object detection.",
  "Experiments": "In this section, we demonstrate the one-for-all ability of our OneDet3D through extensive experiments.Close-vocabulary and open-vocabulary 3D object detection experiments are both conducted. Wemainly conduct multi-dataset joint training on SUN RGB-D , ScanNet , KITTI , andnuScenes datasets, and utilize S3DIS and Waymo for unseen domains in cross-datasetexperiments. We implement OneDet3D with mmdetection3D , and train it with the AdamW optimizer. We use the 0.01m voxel size for indoor datasets and the 0.05m voxel size for outdoorones. Besides this, other architecture-related hyper-parameters are all the same for different datasets.During multi-dataset training, the attribute channel size is set to the least common multiple of theattribute dimensions from the different datasets, which is 6-dim. The attributes of the point cloudsfrom different datasets are repeated accordingly to match this unified channel size.",
  "Closed-Vocabulary 3D Object Detection": "We first conduct multi-dataset joint training on all the above mentioned four datasets, and performclosed-vocabulary inference. Specifically, for the SUN RGB-D dataset, we perform 3D detectionon 10 classes, for ScanNet its 18 classes, while for the KITTI and nuScenes datasets, we focus onthe performance of the car category. The results are listed in Tab. 2. It can be seen that even inthe traditional single-dataset training-and-testing paradigm, existing 3D detectors can only conductdetection in a specific domain. Indoor 3D detectors can only operate on indoor point clouds, while themajority of existing outdoor 3D detectors can only work on one of the KITTI and nuScenes datasetsbecause of their differences in sparsity and scenes. In comparison, our model can directly performtraining and inference on these different domain point clouds. After multi-dataset joint training,OneDet3D can perform 3D detection on all domain point clouds with only one set of parameters.The performance surpasses that of most existing methods trained and tested using single-datasettraining and inference. For instance, on the SUN RGB-D dataset, OneDet3D achieves the 65.0% AP25, surpassing FCAF3D by 1.2%. On the outdoor KITTI dataset, OneDet3D performs comparablyto PV-RCNN, and on nuScenes, its AP surpasses existing methods such as VoxelNeXt and UVTR.Moreover, after multi-domain joint training, the performance of OneDet3D exceeds its own fromsingle-dataset training. On the SUN RGB-D and KITTI datasets, multi-dataset joint training bringsa 1.8% improvement for both. This demonstrates that even with significant differences, OneDet3Dcan learn universal 3D detection knowledge from these diverse point clouds. The necessity ofmulti-domain joint training and the effectiveness of our OneDet3D can thus be demonstrated. : Comparison with more recent 3D objectdetectors for closed-vocabulary 3D object detec-tion. We re-implement these methods on the SUNRGB-D (SUN), ScanNet (Scan), KITTI (KIT) andnuScenes (nuS) datasets, and compare the AP25,APm, AP metrics.",
  "VoxelNeXt 8.39.968.471.0FSD v2 13.612.960.172.4SAFDNet 3.21.938.770.4Uni3D 9.75.675.276.7OneDet3D (ours)65.070.984.280.9": "Comparison with more recent methods.We further compare with some more recent3D detectors and list the comparison in Tab. 3.As can be seen, these recent methods targetat specific 3D scenes. They may outperformOneDet3D in those particular datasets, butAP tends to drop when the scene changes, es-pecially when switching from outdoor to in-door. After multi-dataset training, due to thedataset-aware interference, AP on all datasetsdegrade severely. In such multi-dataset sce-narios, OneDet3D still achieves the best. Be-sides, compared with Uni3D , which hasprovided a unified model for outdoor pointclouds, OneDet3D provides a universal so-lution for all point clouds. As can be seen,even compared with these recent methods,OneDet3D is still the first universal 3D detec-tor that can generalize across various point clouds. The main reason is that the specific designs ofthese detection heads, such as vote-based methods or BEV detection, are influenced by the structureand content of point clouds and thus are only applicable to outdoor scenes. Additionally, thesemethods lack designs to address multi-dataset interference, resulting in performance degradationacross all datasets during multi-dataset joint training. In contrast, the anchor-free detection head ofour OneDet3D is more versatile for both indoor and outdoor scenes. Furthermore, domain-awarepartitioning and language-guided classification can alleviate multi-dataset interference. Therefore,our approach provides a more universal solution for 3D detection.",
  "multi-datasettrainingOneDet3D (ours)12.5944.4920.2215.5235.1119.77": "We then conduct open-vocabulary 3D object detection experiments with our OneDet3D. In ourexperiments, we refer to the setting in CoDA, where SUN RGB-D involves 46 classes and ScanNetinvolves 60 classes in total. Their top 10 classes are used as base categories. For multi-datasetjoint training, we combine the base categories from both datasets to form a union, resulting in atotal of 16 base categories, with the rest as novel categories. We reproduce the results of existingmethods under this new category division and list the comparison in Tab. 4. Its worth noting that fora fair comparison, we used exactly the same setting as CoDA, which utilizes a single-view imagesetting in ScanNet, slightly different from the above closed-vocabulary setting. It can be seen thatthe superiority of our method is more obvious here. On the SUN RGB-D dataset, we achieve theAPnovel improvement of over 5.94% compared to CoDA. On the ScanNet dataset, we achieve the",
  "Cross-Domain 3D Object Detection": "We further conduct cross-domain 3D detection experiments, with S3DIS and Waymo as new domainsfor inference. The comparison results are listed in Tab. 5 and Tab. 6. It can be observed that onS3DIS, in single-dataset training, our method already outperforms existing methods. This is becauseour language-guided classification better alleviates category conflicts. The performance is slightlybetter when trained on ScanNet because the ScanNet domain is more similar to S3DIS. After trainingon both datasets, the cross-domain AP on S3DIS improves by more than 4%, indicating the modelability to integrate information from both domain datasets. Furthermore, with the introduction of twooutdoor datasets, AP25 improves by 0.9%, with AP50 remaining stable. This demonstrates that ourOneDet3D can learn from such highly different domain point clouds for enhancement in cross-domain3D detection. In outdoor point clouds, this is even more pronounced. KITTI is relatively similar toWaymo but only contains small-scale point clouds, while nuScenes is larger-scale but exhibits a largerdomain gap. Training separately on these two datasets thus only yields limited cross-dataset AP3Don Waymo. In comparison, through multi-dataset training, the model can utilize the characteristics ofboth, resulting in a substantial 23.1% improvement. This demonstrates the generalization capabilityof our method to unseen domains and further validates the necessity of multi-dataset training.",
  "Ablation Study": "We finally conduct ablation study in this subsection and list the results in Tab. 7 to evaluate ourdesigns. Here we conduct multi-dataset training on the SUN RGB-D and KITTI datasets, andutilize S3DIS here for cross-domain evaluation. We also list the single-dataset training results as areference. As can be seen, although our model architecture allows for multi-dataset joint training,directly multi-dataset training results in decreased AP on both seen and unseen domains, becauseof the interference problem. After introducing scatter partitioning, which alleviates interferenceamong multiple domains during regularization, the model performance essentially matches andslightly exceeds that of single-dataset training. Then, through context partitioning, the detector can",
  ": The visualized results of OneDet3D on the indoor SUN RGB-D, ScanNet and outdoorKITTI, nuScenes datasets separately": "selectively learn the corresponding global context of different domain point clouds, leading to furtherAP improvement. Especially for the indoor domain, the partitioning of global context learning enablesmulti-domain joint training AP to surpass those of single-domain training. Finally, with language-guided classification, the effect of category conflict can be mitigated. Since the category conflictproblem between SUN RGB-D and KITTI is not severe, the performance improvement on these twodatasets is relatively moderate. The about 0.5% AP improvement here is primarily because of thecommon class-agnostic classification. In cross-dataset experiments on S3DIS, language embeddingscontribute to a more AP increase, more than 2% improvement. This is mainly because of the moresubstantial overlap in categories between SUN RGB-D and S3DIS, making language embeddingsmore effective for these two domains. Such the ablation study thus demonstrates the necessity ofthese designs to address the interference issues for multi-dataset joint training. We provide visualized results from our OneDet3D in . It can be seen that no matter for indooror outdoor point clouds from different domains, OneDet3D can perform 3D detection effectivelyusing only one set of parameters. This further demonstrates its effectiveness and universal ability.",
  "Conclusion": "In this paper, we propose OneDet3D, a universal point cloud based 3D object detector that cangeneralize across various domains, categories, and scenes with only one set of parameters. Thefully sparse structure and the anchor-free detection head serve as the basic model architecture. Withpartitioning in scatter and context, together with the language-guided classification, the interferencecaused by point clouds and categories can be alleviated. Extensive experiments demonstrate the strongone-for-all ability of OneDet3D. For the first time, we implement various scenarios and requirementsof 3D object detection within a unified framework. This demonstrates that our OneDet3D has learnedgeneral 3D representations through multi-domain joint training, thus basically realizing the demandsof universal 3D object detection and 3D foundation models. We believe that our research willstimulate following research along the universal computer vision direction in the future.",
  "ADatasets and Implementation Details": "SUN RGB-D . SUN RGB-D is a single-view indoor dataset with 5,285 training and 5,050validation scenes, annotated with 10 classes and oriented 3D bounding boxes. The point cloudswithin it are from converting RGB-D camera results. We utilize the 0.01m grid size for voxelization.During training, we randomly flip the input data along the x axis, randomly sample 10,000 points,and apply global translation, rotation, scaling for data augmentation. We utilize the AP25 and AP50metrics for evaluation. ScanNet . The ScanNet V2 dataset contains 1,201 reconstructed training scans and 312 validationscans, with 18 object categories for axis-aligned bounding boxes. The point clouds within it arefrom reconstructing from a series of multi-view images. We also utilize the 0.01m grid size forvoxelization. During training, we randomly flip the input data along both the x and y axis, randomlysample 10,000 points, and apply global translation, rotation, scaling for data augmentation. We utilizethe AP25 and AP50 metrics for evaluation. For the open-vocabulary setting, we adopt the same settingas CoDA , dividing the large panoramic scene from the original ScanNet V2 into several smallerpoint cloud scenes, each corresponding to a single-view image. There are 47,841 training samplesand 4,886 validation samples ultimately for the open-vocabulary setting. KITTI . The KITTI dataset consists of 7,481 LiDAR samples for its official training set, andwe split it into 3,712 training samples and 3,769 validation samples for training and evaluation. Weonly utilize the \"car\" category for training and evaluation. During training, we also adopt van classobjects as car objects. The data augmentation operations are basically the same as previous outdoor3D detectors like . For the ground-truth sampling augmentation, we sample at most 20 cars fromthe database. 18000 points are randomly sampled at the training time. The predicted car objects arefiltered at the threshold of 0.6 after inference. We utilize the AP70 metric under 40 recall positions onthe \"car\" category for evaluation. nuScenes . Compared to the KITTI dataset, the nuScenes dataset covers a larger range, with 360degrees around the LiDAR instead of only the front view. Its point clouds are also more sparse (with32-beam LiDAR compared to the KITTI 64 beams). We train on the 28,130 frames of samples inthe training set and evaluate on the 6,010 validation samples. We do not predict the velocities orattributes of objects, to keep consistent with other datasets. We only utilize the \"car\" category fortraining and evaluation. For the ground-truth sampling augmentation, we sample at most 5 cars fromthe database. We utilize its official AP metric, averaging over match thresholds of 0.5, 1, 2, 4 meters.We also utilize the KITTI AP metric for evaluation. S3DIS . S3DIS consists of 3D scans from 6 buildings, 5 object classes annotated with axis-alignedbounding boxes. We use the official split and perform cross-dataset evaluation of our method on 68rooms from Area 5. Its point clouds are also from reconstructing multi-view images. We utilize theAP25 and AP50 metrics for evaluation. : Ablation study about the design details of context partitioning on the SUN RGB-D,KITTI and S3DIS datasets. OneDet3D is joint training on the SUN RGB-D and KITTI datasets,and S3DIS is utilized for cross-domain evaluation. CP is short for context partitioning.",
  "multi-datasettraining": "3D sparse conv63.949.591.983.180.645.827.6CLIP embeddings (frozen)60.244.890.781.479.525.819.2CLIP embeddings (trainable)64.049.591.883.280.846.027.73D sparse conv + CLIP embeddings (frozon)64.449.992.283.580.847.829.0 Waymo . We utilize the Waymo dataset for cross-dataset evaluation. We evaluate on its 39,987samples of its validation set. We also only evaluate on the \"car\" category (i.e., the \"vehicle\" category).We utilize the KITTI metric, AP70 under 40 recall positions in both 3D and BEV, for evaluation. Implementation details. We implement OneDet3D with mmdetection3D . We utilize the 3Dsparse convolution based ResNet50 as backbone, together with FPN for feature extraction.We train the model with the AdamW optimizer for 20 epochs. The initial learning rate is setto 0.0001 and is updated in the cyclic manner. Point clouds from different datasets are sampleduniformly. We remove the ground-truth sampling augmentation at the last 2 epochs.",
  "BMore Ablation Study": "Context partitioning. We discuss the design details about partitioning the learning of global contextand list such ablation study in Tab. 8. As can be seen, without the partitioning, the model can alreadyachieve the performance through multi-dataset training that is better than the single-dataset trainingbaseline, partially due to our design of scatter partitioning. If we apply context partitioning for bothindoor and outdoor point clouds, the 3D detection AP decreases on all datasets. Especially for theoutdoor KITTI dataset, APm decreases by 3.8%. This is because global information in outdoorscenes tends to be disruptive, given the excessive background points and the fact that foregroundobjects occupy only a small portion of the scene. In comparison, introducing context partitioningonly for indoor point clouds can lead to a more than 1% AP improvement, and the cross-datasetAP improvement is 2.5%. This is because indoor scenes are smaller and the object distribution ismore crowded, making global context relatively more important thus contributing to performanceimprovement. Therefore, the rationality of our context partitioning design is validated. Language-guided classification. We then discuss the design details about our language-guidedclassification and list the related 3D detection results in Tab. 9. The anchor-free detection headtypically employs 3D sparse convolution for final classification. However, this approach strugglesto address the category conflict issue among different domains. Especially during cross-datasetevaluation, differences in category definitions across datasets and the potential new categories in newdomains can all restrict the model performance. Utilizing language embeddings as the classificationlayer can help mitigate this issue, due to the generalization ability of the text modality. However,directly using CLIP embeddings for classification results in a decrease in detection AP across alldatasets. This is because a frozen fully connected layer impedes gradient backpropagation in the fullysparse convolution structure. Instead, when CLIP embeddings are trainable instead of frozen, theachieved AP can be comparable to or slightly surpass that of 3D sparse convolution. However, in this : Ablation study about the use of classification loss on the SUN RGB-D, KITTI andS3DIS datasets. OneDet3D is joint training on the SUN RGB-D and KITTI datasets, and S3DIS isutilized for cross-domain evaluation.",
  "way, CLIP embeddings are not available at the inference time, and the model thus cannot generalizeto new domains, especially new categories during inference": "In comparison, what we utilize is the combination of 3D sparse convolution and frozen CLIPembeddings. 3D sparse convolution is utilized for class-agnostic classification, and is shared amongall domains. The frozen CLIP embeddings are utilized for class-specific classification, and can benefitthe model with the help of the text modality. As a result, both detection AP from the indoor andoutdoor domains can be boosted. The cross-dataset AP on S3DIS increases the most, a 2% AP25improvement, because the category conflict problem can be alleviated. The effectiveness of ourdesigns in language-guided classification is thus demonstrated. Classification loss. We then analyze the choice of the loss function for classification, and list thecomparative results in Tab. 10. It can be observed that if we utilize focal loss for classification, whichis widely utilized in the anchor-free detection head, the model cannot converge. This problem appearsafter the introduction of the language-guided classification. Since the class-specific classification isimplemented by CLIP embeddings, which are frozen during training, the model requires a strongeroptimization way for learning. The usual used focal loss thus becomes insufficient in this task. Utiliz-ing soft focal loss, with IoU as the soft target can introduce positional information in classification,thus alleviating this problem. When using IoU3D as the soft target, as IoU3D is relatively hard tooptimize, especially coupled with the classification task, the performance is still limited. The useof decoupled IoU proposed in can alleviate this problem, by decoupling IoU calculation inthe xy plane and the z axis. However, this decoupling is still not enough in our task, because thefrozen CLIP embeddings make optimization more challenging. Therefore, we directly disregard thez direction here, and utilize IoUBEV here. The positional information in the xy plane can alreadyprovide sufficient supervision signals for classification, and disregarding the z direction also makesoptimization easier. Ultimately, the model can conduct supervised learning with the language-guidedclassification, and achieve satisfying results on both indoor and outdoor point clouds.",
  ": The visualized results of OneDet3D on the indoor SUN RGB-D dataset": "We further provide more visualized results on the indoor SUN RGB-D dataset (), indoor ScanNetdataset (), outdoor KITTI dataset (), and outdoor nuScenes dataset (). OneDet3Dobtains satisfying detection results on all these four datasets, with only one set of parameters. Ourmethod can accurately detect objects in various domain point clouds, ranging from indoor crowdedscenes with overlapping objects to outdoor scenes with small-sized objects. This further demonstratesits effectiveness and universality."
}