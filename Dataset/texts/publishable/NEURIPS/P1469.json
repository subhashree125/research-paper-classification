{
  "Abstract": "In offline reinforcement learning, a policy is learned using a static dataset inthe absence of costly feedback from the environment. In contrast to the onlinesetting, only using static datasets poses additional challenges, such as policiesgenerating out-of-distribution samples. Model-based offline reinforcement learningmethods try to overcome these by learning a model of the underlying dynamicsof the environment and using it to guide policy search. It is beneficial but, withlimited datasets, errors in the model and the issue of value overestimation amongout-of-distribution states can worsen performance. Current model-based methodsapply some notion of conservatism to the Bellman update, often implementedusing uncertainty estimation derived from model ensembles. In this paper, wepropose Constrained Latent Action Policies (C-LAP) which learns a generativemodel of the joint distribution of observations and actions. We cast policy learningas a constrained objective to always stay within the support of the latent actiondistribution, and use the generative capabilities of the model to impose an implicitconstraint on the generated actions. Thereby eliminating the need to use additionaluncertainty penalties on the Bellman update and significantly decreasing the numberof gradient steps required to learn a policy. We empirically evaluate C-LAP on theD4RL and V-D4RL benchmark, and show that C-LAP is competitive to state-of-the-art methods, especially outperforming on datasets with visual observations.1",
  "Introduction": "Deep-learning methods are widely used in applications around computer vision and natural languageprocessing, related to the fact that datasets are abundant. But when used for control of physicalsystems, in particular with reinforcement learning, obtaining data involves interaction with anenvironment. Learning through trial-and-error and extensive exploration of an environment canbe done in simulation, but hard to achieve in real world scenarios . Offline reinforcementlearning tries to solve this by using pre-collected datasets eliminating costly and unsafe training inreal-world environments . Using online reinforcement learning methods in an offline setting often fails. A key issue is thedistributional shift: the state-action distribution of the offline dataset, driven by the behavior policy,differs from the distribution generated by a learned policy. This leads to actions being inferred forstates outside the training distribution. Therefore, value-based methods are prone to overestimatingvalues due to evaluating policies on out-of-distribution states. This leads to poor performance andunstable training because of bootstrapping . Offline reinforcement learning methods address",
  "(c) Environment interaction": ": Overview of C-LAP. (a) The model is trained offline. It is encoding observations otand actions at to latent states (gray circle) and latent actions ut (green circle), and decoding themthereafter. Furthermore it is predicting rewards rt. (b) The policy is learned in the latent actionspace, but constrained to the support of the latent action prior, and uses the generative capabilities ofthe action decoder. Gradients are computed by back-propagating estimated values Vt and rewardsrt through the imagined trajectories. (c) The latent action policy is used in the real world, againconstrained to the support of the latent action prior and using the generative action decoder.1",
  "this issue with different approaches and can be categorized into model-free and model-based methods,similar to online reinforcement learning": "Model-free offline reinforcement learning usually follows one of the following paradigms: constrainthe learned policy to the behavior policy ; or introduce some kind of conservatism tothe Bellman update . Model-based reinforcement learning methods transform theoffline to an online learning setting: They approximate the system dynamics and try to resolve theevaluation of out-of-distribution states by using the generalization capabilities of the model andgenerating additional samples. But as the training distribution is fixed, the estimation capabilities ofthe model are limited. Therefore, these model-based methods also rely on a conservative modificationto the Bellman update as a measure to counteract value overestimation which is mostly achievedthrough uncertainty penalties . Apart from the typical approach of usingan auto-regressive model to estimate the dynamics, other model-based methods treat the objectiveas trajectory modeling . These methods aim to combine decision making and dynamicsmodeling into one objective. Instead of learning a policy, they sample from the learned trajectorymodel for planning. We will refer to the first kind of methods, which learn a dynamics model to traina policy, as model-based reinforcement learning. We aim to solve the problem of value overestimation in model-based reinforcement learning by jointlymodeling action and state distributions, without the need for uncertainty penalties or changes to theBellman update. Instead of learning a conditional dynamics model p(s | a), we estimate the jointstate-action distribution p(s, a). This is similar to methods that frame offline reinforcement learningas trajectory modeling, but we use an auto-regressive model and still learn a policy. By formulatingthe objective as a generative model of the joint distribution of states and actions, we create an implicitconstraint on the generated actions, similar to . The goal of this approach is to address theshift in the entire distribution, rather than looking at out-of-distribution actions and states separately.We achieve this using a recurrent state-space model with a latent action space, which we call therecurrent latent action state-space model. Using a latent action space allows us to learn a policy thatuses the latent action prior as an inductive bias. This approach keeps the policy close to the originaldata while allowing it to change when needed, which makes learning the policy much faster. Toachieve this, we treat policy optimization as a constrained optimization problem, similar to enforcinga support constraint . We provide a high level overview of our method in .",
  "Preliminaries": "We consider a partial observable Markov decision process (POMDP) defined by M=(S, A, O, T, R, , ) with S as state space, A as action space, O as observation space, s Sas state, a A as action, o O as observation, T : S A S as transition function, R : S Ras reward function, : S O as emission function and (0, 1] as discount factor. The goal is tofind a policy : O A that maximizes the expected discounted sum of rewards E[Tt=1 trt] . In online reinforcement learning, an agent iteratively interacts with the environment M and optimizesits policy . In offline reinforcement learning, however, the agent cannot interact with the environmentand must refine the policy using a fixed dataset D = {(o1:T , a1:T , r1:T )Nn=1}. Therefore, the agentmust understand the environment using limited data to ensure the policy maximizes the expecteddiscounted sum of rewards when deployed . Auto-regressive model-based offline reinforcementlearning tries to learn a parametric function to estimate the transition dynamics T. The transitiondynamics model is then used to generate additional trajectories which can be used to train a policy. Themajority of these approaches learn a dynamics model directly in observation space T(ot | ot1, at1), while others use a latent dynamics model T(st | st1, at1) .",
  "Constrained Latent Action Policies": "A main issue in offline reinforcement learning is value overestimation, which we address by ensuringthe actions generated by the policy stay within the datasets action distribution. Unlike previous model-based methods, we formulate the learning objective as a generative model of the joint distribution ofstates and actions. We do this by combining a latent action space with a latent dynamics model. Next,we use the generative properties of the action space to constrain the policy to the datasets actiondistribution. A general outline of our method, Constrained Latent Action Policies (C-LAP ), is shownin Appendix B. It starts with learning a generative model, followed by actor-critic agent training onimagined trajectories, similar to the methods in .",
  "p(o1:T , a1:T ) =p(o1:T , a1:T | s1:T , u1:T )p(s1:T , u1:T ) ds du.(1)": "that jointly models the observation and action distribution of a static dataset D={(o1:T , a1:T , r1:T )Nn=1} by using latent states st along with latent actions ut. Unlike other model-based offline reinforcement learning methods that learn a conditional model p(o1:T | a1:T ) and relyon ensemble based uncertainty penalties on the Bellman update to generate trajectories within thedata distribution , our approach uses a latent action space to impose an additionalimplicit constraint. By implementing a policy in the latent action space, generated actions will staywithin the datasets action distribution, thus enabling generalization within the limits of the learnedmodel . We empirically validate this claim in Appendix F. To obtain a state space model withMarkovian assumptions on the latent states st we impose the following structure:",
  "t=1p(ut | st)p(st | st1, ut1).(3)": "We implement the probabilistic model modifying the design of a recurrent state-space model. Thus, the latent dynamics model p(st | st1, ut1) is based on the deterministic transi-tion f(ht1, st1, at1) using the latent action decoder p(at1 | st1, ut1) to generate actions.In the following, we mostly omit deterministic states ht for notational brevity. The resulting recurrentlatent action state-space model is shown in and consists of the following components,specifically",
  "latent state priorp(st | st1, ut1),latent action priorp(ut | st),observation decoderp(ot | st),and action decoderp(at | st, ut)": "The latent state prior predicts the next latent state st given the previous latent state st1 and actionut1 using the deterministic transition and the action decoder. The latent action prior predicts latentactions ut given latent state st. Latent states as well and as latent actions are decoded using theirrespective decoder. Similar to actions are reconstructed given latent state and latent action. Directly maximizing the marginal likelihood is intractable, hence we maximize the evidence lowerbound (ELBO) on the log-likelihood log p(o1:T , a1:T ) instead. To approximate the true posterior, weintroduce",
  "which can be organized into individual terms for reconstruction and consistency of actions andobservations. The derivation can be found in Appendix A": "Maximizing the objective enables us to learn a model which can generate trajectories close to the datadistribution D by sampling from both priors. As we want to use the model to learn a policy via latentimagination, we add a reward p(rt | st) and termination p(tt | st) model. Hence, the completemodel training objective is",
  "t=1Est,utq[log(p(rt | st)) + log(p(tt | st))].(5)": ": Policy constraint through explicit parametrization by using a linear transformation g of thelatent action prior p(ut | st) and the bounded policy (ut | st) . The generated actionsat p(at | st, g((ut | st), p(ut | st)) are implicitly constrained to the data distribution. Constrained latent action policyWe use the sequence model to generate imagined trajectoriesand use an actor-critic approach to train the policy. To predict state values we learn a value modelv(st) alongside the policy. Therefore we use the n-step return of a state",
  "withh = min( + k, t + H)(6)": "as regression target for v(st) . Polices trained on trajectories generated by a model are proneto end up with degrading performance if the model only has access to a limited data distribution, as inthe case of offline reinforcement learning. Compounding modeling errors and value overestimationof edge-of-reach states are reasons for the decline. Since we train a generative action model,generated actions are implicitly constrained to the datasets action distribution by sampling fromthe action decoder at p(at | st, ut). Hence, states outside the datasets observation distributionare hard to reach and our approach is resilient to value overestimation of edge-of-reach states.Compounding modeling errors are still a source of diminishing performance, but can be counteractedby increasing the representation power of the model or generating only short trajectories. To leveragethe generative action model, we learn a policy (ut | st) in the latent action space similar to .But, as both, the latent action prior p(ut | st) and the policy (ut | st) are flexible, it is notensured that they share the same support. Thus, we formulate policy optimization as a constrainedoptimization problem",
  "(7)": "similar to a support constraint . We implement the constraint explicitly through parametrization tostay within the support, but do not impose any restrictions inside the supported limits (). Thisis different to using a divergence measure which on one hand does not strictly ensure support limitsand on the other hand is more restrictive as it also imposes a constraint on the shape of a distribution.Here and in the following ut stands for a latent action sampled from the policy (ut | st). The policy is trained to maximize the n-step return V kN(s) while staying in support of the latentaction prior. Since the latent action prior p(ut | st) is normally distributed as N((st), (st)),we can express the constraint as",
  "Experiments": "In the next section, we evaluate the effectiveness of our approach. It is divided into three parts: first,we assess the performance using standard benchmarks; then, we study how different design choicesaffect value overestimation; lastly, we analyze the influence of the support constraint parameter. Weadditionally provide the final performances in two tables in Appendix E. We limit our benchmark evaluation to the most relevant state-of-the-art offline reinforcement learningmethods to answer the following questions: 1) How do latent action state-space models compare tostate-space models? 2) How comparable are model-free methods focusing on latent action spaces tolatent action state-space models? 3) Does C-LAP suffer from value overestimation? 4) How doesthe support constraint affect the performance? 5) How does the performance differ between visualobservations and observations with low-dimensional features? To focus on the latter, we separatelyevaluate the performance on low-dimensional feature observations using the D4RL benchmark ,and on image observations using the V-D4RL benchmark .",
  "Benchmark results": "D4RLSince most offline model-based reinforcement learning methods are designed for obser-vations with low-dimensional feature observations, there exist many options for comparison. Wemake a selection to include the most relevant methods focusing on latent actions and auto-regressivemodel-based reinforcement learning. Therefore, we include the following methods: PLAS, whichis a model-free method using a latent action space . MOPO, a probabilistic ensemble-basedoffline model-based reinforcement learning method using a modification to the Bellman update topenalize high variance in next state predictions . And MOBILE, which is similar to MOPO,but penalizes high variance in value estimates instead . We compare the algorithms on threedifferent locomotion environments, namely halfcheetah, walker2d and hopper, with four datasets(medium-replay, medium, medium-expert, expert) each and the antmaze navigation environmentwith four datasets (umaze, umaze-diverse, medium-play, medium-diverse). The results, shown in, display the mean and standard deviation of normalized returns over four seeds during thephase of policy training, with steps denoting gradient steps. The dashed lines indicate the asymptoticperformance for MOPO and MOBILE. A detailed summary of all implementation details is providedin the Appendix D. When comparing C-LAP to PLAS, we find that learning a joint generative model of actions andobservations outperforms a generative model of only actions when used with actor-critic reinforcementlearning. Both methods can use the generative nature of the model to speed up policy learning,which becomes especially clear in the results on all locomotion expert and medium-expert datasets.Compared to MOPO and MOBILE, C-LAP shows a superior or comparable performance on alldatasets except halfcheetah-medium-replay-v2, halfcheetah-medium-v2 and hopper-medium-v2.Especially outperforming on the antmaze environment, where MOPO and MOBILE fail to solve thetask for any of the considered datasets. The asymptotic performance of MOBILE on locomotionenvironments sometimes exceeds the results of C-LAP, but needs three times as many gradient steps.",
  ": Evaluation on low-dimensional feature observations using D4RL benchmark datasets. Weplot mean and standard deviation of normalized returns over 4 seeds": "Overall the results indicate that latent action state-space models with constrained latent action policesnot only match the state-of-the-art on observations with low-dimensional features as observations, butalso jump-start policy learning by using the action decoder to sample actions that lead to high rewardsalready after the first gradient steps: If the dataset is narrow, generated actions when sampling fromthe latent action prior will fall into the same narrow distribution. For instance, in an expert dataset,sampled actions will also be expert-level actions. During policy training, instead of sampling fromthis prior, we restrict the support of the policy dependent on the latent action prior. Thus, sampledlatent actions from the policy will always be decoded to fall into the datasets action distribution. Soeven a randomly initialized policy in the beginning of the training can generate a high reward byusing the latent action decoder. This effect is especially prominent in narrow datasets such as expertdatasets. V-D4RLThere are currently few auto-regressive model-based reinforcement learning methods thatspecifically target visual observations, with none emphasizing latent actions. In our evaluation, weinclude LOMPO and Offline DV2 . Both methods use a latent state space model and anuncertainty penalized reward. However the specifics of the penalty calculations are different: whileLOMPO uses standard deviation of log probabilities as penalty, Offline DV2 uses mean disagreement.Additionaly, LOMPO trains an agent on a mix of real and imagined trajectories with an off-policyactor-critic approach, whereas Offline DV2 exclusively trains on imagined trajectories and back-propagates gradients through the dynamics model. Further implementation details are included inAppendix D.",
  "Value overestimation": "Limiting value overestimation plays a central role in offline reinforcement learning. To evaluatethe effectiveness of C-LAP, we report value estimates alongside normalized returns on all walker2ddatasets in . A similar analysis for all considered baselines is provided in Appendix G.To further analyze the influence of different action space design choices, we include the followingablations: a variant no constraint, which does not formulate policy optimization as constrainedobjective, but uses a Gaussian policy distribution to potentially cover the whole Gaussian latent actionspace; and a variant no latent action, which does not emphasize latent actions, but uses a regularstate-space model as in Dreamer . Besides that, we added dashed lines to indicate the datasetsaverage return and average maximum value estimate. The no latent action variant fails to learn : Ablation study, comparing C-LAP to the following variants: no constraint, C-LAP withoutenforcing the policy constraint dependent on the action prior; no latent action, C-LAP without a latentaction space similar to Dreamer . We plot mean and standard deviation of normalized returns andvalue estimates over 3 seeds. Moreover we add the datasets average return and average maximumvalue estimate indicated by dashed lines. an effective policy: normalized returns are almost zero and the datasets reference returns remainunattained; value estimates are significantly exceeding the datasets reference values, indicating valueoverestimation. The no constraint variant can use the generative action decoder to limit generatedactions to the datasets action distribution, but the Gaussian policy is free to move to regions whichare unlikely under the action prior. Thus, nullifying the implicit constraint imposed by the actiondecoder, resulting in collapsing returns and value overestimation. Only C-LAP achieves a high returnand generates value estimates which are close to the datasets reference. The value estimates onwalker2d-medium-replay-v2 are higher than the datasets reference, as the agents performance isalso exceeding the reference performance. The results confirm the importance of limiting valueoverestimation in offline reinforcement learning, and demonstrate that constraining latent actionpolicies can be an effective measure for achieving this.",
  "Support constraint parameter": "To evaluate the influence of the support constraint parameter on the performance of C-LAP, weperform a sensitivity analysis across all walker2d datasets (). Except for the more diversemedium-replay-v2 dataset, adjusting from 0.5 to 3.0 only has a minor impact on the achievedreturn. However, when choosing an unreasonable large value such as = 10.0 or removing theconstraint altogether (), we observe a collapse during training. This highlights a key insight:constraining the policy to the support of the latent action prior is essential. And in many cases, usinga smaller support region closer to the mean (small ) proves sufficient.",
  "Related Work": "Offline reinforcement learning methods fall into two groups: model-free and model-based. Bothtypes aim to tackle problems like distribution shift and value overestimation. This happens becausethe methods use a fixed dataset rather than learning by interacting with the environment. Model-freeCurrent model-free methods typically work by limiting the learned policy or byregularizing value estimates. TD3+BC adds a behavior cloning regularization term to thepolicy update objective to enforce actions generated by the policy to be close to the datasets actiondistribution. Similarly, SPOT includes a regularization term in the policy update, derived from asupport constraint perspective. It also uses a conditional variational auto-encoder (CVAE) to estimatethe behavior distribution. Following a comparable intention, BEAR constraints the policy tothe support of the behavior policy via maximum mean discrepancy. BCQ and PLAS use aCVAE similarly but dont use a regularization term. Instead, they constrain the policy implicitly bymaking the generative model part of the policy. Beside these methods, many other approaches exist,with CQL and IQL being some of the most well-known. CQL uses a conservative policyupdate by setting a lower bound on value estimates to prevent overestimation, while IQL avoidsout-of-distribution values by using a modified SARSA-like objective in combination with expectileregression to only use state-action tuples contained in the dataset. Model-basedModel-based offline reinforcement learning methods learn a dynamics model togenerate samples for policy training. This basically converts offline learning to an online learningproblem. Model-based methods mainly address model errors and value overestimation by using a probabilistic ensemble and adding an uncertainty penalty in the Bellman update. MOPO uses a probabilistic ensemble as in and adds the maximum standard deviation of all ensemblepredictions as uncertainty penalty. Similar to that, MOReL adheres to the same methodology,but uses pairwise maximum difference of the ensemble predictions as penalty instead. Analogously,MOBILE estimates the values for all by the ensemble predicted states and uses the standarddeviation of value estimates as penalty. Edge of reach comes to the conclusion that valueestimation on edge of reach states are the overarching issue compared to model errors. In the end,they come up with a comparable solution to MOBILE, but use an ensemble of value networksalongside the ensemble of dynamic models. COMBO pursues a different approach, as theyintegrate the conservatism of CQL into value function updates, removing the need for uncertaintypenalties. Besides that, some methods use a different class of models: instead of learning a predictivemodel in observation space, they use latent state-space models to make predictions on latent states.Among these methods is LOMPO , which builds up on Dreamer , but integrates an ensembleto predict stochastic states and use the standard deviation of the log probability of the ensemblepredictions as an uncertainty penalty similar to previous methods. The policy is trained on a mix ofimagined and real world samples, hence they use an off-policy actor-critic style approach for policylearning. Offline DV2 uses a similar model, but is based on a different penalty. Namely, they usethe difference between the individual ensemble mean predictions and mean over all ensembles asuncertainty penalty. Furthermore, the policy is trained only on imagined trajectories with gradientscalculated by back-propagating through the dynamics model. Overall, Offline DV2 is the methodmost comparable to our approach, but still different in many ways as we propose a latent actionstate-space model compared to a usual state-space model, and frame policy learning as constrainedoptimization. So far all discussed models operate in an auto-regressive fashion, but another classof methods exists, which casts offline model-based reinforcement learning as trajectory modeling.Instead of learning a policy, these kind of approaches integrate decision making and modeling ofthe underlying dynamics into a single objective and use the model for planning. Among them areDiffuser , which employs guided diffusion for planning; TT , which builds on advances intransformers; and TAP , which uses a VQ-VAE with a transformer-based architecture to create adiscrete latent action space for planning.",
  "Conclusion": "We present C-LAP, a model-based offline reinforcement learning method. To tackle the issue ofvalue overestimation, we first propose an auto-regressive latent-action state space model to learn agenerative model of the joint distribution of observations and actions. Second, we propose a methodfor policy training to stay within the datasets action distribution. We explicitly parameterize thepolicy depending on the latent action prior and formulate policy learning as constrained objectivesimilar to a support constraint. We find that C-LAP significantly speeds-up policy learning, iscompetitive on the D4RL benchmark and especially outperforms on the V-D4RL benchmark, raisingthe best average score across all datasets from previously 31.5 to 58.8. LimitationsDepending on the dataset and environment the effectiveness of C-LAP differs: Datasetswhich only contain random actions are challenging for learning a generative action model, thus we donot include them in our evaluation. The effect of jump-starting policy learning with the latent actiondecoder to already achieve high rewards in the beginning of policy training is prominent in narrowdatasets, but less effective for diverse datasets. While training the model of C-LAP does not requireadditional gradient steps, it still takes more time compared to LOMPO and Offline DV2 asthe latent action state-space model is more complex than a usual latent state-space model.",
  "Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep reinforcementlearning in a handful of trials using probabilistic dynamics models. CoRR, abs/1805.12114,2018": "James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, DougalMaclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and QiaoZhang. JAX: composable transformations of Python+NumPy programs, 2018. Patrick Kidger and Cristian Garcia. Equinox: neural networks in JAX via callable PyTreesand filtered transformations. Differentiable Programming workshop at Neural InformationProcessing Systems 2021, 2021.",
  "for update step t = 1..T do": "Sample batch of trajectories {(ot, at, rt, tt)}k+Lt=k DCompute latent states st q(st | st1, at1, ot)Sample random starting state sinit from each trajectoryCreate dream rollouts {(s, u, a)}t+H=t from sinit using p(st | st1, ut1) andat p(at | st, g((ut | st), p(ut | st))Predict rewards r p(r | s), termination t p(t | s) and valuesv v(u | s)Compute value estimates V kN(s) via Equation (6)Update policy parameters + t+H=t V kN(s)Update critic parameters + t+H=t12v(s) V kN(s)end forend for",
  "CComputational Resources": "We use a different hardware setup for experiments with visual observations and experiments withlow-dimensional features as observations. We run all experiments on a shared local cluster. C-LAPexperiments with visual observations take around 10 hours on a RTX8000 GPU and experiments withlow-dimension feature observations around 11 hours on a A100 GPU. We aim to execute most of ourcode on GPU and parallelize our implementation. Environment evaluations represent a bottleneck asthey require execution on CPU. Overall, it takes around 70 combined GPU days to reproduce thebenchmark results of all methods. This does not include the compute required for the evaluation ofpreliminary implementations or hyper-parameter settings.",
  "DImplementation Details": "We implement all methods in JAX using Equinox . We provide the hyper-parameters of C-LAP in and the constraint values used for the D4RL benchmark in and for the V-D4RLbenchmark in . In general we consider constraint values in the range {0.5, 1.0, 2.0, 3.0}. We implement MOPO and MOBILE following and use the respective hyper-parameters providedin the publication. For MOPO, we select the max-aleatoric version. As no hyper-parameters areprovided for the expert datasets of the D4RL benchmark we sweep through the range specified in and use the one selected in the table. For the antmaze environment we additionally evaluate",
  "ED4RL and V-D4RL benchmark results": "We provide the benchmark results in the usual offline reinforcement learning table format. As ourimplementations might differ from the original one, we include the original scores from the paperalongside our results in tables. We also include the average over all datasets except the expert datasetsto make it comparable to the average scores provided in the respective publication.",
  "FAction distribution": "To evaluate the claim that latent actions generated by the latent action prior are close to the datasetsaction distribution we use the following approach: We randomly select one trajectory from thehopper-expert-v2 dataset and employ k-nearest neighbors to identify the 20 nearest observations andtheir corresponding actions within the whole dataset for each step. We then sample from the actionprior and decoder to generate actions based on the nearest observations at each step. Thereafter,we split the selected trajectory into sections from leaving the ground to touching the ground and",
  ": Results on the V-D4RL benchmark. Showing normalized returns and standard deviations atthe end of policy training": "fit normal distributions to the aggregated dataset actions and reconstructed prior actions. Thus, weend up with an approximation of the datasets action distribution and an approximation of the actiondistribution generated by the prior for each step in the trajectory. shows the aggregateddistributions (from leaving the ground to touching the ground) for one exemplary trajectory of thehopper-expert-v2 dataset.",
  "GValue overestimation for the considered baselines": "We further report value estimates alongside normalized returns on all walker2d datasets for theconsidered baselines (). As they estimate Q-values, we calculate the corresponding valueestimates by averaging over 10 actions sampled from their respective policies. MOPO and MOBILEhave a low value estimate, which can be attributed to the incorporated uncertainty penalties. PLASsvalue estimates are only stable for walker2d-expert-v2 datasets, but collapse for the other considereddatasets. Overall, it seems that value overestimation is not the cause for degrading performance forthese methods. : Comparison of the datasets action distribution to the distribution of actions sampled fromthe latent action prior and latent action decoder. The figure shows the aggregated distributions (fromleaving the ground to touching the ground) for one exemplary trajectory of the hopper-expert-v2dataset : Evaluation of value overestimation for all baselines. We plot mean and standard deviationof normalized returns and value estimates over 3 seeds. Moreover, we add the datasets average returnand average maximum value estimate indicated by dashed lines."
}