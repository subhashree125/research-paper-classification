{
  "Abstract": "We study learning in a dynamically evolving environment modeled as a Markovgame between a learner and a strategic opponent that can adapt to the learnersstrategies. While most existing works in Markov games focus on external regretas the learning objective, external regret becomes inadequate when the adversariesare adaptive. In this work, we focus on policy regret a counterfactual notion thataims to compete with the return that would have been attained if the learner hadfollowed the best xed sequence of policy, in hindsight. We show that if the oppo-nent has unbounded memory or if it is non-stationary, then sample-efcient learn-ing is not possible. For memory-bounded and stationary, we show that learningis still statistically hard if the set of feasible strategies for the learner is exponen-tially large. To guarantee learnability, we introduce a new notion of consistentadaptive adversaries, wherein, the adversary responds similarly to similar strate-gies of the learner. We provide algorithms that achieve",
  "Introduction": "Recent years have witnessed tremendous advances in reinforcement learning for various chal-lenging domains in AI, from the game of Go [Silver et al., 2016, 2017, 2018], real-time strategygames such as StarCraft II [Vinyals et al., 2019] and Dota [Berner et al., 2019], autonomous driving[Shalev-Shwartz et al., 2016], to socially complex games such as hide-and-seek [Baker et al., 2019],capture-the-ag [Jaderberg et al., 2019], and highly tactical games such as poker game Texas holdem [Moravck et al., 2017, Brown and Sandholm, 2018]. Notably, most challenging RL applica-tions can be systematically framed as multi-agent reinforcement learning (MARL) wherein multiplestrategic agents learn to act in a shared environment [Yang and Wang, 2020, Zhang et al., 2021]. Despite the empirical successes, the theoretical foundations of MARL are underdeveloped, espe-cially in settings where the learner faces adaptive opponents who can strategically adapt and reactto the learners policies. Consider for example the optimal taxation problem in the AI economist[Zheng et al., 2020], a game that simulates dynamic economies that involve multiple actors (e.g., thegovernment and its citizens) who strategically contribute to the game dynamics. The governmentagent learns to set a tax rate that optimizes for the economic equality and productivity of its citizens,whereas the citizens who perhaps have their own interests, respond adaptively to tax policies of thegovernment agent (e.g., relocating to states that offer generous tax rates). Such adaptive behaviorof participating agents is a crucial component in other applications as well, e.g., mechanism de-sign [Conitzer and Sandholm, 2002, Balcan et al., 2005], optimal auctions [Cole and Roughgarden,2014, Dtting et al., 2019].",
  ": Summary of main results for learning against adaptive adversaries. Learners policy set isall deterministic Markov policies. m = 0 + stationary corresponds to standard single-agent MDPs": "The question of learning against adaptive opponents has been mostly studied under the frameworkof external regret, wherein the agent is required to compete with the best xed policy in hind-sight [Liu et al., 2022]. However, external regret is not adequate to study adaptive opponents asit does not take into account the counterfactual response of the opponents. This motivates us tostudy MARL using the framework of policy regret [Arora et al., 2012], a counterfactual notion thataims to compete with the return that would have been attained if the agent had followed the best xedsequence of policy in hindsight. Even though policy regret is now a standard notion to study adap-tive adversaries and has been extensively studied in online (bandit) learning [Merhav et al., 2002,Arora et al., 2012, Malik et al., 2022] and repeated games [Arora et al., 2018], it has not receivedmuch attention in a multiagent reinforcement learning setting. In this paper, we aim to ll in thisgap. We consider two-player Markov games (MGs) [Shapley, 1953, Littman, 1994] as a model forMARL, wherein one agent (the learner) learns to act against an adaptive opponent. We provide aseries of negative and positive results for policy regret minimization in Markov games, highlight-ing the fundamental limits of learning and showcasing key principles underpinning the design ofefcient learning algorithms against adaptive adversaries. Fundamental barriers.We rst show that any learner must incur a linear policy regret againstan adaptive opponent who can adapt and remember the learners past policies (Theorem 1). Whenthe opponent has a bounded memory span, any learner must require an exponential number of sam-ples ((SA)H/2) to obtain an -suboptimal policy regret, even with the weakest form of memorywherein the opponent is oblivious (Theorem 2). When the memory-bounded opponents response isstationary, i.e., the response function does not vary with episodes, learning is still statistically hardwhen the learners policy set is exponentially large, as in this case the policy regret necessarily scalespolynomially with the cardinality of the learners policy set (Theorem 3). Efcient algorithms.Motivated by these statistical hardness results, we consider a structural con-dition on the response of the opponents, which we refer to as consistent behavior, wherein the oppo-nent responds similarly to similar sequences of policies (Denition 5). We propose two algorithmsOPO-OMLE (Algorithm 1) and APE-OVE (Algorithm 3) that obtain",
  "For memory length m = 1: We show that OPO-OMLE obtains a policy regret upper bound ofO(H3S2AB +": "H5SA2BT ), when the learners policy set is the set of all deterministic Markovpolicies, where H is the episode length, S is the number of states, A and B are the numbers ofactions for the learner and the opponent, respectively, and T is the number of episodes. For general memory length m 1: We show that APE-OVE obtains a policy regret upper bound",
  "Related work": "Learning in Markov games.Learning problems in Markov games have been studied extensivelyin the MARL literature. Most existing works focus on learning Nash equilibria either with known dy-namics or innite data [Littman, 1994, Hu and Wellman, 2003, Hansen et al., 2013, Wei et al., 2020],or otherwise in a self-play setting wherein we control all the players [Wei et al., 2017, Bai et al.,2020, Bai and Jin, 2020, Xie et al., 2020, Liu et al., 2021], or in an online setting wherein we con- trol one player to learn against other potentially adversarial players [Brafman and Tennenholtz, 2002,Wei et al., 2020, Tian et al., 2021, Jin et al., 2022]. Other related work focuses on exploiting sub-optimal opponents via no-external regret learning [Liu et al., 2022] and studying Stackelberg equi-libria in two-player general-sum turn-based MGs, wherein only one player is allowed to take actionsin each state [Ramponi and Restelli, 2022]. Policy regret in online learning settings.Policy regret minimization has been studied mostly inonline (bandit) learning problems. It was rst studied in a full information setting [Merhav et al.,2002] and extended to the bandit setting and more powerful competitor classes using swap regretand -regret [Arora et al., 2012]. A lower bound of T 2/3 on policy regret in a bandit setting wasprovided by Dekel et al. and was later extended to action space with metric [Koren et al.,2017a,b]. A long line of works studies (complete) policy regret in tallying bandits, wherein anactions loss is a function of the number of the actions pulls in the previous m rounds [Heidari et al.,2016, Levine et al., 2017, Seznec et al., 2019, Lindner et al., 2021, Awasthi et al., 2022, Malik et al.,2022, 2023]. Beyond online (bandit) learning, policy regret has been studied in several more challenging settings.In Arora et al. authors study the notion of policy equilibrium in repeated games (Markovgames with H = S = 1) when agents follow no-policy regret algorithms. A more complete charac-terization of the learnability in online learning with dynamics, where the loss function additionallydepends on time-evolving states, was given in Bhatia and Sridharan . Finally, in Dinh et al., authors study policy regret in online MDP, where an adversary who follows a no-externalregret algorithm generates the loss functions, which effectively alleviates policy regret minimizationto the standard external regret minimization in online MDPs.",
  "Problem setup": "Markov games.In this paper, we use the framework of Markov Games to study an interactivemulti-agent decision-making and learning environment [Shapley, 1953]. Markov games extendMarkov decision processes (MDPs) to multiplayer scenarios, where each agents action affects notonly the environment but also the subsequent state of the game and the actions of other agents. For-mally, a standard two-player Markov Game (MG) is specied by a tuple M = (S, A, B, H, P, r).Here, S denotes the state space with cardinality |S| = S, A is the action space of the rst player(called learner) with cardinality |A| = A, B is the action space of the second player (referred to asan opponent or an adversary) with cardinality |B| = B, H N is the time horizon for each game.P = {P1, . . . , PH} are the transition kernels with each Ph : S A B (S) specifying theprobability of transitioning to the next state given the current state, learners action, and adversarysaction ((S) denotes the set of all probability distributions over S). Finally, r = {r1, . . . , rH} arethe (expected) reward functions with each rh : S A B . For simplicity, we assume thelearner knows the reward function.1 Each episode begins in a xed initial state s1. At step h [H], the learner observes the statesh and picks her action ah A while the opponent/adversary picks an action bh B. As aresult, the learner observes bh, receives reward rh(sh, ah, bh) and the environment transitions tosh+1 Ph(|sh, ah, bh). The episode terminates after H steps. Policies and value functions.A learners policy (also referred to as strategy) is any tuple ={h}h[H] where h : (S A)h1 S (A). A policy = {h}h[H] is said be Markovianif for every h [H], h : S (A). Similarly, an adversarys policy is any tuple = {h}h[H]where h : (S B)h1S (B). is said to be Markovian if for every h, h : S (B). Forsimplicity, we will focus only on Markov policies for both the learner and the adversary in this paper.Let (respectively, ) be the set of all feasible policies of the learner (respectively, the adversary).The value of a policy tuple (, ) at step h in state s, denoted by V ,h(s) is the expectedaccumulated reward starting in state s from step h, if the learner and the adversary follow and respectively, i.e., V ,h(s) := E,[Hl=h rl(sl, al, bl)|sh = s], where the expectation is with respectto the trajectory (s1, a1, b1, r1, . . . , sH, aH, bH, rH) distributed according to P, , and . We alsodenote the action-value function Q,h (s, a, b) := E,[Hl=h rl(sl, al, bl)|(sh, ah, bh) = (s, a, b)].",
  "Given a V : S R, we write PhV (s, a, b) := EsPh(|s,a,b)[V (s)]. For any u : S (A),v:S (B), Q:SAB R, denote Q(s, u, v) := Eau(|s),bv(|s)[Q(s, a, b)] for any s S": "Adaptive adversaries.We allow the adversary to be adaptive, i.e., the adversary can choose theirpolicy in episode t based on the learners policies on episodes 1, . . . , t. We assume that the adversaryis deterministic and has unlimited computational power, i.e., the adversary can plan, in advance,using as much computation as needed, as to how they would react in each episode to any sequence ofpolicies. Formally, the adversary denes in advance a sequence of deterministic functions {ft}tN,where ft : t . The input to each response function ft is an entire history of the learnerspolicies, including her policy in episode t. Therefore, if the learner follows policies 1, . . . , t, theadversary responds with policy ft(1, . . . , t) in episode t. Since the response function ftdepends on the learners policy at round t, our setup is essentially a principal-follower model, akinto Stackelberg games [Letchford et al., 2009, Blum et al., 2014] and mechanism design for learningagents [Braverman et al., 2019]. In this context, the principal agent (mechanism designer or learner)publicly declares a strategy before committing to it, allowing the followers to subsequently choosetheir strategies based on their understanding of the principals decisions. We evaluate the learners performance using the notion of policy regret [Merhav et al., 2002,Arora et al., 2012], which compares the return on the rst T episodes to the return of the best xedsequence of policy in hindsight. Formally, the learners policy regret after T episodes is dened as",
  "t times).(1)": "Policy regret has been studied in online (bandit) learning [Merhav et al., 2002, Arora et al., 2012]and repeated games [Arora et al., 2018], yet, to the best of our knowledge, it has never been studiedin Markov games. Policy regret differs from the more common denition of external regret denedas R(T ) = supTt=1 V ,ft(1,...,t)1(s1) V t,ft(1,...,t)1(s1), which is used in [Liu et al., 2022]. However, external regret is inadequate for measuring the learners performance against anadaptive adversary. Indeed, when the adversary is adaptive, the quantity V ,ft(1,...,t)1is hardlyinterpretable anymore see [Arora et al., 2012] for a more detailed discussion.",
  "As a warm-up, we show in the following example that, policy regret minimization generalizes thestandard Nash equilibrium learning problem in zero-sum two-player Markov games": "Example 3.1 (Nash equilibrium). Consider the adversary with the following behavior: for anyMarkov policy of the learner, the adversary ignores all the learners past policies and respondonly to the current policy with a Markov policy f() such that for all (s, h), V ,f()h(s) =min V ,h(s), where the minimum is taken over all the possible Markov policies for the adver-sary. By Filar and Vrieze , such an f() exists. In addtion, there also exists a Markov policy such that for all (s, h), V ,f()h(s) = sup V ,f()h(s) = inf sup V ,h(s). The policies(, f()) is a Nash equilibrium [Nash, 1950] of the Markov game. For such an adversary, thepolicy regret becomes PR(T ) = Tt=1 V ,f()1(s1)Tt=1 V t,f(t)1(s1). This Nash equilibriumcan be computed using, e.g., the Q-ol algorithm of [Tian et al., 2021] with",
  "Fundamental barriers for learning against adaptive adversaries": "In this section, we show that achieving low policy regret in Markov games against an adaptiveadversary is statistically hard when (i) the adversary has an unbounded memory (see Denition 1),or (ii) the adversary is non-stationary, or (iii) the learners policy set is exponentially large (even ifthe adversary is memory-bounded and stationary).",
  "Q-ol algorithm solves a problem that is a bit more general than the policy regret minimization in": "Example 3.1 in that as long as the benchmark is the Nash value V ,f()1, regardless of the behavior of theadversary, the said rate for the policy regret is guaranteed. V-learning algorithm of Jin et al. solves asimilar problem but in a self-play setting; it is not immediately clear if their rate remains in the online setting.",
  "Theorem 1. For any learner, there exists an adaptive adversary and a Markov game instance suchthat PR(T ) = (T )": "The construction in the proof of Theorem 1, shown in Appendix A.1, takes advantage of the un-bounded memory of the adversary, that can remember the policy the learner takes in the rst episode.This motivates us to consider memory-bounded adversaries, a situation that is quite similar to theonline bandit learning setting of Arora et al. . Denition 1 (m-memory bounded adversaries). An adversary {ft}tN is said to be m-memorybounded for some m 0 if for every t and policy sequence 1, . . . , t, we have ft(1, . . . , t) =ft(min{1,tm+1}, . . . , t). Is it possible to efciently learn against memory-boundedadversaries? Unlike online bandit learning,we show that learning in Markov games is statistically hard even when the adversary is memory-bounded, even for the weakest case of memory m = 0 and the adversarys policy set is small. Theorem 2. For any learner and any L N and S, A, H, there exists an oblivious adversary (i.e.,m = 0) with the policy space of cardinality at least L, a Markov game (with SA + S states, Aactions for the learner, B = 2S actions for the adversary) such that PR(T ) =",
  "T (SA/L)L": "Theorem 2 claims that competing even with an oblivious adversary that employs a small set of poli-cies takes an exponential number of samples (e.g., set S = L = H). The construction of the lowerbound follows the construction used to prove a lower bound for learning latent MDPs [Kwon et al.,2021] and a reduction of a given latent MDP into a Markov game [Liu et al., 2022]; we give com-plete details in Appendix A.2. The proof of Theorem 2 utilizes the fact that the sequence of responsefunction an adversary utilizes can be completely arbitrary. It implies that we need to constrain theadversary further beyond being memory-bounded. A natural restriction we consider given the con-struction is to assume stationarity, i.e. consider adversaries whose response functions do not changeover time.",
  "if there exists an f : m such that for all t and 1, . . . , t, we have ft(1, . . . , t) =f(min{1,tm+1}, . . . , t)": "The stationary behavior is sometimes also referred to as g-restricted in the online learningliterature see the related discussion of Malik et al. . In the special case wherein the adversaryis both stationary and oblivious (i.e., m = 0), the Markov game reduces to the standard single-agentMDP (and the policy regret reduces to standard regret of the MDP) this setting has been studied in[Zhang et al., 2023]. We, therefore, only need to consider m 1. Connections to Stackelberg equilibrium in general-sum Markov games.While seemingly re-strictive, policy regret minimization with m-memory bounded and stationary adversaries alreadysubsumes the problem of learning Stackelberg equilibrium [Von Stackelberg, 2010] in general-sumMarkov games [Ramponi and Restelli, 2022].3 In general-sum Markov games, the adversary (fol-lower) aims at maximizing his own reward function given any policy of the learner (leader). Thatis, the adversary is 1-memory bounded, and the response function f : corresponds to afunction that selects the best response policy to any given policy of the learner. The benchmarkmax V ,f()1in policy regret then becomes the Stackelberg equilibrium. Is sample-efcient learning possible against m-memory bounded and stationary adversaries? Onecan notice an immediate approach to learning against a 1-memory bounded and stationary adver-saries is to simply view the problem as a ||-armed bandit problem and apply any state-of-the-artbandit algorithm [Audibert and Bubeck, 2009] to obtain PR(T ) = O(H T ||). However, scalingpolynomially with the learners policy class is not desirable when the class is exponentially large(e.g., when the learners policy class is the set of all deterministic policies, then || = (AHS)).And in fact, we cannot avoid polynomial scaling with the cardinality of the learners policy class ingeneral. 3Ramponi and Restelli consider a more restrictive setting of turn-based Markov games, wherein ateach state only one player is allowed to take actions. In addition, they require the opponents to respond withonly deterministic policies.",
  "Efcient algorithms for learning against adaptive adversaries": "Thus far, we have shown that learning against an adaptive adversary in Markov games is statisticallyhard, even when the adversary is m-memory bounded and stationary. The reason that stationarity isnot sufcient for efcient learning (which the lower bound in Theorem 3 exploits for the constructionof a hard instance) comes from the unstructured response of the adversary in the worst case. Evenif the learner plays nearly identical sequence of policies differing only on a small number of statesand steps, the adversary can essentially respond completely arbitrarily. In other words, knowingthe policies that the adversary plays in response to the policies of the learner (i.e., observing thevalues of the response function f at specic inputs) reveals zero information about the function fon previously seen inputs. Thus, the learner is required to explore all the policies in to be ableto identify an optimal policy. This motivates us to consider an additional structural assumption onhow the adversary responds to the learners policies. We assume that the adversary is consistent inresponse to two similar sequences of policies of the learner. In essence, given that the learner playstwo sequences of policies that agree on certain states (s) and steps (h) then, we assume that theopponent also responds with two sequences of policies that agree on the same states and steps. Werefer to this behavior as consistent; a formal denition follows.Denition 3 (Consistent adversaries). An m-memory bounded and stationary adversary f is saidto be consistent if, for any two sequences of learners policies 1, . . . , m and 1, . . . , m, and any(s, h) S [H], if ih(|s) = ih(|s), i [m], then f(1, . . . , m)h(|s) = f(1, . . . , m)h(|s).Otherwise, we say that the opponents response f is arbitrary. We argue that the denition above is natural if we are to consider opponents that are self-interestedstrategic agents, and not simply a malicious adversary. So, it would be in an opponents interest toplay in a somewhat consistent manner. Playing optimally after guring out the learners strategywould indeed require playing consistently. An opponent that plays completely arbitrary, while chal-lenging to learn anything from, also does not improve their value function. Some remarks are inorder.Remark 1 (-approximately consistent adversaries). Our algorithms and results for consistent ad-versaries easily extend to -approximately consistent adversaries for any xed constant 0. Anadversary f is said to be -approximately consistent if, for any 1, . . . , m and 1, . . . , m, and",
  "w.r.t. l1 norm, dened as the minimum number of -brackets [l, u] := {P P : l P u} withl u1 , that are needed to cover P": "Intuitively, restricting the adversary to be consistent, allows the learner to predict the opponentsresponse from previous episodes to similar settings. The learner can collect the data from whatthe adversary responds to and learn his response function. Given the consistent behavior, for every(h, s) [H] S, the number of action distributions h(|s) that the adversary can respond withcannot exceed the number of possible action distributions h(|s) that the learner can construct instate s at step h. Given is the set of all deterministic policies, we only need to learn HSA actiondistributions that the adversary can respond at any state and step. We begin with the oblivious caseof m = 1 and end up resolving the general case m 1 after.",
  "We rst consider the memory length of m = 1 for stationary and consistent adversaries": "Algorithm.We propose OPO-OMLE (Algorithm 1), which represents Optimistic Policy Opti-mization with Optimistic Maximum Likelihood Estimation. OPO-OMLE is a variant of the op-timistic value iteration algorithm of [Azar et al., 2017], wherein we build an upper condencebound on the value function V ,f()1for any policy , using a bonus function and optimistic MLE[Liu et al., 2023]. The upper condence bound is based on two levels of optimism: a bonus term that is based on condence intervals on the transition kernels P and the parameter version spaces{hsa} of the adversarys response at each level (h, s, a). The parameter version spaces construct aset of parameters that are close to the MLE solution, up to an error , in terms of the log-likelihoodin the observed actions taken by the adversary.",
  "Algorithm 1 Optimistic Policy Optimization with Optimistic MLE (OPO-OMLE)": "1: Input: Bonus function : N R, and MLE condence parameter 2: Initialize: hsa , Dhsa , Nh(s, a, b) 0, Nh(s, a, b, s) 0, (h, s, a, b, s) S A B S3: for episode t = 1, . . . , T do4:t arg maxDOUBLY_OPTIMISTIC_VALUE_ESTIMATE(N, {Di}, {i}, , ) (Algorithm 2)5:Play t (the opponent responds with f(t)) to observe (st1, at1, bt1, rt1, . . . , stH, atH, btH, rtH)6:h: Nh(sth, ath, bth) Nh(sth, ath, bth)+ 1, Nh(sth, ath, bth, sth+1) Nh(sth, ath, bth, sth+1)+1, Dhsthath Dhsthath {bth}, and hsthath { hsthath : bDhsthathlog P(b)",
  "Theorem 4 shows that OPO-OMLE achieves": "T-policy regret bounds against 1-memory bounded,stationary and consistent adversaries in Markov games. Notably, the policy regret depends onlyon the log-cardinality of the learners policy class and the log-bracketing number of the set ofaction distributions with which the adversary responds to the learner. Since || = AHS, the boundtranslates into PR(T ) = O(H3S2AB +",
  "Memory of any xed length m 1": "We now consider the general case of stationary and consistent adversaries that have a memory of anyxed length m 1. Note that we assume that the learner knows (an upper bound of) m. Playingagainst a 1-memory bounded adversary does not stop the learner from changing her policies often,as the adversary does not remember any policies that the learner has taken previously. However, asublinear policy regret learner against m-memory bounded adversaries should switch her policies asless frequently as possible, and at most only sublinear time switches. The reason is that every policyswitch will add a constant cost to policy regret, as the benchmark in the policy regret is with the bestxed sequence of policy. This makes the regret minimizer OPO-OMLE unable to generalize fromm = 1 to any xed m. Instead, we propose a low-switching algorithm, in which the learner learns toplay exploratory policies repeatedly over consecutive episodes so that the switching cost is reduced.Here, as in Jin et al. , exploratory policies are those with good coverage over the state spacefrom which uniform policy evaluation can be performed to identify near-optimal policies. Algorithm.We propose APE-OVE (Algorithm 3), which represents Adaptive Policy Eliminationby Optimistic Value Estimation. APE-OVE generalizes the adaptive policy elimination algorithm of[Qiao et al., 2022] for MDPs to Markov games with unknown opponents. The high-level idea of ouralgorithm is as follows. The learner maintains a version space k of remaining high-quality policiesafter each epoch which is a sequence of consecutive episodes with an appropriate length (epoch khas a length of HSAB(m 1 + Tk) in APE-OVE). Layerwise exploration (Line 5 of Algorithm 3): Within each epoch, the learner performs layer-wise exploration (Algorithm 4), wherein we devise high-coverage sampling policies khsab thataim at exploring (s, a, b) in step h and epoch k, starting from the lowest layer h = 1 up to the high-est layer h = H. However, some states might not be visited frequently by any policy, thus takinga large amount of exploration. They, fortunately, do not signicantly affect the value functionsof any policy and thus can be identied (by storing in Uk) and removed from exploration quickly(via the truncated transition kernel estimates P obtained in Algorithm 5). Layerwise explorationrequires value estimation uniformly over all policies. However, the learner does not know the ad-versarys response f. To address this, we use optimistic value estimation via the optimistic MLEin the collected data of the adversarys moves (Algorithm 6).",
  "Algorithm 4 LAYERWISE_EXPLORATION(k, Tk)": "1: Input: Policy version space k, number of episodes Tk2: Initialize: P k = { P kh }h[H] arbitrary transition kernels, Uk = , khsa = , (h, s, a), D = ,N kr h(s, a, b, s) = 0, (h, s, a, b, s), and for each (h, s, a, b), 1hsab is the reward function r suchh(s, a, b) = 1{(h, s, a, b) = (h, s, a, b)}3: for h = 1, . . . , H do4:for (s, a, b) S A B do5:khsab = arg maxkOPTIMISTIC_VALUE_ESTIMATE(, 1hsab, P k, k)",
  ":Uk Uk {(h, s, a, b, s) : N kh(h, s, a, b, s) cH2 log(SABHK/)}12:P kh = TRANSITION_ESTIMATE(h, N kh, Uk, s) (Algorithm 5)13:Reset D = 14: end for15: Output: P k = { P kh }h[H], k, Uk": "value estimation based on the empirical transition kernels P k, the parameter version space kand the set of infrequent transition samples Uk given any reward function r. The version spaceis designed in such a way that the expected value for the learner to play any policy from theversion space is guaranteed to be no worse than O(1/",
  "Tk) compared to the optimal, with highprobability": "Note that we do not directly use the reward function r in the version space renement. Instead, weuse a truncated reward function rU k that is zero for any (h, s, a, b, s) in the infrequent transitionset Uk. This truncated design is critical to our analysis and the subsequent guarantees, e.g., seeLemma B.10. For the truncated reward functions, the backup step in Algorithm 6 should be under-stood as: Qh(s, a, b) = Es P kh (|s,a,b)rh(s, a, b)1{(h, s, a, b, s) / Uk} + V h+1(s), (s, a, b).",
  "Algorithm 6 OPTIMISTIC_VALUE_ESTIMATE(, r, P, )": "1: Input: reward function r, policy , transition kernel P, parameter version space 2: Initialize: V H+1() = 03: for h = H, H 1, . . . , 1 do4:Qh(s, a, b) = rh(s, a, b) + [Ph V h+1](s, a, b), (s, a, b)5:V h (s) = maxhsh(s) Qh(s, h(s), P), s Optimistic MLE",
  ": end for7: Output: V 1 (s1)": "The minimum positive visitation probability which has also been used recently to characterizeinstance-dependent bounds for PAC RL [Tirinzoni et al., 2023], is the minimal probability that anystate-action pair can be visited at a time step, given they can be visited at all. This implies thatduring the exploration phase if we try a certain policy for N episodes and encounter (s, a) at steph (in any episode), on average, would visit (h, s, a) for Nd times out of N episodes. This, alongwith the assumption that the adversary is consistent enables us to estimate the adversarys responseto any (h, s, a) that is visited within an estimation error of order 1/ Nd. Note that we do not needto take care of the adversarys response to any (h, s, a) that is not visited as these tuples are deemedinfrequent by any policy and thus have negligible impact on the value estimation.",
  "Theorem 5 asserts a": "T policy regret bound against m-memory bounded, stationary, and consistentadversaries in Markov games. Notably, our bounds grow linearly with memory length m. Comparedto the bound in Theorem 4, given T is sufciently large, the bound in Theorem 5 deals with thegeneral memory length m at the cost of a worse dependence on all other factors H, S, A, B, d.Dealing with -approximately consistent adversaries (see Remark 1) will incur an additional termO(T ) to the policy regret.",
  ",we do not know if the dependence on the minimum positive visitation probability d when learningagainst m-memory bounded opponents is necessary. In other words, can we derive minimax bounds": "that hold for any problem instance, regardless of how small d is, for the case of general m? Whileit seems to us that such a dependence is necessary (as it seems difcult otherwise to learn the oppo-nents response while also learning high-return policies), yet we are unable to prove or reject thisconjecture. Second , as we state in Remark 2, we do not currently know the necessary conditionson the opponents response functions for learnability in this setting. This might as well require analternate condition that generalizes our notion of consistent behaviors and fully characterizes thepredictability of the opponent (in a similar way as the VC dimension characterizes learnability instatistical learning theory). Third , our theory currently views information, and not computation, asthe main bottleneck and aims for policy regret minimization without worrying about computationalcomplexity. As a result, some of the steps in our algorithms happen to be computationally inefcient.In particular, selecting a policy that maximizes the optimistic value function requires iterating overthe learners policy set, which is exponentially large. Can we hope for computationally efcientno-policy regret algorithms in Markov games? Fourth , our policy regret bounds scale with the cardi-nality of the state space and the action space, which could be large in many practical settings. Canwe avoid such dependence by employing function approximation (e.g., neural networks)?",
  "and Disclosure of Funding": "This research was supported, in part, by the DARPA GARD award HR00112020004, NSF CA-REER award IIS-1943251, funding from the Institute for Assured Autonomy (IAA) at JHU, and theSpring22 workshop on Learning and Games at the Simons Institute for the Theory of Computing. Alekh Agarwal, Sham Kakade, Akshay Krishnamurthy, and Wen Sun. Flambe: Structural complex-ity and representation learning of low rank MDPs. Advances in neural information processingsystems, 33:2009520107, 2020. 18",
  "Bowen Baker, Ingmar Kanitscheider, Todor Markov, Yi Wu, Glenn Powell, Bob McGrew, and IgorMordatch. Emergent tool use from multi-agent autocurricula. arXiv preprint arXiv:1909.07528,2019. 1": "M-F Balcan, Avrim Blum, Jason D Hartline, and Yishay Mansour. Mechanism design via machinelearning. In 46th Annual IEEE Symposium on Foundations of Computer Science (FOCS05),pages 605614. IEEE, 2005. 1 Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemysaw Debiak, ChristyDennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with largescale deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019. 1",
  "Ofer Dekel, Jian Ding, Tomer Koren, and Yuval Peres. Bandits with switching costs: T 2/3 regret. InProceedings of the forty-sixth annual ACM symposium on Theory of computing, pages 459467,2014. 3": "Le Cong Dinh, David Henry Mguni, Long Tran-Thanh, Jun Wang, and Yaodong Yang. OnlineMarkov decision processes with non-oblivious strategic adversary. Autonomous Agents and Multi-Agent Systems, 37(1):15, 2023. 3 Omar Darwiche Domingues, Pierre Mnard, Emilie Kaufmann, and Michal Valko. Episodic rein-forcement learning in nite MDPs: Minimax lower bounds revisited. In Algorithmic LearningTheory, pages 578598. PMLR, 2021. 8 Paul Dtting, Zhe Feng, Harikrishna Narasimhan, David Parkes, and Sai Srivatsa Ravindranath.Optimal auctions through deep learning. In International Conference on Machine Learning, pages17061715. PMLR, 2019. 1",
  "Tomer Koren, Roi Livni, and Yishay Mansour. Multi-armed bandits with metric movement costs.Advances in Neural Information Processing Systems, 30, 2017b. 3": "Jeongyeol Kwon, Yonathan Efroni, Constantine Caramanis, and Shie Mannor. RL for latent MDPs:Regret guarantees and a lower bound. Advances in Neural Information Processing Systems, 34:2452324534, 2021. 5, 17 Joshua Letchford, Vincent Conitzer, and Kamesh Munagala. Learning and approximating the opti-mal strategy to commit to. In Algorithmic Game Theory: Second International Symposium, SAGT2009, Paphos, Cyprus, October 18-20, 2009. Proceedings 2, pages 250262. Springer, 2009. 4",
  "Qinghua Liu, Tiancheng Yu, Yu Bai, and Chi Jin. A sharp analysis of model-based reinforcementlearning with self-play. In International Conference on Machine Learning, pages 70017010.PMLR, 2021. 2": "Qinghua Liu, Yuanhao Wang, and Chi Jin. Learning Markov games with adversarial opponents:Efcient algorithms and fundamental limits. In International Conference on Machine Learning,pages 1403614053. PMLR, 2022. 2, 3, 4, 5, 6, 17 Qinghua Liu, Praneeth Netrapalli, Csaba Szepesvari, and Chi Jin. Optimistic MLE: A generic model-based algorithm for partially observable sequential decision making. In Proceedings of the 55thAnnual ACM Symposium on Theory of Computing, pages 363376, 2023. 7, 18",
  "Lloyd S Shapley. Stochastic games. Proceedings of the national academy of sciences, 39(10):10951100, 1953. 2, 3": "David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Masteringthe game of Go with deep neural networks and tree search. nature, 529(7587):484489, 2016. 1 David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of Gowithout human knowledge. nature, 550(7676):354359, 2017. 1 David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez,Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. A general reinforcementlearning algorithm that masters chess, shogi, and go through self-play. Science, 362(6419):11401144, 2018. 1",
  "Yi Tian, Yuanhao Wang, Tiancheng Yu, and Suvrit Sra. Online learning in unknown Markov games.In International conference on machine learning, pages 1027910288. PMLR, 2021. 3, 4": "Andrea Tirinzoni, Aymen Al-Marjani, and Emilie Kaufmann. Optimistic PAC reinforcement learn-ing: the instance-dependent view. In International Conference on Algorithmic Learning Theory,pages 14601480. PMLR, 2023. 10 Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michal Mathieu, Andrew Dudzik, Juny-oung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmasterlevel in StarCraft II using multi-agent reinforcement learning. Nature, 575(7782):350354, 2019.1",
  "A.1Proof of Theorem 1": "Proof of Theorem 1. The construction of a hard problem essentially follows the proof idea ofArora et al. . Policy regret requires the learner to compete with the best xed sequence ofpolicy in hindsight as if she could have changed her past policies. The lower bound utilizes this factto construct an instance such that once the learner picks a particular policy in the rst episode, shewill receive a low reward for the remaining episodes. The only way to achieve a higher reward is togo back in time and select a different policy. More formally, lets consider any learner. Let 1 be a policy that the learner commits in the rstepisode with the highest positive probability p > 0. Note that 1 and p are the inherent propertyof the learner and do not depend on the adversary and the Markov game as in the rst episode, thelearner has zero information about the adversary and the Markov game. Now lets consider theadversary that depends only on the learners policy in the rst episode and nothing else, i.e., for all tand policy sequence 1, . . . , t, ft(1, . . . , t) = f(1) for some function f : . In addition,let f such that f() = if = 1 and f() = otherwise, where and such that for all s,sup=1 V ,1(s) sup V ,1(s) = (1). There exists a Markov game that always guarantees theexistence of such , (the constructions are fairly straightforward). Thus, with probability p, wehave PR(T ) = (T ). Note that the external regret R(T ) for this construction is 0.",
  "A.2Proof of Theorem 2": "Proof of Theorem 2. The proof follows from the two main arguments: (i) a reduction from any latentMDP [Kwon et al., 2021] to a Markov game with an adversary playing policies from a nite set ofMarkov policies, and (ii) a reduction from the notion of regret in latent MDPs to the policy regretw.r.t. an oblivious sequence of Markov policies. Argument (i) is directly taken from [Liu et al., 2022, Proposition 5]. In particular, interacting withany latent MDP [Kwon et al., 2021] of L latent variables, S states, A actions, H time steps, andbinary rewards is equivalent to interacting (from the perspective of the learner) a (simulated) Markovgame against an adversary whose policies are chosen from a set of L Markov policies. In particular,the simulated Markov game has SA+S states, A actions for the learner, 2S actions for the adversary,and 2H time steps (see [Liu et al., 2022, Section A.4] for the detailed construction of the simulatedMarkov game from any latent MDP). Thus, we can utilize any lower bound for latent MDP for theMarkov game (but not vice versa). To continue from Argument (i) and begin with Argument (ii), we recall the denition of latentMDPs [Kwon et al., 2021]. At the beginning of each episode, the nature secretly draws uniformly atrandom from a set of L base MDPs and the learner interacts with this drawn MDP for the episode.[Kwon et al., 2021, Theorem 3.1] show that for any learner, there exists a latent MDP with L baseMDPs such that the learner needs at least ((SA/L)L/2) episodes to identify an -suboptimalpolicy, where the optimality is dened with respect to the average values over the M base MDPs.Note that in the construction of the hard latent MDP instance above, there is a unique optimalpolicy (lets call it ) with respect to the aforementioned optimality notion. Thus, the regret ofthis learner over T episodes competing against is at least ( T (SA/L)L) (the learner suffersan instantaneous regret of every time she fails to identify ). Note again that the regret aboveis the expectation with respect to the uniform distribution over L base MDPs. Thus, there existsa particular realization of a sequence of T base MDPs in a certain order such that the regret withrespect to this sequence when competing with is at least the expected regret with respect to theuniform distribution over L base MDPs, which is ( T (SA/L)L). Finally, note that is alsoan optimal policy with respect to the total value across the sequence of T MDPs since is anoptimal policy for each individual base MDP, per the construction in Kwon et al. . Thus, wecan conclude that, for any learner, there exists a sequence of T MDPs from a set of L MDPs suchthat the regret of the learner with respect to this MDP sequence is (",
  "A.3Proof of Theorem 3": "Proof of Theorem 3. Consider any learner. Consider the adversarys policy space = {, } wherefor all h [H 1], h and h are arbitrary but H(b1|s) = 1, s and H(b2|s) = 1, s, for someb1, b2 B. Let the reactive function f to map all policies but some in to , whereas f() = .Now consider a deterministic Markov game with the following properties. The transition kernel isdeterministic and always traverses through the same sequence of states, regardless of what actionsthe learner and the adversary take. The reward functions are deterministic everywhere, and also zeroeverywhere except that rH(s, a, b2) = 1, s, a. Except for that yields a positive reward if thelearner selects it, all other policies in give zero reward. In addition, since the learner does notknow f and that there is no relation whatsoever between f() and f() for any = , the learnerneeds to play all policies in at least once to be able to identify .",
  "B.1Support lemmas": "Maximum Likelihood Estimation.Let {xi}i[T ] P where . Denote N() the-bracketing number of function class {P : }. The following lemma says that the log-likelihood of the true model in the empirical data is close to that of any model within the modelclass, up to an error that scales logarithmically with the model complexity measured in a bracketingnumber.Lemma B.1. There exists an absolute constant c such that for any (0, 1), with probability atleast 1 , for all t [T ] and , we have",
  "P(xi) c log(N(1/T )T/)": "The following lemma says that any model that is close to the true model in the log-likelihood in thehistorical data would yield a similar data distribution as the true model.Lemma B.2. There exists an absolute constant c such that for any (0, 1), with probability atleast 1 , for all t [T ] and ,",
  "where dT V denotes the total variation distance": "The two lemmas above directly follow from [Liu et al., 2023, Proposition B.1] and [Liu et al., 2023,Proposition B.2], respectively, wherein the analysis built on the classical analysis of MLE [Geer,2000] and the tangent sequence analysis in [Zhang, 2006, Agarwal et al., 2020], respectively. Thefollowing lemma is a direct corollary of Lemma B.1 and Lemma B.2.",
  "Qh(s, a, b) Q,f()h(s, a, b) and V h (s) V ,f()h(s)": "We will prove by induction with h [H + 1]. For h = H + 1, the claim in the lemma triviallyholds. Assume by induction that the claim holds for some h + 1. We will prove that it holds for h.Indeed, for any (s, a, b) such that Qh(s, a, b) = H h+ 1, of course Qh(s, a, b) Q,f()h(s, a, b).Consider any (s, a, b) such that Qh(s, a, b) < H h + 1, we have",
  "V h+1(s) = supV ,f()h+1(s), s": "Note that the optimality above does not require that there exists an optimal policy such thatV h (s) = V ,f()h(s), (h, s). Note that if Qth (xth) = H h + 1, it is trivial that th 0. Thus,we only need to consider when Qth (xth) < H h + 1, and thus",
  "B.3Proof of Theorem 5": "The layerwise exploration stage (Algorithm 4) performs layerwise exploration for each layer h [H] and estimates infrequent transitions into U. Since infrequent transitions do not signicantlyaffect policy evaluation in any way (will be proved precisely later), we can exclude them and quicklyrefrain from exploring them extensively. However, excluding them changes the underlying datadistribution of the experiences that the earner would receive when interacting with the environment.To handle this bias issue, it is often convenient to consider an absorbing Markov game M , arenement of the original Markov game M that excludes all infrequent transitions.Denition 5 (Absorbing Markov games). Given a Markov game M = (S, A, B, r, P, H), a set oftransitions U, and a dummy state s, an absorbing Markov game M = (S {s}, A, B, r, P, H)w.r.t. (M, U, s) is dened as follows: For any (h, s, a, b, s) [H] S A B S,",
  "B.3.1Sampling policies are sufciently exploratory": "We now show that the sampling policies in the reward-free exploration stage are sufciently ex-ploratory over the state-action space of the Markov game. We start with bounding the differencebetween P and P k (Line 5 of Algorithm 3) using empirical Bernsteins inequality.Lemma B.5. Dene the event E: (k, h, s, a, b, s) [K] [H] S A B S such that(h, s, a, b, s) / U,",
  "N kh(s, a, b)": "where := c log(SABHK/) and N kh are the counter at layer h in epoch k obtained at Line 9 byrunning Algorithm 4 in epoch k. Then, we have Pr(E) 1 . In addition, (h, s, a, b, s) U,P kh (s|s, a, b) = Ph(s|s, a, b) = 0. Proof of Lemma B.5. Lemma B.5 is essentially the analogous of [Qiao et al., 2022, Lemma E.2]from MDPs to Markov games. The rst part follows from empirical Bernsteins inequality andunion bound. The second part comes from the denition of the absorbing transition kernels P andthe construction of the empirical transition kernels P k.",
  "Proof of Lemma B.7. The proof essentially follows from the proof of [Qiao et al., 2022,Lemma E.5]": "Lemma B.5 to Lemma B.7 are similar in nature with corresponding lemmas in a single-agentMDP in [Qiao et al., 2022].We now prove a novel lemma thats absent in the single-agent MDP setting yet crucial to our theorem.Recall our notion that,V (r, P, ):=OPTIMISTIC_VALUE_ESTIMATE(, r, P, ) which is given in Algorithm 6.",
  "d2, k [K]. Then, Pr(Ek) 1": "Proof of Lemma B.8. Let us x any (h, s, a, b) and . Note that the value function for any policyunder any dynamic w.r.t. the reward function 1hsab is zero at any step h > h. Also, notice that,prior to the exploration of layer h in the reward-free exploration (Algorithm 4), P k1 , . . . , P kh1 arealready constructed. Additional notations.In OPTIMISTIC_VALUE_ESTIMATE(1hsab, P k, , ) (Algorithm 6),we denote the intermediate value estimates V lby V l (; 1hsab, P k, k) to emphasize the depen-dence on the reward function and the transition dynamics being used. We denote N kof pairs (h, s, a) during the h-th layer exploration of Algorithm 4. We write V ,h(s, a) the counth(s; r, P) in placeof V ,h(s) to emphasize the dependence on the reward function r and transition dynamic P.",
  "+ 1{N kl (s, l(s)) 1} 2maxklsl(s)dT V (f([]m)l(|s), P),": "where we use the convention that max =0, and the last inequality follows from thatP kl (s|s, l(s), b) = 0 if (l, s, l(s), b, s) / Uk, that V l+1(s; 1hsab, P k, k) , and that,for any two distributions p, q |X | over a nite support X, we have dT V (p, q) = 1",
  "2p q1": "If N kl (s, l(s)) = 0, then l(s) = 0. Consider the case N kl (s, l(s)) 1. That means that thestate-action pair (s, l(s)) must be visited in step l at least once by at least one policy klsab forsome (s, a,b) S A B. Note that this policy klsab is run for m 1 + Tk consecutive episodes.Thus, by the denition of the minimum positive visitation probability d, we must have",
  "dTk": "where the rst inequality follows from the rst part of Lemma B.10, the second inequality followsfrom Lemma B.12, the third inequality follows from Lemma B.11, the fourth inequality followsfrom the denition of k+1, the fth inequality follows from Lemma B.11, the sixth inequalityfollows from Lemma B.12, and the seventh inequality follows from the second part of Lemma B.10,and the last inequality follows from Lemma B.14.",
  "B.3.2Uniform policy evaluation": "In this part, we will show that the empirical transition kernel P k constructed from the exploratorydata by our sampling policies is a good surrogate for the true transition kernel P in evaluating thevalue of uniformly all policies. Lemma B.10. Conditioned on the event E in Lemma B.5 and the event Ek in Lemma B.8 and thehigh-probability event in Lemma B.9, with probability at least 1 , for any k [K], any rewardfunction r, and any policy k, we have",
  "First of all, by construction of P k and rU k, we have kh(s) = 0 if s = s or if N kh(s, h(s)) = 0.This explains the very reason we design the truncated reward function rU k": "We now consider s S such that N kh(s, h(s)) > 0. This condition, along with the consistentbehavior and the minimum visitation probability, allows us to estimate the response f([]m)h(|s)sufciently. In particular, f([]m)h(|s) depends only on the data obtained by visiting (h, s, h(s))which is indeed visited at least dTk times, thus can be estimated up to an order of 1/"
}