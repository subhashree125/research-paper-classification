{
  "Abstract": "Partial observability of the underlying states generally presents significant challenges for rein-forcement learning (RL). In practice, certain privileged information, e.g., the access to states fromsimulators, has been exploited in training and has achieved prominent empirical successes. To bet-ter understand the benefits of privileged information, we revisit and examine several simple andpractically used paradigms in this setting. Specifically, we first formalize the empirical paradigmof expert distillation (also known as teacher-student learning), demonstrating its pitfall in find-ing near-optimal policies. We then identify a condition of the partially observable environment,the deterministic filter condition, under which expert distillation achieves sample and computa-tional complexities that are both polynomial. Furthermore, we investigate another useful empir-ical paradigm of asymmetric actor-critic, and focus on the more challenging setting of observablepartially observable Markov decision processes. We develop a belief-weighted asymmetric actor-critic algorithm with polynomial sample and quasi-polynomial computational complexities, inwhich one key component is a new provable oracle for learning belief states that preserve filterstability under a misspecified model, which may be of independent interest. Finally, we also in-vestigate the provable efficiency of partially observable multi-agent RL (MARL) with privilegedinformation. We develop algorithms featuring centralized-training-with-decentralized-execution, apopular framework in empirical MARL, with polynomial sample and (quasi-)polynomial compu-tational complexities in both paradigms above. Compared with a few recent related theoreticalstudies, our focus is on understanding practically inspired algorithmic paradigms, without com-putationally intractable oracles.",
  "Introduction": "In most real-world applications of reinforcement learning (RL), e.g., perception-based robot learn-ing , autonomous driving , dialogue systems , and clinical trials , only partialobservations of the environment state are available for sequential decision-making. Such partial ob-servability presents significant challenges for efficient decision-making and learning, with knowncomputational and statistical barriers under the general model of partially observableMarkov decision processes (POMDPs). The curse of partial observability becomes severer when mul-tiple RL agents interact, where not only the environment state, but also other agents information, arenot fully-observable in decision-making .",
  "arXiv:2412.00985v1 [cs.LG] 1 Dec 2024": "On the other hand, a flurry of empirical paradigms has made partially observable (multi-agent)RL promising in practice. One notable example is to exploit the privileged information that may beavailable (only) during training. The privileged information usually includes direct access to theunderlying states, as well as access to other agents observations/actions in multi-agent RL (MARL),due to the use of simulators and/or high-precision sensors for training. The latter is also knownas the centralized-training-with-decentralized-execution (CTDE) framework in deep MARL, and hasbecome prevalent in practice . These approaches can be mainly categorized intotwo types: i) privileged policy learning, where an expert/teacher policy is trained with privilegedinformation, and then distilled into a student partially observable policy. This expert distillation,also known as teacher-student learning, approach has been the key to some empirical successes inrobotic locomotion and autonomous driving ; ii) privileged value learning, where a valuefunction is trained conditioned on privileged information, and used to improve a partially observablepolicy. It is typically instantiated as the asymmetric actor-critic algorithm , and serves as thebackbone of some high-profiled successes in robotic manipulation and MARL .Despite the remarkable empirical successes, theoretical understandings of partially observableRL with privileged information have been rather limited, except for a few very recent prominentadvances in RL with hindsight observability (see .1 for a detailed discussion). How-ever, most of these theoretically sound algorithms are different from those used in practice, andrequire computationally intractable oracles to achieve provable sample efficiency. The soundness andefficiency of the aforementioned paradigms used in practice remain elusive. In this work, we ex-amine both paradigms of expert distillation and asymmetric actor-critic, with foresight privilegedinformation as in these empirical works. In contrast to , which purely focused on sample ef-ficiency, we aim to understand the benefits of privileged information by examining these practicallyinspired paradigms under several POMDP models, without computationally intractable oracles. Wesummarize our contribution as follows. Contributions.We first formalize the empirical paradigm of expert distillation, and demonstrateits pitfall in distilling near-optimal policies even in observable POMDPs, a model class that wasrecently shown to allow provable partially observable RL without computationally intractable or-acles . We then identify a new condition for POMDPs, the deterministic filter condition, andestablish sample and computational complexities that are both polynomial for expert distillation.The new condition is weaker and thus encompasses several known (statistically) tractable POMDPmodels (see for a summary). Further, we revisit the asymmetric actor-critic paradigm andanalyze its efficiency under the more challenging setting of observable POMDPs above (where ex-pert distillation fails). Identifying the inefficiency of vanilla asymmetric actor-critic, and inspired bythe empirical success in belief-state-learning, we develop a new belief-weighted version of asymmetricactor-critic, with polynomial-sample and quasi-polynomial-time complexities. Key to the results is anew belief-state learning oracle that preserves filter stability under a misspecified model, which maybe of independent interest. Finally, we also investigate the provable efficiency of partially observablemulti-agent RL with privileged information, by studying algorithms under the CTDE framework,with polynomial-sample and (quasi-)polynomial-time complexities in both paradigms above.",
  "Related Work": "Provable partial observable RL.While POMDPs are generally known to be both statistically hard and computationally intractable , a productive line of research has identified several struc-tured subclasses of POMDPs that can be efficiently solved. introduced the class of POMDPs inthe rich-observation setting, where the observation space can be large and fully reveal the under-lying state, where sample-efficient RL becomes possible . introduced k-step decodable",
  "Poly sample +Quasi-poly timeObservablePOSG": ":Comparison of theoretical guaran-tees with/without privileged information.PI:privileged information; STD: structural assump-tions on transition dynamics, e.g., deterministictransition or reachability of all states; SL: su-pervised learning; FA: function approximation;WSE: well-separated emission. : A landscape of POMDP models that par-tially observable RL with privileged informationaddresses, with both statistical and computationalcomplexity considerations. The x and y axes de-note the restrictiveness of the assumptions, onthe emission channels/observations and transitiondynamics, respectively. POMDPs, where the last k observation-action pairs can uniquely determine the state, and proposedpolynomial-sample complexity algorithms (assuming k is a small constant). Beyond settings wherethe underlying state can be exactly recovered, proposed weakly revealing POMDPs, wherethe observations are assumed to be informative enough. Under the weakly revealing condition (andits variant), there has been a fast-growing line of recent works developing sample-efficient RL algo-rithms for various settings, see e.g., . Notably, these algorithms are typi-cally computationally inefficient, requiring access to an optimistic planning oracle for POMDPs. Ona promising note, showed that in observable POMDPs (see Assumption 2.5), one can have quasi-polynomial-time algorithms for computing the near-optimal policy, which further leads to provableRL algorithms with both quasi-polynomial samples and time. RL under hindsight observability.The closest line of research to ours are the recent theoreticalstudies for Hindsight Observable Markov Decision Processes (HOMDPs) , where the underlyingstate is revealed at the end of each episode; see also subsequent related works in with dif-ferent observation feedback models. These works focused purely on sample efficiency, and showedthat polynomial sample complexity can be achieved without (or by further relaxing) aforementionedstructural assumptions of the model (e.g., observability or decodability), in both tabular and/or func-tion approximation settings. However, the algorithms (also) require an oracle for planning or evenoptimistic planning in a learned approximate POMDP, which are not computationally tractable ingeneral. Indeed, without any structural assumption, learning the optimal policy in HOMDPs iscomputationally no easier than the planning problem, which thus remains PSPACE-hard . Mean-while, even under the additional assumption of observability on the underlying POMDP model, itis still not clear if these algorithms can avoid computationally intractable oracles, since the approx-imate POMDP that needs to do planning on at every iteration during learning can be quitedifferent from the underlying model. For example, at the beginning of exploration when not enoughsamples are collected, or when there exist certain states that remain less explored during the entire learning process, the potentially misspecified emission (and transition) may break the observability (orother structural) assumptions made for the underlying POMDP. This makes that single iteration evencomputationally intractable. In contrast, our focus is on better understanding practically inspiredalgorithmic paradigms, which in practice often do access and use the privileged state informationduring each episode (instead of only at the end) , without computationally intractableoracles. Most related empirical works.Privileged information has been widely used in empirical partiallyobservable RL, with two main types of approaches based on privileged policy and privileged valuelearning, respectively. For the former, one prominent example is expert distillation , alsoknown as teacher-student learning , as we analyze in . For the latter, asymmetricactor-critic represents one of the well-known examples, with other studies in . Learningprivileged value functions (to improve the policies) has also been widely used in multi-agent RL, fea-tured in centralized-training-decentralized-execution, see e.g., . Intriguingly,it was shown that if the privileged value function depends only on the state, the associated actor willcause bias . This has thus necessitated the use of history/belief in asymmetric actor-critic,as in our . Notably, the empirical framework in exactly matches ours, where they ex-ploited the privileged state information in training for belief learning, followed by policy optimizationover the learned belief states. Indeed, many empirical works explicitly separate the procedures ofexplicit belief-state learning and planning as we study in , oftentimeswith privileged state information to supervise the belief learning procedure .",
  "Partially Observable RL (with Privileged Information)": "Model.Consider a POMDP characterized by a tuple P = (H,S,A,O,T,O,1,r), where H denotesthe length of each episode, S is the state space with |S| = S, A denotes the action space with |A| =A. We use T = {Th}h[H] to denote the collection of transition matrices, so that Th(|s,a) (S)gives the probability of the next state if action a is taken at state s and step h. In the followingdiscussions, for any given a, we treat Th(a) RSS as a matrix, where each row gives the probabilityfor reaching each next state from different current states. We use 1 to denote the distribution of theinitial state s1, and O to denote the observation space with |O| = O. We use O = {Oh}h[H] to denotethe collection of emission matrices, so that Oh(|s) (O) gives the emission distribution over theobservation space O at state s and step h. For notational convenience, we will at times adopt thematrix convention, where Oh is a matrix with rows Oh(|s) for each s S. Finally, r = {rh}h[H] is acollection of reward functions, so that rh(s,a) is the reward given the state s and action a at steph. When privileged information is available, the agent can observe the underlying state s S directlyduring training (only). We thus denote the trajectory until step h with states as h = (s1:h,o1:h,a1:h1),the one without states as h = (o1:h,a1:h1), and its space as Th. Finally, we use bh(h) to denote theposterior distribution over the underlying state at step h given history h, which is known as thebelief state (c.f. Appendix A.1 for more details).",
  "where the agent bases on the entire (partially observable) history for decision-making. The cor-responding policy class is denoted as h.We further denote = h[H]h.We also define": "gen := {1:H |h : Sh Oh Ah1 (A) for h [H]} to be the most general policy space in par-tially observable RL with privileged state information, which can potentially depend on all historicalstates, observations, and actions. It can be seen that gen. We may also use policies that only re-ceive a finite memory instead of the whole history as inputs: fix an integer L > 0, we define the policyspace L to be the space of all possible policies = 1:H := (h)h[H] such that h : Zh (A) withZh := Omin{L,h} Amin{L,h} for each h [H]. Finally, we define the space of state-based policies as S,i.e., for any = 1:H S, h : S (A) for all h [H].Given the POMDP model P, we use PPs1:H+1,a1:H,o1:H(E) to denote the probability of some eventE when (s1:H+1,a1:H,o1:H) is drawn as a trajectory following the policy in the model P. We willalso use the shorthand notation P,P () if (s1:H+1,a1:H,o1:H) is evident. We write EP[] to denote theexpectation similarly. We define the value function at step h as V ,Ph(yh) := EP[Ht=h rt(st,at)|yh], de-noting the expected accumulated rewards from step h, where yh (s1:h,o1:h,a1:h1), and we slightlyabuse the notation by treating as a set the sequence of states s1:h, the sequence of observationso1:h, and the sequence of actions a1:h1 up to time h, which are the available information to theagent at step h. We say yh is reachable if there exists some policy gen such that P,P (yh) > 0.For h = 1, we adopt the simplified notation vP () = EP[Hh=1 rh(sh,ah)]. Meanwhile, we also de-fine Q,Ph(yh,ah) := EP[Ht=h rt(st,at)|yh,ah]. We denote the occupancy measure on the state space asd,Ph(sh) = P,P (sh). The goal of learning in POMDPs is to find the optimal policy that maximizes theexpected accumulated reward over the policies that take h as input at each step h [H], i.e., those . Formally, we define:",
  "Definition 2.1 (-optimal policy). Given > 0, a policy is -optimal, if vP () max vP ()": "Learning with privileged information.Common RL algorithms for POMDPs deal with the sce-nario where during both the training and test time, the agent can only observe its historical obser-vations and actions h at step h, while the states are not accessible. In other words, the agent canonly utilize policies from to interact with the environment. In contrast, in settings with privilegedinformation, e.g., training in simulators and/or using sensors with higher precision, the underlyingstate can be used in training. Thus, the agent is allowed to utilize policies from the class gen duringtraining. Meanwhile, the objective is still to find the optimal history-dependent policy in the spaceof , since at test time, the agent cannot access the state information anymore, and it is the perfor-mance for such policies that matters eventually. For simplicity, we assume the reward function isknown since under our privileged information setting, learning the reward function is much easierthan learning the transition and emission, and the sample/computational complexity for the formeris dominated by that for the latter. This assumption has also been made for learning in POMDPswithout privileged information .",
  "Partially Observable Multi-agent RL with Information Sharing": "Partially observable stochastic games (POSGs) are a natural generalization of POMDPs withmultipleself-interestedagents.WedefineaPOSGwithnagentsbyatupleG=(H,S,{Ai}ni=1,{Oi}ni=1,T,O,1,{ri}ni=1), where each agent i has its individual action space Ai, obser-vation space Oi, and reward function ri = {ri,h}h[H] with ri,h(s,a) denoting the reward givenstate s and joint action a for agent i at step h. An episode of POSG proceeds as follows: at each step hand state sh, a joint observation is drawn from (oi,h)i[n] Oh(|sh), and each agent receives its own ob-servation oi,h, takes the corresponding action ai,h, obtains the reward ri,h(sh,ah), where ah := (ai,h)i[n],and then the system transitions to the next state as sh+1 Th(|sh,ah). Notably, each agent i may notonly know its local information (oi,1:h,ai,1:h1), but also information from some other agents. There-fore, we denote the information available to each agent i at step h also as i,h (o1:h,a1:h1) and define the common information as ch = i[n]i,h and private information as pi,h = i,h \\ ch. We denote thespace for common information and private information as Ch and Pi,h for each agent i and step h.The joint private information at step h is denoted as ph = (pi,h)i[n], where the collection of the jointprivate information is given by Ph = P1,h Pn,h. We refer more examples of this setting of POSGwith information-sharing to Appendix A.2 (and also ). Correspondingly, the policy eachagent i deploys at test time takes the form of i,h : h Ch Pi,h (Ai), where h is the spaceof random seeds. We denote the space of such policies for agent i as i. If i,h takes the state shinstead of (ch,pi,h) as input, we denote its policy space as S,i, e.g., for agent i and policy 1:H S,i,i,h : S (Ai) for each h [H]. Similar to the POMDP setting, we define gen to be the most generalpolicy space, i.e., gen := {1:H |h : Sh Oh Ah1 (A) for h [H]}. Note that this model coversseveral recent POSG models studied for partially observable MARL, e.g., . For example, ateach step h, if there is no shared information, then ch = ; if all history information is shared, thenpi,h = for all i [n]. In privileged-information-based learning, the training algorithm may exploit notonly the underlying state information, but also the observations and actions of other agents. Solution concepts.Different from POMDPs, where we hope to learn an optimal policy, The solu-tion concepts for POSGs are usually the equilibria, particularly Nash equilibrium (NE) for two-playerzero-sum games (i.e., when n = 2 and r1,h + r2,h = 1),1 and correlated equilibrium (CE) or coarsecorrelated equilibrium (CCE) for general-sum games. For a joint policy (i)ni=1 , we denote theexpected reward of agent i by vGi () = EP[Hh=1 ri,h(sh,ah)].",
  "maxmi vGi(mi i ) i vGi () ,": "where mi is called strategy modification for agent i, and mi = {mi,h,ch,pi,h}h,ch,pi,h, with each mi,h,ch,pi,h :Ai Ai being a mapping from the action set to itself. The space of mi is denoted as Mi. Thecomposition mi i is defined as follows: at step h, when agent i is given ch and pi,h, the joint actionchosen to be (a1,h, ,ai,h, ,an,h) will be modified to (a1,h, ,mi,h,ch,pi,h(ai,h), ,an,h). Note that thisdefinition follows from that in when there exists certain common information,and is a natural generalization of the definition in the normal-form game case . Meanwhile, wealso denote by Mgenithe space of all possible strategy modifications mi if it is conditioned on theentire joint history information including the state, instead of only (ch,pi,h). Similarly, we use MS,ito denote the space of all possible strategy modifications mi if it only conditions on the current state. 1Note that we require r1,h + r2,h to be 1 instead of 0 to be consistent with our assumption that ri,h for eachi , and this requirement does not lose optimality as one can always subtract the constant-sum offset to attain azero-sum reward structure.",
  "Technical Assumptions for Computational Tractability": "A key technical assumption is that the POMDPs/POSGs we consider satisfy an observability assump-tion, as outlined below. This observability assumption allows us to use short memory policies toapproximate the optimal policy, and enables quasi-polynomial-time complexity for both planningand learning in POMDPs/POSGs . Assumption 2.5 (-observability ). Let > 0. For h [H], we say that the matrix Ohsatisfies the -observability assumption if for each h [H], for any b,b (S),Oh b Oh b1 b b1 . A POMDP/POSG satisfies -observability if all its Oh for h [H] do so. Additionally, to solve a POSG without computationally intractable oracles, certain information-sharing is also needed even under the observability assumption . We thus make the followingassumption as in . Assumption 2.6 (Strategy independence of beliefs ). Consider any step h [H], anypolicy , and any realization of common information ch that has a non-zero probability un-der the trajectories generated by 1:h1.Consider any other policies 1:h1, which also give anon-zero probability to ch. Then, we assume that: for any such ch Ch, and any ph Ph,sh S,",
  "Revisiting Empirical Paradigms of RL with Privileged Information": "Most empirical paradigms of RL with privileged information can be categorized into two types: i)privileged policy learning, where the policy in training is conditioned on the privileged information,and the trained policy is then distilled to a policy that does not take the privileged information asinput. This is usually referred to as either expert distillation or teacher-student learning in the literature; ii) privileged value learning, where the value function is conditioned onthe privileged information, and is then used to directly output a policy that takes partial observation(history) as input. One prominent example of ii) is asymmetric-actor-critic . It is worth notingthat asymmetric-actor-critic is also closely related to one of the most successful paradigms for multi-agent RL, centralized-training-with-decentralized-execution , which is usually instantiatedunder the actor-critic framework, with the critic taking privileged information as input in training.Here we formalize and revisit the potential pitfalls of these two paradigms, and further developtheoretical guarantees under certain additional conditions and/or algorithm variants.",
  "h=1Dfh(|sh)|h(|h),(3.1)": "where gen is some given behavior policy to collect exploratory trajectories,argmaxS vP () denotes the optimal fully observable policy, and Df denotes the general f -divergence to measure the discrepancy between and .Such a formulation looks promising since it essentially circumvents the challenging issue of explo-ration in partially observable environments, by directly mimicking an expert policy that can be obtainedfrom any off-the-shelf MDP learning algorithm. However, we point out in the following propositionthat even if the POMDP satisfies Assumption 2.5, the distilled policy can still be strictly suboptimaleven with infinite data, i.e., by solving the expected objective Equation (3.1) completely. We postponethe proof of Proposition 3.1 to Appendix C. Proposition 3.1 (Pitfall of expert policy distillation). For any , (0,1), there exists a -observablePOMDP P with H = 1, S = O = A = 2 such that for any behavior policy gen and choice of Dfin Equation (3.1), it holds that vP () max vP () (1)(1)",
  "Example 3.4 (Block MDP ). We say a POMDP P is a block MDP if for any h [H], sh,sh S,it holds that supp(Oh(|sh)) supp(Oh(|sh)) = when sh sh": "Example 3.5 (k-step decodable POMDP ). We say a POMDP P is a k-step decodable POMDP ifthere exists an unknown decoder = {h : Zh S}h[H] such that for any h [H] and reachabletrajectory h, PP (sh = h(zh)|h) = 1, where Zh = (O A)max{h1,k1} O, zh = ((o,a)k(h):h1,oh), andk(h) = max{h k + 1,1}. Finally, to understand how our condition can extend beyond known examples in the literature,we show that one can indeed allow the decoding length of Example 3.5 to be unknown and arbitrary(instead of being a small known constant as in to have provably efficient algorithms).",
  "Privileged Value Learning: Asymmetric Actor-Critic": "Asymmetric actor-critic iterates between two procedures as in standard actor-critic algorithms: policy improvement and policy evaluation. As the name suggests, its key difference from thestandard actor-critic is that the algorithm maintains Q-value functions (the critic) based on thestate/privileged information, while the policy receives only the (partially observable) history as input. Policy evaluation.At iteration t 1, given the policy t1, the algorithm estimates Q-functions inthe form of {Qt1h(h,sh,ah)}h[H], where we adopt the unbiased version such that Q-functions areconditioned on both the history and the states.3 One key to achieving sample efficiency is by addingsome bonus terms in policy evaluation to encourage exploration, i.e., obtaining some optimistic Q-function estimates, similarly as in the fully-observable MDP setting, see e.g., , for which wedefer the detailed introduction to . Policy improvement.At each iteration t, given the critic {Qt1h(h,sh,ah)}h[H] for t1, the vanillaasymmetric actor-critic algorithm updates the policy according to the sample-based gradient estima-tion via K trajectories {sk1:H+1,ok1:H,ak1:H}k[K] sampled from t1",
  ",(3.2)": "where t is the step-size and Proj is the projection operator onto the space of , which correspondsto projecting onto the simplex of (A) for each h [H]. Here we point out the potential drawbackof the vanilla algorithm as in , where the key insight is that for each iteration of policy evalu-ation and improvement, one roughly only performs the computation of order O(KH), while needingto collect K new episodes of samples. Thus, the sample complexity will scale in the same order asthe computational complexity when the algorithm converges after some iterations to an -optimal so-lution, which will be super-polynomial even for -observable POMDPs . Proof of the result isdeferred to Appendix C. Proposition 3.7 (Inefficiency of vanilla asymmetric actor-critic). Under the tabular parameteriza-tion for both the policy and the value function, the vanilla asymmetric actor-critic algorithm (Equa-tion (3.2)) suffers from super-polynomial sample complexity for -observable POMDPs under stan-dard hardness assumptions. To address such an issue, one may need to perform more computation per iteration, so that al-though the total computational complexity (iteration number per-iteration computational com-plexity) is super-polynomial, the total iteration number can be lower such that the total sample com-plexity may be lower as well. This desideratum is hard to achieve if one computes policy updates 3Note that as pointed out in , the original asymmetric actor-critic , where the value function was only conditionedon the states, is a biased estimate of the actual history-conditioned value function that appears in the policy gradient.Although only showed such a bias in the infinite-horizon discounted setting, we verify that such a state-based valuefunction is indeed also biased under our finite-horizon setting, see Remark C.1 for an example. We will thus use theunbiased value function estimate conditioned on both states and history throughout. only on the sampled trajectories h per iteration, i.e., update asynchronously, since this will couple thescales of computational and sample complexities similarly as Equation (3.2). In contrast, we first pro-pose to update all trajectories per iteration in a synchronous way, with the following proximal-policyoptimization-type policy improvement update with the state-history-dependent Q-functions{Qt1h(h,sh,ah)}h[H]:",
  "th(|h) t1h(|h)expEshbh(h)Qt1h(h,sh,),h [H],h Th,(3.3)": "where we recall bh(h) (S) denotes the belief state and > 0 is the learning rate. This update rulealso reduces to the natural policy gradient (NPG) update under the softmax policy parameteri-zation in the fully-observable case , when updated for each state sh separately . We defer thedetailed derivation of Equation (3.3) to Appendix C.However, such an update presents two challenges: (1) It requires enumerating all possible h,whose number scales exponentially with the horizon, making it still computationally intractable;(2) An explicit belief function bh is needed. Motivated by these two challenges, we propose to con-sider finite-memory-based policy and assume access to an approximate belief function {bapxh: Zh (S)}h[H] (the learning for which will be made clear later). Correspondingly, the policy update ismodified as:",
  "th(|zh) t1h(|zh)expEshbapxh(zh)Qt1h(zh,sh,),h [H],zh Zh": "Then we develop and analyze one possible approach to learning such an approximate belief effi-ciently (c.f. ). It is worth noting that the policy optimization algorithm we aim to developand analyze does not depend on the specific algorithm approximate belief learning. Such a decou-pling enables a more modular algorithm design framework, and can potentially incorporate the richliterature on learning approximate beliefs in practice, see e.g., , which has mostlynot been theoretically analyzed before. We will thus analyze such an oracle in .",
  "Provably Efficient Expert Policy Distillation": "We now focus on the provable correctness and efficiency of expert policy distillation, under the deter-ministic filter condition in Definition 3.2. We will defer all the proofs in this section to Appendix D.Definition 3.2 motivates us to consider only succinct policies that incorporate an auxiliary parameterrepresenting the most recent state, as well as the most recent observations and actions. We considerpolicies that are the composition of two functions: at step h a function gh : S AO S that decodesthe state based on the previous state, the most recent action, and the most recent observation, and apolicy E S that takes as input the current (decoded) underlying state and outputs a distributionover actions.",
  "h[H] ,": "where E stands for an arbitrary expert policy, and D stands for the distilled policy class, andfor h = 1, a0, s0 are some fixed dummy action and state for notational convenience. Intuitively,the distilled policy D executes as follows: it firstly decodes the underlying states by applying{gh}h[H] recursively along the history, and then takes actions using E based on the decoded states. Our goal is to learn the two functions independently, that is, we want to learn an approximatelyoptimal policy E S with respect to the MDP M derived from POMDP P by omitting the obser-vations and observing the underlying state (see Definition 4.2), and for each step h [H], a decoding",
  "vP () vP (E) H": "We can use any off-the-shelf algorithm to learn an approximate optimal policy E for the associ-ated MDP M (see Definition 4.2). Thus, in the rest of the section, we focus on learning the decodingfunction {gh}h[H]. To efficiently learn the decoding function, we model the access to the underlyingstate by keeping track of the most recent pair of the action and the observation, as well as the twomost recent states. We summarize the algorithm of decoding-function learning in Algorithm 1.",
  "Belief-Weighted Optimistic Asymmetric Actor-Critic": "We now introduce our main algorithmic contribution to the privileged policy learning setting. Ouralgorithm is conceptually similar to the natural policy gradient methods in the fully-observable setting, with additional weighting over the states sh using some learned belief states, tohandle the additional state-dependence in the asymmetric critic. The overall algorithm is presentedin Algorithm 2. The algorithm requires a belief-learning subroutine that takes the stored memoryas input and outputs a belief about the underlying state (c.f. {bapxh}h[H]). Additionally, similar tothe fully observable setting, we include a subroutine to estimate the Q-function, which introducesadditional challenges due to partial observability (see Appendix F). We establish the performanceguarantee of Algorithm 2 in the following theorem. We defer the proof to Appendix F. Theorem 5.1 (Near-optimal policy). Fix , (0,1). Given a POMDP P and an approximate belief{bapxh: Zh (S)}h[H], with probability at least 1, Algorithm 2 can learn an approximate optimalpolicy of P in the space of L such that",
  "Approximate Belief Learning": "At a high level, our belief-learning algorithm first learns an approximate POMDP model P by explic-itly exploring the state space. The main technical challenge here is that there may exist states that arereachable with very low probability, making it infeasible to collect enough samples to sufficiently ex-plore them, thus potentially breaking the -observability property of the ground-truth model P. Tocircumvent this issue, we ignore such hard-to-visit states and redirect probabilities flowing to them tocertain other states. Thus, in our truncated POMDP, where each state is sufficiently explored, we canapproximate the transition and emission matrices to a desired accuracy uniformly across all the statesand preserve the -observability property. This ensures that the learned approximate belief functionin the truncated POMDP is sufficiently close to the actual belief function of the original POMDP P.Note that the key to achieving belief learning with both polynomial sample and time complexities isour explicit exploration in the state space, which relies on executing fully observable policies from anMDP learning subroutine. We remark that the belief function may also be learned even if the statespace is only explored by partially observable policies, thus utilizing only hindsight observabilitymay be sufficient for this purpose . However, for such exploration to be computationally tractable,",
  "bility at least 1 , for any L and h [H], EPbh(h) bapxh(zh)1": "Theorem 5.2 shows that an approximate belief can be learned with both polynomial samples andtime, which, combined with Theorem 5.1, yields the final polynomial sample and quasi-polynomialtime guarantee below. In contrast to the case without privileged information , the samplecomplexity is reduced from quasi-polynomial to polynomial for -observable POMDPs. Note thatthe computational complexity remains quasi-polynomial, which is known to be unimprovable evenfor planning . The key to such an improvement, as pointed out in .2, is the more prac-tical update rule of actor-critic (in conjunction with our belief-weighted idea), which allows morecomputation at each iteration (instead of only performing computation at the sampled finite-memory).This allows the total computation to remain quasi-polynomial, while the overall sample complexitybecomes polynomial. A detailed comparison can be found in .",
  "Numerical Validation": "We now provide some numerical results for both of our principled algorithms. Here we mainlycompare with two baselines, the vanilla asymmetric actor-critic , and asymmetric Q-learning ,on two settings, POMDP under the deterministic filter condition (c.f. Definition 3.2) and generalPOMDPs. We defer the introduction for the implementation details to Appendix G. POMDP under deterministic filter condition.We first evaluate our algorithms on POMDPs withcertain structures, i.e., the deterministic filter condition. In particular, we generate POMDPs, whereeither the transition dynamics are deterministic or the emission ensures decodability. We test twoof our approaches, expert policy distillation, asymmetric optimistic natural policy gradient. Wesummarize our results in , where the four cases correspond to POMDPs with (S = A = 2,O =",
  ": Rewards of different approaches for POMDPs under the deterministic filter condition (c.f.Definition 3.2)": "3,H = 5), (S = A = 2,O = 3,H = 10), (S = 3,A = 2,O = 4,H = 5), (S = 3,A = 2,O = 4,H = 5), and wecan see that our approach based on expert distillation outperforms all the other methods, which isconsistent with the fact that such methods have exploited the special deterministic filter structure ofthe POMDPs to achieve both polynomial sample and computational complexity (c.f. Theorem 4.6). General POMDPs.Here we also evaluate our methods for general randomly generated POMDPswithout any structures. Hence, we compare the baselines with asymmetric optimistic natural policygradient and asymmetric optimistic value iteration (i.e., the single-agent version of Algorithm 5). In, we show the performance of different algorithms in POMDPs of different sizes, where thefour cases correspond to POMDPs with (S = A = O = 2,H = 5), (S = A = O = 2,H = 10), (S = O =3,A = 2,H = 10), (S = A = 3,O = 2,H = 10), and our approaches achieve the highest rewards withsmall number of episodes. Empirical insights and interpretation of the experimental results.To understand intuitively whyour approaches outperform those baseline algorithms, we notice the key difference in the value andpolicy update style between our approaches and vanilla asymmetric actor-critic and asymmetric Q-learning. The baselines often roll-out the policies, collect trajectories, and only update the value andthe policies on the states/history the trajectories have visited, i.e., updates in an asynchronous fashion.Therefore, ideally, to learn a good policy for these baselines, the number of trajectories to collect is atleast as large as the history size, which is indeed exponential in the horizon H. This is known as thecurse of history for partially observable RL. In contrast, in our algorithms, we explicitly estimate theempirical transition dynamics and emissions, which are indeed of polynomial size. Thus, the samplecomplexity avoids suffering from the potential exponential dependency on the horizon or the lengthof the finite memory. Finally, we acknowledge that the baselines are developed to handle complex,high-dimensional deep RL problems, while scaling our methods to deep RL benchmarks requiresnon-trivial engineering efforts, and is an important future work.",
  "Proposition 7.1. Definition 3.2 is equivalent to the following: for each h [H], there exists an un-known function h : Th S such that PP (sh = h(h)|h) = 1 for any reachable h Th": "Proposition 7.1 implies that at each step h, given the entire history, the agent can uniquely decodethe current underlying state sh. Thus, we generalize this condition to POSGs by requiring each agentto uniquely decode the current state sh given the information it has collected so far. Definition 7.2 (Deterministic filter condition for POSGs). We say a POSG G satisfies the deterministicfilter condition if for each i [n], h [H], there exists an unknown function i,h : Ch Pi,h S suchthat PG(sh = i,h(ch,pi,h)|ch,pi,h) = 1 for any reachable (ch,pi,h). Here we have required that each agent can decode the underlying state through their own infor-mation individually. Naturally, one may wonder whether one can relax it so that only the joint historyinformation of all the agents can decode the underlying state. However, we point out in the followingthat it does not circumvent the computational hardness of POSG, the proof of which is deferred toAppendix H. Note that the computational hardness result can not be mitigated even with privilegedstate information, as the hardness we state here holds even for the planning problem with modelknowledge, with which one can simulate the RL problems with privileged information.",
  "Proposition 7.3. Computing CCE in POSGs that satisfy that for each step h [H], there exists afunction h : Ch Ph S such that PG(sh = h(ch,ph)|ch,ph) = 1 for any reachable (ch,ph) is stillPSPACE-hard": "Learning multi-agent individual decoding functions with unilateral exploration.Similar to ourframework for POMDPs, our framework for POSGs is also decoupled into two steps: i) learning anexpert equilibrium policy that is fully observable, ii) learning the decoding function, where the firststep can be instantiated by any provable off-the-shelf algorithm of learning in Markov games. Themajor difference from the framework for POMDPs lies in how to learn the decoding function. InTheorem H.1, we prove that the difference of the NE/CE/CCE-gap between the expert policy and thedistilled student policy is bounded by the decoding errors under policies from the unilateral deviationof the expert policy. Hence, given the expert policy , the key algorithmic principle is to performunilateral exploration for each agent i to make sure the decoding function is accurate under policies(i,i) for any i, keeping i fixed. We defer the detailed algorithm to Algorithm 5, and presentbelow the guarantees for learning the decoding functions and the corresponding distilled policy forlearning NE/CCE, and we defer the results for learning CE to Theorem H.6.",
  "For POMDPs, we have used finite-memory policies for computational efficiency. We generalize toPOSGs with information sharing by defining the compression of the common information": "Definition 7.5 (Compressed approximate common information ). For each h [H], givena set Ch, we say Compressh is a compression function if Compressh {f : Ch Ch}. For each ch Ch,we denote ch := Compressh(ch). We also require the compression function to satisfy the regularitycondition that for each h [H], there exists a function h+1 such that ch+1 = h+1(ch,h+1), for anych Ch, h+1 h+1, where we recall ch+1 := ch h+1 and the definition of h+1 in Assumption A.4. Similar to the framework we developed for POMDPs in , we firstly develop the multi-agent RL algorithm based on some approximate belief, and then instantiate it with one provableapproach to learning such an approximate belief. Optimistic value iteration of POSGs with approximate belief.For POMDPs, a sufficient statisticsfor optimal decision-making is the posterior distribution over the underlying state given history. How-ever, for POSGs with information-sharing, as shown in , a sufficient statistics become theposterior distribution over the state and the private information given the common information, in-stead of only the state. Therefore, we consider the approximate belief in the form of Ph : Ch (PhS)for each h [H]. Note that both Ph and thus belief have implicit dependencies on Compressh, asch := Compressh(ch). We outline our algorithm in Algorithm 7, which is conceptually similar to thealgorithm for POMDPs (Algorithm 2), maintaining the asymmetric critic (i.e., value function), andperforming the actor update (i.e., policy update) using the belief-weighted value function. Theorem 7.6 (Equilibria learning; Combining Theorem H.15 and Theorem H.16). Fix , (0,1)and approximate belief {Ph}h[H].We define the error of the approximate belief compared withthe ground-truth belief as belief := maxh[H] max EGsh,ph |PG(sh,ph |ch) Ph(sh,ph |ch)|. Under As-sumption 2.6, with probability at least 1 , Algorithm 7 can learn an ( + H2belief)-NE if G is zero-",
  "Concluding Remarks": "In this paper, we aim to understand the provable benefits of privileged information for partiallyobservable RL problems under two empirically successful paradigms, expert distillation and asymmetric actor-critic , which represent privileged policy and privileged value learning,respectively, with an emphasis on studying both the computational and sample efficiencies of thealgorithms. Our results (as summarized in ) showed that privileged information does improvelearning efficiency in a series of known POMDP subclasses. One potential limitation of our workis that we only focused on the case with exact state information. It remains to explore whethersuch an assumption can be further relaxed, e.g., when privileged state information may be biased,partially observable, or delayed, as usually happens in practice, and how our theoretical resultsmay be affected. Meanwhile, as an initial theoretical study, we have been primarily focusing on thetabular settings (except Appendix E), and it would be interesting to extend the results to function-approximation settings to handle massively large state, action, and observation spaces in practice.",
  "Acknowledgement": "The authors would like to thank the anonymous reviewers and area chair from NeurIPS 2024 for thevaluable feedback. Y.C. acknowledges the support from the NSF Awards CCF-1942583 (CAREER)and CCF-2342642. X.L. and K.Z. acknowledge the support from Army Research Laboratory (ARL)Grant W911NF-24-1-0085. A.O. acknowledges financial support from a Meta PhD fellowship, aSloan Foundation Research Fellowship and the NSF Award CCF-1942583 (CAREER). Part of thiswork was done while the authors were visiting the Simons Institute for the Theory of Computing. Alekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. Optimality and approxi-mation with policy gradient methods in Markov decision processes. In Conference on LearningTheory, pages 6466, 2020. Eitan Altman, Vijay Kambley, and Alonso Silva. Stochastic games with one step delay sharinginformation pattern with application to power control. In 2009 International Conference on GameTheory for Networks, pages 124129. IEEE, 2009. OpenAI: Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal Jozefowicz, Bob McGrew,Jakub Pachocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray, et al. Learning dex-terous in-hand manipulation. The International Journal of Robotics Research, 39(1):320, 2020.",
  "Robert J Aumann, Michael Maschler, and Richard E Stearns. Repeated games with incompleteinformation. MIT press, 1995": "Raphael Avalos, Florent Delgrange, Ann Nowe, Guillermo Perez, and Diederik M Roijers. Thewasserstein believer: Learning belief updates for partially observable environments throughreliable latent space models. In The Twelfth International Conference on Learning Representations,2023. Andrea Baisero and Christopher Amato. Unbiased asymmetric reinforcement learning underpartial observability. In Proceedings of the International Joint Conference on Autonomous Agentsand Multiagent Systems, 2022.",
  "Xiaoyu Chen, Yao Mark Mu, Ping Luo, Shengbo Li, and Jianyu Chen. Flow-based recurrentbelief state learning for pomdps. In International Conference on Machine Learning, pages 34443468. PMLR, 2022": "Christoph Dann, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, andRobert E Schapire. On oracle-efficient pac rl with rich observations. Advances in neural in-formation processing systems, 31, 2018. Simon Du, Akshay Krishnamurthy, Nan Jiang, Alekh Agarwal, Miroslav Dudik, and John Lang-ford. Provably efficient rl with rich observations via latent state decoding. In InternationalConference on Machine Learning, pages 16651674. PMLR, 2019. Yonathan Efroni, Chi Jin, Akshay Krishnamurthy, and Sobhan Miryoosefi. Provable reinforce-ment learning with a short-term memory. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song,Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, International Conference on MachineLearning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings ofMachine Learning Research, pages 58325850. PMLR, 2022. Eyal Even-Dar, Sham M. Kakade, and Yishay Mansour. The value of observation for monitoringdynamic systems. In Manuela M. Veloso, editor, IJCAI 2007, Proceedings of the 20th InternationalJoint Conference on Artificial Intelligence, Hyderabad, India, January 6-12, 2007, pages 24742479,2007. Jakob Foerster, Ioannis Alexandros Assael, Nando De Freitas, and Shimon Whiteson. Learningto communicate with deep multi-agent reinforcement learning. Advances in Neural InformationProcessing Systems, 29, 2016. Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon White-son.Counterfactual multi-agent policy gradients.In Proceedings of the AAAI Conference onArtificial Intelligence, volume 32, 2018.",
  "Noah Golowich, Ankur Moitra, and Dhruv Rohatgi.Planning in observable pomdps inquasipolynomial time. arXiv preprint arXiv:2201.04735, 2022": "Noah Golowich, Ankur Moitra, and Dhruv Rohatgi. Planning and learning in partially observ-able systems via filter stability. In Proceedings of the 55th Annual ACM Symposium on Theory ofComputing, pages 349362, 2023. Noah Golowich, Ankur Moitra, and Dhruv Rohatgi.Exploration is harder than prediction:Cryptographically separating reinforcement learning from supervised learning. arXiv preprintarXiv:2404.03774, 2024.",
  "Geoffrey J Gordon, Amy Greenwald, and Casey Marks. No-regret learning in convex games. InProceedings of the 25th international conference on Machine learning, pages 360367, 2008": "Jiacheng Guo, Minshuo Chen, Huan Wang, Caiming Xiong, Mengdi Wang, and Yu Bai. Sample-efficient learning of pomdps with multiple observations in hindsight. In The Twelfth Interna-tional Conference on Learning Representations, 2023. Abhishek Gupta, Ashutosh Nayyar, Cedric Langbort, and Tamer Basar. Common informationbased markov perfect equilibria for linear-gaussian games with asymmetric information. SIAMJournal on Control and Optimization, 52(5):32283260, 2014.",
  "Hengyuan Hu, Adam Lerer, Noam Brown, and Jakob Foerster. Learned belief search: Efficientlyimproving policies in partially observable settings. arXiv preprint arXiv:2106.09086, 2021": "Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E. Schapire.Contextual decision processes with low bellman rank are pac-learnable. In Doina Precup andYee Whye Teh, editors, Proceedings of the 34th International Conference on Machine Learning,ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings of MachineLearning Research, pages 17041713. PMLR, 2017.",
  "Qinghua Liu, Tiancheng Yu, Yu Bai, and Chi Jin. A sharp analysis of model-based reinforce-ment learning with self-play. In International Conference on Machine Learning, pages 70017010.PMLR, 2021": "Xiangyu Liu, Hangtian Jia, Ying Wen, Yujing Hu, Yingfeng Chen, Changjie Fan, Zhipeng Hu,and Yaodong Yang. Towards unifying behavioral and response diversity for open-ended learn-ing in zero-sum games. Advances in Neural Information Processing Systems, 34:941952, 2021. Xiangyu Liu and Kaiqing Zhang. Partially observable multi-agent RL with (quasi-)efficiency:the blessing of information sharing.In International Conference on Machine Learning, pages2237022419. PMLR, 2023.",
  "Xiangyu Liu and Kaiqing Zhang. Partially observable multi-agent reinforcement learning withinformation sharing, 2024": "Ryan Lowe, Yi I Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multi-agent actor-critic for mixed cooperative-competitive environments. Advances in Neural Informa-tion Processing Systems, 30, 2017. Miao Lu, Yifei Min, Zhaoran Wang, and Zhuoran Yang. Pessimism in the face of confounders:Provably efficient offline reinforcement learning in partially observable markov decision pro-cesses. In The Eleventh International Conference on Learning Representations, 2023. Xueguang Lyu, Andrea Baisero, Yuchen Xiao, and Christopher Amato. A deeper understandingof state-based critics in multi-agent reinforcement learning. In Proceedings of the AAAI conferenceon artificial intelligence, volume 36, pages 93969404, 2022. Xueguang Lyu, Andrea Baisero, Yuchen Xiao, Brett Daley, and Christopher Amato. On cen-tralized critics in multi-agent reinforcement learning. Journal of Artificial Intelligence Research,77:295354, 2023. Weichao Mao, Kaiqing Zhang, Erik Miehling, and Tamer Basar. Information state embeddingin partially observable cooperative multi-agent reinforcement learning. In 2020 59th IEEE Con-ference on Decision and Control (CDC), pages 61246131. IEEE, 2020.",
  "Gabriel B Margolis, Tao Chen, Kartik Paigwar, Xiang Fu, Donghyun Kim, Sang bae Kim, andPulkit Agrawal. Learning to jump from pixels. In 5th Annual Conference on Robot Learning,2021": "Takahiro Miki, Joonho Lee, Jemin Hwangbo, Lorenz Wellhausen, Vladlen Koltun, and MarcoHutter. Learning robust perceptive locomotion for quadrupedal robots in the wild. ScienceRobotics, 7(62):eabk2822, 2022. Dipendra Misra, Mikael Henaff, Akshay Krishnamurthy, and John Langford. Kinematic stateabstraction and provably efficient rich-observation reinforcement learning. In International con-ference on machine learning, pages 69616971. PMLR, 2020. Pol Moreno, Jan Humplik, George Papamakarios, Bernardo Avila Pires, Lars Buesing, NicolasHeess, and Theophane Weber. Neural belief states for partially observed domains. In NeurIPS2018 Workshop on Reinforcement Learning under Partial Observability, 2018. Ashutosh Nayyar, Abhishek Gupta, Cedric Langbort, and Tamer Basar.Common informa-tion based markov perfect equilibria for stochastic games with asymmetric information: Finitegames. IEEE Transactions on Automatic Control, 59(3):555570, 2013. Ashutosh Nayyar, Aditya Mahajan, and Demosthenis Teneketzis. Decentralized stochastic con-trol with partial history sharing: A common information approach. IEEE Transactions on Auto-matic Control, 58(7):16441658, 2013.",
  "Lerrel Pinto, Marcin Andrychowicz, Peter Welinder, Wojciech Zaremba, and Pieter Abbeel.Asymmetric actor critic for image-based robot learning.Robotics: Science and Systems XIV,2018": "Shuang Qiu, Ziyu Dai, Han Zhong, Zhaoran Wang, Zhuoran Yang, and Tong Zhang. Poste-rior sampling for competitive rl: Function approximation and partial observation. Advances inNeural Information Processing Systems, 36, 2024. Tabish Rashid, Mikayel Samvelyan, Christian Schroeder De Witt, Gregory Farquhar, Jakob Foer-ster, and Shimon Whiteson. QMIX: Monotonic value function factorisation for deep multi-agentreinforcement learning. In International Conference on Machine learning, pages 681689, 2018.",
  "John Tsitsiklis and Michael Athans. On the complexity of decentralized decision making anddetection problems. IEEE Transactions on Automatic Control, 30(5):440446, 1985": "Masatoshi Uehara, Ayush Sekhari, Jason D Lee, Nathan Kallus, and Wen Sun. Computationallyefficient pac rl in pomdps with latent determinism and conditional embeddings. In InternationalConference on Machine Learning, pages 3461534641. PMLR, 2023. Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michael Mathieu, Andrew Dudzik, Jun-young Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmas-ter level in StarCraft II using multi-agent reinforcement learning. Nature, 575(7782):350354,2019. Andrew Wang, Andrew C Li, Toryn Q Klassen, Rodrigo Toro Icarte, and Sheila A McIlraith.Learning belief representations for partially observable deep rl. In International Conference onMachine Learning, pages 3597035988. PMLR, 2023. Lingxiao Wang, Qi Cai, Zhuoran Yang, and Zhaoran Wang. Represent to control partially ob-served systems: Representation learning with provable sample efficiency. In The Eleventh Inter-national Conference on Learning Representations, 2022.",
  "Hans S Witsenhausen.A counterexample in stochastic optimum control.SIAM Journal onControl, 6(1):131147, 1968": "Guan Yang, Minghuan Liu, Weijun Hong, Weinan Zhang, Fei Fang, Guangjun Zeng, and YueLin. Perfectdou: Dominating doudizhu with perfect information distillation. Advances in NeuralInformation Processing Systems, 35:3495434965, 2022. Yujie Yang, Yuxuan Jiang, Jianyu Chen, Shengbo Eben Li, Ziqing Gu, Yuming Yin, Qian Zhang,and Kai Yu. Belief state actor-critic algorithm from separation principle for POMDP. In 2023American Control Conference (ACC), pages 25602567. IEEE, 2023.",
  "Uh(b;a,o) = Bh+1 (Th(a) b;o),": "where Th(a) b represents the matrix multiplication. We use the notation bh to denote the beliefupdate function, which receives a sequence of actions and observations and outputs a distributionover states at the step h: the belief state at step h = 1 is defined as b1() = 1. For any 2 h H andany action-observation sequence (a1:h,o1:h), we inductively define the belief state:",
  "A.2Additional Preliminaries for POSGs": "Model.We use a general framework of partially observable stochastic games (POSGs) as themodel for partially observable MARL. Formally, we define a POSG with n agents by a tupleG = (H,S,{Ai}ni=1,{Oi}ni=1,T,O,1,{ri}ni=1), where H denotes the length of each episode, S is the statespace with |S| = S, Ai denotes the action space for agent i with |Ai| = Ai. We denote by a := (a1, ,an)the joint action of all the n agents, and by A = A1An the joint action space with |A| = A = ni=1 Ai.We use T = {Th}h[H] to denote the collection of transition matrices, so that Th(|s,a) (S) gives theprobability of the next state if joint action a A is taken at state s S and step h. In the followingdiscussions, for any given a, we treat Th(a) RSS as a matrix, where each row gives the probabilityfor the next state from different current states. We use 1 to denote the distribution of the initial state s1, and Oi to denote the observation space for agent i with |Oi| = Oi. We denote by o := (o1,...,on) thejoint observation of all n agents, and by O := O1On with |O| = O = ni=1 Oi. We use O = {Oh}h[H]to denote the collection of the joint emission matrices, so that Oh(|s) (O) gives the emission dis-tribution over the joint observation space O at state s and step h. For notational convenience, wewill at times adopt the matrix convention, where Oh is a matrix with rows Oh(|sh). We also denoteOi,h(|s) (Oi) as the marginalized emission for agent i agent. Finally, ri = {ri,h}h[H] is a collectionof reward functions, so that ri,h(sh,ah) is the reward of agent i agent given the state sh and joint actionah at step h.Similar to a POMDP, in a POSG, the states are not observable to the agents, and each agent canonly access its own individual observations. The game proceeds as follows. At the beginning ofeach episode, the environment samples s1 from 1. At each step h, each agent i observes its ownobservation oi,h, where oh := (o1,h,...,on,h) is sampled jointly from Oh(|sh). Then each agent i takesthe action ai,h and receives the reward ri,h(sh,ah). After that the environment transitions to the nextstate sh+1 Th(|sh,ah). The current episode terminates once sH+1 is reached. Information sharing, common and private information.Each agent i in the POSG maintains itsown information, i,h, a collection of historical observations and actions at step h, namely, i,h {o1,a1,o2, ,ah1,oh}, and the collection of such history at step h is denoted by Ti,h.In many practical examples, agents may share part of the history with each other, which mayintroduce some information structures of the game that may lead to both sample and computation ef-ficiencies. The information sharing splits the history into common/shared and private informationfor each agent. The common information at step h is a subset of the joint history h = (i,h)i[n]:ch {o1,a1,o2, ,ah1,oh}, which is available to all the agents in the system, and the collection ofthe common information is denoted as Ch and we define Ch = |Ch|. Given the common informationch, each agent also has her private information pi,h = i,h \\ ch, where the collection of the private in-formation for agent i is denoted as Pi,h and its cardinality as Pi,h. The cardinality of the joint privateinformation is Ph = ni=1 Pi,h. We allow ch or pi,h to take the special value when there is no commonor private information. In particular, when Ch = {}, the problem reduces to the general POSG with-out any favorable information structure; when Pi,h = {}, every agent holds the same history, and itreduces to a POMDP when the agents share a common reward function, where the goal is usually tofind the team-optimal solution.",
  "i,h : h Pi,h Ch (Ai),(A.1)": "where h is the random seed space, which is shared among agents and i,h h is the random seedfor agent i. The corresponding policy class is denoted as i,h. Hereafter, unless otherwise noted,when referring to policies, we mean the policies given in the form of (A.1), which maps the availableinformation of agent i, i.e., the private information together with the common information, to thedistribution over her actions. We define i as a sequence of policies for agent i at all steps h [H], i.e., i = (i,1, ,i,H). Wefurther denote i = h[H]i,h as the policy space for agent i and = i[n] i as the joint policyspace. As a special case, we define the space of deterministic policy as i, where i i maps theprivate information and common information to a deterministic action for agent i and the joint spaceas = i[n] i.A product policy is denoted as = 1 2 n if the distributions of drawing each seedi,h for different agents are independent, and a (potentially correlated) joint policy is denoted as = 1 2 n .",
  "We are now ready to define the value function conditioned on the common information under ourmodel of POSG with information sharing:": "Definition A.2 (Value function with information sharing). For each agent i [n] and step h [H],given common information ch and joint policy = (i)ni=1 , the value function conditioned on thecommon information of agent i is defined as: V ,Gi,h (ch) := EGHh=h ri,h(sh,ah) ch, where the expec-tation is taken over the randomness from the model G, policy , and the random seeds. For anycH+1 CH+1 : V ,Gi,H+1(cH+1) := 0. From now on, we will refer to it as value function for short. Another key concept in our analysis is the belief about the state and the private informationconditioned on the common information among agents. Formally, at step h, given policies from 1 toh 1, we consider the common-information-based conditional belief P1:h1,Gh(sh,ph |ch). This beliefnot only infers the current underlying state sh, but also all agents private information ph. With thecommon-information-based conditional belief, the value function given in Definition A.2 has thefollowing recursive structure:",
  "V ,Gi,h (ch) = EG[ri,h(sh,ah) + V ,Gi,h+1(ch+1)|ch],(A.2)": "where the expectation is taken over the randomness of (sh,ph,ah,oh+1). With this relationship, we candefine the prescription-value function correspondingly, a generalization of the action-value functionin (fully observable) stochastic games and MDPs to POSGs with information sharing, as follows. Definition A.3 (Prescription-value function with information sharing). At step h, given the commoninformation ch, joint policies = (i)ni=1 , and prescriptions (i,h)ni=1 h, the prescription-valuefunction conditioned on the common information and joint prescription of agent i is defined as:",
  "where i,h+1 is a fixed transformation": "Equation (A.3) states that the increment in the common information depends on the new in-formation (ah,oh+1) generated between steps h and h + 1 and part of the old information ph. Theincremental common information can be obtained by certain sharing and communication protocolsamong the agents. Equation (A.4) implies that the evolution of private information only depends onthe newly generated private information ai,h and oi,h+1. These evolution rules are standard in theliterature , specifying the source of common information and private information. Based onsuch evolution rules, we define {fh}h[H] and {gh}h[H], where fh : Ah Oh Ch and gh : Ah Oh Phfor h [H], as the mappings that map the joint history to common information and joint privateinformation, respectively.",
  "Here we take the examples from to illustrate the generality of the information-sharing frame-work": "Example A.5 (One-step delayed sharing). At any step h [H], the common and private informationare given as ch = {o2:h1,a1:h1} and pi,h = {oi,h}, respectively. In other words, the players share allthe action-observation history until the previous step h 1, with only the new observation being theprivate information. This model has been shown useful for power control . Example A.6 (State controlled by one controller with asymmetric delay sharing). We assume thereare 2 players for convenience. It extends naturally to n-player settings. Consider the case wherethe state dynamics are controlled by player 1, i.e., Th(|sh,a1,h,a2,h) = Th(|sh,a1,h,a2,h) for all(sh,a1,h,a2,h,a2,h,h). There are two kinds of delay-sharing structures we could consider: Case A: theinformation structure is given as ch = {o1,2:h,o2,2:hd,a1,1:h1}, p1,h = , p2,h = {o2,hd+1:h}, i.e., player 1sobservations are available to player 2 instantly, while player 2s observations are available to player1 with a delay of d 1 time steps. Case B: similar to Case A but player 1s observation is availableto player 2 with a delay of 1 step. The information structure is given as ch = {o1,2:h1,o2,2:hd,a1,1:h1},p1,h = {o1,h}, p2,h = {o2,hd+1:h}, where d 1. This kind of asymmetric sharing is common in networkrouting , where packages arrive at different hosts with different delays, leading to asymmetricdelay sharing among hosts. Example A.7 (Symmetric information game). Consider the case when all observations and ac-tions are available for all the agents, and there is no private information.Essentially, we havech = {o2:h,a1:h1} and pi,h = . We will also denote this structure as fully sharing throughout. Example A.8 (Information sharing with one-directional-one-step delay). Similar to the previouscases, we also assume there are 2 players for ease of exposition, and the case can be generalizedto multi-player cases straightforwardly. Similar to the one-step delay case, we consider the situationwhere all observations of player 1 are available to player 2, while the observations of player 2 areavailable to player 1 with one-step delay. All the past actions are available to both players. That is, inthis case, ch = {o1,2:h,o2,2:h1,a1:h1}, and player 1 has no private information, i.e., p1,h = , and player2 has private information p2,h = {o2,h}. Example A.9 (Uncontrolled state process). Consider the case where the state transition does not de-pend on the actions, that is, Th( | sh,ah) = Th( | sh,ah) for any sh,ah,ah,h. Note that the agents arestill coupled through the joint reward. An example of this case is the information structure wherecontrollers share their observations with a delay of d 1 time steps. In this case, the common infor-mation is ch = {o2:hd} and the private information is pi,h = {oi,hd+1:h}. Such information structurescan be used to model repeated games with incomplete information .",
  "), O1(o1 |s1) = 1, and": "O1(o1 |s2) = 1 , O1(o2 |s2) = . Therefore, it is direct to see that O1 is exactly -observable. Mostimportantly, we choose r1(s1,a1) = 1, r1(s1,a2) = 0, and r1(s2,a1) = 0, r1(s2,a2) = .Therefore, given such a reward function, the fully observable expert policy is given by 1(a1 |s1) =1 and 1(a2 |s2) = 1, i.e., choosing a1 at state s1 and a2 at state s2 deterministically. Meanwhile, by ourconstruction, one can compute that the belief given observation o1 ensures b1(o1) = Unif(S). Hence,the corresponding distilled partially observable policy under observation o1 is given by",
  "where the equality holds when q(a1) = q(a2) = 1": "2. This indicates that 1(|o1) = Unif(A). On the otherhand, combining the fact that b1(o1) = Unif(S) with < 1, it is direct to see that the optimal partiallyobservable policy argmax vP () satisfies 1(a1 |o1) = 1. Now we are ready to evaluate theoptimality gap between and as follows",
  ". Thisconcludes our proof": "Remark C.1. The counter-example P constructed above can be also used to demonstrate the bias ofthe state-only-based value function as an estimate of the history-based value function that appearedin the policy gradient formula for POMDPs, in the finite-horizon setting, i.e. Eshbh(h)[V ,P h(sh)] V ,P h(h) (mirroring Theorem 4.2 of ). Specifically, in the counter-example above, we considerthe policy such that 1(a1 |o1) = 1 and 1(a2 |o2) = 1. The state-only-based value function can beevaluated as",
  "for some stepsize (0,1), where J (kh,skh,akh) := {j [K]|(jh,sjh,ajh) = (kh,skh,akh)}. Therefore, thecomputational complexity for this procedure is of poly(H,K)": "Computational complexity for policy improvement:For tabular parameterization, computinglogt1h(akh |kh) takes O(1) computation.Hence the policy update in Equation (3.2) performspoly(H,K) computation.Meanwhile, under the exponential time hypothesis, there is no polynomial time algorithm foreven planning an -approximate optimal policy in -observable POMDPs . This implies thatthe vanilla asymmetric actor-critic needs to take super-polynomial time to find an approximatelyoptimal policy. This implies the corresponding sample complexity has to be super-polynomial. Finally, we remark that even if we let the policy and the Q-function not depend on the entirehistory h but only the finite-memory zh, the proof still holds. The key is that whenever one onlycomputes at the sampled history/finite-memories, i.e., updates the policy in an asynchronous way (incontrast to the synchronous one where the policies at all histories/finite-memories are updated), thesample and computational complexities will be coupled with the same order per iteration, which im-plies a super-polynomial sample complexity due to the super-polynomial computational complexity.This completes the proof.",
  "vP (E) H,": "which completes the proof.Proof of Theorem 4.5: For each step h [H] we define Dh to be the distribution over the un-derlying state sh1 at step h 1, taken action ah1 A based on E, the underlying state tran-sitioned to sh S, and the observation oh Oh(|sh).We remind that we include at step zero, a dummy state-observation pair (s0,o0), for notational convenience.Formally, Dh is defined asDh(sh1,ah1,oh,sh) := PE,P [sh1,ah1,oh,sh].We first use union bound to decompose the probability that we incorrectly decode,",
  "h[H]P(sh1,ah1,oh,sh)Dh [gh (sh1,ah1,oh) sh].(D.1)": "For each h [H], we can use M episodes to collect M samples from the distribution Dh. In addition,since state sH+1 is dummy, we need not to collect episodes for DH+1. Denote the set of collectedsamples by DMh . We define the decoding gh for step h [H] as follows:",
  "gh(sh1,ah1,oh) =sh | (sh1,ah1,oh,sh) DMh": "Observe that by Definition 3.2, {sh | (sh1,ah1,oh,sh) DMh } is either the empty set or containsonly a single elements, in which case, it is true that gh(sh1,ah1,oh) = h(sh1,ah1,oh) ( is the realdecoding function, see Definition 3.2). Moreover, we slightly abuse the notation and let DMh denotethe empirical distribution induced by the samples in DMh . Thus, with probability at least 1",
  "EProvably Efficient Expert Policy Distillation with Function Approxi-mation": "We now turn our attention to the rich-observation setting under our deterministic filter condition.Definition 3.2 motivates us to consider only succinct policies that incorporate an auxiliary parameterrepresenting the most recent state, as well as the most recent observations and actions. To handle thelarge observation space, we further assume that for each step h [H], the agent selects a decodingfunction gh from a family of multi-class classifiers Fh {S A O S}. For the function class Fh, wemake the standard realizability assumption. We formally summarize our assumptions in Assump-tion E.1. Assumption E.1. We consider a POMDP that satisfies Definition 3.2. In addition, to derive learningalgorithms that do not dependent on O, for each step h [H], we assume that we have access to aclass of functions Fh : S A O S such that the perfect decoding function h Fh. We aim for our final bounds to depend on a complexity measure of the function class F = {Fh}h[H]rather than the cardinality of the observation space O. We utilize the Daniely and Shalev-Shwartz-Dimension (DS Dimension) (Theorem E.2), which characterizes the PAC learnability for multi-classclassification . Defining the DS dimension is beyond the scope of our paper; we direct interestedreaders to for further details. For intuition, readers can think of the DS Dimension as a certificateof PAC learnability without loss of intuition. Theorem E.2 (Theorem 1 in ). Consider a family of multi-class classifiers F that map features inspace x X to labels in space y Y. Moreover, assume there is a joint probability distribution D overfeatures in X and labels in Y, and that there exists g F such that for each (x,y) supp(D), g(x) = y.Given n samples from D, there exists an algorithm that with probability at least 1 outputs g Fsuch that",
  "We are now ready to present the main theorem of this section": "Theorem E.3. Consider a POMDP P that satisfies Definition 3.2, a policy E S, and let {Fh {S A O S}}h[H] be the decoding function class, and h Fh for each h [H], i.e., {Fh}h[H] isrealizable. Then given access to the classification oracle of , there exists an algorithm learning thedecoding function {gh}h[H] such that with probability at least 1 , for each step h [H]:",
  "Dimension of Fh": "Combining Theorem E.3 and Lemma 4.4, we obtain the final polynomial sample complexity inthis function approximation setting, using classification (supervised learning) oracles (c.f. ).Proof of Theorem E.3:For each step h [H], we define Dh to be the distribution over the underlying state sh1 at steph1, taken action ah1 A from E, the underlying state transitioned to sh S, and the hallucinatedobservation oh Oh(|sh) (we remind readers that for step 0, we use dummy state s0 and action a0).Formally, the probability that the sequence (sh1,ah1,oh,sh) is sampled from Dh equals to",
  "where V h (zh,sh) = Eahh(|zh)[ Qh (zh,sh,ah)]": "Setting = t L LP, and = L LP, where we remind that argmaxL V 1 (s1), and for each zh Zh and h [H], we abuse the notation by letting th( | zh,sh) =th( | zh) and h( | zh,sh) = h( | zh) for all sh S. The above formulation is thus simplified to",
  "T H log(|A|)": "The proof follows by combining Equation (F.1) and the inequality above. Finally, to achieve the nearoptimality in the class of L, we bound the optimistic estimate using Equation (F.2) in Lemma F.3,and its global near-optimality for a large enough L under -observability is a direct consequence ofTheorem 4.1 in .",
  ",": "where the first inequality is by the first inequality in Lemma H.7, the second and third inequalitiesare due to the fact that TV distance does not increase after marginalization, and the last inequality isby Lemma F.9. Since i is a fixed and fully-observable Markov policy, by Lemma F.11, we have",
  ") such that d(h,sh)h(sh) ph(sh)": "2for each sh U(h,1) with probability 1 1. Now we assume this event holds for any h [H] andsh U(h,1). For each sh S and ah A, we have executed in Algorithm 4 the policy (h,sh) followedby an action ah A for N episodes, and denote the number of episodes that sh and ah are visited asNh(sh,ah). Then with probability 1 eN1/8, Nh(sh,ah) Nph(sh)",
  "for any b,b (S) and Oh Oh := maxshS Oh(|sh) Oh(|sh)1. Therefore, if one can ensure thatthe emission at any state sh is learned accurately in the sense that Oh(|sh) Oh(|sh)1": "2 , we canconclude that Oh is also /2-observable. However, the key challenge here is that there could existsome states sh that can only be visited with a low probability no matter what exploration policy is used. Therefore, emissions at such states may not be learned accurately. To address this issue, our keytechnique is to redirect the transition probability into states that cannot be explored sufficiently tosome highly visited states, in a new POMDP that is close to the original one in generating the beliefs.Specifically, first, for any 1 > 0, we define",
  "it is guaranteed that Osubh Osubh": "2 . Therefore, we conclude that Osubhis also /2-observable.Now we are ready to examine b,trunch. We firstly define the following POMDP P sub, which essen-tially deletes all states in Slowhfrom the state space of P trunc at each step h, which does not affect thetrajectory distribution as they were not reachable in P trunc. Notice that the emission of P sub is exactlyOsubh, implying that P sub is a /2-observable POMDP. Therefore, for policy class with finite memoryL with L (4 log(S/), by Theorem 4.1 in , it is guaranteed that for any ,",
  "GMissing Details in": "Implementation details.For each problem setting, we generated 20 POMDPs randomly and reportthe average performance and its standard deviation for each algorithms. For our privileged valuelearning methods, we find that using a finite memory of 3 already provides strong performance.For our privileged policy learning methods, we instantiate the MDP learning algorithm with thefully observable optimistic natural policy gradient algorithm . Meanwhile, for both the decoderlearning and belief learning, we find that just utilizing all the historic trajectories gives us reasonableperformance without additional samples. For baselines, the hyperparameters for Q-value updateand step size for the policy update are tuned by grid search, where controls the update of temporaldifference learning (recall the update rule of temporal difference learning as Q (1)Q+Qtarget).For asymmetric Q-learning, we use -greedy exploration, where we use the seminal decreasing rateof t = H+1",
  "H+t . Finally, all simulations are conducted on a personal laptop with Apple M1 CPU and16 GB memory": "More results for general POMDPs.We further experimented on problems of the larger size. In , we provided additional 9 cases in the following figures corresponds to POMDPs withdifferent combinations of mthe odel size, S = A = O {4,6,10} and H {10,20,30}. More results for POMDPs satisfying the deterministic filter condition.In , we providedadditional 9 cases, which correspond to POMDPs with different combinations of the model size,S = A = O {4,6,10} and H {20,40}.",
  "PP (sh |h) = Uh(bh1(h1),ah1,oh) = PP (sh |sh1 = h1(h1),ah1,oh)": "Meanwhile, since we know PP (|h) is a one-hot vector, we can construct h such that h(sh1,ah1,oh)is the unique sh that makes PP (sh |h) > 0 with sh1 = h1(h1). If this procedure does not completethe definition of for some (sh1,ah1,oh), it implies that either sh1 is not reachable or oh is notreachable given sh1, i.e., PP (oh |sh1,ah1) = 0 for any ah1 A, thus recovering the conditions ofDefinition 3.2.Proof of Proposition 7.3:Note that for any given problem instance of a POMDP, we can construct a POSG by adding adummy agent that has the observation being the exact state at each time step, and has only onedummy action that does not affect the transition or reward. Therefore, even the local private in-formation pi,h of the dummy agent can decode the underlying state, and hence (ch,ph) reveals theunderlying state. Therefore, the corresponding POSG with the dummy agent satisfies the conditionin Proposition 7.3. Meanwhile, it is direct to see that any CCE of the POSG is an optimal policy forthe original POMDP. Now by the PSPACE-hardness of planning for POMDPs , we proved ourproposition.",
  "CE-gap(g) CE-gap() 2nH2 maxi[n] maxmiMimaxj[n],h[H]P(mii)i,G(sh gj,h(ch,pj,h))": "where g is the distilled policy of through the decoding functions g, where at step h, each agenti firstly individually decodes the state by sampling sh gi,h(|ch,pi,h), and then acts according to theexpert i,h, where in the following discussions, we slightly abuse the notation and regard gi,h(ch,pi,h)as a random variable following the distribution of gi,h(|ch,pi,h). In other words, the decoding processdoes not need correlations among the agents. Proof. For notational simplicity, we write vi instead of vGi when the underlying model is clear fromthe context. Firstly, we consider any deterministic decoding function = {i,h}i[n],h[H] with i,h :Ch Pi,h S for each i [n],h [H], and note the following that for any i [n],",
  ") such that maxi[n] di(h,sh)ih(sh) ph(sh)": "2for each sh U(h,1) withprobability 1n1. Now we assume this event holds for any h [H] and sh U(h,1). For each sh Sand ah A, we have executed each policy {i(h,sh) i}i[n] for the first h 1 steps followed by anaction ah A for N episodes and denote the total number of episodes that sh and ah are visited asNh(sh,ah), and Nh(sh) = aA Nh(sh,a). Then, with probability 1 eN1/8, we have Nh(sh,ah) Nph(sh)",
  "which completes the proof": "Note that although our Algorithm 5 and Theorem H.4 are stated for NE/CCE, it can also handleCE with simple modifications, where the key observation is that the strategy modification mi MS,ican also be regarded as a Markov policy in an extended MDP marginalized by i as defined below. Definition H.5. Fix S. We define Mextendedi() to be an MDP for agent i, where for eachh [H], the state is (sh,ai,h), the action is some modified action ai,h, the transition is definedas Textendedh(sh+1,ai,h+1 |sh,ai,h,ai,h) := Eai,hh(|sh,ai,h)[Th(sh+1 |sh,ai,h,ai,h)h+1(ai,h+1 |sh+1)], where weslightly abuse the notation of h(ai,h |sh,ai,h) and h(ai,h |sh) by defining them as the posterior andmarginal distributions induced by the joint distribution h(ah |sh). Similarly, the reward is given byrextendedh(sh,ai,h,ai,h) := Eai,hh(|sh,ai,h)[rh(sh,ai,h,ai,h)].",
  "(H.3)": "in Algorithm 7, for some absolute constant C3 > 0.Before presenting our technical analysis, we define the following notation for the ease of presen-tation. We define the following approximate value functions for any policy in a backward wayfor h [H] when given some approximate belief in the form of {Ph : Ch (Ph S)}h[H] as discussedin .2:",
  ") with probability 1": "Proof. Note that Algorithm 8 is essentially treating the POSG G as a centralized MDP and runningAlgorithm 4, where the only modifications we make in Algorithm 8 is that we take the controllerset (see some examples of the controller set in Appendix A.3) into considerations when learningthe models. Specifically, for the transition Th, what we estimate is only Th(sh+1 |sh,aIh,h) instead ofTh(sh+1 |sh,ah), where Th [n] is the controller set. Therefore, the sample complexity of Algorithm 8will not be worse than that of Algorithm 4. Proof of Theorem 7.7:Note that the proof idea essentially resembles that of Theorem F.6, where we construct the modelGtrunc for G in exactly the same way as constructing P trunc for P. Therefore, by Corollary F.10, wehave",
  "H|P,G(H) P,Gtrunc(H)| 4HS1": "Meanwhile, we can construct Gtrunc and Gsub using exactly the same way as for P trunc and P sub, whereGsub is a /2-observable POSG.Now, according to , for all the examples in Appendix A.3, there exists a compression functionthat maps ch to ch such that the size of the compressed common information is quasi-polynomial, i.e.,Ch (AO)C4 log SH",
  "H.1Background on Bayesian Games": "The Bayesian game is a generalization of normal-form games in partially observable settings. Specifi-cally, a Bayesian game is specified as (n,{Ai}i[n],{i}i[n],{ri}i[n],), where n is the number of players,Ai is the actor space, i is the type space, ri : i[n](i Ai) is the reward function, and is the prior distribution of the joint type. At the beginning of the game, a type = (i)i[n] is drawnfrom the prior distribution (). Then each agent i gets its own type i and takes the action ai.We define a pure strategy of an agent as si STi := {i Ai}. We define Ji(si,si) to be the expectedrewards for agent i, given the pure joint strategy (si,si).By definition, Ji(si,si) can be evaluated as",
  "EsJi(s) EsJi(mi si,si) ,i [n],mi Mi,": "where Mi = {i Ai Ai} is the space for strategy modification, where mi modifies si as follows:given current type i and the recommended action ai, the strategy modification changes the actionto the another action mi(i,ai).Note that Bayesian NE for zero-sum games, and (agent-form) Bayesian CE/CCE are all tractablesolution concepts and can be computed with polynomial computational complexity, e.g., ."
}