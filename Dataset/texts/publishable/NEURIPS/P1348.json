{
  "Abstract": "Understanding the general priniciples behind transformer models remains a com-plex endeavor. Experiments with probing and disentangling features using sparseautoencoders (SAE) suggest that these models might manage linear features em-bedded as directions in the residual stream. This paper explores the resemblancebetween decoder-only transformer architecture and vector symbolic architectures(VSA) and presents experiments indicating that GPT-2 uses mechanisms involvingnearly orthogonal vector bundling and binding operations similar to VSA for com-putation and communication between layers. It further shows that these principleshelp explain a significant portion of the actual neural weights.",
  "Introduction": "Understanding the way transformer models work remains a complex task, impacting AI in-terpretability, safety, and alignment, among other areas. Successes in using linear probing to steermodels and principal component analysis experiments on hidden embeddings indicatethat these models might handle linear features embedded as directions in the residual stream, whichis also known as linear representation hypothesis . Recently, sparse autoencoders havebeen trained for a number of related model architectures based on the idea that hidden embeddingscan be broken down into a combination of sparse feature vectors . One way how a linear additivemodel might function is by using nearly orthogonal vectors instead of arbitrary (or strictly orthogonal)directions. In high-dimensional spaces, we can have many more nearly orthogonal vectors thanstrictly orthogonal ones . Vector symbolic architectures (VSA) , also known as hyperdi-mensional computing, leverage this principle for model construction. For example, we can createbag-of-concept vectors by summing nearly orthogonal vectors, so that individual concept vectorsmaintain a higher dot product with the summed vector, effectively modeling an OR conjunction. Theresidual stream of transformers could represent such a collection of nearly orthogonal concept vectorsto form a distributed code. However, current evidence is limited on whether production models likeGPT effectively use such feature vectors and how. A better understanding of the inner mechanics hasmany implications on model interpretability and the usefulness of training sparse autoencoders, aswell as on architectural design decisions. This paper explores the resemblance between decoder-onlytransformer architectures and vector symbolic architectures and presents experiments indicating thatGPT-2 uses mechanisms involving nearly orthogonal vector bundling and binding operationssimilar to VSA. It further shows that these principles help explain a significant portion of the actualweights in the MLP layers.",
  "Interpreting GPT-2 Through Bundlings and Bindings": "In high dimensional spaces, we can have many more nearly orthogonal vectors than dimensions .For an n-dimensional space, we cannot find more than n distinct vectors such that they are orthogonalto each other, i.e., the inner product (dot product for real spaces) of any vector pair is exactly zero.However, we can find much larger sets of vectors such that the dot product of any vector to any othervector is nearly zero or small. One can think about a unit sphere in which the surface is divided intosmall patches such that the centroids of these areas are sufficiently far away, corresponding to a largeenough angle (i.e., small enough dot product). We can imagine that there are many more such patchesthan the number of dimensions, depending on our dot product threshold. Bundling and binding ofsuch nearly orthogonal concept vectors are important principles of vector symbolic architectures ,but similar ideas are used in many other techniques, e.g., in vector-based linear models . Bundling refers to the bagging of vectors to create composite concept vectors. Imagine we have threenearly orthogonal vectors c1, c2, c3, each representing a specific (binary) concept. We can bundle c1and c2 into a concept vector c1,2 = c1 + c2 that has directional similarity to both vectors c1 and c2but not to c3. We can efficiently test whether a vector cj is similar to either c1 or c1: c1,2 , cj ,essentially corresponding to a single perceptron with a ReLU activation function. In addition to bundling, we can also perform a binding operation by multiplying a suitable matrixM with the concept vector. For instance, this allows us to model a specific order of concepts:cm = Mc1 + c2 is similar to c2, Mc1, and M 1c1, but not to c1 (if M = I). M could be used hereto model that c1 refers to the first concept of the pair (c1, c2). This binding still works even if thematrix is not orthogonal but composed of nearly orthogonal vectors, i.e., M M will lead to a nearlydiagonal matrix (see Appendix A). Binding transformations can be interpreted in two ways. We can regard them as binding operators of(possibly previously bundled) concept vectors. We can also think of them as a construction of a newconcept vector based on binary input features by adding those nearly orthogonal vectors (the rowsof the matrix) where the corresponding input dimension is non-negative, with positive and negativesigns as indications of the (binary) direction of the concept. From a purely technical standpoint, bothinterpretations are valid. The latter interpretation might be more helpful if we can assign semantics tospecific dimensions of the input vectors (monosemanticity). We can further build hierarchical concept vectors through bindings . For instance, if we start witha set of concepts that could apply to people in general, we might first bundle the present conceptsfor two persons, respectively, which we can then bind with two different matrices M1, M2. If wewant to compare whether the first person in two vectors are similar, we can unbind those vectorsusing the inverse of M1 and then take the dot product. In other words, binding can be interpreted as apackaging step, whereas the unbinding process can be used to focus on a specific package. We willlater outline the similarities of this interpretation to causal self attention. Throughout this paper, we will focus on GPT-2 as a specific model architecture and instanceof decoder-only transformers. The flow through the hidden embeddings of transformers is oftenreferred to as residual stream since the outputs of the attention and MLP layers are always added tothe previous outputs. This has motivated the training of sparse autoencoders (SAEs) on those hiddenrepresentations for disentangling the embeddings into a sum of sparse feature vectors, assumingthat the residual stream represents many different concepts in superposition . Both the successfultraining of SAEs and experiments on finding linear representations of concepts in the embeddingspace are hints that transformers might store and process features using directions rather than withhighly dense (and somewhat arbitrary) distributed codes. However, sufficiently large SAEs couldtheoretically approximate any embedding (e.g., by dedicating one feature for every observedtraining input). Further, the sole observation of linear separability for a selection of concepts does not automatically imply that trained transformers effectively employ nearly orthogonal concept vectorsand perform operations that we can interpret as matrix bindings. On the other hand, the modelarchitecture bears similarities with vector symbolic architectures. We outline in this section howtransformers, and GPT-2 in particular, could be interpreted with bundling and binding mechanismsclosely related to VSAs. depicts a simplified architecture of GPT-2 through the lens of VSA.",
  "Word Embeddings": "The word embeddings of GPT-2 are nearly orthogonal to each other, so we could interpret them asconcept vectors. They are not pure or random concept vecors, though, since related tokens do pointto similar directions (e.g., pairs of different versions of the the token have high dot products). Animportant design decision of GPT is that the unembedding matrix at the end is just the transpose ofthe embedding matrix. This means that the layers need to add values to the residual stream such thatthe combined magnitude of those additions outweigh the initial word embedding by far. The finalstep in predicting the actual next token is essentially a determination which direction is the strongestsince we take the dot product with every word embedding vector and then assign model probabilitiesthrough the softmax function.",
  "Attention Layer": "The causal self attention mechanism in GPT-2 first computes query, key, and value vectors vq, vk, vvby linearly transforming the current residual input x with square matrices WQ, WK, WV (we willignore biases for now). Let us first consider the case with just one attention head for simplicity. Then,the output of the attention added to the stream is determined by taking a weighted sum of all vvvectors of the current and previous positions, followed by a linear projection WO. The weights of thesum are based on the dot product between the query and respective value vectors, normalized withthe softmax function. If we assume that our learned matrices WQ, WK, WV are nearly orthogonal,we could interpret this process as a sequence of unbinding operations (WQ, WK, WV ), followed bythe bundling of a selection of such unbound vectors vv based on their relevance, followed by thebinding of the resulting sum vector through WO. In other words, the attention head focuses on a specific package of possibly bundled vectors byrotating the input space appropriately. This package could be different for the current position(query) compared to previous positions (key). It then computes how similar the concepts are for thespecific package. The softmax function effectively acts as a (dynamic) dot product thresholding sinceonly those products that are sufficiently close to the maximum attention value will lead to non-zerocoefficients when taking the weighted sum of the value vectors. The resulting output is bound againto obtain unique concept vectors for the specific attention layer. Production models have more than one attention head, but this requires only minor modificationsto this outlined analogy. Each attention head may be interpreted as a quantized unbinding / bindingoperation, that is, each head focuses on a specific package but only cares about the first few dimensionsof the resulting transformation (the remaining dimensions are implicitly set to zero). Using thisinterpretation, attention heads therefore copy (and repackage) concepts into the current stream,sourced from specific packages of those tokens that have conceptual similarities in specific areas.",
  "LayerNorm and Biases": "In GPT-2, the attention and MLP layers first apply LayerNorm before their processing (the residualstream itself is not modified, though). LayerNorm essentially centers and normalizes the input bysubtracting the mean and then dividing it by the square root of the variance. It then scales the resultwith channel-specific learnt weights and adds a bias to each channel. We can merge the latter step(scaling and translation) into the subsequent weights and biases of the respective layer . Thecentering and normalization depends on the actual input, though. While the normalization does notimpact the angle between vectors, this is not true for translations. Hence, accessing concepts in laterlayers only works if the typically observed input vectors and intermediate embeddings have a meannear zero so that distortions across layers are minimized.",
  "The biases in WQ, WK only add a constant factor to the dot product, which we can ignore. Thebiases of WV can be merged into the biases of WO as the softmax attention values always add up to": "one. The biases of WO, Wout (attention, MLP blocks) are constant additions to the residual stream.Given that they are nearly orthogonal to the word embeddings, they could serve as additional helpersfor cleaning the initial input by adding vectors that gradually decrease the share of the initial inputon the final result without changing the output prediction.",
  "Processing of Concepts": "Earlier experiments indicated that some neurons in GPT-2 focus on very specific tokens, such as the.Assuming that MLP blocks perform boolean operations on concept vectors and that word embeddingsare concept vectors, we would expect some neurons to have weights representing either a single wordvector or a composite vector made of several word vectors (e.g., the sum of all word embeddings thatare variations of the). To test this hypothesis, we conducted experiments. Specifically, we aimed to determine if some neural weights could be explained by simple bundledvectors that are merely sums of word embeddings. For each possible input vector (word embedding),we compiled a list of the most similar neurons. That is, we looked for neurons in the first feedforwardlayer of the MLP block whose input weights have a reasonably high dot product with the centeredcandidate vector. For each neuron and its list of candidates, we then greedily assembled the bundledvector by including those vectors (ranked by similarity) that lead to an increase of the cosine similaritybetween the bundled vector and the respective neural weights. We discarded vectors with a cosinesimilarity of less than 0.05 with the neural weight vector. We also discarded those with less than 0.1if they did not significantly increase the overall cosine similarity by more than 0.04. These thresholdsrepresent hyperparameters and were conservatively chosen to obtain sets of highly relevant vectors.We enforced a minimal similarity to the target vector and only considered unweighted sums since anyvector can be trivially represented by a weighted sum of sufficiently many nearly orthogonal vectors. With this simple approach, we achieve a similarity of 0.5 or higher for more than 80% of the neuronsin the first layer, and at least 0.3 for more than 95% (this includes attributions to attention heads wediscuss in the next section). For instance, our greedily obtained vector for neuron 1844 in the firstlayer has a high cosine similarity of 0.69 with the actual weights of that neuron. The list containsmany first names, most of them male sounding ( Chris, Kevin, Jeff) with some exceptions (Rebecca). Neuronpedia explains this as first names of people. Neuron 20 (similarity of 0.73) focuseson tokens somehow semantically related to maintaining, including Nevertheless and Storage(Neuronpedia says verbs related to maintaining or keeping something). lists more examples.It is important to note that we inferred these neuron explanations directly from the weights rather thanthrough activation observations and correlations, or by training an autoencoder or probing classifier.",
  "Experiments": "The previously outlined framework is one possibility how different layers in transformers couldcommunicate with each other and compute functions that ultimately lead to next token predictions.We performed several experiments on GPT-2 (small) to gather evidence on the suitability of thisconceptual framework for the interpretation of the inner workings. We used the TransformerLens library to extract weights and activations from GPT-2 small. We disabled all in-built post-processingsteps except for the merging of value biases into the projection biases of the attention output projectionWO and the incorporation of the LayerNorm scaling and biases into the respective subsequentweights and biases (MLP blocks, attention blocks, and unembedding matrix). We relied on the FAISSlibrary for an efficient implementation of dot product nearest neighbor search (HNSW algorithm).",
  "Near Orthogonality of Vectors and Matrices": "Near orthogonality of vectors and matrices are an important assumption of concept vectors andbinding operations. We computed the matrix product M M for word embeddings, the attentionmatrices, and the output projections of the MLP blocks. Appendix B shows some results. In allcases, we observed strong diagonals with minimal artifacts, suggesting that these matrices are indeedcomposed of nearly orthogonal vectors. The output biases of the attention and MLP blocks are nearlyorthogonal to the word embeddings. The magnitude of the attention output bias in the last layeris significantly higher, suggesting that this is a final cleaning step to ensure that the initial tokenembedding does not play a role anymore in the unembedding step. The mean of the word embeddingsand intermediate hidden embeddings for a number of prompts are near zero (< 0.05), suggesting lowdistortions of the angles of directions across layers.",
  "NeuronVectorCos. Sim": "0-12CNN +Cold +NBC + . . . +lying + . . . +TRUMP + . . .0.610-186remarked +commented +complained +said + . . .0.630-192being +Being +Having +be +Be +beings + . . .0.650-205selections +eclectic +selection +assortment + . . .0.580-247resulted +based +Given +highlighted + . . .0.62 0-809bring, bringing, ... 0-3028 relating, relation, ... 7-132111-2172to to : Circuit discovery using similarities between Wout and Win vectors based on greedilyobtained concept vectors. The two neurons in the first layer each focus on a set of tokens. The neuron7-1321 seems to model a union of those sets, whose output is similar to the input of the depictedneuron in the last layer and the to unembedding. The last one has many inbound connections toprevious outputs and seems to boost the concept of the to vector.",
  "Processing Circuits": "Using the analogy of concept vectors and matrix bindings, we would expect that the MLP blocksperform (boolean) functions on concept vectors sourced from either the token embeddings, previousresults from MLP blocks (information from computations), or attention blocks (information fromother tokens). The results are then written back to the stream via matrix binding. The sum of all written concept vectors (including those from the attention blocks) should exhibit the highestdirectional similarity with the token embedding that is up next. This led us to investigate whetherneural weights could be explained based on WO vectors (output projection vectors of the attentionlayer) and Wout vectors (final projection in MLP block). This would support the hypothesis thatGPT-2 communicates via near-orthogonal vectors between layers. This communication either aimsto bring the stream closer to the predicted next token or serves as an input for subsequent circuits.Analogous to the previous experiment, we tried to explain neural weights with simple bundled vectors.For this set of experiments, we included output projection vectors of the attention layer WO and ofthe MLP block Wout, in addition to the word embeddings. We were able to reconstruct bundled vectors with a cosine similarity of at least 0.3 with neural inputweights in 30-50% of neurons within the middle layers, as well as the majority of neurons in boththe first and last layer. The majority of bundlings were composed of up to 40 vectors. For someneurons, our simple circuit discovery strategy explained the weights nearly perfectly. For instance,neuron 2537 in layer 1 can be explained by the non-activation of neurons 2977 and 1993 in layer0, showing a similarity of 0.91. Neuron 1073 in layer 1 seems to be dependent on the presence ofconcept 11 in head 1.4 and the absence of concepts 31 in head 1.2 and 1 in head 1.10, with a weightsimilarity of 0.72. illustrates a circuit that seems to increase the likelihood of predicting theword to following certain words like bring or relating. When forcing the output of neuron 7-1321to zero, the logit for the token to decreases for the prompt In relation but not for the prompt I waslistening. Neuron 12 in the first layer appears to trigger on a specific set of tokens including names ofmajor news outlets (e.g., CNN) and the word TRUMP. Using our strategy, we were unable to identifyother neurons significantly influenced by the output of this neuron. However, we observed a cosinesimilarity of 0.07 with the token Greenwald. Although this seems low, it is crucial to remember thattokens potentially encompass a broad array of concepts. The model may have overfitted to newsarticles related to Glenn Greenwald and the US election. The first and last layers have strong similarities to token embeddings. The residual stream in higherlayers already points toward likely tokens that should be predicted next. One possible explanation isthat the last layer therefore weakens or confirms current predictions by also taking into account whichtokens are likely to be predicted next. It is important to note that we employed a greedy method foridentifying related vectors and thus circuits, which assumes a certain degree of monosemanticity inindividual neurons and attention dimensions. Our objective was to gather evidence of matrix bindingand the processing of nearly orthogonal vectors, rather than to identify all communication channelscomprehensively.",
  "Related Work": "Linear representations in models and embeddings have a long history in research, with Word2Vec popularizing the idea of performing simple arithmetic on word embeddings. Numerous probing ex-periments have been performed to investigate linear representations in large language models . Hernandez et al. found evidence that relational decoding in large languagemodels can often be approximated with an affine transformation. Wattenberg and Vegas discussrelational composition in context of sparse autoencoders, noting that matrix bindings bear similaritieswith the attention process. The field of mechanistic interpretability is concerned with reverseengineering and explaining (in parts) the inner workings of models, for instance, through discoveringand describing circuits , or by trying to understand whether certain behavior can be trackeddown to individual neurons or is represented in a more distributed way .",
  "Discussion and Conclusion": "Our experiments indicate that GPT-2 effectively learns mechanisms that are similar to concepts ofvector symbolic architectures. The different blocks seem to communicate by writing and readingnearly orthogonal vectors from the residual stream. We could also show that at least some neuronsimplement simple boolean functions as outlined in . This has several implications onmodel interpretability and architectural design decisions. The presence of bundling and bindingcircuits strengthens the validity of training sparse autoencoders for model disentanglement. However,such nearly orthogonal vectors may not necessarily map to interpretable features or independentlyunderstandable features (decomposability theory ). They may be part of what Engels et al. describe as multidimensional features . Our findings may also inform future architectural decisions.For instance, it may help to understand which architectural components promote or hinder thelearning of bundling and binding operations, possibly leading to better model architectures. A betterunderstanding of the inner workings also helps to hypothesize for which type of tasks GPT-likemodels work better or worse. We want to emphasize that our findings are preliminary and our method has several limitations. Weonly performed experiments on GPT-2 small and further experiments are needed to confirm thatthese behaviors also exist in related and more recent models. We deliberately focused on very simpletechniques and avoided the use of sparse autoencoders in our experiments to mitigate confoundingfactors and uncertainties introduced by additional modeling.",
  "ABundling and Binding of Concept Vectors": "In this paper, we will consider real valued space Rn with n 100. In such high dimensional spaces,we can have many more nearly orthogonal vectors than dimensions . For an n-dimensional space,we cannot find more than n distinct vectors such that they are orthogonal to each other, i.e., the innerproduct (dot product) of any vector pair is exactly zero. However, we can find much larger sets ofvectors such that the dot product of any vector to any other vector is nearly zero or small. One canthink about a unit sphere in which the surface is divided into small patches such that the centroids ofthese areas are sufficiently far away, corresponding to a large enough angle (i.e., small enough dotproduct). We can imagine that there are many more such patches than the number of dimensions,depending on our dot product threshold. Imagine we have three big vectors c1, c2, c3, each representing a specific (binary) concept. If thesevectors are nearly orthogonal, we can create a compositional concept vector c1,2 = c1 + c2 that hasdirectional similarity to both vectors c1 and c2 but not to c3. In vector symbolic architectures (VSA),this is referred to as bundling or superpositioning. We can test whether a vector cj is similar to eitherc1 or c1 with a simple dot product and thresholding, i.e., c1,2 , cj . This summation essentiallyrepresents an OR conjunction (bag of concepts vector). Please note the striking similarity of thistest to a single perceptron with a ReLU activation function. In addition to summation, we can also perform a binding operation by multiplying a suitable invertiblematrix M with the concept vector. For instance, this allows us to model a specific order of concepts:cm = Mc1 + c2 is similar to c2, Mc1, and M 1c1, but not to c1 (if M = I). M could be used hereto model that c1 refers to the first concept of the pair (c1, c2). We can see that if M is chosen well,the dot products are preserved: Mc1 , Mc2 = (Mc1)(Mc2) = c1M Mc2If M is an orthogonal matrix, this corresponds to a basis transformation, preserving the length anddot products of vectors as it is just a rotation (or reflection) in simpler terms and M M is just theidentity matrix. Even if the matrix is not orthogonal but composed of nearly orthogonal vectors,M M will lead to a nearly diagonal matrix. Looking at the inner product definition, we can see thatif the scalars are roughly similar, the angle between vectors does not change much after the bindingtransformation. We will call such matrices nearly orthogonal matrices given that they preserve thedot product sufficiently well. Such transformations can be interpreted in two ways. We can regard them as binding operators of(possibly previously bundled) concept vectors. We can also think of them as a construction of a newconcept vector based on binary input features by adding those nearly orthogonal vectors (the rowsof the matrix) where the corresponding input dimension is non-negative, with positive and negative WQ WQ T Layer 0 Head 0 WK WK T Layer 4 Head 3 WV WV T Layer 7 Head 10 WO WO T Layer 3 Head 1",
  ": The attention matrices are composed of nearly orthogonal vectors. This is an exemplaryselection of attention heads and their corresponding matrix multiplication MM": "signs as indications of the (binary) direction of the concept. From a purely technical standpoint, bothinterpretations are valid. The latter interpretation might be more helpful if we can assign semantics tospecific dimensions of the input vectors (monosemanticity). We can build hierarchical concept vectors through bindings . For instance, if we start with aset of concepts that could apply to people in general, we might first take two sums capturing thepresent concepts of two specific persons, respectively, which we can then bind with two differentmatrices M1, M2. If we want to compare whether the first person in two vectors are similar, we canunbind those vectors using the inverse of M1 and then take the dot product. In other words, matrixbinding can be interpreted as a packaging step, whereas the unbinding process can be used to focuson a specific package. Bundling and binding of nearly orthogonal vectors are important principlesof vector symbolic architectures , but similar ideas are used in many other techniques, e.g., invector-based linear models .",
  "BFigures": "Near orthogonality of vectors and matrices are an important assumption of concept vectors andbinding operations. We computed the matrix product M M for word embeddings, the attentionmatrices, and the output projections of the MLP blocks. In all cases, we observed strong diagonalswith minimal artifacts, suggesting that these matrices are indeed composed of nearly orthogonal Wout Wout T Layer 5 (excerpt) Wout Wout T Layer 0 (excerpt) : The output projection matrices of the MLP blocks are composed of nearly orthogonalvectors. This is an exemplary selection of output projection matrices for specific layers and thecorresponding matrix multiplication MM . We only show the top left cutout of the visualization aseach layer contains 3072 neurons."
}