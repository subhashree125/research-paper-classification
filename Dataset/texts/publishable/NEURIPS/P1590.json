{
  "Abstract": "Understanding treatment effect heterogeneity is vital for scientific and policy re-search. However, identifying and evaluating heterogeneous treatment effects posesignificant challenges due to the typically unknown subgroup structure. Recently, anovel approach, causal k-means clustering, has emerged to assess heterogeneityof treatment effect by applying the k-means algorithm to unknown counterfactualregression functions. In this paper, we expand upon this framework by integratinghierarchical and density-based clustering algorithms. We propose plug-in esti-mators that are simple and readily implementable using off-the-shelf algorithms.Unlike k-means clustering, which requires the margin condition, our proposedestimators do not rely on strong structural assumptions on the outcome process. Wego on to study their rate of convergence, and show that under the minimal regularityconditions, the additional cost of causal clustering is essentially the estimation errorof the outcome regression functions. Our findings significantly extend the capabil-ities of the causal clustering framework, thereby contributing to the progressionof methodologies for identifying homogeneous subgroups in treatment response,consequently facilitating more nuanced and targeted interventions. The proposedmethods also open up new avenues for clustering with generic pseudo-outcomes.We explore finite sample properties via simulation, and illustrate the proposedmethods in voting and employment projection datasets.",
  "Heterogeneity of Treatment Effects": "Causal effects are typically summarized using population-level measures, such as the average treat-ment effect (ATE). However, these summaries may be insufficient when treatment effects vary acrosssubgroups. For example, finding the subgroups that experience the least or greatest benefit from aspecific treatment is of particular importance in personalized medicine or policy evaluation, wherethe subgroup effects of interest may diverge significantly from the population effect. Even whileexperiencing the same treatment effects, some people may have been exposed to a significantly higherbaseline risk. In the presence of effect heterogeneity, the typically unknown subgroup structureposes significant challenges in accurately identifying and evaluating subgroup effects compared topopulation-level effects. To delve deeper than the information provided by the population summaries and to better understandtreatment effect heterogeneity, investigators often estimate the conditional average treatment effect(CATE) defined by",
  "arXiv:2411.01250v1 [stat.ME] 2 Nov 2024": "where Y a is the potential outcome that would have been observed, possibly contrary to fact, undertreatment A = a, and X X is a vector of observed covariates. The estimation of the CATEhas the potential to facilitate the personalization of treatment assignments, taking into account thecharacteristics of each individual. Admittedly, the CATE is the most commonly-used estimandto study treatment effect heterogeneity. Various methods have been proposed to obtain accurateestimates of and valid inferences for the CATE, with a special emphasis in recent years on leveragingthe rapid development of machine learning methods [e.g., 3, 20, 21, 27, 35, 42, 46, 52, 53, 58, 62]. Subgroup analysis has been the most common analytic approach for examining heterogeneity oftreatment effect. Selection of subgroups reflecting ones scientific interest plays a central role in thesubgroup analysis. Statistical methods aimed at finding such subgroups from observed data have beentermed subgroup discovery . The selection of such subgroups may be informed by mechanismsand plausibility (e.g., clinical judgment), taking into account prior knowledge of treatment effectmodifiers. They could be chosen by directly subsetting the covariate space, often in a one-variable-at-a-time fashion [e.g., 49]. Most existing studies on data-driven subgroup discovery identify subgroupswhere the CATE exceeds a prespecified threshold of clinical relevance, allowing researchers toprioritize subgroups with enhanced efficacy or favorable safety profiles [e.g., 6, 11, 44, 47, 51, 63].Some recent advances proposed heuristics for discovering rules based on a specific CATE estimatorsubject to a certain optimality criterion, yet without any theoretical exploration [e.g., 8, 15, 23, 48].Wang and Rudin proposed an algorithm to automatically find a subgroup based on the causal rule:(CATE > ATE). Kallus proposed a subgroup partition algorithm for determining a subgroupstructure that minimizes the personalization risk.",
  "Causal Clustering": "In contrast to earlier work predominantly focused on supervised learning approaches, there is a grow-ing interest in analyzing heterogeneity in causal effects from an unsupervised learning perspective,particularly within the causal discovery literature. Based on the causal graph or structural causalmodel framework, there has been a series of recent attempts to learn structural heterogeneity throughclustering analysis [e.g., 25, 26, 41, 45]. Conversely, the exploration of treatment effect heterogeneityin the potential outcome/counterfactual framework using unsupervised learning methods has receivedsignificantly less attention. To our knowledge, only one paper has developed such methods; Kimet al. have proposed Causal k-Means Clustering, a new framework for exploring heterogeneoustreatment effects leveraging tools from cluster analysis, specifically k-means clustering. It allows oneto understand the structure of effect heterogeneity by identifying underlying subgroups as clusterswithout imposing a priori assumptions about the subgroup structure. To illustrate, we consider binary treatments and project a sample onto the two-dimensional Euclideanspace (E[Y 0 | X], E[Y 1 | X]). It is immediate to see that closer units are more homogeneous interms of the CATE, which provides vital motivation for uncovering subgroup structure via clusteranalysis on this particular counterfactual space. (See (a) & (e) in ). This approach hasthe capability to uncover complex subgroup structures beyond those identified by CATE summarystatistics or histograms. Moreover, it holds particular promise in outcome-wide studies featuringmultiple treatment levels , because instead of probing a high-dimensional CATE surface toassess the subgroup structure, one may attempt to uncover lower-dimensional clusters with similarresponses to a given treatment set. However, the method proposed by Kim et al. only applies to k-means clustering. Despite ispopularity, k-means has some drawbacks. It works best when clusters are at least roughly spherical.It also has trouble clustering data when the clusters are of varying sizes and density, or based onnon-Euclidean distance. Furthermore, the cluster centers (centroids) might be dragged by outliers,or outliers might even get their own cluster. Other commonly-employed clustering algorithms,particularly hierarchical and density-based approaches, could mitigate some of these limitations. Density-based clustering is applicable for identifying clusters of arbitrary sizesand shapes, while concurrently exhibiting robustness to noise and outlier data points. Hierarchicalclustering proves beneficial in scenarios where the data exhibit a nested structure or inherent hierarchy,irrespective of their shape, and can accommodate various distance metrics. It enables for the creationof a dendrogram, which provides insights into the interrelations among clusters across multiplelevels of granularity. illustrates the three methods in the causal clustering framework with",
  "binary treatments, where hierarchical and density-based clustering methods produce more reasonablesubgroup patterns": "In this work, we extend the work of Kim et al. by integrating hierarchical and density-basedclustering algorithms into the causal clustering framework. We present plug-in estimators, whichare simple and readily implementable using off-the-shelf algorithms. Unlike k-means clustering,which requires the margin condition , our proposed estimators do not rely on such strongstructural assumptions on the outcome process. We study their rate of convergence, and show thatunder the minimal regularity conditions, the additional cost of causal clustering is essentially theestimation error of the outcome regression functions. Our findings significantly extend the capabilitiesof the causal clustering framework, thereby contributing to the progression of methodologies foridentifying homogeneous subgroups in treatment response, consequently facilitating more nuancedand targeted interventions. In a broader sense, causal clustering may be construed as a nonparametricapproach to clustering involving unknown functions, a domain that has received far less attention thanconventional clustering techniques applied to fully observed data, notwithstanding its substantiveimportance. Therefore, the proposed methods also open up new avenues for clustering with genericpseudo-outcomes that have never been observed, or have been observed only partially.",
  "Framework": "Following Kim et al. , we consider a random sample (Z1, ..., Zn) of n tuples Z = (Y, A, X) P,where Y R represents the outcome, A A = {1, ..., q} denotes an intervention with finite support,and X X Rd comprises observed covariates. For simplicity, we focus on univariate outcomes,although our methodology can be easily extended to multivariate outcomes. Throughout, we rely onthe following widely-used identification assumptions:Assumption C1 (consistency). Y = Y a if A = a.",
  "(X) = [1(X), . . . , q(X)] .(1)": "Adjacent units in the above counterfactual mean vector space would have similar responses to a givenset of treatments by construction. If all coordinates of a point (X) are identical for a given X, itindicates the absence of treatment effects on the conditional mean scale. Hence, conducting clusteranalysis on the transformed space by allows for the discovery of subgroups characterized by ahigh level of within-cluster homogeneity in terms of treatment effects. Crucially, standard clusteringtheory is not immediately applicable here since the variable to be clustered is , a set of the unknownregression functions that must be estimated. We let {a} be some estimators of {a}. In Sections 3and 4, we analyze the nonparametric plug-in estimators for hierarchical and density-based causalclustering, respectively, where we estimate each a with flexible nonparametric methods and performclustering based on = (1, . . . , q). It is worth noting that can be easily customized for a specific need through reparametrization,without affecting our subsequent results. For example, it is possible that the difference in regressionfunctions may be more structured and simple than the individual components [e.g., 13, 35]. In thiscase, a parametrization such as = (2 1, 3 1, ) could render our clustering task easierby allowing us to harness this nontrivial structure (e.g., smoothness or sparsity) [see 39, ].",
  "Notation. We use the shorthand (i) = (Xi) and (i) = (Xi) = [1(Xi), ..., q(Xi)]. We letxp denote Lp norm for any fixed vector x. For a given function f and r N, we use the notation": "fP,r = [P(|f|r)]1/r =|f(z)|rdP(z)1/r as the Lr(P)-norm of f. We use the shorthandan bn to denote an cbn for some universal constant c > 0. Further, for x Rq and any realnumber r > 0, we let B(x, r) denote an open ball centered at x with radius r with respect to L2 norm,i.e., B(x, r) = {y Rq : x y2 < r} and use the notation B(x, r) for the closed ball. Lastly,we use the symbol to denote equivalence relation between two notationally distinct quantities,especially when introducing a simplified notation.",
  "Hierarchical Causal Clustering": "Hierarchical clustering methods build a set of nested clusters at different resolutions, typicallyrepresented by a binary tree or dendrogram. Consequently, they do not necessitate a predeterminednumber of clusters and allow for the simultaneous exploration of data across multiple granularitylevels based on the users preferred similarity measure. Moreover, hierarchical clustering can beperformed even when the data is only accessible via a pairwise similarity function. There are twotypes of hierarchical clustering: agglomerative and divisive. The agglomerative approach forms adendrogram from the bottom up, finding similarities between data points and iteratively mergingclusters until the entire dataset is unified into a single cluster. The divisive approach employs atop-down strategy, whereby clusters are recursively partitioned until individual data points are reached.Here we only consider the agglomerative approach which is more common in practice . Weremark that the similar argument in this section may be applicable to the divisive approach as well. Consider a distance or dissimilarity between points, i.e., d : Rq Rq . As in previous studies[e.g., 14, 16, 30], we extend d so that we can compute the distance, or linkage, between sets of pointsS1() S1 and S2() S2 in the conditional counterfactual mean vector space as D(S1, S2).There are three common distances between sets of points used in hierarchical clustering: letting N1be the number of points in S1 and similarly for N2, we define the single, average, and completelinkages by mins1S1,s2S2 d(s1, s2),1 N1N2s1S1,s2S2 d(s1, s2), and maxs1S1,s2S2 d(s1, s2),respectively. Single linkage often produces thin clusters while complete linkage is better at sphericalclusters. Average linkage is in between. Causal clustering entails estimating the nuisance regressionfunctions {a}, which necessitates the following assumption.Assumption A1. Assume that either (i) {a} and {a} are contained in a Donsker class, or (ii){a} is constructed from a separate independent sample of same size. Assumption A1 is required essentially because in our estimation procedure, we use the sample twice,once for estimating the nuisance functions {a} and again for determining the clusters. One may usethe full sample if we restrict the flexibility and complexity of each a through the empirical processconditions, as in (i), which may not be satisfied by many modern machine learning tools. In orderto accommodate this added complexity from employing flexible machine learning, we can insteaduse sample splitting [e.g., 12, 64], as in (ii). We refer the readers to Kennedy for moredetails.",
  "aAa a": "Proof of Proposition B.2. Recall that we are given a pair of points s1 = (1(X1), ..., q(X1)), s2 =(1(X2), ..., q(X2)), and their estimates s1 = (1(X1), ..., q(X1)), s2 = (1(X2), ..., q(X2))for X1, X2 X. To prove the theorem, first we upper bound the maximum discrepancy betweend(s1, s2) and d(s1, s2). Since our distance function satisfies",
  "and Pnoise is a valid probability distribution": "The good-neighborhood property in Definition 3.1 is a distributional generalization of both the -strictseparation and the -good neighborhood property from Balcan et al. . , can be viewed asnoise parameters indicating the proportion of data susceptible to erroneous behavior. Next, we assumethe following mild boundedness conditions on the population distribution and outcome regressionfunction.",
  "Assumption A3. P, in Definition 3.1 has a bounded Lebesgue density": "In the next theorem, we analyze the inductive version of the robust hierarchical clustering [5,Algorithm 2] in the causal clustering framework. We prove that when the data satisfies the goodneighborhood properties, the algorithm achieves small error on the entire data set, requiring only asmall random sample whose size is independent of that of the entire data set. Theorem 3.2. Suppose that UN consists of N i.i.d samples from P, that satisfies the (, )-good neighborhood property in Definition 3.1. For n N, consider a random subset Un ={(1), . . . , (n)} UN and its estimates Un = {(1), . . . , (n)} in which clustering to be performed.Let =",
  "Density-based Causal Clustering": "The idea of density-based clustering was initially proposed as an effective algorithm for clusteringlarge-scale, noisy datasets . The density-based methods work by identifying areas of highpoint concentration as well as regions of relative sparsity or emptiness. It offers distinct advantagesover other clustering techniques due to their adeptness in handling noise and capability to find clustersof arbitrary sizes and shapes. Further, it does not require a-priori specification of number of clusters.Here, we focus on the level-set approach [see 50, and the references therein]. With a slight abuse of notation, we let P be the probability distribution of to distinguish it from theobservational distribution P, and p be the corresponding Lebesgue density. We also let K denote avalid kernel, i.e., a nonnegative function satisfyingK(u)du = 1. We construct the oracle kerneldensity estimator ph with the bandwidth h > 0 as",
  ",": "for Rq. Then we define an average oracle kernel density estimator by E(ph) ph and thecorresponding upper level set by Lh,t = { : ph() > t}. Suppose that for each t, Lh,t can bedecomposed into finitely many disjoint sets: Lh,t = C1 Clt. Then Ct = {C1, ..., Clt} is thelevel set clusters of our interest at level t. With regard to the analysis of topological properties of the distribution P, the upper level set of phplays a role akin to that of the upper level set of the true density p, yet it presents various advantages,as indicated in previous studies [e.g., 19, 37, 50, 60]; ph is well-defined even when p is not, phprovides simplified topological information, and the convergence rate of the kernel density estimatorwith respect to ph is faster than with p. For such reasons, we typically target the level set Lh,t inducedfrom ph in lieu of that from p [see, e.g., 38, ]. When each (i) is known (or has it been observed), the level sets could be estimated by computingLh,t = { : ph() > t}. Specifically, for each t we let Wt = { : ph() > t}, and construct agraph Gt where each (i) Wt is a vertex and there is an edge between (i) and (j) if and only if(i) (j)2 h. Then the clusters at level t are estimated by taking the connected components ofthe graph Gt, which is referred to as a Rips graph. Persistent homology measures how the topologyof Rt varies by the value of t. See, for example, Bobrowski et al. , Fasy et al. , Kent et al. more information on the algorithm and its theoretical features. However, in our causal clustering framework, the oracle kernel density estimator ph is not computablesince we do not observe each (i). Thus we construct a plug-in version of the kernel density estimator:",
  "where for i = 1, 2, Si, := {y Rq : there exists x Si with x y2 }": "To estimate the target level set Lt,h = {ph > t} using the estimator Lt,h = {ph > t}, we normallyassume that the function difference ph ph is small. To apply this condition to the set differenceH(Lt,h, Lt,h), we have to ensure that the target level set Lt,h does not change drastically when thelevel t perturbs. We formalize this notion as follows.",
  "(c)": ": (a) Histogram of the true CATE in the test set. In the original study , individualswith zero treatment effects are assigned to the label L = 0. (b) The result of density-based causalclustering. Units in Cluster C1 appear to have higher baseline risk (0). (c) We observe that points inClusters C1 and C2 are more concentrated around the right upper area (larger 0, 1) and the lowerleft area (smaller 0, 1), respectively.",
  "Simulation Study": "Here, we explore finite-sample properties of our proposed plug-in procedures via simulation. Inparticular, we investigate the effect of nuisance estimation on the performance of causal clustering toempirically validate our theoretical findings in Sections 3 and 4. For hierarchical causal clustering, we use the simulation setup akin to that of Kim et al. . Lettingn = 2500, we randomly pick 10 points in a bounded hypercube 3: {c1, ..., c10}, and assignroughly n/10 points following truncated normal distribution to each Voronoi cell associated withcj; these are our {(i)}. Next, we let a = a + with N(0, n), which ensures thata a = OP(n). Following Balcan et al. , by repeating simulations 100 times, we computeclassification error as a proxy of the clustering performance using different values of parameter fixing the value of and . The results are presented in (a) & (b) with standard deviationerror bars. The simulation result supports our finding in Theorem 3.2, indicating that the price wepay for the proposed hierarchical causal clustering is inflated due to the nuisance estimation error. For density-based causal clustering, we utilize the toy example from Fasy et al. , originally usedto illustrate the cluster tree. We consider a mixture of three Gaussians in R2. Then, roughly n/3points for each of the three clusters are generated, which are our {(i)}. Similarly as before, welet a = a + with N(0, n). Next, letting h = 0.01, we compute ph and ph, and thecorresponding level sets Lh,t and Lh,t for different values of t. For each n, we calculate the meanHausdorff distance between Lh,t and Lh,t through 100 repeated simulations, and present the results : The estimated causal clusters on two principal-component hyperplanes with axes repre-senting the first and second, second and third principal components in the conditional counterfactualmean vector space, respectively. : The density plots of the pariwise CATE of six other education levels relative to the doctoraldegree across clusters. We observe a substantial degree of effect heterogeneity. The red dashedvertical lines denote the zero CATE.",
  "In this section, we illustrate our method through two case studies. We use semi-synthetic data on thevoting study and real-world data on employment projections": "Voting study. Nie and Wager considered a dataset on the voting study originally used byArceneaux et al. . They generated synthetic treatment effects to render discovery of heterogeneoustreatment effects more challenging. We use the same setup as Nie and Wager [46, Chapter 2], wherewe have binary treatments, binary outcomes, and 11 pretreatment covariates. While Nie and Wager specifically focused on accurate estimation of the CATE, here we aim to illustrate how theproposed method can be used to uncover an intriguing subgroup structure. We randomly chose atraining set of size 13000 and a test set of size 10000 from the entire sample. Then we estimate {(i)}using the cross-validation-based Super Learner ensemble to combine regression splines, supportvector machine regression, and random forests on the training sample, and perform the density-basedcausal clustering on the test sample using DeBaCl function in TDA R package . In -(b), we see two clusters in our conditional counterfactual mean vector space that areclearly separable from each other, one with nearly zero subgroup effect (Cluster C2) and the otherwith negative effect (Cluster C1). They correspond to the two largest branches at the bottom of thetree (-(c)). Roughly 4% of the points are classified as noise. Interestingly, units in ClusterC1 appear to have higher baseline risk 0 than Cluster C2. This is indeed more clearly noticeable in",
  "Employment projection data": "The dataset, obtained from the US Bureau of Labor Statistics (BLS), provides projected employmentby occupation. Specifically, the dataset consists of projected 10-year employment changes (2018-2028) computed from the BLS macroeconomic model across various occupations. We have eighteducation levels (No formal education, High school, Bachelors degree, etc.). Here, we aim to uncoversubgroup structure in the effects of entry-level education on projected employment. Our data alsoinclude four covariates: baseline employment in 2018, median annual wage in 2019, work experience,and on-the-job training. Again we randomly split the data into two independent sets and use the super learner ensemble toestimate the nuisance regression functions. We then find clusters using robust hierarchical causalclustering described in . Since we have multi-level treatments this time (q = 8), for easeof visualization, in we present the resulting clusters in two-dimensional hyperplanes withaxes representing the first and second, second and third principal components, respectively. We alsopresent the density plots for some of the pairwise CATEs across clusters in . In , we observe four distinct clusters which are quite clearly separable from each other onthe principal component hyperplanes. It appears that some clusters show considerably differenteffects from the others (e.g., Cluster 3), as shown in . Our findings indicate a substantialheterogeneity in the effects of entry-level education on projected employment.",
  "Discussion": "Causal clustering is a new approach for studying treatment effect heterogeneity that draws on clusteranalysis tools. In this work, we expanded upon this framework by integrating widely-used hierarchicaland density-based clustering algorithms, where we presented and analyzed the simple and readilyimplementable plug-in estimators. Importantly, as we do not impose any restrictions on the outcomeprocess, the proposed methods offer novel opportunities for clustering with generic unknown pseudooutcomes. There are some caveats and limitations which should be addressed. First, causal clustering plays amore descriptive and discovery-based than prescriptive role compared to other approaches. It enablesefficient discovery of subgroup structures and intriguing subgroup features as illustrated in our casestudies, yet will likely be less useful for informing specific treatment decisions. Understandingthis trade-off is thus important, and we recommend using our methods in conjunction with otherapproaches. Nonetheless, the clustering outputs could be potentially utilized as an useful inputfor subsequent learning tasks, such as precision medicine or optimal policy. Next, our theoreticalfindings show that when the nuisance regression functions {a} are modeled nonparametrically, theclustering performance essentially inherits from that of {a}. The convergence rate of a can bearbitrarily slow as the dimension of the covariate space increases. Kim et al. addressed thisissue by developing an efficient semiparametric estimator that achieves the second-order bias, andso can attain fast rates even in high-dimensional covariate settings . In future work, we aim todevelop more efficient semiparametric estimators for hierarchical and density-based causal clustering.Extension to other robust clustering methods, such as hierarchical density-based clustering ,would be a promising direction for future research as well. Lastly, our proposed methods are currentlylimited to the standard identification strategy under the no-unmeasured-confounding assumption,which is typically vulnerable to criticism [e.g., 28, Chapter 12]. To widen the breadth of the causalclustering framework, we will also be exploring extensions to other identification strategies, such asinstrumental variable, mediation, and proximal causal learning.",
  "Broader Impact": "The proposed method provides a general framework for causal clustering that is not specificallytailored to any particular application, thereby reducing the potential for unintended societal or ethicalimpacts. Nonetheless, it is important to note that the identified subgroup structure was constructedentirely based on treatment effect similarity, without accounting for fairness or bias.",
  "Acknowledgements": "This work was partially supported by a Korea University Grant (K2407471) and the NationalResearch Foundation of Korea (NRF) grant funded by the Korea governement (MSIT)(No. NRF-2022M3J6A1063595). This work was also partially supported by Institute of Information & commu-nications Technology Planning & Evaluation (IITP) grant funded by the Korea government(MSIT)[NO.RS-2021-II211343, Artificial Intelligence Graduate School Program (Seoul National Univer-sity)] and the New Faculty Startup Fund from Seoul National University. Part of this work wascompleted while Kwangho Kim was a Ph.D. student at Carnegie Mellon University.",
  "Shuai Chen, Lu Tian, Tianxi Cai, and Menggang Yu. A general statistical framework forsubgroup identification and comparative treatment scoring. Biometrics, 73(4):11991209, 2017": "Victor Chernozhukov, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, andWhitney K Newey. Double machine learning for treatment and causal parameters. Technicalreport, cemmap working paper, 2016. Victor Chernozhukov, Mert Demirer, Esther Duflo, and Ivan Fernandez-Val. Generic machinelearning inference on heterogeneous treatment effects in randomized experiments, with anapplication to immunization in india. Technical report, National Bureau of Economic Research,2018.",
  "Sanjoy Dasgupta and Philip M Long. Performance guarantees for hierarchical clustering.Journal of Computer and System Sciences, 70(4):555569, 2005": "Raaz Dwivedi, Yan Shuo Tan, Briton Park, Mian Wei, Kevin Horgan, David Madigan, and BinYu. Stable discovery of interpretable subgroups via calibration in causal studies. InternationalStatistical Review, 88:S135S178, 2020. Brian Eriksson, Gautam Dasarathy, Aarti Singh, and Rob Nowak. Active clustering: Robustand efficient hierarchical clustering using adaptively selected similarities. In Proceedings of theFourteenth International Conference on Artificial Intelligence and Statistics, pages 260268,2011.",
  "Alexander Hinneburg, Daniel A Keim, et al. An efficient approach to clustering in largemultimedia databases with noise. In KDD, volume 98, pages 5865, 1998": "Shoubo Hu, Zhitang Chen, Vahid Partovi Nia, Laiwan Chan, and Yanhui Geng. Causal inferenceand mechanism clustering of a mixture of additive noise models. Advances in neural informationprocessing systems, 31, 2018. Biwei Huang, Kun Zhang, Pengtao Xie, Mingming Gong, Eric P Xing, and Clark Glymour.Specific and shared causal relation modeling and mechanism-based clustering. Advances inNeural Information Processing Systems, 32, 2019.",
  "Brian P Kent, Alessandro Rinaldo, and Timothy Verstynen. Debacl: A python package forinteractive density-based clustering. arXiv preprint arXiv:1307.8136, 2013": "Jisu Kim, Jaehyeok Shin, Alessandro Rinaldo, and Larry Wasserman. Uniform convergencerate of the kernel density estimator adaptive to intrinsic volume dimension. In KamalikaChaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conferenceon Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 33983407, Long Beach, California, USA, 0915 Jun 2019. PMLR. URL",
  "Kwangho Kim, Jisu Kim, and Edward H Kennedy. Causal k-means clustering. arXiv preprintarXiv:2405.03083, 2024": "Hans-Peter Kriegel and Martin Pfeifle. Density-based clustering of uncertain data. In Proceed-ings of the eleventh ACM SIGKDD international conference on Knowledge discovery in datamining, pages 672677, 2005. Erich Kummerfeld and Joseph Ramsey. Causal clustering for 1-factor measurement models. InProceedings of the 22nd ACM SIGKDD international conference on knowledge discovery anddata mining, pages 16551664, 2016.",
  "Ilya Lipkovich, Alex Dmitrienko, and Ralph B DAgostino Sr. Tutorial in biostatistics: data-driven subgroup identification and analysis in clinical trials. Statistics in medicine, 36(1):136196, 2017": "Wei-Yin Loh, Luxi Cao, and Peigen Zhou. Subgroup identification for precision medicine:A comparative review of 13 methods. Wiley Interdisciplinary Reviews: Data Mining andKnowledge Discovery, 9(5):e1326, 2019. Alex Markham, Richeek Das, and Moritz Grosse-Wentrup. A distance covariance-based kernelfor nonlinear causal clustering in heterogeneous populations. In Conference on Causal Learningand Reasoning, pages 542558. PMLR, 2022.",
  "Xinkun Nie and Stefan Wager. Quasi-oracle estimation of heterogeneous treatment effects.Biometrika, 108(2):299319, 2021": "Thomas Ondra, Alex Dmitrienko, Tim Friede, Alexandra Graf, Frank Miller, Nigel Stallard,and Martin Posch. Methods for identification and confirmation of targeted subgroups in clinicaltrials: a systematic review. Journal of biopharmaceutical statistics, 26(1):99119, 2016. W Qi, Ameen Abu-Hanna, Thamar Eva Maria van Esch, Derek de Beurs, Yuntao Liu, Linda EFlinterman, and Martijn C Schut. Explaining heterogeneity of individual treatment causaleffects by subgroup discovery: An observational case study in antibiotics treatment of acuterhino-sinusitis. Artificial Intelligence in Medicine, 116:102080, 2021. Rosalba Radice, Roland Ramsahai, Richard Grieve, Noemi Kreif, Zia Sadique, and Jasjeet SSekhon. Evaluating treatment effectiveness in patient subgroups: a comparison of propensityscore methods with an automated matching approach. The international journal of biostatistics,8(1), 2012.",
  "Alessandro Rinaldo, Larry Wasserman, et al. Generalized density clustering. The Annals ofStatistics, 38(5):26782722, 2010": "Patrick M Schnell, Qi Tang, Walter W Offen, and Bradley P Carlin. A bayesian crediblesubgroups approach to identifying patient subgroups with positive treatment effects. Biometrics,72(4):10261036, 2016. Uri Shalit, Fredrik D Johansson, and David Sontag. Estimating individual treatment effect:generalization bounds and algorithms. In International conference on machine learning, pages30763085. PMLR, 2017.",
  "This section summarizes definitions and theoretical results in Balcan et al.": "We first define a clustering problem (U, l) as follows. Assume we have a data set U of N objects.Each point U has a true cluster label l() in {C1, . . . , Ck}. Further we let C() denote acluster corresponding to the label l(), and nC() denote the size of the cluster C(). The good-neighborhood property in Definition A.1 is a generalization of both the -strict separationand the -good neighborhood property in Balcan et al. . It roughly means that after a portionof points are removed, each point might allow some bad immediate neighbors but most of theimmediate neighbors are good. , can be viewed as noise parameters indicating the proportion ofdata susceptible to erroneous behavior. Definition A.1 (Property 3 in ). Suppose a clustering problem (U, l) with |U| = N, and a similarityfunction d : UU R. We say the similarity function d satisfies (, )-good neighborhood propertyfor the clustering problem (U, l), if there exists U U of size (1 )N so that for all points Uwe have that all but N out of their nC()U nearest neighbors in S belong to the cluster C(). In the inductive setting, Algorithm 2 in Balcan et al. uses a random sample over the data set andgenerates a hierarchy over this sample, and also implicitly represents a hierarchy over the entire dataset. When the data satisfies the good neighborhood properties, Algorithm 2 in achieves smallerror on the entire data set, requiring only a small sample size independent of that of the entire dataset, as in Theorem A.1.",
  "BProofs": "Throughout the development, we let P denote the conditional expectation given the sample operatorf, as in P( f) = f(z)dP(z). Notice that P( f) is random only if f depends on samples, in whichcase P( f) = E( f). Otherwise P and E can be used exchangeably. For example, if f is constructedon a separate (training) sample Dn = (Z1, ..., Zn), then Pf(Z)= Ef(Z) | Dnfor a newobservation Z P. Lastly, we let d2 : Rq Rq R be the Euclidean distance on Rq, i.e.,",
  "We rewrite Proposition 3.1 with detailed constants relation": "Proposition B.2. Let D denote the single, average, or complete linkage between sets of points,induced by the distance function such that d(x, y) C x y1 for some constant C > 0. Thenunder Assumption A1, for any two sets S1, S2 in {(i)} and their estimates S1, S2 with {(i)},D(S1, S2) D(S1, S2) 2C",
  "Proof of Theorem 3.2": "Proof. From Lemma B.6, the distance d2 satisfies (, )-good property for the clustering problem(UN,l). So there exists a subset U UN of size (1 )N such that for any point U all butN out of nC()U neighbors in U belongs to the cluster C(). For each U, let rU, be thedistance to the nC()U-th nearest neighbor of in U, i.e.,",
  "q ((B(w, r + s)\\B(w, r)) supp(p)) q ((B(w, r + s)\\B(w, r)) [2B, 2B]q)": "Now, we bound q1(B(w, t) [2B, 2B]q) for any t R. First, note that for any u 0, byconsidering that the map : B(w, t) [2B, 2B]q B(w, t + u) [2B u, 2B + u]q by(w + tv) = w + (t + u)v for unit vector v satisfies (x) (y) x y, we have",
  "Proof of Theorem 4.1": "First, we give the following new result on bounding the Hausdorff distance between sets in thecounterfactual function space.Theorem B.7. Suppose that Lh,t is stable and let H(, ) be the Hausdorff distance between twosets. Suppose that Assumptions A1, A2, A3, and A4 hold. Let (0, 1) and {hn}nN (0, h0) besatisfying",
  "which is RHS of the inequality in Lemma B.8": "Suppose that we are given a sufficiently large n so that phn phn < rn holds with probabilityat least 1 where rn < a for some constant a > 0. We aim to show two things: (a) for everyx Lhn,t there exists y Lhn,t with x y2 Crn, and (b) for every x Lhn,t there existsy Lhn,t with x y2 Crn."
}