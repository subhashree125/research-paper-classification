{
  "Abstract": "Model selection in Gaussian processes scales prohibitively with the size of thetraining dataset, both in time and memory. While many approximations exist,all incur inevitable approximation error. Recent work accounts for this error inthe form of computational uncertainty, which enablesat the cost of quadraticcomplexityan explicit tradeoff between computation and precision. Here weextend this development to model selection, which requires significant enhance-ments to the existing approach, including linear-time scaling in the size of thedataset. We propose a novel training loss for hyperparameter optimization anddemonstrate empirically that the resulting method can outperform SGPR, CGGPand SVGP, state-of-the-art methods for GP model selection, on medium to large-scale datasets. Our experiments show that model selection for computation-awareGPs trained on 1.8 million data points can be done within a few hours on a singleGPU. As a result of this work, Gaussian processes can be trained on large-scaledatasets without significantly compromising their ability to quantify uncertaintya fundamental prerequisite for optimal decision-making.",
  "Introduction": "Gaussian Processes (GPs) remain a popular probabilistic model class, despite the challenges in scal-ing them to large datasets. Since both computational and memory resources are limited in practice,approximations are necessary for both inference and model selection. Among the many approx-imation methods, perhaps the most common approach is to map the data to a lower-dimensionalrepresentation. The resulting posterior approximations typically have a functional form similar tothe exact GP posterior, except where posterior mean and covariance feature low-rank updates. Thisstrategy can be explicitby either defining feature functions (e.g. Nystrom , RFF )or alower-dimensional latent inducing point space (e.g. SoR, DTC, FITC , SGPR , SVGP ), orimplicitby using an iterative numerical method (e.g. CGGP ). All of these methods thencompute coefficients for this lower-dimensional representation from the full set of observations bydirect projection (e.g. CGGP) or via an optimization objective (e.g. SGPR, SVGP). While effective and widely used in practice, the inevitable approximation error adversely impactspredictions, uncertainty quantification, and ultimately downstream decision-making. Many pro-posed methods come with theoretical error bounds [e.g. 2, 1114], offering insights into the scalingand asymptotic properties of each method. However, theoretical bounds often require too many as-sumptions about the data-generating process to offer real-world guarantees , and in practice,the fidelity of the approximation is ultimately determined by the available computational resources.",
  "GP meanGP uncertainty (latent + noise)Training dataPosterior for Data-Generating Hyperparameters": ": Comparison of an exact GP posterior (CholeskyGP) and three scalable approximations:SVGP, CaGP-CG and CaGP-Opt (ours). Hyperparameters for each model were optimized usingmodel selection strategies specific to each approximation. The posterior predictive given the data-generating hyperparameters is denoted by gray lines and for each method the posterior (dark-shaded)and the posterior predictive are shown (light-shaded). While all methods, including the exact GP,do not recover the data-generating process, CaGP-CG and CaGP-Opt are much closer than SVGP.SVGP expresses almost no posterior variance near the inducing point in the data-sparse region andthus almost all deviation from the posterior mean is considered to be observational noise. In contrast,CaGP-CG and CaGP-Opt express significant posterior variance in regions with no data. One central pathology is overconfidence, which has been shown to be detrimental in key applica-tions of GPs such as Bayesian optimization [e.g. variance starvation of RFF, 16], and manifests itselfeven in state-of-the-art variational methods like SVGP. SVGP, because it treats inducing variablesas virtual observations, can be overconfident at the locations of the inducing points if they are notin close proximity to training data, which becomes increasingly likely in higher dimensions. Thisphenomenon can be seen in a toy example in , where SVGP has near zero posterior varianceat the inducing point away from the data. See also Section S5.1 for a more detailed analysis. These approximation errors are a central issue in inference, but they are exacerbated in model selec-tion, where errors compound and result in biased selections of hyperparameters . Con-tinuing the example, SVGP has been observed to overestimate the observation noise , which canlead to oversmoothing. This issue can also be seen in , where the SVGP model producesa smoother posterior mean than the exact (Cholesky)GP and attributes most variation from the pos-terior mean to observational noise (see also Figure S3(b)). There have been efforts to understandthese biases and to mitigate the impact of approximation error on model selection for certainapproximations [e.g. CGGP, 12], but overcoming these issues for SVGP remains a challenge. Recently, Wenger et al. introduced computation-aware Gaussian processes (CaGP), a class ofGP approximation methods whichfor a fixed set of hyperparametersprovably does not sufferfrom overconfidence. Like SVGP and the other approximations mentioned above, CaGP also re-lies on low-rank posterior updates. Unlike these other methods, however, CaGPs posterior updatesare constructed to guarantee that its posterior variance is always larger than the exact GP variance.This conservative estimate can be interpreted as additional uncertainty quantifying the approxima-tion error due to limited computation; i.e. computational uncertainty. However, so far CaGP hasfallen short in demonstrating wallclock time improvements for posterior inference over variationalmethods and model selection has so far remained an open problem. ContributionsIn this work, we extend computation-aware Gaussian processes by demonstratinghow to perform inference in linear time in the number of training data, while maintaining its the-oretical guarantees. Second, we propose a novel objective that allows model selection without asignificant bias that would arise from naively conducting model selection on the projected GP. Indetail, we enforce a sparsity constraint on the actions of the method, which unlocks linear-timeinference, in a way that is amenable to hardware acceleration. We optimize these actions end-to-endalongside the hyperparameters with respect to a custom training loss, to optimally retain as muchinformation from the data as possible given a limited computational budget. The resulting hyperpa-rameters are less prone to oversmoothing and attributing variation to observational noise, as can beseen in , when compared to SVGP. We demonstrate that our approach is strongly competi-tive on large-scale data with state-of-the-art variational methods, such as SVGP, without inheritingtheir pathologies. As a consequence of our work, one can train GPs on up to 1.8 million data pointsin a few hours on a single GPU without adversely impacting uncertainty quantification.",
  "We aim to learn a latent function mapping from X Rd to Y R given a training dataset X =(x1, . . . , xn) Rnd of n inputs xj Rd and corresponding targets y = (y1, . . . , yn)T Rn": "Gaussian ProcessesA Gaussian process f GP(, K) is a stochastic process with mean func-tion and kernel K such that f = f(X) = (f(x1), . . . , f(xn))T N(, K) is jointly Gaussianwith mean i = (xi) and covariance Kij = K(xi, xj). The kernel K depends on hyperparam-eters Rp, which we omit in our notation. Assuming y | f(X) Nf(X), 2I, the posterioris a Gaussian process GP(, K) where the mean and covariance functions evaluated at a test inputx Rd are given by",
  "and its gradient. Computing (2) and its gradient via a Cholesky decomposition has time complexityO(n3) and requires O(n2) memory, which is prohibitive for large n": "Sparse Gaussian Process Regression (SGPR) Given a set of m n inducing pointsZ = (z1, . . . , zm)T and defining u := f(Z) = (f(z1), . . . , f(zm))T, SGPR defines a varia-tional approximation to the posterior through the factorization p(f() | y) q(f()) =p(f() |u) q(u)du, where q(u) is an m-dimensional multivariate Gaussian. The mean and covariance ofq(u) (denoted as m := Eq(u), := Covq(u)) are jointly optimized alongside the kernel hyperpa-rameters using the evidence lower bound (ELBO) as an objective:",
  "ELBO(m, , , Z) :=NLL() + KL(q(f) p(f | y, ))= Eq(f)(log p(y | f)) + KL(q(u) p(u)) log p(y | ).(4)": "The inducing point locations Z can be either optimized as additional parameters during training orchosen a-priori, typically in a data-dependent way [see e.g. Sec. 7.2 of 20]. Following Titsias ,ELBO optimization and posterior inference both require O(nm2) computation and O(nm) memory,a significant improvement over the costs of exact GPs. Stochastic Variational Gaussian Processes (SVGP) SVGP extends SGPR to reduce com-plexity further to O(m3) computation and O(m2) memory. It accomplishes this reduction by re-placing the first term in Equation (4) with an unbiased approximation",
  "Eq(f)(log p(y | f)) = Eq(f)(ni=1 log p(yi | f(xi))) n Eq(f(xi))(log p(yi | f(xi)))": "Following Hensman et al. , we optimize m, alongside , Z through joint gradient updates.Because the asymptotic complexities no longer depend on n, SVGP can scale to extremely largedatasets that would not be able to fit into computer/GPU memory. Computation-aware Gaussian Process Inference (CaGP) CaGP1 maps the data y into alower dimensional subspace defined by its actions Si Rni on the data, which defines an approx-imate posterior GP(i, Ki) with",
  "i(x) = (x) + K(x, X)viKi(x, x) = K(x, x) K(x, X)CiK(X, x),(5)": "1Wenger et al. named their computation-aware inference algorithm IterGP, to emphasize its itera-tive nature (see Algorithm S1). We adopt the naming CaGP instead, since if the actions S are not chosensequentially, it is more efficient to compute the computation-aware posterior non-iteratively (see Algorithm S2). where Ci = Si(STi KSi)1STi K1 is a low-rank approximation of the precision matrix andvi = Ci(y ) v approximates the representer weights. Both converge to the correspondingexact quantity as the number of iterations, equivalently the downdate rank, i n. Note that theCaGP posterior only depends on the space spanned by the columns of Si, not the actual matrix(Lemma S4). Finally, the CaGP posterior can be computed in O(n2i) time and O(ni) memory. CaGP captures the approximation error incurred by limited computational resources as additionaluncertainty about the latent function. More precisely, for any data-generating function y HK inthe RKHS HK defined by the kernel, the worst-case squared error of the corresponding approxi-mate posterior mean yi is equal to the approximate predictive variance (see Theorem S2):",
  "supyHK ,yHK 1(y(x) yi (x))2 = Ki(x, x) + 2(6)": "This guarantee is identical to one for the exact GP posterior mean and variance (see Theorem S1),except with the approximate quantities instead.2 Additionally, it holds that CaGPs marginal (predic-tive) variance is always larger or equal to the (predictive) variance of the exact GP and monotonicallydecreasing, i.e. Ki(x, x) Kj(x, x) Kn(x, x) = K(x, x) for i j n (see Propo-sition S1). Therefore, given fixed hyperparameters, CaGP is guaranteed to never be overconfidentand as the computational budget increases, the precision of its estimate increases. Here we call sucha posterior computation-aware, and we will extend the use of this object to model selection.",
  "Model Selection in Computation-Aware Gaussian Processes": "Model selection for GPs most commonly entails maximizing the evidence log p(y | ) as a functionof the kernel hyperparameters Rp. As with posterior inference, evaluating this objective and itsgradient is computationally prohibitive in the large-scale setting. Therefore, our central goal will beto perform model selection for computation-aware Gaussian processes in order to scale to a largenumber of training data while avoiding the introduction of pathologies via approximation. We begin by viewing the computation-aware posterior as exact inference assuming we can onlyobserve i linear projections y of the data defined by (linearly independent) actions Si Rni, i.e.y := SiTy Ri,whereSi = Si chol(SiTSi)T Rni(7)is the action matrix with orthonormalized columns. The corresponding likelihood is given byp(y | f(X)) = Ny; SiTf(X), 2I.(8)As we show in Lemma S1, the resulting Bayesian posterior is then given by Equation (5). Recallthat the CaGP posterior only depends on the column space of the actions Si (see Lemma S4), whichis why Equation (5) can be written in terms of Si directly rather than using Si.3 Projected-Data Log Marginal LikelihoodThe reinterpretation of the computation-aware poste-rior as exact Bayesian inference immediately suggests using evidence maximization for the projecteddata y = SiTy Ri as the model selection criterion, leading to the following loss (see Lemma S2):",
  "(9)": "Equation (9) involves a Gaussian random variable of dimension i n, and so all previously in-tractable quantities in Equation (2) (i.e. the inverse and determinant) are now cheap to compute.Analogous to the CaGP posterior, we can express the projected-data log marginal likelihood fully interms of the actions Si without having to orthonormalize, which results in an additional term penal-izing colinearity. Unfortunately, this training loss does not lead to good generalization performance,as there is only training signal in the i-dimensional space spanned by the actions Si the data areprojected onto. Specifically, the quadratic loss term only promotes fitting the projected data y, notall observations y. See Figures S1 and S2 for experimental validation of this claim. 2At first glance, SVGP satisfies a similar result (Theorem S3). However, this statement does not express thevariance in terms of the worst-case squared error to the true latent function. See Section S1.2 for details.3Given this observation, we sometimes abuse terminology and refer to the actions as projecting the datato a lower-dimensional space, although Si does not need to have orthonormal columns. ELBO Training LossMotivated by this observation, we desire a tractable training loss that en-courages maximizing the evidence for the entire set of targets y. Importantly though, we need toaccomplish this evidence maximization without incurring prohibitive O(n3) computational cost. Wedefine a variational objective, using the computation-aware posterior qi(f | y, ) in Equation (5)to define a variational family Q :=qi(f) = N(f; i(X), Ki(X, X)) | S Rniparametrizedby the action matrix S. We can then specify a (negative) evidence lower bound (ELBO) as follows:",
  "regularizes s.t. qi p log p(y | ).(10)": "This loss promotes learning the same hyperparameters as if we were to maximize the computa-tionally intractable evidence log p(y | ) while minimizing the error due to posterior approximationqi(f) p(f | y, ). In the computation-aware setting, this translates to minimizing computationaluncertainty, which captures this inevitable error. Although both the evidence and KL terms of the ELBO involve computationally intractable terms,these problematic terms cancel out when combined. This results in an objective that costs the sameas evaluating CaGPs predictive distribution, i.e.",
  "+ vTi STKSvi tr((ST KS)1STKS) + log det(ST KS) log det(STS) (11)": "where vi = (ST KS)1ST(y). For a derivation of this expression of the loss see Lemma S3. Ifwe compare the training loss ELBOCaGP in Equation (10) with the projected-data NLL in Equation (9),there is an explicit squared loss penalty on the entire data y, rather than just for the projected data y,resulting in better generalization as Figures S1 and S2 show on synthetic data. In our experiments,this objective was critical to achieving state-of-the-art performance.",
  "So far we have not yet specified the actions S Rni mapping the data to a lower-dimensionalspace. Ideally, we would want to optimally compress the data both for inference and model selection": "Posterior Entropy MinimizationWe can interpret choosing actions S as a form of active learn-ing, where instead of just observing individual datapoints, we allow ourselves to observe linearcombinations of the data y. Taking an information-theoretic perspective , we would then aimto choose actions such that uncertainty about the latent function is minimized. In fact, we show inLemma S5 that given a prior f GP(, K) for the latent function and a budget of i actions S, theactions that minimize the entropy of the posterior at the training data(s1, . . . , si) = arg minSRni Hp(f(X)|STy)(f(X))(12) are the top-i eigenvectors s1, . . . , si of K in descending order of the eigenvalue magnitude (seealso Zhu et al. ). Unfortunately, computing these actions is just as prohibitive computationallyas computing the intractable GP posterior. (Conjugate) Gradient / Residual ActionsDue to the intractability of choosing actions to min-imize posterior entropy, we could try to do so approximately. The Lanczos algorithm is aniterative method to approximate the eigenvalues and eigenvectors of symmetric positive-definitematrices. Given an appropriate seed vector, the space spanned by its approximate eigenvectors isequivalent to the span of the gradients / residuals ri = (y) Kvi1 of the method of ConjugateGradients (CG) when used to iteratively compute an approximation vi v = K1(y )to the representer weights v. We show in Lemma S4 that the CaGP posterior only depends on thespan of its actions. Therefore choosing approximate eigenvectors computed via the Lanczos processas actions is equivalent to using CG residuals. This allows us to reinterpret CaGP-CG, as introducedby Wenger et al. , as approximately minimizing posterior entropy.4 See Section S3.1 for details.",
  "RandomCaGP-CGCaGP-Opt": ": Visualization of action vectors defining the data projection. We perform model selectionusing two CaGP variants, with CG and learned sparse actionsdenoted as CaGP-CG, and CaGP-Opton a toy 2-dimensional dataset. Left: For each xj {x1, . . . , xn}, we plot the magnitudeof the entries of the top-5 eigenvectors of K and of the first five action vectors. Yellow denoteslarger magnitudes; blue denotes smaller magnitudes. Right: We compare the span of the actions Sagainst the top-i eigenspace throughout training by measuring the Grassman distance between thetwo subspaces (see also Section S5.2). CaGP-CG actions are closer to the kernel eigenvectors thanthe CaGP-Opt actions, both of which are more closely aligned than randomly chosen actions. As illustrates, CG actions are similar to the top-i eigenspace all throughout hyperparameteroptimization. However, this choice of actions focuses exclusively on posterior inference and incursquadratic time complexity O(n2i). Learned Sparse ActionsSo far in our action choices we have entirely ignored model selectionand tried to choose optimal actions assuming fixed kernel hyperparameters. The second contributionof this work, aside from demonstrating how to perform model selection, is recognizing that the ac-tions should be informed by the outer optimization loop for the hyperparameters. We thus optimizethe actions alongside the hyperparameters end-to-end, meaning the training loss for model selectiondefines what data projections are informative. This way the actions are adaptive to the hyperpa-rameters without spending unnecessary budget on computing approximately optimal actions for thecurrent choice of hyperparameters. Specifically, the actions are chosen by optimizing ELBOCaGP as afunction of the hyperparameters and the actions S, s.t.",
  "(14)": "Naively this approach introduces an ni dimensional optimiza-tion problem, which in general is computationally prohibitive.To keep the computational cost low and to optimally leveragehardware acceleration via GPUs, we impose a sparse block struc-ture on the actions (see Eq. 14) where each block is a columnvector sj Rk1 with k = n/i entries such that the to-tal number of trainable action parameters, i.e. non-zero entriesnnz(S) = k i = n, equals the number of training data. Dueto the sparsity, these actions cannot perfectly match the maxi-mum eigenvector actions. Nevertheless, we see in that optimizing these sparse actions inconjunction with hyperparameter optimization produces a nontrivial alignment with optimal actionchoice minimizing posterior entropy. Importantly, the sparsity constraint not only reduces the di-mensionality of the optimization problem, but crucially also reduces the time complexity of posteriorinference and model selection to linear in the number of training data points.",
  "We give algorithms both for iteratively constructed dense actions (Algorithm S1), as used in CaGP-CG, and for sparse batch actions (Algorithm S2), as used for CaGP-Opt, in the supplementary": "material.5 The time complexity is O(n2i) for dense actions and O(ni max(i, k)) for sparse actions,where k is the maximum number of non-zero entries per column of Si. Both have the same linearmemory requirement: O(ni). Since the training loss ELBOCaGP only involves terms that are also presentin the posterior predictive, both model selection and predictions incur the same complexity.",
  "Related Work": "Computational Uncertainty and Probabilistic NumericsAll CaGP variants discussed in thispaper fall into the category of probabilistic numerical methods , which aim to quantify ap-proximation error arising from limited computational resources via additional uncertainty about thequantity of interest [e.g. 2831]. Specifically, the iterative formulation of CaGP (i.e. Algorithm S1)originally proposed by Wenger et al. employs a probabilistic linear solver . Scalable GPs with Lower-Bounded Log Marginal LikelihoodsNumerous scalable GP approxi-mations beyond those in Sections 1 and 2 exist; see Liu et al. for a comprehensive review. ManyGP models [e.g., 4, 5, 3739] learn hyperparameters through maximizing variational lower boundsin the same spirit as SGPR, SVGP and our method. Similar to our work, interdomain inducing pointmethods learn a variational posterior approximation on a small set of linear functionals ap-plied to the latent GP. However, unlike our method, their resulting approximate posterior is usuallyprone to underestimating uncertainty in the same manner as SGPR and SVGP. Finally, similar to ourproposed training loss for CaGP-CG, Artemev et al. demonstrate how one can use the methodof conjugate gradients to obtain a tighter lower bound on the log marginal likelihood. GP Approximation Biases and Computational UncertaintyScalable GP methods inevitablyintroduce approximation error and thus yield biased hyperparameters and predictive distributions,with an exception of Potapczynski et al. which trade bias for increased variance. Numerousworks have studied pathologies associated with optimizing variational lower bounds, especially inthe context of SVGP , and various remedies have been proposed. In order to mitigatebiases from approximation, several works alternatively propose replacing variational lower boundswith alternative model selection objectives, including leave-one-out cross-validation and lossesthat directly target predictive performance .",
  "Experiments": "We benchmark the generalization of computation-aware GPs with two different action choices,CaGP-Opt (ours) and CaGP-CG , using our proposed training objective in Equation (10) ona range of UCI datasets for regression . We compare against SVGP , often considered to bestate-of-the-art for large-scale GP regression. Per recommendations by Ober et al. , we also in-clude SGPR as a strong baseline for all datasets where this is computationally feasible. We alsotrain Conjugate Gradient-based GPs (CGGP) [e.g. 7, 9, 10] using the training procedure proposedby Wenger et al. . Note that CaGP-CG recovers CGGP in its posterior mean and produces nearlyidentical predictive error at half the computational cost for inference [Sec. 2.1 & 4 of 19], which iswhy the main difference between CaGP-CG and CGGP in our experiments is the training objective.Finally, we also train an exact CholeskyGP on the smallest datasets, where this is still feasible. Experimental DetailsAll datasets were randomly partitioned into train and test sets using a(0.9, 0.1) split for five random seeds. We used a zero-mean GP prior and a Matern(3/2) kernelwith an outputscale o2 and one lengthscale per input dimension l2j, as well as a scalar observationnoise for the likelihood 2, s.t. = (o, l1, . . . , ld, ) Rd+2. We used the existing implementa-tions of SGPR, SVGP and CGGP in GPyTorch and also implemented CaGP in this framework(see Section S4.2 for our open-source implementation). For SGPR and SVGP we used m = 1024inducing points and for CGGP, CaGP-CG and CaGP-Opt we chose i = 512. We optimized the hy-perparameters either with Adam for a maximum of 1000 epochs in float32 or with LBFGS for 100 epochs in float64, depending on the method and problem scale. On the largest datasetPower, we used 400 epochs for SVGP and 200 for CaGP-Opt due to resource constraints. ForSVGP we used a batch size of 1024 throughout. We scheduled the learning rate via PyTorchs LinearLR(end factor=0.1) scheduler for all methods and performed a hyperparameter sweep",
  "For a detailed analysis see Algorithms S1 and S2 in the supplementary material, which contain line-by-linetime complexity and memory analyses": ": Generalization error (NLL, RMSE, and wall-clock time) on UCI benchmark datasets. Thetable shows the best results for all methods across learning rate sweeps, averaged across five randomseeds. We report the epoch where each method obtained the lowest average test NLL, and all per-formance metrics (NLL, RMSE, and wall-clock runtime) are reported for this epoch. Highlighted inbold and color are the best approximate methods per metric (difference > 1 standard deviation).",
  "CaGP-OptAdam0.100200-2.1030.0060.0300.0004h 32min 48s": "for the (initial) learning rate. All experiments were run on an NVIDIA Tesla V100-PCIE-32GBGPU, except for Power, where we used an NVIDIA A100 80GB PCIe GPU to have sufficientmemory for CaGP-Opt with i = 512. Our exact experiment configuration can be found in Table S1. Evaluation MetricsWe evaluate the generalization performance once per epoch on the test set bycomputing the (average) negative log-likelihood (NLL) and the root mean squared error (RMSE),as well as recording the wallclock runtime. Runtime is measured at the epoch with the best averageperformance across random seeds. CaGP-Opt Matches or Outperforms SVGP and show generalization perfor-mance of all methods for the best choice of learning rate. In terms of both NLL and RMSE, CaGP-Opt outperforms or matches the variational baselines SGPR and SVGP at comparable runtime (ex-cept on Bike). SGPR remains scompetitive for smaller datasets; however, it does not scale to thelargest datasets. There are some datasets and metrics in which specific methods dominate, for ex-ample on Bike SGPR outperforms all other approximations, while on Protein methods based onCG, i.e. CGGP and CaGP-CG, perform the best. However, CaGP-Opt consistently performs eitherbest or second best and scales to over a million datapoints. These results are quite remarkable for nu-merous reasons. First, CaGP is comparable in runtime to SVGP on large datasets despite the fact thatit incurs a linear-time computational complexity while SVGP is constant time.6 Second, while allof the methods we compare approximate the GP posterior with low-rank updates, CaGP-Opt (with",
  "CholeskyGPSGPRSVGPCGGPCaGP-CGCaGP-Opt": ": Learning curves of GP approximation methods on UCI benchmark datasets. Rows showtrain and test loss as a function of wall-clock time for the best choice of learning rate per method.CaGP-Opt generally displays a ramp-up phase early in training where performance is worse thanthat of SVGP. As training progresses, CaGP-Opt matches or surpasses SVGP performance. i = 512) here uses half the rank of SGPR/SVGP m = 1024. Nevertheless, CaGP-Opt is able to sub-stantially outperform SVGP even on spatial datasets like 3DRoad where low-rank posterior updatesare often poor . These results suggest that CaGP-Opt can be a more efficient approximationthan inducing point methods, and that low-rank GP approximations may be more applicable thanpreviously thought . shows the NLL and RMSE learning curves for the best choiceof learning rate per method. CaGP-Opt often shows a ramp-up phase, compared to SVGP, butthen improves or matches its generalization performance. This gap is particularly large on Road,where CaGP-Opt is initially worse than SVGP but dominates in the second half of training.",
  "SVGPCaGP-Opt": ":Uncertainty quantification forCaGP-Opt and SVGP. Difference between thedesired coverage (95%) and the empirical cov-erage of the GP 95% credible interval on thePower dataset. After training, CaGP-Opt hasbetter empirical coverage than SVGP. SVGP Overestimates Observation Noise and(Often) LengthscaleIn Figure S5 we show thatSVGP typically learns larger observation noisethan other methods as suggested by previous work and hinted at by observations on syn-thetic data in and Figure S3(b). Addi-tionally on larger datasets SVGP also often learnslarge lengthscales, which in combination with alarge observation noise can lead to an oversmooth-ing effect. In contrast, CaGP-Opt generally learnslower observational noise than SVGP. Of course,learning a small observation noise, in particular, isimportant for achieving low RMSE and thus alsoNLL, and points to why we should expect CaGP-Opt to outperform SVGP. These hyperparameterresults suggest that CaGP-Opt interprets more ofthe data as signal, while SVGP interprets more ofthe data as noise. CaGP Improves Uncertainty Quantification Over SVGPA key advantage of CaGP-Opt andCaGP-CG is that their posterior uncertainty estimates capture both the uncertainty due to limiteddata and due to limited computation. To that end, we assess the frequentist coverage of CaGP-Optsuncertainty estimates. We report the absolute difference between a desired coverage percentage",
  "and the fraction of data that fall into the credible interval of the CaGP-Opt posterior; i.e. coverage =| 1": "ntestntesti=1 1(yi Iq(xi))|. compares the 95% coverage error for both CaGP-Optand SVGP on the largest dataset (Power). From this plot, we see that the CaGP credible intervalsare more aligned with the desired coverage. We hypothesize that these results reflect the differentuncertainty properties of the methods: CaGP-Opt overestimates posterior uncertainty while SVGPis prone towards overconfidence.",
  "Conclusion": "In this work, we introduced strategies for model selection and posterior inference for computation-aware Gaussian processes, which scale linearly with the number of training data rather than quadrat-ically. The key technical innovations being a sparse projection of the data, which balances mini-mizing posterior entropy and computational cost, and a scalable way to optimize kernel hyperpa-rameters, both of which are amenable to GPU acceleration. All together, these advances enablecompetitive or improved performance over previous approximate inference methods on large-scaledatasets, in terms of generalization and uncertainty quantification. Remarkably, our method outper-forms SVGPoften considered the de-facto GP approximation standard even when compressingthe data into a space of half the dimension of the variational parameters. Finally, we also demon-strate that computation-aware GPs avoid many of the pathologies often observed in inducing pointmethods, such as overconfidence and oversmoothing. LimitationsWhile CaGP-Opt obtains the same linear time and memory costs as SGPR, it is notamenable to stochastic minibatching and thus cannot achieve the constant time/memory capabilitiesof SVGP. In practice, this asymptotic difference does not result in substantially different wall clocktimes, as SVGP requires many more optimizer steps than CaGP-Opt due to batching. (Indeed,on many datasets we find that CaGP-Opt is faster.) CaGP-Opt nevertheless requires larger GPUsthan SVGP on datasets with more than a million data points. Moreover, tuning CaGP-Opt requireschoosing the appropriate number of actions (i.e. the rank of the approximate posterior update),though we note that most scalable GP approximations have a similar tunable parameter (e.g. numberof inducing points). Perhaps the most obvious limitation is that CaGP, unlike SVGP, is limited toGP regression with a conjugate observational noise model. We leave extensions to classification andother non-conjugate likelihoods as future work. Outlook and Future WorkAn immediate consequence of this work is the ability to applycomputation-aware Gaussian processes to real-world problems, as our approach solves CaGPsopen problems of model selection and scalability. Looking forward, an exciting future vision is ageneral framework for problems involving a Gaussian process model with a downstream task wherethe actions are chosen optimally, given resource constraints, to solve said task. Future work willpursue this direction beyond Gaussian likelihoods to non-conjugate models and downstream taskssuch as Bayesian optimization.",
  "and Disclosure of Funding": "JW and JPC are supported by the Gatsby Charitable Foundation (GAT3708), the Simons Foundation(542963), the NSF AI Institute for Artificial and Natural Intelligence (ARNI: NSF DBI 2229929)and the Kavli Foundation. JG and KW are supported by the NSF (IIS-2145644, DBI-2400135). PHgratefully acknowledges co-funding by the European Union (ERC, ANUBIS, 101123955). Viewsand opinions expressed are however those of the author(s) only and do not necessarily reflect those ofthe European Union or the European Research Council. Neither the European Union nor the grantingauthority can be held responsible for them. PH is a member of the Machine Learning Cluster ofExcellence, funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation)under Germanys Excellence Strategy EXC number 2064/1 Project number 390727645; he alsogratefully acknowledges the German Federal Ministry of Education and Research (BMBF) throughthe Tubingen AI Center (FKZ: 01IS18039A); and funds from the Ministry of Science, Research andArts of the State of Baden-Wurttemberg. GP acknowledges support from NSERC and the CanadaCIFAR AI Chair program.",
  "M. Gibbs. Bayesian Gaussian processes for classification and regression. PhD thesis. 1997(cit. on p. 1)": "J. R. Gardner, G. Pleiss, D. Bindel, K. Q. Weinberger, and A. G. Wilson. GPyTorch: Black-box matrix-matrix Gaussian process inference with GPU acceleration. In: Advances in Neu-ral Information Processing Systems (NeurIPS). 2018, pp. 75767586 (cit. on pp. 1, 5, 7). G. Pleiss, J. Gardner, K. Weinberger, and A. G. Wilson. Constant-time predictive distribu-tions for Gaussian processes. In: International Conference on Machine Learning (ICML).2018, pp. 41144123 (cit. on p. 1). K. A. Wang, G. Pleiss, J. R. Gardner, S. Tyree, K. Q. Weinberger, and A. G. Wilson. ExactGaussian processes on a million data points. In: Advances in Neural Information ProcessingSystems (NeurIPS) 32 (2019) (cit. on pp. 1, 5, 7). J. Wenger, G. Pleiss, P. Hennig, J. P. Cunningham, and J. R. Gardner. Preconditioning forScalable Gaussian Process Hyperparameter Optimization. In: International Conference onMachine Learning (ICML). 2022 (cit. on pp. 1, 5, 7).",
  "F. Bach. On the equivalence between kernel quadrature rules and random feature expan-sions. In: Journal of Machine Learning Research 18.21 (2017), pp. 138 (cit. on p. 1)": "A. Potapczynski, L. Wu, D. Biderman, G. Pleiss, and J. P. Cunningham. Bias-Free ScalableGaussian Processes via Randomized Truncations. In: International Conference on MachineLearning (ICML). 2021 (cit. on pp. 1, 2, 7). D. R. Burt, C. E. Rasmussen, and M. van der Wilk. Rates of Convergence for Sparse Vari-ational Gaussian Process Regression. In: International Conference on Machine Learning(ICML). 2019. URL: (cit. on p. 1).",
  "M. Kang, F. Schafer, J. Guinness, and M. Katzfuss. Asymptotic properties of Vecchia ap-proximation for Gaussian processes. 2024. DOI: 10.48550/arXiv.2401.15813 (cit. onp. 1)": "S. W. Ober, D. R. Burt, A. Artemev, and M. van der Wilk. Recommendations for Baselinesand Benchmarking Approximate Gaussian Processes. In: NeurIPS Workshop on GaussianProcesses, Spatiotemporal Modeling, and Decision-making Systems. 2022. URL: (cit. on pp. 1, 7). Z. Wang, C. Gehring, P. Kohli, and S. Jegelka. Batched Large-scale Bayesian Optimiza-tion in High-dimensional Spaces. In: International Conference on Artificial Intelligence andStatistics (AISTATS). 2018. DOI: 10.48550/arXiv.1706.01445. URL: (cit. on pp. 2, 7). R. E. Turner and M. Sahani. Two problems with variational expectation maximisation fortime series models. In: Bayesian Time Series Models. Cambridge University Press, 2011,pp. 104124. DOI: 10.1017/CBO9780511984679.006 (cit. on pp. 2, 7). M. Bauer, M. van der Wilk, and C. E. Rasmussen. Understanding probabilistic sparseGaussian process approximations. In: Advances in Neural Information Processing Systems(NeurIPS). Vol. 29. 2016 (cit. on pp. 2, 7, 9). J. Wenger, G. Pleiss, M. Pfortner, P. Hennig, and J. P. Cunningham. Posterior and Compu-tational Uncertainty in Gaussian processes. In: Advances in Neural Information ProcessingSystems (NeurIPS). 2022 (cit. on pp. 2, 3, 5, 7, 16, 17, 23). D. R. Burt, C. E. Rasmussen, and M. v. d. Wilk. Convergence of Sparse Variational Inferencein Gaussian Processes Regression. In: Journal of Machine Learning Research (Aug. 2020).DOI: 10.48550/arXiv.2008.00323 (cit. on p. 3).",
  "J. Wenger and P. Hennig. Probabilistic Linear Solvers for Machine Learning. In: Advancesin Neural Information Processing Systems (NeurIPS). 2020. DOI: 10.48550/arXiv.2010.09691. URL: (cit. on p. 7)": "H. Liu, Y.-S. Ong, X. Shen, and J. Cai. When Gaussian process meets big data: A reviewof scalable GPs. In: Transactions on Neural Networks and Learning Systems 31.11 (2020),pp. 44054423 (cit. on p. 7). J. Hensman, A. Matthews, and Z. Ghahramani. Scalable variational Gaussian process clas-sification. In: International Conference on Artificial Intelligence and Statistics (AISTATS).PMLR, 2015, pp. 351360 (cit. on p. 7). H. Salimbeni, C.-A. Cheng, B. Boots, and M. Deisenroth. Orthogonally Decoupled Vari-ational Gaussian Processes. In: Advances in Neural Information Processing Systems(NeurIPS). Vol. 31. 2018 (cit. on p. 7).",
  "J. Nocedal. Updating quasi-Newton matrices with limited storage. In: Mathematics of Com-putation 35.151 (1980), pp. 773782 (cit. on p. 7)": "A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N.Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Te-jani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala. PyTorch: An Impera-tive Style, High-Performance Deep Learning Library. In: Advances in Neural InformationProcessing Systems (NeurIPS). 2019. DOI: 10.48550/arXiv.1912.01703. URL: (cit. on p. 8). G. Pleiss, M. Jankowiak, D. Eriksson, A. Damle, and J. Gardner. Fast matrix square rootswith applications to Gaussian processes and Bayesian optimization. In: Advances in NeuralInformation Processing Systems (NeurIPS). 2020, pp. 2226822281 (cit. on p. 9). A. Datta, S. Banerjee, A. O. Finley, and A. E. Gelfand. Hierarchical nearest-neighbor Gaus-sian process models for large geostatistical datasets. In: Journal of the American StatisticalAssociation 111.514 (2016), pp. 800812 (cit. on p. 9).",
  "S1.1Alternative Derivation of CaGP Posterior": "Lemma S1 (CaGP Inference as Exact Inference Given a Modified Observation Model)Given a Gaussian process prior f GP(, K) and training data (X, y) the computation-awareGP posterior GP(i, Ki) (see Equation (5)) with linearly independent and fixed actions S is equiv-alent to an exact batch GP posterior (f | y) given data y = STy observed according to thelikelihood y | f(X) NSTf(X), 2I, where S = S chol(STS)T.",
  "S1.2Worst Case Error Interpretations of the Variance of Exact GPs, CaGPs and SVGPs": "In order to understand the impact of approximation on the uncertainty quantification of both CaGPand SVGP, it is instructive to compare the theoretical guarantees they admit, when the latent functionis assumed to be in the RKHS of the kernel. In the context of model selection, this corresponds tothe ideal case where the optimization has converged to the ground truth hyperparameters. Let f GP(0, K) be a Gaussian process with kernel K and define the observed process y() =f() + () where GP(0, ) is white noise with noise level 2 > 0, i.e. (x, x) = 1(x = x).Consequently, the covariance kernel of the data-generating process y() is given by K(x, x) :=K(x, x) + 2(x, x) and we denote the corresponding RKHS as HK. For exact Gaussian process inference, the pointwise (relative) worst-case squared error of the poste-rior mean is precisely given by the posterior predictive variance.Theorem S1 (Worst Case Error Interpretation of GP Variance )Given a set of training inputs x1, . . . , xn X, the GP posterior GP(, K) satisfies for anyx = xj thatsupyHK ,yHK 1(y(x) y(x))2",
  "Proof. See Proposition 3.8 of Kanagawa et al.": "CaGP admits precisely the same guarantee just with the approximate posterior mean and covariancefunction. The fact that the impact of the approximation on the posterior mean is exactly capturedby the approximate predictive variance function is what is meant by the method being computation-aware.Theorem S2 (Worst Case Error Interpretation of CaGP Variance )Given a set of training inputs x1, . . . , xn X, the CaGP posterior GP(i, Ki) satisfies for anyx = xj that",
  "Proof. See Theorem 2 of Wenger et al.": "While SVGP also admits a decomposition of its approximate predictive variance into two (relative)worst-case errors, neither of these is the error we care about, namely the difference between thedata-generating function y HK and the approximate posterior mean SVGP(x). It only involvesa worst-case error term over the unit ball in the RKHS of the approximate kernel Q K.Theorem S3 (Worst Case Error Interpretation of SVGP Variance )Given a set of training inputs x1, . . . , xn X and (fixed) inducing points Z Rmd, the optimalvariational posterior GP(SVGP, KSVGP) of SVGP is given by",
  "S1.3CaGPs Variance Decreases Monotonically as the Number of Iterations Increases": "Proposition S1 (CaGPs Variance Decreases Monotonically with the Number of Iterations)Given a training dataset of size n, let GP(i, Ki) be the corresponding CaGP posterior defined inEquation (5) where i n denotes the downdate rank / number of iterations and assume the CaGPactions Si Rni are linearly independent. Then it holds for arbitrary x X and i j n,thatKi(x, x) Kj(x, x) Kn(x, x) = K(x, x)(S18)where K(x, x) is the variance of the exact GP posterior in Equation (1). Proof. Wenger et al. originally defined the approximate precision matrix Ci=i=11 ddT = i=1 d dT as a sum of rank-1 matrices and show that this definition is equiv-alent to the batch form Ci = Si(STi KSi)1STi we use in this work [see Lemma S1, Eqn. (S37) in",
  "S3Choice of Actions": "We begin by proving that the CaGP posterior in Equation (5) is uniquely defined by the spacespanned by the columns of the actions colsp(S), rather than the specific choice of the matrix S.Lemma S4 (The CaGP Posterior Is Uniquely Defined by the Column Space of the Actions)Let S, S Rni be two action matrices, each of which consists of non-zero and linearly indepen-dent action vectors, such that their column spaces are identical, i.e.",
  "then the corresponding CaGP posteriors GP(i, Ki) and GP(i, Ki) are equivalent": "Proof. By assumption (S20) there exists W Rii such that S = SW . Since action vectorsare assumed to be linearly independent and non-zero, it holds that i = rank(S) = rank(SW ) =rank(W ), where the last equality follows from standard properties of the matrix rank. ThereforeW is invertible, and we have that",
  "Lanczos process The Lanczos process is an iterative method, which computes approximateeigenvalues = diag(1, . . . , i) Rii and approximate eigenvectors U = (u1 ui) Rni": "for a symmetric positive definite matrix K by repeated matrix-vector multiplication. Given anarbitrary starting vector q1 Rn, s.t. q12 = 1, it returns i orthonormal vectors Q = (q1 qi) Rni and a tridiagonal matrix T = QT KQ Rii. The eigenvalue approximations are givenby an eigendecomposition of T = W W T, where W Rii orthonormal, and the eigenvectorapproximations are then given by U = QW Rni [e.g. Sec. 10.1.4 of 62]. Conjugate Gradient Method The conjugate gradient method is an iterative method to solvelinear systems with symmetric positive definite system matrix by repeated matrix-vector multiplica-tion. When applied to Equation (S21), it produces a sequence of representer weights approximationsvi v = K1(y ). Its residuals ri = y Kvi are proportional to the Lanczos vectorsfor a Lanczos process initialized at q1 =r0",
  "S3.2Information-theoretic Policy": "In information-theoretic formulations of active learning, new data is selected to minimize uncertaintyabout a set of latent variables z. In other words, we would aim to minimize the entropy of theposterior Hp(z|X)(z) = log p(z | X)p(z | X) dz as a function of the data X . In analogyto active learning, in our setting we propose to perform computations y STi y to maximallyreduce uncertainty about the latent function f(X) evaluated at the training data.Lemma S5 (Information-theoretic Policy)The actions S minimizing the entropy of the computation-aware posterior p(f(X) | STy) at thetraining data, or equivalently the actions maximizing the mutual information between f(X) and theprojected data STy, are given by(s1, . . . , si) = arg minSRni Hp(f(X)|STy)(f(X))(S22)",
  "S5.1Inducing Points Placement and Uncertainty Quantification of SVGP": "To better understand whether the overconfidence of SVGP at inducing points observed in the visu-alization in holds also in higher dimensions, we do the following experiment. For varyinginput dimension d {1, 2, . . . , 25}, we generate synthetic training data by sampling n = 500inputs X uniformly at random with corresponding targets sampled from a zero-mean Gaussian pro-cess y GP(0, K), where K(, ) = K(, ) + 2(, ) is given by the sum of a Matern(3/2)and a white noise kernel with noise scale . We optimize the kernel hyperparameters, variationalparameters and inducing points (m = 64) jointly for 300 epochs using Adam with a linear learningrate scheduler. At convergence we measure the average distance between inducing points and thenearest datapoint measured in lengthscale units, i.e.",
  "Kposterior(zi, zi) + 2 .(S26)": "The results of our experiments are shown in Figure S3. We find that as expected the inducing pointsare optimized to lie closer to datapoints than points sampled uniformly at random. However, theinducing points lie increasingly far away from the training data as the dimension increases relative tothe lengthscale that SVGP learns. Therefore this experiment suggests that the phenomenon observedin , that SVGP can be overconfident at inducing points if they are far away from trainingdatapoints, to be increasingly present as the input dimension increases. This is further substantiatedby Figure S3(b) since the proportion of posterior variance to predictive variance at the inducingpoints is very small already in d = 4 dimensions. This illustrates both SVGPs overconfidence atthe inducing points (in particular in higher dimensions) and that its predictive variance is dominatedby the learned observation noise, as we also saw in the illustrative .",
  "(b) Average ratio of posterior to predictive variance atSVGPs inducing point locations": "Figure S3: SVGPs inducing point placement and uncertainty in higher dimensions. (a) As thedimension increases, the inducing points SVGP learns lie increasingly far away from the data mea-sured in lengthscale units given a fixed training data set size and number of inducing points. (b)SVGPs variance at the inducing points is dominated by the learned observational noise in higherdimensions, rather than by the posterior variance. The comparison to a CholeskyGP with the data-generating hyperparameters shows that SVGP compensates for a lack of posterior variance at the in-ducing points by artificially inflating the observation noise. This illustrates both the overconfidence(in terms of posterior variance) of SVGP at the inducing points and its tendency to oversmooth.",
  "S5.2Grassman Distance Between Subspaces": "In we compute the distance between the subspaces spanned by random vectors, the actionsS of CaGP, and the space spanned by the top-i eigenvectors. The notion of subspace distance weuse is the Grassman distance, i.e. for two subspaces spanned by the columns of matrices A Rnpand B Rnp s.t. p q the Grassman subspace distance is defined by",
  "S5.3.1Impact of Learning Rate on Generalization": "To show the impact of different choices of learning rate on the GP approximations we consider, weshow the test metrics for the learning rate sweeps in our main experiment in Figure S4. Note thatnot all choices of learning rate appear since a small minority of runs fail outright, for example if thelearning rate is too large."
}