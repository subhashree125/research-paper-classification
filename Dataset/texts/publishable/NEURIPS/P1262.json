{
  "Abstract": "The Benjamini-Hochberg (BH) procedure is widely used to control the false de-tection rate (FDR) in multiple testing. Applications of this control abound indrug discovery, forensics, anomaly detection, and, in particular, machine learning,ranging from nonparametric outlier detection to out-of-distribution detection andone-class classification methods. Considering this control could be relied upon incritical safety/security contexts, we investigate its adversarial robustness. Moreprecisely, we study under what conditions BH does and does not exhibit adversarialrobustness, we present a class of simple and easily implementable adversarialtest-perturbation algorithms, and we perform computational experiments. With ouralgorithms, we demonstrate that there are conditions under which BHs control canbe significantly broken with relatively few (even just one) test score perturbation(s),and provide non-asymptotic guarantees on the expected adversarial-adjustmentto FDR. Our technical analysis involves a combinatorial reframing of the BHprocedure as a balls into bins process, and drawing a connection to general-ized ballot problems to facilitate an information-theoretic approach for derivingnon-asymptotic lower bounds.",
  "Introduction": "Multiple testing has broad applications in drug discovery, forensics, candidate screening, anomalydetection, and in particular, machine learning. Indeed, recent works , in nonparametricoutlier detection, out-of-distribution detection (OOD), and one-class classification have all adoptedmultiple testing methodology in developing principled decision rules with statistical guarantees.In fact, the Benjamini-Hochberg (BH) multiple testing procedure, widely used to control the falsedetection rate (FDR), is either used or modified in all these recent methods. Considering this FDRcontrol could be relied upon in some critical (safety/security) contexts, for which false positives incurcosts, we investigate its adversarial robustness. Adversarial corruption presents a challenge to statistical methodology, and is a modern-day concerndue to not only the ease with which high volumes of data can now be accessed/processed butalso the increasingly widespread use of statistical procedures. This threat poses vulnerabilities tomachine learning tasks like OOD, which would aim to fortify security systems like fraud detection .",
  "arXiv:2501.03402v1 [math.ST] 6 Jan 2025": "Manipulation of data and experimental results are common means by which incorrect conclusionscan be reached. Worse, strategic perturbation can dramatically decrease the fidelity of the modelsand methods used. A burgeoning field of adversarial corruption has gained traction in recent years tomeet this concern, most notably in the area of (deep) machine learning; see, for example, .In this work we address adversarial corruption in hypothesis testing, specifically in the large-scalecontext in which the primary focus is on the aggregate metric: FDR. BH is one of the most widely used multiple testing procedures, which upon input of a collectionof p-values, outputs a rejection region ensuring that the FDR is no greater than a user-definedthreshold q (0,1). This control of FDR holds under independently generated p-values as wellas some restricted forms of dependency like positive regression dependent on a subset (PRDS) but it generally holds without strong assumptions on the alternative distributions. This degree ofdistributional robustness, however, could be said to come at the cost of adversarial robustness, as weshow in this work.",
  "Literature Review": "Although OOD methods are often complex and not always supported by statisticalguarantees, conformal inference has made possible the use of one-class classifiers to generateconformal p-values for which OOD can now be conducted via multiple testing. This has led tothe adoption of the BH procedure in OOD. Indeed, leverages the FDR control afforded by BHover conformal p-values (shown to be PRDS) to test for outliers. More precisely, given a test set ofobservations for which we wish to identify as inliers or outliers (out of distribution), a conformalp-value is generated for each observation, which is then processed by BH to decide which are likelyoutliers. We refer the reader to for other recent works along this vein. In recent years, concerns have risen over the possibility of adversarial manipulation of statisticalmethodologies. This manipulation commonly occurs at the level of data collection and training, ofteninvalidating the assumptions made regarding how data is drawn, but it can also occur at test time.There is a growing literature on adversarial robustness, which is concerned with securing statisticalmethods like (deep) machine learning , linear regression , M-estimation , and onlinelearning . In particular, considers contamination models that incorporate (adaptive)adversarial perturbation of up to an fraction of drawn data. Indeed, we adopt this modeling in ourown study - see (c-Perturb). As well, a similar concept to the notion of adversarial robustness thatwe adopt in this paper is one the literature refers to as perturbation resilience. Generally speaking,a problem instance is called perturbation resilient when despite a degree (parameterized by )of perturbation to the instance, the optimal solution does not change. First introduced in forcombinatorial optimization (in particular, MAX-CUT), the concept has since also inspired researchinto devising resilient unsupervised learning, particularly in clustering . With respect to the hypothesis testing literature, there are recent adversarial robust studies focusedon simple and sequential hypothesis testing from a game theoretic perspective, in whichprotection of statistical power, risk, or sample size from corruption is of chief concern. Comple-menting the adversarial robust perspective are several distributionally robust studies, in which thedata-generating distribution is known only to lie in a parametric family. Recent works include which focus on test risk in single and sequential hypothesis testing settings employinguncertainty sets of distributions of fixed distance (e.g. Wasserstein, phi-divergence) for the null andalternative hypotheses. In contrast to these works, this paper is focused on FDR, not individual testrisk. Furthermore, distributional robustness is not equivalent to the perturbation-robustness that thispaper and other adversarial robust studies seek in general. Indeed, shows that the BH proceduresFDR control exhibits a distributional robustness to possible dependence between null and non-nullhypotheses. On the other hand, our work would illustrate that, distributional robustness aside, BHcan lack adversarial robustness.",
  "Preliminaries": "Let N = {1,...,N}, where N Z+ , be a finite set for which each member i N denotes a binaryhypothesis test deciding between a null and alternative hypothesis. Further, there exists a partitioning,N = H0 H1, such that the correct decision for test i N is either null if i H0, or alternative ifi H1. Here, H0 and H1 are the (unknown) sets of null and alternative test indices, respectively; consequently, for any test i, the correct decision (i.e. set membership) is unknown to the decisionmaker. In fact, while the number of tests N is known (and large, on the order of thousands), neitherN0 = H0 nor 0 = N0",
  "N is known to the decision maker, although 0 0.90 is reasonable in mostlarge-scale testing situations\" - (, p. 285)": "For each test i N, p-value pi is randomly generated (independent of all other pj, j i),which we model as a draw from either U(0,1) when i H0 (pi then referred to as a null p-value) orsome alternative distribution P1i on when i H1 (pi then referred to as an alternative p-value).A multiple-testing algorithm A takes as input a randomly generated collection of p-values p = {pi}iNand outputs for each test i a determination A(i) {0,1} with A(i) = 1 iff the determination is toreject the null hypothesis (i.e., claim i H1), or sometimes referred to as make a discovery\" forthe i-th test. With ap = iH0 A(i) denoting the number of false discoveries, and Rp = A1(1)denoting the number of rejections/discoveries made, we refer to FDP[A;p] =ap Rp1 as the falsedetection proportion summarizing As decisions on p-values p, where xy is shorthand for max(x,y)for any x,y R. We refer to its expectation with respect to the random generation of p as the falsedetection rate, FDR(A) = EpFDP[A;p].",
  "The Adversary and the c-Perturbation Problem": "We model an (omniscient) adversary with knowledge of H0, H1, and that knows the decision makerschoice of control level q. The adversary receives the p-values p = (pi)Ni=1 after they are generated butbefore they are received by the decision maker, or before test time. Given the ability to perturb c 1p-values, the adversary solvesmaxppp0cFDP[BHq;p],(c-Perturb) where p denotes the p-values derived from an adjusted collection of p-values p = (pi)Ni=1. In words,the adversary finds the perturbation of at most cmany p-values before the execution of BHq so asto maximize the adversarially-adjusted false detection proportion FDP[BHq,p]. Perturbation ofp-values is implicitly the result of data perturbation, and we refer to both Remark 4.6 and .2for examples and experiments involving direct perturbation of data. While we assume omniscience for the adversary throughout, we will briefly address modifications inanalysis for an oblivious adversary that has no knowledge of H0, nor H1. Indeed, our algorithms tobe presented can be modified naturally for implementation by an oblivious adversary (see commentsin ); further, there is nearly equivalent performance when 0 is large, as is typical. Hence, itis for the sake of brevity that we omit explicit analysis of the oblivious adversary.",
  "Main Results": "Intuitively, the effect of a small number of p-value perturbations becomes insignificant in settingswhere a large number of tests are rejected (see Theorem 3.2). This happens, for instance, when eitherthe number of tests N, or the control level q, or the distance (e.g. KL-divergence) between the nulland alternative distributions, is large. For this reason, we focus on results that are non-asymptotic inthe number of tests N. In , we present the algorithm INCREASE-c that uses strategic increases to c null p-values toinduce expansion of the BH rejection region. We also present an efficient, optimal algorithm MOVE-1(Appendix .2) for the adversarys maximization of FDR with at most one (i.e. c = 1) p-valueperturbation.",
  "BH as Balls into Bins": "In this section we will establish important notation for the discussions to follow. We reduce the realline to a collection of N + 1 bins\". N0 and N1 balls will each be assigned to one of these binsindependently of each other, from discrete distributions that are specified in the next subsection. Themain motivation for this reduction is to facilitate discussion of effective perturbations in ,and for the technical analysis in .",
  "N },Finally, we write B0i = Bi {pj}jH0 and BNi= Bi {pj}jN for bin is, respectively, null-loadand total load. The alternative- load B1i is defined analogously, as are B01i BN1i, and B11i": "Rejection Count:Borrowing terminology from the classic balls into bins problem of probabilitytheory , this framework facilitates a re-interpretation of the random drawing of p-values as ballsbeing randomly placed into an ordered collection of bins, enumerated 1 up to N + 1. Framed in thisway, we see that BHq operates by identifying the rejection count",
  "k = max{i [0,N]Z BN1i = i},(2)": "which corresponds to the largest collection of consecutive bins 1,...,i that collectively containprecisely i balls, so that BHq rejects all tests with p-values lying in the first k bins. The case k = 0corresponds to rejecting no tests. In fact, k is a stopping time under a filtration F that we define next. Filtration F = {Fi}Ni=0:Let = N be a sample space with the classical Borel - algebraB and (with slight abuse of notation) probability measure P = (iH0U(0,1)) (iH1P1i ). Wedefine a filtration beginning with FN = (BNN+1,B0N+1), and continuing inductively (N towards 0),let Fi be the -algebra generated by {BNj }N+1j=i+1 and {B0j }N+1j=i+1. In words, this filtration correspondsto what is cumulatively learned about the bin loads (null and total) upon examination of the bins insequence starting with bin N + 1 and concluding with bin 1, assuming each observed p value comeswith correct identification of whether or not i H0.",
  "Adversarial Algorithm: INCREASE-c": "Throughout this section, we don the role of the adversary and study the c-Perturb problem, in whichwe are given q (0,1), a realized collection p = {pi}iN (along with the labels of null or alternativefor each pi), and a budget c 1, and our task is to produce a perturbed collection p. Toward this, wefocus on a procedure called INCREASE-c that despite its sub-optimality (see Appendix .2)is intuitive and simple to execute; further, as theoretical and empirical analysis in sections 4 and 5respectively show, it has strong performance in expectation.",
  "and we choose to write k+ in place of k+1": "Increasing the Rejection CountThe interest in k+c is that if we moved any selection of c nullp-values from bin N + 1 into bin k+c (in fact any bin i k+c), then BHq would output a new,increased rejection count k+c. We formally study this in . In the meantime, we comment onthe increase k+c k, which is a difference between two stopping times It is easy to see that k+c k c whenever B0N+1 c; hence, the increase in the rejection count is atleast c, but possibly more. We provide a stronger lower bound on this increase by utilizing the ratiobetween the number B0k+2N of nulls not rejected by BHq and the number N (k + 1) of bins leftoutside of the BHq rejection region in the case of no corruption. Computational experiments indicatecomparable performance of this bound with those of simulations presented in s .Theorem 3.1. If c 1, then",
  ". ELSE leave the p-values unperturbed": "It in fact suffices for the cmany p-values to be placed in any bin i k+c. We remark that since anoblivious adversary cannot discern null-drawn from alternative-drawn in the collection p, INCREASE-c as written is unimplementable in such a case. Hence, for the oblivious adversary, we modifyINCREASE-cs criterion to BNN+1 c and have the oblivious adversary now take the cmanyp-values uniformly at random from among the p-values in the (N + 1)-th bin. Intuitively, thismodification for the oblivious adversary should yield nearly c null p-values being moved (on average)just as in the non-oblivious case, assuming the proportion of nulls among the BNN+1 - many p-valuesis high, as a typically large 0 would entail. We conclude this section with a characterization of the average increase in FDR, denoted c thatINCREASE-c induces.Theorem 3.2. Given c 1, let p+c denote the perturbed form of p that INCREASE-c produces. Thenthe adversarially-adjusted FDR induced by INCREASE-c isEFDP[BHq;p+c] = EFDP[BHq;p] + c,",
  "k+c;B0N+1 c].(5)": "In to follow, we provide analytical lower bounds for c as part of a discussion on BHsadversarial robustness. presents computational experiments (e.g. ). We remarkthat INCREASE-c is not optimal for all instances of c-Perturb; indeed, for c = 1, we present aprovably optimal algorithm MOVE-1 in Appendix .2, which in contrast to INCREASE-1 sometimes induces a reduced rejection count. However, INCREASE-c remains a formidableadversarial procedure, as the results of demonstrate on not only i.i.d. p-values but alsoPRDS conformal p-values.",
  "Theoretical Analysis: Performance Guarantees and Insights intoAdversarial Robustness": "BHs FDR control Lemma 1.1 is (distributionally) robust in the sense that it holds no matterthe alternative distributions {P1i }iH1. However, as Theorem 3.2 indicates, the degree to whichthis control can withstand data perturbations at test time, i.e., its adversarial robustness, very muchdepends on {P1i }iH1. Recalling Lemma 1.1, we may assume without loss of generality that no alternative distribution P1istochastically dominates U(0,1) (equiv., P1i U(0,1)). That being said, the degree\" to which thealternative distributions {P1i }iH1 are (stoch.) dominated by the null distribution U(0,1) (equiv.,P1i U(0,1)) is critical. We briefly preview two regimes of special interest for which each of thenext two subsections cover. High sub-uniformity: When the alternatives are sub-uniform P1i U(0,1) for all i H1, and highlyso, such that for all i H1 it holds that P1i (pi < ) 1 for some small > 0, then it follows that k islarge and B1k should contain most alternative p-values. Consequently, in order for INCREAES-c toinduce any sizeable increase to the FDR, the adversary will need to expand the BH rejection regionsignificantly so as to introduce a commensurate number of nulls. indicates c may need to bequite large to make a dent in FDR control. This message is made more precise in Theorem 4.1. Low sub-uniformity: As we will see, when the alternative p-values are barely dominated by U(0,1),c can be rather large. In fact, in the special case that there is no dominance such that P1i = U(0,1)for all i H1, a strikingly vulnerable state occurs with high probability. Indeed, in this case wherenulls and alternatives are virtually indistinguishable, BHq (in fact any A) admits an FDR of 0whenever any rejections are made (i.e., E[FDP[BHq;p]k 1] = 0) so that BHq accordinglycompensates by making no rejections with high probability (P(k = 0) = 1 q), which follows bythe distributional robust control (1) from Lemma 1.1. But those times when k = 0 is precisely whenINCREASE-cs simultaneous expansion of the rejection region and injection of nulls into this regionis most damaging. That this event and other similarly vulnerable events occurs with high probability isthe fault of the distributional robustness. This message is made rigorous in the forthcoming Theorem4.5.",
  "Case of High Sub-Uniformity in Alternatives {P1i }iH1": "If P1i U(0,1) for all i H1, with P1i (pi < ) 1 for some small > 0, then it is clear that the num-ber of alternatives rejected by BHq should be nearly the maximum number N1 of correct rejectionspossible (i.e., B11k N1) with high probability, limiting any potential impact of INCREASE-c.",
  "E[B01NN B01N N0 c].(6)": "In words, for fixed c, as the alternative distributions concentrate more and more on 0, it followsthat P(B11c = N1) 1, so that the effect c of INCREASE-c on BHs FDR is dampened. Andthis occurs despite the fact that the increase k+c k in rejection count produced by INCREASE-cconsists of mostly the introduction of nulls, and tends to a magnification of (N1 + c) by at leasta factor of the inverse of 1 E[ B01NN B01N N0 c], which is straightforward to compute since",
  "Lower Bounding c": "In view of 5, the event [k+c = c] is clearly of significance in the computation of c. In words,this event describes the case that the BHq rejection region captures nothing, and yet when theadversary successfully executes INCREASE-c the rejection will now capture only nulls - generatinga false detection proportion of 1. Hence, we lower bound the adjustment c of Theorem 3.2 bylower-bounding the probability of this event. Our strategy: (1) first, characterize this probability under the special case that P1i = U(0,1) for alli H1; (2) second, to handle when P1i U(0,1) for some i H1, we translate the KL divergence ofthe resulting discrete, bin-assignment distributions into a bound via Pinskers Inequality. The key to the first step will be to recognize that when P1i = U(0,1) for all i H1, the vector oftotal loads (BN1 ,...,BNN ) is exchangeable (i.e., its law is invariant under permutations), given BN1N.Indeed, exchangeability, combined with the generalized Ballot Theorem of yields the followingresult:Corollary 4.2. Let n 1, p , and x a non-negative integer such that 0 x n. IfB = ( B1,..., Bn) Multinomial(x,(p,...,p)), then P B (nr=1 [ri=1 Bi < r]) = 1 x",
  "(Nc)(q/N+j) )": "Remark 4.6. For a more concrete application/example, consider when the p-values {pi}iN arederived from z-scores {zi}iN via pi = P(Z > zi), where Z N(0,1), with null z-scores {zi}iH0i.i.d. N(0,1) and alternative z-scores {zi N(i1,1)}iH1 (with all i1 0). Indeed, under thisframework, we may view the i1 as the distance between P1i and U(0,1).Towards satisfying Assumption 4.4, we can assume throughout that there exists a nonnegative pa-rameter 1 such that the alternative parameters (i1,i) = (1,1) for all i H1. Indeed, if everymember of the collection {i1}iH1 is in reality only close to zero, then the forthcoming estimates(lower bounds) would provide close, conservative estimates when setting 1 = max{i1 i H1}.When 1 > 0, it follows that (1) = (1 q) ( 1(1q)1",
  "Simulations and Data Experiments": "In this section, we provide computations (performed in R and Python on a Macbook Air-M2 chip,8GB memory, with no experiment time exceeding 5 minutes) to demonstrate the performance ofthe adversarial algorithm INCREASE-c. We demonstrate its performance through simulation onsynthetic data to make comparisons to the theoretical estimates provided in . We thendemonstrate its performance on a real-data experiment in outlier detection.",
  "erated via pi = 1 ( zi1": "1), with {zi}iN0iid N(1,1); (2) FDP[BHq;p] and FDP[BHq;p+c]are calculated. In each of the 104- many (FDP[BHq;p], FDP[BHq;p+c]) pairs are plot-ted. As can be seen, the vast majority of the pairs satisfy FDP[BHq;p+c] > FDP[BHq;p], and,further, all pairs lie above the horizontal line situated at the level of the BH control level 0 q = .09,i.e, FDP[BHq;p+c] > 0 q. In , we present the effectiveness of INCREASE-c over ranges of corruption budget c and1 from small to large. As can be seen, when 1 = 0, any amount of corruption budget c yieldslarge post-corruption FDR EzFDP[BHq;z+c]. When 1 > 0 grows, however, the budget c mustcorrespondingly grow in order for there to be nontrivial post-corruption FDR. Finally, we note thatfor any fixed c, the increase in rejection count k+c k is on the average larger when 1 is larger. Forexperiments on non- i.i.d., PRDS p-values, we refer the interested reader to .2 or Appendix.4.",
  "INCREASE-1 Simulations versus Theoretical Bounds Under Small 1": "In Figures 2 and 3 we illustrate how well the insights discussed in .2 capture the sensi-tivity of BH to adversarial perturbations when 1 is near 0. Specifically, for each q in the grid{.01,.02,...,.99}, we computed the difference FDP[BHq;p+] FDP[BHq;p] across 103 repli-cations of the setup as in .1.1, with the average of this difference being an estimate of 1.The plot of these 1 estimates with respect to q is then compared with the plot of our lower boundL1 as a function of q.",
  "Real-Data Experiment: Credit Card Fraud Detection": "The Credit Card2 dataset D = {(Xi,Yi)}ni=1 R30 {0,1} contains n = 284,807 credit cardtransactions in September 2013 by European cardholders over the course of two days - 492 of whichwere frauds. Each Xi R30 consists of numerical input variables that are the result of a PCAtransformation. The Class\" label Yi takes value 1 in case of fraud and 0 otherwise (/genuine),yielding the partition [n] = n0 n1, with n0 = 284,315 and n1 = 492.",
  "Fraud Detection Experiment": "Given a set of unlabeled transactions {Xi}iS, where S [n], the fraud detection task is to identifywhich members of S belong to n1, i.e., are fraudulent. We experiment with the BH-based, outlierdetection method of on this fraud detection task - specifically, we consider the false detectionproportion of this method in the absence and presence of an adversary. The details of the experimentalsetup will now be discussed.",
  "Training": "We begin by training an unsupervised decision-tree-based algorithm on a training set. From the setn0 of genuine transactions, we uniformly at random select a subset ntrain n0 of size 141,758 toform a training set Dtrain = {Xi}intrain upon which we train an isolation forest s R30 R+using the R library isotree3, where, in principle, s(Xi) returns an isolation depth that is smaller ifYi = 1 (i.e. is an outlier) and larger if Yi = 0 (i.e. is an inlier)",
  "Calibration and (Adversarially-Perturbed) Testing": "Then for each of 102 simulations, we uniformly at random selected a subset ncal n0 ntrain ofsize 141,657 to form a calibration set Dcal = {Xi}incal of strictly genuine transactions. As well,we uniformly at random selected a subset ntest,1 n1 of 100 fraudulent transactions to append tothe 900 remaining genuine transactions comprising ntest,0 = n0 ntrain ncal to form a test setDtest = {Xi}intest, where ntest = ntest,0 ntest,1. Finally, we transformed Xi pi (0,1)for each i ntest via pi = 1+{jncals(Xj)s(Xi)} ncal. The resulting collection of conformal p-valuesp = (pi)intest is PRDS, as proven in , and hence the FDR control of Lemma 1.1 holds (see). In contrast, upon executing INCREASE-c to generate a corresponding adversarially-perturbedcollection p+c = (p+c,i)intest, we obtain a collection for which BHs FDR control no longer holds.",
  "Experimental Results": "We executed BH0.1, on both p and p+c, with an execution providing the decision for each i ntestwhether to report it as genuine (null) or fraudulent (alternative). We report the average (over the 102simulations) false detection proportion (FDP) produced by BH0.1, i.e., both E[FDP[BH0.1; p]]and E[FDP[BH0.1; p+c]] (for c = 1,5,10). As well, we report the average number of allegedfrauds E[k] and E[k+c]. As indicates, although the method of can ordinarily control the",
  "Conclusions": "This is the first work to consider adversarial corruption of the popular Benjamini Hochberg multipletesting procedure to break its FDR control. While BH may exhibit robustness when the alternativedistributions are far\" from the null, it exhibits great sensitivity in practical cases when the alternativesare closer\" to the null. In such cases, with the modification of few p-values (as few as one), theattacker can increase the expected FDR well past the guarantee stipulated by the BH procedure. Thisstudy suggests some caution may be necessary when using BH, especially in safety-security settings.Numerical experiments support the analytical results. Finally, BH is but one member of the family ofstep-up multiple testing procedures, which generally entail rejection regions decided via a stoppingtime, which since our paper shows can be manipulated in the case of BH, it means other step-upprocedures can be similarly prone.",
  "Yoav Benjamini and Daniel Yekutieli. The control of the false discovery rate in multiple testingunder dependency. The Annals of Statistics, 29(4):11651188, 2001": "Daniel Berend, Aryeh Kontorovich, Lev Reyzin, and Thomas Robinson. On biased randomwalks, corrupted intervals, and learning under adversarial design. Annals of Mathematics andArtificial Intelligence, 88:887905, 2020. Sujay Bhatt, Guanhua Fang, Ping Li, and Gennady Samorodnitsky. Minimax m-estimationunder adversarial corruption. In Proceedings of the 39th International Conference on MachineLearning (ICML), Baltimore, MD, 2022.",
  "Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. Isolation forest. In 2008 eighth ieeeinternational conference on data mining, pages 413422. IEEE, 2008": "Thodoris Lykouris, Vahab Mirrokni, and Renato Paes Leme. Stochastic bandits robust toadversarial corruptions. In Proceedings of the 50th Annual ACM SIGACT Symposium on Theoryof Computing, pages 114122, 2018. Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.Towards deep learning models resistant to adversarial attacks. In International Conference onLearning Representations, 2018.",
  "for any collection of alternative hypothesis distributions {P1i }iH1": "Proof. Since the statement in the case of c = 1 is trivially true, we henceforth assume that c > 1. Recallk+c = max{i [c,N]Z B1i = i c}. Let us define k0+c = max{i [c,N]Z B01i = i (B11k + c)}.Then we begin by establishing thatk+c k0+c > k + 1. To see why this holds, we note that B01k+1 = k B11k+1 = k + 1 (B11k+1 + 1) > k + 1 (B11k+1 + c),and this implies k + 1 < k0+c. Further, B1k0+c = B01k0+c + B11k0+c = k0+c (B11k + c) + B11k0+c =k0+c c + (B11k0+c B11k) k0+c c, which implies k+c k0+c.",
  "k0+c (k + 1) k,B11k,[B0N+1 c]": "It suffices to establish it for any fixed, joint realization of k,B11k that occurs consistent with the event[B0N+1 c] with positive probability. With slight abuse of notation, we will continue to use k,B11kfor such a fixed realization, and write E[] for E[k,B11k,[B0N+1 c]]. The plan is to apply theOptional Stopping Theorem on a martingale sequence. To do so, let us form a (backwards-running)filtration: let FN be the sigma-algebra generated by the event [B0N+1 c] as well as the randomvariable B0N+1, and let Fi be the sigma-algebra generated by the event [B0N+1 c] as well as therandom variables {B0j }N+1j=i+1. Then for any integers k, {N,N 1,...1} such that > k,",
  "Since p p0 1, these two conclusions are at odds, presenting a contradiction": "Considering Lemma 7.1, the adversarys p N decision simplifies to deciding on an adjustedrejection count k from among a constrained set of integers consistent with the constraint pp0 1.An optimal k can indeed be larger or smaller than, or even equal to k. Hence, towards understandingthis new search space, it suffices to characterize the set of feasible k that are larger than k, andsmaller.",
  "Lemma 7.2. If i {k + 1,...,N}, then BN1i i 1. In particular, BN1k+1 = i 1": "Proof. Suppose there exists i {k + 1,...,N} such that BN1i > i 1. If BN1i = i, then i > kpresents a contradiction of (2). However, proceeding with BN1i i + 1, we see that there necessarilyexists j {i + 1,...,N} for which BN1j = j, for if this were not the case, then BN1j j + 1 forall j {i + 1,...,N}, meaning BN1N N + 1, yet another contradiction since there are only Np-values.",
  "Proposition 7.3. L = {k k > k,p p0 1}": "Proof. To prove the first statement, we recall that by Lemma 7.2, if i > k, then BN1i i 1. So, ifi k + 1 with BN1i < i 1, then no decrease of a single p-value could increase BN1i to i (Lemma 7.5),precluding the possibility of k = i. In other words, no i L is achievable for the new rejection countk.",
  "iL > iL1 > ... > i1 = k + 1": "For the case of iL, k = iL is achieved if and only if a p-value is decreased from any bin Bj withj > i to a bin Bjs where iL j s 1. For the case of i, k = i is achieved if and only if a p-valueis decreased from any bin Bj with i+1 j > i to a bin Bjs where i j s 1. Finally, all thesemovements of p-value just described are always possible for any given p = {pi}iN .",
  "Proposition 7.4. R = {k k > k,p p0 1}": "Proof. We begin by proving the first statement that k < k implies k R. To do so, we proceed intwo steps. First we show that k i, so that i k k 1. Then we show that if k = i for somei {i + 1,..., k 1} in which BN1i i, we arrive at a contradiction. To see that k i, if i > k, then BN1i = i + 1 becomes BN1i i 1 by Lemma 7.2; inother words, the change in magnitude of BN1i is at least 2, which contradicts Lemma 7.6. Next, ifi {i + 1,..., k 1}, then BN1i i by the definition of i. This means if BN1i i, then BN1i < i, sothat Lemma 7.6 indicates k could not be i.",
  "iR > iR1 > ... > i1 = i": "For the case of iR, k = iR is achieved if and only if a p-value is moved from bin Bj with k j > iRto a bin Bj+s > k. For R 1 r 2, by Lemma 7.6 it holds that k = ir is achieved if and only if ap-value is moved from a bin Bj with ir+1 j > ir to a bin Bj+s with j + s > k. Finally, for the caseof i1, k = i1 = i is achieved if and only if a p-value is moved from Bj with i j to a Bj+s withj + s > k. Finally, all these movements of p-values just described are always possible for any givenp = {pi}iN . It follows that k k if and only if k L R, so an efficient, optimal search procedure be-comes straightforward. Informally, we iterate over the bins in reverse order, beginning with N + 1and terminating with i. At iteration (/bin number) i, if i L R, then the trivial subproblemmaxppp01,k=i FDP[BHq;p] is solved; otherwise, nothing is done. Upon termination, thebest FDP encountered is the answer. This is summarized in Theorem 7.7",
  "maxppp01FDP[BHq;p] = (maxiLR{k}FDPi)": "This result explains that with one pass of the p-values from the largest to the smallest in the collection,we can ascertain the optimal perturbation p. As the execution based on this result is straightforward,we omit the pseudocode for the sake of brevity. In , we compare the average performance of INCREASE-1 against that of the optimalMOVE-1 over 104 simulations. The experiments followed the setup described in Remark 4.6, inwhich N = 103 p-values are derived from independently generated z-scores, with N0(= 900) nullz-scores i.i.d. N(0,1) and N1(= 100) alternative z-scores i.i.d. N(1,1), for 1 = 1,2. As indicates, INCREASE-1 can provide nearly identical performance in adjustment to FDR; however,the perturbation distance z z is on the average much greater than in MOVE-1.",
  "where we now proceed to lower bound (#). The following discussion outlines how we proceed in thecase of c = 1, but this is without loss of generality": "Let B0N+1 and B12N be given, along with the event [BN1= 0]. Then there are N0 B0N+1 manynull p-values and B12N many alternative p-values each of whose assignment to one of bin number2,...,N remains stochastic. Let there be an arbitrary enumeration of these null p-values, upon whichwe let the collection of their bin-assignment random variables be denoted (0i )N0B0N+1i=1. Let therealso be an arbitrary enumeration of these alternative p-values, upon which we let the collection of theirbin-assignment random variables be denoted (1i )B12Ni=1 . More precisely, 0i Unif({2,...,N}),and 1i = j with probabilityq/N+j",
  "INCREASE-c Simulations on PRDS p-values": "In we illustrate the effectiveness of INCREASE-c in disrupting the nonparametric outlierdetection method of that is based on the application of BH on conformal p-values, and in doingso, demonstrate effectiveness of INCREASE-c on PRDS p-values. We follow the simulation settingof .2 in , using their publicly available source code to generate the conformal p-values. Inshort, a data set is generated in R50, along with 103 training observations used to fit a one-class SVMclassifier, as well as 103 observations forming a calibration set to be used with a test set to derive(marginal) conformal p-values. In each of 103 independent replications, INCREASE-c was applied toa new test set consisting of 103 conformal p-values, designed to discern inliers (signals drawn froma mixture of multivariate gaussians with identity covariance matrices) from outliers (signals drawnfrom a mixture of multivariate gaussians with identity covariance matrices scaled by a strength a).This was performed for a {1,1.5,2,2.5,3}; a signal strength a = 1 corresponds to identical nulland alternative distributions, while larger values of a make it easier to detect outliers. We set thefraction of outliers in each test set to 1 = .1, so that a fraction 0 = .90 of the observations are inliersin each data set."
}