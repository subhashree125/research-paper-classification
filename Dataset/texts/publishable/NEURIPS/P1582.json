{
  "Abstract": "Multi-scene absolute pose regression addresses the demand for fast and memory-efficient camera pose estimation across various real-world environments. Nowa-days, transformer-based model has been devised to regress the camera pose directlyin multi-scenes. Despite its potential, transformer encoders are underutilized dueto the collapsed self-attention map, having low representation capacity. This workhighlights the problem and investigates it from a new perspective: distortion ofquery-key embedding space. Based on the statistical analysis, we reveal that queriesand keys are mapped in completely different spaces while only a few keys areblended into the query region. This leads to the collapse of the self-attention mapas all queries are considered similar to those few keys. Therefore, we proposesimple but effective solutions to activate self-attention. Concretely, we present anauxiliary loss that aligns queries and keys, preventing the distortion of query-keyspace and encouraging the model to find global relations by self-attention. Inaddition, the fixed sinusoidal positional encoding is adopted instead of undertrainedlearnable one to reflect appropriate positional clues into the inputs of self-attention.As a result, our approach resolves the aforementioned problem effectively, thusoutperforming existing methods in both outdoor and indoor scenes.",
  "Introduction": "Camera pose estimation is a fundamental and essential computer vision task, adopted in numerousapplications such as augmented reality and autonomous driving. Geometric pipelines with 2Dand 3D data have been a mainstream with high accuracy. After extracting features and matching2D-3D correspondences, camera pose is approximated via Perspective-n-Points (PnP) algorithm andRANSAC . However, there still remains several challenges for real-world applications, includinghigh computational cost and a huge amount of 3D point cloud. Absolute Pose Regression (APR) tackles these issues by directly estimating the 6-DoF pose from asingle RGB image in an end-to-end manner. First introduced by Kendall et al. , subsequent APRmethods have been devised based on convolutional neural networks (CNN). However, theystill demand multiple models and individual optimization to be applied in real-world multi-scenescenarios. In this regard, Multi-Scene Absolute Pose Regression (MS-APR) has emerged to satisfythe needs of speed and memory efficiency across multiple scenes . MSTransformer pioneersa streamlined one-stage MS-APR approach with transformer architecture. It leverages the transformerdecoder not only to improve memory efficiency but also to enhance accuracy significantly. However, we point out that the learning capacity of transformer encoders is underutilized. As shownin Tab. 1, MSTransformers encoder self-attention modules do not significantly improve or evendegrade performance. Although we discovered low-rank, collapsed attention maps, which are known",
  "Attention Map": ": The figure shows query-key spaces, self-attention maps, and attended keys from theorientation transformer encoder of the baseline and ours, respectively. (a) In the case of the baseline,queries and keys are mapped in separate regions, while only a few keys are blended into the queryregion. Consequently, the self-attention map collapses and whole image features are represented bymeaningless few keys, indicating waste of learning capacity of transformer encoder. (b) However,our solution makes queries and keys interact with each other, activating self-attention. This allowsthe model to obtain crucial global relations within image features, capturing salient global features. to cause gradient vanishing or training instability , general solutions were not correctthe problem in the case of MSTransformer. This challenge motivates us to elucidate the phenomenonfrom another perspective in the context of MS-APR. Therefore, this work proposes brand new analysis: distortion of query-key embedding space, wherethe attention score originates. We statistically substantiate that queries and keys are mapped incompletely separated regions in the embedding space while only a few keys are blended into thequery region. In this situation, attention maps inevitably collapse, as all queries are representedsimilarly to those few keys. Intuitively, it means that all image patches are just represented by only afew image patches, as illustrated in . We conjecture that the task difficulty contributes to theissue; the model should extrapolate the camera pose from a single RGB image across multiple scenes.To support this, we empirically reveal that the model tends to avoid exploring self-relationships atthe beginning of the training. These findings align with our additional observations that learnablepositional embeddings used in self-attention are also undertrained. Nevertheless, we expect that image features incorporating global self-relations will be valuableto estimate the camera pose by capturing salient features such as long edges and corners. Thiswork thus introduces simple but effective solutions that activate encoder self-attention modulesfor MS-APR. Concretely, we design an auxiliary loss which aligns the query region and the keyregion. By forcing all queries and keys to be mapped into the close and dense space, they arehighly encouraged to interact with each other by self-attentionlike putting boxers in the ringand blowing the whistle! Furthermore, we explore various positional encoding methods basedon empirical evidence since current undertrained one confuses the model to estimate the camerapose with incorrect positions. Finally, fixed 2D sinusoidal encoding is adopted instead of learnableparameter-based methods . This enables the model to consider appropriate positional clues ofeach image feature during exploring self-relationships from the beginning of the training. As such, the model can obtain rich global relations from activated self-attention as shown in .These relations are incorporated into image features, acquiring more informative encoder output.Extensive experiments demonstrate that our solution recovers the self-attention successfully bypreventing the distortion of query-key space and keeping high capacity of self-attention map .As a result, our model outperforms existing MS-APR methods in both outdoor and indoor scenes without additional memory during inference, upholding the original purpose of MS-APR.",
  "Related Work": "Absolute Pose Regression (APR) is to estimate 6-DoF camera pose directly using only an RGBimage as input. First proposed by Kendall et al. , subsequent works haveintroduced advanced architectures and training methods. However, Sattler et al. demonstratedthat these models have a tendency to memorize training data, resulting in poor generalization whenpresented with novel views. In response, IRPNet attempted fast APR by adopting pre-trainedimage retrieval model as feature extractor. On the other hand, E-PoseNet substituted vanillaCNN with Group-Equivalent CNN to extract geometric-aware image features. Recently, NeRF-based models have improved the APR performance significantly by combining existingAPR models with NeRF-W to capture geometric information or synthesize additional images.However, these models have the limitation of being scene-specific, making them impractical forreal-world applications . NeRF-based models are particularly vulnerable sinceNeRF-W is not just scene-specific, but also requires lots of time for taking structure-from-motion,training, and generating additional data. Multi-Scene Absolute Pose Regression (MS-APR) aims to estimate a camera pose from multiplescenes with a single model for scalability. Initially, MSPN made a feature weight databasefor each scene using CNN, and then fed the indexed features into scene-specific regressors forestimating camera pose in multi-scenes. However, it still had drawbacks of training time and memoryinefficiency. Against the backdrop, MSTransformer enabled sound one-stage MS-APR withtransformer-based architecture. They proposed to use decoder queries as scene queries to make bothscene classification and camera pose regression possible at once. The whole decoder outputs arefirstly used to predict the scene index, and then the decoder query corresponding to the scene is usedto regress the camera pose. This allows it to become more general through various scenes. Recently,additional studies have been proposed fine-tuning methods, but reverting scene-specific againand using additional parameters. In contrast, we propose simple but effective training methods fortraining MSTransformer from scratch, maintaining the original purpose of MS-APR. Self-Attention is a attention mechanism which represents input sequences by inter-relationshipsamong all elements. Following the remarkable performance of transformer , it became widelyadopted in various fields. MSTransformer also used self-attention both the transformer encoderand decoder. However, recent studies have pointed out the failure of self-attention.Firstly, several studies on natural language processing have analyzed that self-attentionmap collapses to a low-rank matrix, by focusing attention scores to meaningless tokens such asCLS and SEP. In the case of vision, several works have addressed the issue of collapsedself-attention in transformer encoder. There are also studies which fundamentallydemonstrated that self-attention maps collapse into low-rank matrices in certain conditions. Similarto these findings, self-attention modules in MSTransformers encoders do not significantly improve oreven impair performance due to the collapse. However, we found that pioneers are not very effectivein the case of MS-APR, which is shown in our experiments. Therefore, we analyse the problem froma new viewpoint, distortion of query-key space and undertrained learnable positional embedding, andpropose solutions that settle down the problem successfully in MS-APR.",
  "Preliminary": "Camera Pose. The location of a captured image I can be calculated from camera intrinsic andextrinsic parameters. The former is specific to the camera itself, and the latter corresponds to thepose of the camera with respect to a fixed world coordinate system. As such, the camera pose pcan be represented as (t, r). Here, t R3 the camera position (translation) and r R4 the cameraorientation (rotation). Note that r is an unit quaternion vector so p has 6-DoF. Model Architecture. MSTransformer suggests the model architecture inspired by DETR .It is composed of a CNN, transformer encoder-decoders, a scene classifier, and regressors. Firstly,given an image I, CNN features for the orientation fr is extracted from middle level while those forthe position ft is computed from middle-high level. ft and fr are projected by 11 convolution andreshaped as ft RNtD and fr RNrD to be transformer-compatible inputs, where Nt and Nrare the number of tokens for the position transformer and the orientation transformer, respectively.",
  "(a) Purity Levels in Query Regions(b) Increase Tendency of Distance": ": (a) shows the purity levels in query regions with the baseline on 7Scenes, referring the Eq. 2.Note that the purity is 1.0 when the query region is composed only with queries, but slightly lowerthan 1.0 when a small subset of keys resides in the query region. According to (a), statistical evidencesupports the prevalent occurrence of the blending of a few keys into the query region across theentire dataset, both in the position and orientation transformer encoders. (b) illustrates the increasingtendency of distance between the query region and the key region in the encoder. They lean awayeach other even at the beginning of the training. Here, the distance between query region and keyregion is an average value across layers and heads. The architecture of transformer encoder-decoder follows the general transformer . It utilizesdecoder query as scene query to make it possible for both scene classification and camera poseregression at once. There are the position and the orientation transformers, each of which has scenequeries zt RMD and zr RMD where M is the number of scenes, respectively. In the encoder,there are several layers to reinforce the CNN features. Every layer of the encoder consists of amulti-head self-attention module and multi-layer perceptron (MLP) with layer normalization andresidual connection. Meanwhile, the decoder is designed to not only enhance the decoder queries but also learn therelationship between encoder features and decoder queries. It has multiple layers like the encoderbut additionally includes a multi-head cross-attention module in each layer. Afterwards, the sceneclassifier predicts the scene index m from concatenated decoder outputs z = [zt; zr] RM2D. Inthe end, t and r are predicted by MLP regressors from the m-th decoder outputs ztm RD andzrm RD, respectively. Self-Attention. There are three main components in attention mechanism: query Q, key K, andvalue V . The final output is computed as a weighted sum of V , with weights derived from thesimilarity score between Q and K. Formally, given the input X RND with N tokens ofdimension D, the h-th attention head is defined as follows:",
  "Distortion of Query-Key Embedding Space": "We introduce our viewpoint why encoder self-attention modules in MSTransformer are not utilized asshown in Tab. 1. Our main intuition is that the model tends to avoid the self-attention mechanism by distorting the query-key embedding space due to the learning difficulty. On the face of it,collapsed self-attention maps may seem to be the root cause. However, we further investigate theissue, focusing on the query-key embedding space. The attention map is determined by computedsimilarity scores between queries and keys, as expressed in Eq. 1. In other words, the projectionmatrices and bias are what the model learns, and the attention map is merely a calculated consequence.As such, the collapsed attention map which induces the models bypassing comes from the distortedquery-key space, therefore we highlight it to address the problem.",
  "(a) Position Transformer Encoder(b) Orientation Transformer Encoder": ": The figure shows L2 distances between the top-left token and other tokens based on thefixed 2D sinusoidal positional encoding and learnable positional embedding, respectively. Here,the learnable positional embedding is the result of training with the baseline. The fixed positionalencoding preserves the order of input sequences, but in the case of the learnable positional embedding,tokens not aligned at the same height or width were all treated randomly further away. When collapsing, queries and keys are completely separated while a small subset of keys are blendedinto the query region across all layers and heads, both in the position and the orientation encoders ofthe baseline . To gauge the statistical incidence of such cases, we first clarify the query regionand the key region in the query-key embedding space. By clustering Q K through k-means using amean vector of queries q and a mean vector of keys k as initial centers, query-dominant region Q isobtained from the cluster with center q. Then we define the purity of query region P as follows:",
  "| Q|| Q Q|.(2)": "Intuitively, P is 1.0 when the query region is composed only with queries, slightly lower than 1.0when small subset of keys are blended into the query region, and about 0.5 when queries and keysare mixed together. (a) illustrates the statistical results on 7Scenes dataset. It shows thatthe phenomenon we point out is predominant throughout the whole dataset both for the position andorientation transformer encoders. Under this condition, the attention map inevitably collapses as allqueries are computed to be similar to those keys. We also verify that queries and keys lean away each other from the beginning of the training as shownin (b). The model is trained to locate only a few keys close to the query region while keepingthe long distance between the query region and the key region. Here, the distance is calculated usingthe L2 metric between their centers. These investigations imply that the model avoids the process ofidentifying self-relations due to the task difficulty only with the original task loss.",
  "Undertrained Positional Embedding": "Moreover, we discuss the importance of appropriate positional encoding for the task and the negativeeffects of the current positional embeddings in this section. As self-attention is a permutation-invariantmechanism, positional encoding is widely adopted to the queries and keys to account for the orderof the input sequence . It plays a vital role to utilize the encoder self-attention modules,particularly in MS-APR. To be specific, it is not only important to detect features like roofs andwindows in the image, but also crucial to know where they are located in the image for accurate poseestimation. In other words, the model has to estimate the camera pose with shuffled image patches ifit uses inappropriate positional embedding. Despite its importance, learnable positional embeddings in the baseline remain undertrained withdeactivated self-attention modules. shows the visualization results of the distances betweenthe top-left token and other tokens, based on the fixed 2D sinusoidal positional encoding and thelearnable positional embedding trained in the baseline. One can observe that distances betweentokens are drastically larger if the token has different vertical or horizontal position with the top-lefttoken, in the case of learnable positional embeddings. This makes it even harder for the model tolearn geometric relationships between image features by self-attention mechanism.",
  "Query-Key Space": ": illustrates the training pipeline with our solutions. We apply additional objectivesLQKAt and LQKAr to the model to activate the self-attention modules. Specifically, queries Q andkeys K interact with each other by forcing the centroid of query region q and the centroid of keyregion k to become closer. Here, we encode all input queries and keys with fixed 2D sinusoidalpositional encoding to ensure active interaction between Q and K with reliable positional clues.",
  "Activating Self-Attention for MS-APR": "In this section, we introduce our solutions with full objectives for training our transformer-basedMS-APR model successfully by mitigating self-attention map collapse. Firstly, we employ the L2loss to regress the camera position and orientation. Let us denote the ground-truth camera pose as(t, r) and the estimated camera pose as (t, r). Then, the position loss and the orientation loss aredefined as Lt = t t2 and Lr = r r r2, respectively. Here, it is required to balance twolosses to make the model learn both camera position and orientation since the former is a scalar valuebut the latter is a unit quaternion . According to , the final pose loss is given by:",
  "where st and sr are the learnable parameters to adjust the uncertainty": "Secondly, the model should be capable of classifying the scene from the input image to work withmulti-scene dataset. Let us denote y RM as one-hot vector encoding the ground-truth scene indexof the input image, and y RM as the predicted probability distribution of scene classes fromconcatenated decoder outputs z. Then, the scene classification loss proposed in is given by:",
  "j=1yj log yj.(4)": "On top of it, we introduce an auxiliary loss to prevent the model from avoiding the self-attentionas depicted in . As we discussed in Sec. 4.1, it is essential to ensure that the queries and thekeys do not become alienated from each other while encouraging their interaction by embedding inclose proximity. To achieve this, we propose the Query-Key Alignment (QKA) loss as an additionalobjective. It is defined as follows:",
  "h=1qlh klh2,(5)": "where L is the number of encoder layers, H is the number of heads, and qlh and klh are the meanvectors of the queries and the keys per each encoder layer and head, respectively. By inducing queriesand keys to be placed in a shared compact area, we ensure that they interact with each other in the",
  "MethodChessFireHeadsOfficePumpkinKitchenStairsAverage": "MSPN 0.09/4.76 0.29/10.50 0.16/13.10 0.16/6.80 0.19/5.50 0.21/6.61 0.31/11.63 0.20/8.41MST 0.11/4.66 0.24/9.60 0.14/12.19 0.17/5.66 0.18/4.44 0.17/5.94 0.26/8.45 0.18/7.28+Ours0.10/4.15 0.24/8.79 0.14/11.59 0.17/5.28 0.17/3.48 0.17/5.62 0.22/7.58 0.17/6.64 self-attention module. Here, we encode all input queries and keys with fixed 2D sinusoidal positionalencoding instead of undertrained learnable positional embedding. This guides the model to stablylearn self-relationships from the beginning of the training. As a result, the accurate interaction can beaccomplished by reflecting the verified correct positional information to input queries and keys. We apply our QKA loss to both the position and orientation transformer encoders, while adjustingthe weight of the loss to integrate with the original task loss. Accordingly, the final auxiliary lossLaux = auxLQKAt + LQKAr, where aux is the weight of the auxiliary loss. LQKAt and LQKAr areQKA losses for the position and orientation transformer encoders, respectively. Putting all together,our full objective L = Lpose + Lscene + Laux.",
  "Experimental Setup": "Datasets. We train and evaluate the model on outdoor and indoor datasets , which include RGBimages labeled with 6-DoF camera poses. Firstly, we use the Cambridge Landmarks which consist ofsix outdoor scenes scaled from 875m2 to 5600m2. Each scene contains 200 to 1500 training data.We conduct the experiment on four scenes in Cambridge Landmarks, which are typically adoptedfor evaluating absolute pose regression . On the other hand, we use the 7Scenes datasetwhich consists of seven indoor scenes scaled from 1m2 to 18m2. Each scene includes from 1000 to7000 images. Training Details. We entirely follow the configuration of the baseline . We train the modelwith a single RTX3090 GPU, Adam optimizer with 1 = 0.9, 2 = 0.999, = 1010, and the batchsize of 8. For the 7Scenes dataset, we train the model for 30 epochs with the initial learning rate of1 104, reducing the learning rate by 1/10 every 10 epochs. In the case of Cambridge Landmarksdataset, we train the model for 500 epochs with the initial learning rate of 1 104, reducing thelearning rate by 1/10 every 200 epochs. Afterwards, we freeze the CNN and the orientation branch,then fine-tune the position branch as same as the baseline . The model is trained for 60 epochswith the initial learning rate 1 104, reducing the learning rate by 1/10 every 20 epochs. The learnable parameters st and sr for the original task loss are initialized as in . Both for theposition and orientation transformer encoder-decoder, the number of layers L is 6 and the number ofheads H is 8. Lastly, we set the aux for our query-key alignment loss as 0.1.",
  "Comparative Analysis": "MS-APR Methods. We compare the model trained our solutions with other single-frame multi-sceneAPR models for fair comparison, i.e., excluding scene-specific methods. Tab. 2, Tab. 3, and Tab. 4show the experimental results on four outdoor scenes in Cambridge Landmarks and seven indoorscenes in 7Scenes , respectively. With our solutions, the model shows better performance throughall scenes, compared to other MS-APR methods. This results verify that activating self-attention has",
  "the potential to become a general module, generating task-relevant features across multiple scenes,regardless of the specific features of each scene": "Alternative Methods for Collapsed Self-Attention. We apply general-purpose techniques ,proved to alleviate collapsed self-attention, to the baseline to compare the effectiveness of eachmethod and ours in MS-APR. Here, we conduct experiments with fixed positional encoding, asincorrect inputs may suppress the potential of the solutions. Tab. 5a shows that existing techniqueshave little or no effect in terms of recovering the self-attention for the task. Meanwhile, ourssignificantly improves the baseline in both conditions. We conjecture that it is required to guide themodel in a more direct manner to utilize self-attention for MS-APR. Alternative Positional Encoding Methods. We conduct experiments with recent advanced learnablepositional encoding methods with our QKA loss. As shown in Tab. 5b, even advancedmethods are not suitable to the task as they mainly depend on learnable parameters and relativeposition. We assume that absolute, purified positional clues are important to stabilize the training ofthe embedding space in the case of MS-APR.",
  "Note that high attention entropy indicates training stability and high representation capacity ofself-attention map": "shows the attention entropy for each encoder layer in both the position and orientationtransformers. As illustrated, the model employing our solutions has significantly higher attentionentropy than the original baseline model across all encoder layers. These results validate that oursolutions successfully induce the transformer-based model to activate the encoder self-attention, thusimproving the representation quality of self-attention map. Purity Levels in Query Regions. To verify the effectiveness of our auxiliary loss on rectifying thedistortion of query-key space, we measure the purity levels in query regions defined in Eq. 2. Thestatistical results for both the baseline and the model employing our solutions are presented in .",
  ": The figure shows the attention entropy of the baseline and ours for each encoder layer. Itdemonstrates that our solutions significantly improve the utilization of encoders learning capacity": ": The results show the change of purity of query region P, defined in Eq. 2, across 7Scenesdataset. Note that the purity is 1.0 when the query region is composed only with queries, but slightlylower than 1.0 when few keys resides in the query region. Compared to the baseline, there are nocases where only a few keys are blended into the query region with our solution. Here, the purity levels represent the proportion of the queries allocated to the query-dominant cluster.Our observations reveal that the nearly half keys belong to the query region in almost all cases,indicating that the query and key regions are highly interleaved. This demonstrates that our solutionsrestrain all queries from being treated as similar to only a few keys, thus preventing self-attentioncollapse and activating self-attention.",
  "Qualitative Analysis": "reports visualization of query-key space by t-SNE and attention results of the baseline andthe model employing our solution. Note that the attention results depict the scores of the keys,averaged over the queries in the self-attention map. One can observe that the query regions and keyregions of the baseline are separate. In contrast, the regions are highly aligned by our solutions,reducing the occurrence of only a few keys blended into the query region. This condition necessitatesthat the model reflects global interactions between queries and keys, therefore, global relations areincorporated into image features as depicted in . We can see that geometric relations such aslong edges are usually obtained, which are the critical points for the task. More examples are in thesupplementary material.",
  "Ablation Study": "We conduct an ablation study on our solutions to verify the effectiveness of each component. Asshown in Tab. 6, our QKA loss enhances the performance in both the outdoor and indoor datasets.Additionally, overall performances are further improved with fixed positional encoding, rectifying theincorrect inputs in the self-attention. This ablation study indicates that several useful self-relation canbe extracted from noisy content features by self-attention, but reliable positional clues are particularlyimportant to learn various crucial relationship for estimating more accurate camera pose.",
  "QueryKey": ": The figure shows the examples of t-SNE results of query-key space and attention results ofthe baseline and the model employing our solution. The query and key regions are separate in thebaseline while our solutions align the regions, enabling the model to focus on salient global featuresand incorporate self-relations into image features.",
  "MST 1.28m/2.730.18m/7.28MST+Fixed PE1.29m/2.460.18m/6.80MST+QKA1.22m/2.680.18m/7.09MST+Fixed PE+QKA1.19m/2.290.17m/6.64": "could be a side effect if only a few key features are in the image; i.e., if a dynamic moving objectmainly occupies the image. Developing an algorithm to decide whether global self-attention is usefulfor the image or not could be an interesting future work. Broader Impacts. The camera pose estimation technology with this work can be expected to haveseveral positive social impacts, such as being utilized in programs to locate missing persons. On theother hand, it could potentially be misused for purposes such as illegal location tracking programs.",
  "Conclusion": "This work has exposed that self-attention modules in transformer encoders are not activated inMS-APR. Looking beyond the collapsed self-attention maps, we have discovered the distortion ofquery-key embedding space; queries and keys are entirely separate while a small subset of keysresides in the query region. We have examined the predominance of the phenomenon, adducingstatistical and empirical evidence. Additionally, the importance of accounting for the order of theinput sequence has been addressed in the context of the task. Based on these analyses, this work hasproposed simple but effective solutions to activate self-attention by rectifying the distorted query-keyspace with alignment loss and appropriate positional clues. As a result, the model could learn salientglobal features through self-attention, thus achieving state-of-the-art performance both in outdoor andindoor scenes without additional memory in inference. Our extensive experiments have demonstratedthe effectiveness of proposed solutions in terms of recovering self-attention for MS-APR.",
  "Acknowledgement": "This work was supported in part by MSIT&KNPA/KIPoT (Police Lab 2.0, No. 210121M06),MSIT/IITP (No.2022-0-00680, 2020-0-01821, 2019-0-00421, RS-2024-00459618, RS-2024-00360227, RS-2024-00437102, RS-2024-00437633), and MSIT/NRF (No. RS-2024-00357729). Torsten Sattler, Bastian Leibe, and Leif Kobbelt. Efficient & effective prioritized matchingfor large-scale image-based localization. IEEE transactions on pattern analysis and machineintelligence, 39(9):17441756, 2016. Eric Brachmann and Carsten Rother. Learning less is more-6d camera localization via 3dsurface regression. In Proceedings of the IEEE conference on computer vision and patternrecognition, pages 46544662, 2018. Kiru Park, Timothy Patten, and Markus Vincze. Pix2pose: Pixel-wise coordinate regression ofobjects for 6d pose estimation. In Proceedings of the IEEE/CVF International Conference onComputer Vision, pages 76687677, 2019. Xiaotian Li, Shuzhe Wang, Yi Zhao, Jakob Verbeek, and Juho Kannala. Hierarchical scenecoordinate classification and regression for visual localization. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition, pages 1198311992, 2020. Paul-Edouard Sarlin, Ajaykumar Unagar, Mans Larsson, Hugo Germain, Carl Toft, ViktorLarsson, Marc Pollefeys, Vincent Lepetit, Lars Hammarstrand, Fredrik Kahl, et al. Back tothe feature: Learning robust camera localization from pixels to pose. In Proceedings of theIEEE/CVF conference on computer vision and pattern recognition, pages 32473257, 2021.",
  "Eric Brachmann and Carsten Rother. Visual camera re-localization from rgb and rgb-d imagesusing dsac. IEEE transactions on pattern analysis and machine intelligence, 44(9):58475865,2021": "Martin A Fischler and Robert C Bolles. Random sample consensus: a paradigm for modelfitting with applications to image analysis and automated cartography. Communications of theACM, 24(6):381395, 1981. Alex Kendall, Matthew Grimes, and Roberto Cipolla. Posenet: A convolutional network forreal-time 6-dof camera relocalization. In Proceedings of the IEEE international conference oncomputer vision, pages 29382946, 2015. Samarth Brahmbhatt, Jinwei Gu, Kihwan Kim, James Hays, and Jan Kautz. Geometry-awarelearning of maps for camera localization. In Proceedings of the IEEE conference on computervision and pattern recognition, pages 26162625, 2018. Ming Cai, Chunhua Shen, and Ian Reid. A hybrid probabilistic model for camera relocalization.In British Machine Vision Conference 2018, BMVC 2018, Newcastle, UK, September 3-6, 2018,page 238. BMVA Press, 2018. URL",
  "Shuai Chen, Xinghui Li, Zirui Wang, and Victor A Prisacariu. Dfnet: Enhance absolute poseregression with direct feature matching. In European Conference on Computer Vision, pages117. Springer, 2022": "Alex Kendall and Roberto Cipolla. Modelling uncertainty in deep learning for camera relocal-ization. In 2016 IEEE international conference on Robotics and Automation (ICRA), pages47624769. IEEE, 2016. Alex Kendall and Roberto Cipolla. Geometric loss functions for camera pose regressionwith deep learning. In Proceedings of the IEEE conference on computer vision and patternrecognition, pages 59745983, 2017. Mohamed Adel Musallam, Vincent Gaudilliere, Miguel Ortiz Del Castillo, Kassem Al Ismaeil,and Djamila Aouada. Leveraging equivariant features for absolute pose regression. In Pro-ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages68766886, 2022.",
  "Xin Wu, Hao Zhao, Shunkai Li, Yingdian Cao, and Hongbin Zha. Sc-wls: Towards interpretablefeed-forward camera re-localization. In European Conference on Computer Vision, pages585601. Springer, 2022": "Hunter Blanton, Connor Greenwell, Scott Workman, and Nathan Jacobs. Extending absolutepose regression to multiple scenes. In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition Workshops, pages 3839, 2020. Yoli Shavit, Ron Ferens, and Yosi Keller. Learning multi-scene absolute pose regression withtransformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision,pages 27332742, 2021. Yihe Dong, Jean-Baptiste Cordonnier, and Andreas Loukas. Attention is not all you need: Pureattention loses rank doubly exponentially with depth. In International Conference on MachineLearning, pages 27932803. PMLR, 2021. Shuangfei Zhai, Tatiana Likhomanenko, Etai Littwin, Dan Busbridge, Jason Ramapuram, YizheZhang, Jiatao Gu, and Joshua M Susskind. Stabilizing transformer training by preventingattention entropy collapse. In International Conference on Machine Learning, pages 4077040803. PMLR, 2023.",
  "Kwonjoon Lee, Huiwen Chang, Lu Jiang, Han Zhang, Zhuowen Tu, and Ce Liu. Vitgan: Train-ing gans with vision transformers. In International Conference on Learning Representations,2021": "Lorenzo Noci, Sotiris Anagnostidis, Luca Biggio, Antonio Orvieto, Sidak Pal Singh, andAurelien Lucchi. Signal propagation in transformers: Theoretical perspectives and the role ofrank collapse. Advances in Neural Information Processing Systems, 35:2719827211, 2022. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unifiedtext-to-text transformer. The Journal of Machine Learning Research, 21(1):54855551, 2020.",
  "Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer:Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024": "Jamie Shotton, Ben Glocker, Christopher Zach, Shahram Izadi, Antonio Criminisi, and AndrewFitzgibbon. Scene coordinate regression forests for camera relocalization in rgb-d images.In Proceedings of the IEEE conference on computer vision and pattern recognition, pages29302937, 2013. Torsten Sattler, Qunjie Zhou, Marc Pollefeys, and Laura Leal-Taixe. Understanding the limita-tions of cnn-based absolute camera pose regression. In Proceedings of the IEEE/CVF conferenceon computer vision and pattern recognition, pages 33023312, 2019.",
  "Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. Bert: Pre-training of deepbidirectional transformers for language understanding. In Proceedings of NAACL-HLT, pages41714186, 2019": "Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, andSergey Zagoruyko. End-to-end object detection with transformers. In European conference oncomputer vision, pages 213229. Springer, 2020. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for imagerecognition at scale. In International Conference on Learning Representations, 2021. URL Olga Kovaleva, Alexey Romanov, Anna Rogers, and Anna Rumshisky. Revealing the darksecrets of bert. In Proceedings of the 2019 Conference on Empirical Methods in Natural Lan-guage Processing and the 9th International Joint Conference on Natural Language Processing(EMNLP-IJCNLP), pages 43654374, 2019. Goro Kobayashi, Tatsuki Kuribayashi, Sho Yokoi, and Kentaro Inui. Attention is not only aweight: Analyzing transformers with vector norms. In Proceedings of the 2020 Conference onEmpirical Methods in Natural Language Processing (EMNLP), pages 70577075, 2020.",
  "We entirely follow the setting of the baseline . The settings are as below": "Data Augmentation. Data augmentation is performed in the same way as , which is also adoptedin the baseline. Afterwards, brightness, contrast, and saturation are randomly jittered for photometricrobustness. Test images are also resized to 256 256, but they are center-cropped to 224 224,without any photometric augmentation. Additionally, following the baseline, smaller scenes in theCambridge Landmarks dataset are oversampled during training to deal with data imbalanceproblem. Backbone. As did in the baseline , we utilize a CNN model, the EfficientNet-B0 , to extractlocal image features from input images. The image features ft for the position and fr for theorientation are extracted from the fourth block and the third block, respectively. Accordingly, the sizeof ft is 14 14 112 and the size of fr is 28 28 40, i.e., the number of transformer encodertokens Nt and Nr are 196 and 784, respectively.",
  "A.2Visualization": "Query-Key Spaces. In addition to the statistical and quantitative results introduced in the main paper,we visualize the query-key spaces across heads and layers. Fig. A1 reports the visualization resultsof query-key spaces from the baseline across all even layers and heads. It qualitatively showsthat the distortion of query-key space occurs in most heads in transformer encoder of the baseline. Incontrast, one can observe that queries and keys properly intertwine each other with our solutions asshown in Fig. A2."
}