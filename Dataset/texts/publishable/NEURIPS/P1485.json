{
  "Abstract": "Neural networks are traditionally trained under the assumption that data comefrom a stationary distribution. However, settings which violate this assumption arebecoming more popular; examples include supervised learning under distributionalshifts, reinforcement learning, continual learning and non-stationary contextualbandits. In this work we introduce a novel learning approach that automaticallymodels and adapts to non-stationarity, via an Ornstein-Uhlenbeck process with anadaptive drift parameter. The adaptive drift tends to draw the parameters towardsthe initialisation distribution, so the approach can be understood as a form ofsoft parameter reset. We show empirically that our approach performs well innon-stationary supervised and off-policy reinforcement learning settings.",
  "Introduction": "Neural networks (NNs) are typically trained using algorithms like stochastic gradient descent (SGD),assuming data comes from a stationary distribution. This assumption fails in scenarios such ascontinual learning, reinforcement learning, non-stationary contextual bandits, and supervised learningwith distribution shifts . A phenomenon occurring in non-stationary settings is the loss ofplasticity , manifesting either as a failure to generalize to new data despite reduced trainingloss , or as an inability to reduce training error as the data distribution changes . In , the authors argue for two factors that lead to the loss of plasticity: preactivation distributionshift, leading to dead or dormant neurons , and parameter norm growth causing training instabili-ties. To address these issues, strategies often involve hard resets based on heuristics like detectingdormant units , assessing neuron utility , or simply after a fixed number of steps .Though effective at increasing plasticity, hard resets can be inefficient as they can discard valuableknowledge captured by the parameters.",
  "arXiv:2411.04034v1 [cs.LG] 6 Nov 2024": "while keeping them close to their previous values. It also increases learning rate of the learningalgorithm, allowing new NN parameters to adapt faster to the changing data. The amount by whichthe parameters move towards the initialization and the amount of learning rate increase are controlledby the drift parameters which are learned online. The exact implementation of soft reset mechanismis based on the use of a drift model in NN parameters update before observing new data. Similarideas which modify the starting point of SGD as well as increase the learning rate of SGD dependingon non-stationarity were explored (see ) in an online convex optimization setting. Specifically,in , the authors assume that the optimal parameter of SGD changes according to some dynamicalmodel out of a finite family of models. They propose an algorithm to identify this model and a way toleverage this model in a modified SGD algorithm. Compared to these works, we operate in a generalnon-convex setting. Proposed drift model can be thought as a dynamical Bayesian prior over NeuralNetwork parameters, which is adapted online to new data. We make a specific choice of drift modelwhich implements soft resets mechanism. Our contributions can be summarized as follows. First, we propose an explicit model for the drift inNN parameters and describe the procedure to estimate the parameters of this model online from thestream of data. Second, we describe how the estimated drift model is incorporated in the learningalgorithm. Third, we empirically demonstrate the effectiveness of this approach in preventing theloss of plasticity as well as in an off-policy reinforcement learning setting.",
  "TTt=1 (Lt(t) Lt(t )) ,(2)": "with a reference sequence = (1, . . . , T ), satisfying t = arg min Lt(). A common approachto the online learning problem is online stochastic gradient descent (SGD) . Starting from initialparameters 0, the method updates these parameters sequentially for each batch of data {(xit, yit)}Bi=1s.t. (xit, yit) pt(xt, yt). The update rule is:",
  "BBi=1 Lt+1(t, xit, yit) and t is learning rate. See also Appendix G forthe connection of SGD to proximal optimization": "Convex Setting. In the convex setting, online SGD with a fixed learning rate can handle non-stationarity . By selecting appropriately potentially using additional knowledge about thereference sequencewe can optimize the dynamic regret in (2). In general, algorithms that adapt tothe observed level of non-stationarity can outperform standard online SGD. For example, in , theauthors propose to adjust the learning rate t, while in and in , the authors suggest modifyingthe starting point of SGD from t to an adjusted t proportional to the level of non-stationarity. Non-Convex Setting. Non-stationary learning with NNs is more complex, since now there is achanging set of local minima as the data distribution changes. Such changes can lead to a loss ofplasticity and other pathologies. Alternative optimization methods like Adam , do not fullyresolve this issue . Parameter resets partially mitigate the problem, butcould be too aggressive if the data distributions are similar.",
  "Online non-stationary learning with learned parameter resets": "Notation.We denote by N(; , 2) a Gaussian distribution on with mean and variance 2.We denote i the i-the component of the vector = (1, . . . , D). Unless explicitly mentioned,we assume distributions are defined per NN parameter and we omit the index i. We denote asLt+1() = log p(yt+1|xt+1, ) the negative log likelihood on (yt+1, xt+1) for parameters .",
  "Toy Bayesian Inference example": ": Left: graphical model for data generating process in the (a) stationary case and (b) non-stationary case with drift model p(t+1|t, t). Right: (c) In a stationary online learning regime, theBayesian posterior (red dashed circles) in the long run will concentrate around (red dot). (d) In anon-stationary regime where the optimal parameters suddenly change from current value t to newvalue t+1 (blue dot) online Bayesian estimation can be less data efficient and take time to recoverwhen the change-point occurs. (e) The use of p(|t, t) and the estimation of t allows to increasethe uncertainty, by soft resetting the posterior to make it closer to the prior (green dashed circle), sothat the updated Bayesian posterior pt+1() (blue dashed circle) can faster track t+1. non-i.i.d. fashion such that a change in the data distribution is modeled by the drift in the parametersp(t+1|t, t) at every time t + 1 before new data is observed. We assume a class of drift modelsp(t+1|t, t) which encourages the parameters to move closer to the initialization. The amount ofdrift (and level of non-stationarity) is controlled by t which are estimated online from the data. As can be seen below, in the context of SGD, this approach adjusts the starting point t of the updateto a point t(t), which is closer to the initialization and increases the learning rate proportionallyto the drift. In the context of Bayesian inference, this approach shrinks the mean of the estimatedposterior towards the prior and increases the variance proportional to t. This approach is inspired byprior work in online convex optimization for non-stationary environments [e.g., 25, 21, 8, 18, 29].",
  "Toy illustration of the advantage of drift models": "Consider online Bayesian inference with 2-D observations yt = + t , where R2 areunknown true parameters and t N(0; 2I) is Gaussian noise with variance 2. Starting froma Gaussian prior p0() = N(; 0; 0), the posterior distribution pt+1() = p(|y1, . . . , yt) =N(; t+1, t+1) is updated using Bayes rule",
  "pt+1() p(yt+1|)pt().(3)": "The posterior update (3) comes from the i.i.d. assumption on the data generation process (a),since pt+1() p0() t+1s=1 p(ys|). By the Central Limit Theorem (CLT), the posterior mean tconverges to and the covariance matrix t shrinks to zero (the radius of red circle in c). Suppose now that the true parameters t (kept fixed before t) change to new parameters t+1 at timet + 1. The i.i.d. assumption (a) is violated and the update (3) becomes problematic becausethe low uncertainty (small radius of red dashed circle in d) in pt() causes the posteriorpt+1() (see blue circle) to adjust slowly towards t+1 (blue dot) as illustrated in d. To address this issue, we assume that before observing new data, the parameters drift accordingto p(t+1|t, t) where the amount of drift is controlled by t. The corresponding conditionalindependence structure is shown in b. The posterior update then becomes:",
  "pt+1() p(yt+1|)p(|t, t)pt(t)dt.(4)": "For a suitable choice of drift model p(t+1|t, t), this modification allows pt+1() (blue circle) toadjust more rapidly towards the new t+1 (blue dot), see e. This is because the new priorp(|t, t)pt(t)dt has larger variance (green circle) than pt() and its mean is closer to the centerof the circle. Ideally, the parameter t should capture the underlying non-stationarity in the datadistribution in order to control the impact of the priorp(|t, t)pt(t)dt. For example, if atsome point the non-stationarity disappears, we want the drift model to exhibit no-drift to recover theposterior update (3). This highlights the importance of the adaptive nature of the drift model.",
  "Ornstein-Uhlenbeck parameter drift model": "We motivate the specific choice of the drift model which is useful for maintaining plasticity. Weassume that our Neural Network has enough capacity to learn any stationary dataset in a fixed numberof iterations starting from a good initialization 0 p0() [see, e.g., 24, 16]. Informally, we call theinitialization 0 plastic and the region around 0 a plastic region. Consider now a piecewise stationary datastream that switches between a distribution pa, with a setof local minima Ma of the negative likelihood L(), to a distribution pb at time t + 1, with a set oflocal minima Mb. If Mb is far from Ma, then hard reset might be beneficial, but if Mb is close to Ma,resetting parameters is suboptimal. Furthermore, since is high-dimensional, different dimensionsmight need to be treated differently. We want a drift model that can capture all of these scenarios.",
  "p(|t, t) = N(; tt + (1 t)0; (1 2t )20),(5)": "which is separately defined for every parameter dimension i where p0(i0) N(i0; i0;i02) isthe per-parameter prior distribution and t = (1t , . . . , Dt ). The model is a discretized Ornstein-Uhlenbeck (OU) process (see Appendix A for the derivation). The parameter t is a drift parameter and controls the amount of non-stationarity in eachparameter. For t = 1, there is no drift and for t = 0, the drift model reverts the parameters back tothe prior. A value of t (0, 1) interpolates between these two extremities. A remarkable propertyof (5) is that starting from the current parameter t, if we simulate a long trajectory, as T , thedistribution of p(T |t) will converge to the prior p(0). This is only satisfied (for t (0, 1)) dueto the variance 20(1 2t ). Replacing it by an arbitrary variance 2 would result in the varianceof p(T |t) either going to 0 or growing to , harming learning. Thus, the model (5) encouragesparameters to move towards plastic region (initialization). In Appendix B, we discuss this further andother potential choices for the drift model.",
  "Online estimation of drift model": "The drift model p(t+1|t, t) quantifies prior belief about the change in parameters before seeingnew data. A suitable choice of an objective to select t is predictive likelihood which quantifies theprobability of new data under our current parameters and drift model. From Bayesian perspective, itmeans selecting the prior distribution which explains the future data the best. We derive the drift estimation procedure in the context of approximate online variational infer-ence with Bayesian Neural Networks (BNN). Let t = (1, . . . , t) be the history of ob-served parameters of the drift model and St = {(x1, y1), . . . , (xt, yt)} be the history of ob-served data. The objective of approximate online variational inference is to propagate an ap-proximate posterior qt(|St, t1) over parameters, such that it is constrained to some familyQ of probability distributions.In the context of BNNs, it is typical to assume a familyQ = {q() : q() Di=1 N(i; i,i2); = (1, . . . , D)} of Gaussian mean-field distributionsover parameters RD (separate Gaussian per parameter). For simplicity of notation, we omit theindex i. Let qt() qt(|St, t1) Q be the Gaussian approximate posterior at time t with meant and variance 2t for every parameter. The new approximate posterior qt+1() Q is found by",
  "qt(|t) =qt(t)p(|t, t)dt = N(; t(t), 2t (t))(7)": "that has parameters t(t) = tt + (1 t)0, 2t (t) = 2t 2t + (1 2t )20, see Appendix I.1 forderivation. The form of this prior qt(|t) comes from the non i.i.d. assumption (see b) andthe form of the drift model (5). For new batch of data (xt+1, yt+1) at time t + 1, the approximatepredictive log-likelihood equals to",
  "t,k+1 = t,k + logp(yt+1|xt+1, t(t,k) + t(t,k))N(; 0, I)d,(10)": "The integral is evaluated by Monte-Carlo (MC) using M samples i N(; 0, I), i = 1, . . . , Mp(yt+1|xt+1, t(t,k) + t(t,k))N(; 0, I)d 1MMi=1 p(yt+1|xt+1, t(t,k) + it(t,k))(11)Inductive bias in the drift model is captured by 0t , where t,0 = 1 encourages stationarity, whilet,0 = t1,K promotes temporal smoothness. In practice, we found t,0 = 1 was the most effective. Structure in the drift model. The drift model can be defined to be shared across different subsets ofparameters which reduces the expressivity of the drift model but also provides regularization to (10).We consider t to be either defined for each parameter or for each layer. See for details aswell as corresponding results in Appendix H.",
  "((202t )gt+1)T gt+1+,(12)": "where we also clip parameters t to . The expression (12) gives us the geometric interpretationfor t. The value of t depends on the angle between (t 0) and gt+1 When these vectors arealigned, t is high and is low otherwise. When these vectors are orthogonal or the gradient gt+1 0,the value of t is heavily influenced by 0t . Moreover, when gt+1 0, we can interpret it as beingclose to a local minimum, i.e., stationary, which means that we want t 1, therefore adding the 2penalty is important. Also, when the norm of the gradients gt+1 is high, the value of t is encouragedto decrease, introducing the drift. This means that using t in the parameter update (see .5)encourages the norm of the gradient to stay small. In practice, we found that update (12) was unstablesuggesting that linearization of the log-likelihood might not be a good approximation for learning t.",
  "logi2,(13)": "where it > 0 are per-parameter temperature coefficients. The use of small temperature > 0parameter (shared for all NN parameters) was shown to improve empirical performance of BayesianNeural Networks . Given that in (13), the variance 2t (t) can be small, in order to control thestrength of the regularization, we propose to use per parameter temperature it = it2, where > 0 is a global constant. This leads to the following objective",
  "i rit": "(i it(t))2 + [i]2 it(t)2 log[i]2,(14)where the quantity rit = [it]2/[it(t)]2 is a relative change in the posterior variance due to the drift.The ratio rit = 1 when t = 1. For t < 1 since typically 2t < 20, the ratio is rit < 1. Thus, as longas there is non-stationarity (t < 1), the objective (14) favors the data term EN (0;I) [Lt+1( + )]",
  "Fast MAP update of posterior qt()": "As a faster alternative to propagating the posterior (6), we do MAP updates with the prior p0() =N(; 0; 20) and the approximate posterior qt() = N(; t; 2t = s220), where s 1 is ahyperparameter controlling the variance 2t of qt(). Since a fixed s may not capture the trueparameters variance, using a Bayesian method (see .4) is preferred but comes at a highcomputational cost (see Appendix E for discussion). The MAP update is given by (see Appendix I.2for derivations) finding a minimum of the following proximal objective",
  "t+1 = t(t) t(t) Lt+1(t(t)),(19)": "where is element-wise multiplication. For t = 1, we recover the ordinary SGD update, whilethe values t < 1 move the starting point of the modified SGD closer to the initialization aswell as increase the learning rate. Algorithm 1 describes the full procedure. In Appendix C wedescribe additional practical choices made for the Soft Resets algorithm. Similarly to the Bayesianapproach (15), we can do multiple updates on (16). We describe this Soft Resets Proximal algorithmin Appendix I.2 and full procedure is given in Algorithm 3.",
  "Related Work": "Plasticity loss in Neural Networks. Our model shares similarities with reset-based approaches suchas Shrink & Perturb (S&P) and L2-Init ; however, whereas we learn drift parameters fromdata, these methods do not, leaving them vulnerable to mismatch between assumed non-stationarityand the actual realized non-stationarity in the data. Continual Backprop or ReDO applyresets in a data-dependent fashion, e.g. either based on utility or whether units are dead. But theyuse hard resets, and cannot amortize the cost of removing entire features. Interpretation (12) of tconnects to the notion of parameters utility from , but this quantity is used to prevent catastrophicforgetting by decreasing learning rate for high t. Our method increases the learning rate for low tto maximize adaptability, and is not designed to prevent catastrophic forgetting. Non-stationarity. Non-stationarity arises naturally in a variety of contexts, the most obvious beingcontinual and reinforcement learning. The structure of non-stationarity may vary from problem toproblem. At one extreme, we have a piece-wise stationary setting, for example a change in thelocation of a camera generating a stream of images, or a hard update to the learners target network invalue-based deep RL algorithms. This setting has been studied extensively due to its propensity toinduce catastrophic forgetting [e.g. 31, 45, 51, 10] and plasticity loss . At the otherextreme, we can consider more gradual changes, for example due to improvements in the policy ofan RL agent or shifts in the data generating process . Further, thesescenarios might be combined, for example in continual reinforcement learning where thereward function or transition dynamics could change over time. Non-stationary online convex optimization. Non-stationary prediction has a long history in onlineconvex optimization, where several algorithms have been developed to adapt to changing data [see,e.g., 25, 8, 22, 17, 21, 18, 29]. Our approach takes an inspiration from these works by employing adrift model as, e.g., and by changing learning rate as . Further, our OU drift modelbears many similarities to the implicit drift model introduced in the update rule of (see also), where the predictive distribution is mixed with a uniform distribution to ensure the predictioncould change quickly enough if the data changes significantly, where in our case p0 plays the samerole as the uniform distribution. Bayesian approaches to non-stationary learning. A standard approach is Variational ContinualLearning , which focuses on preventing catastrophic forgetting and is an online version of BayesBy Backprop . This method does not incorporate dynamical parameter drift components. In ,the authors applied variational inference (VI) on non-stationary data, using the OU-process andBayesian forgetting, but unlike in our approach, their drift parameter is not learned. Further, in ,the authors considered an OU parameter drift model similar to ours, with an adaptable drift scalar and analytic Kalman filter updates, but is applied over the final layer weights only, while theremaining weights of the network were estimated by online SGD. In , the authors propose to dealwith non-stationarity by assuming that each parameter is a finite sum of random variables followingdifferent OU process. They derive VI updates on the posterior of these variables. Compared to thiswork, we learn drift parameters for every NN parameter rather than assuming a finite set of driftparameters. A different line of research assumes that the drift model is known and use differenttechniques to estimate the hidden state (the parameters) from the data: in , the authors use ExtendedKalman Filter to estimate state and in , they propagate the MAP estimate of the hidden statedistribution with K gradient updates on a proximal objective similar to (43), whereas in BayesianOnline Natural Gradient (BONG) , the authors use natural gradient for the variational parameters.",
  "Experiments": "Soft reset methods. There are multiple variations of our method. We call the method implementedby Algorithm 1 with 1 gradient update on the drift parameter Soft Reset, while other versions showdifferent parameter choices: Soft Reset (K = 10) is a version with 10 updates on the drift parameter,while Soft Reset (K = 10, K = 10) is the method of Algorithm 3 in Appendix I.2 with 10 updateson drift parameter, followed by 10 updates on NN parameters. Bayesian Soft Reset (K = 10,K = 10) is a method implemented by Algorithm 2 with 10 updates on drift parameter followedby 10 updates on the mean t and the variance 2t (uncertainty) for each NN parameter. Bayesianmethod performed the best overall but required higher computational complexity (see Appendix E).Unless specified, t is shared for all the parameters in each layer (separately for weight and biases). Task id 0.65 0.70 0.75 0.80 0.85",
  "Average Task Accuracy": "Perfect Soft-Reset with higher l.r. at switch Online SGDHard ResetHard Reset (only last)Soft ResetSoft Reset ( = 0.2)Soft Reset ( = 0.4)Soft Reset ( = 0.6) Soft Reset ( = 0.8) : Perfect soft-resets on data-efficient random-label MNIST. Left, Soft Reset method doesnot use higher learning rate when < 1. Right, Soft Reset increases the learning rate when < 1,see (18). The x-axis represents task id, whereas the y-axis is the average training accuracy on thetask.",
  "TTt=1 At": "The results are provided in . We observe that Soft Reset is always better than Hard Resetand most baselines despite the lack of knowledge of task boundaries. The gap is larger in the dataefficient regime. Moreover, we see that L2 Init only performs well in the memorization regime, andachieves comparable performance to Hard Reset in the data efficient one. The method L2 Init couldbe viewed as an instantiation of our Soft Reset Proximal method optimizing (16) with t = 0 at everystep, which is sub-optimal when there is similarity in the data. Bayesian Soft Reset demonstratessignificantly better performance overall, see also discussion below.",
  ": Left: the minimum encountered t for each layer on random-label MNIST and CIFAR-10.Center: the dynamics of t on the first 20 tasks on MNIST. Right: the same on CIFAR-10": "parameters (thus, more accurately adapting to non-staionarity, K = 10) leads to better performance.All variants of Soft Reset t parameters are shared for each NN layer, except for the Bayesian method.This variant is able to take advantage of a more complex per-parameter drift model, while othervariants performed considerably worse, see Appendix H.4. We hypothesize this is due to the NNparameters uncertainty estimates t which Bayesian method provide, while others do not, whichleads to a more accurate drift model estimation, since uncertainty is used in this update (10). But,this approach comes at a higher computational cost, see Appendix E. In Appendix H, we provideablations of the structure of the drift model, as well as of the impact of learning the drift parameter. Qualitative behavior of Soft Resets. For Soft Reset, we track the values of t for the first MLPlayer when trained on random-label tasks studied above (only 20 tasks), as well as the minimumencountered value of t for each layer, which highlights the maximum amount of resets. b,cshows t as a function of t, and suggests that t aggressively decreases at task boundaries (red dashedlines). The range of values of t depends on the task and on the layer, see a. Overall, tchanges more aggressively for long duration (memorization) random-label CIFAR-10 and less forshorter (data-efficient) random-label MNIST. See Appendix H.2 for more detailed results. To study the behavior of Soft Reset under input distribution non-stationarity, we consider a variant ofPermuted MNIST where each image is partitioned into patches of a given size. The non-stationarityis controlled by permuting the patches (not pixels). a shows the minimum encountered tfor each layer for different patch sizes. As the patch size increases and the problem becomes morestationary, the range of values for t is less aggressive. See Appendix H.3 for more detailed results. Impact of non-stationarity. We consider a variant of random-label MNIST where for each task, animage has either a random or a true label. The label assignment is kept fixed throughout the taskand is changed at task boundaries. We consider cases of 20%, 40% and 60% of random labels andwe control the duration of each task (number of epochs). In total, the stream contains 200 tasks.In b, we show performance of Online SGD, Hard Reset and in c, the one of SoftReset and of Bayesian Soft Reset. See Appendix D.2 for more details. The results suggest that forthe shortest duration of the tasks, the performance of all the methods is similar. As we increase theduration of each of the task (moving along the x-axis), we see that both Soft Resets variants performbetter than SGD and the gap widens as the duration increases. This implies that Soft Resets is moreeffective with infrequent data distribution changes. We also observe that Bayesian method performsbetter in all the cases, highlighting the importance of estimating uncertainty for NN parameters.",
  "Reinforcement learning": "Reinforcement learning experiments. We conduct Reinforcement Learning (RL) experiments in thehighly off-policy regime, similarly to , since in this setting loss of plasticity was observed. We ranSAC agent with default parameters from Brax on the Hopper-v5 and Humanoid-v4 GYM environments (from Brax ). To reproduce the setting from , we control the off-policyness ofthe agent by setting the off-policy ratio M such that for every 128 environment steps, we do 128Mgradient steps with batch size of 256 on the replay buffer. As baselines we consider ordinary SAC,hard-coded Hard Reset where we reset all the parameters K = 5 times throughout training (every200000 steps), while keeping the replay buffer fixed (similarly to ). We employ our Soft Resetmethod as follows. After we have collected fresh data from the environment, we do one gradientupdate on t (shared for all the parameters within each layer) with batch size of 128 on this newchunk of data and the previously collected one, i.e., two chunks of data in total. Then we initializet(t) and we employ the update rule (43) where the regularization t(t) is kept constant for all theoff-policy gradient updates on the replay buffer. See Appendix D.3 for more details. conv2_d_1 conv2_d_2 conv2_d_3 conv2_d_4 linear linear_1 0.825 0.850 0.875 0.900 0.925 0.950 0.975 Minimum t encountered (a) Permuted Patch MNIST Permuted patch size Number of epochs per task 0.2 0.4 0.6 0.8 1.0 Average accuracy over all tasks (b) Online SGD and Hard Reset performance",
  "Methods": "Online SGDSoft ResetsBayesian Soft Resets (K = 10, K = 10, per parameter)Hard Reset : Non-stationarity impact. The x-axis denotes task id, each column denotes the duration,whereas a row denotes the amount of label noise. Each color denotes the method studied. The y-axisdenotes average over 3 seeds online accuracy. 0.00.20.40.60.81.01e6 1e5+9.999e1 Replay Ratio = 1.0, for policy 0.00.20.40.60.81.01e6 0.2 0.4 0.6 0.8 1.0 Replay Ratio = 32.0, for policy 0.00.20.40.60.81.01e6 0.4 0.6 0.8 1.0 Replay Ratio = 128.0, for policy Layer # 0 : WeightsLayer # 1 : BiasesLayer # 1 : WeightsLayer # 1 : BiasesLayer # 2 : Weights 0.00.20.40.60.81.01e6 0.96 0.97 0.98 0.99 1.00 Replay Ratio = 1.0, for Q1 0.00.20.40.60.81.01e6 0.7 0.8 0.9 1.0 Replay Ratio = 32.0, for Q1 0.00.20.40.60.81.01e6 0.4 0.6 0.8 1.0 Replay Ratio = 128.0, for Q1 Layer # 0 : WeightsLayer # 1 : BiasesLayer # 1 : WeightsLayer # 1 : BiasesLayer # 2 : Weights 0.00.20.40.60.81.01e6 0.96 0.97 0.98 0.99 1.00 Replay Ratio = 1.0, for Q2 0.00.20.40.60.81.01e6 0.7 0.8 0.9 1.0 Replay Ratio = 32.0, for Q2 0.00.20.40.60.81.01e6 0.4 0.6 0.8 1.0 Replay Ratio = 128.0, for Q2 Layer # 0 : WeightsLayer # 1 : BiasesLayer # 1 : WeightsLayer # 1 : BiasesLayer # 2 : Weights : Visualization of the t dynamics for the run on Humanoid environment. Each columncorresponds to the replay ratio studied. First row denotes the t for the policy . The second and thethird rows denote the t for the two Q-functions. that using proximal constant of 0 for the policy led to the best empirical results. The range for theproximal constants was {0.1, 0.01, 0.001} and for s was {0.8, 0.9, 0.95, 0.97, 1.0}. We used p = 1for all the experiments. For each experiment, we used a 3 hours of the A100 GPU with 40 Gb ofmemory.",
  "Conclusion": "Learning efficiently on non-stationary distributions is critical to a number of applications of deepneural networks, most prominently in reinforcement learning. In this paper, we have proposed a newmethod, Soft Resets, which improves the robustness of stochastic gradient descent to nonstationaritiesin the data-generating distribution by modeling the drift in Neural Network (NN) parameters. Theproposed drift model implements soft reset mechanism where the amount of reset is controlled by thedrift parameter t. We showed that we could learn this drift parameter from the data and thereforewe could learn when and how far to reset each Neural Network parameter. We incorporate the driftmodel in the learning algorithm which improves learning in scenarios with plasticity loss. The variantof our method which models uncertainty in the parameters achieves the best performance on plasticitybenchmarks so far, highlighting the promise of the Bayesian approach. Furthermore, we found thatour approach is particularly effective either on data distributions with a lot of similarity or on slowlychanging distributions. Our findings open the door to a variety of exciting directions for future work,such as investigating the connection to continual learning and deepening our theoretical analysis ofthe proposed approach.",
  "Saurabh Kumar, Henrik Marklund, and Benjamin Van Roy. Maintaining plasticity in continuallearning via regenerative regularization, 2023": "Richard Kurle, Botond Cseke, Alexej Klushyn, Patrick van der Smagt, and Stephan Gnnemann.Continual learning with bayesian neural networks for non-stationary data. In InternationalConference on Learning Representations, 2020. Zhiqiu Lin, Jia Shi, Deepak Pathak, and Deva Ramanan. The clear benchmark: Continuallearning on real-world imagery. In Thirty-fifth Conference on Neural Information ProcessingSystems Datasets and Benchmarks Track, 2021.",
  "John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximalpolicy optimization algorithms, 2017": "Ghada Sokar, Rishabh Agarwal, Pablo Samuel Castro, and Utku Evci. The dormant neuronphenomenon in deep reinforcement learning. In Andreas Krause, Emma Brunskill, KyunghyunCho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the40th International Conference on Machine Learning, volume 202 of Proceedings of MachineLearning Research, pages 3214532168. PMLR, 2329 Jul 2023.",
  "Tim van Erven, Wouter M. Koolen, and Dirk van der Hoeven. Metagrad: Adaptation usingmultiple learning rates in online learning. Journal of Machine Learning Research, 22(161):161,2021": "Eli Verwimp, Rahaf Aljundi, Shai Ben-David, Matthias Bethge, Andrea Cossu, AlexanderGepperth, Tyler L. Hayes, Eyke Hllermeier, Christopher Kanan, Dhireesha Kudithipudi,Christoph H. Lampert, Martin Mundt, Razvan Pascanu, Adrian Popescu, Andreas S. Tolias,Joost van de Weijer, Bing Liu, Vincenzo Lomonaco, Tinne Tuytelaars, and Gido M. van de Ven.Continual learning: Applications and the road forward, 2024. Florian Wenzel, Kevin Roth, Bastiaan S. Veeling, Jakub Swi atkowski, Linh Tran, StephanMandt, Jasper Snoek, Tim Salimans, Rodolphe Jenatton, and Sebastian Nowozin. How good isthe bayes posterior in deep neural networks really?, 2020. Runtian Zhai, Stefan Schroedl, Aram Galstyan, Anoop Kumar, Greg Ver Steeg, and PradeepNatarajan. Online continual learning for progressive distribution shift (OCL-PDS): A practi-tioners perspective, 2023. Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. InProceedings of the Twentieth International Conference on International Conference on MachineLearning, ICML03, page 928935. AAAI Press, 2003.",
  "The answer NA means that the paper has no limitation while the answer No means thatthe paper has limitations, but those are not discussed in the paper": "The authors are encouraged to create a separate \"Limitations\" section in their paper. The paper should point out any strong assumptions and how robust the results are toviolations of these assumptions (e.g., independence assumptions, noiseless settings,model well-specification, asymptotic approximations only holding locally). The authorsshould reflect on how these assumptions might be violated in practice and what theimplications would be. The authors should reflect on the scope of the claims made, e.g., if the approach wasonly tested on a few datasets or with a few runs. In general, empirical results oftendepend on implicit assumptions, which should be articulated. The authors should reflect on the factors that influence the performance of the approach.For example, a facial recognition algorithm may perform poorly when image resolutionis low or images are taken in low lighting. Or a speech-to-text system might not beused reliably to provide closed captions for online lectures because it fails to handletechnical jargon.",
  "If applicable, the authors should discuss possible limitations of their approach toaddress problems of privacy and fairness": "While the authors might fear that complete honesty about limitations might be used byreviewers as grounds for rejection, a worse outcome might be that reviewers discoverlimitations that arent acknowledged in the paper. The authors should use their bestjudgment and recognize that individual actions in favor of transparency play an impor-tant role in developing norms that preserve the integrity of the community. Reviewerswill be specifically instructed to not penalize honesty concerning limitations.",
  ". Experimental Result Reproducibility": "Question: Does the paper fully disclose all the information needed to reproduce the main ex-perimental results of the paper to the extent that it affects the main claims and/or conclusionsof the paper (regardless of whether the code and data are provided or not)?Answer: [Yes]Justification: We disclose the experimental information in Experimental and Appendixsections.Guidelines: The answer NA means that the paper does not include experiments. If the paper includes experiments, a No answer to this question will not be perceivedwell by the reviewers: Making the paper reproducible is important, regardless ofwhether the code and data are provided or not.",
  "If the contribution is a dataset and/or model, the authors should describe the steps takento make their results reproducible or verifiable": "Depending on the contribution, reproducibility can be accomplished in various ways.For example, if the contribution is a novel architecture, describing the architecture fullymight suffice, or if the contribution is a specific model and empirical evaluation, it maybe necessary to either make it possible for others to replicate the model with the samedataset, or provide access to the model. In general. releasing code and data is oftenone good way to accomplish this, but reproducibility can also be provided via detailedinstructions for how to replicate the results, access to a hosted model (e.g., in the caseof a large language model), releasing of a model checkpoint, or other means that areappropriate to the research performed. While NeurIPS does not require releasing code, the conference does require all submis-sions to provide some reasonable avenue for reproducibility, which may depend on thenature of the contribution. For example(a) If the contribution is primarily a new algorithm, the paper should make it clear howto reproduce that algorithm.",
  "(b) If the contribution is primarily a new model architecture, the paper should describethe architecture clearly and fully": "(c) If the contribution is a new model (e.g., a large language model), then there shouldeither be a way to access this model for reproducing the results or a way to reproducethe model (e.g., with an open-source dataset or instructions for how to constructthe dataset). (d) We recognize that reproducibility may be tricky in some cases, in which caseauthors are welcome to describe the particular way they provide for reproducibility.In the case of closed-source models, it may be that access to the model is limited insome way (e.g., to registered users), but it should be possible for other researchersto have some path to reproducing or verifying the results.",
  "Guidelines:": "The answer NA means that the paper does not use existing assets. The authors should cite the original paper that produced the code package or dataset. The authors should state which version of the asset is used and, if possible, include aURL. The name of the license (e.g., CC-BY 4.0) should be included for each asset. For scraped data from a particular source (e.g., website), the copyright and terms ofservice of that source should be provided. If assets are released, the license, copyright information, and terms of use in thepackage should be provided. For popular datasets, paperswithcode.com/datasetshas curated licenses for some datasets. Their licensing guide can help determine thelicense of a dataset.",
  "According to the NeurIPS Code of Ethics, workers involved in data collection, curation,or other labor should be paid at least the minimum wage in the country of the datacollector": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with HumanSubjectsQuestion: Does the paper describe potential risks incurred by study participants, whethersuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)approvals (or an equivalent approval/review based on the requirements of your country orinstitution) were obtained?Answer: [NA]Justification:Guidelines:",
  "The answer NA means that the paper does not involve crowdsourcing nor research withhuman subjects": "Depending on the country in which research is conducted, IRB approval (or equivalent)may be required for any human subjects research. If you obtained IRB approval, youshould clearly state this in the paper. We recognize that the procedures for this may vary significantly between institutionsand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and theguidelines for their institution.",
  "p(xt|xs) = N(xse(ts), (1 e2(ts))20I),for any pair of times t > s. Based on this as a drift model for the parameters t (so t is the state xt)we use the conditional densityp(t+1|t) = N(tt, (1 2t )20I),": "where t = et and t 0 corresponds to the learnable discretization time step. In other words,by learning t online we equivalently learn the amount of a continuous time shift t between twoconsecutive states in the OU process. This essentially models parameter drift since e.g. if t = 1,then t = 0 and there is no time shift which means that the next state/parameter remains the sameas the previous one, i.e. t+1 = t.",
  "In this section, we discuss alternative choices of a drift model instead of (5)": "Independent mean and variance of the drift.We consider the drift model where the mean andthe variance are not connected, i.e.,p(t+1|t, t, t) = N(t+1; tt + (1 t)0; 2t ),(20)where t is the parameters controlling the mean of the distribution and t is the learnedvariance. When is fixed, this would be similar to our experiment in where we assumedknown task boundaries and we do not estimate the drift parameters but assume it as a hyperparameter., left corresponds to the case when t is a fixed parameter independent from t whereas, right corresponds to the case when t = 1 2t 0, i.e., when we use the drift model (5).We see from the results, using drift model (5) leads to a better performance. In case when t arelearned, estimating the parameters of this model will likely overfit to the noise since there is a lot ofdegrees of freedom. Shrink & Perturb .When we do not use the mean of the initialization, we can use the followingdrift modelp(t+1|t, t, t) = N(t+1; tt; 2t )Similarly to the case of (20), estimating both parameters t and t from the data will likely overfit tothe noise. Arbitrary linear model.We can use the arbitrary linear model of the formp(t+1|t, At, Bt) = N(t+1; Att; Bt),but estimating the parameters At and Bt has too many degrees of freedom and will certainly overfit. Gaussian Spike & SlabWe consider a Gaussian approximation to Spike & Slab priorp(t+1|t, t) = tp(t+1|t) + (1 t)p0(t+1),which is a mixture of two distributions - a Gaussian p(t+1|t) = N(t+1; t, 2) centered aroundthe previous parameter t and an initializing distribution p0(t+1) = N(t+1; 0, 20). This model,however, implements the mechanism of Hard reset as opposed to the soft ones. Moreover, estimatingsuch a model and incorporating it into a learning update is more challenging since the mixtureof Gaussian is not conjugate with respect to a Gaussian which will make the KL term (34) to becomputed only approximately via Monte Carlo updates.",
  "Stochastic approximation for drift parameters estimationIn practice, we use M = 1, whichleads to the stochastic approximationp(yt+1|xt+1, t(kt ) + t(kt ))N(; 0, I)d pyt+1|xt+1, t(kt ) + t(kt )(21)": "Using NN initializing distribution.In the drift model (5), we assume that the initial distributionover parameters is given by p0() = N(; 0; 20). In practice, we have access to the NN initializerpinit() = N(; 0; 20) where 0 = 0 (for most of the NNs). This means that we can replace from (10) by10 0 where 0 pinit(). This means that the term in (21) can be replaced by",
  "where we used the fact that 2t (t) = 2t 2t + (1 2t )20. Note that in (22), we only need to knowthe ratio 2": "20 rather than both of these. We will see that in .5, only this ratio is used for theunderlying algorithm. Finally, in practice, we can tie p0() to the specific initialization 0 pinit().It was observed empricially that using a specific initialization in gradient updates led to betterperformance than using samples from the initial distribution. This would imply that",
  "D.1Plasticity experiments": "TasksIn this section we provide experimental details. As plasticity tasks, we use a randomlyselected subset of size 10000 from CIFAR-10 and from MNIST. This subset is fixed for allthe tasks. Within each task, we randomly permute labels for every image; we call such problemsrandom-label classification problems. We study two regimes data efficient, where we do 400 epochson a task with a batch size of 128, and memorization, a regime where we do only 70 epochs with abatch size of 128. As the main backbone architecture, we use MLP with 4 hidden layers each havinga hidden dimension of 256 hidden units. We use ReLU activation function and do not use any batchor layer normalization. For the incoming data, we apply random crop, for MNIST to produce imagesof size 24 24 and for CIFAR-10 to produce images of size 28 28. We normalize images to bewithin range by dividing by 255. On top of that, we consider permuted MNIST task with asimilar training ragime as in we consider a subset of 10000 images, with batch size 16 andeach task is one epoch. As a backbone, we still use MLP with ReLU activation and 4 hidden layers.Moreover, we considered permuted Patch MNIST, where we permute patches, not individual pixels.In this case, we used a simple 4 layer convolutional neural network with 2 fully connected layers atthe end.",
  "where ati are the online accuracies collected on the task t via N timesteps": "BaselinesFirst baseline is Online SGD which sequentially learns over the sequence of task, with afixed learning rate. Hard Reset is the Online SGD which resets all the parameters at task boundaries.L2 init adds a regularizer || 0||2 term to each Online SGD update where the regularizationstrength is a hyperparameter. Shrink & Perturb applies the transformation t + , N(; 0, I)to each parameter before the gradient update. The hyperparameters are and . Soft Resetcorresponds to one update (10) starting from 1 using 1 Monte Carlo estimate. We alwaysuse 1 Monte Carlo estimate for updating t as we found that it worked well in practice on these tasks.The hyperparameters of the method 20 initial variance of the prior, which we set to be equal to",
  "p2 1": "N where N is the width of the hidden layer and p is a constant (hyperparameter). It always equalsto p = 0.1. On top of that the second hyperparameter is s, such that t = s0, which controls therelative decrease of the constant posterior variance. This is the hyperparameter over which we sweepover. Another hyperparameter is the learning rate for learning t. For Soft Reset Proximal, we alsohave a proximal coefficient regularization constant . Besides that, we also sweep over the learningrate for the parameter. For the Bayesian Soft Reset, we just add an additional learning rate for thevariance and we do 1 Monte Carlo sample for each ELBO update. Hyper parameters selection and evaluationFor all the experiments, we run a sweep over thehyperparameters. We select the best hyperparameters based on the smallest cumulative error (sumof all 1 ati throughout the training). We then report the mean and the standard deviation across 3seeds in all the plots. Hyperparameter ranges. Learning rate which is used to update parameters, for all the methods,is selected from {1e4, 5e4, 1e3, 5e3, 1e2, 5e2, 1e1, 5e1, 1.0}. The init parameter inL2 Init, is selected from {10.0, 1.0, 0.0, 1e1, 5e1, 1e2, 5e2, 1e3, 5e3, 1e4, 5e4, 1e5, 5e5, 1e6, 5e6, 1e7, 5e7, 1e8, 5e8, 1e9, 5e9, 1e10, }. For S&P, the shrinkparameter is selected from {1.0, 0.99999, 0.9999, 0.999, 0.99, 0.9, 0.8, 0.7, 0.5, 0.3, 0.2, 0.1}, andthe perturbation parameter is from {1e 1, 1e 2, 1e 3, 1e 4, 1e 5, 1e 6}. As noisedistribution, we use the Neural Network initial distribution. For Soft Resets, the learning ratefor t is selected from {0.5, 0.1, 0.05, 0.01, 0.005, 0.001, 0.0005, 0.0001}, the constant s is se-lected from {1.0, 0.95, 0.9, 0.8, 0.7, 0.6, 0.5, 0.3, 0.1}, the proximal cost in (41) is selected from{1.0, 0.1, 0.01}, the same is true for the proximal cost in the Bayesian method (38). On top of thatfor the Bayesian method, we always use p (see Algorithm 2) equal to p = 0.05 and s = 0.9, i.e. theposterior is always slightly smaller than the prior. Finally for the Bayesian method we had to learnthe variance with learning rate from {0.01, 0.1, 1, 10} range. In practice, we found that there is one learning rate of 0.1, which was always the best in practicefor most of the methods and only proximal Soft Resets on memorization CIFAR-10 required smallerlearning rate 0.01. This allowed us to significantly reduce the hyperparameter sweep.",
  "D.2Impact of non-stationarity experiments": "In this experiment, we consider a subset of 10000 images from MNIST (fixed throughtout allexperiment) and a sequence of tasks. Each task is constructed by assigning either a true or a randomlabel to each image from MNIST, where the probability of assignment is controlled by the experiment.The duration of each is controlled by the number of epochs with batch size of 128. As backbone weuse MLP with 4 hidden layers and 256 hidden units and ReLU activation. For all the methods, thelearning rate is 0.1. For Soft Resets, we use s = 0.9 and p = 1 and = 0.01. Bayesian method usesproximal cost = 0.01. Detailed results are given in .",
  "D.3Reinforcement learning experiments": "We conduct experiments in the RL environments. We take the canonical implementation of Soft-ActorCritic(SAC) from Brax repo in github, which uses 2 layer MLPs for both policy and Q-function.It employs ReLU activation functions for both. On top of that, it uses 2 MLP networks to parameterizeQ-function (see Brax ) for more details. To employ Soft Reset, we do the following. After wehave collected a chunk of data (128) time-steps, we do one update (10) on t starting from 1 atevery update of t, where t is shared for all the parameters within each layer of a Neural Network,separately for weights and biases. On top of that, since we have policy and value function networks,we have separate t for each of these. After the update on t, we compute t(t) and t(t), see.5. After that, we employ the proximal objective (41) with a fixed regularization targett(t). Concretely, we use the update rule (43) where for each update the gradient is estimate onthe batch of data from the replay buffer. This is not exactly the same as what we did with plasticitybenchmarks since there the update was applied to the same batch of data, multiple times. Nevertheless,we found this strategy effective and easy to implement on top of a SAC algorithm. In practice, weswept over the parameter s (similar for both, policy and the value function) which controls therelative learning rate increase in (18). Moreover, we swept over the proximal regularization constant from eqn. (41), which was different for the policy and for the value function. In practice, we found 0.0 0.2 0.4 0.6 0.8 1.0 20% of random labels | 30 epochs 20% of random labels | 100 epochs 20% of random labels | 400 epochs 0.0 0.2 0.4 0.6 0.8 1.0 40% of random labels | 30 epochs 40% of random labels | 100 epochs 40% of random labels | 400 epochs Task id 0.0 0.2 0.4 0.6 0.8 1.0 Task accuracy 60% of random labels | 30 epochs Task id Task accuracy 60% of random labels | 100 epochs Task id Task accuracy 60% of random labels | 400 epochs",
  ": Comparison of methods, computational cost, and memory requirements": "The general theoretical cost of all the proposed approaches is given in . In practice, for allthe experiments, we assume that M = 1 and M = 1. Moreover, we used K = 1 and K = 1for Soft Reset, K = 10 and K = 1 for Soft Reset with more computation. On top of that, for SoftReset proximal and all Bayesian methods, we used K = 10 and K = 10. , quantifying thecomplexity of all the methods from .",
  ": Comparison of methods, computational cost, and memory requirements for methods in": "The complexity O(2S) of Soft Resets comes from one update on drift parameter and one updateon NN parameters. The memory complexity requires storing O(L) parameters gamma (one foreach layer), parameters t with O(P) and sampled parameters for drift model update which requiresO(P). Note that as suggests, it is beneficial to spend more computational cost on optimizing gammaand on doing multiple updates on parameters. However, even the cheapest version of our method SoftResets still leads to a good performance as indicated in . The complexity of soft resets in reinforcement learning setting requires only one gradient update on after each new chunk of fresh data from the environment. In SAC, we do G gradient updates onparameters for every new chunk of data. Assuming that complexity of one gradient update in SAC isO(S), soft reset only requires doing one additional gradient update to fit parameter.",
  "Soft Reset": "Soft Reset K = 10 Soft Reset Proximal K = 10, K = 10 Bayesian Soft Reset K = 10, K = 10 Bayesian Soft Reset K = 10, K = 10, per parameter 0.80 0.85 0.90 0.95 Random Label CIFAR-10 -- memorization : Compute-performance tradeoff. The x-axis indicates the method going from the cheapest(left) to the most expensive (right). See for complexity analysis. The y-axis is the averageperformance on all the tasks across the stream.",
  "FSensitivity analysis": "We study the sensitivity of Soft Resets where is defined per layer when trained on random-labelMNIST (data efficient). We fix the learning rate to = 0.1. We study the sensitivity of learning ratefor the drift parameter, , as well as p initial prior standard deviation rescaling, and s posteriorstandard deviation rescaling parameter. On top of that, we conduct the sensitivity analysis of L2 Init and Shrink&Perturb methods.The x-axis of each plot denotes one of the studied hyperparameters, whereas y-axis is the averageperformance across all the tasks (see Experiments section for tasks definition). The standard deviationis reported over 3 random seeds. A color indicates a second hyperparameter which is studied, ifavailable. In the title of each plot, we write hyperparameters which are fixed. The analysis is providedin for Soft Resets and in for the baselines. The most important parameter is the learning rate of the drift model . For each method, thereexists a good value of this parameter and performance is sensitive to it. This makes sense since thisparameter directly impacts how we learn the drift model. The performance of Soft Resets is robust with respect to the posterior standard deviation scaling sparameter as long as it is s 0.5. For s < 0.5, the performance degrades. This parameter is definedfrom t = s0 and affects relative increase in learning rate given by1",
  "+(12)/s2) which could beill-behaved for small s": "We also study the sensitivity of the baseline methods. We find that L2 Init is very sensitive to theparameter , which is a penalty term for || 0||2. In fact, , left shows that there is onlyone good value of this parameter which works. Shrink&Perturb is very sensitive to the shrinkparameter . Similar to L2 Init, there is only one value which works, 0.9999 while values 0.999 andvalues 0.99999 lead to bad performance. This method however, is not very sensitive to the perturbparameter provided that 0.001. Compared to the baselines, our method is more robust to the hyperparameters choice. Below, wealso add sensitivity analysis for other method variants. shows sensitivity of Soft Resets,K = 10, shows sensitivity of Soft Resets, K = 10, K = 10, shows sensitivityof Bayesian Soft Resets, K = 10, K = 10 with t per layer, shows sensitivity ofBayesian Soft Resets, K = 10, K = 10 with t per parameter. 0.20.40.60.81.0 Posterior standard deviation scaling `s` 0.1 0.2 0.3 0.4 0.5 0.6 Average accuracy Sensitivity Soft-Reset (MNIST). Prior standard deviation scaling p = 0.05 Learning rate 0.00010.00050.0010.0050.010.050.10.5 0.20.40.60.81.0 Posterior standard deviation scaling `s` 0.1 0.2 0.3 0.4 0.5 0.6 Average accuracy Sensitivity Soft-Reset (MNIST). Drift learning rate = 0.1 Prior std scaling p 0.010.050.1 : Soft Reset, sensitivity analysis of performance with respect to the hyperparameters ondata-efficient random-label MNIST. The x-axis denotes the studied hyperparameter, whereas they-axis denotes the average performance across the tasks. The standard deviation is computed over3 random seeds. The color indicates additional studied hyperparameter. (Left) shows sensitivityanalysis where the x-axis is the posterior standard deviation scaling s and the color indicates thedrift model learning rate . (Right) shows sensitivity of Soft Reset where the x-axis is the posteriorstandard deviation scaling s and the color indicates initial prior standard deviation scaling p. L2 term regularization cost `s` 0.0 0.2 0.4 0.6 0.8 Average accuracy Sensitivity L2 Init (MNIST). Learning rate = 0.1 Perturb parameter ` ` 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45 Average accuracy Sensitivity Shrink and Perturb (MNIST). Learning rate = 0.1 Shrink parameter 0.90.990.9990.99990.999991.0 : L2 Init and Shrink&Perturb sensitivity analysis of performance with respect to thehyperparameters on data-efficient random-label MNIST. The x-axis denotes the studied hyperparame-ter, whereas the y-axis denotes the average performance across the tasks. The standard deviation iscomputed over 3 random seeds. The color optionally indicates additional studied hyperparameter.(Left) shows sensitivity of L2 Init with respect to the L2 penalty regularization cost applied to|| 0||2 term. We do not use an additional hyperparameter, therefore there is only one color.(Right) shows sensitivity of Shrink&Perturb method where the x-axis is the perturb parameter while the color indicates the shrink parameter .",
  "where t can now also be interpreted as the learning rate": "0.20.40.60.81.0 Posterior standard deviation scaling `s` 0.1 0.2 0.3 0.4 0.5 0.6 Average accuracy Sensitivity Soft-Reset, K = 10 (MNIST). Prior standard deviation scaling p = 0.05 Learning rate 0.00010.00050.0010.0050.010.050.10.5 0.20.40.60.81.0 Posterior standard deviation scaling `s` 0.1 0.2 0.3 0.4 0.5 0.6 Average accuracy Sensitivity Soft-Reset K = 10 (MNIST). Drift learning rate = 0.01 Prior std scaling p 0.010.050.1 : Soft Reset, K = 10, sensitivity analysis of performance with respect to the hyperparam-eters on data-efficient random-label MNIST. The x-axis denotes the studied hyperparameter, whereasthe y-axis denotes the average performance across the tasks. The standard deviation is computedover 3 random seeds. The color indicates additional studied hyperparameter. (Left) shows sensitivityanalysis where the x-axis is the posterior standard deviation scaling s and the color indicates the driftmodel learning rate . (Right) shows sensitivity analysis where the x-axis is the posterior standarddeviation scaling s and the color indicates initial prior standard deviation scaling p. 0.20.40.60.81.0 Posterior standard deviation scaling `s` 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Average accuracy Sensitivity Soft-Reset, K = 10, K heta = 10 (MNIST). Prior standard deviation scaling p = 0.05 Learning rate 0.00010.00050.0010.0050.01 0.20.40.60.81.0 Posterior standard deviation scaling `s` 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Average accuracy Sensitivity Soft-Reset, K = 10, K heta = 10 (MNIST). Drift learning rate = 0.0001 Prior std scaling p 0.010.050.10.51.0 : Soft Reset, K = 10, K = 10, sensitivity analysis of performance with respectto the hyperparameters on data-efficient random-label MNIST. The x-axis denotes the studiedhyperparameter, whereas the y-axis denotes the average performance across the tasks. The standarddeviation is computed over 3 random seeds. The color indicates additional studied hyperparameter.(Left) shows sensitivity analysis where the x-axis is the posterior standard deviation scaling s and thecolor indicates the drift model learning rate . (Right) shows sensitivity analysis where the x-axisis the posterior standard deviation scaling s and the color indicates initial prior standard deviationscaling p. Prior standard deviation scaling `p` 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Average accuracy Sensitivity Bayesian Soft-Reset, per layer (MNIST). KL cost = 0.01 Learning rate 0.00010.00050.0010.0050.010.1 KL cost, 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Average accuracy Sensitivity Bayesian Soft-Reset, per layer (MNIST). Initial prior rescaling p = 0.05 Learning rate 0.00010.00050.0010.0050.010.1 : Bayesian Soft Reset, K = 10, K = 10 with t per layer, sensitivity analysis ofperformance with respect to the hyperparameters on data-efficient random-label MNIST. The x-axisdenotes the studied hyperparameter, whereas the y-axis denotes the average performance across thetasks. The standard deviation is computed over 3 random seeds. The color indicates additional studiedhyperparameter. (Left) shows sensitivity analysis where the x-axis is the prior standard deviationinitial scaling p and the color indicates the drift model learning rate . (Right) shows sensitivityanalysis where the x-axis is the KL divergence coefficient while the color indicates the learningrate . Prior standard deviation scaling `p` 0.2 0.4 0.6 0.8 1.0 Average accuracy Sensitivity Bayesian Soft-Reset, per parameter (MNIST). KL cost = 0.01 Learning rate 0.00010.00050.0010.0050.01 KL cost, 0.0 0.2 0.4 0.6 0.8 1.0 Average accuracy Sensitivity Bayesian Soft-Reset, per parameter (MNIST). Initial prior rescaling p = 0.05 Learning rate 0.00010.00050.0010.0050.01 : Bayesian Soft Reset, K = 10, K = 10 with t per parameter, sensitivity analysis ofperformance with respect to the hyperparameters on data-efficient random-label MNIST. The x-axisdenotes the studied hyperparameter, whereas the y-axis denotes the average performance across thetasks. The standard deviation is computed over 3 random seeds. The color indicates additional studiedhyperparameter. (Left) shows sensitivity analysis where the x-axis is the prior standard deviationinitial scaling p and the color indicates the drift model learning rate . (Right) shows sensitivityanalysis where the x-axis is the KL divergence coefficient while the color indicates the learningrate . Task id 0.0 0.2 0.4 0.6 0.8",
  "H.1Perfect Soft Resets": "To understand the impact of drift model (5), we study the data efficient random-label MNIST settingwhere task boundaries are known. We run Online SGD, Hard Reset which resets all parameters at taskboundaries, and Hard Reset (only last) which resets only the last layer. We use Soft Reset method (19)where t = 1 all the time and becomes t = t (with manually chosen t) at task boundaries. Weconsider constant learning rate t(t) and increasing learning rate (18) at task boundary for SoftReset. On top of that, we run Soft Reset method unaware of task boundaries which learns t. Wereport Average training task accuracy metric in . See Appendix D.1 for details. The resultssuggest that with the appropriate choice of t, Soft Reset is much more efficient than Hard Reset andthe effect becomes stronger if the learning rate t(t) increases. We also see that Soft Reset couldlearn an appropriate t without the knowledge of task boundary.",
  "H.3Qualitative Behaviour on Soft Resets on permuted patches of MNIST": "We consider a version of permuted MNIST where instead of permuting all the pixels, we permutepatches of pixels with a patch size varying from 1 to 14. The patch size of 1 corresponds to permututedMNIST and therefore the most non-stationary case, while patch size of 14 corresponds to least non-stationary case. We use a convolutional Neural Network in this case. In , we report thebehavior of for different convolutional and fully connected layers on first few tasks.",
  "H.4Bayesian method is better than non-Bayesian": "As discussed in , we found that in practice Soft Reset and Soft Reset Proximal where islearned per-parameter, did not perform well on the plasticity benchmarks. However, the Bayesianvariant described in Section I.1, actually benefited from specifying for every parameter in NeuralNetwork. We report these additional results in . We see that the non Bayesian variantswhere t is specified per parameter, do not perform well. The fact that the Bayesian method performsbetter here suggests that it is important to have a good uncertainty estimate 2t for the update (10)on t. When, however, we regularize t to be shared across all parameters within each layer, thisintroduces useful inductive bias which mitigates the lack of uncertainty estimation in the parameters.This is because for non-Bayesian methods, we assume that the uncertainty is fixed, given by ahyperparameter assumption which would not always hold in practice.",
  "H.5Qualitative behavior of soft resets": "In this section, we zoom-in in the data-efficient experiment on random-label MNIST. We use SoftReset Proximal ( per layer) method with separate for layer (different for each weight and for eachbias) and run it for 20 tasks on random-label MNIST. In we show the online accuracy as welearn over this sequence of tasks. In , we visualize the dynamics of parameters for eachlayer. First of all, we see that t seems to accurately capture the task boundaries. Second, we see thatthe amount by which each t changes depends on the parameter type weights versus biases, andit depends on the layer. The architecture in this setting starts form linear and goes up to linear4,which represent the 4 MLP hidden layers with a last layer linear4.",
  ": Visualization of accuracy when trained on data efficient random-label MNIST task. Thedashed red lines correspond to a task boundary": "0.00014 0.00012 0.00010 0.00008 0.00006 0.00004 0.00002 0.00000 +1 Per Layer linear b 0.00006 0.00004 0.00002 0.00000 0.00002 0.00004 0.00006 0.00008 0.00010 +9.999e1 Per Layer linear_1 b 0.9992 0.9994 0.9996 0.9998 1.0000 Per Layer linear_2 b 0.99982 0.99984 0.99986 0.99988 0.99990 0.99992 0.99994 0.99996 Per Layer linear_3 b 4.0 4.5 5.0 5.5 6.0 1e5+9.999e1 Per Layer linear_4 b 0.9970 0.9975 0.9980 0.9985 0.9990 0.9995 1.0000 Per Layer linear w 0.9970 0.9975 0.9980 0.9985 0.9990 0.9995 1.0000 Per Layer linear_1 w 0.9975 0.9980 0.9985 0.9990 0.9995 1.0000 Per Layer linear_2 w 0.9965 0.9970 0.9975 0.9980 0.9985 0.9990 0.9995 1.0000 Per Layer linear_3 w 0.9965 0.9970 0.9975 0.9980 0.9985 0.9990 0.9995 1.0000 Per Layer linear_4 w",
  "H.6Impact of specific initialization": "In this section, we study the impact of using specific initialization 0 pinit() in p0() as discussedin Appendix C. Using the specific initialization in Soft Resets leads to fixing the mean of the p0() tobe 0, see (23). This, in turn, leads to the predictive distribution (24). In case when we are not usingspecific initialization 0, the mean of p0() is 0 and the predictive distribution is given by (22). Tounderstand the impact of this design decision, we conduct an experiment on random label MNISTwith Soft Reset, where we either use the specific initialization or not. For each of the variants, wedo a hyperparameters sweep. The results are given in . We see that both variants performsimilarly.",
  "In this section, we provide a Bayesian Neural Network algorithm to learn the distributions of NNparameters when there is a drift in the data distribution. Moreover, we provide a MAP-like inference": "0.3 0.4 0.5 0.6 Soft Reset -- no specific init in p0( ) Soft Reset -- with specific init in p0( ) : Impact of specific initialization 0 as a mean of p0() in Soft Resets. The x-axis representstask id. The y-axis represents the average task accuracy with standard deviation computed over 3random seeds. The task is random label MNIST data efficient.",
  "i=1N(i; i, 2i ); = (1, . . . , D)},(28)": "which is the family of Gaussian mean-field distributions over parameters RD (separate Gaussianper parameter). For simplicity of notation, we omit the index i. Let t = (1, . . . , t) be thehistory of observed parameters of the drift model and St = {(x1, y1), . . . , (xt, yt)} be the historyof observed data. We denote by qt() qt(|St, t1) Q the Gaussian approximate posterior attime t with mean t and variance 2t for every parameter. The approximate predictive look-aheadprior is given by",
  "(35)": "Since the posterior variance of NN parameters may become small, the optimization of (35) maybecome numerically unstable due to division by 2t,i(t). It was shown that using smalltemperature on the prior led to better empirical results when using Bayesian Neural Networks, aphenomenon known as cold posterior. Here, we define a temperature per-parameter, i.e., t,i > 0 forevery time-step t, such that the objective above becomes",
  "2t 2t,i+(12t )20 ,": "which represents the relative change in the posterior variance due to the drift. In the exact stationarycase, when t = 1, this ratio is rt,i = 1 while for t < 1 , since typically 2t < 20, we have rt,i < 1.This means that in the non-stationary case, the strength of the regularization in (38) in favor of thedata term EN (0;I) [Lt+1( + )], allowing the optimization to respond faster to the change in thedata distribution. In practice, this data term is approximated via Monte-Carlo, i.e.",
  "Neural Network initial variance for every parameter 20 coming from standard NN library": "NN initializer pinit()Proximal cost 0.Initial prior variance rescaling p .Initial posterior variance rescaling f .Learning rate for the mean and for the standard deviation Number of gradient updates K to be applied on and Number of Monte-Carlo samples M for estimating and in (39)Number of gradient updates K on drift parameter t in (10)Number of Monte-Carlo samples M to estimate t in (11)Learning rate for drift parameterInitial drift parameters 0 = 1 for every iteration.Initialization:Initialize NN parameters 0 pinit()Initialize prior distribution p0() = N(; 0; p220) to be used for drift model (5).",
  "I.2Modified SGD with drift model": "Instead of propagating the posterior (6), we do MAP updates on (4) with the prior p0() =N(; 0; 20) and the posterior qt() = N(; t; s220), where s 1 is hyperparameter control-ling the variance 2t of the posterior qt(). Since fixed s may not capture the true parameters variance,using Bayesian method (see Appendix I.1) is preferred but comes at a high computational cost.Instead of Bayesian update (33), we consider maximum a-posteriori (MAP) update",
  "Input: Data-stream ST = {(xt, yt)})Tt=1": "Neural Network (NN) initializing distribution pinit() and specific initialization 0 pinit()Learning rate t for parameters and for drift parametersNumber of gradient updates K on drift parameter tNumber of gradient updates K on NN parametersProximal term cost 0NN initial standard deviation (STD) scaling p 1 (see (23)) and ratio s =tp0 .",
  "KToy illustrative example for SGD underperformance in the non-stationaryregime": "Illustrative example of SGD on a non-stationary stream.We consider a toy problem of tracking achanging mean value. Let the observations in the stream St follow yt = t + , where N(0, 1), = 0.01. Every 50 timesteps the mean t switches from 2 to 2. We fit a 3-layer MLP with layersizes (10, 5, 1) and ReLU activations, using SGD with two different choices for the learning rate: = 0.05 and = 0.15. Moreover, given that we know when a switch of the mean happens, wereset (or not reset) all the parameters at every switch as we run SGD. Only during the reset, we usedifferent learning rate = 0.05 or = 0.15. Using higher learning rate during reset allows SGD tolearn faster from new data. We also ran SGD with = 0.05 and = 0.15, where the higher learningrate is used during task switch but we do not reset the parameters. We found that it performed thesame as SGD with = 0.05, which highlights the benefit of reset. Number of timesteps Predicted mean True dataSGD (= 0.05) SGD (= 0.05) + reset (= 0.05) SGD (= 0.15) SGD (= 0.05) + reset (= 0.15)",
  ": Non-stationary mean tracking with SGD": "We report the predicted mean t for all SGD variants in . We see that after the first switchof the mean, the SGD without reset takes more time to learn the new mean compared to the versionwith parameters reset. Increasing the learning rate speeds up the adaptation to new data, but it stillremains slower during the mean change from 2 to 2 compared to the version that resets parameters.This example highlights that resets could be highly beneficial for improving the performance of SGDwhich could be slowed down by the implicit regularization towards the previous parameters t andthe impact of the regularization strength induced by the learning rate.",
  "2g(t; t)|| f(t; t)||2(45)": "The choice of f(t; t) and g(t; t) affects the behavior of the estimate t+1 from (45) and ulti-mately depends on the problem in hand. The objective function of the form (45) was studied incontext of online convex optimization in ,, where the underlying algorithms estimated thedeterministic drift model online. These worked demonstrated improved regret bounds depending onmodel estimation errors. This approach could also be used together with a Bayesian Neural Network(BNN)."
}