{
  "Abstract": "While 3D object bounding box (bbox) representation has been widely used inautonomous driving perception, it lacks the ability to capture the precise details ofan objects intrinsic geometry. Recently, occupancy has emerged as a promisingalternative for 3D scene perception. However, constructing a high-resolution occu-pancy map remains infeasible for large scenes due to computational constraints.Recognizing that foreground objects only occupy a small portion of the scene, weintroduce object-centric occupancy as a supplement to object bboxes. This repre-sentation not only provides intricate details for detected objects but also enableshigher voxel resolution in practical applications. We advance the development ofobject-centric occupancy perception from both data and algorithm perspectives. Onthe data side, we construct the first object-centric occupancy dataset from scratchusing an automated pipeline. From the algorithmic standpoint, we introduce anovel object-centric occupancy completion network equipped with an implicitshape decoder that manages dynamic-size occupancy generation. This networkaccurately predicts the complete object-centric occupancy volume for inaccurateobject proposals by leveraging temporal information from long sequences. Ourmethod demonstrates robust performance in completing object shapes under noisydetection and tracking conditions. Additionally, we show that our occupancyfeatures significantly enhance the detection results of state-of-the-art 3D objectdetectors, especially for incomplete or distant objects in the Waymo Open Dataset.",
  ": Bounding Box vs. Occupancy. Oc-cupancy can better represent the cranes shapethan the bounding box": "In autonomous driving, accurate and robust 3Dscene perception is crucial for safe and efficientnavigation. Conventional perception systems pri-marily adopt 3D object bounding boxes as the per-ception representation . However, thelimitations of 3D bounding boxes (bboxes) are be-coming increasingly pronounced as the demandsfor perception accuracy continue to escalate. Sincea 3D bbox is essentially a cuboid that encapsu-lates the object, it fails to capture the precise de-tails of the objects shape, particularly for objectswith irregular geometries. As shown in (a),",
  "arXiv:2412.05154v1 [cs.CV] 6 Dec 2024": "the crane is perfectly enclosed by a 3D boundingbox. However, its boom, which is a long protrusion relative to the cab, results in a significantamount of unoccupied space within the 3D bounding box. Nevertheless, algorithms that em-ploy 3D bounding boxes as a perception result inherently assume that the space within the bboxis fully occupied, thereby deeming the space enclosed by the 3D bounding box as impassable.Consequently, when addressing complex and irregularly shaped objects, bounding boxes are inade-quate in providing fine-grained perceptual outcomes, which can consequently impact the precision ofsubsequent tasks, such as planning and control.",
  ": Generating occupancy from LiDARscans is non-trivial for foreground objects dueto sparsity and detection drifts": "Considering the limitation of 3D bounding boxes,occupancy representation has emerged as a promis-ing alternative for 3D scene perception .As shown in (b), occupancy representationdiscretizes the 3D space into a volumetric grid,wherein each voxel is classified as occupied or free.Compared to 3D bboxes, this representation moreeffectively captures irregular shapes, thereby en-hancing accurate planning and control. Real-timescene-level occupancy generation from sensor in-puts is non-trivial, presenting challenges not onlyfor vision-centric inputs due to the absence of depthsensing, but also for LiDAR sensors because of thesparsity of each LiDAR scan (see (b)). Thus,existing approaches leverage neural networks to predict occupancy in a data-driven manner.Due to computational constraints, these methods typically produce low-resolution occupancy gridsfor large scene perception (e.g., 200 200 16 with a voxel size of (0.4m)3 in ) or requiresintensive training for implicit representation , which remains insufficient and inefficiency forpractical use. Another feasible way to build occupancy grids is directly voxelizing the LiDAR point cloud. Toalleviate the sparsity problem ( (b)), aggregating multiple LiDAR scans is an effective way forbackground. However, for foreground objects, the occupancy construction becomes challenging as itrequires accurate detection and tracking to compensate for their potential movements. In real-timeapplications, 3D detection is prone to drift, and tracking algorithms may lose or mismatch objects,resulting in inaccurate tracklets. As illustrated in (d), directly aggregating point clouds frominaccurate tracklets can lead to extremely blurry shape representations. Such inaccuracies accumulateover time, progressively degrading the reliability of the shape representation. Based on these observations, we introduce object-centric occupancy as a supplement to objectbounding boxes, providing a more detailed structural description for objects intrinsic geometry. Incontrast to its scene-level counterpart, object-centric occupancy exclusively focuses on foregroundobjects, allowing for higher voxel resolutions even in large scenes. To encourage the advancementof object-centric occupancy perception, we present a novel object-centric occupancy dataset, whichis constructed from scratch using an automated pipeline. We then propose a robust sequence-basedoccupancy completion network. By aggregating temporal information from history observations usingattention, our network effectively handles detection drifts and accurately predicts the complete object-centric occupancy. Furthermore, our method employs an implicit shape decoder to generate dynamic-size occupancy and reduce training costs through queries on selective position. Our experimentsunder Waymo Open Dataset (WOD) reveal that our method exhibits robust performance incompleting object shapes even under noisy detection and tracking conditions. With the implicitshape descriptor, we demonstrate that performance of state-of-the-art 3D object detectors can also beimproved, particularly for incomplete or distant objects.",
  "D Occupancy Prediction and Shape Completion": "3D semantic occupancy prediction (SOP) has become a critical task in vision-centric autonomous driving, where algorithms primarily perceive the environment using RGB cameras.These vision-centric models typically discretize the surrounding environment into a volumetric grid and predict the occupancy status of each voxel by properly aggregating information from single-/multi-view RGB image(s). For occupied voxels, the models additionally predict the correspondingsemantic class. Another similar task is 3D semantic scene completion (SSC) . Unlike SOP,which only needs to predict the occupancy for visible regions, SSC additionally requires the model todetermine the occupancy status at unseen regions. It is worth noting that although SOP and SSC arepredominantly associated with vision-centric approaches, they are also applicable to sparse LiDARor multi-modal inputs . Existing SOP and SSC methods primarily focus on scene-leveloccupancy, while our work concentrates on object-centric occupancy for better shape representation.Besides, semantics for occupied voxels are not necessary for our setup, as our primary concern is thegeometric structure within an object bbox, whose class label is given. Unlike our occupancy-basedmethod, a majority of shape completion approaches focus on surface reconstruction of objects .However, surface-based representations are less suitable for autonomous driving perception, as theydo not directly support tasks like collision avoidance.",
  "D Object Detection with Long Sequences": "As demonstrated in , a single-frame detector can directly benefit from temporal infor-mation by taking the concatenation of several history frames as inputs. Although such a simplemulti-frame strategy shows noticeable improvements, the performance becomes easily saturated asthe number of input frames increases (e.g.,2 4 frames). Besides, the computational cost growssignificantly as the number of input frames increases, which is not ideal for real-time applications.To remedy this issue, employs a residual point probing strategy to remove redundant points inthe multi-frame inputs. Besides, opts for an object-centric approach that conducts the temporalaggregation at the level of tracklet proposals, which allows for longer sequences (i.e.,16 frames) to beprocessed with lower computational costs. Furthermore, demonstrate human-level detectionperformance by leveraging past and future information of entire object tracks. However, they arelimited to offline applications since they require access to future frames. More recently, MoDARimproves detection by augmenting LiDAR point clouds using motion forecasting outputs, whichconsist of future trajectory points predicted from long history subsequences (i.e., 90 frames). Com-pared to MoDAR, our method is able to aggregate all the historical information via the compactimplicit latent embeddings. Besides, our method goes beyond detection by predicting the completeobject-centric occupancy for each proposal.",
  "Implicit Neural Representation": "Implicit shape representation represents 3D shapes with a continuous function. Compared totraditional explicit representations (e.g., point clouds, meshes, volumetric grids), implicit representa-tions can describe shape structure in continuous space, and are more memory-efficient. Rather thanmanually designing the implicit function, recent works propose to learn the implicitfunction from data. Specifically, they employ neural networks to approximate the implicit function,which can be trained in a data-driven manner. These neural functions typically take continuous 3Dcoordinates as inputs and output the related shape attributes at the queried positions (e.g., color,density, signed distance, etc.) For example, learns a signed distance function (SDF) fromhigh-quality 3D meshes for better shape representation. While learns a neural radiance field frommulti-view images to achieve better view synthesis. Our implicit shape decoder shares similaritieswith DeepSDF introduced in . However, instead of predicting the signed distance at a queriedposition, we predict its occupancy probability.",
  "Object-Centric vs. Scene-Level Occupancy": "Occupancy representation discretizes the 3D space into a volumetric grid, wherein each voxel isclassified as occupied or free. Given our objective is to more accurately represent complex objectstructures, background elements despite their extensive coverage are not our primary focus.Therefore, we define object-centric occupancy as a 3D grid centered on the objects coordinate. Asdifferent object instances vary in size, their corresponding occupancy resolutions also vary, if givena predefined voxel size. In contrast, existing scene-level occupancy datasets use anoccupancy volume to represent an entire scene centered at the ego vehicles coordinate system. Sinceall scenes are bounded by a fixed range, the occupancy resolution remains constant when the voxelsize is given. One convenient way to construct our object-centric occupancy dataset is to extract object occupancyfrom existing ego-centric datasets using object detection annotations. However, this approach has twosignificant limitations. Firstly, as scene-level occupancy is centered at the ego vehicles coordinatesystem, the extracted object voxels may appear jagged due to coordinate misalignments, as shown in. Transforming these jagged object voxels to the objects coordinate system inevitably leadsto information loss. Secondly, existing scene-level datasets have adopted a large voxel size (e.g.,(0.4m)3) to save computational costs for large scenes. However, this voxel size is inadequate forcapturing the fine-grained details of objects, especially for smaller objects. For this reason, weintroduce an automated pipeline to annotate the object-centric occupancy dataset from scratch.",
  ": Occupancy grids defined in the ego-vehicle (left) and object-centric (right) coordi-nate systems. The object shape is jagged in theego-vehicle occupancy grid due to coordinatemisalignment": "Similar to previous scene-level approaches , we can construct object-centric occupancyannotations based on any existing 3D detectiondatasets . However, instead of generatingan occupancy volume for the entire scene, we cre-ate it for each annotated object instance under itslocal coordinate system. For each designated object, we gather points withinits annotated bounding boxes over time, transformthese points from sensor coordinates to the bound-ing box coordinates and aggregate them into a densepoint cloud. After that,we directly voxelize it un-der the local object coordinate system, yielding theobject-centric occupancy grid. Additionally, we perform occlusion reasoning toclassify unoccupied voxels as either free or unobserved by comparing each voxel centers rangevalue to raw range images from LiDAR scans. This strategy is significantly faster than traditionalray-casting . After finishing the annotation, every tracked object within the detection dataset isassociated with an object-centric occupancy grid. This grid is centered at the local coordinate and hasa size determined by the objects size and the desired resolution. Please refer to Appendix A.1 formore details about the dataset generation pipeline.",
  "Sequence-based Occupancy Completion Network": "illustrates the architecture of our object-centric occupancy completion network. Our methodutilizes an object sequence as input, formulated as a {(Pt, Bt)}Tt=0, where Pi RN3 is the pointcloud at timestamp t and Bt R7 is the corresponding noisy 3D object bbox. The input sequence canbe generated using off-the-shelf 3D detection and tracking systems. Our main objective isto predict the complete object-centric occupancy grid for each proposal in the trajectory. Additionally,we use the occupancy features to further refine the detection results of the 3D detector.",
  "Dynamic-Size Occupancy Generation via Implicit Decoding": "Our network primarily focuses on Regions of Interest (RoIs) defined by object proposals. Giventhat different objects have varying sizes and proposals for the same object may also vary dueto inaccurate detection, efficiently decoding the occupancy volume from feature space for eachdynamic-sized proposal poses a significant challenge. Conventional scene-level occupancy perceptionapproaches typically apply dense convolution layers to decode the occupancy volume.However, this strategy encounters several limitations in the context of dynamic-size object-centricoccupancy. First, since we require feature interaction across timestamps, the features for differentproposals are better if in the same size. However, decoding a dynamic-sized volume from a fixed-sizefeature map is non-trivial for convolution. Secondly, the dense convolution operation becomescomputationally expensive for high occupancy resolution. One alternative is sparse convolution , however, it cannot fill the unoccupied voxels with the correct occupancy status. Drawing inspiration from the recent success of implicit shape representations , we tackle theaforementioned challenge through an implicit shape decoder D. This decoder is capable of predictingthe occupancy status of any position within the RoI based on its corresponding latent embedding.Specifically, the decoder takes in the latent embedding z along with a query position q R3 at theRoI coordinate, and subsequently outputs the occupancy probability at q:",
  "p = D(z, q),(1)": "where D : Re R3 R is implemented as an MLP. The latent z Re is a fixed-lengthembedding depicting the geometrics within the RoI. The latent z and query position q are concatenatedbefore being sent to D. Besides enabling flexible feature interaction and efficient computation, theimplicit shape decoder also allows for easier occupancy interpolation or extrapolation with continuousquery positions.",
  "Dual Branch RoI Encoding": "Having the implicit shape decoder in place, the next step is to obtain a latent embedding z thataccurately represents the complete object shape within the RoI. To achieve accurate shape completionand detection, two information sources are essential: 1) the partial geometric structure of eachRoI, and 2) the motion information of the object over time. To make different RoIs share thesame embedding space, we encode each RoI under a canonical local coordinate system. However,transforming the RoI to the local coordinate system inevitably loses the global motion dynamics ofthe object, reducing the networks ability to handle detection drifts. Therefore, we encode each RoIusing two separate encoders: Elocal that encodes the RoI in the local coordinate system and Eglobal inthe global coordinate system. Specifically, we employ the sparse instance recognition (SIR) module in FSD as our RoI encoder.SIR is a PointNet-based network characterized by multiple per-point MLPs and max-poolinglayers. Drawing inspiration from LiDAR R-CNN , we additionally enhance the point cloudwith size information of the RoI. This augmentation involves decorating each point within the RoI with its offset relative to the boundary of the RoI, enabling it to be box-awared. All points aretransformed to the local coordinate system defined by the detected bounding box before being sent toElocal. Conversely, Eglobal directly encodes the RoI in the global coordinate system. For a given objectsequence {(Pt, Bt)}Tt=0, we separately encode each RoI using Elocal and Eglobal, yielding two sets oflatent embeddings Zl and Zg RT e.",
  "Feature Enhancement via Temporal Aggregation": "After RoI encoding, we use the motion information from Zg to enrich the local shape latent embed-dings Zl. First, we employ a transformer mechanism to Zg to enable feature interaction acrosstimestamps. To ensure online applications, we restrict each RoI feature in Zg to only attend to itshistorical features, thereby preventing information leakage from future timestamps:",
  "Occupancy Completion and Detection Refinement": "Given the final latent embeddings Z, we can predict the complete object-centric occupancy volumefor each proposal by querying the implicit shape decoder D at different positions. During training,we randomly sample a fixed number of query positions within each RoI to compute the loss. Duringinference, we query the decoder at all voxel centers within the RoI to obtain the complete occupancyvolume. Since Z now encodes information of complete object shapes, it provides more geometricinformation for better detection. To retain motion information, we additionally fuse Z with the globalRoI feature Zg:",
  "Implementation Details": "Position Query Sampling. During training, we randomly sample 1024 voxel centers and corre-sponding occupancy statuses from each annotated occupancy as the position queries. To ensurethe occupancy prediction is not biased, we adopt a balanced sampling strategy, where 512 pointsare sampled from the occupied voxels and 512 from the free voxels. For an RoI that matches aground-truth (GT) bbox, we transform the corresponding query set to its coordinate system usingthe relative pose between the RoI and the bbox. These position queries are then sent to the implicit decoder D to compute the occupancy loss. During the inference, we generate the dense occupancyvolume for each RoI by querying the decoder at all voxel centers within the RoI under the localcoordinate system. Network Training. In order to generate inputs for our network, we first use FSD and Center-Point as our base detectors to generate object proposals. Then we leverage ImmortalTracker to associate the detection results into object tracklet proposals. We use the generated object trackletproposals in addition to GT tracklets as our training sequences. To facilitate parallel training, weregularize each tracklet to a fixed length of 32 frames via padding or cutting during training. Toachieve faster convergence, we compute the loss at all timestamps within each tracklet instead of onlyat the last one. During the inference, the model outputs the refined box at timestamp t by looking atall the history boxes.",
  "Dataset and Evaluation Metrics": "Dataset. Our method is evaluated on the Waymo Open Dataset (WOD). We use the officialtraining set, comprising 798 sequences for training, and 202 sequences for evaluation. We apply ourautomatic pipeline on WOD to construct the object-centric occupancy annotations with the voxelsize set to 0.2m. All experiments are conducted on rigid objects (i.e., vehicles) to ensure accurateevaluation of shape completion using our annotated ground-truths.",
  ": Illustration for occupancy evaluation": "Evaluation Metrics. For shape completion, weadopt the widely-used intersection-over-union (IoU)to evaluate the quality of the predicted occupancyvolumes. Due to the object-centric nature of ourmethod, we cannot calculate the IoU directly be-tween the predicted and the ground-truth occupancyvolumes because they are in different coordinatesystems and may have different sizes (noisy RoIvs. GT box). To overcome this issue, we employ atwo-step process as illustrated in . Firstly, wetransform the ground-truth (GT) box to the coordi-nate system of the RoI using the relative pose. Thistransformation aligns the GT box with the RoI, enabling a consistent comparison. Subsequently, wedetermine the predicted occupancy status of each voxel center within the transformed GT box. Forvoxels falling inside the RoI (hit), their occupancies are determined by the corresponding predictedoccupancies within the RoI. On the other hand, voxels located outside the RoI (missed) are consideredas free. By applying this process, we construct a predicted occupancy volume within the GT box.Finally, we compute the IoU by comparing the predicted occupancy volume in the GT box with theground-truth occupancy volumes. During the IoU calculation, we ignore unobserved voxels in theGT volume for a fair assessment. Besides, RoIs that do not intersect with any GT boxes are excludedfrom the evaluation. We also report mean IoU that is respectively averaged at track and box levels toprovide a more detailed evaluation. For object detection, we adopt the official 3D detection metrics in WOD , including AveragePrecision (AP) and Average Precision Weighted by Heading (APH) at IoU thresholds of 0.7 forvehicles. Meanwhile, based on the number of points contained within each object, the data is dividedinto two difficulty levels: LEVEL 1, where the number of points is greater than 5, and LEVEL 2,where the number of points is between 1 and 5.",
  "Shape Completion Results": "Comparison against Baseline. Since object-centric occupancy is a novel task, no learning-basedmethods can be used for comparison as far as we are acknowledged. We compare our method with thebaseline that directly accumulates and voxelizes the history point clouds within the noisy tracklet pro-posals. We evaluate the shape completion performance on three types of tracklet inputs: ground-truth(GT) tracklets, tracklets generated by CenterPoint (CP) , and tracklets generated by FSD . Asshown in Tab. 1, the shape completion performance is strongly correlated with the quality of the inputtracklets, where better tracklets lead to better shape completion. In all cases, our method outperformsthe baseline, even when the input tracklets are noise-free GTs. This is because our method can effec- tively complete the object shape even at early timestamps by leveraging learned knowledge from train-ing data, whereas the baseline only becomes effective at later timestamps when more views are visible.",
  ": Shape completion results on WOD val set.\"-E\" denotes using GT bbox which may outsidethe predicted RoIs": "Robustness. To simulate unsatisfied detectionand tracking results, we add some slight noiseto GT box proposals. From Tab. 1 we can findthat the baseline performance drops significantly(>10% IoU), while our method maintains a sta-ble performance in this case (<5% IoU), demon-strating the robustness of our method to noisyinputs. Compared to noisy GT tracklets, track-lets generated by CP and FSD may additionallycontain mismatched or missed targets, leadingto a more significant performance drop fromthe baseline. In contrast, our method demon-strates its strong robustness to these noisy andinaccurate tracklets. These results indicates thatour method can effectively complete the objectshape even when the input tracklets are noisy orinaccurate. Results with GT bbox. Thanks to the implicit shape decoder, our method has the potential to predictthe occupancy status at any position even \"outside\" the RoI, which is non-trivial for the baseline orCNN-based methods. To demonstrate this ability, we conduct an experiment by querying the implicitdecoder at all voxel centers within the GT box (even for those outside the RoI). Unlike our standardevaluation, where we simply treat the missed positions as free (see ), we query the implicitdecoder at these positions to obtain the predicted occupancy status. As shown in Tab. 1, the shapecompletion performance is further improved when considering the extrapolated results outside theRoIs (Ours-E), demonstrating the flexibility of our implicit shape representation. Generalization. The last row in Tab. 1 presents occupancy completion results obtained by directlyapplying our trained model to the tracklet proposals generated by FSDv2 . Due to better detection,our method with FSDv2 still outperforms the version with CenterPoint even without retraining.However, it performs slightly worse compared to using FSD tracklets, despite FSDv2 having betterdetection results than FSD. This indicates that significant detection improvements generally lead tobetter shape completion (FSDv2 vs. CenterPoint). However, for detectors with similar performance(e.g., FSD vs. FSDv2), improved detections do not necessarily guarantee better shape completionwithout retraining.",
  "Object Detection Results": "Main Results Tab. 2 presents the 3D detection results on the WOD validation set. Significant improve-ments are observed when applying our methods to the tracklet proposals generated by CenterPoint and FSD . Compared to the previous state-of-the-art MoDAR , our method achieves notablygreater enhancements on 1-frame CenterPoint (e.g., 8.6% vs. 3.2% improvement in L1 AP). Applyingour method to a more advanced detector, 1-frame FSD , still results in a noticeable improvement.This enhancement is more significant compared to adding MoDAR to a detector with similar perfor-mance (i.e., 3-frame SWFormer ). Furthermore, we achieve new state-of-the-art online detectionresults by applying our method to 7-frame FSD, attaining 83.3% AP and 75.7% APH on L1 and L2,respectively. This indicates our methods effectiveness in aggregating long-sequence information forobject detection in addition to shape completion. Moreover, our method can be seamlessly integratedwith other state-of-the-art detectors without requiring retraining on their respective tracklets in thetraining data. For example, applying our method (trained on CP and FSD tracklets) to FSDv2 yields significant improvements, showcasing the strong generalization capability of our approach.",
  "In this section, we evaluate different designchoices in our method and analyze their impacton the shape completion and detection perfor-mance. All the results are based on 1-frameFSD tracklets": "Single Branch vs. Dual Branch. We first eval-uate the performance when using only a singlebranch for RoI encoding. In this setting, onlya local encoder Elocal is used to encode the RoIin the local coordinate system. The encodedfeatures are enhanced by the causal transformer and then used to generate occupancy and detectionoutputs. As shown in Tab. 4, the single-branch model is inferior to our dual-branch model in bothshape completion and detection. This indicates that the motion information from the global branch isessential for accurate shape completion and detection refinement. Explicit vs. Implicit. We then attempt to refine detection results using the explicit occupancypredictions. Specifically, we sample occupied voxel centers from each predicted occupancy volumeand apply the RoI encoder Eglobal to generate the final feature used for detection (more details are in theAppendix A.2). However, as demonstrated in Tab. 4, this strategy leads to a significant performancedrop. Due to the non-differentiable nature of the occupancy sampling process, the detection errorscannot be back-propagated to other components when relying on explicit occupancy predictions,resulting in unstable training. In contrast, our implicit shape representation allows for joint end-to-endtraining of shape completion and detection, leading to better performance. Occupancy Helps Detection.Finally, we evaluate the impact of the occupancy task on de-tection performance.We removed the OCC head from our full model and retrained it us-ing only the detection loss.As shown in the last row of Tab. 4, the absence of the occu-pancy decoder results in a noticeable decline in detection performance.This suggests thatthe occupancy completion task not only explicitly enriches the object shape representation butalso enhances detection by contributing additional geometric information to the latent space.",
  ": Results for various sequence lengths": "Training & Testing Length. Tab. 5 shows howthe sequence lengths affect the performance ofour method. We retrain our method using 8-frame and 16-frame tracklets, respectively. Asindicated in the first 3 rows in Tab. 5, usinglonger sequences for training leads to betterresults. However, the performance improve-ment diminishes as the sequence length doubles.To strike a balance between performance andcomputational cost, we set our default traininglength to 32. Even trained with 32-frame track-lets, our method is flexible to handle various-length tracklets during inference. By default,we leverage all history frames to generate pre-dictions at each timestamp. However, we can also generate predictions using a subset of historicalframes to reduce computational costs. As shown in Tab. 5, frames for inference achievessimilar performance as using all history frames. Moreover, our method can also be extended to handleoffline scenarios. When the transformer attends to all timestamps including those future ones, theperformance improves further, as demonstrated in the last row of Tab. 5.",
  ": Cost analysis of the shape decoder": "Computational Efficiency. Tab. 6 shows thetime and GPU memory cost of the proposedshape decoder. Since object tracklets vary inlength, our methods running time may also varywith different inputs. Additionally, the dimen-sion of the decoded object-centric occupancydepends on the detected bounding box. To en-sure fair testing of running time, we standardized the input length to 32 and set the number of decodequeries to 4096. As demonstrated in Tab. 6, the shape decoder only introduces a slight increase incomputational cost, demonstrating its efficiency.",
  "Limitations": "Technically speaking, our automatic occupancy annotation relies on the rigid-body assumption, whichmay not be accurate for deformable objects. Consequently, our experiments focus on vehicle objectssince they are rigid. Although our method can be applied to other deformable object categories,accurate evaluation for deformable objects cannot be guaranteed due to considerable noise in theground-truth data.",
  "Conclusion": "In this work, we introduce a novel task, object-centric occupancy, which extends the traditional objectbounding box representation to provide a more detailed description of the object shape. Compared toits scene-level counterpart, object-centric occupancy enables higher voxel resolution in large scenesby focusing on foreground objects. To facilitate object-centric occupancy learning, we build anobject-centric occupancy dataset using LiDAR data and box annotations from the Waymo OpenDataset (WOD). We further propose a novel sequence-based occupancy completion network thatlearns from our dataset to complete object shapes from noisy object proposals. Our method achievesstate-of-the-art performance on both shape completion and object detection tasks on WOD. Webelieve that our work will inspire future research in perception tasks in the context of autonomousdriving.",
  "Acknowledgements": "This work was supported by NSFC with Grant No. 62293482, by the Basic Research ProjectNo.HZQB-KCZYZ-2021067 of Hetao Shenzhen HK S&T Cooperation Zone, by ShenzhenGeneral Program No. JCYJ20220530143600001, by Shenzhen-Hong Kong Joint Funding No.SGDX20211123112401002, by the Shenzhen Outstanding Talents Training Fund 202002, by Guang-dong Research Project No. 2017ZT07X152 and No. 2019CX01X104, by the Guangdong Provin-cial Key Laboratory of Future Networks of Intelligence (Grant No. 2022B1212010001), by theGuangdong Provincial Key Laboratory of Big Data Computing, CHUK-Shenzhen, by the NSFC61931024&12326610, by the Key Area R&D Program of Guangdong Province with grant No.2018B030338001, by the Shenzhen Key Laboratory of Big Data and Artificial Intelligence (GrantNo. ZDSYS201707251409055), and by Tencent & Huawei Open Fund. Armen Avetisyan, Manuel Dahnert, Angela Dai, Manolis Savva, Angel X Chang, and MatthiasNiener. Scan2cad: Learning cad model alignment in rgb-d scans. In IEEE Conf. Comput. Vis.Pattern Recog., pages 26142623, 2019. Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, AnushKrishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: A multimodal dataset forautonomous driving. In IEEE Conf. Comput. Vis. Pattern Recog., pages 1162111631, 2020. Xuesong Chen, Shaoshuai Shi, Benjin Zhu, Ka Chun Cheung, Hang Xu, and Hongsheng Li.Mppnet: Multi-frame feature intertwining with proxy points for 3d temporal object detection.In Eur. Conf. Comput. Vis., pages 680697. Springer, 2022. Yukang Chen, Jianhui Liu, Xiangyu Zhang, Xiaojuan Qi, and Jiaya Jia. Voxelnext: Fully sparsevoxelnet for 3d object detection and tracking. In IEEE Conf. Comput. Vis. Pattern Recog., pages2167421683, 2023.",
  "Zhichao Li, Feng Wang, and Naiyan Wang. Lidar r-cnn: An efficient and universal 3d objectdetector. In IEEE Conf. Comput. Vis. Pattern Recog., pages 75467555, 2021": "Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chonghao Sima, Tong Lu, Yu Qiao, andJifeng Dai. Bevformer: Learning birds-eye-view representation from multi-camera images viaspatiotemporal transformers. In Eur. Conf. Comput. Vis. Springer, 2022. Xiaoyang Lyu, Peng Dai, Zizhang Li, Dongyu Yan, Yi Lin, Yifan Peng, and Xiaojuan Qi.Learning a room with the occ-sdf hybrid: Signed distance function mingled with occupancyaids scene representation. In Int. Conf. Comput. Vis., pages 89408950, 2023. Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoor-thi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis.Communications of the ACM, 65(1):99106, 2021. Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove.Deepsdf: Learning continuous signed distance functions for shape representation. In Proceed-ings of the IEEE/CVF conference on computer vision and pattern recognition, pages 165174,2019.",
  "Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on pointsets for 3d classification and segmentation. In IEEE Conf. Comput. Vis. Pattern Recog., pages652660, 2017": "Charles R Qi, Yin Zhou, Mahyar Najibi, Pei Sun, Khoa Vo, Boyang Deng, and DragomirAnguelov. Offboard 3d object detection from point cloud sequences. In IEEE Conf. Comput.Vis. Pattern Recog., pages 61346144, 2021. Martin Runz, Kejie Li, Meng Tang, Lingni Ma, Chen Kong, Tanner Schmidt, Ian Reid, LourdesAgapito, Julian Straub, Steven Lovegrove, et al. Frodo: From detections to 3d objects. In IEEEConf. Comput. Vis. Pattern Recog., pages 1472014729, 2020. Renato F Salas-Moreno, Richard A Newcombe, Hauke Strasdat, Paul HJ Kelly, and Andrew JDavison. Slam++: Simultaneous localisation and mapping at the level of objects. In IEEE Conf.Comput. Vis. Pattern Recog., pages 13521359, 2013. Shaoshuai Shi, Li Jiang, Jiajun Deng, Zhe Wang, Chaoxu Guo, Jianping Shi, Xiaogang Wang,and Hongsheng Li. Pv-rcnn++: Point-voxel feature set abstraction with local vector representa-tion for 3d object detection. Int. J. Comput. Vis., 131(2):531551, 2023.",
  "Shaoshuai Shi, Xiaogang Wang, and Hongsheng Li. Pointrcnn: 3d object proposal generationand detection from point cloud. In IEEE Conf. Comput. Vis. Pattern Recog., pages 770779,2019": "Shuran Song, Fisher Yu, Andy Zeng, Angel X Chang, Manolis Savva, and Thomas Funkhouser.Semantic scene completion from a single depth image. In IEEE Conf. Comput. Vis. PatternRecog., pages 17461754, 2017. Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, PaulTsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, et al. Scalability in perception forautonomous driving: Waymo open dataset. In IEEE Conf. Comput. Vis. Pattern Recog., pages24462454, 2020. Pei Sun, Mingxing Tan, Weiyue Wang, Chenxi Liu, Fei Xia, Zhaoqi Leng, and DragomirAnguelov. Swformer: Sparse window transformer for 3d object detection in point clouds. InEur. Conf. Comput. Vis., pages 426442. Springer, 2022. Xiaoyu Tian, Tao Jiang, Longfei Yun, Yucheng Mao, Huitong Yang, Yue Wang, Yilun Wang,and Hang Zhao. Occ3d: A large-scale 3d occupancy prediction benchmark for autonomousdriving. Adv. Neural Inform. Process. Syst., 36, 2024.",
  "Yi Wei, Linqing Zhao, Wenzhao Zheng, Zheng Zhu, Jie Zhou, and Jiwen Lu. Surroundocc:Multi-camera 3d occupancy prediction for autonomous driving. In Int. Conf. Comput. Vis.,pages 2172921740, 2023": "Zhaoyang Xia, Youquan Liu, Xin Li, Xinge Zhu, Yuexin Ma, Yikang Li, Yuenan Hou, andYu Qiao. Scpnet: Semantic scene completion on point cloud. In IEEE Conf. Comput. Vis.Pattern Recog., pages 1764217651, 2023. Xu Yan, Jiantao Gao, Jie Li, Ruimao Zhang, Zhen Li, Rui Huang, and Shuguang Cui. Sparsesingle sweep lidar point cloud segmentation via learning contextual shape priors from scenecompletion. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages31013109, 2021.",
  "A.1Dataset Generation Pipeline": "Our annotation pipeline is illustrated in . Leveraging LiDAR scans and detection annotationsfrom a base 3D dataset, our pipeline probes dense occupancy grids by aggregating multi-frame LiDARpoint clouds and then executes occlusion reasoning to discriminate between free and unobservedvoxels. Compared to ego-centric approaches , our methodology primarily differs by focusingon annotated objects instead of the entire scene. For each designated object, we gather points within itsannotated bounding boxes over time, transform these points from sensor coordinates to the boundingbox coordinates and aggregate them into a dense point cloud. Unlike scene-level occupancy, we donot transform the densified object point cloud back to each ego-vehicle coordinate for occupancyconstruction. Instead, we directly voxelize it under the local object coordinate system, maintainingobject-centric precision. While densified object point clouds encode better shape information than asingle LiDAR scan, its important to note that unoccupied voxels (grey voxels in ) does notnecessarily indicate free space; they may be unobserved by the LiDAR due to occlusion. Hence, anocclusion reasoning process is required to distinguish between voxels that are truly free and thosethat are unobserved. Basically, an unoccupied voxel is considered free if it is traversed trough by aLiDAR ray, and unobserved otherwise. Instead of the time-consuming ray-casting operation used in, we adopt a more efficient approach by leveraging range information from raw range images.Specifically, for each unoccupied voxel, we first convert its center to the range image format usingsensor intrinsics and extrinsics at a specific timestamp t, yielding a 2D-pixel index (ut, vt) and arange value rt. Next, we decide its status by comparing its range with the original range image attimestamp t:",
  "A.2Alternative Design Choices": "and illustrate the pipelines of the single-branch model and the model using explicitoccupancy for detection, respectively. The two global RoI encoders Eglobal in share the sameweights. We additionally add an extra channel to each point feature to indicate whether it is from rawpoint clouds or from the predicted occupancy volume.",
  "A.3Training Details & Hyper-parameters": "We train our model using the Adam optimizer with an initial learning rate of 1e-4 and a batch sizeof 8. The model is trained for 24 epochs with the learning rate scheduled by the cosine annealingstrategy. We use a transformer with 3 layers, 4 heads, and a hidden dimension of 512. The model isimplemented using PyTorch and trained on 8 NVIDIA 3090 GPUs.",
  "A.4Visualization of the Occupancy Prediction": "shows some examples of the occupancy prediction. Our method effectively predicts theobjects shape even when it is extremely occluded. Additionally, our method effectively completesthe object shape even at early timestamps, with shape completion improving as the sequence extends. weve also included several surface renderings of the predicted occupancy in and .These renderings were obtained by applying marching cubes to the decoded volumetric grids usinga level of 0.5. The renderings demonstrate that our method can complete shapes even when thecurrent point cloud is extremely sparse. Due to the use of 0.2m voxel size, the resolution of ourpredicted occupancy may not support high-quality rendering. For example, the resolution for a typicalsedan (lets assume its dimensions are 4.5m* 1.8m * 1.4m) under our voxel size is 23 * 9 * 7. Incontrast, common shape completion methods typically use a resolution of 128 x 128 x 128 or higherto facilitate high-quality rendering. It should be noted that for our purposes, high-quality renderingis not required. Although the selected voxel size of 0.2 meters may not provide highly detailedrendering, it is sufficient for downstream driving tasks and ensures computational affordability."
}