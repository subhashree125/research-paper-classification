{
  "Abstract": "Continual reinforcement learning poses a major challenge due to the tendencyof agents to experience catastrophic forgetting when learning sequential tasks.In this paper, we introduce a modularity-based approach, called HierarchicalOrchestra of Policies (HOP), designed to mitigate catastrophic forgetting in lifelongreinforcement learning. HOP dynamically forms a hierarchy of policies based ona similarity metric between the current observations and previously encounteredobservations in successful tasks. Unlike other state-of-the-art methods, HOP doesnot require task labelling, allowing for robust adaptation in environments whereboundaries between tasks are ambiguous. Our experiments, conducted acrossmultiple tasks in a procedurally generated suite of environments, demonstrate thatHOP significantly outperforms baseline methods in retaining knowledge acrosstasks and performs comparably to state-of-the-art transfer methods that require tasklabelling. Moreover, HOP achieves this without compromising performance whentasks remain constant, highlighting its versatility.",
  "Introduction": "Neural networks are typically trained on data drawn independently and identically from a staticdistribution. While this approach works well in many cases, it becomes challenging in environmentsthat are continuously changing or when new environments are introduced. In dynamic settings, suchas reinforcement learning, robotics, or dialogue systems, models must adapt to new information whilepreserving knowledge from previous tasks (Parisi et al., 2019). However, neural networks often sufferfrom catastrophic forgetting, where learning new tasks leads to the rapid loss of previously acquiredknowledge. The ability to learn new skills while maintaining existing knowledge is referred to ascontinual learning (Ring, 1994). To address the challenge of catastrophic forgetting, researchers have developed three primary cat-egories of methods. The first category, regularization-based, works by constraining updates tonetwork parameters, thereby penalizing deviations from learned weight values that are critical forprevious tasks. Notable examples of this approach include Elastic Weight Consolidation (EWC)and Synaptic Intelligence (SI) (Kirkpatrick et al., 2017; Zenke et al., 2017). The second category,replay-based, mitigates forgetting by periodically rehearsing past experiences, either through actualdata or synthetic generations, ensuring that the network continues to perform well on earlier tasks(Rolnick et al., 2019; Shin et al., 2017). The third category, modularity-based, addresses the issueby structurally separating the network into modules, with each module dedicated to a specific task,thereby minimizing interference between tasks, prominent examples of this method are ProgressiveNeural Networks (PNN) by (Rusu et al., 2016) and adaptive multi-column stacked sparse denoisingautoencoder (AMC-SSDA) by Agostinelli et al. (2013). Finally there are some methods which use acombination of these, for example, Schwarz et al. (2018) uses an active network and a knowledge-base",
  "network similar to the modularity-based methods, however it periodically compresses knowledgefrom the active into the knowledge network using EWC regularisation": "Our method, the Hierarchical Orchestra of Policies (HOP), is a modularity-based approach and is mostsimilar to PNN. However, unlike PNN, HOP does not rely on a task identifier during training, whichmakes it more suitable for domain-incremental learning (Van de Ven and Tolias, 2019). Additionally,HOP differs in that it combines network probability outputs directly through hierarchical weightings,rather than using latent connections between networks. Finally, we demonstrate HOP at a significantlylarger scale 18 hierarchical policy levels compared to only 3 in PNN. We show that HOP performscomparably to PNN, even when PNN is provided with task labels while HOP is not. Furthermore,HOP mitigates catastrophic forgetting across several Procgen environments (Cobbe et al., 2020),achieving notable improvements over Proximal Policy Optimization (PPO), a reinforcement learningalgorithm with inherent regularization properties (Schulman et al., 2017).",
  "Hierarchical Orchestra of Policies": "Hierarchical Orchestra of Policies (HOP) is a modularity-based deep learning framework designedto mitigate catastrophic forgetting when learning new tasks. In this framework, a task is defined asa specific Markov Decision Process (MDP), where distinct levels within a procedurally generatedenvironment, or levels across different environments, are considered separate tasks (Puterman, 2014).Although HOP is task-agnostic, all tasks are treated as episodic. HOP relies on reinforcement learning algorithms that output stochastic policies, represented as(a | s) (Sutton, 2018). In our work, PPO serves as the base algorithm for HOP. The frameworkintroduces three key mechanisms to form and use a collection of policies: 1. Checkpoints to freeze and store policies at a certain stage of training.2. Orchestration of policy activation based on state similarity.3. Hierarchical weightings to balance the influence of previous and new policies.",
  "These mechanisms enable the agent to recover and maintain performance across diverse tasks withoutsignificant interference, thereby promoting continual learning in complex environments": "Checkpoints. The agent initially learns a policy using a base algorithm. After Tcheckpoint time-steps, HOP initializes a checkpoint, where the current learning policy is frozen and evaluated inthe currently available tasks. During checkpoint evaluation, if the episodic return R surpasses apredefined threshold Rthreshold, all states encountered during that task episode are stored in a setof trusted states Sm which is linked with the policy checkpoint m, where m is the count of thecheckpoint. The Orchestra. When the agent resumes learning, it dynamically activates checkpoint policies mdetermined by the similarity between the current state st and any sm Sm. If the current state st issimilar to any state in Sm, then the corresponding frozen policy m is activated (Im = 1). Similarityis determined by a threshold value , which, in all of our experiments, has been defined as anysm Sm with a cosine similarity greater than 0.98 with st. Rather than selecting actions directly from the distribution of a single frozen policy (at m(st)),which could lead to conflicts when multiple policy checkpoints are activated, HOP combines thedistributions from activated policy checkpoints (Imm) and the current learning policy n intoa joined action policy, denoted as na (see equation 1). Here, n represents the current count ofall policies, and the subscript a denotes the combined policy from which the action is sampled.This approach allows the agent to leverage past knowledge while adapting to new tasks, promotingcontinual learning. To avoid significant and undesired output shifts caused by small changes in the state, frozen policiespredict actions based on the most similar state, sm Sm, to the current state st (Szegedy, 2013).This state is referred to as sm, and actions are chosen as m(a | sm) rather than directly from st.This dynamic activation of multiple policies is called the orchestra of policies, a term borrowed fromJonckheere et al. (2023) but applied differently in this work. Hierarchical Weightings. As the agent learns, it is expected to achieve higher task-specific rewards,which suggests that newer policies for the same tasks are likely to outperform older policy checkpoints",
  "for the same task. Thus, simply averaging all policies, as represented by na = 1": "nnm=0 m, isimpractical. Moreover, because the agent does not know the identities of tasks, multiple policycheckpoints may activate; therefore, simply sampling actions from newer policy checkpoints isnot possible. To address this, HOP introduces a hierarchical discount factor, denoted as W, todetermine the contributions of policy checkpoints. Each joined action policy at time of checkpoint isa combination of previous policy checkpoints, creating a hierarchical structure, as shown in and Equation 1. This hierarchical structure assigns a higher weight to more recent activated policies.For some examples of how this concept functions, please refer to Appendix A.4. Policy Updates: The update process for the learning policy (n) follows the same procedure asthat used by the base algorithm. Specifically, all necessary attributes associated with the policy aresampled from n, except for the action, which is instead sampled from the current joined actionpolicy (na). We also allow gradients to propagate to the policy checkpoints m, but only for thestates where they are activated. For more detail and pseudo code refer to Appendix A.1.",
  "smst > .0otherwise.(3)": "Here, sm is the most similar state from all s Sm, and ma represents the logits of the m-th joinedaction policy checkpoint. M is the total number of checkpoints conducted. n is the current count ofall policies. Im is the activation related to each policy m. And is the similarity threshold of thecurrent state with previous states in Sm. depicts the flow of information within the HOP framework, illustrating how the agentevaluates observations, selects relevant policies, and takes actions to adapt continually across tasks.The agent begins by assessing the current state st against a similarity threshold for each checkpointpolicy m , where denotes all previously stored policies. For each policy, it identifies themost similar reference state sm Sm and calculates an activation Im based on this similarity. If thesimilarity metriccalculated as the cosine similarity between st and smexceeds the predefinedthreshold , the corresponding policy m is activated (Im = 1), otherwise, it remains inactive(Im = 0). The activated policies then contribute to the current joint action policy na, whichcombines outputs from the activated checkpoint policies Imm and the current learning policy n,weighted hierarchically according to their relative recency and activations, as shown in Equation 1. The agent samples actions from na rather than directly from the learning policy n, enablingit to leverage past knowledge while adapting to new tasks. The chosen action interacts with theenvironment, producing a new state and a corresponding reward. These states, actions, rewards, andactivations are stored in the replay buffer to support continual learning and mitigate forgetting byallowing the agent to revisit past experiences. During training, the replay buffers stored activationsalso guide the gradient computations, allowing only the activated policies to contribute to the policyupdate. This targeted update process refines gradient flow selectively based on activation, promotingmodularity and stability in learning across diverse task environments.",
  "Results": "We evaluated the performance of HOP using the Procgen suite of environments (Cobbe et al., 2020).The experimental setup consisted of three phases of training. In the first phase, the agent trains forthree million time-steps on multiple levels of a selected Procgen environment to develop its ability tolearn and generalize. In the second phase, the environment was switched to a different one, and theagent continued training for another three million time-steps, assessing its adaptability and ability",
  ": Hierarchical formation of the fourth level of a HOP action policy": "to transfer learning. Finally, in the third phase, the agent returned to the original environment foran additional three million time-steps to evaluate retention of skills and re-adaptation. Throughouttraining, the agents objective is to optimize the reward functions defined by the Procgen environments,which typically involve maximizing cumulative rewards for task-specific objectives such as reachinggoals, collecting items, or avoiding obstacles. This is a simplified experimental set-up to thatconducted by Schwarz et al. (2018) in their examination of P&C. We conducted experiments with three different environment combinations: StarPilot and Climber,Ninja and StarPilot, and Ninja and CoinRun, repeating each with four random seeds. During training,periodic evaluation episodes were performed to measure performance, and checkpoints were savedevery 500,000 time-steps. HOP was compared with standard PPO and a modified version of Progressive Neural Networks(PNN) for use with PPO see appendix A.3 for full details of the modifications. We allowed PNN tohave task identifiers but not HOP. Results presented in indicate that HOP outperformed PPOin both the rate of performance recovery and the final averaged evaluation return after training. Wefound that HOP had comparable performance to PNN in all but the very beginning of the third phaseof learning. summarizes the total steps after the second phase of learning required for eachmethod to recover to the performance level achieved at the end of the initial phase of training, and the",
  "PNN": ": Training performance of HOP, PNN and PPO on three experiments where environments areperiodically changed. The red dashed lines indicate the points when the environment are switched.The green dashed lines show when HOP returns to the highest average evaluation reward achieved inthe first environment before the change. The black dashed lines represents this point for PPO. Shadedareas are the standard error. All experiments are conducted with the Procgen easy setting.",
  "StarPilot - Climber2.681.04 -61.2%12.1418.15 49.5%15.98 31.6%Ninja - StarPilot3+1.70 -43+%6.798.73 28.6%7.97 17.37%Ninja - Coinrun1.370.72 -47.7%8.338.37 0.48%7.83 -6.00%": ": A comparison of PPO, HOP, and PNN. Steps-to-return represents the number of steps (inmillions) to re-acquire the same average evaluation reward at the end of the first period of learningin that environment, PNN is not included in these comparisons as it uses a separate actor and criticnetwork per task. Final rewards display the final average evaluation rewards at the end of all training.The percentages show the difference compared to the baseline PPO method.",
  "Summary and Discussion": "We present a novel modularity-based approach, the Hierarchical Orchestra of Policies, to addresscatastrophic forgetting in continual life-long reinforcement learning. In our empirical evaluation,HOP outperforms PPO in continual learning scenarios, achieving a faster recovery of performanceand final performance. Both HOP and PNN demonstrate substantial transfer between environmentswith similar dynamics and state spaces such as Ninja and CoinRun. In these scenarios HOP canactivate relevant frozen policies learned from Ninja while acting in CoinRun, similar to PNNsadapter networks connecting separate columns. However, unlike PNN, HOP does not require tasklabels, making it more versatile for real-world applications where task boundaries are not clearlydefined. However, the effectiveness of HOP depends on the careful tuning of some hyper-parameters, particu-larly the similarity threshold (w) and reward threshold (P), which must be set appropriately for allexpected tasks. See Appendix A.2 Future work could expand HOPs evaluation by testing transitions between highly diverse tasks andenvironments where task boundaries are ambiguous, a setting in which PNN and similar methodsare less effective. Additionally, HOP could be adapted to continuous environments with fluid tasktransitions, further highlighting its robustness in real-world scenarios. To address performance dropsimmediately following task distribution changes, a learnable parameter could be introduced whichcould dynamically adjust the influence of previous checkpoints, enabling immediate adaptation whilemaintaining learning.",
  "A.1HOP Algorithm details": "The logic provided in algorithm 1, is suitable for use with PPO (or any other actor critic style basefunction. It is expected that HOP would work with any method with a stochastic policy, however thishas yet to be tested. details all of the extra parameters that HOP requires. Algorithm 1 Hierarchical Orchestra of Policies (HOP) with PPOInitialize: Current hierarchy depth n = 1, Policy n == na, similarity threshold w, and rewardthreshold P, total steps D, step = 0, checkpoint interval C, state st, done = 0, value function V,batch size T, buffers B, and other PPO parameters .",
  "A.2Experiment details": "Our experiments are conducted in the Procgen suite of environments introduced by Cobbe et al.(2020). Specifically, we use Ninja, StarPilot, Climber, and CoinRun as our environments. These canbe viewed at and we also provide a table with a snapshot of eachenvironment in . In Procgen, there are options that can reduce the complexity of the environ-ments. We activate the following options: use_backgrounds=False, restrict_themes=True, distribution_mode=easy, and use_sequential_levels=True. However, we do not activateuse_monochrome_assets, as we found that it lacked proper indications for agent direction. Inall of our experiments, the agents goal was to maximize the cumulative reward provided by theenvironment. The state is represented as an 84x84 pixel image, and the agent has 15 possible actions.",
  ". The agent trains in environment 1 with T1 tasks in distribution.2. Learning is switched to environment 2 with T2 tasks.3. Learning is switched back to environment 1 for the same T1 tasks": "In our experiments, X = 9, 000, 000, and T1 = T2 = 30. PNN is given a task identifier for eachenvironment, enabling it to use the correct networks and adapters. HOP, on the other hand, doesnot require these and is not given them. Every 163,840 time steps, the agent is evaluated in thecurrent distribution of tasks, which in this case consists of 30 Procgen levels in the current trainingenvironment of that phase, the reward in this evaluation phase is the cumulative reward - 0.01*totalsteps taken in the environment, which gives a better indication of efficiency. The results we reportare based on these evaluated tasks. Conducting evaluation episodes at fixed intervals provides theclearest and most accurate representation of agent performance. The three experiments shown in are conducted using the combinations of environmentslisted in . In the first experiment, the environments are completely different, with distinctdynamics and little to no shared understanding between them. For the second experiment, we believedthere would be some overlap; while StarPilot scrolls from right to left, Climber scrolls verticallyfrom bottom to top. The final experiment features considerable shared dynamics, as both Ninja andCoinRun are platforming games. We hypothesized that this setup would demonstrate the transferand recovery of performance across different levels of difficulty. However, we observed that only theNinja and CoinRun experiment exhibited meaningful transfer for both PNN and HOP. We use PPO as a baseline algorithm and as the foundation for both HOP and PNN. Our PPOimplementation is based on the version by Huang et al. (2022). The only modification we made wasseparating the actor and critic networks, which we found easier to work with and which outperformedthe shared convolutional layer approach. illustrates our implementation. We kept the PPO-specific hyperparameters fixed for its use with HOP, PNN, and the base PPO. These hyperparameterswere optimized for the base PPO, and while a small benefit might have been observed for HOPand PNN if a hyperparameter sweep had been conducted, both performed as expected, so we didnot pursue this. The PPO-specific hyperparameters are shown in , and all other relevantparameters are shown in .",
  "A.3PNN with PPO Algorithm Details": "Progressive Neural Networks (PNN) were introduced by Rusu et al. (2016). In their paper, theydescribe how separate policy networks (referred to as columns) and links between columns (adapters)are used to improve continuous learning. However, they report their results using the AsynchronousAdvantage Actor-Critic (A3C) algorithm Mnih (2016). We could not find any evidence indicatingwhether they initialized separate columns and adapters for the value (critic) network as well as thepolicy (actor) network. Additionally, we could not locate any official implementation online, nor anyimplementation using actor-critic methods. Intuitively, we expect that if PNN works for the policy, it should also work for the value function.Therefore, we implemented separate columns for both the critic and actor networks, along withadapters for each new task. We encountered another issue: PPO is generally expected to outperformA3C in ProcGen environments. Thus, comparing HOP-PPO or base PPO with PNN-A3C wouldbe unfair to PNN. To address this, we modified the PNN implementation to use a PPO update thatpropagates through separate columns and adapters.",
  "Since the inputs for ProcGen environments are image-based, we use a single ReLU-activated convo-lutional layer as each adapter network. The input to the adapter network is the final convolutional": "output from the Critic or Actor of each column. The adapter outputs of the previous column areadded to the original output and passed to the fully connected layers. All adapters are included in thegradient graphs to promote transfer, but the actor and critic columns are not included unless they arethe active column.",
  "A.4Hierarchical Weighting Examples": "Hierarchical weightings. This hierarchical structure implies that as the agent continues learning,the contributions of older policies diminish if more recent checkpoints are activated. Conversely, ifmore recent policies do not activate, the older policies will have a stronger influence. For example,consider the policy at the fourth checkpoint (4) as depicted in . If activations As3, As2, andAs1 occur, the policy output for the current state st is given by:"
}