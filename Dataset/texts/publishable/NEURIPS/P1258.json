{
  "Abstract": "Neutrino telescopes, an extension of traditional multiwavelength astronomy, pro-vide a complementary view of the universe using neutrinos. Differences in detectorgeometry and detection medium mean that improvements to reconstruction tech-niques made at one experiment are not readily applicable to another. Recently, deeplearning has been shown to improve prediction speed and accuracy and offer indif-ference to detector geometry and detection medium, providing a unique opportunityfor collaboration. This work introduces GraphNeT 2.0, an open-source, detector-agnostic deep learning library for neutrino telescopes and related experiments.GraphNeT enables inter-experimental collaboration on the use and developmentof advanced methods based on major deep learning paradigms like transformers,normalizing flows, graph neural networks, and more.",
  "Introduction": "In order to minimize the dominant background of atmospheric muons, neutrino telescopes areconstructed in exotic sub-surface locations, such as at the bottom of the Mediterranean Sea or deepinto the antarctic ice , . They span volumes up to the cubic-kilometer scale with instrumentation, : Two simulated neutrino events in two different detectors with different detector geometriesand detection mediums. Left) A simulated neutrino event from IceCube, a neutrino telescope installedin the ice at the south pole . Right) A simulated neutrino interaction in a detector installed in waterwith a geometry similar to KM3NeT-ARCA .",
  "arXiv:2501.03817v1 [hep-ex] 7 Jan 2025": "Optical Modules (OMs) with one or more Photo-Multiplier-Tube (PMT) are often placed on verticallines arranged in irregular geometries spanning the detector volume. These OMs detect Cherenkovradiation emitted by charged particles induced by neutrino interactions. Despite their differences in geometry and detection medium, the detection principle and resultinglow-level observations are virtually identical for neutrino telescopes. The low-level data consists oftriggered events representing individual neutrino interaction candidates. Each event is a geometrictime series, where every step represents a PMT at a given location x,y,z that observed Cherenkovphotons at time t. The topology and length of the sequences depend primarily on the neutrino energy,detector geometry, and number of OMs. The length of the sequences may range from a few photonsto upwards of a million. Illustration of events for IceCube and a water-based detector can be seen in. Inference of the physical properties of a neutrino inducing a detector response, such as its energyand direction, is a central task of most experiments. Maximum-likelihood estimators (MLEs) havepredominantly been used for this task. However, the reconstruction likelihood is often intractable,and the MLEs, therefore, rely on complex approximations that are both time-consuming and requireassumptions on detector geometry and detection medium, making cross-experimental collaborationimpractical , . In recent times, it has been shown that phrasing neutrino event reconstruction and classification assupervised learning tasks can provide both superior accuracy and reconstruction speeds, withoutrelying on assumptions on either geometry or detection medium , and has enabled highprofile results in the field , . The indifference to geometry and detection medium in deeplearning methods provides an opportunity for various neutrino experiments to collaborate on usingand developing deep learning-based techniques, but incompatible code bases and closed data policieshave made it challenging.",
  "GraphNeT": "GraphNeT is an open-source deep learning library written in Python for use by neutrinotelescopes and related experiments. It is built as a purpose-specific extension of popular deeplearning libraries, such as PyTorch , PyTorch Geometric , Lightning and employs codeconventions such as Black and type hints to harmonize readability of code from manycontributors. It is designed to enable re-usability of models developed for one telescope to anotherand provides a way for physics domain experts to apply complex methods to their physics analyseswithout being an expert in them. Reconstruction tasks are phrased in a way that is accessible to : High-level overview of the different library components of GraphNeT. The librarysfunctionality extends from converting experiment-specific files to configuring and training deeplearning models. Lastly, parts of GraphNeT are dedicated to aiding users in the deployment of trainedmodels. members of the general deep learning community, which may contribute with techniques we, inreturn, may use for physics. As seen in the high-level overview of GraphNeT in , the librarycontains functionality for converting experiment-specific files to formats suitable for deep learningand for designing and training models. Finally, GraphNeT contains functionality to aid users inevaluating trained models on unlabelled, experiment-specific files. The major changes introducedin this coming update of GraphNeT are support for models from most established deep learningparadigms (GraphNeT 1.0 supported just GNNs ) and data conversion functionality. Each ofthese two areas of the library is elaborated upon in the following sections.",
  "Data conversion in GraphNeT": "GraphNeT contains modularized data conversion. This optional data conversion functionality, referredto as the DataConverter henceforth, follows a reader/writer scheme, as illustrated in . Thereader is a small piece of code able to parse a single experiment-specific file and extract relevantquantities from it. The DataConverter presents the extracted data in a standardized way to the writer",
  ": Illustration of the reader/writer structure of the DataConverter": "module, which saves the extracted data to a specific file format suitable for deep learning. Becauseexperiment-specific details related to data conversion is isolated in the reader module, extendingsupport for a new experiment only requires users to provide a new reader. Similarly, support forexporting data to new file formats requires only adding a new writer module.",
  "Models in GraphNeT": "A central idea of GraphNeT is to enable users to reuse models across collaboration boundaries and tore-purpose models to solve different problems. In order to achieve this, GraphNeT provides standardsfor model design that seek to compartmentalize model functionality into reusable modules, referredto as model components henceforth. The designs are self-contained, meaning the models depend onlyon raw observations so that collaborations can deploy them in either offline or real-time settings. Inthis section, we elaborate on the standards and components that make up models in GraphNeT.",
  ": An illustration of model components in GraphNeT. GraphNeT allows users to reconfigureexisting methods to solve new problems by abstracting a deep learning solution into reusablecomponents": "In a typical deep learning workflow, raw data is loaded into memory and then processed into a suitablerepresentation that is then passed to a neural network, which returns a prediction. Along the way,details specific to either the dataset or its source may linger. In GraphNeT, this overall process is broken into 1 or more interchangeable components, depending onthe method, as visualized in . The Standard model (top ), which encompasses the vastmajority of methods in GraphNeT, consists of 4 model components: Detector, Data Representation,Architecture, and Task(s). The Detector component holds all experiment-specific details, which include column names of inputfeatures, detector geometry, and standardization functions to map raw data into a numerical rangesuitable for stochastic gradient descent. The Detector component is the only part of a Model withexperiment-specific details, allowing the remaining components to be detector-agnostic. The Data Representation component contains functionality for transforming raw observations intoa suitable data representation on an event-by-event basis and is used directly in the data-loadingprocedure. The transformation from raw observations to a chosen data representation happens in real-time, parallelized by PyTorch DataLoader, and is independent of file formats. Such representationscould be images, sequences, and graphs. The Architecture represents the bulk of learnable parameters in the Model and may rely on methodsfrom established deep learning paradigms such as graph neural networks, convolutional neuralnetworks, and sequence-based methods such as RNNs, transformers, etc. This component receivesthe data in the chosen representation and produces a latent representation of the input that is passedto the Task(s). The Task component defines the part of a model that is specific to a particular way of solving a partic-ular deep learning task. This includes problem-specific handling of the latent representation, finalactivation layers, and mapping the latent representation to the target space. Most Tasks in GraphNeTact as learnable prediction heads but come with additional logic for loss function evaluations, lossweighting, and loss regularization. The Task takes the loss function as an argument, which allowsusers to experiment with different loss functions for the same Task. By categorizing model functionality into the model components, users can experiment with differentchoices in these components for a given problem. For example, evaluating the impact on a particulartask by varying the choice in Data Representation or configuring the model to function in a newexperiment by changing the Detector component. While the structure of the Standard model in can accommodate most deep learning appli-cations, certain techniques may be impractical to abstract into those 4 model components. Suchtechniques may include auto-encoders, which rely on both an encoder and a decoder architecture, andwith a particular Task (reconstructing the input given the encoding), hybrid methods that combinetechniques from both deep learning and traditional MLE, etc. To accommodate these relevant meth-ods, a second Model design, referred to as the Generic model in , imposes only the existenceof an interchangeable Detector component, which allows the method to be applicable to differentexperiments.",
  "Recent Applications": "At the time of writing, the authors are familiar with specific applications of methods in GraphNeTby the user community in at least six different experiments. These experiments range from neutrinotelescopes such as IceCube and KM3NeT-ARCA to related experiments such as SNO+ ,MAGIC/CTAO , , ESSSB and Liquid-O . In SNO+, GNNs in GraphNeT are beingapplied in a dark matter search for event classification. In MAGIC, a technical study is underwayusing GNNs in GraphNeT to distinguish between proton, gamma, and muon events and to apply thesemethods to events that are detected by multiple Cherenkov telescopes in the larger CTAO network,combining multiple telescope observations into a single graph object . In ESSSB, methods inGraphNeT has been applied for fast event reconstruction , and for gamma/positron classificationand interaction vertex reconstruction in LiquidO .",
  "Noise cleaning & reconstruction for IceCubes detector extension Upgrade": "IceCube Upgrade is a planned detector extension of IceCube aimed at significantly improvingcapabilities in the GeV energy range. It will feature new multi-PMT OMs and will more than triplethe number of PMT channels. Due to the increased number of PMTs and higher levels of radioactivityin the new OMs, traditional noise-cleaning methods were insufficient, and MLE-based methodsused in this energy range were incompatible with multi-PMT OMs. To address this, the eventswere represented as point-cloud graphs, and the noise-cleaning task was phrased as a binary nodeclassification problem. A Graph Neural Network (GNN) nicknamed DynEdge was trainedto distinguish between signal- and noise-induced OM output could reduce the amount of noise byaround a factor of 10 with only a minor signal loss. After cleaning, a separate instance of DynEdgewas trained to provide predictions on event-level tasks for the analysis, such as neutrino energy, zenithangle, and particle/interaction-type identification.",
  "Kaggle Competition \"IceCube - Neutrinos in Deep Ice\"": "In the Kaggle competition \"IceCube - Neutrinos in Deep Ice\" , close to a thousand participantscompeted to develop the bst reconstruction algorithm for direction reconstruction on around 140million simulated neutrinos from IceCube. The competition metric was defined as the mean openingangle between the true and estimated direction vector computed over a a large sample of events .During the competition a baseline utilizing DynEdge was shared with the participants. The baseline",
  ": Primary mode of opening angle distribution vs. neutrino energy for both up- and down-going ,CC events": "was trained on less than 8% of the data. Many participants, including the winning solutions, tookinspiration from methods used in the baseline and produced their own versions for the competition.Performance of the 1st and 2nd place solutions on up- and down-going track events can be seenin . Here the baseline is shown in grey, and the kinematic angle between the neutrino andout-going muon, which represents the expected information limit, is added in dotted grey .",
  "Conclusion & Outlook": "GraphNeT is an open-source deep learning library built by and for physicists working at the in-tersection of deep learning and neutrino physics. GraphNeT enables researchers across differentneutrino experiments to develop, share, and adapt models to their work. By using popular deeplearning libraries, GraphNeT makes it possible for members of the deep learning community to makemeaningful contributions to the field without expertise in neutrino physics. At the time of writing, users of GraphNeT have applied techniques from the library to problems inat least 6 different experiments. Particularly, methods in GraphNeT have outperformed traditionalMLE methods on several tasks in the GeV energy range of IceCube, such as angular reconstructionand event classification . Recently, methods in GraphNeT were used to remove stochasticnoise induced by radioactive decays in the glass housing of OMs and to project the sensitivities ofIceCube Upgrade to atmospheric neutrino oscillations . In addition, GraphNeT was used byboth organizers and participants in \"IceCube - Neutrinos in Deep Ice\" and in reconstruction ofneutrino energy for point source searches . To address the challenge of closed-data policies for low-level data in the field, more than 100 millionsimulated neutrino events for at least 6 different detector geometries in both ice and water detectionmediums are expected to be released at the beginning of 2025 through GraphNeT for benchmarkingdeep learning based techniques .",
  "and Disclosure of Funding": "Wed like to acknowledge the work carried out by the many contributors to GraphNeT for theirvaluable contributions both large and small. The work carried out by Rasmus F. rse was supportedby the PUNCH4NFDI consortium via DFG fund NFDI39/1. The work by Aske Rosted is partiallysupported by the Institute for Advanced Academic Research of Chiba University.",
  "R. Abbasi et al., Low energy event reconstruction in IceCube DeepCore, Eur. Phys. J.C, vol. 82, no. 9, p. 807, 2022. DOI: 10.1140/epjc/s10052- 022- 10721- 2. arXiv:2203.02303 [hep-ex]": "M. G. Aartsen et al., Improvement in Fast Particle Track Reconstruction with Robust Statis-tics, Nucl. Instrum. Meth. A, vol. 736, pp. 143149, 2014. DOI: 10.1016/j.nima.2013.10.074. arXiv: 1308.5501 [astro-ph.IM]. R. Abbasi et al., A Convolutional Neural Network based Cascade Reconstruction for theIceCube Neutrino Observatory, JINST, vol. 16, P07041, 2021. DOI: 10.1088/1748-0221/16/07/P07041. arXiv: 2101.11589 [hep-ex].",
  "S. Aiello et al., Event reconstruction for KM3NeT/ORCA using convolutional neural net-works, JINST, vol. 15, no. 10, P10005, 2020. DOI: 10.1088/1748-0221/15/10/P10005.arXiv: 2004.08254 [astro-ph.IM]": "R. Abbasi, M. Ackermann, J. Adams, et al., Evidence for neutrino emission from the nearbyactive galaxy NGC 1068, Science, vol. 378, no. 6619, pp. 538543, 2022. DOI: 10.1126/science.abg3395. eprint: [Online]. Available: I. Collaboration, R. Abbasi, M. Ackermann, et al., Observation of high-energy neutrinosfrom the galactic plane, Science, vol. 380, no. 6652, pp. 13381343, 2023. DOI: 10.1126/science.adc9818. eprint: [Online]. Available:",
  "P. Eller, Public Kaggle Competition IceCube Neutrinos in Deep Ice, in 38th InternationalCosmic Ray Conference, Jul. 2023. arXiv: 2307.15289 [astro-ph.HE]": "C. Bellenghi et al., Extending the IceCube search for neutrino point sources in the Northernsky with additional years of data, PoS, vol. ICRC2023, p. 1060, 2023. DOI: 10.22323/1.444.1060. arXiv: 2308.12742 [astro-ph.HE]. J. Lazar, S. Meighen-Berger, C. Haack, D. Kim, S. Giner, and C. A. Argelles, Prometheus:An open-source neutrino telescope simulation, Comput. Phys. Commun., vol. 304, p. 109 298,2024. DOI: 10.1016/j.cpc.2024.109298. arXiv: 2304.14526 [hep-ex]."
}