{
  "Abstract": "B-cos Networks have been shown to be effective for obtaining highly humaninterpretable explanations of model decisions by architecturally enforcing strongeralignment between inputs and weight. B-cos variants of convolutional networks(CNNs) and vision transformers (ViTs), which primarily replace linear layers withB-cos transformations, perform competitively to their respective standard variantswhile also yielding explanations that are faithful by design. However, it has so farbeen necessary to train these models from scratch, which is increasingly infeasiblein the era of large, pre-trained foundation models. In this work, inspired by thearchitectural similarities in standard DNNs and B-cos networks, we propose B-cosification, a novel approach to transform existing pre-trained models to becomeinherently interpretable. We perform a thorough study of design choices to performthis conversion, both for convolutional neural networks and vision transformers. Wefind that B-cosification can yield models that are on par with B-cos models trainedfrom scratch in terms of interpretability, while often outperforming them in terms ofclassification performance at a fraction of the training cost. Subsequently, we applyB-cosification to a pretrained CLIP model, and show that, even with limited dataand compute cost, we obtain a B-cosified version that is highly interpretable andcompetitive on zero shot performance across a variety of datasets. We release ourcode and pre-trained model weights at",
  "Introduction": "Despite their strong performance on a variety of tasks, understanding decisions of deep neuralnetworks (DNNs) remains challenging. Explanation methods, such as feature attributions , have been proposed in an attempt to explain such decisions post-hoc, but have often found to beunfaithful to the model being explained . Inherently interpretable Deep Neural Network (DNN) models have recently gained popularity. Incontrast to the common approach of explaining existing DNNs in a post-hoc fashion, these modelstypically feature certain architectural constraints that allow for extracting human-interpretable, model-faithful simplifications of the models computations by design; examples of this include prototype-based , dynamic linear , or concept-bottleneck models . However,given those architectural changes, this comes at a price: specifically, the models need to be trainedfrom scratch, whichespecially in the case of large foundational models, which are increasinglypopularcan cost millions of dollars. To mitigate this, in this work, we explore a novel approach of fine-tuning DNNs for inherentinterpretability and propose to B-cosify existing DNNs. Specifically, we investigate whetherpre-trained DNNs can simply be efficiently fine-tuned to obtain a similar degree of interpretability38th Conference on Neural Information Processing Systems (NeurIPS 2024).",
  "B-cosied (Ours)B-cos": ": B-cosification: Obtaining inherently interpretable models with competitive accuracy at low cost.Left: Accuracy progression over epochs for a DenseNet-121 and a ViT-S, comparing B-cosified (blue) and B-cos(orange) training curves. B-cosified models achieve equivalent accuracy with a substantial reduction in trainingtime, yielding 4.7x speedup for DenseNet-121 and 9.0x speedup for ViT-S. Right: Qualitative comparisonof explanations for various images for B-cos and our B-cosified models at various stages of training.Specifically, we show the dynamic linear mappings W(x) computed by the models in color as in ; notethat by formulating conventional models (initial in the plot) as a specific version of B-cos models, we are ableto visualise the corresponding explanations in color too, see Sec. 3.2.1 for further details. We find that after onlyone epoch of training, the B-cosified models exhibit similar explanations as B-cos models. as the recently proposed B-cos Networks . In contrast to the original B-cos Networks, whichleverage existing architectures to obtain performant and interpretable models, we investigate whetherwe can additionally leverage the existing pre-trained weights, thus aiming to take advantage of thesignificant amount of resources that have been invested in training existing models. As a result, wehope to make inherently interpretable models more easily accessible to the community. To do so, we first conduct a detailed analysis of how B-cos DNNs differ from their conventionalcounterparts. Interestingly, we find that many existing models can be converted into functionallyequivalent B-cos models by a small set of targeted implementational modifications (Tab. 1). Toincrease the interpretability of the models, we then increase the alignment pressure via theparameter B of the B-cos transformations and fine-tune the models on their respective tasks, whichleads to significantly more interpretable explanations (). On supervised settings, we find that B-cosified models often outperform both conventional andB-cos DNNs at a fraction of the full training cost (, left), whilst exhibiting a similar degreeof interpretability as the original B-cos DNNs (, right). We further apply B-cosification to apre-trained CLIP model , a large foundational vision-language model (VLM), and show thatdespite using comparatively limited data and compute cost, B-cosified CLIP models yield highlyinterpretable explanations whilst being competitive on zero-shot performance across a variety ofdownstream datasets. Our work thus opens a new perspective on how to design inherently interpretable models in a cost-effective manner. Importantly, on the one hand it highlights that conventional models might be closerto inherently interpretable models than previously understood. On the other hand, it highlights thebenefits of designing inherently interpretable models via minor architectural modifications, such ase.g. the B-cos DNNs, as this can allow for leveraging the large array of existing, pre-trained DNNs.",
  "We thoroughly study different design choices to find an optimal strategy for B-cosification": "We apply B-cosification to supervised image classifiers on ImageNet , including bothCNNs and ViTs, and show that the B-cosified variants perform on par on interpretabilitymetrics while often outperforming in terms of accuracy. Overall, we find that B-cosifying apre-trained black box DNN to be superior on both metrics as compared to training a B-cosDNN from scratch, while being computationally significantly cheaper.",
  "B-cosied CLIP": ": B-cosified CLIP Models. After B-cosifying a CLIP model and fine-tuning it according to our proposedB-cosification scheme, see Sec. 3.2, we find that it is possible to endow the model with the same level of inherentinterpretability as the B-cos models proposed in , whilst maintaining CLIPs zeroshot ability (see ).The resulting linear summaries of the models (W(x)) can be visualised in color (row 3) and provide significantlymore detail than GradCAM explanations (row 2), which are often used to explain conventional CLIP models. We extend B-cosification to CLIP, a foundational VLM, and show that B-cosified CLIPremains highly competitive on zero-shot performance across a variety of downstreamdatasets, while also yielding similar interpretability benefits as B-cos models.",
  "Related Work": "Explanation Methods. Post-hoc attributions have popularly been used tounderstand the decisions of trained DNNs, but have often been shown to be unfaithful to the modelbeing explained . Inherently interpretable models , in contast, incorporatearchitectural changes to the model and can yield explanations that are interpretable and faithful to themodel by design. However, such models need to be trained from scratch, which imposes a significantadditional cost. In this work, we explore fine-tuning for interpretability, and propose a method totransform existing black-box DNNs to inherently interpretable B-cos DNNs, bringing together thebest of both worlds. Attribution Priors have often been used to train or fine-tune models to haveexplanations with desirable properties, such as inducing smoother explanations , consistentexplanations , or to guide models to be right for the right reasons . Similarto such work, we fine-tune black-box DNNs for interpretability, but in contrast, we only makearchitectural modifications to transform the DNNs to B-cos DNNs, and do not use any additionalconstraints on the explanations themselves while training. CLIP Interpretability and Localization. Post-hoc attribution methods have alsobeen used to explain VLMs such as CLIP , however, as with supervised DNNs, their faithfulnessto the model is not guaranteed and the explanations are often coarse-grained and not very humaninterpretable. While inherently interpretable architectures could address this, the high costs oftraining such large models from scratch makes their use unappealing. In this work, we bridgethe gap by instead fine-tuning from pre-trained black-box CLIP models to inherently interpretableB-cosified CLIP variants, and find that the B-cosification process is effective in yielding performantand interpretable models. A separate line of work involves improving localizability of VLMs, andis orthogonal to our work since our goal is to obtain explanations that are faithful to the model. Learning mappings between model features. Recent work has explored using simplelinear transforms to map features between models, and in particular also mapping features fromarbitrary models to CLIPs representation space. In the context of our work, such methods can beused to map a supervised B-cos feature extractor to CLIP using a linear transform, to obtain aninherently interpretable DNN that can mimic CLIP. In our evaluation, we compare with such anapproach, and find that our approach of architecturally transforming the full model and fine-tuningfor interpretability yields improved zero shot performance.",
  "From conventional to B-cos models": "In the following, we describe the process of fine-tuning standard black box DNNs into inherentlyinterpretable B-cos DNNs. In Sec. 3.1, we first introduce the B-cos models and enumerate the keyways in which they differ from standard models. In Sec. 3.2, we then perform a detailed study onstrategies to bridge each of these differences for effective B-cosification.",
  "B-cos Models: Background": "Many common DNNs consist of a series of blocks of linear layers followed by non-linear ReLUactivations , and are thus piece-wise linear functions1: i.e., for every input x, they effectivelycompute a linear transformation of that input: y(x) = W(x)x + b(x), cf. . In , modelsof this kind have been called dynamic linear, a naming convention that we adopt in this paper. Interestingly, for piece-wise linear models, W(x) is given by the models gradient with respect to xexcept for the input-dependent bias b(x), the gradient thus constitutes an exact summary of themodels computations. This linear mapping W(x) is unfortunately typically not easily interpretable,and many techniques have been proposed to derive qualitatively more convincing explanations. These, however, have been shown to often not faithfully reflect the underlying model . Further, if the models employ bias terms, W(x) does not yield a complete explanation , i.e.y(x) = W(x)x. Integrating bias terms as proposed by yields a set of importance attributionmaps, summarizing which requires carefully selecting a post-processing function with inherenttradeoffs. Even when not using bias terms , however, the resulting matrices W(x) are often noteasily human interpretable, and the resulting models can suffer from significant drops in performance. To address this, propose to architecturally modify the DNNs to introduce additional alignmentpressure during model optimisation. For this, they replace the ubiquitously used linear transformationby the B-cos transformation, which dynamically scales the output of the linear transformations:",
  "B-cos transformation:fB-cos(x; w) =|cos(x, w)|B1 wT x = wT (x)x ,(1)with B a hyperparameter, cos the cosine similarity between x and the weights w, and w=w/w": "Like piece-wise linear models, B-cos models are dynamic linear and thus accurately summarised bya single linear transformation W(x) s.t. y(x) = W(x)x; as B-cos models do not employ bias terms,this model summary is complete. Crucially, it has been shown that with B>1, the matrix W(x)aligns with task-relevant input patterns, making it more easily human interpretable (e.g. , right). Importantly, as the B-cos transformation can serve as a drop-in replacement for linear transformationsat every layer of a DNN, it is possible to leverage existing DNN architectures and the resultingB-cos models obtain similar classification accuracies as their conventional counterparts (Tab. 4, cols.pretrained and B-cos). Extending this, we investigate if it is possible to leverage existing DNN weightsi.e., our goal is tofine-tune existing models to be similarly interpretable as B-cos models, whilst not requiring to trainthem from scratch. However, despite the architectural similarities between B-cos and conventionalmodels, there are multiple key differences that make transforming pre-trained models into B-cosmodels non-trivial: e.g., apart from replacing linear transformations with the B-cos transformationand not employing biases, B-cos models are trained on image representations with 6 color channels as[r, g, b, 1r, 1g, 1b] to be able to visualise the model-inherent linear summaries W(x) in color,whereas conventional models use 3 channels (see also Tab. 1). In the next section, we show how toovercome these differences and convert existing models into functionally equivalent B-cos models.",
  "B-cosification of Deep Neural Networks": "We analysed the differences between B-cos models and their conventional counterparts in detail andcompiled the results in Tab. 1. In this section, we discuss one by one how to bridge these differences.In particular, we show that a conventional model can be framed as a functionally equivalent B-cosmodel as in with B=1, which additionally employs bias terms. Only upon modifying these twoaspects, i.e. biases and B, does the model need to be fine-tuned to adapt the weights to those changes. 1As noted by , piece-wise linear is actually a misnomer. As the models additionally employ biases, theresulting DNNs are in fact piece-wise affine. For simplicity, we maintain the common naming convention. : Overview. To allow for comparing the models, we compiled the identified differences between theconventional models (Standard), their B-cosified version (B-cosified) and the original B-cos models (B-cos).For each design choice in the B-cosified models, we summarise the respective discussion in Sec. 3.2 (reason)).",
  "Functionally Equivalent B-cos Models": "Input Encoding and Normalisation. As mentioned in Sec. 3.1, B-cos models use input represen-tations with six color channels [r, g, b, 1r, 1g, 1b] to be able to visualise the explanations incolor, cf. . However, most conventional DNNs (e.g. models from Torchvision , CLIP )are applied to 3-channel inputs in which images are encoded via [r, g, b]. As a result, visualising thedynamic matrices W(x) of piece-wise linear models (cf. Sec. 3.1) in color would not seem possible. However, we note that in combination with the commonly used input normalisation, we can convertthe first linear transformation in conventional models (e.g., a convolutional layer) into an equivalenttransformation that accepts 6-channel inputs. Specifically, for input normalisation, the channel-wisemeans s are subtracted from the individual channels, followed by a division by the standard devia-tions s, yielding s=(s s)/s for s{r, g, b}. Conversely, mean-normalising the 3 additionalcolor channels yields s. Leveraging this, we use the models weights learnt for 3-channel inputs,wj = [wj,r, wj,g, wj,b] for every feature j, to construct an equivalent 6-channel transformation:",
  "Note that applying wj to the mean-normalised, 6-channel inputs yields the same results as applyingwj to the original mean-normalised inputs that the pre-trained models have seen during training": "Activation Functions. Owing to the non-linearity inherent to the B-cos transform, explicit activationfunctions are not necessary in between B-cos layers. However, the authors of showed that themodel-inherent explanations are compatible with MaxOut . Note that the very commonly usedReLU non-linearity applied to vT x for any weight vector v, is just a special case of MaxOut:",
  "As the pre-trained models weights have been optimised for the ReLU non-linearity and given itscompatibility with the B-cos explanations, we leave them untouched in the B-cosification process": "Weight normalization. B-cos transformations employ unit norm weights, see also Eq. (1), which theauthors motivated by the fact that the only way any given neuron can achieve its maximal output is byincreasing the weight-input alignment, which in turns leads to the improvements of the explanations. However, conventional models have been trained with unconstrained weights and using unit normweights would thus lead to unpredictable model behaviour. Interestingly, we note that the weightnormalisation in the latest version of the B-cos models can actually not impact the explanation quality,as the authors of re-introduce normalisation layers into the B-cos models. To better understandthis, let us consider the compound function of a batch normalisation layer and a B-cos layer:",
  "In particular, the output of f(x) is thus invariant to weight normalisation, as the output of B-cos (x)scales linearly with the weight norm, cf. Eq. (1)": "This is of course only true if every B-cos layer were always followed by a normalisation layer, whichis not necessarily the case. Nonetheless, we find that not using normalised weights yields consistentlygood results across all models. Therefore, we use B-cos transformations without weight normalisationthroughout our experiments; for an ablation, see Tab. B2 in the appendix. In summary, we showed that it is possible to adapt the implementation of existing models in away that allows us to integrate certain aspects of B-cos models without functionally changing thepre-trained models. Notably, we can now visualise color explanations similar to B-cos models (,right, col. 3); unsurprisingly, however, these explanations have poor interpretability due to the absenceof the alignment pressure imposed during B-cos training. In the next section, we discuss the necessaryfunctional changes for B-cosification to obtain interpretable explanations.",
  "Fine-tuning for Interpretability": "The changes introduced in the preceding section have not functionally changed the pre-trainedmodels, but rather allow us to interpret the existing models as a special case of B-cos models. Now weintroduce the necessary changes to increase the interpretability of the dynamic matrices W(x). Asthese functionally change the models, they need to be fine-tuned to recover their original performance. In particular, the remaining differences between conventional and B-cos models are (1) the value of B,and (2) the use of biases, Tab. 1. We will now discuss how we bridge these differences individually. Ablation Setup. We evaluate various fine-tuning strategies using a ResNet-18 model supervisedon ImageNet from Torchvision for B-cosification, and compare with a B-cos ResNet-18from . We optimize using AdamW with cosine scheduling and train for 90 epochs, andevaluate both classification accuracy as well as interpretability using the GridPG metric . (1) Increasing B. As shown in , using B>1 is critical to obtain easily interpretable explanations.To increase B for the pre-trained models, we investigate three strategies: (1) immediately setting Bto a higher value and then fine-tuning, (2) linearly interpolating from B = 1 to B = 2 throughoutfine-tuning, and (3) setting B as a learnable parameter. (2) has the advantage of changing the model insmall steps, making it more likely that it maintains performance while fine-tuning, but requires usingthe full number of epochs to reach the target value of B. (1) on the other hand is likely to adverselyaffect the utility of the weights, but offers the opportunity to stop fine-tuning early if performance andinterpretability metrics are sufficiently high. (3) offers the most flexibility, but also adds a new set ofparameters that need to be optimized. We show the results of this evaluation in Tab. 2. Interestingly,we find that using (1), i.e. setting B = 2 and then fine-tuning, yields performance that is on par withlearnable B parameters, whilst being significantly simpler to implement. To easily test the generalityof the B-cosification scheme, we therefore opt for this approach in Sec. 4.1. (2) Decreasing biases. As discussed in Sec. 3.1, dynamic linear models with bias terms are notexactly summarised by the matrix W(x), cf. . To obtain the same level of faithfulness of theexplanations as B-cos models (in particular w.r.t. explanation completeness, cf. ), we need toremove the biases from the model. To do so, we investigate two approaches: (1) removing all biasesfirst and then fine-tuning, and (2) fine-tuning while decaying biases using weight decay. Similarto the setup with B, (2) has the advantage of avoiding drastic changes to the model, but requirespotentially fine-tuning for longer. Further, the weight given to the bias decay in the loss constitutes atradeoff between maintaining classification performance and pushing the biases to be close to zero.We report the results of this evaluation in Tab. 3. Similarly to the experiments for B, we find thatimmediately setting the biases to zero constitutes a simple yet performant approach to achieve bothgood localisation and accuracy. To assess the generality of the B-cosification scheme across a widerange of models, we thus choose this the simpler approach of setting biases to zero in Sec. 4.1.",
  "B-cosification Results": "In the following, we evaluate the effectiveness of the B-cosification strategy we developed in Sec. 3. InSec. 4.1, we first apply B-cosification to supervised models across various architectures, and evaluatefor classification performance and interpretability. In Sec. 4.2, we B-cosify CLIP , a largefoundational vision-language model, and show that despite fine-tuning at a fraction of the trainingcost, the B-cosified CLIP shows strong zero shot generalization whilst being highly interpretable.",
  "Supervised Classification Models": ": Classification Accuracy. We report the top-1 classification accuracy on the ImageNet validation set ofthe pre-trained models (pretrained) and the B-cosified models (B-cosified) after fine-tuning them. Additionally,we report the accuracy of the corresponding B-cos models trained from scratch (B-cos) as well as the differenceto them (acc), and how much faster and at which epoch (t) the same accuracy as in was achieved (speedup).Results for B-cosified models are averaged over three runs; full results including standard deviation in appendix.",
  "ViTc-B76.877.176.7-0.4--ViTc-L77.977.877.1-0.7--": "Setup. We B-cosify models from Torchvision supervised on ImageNet . We use a diverseset architectures, including both CNNs (ResNet-18 , ResNet-50 , and DenseNet-121 ),and ViTs with (ViTc-Ti, ViTc-S, ViTc-B, ViTc-L) and without (ViT-Ti, ViT-S, ViT-B, ViT-L)convolutional stems. For ResNet-50, we use both the weights originally released by Torchvision andthe updated V2 weights, which constitute models trained for longer and with more augmentations. As in Sec. 3.2, we evaluate both for classification accuracy and for interpretability using theGridPG metric. We compare both accuracy and interpretability of the B-cosified models withB-cos models trained from scratch from . For interpretability, we also compare with severalpost-hoc attribution methods as baselines, namely Guided Backprop , Gradient , DeepLIFT, IxG , IntGrad , and GradCAM . For full details, see Appendix C.1. Classification performance. Tab. 4 reports the classification accuracy of the B-cosified modelsacross architectures, and compares them with their conventional counterparts from Torchvision andB-cos models trained from scratch. We find that across architectures (col. 1), B-cosified modelsperform competitively with conventional DNNs (cols. 2-4) and interestingly, in contrast to thefindings reported by , often outperform them, i.e. for five out of twelve architectures. Notably,we find (col. 5) that our B-cosified models significantly outperform B-cos models trained from",
  "Standard Model W(x)xB-cosied Model W(x)xB-cos Model W(x)x": ": Localisation Performance of W(x)x. We compute the contribution maps according to the dynamiclinear summaries W(x) of the pre-trained models (Standard), their B-cosified versions, and the originalpre-trained B-cos models and evaluate their localisation performance on the Grid Pointing Game as in . Wefind localisation to significantly improve for B-cosified models, achieving results on par with the models of .",
  "GBGradDeepLIFTIxGIntGradGradCAMW(x)x B-cosied": ": Comparison to Post-hoc Methods. For two of the models in (ResNet-50-v1, DenseNet-121) wecompare the localisation performance of the dynamic matrices W(x)x to post-hoc explanations for the pre-trained models. Similar to the original B-cos models , the model-inherent explanations perform favourably. scratch across all but the two largest ViTc architectures. Further, B-cosified models often achievethe same performance as their corresponding B-cos models at a fraction of the training cost (col. 6).Specifically, we find that averaged across architectures, B-cosified models outperform B-cos modelstrained from scratch by 2.5 pp, with an average training speedup (to match performance) of 5.2x.These results strongly advocate for B-cosification as a superior alternative to training from scratch forobtaining performant inherently interpretable models at a low compute cost. Interpretability. To evaluate the interpretability of our B-cosified models, we report the GridPGlocalization scores in , and compare with conventional and B-cos models; following , wereport the results of 3x3 image grids for convolutional models, and of 2x2 grids for the ViTs. For a faircomparison, for all models, we evaluate the localization of the dynamic linear summary of the model2W(x)x (see Sec. 3.1). We find that across architectures, B-cosified models significantly outperformconventional DNNs in terms of localization (32.7pp-71.0pp) and perform on par with B-cos models.Since post-hoc attribution methods (e.g. ) are often used to interpret conventional DNNs,similar to , in , we compare the localization of the model inherent explanations from two ofour B-cosified models with post-hoc explanations applied to the corresponding conventional models.Similar to the results reported by , we find our B-cosified models to strongly outperform allpost-hoc methods, including GradCAM , with a near perfect localization score, showing thatB-cosification is effective in yielding highly interpretable yet model-faithful explanations. Impact of pre-trained weights. Since our aim is to fine-tune for interpretability, we investigatehow crucial the quality of the weights of the conventional model are for effective B-cosification.Specifically, we expect weights from stronger models to be a better starting point for B-cosification.We evalaute this by performing B-cosification both with v1 and v2 variants of ResNet-50 from Torchvision , where the latter is trained for longer and with stronger augmentations .From Tab. 4, we find that using a strong initialization is highly useful for effective B-cosification, 2Note that ViTs differ from the CNNs discussed in Tab. 1 via the attention mechanism and the GELUactivation with GELU(x) = x (0.5 + 0.5 erf(x/x). As attention is also dynamic linear, cf. , it canseamlessly be integrated into the model summary W(x). Similarly, we interpret the second factor in GELU as adynamic weight w(x), thus allowing us to integrate it in a similar fashion.",
  "Text2ConceptB-cosied (IMN)B-cosied (CC3M)Standard": ": Classification performance on the CLIP Benchmark of various CLIP models for the zero-shotsetting (left) and linear probing (right). Specifically, we compare two B-cosified CLIPstrained on ImageNet(IMN) and CC3M respectivelyto the Text2Concept approach by and the original pre-trained CLIP model.We find B-cosified versions of CLIP to consistently outperform Text2Concept on natural and specialised data. specifically, fine-tuning starting from v2 weights outperforms B-cos models by 0.8pp as compared tov1, and achieves equal accuracy with a 9x speedup as compared to 2x with v1; similar results areobserved for initialising the weights with models pretrained via the self-supervised DINO paradigm (final accuracy: 77.0, speedup: 3.2x), for further discussion see Tab. B3 in the appendix.",
  "In this section, we evaluate our B-cosification paradigm on CLIP , a powerful pre-trainedvision-language model, and evaluate its interpretability and zero shot performance": "Setup. We B-cosify a CLIP with a ResNet-50 backbone using the procedure describedin Sec. 3.2. We use the recently proposed SigLIP loss between the image embeddings of thepre-trained CLIP and the B-cosified CLIPs and train the models on either the ImageNet or theCC3M datasets . For evaluation, we rely on the CLIP Benchmark and report zeroshot andlinear probing results for accuracy. To assess the models interpretability, we explain the similaritybetween the image embeddings and the text embedding of the pre-trained CLIP model via the dynamiclinear summaries, see Sec. 3.1 or GradCAM, and report the EPG scores on the VOC dataset. For full details, see Appendix C.2. Evaluating Model Performance. In , we report the zeroshot and linear probing accuracies ofthe two B-cosified CLIP models (trained on ImageNet or CC3M) and compare it to the original CLIP(Standard) and the recently proposed Text2Concept technique ; for the latter, we train a linearlayer on top of a frozen, pre-trained B-cos ResNet-50 from to mimic the embeddings of CLIP. We find that the B-cosified models significantly outperform the Text2Concept approach andachieve accuracies that are more similar to the original CLIPs zeroshot and linear probing accuracies. Evaluating Model Interpretability. We evaluate the B-cosified CLIPs ability to localise classesin the VOC dataset in two ways. On the one hand, we directly explain the similarity of the modelsembedding to the text embedding of a given prompt such as A photo of a cow.. On the other hand,we note that the final attention pooling layer in the CLIP model only computes a weighted sum ofthe last layers value vectors. Therefore, we additionally evaluate whether we can also explain thesimilarity between the text embeddings and these value vectors to improve the localisation ability. In this context, we notice that explaining the average similarity to the text embedding yields highlydistributed attribution maps, see b, col. 2. On the other hand, explaining only the most similarembedding localises very well, see b, col. 5. To better understand this phenomenon, weadditionally interpolate between these two approaches and compute weighted means",
  "i wivi ofthose value vectors vi, in which the weights are determined by the cosine similarity between thevalue vectors vi and the text embedding t, i.e. with weights wi = cosp(t, vi) for various p": "We find that this not only significantly improves the explanations qualitatively, see Figs. 2 and 6b,but also quantitatively: in a we report results for explaining the final image embedding (B-cosified CLIP), the dynamic linear summary for the CLIP ResNet-50 (CLIP W(x)x), its GradCAMexplanations (CLIP GradCAM), and the weighted mean of the value vectors, which we call",
  "(b)": ": CLIP Localisation. In (a), we compare GradCAM and dynamic linear explanations for the pre-trainedCLIP model to the inherent explanations of the B-cosified CLIP, as well as to our proposed B-cos FeatureCLIPapproach. We find that good localisation with highly detailed explanations (b) is possible via the B-cosifiedCLIP models, especially for high cosine powers, despite only explaining the similarity to text prompts. B-cosified FeatureCLIP. While B-cos CLIP already yields a noticeable improvement over thepre-trained CLIP explained via GradCAM and significantly improves the localisation ability of thelinear summary W(x)x, the weighted means yield even stronger performance for sufficiently high p.",
  "Discussion": "The B-cosification approach presented in this work addresses a common issue with developing inher-ently interpretable models: achieving model interpretability without compromising on performanceor incurring high training costs. By leveraging pre-trained models, B-cosification opens a new pathtowards developing interpretable yet performant models, which can be of particular interest in thecontext of foundation models such as CLIP , which might otherwise be prohibitively expensiveto train on a limited budget. Our results suggest that B-cosification not only maintains but, in severalcases, even enhances model accuracy, whilst yielding significant improvements on interpretabilitymetrics, providing a viable and resource-efficient alternative to training B-cos models from scratch. Specifically, we find B-cosified models to much faster reach the same levels of interpretability andaccuracy than their counterparts trained from scratch, with training speedups of up to 9x in somemodels. The approach appears to be general, being applicable for both CNNs and ViT models. Wehope that this increase in efficiency will make interpretable models much more accessible in settingswith constrained computational resources and could thus facilitate their adoption. In particular, whenapplying our proposed B-cosification scheme to a foundational modelCLIPwe find that theB-cosified CLIP model is able to maintain competitive zero-shot performance while at the same timeproviding interpretable and model-faithful explanations. Despite these advancements, certain aspects remain open for further exploration. Specifically,while some models quickly recover original performance after B-cosification, others exhibit slowerconvergence rates, suggesting potential for optimisations in the fine-tuning process. Additionally,for the larger B-cosified ViTc models, while yielding results that are on par with those trained fromscratch, the B-cosification process did not succeed in achieving speed-ups, indicating that the interplaybetween model architecture and the proposed B-cosification might require further exploration. In summary, our results establish B-cosification as an effective method for enhancing interpretability inpre-trained models with low computational cost. The method consistently enables high interpretabilitywithout compromising performance, even achieving substantial training speedups in many cases.",
  "AAdditional Qualitative Results": "In Fig. A1, we provide additional qualitative examples to illustrate the interpretability gains achievedby B-cosifying a CLIP model. Specifically, we show explanations generated by the original CLIPmodel using GradCAM (row 2) for a diverse set of input images (row 1), for which explanationsare generally coarse and lack clear localization. In contrast, the third row displays explanationsproduced by B-cosified CLIP, which yields finer-grained, more visually interpretable explanations. Fig. A1: Additional, randomly sampled examples for comparing GradCAM explanations of the original CLIPmodel to the inherent explanations of the B-cosified CLIP. The top row shows input images from various classes.The middle row provides explanations generated by the original CLIP model, which tend to be coarse and lackprecise localization. The bottom row shows explanations from B-cosified CLIP, which produce more focused,detailed visualizations, highlighting class-relevant features with greater clarity. In Fig. A2, we show further comparisons on specific object classes with using different cosine powersp (cos, cos-7, cos-19, and cos-inf) to qualitatively demonstrate the effect of increasing the exponentp in gathering the value vectors, see also Sec. 4.2. Higher cosine thresholds result in increasinglyfocused and interpretable representations, capturing fine details that are often absent in the originalCLIP explanations. In Fig. A3, we show additional qualitative examples for prompting the B-cosified CLIP model withdifferent prompts for the same image, thus highlighting the class-specificity of the explanations aswell as the potential that inherently interpretable CLIP models might yield. Specifically, B-cosifiedCLIP models allow to explain the similarity of a given image with a free-form textual prompt, whichshows that the zero-shot performance of CLIP with respect to classification also transfers well to thecorresponding explanations. Fig. A2: Additional randomly chosen examples highlighting the effect of increasing cosine power p on thespecificity of the explanations in the B-cosified CLIP model. Each row corresponds to a specific object class,with explanations generated at different cosine power levels: cos, cos-7, cos-19, and cos-inf. Higher cosine powervalues result in increasingly precise and interpretable representations, capturing finer details and producingsharper focus on class-relevant features; further examples in b in the main paper.",
  "Pop Bottle": "Fig. A3: B-cosified CLIP text-based localisation. We find different objects can be localised in high-qualitycolor using B-cosified CLIP. For example, we compute B-cos explanations using text-prompts: a picture of{x} encoded using standard CLIP text-encoder for an image taken from ImageNet . The first columnshows the original image with the class caption, the second column shows the colored B-cos explanations withthe text caption used at the top, and the third column shows the raw attributions with red denoting the positivecontribution towards the text input and blue denoting the negative contribution.",
  "BAdditional Quantitative Results": "In this section, we provide a series of additional quantitative results on the performance and in-terpretability of B-cosified models. These tables cover various ablation studies, comparisons withstandard and B-cos models, and performance across different configurations. Specifically, in Tab. B1,we extend our evaluation to compare models that are trained for the same effective number of epochs3.Further, in Tab. B2, we evaluate the impact of using normalized weights in the B-cos layers. InTab. B3, we show additional results for B-cosified ResNet-50 models that are initialised from differentpre-trained checkpoints. Specifically, we find that apart from initialising the ResNet-50 from CLIPweights, we observe consistent improvements through B-cosification, with stronger pre-trainingparadigms that are based on the ImageNet dataset (V2, DINO ) leading to larger improvements.Finally, we report full ablation results on the impact of the individual changes that we perform on thepre-trained models Tab. B4, B-cosifying models with different strategies for setting the parameter Bin the B-cos transformation Tab. B5, decaying the bias term Tab. B6, as well as full zero-shot andlinear probing results for the CLIP benchmark, see Tabs. B7 and B8. Table B1: Classification accuracy comparison with further trained standard and B-cos models. ExtendingTab. 4, we report the comparison of top-1 classification accuracy on the ImageNet validation set between theB-cosified models and comparison with standard (block 3) and B-cos (block 4) pre-trained models fine-tunedusing the same process as B-cosification. All models are thus effectively trained for 180 epochs. Results forB-cosified models are averaged over three runs.",
  "ViTc-B76.90.277.2-0.3-77.2-0.3-ViTc-L77.10.077.6-0.5-77.7-0.6-": "3B-cosified models have effectively been trained for 180 epochs, i.e., 90 epochs of pre-training and 90 epochsof fine-tuning. To fairly compare the impact of the additional training, we show how this would impact both thepre-trained models themselves, as well as the B-cos models from . Note that however the primary goal is toleverage pre-trained weights to more efficiently train B-cos models, and as shown in Tab. 4, B-cosification helpsobtaining similarly accurate and interpretable models at a much lower training cost. Table B2: Ablation normed weights for the ViT models. Extending Tab. 4 for a subset of models, wereport the top-1 classification accuracy on the ImageNet validation set of the pre-trained models (pretrained)and the B-cosified models (B-cosified) after fine-tuning them along with the difference between them (1acc).Additionally, we report the accuracy of the corresponding B-cos models trained from scratch (B-cos) as well asthe difference to them (2acc), and how much faster and at which epoch (t) the same accuracy as in wasachieved (speedup). Results for B-cosified models are averaged over three runs.",
  "ViTc-B76.878.30.1+1.577.1+1.2323.8": "Table B3: Impact of Pre-trained Weights. We report the top-1 classification accuracy on the ImageNetvalidation set of the B-cosified ResNet-50 model with different weight initialisations (acc), the differenceto the results reported in when training the pre-trained B-cos ResNet-50 model (acc) under the sameB-cosification recipe, as well as how much faster the same accuracy was achieved (speedup). Additionally,we report the GridPG localisation scores (loc) similar to those reported in and the difference to the B-cosResNet-50 models localisation trained (loc) under the same B-cosification training recipe. The pre-trainedaccuracy (in %) for: B-cos = 75.9, CLIP = 73.3, and DINO = 75.3. Random Init is the baseline with randomlyinitialized B-cosified model training. Results are for a random initialized single run.",
  "DINO77.0+1.43.290.9+0.5": "Table B4: Experimental ablations results for the B-cosification design choices. The table shows the ablationresults for the B-cosified ResNet-18 model (with B = 2 and no Bias) shown in row 1. Further in rows 2-6, theablated components are shown in col. 1, the accuracy of the ablated model (acc) in col. 2, and the change inaccuracy (acc) compared to the non-ablated model (row 1) is shown in col. 3. Further, col. 4 shows the GridPGlocalisation scores (loc) and the difference to the non-ablated B-cosified model (loc). Similar to componentsused in standard models, we replace the BatchNorm Uncentered with BatchNorm Centered, change the order ofthe Global Average Pool where features are first averaged and then passed to the last linear layer, and replaceaverage pooling with max pooling at the stem. Further, we also replaced ReLU activation with Identity; andremoved the Logit Bias layer. Results are for a random initialized single run.",
  "Max Pool (Stem)71.4-0.187.7+0.3": "Table B5: Increasing B for B-cosification Extended. Extending Tab. 2 for different convolutional models,we compare various strategies to increase the B parameter. We evaluate these strategies using accuracy andlocalization scores to measure interpretability performance. Columns 2-3 represent the baseline models: StandardResNet-18 and B-cos ResNet-18 . Columns 4-8 set the B value directly {1, 1.25, 1.5, 1.75, 2}.Columns 9-13 apply B = 2 over n epochs with n {5, 10, 20, 45, 90}. Column 14 shows results forincreasing B as a learned parameter. Further, row blocks denote different models, each denoting accuracy,followed by localisation scores. Results are for a random initialized single run.",
  "DenseNet-121Accuracy74.473.675.8376.376.476.676.476.476.476.476.375.876.5Localisation20.292.330.477.087.089.991.291.491.791.892.292.293.6": "Table B6: Decreasing biases for B-cosification extended. Extending Tab. 3 for different convolutional models,we compare various strategies to decrease the bias parameter. We evaluate these strategies using accuracy andlocalization scores to measure interpretability performance. Columns 2-3 represent the baseline models: StandardResNet-18 and B-cos ResNet-18 . Columns 4-5 show the setting with bias (col. 4) and without bias (col.5). Columns 6-8 show the bias decay setup using the weight decay with different values {0.2, 0.5, 0.9}.Further, row blocks denote different models, each denoting accuracy, followed by localisation scores. Resultsare for a random initialized single run.",
  "DenseNet-121Accuracy74.473.676.376.476.876.976.9Localisation20.292.386.991.290.090.390.7": "Table B7: Zero-shot performance of various CLIP-based models over 38 datasets using CLIP Benchmark. Scores within the 99.5% Clopper-Pearson confidence interval of each datasets top score are shown in bold.Baselines contain results for the Standard CLIP and Text2Concept (T2C) models; ImageNet and CC3Mcolumn sections contain the B-cosified RN-50 CLIP models trained with cosine and cyclic learning schedulerstrained with ImageNet and CC3M datasets, respectively. The cyclic learning training inspired from. Dataset type is taken from .",
  "Standard T2C CosineCyclicCosineCyclic": "Naturalcars0.540.020.370.380.350.36country2110.150.030.120.120.120.13fer20130.350.180.210.190.200.23fgvc_aircraft0.170.020.110.120.100.11gtsrb0.350.050.210.200.320.31imagenet-a0.230.040.160.160.140.13imagenet-o0.570.680.650.640.570.57imagenet-r0.610.280.520.520.530.53imagenet1k0.600.520.590.590.520.52imagenet_sketch0.350.150.280.280.290.30imagenetv20.530.420.480.480.440.44objectnet0.410.230.330.320.320.31stl100.940.910.940.940.930.93sun3970.600.290.620.600.550.54voc20070.650.650.630.620.620.61vtab/caltech1010.770.740.740.740.720.72vtab/cifar100.710.350.710.710.710.72vtab/cifar1000.400.090.400.410.370.36vtab/dtd0.410.290.410.420.370.38vtab/flowers0.660.070.580.580.560.58vtab/pets0.860.690.830.850.800.81vtab/svhn0.300.080.110.150.130.14 Specializedimagenet_sketch0.350.150.280.280.290.30mnist0.580.170.380.370.380.37renderedsst20.560.500.500.500.500.50vtab/diabetic_retinopathy0.170.690.380.430.290.35vtab/eurosat0.410.270.360.330.410.42vtab/pcam0.640.500.520.690.520.50vtab/resisc450.450.150.280.290.350.37 Structuredvtab/clevr_closest_object_distance0.150.150.140.150.250.24vtab/clevr_count_all0.220.150.220.210.270.29vtab/dmlab0.150.190.160.190.160.17vtab/dsprites_label_orientation0.010.020.050.060.060.05vtab/dsprites_label_x_position0.030.030.060.060.060.06vtab/dsprites_label_y_position0.030.030.110.120.120.13vtab/kitti_closest_vehicle_distance0.170.180.120.110.170.17vtab/smallnorb_label_azimuth0.060.060.050.060.050.06vtab/smallnorb_label_elevation0.110.120.120.120.120.13 Table B8: Linear-Probe performance of various CLIP-based models over 29 datasets using CLIP Benchmark. Scores within the 99.5% Clopper-Pearson confidence interval of each datasets top score are shown inbold. Baselines contain results for the Standard CLIP and Text2Concept (T2C) models; ImageNetand CC3M column sections contain the B-cosified RN-50 CLIP models trained with cosine and cyclic learningschedulers trained with ImageNet and CC3M datasets, respectively. datasets, respectively. Thecyclic learning training inspired from . Dataset type is taken from .",
  "C.1.1Models": "We B-cosify models from Torchvision supervised on ImageNet . We use a diverse setarchitectures, including both CNNs (ResNet-18 , ResNet-50 , and DenseNet-121 ),and ViTs with (ViTc-Ti, ViTc-S, ViTc-B, ViTc-L) and without (ViT-Ti, ViT-S, ViT-B, ViT-L)convolutional stems. For ResNet-50, we use both the weights originally released by Torchvision andthe updated V2 weights, which constitute models trained for longer and with more augmentations.",
  "C.1.2Datasets": "We use ImageNet to fine-tune all the B-cosified standard models and evaluate them on ImageNetsvalidation set. For training, we use train transforms - crop size of 224, horizontal flip with 0.5probability, random resized crop of 224 with bilinear interpolation, Add Inverse transform andmodified mean-std normalisation (to accommodate for 6 channel input from the AddInverse). Forevaluation, instead of a random resized crop, we do a center crop with a crop size of 224.",
  "C.1.3Optimization": "For each architecture, we use the B-cosification stategy derived in Sec. 3, and fine-tune for 90 epochsusing the AdamW optimizer and cosine scheduling for the learning rate learning rate of 104for the convolutional models (since the standard pre-trained models end with a learning rate of 104at the 90th epoch, from which we want to fine-tune further). For ViTs, as the learning rate decays to avery small value, we tested with different learning rates (103, 104, 105) and found 103 workedbest for all the models. Also, we only use a linear learning rate warmup of 10,000 steps with a decayof 0.01 for the base and large ViT models.",
  "C.1.4Experiments": "Increasing B: We tested three different setups for increasing B. 1) Discrete B setting to1, 1.25, 1.5, 1.75, 2, 2.5, 3, 5, 7; 2) Linear increase of B in n epochs from B=1 to B=2. We usedn = 5, 10, 20, 45, 90; 3) Learning B parameter to increase to B=2 using weight decay with coeffi-cients 0.2, 0.5 and 0.9. See Tab. 2 for results. Removing biases: We test two setups for removing the biases from the network: 1) Removing all thebias parameters; 2) Decay the bias parameter using the weight decay with coefficients 0.5 and 0.9.See Tab. 3 for results. Impact of pre-trained weights: To check the impact of pre-trained weights on fine-tuning, wefine-tuned weights from CLIP ResNet-50 , DINO ResNet-50 , and Torchvision ResNet-50 weights v1 and v2 (long trained recipe) .",
  "C.1.5Evaluation": "As in Sec. 3.2, we evaluate both for classification accuracy and for interpretability using the GridPG metric. We compare both accuracy and interpretability of the B-cosified models with B-cosmodels trained from scratch from . For interpretability, we also compare with several post-hocattribution methods as baselines, namely Guided Backprop , Gradient , DeepLIFT , IxG, IntGrad , and GradCAM . Qualitatively, we visualize the colored B-cos explanationsand the attribution maps .",
  "C.2.1Datasets": "We use ImageNet and CC3M to fine-tune all the B-cosified CLIP models and test themon multiple datasets from CLIP benchmark . For training, we use the same transform setup asthe standard B-cosified models. We use train transforms - crop size of 224, horizontal flip with 0.5probability, random resized crop of 224 with bilinear interpolation, and Add Inverse transform and modified mean-std normalisation (to accommodate for 6 channel input from the AddInverse) asdiscussed in the paper. For evaluation, instead of a random resized crop, we do a center crop with acrop size of 224.",
  "C.2.2Evaluation": "We use the CLIP benchmark for zeroshot and linear probing experiments with the defaultparameters provided in the official benchmarking code. For text-based localisations, we use thetext-based templates from the CLIP for the ImageNet dataset and use them to encode the text features.As text encoder, we use the CLIP ResNet-50 text encoder. The cosine scores between the B-cosifiedCLIPs image encoding and the pre-trained text encoder are used to do B-cos style localisations andcalculate the GridPG scores. We use the unpooled features technique at inference to increase thelocalisation focus.",
  "C.2.3Optimization": "We use the Adam optimizer and fine-tuned models till 90 epochs, while the CC3M models arefine-tuned for 30 epochs. The size of CC3M is approximately three times that of ImageNet, so thetrained models are comparable. Keeping consistent with the Standard B-cosification recipe, we trainwith a learning rate of 1e-4 using cosine scheduling. SigLIP contrastive loss is used to train themodels.",
  "Answer: [Yes]": "Justification: Our theoretical arguments are contained in Sec. 3.2.1, namely that (1) itis possible to convert the first layer of DNNs such that they accept 6 channel inputs asrequired by B-cos models, that (2) ReLU is a special case of MaxOut, and that (3) weightnormalisation is irrelevant if a layer is followed by a batch normalisation layer. These claimsare shown to be true in the context of Eq. (2) (1), Eq. (3) (2) and Eq. (5) (3).",
  "The answer NA means that the paper has no limitation while the answer No means thatthe paper has limitations, but those are not discussed in the paper": "The authors are encouraged to create a separate \"Limitations\" section in their paper. The paper should point out any strong assumptions and how robust the results are toviolations of these assumptions (e.g., independence assumptions, noiseless settings,model well-specification, asymptotic approximations only holding locally). The authorsshould reflect on how these assumptions might be violated in practice and what theimplications would be. The authors should reflect on the scope of the claims made, e.g., if the approach wasonly tested on a few datasets or with a few runs. In general, empirical results oftendepend on implicit assumptions, which should be articulated. The authors should reflect on the factors that influence the performance of the approach.For example, a facial recognition algorithm may perform poorly when image resolutionis low or images are taken in low lighting. Or a speech-to-text system might not beused reliably to provide closed captions for online lectures because it fails to handletechnical jargon.",
  "If applicable, the authors should discuss possible limitations of their approach toaddress problems of privacy and fairness": "While the authors might fear that complete honesty about limitations might be used byreviewers as grounds for rejection, a worse outcome might be that reviewers discoverlimitations that arent acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an impor-tant role in developing norms that preserve the integrity of the community. Reviewerswill be specifically instructed to not penalize honesty concerning limitations.",
  "If the contribution is a dataset and/or model, the authors should describe the steps takento make their results reproducible or verifiable": "Depending on the contribution, reproducibility can be accomplished in various ways.For example, if the contribution is a novel architecture, describing the architecture fullymight suffice, or if the contribution is a specific model and empirical evaluation, it maybe necessary to either make it possible for others to replicate the model with the samedataset, or provide access to the model. In general. releasing code and data is oftenone good way to accomplish this, but reproducibility can also be provided via detailedinstructions for how to replicate the results, access to a hosted model (e.g., in the caseof a large language model), releasing of a model checkpoint, or other means that areappropriate to the research performed. While NeurIPS does not require releasing code, the conference does require all submis-sions to provide some reasonable avenue for reproducibility, which may depend on thenature of the contribution. For example",
  "(b) If the contribution is primarily a new model architecture, the paper should describethe architecture clearly and fully": "(c) If the contribution is a new model (e.g., a large language model), then there shouldeither be a way to access this model for reproducing the results or a way to reproducethe model (e.g., with an open-source dataset or instructions for how to constructthe dataset). (d) We recognize that reproducibility may be tricky in some cases, in which caseauthors are welcome to describe the particular way they provide for reproducibility.In the case of closed-source models, it may be that access to the model is limited insome way (e.g., to registered users), but it should be possible for other researchersto have some path to reproducing or verifying the results.",
  ". Open access to data and code": "Question: Does the paper provide open access to the data and code, with sufficient instruc-tions to faithfully reproduce the main experimental results, as described in supplementalmaterial?Answer: [Yes]Justification: We mainly rely on publicly available code and established training paradigmsand clearly describe any changes we introduce. That said, we make our code available toensure full reproducibility.Guidelines:",
  ". Experimental Setting/Details": "Question: Does the paper specify all the training and test details (e.g., data splits, hyper-parameters, how they were chosen, type of optimizer, etc.) necessary to understand theresults?Answer: [Yes]Justification: We rely on standard datasets (CC3M, ImageNet-1k) or publicly availablebenchmarks (github.com/LAION-AI/CLIP_benchmark) and describe our training and eval-uation procedure in detail; see the setup sections in Sec. 3.2.2, Sec. 4.1, Sec. 4.2, andSec. C.Guidelines: The answer NA means that the paper does not include experiments. The experimental setting should be presented in the core of the paper to a level of detailthat is necessary to appreciate the results and make sense of them.",
  ". Experiment Statistical Significance": "Question: Does the paper report error bars suitably and correctly defined or other appropriateinformation about the statistical significance of the experiments?Answer: [Yes]Justification: Given the scope of ablations (Sec. 3.2.2), datasets and models (Sec. 4), aswell as computational cost (see question 8 in the checklist), each experimental result iscurrently based on three training runs. That said, we observe consistent behaviour across allour experiments, which corroborates the validity of our claims (see, e.g., Tab. 2 and Tab. 3).Guidelines: The answer NA means that the paper does not include experiments. The authors should answer \"Yes\" if the results are accompanied by error bars, confi-dence intervals, or statistical significance tests, at least for the experiments that supportthe main claims of the paper. The factors of variability that the error bars are capturing should be clearly stated (forexample, train/test split, initialization, random drawing of some parameter, or overallrun with given experimental conditions).",
  ". Experiments Compute Resources": "Question: For each experiment, does the paper provide sufficient information on the com-puter resources (type of compute workers, memory, time of execution) needed to reproducethe experiments?Answer: [Yes]Justification: We use NVIDIA A100-SXM4-40GB and Quadro RTX 8000 GPUs frominternal cluster. We require 4 GPUs per run (except ViTs with 8 A100GPUs for ViT models).Runtime per run: 1-2 days depending on model size for standard models and ImageNetfine-tuned CLIP; for CC3M fine-tuned CLIP - 28 days for 90 epochs runs on Quadro RTX8000 GPUs. We have 500 experiment runs in total approximately (CC3M - 5 runs, ViTs- 150 runs, 345 rest of the models). For storing all the experimental results, we utilizeapproximately 2.5 TB of storage.Guidelines: The answer NA means that the paper does not include experiments. The paper should indicate the type of compute workers CPU or GPU, internal cluster,or cloud provider, including relevant memory and storage.",
  "Guidelines:": "The answer NA means that there is no societal impact of the work performed. If the authors answer NA or No, they should explain why their work has no societalimpact or why the paper does not address societal impact. Examples of negative societal impacts include potential malicious or unintended uses(e.g., disinformation, generating fake profiles, surveillance), fairness considerations(e.g., deployment of technologies that could make decisions that unfairly impact specificgroups), privacy considerations, and security considerations. The conference expects that many papers will be foundational research and not tiedto particular applications, let alone deployments. However, if there is a direct path toany negative applications, the authors should point it out. For example, it is legitimateto point out that an improvement in the quality of generative models could be used togenerate deepfakes for disinformation. On the other hand, it is not needed to point outthat a generic algorithm for optimizing neural networks could enable people to trainmodels that generate Deepfakes faster. The authors should consider possible harms that could arise when the technology isbeing used as intended and functioning correctly, harms that could arise when thetechnology is being used as intended but gives incorrect results, and harms followingfrom (intentional or unintentional) misuse of the technology. If there are negative societal impacts, the authors could also discuss possible mitigationstrategies (e.g., gated release of models, providing defenses in addition to attacks,mechanisms for monitoring misuse, mechanisms to monitor how a system learns fromfeedback over time, improving the efficiency and accessibility of ML).",
  "Justification: We follow standard practice and cite every author of any existing asset that weuse. OpenAI CLIP - MIT License, CLIP Benchmark - MIT License, B-cosV2 - Apache-2.0license.Guidelines:": "The answer NA means that the paper does not use existing assets. The authors should cite the original paper that produced the code package or dataset. The authors should state which version of the asset is used and, if possible, include aURL. The name of the license (e.g., CC-BY 4.0) should be included for each asset. For scraped data from a particular source (e.g., website), the copyright and terms ofservice of that source should be provided. If assets are released, the license, copyright information, and terms of use in thepackage should be provided. For popular datasets, paperswithcode.com/datasetshas curated licenses for some datasets. Their licensing guide can help determine thelicense of a dataset.",
  ". New Assets": "Question: Are new assets introduced in the paper well documented and is the documentationprovided alongside the assets?Answer: [NA]Justification: We do not provide any new assets alongside this submission.Guidelines: The answer NA means that the paper does not release new assets. Researchers should communicate the details of the dataset/code/model as part of theirsubmissions via structured templates. This includes details about training, license,limitations, etc.",
  "According to the NeurIPS Code of Ethics, workers involved in data collection, curation,or other labor should be paid at least the minimum wage in the country of the datacollector": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with HumanSubjectsQuestion: Does the paper describe potential risks incurred by study participants, whethersuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)approvals (or an equivalent approval/review based on the requirements of your country orinstitution) were obtained?Answer: [NA]Justification: This question does not apply to our submission.Guidelines:",
  "The answer NA means that the paper does not involve crowdsourcing nor research withhuman subjects": "Depending on the country in which research is conducted, IRB approval (or equivalent)may be required for any human subjects research. If you obtained IRB approval, youshould clearly state this in the paper. We recognize that the procedures for this may vary significantly between institutionsand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and theguidelines for their institution."
}