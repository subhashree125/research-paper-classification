{
  "Abstract": "A computational problem in biological reward-based learning is how creditassignment is performed in the nucleus accumbens (NAc) to update synapticweights. Much research suggests that NAc dopamine encodes temporal-difference(TD) errors for learning value predictions. However, dopamine is synchronouslydistributed in regionally homogeneous concentrations, which does not supportexplicit credit assignment (like used by backpropagation). It is unclear whetherdistributed errors alone are sufficient for synapses to make coordinated updatesto learn complex, nonlinear reward-based learning tasks. We design a new deepQ-learning algorithm, ARTIFICIAL DOPAMINE, to computationally demonstratethat synchronously distributed, per-layer TD errors may be sufficient to learnsurprisingly complex RL tasks. We empirically evaluate our algorithm on MinAtar,the DeepMind Control Suite, and classic control tasks, and show it often achievescomparable performance to deep RL algorithms that use backpropagation.",
  "Introduction": "Computer science and neuroscience have enjoyed a longstanding and mutually beneficial relationship.This synergy is exemplified by the inception of artificial neural networks, which drew inspiration frombiological neural networks. Neuroscience also adopted temporal-difference (TD) learning fromreinforcement learning (RL) as a framework for biological reward-based learning in the midbrain . At the intersection of these ideas, deep RL has much benefited from and contributed tointerdisciplinary progress between the two fields . An interesting problem raised in biological learning is how signals transmitted by the neuromodulatordopamine computationally induce coordinated reward-based learning. In the mesolimbic system,dopamine is synthesized by dopamine neurons in the ventral tegmental area (VTA) and transmittedthrough the mesolimbic pathway to several regions, including the nucleus accumbens (NAc). There,it is synchronously distributed in regionally homogeneous concentrations , and serves as a rewardprediction error signal for synaptic adjustments via TD learning .1 shows a conceptualillustration: the medium spiny neurons in the NAc receive error signals distributed locally in theirregion via dopamine. Computationally, however, this theory faces the credit assignment problem :the individual synaptic updates using just local errors must somehow work in coordination to improvethe collective prediction.2 Are distributed error signals alone sufficient to coordinate neurons to learncomplex reward-based learning tasks? 1NAc dopamine also serves many roles beyond signaling reward prediction errors ; its full responsibilitiesare an active area of research. We only focus on its role in error signaling, which is most pertinent to our problem.2For clarity, this is different from the temporal credit assignment problem, oft discussed in RL literature.",
  "arXiv:2411.03604v1 [cs.LG] 6 Nov 2024": ": Simplified illustration of dopaminedistribution in the NAc. Dopamine is syn-thesized in the VTA and transported alongaxons to the NAc, where it is picked up by re-ceptors in medium spiny neurons. Dopamineconcentrations (error signals) are locally ho-mogenous, but can vary across regions. Con-nections between NAc neurons not shown. Deep RL typically solves the credit assignmentproblem using backpropagation (BP) .BPpropagates the global error backwards through thenetwork, and computes the gradient (w.r.t. the globalerror) of each layers synaptic weights sequentiallyvia the chain rule. In contrast to the synchronouslydistributed errors in the NAc, BP involves neuronssequentially communicating error signals with eachother. This sequential propagation explicitly coor-dinates learning, but also creates dependencies: eachlayers updates depend on the error of subsequentlayers. This is known as the update locking problem,which is biologically implausible ,limitsparallelization, and cannot explain how distributederror signals may support coordinated learning. Recent ML research on more biologically plausiblealternatives to BP may offer critical insights.PEPITA and Forward-Forward (FF) bothreplace BPs backward learning pass with a secondforward pass to address update locking.Mostrelevantly, Hinton made a surprising discovery:layers can learn useful representations for subsequentlayers even when trained independently of the errorsof those subsequent layers. In FF, each layer generates its own prediction and error, and is onlytrained to learn hidden representations that minimize the local error. The subsequent layer takesthese representations as input, and achieves better performance over training, despite being unableto send errors to the previous layer. This improves the collective global prediction without explicit,sequential coordination of error signals. To the best of our knowledge, these learning principles havenot been explored in RL or biological reward-based learning. Drawing a novel connection, we hypothesize that the computational mechanisms that enable FFsindependent, per-layer training may also enable distributed error signals to support coordinated reward-based learning. To test our hypothesis, we design ARTIFICIAL DOPAMINE (AD), a new deep Q-learning algorithm that trains RL agents using only synchronously distributed, per-layer TD errors, andevaluate its performance on a range of discrete and continuous RL tasks. This provides a potential ex-planation for credit assignment in NAc dopaminergic learning at the algorithmic level of analysis .Our results show that AD can solve many common RL tasks often as well as deep RL algorithms thatuse backpropagation, despite not propagating error signals between layers. Thus, we computationallydemonstrate that distributed errors alone may be sufficient for coordinated reward-based learning. AD networks inherit several ideas from FF, which differ from traditional neural networks in twosignificant ways. First, each layer in an AD network computes its own prediction and receives acorresponding error (.1). This per-layer error mirrors the locally homogenous distributionof dopamine, and the computation of error and updates can be synchronously parallelized acrosslayers; there are no dependencies across layers. Second, we use forward3 connections in time tosend activations from upper to lower layers (.2). This provides an information pathwayfor upper layers to communicate with lower layers using activations, rather than error signals, andempirically improves performance. outlines our architecture, unfolded in time. The AD cell () is where we differ most significantly from FF. Its role is to compute thelocal Q-prediction and TD error. FF is designed to separate real from fake (generated) data, a binaryclassification task. But the NAc is theorized to predict value, a regression task, and therefore needsmore precision. To achieve this, we introduce an attention-like mechanism for non-linear regressionwithout using error propagation (.1).",
  "We evaluate AD on 14 discrete and continuous RL tasks from the MinAtar testbed , the DeepMindControl Suite (DMC) , and classic control environments implemented in Gymnasium": "3Forward\" and backward\" are widely used in deep learning literature both to describe direction in time andin the order of layers. This can be confusing. For the remainder of this paper, we use forward/backward\" whendescribing time, and upper/lower\" when describing position among layers, as shown in . : Network architecture of a 3-layer AD network. h[l]t represents the activations of layer l attime t, and st the input state. The blocks are AD cells, as shown in . Similar to how dopamineneurons compute and distribute error used by a local region, each cell computes its own local TD errorused by its updates; errors do not propagate across layers. To relay information, upper layers sendactivations to lower layers in the next timestep. For example, red shows all active connections at t = 1. MinAtar tasks are miniaturized versions of Atari games, and DMC contains continuous controltasks with simulated physics. These environments are complex enough to reflect many challengesin modern RL , yet remain adequately tractable so as not to necessitate extra components likeconvolutional layers, which may be confounding when attributing performance. We benchmark ADagainst DQN , SAC , and TD-MPC2 baselines, and conduct ablation studies to examinethe effects of the forward connections and additional layers. Our results in Figures 4 and 8 showthat AD learns to solve many of these tasks with comparable performance to the baselines, using justper-layer TD errors. Our code is available at",
  "To summarize our core contributions:": "Are distributed TD error signals sufficient to solve credit assignment and learn complexreward-based learning tasks? We construct a computational example of an RL agent that learnsusing only distributed, per-layer TD errors. This provides evidence that dopamine-distributedsignals alone may be enough to support reward-based learning in the nucleus accumbens. We design a Q-learning algorithm, ARTIFICIAL DOPAMINE, to train our agent. Like Forward-Forward, AD does not propagate error signals across layers. Unlike FF, we introduce a novel cellarchitecture to compute Q-value predictions, as Q-learning is a regression task.",
  "Reward-Based Learning in the NAc4": "The nucleus accumbens (NAc) plays a critical role in the mesolimbic reward system, and is theorizedto predict action value . The predicted action value is encoded via the firing rate of NAcneurons , which reach dopamine neurons in the ventral tegmental area (VTA) via projectionsand influence their activity . In addition, the VTA receives reward signals from sensory inputs,such as the detection of sugar on the tongue . The value predictions and reward signals enablethe VTA to compute reward prediction errors, i.e. differences between expected and actual reward.These reward prediction errors (more specifically TD errors) are encoded via dopamine, and projectedthrough the mesolimbic pathway to regions of the ventral striatum, including the NAc, and lead tosynaptic adjustments . When there is a positive reward prediction error, the average activity ofdopamine neurons increases; when there is a negative error, the average activity decreases . 4We give a simplified overview of reward-based learning in the NAc and its connections to TD learning hereto provide intuition; this is not a complete picture of biological reward-based learning. There exists competingtheories for alternative mappings of the TD learning framework to the brains reward circuit; see . The exchange of value predictions and error signals between the NAc and VTA sets the stage for TDlearning. TD models have shown strong congruency with observed dopamine activity , and arewidely established as the primary theory for mesolimbic dopaminergic learning. What is less known is how NAc neurons computationally use the dopamine-encoded error signals tocoordinate learning. Dopamine-encoded error signals are distributed , which makes credit assign-ment difficult, and there are no other known mechanisms that NAc neurons utilize to communicate er-ror signals for explicit coordination. We illustrate the distribution process in . Each dopamineneuron projects dopamine along its axon through the mesolimbic pathway, then synaptically releasesit in synchronous bursts to the immediate juxtasynaptic area. This causes dopamine concentration inthe region to peak, activating dopamine receptors in NAc medium spiny neurons. Dopamine concen-tration levels are locally homogeneous near the synapses , but can vary across different regionsof the NAc , as dopamine neurons need not all fire at once. This supports the distribution of local-ized error signals. While is clearly not to scale, dopamine neurons are indeed significantlylarger than medium spiny neurons. Their large cell body size enables them to support large terminalfields , allowing each dopamine neuron to widely distribute its signal to groups of NAc neurons.",
  "Temporal-Difference Learning": "Temporal-difference learning (TD learning) methods are a family of RL approaches that aim to solvethe problem of estimating the recursively defined Q function . In their most basic form, TDlearning methods can be implemented as lookup tables, where an estimate of the Q function is keptfor each state-action pair. However, in many practical applications with large or continuous statespaces, a tabular representation of the Q values for all distinct state-action pairs is computationallyinfeasible. In these cases, function approximation, for example via linear regression or neuralnetworks, is necessary to obtain an estimate of the Q function. These methods, called Fitted QIteration or Deep Q Learning , use the squared temporal difference error as a regression lossL(s, a, s) = (s, a, s)2, where is the TD error, and update a parametric Q function approximationvia gradient descent. To prevent the double sampling bias and other instabilities, only Q(s, a) isupdated and the next states value is estimated with an independent copy Q that is not updated. This iscommonly called the bootstrapped Q loss. Given parameters , this loss can be written as:",
  "Forward-Forward": "The Forward-Forward (FF) algorithm is a greedy multi-layer learning algorithm that replacesthe forward and backward passes of backpropagation with two identical forward passes, positiveand negative. The positive pass is run on real data; the negative on fake (generated) data. The goalof the model is to learn to separate real from fake data. During training, each layer of the networkperforms this classification independently using a measure called goodness, which computationallyacts as the logit for this binary classification task. Each layer computes its own per-layer error forupdating. FFs de facto measure of a layers goodness g is the sum of its squared hidden activationshi minus some threshold , i.e. g = i h2i . However, due to the simple goodness formula, ifeach layer directly passes its activations to the next, the next layer often trivially learns the identityfunction. It then uses the sum of its last layers hidden activations as its goodness. This traps thelayer in a local optimum. To avoid this, FF uses layer normalization to normalize the activationsbefore passing them on, which keeps the relative values of the activations, but makes them sum to 0. Compared to BP, a global algorithm that requires the entire network to be updated sequentially, FFis a local algorithm that updates each layer independently via local, layer-wise losses. Critically,the authors of showed that layers can learn useful representations that help subsequent layerseven though they are trained on local errors, without explicit credit assignment. However, for afeedforward architecture, one glaring limitation is that later layers cannot relay any information toearlier ones. Hinton suggests addressing this with a multi-layer recurrent architecture, where foreach layer l in the network, the input is determined by output of the layer l 1 and that of l andl + 1 at the previous time step t 1. This allows for top-to-bottom information flow through thenetwork via activations, which is more biologically plausible. : Inner workings of our proposed AD cell (i.e., hidden layer). h[l]t is the activations of thecell l at time t, and Q[l]t is a vector of Q-value predictions given the current state and each action. Wecompute the cells activations h[l]t using a ReLU weight layer, then use an attention-like mechanismto compute Q[l]t . Specifically, we obtain Q[l]t by having the cells tanh weight layers, one for eachaction, compute attention weights that are then applied to h[l]t . Each cell computes its own error.",
  "Artificial Dopamine": "ARTIFICIAL DOPAMINE (AD) is a deep Q-learning algorithm that trains deep RL agents usingdistributed, per-layer TD errors. AD inherits the per-layer predictions and local computation oferrors from FF, and adopts the forward-in-time downward connections from FFs recurrent variant.Informally, the intuition is that the per-layer error acts similarly to the locally homogeneous distri-bution of dopamine in the NAc. The per-layer errors can be computed in a parallelized, distributedfashion; each layer computes its own local error, which the neurons of the layer use to adjust weightsaccording to their contributions to the error. Similarly, the NAc neurons near the synapses of eachdopamine neuron receive the same error signal (encoded via dopamine), which they use to adjusttheir synaptic weights, according to their previous activity shortly before receiving the error. Duringinference, the network uses the average Q-values across the layers to produce a final prediction. Since the NAc is theorized to predict action value , i.e. Q-value, our prediction task deviates fromthat of FF, which performs binary classification. In general, predicting action value is a nonlinearregression task. To learn this task using a neural network, without resorting to the biologicallyimplausible BP, we design an attention-like mechanism that learns sets of attention\" weights oneset per action head, which introduces nonlinearity. We encapsulate this process in AD cells.",
  "AD Cell Internals": "Our network architecture is composed of layers of AD cells, each of which makes its own Q-valueprediction, computes its own local TD error, and updates its own weights. At inference, the finalprediction of the network is the average of each cells Q-value predictions. We use an attention-likemechanism that learns a weighted sum of the cells hidden activations to predict the Q-value. Thehidden activations are passed to other cells, but the attention weights and Q-value are withheld. Thismechanism simply serves to functionally simulate the complex nonlinear capabilities of biologicalneurons ; we are not attempting to draw an analog between our mechanism and any biologicalcounterpart, and design choices are primarily made based on empirical performance. The concept of a cell is reminiscent of the design of a recurrent neural network. In our case, there is asingle cell per layer, so we use the terms somewhat interchangeably for clarity of exposition. Thevital difference is that no loss information is propagated between cells via BP; that is, there is noBP through time. Instead, the same BP are passed to the cell above (i.e., hidden layer) at the sametimestep and to the layer below at the next timestep. In the absence of BP, these activations providea pathway for upper cells to communicate information to lower cells (see ). We discuss inmore detail how these connections operate in .2. presents the cell design. Each cell is mechanistically identical and takes in two inputs: the hid-den activations of the cell below at the current timestep (or observation, if lowest cell), and the hidden activations of the cell above at the previous timestep. It produces two identical outputs, sent to the cellabove immediately, and the cell below at the next time step. At the start of an episode, the activationsfrom the previous timestep are zeroes. The top cell only receives one input and produces one output. The attention-like mechanism for Q-value prediction works as follows. Each cell computes itshidden activation, h[l]t , using the layer aboves hidden activations from the previous timestep, h[l+1]t1 ,and the layer belows hidden activations at the current timestep, h[l1]t. Specifically, it passes theconcatenation of h[l+1]t1 and h[l1]tthrough a ReLU weight layer (shown in ) to get h[l]t . TheReLU weight layer multiplies the concatenated input [h[l1]t, h[l+1]t1 ] by its learned weight matrixW [l], then applies the ReLU nonlinearity function. In parallel, the cell also uses [h[l1]t, h[l+1]t1 ] tocompute attention\" weights, by multiplying it with the learned weight matrix W [l]att then applyingthe tanh function. Finally, the cell takes the dot product between the output of the tanh layers,tanh(W [l]att [h[l1]t, h[l+1]t1 ]), and the hidden activations h[l]t , to compute the cells Q-value predictionQ[l](st, a). Each cell reuses its internal weights over time; for example the matrix W in is used ateach timestep in the first cell. Therefore, the full computation performed by a cell is:",
  "Network Connections": "As shown in , each cell passes its state to the cell l + 1 above at the current timestep t, and tothe cell l 1 below in the next timestep t + 1. The information flow is strictly unidirectional to matchthe direction of time flow in RL environments. This is necessary as interacting with the environmenthappens sequentially, meaning future information will not be available when acting. Although we do not backpropagate gradients across cells, information does flow from upper layers tolower layers via the temporal connection (forward in time). The upper layers use the connections tocommunicate with lower layers via activations, which is more biologically plausible . Our resultsin suggest that these connections can greatly increase network performance in the absenceof BP. The intuition for adopting these forward-in-time connections is that they are well-suited totake advantage of the temporal structure of the Q-values of trajectories for better learning. Given agood policy, the Q-value predictions of a well-trained model should remain stable through each stateof a trajectory (assuming the dynamics are reasonably deterministic). This means that the Q-valueprediction of the current timestep, and the hidden activations used to make this prediction, canoften still be useful for predicting the Q-value of the next timestep. In contrast, in FFs experimentson image classification, this effect is forced FF repeats the same input every timestep, reducingcomputational efficiency. Our results empirically support the effectiveness of forward connectionsfor Q-learning, particularly in more complex environments.",
  "Experiments": "The main goal of our experiments is to evaluate whether distributed, per-layer TD errors are sufficientfor learning complex RL tasks that are typically only solved with backpropagation and sequentialerrors. Our criterion of sufficiency is how well the agent learns to solve the given task, measuredin terms of average episodic return. Since there are no official standards that define solving theseenvironments, we use the performance of established RL algorithms (i.e. DQN, TD-MPC2, andSAC) as the gold standard to compare against. These algorithms are commonly used to solve theenvironments we choose and are known to be strong performers . Like Hinton , our aim is to investigate the learning capabilities of a novel algorithm that operatesunder additional biological constraints, rather than pursue state-of-the-art performance. Thus, we opt for a simple implementation with few extensions. The only extensions we employ are experiencereplay and the Double-Q learning trick which are standard for deep Q-learning and skipconnections from the input to upper layers for the DMC environments. We do not use convolutionallayers, as these more closely resemble the brains visual cortex , and similar structures are notfound in the mesolimbic system. We implement our algorithm in Jax . Training Process.The training process of our RL agent is based on the standard DQN trainingalgorithm with experience replay . During training, we use a randomly sampled replay buffer thatstores every transition observed from the environment. We replay sequences of short length, similarto how other recurrent architectures are trained , and compute the local updates for each cellsequentially according to the network graph. Since the local updates are distributed and per-layer, theycan be computed in parallel. We provide a formal description of our training process in Appendix B. Environments.We run our experiments on 2 RL benchmarks and 4 classic control tasks, totaling14 tasks. MinAtar is a simplified implementation of 5 Atari 2600 games: Seaquest, Breakout,Asterix, Freeway, and Space Invaders. The DeepMind Control (DMC) Suite is a set of low-levelrobotics control environments, with continuous state spaces and tasks of varying difficulty. For ourexperiments, we used a discretized action space, following Seyde et al. , as our architecture iscurrently only developed for discrete Q-learning. From the DMC tasks we select Walker Walk, WalkerRun, Hopper Hop, Cheetah Run, and Reacher Hard. In addition, we provide results on the classiccontrol tasks Cart Pole, Mountain Car, Lunar Lander, and Acrobot, which we include in Appendix C.For a more elaborate discussion on these environments and our task choice, see Appendix I. Baselines.On MinAtar, we compare our results against a fully-connected DQN to make perfor-mance comparisons more direct and informative. As the original baselines presented in Young andTian used a CNN, we replaced the CNN with fully connected layers and tuned hyperparametersfor fairness of comparison. We find that the new architecture performs as well as or better than theone presented by Young and Tian . Specifically, we use a double DQN with 3 hidden layers of1024, 512, and 512 units with ReLU activations. This baseline achieves strong performance and has anumber of trainable parameters comparable to our networks. For the continuous control tasks, we show that our method almost reaches the performance ofstate-of-the-art algorithmic approaches such as SAC and TD-MPC2 , which rely onbackpropagation. The results were taken from Yarats and Kostrikov and Hansen et al. respectively. We do not change the underlying architectures or hyperparameters. Network architecture and hyperparameters.On MinAtar, we use a 3-hidden-layer networkwith forward activation connections. The cell output sizes are 400, 200, and 200. Due to additionalconnections within cells and from upper to lower cells, this architecture has a similar number oftrainable parameters as the DQN. On DMC, we use a smaller network with cell output sizes 128, 96,and 96, and discretize the action space following Seyde et al. . For more details, see Appendix G.For each benchmark, we use the same network and hyperparameters across all tasks to test therobustness of our architecture and learning algorithm.",
  "Results": "We present the results of AD on MinAtar and DMC environments in , and compare its per-formance against DQN, SAC, and TD-MPC2. shows the mean episodic return over episodesacross 10 random seeds, with standard error. We also provide 95% bootstrap confidence intervals in Appendix D and aggregate statistics in Appendix E. We additionally perform ablation studies byremoving the forward connections to lower layers, and measuring the performance of a single-layerAD cell. We show both the forward connections and multiple layers contribute to performance (Fig-ures 5 and 6). Finally, we evaluate an implementation of AD that learns distributions over Q-values,based on recent work by Dabney et al. , and find that AD shows promising results (). Comparison against baselines.We find that AD is able to learn stable policies reliably in all testenvironments. On MinAtar tasks, our agent achieves comparable performance to DQN on Breakout,and slightly surpasses DQNs performance on Asterix and Freeway, while DQN performs better on : Episodic returns of AD in MinAtar and DMC environments, compared to DQN, TD-MPC2and SAC. Lines show the mean return over 10 seeds and the shaded area conforms to 3 standarderrors. The axes are return and environmental steps. : Ablation study comparing the performance of AD against AD without the forward-in-timeconnections, and a single-layer AD cell. In Seaquest and Asterix, AD achieves qualitatively strongerperformance. In Seaquest the line for AD single layer is overlapped by the line for AD no forward. Seaquest and Space Invaders. On all evaluated DMC tasks, we see that our methods results are closeor on par with those of SAC and TD-MPC2 when using both backward connections and multiplelayers, however, sample efficiency is marginally lower. We note that the action space discretization we utilize for DMC tasks may complicate the comparison,as it both slows down our training but can benefit algorithms (compare Seyde et al. ). In addition,we do not show results on the hardest DMC tasks because of the difficulties in scaling our approachto large action spaces, which leads to rapid growth in network parameters. Overall, our results demonstrate that AD shows biological distributed error signals may allowfor coordinated learning in several RL domains. Therefore, further refining and improving thearchitecture for state-of-the-art RL benchmark performance may be an exciting and promisingdirection for future work. Forward connections to lower layers.To further measure the performance impact of the forwardconnections in time, we compared the temporal forward version of the network to a 3-layer ADnetwork without the forward-in-time connections. All other hyperparameters are the same as the3-layer AD network. As shown in , this resulted in a moderate drop in performance in mostenvironments, increased variance in the training, and devastating drops in performance on Seaquestand Asterix. These tasks are the most complex out of the five. We provide additional discussion onADs performance in Seaquest, where the performance difference is most significant, in Appendix N.These results suggests that the information channels from the upper to lower layers are vital forperformance on many tasks. On this same note, ADs ability to achieve similar performance toDQN when the forward connections are added suggest that forward connections may be an effectivereplacement for backpropagation in certain tasks. Single-layer performance.Another important question about our proposed architecture is whetherthe cells learn to coordinate with each other. A concern is that the majority of the learning may beaccomplished by the lowest cell, If the performance of the multi-layer AD network does not improveover the single cell, it would suggest that we cannot explain ADs performance as a result of the cellscoordinating their learning using distributed errors. We show in that the multi-layer versionof AD outperforms the single-layer at all tasks, and the single-layer cell fails at Seaquest and Asterix. : Episodic returns of different-sized single-layer AD, compared to the standard 3-layer AD.Single 128 is a single-layer with 128 hidden activations. Overall, increasing the layer size of thesingle layer does not result in clear increases in performance. Lines show the mean return over 8seeds and the shaded area conforms to 3 standard errors. The axes are return and environmental steps. : Episodic returns of the distributional RL version of AD, implemented with QuantileRegression (QR). Lines show the mean return over 8 seeds and the shaded area conforms to 3standard errors. The axes are return and environmental steps. An additional concern along these same lines is whether a wider, single-layer AD cell may achievethe same level of performance as multi-layer AD. In , we show that increasing the layer sizeof a single-layer AD cell does not result in clear increases in performance in DMC tasks. We alsoexperimented with increasing the layer size of a single-layer cell for Seaquest and Asterix from 400to 600 and 800, and did not find noticeable improvements in either case. Distributional RL.Recent work by Dabney et al. suggest that the brains value predictorsmay be learning distributions over future values, rather than just the mean, as previously believed.Dabney et al. argue that different dopamine neurons in the VTA may have different scalings forpositive and negative reward prediction errors intuitively, they can be more optimistic or pessimisticwhich results in the predictors learning a distribution over the values. Interestingly, this coincideswith the development of distributional RL, whose algorithms aim to learn such distributions. To better align our work with the findings of Dabney et al. , we additionally implement a versionof AD that learns distributions over values, and evaluate it on the DMC tasks. Our implementation isbased on Quantile Regression (QR) DQNs , and requires just a simple modification to each ADcell. Rather than predicting a single Q-value, each cell predicts 10 Q-values that each match to onequantile of a 10-quantile QR-DQN. The tradeoff is that this requires additional compute. Our results in suggest that AD may be well-suited for distributional learning. In each ofthe tasks, our agent achieves similar performance to the standard version of AD, and only slightlylags behind on Hopper Hop. This may be a result of the greater sparsity of the Hopper environment,which makes it more difficult for distributional RL algorithms to learn.",
  "Limitations": "We proposed an algorithm that trains an RL agent using only distributed TD errors, which provides acomputational example of how distributed error signals may be sufficient to solve credit assignment inreward-based learning in the NAc. However, our model does not accurately capture all aspects of therelevant biology. In our model, we use per-layer TD errors as an analogy for dopamine neurons dis-tributing error signals to local regions around their synapses. But unlike in artificial neural networks,which form the basis of our architecture, neurons in the NAc are not clearly organized into layers.ADs hierarchical message passing architecture is a design choice we inherit from deep learning prac-tices, and not meant as a mechanistic model of the NAc. Furthermore, activations in biological neuronsare communicated asynchronously, and can form recurrent loops, which we do not account for. In addition, we make some assumptions regarding biological reward-based learning that are notyet conclusive in neuroscience. Most importantly, we assume that neurons in the NAc learn tooutput action values, and dopamine neurons in the VTA receive reward signals and the predictedaction values to compute and return TD errors.5 While these assumptions are widely supportedby research , there exist other theories and empirical results that provide alternativeexplanations for mesolimbic reward-based learning. Some of these works include Roesch et al., who suggest that on-policy value-based learning (SARSA) better explains dopamine activitythan off-policy value-based learning (e.g. Q-learning); Takahashi and Chen and Goldberg ,who provide evidence that map subregions of the striatum to actor-critic models; Ito and Doya and Weglage et al. who suggest the dorsal striatum signals action values, whereas the ventralstriatum signals state values; and Akam and Walton and Coddington et al. , who respectivelypropose how dopamine may be used by the brain to perform model-based and policy-based learningrather than just value-based learning. Furthermore, while there is strong evidence that some formof TD learning is used by the brain, mappings between RL frameworks and dopaminergic learningmay not be mechanistically accurate even if they are behaviorally accurate. There are also several technical limitations of our work. First, like other Q-learning algorithms, ADrequires discrete action spaces. To solve the DMC tasks, which have continuous action spaces, wediscretized the action space, following the method used by Seyde et al. (). While thisis consistent with other work in RL, it introduces additional complexities and may not reflectbiology. Second, within each AD cell, the number of tanh weight layers grows in proportion tothe size of the action space (). This limits the scalability of AD for tasks with large actionspaces. We can mitigate the effects of this issue using a matrix decomposition trick described inAppendix F, but AD currently cannot scale to very large action space DMC tasks like Humanoidor Dog . Third, computational constraints limited the number of runs we perform per task.Additional experiments can further improve the robustness and generalizability of our results. Finally, our work isolates one system of biological learning, and attempts to provide a computationalexplanation without accounting for other systems of learning, for example hippocampal contributionsto value-based learning . But the brains learning capabilities are likely a result of combiningsignals from several systems, which may not be divisible , and the NAc may be just one part of alarger value-based system . Unlike backpropagation, the brain utilizes multiple types of learning inconjunction, both supervised and unsupervised, using local and global signals. AD only models onetype of learning, i.e. error-driven learning using distributed reward signals to induce local updates.Other biologically plausible algorithms, such as ANGC , may provide explanations for other formsof learning, which may be critical to building a more complete understanding of biological reward-based learning. Indeed, aspects of Hebbian learning or active inference are likely critical to achieving afully biologically plausible, efficient, and powerful learning system. In that light, we explore learningwith just distributed error signals not to rule out the importance of other methods, but to demonstratethat this one principle alone may be sufficient to solve some complex RL tasks nearly as well as BP. Webelieve that a key to achieving general, human-like intelligence will be the integration of these differentlearning methods and learning signals; this is an exciting direction we aim to explore in future work.",
  "Acknowledgements": "We gratefully acknowledge our sponsors, who support our research with financial and in-kindcontributions: Amazon, Apple, CIFAR through the Canada CIFAR AI Chair program and theCanadian Foundation for Innovation, DARPA through the GARD project, Meta, NSERC through theDiscovery Grant and funding reference number RGPIN-2018-05946, the Ontario Early ResearcherAward, the Sloan Foundation, and the Schwartz Reisman Institute for Technology and Society.Resources supporting this research were also provided by the Province of Ontario, the Governmentof Canada through CIFAR, and the sponsors of the Vector Institute. We extend our sincere gratitudeto Mete Kemertas for offering insights that significantly accelerated the initial exploration of thiswork. We also thank Congyu Fang, David Glukhov, Stephan Rabanser, Anvith Thudi, Sierra Wyllie,and other members of the CleverHans and SocialAI labs, as well as our anonymous reviewers forinvaluable discussions and feedback. 5Note that AD does not assume action selection need happen in the NAc. Rather, we just take the assumptionthat the NAc computes the Q-value function, which takes in the state and action as parameters, and performsQ-learning. The action may be provided from another region like the dorsal striatum. Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron Courville, and Marc G Belle-mare. Deep reinforcement learning at the edge of the statistical precipice. Advances in NeuralInformation Processing Systems, 2021.",
  "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointlylearning to align and translate. arXiv preprint arXiv:1409.0473, 2014": "Akram Bakkour, Daniela J Palombo, Ariel Zylberberg, Yul HR Kang, Allison Reid, MiekeVerfaellie, Michael N Shadlen, and Daphna Shohamy. The hippocampus supports deliberationduring value-based decisions. elife, 8:e46080, 2019. Ulrike Basten, Guido Biele, Hauke R Heekeren, and Christian J Fiebach. How the brainintegrates costs and benefits during decision making. Proceedings of the National Academy ofSciences, 107(50):2176721772, 2010. Hannah M. Bayer and Paul W. Glimcher. Midbrain Dopamine Neurons Encode a QuantitativeReward Prediction Error Signal. Neuron, 47(1):129141, July 2005. ISSN 08966273. doi: 10.1016/j.neuron.2005.05.020. URL David Beniaguev, Idan Segev, and Michael London. Single cortical neurons as deep artificialneural networks. Neuron, 109(17):27272739.e3, September 2021. ISSN 08966273. doi: 10.1016/j.neuron.2021.07.002. URL Matthew Botvinick, Jane X. Wang, Will Dabney, Kevin J. Miller, and Zeb Kurth-Nelson. DeepReinforcement Learning and Its Neuroscientific Implications. Neuron, 107(4):603616, August2020. ISSN 08966273. doi: 10.1016/j.neuron.2020.06.014. URL James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, DougalMaclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, andQiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL",
  "Anne G E Collins and Michael J Frank. Surprise! Dopamine signals mix action, value anderror. Nature Neuroscience, 19(1):35, January 2016. ISSN 1097-6256, 1546-1726. doi:10.1038/nn.4207. URL": "Will Dabney, Mark Rowland, Marc Bellemare, and Rmi Munos. Distributional reinforce-ment learning with quantile regression. In Proceedings of the AAAI conference on artificialintelligence, volume 32, 2018. Will Dabney, Zeb Kurth-Nelson, Naoshige Uchida, Clara Kwon Starkweather, Demis Hassabis,Rmi Munos, and Matthew Botvinick. A distributional code for value in dopamine-basedreinforcement learning. Nature, 577(7792):671675, 2020. Giorgia Dellaferrera and Gabriel Kreiman.Error-driven input modulation: Solving thecredit assignment problem without a backward pass.In Kamalika Chaudhuri, StefanieJegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedingsof the 39th International Conference on Machine Learning, volume 162 of Proceedingsof Machine Learning Research, pages 49374955. PMLR, 1723 Jul 2022. URL",
  "Geoffrey Hinton. The forward-forward algorithm: Some preliminary investigations. arXivpreprint arXiv:2212.13345, 2022": "Shengyi Huang, Rousslan Fernand Julien Dossa, Chang Ye, Jeff Braga, Dipam Chakraborty,Kinal Mehta, and Joo G.M. Arajo. Cleanrl: High-quality single-file implementations of deepreinforcement learning algorithms. Journal of Machine Learning Research, 23(274):118, 2022.URL Makoto Ito and Kenji Doya. Distinct neural representation in the dorsolateral, dorsomedial, andventral parts of the striatum during fixed-and free-choice tasks. Journal of Neuroscience, 35(8):34993514, 2015.",
  "Adrien Journ, Hector Garcia Rodriguez, Qinghai Guo, and Timoleon Moraitis. Hebbian deeplearning without feedback. arXiv preprint arXiv:2209.11883, 2022": "Steven Kapturowski, Georg Ostrovski, John Quan, Remi Munos, and Will Dabney. Recurrentexperience replay in distributed reinforcement learning. In International conference on learningrepresentations, 2018. Daniel Kunin, Aran Nayebi, Javier Sagastuy-Brena, Surya Ganguli, Jonathan Bloom, and DanielYamins. Two routes to scalable credit assignment without weight symmetry. In Hal DaumIII and Aarti Singh, editors, Proceedings of the 37th International Conference on MachineLearning, volume 119 of Proceedings of Machine Learning Research, pages 55115521. PMLR,1318 Jul 2020. URL Dong-Hyun Lee, Saizheng Zhang, Asja Fischer, and Yoshua Bengio. Difference target propa-gation. In Machine Learning and Knowledge Discovery in Databases: European Conference,ECML PKDD 2015, Porto, Portugal, September 7-11, 2015, Proceedings, Part I 15, pages498515. Springer, 2015.",
  "Thomas Miconi, Aditya Rawal, Jeff Clune, and Kenneth O. Stanley. Backpropamine: trainingself-modifying neural networks with differentiable neuromodulated plasticity, 2020": "Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, DaanWierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprintarXiv:1312.5602, 2013. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc GBellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al.Human-level control through deep reinforcement learning. nature, 518(7540):529533, 2015. Genela Morris, Alon Nevet, David Arkadir, Eilon Vaadia, and Hagai Bergman. Midbraindopamine neurons encode decisions for future action. Nature Neuroscience, 9(8):10571063,Jul 2006. doi: 10.1038/nn1743.",
  "Richard S Sutton. Learning to predict by the methods of temporal differences. Machine learning,3:944, 1988": "Yuji Takahashi.Silencing the critics: Understanding the effects of cocaine sensitizationon dorsolateral and ventral striatum in the context of an actor/critic model.Frontiers inNeuroscience, 2(1):8699, Jul 2008. doi: 10.3389/neuro.01.014.2008. Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, DavidBudden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, Timothy P. Lillicrap, and Martin A.Riedmiller. Deepmind control suite. CoRR, abs/1801.00690, 2018. URL Mark Towers, Jordan K. Terry, Ariel Kwiatkowski, John U. Balis, Gianluca de Cola, TristanDeleu, Manuel Goulo, Andreas Kallinteris, Arjun KG, Markus Krimmel, Rodrigo Perez-Vicente, Andrea Pierr, Sander Schulhoff, Jun Jet Tai, Andrew Tan Jin Shen, and Omar G.Younis. Gymnasium, March 2023. URL",
  "Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with doubleq-learning. In Proceedings of the AAAI conference on artificial intelligence, volume 30, 2016": "Moritz Weglage, Emil Wrnberg, Iakovos Lazaridis, Daniela Calvigioni, Ourania Tzortzi, andKonstantinos Meletis. Complete representation of action space and value in all dorsal striatalpathways. Cell reports, 36(4), 2021. R Mark Wightman, Michael LAV Heien, Kate M Wassum, Leslie A Sombers, Brandon JAragona, Amina S Khan, Jennifer L Ariansen, Joseph F Cheer, Paul EM Phillips, and Regina MCarelli. Dopamine release is heterogeneous within microenvironments of the rat nucleusaccumbens. European Journal of Neuroscience, 26(7):20462054, 2007.",
  "AFormal Definition of TD Learning": "We consider a standard discounted infinite horizon Markov Decision Process (MDP) setting withstates S, actions A, a transition kernel p(s|s, a), a reward function r : S A R and a discountfactor . This is a typical setting for reinforcement learning . The goal of TD learning is to obtain a policy (a|s) that maximizes the discounted future sum ofrewards. The value function Q : S A R measures how valuable a given action a A isin a given state s S, and can be used to directly compute a policy. It is defined via a recursiveformula Q(s, a) = r(s, a) +",
  "CExperiments on Classic Control Environments": "Beyond MinAtar and DMC, we also evaluate AD on 4 classic control environments: Cart Pole(also known as inverted pendulum), Mountain Car, Lunar Lander, and Acrobot, and compare itsperformance against DQN. We show our results in . The main purpose of these experiments is to further support ADs robustness across different RLenvironments, and indirectly compare its performance against Active Neural Generative Coding(ANGC) , another biologically-inspired deep RL algorithm that provides a more plausible",
  "alternative to backpropagation. Ororbia and Mali evaluates ANGC in 4 environments: Cart Pole,Mountain Car, Lunar Lander, and a custom robot-arm-reaching environment": "In addition, we implement 2 DQNs with different hyperparameters as baselines for comparison: firstis CleanRLs reference DQN for classic control problems , and second is the DQN tuned by for comparison against their ANGC agent, which we refer to as ANGCs DQN. The CleanRL DQNprovides a well-established, publicly-vetted baseline, whereas ANGCs DQN serves as a referencepoint for us to indirectly compare ADs performance with ANGC. To better demonstrate ADs robustness across environments, we use the same set of hyperparametersfor each of the environments. Specifically, we use a 2-layer AD network, with output sizes 128and 96, a fixed learning rate of 2.5e4 with Adam optimization, an exploration fraction of 0.2, andfinal epsilon of 0.05. All other hyperparameters are the same as our network for MinAtar, shownin Appendix G. As neither CleanRL nor ANGCs DQN uses a learning rate scheduler, we alsoremoved ours for better comparison. CleanRLs DQN is a 2-layer network with 120 and 84 units;the same DQN is used for all 4 environments. ANGC uses different 2-layer DQNs for each of theirenvironments, which they tuned for environment-specific performance. Their Cart Pole DQN has256, 256 hidden units; Mountain Car has 256, 192, and Lunar Lander 512, 384. For Acrobot, we usedthe same hyperparameters as the Cart Pole DQN, given the similarity of the environments, whichworked well. For more hyperparameter details, we refer the reader to and ; both workspresent excellent detail. We made two adjustments when reproducing CleanRL and ANGCs DQNs. First, since AD utilizesdouble Q-learning to improve its learning stability, for better comparison, we also enhanced bothDQNs with double Q-learning, given that it is a simple enhancement without additional computationalcosts. Second, specific to ANGCs Cart Pole DQN, we found that the agents learning has veryhigh variance with the given hyperparameters, and the agent does not consistently achieve highperformance. To counter this, we decreased the learning rate from the provided 5e4 to 2.5e4, andincreased the target network update frequency (referred to as C in ) from 128 to 500, which isCleanRLs value. This improved both the agents stability and average return. Our results in show that AD achieves strong performance across all 4 tasks, learning morestably and achieving higher average episodic return than the DQNs in all environments. Most notably,in comparison to DQN, AD more consistently reaches the maximum 500 return in Cart Pole, andachieves significantly higher return in Mountain Car. It also surpasses DQNs performance slightly inboth Lunar Lander and Acrobot. These results suggest that AD is highly competitive against ANGCin these tasks. However, we note that as ANGC and AD learn with different signals, they are notcompeting approaches, and may possibly be integrated to achieve greater performance we also lookforward to exploring this potential in future work.",
  "EAggregate Statistics": "Using the RLiable library provided by Agarwal et al. , we aggregate the performance of AD andbaselines on the Minatar and Mujoco tasks. Since the MinAtar and DMC environments do not offerhuman-normalized scores, we used the performance of common baseline algorithms (SAC and DQN,respectively) for normalization. We find that AD is within the confidence interval of TD-MPC2 on Mujoco and DQN on Minatar(compare ). It shows better mean and median performance than SAC on the Mujoco tasks.IQM excludes the harder hopper environment, which results in slightly worse performance of AD onthis metric, as our method shines in this task.",
  "FImproving Computational Efficiency in AD Cells": "A limitation of our AD cells is that the W [l]att matrix scales in the size of the action space |A| and thesize of the hidden layer, d. When training on more complex environments with large action spacesand require larger hidden layers, W [l]att can become expensive to compute and to learn. One trick weemployed in DMC environments to improve the computational efficiency of our AD cells is to learnthe W [l]att matrix as a product of two smaller matrices, one |A| k and the other k d where k is asmall constant. For small k, the number of parameters in the matrix product is significantly smallerthan the original W [l]att matrix, especially when either |A| or d are large, allowing for more efficientlearning at the cost of expressiveness. We found that empirically in the DMC environments usingk = 8 does not notably negatively impact performance while reducing run times.",
  "We show the hyperparameters of the AD network and DQN used in our MinAtar experiments": "HyperparameterADDQNLearning rate104104Exploration fraction0.10.1Final epsilon at end of exploration0.010.01Loss functionMean squared errorMean squared errorMax gradient norm1.01.0Batch size512512Gamma0.990.99Training frequency44Replay buffer size5 1065 106Target network update frequency10001000 We also use a learning rate scheduler for both networks, which linearly increase learning rate to104 in 500000 steps, then cosine decay until 3 105 over remaining training steps. Thesehyperparameters were jointly tuned and shared by both the baseline DQN and AD network.",
  "We run our experiments on two standard RL benchmarks and 4 classic control tasks": "MinAtar is an simplified implementation of 5 Atari 2600 games: Seaquest, Breakout, Asterix,Freeway, and Space Invaders. We use version 1 for all environments. The input frame size is 10 10,and the n different objects in each game are placed on separate frames, resulting in a 10 10 ninput observation. All tasks have discrete action spaces, with up to 6 actions. The DeepMind Control (DMC) Suite is a set of low-level robotics control environments, withcontinuous state spaces and tasks of varying difficulty. For our experiments, we used a discretizedaction space, following Seyde et al. , as the architecture is currently only developed for discreteQ learning. From the DMC tasks we selected Walker Walk, Walker Run, Hopper Hop, Cheetah Run,and Reacher Hard. These tasks were chosen based on the size of the action space, as our discretizedaction heads currently scale exponentially with increasing action dimension. Using better fine-grainedmotor control is a valuable path for future work; for now we present the simplified variant as a proofof concept that our algorithm is able to handle a wide range of tasks and input modalities. We chose these environments because they reflect challenges in solving complex, non-linear controltasks, from which meaningful algorithmic insight can be obtained , but are not so complex theyrequire additional components like convolutional layers to solve. This makes them excellent testbedsfor novel learning approaches. Although the DQN baselines published by Young and Tian are",
  "CIFAR-1020000.48410.54400.56100.5725": ": Test accuracy of each layer of 4-layer AD networks trained on MNIST and CIFAR-10. Eachlayer has the same number of hidden activations; for example, the first row refers to an AD networkwith layers of 500, 500, 500, and 500 activations. Since each layer makes its own prediction, we caneasily see how performance increases per layer.",
  "JExperiments on MNIST and CIFAR-10": "To further investigate the generalizability of AD across different learning tasks, we also evaluate ADon two supervised learning datasets, MNIST and CIFAR-10, and present our results here. Q-learningis a regression problem; to adapt our architecture to solve these classification problems, we simplychange the loss function to cross entropy, and make each of the tanh weight layers correspond toa class, instead of an action. Using a 4-layer AD network with output sizes 2000, 2000, 2000 and2000, we achieved 98.74% test accuracy on MNIST, and 57.25% test accuracy on CIFAR-10. Theseresults are in line with the results achieved by Forward Forward in . We show our results with different sized networks in . As each layer in the network makesits own prediction, we additionally show the test accuracy of every layer. Remarkably, for almostevery network size, the test accuracy steadily increases per layer. Like our experiments in RL, thissupports that AD cells can learn to coordinate with each other for better performance without theneed to backpropagate error signals across layers. For clarity, we want to note that these experiments are performed simply to add another perspectiveto evaluate the robustness of ADs learning across tasks. AD is intentionally designed for Q-learning,rather than supervised learning, as it imitates biological processes of reward-based learning.",
  "KCompute Resources": "We ran our experiments on a shared scientific computing cluster using an assortment of CPU andGPUs. Each instance is run with a GPU (to improve the speed of training the neural network) and 12CPU cores (to improve the speed of the environment simulation), and 20GB of RAM. On an NvidiaRTX 2080 GPU, a full training run of AD takes approximately 5 hours on the MinAtar environments,and 3 hours on the DMC environments. On an Nvidia A100 GPU, the run takes approximately3.5 hours on MinAtar, and 2.5 hours on DMC. We additionally expended compute resources forhyperparameter tuning and ablation studies. In total, our experiments consumed approximately 9000hours of compute using the aforementioned instances. While we did not prioritize computational efficiency in the current design of our algorithm, we notethat like Chen et al. , Lillicrap et al. , Journ et al. and other biologically-plausible deeplearning algorithms that do not suffer from the weight transport and update locking problems, ouralgorithm can be adapted for neuromorphic hardware that keep data local to computational units .These hardware have the potential for significant energy efficiency gains.",
  "MSocial Impacts": "Our work introduces a new learning algorithm that computationally demonstrates how the creditassignment problem may be solved in the nucleus accumbens, where reward prediction errorsare synchronously distributed. This research direction may yield significant positive impacts tohealthcare and the social sciences. By improving our understanding of biological reward-basedlearning, our research has the potential to inform novel therapeutic strategies for neurologicaldisorders characterized by dysfunctional reward processing mechanisms. By better aligning AI withbiological intelligence, we also provide social science with more faithful AI agents for simulatingand studying human social behavior, a practice that is increasingly adopted. However, aligning neuroscience and AI is not without risk of negative social impacts. The devel-opment of more faithful AI agents may encourage social sciences to increasingly replace humansubjects, as AI agents may be cheaper or more malleable to certain experimental setups, and exoge-nous variables may be more easily controlled for. Yet AI agents are inherently biased by the datathey are trained upon; replacement of human subjects may further marginalize underrepresentedcommunities, which is already a large challenge in AI fairness research. In addition, deep reinforcement learning research, such as our work, consumes considerable amountsof computational power and energy (see Appendix K). While such research often yields valuableinsights into intelligence, these gains should always be balanced against the carbon emissions causedby large-scale experiments.",
  "NDiscussion on Seaquest": "In Seaquest, where the performance difference between 3-layer AD, AD without forward-in-timeconnections, and single layer AD is most apparent, the latter agents struggle may be attributed to thefailure of learning the resurfacing mechanism, which traps it at a local minima. In this environment,the agent must regularly resurface while carrying a diver to replenish oxygen. In the short term,the agent can acquire more reward if it prioritized attacking enemies, rather than managing oxygen,but if it runs out of oxygen the episode ends. We observed that the AD agent without forwardconnections and the single layer agent both struggled to manage oxygen, often resurfacing randomlyor without first acquiring a diver. In contrast, the AD agent with the forward connections learnsto manage their oxygen more optimally, and often sacrifices short-term rewards to maintain higheroxygen. In fact, during some trials the AD agent became ultraconservative with oxygen, which ledto the agent surviving on average over 1500 timesteps per episode over several thousand episodesbefore it changed strategy. Interestingly, this same behavior was not observed in DQN agents, whoseaverage episode lengths are consistently under 800. This suggests that AD has the potential to learncompletely different policies from DQNs.",
  "OAdditional Related Work": "Biologically inspired learning algorithms seek to reconnect machine learning with neurobiology.This supports better transdisciplinary collaboration across ML and neuroscience, which may beessential to advancements in both biological and artificial intelligence research . Similar to our work, Ororbia and Mali proposes a deep Q-learning algorithm, ANGC, basedon the Neural Generative Coding framework , which is centered around the idea of activeinference . We indirectly compare the performance of AD and ANGC in Appendix C. Alsoclosely relevant, Guerguiev et al. proposes a BP alternative that solves credit assignment forsupervised learning tasks, and show promising results on the MNIST dataset . They point outthat current neuroscience lacks sufficient understanding of how credit assignment is performed in thebrain, and provide a method to train neural networks using segregated dendrites. Many other related works focus on other aspects of biological plausibility in deep learning; wemention a few here for the interested reader. Lillicrap et al. , Lillicrap et al. , Kunin et al. ,Akrout et al. and Lee et al. proposed algorithms that tackle the weight transport problem andnon-locality of errors, which have long plagued backpropagation . Journ et al. adopts aHebbian learning approach to address weight transport, locality and update locking. Miconi et al. improves the plasticity of neural networks, inspired by the brains mechanisms of neuromodulation."
}