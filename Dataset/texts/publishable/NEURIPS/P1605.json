{
  "Abstract": "Per-example gradient norms are a vital ingredient for estimating gradient noisescale (GNS) with minimal variance. Observing the tensor contractions required tocompute them, we propose a method with minimal FLOPs in 3D or greater tensorregimes by simultaneously computing the norms while computing the parametergradients. Using this method we are able to observe the GNS of different layers athigher accuracy than previously possible. We find that the total GNS of contempo-rary transformer models is predicted well by the GNS of only the normalizationlayers. As a result, focusing only on the normalization layer, we develop a customkernel to compute the per-example gradient norms while performing the Layer-Norm backward pass with zero throughput overhead. Tracking GNS on only thoselayers, we are able to guide a practical batch size schedule that reduces trainingtime by 18% on a Chinchilla-optimal language model.",
  "Introduction": "The gradients gathered during the backward pass while training a neural network are typicallyinspected via their Frobenius norm, the magnitude of the vector. This gradient vector may be viewedas the sum of gradients computed over each individual example in the minibatch. Each of these hasits own norm. In this work, we develop a method to access these norms that works at any scale, forthree common layer types in deep learning models: linear, normalization and embedding layers. One primary application of a per-example gradient norm is in estimating the Gradient Noise Scale(GNS) , a metric that has been shown to be useful in training large scale models . Theuncertainty of the GNS estimator depends directly on the size of the batch used to compute the smallbatch gradient norm as shown in .1. So, the most precise estimate of the GNS is obtained bycomputing the gradient norms for each example in the minibatch: the per-example gradient norm. To demonstrate GNS measurement in practice we perform experiments on contemporary languagemodel architectures, providing a detailed visualisation of the movement of the GNS componentsthroughout training, presented in . By inspecting these components it was found that theGNS of the model is highly correlated between layer types, which we give an intuition for in . However, the practical utility of measuring GNS with per-example gradient norms is only present if itcan be gathered without affecting training time. Focusing on LayerNorm layers, we note the mainspeed bottleneck is the memory I/O when not implemented as a fused kernel. To demonstrate this,we develop a custom kernel to compute both the backward pass and the per-example gradient normsat the same time. Using this kernel the throughput overhead of gathering the per-example gradient iszero, even outperforming PyTorchs LayerNorm at larger dimensions. We apply this to a practicalbatch size schedule case study in .",
  "Aggregated": ": Gradient noise scale (GNS) is typically computed by comparing per-minibatch (aggregated-across-layers) gradients to gradients Aggregated across minibatches. We estimate GNS with lowervariance by making each minibatch a single example, and maintain per-layer GNS estimates. We findthe magnitude of gradients (visualized by the length of red arrows) to be consistent across layers,enabling overall GNS to be computed very cheaply using only gradient stats from LayerNorm layers.",
  ".(2)": "Where is the learning rate and H is the Hessian of the loss. On the right hand side is a factor thatdepends on B. It may be shown that the optimal step size and optimal change in the loss isachieved when B = Bnoise := tr(H)/GT HG. Averaging this optimal step over an entire run, andmeasuring this value by a grid search, yields Bcrit which describes a batch size that meets an optimaltradeoff between cost and training speed. It is shown by analysis and experiment that Bnoise Bcrit.",
  "Similar algorithms for other layer types described in Appendix B": "0.000 0.005 0.010 0.015 0.020 0.025 GNS stderr l=4, s=1l=8, s=1l=16, s=1l=32, s=1l=64, s=1l=128, s=1 0.000 0.005 0.010 0.015 0.020 0.025 GNS stderr l=128, s=1l=128, s=2l=128, s=4l=128, s=8l=128, s=16l=128, s=32l=128, s=64 : The variance of the GNS estimator for different Bbig (left) and Bsmall (right) sizes. Bbig = land Bsmall = s in legends. Stderr is estimated using a jackknife resampling method for ratioestimators . For the same number of samples processed, a smaller Bsmall always has a lowerstandard error, while the size of the large batch, Bbig does not affect the standard error.",
  "tr(),(5)": "where Bbig and Bsmall are the batch sizes used to compute the gradients GBbig and GBsmall, respectively(potentially corresponding to Aggregated and Minibatch gradients as depicted in ).GBbig2 is trivially computed using the gradients accumulated for the optimizer but GBsmall2 is not.One option is to use the gradients communicated between Distributed Data Parallel (DDP) nodes,but this has two downsides: (1) the variance of the estimate is tied to the DDP configuration and(2) the estimate is not available in all training configurations. For example, experiments on a singleGPU cannot use this method. One can also access the gradients during gradient accumulation, butthis similarly depends on the training configuration. A full taxonomy of the options for computingGBsmall2 is provided in Appendix A. For each observation ofGBbig2 we may observe multiple GBsmall2, typically Bbig/Bsmall of them.On each step the estimate of GBsmall22 is therefore a mean over Bbig/Bsmall samples, whose varianceis reduced according to the law of large numbers. However, the GNS is a ratio of the unbiasedestimators in Equations 4 and 5, so it may not be clear how this affects uncertainty in the GNSestimate. explores this relationship by simulation of a setting where the GNS is set to 1 whilevarying Bbig and Bsmall. We find it is always better (less uncertainty) to use the smallest possibleBsmall to estimate the GNS, while the choice of Bbig is irrelevant.",
  "Efficient Per-example Gradient Norms": "Goodfellow proposes a trick to compute gradient norms for individual examples in a minibatch,which would provide the minimum variance estimate of the GNS as described in .1.Neglecting the original derivation, by writing the desired squared norm as a tensor contraction thetrick may be reproduced automatically via einsum path optimization . The tensor contractionfor per-example gradient norms, n2b, of a linear layer in the 2D setting is,",
  "txbtiybtk)2 = xbtiybtkxbuiybuk = XXT , YYT 2F ,": "which has O(T 2) memory complexity in the sequence length T.2. Index sets are b [1, B], i [1, I], k [1, K], t, u [1, T]. At some point, the I/O cost of computing the per-example gradientnorms by computing the full wb explicitly will be cheaper. Noting this fact motivated the work in and the practical relationship between these resource costs is explored in .1.",
  "Related Work": "Gradient normsOne common motivation for computing per-example gradient norms is for differ-ential privacy. By bounding the gradient for any single example, we can ensure each example has alimited impact on the final parameters . Per-example gradient clipping has been performedwith convolutional networks and sequential models, e.g., LLMs . These methods allowcontrol over per-example gradient norms even when training with large batch sizes. Approaches likethese are implemented in the differential-privacy library Opacus , and have support natively inPyTorch, but are less efficient than the methods proposed in this paper. An alternative mechanismto manifest per-example gradient norms is to simply use a batch size of one. While not efficientenough for training large-scale networks, such sequential training may arise in situations such asreinforcement learning, where per-example gradient clipping has also been performed (to improvestability ). Gradient noise scaleThe Gradient Noise Scale has been widely used for training large-scaleneural networks. For example, Brown et al. note the GNS was measured during training and usedto guide batch sizing when training GPT-3. Dey et al. mention that operating near the criticalbatch size, as dictated by the GNS, is important for hyperparameter transfer under the maximalupdate parameterization . Even when not explicitly mentioned in publications, open source codeoften implements the GNS (e.g., see codebases for GPT-NeoX and Hourglass DiffusionTransformer ). Measurements similar to the GNS have also been used in a range of prior work to guide batch sizingfor minibatch SGD . Chen et al. show experimentally that wider networks canbe trained using larger batches; they also establish a theoretical connection between wider networksand gradient variance, albeit for simple two-layer networks. In contrast, Shallue et al. foundempirically that narrower Transformers scale better to larger batch sizes. Smith and Le proposea noise scale based not on gradient variance, but on the learning rate, dataset size, and batch size(similar to the notion of temperature in .1). Zhang et al. find the critical batch sizedepends on the choice of optimizer. Faghri et al. introduce a gradient clustering and stratifiedsampling approach to minimize minibatch gradient variance, and use this approach as a tool to helpunderstand optimization. Gradient varianceBeyond computing the GNS, our method can support other applications wheremeasuring the distribution of per-example gradients is useful or informative. Gradient variancehas been used to classify the difficulty of examples , which can be used, for example, to surfaceproblematic examples for human auditing. The question of whether gradient distributions tend towardGaussian in the (central) limit is of theoretical significance , with implications toward the abilityof SGD to escape sharp minima and land in wide basins . Bounded gradient variance isalso assumed in some convergence analysis , as noted in . Perhaps the most familiar use of gradient variance is of course in adaptive optimizers like Adagrad,Adam, and others that reduce step sizes in high-gradient-noise directions . Hiltonet al. [28, App. C] directly relate Adam second moment statistics to a component-wise version of theGNS. Optimizers typically estimate gradients jointly across training steps and minibatches, howevervSGD leverages separate components for gradient momentum and for gradient variation acrosssamples. Zhang et al. find the variance of gradient norms across examples predictive of whether",
  "Simultaneous Per-example Gradient Norms": "As described in , computing GNS requires small batch gradient norms. Typically, these maybe gathered during gradient accumulation or DDP communication.3 However, these methods arenot universally applicable and may not be available in all training configurations. In this section wedescribe a method for baking the computation of the per-example gradient norms into the computationgraph, making it universally applicable. The typical tensor contraction used to compute the backwardgradient in a linear layer using the input activations, x, and gradients, g, is,",
  "wk,l =x...kg...l,": "in other words, a sum over vector outer products for every vector in the trailing dimension. In principle,it is possible to access the intermediate tensor containing the batch dimension wbkl = xb...kgb...l.This allows us to compute the per-example gradient norms with FLOPs scaling at the same rate asthe normal, non-per-example backward pass (), albeit at increased I/O cost due to having tomaterialize the intermediate tensor. A generic algorithm to compute the per-example gradient norms simultaneously with the weightgradient in a standard linear layer is provided in Algorithm 1 using einsum for readability andportability.4 The reason for the correction in step 4 can be seen by considering the gradient of lossfunction L with respect to the weights on a single example b, wb,",
  "FLOPs and I/O Costs": "The computational cost of computing per-example gradient norms can be broken down into FLOPs,in , and I/O, in , with matrix multiplication on current devices being potentiallybottlenecked by both. We estimate ideal FLOP and DRAM I/O costs, assuming optimal reuse ofdata loaded from DRAM into SRAM with no recomputation. In practice, duplicate computationmay be used to improve wall-clock time and to fit within hardware limitations of the amount ofshared memory available. We compare here against the efficient per-example gradient norm methoddescribed by Li et al. , which the authors note is only efficient (in terms of I/O cost) when2T 2 < PD, where T is the sequence length, P is input and D is output dimension of the linear layer.This bound is discussed further in Appendix E. In terms of FLOPS, shows the simultaneous per-example gradient norms are almost alwayspreferable, only being more expensive for very short sequence lengths in small models. The reasonfor this is shown on the right hand side; the number of FLOPs required to compute the simultaneousper-example gradient norms is independent of the sequence length.",
  "Parameters": "IO (proportion of model forward pass IO) AlgorithmLi et alSimultaneousLN onlySequence Length2561024204840968192163843276865536 : Total I/O cost of computing per-example gradient norms, assuming gradients and parametersare stored with 4 bytes of precision. The relative IO cost of Simultaneous per-example gradientnorms is less than Li et al. for very long contexts for all model scales, approximately equivalentfor models of 10B parameters and 4096 context length, and higher for shorter contexts with largermodels. The IO cost of LN (LayerNorm) per-example gradient norms alone is much lower than eithermethod. The I/O cost shown in 4 illustrates a tradeoff in computing the per-example gradient norm. Thesimultaneous method is more expensive at large model sizes with short sequence length because itmust act on a large intermediate tensor. To estimate model flops, we use PyTorchs FLOPCounterMode, which only measures the FLOPs inmatrix multiplications and attention computation, however these make up the vast majority of theFLOPs in a Transformer model.",
  "Gradient Noise Scale in Transformer Language Models": "Using the methods described in previous sections to measure per-example gradient norms and estimatethe GNS, we perform experiments on a 111M parameter Chinchilla-optimal language model using the OpenWebText dataset .5 As the prior work was performed on Pile , Appendix C.1describes an experiment to check the optimality of the Chinchilla model on this dataset. We alsofound Flash attention led to numerical instability, which we were able to mitigate with an architecturalmodification described in Appendix C.2.",
  ": GNS phase plot: Linear/Embedding layers are separated from LayerNorm layers by row.Component estimators of Equations 4 and 5 are shown (left) with the GNS over the course of trainingon the (right)": "All experiments computed per-example gradient norms for all layers in the model with the exceptionof the performance results of Sections 5.1 and 5.2, which only computed per-example gradient normsfor the normalization layers. Each experiment was run on Nvidia A10 GPUs, in either 12 or 24 hoursdepending on the precision used, Bfloat16 or Float32 respectively. We used the nanoGPT6 codebasewith the layers described in added. Having an accurate estimate of the GNS statistics G22 and S allows us to visualize the movementof both in a phase space during training as shown in . LayerNorm layers are separate fromthe rest of the network because their statistics are much smaller and to illustrate how the resultingGNS estimates on the right track each other. To observe these trends in another training regime, see in Appendix D.1.",
  "The Temperature of Training": "McCandlish et al. [39, App. C] observed that the GNS measurement depends on the batch size andlearning rate used in training. In fact, from the derivation outlined in .1, the gradient noisescale is only well-defined at the optimal learning rate. Using a toy model of a quadratic loss function,they observed that the GNS should be inversely proportional to the temperature, T, a ratio of batchsize B to learning rate :",
  "This enables a testable prediction that the GNS will increase with increasing batch size or withdescending learning rate. This prediction was found to accurately describe experiments on a small": "5.56.06.57.07.58.08.5 Tokens processed1e8 GNS original , 16B /16, B/4, 4B/4, B/4 : During the middle of training a 111M parameter language model on OpenWebText, thelearning rate, or batch size, B were varied, restarting the run from the same point. This Figurereplicates an experiment from McCandlish et al. showing how varying the ratio causes changesin the measured GNS, but here only due to changes in the learning rate. Changes in the batch size donot have the predicted effect. convolutional model on the SVHN dataset. We repeat it here in the setting described above in .To match the results of McCandlish et al. , all interventions tested should yield the same result.We find the GNS does indeed react predictably to changes in the learning rate, but the reactions tochanges in the batch size are not predicted by the theory.",
  "GNS Correlates Between Layer Types": "Inspection of suggests the LayerNorm layers produce a similar GNS, when combined, asthe total GNS of the model. Before describing how to quantify this relationship we must first notethat the unbiased estimators G22 and S are noisy. All GNS figures presented in this paper and otherwork smooth both of these estimators, typically with an Exponential Moving Average (EMA) filter,before computing the GNS ratio.7 So, when quantifying the relationship between the GNS of different layers, it must be compared fordifferent smoothing factors. Here, we show the regression coefficients with respect to the alpha ofthe EMA filter in . The results show that the GNS of the LayerNorm and Attention layersare highly predictive of the total GNS of the model. In both cases, the slope is approximately 1.4,meaning the total GNS is approximately 1.4 times the GNS of the LayerNorm or Attention layers. Comparing the quality of this fit versus the quality of prior works overall fit of the GNS to the criticalbatch size (measured empirically) , the quality seems acceptable and we do not need to apply this1.4x correction factor, rather we just note that the true Bcrit may be greater than the measured Bsimple.",
  "Batch Size Scheduling": "We focus on two concerns that affect the practicality of batch size scheduling. First, measuring theappropriate batch size without incurring any additional training time. We find this is possible with themethod described in .1. Second, whether batch size scheduling is effective in practice. Wefind it can offer significant savings in the required number of tokens processed in .2.",
  "Universal GNS with Zero Overhead": "Capturing a GNS estimate for a linear layer is powerful, but efficiently doing so presents a challenge.Such an estimate requires accumulating per-example gradients of hidden_size2 across the sequencedimension, compared to just hidden_size with LayerNorm. This increased size requires using morecomplex reductions in the kernel, rather than a simple warp reduction followed by shared-memoryatomic reduction with a final atomic global reduction (as we can implement for LayerNorm per-example gradients within shared memory). In addition, linear layer kernels are already highlyoptimized and require using advanced techniques to keep GPU tensor cores fed with data, so",
  "The results described in are explored at a 1.3B parameter scale in Appendix D.3": "Tokens1e9 GNS AttentionLayerNormMLPEmbeddingTotal 0.960.981.00 alpha 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 slope 0.960.981.00 alpha 0.96 0.97 0.98 0.99 1.00 r : Regression of total GNS using the GNS of each layer type. (Left) GNS of each layertype and the total GNS are plotted against the number of tokens processed for varying EMA alphasettings. (Center & Right) The slope and Pearsons correlation coefficient of the regression of thetotal GNS against the GNS of each layer type, respectively, as a function of the same EMA alphavalues. The total GNS (black) on the left is predicted well by individual layer types as indicated bythe correlation coefficients (right), however the type with slope closest to 1 is LayerNorm (center),only overestimating the GNS by less than 40% across EMA alpha values.",
  "Case Study: Batch Size Schedule": "As a case study we continue with the 111M parameter language model on OpenWebText describedabove. Over three seeds, we run both a fixed batch size and a batch size schedule that increaseslinearly with the number of tokens processed to the original batch size. We vary the batch size duringtraining by varying the number of gradient accumulation steps. Tokens processed 3 100 4 100 6 100 Train loss OriginalWith batch size schedule 1013 1004 1006 100 Train loss Tokens saved 1e8 : (Left) Linear batch size schedule tracking the GNS over 2.2 billion tokens processed. Lossis plotted over a smoothed range from 3 runs using different Seeds. (Right) The number of tokenssaved over the fixed batch size run to achieve the same loss. The results of this experiment are shown in . The left plot shows the progression of the lossfor both models, with the range of values captured over different seeds. The mean loss for the linearbatch size schedule leads the fixed batch size throughout training. On the right, this lead is quantifiedby interpolating the number of tokens saved to achieve the same loss. The precise schedule used isshown in in Appendix D.2.",
  "Limitations": "In this paper, we only studied Transformers, which include Normalization sub-layers natively. WhileTransformers are ubiquitous in machine learning, there are many models, including variations ofRNNs, CNNs, and state-space models, that do not use such layers conventionally. However, wenote LayerNorm could be added to these networks with very little overhead (in fact, the desireto normalize activations in RNNs was one of the original motivations for developing LayerNorm;application of batch normalization to RNNs was not obvious ). Nevertheless, investigatingLayerNorm-based GNS in these other models requires further work. Our work is also part of efforts to improve efficiency and address the increasing costs of trainingand tuning large neural networks . We provide both a more-efficient technique for computingthe GNS, and also, by enabling use of GNS statistics, we support compute-efficient training recipes,such as use of dynamic batch sizes. While some have argued that hyperscalers may re-invest anyefficiency savings into ever-larger models , for academic researchers, such savings could allowpushing the state-of-the-art, while still getting results in a reasonable timeframe. Recent effortsto enable frontier-model-performance within academic budgets are encouraging, both to reducememory and save compute . Of course, even for such economical approaches,extensive hyperparameter search may still be required . There is a growing awareness thathyperparameter tuning has a negative impact on equity in AI research, as tuning success dependsdirectly on researcher finances . A correlated trend is to use better training measurements(such as gradient noise in batch and step size optimizers (.3)) to reduce dependence onhyperparameters, and in this way we hope our work can also ultimately improve research equity.",
  "Conclusion": "This work set out to provide a practical method for computing the per-example gradient normsnecessary to compute the GNS independent of the training configuration. In the process we discoveredthat not all the layers are necessary for a practical estimate of the GNS and that the per-examplegradient norms can be computed for the normalization layers with zero overhead. This enabledpractical experiments, such as a batch size schedule and replicating prior GNS observations. Weare hopeful that democratising access to GNS statistics, on any device, will enable subsequentdiscoveries. Chirag Agarwal, Daniel Dsouza, and Sara Hooker. 2022. Estimating example difficulty usingvariance of gradients. In Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition. 1036810378.",
  "Lukas Balles, Javier Romero, and Philipp Hennig. 2016. Coupling adaptive batch sizes withlearning rates. arXiv preprint arXiv:1612.05086 (2016)": "Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021.On the dangers of stochastic parrots: Can language models be too big?. In Proceedings of the2021 ACM conference on fairness, accountability, and transparency. 610623. Sidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding,Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, Usvsn Sai Prashanth,Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. 2022.GPT-NeoX-20B: An Open-Source Autoregressive Language Model. In Proceedings of the ACLWorkshop on Challenges & Perspectives in Creating Large Language Models.",
  "Lon Bottou, Frank E Curtis, and Jorge Nocedal. 2018. Optimization methods for large-scalemachine learning. SIAM review 60, 2 (2018), 223311": "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, ArielHerbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, MateuszLitwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, AlecRadford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners.arXiv:2005.14165 [cs.CL]",
  "Jacob Hilton, Karl Cobbe, and John Schulman. 2022. Batch size-invariance for policy optimiza-tion. arXiv:2110.00641 [cs.LG]": "Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, ElizaRutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, TomHennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, AureliaGuy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and LaurentSifre. 2022. Training Compute-Optimal Large Language Models. arXiv:2203.15556 [cs.CL]",
  "Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXivpreprint arXiv:1412.6980 (2014)": "Frederik Kunstner, Jacques Chen, Jonathan Wilder Lavington, and Mark Schmidt. 2023. Noiseis not the main factor behind the gap between SGD and Adam on transformers, but sign descentmight be. arXiv preprint arXiv:2304.13960 (2023). Frederik Kunstner, Robin Yadav, Alan Milligan, Mark Schmidt, and Alberto Bietti. 2024.Heavy-tailed class imbalance and why Adam outperforms gradient descent on language models.arXiv preprint arXiv:2402.19449 (2024).",
  "Xuechen Li, Florian Tramr, Percy Liang, and Tatsunori Hashimoto. 2022. Large LanguageModels Can Be Strong Differentially Private Learners. arXiv:2110.05679 [cs.LG]": "Xianhang Li, Zeyu Wang, and Cihang Xie. 2023. CLIPA-v2: Scaling CLIP Training with81.1% Zero-shot ImageNet Accuracy within a $10,000 Budget; An Extra $4,000 Unlocks 81.8%Accuracy. arXiv preprint arXiv:2306.15658 (2023). Sadhika Malladi, Tianyu Gao, Eshaan Nichani, Alex Damian, Jason D Lee, Danqi Chen, andSanjeev Arora. 2023. Fine-tuning language models with just forward passes. Advances inNeural Information Processing Systems 36 (2023), 5303853075.",
  "Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. 2018. SpectralNormalization for Generative Adversarial Networks.arXiv:1802.05957 [cs.LG]": "Thanh Huy Nguyen, Umut Simsekli, Mert Gurbuzbalaban, and Gal Richard. 2019. First exittime analysis of stochastic gradient descent under heavy-tailed gradient noise. Advances inneural information processing systems 32 (2019). David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, DanielRothchild, David So, Maud Texier, and Jeff Dean. 2021. Carbon emissions and large neuralnetwork training. arXiv preprint arXiv:2104.10350 (2021).",
  "Emma Strubell, Ananya Ganesh, and Andrew McCallum. 2019. Energy and policy considera-tions for deep learning in NLP. arXiv preprint arXiv:1906.02243 (2019)": "Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, and Nando Freitas. 2016.Dueling network architectures for deep reinforcement learning. In International conference onmachine learning. PMLR, 19952003. Mitchell Wortsman, Peter J. Liu, Lechao Xiao, Katie Everett, Alex Alemi, Ben Adlam, John D.Co-Reyes, Izzeddin Gur, Abhishek Kumar, Roman Novak, Jeffrey Pennington, Jascha Sohl-dickstein, Kelvin Xu, Jaehoon Lee, Justin Gilmer, and Simon Kornblith. 2023. Small-scaleproxies for large-scale Transformer training instabilities. arXiv:2309.14322 [cs.LG] Greg Yang, Edward Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, NickRyder, Jakub Pachocki, Weizhu Chen, and Jianfeng Gao. 2021. Tuning Large Neural Networksvia Zero-Shot Hyperparameter Transfer. In Advances in Neural Information Processing Systems. Dong Yin, Ashwin Pananjady, Max Lam, Dimitris Papailiopoulos, Kannan Ramchandran, andPeter Bartlett. 2018. Gradient diversity: a key ingredient for scalable distributed learning. InInternational Conference on Artificial Intelligence and Statistics. PMLR, 19982007. Ashkan Yousefpour, Igor Shilov, Alexandre Sablayrolles, Davide Testuggine, Karthik Prasad,Mani Malek, John Nguyen, Sayan Ghosh, Akash Bharadwaj, Jessica Zhao, Graham Cormode,and Ilya Mironov. 2022. Opacus: User-Friendly Differential Privacy Library in PyTorch.arXiv:2109.12298 [cs.LG]",
  "Biao Zhang and Rico Sennrich. 2019.Root Mean Square Layer Normalization.arXiv:1910.07467 [cs.LG]": "Guodong Zhang, Lala Li, Zachary Nado, James Martens, Sushant Sachdeva, George E. Dahl,Christopher J. Shallue, and Roger Grosse. 2019. Which Algorithmic Choices Matter at WhichBatch Sizes? Insights From a Noisy Quadratic Model. arXiv:1907.04164 [cs.LG] Jingzhao Zhang, Sai Praneeth Karimireddy, Andreas Veit, Seungyeon Kim, Sashank Reddi,Sanjiv Kumar, and Suvrit Sra. 2020. Why are adaptive methods good for attention models?Advances in Neural Information Processing Systems 33 (2020), 1538315393. Yushun Zhang, Congliang Chen, Naichen Shi, Ruoyu Sun, and Zhi-Quan Luo. 2022. Adam canconverge without any modification on update rules. Advances in neural information processingsystems 35 (2022), 2838628399. Zhanxing Zhu, Jingfeng Wu, Bing Yu, Lei Wu, and Jinwen Ma. 2018. The anisotropic noisein stochastic gradient descent: Its behavior of escaping from sharp minima and regularizationeffects. arXiv preprint arXiv:1803.00195 (2018).",
  "Gray et al. included an prior version of this taxonomy in their work": "The following taxonomy describes the different methods available to compute the GBsmall22 necessaryto compute the GNS as described in .1. Gradient norm cost below refers to the cost ofcomputing the norm of the gradient for all parameters in the model, which is typically orders ofmagnitude smaller than the cost of forward or backward passes.",
  "Approximation: GBsmall22 is approximated by assuming input activations are normallydistributed with mean zero .Pros: Fewer FLOPs than Exact methods.Cons: Not exact": "All of the methods described above can be measured either online or offline. The description abovefocuses on the online case; i.e. measuring the gradient norms during training. To use these methodsoffline: run the models without performing weight updates and measure gradient norms the sameway. The estimators of Equation 4 and 5 can then be aggregated using a mean rather than an EMA orby using a method to estimate measurement uncertainty such as the jackknife mentioned in (described in the context of GNS by Gray et al. [27, App.B]). This can be useful to estimate howlong to run the offline estimate.",
  "BAdditional Simultaneous Per-Example Gradient Norm Computations": "Algorithms 3 and 2 describe the process for computing the per-example gradient norms for theembedding and LayerNorm layers, which are typically the remaining layers in Transformer models.RMSNorm is practically identical to LayerNorm in this case because the parameters the gradientis computed wrt are in the affine transform, which is the same in both layer types.",
  "C.1Optimality on OpenWebText": "We chose to use the Cerebras-GPT recipes for experiments as they are designed to be Chinchillaoptimal. This means that each model size should achieve the lowest possible loss for a given FLOPbudget . However, these recipes were tuned on the Pile dataset and we used the OpenWebTextdataset so that results could be replicated (Pile is no longer publicly available). To verify that the training protocol is optimal on OpenWebText, we performed a small study toillustrate how the performance would vary as we vary the size and total tokens trained on. Model sizewas varied by changing the hidden size: the 70M model has a hidden size of 576, the 111M modelhas a hidden size of 768 and the 161M model has a hidden size of 960. The token budget for eachmodel size was chosen to keep the total FLOPs constant. The learning rate was varied to observe a minima in the loss at each model scale. The results areshown in . While we found that the learning rate may be increased overall, the 111Mmodel was found to have the lowest loss of the three models. From these results we conclude that thetraining protocol is optimal within this range of model sizes and we assume 111M is good enough. Inother words, a better model might exist between 70M and 161M parameters for this FLOP budget butit isnt outside of this range.",
  "C.2Flash Attention Numerical Instability": "The experiments described in Sections 4 and 5.2 involve Chinchilla optimal language models at a111M scale . These experiments were replicated according to the published information. Weencountered diverging runs when executing in bfloat16 Automatic Mixed Precision (AMP) consistentwith the default settings in nanoGPT. These experiments were executed on NVIDIA A10 GPUs foraccessible replication at small scale. By ablation it was found that these runs would diverge:",
  "When using PyTorchs AMP in bfloat16 precision When using Flash attention": "This was surprising because prior work had trained these models successfully . In that work themodel was also trained using bfloat16 AMP precision, but it was trained on a Cerebras CS-2 system.Due to this difference, we suspected the issue was due to a difference between the efficient attentionkernel and the Flash attention kernel in PyTorch. By inspecting the histograms of weights and biases in the Query, Key, Value (QKV) projectionduring training, we found that range grew the fastest in block 1 (the second block in the model). Inaddition, we observed that the histogram of the query and key projection weights became bimodalas the gradient norm diverged. This is illustrated in . Further analysis of a checkpointtaken at this point in training focused on the difference between gradients computed using the flashattention kernel and the nanoGPT pure PyTorch attention implementation using float32 precision. Atinitialization the gradients were not significantly different but at the point of divergence there was asignificant difference coinciding with increased parameter norms in that layer. To replicate the issue from scratch, we came up with a simulation from a generic initialization. Inspiredby the teacher-student experiment protocol proposed by Ba et al. (although otherwise unrelated)we set up a learning task with a teacher and student model with the same architecture. Bothnetworks begin with the same weights but we add a small amount of noise to the teachers weights.The student is trained to match the teachers output. After experimenting with hyperparameters wewere able to replicated the divergence seen during training9, as illustrated in . Using this isolated simulation we were able to test different methods to mitigate the divergence.Karras et al. suggested that cosine attention could address similar divergences attributed toself-attention. In we replicated the experiment described in using cosine attentionand found that the divergence no longer occurred. Separately, experimenting with precision ablation found that if float32 precision was used only inblock 1 (2nd) then the divergence would also not occur. Based on this and the above, we found thefollowing two architectural mitigations for the divergence, in only block 1 (2nd):",
  "Teacher Distance Difference": ": Two student networks, identical to the teacher network except for the addition of asmall amount of noise to the teachers QKV projection bias. As training progresses, the student usingFlash attention diverges for the same inputs. Plots are, clockwise from top left: Bias Norms showsthe norms of the bias layer in each of the networks, Distances to Teacher shows the L2 distancefrom each student to the teacher. Flash to Non-Flash Distance shows the L2 distance between thestudent using Flash attention and not, Teacher Distance Difference is the difference between thedistances to the teacher for both cases.",
  "Use spectral normalization on the QKV projection": "Critically, only modifying a single layer does not affect the throughput of the model, the observedModel FLOPs Utilization (MFU) did not decrease by more than 1% in either case. Both of thesebound the norm of the query and key head vectors prior to attention. Spectral normalization achievesthis because the QKV projection is preceded by a LayerNorm layer. Using this mitigation on the111M model allowed the experiment to be replicated on an NVIDIA A10 GPU and we observed thesame behaviour as running the model more slowly in float32 precision. Similar divergences are discussed in prior literature (and in nanoGPTs issue tracker) but we areunable to verify that it is the same problem. Wortsman et al. discuss how to build similarexperiments to those described above but do not investigate flash attention specifically. Goldenet al. investigate the numerical stability of Flash attention but neglect to demonstrate a failuremode that affects real training runs. Zhai et al. focus on the numerical stability of attention ingeneral and propose a similar mitigation (their method, Reparam, is a scaled version of spectralnormalization) but do not investigate flash attention specifically. It is likely that the mitigation proposed will not work in all cases, such as for larger models. However,we only needed to replicate at the scale we were working at. The experiments in and are included to illustrate how bounding the norm of the query and key head vectors seemsto be important for numerical stability. However, this may change in future versions of the flashattention kernel, these results were obtained with PyTorch 2.4.0.",
  "D.3Larger Scale Training": "To demonstrate that the method scales to larger models, we trained a 1.3B parameter GPT model10 onOpenWebText using 8 H100 GPUs. The results of this experiment are shown in . The leftplot shows the per-example gradient norms for all layers, while the right plot shows the per-examplegradient norms for only the LayerNorm layers. The GNS computed using the traditional DDP methodis also shown for comparison. In a we observe that the LayerNorm remains predictive of thetotal GNS, as in the 111M model results of . When all non-fused simultaneous per-examplegradient norms were collected we observed an MFU of 40% and when only the fused LayerNormlayers were collected we observed an MFU of 57%. After completing this experiment a bug was discovered in the code that decreased per-examplegradient norms by a constant factor. This caused an underestimation of the GNS. In bthis can be seen when we compare the GNS estimated via DDP method. Initially, we assumedthat this constant factor was due a failure of the LayerNorm GNS approximation to larger models.Unfortunately, we did not have the budget in time or resources to rerun the experiment so we correctedthe results by multiplying by the constant factor observed in the comparison to the DDP method. This may be representative of real world scenarios where a large model is pretrained over many DDPnodes. As the user has access to two methods to estimate the GNS, they may account for any bias orslope between the estimates. Then, if it is necessary to continue training on a single node, they canuse the per-example gradient norms to estimate the GNS. Similar techniques can involve enablingper-example GNS estimation for all layers for a short time, or estimating the GNS offline as describedin Appendix A.",
  "Batch Size": "GNS and Batch Size over Tokens Processed (alpha=0.999) GNSBatch Size : The batch size schedule used and GNS observed in the 111M batch size scheduleexperiment illustrated in . An aliasing issue is noticeable in the interpolated linear batchsize schedule that was used. This has since been fixed in the published code. 0.960.981.00 alpha 0.5 1.0 1.5 2.0 slope AttentionLayerNormMLPEmbedding 0.960.981.00 alpha 0.4 0.5 0.6 0.7 0.8 0.9 1.0 r"
}