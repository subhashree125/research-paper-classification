{
  "Abstract": "Large Language Models are known to capture real-world knowledge, allowingthem to excel in many downstream tasks. Despite recent advances, these modelsare still prone to what are commonly known as hallucinations, causing them toemit unwanted and factually incorrect text. In this work, we propose a novelcalibration method that can be used to combat hallucinations. We add a special[IDK] (I dont know) token to the models vocabulary and introduce an objectivefunction that shifts probability mass to the [IDK] token for incorrect predictions.This approach allows the model to express uncertainty in its output explicitly. Weevaluate our proposed method across multiple model architectures and factualdownstream tasks. We find that models trained with our method are able to expressuncertainty in places where they would previously make mistakes while sufferingonly a small loss of encoded knowledge. We further perform extensive ablationstudies of multiple variations of our approach and provide a detailed analysis ofthe precision-recall tradeoff of our method.1",
  "Introduction": "Large Language Models (LLMs) are pretrained on massive amounts of text to understand and generatelanguage. This training text includes a large portion of written human knowledge such as books,newspapers, Wikipedia, and scientific articles. During this process, LLMs also retain a remarkableamount of the information seen during pre-training, allowing them to encode real-world knowledgein their parameters and act as knowledge bases [Petroni et al., 2019, Roberts et al., 2020, Cohen et al.,2023a, Pan et al., 2023]. Owing to this phenomenon, LLMs can be used in multiple settings requiringthis real-world knowledge, such as closed-book question answering [Brown et al., 2020, Robertset al., 2020] and information retrieval [Tay et al., 2022]. Despite the popularity of LLMs, they are prone to what is commonly referred to as hallucinations,which severely hinder their performance and reliability [Ji et al., 2023, Manduchi et al., 2024].Examples of hallucinations include factually incorrect [Maynez et al., 2020, Devaraj et al., 2022, Tamet al., 2023], inconsistent [Elazar et al., 2021, Mndler et al., 2023], self-contradicting [Cohen et al.,2024] or non-attributable text [Bohnet et al., 2022, Rashkin et al., 2023, Yue et al., 2023].",
  "[Paris][Berlin]": ": Illustration of our proposed IDK objective. During continual pretraining, we shift someprobability mass of wrong predictions towards a special [IDK] token. The amount of shiftedprobability mass depends on the uncertainty in the models prediction. We detail our method in. are better aligned with their quality. This calibration allows LLMs to explicitly express uncertainty,allowing them to caveat their responses or even refrain from answering. Although many of theproposed methods do lead to an improvement in model calibration [Geng et al., 2024], they have stillbeen found to be lacking [Chen et al., 2023]. In this work, we propose a novel objective function that allows LLMs to explicitly express uncertainty.We add a new special [IDK] (I Dont Know) token to the vocabulary of the language model.During a continued pretraining phase, we modify the conventional cross-entropy objective to expressuncertainty in a next-token prediction as probability mass on the [IDK] token. Specifically, each timethe model fails to predict the gold label, some of the probability mass of the target is shifted to the[IDK] token based on an Uncertainty Factor we calculate based on the predicted logits. We refer toour method as IDK-tuning. Our proposed IDK objective differs from previous work as we intervene during a continued pretrainingphase with the language modeling task. Crucially, we do not rely on any labeled data. Moreover,this allows the model to be later finetuned on specific tasks while the model has already learned toexpress uncertainty. We conduct IDK-tuning using various model architectures and sizes, and then evaluate them on diversefactual downstream tasks. Our results show a large increase in factual precision of IDK-tuned modelswhile causing only a small decrease in recall of factual knowledge that was contained in the basemodel. We conduct extensive ablation studies for the individual components of our IDK objectiveand analyze its effect on optimization dynamics. We finally show that IDK-tuning does not harm thegeneral language modeling ability of models, such as long text generation.",
  "IDK-tuning": "Our goal is to train a model to be aware of its unawareness and to effectively express it. For this, weintroduce a new special token to its vocabulary: [IDK]. The model is intended to express uncertaintyby putting probability mass on the [IDK] token in its predictions. In practice, we adapt the modelspretraining objective, aiming to teach it to use the [IDK] token effectively. Our objective does notrequire annotations of uncertainty or specifically crafted datasets (e.g., Q&A). Instead, we leveragethe uncertainty captured by the pretraining objective on its pretraining data and use it to encourageprobability mass on the [IDK] token in cases of uncertainty. We hypothesize that this generalizes touncertainty expressed on downstream tasks like Q&A, which we experimentally verify later on.",
  "The [IDK] token": "The purpose of the new token is to represent lack of knowledge. Ideally, whenever the model wouldhave been making a mistake, we want it to instead predict this token. That is, rather than generatinga wrong token, we would like to model to generate the [IDK] token, as a means of conveying itsuncertainty. We can consider this as a model expressing its lack of knowledge and may then chooseto ignore its outputs. The more the model opts for this token rather than predicting the wrong answer,the more we improve the models precision. For instance, let us consider the setup of Factual Sentence Completion. In this setup, the modelreceives an incomplete sentence as an input and is expected to complete it factually. For example,a valid input would be Paris is the capital of, and a factually correct output by the modelwould be France. In this setup, if the model was going to predict Germany, using the [IDK]token instead increases factual precision by refusing to answer a question where the answer wouldhave been wrong. Naturally, almost universally predicting [IDK] indiscriminately may yield highprecision but is not helpful. Therefore, taking into account the recall of factually correct answers iscrucial in evaluating our method. We analyze both the precision and recall of our method in . We add this new [IDK] token to the models vocabulary and initialize its embedding randomly.The embedding is optimized alongside the rest of the models parameters during training. We nextdescribe our proposed IDK objective.",
  "The IDK Training Objective": "We modify the conventional cross-entropy objective between the softmax distribution over the modelsprediction and the correct answer, such that each time the model fails to predict the correct token, it isencouraged to instead put some probability mass on the [IDK]. This encouragement is modulated byan Uncertainty Factor denoted as that is larger the more uncertain the model is and exactly0 when the model predicts the correct token. We now define our modified cross-entropy objective. We use [gold] to denote the gold token (correcttarget) for each prediction. We denote the probability mass assigned to an arbitrary token [tok]in the prediction of a model as prob(yt = [tok]|y<t, x) We further use 1[IDK] to denote a one-hottarget vector with one at the index of the [IDK] token. Per convention, y denotes the one-hot targetvector for the [gold] token. The modified objective is defined as follows:",
  "LIDK = LCE(y, (1 ) y + 1[IDK])(1)": "If the model is uncertain in its prediction, the target is shifted away from predicting the [gold] tokenand towards the [IDK] token. This is modulated by . Note that in case the model makes the correctprediction, = 0 and LIDK therefore reduces to the regular cross-entropy loss. When the model iscorrect, LIDK simply provides the signal for the correct prediction. When the model is incorrect, LIDKprovides both the signal for the correct prediction and a signal to express uncertainty. We now detailthe construction of the Uncertainty Factor . The Uncertainty Factor. is constructed as a scalar weight with . Intuitively, we want to be close to 1 when the model is very uncertain and 0 when the model makes the correct prediction.Based on this, we define as one minus the probability mass on the gold token divided by themaximum probability mass put on any token:",
  ",(2)": "where is a hyperparameter to control the influence of our objective. When the goldtoken probability is close to the maximum probability, is close to 0. If the model makes a correctprediction (the gold token is assigned the maximum probability), is 0, thereby reducing Equation 1to the regular cross-entropy loss. When the gold token probability is much lower than the maximumprobability, is close to 1, which translates to shifting almost all the probability mass of the targetin Equation 1 to the [IDK] token. specifies the upper bound of target probability mass that canbe shifted to the [IDK] token. For example, = 1",
  "in the target can be shifted to [IDK] while the rest remains with the gold token. In practice, we donot tune this and set = 1": "2. This prevents the [IDK] token from ever becoming a better predictionthan the gold token while still providing enough signal to predict [IDK] for uncertain predictions.We perform an ablation of the influence of in .2. Uncertainty Regularization.An important consideration in designing the LIDK objective is toprevent a collapse where the model is miscalibrated with too many false positive [IDK]s, putting toomuch probability mass on [IDK], although it could have made the correct prediction. Therefore, weadd the following anti-false positive regularization to our objective:",
  "LFP-reg = log(1 prob(yt = [IDK]|y<t, x)),(3)": "which is exactly the binary cross-entropy objective with 0 as the target and the probability massassigned to the [IDK] as the input. We only add this regularization objective when the modelsprediction is correct. This aims to minimize the [IDK] tokens probability mass the model learns topredict in cases it knows the answer thus teaching it to minimize the use of this token in cases it ismore certain, and is designed to reduce a decrease of its recall. We perform an ablation of LFP-reg in.2.",
  "Experiments": "We use our proposed IDK objective to tune various pretrained models to use the new [IDK] token. Wedub this process IDK-tuning. We then report the results of the IDK-tuned models on commonly usedfactual benchmarks, showing that our method improves factuality while paying only a small price interms of knowledge recall. We also show that model size plays a significant role in the success of ourmethod to create an effective uncertainty-aware model. We employ continual training of pretrained models rather than training from scratch for two reasons:(i) the computational cost of training models that perform competitively on current benchmarks fromscratch would be prohibitive, and (ii) starting from a model that is already a strong language modelerhelps during the optimization process by providing a rough initial calibration that we utilize to derivethe Uncertainty Factor.",
  "IDK-tuning Setup": "We use bert-base-cased [Devlin et al., 2019], mistralai/Mistral-7B-v0.1 [Jiang et al., 2023],and EleutherAI/pythia-70m 2.8B [Biderman et al., 2023] for our base models for IDK-tuning.For IDK-tuning Mistral-7B-v0.1, we train on data randomly sampled from The Pile [Gao et al.,2020]2 with a context length of 4,096. We use example packing to fill the entire context length. Weuse a maximum learning rate of 4 105 with a linear warmup for 10% of the training steps and acosine decay down to 2 106. We use a batch size of 256, weight decay of 0.05, gradient clippingof 1.0 and AdamW betas (0.9, 0.95). We train for 1,024 optimizer steps resulting in a total of 1Btraining tokens. For the pythia-70m 2.8B models, we use the same hyperparameters but reducethe context length to 2,048 to match the models positional embeddings. We use bfloat16 andfloat16 mixed-precision training to match Mistral-7B-v0.1 and pythia-410m 2.8B pretraining,respectively. For pythia-70m, pythia-160m and bert-base-cased, we observed NaN errors in thepredicted logits irrespective of our loss modifications. Since the models are small enough, we switchto pure float32 for these models without using mixed-precision. In addition, for bert-base-casedwe apply MLM [Devlin et al., 2019], while for each input, we randomly mask one of the tokens.",
  "Evaluation Setup": "Evaluation Data.We consider the following datasets: LAMA [Petroni et al., 2019], TriviaQA[Joshi et al., 2017], and PopQA [Mallen et al., 2022]. These cover a wide range of queries, forexample trivia questions (TriviaQA), and subject-relation-object facts phrased as queries (LAMA,PopQA). We consider the closed-book open-ended setting, where we do not provide any contextor answer choices to the model. Importantly, in the case of TriviaQA and PopQA, where the inputis formed as a question, we reduce it into a sentence completion task, using GPT4. Specifically,we prompt it to phrase the question as a sentence, while also providing it with some in-contextexamples that we manually created. See Appendix C for more details and the full prompt. Toevaluate multiple-choice question answering, we use EleutherAIs lm-evaluation-harness [Gaoet al., 2023]. Specifically, we use ARC [Clark et al., 2018], HellaSwag [Zellers et al., 2019], MMLU[Hendrycks et al., 2020], TruthfulQA [Lin et al., 2022a], WinoGrande [Sakaguchi et al., 2021], andGSM8k [Cobbe et al., 2021].",
  "Baselines.For each of the evaluation datasets, we compare the IDK-tuned model with its originalbase model without any further training. Furthermore, we consider three different baselines:": "1. confidence-based baseline: We use the predicted probability mass in the language modelinghead of the LM as a measure of confidence in the prediction [Yoshikawa and Okazaki, 2023].We consider the first token generated by the LM. In case the corresponding probability massof this token is greater than a fixed threshold, we consider the generation as valid. Otherwise,we consider this as an uncertainty expression (analogous to an [IDK] token generation inour model). To create a strong baseline, we search for the best threshold via hyperparametertuning on the development set. 2. P(True) baseline [Kadavath et al., 2022]: Given an input sentence to complete, which werefer to as I, we use the original model to generate the completion, which we refer to asA. We then concatenate I and A and ask the model: \"Please answer either with true orfalse only. Is it true that: IA\". If the model answer is not true, we consider this specificexample as unknown for the model namely the same case as if the IDK-tuned model wouldgenerate the [IDK]. 3. Semantic Entropy baseline [Kuhn et al., 2023, Aichberger et al., 2024]: We sample Ktext generations from the model, encode them using a state-of-the-art semantic encoder andcluster their encodings. If the largest cluster size is larger than K",
  ", then we take a randomgeneration out of this cluster as the models answer. Otherwise, we consider this example asunknown": "Evaluation.We evaluate how well our models use the new [IDK] token by measuring theirfactuality and knowledge memory, using the following metrics: (i) Precision: the portion of factuallycorrect completions, out of all the claims that have been completed with any token that is differentfrom the [IDK] token, i.e., the claims that the model was certain enough about, and tried to factuallycomplete. (ii) Recall: the portion of factually correct completions, out of all the claims in the dataset.Namely, the portion of knowledge memory the model has, out of the entire test set we evaluate on.(iii) F1: the harmonic mean of precision and recall. In the case of base models without additionalcalibration methods, the precision, recall, and F1-scores all correspond to their accuracy on the task. In .2, we use two further metrics to analyze the patterns when IDK-tuned models predict[IDK]. For this, we use the notion of correctly predicting [IDK]: We consider an [IDK] prediction tobe correct if the base model does not predict the correct answer for an instance. We define (i) IDKrecall: the fraction of instances the model predicted [IDK] correctly out of all instances where thebase model did in fact not predict the correct answer, and (ii) IDK error rate: the fraction of instanceswhere the model predicted [IDK] incorrectly out of all instances where the base model did indeedpredict the correct answer.",
  "Mistral-7B-v0.1 + IDK-tuning on The Pile71.140.651.788.565.575.372.044.354.972.552.060.678.120.532.5": ": Precision (P), Recall (R), and F1-scores for Mistral-7B-v0.1. Our IDK-tuning achievesthe best precision with minor decreases in recall, outperforming previous work. Mistral-7B-v0.1+ Confidence Threshold refers to the baseline based on the probability mass of the predicted an-swer [Yoshikawa and Okazaki, 2023]. Mistral-7B-v0.1 + The Pile refers to the ablation discussedin .2.",
  "Main Results": "Mistral-7B-v0.1 results. shows the results of our largest model Mistral-7B-v0.1on factual closed-book sentence completion datasets.Our results show that the IDK-tunedMistral-7B-v0.1 has a much higher precision namely the model generates significantly fewerfactually incorrect completions and instead puts probability mass on the [IDK] token. However, themodel does show decreased knowledge recall on some tasks. Overall, we observe an increase inthe average F1-score. shows the averaged results on the lm-eval-harness datasets. Thetrend here is similar, although the increase in precision compared to baselines is slightly lower. Thissuggests that the model tends to be more certain when it comes to multiple-choice questions. Scaling behavior of IDK-tuning.We further investigate the effect of model size on the suc-cess of IDK-tuning. We conduct IDK-tuning for each of the pythia-70m 2.8B models as de-tailed in .1. In , we plot the average precision, recall, and F1-score for each ofpythia-70m 2.8B as well as Mistral-7B-v0.1, over all the closed-book sentence completiondatasets. We observe a clear trend of recall and F1-score increasing log-linearly with the modelsize. The precision of IDK-tuned models increases only slightly as the model size increases. For thetwo smallest models we investigate (pythia-70m and pythia-160m), our method is arguably noteffective, as the IDK-tuned models recall collapses (we further analyze this in .3).",
  "Ablations": "We perform an ablation study of our method to further investigate the effectiveness of each of itscomponents. For our study, we calculate the IDK recall and IDK error rate on the closed-book factualsentence completion datasets. We study the effect of , and the LFP-reg term. For this, we performIDK-tuning using Mistral-7B-v0.1 with the same hyperparameters as our main runs with differentcombinations of the studied components3. We plot the IDK recall for different values of in .In , we plot the IDK recall vs. IDK error rate tradeoff. IDK recall and IDK error rate are definedin .2. We study different aspects of these results below: 1. Analysis of the adaptive nature of the Uncertainty Factor .The Uncertainty Factor definedin Equation 2 is adaptive, meaning the amount of probability mass shifted to [IDK] depends on thepredicted probability distribution. Another possible choice is to use a fixed . We analyzethis in and . We can see that using the adaptive formulation results in a lowerIDK error rate without a major decrease in IDK recall. 2. Effect of the LFP-reg regularization.We also study the effect of the LFP-reg term (see Sec-tion 2.2). Again, we see that using LFP-reg results in a reduced IDK error rate without decreasing IDKrecall significantly.",
  "Due to computational constraints, we run this for a reduced set of for the cases with adaptive .4For a fixed , we set =": "3. Effect of the upper bound hyperparameter .We also study the effect of , which is the upperbound of the Uncertainty Factor (see Equation 2). Our ablation study demonstrates that increasing results in an increase in correct predictions of [IDK] (higher IDK recall), at the cost of a smallincrease of erroneous [IDK] predictions (IDK error rate). The IDK error rate increases less when usingboth our proposed adaptive and LFP-reg. Effect of knowledge contained in The Pile.Since we conduct further pretraining on The Pile,improved performance of our method could be partly explained by additional knowledge that themodel learns during IDK-tuning. However, we show that this is not the case. In the case of thepythia-70m 2.8B models, our data used for IDK-tuning exactly matches their pretraining data. ForMistral-7B-v0.1, this is not known although The Pile was likely also included. We note that thelanguage modeling performance on The Pile of our models during IDK-tuning actually very slightlydecreases rather than improving, suggesting the absence of any newly learned knowledge. However,to completely rule out any such effects, we trained Mistral-7B-v0.1 on the exact sample of ThePile used for IDK-tuning but with the regular cross-entropy objective. We report the performance ofthis model in . Indeed, Mistral-7B-v0.1 with further training on The Pile performs similarlyto the base Mistral-7B-v0.1 on average.",
  "Analysis of Optimization Stability": "Collapse to [IDK].Highly optimizing every component of the standard language modeling taskwith Transformers has made it easy to forget that optimization processes of deep neural networkscan be brittle and divergent. Naively replacing the regular cross-entropy objective with our IDK-lossLIDK leads to a collapse of training where the model simply always learns to put most probabilitymass on the [IDK]. We already account for this by (i) introducing the inhibitor to allow us to setan upper bound on the maximum probability mass that is assigned to the [IDK] in the target vectorand (ii) introducing the additional LFP-reg regularization to provide an additional signal that punishesprobability mass being assigned to [IDK] when the models prediction is already correct. In practice, we see that the regular cross-entropy loss shows a small uptick at the very beginning ofIDK-tuning. In almost all runs, this recovers quickly back to baseline levels, where it remains. Wefind that with = 0.5 and the LFP-reg regularization, most training runs are stable without furthermodel-specific tuning. Collapse for small models pythia-70m and pythia-160m.However, for pythia-160m andpythia-70m, which are the only runs in our experiments that diverge even with our added reg-ularization losses, the regular cross-entropy keeps on rising with a large spike. Concretely, thepredicted distributions not only show an increased cross-entropy with the targets but also a sharplyincreasing entropy: we observe that the predicted distributions collapse towards a uniform distribu-tion. At the worst point, 0% of the predictions of pythia-160m are correct. However, both modelssomewhat recover towards the end of training but stay well below baseline levels in terms of languagemodeling performance. We note that this is a different collapse pattern than the collapse towardsalmost always predicting [IDK] observed without our regularization terms. We further analyzed this and observe that for both pythia-160m and pythia-70m, the initial prob-ability mass assigned to the [IDK] token is so small that it gets rounded to zero even when usingfloat32 precision. This causes the LIDK loss to be very large, resulting in large gradient norms.Already for pythia-410m, the initial probability mass on [IDK] is substantial enough to preventthis (albeit still a very small value smaller than 5 109). Both pythia-160m and pythia-70m alsoshow a larger initial entropy in their predicted distributions (i.e., flatter predicted distributions). Weconjecture that an adapted initialization of the [IDK] token and/or a small bias towards [IDK] at thebeginning of training could prevent this divergence. As we only encounter this issue for the smallpythia-160m and pythia-70m models, we leave further investigation of this for future work.",
  "Text Generation": "To assess whether our IDK-tuning might harm other different downstream language skills, whichare not necessarily only factual, we evaluate the IDK-tuned Mistral-7B-v0.1 on the task of textsummarization, and compare its results to those of the original model. For this, due to the highlikelihood of the [IDK] token being generated during a longer text generation process, we usegreedy decoding and ignore the [IDK] token. For this experiment, we use four different common",
  ": Error type distribution on 200 failures of our IDK-tuned models": "summarization benchmarks: Legal Plain English [Manor and Li, 2019], TLDR [Vlske et al., 2017],and SPEC5G [Karim et al., 2023]. We measure performance using RougleL [Lin, 2004], as it iswidely used in related work, and report the results in . The IDK-tuned Mistral-7B-v0.1performs only slightly worse than the original base model. This is an encouraging result, as it meansthat IDK-tuning does not necessarily harm other language skills of pretrained language models.",
  "Error Analysis": "To gauge the effect of IDK-tuning on model responses to factual prompts and questions, we conduct anin-depth manual analysis on a random sample of 200 (40 from each dataset) of the models incorrectgenerations (generations that do not contain the correct answer). We conduct this analysis for threemodels across model sizes: pythia-70m, pythia-2.8B, and Mistral-7B-v0.1. We then categorizeeach of these incorrect generations to one of the following categories:",
  ". Abstain: The IDK-tuned model abstains from answering by generating text such as un-known\" or mystery\"": "The results are shown in . Our analysis suggest that first, the bigger the model, the fewerchanges our training approach causes in the models generations, and second, the bigger the model,the greater its ability to abstain from answering via words (which generally can be interpreted asequal to generating an [IDK] token, although harder to evaluate automatically).",
  "Related Work": "Model Calibration.Our goal is closely related to the key challenge of model calibration [Guo et al.,2017b]: to provide a measure of the probability that a prediction is incorrect alongside the actualprediction. The problem of factual error detection can be viewed as a variation of calibration, whereinstead of a continuous probability, we provide a binary prediction for whether the model is corrector not. This is also related to the setting of selective prediction, where models can abstain fromanswering a query [Varshney et al., 2022, Kamath et al., 2020]. Common approaches to calibrationare to perform various transformations on a models output logits [Desai and Durrett, 2020, Jianget al., 2021], and measuring uncertainty [e.g., see Kuhn et al., 2023]. More recent works have studiedthe use of LMs for providing calibration, by training them on statements known to be factually correct or incorrect. This supervised approach has been explored via fine-tuning [Kadavath et al.,2022, Lin et al., 2022b], in-context learning [Cohen et al., 2023a, Alivanistos et al., 2022], zero-shotinstruction-oriented [Cohen et al., 2023b] and consistency sampling [Yoran et al., 2023] techniques.A more recent work [Azaria and Mitchell, 2023] uses the internal state of the model for classifyingwhether it is certain or not. Our work builds upon this, aiming to teach the model to assess andexpress its own uncertainty via the new [IDK] token we introduced. Attribution.Another related line of work focuses on checking whether LM-generated texts arefaithful to a given source text [Bohnet et al., 2022, Honovich et al., 2022]. This problem has beenaddressed via several approaches, including question generation [Wang et al., 2020, Honovich et al.,2021, Scialom et al., 2021], NLI [Thorne et al., 2018, Welleck et al., 2019, Maynez et al., 2020,Dziri et al., 2022, Gao et al., 2022, Kamoi et al., 2023], data augmentation [Atanasova et al., 2022,Wright et al., 2022, Gekhman et al., 2023], and planning schemes that allow the model to self-edit itsown generation [Schick et al., 2022]. Unlike these works, we are not assuming any reference textor external knowledge bases. Instead, we aim to teach the model to decide on its own whether it islikely to be able to factually complete a sentence correctly.",
  "Conclusion": "We propose a novel method for improving LMs factuality by adding a special [IDK] token to anLMs vocabulary. Alongside the new [IDK] token, we introduce a novel pretraining objective calledIDK-tuning to model uncertainty in the models prediction as the probability mass assigned to the[IDK]. Crucially, IDK-tuning requires no labeled data and is instead a drop-in replacement of theconventional cross-entropy loss used for self-supervised language modeling on web-crawled texts.This allows us to explore uncertainty-aware training at a large scale. In our experiments, we conductcontinued pretraining of a diverse range of pretrained models using the IDK objective. Evaluation on factual sentence completion and multiple-choice benchmarks shows that IDK-tunedmodels can complete these tasks with much higher precision by refusing to answer (assigning highprobability mass to the [IDK] token) in cases when the base model would have given a wrong answer.This comes at only small decreases in recall. We investigate the scaling behavior of our method withrespect to model size using the Pythia model suite [Biderman et al., 2023], perform several ablationstudies for individual components of our IDK objective, and verify that the general language modelingability of IDK-tuned models does not degrade. Our work can be extended in several ways. For example, since we do not rely on any labels ofour training data used for IDK-tuning, we potentially apply our objective for next-token predictionswhere it might be ill-posed. Instead, we can perform lightweight filtering of relevant next-tokenpredictions, such as named entities, focusing our objective more on factual next-token predictions.Also, IDK-tuning can be applied during pretraining from scratch, where our IDK objective could haveinteresting interactions with the acquisition of new knowledge during this stage.",
  "Limitations": "We note a few limitations of our proposed method. First, it requires a full pretraining of LMs onrelatively large corpus. This of course is both highly computationally expensive and time-consuming.It is likely often the case that this kind of training cannot be conducted on typical academic labresources, on a large enough model, in a reasonable amount of time. Second, as discussed in .4, our method may slightly harm certain language skills, such aslong text generation. Other downstream skills may be affected more significantly. We further discusspotential risk and biases in Appendix A.",
  "Acknowledgements": "Roi Cohen and Gerard de Melo received funding from The Goldman Sachs Group, Inc., NewYork, NY, USA. Konstantin Dobler thanks the German Federal Ministry for Education and Research(BMBF) through the project KI-Servicezentrum Berlin Brandenburg (01IS22092) and the EuropeanLaboratory for Learning and Intelligent Systems (ELLIS) PhD program for support. We furtherexpress our gratitude to the NeurIPS 2024 reviewers for their helpful comments. Lukas Aichberger, Kajetan Schweighofer, Mykyta Ielanskyi, and Sepp Hochreiter. Semanticallydiverse language generation for uncertainty estimation in language models.arXiv preprintarXiv:2406.04306, 2024. Dimitrios Alivanistos, Selene Bez Santamara, Michael Cochez, Jan-Christoph Kalo, Emile vanKrieken, and Thiviyan Thanapalasingam. Prompting as probing: Using language models forknowledge base construction. arXiv preprint arXiv:2208.11057, 2022. Pepa Atanasova, Jakob Grue Simonsen, Christina Lioma, and Isabelle Augenstein. Fact checking withinsufficient evidence. Transactions of the Association for Computational Linguistics, 10:746763,2022. doi: 10.1162/tacl_a_00486. URL Amos Azaria and Tom Mitchell.The internal state of an LLM knows when its lying.InHouda Bouamor, Juan Pino, and Kalika Bali, editors, Findings of the Association for Com-putational Linguistics: EMNLP 2023, pages 967976, Singapore, December 2023. Associa-tion for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.68. URL Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle OBrien, Eric Hallahan,Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron,Lintang Sutawika, and Oskar Van Der Wal. Pythia: a suite for analyzing large language modelsacross training and scaling. In Proceedings of the 40th International Conference on MachineLearning, ICML23. JMLR.org, 2023. Bernd Bohnet, Vinh Q Tran, Pat Verga, Roee Aharoni, Daniel Andor, Livio Baldini Soares, JacobEisenstein, Kuzman Ganchev, Jonathan Herzig, Kai Hui, et al. Attributed question answering:Evaluation and modeling for attributed large language models. arXiv preprint arXiv:2212.08037,2022. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models arefew-shot learners. Advances in neural information processing systems, 33:18771901, 2020. Miles Brundage, Shahar Avin, Jasmine Wang, Haydn Belfield, Gretchen Krueger, Gillian K. Hadfield,Heidy Khlaaf, Jingying Yang, Helen Toner, Ruth Fong, Tegan Maharaj, Pang Wei Koh, SaraHooker, Jade Leung, Andrew Trask, Emma Bluemke, Jonathan Lebensbold, Cullen OKeefe,Mark Koren, Tho Ryffel, JB Rubinovitz, Tamay Besiroglu, Federica Carugati, Jack Clark, PeterEckersley, Sarah de Haas, Maritza L. Johnson, Ben Laurie, Alex Ingerman, Igor Krawczuk,Amanda Askell, Rosario Cammarota, Andrew J. Lohn, David Krueger, Charlotte Stix, PeterHenderson, Logan Graham, Carina E. A. Prunkl, Bianca Martin, Elizabeth Seger, Noa Zilberman,Sean O hEigeartaigh, Frens Kroeger, Girish Sastry, Rebecca Kagan, Adrian Weller, BrianTse, Elizabeth Barnes, Allan Dafoe, Paul Scharre, Ariel Herbert-Voss, Martijn Rasser, ShagunSodhani, Carrick Flynn, Thomas Krendl Gilbert, Lisa Dyer, Saif Khan, Yoshua Bengio, andMarkus Anderljung. Toward trustworthy AI development: Mechanisms for supporting verifiableclaims. ArXiv, abs/2004.07213, 2020. URL Yangyi Chen, Lifan Yuan, Ganqu Cui, Zhiyuan Liu, and Heng Ji. A close look into the cali-bration of pre-trained language models. In Anna Rogers, Jordan Boyd-Graber, and NaoakiOkazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computa-tional Linguistics (Volume 1: Long Papers), pages 13431367, Toronto, Canada, July 2023.Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.75. URL Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, andOyvind Tafjord. Think you have solved question answering? try ARC, the AI2 reasoning challenge.arXiv preprint arXiv:1803.05457, 2018. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and JohnSchulman. Training verifiers to solve math word problems. ArXiv, abs/2110.14168, 2021. URL Roi Cohen, Mor Geva, Jonathan Berant, and Amir Globerson. Crawling the internal knowledge-base of language models. In Andreas Vlachos and Isabelle Augenstein, editors, Findings of theAssociation for Computational Linguistics: EACL 2023, pages 18561869, Dubrovnik, Croatia,May 2023a. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-eacl.139.URL Roi Cohen, May Hamri, Mor Geva, and Amir Globerson. LM vs LM: Detecting factual errors viacross examination. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the2023 Conference on Empirical Methods in Natural Language Processing, pages 1262112640,Singapore, December 2023b. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.778. URL Roi Cohen, Eden Biran, Ori Yoran, Amir Globerson, and Mor Geva. Evaluating the ripple effectsof knowledge editing in language models. Transactions of the Association for ComputationalLinguistics, 12:283298, 2024. Shrey Desai and Greg Durrett. Calibration of pre-trained transformers. In Bonnie Webber, TrevorCohn, Yulan He, and Yang Liu, editors, Proceedings of the 2020 Conference on EmpiricalMethods in Natural Language Processing (EMNLP), pages 295302, Online, November 2020.Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.21. URL Ashwin Devaraj, William Sheffield, Byron Wallace, and Junyi Jessy Li. Evaluating factuality in textsimplification. In Proceedings of the 60th Annual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers), pages 73317345, Dublin, Ireland, May 2022. Association forComputational Linguistics. doi: 10.18653/v1/2022.acl-long.506. URL Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deepbidirectional transformers for language understanding. In Proceedings of the 2019 Conference ofthe North American Chapter of the Association for Computational Linguistics: Human LanguageTechnologies, Volume 1 (Long and Short Papers), pages 41714186, Minneapolis, Minnesota,June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL Nouha Dziri, Hannah Rashkin, Tal Linzen, and David Reitter. Evaluating attribution in dialoguesystems: The BEGIN benchmark. Transactions of the Association for Computational Linguistics,10:10661083, 2022. doi: 10.1162/tacl_a_00506. URL Yanai Elazar, Nora Kassner, Shauli Ravfogel, Abhilasha Ravichander, Eduard Hovy, HinrichSchtze, and Yoav Goldberg. Measuring and improving consistency in pretrained languagemodels. Transactions of the Association for Computational Linguistics, 9:10121031, 2021. doi:10.1162/tacl_a_00410. URL Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang,Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The Pile: An 800GBdataset of diverse text for language modeling, 2020. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster,Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff,Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika,Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shotlanguage model evaluation, 12 2023. URL Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun Tejasvi Chaganty, Yicheng Fan,Vincent Y Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, et al. Rarr: Researching and revising whatlanguage models say, using language models. arXiv preprint arXiv:2210.08726, 2022.",
  "Zorik Gekhman, Jonathan Herzig, Roee Aharoni, Chen Elkind, and Idan Szpektor. Trueteacher: Learn-ing factual consistency evaluation with large language models. arXiv preprint arXiv:2305.11171,2023": "Jiahui Geng, Fengyu Cai, Yuxia Wang, Heinz Koeppl, Preslav Nakov, and Iryna Gurevych. A surveyof confidence estimation and calibration in large language models. In Kevin Duh, Helena Gomez,and Steven Bethard, editors, Proceedings of the 2024 Conference of the North American Chapterof the Association for Computational Linguistics: Human Language Technologies (Volume 1:Long Papers), pages 65776595, Mexico City, Mexico, June 2024. Association for ComputationalLinguistics. doi: 10.18653/v1/2024.naacl-long.366. URL Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On calibration of modern neuralnetworks. In Proceedings of the 34th International Conference on Machine Learning - Volume 70,ICML17, page 13211330. JMLR.org, 2017a. Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On calibration of modern neuralnetworks. In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th InternationalConference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages13211330. PMLR, 0611 Aug 2017b. URL",
  "Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, andJacob Steinhardt.Measuring massive multitask language understanding.arXiv preprintarXiv:2009.03300, 2020": "Or Honovich, Leshem Choshen, Roee Aharoni, Ella Neeman, Idan Szpektor, and Omri Abend. q2:Evaluating factual consistency in knowledge-grounded dialogues via question generation andquestion answering. In Proceedings of the 2021 Conference on Empirical Methods in NaturalLanguage Processing, pages 78567870, Online and Punta Cana, Dominican Republic, November2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.619. URL Or Honovich, Roee Aharoni, Jonathan Herzig, Hagai Taitelbaum, Doron Kukliansy, Vered Cohen,Thomas Scialom, Idan Szpektor, Avinatan Hassidim, and Yossi Matias. True: Re-evaluating factualconsistency evaluation. arXiv preprint arXiv:2204.04991, 2022. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang,Andrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation.ACM Comput. Surv., 55(12), mar 2023. ISSN 0360-0300. doi: 10.1145/3571730. URL Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot,Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al.Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. Zhengbao Jiang, Jun Araki, Haibo Ding, and Graham Neubig. How can we know when languagemodels know? on the calibration of language models for question answering. Transactions of theAssociation for Computational Linguistics, 9:962977, 2021. doi: 10.1162/tacl_a_00407. URL",
  "Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. TriviaQA: A large scale distantlysupervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551, 2017": "Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, NicholasSchiefer, Zachary Dodds, Nova Dassarma, Eli Tran-Johnson, Scott Johnston, Sheer El-Showk,Andy Jones, Nelson Elhage, Tristan Hume, Anna Chen, Yuntao Bai, Sam Bowman, StanislavFort, Deep Ganguli, Danny Hernandez, Josh Jacobson, John Kernion, Shauna Kravec, LianeLovitt, Kamal Ndousse, Catherine Olsson, Sam Ringer, Dario Amodei, Tom B. Brown, JackClark, Nicholas Joseph, Benjamin Mann, Sam McCandlish, Christopher Olah, and Jared Kaplan.Language models (mostly) know what they know. arXiv preprint arXiv:2207.05221, 2022.",
  "Amita Kamath, Robin Jia, and Percy Liang. Selective question answering under domain shift. arXivpreprint arXiv:2006.09462, 2020": "Ryo Kamoi, Tanya Goyal, and Greg Durrett. Shortcomings of question answering based factualityframeworks for error localization. In Andreas Vlachos and Isabelle Augenstein, editors, Proceed-ings of the 17th Conference of the European Chapter of the Association for Computational Linguis-tics, pages 132146, Dubrovnik, Croatia, May 2023. Association for Computational Linguistics.doi: 10.18653/v1/2023.eacl-main.11. URL Imtiaz Karim, Kazi Samin Mubasshir, Mirza Masfiqur Rahman, and Elisa Bertino. SPEC5G: Adataset for 5G cellular network protocol analysis. In Jong C. Park, Yuki Arase, Baotian Hu,Wei Lu, Derry Wijaya, Ayu Purwarianti, and Adila Alfa Krisnadhi, editors, Findings of theAssociation for Computational Linguistics: IJCNLP-AACL 2023 (Findings), pages 2038, NusaDua, Bali, November 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-ijcnlp.3. URL",
  "Stephanie Lin, Jacob Hilton, and Owain Evans. Teaching models to express their uncertainty inwords. arXiv preprint arXiv:2205.14334, 2022b": "Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Hannaneh Hajishirzi, and Daniel Khashabi.When not to trust language models: Investigating effectiveness and limitations of parametric andnon-parametric memories. arXiv preprint arXiv:2212.10511, 2022. Laura Manduchi, Kushagra Pandey, Robert Bamler, Ryan Cotterell, Sina Dubener, Sophie Fellenz,Asja Fischer, Thomas Grtner, Matthias Kirchler, Marius Kloft, Yingzhen Li, Christoph Lippert,Gerard de Melo, Eric Nalisnick, Bjrn Ommer, Rajesh Ranganath, Maja Rudolph, Karen Ullrich,Guy Van den Broeck, Julia E Vogt, Yixin Wang, Florian Wenzel, Frank Wood, Stephan Mandt,and Vincent Fortuin. On the challenges and opportunities in Generative AI. arXiv preprintarXiv:2403.00025, 2024. URL Laura Manor and Junyi Jessy Li. Plain English summarization of contracts. In Proceedings ofthe Natural Legal Language Processing Workshop 2019, pages 111, Minneapolis, Minnesota,June 2019. Association for Computational Linguistics. doi: 10.18653/v1/W19-2201. URL Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. On faithfulness and factualityin abstractive summarization. In Proceedings of the 58th Annual Meeting of the Association forComputational Linguistics, pages 19061919, Online, July 2020. Association for ComputationalLinguistics. doi: 10.18653/v1/2020.acl-main.173. URL",
  "Niels Mndler, Jingxuan He, Slobodan Jenko, and Martin Vechev. Self-contradictory hallucinationsof large language models: Evaluation, detection and mitigation. arXiv preprint arXiv:2305.15852,2023": "Jeff Z. Pan, Simon Razniewski, Jan-Christoph Kalo, Sneha Singhania, Jiaoyan Chen, Stefan Dietze,Hajira Jabeen, Janna Omeliyanenko, Wen Zhang, Matteo Lissandrini, Russa Biswas, Gerardde Melo, Angela Bonifati, Edlira Vakaj, Mauro Dragoni, and Damien Graux. Large LanguageModels and Knowledge Graphs: Opportunities and Challenges. Transactions on Graph Data andKnowledge, 1(1):2:12:38, 2023. doi: 10.4230/TGDK.1.1.2. URL",
  "Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into theparameters of a language model? arXiv preprint arXiv:2002.08910, 2020": "Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. WinoGrande: Anadversarial winograd schema challenge at scale. Commun. ACM, 64(9):99106, aug 2021. ISSN0001-0782. doi: 10.1145/3474381. URL Timo Schick, Jane Dwivedi-Yu, Zhengbao Jiang, Fabio Petroni, Patrick Lewis, Gautier Izacard,Qingfei You, Christoforos Nalmpantis, Edouard Grave, and Sebastian Riedel. Peer: A collaborativelanguage model. arXiv preprint arXiv:2208.11663, 2022. Thomas Scialom, Paul-Alexis Dray, Sylvain Lamprier, Benjamin Piwowarski, Jacopo Staiano,Alex Wang, and Patrick Gallinari.QuestEval: Summarization asks for fact-based evalua-tion.In Proceedings of the 2021 Conference on Empirical Methods in Natural LanguageProcessing, pages 65946604, Online and Punta Cana, Dominican Republic, November 2021.Association for Computational Linguistics.doi: 10.18653/v1/2021.emnlp-main.529.URL Derek Tam, Anisha Mascarenhas, Shiyue Zhang, Sarah Kwan, Mohit Bansal, and Colin Raf-fel.Evaluating the factual consistency of large language models through news summariza-tion. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Findings of the As-sociation for Computational Linguistics: ACL 2023, pages 52205255, Toronto, Canada, July2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.322. URL Yi Tay, Vinh Tran, Mostafa Dehghani, Jianmo Ni, Dara Bahri, Harsh Mehta, Zhen Qin, Kai Hui, ZheZhao, Jai Gupta, et al. Transformer memory as a differentiable search index. Advances in NeuralInformation Processing Systems, 35:2183121843, 2022. James Thorne, Andreas Vlachos, Oana Cocarascu, Christos Christodoulopoulos, and Arpit Mit-tal. The fact extraction and VERification (FEVER) shared task. In Proceedings of the FirstWorkshop on Fact Extraction and VERification (FEVER), pages 19, Brussels, Belgium, Novem-ber 2018. Association for Computational Linguistics.doi: 10.18653/v1/W18-5501.URL Neeraj Varshney, Swaroop Mishra, and Chitta Baral. Investigating selective prediction approachesacross several tasks in IID, OOD, and adversarial settings.In Findings of the Associationfor Computational Linguistics: ACL 2022, pages 19952002, Dublin, Ireland, May 2022.Association for Computational Linguistics.doi: 10.18653/v1/2022.findings-acl.158.URL Michael Vlske, Martin Potthast, Shahbaz Syed, and Benno Stein. TL;DR: Mining Reddit to learnautomatic summarization. In Proceedings of the Workshop on New Frontiers in Summarization,pages 5963, Copenhagen, Denmark, September 2017. Association for Computational Linguistics.doi: 10.18653/v1/W17-4508. URL Alex Wang, Kyunghyun Cho, and Mike Lewis. Asking and answering questions to evaluate thefactual consistency of summaries. In Proceedings of the 58th Annual Meeting of the Association forComputational Linguistics, pages 50085020, Online, July 2020. Association for ComputationalLinguistics. doi: 10.18653/v1/2020.acl-main.450. URL Sean Welleck, Jason Weston, Arthur Szlam, and Kyunghyun Cho. Dialogue natural language infer-ence. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,pages 37313741, Florence, Italy, July 2019. Association for Computational Linguistics. doi:10.18653/v1/P19-1363. URL Dustin Wright, David Wadden, Kyle Lo, Bailey Kuehl, Arman Cohan, Isabelle Augenstein, andLucy Lu Wang. Generating scientific claims for zero-shot scientific fact checking. In Proceedingsof the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: LongPapers), pages 24482460, Dublin, Ireland, May 2022. Association for Computational Linguistics.doi: 10.18653/v1/2022.acl-long.175. URL Ori Yoran, Tomer Wolfson, Ben Bogin, Uri Katz, Daniel Deutch, and Jonathan Berant.An-swering questions by meta-reasoning over multiple chains of thought.In Houda Bouamor,Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Meth-ods in Natural Language Processing, pages 59425966, Singapore, December 2023. Associ-ation for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.364. URL Hiyori Yoshikawa and Naoaki Okazaki. Selective-LAMA: Selective prediction for confidence-awareevaluation of language models. In Andreas Vlachos and Isabelle Augenstein, editors, Findings ofthe Association for Computational Linguistics: EACL 2023, pages 20172028, Dubrovnik, Croatia,May 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-eacl.150.URL Xiang Yue, Boshi Wang, Ziru Chen, Kai Zhang, Yu Su, and Huan Sun. Automatic evaluation ofattribution by large language models. In Houda Bouamor, Juan Pino, and Kalika Bali, editors,Findings of the Association for Computational Linguistics: EMNLP 2023, pages 46154635,Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.307. URL Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can amachine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Associationfor Computational Linguistics, pages 47914800, Florence, Italy, July 2019. Association forComputational Linguistics. doi: 10.18653/v1/P19-1472. URL",
  "AImpact": "As discussed in , one of the main disadvantages of current LMs is their tendency to factuallymislead the user by generating factual incorrect statements. Hence, the main impact of our work is toreduce such factual mistakes via our proposed method. Still, it is evident that this sort of approachcan by no means completely eliminate hallucinations. It is important to stress that we propose asingle method, not a system design for safe deployment of LLMs. In practice, we anticipate ourmethod to be coupled with other checks and balances, forming a safe system. Additionally, in this work, we use The Pile as a dataset to train models. The Pile is a web-crawledcorpus, which likely harbors text reflecting various forms of biases. One impact of applying IDK-tuning is that the model may learn to answer in a biased way if this bias appears in its training data,while avoiding answers that rarely appear in its training data. This shows the need for more researchon compiling high-quality training corpora.",
  "CQuestions Rephrasing": "As mentioned in .2, for TriviaQA and PopQA, where the input is formed as a question, wereduce each of these input examples into a sentence completion task input, using GPT4. If we denotea random input question from one of these datasets by x, then our prompt to GPT4 is the following:",
  "The answer NA means that the paper has no limitation while the answer No means thatthe paper has limitations, but those are not discussed in the paper": "The authors are encouraged to create a separate \"Limitations\" section in their paper. The paper should point out any strong assumptions and how robust the results are toviolations of these assumptions (e.g., independence assumptions, noiseless settings,model well-specification, asymptotic approximations only holding locally). The authorsshould reflect on how these assumptions might be violated in practice and what theimplications would be. The authors should reflect on the scope of the claims made, e.g., if the approach wasonly tested on a few datasets or with a few runs. In general, empirical results oftendepend on implicit assumptions, which should be articulated. The authors should reflect on the factors that influence the performance of the approach.For example, a facial recognition algorithm may perform poorly when image resolutionis low or images are taken in low lighting. Or a speech-to-text system might not beused reliably to provide closed captions for online lectures because it fails to handletechnical jargon.",
  "If applicable, the authors should discuss possible limitations of their approach toaddress problems of privacy and fairness": "While the authors might fear that complete honesty about limitations might be used byreviewers as grounds for rejection, a worse outcome might be that reviewers discoverlimitations that arent acknowledged in the paper. The authors should use their bestjudgment and recognize that individual actions in favor of transparency play an impor-tant role in developing norms that preserve the integrity of the community. Reviewerswill be specifically instructed to not penalize honesty concerning limitations.",
  ". Experimental Result Reproducibility": "Question: Does the paper fully disclose all the information needed to reproduce the main ex-perimental results of the paper to the extent that it affects the main claims and/or conclusionsof the paper (regardless of whether the code and data are provided or not)?Answer: [Yes]Justification: We provide the formulation of our objective in and hyperparametersas well as further details to reproduce our trainings in .1.Guidelines: The answer NA means that the paper does not include experiments. If the paper includes experiments, a No answer to this question will not be perceivedwell by the reviewers: Making the paper reproducible is important, regardless ofwhether the code and data are provided or not.",
  "If the contribution is a dataset and/or model, the authors should describe the steps takento make their results reproducible or verifiable": "Depending on the contribution, reproducibility can be accomplished in various ways.For example, if the contribution is a novel architecture, describing the architecture fullymight suffice, or if the contribution is a specific model and empirical evaluation, it maybe necessary to either make it possible for others to replicate the model with the samedataset, or provide access to the model. In general. releasing code and data is oftenone good way to accomplish this, but reproducibility can also be provided via detailedinstructions for how to replicate the results, access to a hosted model (e.g., in the caseof a large language model), releasing of a model checkpoint, or other means that areappropriate to the research performed. While NeurIPS does not require releasing code, the conference does require all submis-sions to provide some reasonable avenue for reproducibility, which may depend on thenature of the contribution. For example(a) If the contribution is primarily a new algorithm, the paper should make it clear howto reproduce that algorithm.",
  "(b) If the contribution is primarily a new model architecture, the paper should describethe architecture clearly and fully": "(c) If the contribution is a new model (e.g., a large language model), then there shouldeither be a way to access this model for reproducing the results or a way to reproducethe model (e.g., with an open-source dataset or instructions for how to constructthe dataset). (d) We recognize that reproducibility may be tricky in some cases, in which caseauthors are welcome to describe the particular way they provide for reproducibility.In the case of closed-source models, it may be that access to the model is limited insome way (e.g., to registered users), but it should be possible for other researchersto have some path to reproducing or verifying the results.",
  "Guidelines:": "The answer NA means that the paper does not use existing assets. The authors should cite the original paper that produced the code package or dataset. The authors should state which version of the asset is used and, if possible, include aURL. The name of the license (e.g., CC-BY 4.0) should be included for each asset. For scraped data from a particular source (e.g., website), the copyright and terms ofservice of that source should be provided. If assets are released, the license, copyright information, and terms of use in thepackage should be provided. For popular datasets, paperswithcode.com/datasetshas curated licenses for some datasets. Their licensing guide can help determine thelicense of a dataset.",
  "The answer NA means that the paper does not involve crowdsourcing nor research withhuman subjects": "Depending on the country in which research is conducted, IRB approval (or equivalent)may be required for any human subjects research. If you obtained IRB approval, youshould clearly state this in the paper. We recognize that the procedures for this may vary significantly between institutionsand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and theguidelines for their institution."
}