{
  "Abstract": "We introduce MVSplat360, a feed-forward approach for 360 novel view synthesis(NVS) of diverse real-world scenes, using only sparse observations. This settingis inherently ill-posed due to minimal overlap among input views and insufficientvisual information provided, making it challenging for conventional methodsto achieve high-quality results. Our MVSplat360 addresses this by effectivelycombining geometry-aware 3D reconstruction with temporally consistent videogeneration. Specifically, it refactors a feed-forward 3D Gaussian Splatting (3DGS)model to render features directly into the latent space of a pre-trained Stable VideoDiffusion (SVD) model, where these features then act as pose and visual cuesto guide the denoising process and produce photorealistic 3D-consistent views.Our model is end-to-end trainable and supports rendering arbitrary views withas few as 5 sparse input views. To evaluate MVSplat360s performance, weintroduce a new benchmark using the challenging DL3DV-10K dataset, whereMVSplat360 achieves superior visual quality compared to state-of-the-art methodson wide-sweeping or even 360 NVS tasks. Experiments on the existing benchmarkRealEstate10K also confirm the effectiveness of our model. The video results areavailable on our project page: donydchen.github.io/mvsplat360.",
  "Introduction": "The rapid advancement in 3D reconstruction and NVS has been facilitated by the emergence ofdifferentiable rendering . These methods, while fundamental and impressive, areprimarily tailored for per-scene optimization, requiring hundreds or even thousands of images tocomprehensively capture every aspect of the scene. Consequently, the optimization process for eachscene can be time-consuming, and collecting thousands of images is impractical for casual users. In contrast, we consider the problem of novel view synthesis in diverse real-world scenes using a lim-ited number of source views through a feed-forward network. In particular, this work investigates thefeasibility of rendering wide-sweeping or even 360 novel views using extremely sparse observations,like fewer than 5 images. This task is inherently challenging due to the complexity of scenes, wherethe limited views do not contain sufficient information to recover the whole 3D scene. Consequently,there is a necessity to ensemble visible information under minimal overlap accurately and generatemissing details reasonably. This represents a new problem setting in sparse-view feed-forward NVS. Existing feed-forwardmethods typically focus on two distinct scenarios: 360 NVS with extremely sparse observations, butonly at object-level , or generating reasonable resultsfor scene-level synthesis, but only for nearby viewpoints .In contrast, we argue that the time is ripe to unify these previously distinct research directions.",
  "MVSplat": ": Examples of our MVSplat360. Given sparse and wide-baseline observations of diversein-the-wild scenes, MVSplat360 can directly render 360 novel views (inward or outward facing) orother natural camera trajectory views in a feed-forward manner, without any per-scene optimization. Our goal should be to develop systems capable of synthesizing wide-sweeping or even 360 novelviews of large, real-world scenes with complex geometry and significant occlusion. Specifically, thiswork explores synthesising 360 novel views from fewer than 5 input images. We show that in thischallenging setting, existing feed-forward scene synthesis approaches struggleto succeed. This failure arises from two main factors: i) the limited overlap among input views causesmany contents to appear in only a few views or even a single one, posing significant challenges for3D reconstruction; ii) the extremely sparse observations lack sufficient information to capture thecomprehensive details of the whole scene, resulting in regions unobserved from novel viewpoints. In this paper, we propose a simple yet effective framework to address these limitations and introducethe first benchmark for feed-forward 360 scene synthesis from sparse input views. Our key ideais to leverage prior knowledge from a large-scale pre-trained latent diffusion model (LDM) toimagine plausible unobserved and disoccluded regions in novel views, which are inherently highlyambiguous. Unlike existing 360 object-level NVS approaches , large-scalereal-world scenes comprise multiple 3D assets with complex arrangements, heavy occlusions, andvarying rendering trajectories, which makes it particularly challenging to condition solely on cameraposes, as also verified by concurrent work ViewCrafter . To develop a performant framework for scene-level synthesis, we opt to treat the LDM as a refinementmodule, while relying on a 3D reconstruction model to process the complex geometric informa-tion. Broadly, we build upon the feed-forward 3DGS model, MVSplat , to obtain coarsenovel views by matching and fusing multi-view information with the cross-view transformer andcost volume. Although these results are imperfect, exhibiting visual artifacts and missing regions(see ), they represent the reasonable geometric structure of the scene, as they are renderedfrom 3D representation. Furthermore, we choose Stable Video Diffusion (SVD) over otherimage-based LDM as the refinement module, since its strong temporal consistency capabilities alignbetter with the view-consistent requirement of the NVS task, as also observed by concurrent work3DGS-Enhancer . Conditioning SVD with the 3DGS rendered outputs, our MVSplat360 producesvisually appealing novel views that are multi-view consistent and geometrically accurate (see ).",
  "Importantly, the original MVSplat outputs only RGB images, which is not the optimal condition forthe generator, and is difficult to optimize jointly with the SVD denoising module. To tackle this, we": "propose a simple Gaussian feature rendering with multi-channels, supervised with an introducedlatent space alignment loss. Despite a seemingly minor change, the additional feature conditionfor SVD leads to a significant impact: It bypasses the SVDs frozen image encoder, allowing thegradients from SVD to backpropagate to enhance the geometry backbone and lead to improved visualquality, especially on the new challenging DL3DV-10K dataset. While related work Reconfusion ,CAT3D and latentSplat also combine the 3D representation with 2D generators, the formertwo focus more on per-scene optimisation, while the latter only shows 360 NVS at the object level. We conduct a series of experiments, mainly on two datasets. First, we establish a new benchmark onDL3DV-10K dataset , creating a new training and testing split for feed-forward wide-sweepingand 360 NVS. In this challenging setting, our MVSplat360 achieves photorealistic 360 NVSfrom sparse observations and demonstrates significantly better visual quality, where the previousscene-level feed-forward methods fail to achieve plausible results. Second, we deployMVSplat360 on the existing RealEstate10K benchmark. Following latentSplat , we estimateboth interpolation and extrapolation NVS, and report state-of-the-art performance. Our main contributions can be summarized as follows. 1) We introduce a crucial and pressingproblem for novel view synthesis, i.e., how to do wide-sweeping or even 360 NVS from sparseand widely-displaced observations of diverse in-the-wild scenes (not objects) in a feed-forwardmanner (no any per-scene optimization). 2) We propose an effective solution that nicely integrates thelatest feed-forward 3DGS and the pre-trained Stable Video Diffusion (SVD) model with meticulousintegration designs, where the former is for reconstructing coarse geometry and the latter is forrefining the noisy and incomplete coarse reconstruction. 3) Extensive results on the challengingDL3DV-10K and RealEstate10K datasets demonstrate the superior performance of our MVSplat360.",
  "Related Work": "Sparse view per-scene reconstruction and synthesis. Differentiable rendering methods, suchas NeRF and 3DGS , are mainly designed for very dense views (e.g., 100) as inputs forper-scene optimization, which is impractical to collect for casual users in real applications. Tobypass the requirement for dense views, various regularization terms have been proposed in per-sceneoptimization . Recently, ZeroNVS , Reconfusion and concurrent submissions,including CAT3D , ReconX , ViewCrafter , LM-Gaussian , 3DGS-Enhancer ,have leveraged large-scale diffusion models for generating pseudo dense views of a 3D scene, whichare then input into a per-scene reconstruction pipeline. However, these methods are inherently slowfor reconstructing unseen scenes due to the necessity of per-scene optimisation. Feed-forward scene reconstruction and synthesis. To mitigate these limitations, early approacheslike Light Field Networks use ray querying to predict novel views. Subsequent methods employ epipolar attention for multi-view geometry estimation. Later, pixelNeRF devisedpixel-aligned features for NeRF reconstruction , leading to a range of subsequent methods thatincorporate feature matching fusion , Transformers and 3D volume representation . Recently, 3D Gaussian Splatting has been implemented into feed-forward networks, suchas pixelSplat , MVSplat , Splatter Image , Flash3D and latentSplat . Whilethese methods were successful in novel view synthesis from sparse views, they fail to achieve thisin a wide-sweeping or 360 setting. Concurrent yet unpublished submissions, DepthSplat andLong-LRM , also show promising results in the 360 setting, but their frameworks have limitedgeneration capabilities, necessitating the use of denser inputs, e.g., 12 or 32 views. Camera trajectory controllable synthesis. Generative models have achieved remarkable results forimage/video synthesis , but they lack precise control over the viewpointof generated images. To address this, several approaches fine-tune large-scale pre-trained diffusionmodels with explicit image and pose conditions . However, thesemethods mainly show 360 NVS results on single objects, leaving the complex scene synthesisproblem unsolved. Natural scenes comprise multiple objects with intricate occlusion relationships,presenting greater challenges that are not easily addressed by these single-object NVS models.Besides, camera trajectories can be highly irregular and varied when roving around such complexscenes. Although related works have explored training or fine-tuning diffusion modelswith camera control for scene synthesis, they often struggle with precise camera pose control ,and still rely on per-scene optimization for 3D reconstruction .",
  "CLIP": ": Overview of our MVSplat360. (a) Given sparse posed images as input, we first matchand fuse the multi-view information using a multi-view Transformer and cost volume-based encoder.(b) Next, a 3DGS representation is constructed to represent the coarse geometry of the entire scene.(c) Considering such coarse reconstruction is imperfect, we further adapt a pre-trained SVD, usingfeatures rendered from the 3DGS representation as conditions to achieve 360 novel view synthesis.",
  "Methodology": "Given N sparse views I = {Ii}Ni=1 and the corresponding camera poses P = {P i}Ni=1, withP i = (Ki, Ri, Ti) comprising intrinsic Ki, rotation Ri and translation Ti, our goal is to learn amodel that synthesizes wide-sweeping or even 360 novel view synthesis (NVS). We opt to go beyond per-scene optimisation , and to deal with a more general feed-forward network capable of achieving 360 NVS for unseen scenes, yet without the need of additionalper-scene training. This requires effectively matching information between sparse views in 3Dspace, as well as generating sufficient content based on only partial observations. To achieve that,our MVSplat360 framework, illustrated in , comprises two main components: a multi-viewgeometry reconstruction module (.1) and a multi-frame consistent appearance refinementnetwork (.2). The former is responsible for matching and fusing multi-view informationfrom sparse observations to create a coarse geometry reconstruction, whereas the latter is designedto refine the appearance with a pre-trained latent video diffusion model. While similar two-stepapproaches have been explored in recent related works, e.g., , we are the first (to thebest of our knowledge) to explore it on wide-sweeping or even 360 NVS for large-scale scenes fromsparse views (as few as 5), in a feed-forward manner.",
  "Multi-View Coarse Geometry Reconstruction": "The first module is built upon a feed-forward 3DGS reconstruction model, i.e., MVSplat as in our implementation. Specifically, given sparse-view observations I = {Ii}Ni=1 and theircorresponding camera poses P = {P i}Ni=1, the model learns to predict 3D Gaussian parameters{(i, i, i, ci)}HW Ni=1, which can then be splatted to obtain a set of RGB images Itgt usingthe target camera poses Ptgt. To ensure better integration with the following diffusion module, wepredict an additional Gaussian feature fi, in parallel with other parameters, which can be rasterizedto the corresponding latent features Ftgt. Furthermore, we also improve the view selection strategyto improve the models robustness in handling widely displaced inputs. Coarse geometry reconstruction. Our backbone comprises multi-view feature extraction, costvolume construction, depth estimation, and 3D Gaussian parameter predictions. First, a cross-viewtransformer encoder is applied to fuse multi-view information and obtain cross-view aware featuresF = {F i}Ni=1. Then, N cost volumes C = {Ci}Ni=1 are constructed by matching feature correlationsbetween cross-views. Specifically, it uniformly divides the depth into L layers in the near andfar depth ranges, i.e. D = {Dm}Lm=1, and then warps the features from one view j to anotherview i via F jiDm = W(F j, P i, P j, Dm). The cost volume Ci = [CiD1, CiD2, . . . , CiDL] is then",
  "collected by L correlations, where each correlation is expressed as CiDm =F jiDm F i": "C, with C denotingchannel dimension. Finally, the per-view estimated depth d is obtained by applying the softmaxoperation on the cost volumes in the depth dimension. After that, the Gaussian mean is computed by = K1ud + , where K is the camera intrinsic, u = (ux, uy, 1) denotes each pixel, and R3",
  "where S is the order of the spherical harmonics . Once the model predicts a set of 3D Gaussianparameters {(i, i, i, ci)}HW Ni=1, the target view Itgt can be rendered through rasterization": "Gaussian feature rendering. Given sparse-view observations, MVSplat tends to render imageswith noticeable artifacts in wide-sweeping novel viewpoints (see ), resulting in suboptimalconditioning for the subsequent SVD. Since the rendered images must first be encoded into the latentspace using a frozen encoder (see .2), enhancing the backbone with gradients from SVDwould be computationally expensive. To address this issue, we propose directly rasterising featuresF into the latent space of SVD, by predicting an additional parameter fi for each 3D Gaussian. Thisoperation offers two advantages: (i) The latent feature includes multi-channel information, providinga more comprehensive representation of the scene; (ii) The entire framework is end-to-end connectedby conditioning SVD on the rendered latent features instead of the image-encoded ones. It enablesthe SVD loss to optimize the Gaussian features, further enhancing the reconstruction backbone. Observed and novel viewpoints selection. To enable 360 scene synthesis, it is crucial to choose thecorrect camera viewpoints, so that they can cover most contents in diverse and complex scenes .It is impractical to assume a circular orbital camera trajectory like those object-level 360 viewsynthesis , whereas it is suboptimal to randomly choose a video sequence like existingscene-level nearby viewpoint synthesis . To this end, we propose to choose views evenlydistributed within a set of targeted viewpoints as input. Specifically, for a given set of candidate views,we apply farthest point sampling over the camera locations to identify the input views and randomlychoose from the rest as target views. The number of candidate views gradually increases throughoutthe training, stably improving the models capability toward handling 360 scene synthesis. View interaction within the local group. Recalling that the 3D reconstruction backbone MVSplat isprimarily designed for nearby viewpoints, with key components like multi-view transformers andcost volume assuming sufficient overlap among input views. However, in the more challenging360 settings, the widely displaced input views lead to minimal overlap between specific view pairs,hindering the effectiveness of the backbone. To mitigate this limitation, we refactor our backbone touse cross-view attention and construct the cost volume only within a local group of input views basedon camera locations, reducing memory consumption and ensuring stable model convergence.",
  "Multi-Frame Appearance Refinement": "Video diffusion model. MVSplat360 utilizes an off-the-shelf multi-frame diffusion model, i.e.Stable Video Diffusion (SVD) , to refine the visual appearance of the aforementioned coarsereconstruction. SVD is pre-trained on large-scale video datasets and has strong prior knowledge oftemporal consistency. It adheres to the original formulation used by Stable Diffusion (SD) thatconducts the denoising processing in the latent space. In particular, given a target sequence of x1:Mwith M images, they are initially embedded into the latent space by a frozen encoder E, yieldingz1:M0= E(x1:M), and then perturbed by adding Gaussian noise N(0, I) in a Markov process:",
  "where x1:M is the target images, and y is the conditional inputs. After is trained, the model cangenerate a video by performing iterative denoising from pure Gaussians z1:MTconditioned on y": "Note that SVD is trained with the latest v-prediction formulation , instead of the original -prediction in SD. Hence, the final loss is calculated in latent space using the mean squared error(MSE) between the ground truth and its prediction ||z1:M0 z1:Mt||22 , where z1:Mtis obtained bytranslating the velocity v = (z1:Mt, t, y) to latent space, i.e., z1:Mt= tz1:Mt tv. Multi-view hybrid conditions. To ensure an accurate understanding of the scene, the model requiresthe integration of both low-level perception (e.g., depth and texture) and high-level understanding(e.g., semantics and geometry). Following , we adopt a hybrid conditioning mechanismto fine-tune the SVD model for wide-sweeping NVS with sparse observations.",
  "In one stream, a CLIP image embedding token of the original visible views I is used as a globaltype and text prompt. At each UNet block, a cross-attention operation is applied to capture high-level": "semantics of the input images to the model. Since we have sparse views, we average these tokensto become one global token. In the other stream, the spatial conditions from the coarse geometryrendered features Ftgt = { Fi}Mi=1 is channel-concatenated with the noised latent z1:Mt. Thesespatially conditional features assist the model to capture the view information, and learn low-levelperception to maintain the texture of the scenes. Compared to the concurrent work CAT3D , thiscoarse feature conditioning not only provides accurate pose information from the 3DGS rendering,but also offers reasonable visual information. Color adjustment. While our MVSplat360 can achieve photorealistic NVS, the synthesized videossometimes exhibit oversaturated colors (detailed in Appendix C). This may be visually acceptable forvideo generation, but it can decrease performance when evaluated on NVS task. To mitigate this, weapply post-processing by matching the color histogram between the SVD refined views Itgt and our3DGS rendered views Itgt before performing pixel-aligned measurements, i.e., PSNR and SSIM.",
  "Training Objectives": "Our MVSplat360 predicts two sets of images, including the coarse one Itgt from the 3DGS moduleand the refined one Itgt from the SVD module, where the former is mainly rendered to help supervisethe geometry backbone. The entire model is end-to-end trainable, using three groups of loss functions,namely reconstruction loss, diffusion loss and latent space alignment loss. In particular, the reconstruction loss is a linear combination of 2 and LPIPS , applied betweenthe coarse outputs Itgt and the corresponding ground truth Itgt. The other two loss functions areapplied to the following SVD module, whose gradients will backpropagate to the 3DGS module butwill not update those structural parameters, i.e., , , . This is achieved by stopping the gradientsfrom the structural parameters when rendering the latent features Ftgt, since keeping those gradientflows will lead to unstable training, as also observed by latentSplat . We use the standardv-prediction formulation (detailed in .2) as the diffusion loss to fine-tune the denoisingnetwork of the SVD, keeping the first-stage encoder and decoder frozen. Since the SVDs releasedmodel is conditioned on image-encoded features while our diffusion module is on 3DGS-renderedones, we find it beneficial to align these two spaces by regularizing with a latent space alignmentloss, ming E Fg(I)E(Itgt) Ftgt22, where g refers to the geometry backbone with trainableparameters and E is the frozen SVD encoder.",
  "Experimental Details": "Datasets. To verify the effectiveness of MVSplat360 in synthesizing wide-sweeping and 360 novelviews, we have established a challenging benchmark derived from DL3DV-10K . It comprises51.3 million frames from 10,510 real-world scenes, adhering to 65 point-of-interest (POI) categories. For training, we use a subset in subfolders 3K and 4K, resulting in ~2,000 scenes. Wetested on the 140 benchmark scenes and filtered them out from the training set to ensure correctness.For each scene, we selected 5 input views using farthest point sampling based on camera locationsand evaluated 56 views by equally sampling from the remaining, yielding a total of 7,840 test views.Additionally, since most DL3DV-10K scenes contain a two-round trajectory, we also report anothersetting by focusing only on half of the sequence, intending to cover the camera trajectory of oneround. We denote the two settings as n = 300 and n = 150, where n refers to the frame distancespan across all test views, as most scenes contain roughly 300 frames. We also assess our model onRealEstate10K , which contains real estate videos downloaded from YouTube. Consistent withexisting works , we train MVSplat360 on 67,477 scenes and test it on 7,289 scenes. Metrics. To measure models from different perspectives, we follow to report both the pixel-alignmetrics, i.e., PSNR and SSIM , and the perceptual metrics, i.e., LPIPS and DISTS .Since MVSplat360 aims to generate plausible contents for unobserved and disoccluded regions, wealso reported the distribution metric, i.e., Frchet Inception Distance (FID) 1, which measures thesimilarity between distributions of the generated images and the real ones. : Comparison with SoTA methods on DL3DV-10K. Below, n is the frame distance spanacross all the tested novel views within each scene, which is set to 300 by default as most DL3DV-10Kscenes contain roughly 300 extracted frames. Since most DL3DV-10K scenes contain a two-roundtrajectory, we also report another setting of n = 150 aiming for coverage of one round.",
  "PSNR SSIM LPIPS DISTS FIDPSNR SSIM LPIPS DISTS FID": "pixelSplat 14.830.4010.5760.383142.8316.050.4530.5210.348134.70MVSplat 15.720.4330.5010.29178.9517.050.4990.4350.24761.92latentSplat 16.680.4690.4390.23437.6817.790.5270.3910.20634.55MVSplat36016.810.5140.4180.17517.0117.810.5620.3520.15118.89 Implementation details. MVSplat360 is implemented with PyTorch and a CUDA-implemented3DGS renderer. For coarse geometry reconstruction (.1), we set hyperparameters followingMVSplat , except that we apply cross-view attention and build each cost volume within thenearest 2 views rather than all other views. For multi-frame appearance refinement (.2),we fine-tune from the 14-frame SVD pre-trained model, but using rendered Gaussian featuresas conditions. We also remove the original motion value and fps conditions, since they areunrelated to our NVS task. We rescale the rendered feature to have a similar shape as the originalimage-encoded latent feature in the pre-trained model, which is critical for getting better details as itaffects the decoder (more discussions are in Appendix A). We train SVD using 14 frames sampledalong natural camera trajectories, captured by the initial videos. At inference, we directly feed 56views to SVD but change all related temporal attention blocks to local attention with a window sizeof 14 to better align with the training. More implementation details can be found in Appendix B, andthe codes are publicly available at",
  "We first assess the ability of MVSplat360 and baselines to synthesize wide-sweeping and 360 NVSin the newly constructed challenging benchmark with diverse scene categories": "Baselines. We perform a thorough comparison of MVSplat360 to the latest state-of-the-art (SoTA)3DGS-based models, including pixelSplat , MVSplat and latentSplat . All models aretrained on the same training split and evaluated on the publicly available 140 scenes. Quantitative results. All models are trained to 100K steps and reported at , except forthe latentSplat, which suffers from unstable training due to its GAN-based architecture. Hence,we report its best performance at around 60K training steps before the subsequent collapse. OurMVSplat360 outperforms all existing SoTA models in all metrics on the two settings n = 300 andn = 150. All models generally perform better on the n = 150 setting than on the n = 300 onesince the latter spans larger viewpoints. It can be seen that the two generative models (latentSpaltand MVSplat360) generally perform better than the other two regression models, suggesting theimportance of additional refinement in addressing feed-forward scene reconstruction. Although our improvement on pixel-aligned metrics appears minor, this is expected since refinementvia either interpolation (for disoccluded regions) or extrapolation (for unobserved regions) does notguarantee matching the ground truth at the pixel level. It mainly aims to provide a reasonable solutionto refine the images and ensure they align with real-world image distribution. This is verified by thefact that our improvements on perceptual metrics are larger, and it is even more apparent on FID,which measures the distribution deviation. The superiority of our MVSplat360 stands out more fromthe qualitative results presented below. Qualitative results. The qualitative comparisons are visualized in . MVSplat360 achievesremarkable visual results even under challenging conditions. pixelSplat and MVSplat exhibitobvious artifacts due to the issue of floating Gaussians. latentSplat improves the results with anadditional decoder and adversarial training. However, its resulting object geometry and image qualityare still far from satisfactory, suggesting that the GAN-based framework cannot provide enoughprior knowledge for refining 360 NVS in diverse real-world scenes. Readers are referred to ourproject page for video results with more comprehensive comparisons, where our MVSplat360 shows",
  "Input Views": ": Qualitative comparisons on DL3DV-10K. MVSplat360 shows significant improvementcompared to existing SoTA models. Here, we showcase with a rich mix of diversity and complexity,including indoor (bounded) vs. outdoor (unbounded), high vs. low texture frequency, more vs. lessreflection, and more vs. less transparency. More results are provided in Appendix E. : Comparison with SoTA methods on RealEstate10K. We report interpolation scores usingthe settings of , where we retrain latentSplat to maintain fair comparison (indicates with*). We report extrapolation scores by following .",
  "(b) Number of input views. The default modelis trained and tested with 5 views, while the othersare directly evaluated with different numbers of inputviews during testing": "Quantitative results. shows quantitative comparisons on RealEstate10K of MVSplat360and other approaches. Our MVSplat360 surpasses all previous state-of-the-art methods, mainly interms of the perceptual metrics and the distribution metric. The former implies that our rendered viewsare more aligned with human perception, while the latter shows that our refined images correspondbetter to the dataset distribution. These observations can be further confirmed by visual assessment. Qualitative results. The qualitative comparisons of the top four best models are in . Pixel-Splat and MVSplat fail to render any content for the unobserved regions due to the lack ofgenerative capability. In contrast, latentSplat can perform extrapolation via its GAN-based decoder,improving the overall visual quality. However, we observed that the content generated by latentSplatis not visually reasonable. Our MVSplat360 generates more plausible content (see the window in1st row and chair in 2nd row), thanks to the stronger generative capability of the diffusion model.",
  "Accessing model components. The baseline refers to MVSplat since our model is built on top ofit. (i) A natural extension is to render novel views from MVSplat and use them directly as conditions": ": SfM on input and rendered views. Images with red borders are the input views, whileothers are rendered by our MVSplat360. The reasonably recovered camera poses and 3D point cloudsvia VGGSfM imply that our outputs are multi-view consistent and geometrically correct. in the SVD denoising process. However, this straightforward approach performs slightly worse thanthe original MVSplat, likely because SVD struggles to infer pose and visual cues from the noisyimage-encoded features. (ii) To better utilize the input context views, we average CLIP-embeddedtokens from all views instead of just the first. This provides SVD with richer scene information via thecross-attention blocks, leading to a noticeable improvement. (iii) Lastly, we render high-dimensionalfeatures via the 3DGS rasterizer and concatenate them directly into the diffusion latent space, enablinggradients from the denoising UNet to backpropagate through the geometry backbone. This end-to-endtraining improves performance significantly and is used in our default model. Accessing the number of input views. As shown in b, while our model is only trained with5 input views, the performance can be gradually improved by adding more input views at testing.This is reasonable as more input views can provide more observable areas. On the contrary, reducinginput views will inevitably result in worse performance. Surprisingly, even with 3 sparse views, ourMVSplat360 still outperforms regression models (pixelSplat and MVSplat) that use 5 views. Assessing the geometry accuracy. Our MVSplat360 builds on the video diffusion model SVD, whichdoes ensure strong temporal/multi-frame consistency but does not inherently guarantee geometricaccuracy. To confirm that our MVSplat360 produces geometrically accurate outputs, we run structure-from-motion (SfM) on both the input source views and the rendered novel views using VGGSfM .As shown in , VGGSfM recovers reasonable camera poses and 3D point clouds, confirmingthat our novel views are both multi-view consistent and geometrically correct. This highlights howthe 3DGS backbones latent features provide essential geometric cues, enhancing 3D consistency inthe final SVD-based outputs.",
  "Conclusion": "We present MVSplat360, a feed-forward model that synthesises 360 novel views of diverse real-world scenes from sparse input views. Our MVSplat360 leverages a feed-forward 3DGS model forrecovering the coarse geometry and appearance from sparse observations of a 3D scene, which are thenused to render latent features as the pose and visual cues to guide the following SVD in generating 3Dconsistent 360 novel views. To demonstrate its effectiveness, we construct a challenging benchmarkfor 360 NVS of real-world scenes. Experimental results show that MVSplat360 achieves superiorvisual quality compared to other SoTA feed-forward approaches.",
  "Barron, J.T., Mildenhall, B., Verbin, D., Srinivasan, P.P., Hedman, P.: Mip-nerf 360: Unboundedanti-aliased neural radiance fields. In: CVPR. pp. 54705479 (2022)": "Blattmann, A., Dockhorn, T., Kulal, S., Mendelevitch, D., Kilian, M., Lorenz, D., Levi, Y.,English, Z., Voleti, V., Letts, A., et al.: Stable video diffusion: Scaling latent video diffusionmodels to large datasets. arXiv preprint arXiv:2311.15127 (2023) Chan, E.R., Nagano, K., Chan, M.A., Bergman, A.W., Park, J.J., Levy, A., Aittala, M., Mello,S.D., Karras, T., Wetzstein, G.: GeNVS: Generative novel view synthesis with 3D-awarediffusion models. In: Proceedings of the International Conference on Computer Vision (ICCV)(2023)",
  "Suhail, M., Esteves, C., Sigal, L., Makadia, A.: Light field neural rendering. In: CVPR (2023)": "Szymanowicz, S., Insafutdinov, E., Zheng, C., Campbell, D., Henriques, J.F., Rupprecht, C.,Vedaldi, A.: Flash3d: Feed-forward generalisable 3d scene reconstruction from a single image.arXiv preprint arXiv:2406.04343 (2024) Szymanowicz, S., Rupprecht, C., Vedaldi, A.: Splatter image: Ultra-fast single-view 3dreconstruction. In: Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR) (2024)",
  "Truong, P., Rakotosaona, M.J., Manhardt, F., Tombari, F.: Sparf: Neural radiance fields fromsparse and noisy poses. In: CVPR (2023)": "Tseng, H.Y., Li, Q., Kim, C., Alsisan, S., Huang, J.B., Kopf, J.: Consistent view synthesiswith pose-guided diffusion models. In: Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition (CVPR). pp. 1677316783 (2023) Voleti, V., Yao, C.H., Boss, M., Letts, A., Pankratz, D., Tochilkin, D., Laforte, C., Rombach, R.,Jampani, V.: Sv3d: Novel multi-view synthesis and 3d generation from a single image usinglatent video diffusion. arXiv preprint arXiv:2403.12008 (2024) Wang, J., Karaev, N., Rupprecht, C., Novotny, D.: Vggsfm: Visual geometry grounded deepstructure from motion. In: Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition. pp. 2168621697 (2024)",
  "Yang, J., Cheng, Z., Duan, Y., Ji, P., Li, H.: Consistnet: Enforcing 3d consistency for multi-viewimages diffusion. arXiv preprint arXiv:2310.10343 (2023)": "Ye, J., Wang, P., Li, K., Shi, Y., Wang, H.: Consistent-1-to-3: Consistent image to 3d viewsynthesis via geometry-aware diffusion models. In: Proceedings of the International Conferenceon 3D Vision (3DV) (2024) Ye, M., Yin, P., Lee, W.C., Lee, D.L.: Exploiting geographical influence for collaborative point-of-interest recommendation. In: Proceedings of the 34th international ACM SIGIR conferenceon Research and development in Information Retrieval. pp. 325334 (2011)",
  "AMore Experiment Results": "Assessing the robustness of SVDs first-stage autoencoder to input resolution. Since our MVS-plat360 fine-tunes the denoising process in latent space, it is essential to ensure the autoencoder accu-rately maps between pixel and latent spaces. We observe that when the input resolution (256 480)differs significantly from the autoencoders pre-trained resolution (768 1280), the autoencodingprocess causes noticeable information loss, leading to missing details (see Fig. A 2nd column) withan average PSNR of 26.77dB on the test set. Although fine-tuning the autoencoder might addressthis limitation, it risks overfitting due to our relatively small-scale training set (around 2000 scenes).Instead, we find that simply upscaling the input 2 via bilinear interpolation before feeding themto the encoder helps preserve details effectively (see Fig. A 3rd column) and improves the PSNR to32.52dB, ensuring the latent space remains accurate.",
  "BMore Implementation Details": "Following MVSplat , we set the near and far depth range used in the cost volume construction as1 and 100, respectively. We render features from the 3D Guassians rasterizer with shape 4 h w,where h and w refer to the image height and width, respectively, and the channel dimension is set to4 to align with that of the SVD initial conditional vector. We bilinearly interpolate the latent featuresto a resolution of 1/4h 1/4w, matching the encoded features from images of size 2h 2w, as theencoder downsamples inputs by a factor of 8. This design ensures proper projection into the initiallatent space, as detailed in Appendix A. The SVD decoder outputs are then bilinearly interpolatedfrom 2h 2w back to h w. Due to resource limitations, we mainly experiment on the images_8branch of the DL3DV-10K dataset, which contains images with resolution 256 480. Our defaultmodel is trained with the Adam optimizer, and the learning rate is set to 1.e 5 and decayed withthe one-cycle strategy. All models are trained for 100,000 steps with an effective batch size of 8 on1 to 8 A100 GPUs, and we apply the gradient accumulation technique whenever needed. Duringtraining, we sampled 5 views as input views and another 14 views as targeted rendering views, withthe intention of better aligning with the following SVD module, which is trained on videos with 14frames.",
  "CLimitations and Discussions": "Although MVSplat360 achieves plausibly consistent 360 NVS and significantly outperforms previousworks in visual quality by leveraging SVD, it also inherits several limitations from SVD. For instance,the results may exhibit oversaturated colors (see Fig. B, left), limiting improvements on pixel-alignedmetrics like PSNR and SSIM. This is likely because SVD is primarily trained on artistic videos withvibrant colors, and fine-tuning from its pre-trained weight can bias the outputs toward the original",
  "Figure B: Limitations. Left: Views may appear oversaturated; Right: Refined views can containover-hallucinated contents. Both limitations stem from the diffusion modules prior knowledge": "training data distribution . Additionally, the outputs might exhibit hallucinations and containcontents not existing in the input views (see Fig. B, right). Lastly, the inference is slow due to themultiple sampling steps in the diffusion process. We expect these limitations to be mitigated as bettervideo diffusion models and pre-trained weights become available in the future.",
  "DBroader Social Impacts": "Our MVSplat360 renders 360 novel views from sparse observations, making it a valuable tool foraugmented reality applications. It can enhance immersive experiences in entertainment and media,including 3D videos and video games, and support historical reconstruction for educational purposes.However, MVSplat360 s powerful generative capabilities could be misused to create fake videos.Additionally, while the rendered views are high quality, they may not fully capture real-world details.Therefore, precautions are necessary when using the generated data in safety-critical applications,such as training autonomous driving models."
}