{
  "Abstract": "Intelligent agents designed for interactive environments face significant challengesin text-based games, a domain that demands complex reasoning and adaptability.While agents based on large language models (LLMs) using self-reflection haveshown promise, they struggle when initially successful and exhibit reduced effec-tiveness when using smaller LLMs. We introduce Sweet&Sour, a novel approachthat addresses these limitations in existing reflection methods by incorporatingpositive experiences and managed memory to enrich the context available to theagent at decision time. Our comprehensive analysis spans both closed- and open-source LLMs and demonstrates the effectiveness of Sweet&Sour in improvingagent performance, particularly in scenarios where previous approaches fall short.",
  "Introduction": "Intelligent agents, designed to interact with and make decisions in dynamic environments, havebecome a central focus in AI research, with text-based games (TBGs) emerging as a particularlychallenging domain for evaluating these agents reasoning, adaptability, and learning abilities .Originally popular in the 1970s as text adventure games,1 TBGs present players with textual descrip-tions of environments, requiring them to input natural language commands to achieve objectives .For instance, determining if a metal fork is conductive involves locating the fork, assembling a circuit,and analyzing the result. Navigating TBGs demands that agents exhibit a combination of abilities,including planning, memory retention, spatial reasoning, and common sense knowledge . Previously, deep reinforcement learning and behavior cloning were the primary approaches to developagents to play TBGs . However, recent research shows that agents based on pretrained largelanguage models (LLMs) are more effective at navigating TBGs . A key factor in their success isthe integration of internal reflection to improve planning . Self-reflection, closely related to self-refinement, is a form of reasoning that occurs after receivingbinary or scalar feedback from the environment . In this process, the LLM reviews its actionsand their outcomes, considering what went wrong and potential ways to improve . By iterativelyadjusting its strategy based on verbal reinforcement, conveyed through textual feedback, the agentrefines its planning for subsequent attempts . However, reflection also has several limitations,including 1) underwhelming performance when agents are correct initially , 2) significantly worseefficacy when using smaller LLMs , and 3) dependence on external feedback . Our Contributions In this work we conduct a comprehensive analysis of LLM-based agents employ-ing reflection approaches in TBGs and evaluate their performance across closed- and open-sourceLLMs. To address the limitations of poor performance when agents are initially successful and thediminished efficacy of smaller LLMs, we propose Sweet&Sour to leverage positive experiences and",
  "ReActReflexionSweet&Sour": ": Comparison of used prompting methods to play ScienceWorld. ReAct introduces a THINKaction to explicitly reason regarding the next step. Reflexion leverages self-reflection across attemptsto learn from unsuccessful tries and stores these in memory. Sweet&Sour not only performs self-reflection after failures but also after each completed sub goal, making its reflection instantly available. managed memory to create a richer context for self-reflection. Our findings demonstrate that ourmethod improves the performance of agents using reflection, particularly in scenarios where theypreviously struggled, enabling more robust and generalizable learning across tasks and model sizes.",
  "Methodology": "Background Assuming an LLM behaving as an actor model as part of our agent i.e. generatingactions based on the current state and policy, analogous to traditional policy-based RL setups we sample an action at from the current policy at time t and receive an observation from theenvironment ot. When a game begins, the agent makes its first observation o0 at time step t = 0.This first observation differs from subsequent ones, as it consists of the goal description d, as wellas an analysis of the starting room (i.e. the output of the look around command). Subsequently,the agent can perform an action at at each time step and receives a corresponding observation otfrom the environment. The agent has an inventory it in which to store items. Each task consists of anumber of sub tasks (such as finding a key object), the completion of which grants the agent a sparsereward, which adds to its current reward rt. The game continues until the agent has achieved the goaloutlined in d and receives the full reward as final score, or the maximum number of steps which weset to 150 is reached, in which case rt will become the final score. A detailed problem formulationand assumptions we make are given in appendix A. Self-Reflection Reflection occurs in addition to the acting LLM. Here, the agent reviews the at andot associated with previous unsuccessful attempts to verbalize the reason for failure. This processtypically involves maintaining a persistent history of insights gained across attempts, which the LLMuses as additional context for its reflections to improve future decision making for the next attempt .However, since other self-reflection methods focus on learning from failures , theyoverlook the importance of reinforcing successful behaviors in a similar way. Sweet&Sour To address the limitations of existing reflection methods, we enhance self-reflection byincorporating both positive (sweet) and negative (sour) experiences into the reflection process. Thisenables the agent to learn effectively from successful actions by reinforcing strategies that lead topositive outcomes while still learning from failures. When the current policy is achieving rewards, wequery the agent to extrapolate from it, encouraging the agent to verbalize what made its current policysuccessful and what can be generalized from this. This is visualized in figure 1 and an example of thisis shown in appendix B. Our method is broadly applicable to agents in interactive text environments with feedback using self-reflection, including those that build additional complexity on top of thecore reflection loop, such as grounding or gradient learning . We leave the study of theseadditional use cases for future work. Previous works store their reflections gained from unsuccessful attempts in something akin to a long-term memory to make them available to the agent across attempts . Instead, to complementSweet&Sour, we propose a managed memory approach to store and retrieve relevant reflections.This is implemented using a dual-buffer structure, where experiences are stored in two categories:short-term memory and long-term memory, based on their outcome (success or failure) and recency.Initially, if sub goals are reached, short-term memories of the made reflections are stored in atemporary buffer. Each short-term memory consists of a tuple (reflectiont, ot, at, rt). Once a taskis completed or an attempt ends, all short-term memories are moved to long-term memory. Failedattempts reflections are immediately added to long-term memory for the next attempt and short-termmemory collection ends.",
  "Experiments": "Data & Environment We use the ScienceWorld benchmark , which provides a versatile setting forevaluating agents in science experiment tasks across 10 interconnected locations, such as a greenhouseand a workshop, with over 200 objects and 25 action templates, generating a vast and dynamic searchspace. We use the test set for our evaluation, which provides up to 10 variations of each of the 30distinct tasks. These interactive tasks cover various topics, including chemistry and electricity, andhave an average optimal decision depth of 50 steps. An example task is shown in appendix C. Fordetails of all tasks and the environment, we refer to . We elect to use ScienceWorld instead ofprevious interactive text environment benchmarks such as TWC and ALFWorld , due totheir relative simplicity for current LLM-based agents. We measure performance using the successscore, which is always between 0 and 100. Completing a task implies completing every sub task,receiving the full reward, and thus a score of 100. Baselines CALM is a reranking method that integrates a deep reinforced relevance network(DRRN) with a causal language model fine-tuned using oracle trajectories. The causal languagemodel acquires task- and environment-specific knowledge through imitation learning, while DRRNtrains a policy network to rerank the language models predictions. We use ReAct as our baselineLLM-based agent. ReAct composes useful information at each time step by reasoning over the currentcontext (e.g. decomposing task or common sense knowledge query) and carries it forward to thecontext of the following time step. This reasoning action does not affect the environment and may beconsidered few-shot in-context learning. To contextualize our work, we compare our method againstReflexion , an agent built on ReAct that employs a self-reflection mechanism to iteratively improveits performance across rounds upon encountering failure based on feedback from the environment. Assuch, it runs over up to four rounds as it builds up its long-term memory. For all agents, we evaluatetheir performance using LLMs of different sizes and complexities to assess the performance of eachmethod across varying computational resources. In descending order of parameter count, we selectGPT-4o (gpt-4o-2024-08-06), Mistral Large 2 (mistral-large-2407), and Llama 3.1 8B(llama-3.1-8b-instruct), accessing each through its respective APIs. Main Results The results are shown in table 1. We find that Sweet&Sour outperforms the baselinemethods across all LLMs, setting the highest average score at 54.6 using GPT-4o. The performancegap between Sweet&Sour and the other methods widens for smaller models with a lower parametercount. For instance, it achieves 44.6 compared to Reflexions 27.6 on Mistral Large 2, and 32.5compared to 21.7 on Llama 8B indicating that our method is more suitable for scenarios with limitedcomputational resources. When we modify our method to only sample from failures, performancedrops significantly to a level similar to Reflexion scores decrease to 24.6, 31.1, and 44.9 for Llama8B, Mistral Large 2, and GPT-4o, respectively. As such, incorporating positive experiences indeedleads to better reflections, mimicking how humans learn from both positive and negative experiences,resulting in improved decision-making. Anti-Tilt In highly challenging tasks, such as 1-1 and 8-2, all methods tend to struggle, while insimpler tasks, most methods succeed based on the LLMs inherent capabilities alone. However,medium-difficulty tasks, such as 3-2 and 3-3, reveal a critical performance gap between our methodand previous approaches. We note that this occurs because traditional methods fail to reflect on early",
  "CALML8BML2GPTL8BML2GPTL8BML2GPT": "1-1 (Boil)0.00.00.03.80.00.05.10.07.29.61-2 (Melt)0.08.410.311.80.00.010.011.412.112.81-3 (Freeze)0.01.50.08.10.02.38.32.43.18.91-4 (Change state)0.01.04.710.00.00.04.21.72.99.22-1 (Thermometer)1.05.17.87.73.44.27.67.89.710.92-2 (Melting)1.06.76.35.93.33.326.27.936.846.02-3 (Melting)5.09.111.823.413.214.722.615.229.038.33-1 (Power 1)7.018.824.657.221.251.578.428.675.481.13-2 (Power 2)2.010.224.755.69.511.924.723.344.558.03-3 (Conductivity 1)2.052.451.773.09.225.872.159.169.275.73-4 (Conductivity 2)10.054.264.989.735.441.675.162.760.367.34-1 (Find 1)54.017.318.727.544.648.162.341.771.774.24-2 (Find 2)10.069.171.680.368.475.787.376.8100.0100.04-3 (Find 3)8.021.342.847.718.416.517.320.921.534.34-4 (Find 4)2.015.715.219.339.646.6100.055.187.8100.05-1 (Grow plant)4.010.810.810.07.27.27.914.214.617.45-2 (Grow fruit)3.018.118.519.230.851.434.651.555.660.26-1 (Chemistry 1)6.037.842.958.627.129.770.237.961.170.26-2 (Chemistry 2)3.025.027.150.614.428.069.827.251.983.16-3 (Chemistry 3)6.014.417.539.738.931.116.745.353.761.57-1 (Lifespan 1)10.037.041.760.075.075.0100.075.088.2100.07-2 (Lifespan 2)4.050.550.767.560.071.981.470.577.080.07-3 (Lifespan 3)4.033.738.250.029.533.775.051.154.284.68-1 (Identify life 1)0.05.118.925.31.71.73.411.110.314.28-2 (Identify life 2)0.06.47.48.07.48.08.05.07.47.49-1 (Measure angle)0.028.533.042.556.955.157.168.470.375.09-2 (Friction 1)3.014.522.643.123.429.3100.033.336.762.09-3 (Friction 2)2.02.914.542.81.333.659.67.251.963.110-1 (Genetics 1)2.025.727.326.45.69.850.438.948.678.810-2 (Genetics 2)2.013.219.117.26.221.522.723.624.054.8",
  "Average5.0720.524.836.021.727.645.332.544.654.6": "successes, leaving them with less contextual understanding to carry momentum forward, leading to asustained performance decline or tilt. By contrast, we theorize that Sweet&Sours reflection onboth successes and failures provides a richer context, enabling it to sustain and build upon initialsuccesses. For instance, on task 3-2, Sweet&Sour achieves a score of 68.0 with GPT-4o, significantlyoutperforming Reflexions 24.7 while beating ReActs 55.6 by a smaller margin, indicating thatReflexion, although it is overall more capable, got stuck despite its self-reflection.",
  "Conclusion": "In an attempt to improve agent performance and reduce sensitivity to the LLM used, our studyembarks on an analysis of reflection mechanisms for LLM-based agents. Existing work focuses onlearning from failures only. Although these methods improve planning by analyzing past mistakes,they struggle when agents are initially successful and when using smaller LLMs. The primarycontribution of our work is Sweet&Sour, a novel reflection technique for LLM-based agents inTBGs that leverages positive experiences to improve agent self-reflection. Our comprehensiveanalysis demonstrates the effectiveness of Sweet&Sour in enhancing agent adaptability and reasoning,particularly in challenging situations where previous approaches struggled. Limitations Despite promising results, our work has limitations. LLMs do not provide guaranteesregarding their reasoning capabilities. Additionally, our evaluation is conducted using a singleenvironment, which, while comprehensive, does not cover all types of interactive scenarios. We leavethe exploration of additional environments to future work.",
  "This work was supported by an Oracle for Research Grant Award, as well as SURF Grant EINF-8535": "Prithviraj Ammanabrolu and Mark Riedl. Playing text-adventure games with graph-based deepreinforcement learning. In Proceedings of the 2019 Conference of the North American Chapterof the Association for Computational Linguistics: Human Language Technologies, Volume 1(Long and Short Papers), pages 35573565, Minneapolis, Minnesota, June 2019. Associationfor Computational Linguistics. doi: 10.18653/v1/N19-1358. URL Marc-Alexandre Ct, kos Kdr, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine,James Moore, Ruo Yu Tao, Matthew Hausknecht, Layla El Asri, Mahmoud Adada, WendyTay, and Adam Trischler. Textworld: A learning environment for text-based games. CoRR,abs/1806.11532, 2018. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle,Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, AnthonyHartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark,Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere,Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi,Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, CorinneWong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, DaniellePintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano,Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, EmilyDinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee,Georgia Lewis Anderson, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, HaileyNguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, IsabelKloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes,Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong,Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, JoannaBitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia,Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, KevinStone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, LaurenRantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin,Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, MadelineMuzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Mathew Oldham,Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh,Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, NiladriChatterji, Olivier Duchenne, Onur elebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, PetarVasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura,Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer,Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohit Girdhar, Rohit Patel, RomainSauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang,Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, SergeyEdunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, ShrutiBhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla,Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, TarekSheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, TongXiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez,Vincent Gonguet, Virginie Do, Vish Vogeti, Vladan Petrovic, Weiwei Chu, Wenhan Xiong,Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaoqing Ellen Tan, XinfengXie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen,Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan,Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aaron Grattafiori, Abha Jain, Adam Kelsey,Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, AjaySharma, Alex Boesenberg, Alex Vaughan, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu,Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Franco, AparajitaSaraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, AzadehYazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto DePaola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, BrandonSpence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, CatalinaMejia, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, ChrisCai, Chris Tindal, Christoph Feichtenhofer, Damon Civin, Dana Beaty, Daniel Kreymer, DanielLi, Danny Wyatt, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh,Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling,Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Erik Brinkman,Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Firat Ozgenel,Francesco Caggioni, Francisco Guzmn, Frank Kanayet, Frank Seide, Gabriela Medina Florez,Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Govind Thattai, Grant Herman,Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hamid Shojanazeri, Han Zou,Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren,Hunter Goldman, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Irina-Elena Veliche, ItaiGat, Jake Weissman, James Geboski, James Kohli, Japhet Asher, Jean-Baptiste Gaya, JeffMarcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, JessicaZhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie,Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, KarthikPrasad, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, KellyMichelena, Keqian Li, Kun Huang, Kunal Chawla, Kushal Lakhotia, Kyle Huang, Lailin Chen,Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu,Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, MariaTsimpoukelli, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, MaximGroshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Michael L. Seltzer, Michal Valko,Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey,Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal,Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, NickEgebo, Nicolas Usunier, Nikolay Pavlovich Laptev, Ning Dong, Ning Zhang, Norman Cheng,Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, PaulSaab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina,Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, RafiAyub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Raymond Li, Rebekkah Hogan, RobinBattey, Rocky Wang, Rohan Maheswari, Russ Howes, Ruty Rinott, Sai Jayesh Bondu, SamyakDatta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Verma,Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, ShenghaoLin, Shengxin Cindy Zha, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang,Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe,Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Sungmin Cho, Sunny Virk, SurajSubramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, ThiloKohler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, TzookShaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar,Vishal Mangla, Vtor Albiero, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, VladimirIvanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, XiaochengTang, Xiaofang Wang, Xiaojian Wu, Xiaolan Wang, Xide Xia, Xilun Wu, Xinbo Gao, YanjunChen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam,Yu, Wang, Yuchen Hao, Yundi Qian, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick,Zhaoduo Wen, Zhenyu Yang, and Zhiwei Zhao. The llama 3 herd of models, 2024. URL Matthew Hausknecht, Prithviraj Ammanabrolu, Marc-Alexandre Ct, and Xingdi Yuan. Inter-active fiction games: A colossal adventure. In Proceedings of the AAAI Conference on ArtificialIntelligence, volume 34, pages 79037910, 2020. Ji He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Lihong Li, Li Deng, and Mari Osten-dorf. Deep reinforcement learning with a natural language action space. In Katrin Erk andNoah A. Smith, editors, Proceedings of the 54th Annual Meeting of the Association for Com-putational Linguistics (Volume 1: Long Papers), pages 16211630, Berlin, Germany, Au-",
  "gust 2016. Association for Computational Linguistics. doi: 10.18653/v1/P16-1153. URL": "Ji He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Lihong Li, Li Deng, and Mari Ostendorf.Deep reinforcement learning with a natural language action space. In Proceedings of the 54thAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),pages 16211630, Berlin, Germany, August 2016. Association for Computational Linguistics.doi: 10.18653/v1/P16-1153. URL",
  "Yanhong Li, Chenghao Yang, and Allyson Ettinger. When hindsight is not 20/20: Testing limitson reflective thinking in large language models, 2024. URL": "Bill Yuchen Lin, Yicheng Fu, Karina Yang, Prithviraj Ammanabrolu, Faeze Brahman, ShiyuHuang, Chandra Bhagavatula, Yejin Choi, and Xiang Ren. Swiftsage: A generative agent withfast and slow thinking for complex interactive tasks. ArXiv preprint, abs/2305.17390, 2023.URL Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, UriAlon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa PrasadMajumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. Self-refine:Iterative refinement with self-feedback. In Thirty-seventh Conference on Neural InformationProcessing Systems, 2023. URL Keerthiram Murugesan, Mattia Atzeni, Pavan Kapanipathi, Pushkar Shukla, Sadhana Ku-maravel, Gerald Tesauro, Kartik Talamadupula, Mrinmaya Sachan, and Murray Campbell.Text-based rl agents with commonsense knowledge: New challenges, environments and base-lines. Proceedings of the AAAI Conference on Artificial Intelligence, 35(10):90189027, May2021. doi: 10.1609/aaai.v35i10.17090. URL Karthik Narasimhan, Tejas Kulkarni, and Regina Barzilay. Language understanding for text-based games using deep reinforcement learning. In Proceedings of the 2015 Conference onEmpirical Methods in Natural Language Processing, pages 111, Lisbon, Portugal, September2015. Association for Computational Linguistics. doi: 10.18653/v1/D15-1001. URL",
  "Ruo Yu Tao, Marc-Alexandre Ct, Xingdi Yuan, and Layla El Asri. Towards solving text-basedgames by producing adaptive action spaces, 2018": "Ruoyao Wang, Peter Jansen, Marc-Alexandre Ct, and Prithviraj Ammanabrolu. Science-World: Is your agent smarter than a 5th grader? In Yoav Goldberg, Zornitsa Kozareva, andYue Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in NaturalLanguage Processing, pages 1127911298, Abu Dhabi, United Arab Emirates, December 2022.Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.775. URL Zekun Wang, Ge Zhang, Kexin Yang, Ning Shi, Wangchunshu Zhou, Shaochun Hao,Guangzheng Xiong, Yizhi Li, Mong Yuan Sim, Xiuying Chen, Qingqing Zhu, ZhenzhuYang, Adam Nik, Qi Liu, Chenghua Lin, Shi Wang, Ruibo Liu, Wenhu Chen, Ke Xu, Day-iheng Liu, Yike Guo, and Jie Fu.Interactive natural language processing, 2023.URL Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe,explain, plan and select: Interactive planning with large language models enables open-worldmulti-task agents, 2024. URL Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang,Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan, Xiao Wang, Limao Xiong,Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou, Xiangyang Liu, Zhangyue Yin,Shihan Dou, Rongxiang Weng, Wensen Cheng, Qi Zhang, Wenjuan Qin, Yongyan Zheng,Xipeng Qiu, Xuanjing Huang, and Tao Gui. The rise and potential of large language modelbased agents: A survey, 2023. URL Shunyu Yao, Rohan Rao, Matthew Hausknecht, and Karthik Narasimhan. Keep CALM andexplore: Language models for action generation in text-based games. In Bonnie Webber, TrevorCohn, Yulan He, and Yang Liu, editors, Proceedings of the 2020 Conference on EmpiricalMethods in Natural Language Processing (EMNLP), pages 87368754, Online, November2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.704.URL",
  "Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and YuanCao. React: Synergizing reasoning and acting in language models, 2023. URL": "Weiran Yao, Shelby Heinecke, Juan Carlos Niebles, Zhiwei Liu, Yihao Feng, Le Xue, Rithesh RN, Zeyuan Chen, Jianguo Zhang, Devansh Arpit, Ran Xu, Phil L Mui, Huan Wang, CaimingXiong, and Silvio Savarese. Retroformer: Retrospective large language agents with policygradient optimization. In The Twelfth International Conference on Learning Representations,2024. URL Ceyao Zhang, Kaijie Yang, Siyi Hu, Zihao Wang, Guanghe Li, Yihang Sun, Cheng Zhang,Zhaowei Zhang, Anji Liu, Song-Chun Zhu, Xiaojun Chang, Junge Zhang, Feng Yin, YitaoLiang, and Yaodong Yang. Proagent: Building proactive cooperative agents with large languagemodels, 2024. URL",
  "ABackground": "One may consider every TBG to be a partially observable Markov decision process (POMDP) where the environment state is never observed directly.This POMDP may be formalized asS, T, A, , R, , where denotes the reward discount factor. S denotes the set of states sthat contain the internal information of the game such as objects found throughout the game or theplayers location not all of which may be visible to the agent at any given time. A denotes the actionspace made up of individual text actions a issued by the player. denotes the observation function.Further, o O denotes the observations made by the player. The observation ot of the agent at timet depends on the current state st, as well as the previous action at1, which may be formalized as (ot | st, at1). Seeing as the agent can only observe and interact with the environment of a TBGvia natural language, each observation is composed of a sequence of tokens ot =o1t, , oNt, asare their actions at =a1t, , aMt. In the context of TBGs, an action at is considered admissible at a state st if it is capable of changingthe games state, i.e., if it can lead to a transition to a new state st+1 that is different from thecurrent state st. The environments state transition is modeled through a probabilistic functionT (st+1 | st, at). Traditionally, admissible actions in state st could deterministically lead to a newstate st+1. However, we use a more general approach where all actions, whether admissible or not,are included in the state transition function. Non-admissible actions, which do not lead to a changein the games state, result in a transition back to the original state st with probability 1. In contrast,admissible actions lead to different states with their own probability. The admissible action set isbound to be significant for quests with a sufficiently large branching factor. While admissible, manyaction candidates are bound to be suboptimal.",
  "t trt], it aims to maximize are obtained by rt = R (st, at). In practice, TBGs typically provide sparse positive rewards forcompleting subgoals and advancing towards completing the game": "A TBGs interpreter can accept any text sequence, but will only recognize text that follows a certainstructure. Typically, commands take the form of either a single keyword (such as look) or acombination involving verbs, objects, and occasionally prepositions. Previous works on TBGsmade the assumption that we have access to a predefined set of all admissible actions at each gamestate and must select the correct action to progress . This is consistent with how some textadventure games are played in the real world . In line with more recent works , we make theassumption that we have access to a number of action templates (e.g. connect A to B, pick upC) and subsequently ask the agent to generate the actions as tokens using these templates. This leadsto a more challenging action generation process for the agent.",
  "BLLM Reflection": "After successfully completing a sub task, the agent is prompted to reflect on its most recent observa-tions and identify the key factors that contributed to its success. This reflection process encouragesthe agent to verbalize the steps and strategies that led to the desired outcome, helping it create aplan that can be stored in managed memory for future use. A similar procedure is applied in casesof failure. When a task is unsuccessful, the agent is instructed to reflect on alternative actions itcould have taken and to devise a revised plan for the next attempt, ensuring continuous learning andimprovement. This plan is also stored in memory. An example trajectory of a ScienceWorld task where the agent reflects on positive and negativeexperiences is shown in . At the end of this trajectory, we show for a single command what areflection would look like for a successful or unsuccessful choice, in this case picking up an animal.For the unsuccessful case (red), where the agent does not pick up an animal, we reflect on whetheranother choice would have constituted an animal and thus resulted in a reward. For the successfulcase where a sub goal is reached (yellow), we reflect on what made the current actions successful andsubsequently commit this to memory.",
  "a substance called air a picture the agentYou also see:": "A door to the greenhouse (open) A door to the living room (open) A door to the art studio (open) A door to the kitchen (open) A door to the bedroom (open) A door to the workshop (open)> open door to greenhouseThe door is already open.> go to greenhouseYou move to the greenhouse.> open door to outsideThe door is already open.> go to outsideYou move to the outside.> look aroundThis outside location is called the outside. Here, you see: a substance called air an axe a substance called wood the ground a fire pit (containing nothing) a fountain (containing a substance called water) a butterfly egg a blue jay egg a dove egg the agentYou also see: A door to the kitchen (open) A door to the foundry (open) A door to the greenhouse (open)Case #1: Failure> focus on axeYou focus on the axe.> pick up axeYou move the axe to the inventory.Reflection(I failed because I assumed eggs are not animals. My plan was to go outside tofind animals. I tried to pick up an axe. An alternative course of action would have been to gooutside and pick up an egg, as it might be considered an animal.) Managed Memory Case #2: Success> focus on dove eggYou focus on the dove egg.> pick up dove eggYou move the dove egg to the inventory.Reflection(Picking up an egg was the correct choice. Even though the animal was in an egg,it is still considered an animal. My plan was to go outside and find an animal in the wild.Items such as an axe, wood, or landscape elements are not related to animals.) Managed Memory",
  "CExample ScienceWorld Task": "In this section, we provide a successfully completed task, a variation of Task 1-1, which concernsitself with boiling a substance, in this case water, to change its state. Once the agent has viewed itssurroundings and moved to the kitchen, it collects the necessary items and begins its experiment byboiling the water in a pot on the stove. Finally, the agent examines steam and completes the task(highlighted in green). The example is truncated to improve readability. The trajectory is shown in.",
  "> look aroundThis room is called the hallway. In it, you see:": "a picture a substance called air the agent A door to the green house (open) A door to the living room (open) A door to the art studio (open) A door to the kitchen (open) A door to the bedroom (open) A door to the workshop (open)> open door to kitchenThe door is already open.> go to kitchenYou move to the kitchen.> look aroundThis room is called the kitchen. In it, you see:",
  "a substance called soap a painting a thermometer reading 10C a counter with various items (banana, potato, red apple, orange) a sink (off, empty)": "a table with a glass cup (empty) a chair (empty) a freezer (closed) a lighter a stopwatch (deactivated) a fridge (closed) a cupboard (closed) a stove (turned off) a glass jar containing sodium chloride various other items (air, the agent, etc.)> pick up thermometerYou move the thermometer to the inventory.> open cupboardThe cupboard is now open.> pick up metal potYou move the metal pot to the inventory.> move metal pot to sinkYou move the metal pot to the sink.> activate sinkThe sink is now activated.> deactivate sinkThe sink is now deactivated.> pick up metal potYou move the metal pot to the inventory.> focus on substance in metal potYou focus on the water.> move metal pot to stoveYou move the metal pot to the stove.> activate stoveThe stove is now activated.> use thermometer in inventory on substance in metal potThe thermometer measures a temperature of 13 degrees Celsius.> examine substance in metal potA substance called water.> use thermometer in inventory on substance in metal potThe thermometer measures a temperature of 28 degrees Celsius.> examine substance in metal potA substance called water.> use thermometer in inventory on substance in metal potThe thermometer measures a temperature of 58 degrees Celsius.> examine substance in metal potA substance called water.> use thermometer in inventory on substance in metal potThe thermometer measures a temperature of 78 degrees Celsius.> examine substance in metal potA substance called water.> use thermometer in inventory on substance in metal potThe thermometer measures a temperature of 98 degrees Celsius.> examine steamA substance called steam. Task completed."
}