{
  "ABSTRACT": "This work presents a modification of the self-attention dynamics proposed by Geshkovski et al. (2023)to better reflect the practically relevant, causally masked attention used in transformer architecturesfor generative AI. This modification translates into an interacting particle system that cannot beinterpreted as a mean-field gradient flow. Despite this loss of structure, we significantly strengthenthe results of Geshkovski et al. (2023) in this context: While previous rigorous results focused oncases where all three matrices (Key, Query, and Value) were scaled identities, we prove asymptoticconvergence to a single cluster for arbitrary key-query matrices and a value matrix equal to the identity.Additionally, we establish a connection to the classical Rnyi parking problem from combinatorialgeometry to make initial theoretical steps towards demonstrating the existence of meta-stable states.",
  "Introduction": "The introduction of the Transformer architecture Vaswani et al. (2017) has markedly impacted the landscape of naturallanguage processing (NLP), signaling the advent of large language models. Central to the Transformer architecture isthe self-attention mechanism, a special kind of layer that distinguishes it from preceding models such as ResNets. Thisinnovation has yielded unprecedented performance not only in machine translation and text summarization but alsoin areas beyond NLP, including computer vision, speech recognition, and robotics. The flexibility and efficiency ofTransformers underscore their integral role in the progression of artificial intelligence. Despite their widespread use, thetheoretical foundations underlying their success remain underexplored. Following Sander et al. (2022), recent studies by Geshkovski et al. (2024b) and Geshkovski et al. (2023) have proposeda mathematical framework to analyze Transformers as interacting particle systems, demonstrating that tokens, whenmodeled as particles, exhibit clustering under certain conditions on the Key, Query, and Value matrices. These worksprimarily focus on full (mean-field) attention mechanisms, where each token can interact with every other token.Building upon this foundation, our research extends the analysis to causal attention mechanisms, wherein each tokenis restricted to interact only with preceding tokens. This distinction is crucial, as causal attention is prevalent inTransformer models employed in generative AI and known as decoder architectures. Causal attention is crucial for sequence generation tasks, ensuring that each token only attends to previous tokens andnot future ones, thereby preserving the correct temporal order. This mechanism, also known as autoregressive attention,masks future tokens during attention computation to prevent the model from accessing information it hasnt generatedyet. At inference time, causal attention allows the model to generate text one token at a time, using previously generatedtokens to inform the next, ensuring coherent and contextually accurate sequences. This step-by-step generation processis computationally efficient, as each token is produced in a forward pass without needing to revisit previous steps. Incontrast to full attention, which considers all tokens simultaneously and is suitable for tasks like machine translationwhere the entire sequence is known, causal attention is essential for tasks requiring real-time, sequential output. Thiscomputational advantage explains the pervasiveness of causal attention not only in natural language processing butalso in image generation with tools like DALL-E (Ramesh et al., 2021), VQGAN (Esser et al., 2021), or Parti (Yuet al., 2022) and multimodal foundation models, notably Chameleon (Team, 2024). More generally, the use of maskedattention where tokens pay attention to a subset of other tokens has been driving recent scaling efforts and has led tostate-of-the-art models such as MUSE (Chang et al., 2023) or Alphafold 3 (Abramson et al., 2024). Causal attention",
  "(e) V = diag(-1, -3, -3)": ": Particle trajectories for different Value matrices. In all cases we take simple Query and Key matricesK = Q = Id, temperature = 9 and final time T = 5000 for n = 32 particles initialized uniformly at random on thesphere. Positions of particles at time T are indicated by a red dot.",
  "can also be recast as an interacting particle system but it requires different analytical techniques. This is the goal of ourpaper": "Our contributions. Our main theoretical result establishes asymptotic clustering of tokens for causal self-attentiontransformer modeled as an interacting particles system on the sphere (Theorem 4.1). While mathematically accurate,this asymptotic collapse to a single cluster is seldom observed numerically. Instead, particles collapse to multipleclusters and stay in this configuration for a very long time (see for a representative example) such meta-stablestates were already alluded to in Geshkovski et al. (2023) and their study was recently initiated in Geshkovski et al.(2024a). In we describe such meta-stable states using analogy with the Rnyi parking process (Lemma 5.1,Theorem 5.2). Additionally, Theorem 5.2 covers asymptotic clustering of tokens for causal self-attention with additionalcross-attention component. Moreover, we predict that, akin to linear dynamical systems, the most important factorsthat qualitatively describe final particles configuration both in causal and full-attention cases are the eigenvalue ofthe Value matrix V with the largest real part max and its eigenspace L, while Query and Key matrices Q, K andtemperature parameter do not matter. Our conjectured atlas of possible meta-stable configurations is listed in .We prove the result stated as the first line of this table, namely that particles eventually collapse into a point whenV = Id in Theorem 4.1. We remark that assumptions of Theorem 4.1 are much weaker than for the similar resultsin the full-attention case Geshkovski et al. (2023), in particular we put no constraints on K, Q or . This work is acombination of rigorous mathematical results and non-trivial predictions based on analytical insights and numericalsimulations. We summarize all limitations in . Related work. Our work builds upon the framework of Geshkovski et al. (2023, 2024b) where clustering properties oftransformers are analyzed as systems of particles interacting on the sphere. Specifically, Geshkovski et al. (2023) provedthat encoder-only (i.e. unmasked) self-attention with (post) LayerNorm leads to tokens clustering to a single point, in",
  "Clustering in Causal Attention Masking3": "the limit of number of layers going to infinity. This phenomenon is also known as consensus in the related literature ofmulti-agent systems Markdahl et al. (2017); Criscitiello et al. (2024) and Kuramoto oscillators Strogatz (2000); Abdallaet al. (2022). Work Geshkovski et al. (2023) in turn expands on the original perspective brought forward by Sanderet al. (2022) that identify the self-attention layer as a measure-to-measure map, see also Vuckovic et al. (2021). Morerecently, Castin et al. (2024) studied the smoothness of this map in a framework that also covers causal attention. Thiswork introduces a clever reparametrization that allows them to recast causal attention as mean-field dynamics, akin totheir full attention counterpart. Using various approximations, Cowsik et al. (2024) were able to study a more realisticarchitecture that also includes MLP layers and produce accurate predictions for the final configuration of particles. Thissetup was further investigated by Agrachev and Letrouit (2024) from a geometric control perspective. We note also thatclustering in the absence of a residual connection (replace xk(t) with xk(t + 1) in (SA)) was established in Wu et al.(2023). Additional effects of the residual connection are studied in Dong et al. (2021) and Zhang et al. (2024).",
  "Causal attention": "Before describing our model of causal attention dynamics, we review the idea of Geshkovski et al. (2023) formodeling the full attention dynamics. In that work, the evolution of representations of tokens through the layersis modeled as a system of n coupled Ordinary Differential Equations (ODEs) describing dynamics of a system ofparticles x1(t), . . . , xn(t). A brief part of their derivation of the dynamics from the transformers architecture is writtenin Section A.1. The particle position xk(t) corresponds to representation of the k-th token at layer t (where forconvenience, t is allowed to take non-integer values) and due to RMSNorm the particles are forced to live on a unitsphere Sd1. (RMSNorm layer usually also includes a multiplication by a trainable diagonal matrix D, but the effectof this step can be equivalently achieved by multiplying K, Q, V matrices by D.) These ODEs are parametrized bythree matrices, known as the query Q, the key K and the value V , respectively, and that are assumed to be square d dmatrices. More specifically, token k evolves according to",
  "j=1eQxk(t),Kxj(t)": "is a normalizing factor. Note that the dynamics of the k-th token depend on the positions of all tokens j [n], whichis a landmark characteristic of full attention leading to the so-called mean-field dynamics studied in Geshkovski et al.(2023); see also Geshkovski et al. (2024b); Castin et al. (2024); Paul and Trlat (2024). In this work we focus on causal attention, where the dynamics of token k depend only on the position of tokens j k.As described in the introduction, this modification is by now the dominant type of transformer architecture in generativeAI. To reflect causal masking, we modify the ODE governing the dynamics of token k as follows:",
  "Clustering in Causal Attention Masking4": "with their real part equal to max. Let L L be the subspace generated by only the eigenvectors in L with the largestcorresponding Jordan block (the vectors might correspond to different blocks and even to different eigenvalues).Lemma 3.1. Let x(t) be a solution of an ODE x(t) = Px(t)(V x(t)) defined on the unit sphere Sd1. Then, for almostevery initial value x(0) Sd1, there exists C, c > 0 such that the following convergence rates for the geodesicdistance dist hold:",
  "(ii) linear convergence to L: dist(x(t), L Sd1) ct1": "This result can be derived from standard results on the theory of linear ODEs (proof in Section B.1). We note that thisresult is important for other tokens as well. Indeed, for every token xk, the contribution to xk in (CSA) from the termwith j = k often has the biggest weight, an effect amplified by large . In general, eigenvectors corresponding to a real eigenvalue = max create a fixed set in L Sd1, while the complexeigenvalues with the largest real part produce a limit torus in L Sd1. In what follows, we only consider the casewhere the eigenvalue with the largest real part is real itself and it only has Jordan blocks of size 1. Then, L = L andconvergence to L is exponentially fast. Note also that when dim L = 1, we have L S1 = {} for some unit vector. In this case, x(t) as t , again with exponential speed. These observations will be important for the nextsection, when we describe asymptotic configurations of tokens.",
  "Final Configuration": "The system of n tokens that we are studying is far more complicated than for a single token. Even establishingconvergence to some point as t is challenging. In Geshkovski et al. (2023), similar models were analyzedanalytically by noticing that the dynamical system has the structure of the gradient flow of some potential function:",
  "For such systems, groundbreaking results of ojasiewicz (1962, 1965, 1984) (see Haraux (2012) for a self-containedoverview) guarantee convergence to a critical point of H assuming it is real-analytic": "However, our system (CSA) does not have a gradient-flow structure and thus techniques of ojasiewicz are notapplicable. On the other hand, we have a significant advantage in the hierarchical structure of our system, allowing usto study tokens sequentially. We have already understood the evolution of the first token. In this section, we do two things. First, we describe, basedon our analytical and numerical insights, conjectures about the asymptotic configuration x(t) for t . The surprisingresult here is that only the spectral properties of V (and not K or Q) affect asymptotics. Second, we rigorously proveconvergence to a single point for the special case of V = Id. We note that unlike the proof in Geshkovski et al. (2023)(see also Markdahl et al. (2017); Criscitiello et al. (2024)), our result works for all K and Q matrices, while the proofin Geshkovski et al. (2023) works only for QK = V and Markdahl et al. (2017); Criscitiello et al. (2024) is restrictedto QK = Id. Our main insight is that there are two major forces that drive each token: its internal force which is described byLemma 3.1, and the external force induced by all the particles preceding it, which is either attractive or repulsivedepending on the sign of the top eigenvalue(s) of V . The balance between the two forces is defined via attention. To get a better grasp of how the external force works, we consider the case where the first (internal) force vanishes, thatis, V = Id. In this case, the tokens collapse asymptotically to a single point.Theorem 4.1. Let V = Id and Q, K be arbitrary matrices. Then, for almost any starting point (x1(0), . . . , xn(0))with respect to the volume measure on (Sd1)n, the causal transformer dynamics (CSA) converge to a single cluster:",
  "k [n], limt xk(t) = x1(0)": "We prove this result in Section B.2. In the proof, weight functions are only required to be positive and continuouslydifferentiable (C1). This ambiguity suggests that incorporating time-dependence of Q and K might not alter thetheorems validity, but it significantly adds complexity to the proof in dealing with non-autonomous systems. Steps similar to our proof of Theorem 4.1 can be followed to study the more general case of the matrix V = Id.Unfortunately, one runs into multiple technical issues with application of the stable-manifold theorem from dynamicalsystems due to the emergence of critical manifolds (as opposed to critical points in the V = Id) case. Thus, we leavethe general case at the status of conjectures, which we describe next.",
  "Clustering in Causal Attention Masking5": "In what follows, we denote the eigenvalue of V with the largest real part as max and assume that it is real. If it is not,the limiting configuration is additionally rotating with a constant speed, which complicates the discussion and so isomitted. Let L denote the eigenspace of max. If max has multiplicity 1, then we denote the corresponding unit eigenvectors as. For simplicity we assume that max has all of the corresponding Jordan blocks of size 1. First of all, if dim L = 1 then, according to Lemma 3.1, every token is driven towards or by their own force.Moreover, for max > 0 the force of other tokens is attractive, while for max < 0 it is repulsive. Thus, for max > 0 all the particles collapse into and , whereas for max < 0 the repulsion force prevents theparticles from going all the way to and instead the particles stabilize at two clouds around and . This behavioris captured in Figures 1c and 1e4. For the case max > 0 we formally express it as: Conjecture 1. Let Q, K be arbitrary matrices and V be diagonalizable with d different positive real eigenvalues.Denote the largest eigenvalue as max and unit vector : V = max. Then, for almost any starting point(x1(0), . . . , xn(0)) with respect to the volume measure on (Sd1)n, the causal transformer dynamics (CSA) convergeto two clusters k [n], limt xk(t) {, }. If max has multiplicity at least 2, then from Lemma 3.1 each token internally gets attracted by the eigenspace L. Whentokens are close to L, the action of V becomes close to maxId, which for max > 0 according to Theorem 4.1 forcestokens to collapse to a singleton, while for max < 0 other tokens exude a repelling force, causing particles to spreadout around L. This behavior is captured in Figures 1b and 1d. For the case max > 0 we formalize it as follows: Conjecture 2. Let Q, K be arbitrary matrices and V be any matrix such that its largest eigenvalue max > 0 is realand has an eigenspace L of dimension dim L 2, while for any z L one has V z L with V z, z < max|z|2.Then, for almost any initialization (x1(0), . . . , xn(0)), the causal attention dynamics (CSA) converge to one cluster.More specifically, if we define as the normalized L-component of x1(0), i.e., for y1 := PL(x1(0)), := y1/|y1|,then k [n], limt xk(t) = .",
  "(Note that is undefined when x1(0) L, but this happens with probability zero.)": "An important practical observation is that these conjectures explain that V performs dimensionality reduction in thefollowing way. Tokens converge to L Sd1 and, in that space, they move as if acted upon by the Id matrix on asphere Sdim L1. For the pre-trained Lan et al. (2020) the spectra of value matrices is depicted in . Interestingly,there are heads with negative max. Future work will be concerned with studying real-world matrices V and connectingtheir top eigenspaces to semantic meaning of layers and tokens.",
  "Meta-stable clustering": "As we discussed earlier, perhaps the most fascinating discovery of Geshkovski et al. (2023) is the existence of meta-stable clusters in the full-attention dynamics. It turns out that the same phenomenon persists in the causal-attentiondynamics that we study here. The dynamical evolution of the system is illustrated in . At t = 150, the initially uniform distribution of 200particles consolidates into seven distinct clusters. While Theorem 4.1 establishes the eventual collapse into a singlecluster, these intermediate clusters exhibit remarkable metastability, persisting with negligible movement over extendedtime periodsat least until t = 500 according to before sequential merging occurs. We define these meta-stableconfigurations as meta-stable clusters, with three-dimensional analogues shown in a and 1b. Given that the time parameter in our dynamics corresponds to network depth in transformer architectures, the meta-stable configurations, rather than final states (achieved at t = exp(())), hold greater practical significance. Theemergence of meta-stable clustering and its associated dimensionality reduction may provide fundamental insights intotransformers capacity for generating efficient context-dependent representations. From a theoretical perspective, understanding meta-stable clustering presents significant challenges, as traditionaltechniques for asymptotic analysissuch as those used in our Theorem 4.1prove insufficient. Recent work on full 4The spread of the clouds depends on the relative importance of each tokens own attention, that differs with variousK, Q. There are choices of K and Q that result in complex interactions without structure. For example, when QK =[, , ], V = I3",
  "(f) Time 500": ": Evolution of the system (CSA) with K = Q = V = I2 with n = 200, d = 2, = 64, strong Rnyicenters (red) and Rnyi centers (black) with = 41/2. Note that strong Rnyi centers are visually stationary (as perLemma 5.1) but do not explain all clusters. In turn, Rnyi centers are moving and merging (one disappears betweent = 75 and t = 150), but capture more meta-stable clusters. attention transformers has made partial progress in this direction. Geshkovski et al. (2024a) demonstrated that whenself-attention dynamics approach a nearly clustered state, they will converge to a tightly clustered configuration andremain stable for an exponential time period. Complementing this, Bruno et al. (2024) proved that tokens initializednear a uniform distribution on the sphere will spontaneously organize into a loosely clustered state. However, thebounds on the clustering tightness in this second line of work are not sufficient to trigger the convergence conditionsrequired by Geshkovski et al. (2024a)s theorem. This Section presents a fundamental discovery regarding the identification of cluster centers in causal-attention dynamics.We establish three key claims: First, we demonstrate that initialization irregularities generate distinctive tokens, termedRnyi parking centers, which evolve into meta-stable cluster nuclei. While this phenomenon is primarily supported bynumerical evidence (), it provides crucial insight into the clustering mechanism. Second, we prove that a subset ofthese special tokens, called strong Rnyi centers, maintains near-stationarity over extended time periods (Lemma 5.1).Both Rnyi and strong Rnyi centers occur with frequency (d1 2 ), confirming the scaling predicted for d = 2 byGeshkovski et al. (2023); see also Geshkovski et al. (2024a); Bruno et al. (2024). Third, we establish in Theorem 5.2that as t , all remaining tokens will converge to the vicinity of one of these stationary tokens, completing themeta-stable clustering process. This section restricts our analysis to the case where V = Id. For general matrices V , our empirical observationssuggest that particles rapidly converge to a lower-dimensional subspace spanned by d1 d principal eigenvectors.Consequently, we conjecture that the number of meta-stable clusters should rather be d11 2, where the ambientdimension d is replaced by the effective dimension d1. While a rigorous proof of this dimension-reduction remains anopen problem for future investigation, this phenomenon motivates our focus on low-dimensional cases (specificallyd = 2) throughout this section. For convenience, we also fix Q = K = Id, though this condition could be easily relaxed (e.g. to QK = KQ = V ).Under these assumptions, the system can be rewritten in polar coordinates xk = [cos(k), sin(k)] as",
  "By construction, the set of Strong Rnyi centers forms a subset of Rnyi centers": "As demonstrated in Section A.2, particles in our system exert maximal attractive force at distances of order at most1/2, with rapid decay beyond this scale. For strong Rnyi centers defined with separation parameter = c1/2(where c is sufficiently large), this decay ensures negligible influence from preceding particles and thus remain stablefor a long timea phenomenon formally established in Lemma 5.1. This metastability, coupled with rapid particleaggregation tokens, indicates that strong Rnyi centers serve as primary attractors for subsequent tokens. Rnyi centers are unaffected by previous particles but only by previous Rnyi centers, thereby generating new clusteringcenters. For fixed , there are more Rnyi centers than strong Rnyi centers (see Section C.4 for exact cardinalityanalysis). While Rnyi centers better capture the meta-stable clustering effect, as illustrated in Figures 3 and 2, theylack positional stability and may converge to other centers over time. Although Rnyi centers rapidly aggregate a largefraction of particles, some of these particles continue to migrate and eventually converge to strong Rnyi centers. The next result shows that strong Rnyi centers remain nearly fixed for a long time.Lemma 5.1. Let d = 2 and Q = K = V = I2. Consider a subsequence of strong Rnyi centers xs1, . . . , xsmsatisfying the separation condition with constants , c > 0",
  "maxt[0,Tj] |xsj(t) xsj(0)| < c1/2": "The key observation driving this result is that strong Rnyi centers are weakly affected by all previous particles.However, though this is correct on short time scales, it should be checked for all times in [0, T]. A complete proof canbe found in Section C.1.Remark. Using the properties of h derived in Section A.2 it can be shown that",
  "Tjsj < ec2/2c4/(24)": "Moreover, it is easy to prove that indexes sj are mostly small. Thus, we see that early strong Rnyi centers are almoststationary for a time that is exponential with the square of separation magnitude. Rnyi centers and strong Rnyi centers play a fundamental role in meta-stable clustering, warranting analysis of theirproperties as extreme points in a sequence. While defined here using geodesic distance on a sphere, the definitionextends naturally to distances induced by Qx, Ky under appropriate conditions. This generalization aligns with ourobservation that meta-stable clustering occurs in the subspace L where V sends tokens and acts as the identity on L. The distribution of these centers under various initialization schemes presents a key analytical challenge. Section C.4addresses the uniform i.i.d. case, where Rnyis classical result characterizes the expected number of centers. Extensionsto general distributions and Markov processesmore relevant to language processing applicationsremain open forRnyi centers due to their structural complexity, particularly in higher dimensions (d > 2). In contrast, strong Rnyicenters are much easier to handle: even our computation of the average number of centers in Section C.4 works for anydistribution regardless of the dimension.",
  "Fixed Meta-stable Clustering Centers": "Having established the existence of O() quasi-stationary tokens for d = 2 and n 1, we next examine their role ascluster centers. While Figures 3 and 2 provide substantial numerical evidence that these tokens attract and aggregatenearby particles, a rigorous proof remains elusive. We establish instead a weaker result: when quasi-stationary tokensare artificially frozen (analogous to cross-attention in encoder-decoder architectures), all other tokens converge to thesefrozen centers. This simplified model, while instructive, differs from true meta-stable clustering in important aspectsdetailed in .",
  "We only state our result for d = 2 and identity parameter matrices as in (CSA-2d)": "Theorem 5.2 (Clustering to frozen tokens for K = Q = V = I2). Let 1, . . . , m be fixed tokens that are well-separated, namely |i j| > c1/2. Let 0 be an absolutely continuous probability measure on (S1)n and let1(0), . . . , n(0) 0. Consider causal attention dynamics (CSA-2d), with additional influence from the fixed tokensj, which enter evolution with additional weights aj 1. Specifically, we have",
  "k [n], j [m] : |k j| < 1/2": "Since our dynamical system is not a gradient flow, the classical ojasiewicz convergence theorem does not apply.Instead, we establish convergence by observing that the causal dynamics (both with and without frozen tokens) is, infact, a sequential gradient flow, where each particle minimizes a slightly different energy potential. For such systems onS1, we demonstrate convergence through an alternative approach that circumvents the ojasiewicz framework.",
  "Limitations": "Our analysis presents both theoretical and practical limitations. From a theoretical perspective, we establish twokey results: (1) strong Rnyi centers with separation distance c1/2 maintain quasi-stationarity for time scales oforder exp(c2/2) per Lemma 5.1, and (2) exactly stationary centers attract all remaining particles (Theorem 5.2). Inparticular, by choosing c , we can get exponential in time of quasi-stationarity. However, this falls short ofproving meta-stable clustering, as Theorem 5.2 provides no bounds on the convergence time. Consequently, we cannotguarantee that strong Rnyi centers remain sufficiently stationary during particle aggregation. A complete meta-stabilitytheory would require demonstrating that each Rnyi center captures (n) particles in O(1) time as n . Currently,even the weaker claim of capturing (1) particles remains unproven, presenting a crucial direction for future research. The practical limitations stem from two model simplifications: the use of tied weights across layers (though supportedby successful implementations, see Lan et al. (2020)), and the omission of the MLP layer central to Transformerarchitectures. Incorporating the MLP dynamics into our theoretical framework remains a significant open challenge. The work of NK and YP was partially supported by the MIT-IBM Watson AI Lab and the National Science Foundationunder Grant No CCF-2131115. PR is supported by NSF grants DMS-2022448 and CCF-2106377, and a gift fromApple.",
  "Abdalla, P., Bandeira, A. S., Kassabov, M., Souza, V., Strogatz, S. H., and Townsend, A. (2022). Expander graphs areglobally synchronising. arXiv preprint arXiv:2210.12788": "Abramson, J., Adler, J., Dunger, J., Evans, R., Green, T., Pritzel, A., Ronneberger, O., Willmore, L., Ballard, A. J.,Bambrick, J., Bodenstein, S. W., Evans, D. A., Hung, C.-C., ONeill, M., Reiman, D., Tunyasuvunakool, K., Wu, Z.,emgulyte, A., Arvaniti, E., Beattie, C., Bertolli, O., Bridgland, A., Cherepanov, A., Congreve, M., Cowen-Rivers,A. I., Cowie, A., Figurnov, M., Fuchs, F. B., Gladman, H., Jain, R., Khan, Y. A., Low, C. M. R., Perlin, K., Potapenko,A., Savy, P., Singh, S., Stecula, A., Thillaisundaram, A., Tong, C., Yakneen, S., Zhong, E. D., Zielinski, M., dek,A., Bapst, V., Kohli, P., Jaderberg, M., Hassabis, D., and Jumper, J. M. (2024). Accurate structure prediction ofbiomolecular interactions with alphafold 3. Nature.",
  "Castin, V., Ablin, P., and Peyr, G. (2024). How smooth is attention? In International Conference on Machine Learning": "Chang, H., Zhang, H., Barber, J., Maschinot, A., Lezama, J., Jiang, L., Yang, M.-H., Murphy, K. P., Freeman, W. T.,Rubinstein, M., Li, Y., and Krishnan, D. (2023). Muse: Text-to-image generation via masked generative transformers.In Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S., and Scarlett, J., editors, Proceedings of the 40thInternational Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages40554075. PMLR.",
  "Strogatz, S. H. (2000). From Kuramoto to Crawford: exploring the onset of synchronization in populations of coupledoscillators. Physica D: Nonlinear Phenomena, 143(1-4):120": "Team, C. (2024). Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818.Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, ., and Polosukhin, I. (2017).Attention is all you need. Advances in Neural Information Processing Systems, 30. Vuckovic, J., Baratin, A., and des Combes, R. T. (2021). On the regularity of attention. arXiv preprint arXiv:2102.05628.Wu, X., Ajorlou, A., Wu, Z., and Jadbabaie, A. (2023). Demystifying oversmoothing in attention-based graph neuralnetworks. In Oh, A., Naumann, T., Globerson, A., Saenko, K., Hardt, M., and Levine, S., editors, Advances in NeuralInformation Processing Systems, volume 36, pages 3508435106. Curran Associates, Inc. Yu, J., Xu, Y., Koh, J. Y., Luong, T., Baid, G., Wang, Z., Vasudevan, V., Ku, A., Yang, Y., Ayan, B. K., Hutchinson,B., Han, W., Parekh, Z., Li, X., Zhang, H., Baldridge, J., and Wu, Y. (2022). Scaling autoregressive models forcontent-rich text-to-image generation. Transactions on Machine Learning Research. Featured Certification.",
  "The derivation of the equation (SA) was thoroughly described in Geshkovski et al. (2023), but for completeness, webriefly repeat it here to explain how the problem arises": "In general, a typical Transformer architecture consists of repeated layers of multi-head attention, multi-layer perceptrons(MLP), normalization, and residual connections Vaswani et al. (2017). In this work, we simplify this setting by focusingonly on the geometric behavior of a single-head attention layer with normalization and residual connections, omittingthe MLP for brevity. One head of a standard attention layer is defined as follows. Given an input sequence represented by the tokenembeddings X Rnd, where n is the number of tokens and d is the dimension of the embedding space, and matricesWQ, WK, WV to compute queries, keys, and values, the attention mechanism computes a weighted sum of valuesbased on their relevance to a query in the following form",
  "Xt+1 = Xt + RMSNorm(Attention(Xt)).(4)": "Here, different tokens are represented as rows of the matrix X for computational reasons. For consistency with theconvention that vectors are represented as columns, we transpose everything and denote a sequence of tokens encoded asparticles in the d-dimensional embedding space Rd as (x1, . . . , xn), corresponding to the columns of X. Additionally,to simplify the notation we denote V := W V , Q := W Q , and K := W K, and introduce an arbitrary temperatureparameter instead of the fixed scaling factor 1/",
  "j=1eQxk,Kxj": "The equation (4) can be interpreted as a discrete derivative, with Xt+1 Xt representing the difference between layers.Therefore, the trajectory Xt can be viewed as a discretization of a continuous flow. RMS normalization ensures thattokens remain on the scaled unit sphere, but from properly rescaling Q, K, V we can assume that they stay on thestandard unit sphere Sd1. Combining all these observations, the dynamics of token propagation through layers can beexpressed as:",
  "j=1ckj kj = x(0)": "For almost all initial conditions x(0) with respect to the surface measure on the sphere, all coefficients ckj are non-zero.For complex eigenvalues k, we combine conjugate terms to obtain real-valued solutions involving trigonometricfunctions. The asymptotic behavior follows from two observations: (i) Terms with largest (k) dominate as t , correspond-ing to convergence to L Sd1 at exponential rate, and (ii) Among these terms, those with highest power of t (i.e.,tnk1k1 terms) determine the slower convergence to L Sd1",
  "where we omit t from the notation for simplicity": "This system is autonomous, so we first explore its critical points and their stability. For autonomous systems withestablished convergence, it is well-known that for any absolutely continuous initialization, the limiting point is stronglyunstable with probability zero (see (Shub, 2013, Thm. III.7, Ex. III.3) and (Geshkovski et al., 2023, Lemma B.1)). Notethat the proof in Geshkovski et al. (2023) is stated for gradient ascent dynamics but it readily extends to any smoothautonomous dynamics on a compact Riemannian manifold.",
  "Indeed, if the first condition fails, consider the first token xs where xs = x1. Then fs(x) = 0 implies xs xs, = 0,forcing xs = so that xs = since we required xs = x1": "Our goal is to show that stationary points of the second kind are limiting points with probability zero with respect to theinitialization distribution. Observe that since the system formed by the first s particles is independent of subsequentones, it suffices to show:P((x1, . . . , xs1, xs) (, . . . , , )) = 0.",
  "This linear operator acts on and has eigenvalues w(, ) with multiplicity d 1, which are all real positive,confirming strong instability": "By the center-stable manifold theorem (Shub, 2013, Thm. III.7, Ex. III.3), if a point has at least one eigenvalue withpositive real part, then: (i) The center-stable manifold W csloc has positive co-dimension and (ii) Points converging to thisequilibrium must enter W csloc at some finite time and (iii) the set of such points has measure zero.",
  ". The majority of heads exhibit real maximal eigenvalues max, validating our theoretical emphasis on the realcase": "2. Notably, two heads1 and 6display negative max. This is particularly interesting since standard Gaussianinitialization (see ) places the initial spectrum far from the left half-plane. The emergence of negativemaximal eigenvalues through training suggests that such configurations may serve specific functional purposes. These observations have important implications for transformer architecture design, particularly regarding the initializa-tion of value matrices V . The presence and apparent utility of negative max indicates that alternative initializationschemes, beyond standard Gaussian, might better accommodate the learned spectral properties.",
  "nE(1, . . . , n) <": "is inside a collection of distinct intervals around 1, . . . , m for small enough . Therefore, due to (10), from somemoment n(t) stays only in one of those intervals. They collapse into points as we take 0 and t , provingthat n converges to some n E. It remains to prove that observed convergence is to a strongly stable point. We know that the limiting point is critical and for almost any initialisation it is not strongly unstable. This is awell-known fact that for autonomous systems convergence to a strongly unstable point happens with probability zero,that we used in Section B.2. One could find it in (Geshkovski et al., 2023, Lemma B.1), based on the center manifoldtheorem (Shub, 2013, Thm. III.7, Ex. III.3). Therefore, from the assumption on critical points, the limiting pointis strongly stable. Then, the convergence happens exponentially fast, because locally the whole neighbourhood of astrongly stable point is its stable manifold W sloc, see (Shub, 2013, Thm. III.7, Ex. III.3).",
  "j=1aje(cos(kj)1)": "Therefore, if conditions of Lemma 5.3 are satisfied, we have convergence to a strongly stable critical point. In thatcase, it is sufficient to prove that for any strongly stable critical point , all particles k are 1/2-close to one of thecenters j. We show this via induction on k, because new particles do not affect the movement of the previous ones. To justify our use of Lemma 5.3, we need to check that all critical points are either strongly stable or strongly unstable,i.e. that the Jacobian at any critical point with only non-positive eigenvalues actually has no zero eigenvalues. Moreover,for our conclusion, we need to check that if all the eigenvalues are negative, then the critical point is clustered around .Let us start with that.",
  "j=2ajh(k j)": "All of the terms on the left hand side have the same sign, because j, j S are 1/2-close to 1 and k is( + 1/2)-close to 1. Any other particle/center is at least (c 2)1/2 far from k, because centers arec1/2-separated. The absolute value of the l.h.s. can be lower bounded",
  "strong Rnyi centers is Xsj, j 1 such that j mini<sj dist(Xsj, Xi) >": "Estimating the number of elements in the first sub-sequence is well-known as famous Rnyi parking problem Renyi(1958). In the case d = 2, the result from Dvoretzky and Robbins (1964) implies that as 0, the average number ofelements in the sequence approaches c2/ superexponentially fast, where c 0.75 is the Rnyi constant. However,this problem becomes significantly harder in higher dimensions. In contrast, it is easy to compute the average numberof elements in the second sequence in any dimension and even for a wider class of distributions.Lemma C.1. In an infinitely long sequence Xi the average number of variables Xsj chosen by strong Rnyi parkingis the inverse spherical cap surface area 1/d1(B). In particular, it grows as 1/d1 with dimension and can becomputed directly in lower dimensions"
}