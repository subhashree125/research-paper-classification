{
  "Abstract": "This paper makes a step towards modeling the modality discrepancy in the cross-spectral re-identification task. Based on the Lambertain model, we observe that thenon-linear modality discrepancy mainly comes from diverse linear transformationsacting on the surface of different materials. From this view, we unify all dataaugmentation strategies for cross-spectral re-identification by mimicking suchlocal linear transformations and categorizing them into moderate transformationand radical transformation. By extending the observation, we propose a RandomLinear Enhancement (RLE) strategy which includes Moderate Random LinearEnhancement (MRLE) and Radical Random Linear Enhancement (RRLE) topush the boundaries of both types of transformation. Moderate Random LinearEnhancement is designed to provide diverse image transformations that satisfy theoriginal linear correlations under constrained conditions, whereas Radical RandomLinear Enhancement seeks to generate local linear transformations directly withoutrelying on external information. The experimental results not only demonstratethe superiority and effectiveness of RLE but also confirm its great potential as ageneral-purpose data augmentation for cross-spectral re-identification. The code isavailable at",
  "Introduction": "Identity recognition has attracted intensive attention in the last few years due to its wide applications insurveillance systems . Since silicon-based digital cameras are naturally sensitive to near-infrared (NIR), most cameras provide infrared (IR) images instead of visible (VIS) images for bettervisual quality under poor illumination conditions. In practice, this puts the re-identification (Re-ID)problem in a cross-spectral setting and requires the approaches to properly handle both the intra-classvariance and the more significant modality discrepancies between cross-spectral images .Encouraged by the great success of single-modality re-identification, substantial research effortsin cross-spectral re-identification attempt to transform the cross-spectral re-identification challengeinto a single-modality learning task. To achieve this goal, previous efforts utilize DNN-basedimage processing such as Generative Adversarial Networks (GANs) , to construct the translationfrom one spectrum to another. These methods generally provide good visual effects andadjustability. However, the limited visual quality of the generated images and the lack of large-scaledatabases providing cross-spectral image pairs make GAN training challenging, thus limiting theperformance of these methods. Another mainstream strategy focuses on the channel difference",
  "arXiv:2411.01225v1 [cs.CV] 2 Nov 2024": "between infrared images . Methods such as grayscale transformation and random channelselection attempt to use image transformation strategies to mimic the transformation between cross-spectral images, thereby pushing the network to adapt to such a transformation. While thesemethods make sense and decrease the modality discrepancy, lacking the modeling of cross-spectraltransformation, they usually tend to pursue the similarity in human visual perception rather than realcross-spectral transformation. 0.05 0.1 0.15 0.2 0.25 0.3 00.10.20.30.4 Channl G NIR Image",
  "G Channel ImageNIR ImageG-NIR Scatter Plot": "0.55 0.6 0.65 0.7 0.650.750.850.95 Channl G NIR Image : Illustration of the cross-spectral transformation.G refers to the green channel of the visible image. Un-der the same illumination, the cross-spectral transformationcould be described as a linear transformation in a material-similar surface. Still, in the whole image level, the transfor-mation is nonlinear due to the diversity of materials. Sincethe Re-ID image pairs are not well aligned, we select thecross-spectral image pairs from . In this paper, we attempt to explorethe possibility of modeling the multi-spectral transformation to providemore interpretability, and thus furtherpush the boundary of cross-spectralRe-ID approaches. Based on the Lam-bertian reflection model ,we find that the illuminations of thesame region in VIS and NIR photosshould be able to be described us-ing a simple linear model, as long asthe region is composed of one consis-tent material (details are discussed inSec. 3). This is illustrated in .Here, we use paired VIS-NIR imagesfrom the dataset in . For the red andyellow regions in the middle of the im-age, with a simple linear model, wecan accurately predict the pixel valuesof the NIR image based on the VISimage, as long as the region only hasone material. Although the linear transformation exists at the pixel level of cross-spectral imagepairs, the materials reflection function determines the linear factor. It means that the linear factoris inconsistent across different surfaces, resulting in an image-level non-linear transformation. In, we analyze and visualize the result to confirm whether the different linear factors ondifferent surfaces are the main culprit that induces the modality discrepancy in the cross-spectralimages. It is interesting to find that the modality discrepancy occurs when using variable linear factorsamong different patches in the image. The above observation provides us with a fresh perspective on the cross-spectral Re-ID task. Em-pirically, adopting observation in image generation seems to be the most intuitive way. As long aswe are able to identify regions materials with their visible or infrared input and calculate the linearcoefficients to transform the input image from one spectrum to another, the modality discrepancywould be easy to bridge. Unfortunately, the correlation between visible or infrared input and regionalmaterials is quite limited, which also confines generative strategies in this task to a clear upper bound.Besides exposing the bottleneck of the generative strategy, the observation also provides a unifiedperspective to rethink the augmentation strategies within this topic. From this perspective, we dis-cover that data augmentation for cross-spectral re-identification is formed to achieve non-lineartransformations with different distinct local linear factors, thus encouraging the network to berobust to such a transformation. Therefore, under this view, we can easily categorize all the dataaugmentation strategies designed for cross-spectral re-identification into moderate transformation andradical transformation based on the extent of changes to images. We assign moderate transformationas a strategy that can still keep the original linear correlation after the transformation. Generally,achieving moderate transformation may require precise material labels on each pixel. However,with benefits from the diversity of different channels in visible images, we can obtain a moderatetransformation by a linear calculation based on the original image channels. Methods like channelexchange and grayscale transformation are both special cases under moderate transformation. Withinthe unified formulation of moderate transformation, in this paper, we further provide a more generalmoderate transformation as Moderate Random Linear Enhancement (MRLE), which aims to use anunfixed mixing of different channels to provide more diverse augmentation results. In contrast tomoderate transformations, radical transformations attempt to apply linear transformations to randomlyselected local areas. Compared to moderate transformations, which have a limited transformationspace and are only effective on multi-channel visible images, radical transformations can produce a more diverse range of results even on single-channel infrared images. However, due to the lack ofconstraints, these transformations often introduce additional noise into the original image. Methodssuch as random erasing and channel random erasing can be considered special cases ofradical transformation where the linear factor is set to 0. Similarly, based on the above perspective, wealso provide a Radical Random Linear Enhancement strategy, that yields competitive augmentationresults by directly applying linear transformations to randomly selected local areas.",
  "In summary, our contributions are threefold:": "As an effort to model the transformation behind the modality discrepancy in the cross-spectral Re-ID task, we discover that the cross-spectral modality discrepancy mainly comesfrom different local linear transformations caused by the diversity of materials. Based onthis observation, we further categorize the cross-spectral data augmentation strategies intomoderate and radical transformations under a unified perspective. By extending the observation, we propose a Random Linear Enhancement (RLE) strategy,which includes Moderate Random Linear Enhancement (MRLE) and Radical RandomLinear Enhancement (RRLE). The RLE effectively takes advantage of the aforementionedunified perspective and embeds it in a controllable linear transformation.",
  "Related Works": "Cross-spectral re-identification is a challenging task due to the significant modality discrepancy. Twotypical frameworks have been proposed to solve such a challenging task. The first one is feature-levellearning , which aims to bridge the modality gap through well-designed lossfunctions and end-to-end training. Such a strategy works well in both supervised, semi-supervised,and unsupervised cross-spectral re-identification tasks due to the great power of deep learning.However, these approaches usually do not use any real physics models, making it not uncommonfor them to make strange mistakes. To make things worse, due to the high complexity and lackof interpretability, the models are hard to adjust or improve. The other mainstream method tosolve cross-spectral re-identification is the image-level strategy, which aims to construct an efficienttransformation between different spectrums. Under this condition, the cross-modality discrepancy isconsidered an individual problem alongside the Re-ID problem. D2RL makes the first attemptby using variational autoencoders (VAE) for style disentanglement and generates synthesis imagesfrom one spectrum to another. AlignGan improves this framework by proposing a unified GANframework with efficient constraints. Although playing a min-max game between the complexgenerator and discriminator offers visually impressive results, the generated images are still far fromphotorealistic, and in turn, limit the final performance. Therefore, these methods were subsequentlysuperseded by lighter-weight modality generate strategies. This improvement suggests that cross-spectral transformations may not be as complicated as previously envisaged. X-modality designsa lightweight network to learn an intermediate mediator from visible images, while MMN improves it by extending an infrared side. Recently, CAJ and CAJ+ directly removedthe extra generator and utilized several types of grayscale images as an assistant for training whichalso achieves satisfying performance. Although recent methods have made some progress in thistopic, due to the lack of analysis and modeling for cross-spectral transformation, the methods tendto pursue the similarity of transformation in human visual perception rather than real cross-spectraltransformation.",
  "Reflection Prior for Cross-Spectral Images": "VIS-NIR matching is a longstanding computer vision problem that has been explored for decades . One of the main challenges is to formulate and thus alleviate the modality discrepancy. Usingthe Lambertian model to analyze the digital image from multi-sensor cameras is widely applied insome pioneer works . With a light source emitting photons across different wavelengths , the VISNIRR-NIRG-NIRB-NIR : Example images from the VIS-NIR scene dataset . After we divide the visible imageinto the red, green, and blue channels and form chromaticity band ratios from these three spectra andthe NIR image, it is clear that the ratio for pixels from the surface with high material-similarity isnearly constant.",
  "(a)(b)(c)": ": A example about how modality discrepancy occurs. Feature space visualization of 100randomly selected images with (dot) and without (fork) the local linear transformation on the originalimage. (a)(b): The same linear factor takes effect on the whole image bringing limited modalitydiscrepancy. (c): Variable linear factors take effect on different parts showing a huge modalitydiscrepancy. The cross and dot marks indicate the samples from the original one and the generatedone respectively.response of each pixel (x, y) in the camera sensors can be formulated as:",
  "jEj(, x, y)S(, x, y)Qj()d,(1)": "where is the wavelength, as well as E() and S() denote the spectral power distribution (SPD)of incident light and surface spectral reflectance. Q() is the spectral sensitivity of the camerasensor. j = {R, G, B, N} indicates the channel (spectrum). (x, y) is the Lambertian reflectionterm which is a constant factor and can be calculated by the dot product of the surface normal withthe illumination direction. Following Eq. (1), we leverage a mild assumption to derive a representation between the SPD ofthe light source and incident light. Generally, we could describe the SPD of the light source by arelative spectral power distribution F(, x, y) together with a variable that reflects the illuminationintensity. We assume that the SPD of incident light in the whole image keeps the same relativespectral power distribution as the light source. Then we could formulate the E(, x, y) as:",
  "Ej(, x, y) = j(x, y)jFj(),(2)": "where is a parameter to reflect the ratio of intensity between the incident light and the light source.Then from Eq. (1) and Eq. (2), if we now consider the images under different spectra, such as Gchannel images and NIR images, it is clear that the transformation of G-NIR could be described as:",
  "(a) Definite linear transformation on Definite image patch.(b) Random linear transformation on Definite image patch.(c) Random linear transformation on Random image patch": ": The motivation of RLE. Herein, we construct an ideal person with only two differentsurfaces and ignore the background. (a): As demonstrated above, to obtain a spectral-invariant featurerepresentation, the network should be robust to such a transformation that takes effect upon definitesurfaces by definite linear factors. (b): An ideal data augmentation strategy that takes effect upondefinite surfaces by random linear factors. However, this method needs a hard-achieved extra material-aware network for segmentation. (c): The idea of RRLE. By taking effect upon random surfacesby random linear factors, the RRLE encourages the network to be robust to a linear transformationanywhere in the image. Under this condition, the cross-spectral transformation can be considered asan easy state of RRLE space. Since F(), Q(), and S() are three inner functions depending on the SPD of the light source,the sensitivity of the camera sensor, and the reflection function of the surface material, we replacethe Riemann integral with a function M(x, y, j). In addition, N G could be considered as a constantfactor in two determined spectra. From this representation, one could observe that the cross-spectraltransformation is a linear transformation in those regions of the same material and under the sameillumination condition, as shown in . If we further extend it to the entire image, the factor isonly influenced by S(, x, y) which is determined by the material. To verify whether the above equation could be used in various real-world scenarios, we used thepaired VIS-NIR scene image dataset introduced by . In , we form chromaticity band ratiosbetween three VIS spectra and NIR spectrum at each pixel and use the color to reflect the ratio. Wediscovered that the ratio is nearly constant within a region with a consistent material, which holdsacross R, G, B, and NIR spectra. After observing the above linear transformation, we further explore whether the variable linear factorin different surfaces is the main culprit that induced the modality gap in such an application. Due tothe lack of material labels that are available to guide sample generation, we uniformly segmented100 randomly selected images into six parts from the top to the bottom and multiplied each part by alinear factor. Then, we send the new images and original images into an ImageNet pre-trainedResnet-50 . Although not so well-aligned, benefiting from the body structure prior from head totoe, we still find that the modality discrepancy occurs when suffering from variable linear factors.",
  "Random Linear Enhancement": "From the above observation, we can unify all data augmentation strategies for cross-spectral re-identification as mimicking such a local linear transformation, thereby encouraging the network tobe robust to transformation, as shown in (a). Based on this perspective, by considering theinfluence on the original image, the data augmentation strategies can easily be categorized into twotypes: moderate transformation and radical transformation. By extending the observation, this paperpushes the boundary of both types by proposing a Random Linear Enhancement strategy (RLE).",
  "Imt = rIr + gIg + bIb,s.t.r + g + b = 1,(5)": "where the Imt indicates the transformed image, as well as Ir, Ig, and Ib refer to the red, green, andblue channels of the visible image, respectively. Here, r, g, and b are hyper-parameters to controlthe mixing percentage. It is evident from Eq. (5) that random channel selection corresponds to thespecific cases where the parameters r, g, and b are specified as , , or . Also,the grayscale transformation is the specific cases where the parameters r, g, and b are specifiedas [0.299, 0.587, 0.114]. It is apparent that previous strategies exhibited significant limitations in their parameter settings,leading to highly restricted augmentation results. Therefore, we attempt to relax the settings of r,g, and b and propose a Moderate Random Linear Enhancement (MRLE). Generally, samplingfrom a uniform distribution to determine the values of three hyper-parameters is identified as thesimplest and most efficient approach. However, while a uniform distribution uniformly covers theentire feasible transformation domain, in practice, those samples at the boundaries always contributemore to learning decision boundaries. Consequently, we employ a U-shaped beta distributioninstead of a uniform distribution for hyper-parameter sampling. This not only maintains the feasibletransformation domain but also enhances the sampling probability of boundary samples. In general,the formulation of MRLE can be given as follows:",
  "Radical Random Linear Enhancement": "Although MRLE can provide diverse transformation results that obey the original linear correlationin the image, it can only take effect on the multi-channel visible image and shows quite limitedtransformation space. The ideal data augmentation appears to be using random linear factors ondifferent surfaces as shown in (b). However, this approach heavily relies on pixel-levelmaterial labels, which are hard to obtain. Therefore, achieving such a local linear transformationwithout adequate material labels may inevitably involve some risk-taking. To achieve this goal, asshown in (c), we propose the Radical Random Linear Enhancement (RRLE) that randomlyselects several image patches and multiplies them with a variable linear parameter to directly mimic thelocal linear transformation. Under the RRLE, the cross-spectral transformation could be consideredas a sub-state of the whole state space. Concretely, for an input image I, the RRLE randomly selects a rectangle region Iselect following thesame setting of random erasing and multiplies it with a linear factor . In case it may exceed theupper bound, we calculate the maximum feasible linear factor in Irle as max. The linear factor is calculated by multiplying max and a random factor fg between 0 to 1. Typically, small linearfactors may not be sufficient to effectively provide enough variation in the original image2. Therefore,a U-shaped Beta distribution is utilized for fg to obtain and provide high-quality training samples. Ingeneral, the formulation of RRLE can be given as follows:",
  "Visualization results are provided in the appendix": "times to obtain a higher modality discrepancy. Since the repeat will bring extra noise, we set amemory matrix M to store the cumulative changes at each pixel. We set a tmin to terminate theRRLE when min(M) < tmin. To better explain the processing, we provide detailed procedure ofRRLE in the appendix.",
  "We conduct experiments on two publicly available visible-infrared person re-identification datasetsSYSU-MM01 and RegDB": "SYSU-MM01 is a large-scale dataset captured by four visible cameras and two infrared cameras inboth indoor and outdoor environments. The training set contains 395 identities with 22, 258 visibleimages and 11, 909 infrared images, while the testing set includes 96 identities with 3, 803 infraredimages as the query. This dataset contains two different search modes, the all-search mode and theindoor-search mode. In the all-search mode, the gallery images are from all the visible cameras. Forthe indoor-search mode, the source of the gallery set excludes two outdoor cameras. RegDB dataset is collected by two aligned cameras, one for visible and the other for far-infrared(thermal). It contains 412 identities, each with 10 visible images and 10 infrared images. Followingthe evaluation protocol of previous works , we choose half of the identities at random fortraining and the other half for testing. The results are the average of 10 repeating. We follow the evaluation settings in existing VI-ReID methods and adopt the CumulativeMatching Characteristic (CMC), mean Average Precision (mAP) and mean Inverse Negative Penalty(mINP) as evaluation metrics.",
  "Implementation details": "We use Pytorch to implement our method and finish all the experiments on a single RTX 3090 GPU.The mini-batch size is set to 48. For each mini-batch, we randomly select 4 identities, each with6 visible images and 6 infrared images. We select the ResNet-50-based PCB with the globalbranch as the baseline, which is a widely used fine-grid part feature learning framework in bothRe-ID and visible-infrared Re-ID . We also divide the first convolutional layer to tackle thetwo modalities input as usual . We resize all of the images to 384 192 and use randomflipping as basic data augmentation. The initial learning rate is set to 0.1, and decayed by 0.1 and0.01 at 20, and 50 epochs. Following previous works, we apply a warm-up strategy inthe first 10 epochs. To better verify the ability of the proposed data augmentation strategy, we justuse the basic softmax cross-entropy loss and triplet loss during the training without adding any extraconstraints to solve the modality discrepancy.",
  "Ablation Study": "In this section, we conduct empirical experiments to show the performance under different data aug-mentation strategies. Since we have categorized all data augmentation strategies for cross-spectrumre-identification into moderate transformations and radical transformations, we have conductedrelevant discussions on these two aspects and mixed transformations. Moderate transformation. Here, we evaluate the influence of the performance with differentmoderate transformation strategies and show the quantitative results in . In particular, wecompare the proposed MRLE with the widely used grayscale transformation (Gray) and randomchannel selection (RC). In previous works , the Gray and RC are usually used togetherto obtain more diverse results. Therefore, we also give the result under both Gray and RC. Compared to the baseline, every moderate transformation yielded positive gains showing the effective-ness of moderate transformation. However, although methods such as RC and Gray do simulatecross-spectrum transformations to a degree, their restricted transformation spaces result in smallerperformance enhancements compared to MRLE. By fully exploring the feasible transformation space,MRLE managed to surpass the limits of earlier moderate transformation strategies, achieving asignificant increase in performance in all metrics. : Ablation study of different data augmentation strategies on the cross-spectral re-identificationtask. Gray denotes the grayscale transformation, RC refers to the random channel selection.MRLE indicates the moderate random linear enhancement. RE refers to the random erasing, andRRLE means the radical random linear enhancement.",
  "Mixed TransformationBaseline+CAJ 73.592.997.499.469.455.480.796.198.699.883.579.8Baseline+RLE+RE75.493.597.799.672.460.984.797.999.399.987.083.7": "Radical transformation. Besides the moderate transformation, we also provide a detailed empiricalstudy of the radical transformation including random erasing (RE) and RRLE in . We canobserve that due to a more flexible transformation space, radical transformations reach an even betterperformance than the best moderate transformation MRLE. Although RE can be considered a special case of RRLE with a linear factor of 0, RRLE encouragesimages to undergo more transformations while preventing the loss of information. Therefore, RRLEand RE tend towards different valuation perspectives and can be used together. As shown in ,the peak performance under radical transformation is reached when combining both RE and RRLE. Mixed transformation. Given that moderate and radical transformations do not conflict formally,they can be combined during the training. Accordingly, we present the performances under amixed transformation in . The results indicate that moderate transformations and radicaltransformations can be used simultaneously and lead to significant performance improvements. Also,for better comparison, we add the recently proposed mixed transformation CAJ which combines thegrayscale transformation and random erasing strategy. It is shown that the proposed RLE also worksbetter than the CAJ in cross-spectral re-identification task.",
  "Comparison with State-of-the-Arts": "In , we combine the RLE+RE with a basic framework and evaluate it against the previouslyreported state-of-the-art methods on the SYSU-MM01 and RegDB. Compared to previous works,it is worth noticing that the basic network doesnt have any extra modules or constraints to copewith the modality discrepancy in cross-spectral re-id. Just combining the basic network with theRLE+RE can achieve comparable performance with state-of-the-art methods, which indicates thegreat adaptability of RLE in the cross-spectral re-id task.",
  "Discussion": "Hyper-parameter settings of RLE. RLE contains several hyper-parameters to ensure its effective-ness, such as the m in Eq. (6), as well as the r and tmin in Eq. (7). Therefore, this part evaluatesthe performance under different hyper-parameter settings and shows the results in . Compared to a uniform distribution, sampling from a U-shape beta distribution performs better inMRLE. The optimal reach when m is set to 0.3. For RRLE, as a radical transformation, the boundaryis more sensitive. Over-transformation may easily destroy the original image. In this framework,peak performance is achieved when r = 0.4 and tmin = 0.1. Applicability of RLE for other methods. Besides the basic framework employed above, we alsoexplored the integration of the proposed RLE strategy with the current state-of-the-art method toevaluate its extensive adaptability in cross-spectral re-identification tasks.",
  "ViT-B 66.063.169.975.1+Ours70.2(+4.2)66.7(+3.6)71.9(+2.0)76.4(+1.3)": "Herein, we add the RLE in the open-sourcedmethod DEEN and show the result in Ta-ble 4. Specifically, since the DEEN already con-tains the random grayscale and random erasingfor data augmentation, we remove the randomgrayscale and add the RLE. Although the DEENalready contains strong augmentations, addingRLE can also bring a performance gain. Beyondthe CNN models, we also investigated whetherRLE could be applied to a ViT-based structure.Since there is no open-source ViT model for cross-spectral re-id, we use the vanilla ViT-B withrandom erasing augmentation as the basic framework in this part. From , we can observethat RLE can still work well in a ViT structure. To ensure the generalization ability of RLE, whenapplying it to other methods, we keep the same hyperparameters setting of RLE with the previousexperiments. Therefore, better performance may be achieved on specific methods by fine-tuning thehyperparameters. Visualization results of RLE. To gain a deeper understanding of RLE processing, we visualize theRLE augmented images from both the visible and infrared sides in . It can be seen thatMRLE provides an efficient way to provide diverse transformations from multi-spectral images to",
  "VisibleMRLERRLERLEInfraredMRLERRLERLE": ": Visualization results of RLE. Since the MRLE can not take effect on the infrared images, weuse instead. Meanwhile, Both MRLE and RRLE are used with a certain probability. Therefore,all of the augmentation images above are potential results. single-spectral images, while the RRLE gets rid of the dependence on the multiple spectral imagesand makes such a linear transformation directly on the local part. In general, adding such a randomlinear transformation in the local area of the images largely breaks the color information of the imagewhile preserving the semantic.",
  "Limitations and Broader Impact": "Based on the specific observation in the cross-spectral re-identification, the proposed RLE may notbe as general as a data augmentation strategy like random flipping. Whether breaking the modality-similarity between the image pairs could make sense in other computer vision tasks still needs to beevaluated. Meanwhile, under extremely bad weather, such as heavy rain, fog, or limited illumination,the Lambertain model may not work well. So, whether RLE can still perform well in these complexweather is ambiguous. On the other hand, the RegDB and SYSU-MM01 datasets are limited inscale and environment. Although the proposed RLE shows a strong ability to boost the methodsin both two datasets, the performance of RLE in an open-world scenario has not yet been verified.Nevertheless, we still believe that the proposed RLE can boost the research of image generation anddata augmentation on more general cross-spectral scenarios.",
  "Conclusion": "This paper provides a unified perspective on data augmentation strategies for cross-spectral re-identification. We observe the non-linear modality discrepancy mainly comes from the diverse lineartransformation taking effect on different material surfaces; all data augmentation strategies for cross-spectral re-identification aim to simulate this kind of transformation. By extending the observation,we introduce a more general augmentation Random Linear Enhancement (RLE), further pushingthe boundary of moderate transformation by Moderate Random Linear Enhancement (MRLE) andradical transformation by Radical Random Linear Enhancement (RRLE). Experimental results showthat RLE is effective and applicable in cross-spectral re-identification tasks. Acknowledgements. This work was supported by the National Key R&D Program of China(No.2022ZD0118202), the National Science Fund for Distinguished Young Scholars (No.62025603),the National Natural Science Foundation of China (No.U21B2037, No. U22B2051, No. 62176222,No. 62176223,No. 62176226, No. 62072386, No. 62072387, No. 62072389, No. 62002305, andNo. 62272401), and the Natural Science Foundation of Fujian Province of China (No.2021J01002,No.2022J06001).",
  "G. D. Finlayson, S. D. Hordley, C. Lu, and M. S. Drew. On the removal of shadows fromimages. IEEE Transactions on Pattern Analysis and Machine Intelligence, 28(1):5968, 2005": "C. Fu, Y. Hu, X. Wu, H. Shi, T. Mei, and R. He. Cm-nas: Cross-modality neural archi-tecture search for visible-infrared person re-identification. In Proceedings of the IEEE/CVFInternational Conference on Computer Vision, pages 1182311832, 2021. Y. Gong, L. Huang, and L. Chen. Person re-identification method based on color attack andjoint defence. In Proceedings of the IEEE/CVF conference on computer vision and patternrecognition, pages 43134322, 2022.",
  "K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. InProceedings of the CVPR, pages 770778, 2016": "B. Horn, B. Klaus, and P. Horn. Robot vision. MIT press, 1986. D. Li, X. Wei, X. Hong, and Y. Gong. Infrared-visible cross-modal person re-identification withan x modality. In Proceedings of the AAAI, pages 46104617, 2020. J. Liu, Y. Sun, F. Zhu, H. Pei, Y. Yang, and W. Li. Learning memory-augmented unidirectionalmetrics for cross-modality person re-identification. In Proceedings of the IEEE/CVF conferenceon computer vision and pattern recognition, pages 1936619375, 2022.",
  "B. T. Phong. Illumination for computer generated pictures. Communications of the ACM, 18(6):311317, 1975": "J. Shi, Y. Zhang, X. Yin, Y. Xie, Z. Zhang, J. Fan, Z. Shi, and Y. Qu. Dual pseudo-labels inter-active self-training for semi-supervised visible-infrared person re-identification. In Proceedingsof the IEEE/CVF International Conference on Computer Vision, pages 1121811228, 2023. J. Shi, X. Yin, Y. Chen, Y. Zhang, Z. Zhang, Y. Xie, and Y. Qu. Multi-memory matching forunsupervised visible-infrared person re-identification. In European Conference on ComputerVision, pages 456474. Springer, 2024. J. Shi, X. Yin, Y. Wang, X. Liu, Y. Xie, and Y. Qu. Progressive contrastive learning withmulti-prototype for unsupervised visible-infrared person re-identification.arXiv preprintarXiv:2402.19026, 2024.",
  "A. Wu, W.-S. Zheng, H.-X. Yu, S. Gong, and J. Lai. Rgb-infrared cross-modality personre-identification. In Proceedings of the ICCV, pages 53805389, 2017": "Q. Wu, P. Dai, J. Chen, C.-W. Lin, Y. Wu, F. Huang, B. Zhong, and R. Ji. Discover cross-modality nuances for visible-infrared person re-identification. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition, pages 43304339, 2021. M. Yang, Z. Huang, P. Hu, T. Li, J. Lv, and X. Peng. Learning with twin noisy labels for visible-infrared person re-identification. In Proceedings of the IEEE/CVF conference on computervision and pattern recognition, pages 1430814317, 2022.",
  "M. Ye, J. Shen, D. J. Crandall, L. Shao, and J. Luo. Dynamic dual-attentive aggregation learningfor visible-infrared person re-identification. In Proceedings of the ECCV, 2020": "M. Ye, W. Ruan, B. Du, and M. Z. Shou. Channel augmented joint learning for visible-infraredrecognition. In Proceedings of the IEEE/CVF International Conference on Computer Vision,pages 1356713576, 2021. M. Ye, J. Shen, G. Lin, T. Xiang, L. Shao, and S. C. Hoi. Deep learning for person re-identification: A survey and outlook. IEEE transactions on pattern analysis and machineintelligence, 44(6):28722893, 2021.",
  "M. Ye, Z. Wu, C. Chen, and B. Du. Channel augmentation for visible-infrared re-identification.IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023": "Q. Zhang, C. Lai, J. Liu, N. Huang, and J. Han. Fmcnet: Feature-level modality compensationfor visible-infrared person re-identification. In Proceedings of the IEEE/CVF conference oncomputer vision and pattern recognition, pages 73497358, 2022. Y. Zhang and H. Wang. Diverse embedding expansion network and low-light cross-modalitybenchmark for visible-infrared person re-identification. In Proceedings of the IEEE/CVFconference on computer vision and pattern recognition, pages 21532162, 2023. Y. Zhang, Y. Yan, Y. Lu, and H. Wang. Towards a unified middle modality learning for visible-infrared person re-identification. In Proceedings of the 29th ACM International Conference onMultimedia, pages 788796, 2021. Y. Zhang, Y. Yan, J. Li, and H. Wang. Mrcn: a novel modality restitution and compensationnetwork for visible-infrared person re-identification. In Proceedings of the AAAI Conference onArtificial Intelligence, pages 34983506, 2023."
}