{
  "Abstract": "Non-uniform goal selection has the potential to improve the reinforcement learning(RL) of skills over uniform-random selection. In this paper, we introduce a methodfor learning a goal-selection policy in intrinsically-motivated goal-conditionedRL: Diversity Progress (DP). The learner forms a curriculum based on observedimprovement in discriminability over its set of goals. Our proposed method isapplicable to the class of discriminability-motivated agents, where the intrinsicreward is computed as a function of the agents certainty of following the true goalbeing pursued. This reward can motivate the agent to learn a set of diverse skillswithout extrinsic rewards. We demonstrate empirically that a DP-motivated agentcan learn a set of distinguishable skills faster than previous approaches, and doso without suffering from a collapse of the goal distributiona known issue withsome prior approaches. We end with plans to take this proof-of-concept forward.",
  "Introduction": "Intrinsically-motivated learning has been studied extensively in the reinforcement learning (RL)literature (see recent reviews by Colas et al., 2022; Aubret et al., 2023; Lidayan et al., 2024). Here,we focus on intrinsically-motivated skill acquisition, often referred to as competence-based intrinsicmotivations (CB-IMs), an area of control problems requiring multiple skills. How to learn a diverseset of skills is a key subproblem of CB-IMs (Colas et al., 2022, p. 1161). In answer, one class ofCB-IMs uses rewards computed as functions of the agents certainty of following the goal it chose topursue, that is, the goals discriminability. It is postulated in existing work that the more discriminablea set of goals is, the more diverse we expect the skills to be in terms of observed behaviour (see.1). Many such discriminator-based models select goals uniformly during training; yet,learning a distribution over goals has the potential to speed up learning (see ). Our contribution is threefold. (1) We present Diversity Progress (DP) (), a method forlearning a goal-selection policy prioritising goals based on the observed learning progress over a setof goals. (2) We complement the formalism with empirical findings (), suggesting that aDP-motivated agent can improve existing discriminator-based CB-IMs by speeding up the learningof a diverse set of skills. (3) We detail plans for taking this proof-of-concept forward ().",
  "arXiv:2411.01521v2 [cs.AI] 6 Nov 2024": "mutual information (BMI) (termed by Hansen et al., 2020, p. 2; e.g., Gregor et al., 2016; Warde-Farley et al., 2019; Eysenbach et al., 2019; Baumli et al., 2021; Laskin et al., 2022), though ourmethod could be used with any discriminator-based CB-IM. These rewards approximate the mutualinformation between the goal-defining variable, g, and some function of the trajectory drawn from thecorresponding skill, f(T g). Typically, f maps a trajectory to a single state, but some formulationsmap to multiple states (e.g., Gregor et al., 2016, use the initial and final states). Following Hansenet al. (2020, p. 4), the BMI objective can be defined as the maximisation of:",
  "I(g; f(T g)) H(g) Egp(g),T g (g)log q(g | f(T g)),(2)": "where q is an arbitrary variational distribution (Barber and Agakov, 2003, p. 2). Given f(T g), qdefines a probability distribution over goals. That is, a probability model, q, predicts, for each skill,the probability that the skill has induced the observationsthus the model is canonically known as adiscriminatorand is typically used to compute the reward. Successfully discriminating skills in theobservation space requires the agent to observe distinct regions of the state space, encouraging theagent to learn a set of diverse behaviours. If a goal is not discriminable based on observations, two ormore skills are producing overlapping behaviours (and therefore, the skills lack diversity); conversely,if a goal is discriminable, then the corresponding skill is inducing trajectories unique to that skill.",
  "Learning Progress": "Learning Progress (LP) (e.g., Oudeyer et al., 2007; Schmidhuber, 2010) is designed to select goalsthat are of learning-optimal difficulty with respect to [the agents] current capabilities (Lintunenet al., 2024, p. 39). Oudeyer et al. (2007, pp. 270271) formulate LP as an intrinsic reward measuringhow much the agent has improved in some prediction over a window of time. For any decision thataffects learning, the LP at time t + 1 can be computed for each option, indexed by n:",
  "Related work": "BMI-maximising CB-IMsIn VIC (Gregor et al., 2016), one of the earliest BMI-related approaches,the agent learns a categorical distribution over goals, g p(g)like our method. VIC reinforcesp(g) to maximise the same BMI-based intrinsic reward as is used for learning the skills (p. 4).However, Eysenbach et al. (2019, p. 6) showed that in VIC the probability mass collapses to a handfulof skills. In response, Eysenbach et al. fixed p(g) as uniform in DIAYN, so all skills receive, inexpectation, equal training signal. A side effect is inefficient use of training samples: the curriculumis not optimised for improving discriminability. DP aims to train more efficiently, via a learned goal-selection policy that could be used with any BMI-maximising CB-IMs. Laskin et al. (2022, ,p. 10) list several discriminator-based approaches, noting key differences in reward formulations. LP-based CB-IMsLP has been used to motivate goal selection in a variety of ways (Colas et al.,2022, p. 1185). Learning p(g) can significantly improve the speed of learning a set of skills (Stout andBarto, 2010, pp. 260261). Our method formulates goal selection as a bandit problem rewarded usingLP, much like the approach used by CURIOUS (Colas et al., 2019), one such LP-based CB-IM. Thisformulation is in contrast with VIC, which sets an RL problem, conditioning its goal-selection policy on a state. However, while DP selects between single goals in the full sensory space, CURIOUSsupports the provision of predefined subspaces, which the authors call modules. The CURIOUSagent learns a distribution over the modules, learning to prioritise subspaces of goals it is increasinglyor decreasingly successful in reaching (success is determined by some binary function).",
  "eg(t + 1) :=1 q(g | st+1),if g corresponds to the skill being followedq(g | st+1),otherwise,(5)": "where eg(t + 1) is an element of e(t + 1) R|G|, a vector composed of the error, at t + 1, for eachgoal, and q is a discriminator. Then, the errors over an epoch of fixed time length T form a T |G|matrix. Given hyperparameters offset, , and smoothing, , (see .2), we use Equation 4 tocompute the most recent average errors, e(t + 1), and the average errors from time steps earlier,e(t + 1 ). Following Equation 3, we compute the LP for each goal, with goal g corresponding tooption n. Then, the LP values are averaged over goals to compute the Diversity Progress:",
  "RQ1. Does the probability mass of the goal distribution collapse?": "To learn a diverse set of skills, it is material to train a substantial number of skills. Eysenbachet al. (2019, p. 6) showed that, with VICs goal-selection method, the effective number of skills (seeAppendix D.2) collapses to a handful. The method by Eysenbach et al. (2019), DIAYN, thereforeselects goals uniformly. Does DP avoid a collapse like VICs? shows the effective numberof skills over training time for all three methods in three environments (see Appendix D.1). With thefixed p(g) in DIAYN, its effective number of skills is constantequal to |G|. DP does not collapse,and, additionally, we have control over the effective number of skills via the softmax temperature. epoch effective #skills (a) 2D epoch (b) Half-Cheetah epoch (c) Ant VICDIAYNDP (TSM = . 1) DP (TSM = . 3) : The effective number of skills over training time in three environments. The agent islearning 20 skills. We compare VIC, DIAYN, and DP with two different softmax temperatures (0.1and 0.3) determining how greedy the policy is. The linear decline of the effective number of skillsfor epochs up to the number of skills is due to DPs initialisation, that is, randomly selecting goalswithout replacement (see Algorithm 1, ll. 610). Results from five random seeds; each line is a seed.",
  "RQ2. What are the effects of DP on the dynamics of goal selection?": "shows DP values, goal-selection probabilities, and cumulative frequencies (counts of howmany times each goal has been selected thus far) for a small number of skills (five). Initially, withoutevidence on all goals, each goal is selected once. Then, the agent should focus on the easiest-to-discriminate skills, up until repeated attempts generate less progress than other goals. Goals that aretoo hard generate little progress, so should be avoided until other goals are sufficiently mastered. 0.0 0.2 0.4 0.6 0.8 1.0 DP epoch 0.0 0.2 0.4 0.6 0.8 1.0 probability epoch cumulative frequency purple red purple red purple red purple red purple red : The effects of DP on goal selection over training time in the Half-Cheetah environment.The agent is learning five skills, each shown in a different colour. Upper left: DP values, DP; updatedfor the current skill at the end of an epoch. Lower left: goal-selection probabilities after the softmaxtransformation. Right: cumulative frequencies of goal selection. The initial trend for epochs up to thenumber of skills is due to DPs initialisation, where goals are selected randomly without replacement(see Algorithm 1, ll. 610). The plotted traces represent data from running a single random seed. Consider the purple and red goals. Red initially provided low DP, so was largely ignored until it wassufficiently easy (the policy is not completely greedy, so red is still updated occasionally despite itsdifficulty) or others provide less DP in comparison (e.g., purple declining). From the cumulativefrequencies of selection, we see the red goal eventually caught upexplained by its increase in DP.",
  "RQ3. Does DP-guided selection of goals speed up learning distinguishable skills?": "LP should prioritise goals that are providing the most improvement to the discriminator. Therefore,we hypothesise that a DP-motivated agent learns a set of diverse skills faster than a learner samplinggoals uniformly. We probe this by visualising trajectories in a dimension-reduced feature space tofind out whether trajectories from early training are distinguishable based on their mean observations(; details in Appendix D.3). Compared to random skills (no learning) and skills learned withuniform goal selection (DIAYN), the set of skills learned with DP is distinguishable earlier. random skills (initialisation)DIAYNDIAYN + DP : A dimension-reduced feature space (t-SNE) for 20 skills in the Half-Cheetah environment.For each skill (one colour), the 100 data points represent i.i.d. draws of trajectories. Left: randomlyinitialised skills with no training. Middle, Right: trajectories sampled after 100 epochs of training.Note both DP and DIAYN eventually learn distinguishable skills. Experiment details in Appendix D.3.",
  "Conclusion and future work": "We propose DP as a method for learning a goal-selection policy in discriminability-motivated RL,prioritising goals based on overall improvements in discriminability. We have shown in threeexperiments, that: (1) a DP-motivated agent learns a distribution over goals without the probabilitymass collapsing; (2) the DP values motivate goal selection with respect to observed LP; and (3) withDP, an agent can learn a diverse set of skills in less training time than with uniform-random selection. In future work, we aim to better understand how different factors affect goal selection. We plan to testother intrinsic rewards combining discriminability with LP, including absolute LP, where the agentalso attends to goals that it is forgetting (i.e., skills decreasing discriminability). Different entropyregularisation regimes may benefit diversity in terms of increased state-space coverage but makediscrimination of skills harder. Following Eysenbach et al. (2019), the utility of DP can be tested ontransfer learning and hierarchical tasks (pp. 6, 78), comparing with other discriminator-based CB-IMs, and on a range of environments including non-episodic and stochastic ones. However, evaluatingopen-ended learning is notoriously difficult. When the aim is to learn a diverse set of interestingskills, what is interesting is typically subjectively defined by the researchers and the community, andnot clearly designed to maximise a well-defined objective (Colas et al., 2022, p. 1168). We planto improve the evaluation of skill diversity by considering different diversity metrics, and use ourfindings to quantitatively evaluate DP against the current state-of-the-art BMI-maxisiming CB-IMs.",
  "Author contributions": "We follow the Contributor Role Taxonomy (CRediT) introduced by Brand et al. (2015). Conceptuali-sation: initial equals EML, NMA; evolution equals EML, NMA, CG. Data curation: EML. Formalanalysis: EML. Funding acquisition: CG. Investigation: EML. Design of methodology: lead EML;supporting NMA, CG. Creation of models: leads EML, NMA; supporting CG. Software: EML.Resources: CG. Oversight: equals NMA, CG. Leadership responsibility: CG. Visualisation: leadEML; supporting NMA, CG. Writing: original draft EML; review & editing equals EML, NMA, CG.",
  "Acknowledgements and disclosure of funding": "EML and CG received financial support from the Research Council of Finland (NEXT-IM, grant349036) and NMA from the Helsinki Institute for Information Technology. We acknowledge thecomputational resources provided by the Aalto Science-IT project. We thank the Aalto ScientificComputing team, particularly Simo Tuomisto, Mira Salmensaari, and Hossein Firooz, for theirsupport. For their time, ideas, and feedback on the topic of measuring skill diversity, we thank PerttuHmlinen, Marlos Machado, Sebastian Berns, Nam Hee Kim, and Luigi Acerbi. We thank thereviewers for their service and for providing useful future-looking feedback.",
  "Arthur Aubret, Laetitia Matignon, and Salima Hassas. An information-theoretic perspective onintrinsic motivation in reinforcement learning: A survey. Entropy, 25(2), 2023. URL": "David Barber and Felix Agakov. The IM algorithm: a variational approach to information maximiza-tion. In Proceedings of the 16th International Conference on Neural Information Processing Sys-tems, pages 201208. MIT Press, 2003. URL Kate Baumli, David Warde-Farley, Steven Hansen, and Volodymyr Mnih. Relative variationalintrinsic control. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35(8),pages 67326740. AAAI Press, 2021. URL",
  "Amy Brand, Liz Allen, Micah Altman, Marjorie Hlava, and Jo Scott. Beyond authorship: attribution,contribution, collaboration, and credit. Learned Publishing, 28(2):151155, 2015. URL": "Cdric Colas, Pierre Fournier, Mohamed Chetouani, Olivier Sigaud, and Pierre-Yves Oudeyer.CURIOUS: Intrinsically motivated modular multi-goal reinforcement learning. In Proceedings ofthe 36th International Conference on Machine Learning, volume 97, pages 13311340. PMLR,2019. URL Cdric Colas, Tristan Karch, Olivier Sigaud, and Pierre-Yves Oudeyer. Autotelic agents withintrinsically motivated goal-conditioned reinforcement learning: a short survey. Journal of ArtificialIntelligence Research, 74:11591199, 2022. URL Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine.Diversity is all youneed: Learning skills without a reward function. In 7th International Conference on LearningRepresentations, 2019. URL",
  "Karol Gregor, Danilo Jimenez Rezende, and Daan Wierstra. Variational intrinsic control. arXivpreprint arXiv:1611.07507, 2016. URL": "Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policymaximum entropy deep reinforcement learning with a stochastic actor. In Proceedings of the 35thInternational Conference on Machine Learning, volume 80, pages 18611870. PMLR, 2018. URL Steven Hansen, Will Dabney, Andr Barreto, David Warde-Farley, Tom Van de Wiele, and VolodymyrMnih. Fast task inference with variational intrinsic successor features. In 8th InternationalConference on Learning Representations, 2020. URL R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, AdamTrischler, and Yoshua Bengio. Learning deep representations by mutual information estimationand maximization. In 7th International Conference on Learning Representations, 2019. URL Leslie Pack Kaelbling. Learning to achieve goals. In Proceedings of the Thirteenth International JointConference on Artificial Intelligence, volume 2, pages 10948. International Joint Conferenceson Artificial Intelligence Organization, 1993. URL Michael Laskin, Hao Liu, Xue Bin Peng, Denis Yarats, Aravind Rajeswaran, and Pieter Abbeel.Unsupervised reinforcement learning with contrastive intrinsic control. In Advances in Neu-ral Information Processing Systems, volume 35, pages 3447834491. Curran Associates,Inc., 2022.URL",
  "Aly Lidayan, Michael Dennis, and Stuart Russell. BAMDP shaping: a unified theoretical frameworkfor intrinsic motivation and reward shaping. arXiv preprint arXiv:2409.05358, 2024. URL": "Erik M Lintunen, Nadia M Ady, Christian Guckelsberger, and Sebastian Deterding. Advancing self-determination theory with computational intrinsic motivation: The case of competence. PsyArXivpreprint 10.31234/osf.io/7qy35, 2024. URL Minghuan Liu, Menghui Zhu, and Weinan Zhang. Goal-conditioned reinforcement learning: Prob-lems and solutions. In Proceedings of the Thirty-First International Joint Conference on ArtificialIntelligence, pages 55025511. International Joint Conferences on Artificial Intelligence Organiza-tion, 2022. URL Ashvin V Nair, Vitchyr Pong, Murtaza Dalal, Shikhar Bahl, Steven Lin, and SergeyLevine.Visual reinforcement learning with imagined goals.In Advances in Neu-ral Information Processing Systems, volume 31, pages 92099220. Curran Associates,Inc., 2018.URL",
  "Andrew Stout and Andrew G. Barto. Competence progress intrinsic motivation. In 2010 IEEE9th International Conference on Development and Learning, pages 257262. IEEE, 2010. URL": "Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 50265033.IEEE, 2012. URL David Warde-Farley, Tom Van de Wiele, Tejas Kulkarni, Catalin Ionescu, Steven Hansen, andVolodymyr Mnih. Unsupervised control through non-parametric discriminative rewards. In 7thInternational Conference on Learning Representations, 2019. URL",
  "RG : S A S G R,(7)": "where S is the set of possible states, A is the set of actions available to the agent, and g G, knownas a goal, or a goal-defining variable, is a parameter to the reward function (cf., Colas et al., 2022,p. 1165; Aubret et al., 2023, p. 6). Note that RG(s, a, s, g) is a random variable, even for fixeds, s S, a A, and g G. Then, a skill is a policy paired with a goal, optimising for the returnaccording to the reward conditioned on that goal. For example, the goal-defining variable, g, can bean index (e.g., Eysenbach et al., 2019) or an element drawn from a learned distribution (e.g., Nairet al., 2018). Its role is simply to indicate which reward function the agent is aiming to maximise. Goals can be viewed as a set of constraints ... that the agent seeks to respect (Colas et al., 2022,p. 1165, emphasis in original). While the most immediate intuition of a goal is often as a desired statefor the agent to reach (e.g., Kaelbling, 1993, p. 1; Schaul et al., 2015, p. 2), the formalism allowsfor a more general set of constraints on behaviour (Lintunen et al., 2024, pp. 2223). In effect, anybehaviour that can be defined by attempting to maximise some reward function on the environmentcan be formulated as a goalskill pairing.",
  "C.1Learning objective": "Our implementation is based on DIAYN, so following Eysenbach et al. (2019, p. 3) we wish to ensure:(1) that statesnot actionsare used to distinguish skills; and (2) that the agent is maximising theentropy of its policies. This leads to the following instantiation of the BMI objective (see .1for BMI; see Eysenbach et al., 2019, Equations 12, p. 4, for the full derivation):",
  "C.2Optimiser and reward function": "In optimising its policy to maximise Equation 9, the agent relies on soft actor-critic (SAC) (Haarnojaet al., 2018), an off-policy maximum entropy actor-critic algorithm. In maximum entropy RL, wherelearning a stochastic policy is desirable, the standard RL objective of maximising expected return isaugmented with an entropy maximisation term, so the optimiser takes care of the entropy term (a)(cf., Eysenbach et al., 2019, p. 4). The expectation (b) is maximised with the intrinsic reward function",
  "RiG := log q(g | s) log p(g).(10)": "This expresses that the agent is rewarded for its ability to discriminate the skill being followed (see.1). The distribution over goals, p(g), is: (1) categorical; and (2) learned using DP (wedescribe the method for learning a goal-selection policy in ). At the beginning of an epoch,the agent samples a skill, gp(g), and follows it until termination. That is when p(g) is updated.",
  "C.3Action-selection policy": "At the action-selection level, the agent learns a goal-conditioned policy, (a | s, g). FollowingDIAYN (mujoco_all_diayn.py, ll. 210216), the policy is a Gaussian mixture model (GMM) withK components, where an MLP, parametrised by , maps from stategoal pairs to the (log) weightwk, mean vector k, and vector k of (log) standard deviations, i.e., of the diagonal entries of thecovariance matrix k, of each Gaussian component:",
  "C.4Hyperparameters": "We tested various candidates for the DP hyperparameters: smoothing, , offset, , and softmaxtemperature TSM. However, since we currently lack a good diversity metric to evaluate differencesin performance, the hyperparameter choices in our experiments were mainly based on qualitativeobservations. Smoothing ranged 100250 time steps, offset 250900, and softmax temperature0.10.75. The hyperparameter values used in producing the figures are listed in . The mostnoticeable difference was given by varying TSM. To achieve stability in the level of greediness of thegoal-selection policy, we normalised the mean LP before computing the mean progress over goals. For reference, in the 2D environment, episodes last for 100 time steps, so a goal is sampled every10 episodes. In the Half-Cheetah environment, episodes last for 1000 time steps, so a new goal issampled for every episode. In the Ant environment, episodes last for a maximum of 1000 time steps(there is a potential termination condition before the maximum episode length), so a sampled goal",
  "D.1Environments": "We carried out experimental comparisons of DP against DIAYN, our baseline method, in environmentswhere DIAYN has been shown to perform well, following the advice of Patterson et al. (2023, p. 26).This choice eases comparison with related work. Specifically, we used three environments tested byEysenbach et al. (2019): a slightly modified version of their 2D Navigation environment and twoMuJoCo environments (Todorov et al., 2012), namely Half-Cheetah and Ant. The 2D Navigationenvironment affords diagnostics into the algorithms function, and with the MuJoCo environments, werepresent tasks of increasing complexity in terms of both observation space and degrees of freedom. 2D NavigationOur 2D environment () is a slightly modified version of the one constructedby Eysenbach et al. (2019). An agent starts in the center of the unit box s0 = (0.5, 0.5) and observesstates s 2. We modified the action space to a [0.05, 0.05]2 (whereas the original usedlarger actions, in [0.1, 0.1]2). The environment is bounded, in that if an action takes the learneroutside of the box, they are projected back to the closest point in the support of the observation space.",
  "MuJoCoHalfCheetah-v1 and Ant-v1 with no modifications (Todorov et al., 2012)": "0.00.20.40.60.81.0 0.0 0.2 0.4 0.6 0.8 1.0 random skills (initialisation) 0.00.20.40.60.81.0 0.0 0.2 0.4 0.6 0.8 1.0 DIAYN-learned skills (after 100 epochs) : Trajectories drawn from eight stochastic skills in our modified version of the 2D Naviga-tion environment constructed by Eysenbach et al. (2019). Left: random skills with no training. Right:DIAYN-learned skills. Note, these visualisations are provided for intuition, showing trajectories only15 steps long. In (a), we included 20 skills and trajectories were 100 steps long.",
  "D.2Effective number of skills": "Following Eysenbach et al. (2019, Appendix E.2, p. 18), we compute the effective number of skillsby exponentiating the entropy of the goal-selection policy, quantifying the number of skills that thepolicy is effectively sampling from at the time of selection. For intuition, consider an example of anoccasion in which one goal is providing significantly more DP than all other goals. This will decreasethe effective number of skills, as the learned distribution over goals is less uniform (favouring thatone goal). If the DP is similar for all goals, then the distribution is close to uniform and the numberof effective skills is close to the total number of skills being learned. With DP, we have some controlover the effective number of skills via the softmax temperature: higher temperatures lead to lessemphasis on the differences in DP and thus a more uniform sampling distribution; whereas, with lowtemperatures, we observe higher variance in the effective number of skills over time. High variancein the effective number of skills is likely desirable, since we wish to favour goals only if they providemore DP in comparison to others, and wish for the learner to sample more uniformly in cases ofuncertainty (in terms of small differences between the DP values). Like Eysenbach et al. (2019, p. 6), we believe that VICs goal-selection policy collapses due to thechoice of optimiser (REINFORCE; Williams, 1992). But our results, as well as those from the widerLP literature, demonstrate their claim that learning p(g) results in learning fewer diverse skills(Appendix E.2, p 18) to be overgeneralised.",
  "D.3Evaluating skill diversity with dimensionality reduction": "To generate , we first draw 100 trajectories from each skill, for each: (1) a set of randomskills (no learning), (2) skills learned using DIAYN (uniform goal selection), and (3) skills learnedusing DIAYN with DP motivating goal selection. Then, we compute the multivariate means over eachtrajectory, and dimension-reduce the feature space to two dimensions using t-distributed stochasticneighbor embedding (t-SNE) with the following hyperparemeters: a perplexity of 30, a learning rateof 10, and running the optimisation for a maximum of 5000 iterations. In the plot, the distancesbetween clusters may mean nothing (Wattenberg et al., 2016). The same random seeds were used for the training of DIAYN and DIAYN+DP. Similarly, the seedsfor the random number generators of both the policy simulator and t-SNE algorithm were fixed forthe processes of simulating trajectories and dimensionality reduction across all three methods."
}