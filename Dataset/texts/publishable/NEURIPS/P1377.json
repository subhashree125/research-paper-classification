{
  "Kristoffer Wickstrm1 , Marina Hhne2,3,6 , and Anna Hedstrm2,4,5": "1 Department of Physics and Technology, UiT The Arctic University of UMI Lab, Leibniz Institute of Agricultural Engineering and Bioeconomy e.V.(ATB)3 Department of Computer Science, University of Department of Electrical Engineering and Computer Science, TU Berlin5 Department of Artificial Intelligence, Fraunhofer HHI, Berlin, Germany6 BIFOLD - Berlin Institute for the Foundations of Learning and Data Abstract. The lack of ground truth explanation labels is a fundamentalchallenge for quantitative evaluation in explainable artificial intelligence(XAI). This challenge becomes especially problematic when evaluationmethods have numerous hyperparameters that must be specified by theuser, as there is no ground truth to determine an optimal hyperparam-eter selection. It is typically not feasible to do an exhaustive search ofhyperparameters so researchers typically make a normative choice basedon similar studies in the literature, which provides great flexibility forthe user. In this work, we illustrate how this flexibility can be exploitedto manipulate the evaluation outcome. We frame this manipulation as anadversarial attack on the evaluation where seemingly innocent changesin hyperparameter setting significantly influence the evaluation outcome.We demonstrate the effectiveness of our manipulation across severaldatasets with large changes in evaluation outcomes across several ex-planation methods and models. Lastly, we propose a mitigation strategybased on ranking across hyperparameters that aims to provide robust-ness towards such manipulation. This work highlights the difficulty ofconducting reliable XAI evaluation and emphasizes the importance of aholistic and transparent approach to evaluation in XAI. Code is availableat",
  "Introduction": "Explainable artificial intelligence (XAI) is a crucial research area to ensure trust-worthiness in computer vision , which contains a wide range of methods thatprovide explanations for the output of a predictive model . To determinewhich XAI method is suitable for a given problem setting, quantitative evaluationanalysis is necessary to provide an objective measurement for comparison. Such",
  "LRP19.31Saliency22.96Kernel SHAP24.87": ": Faithfulness comparison of XAI methods on MNIST before (left table) andafter manipulation (right). Here, the different between the left and right table is the per-turbation methods used (uniform noise vs. blurring, respectively). Both perturbationmethods are commonly used, but completely change the outcome of the evaluation. quantitative analysis of XAI methods has made great leaps forward over the lastcouple of years , and generally consists of evaluating several metrics thatmeasure desirable properties that an XAI method should have i.e., metric-basedquality estimation . However, the progress in XAI and its evaluation has ledto an overwhelming variety of methods and metrics, making it challenging forresearchers to navigate their choices .A fundamental limitation in XAI evaluation is the lack of ground truth ex-planation labels . Since such information is generally not available, we ap-proximate explanation quality by measuring desirable properties like faithful-ness , complexity , or robustness andtranslate these properties into empirical tests . In this translation, a challengeappears in the parameterization of the empirical tests. For example, how do wemask out pixels and how large should the masks be? Preliminary works have shown that the evaluation outcomes are sensitive to choices like these. Thissensitivity underscores the need to investigate the impact of hyperparameterchoice, making it an important research area to ensure the reliability of XAIevaluations.This challenge becomes particularly prominent for evaluation methods withmany hyperparameters that must be set, since it is generally not possible tofind an objective measure of the optimal set of hyperparameters. For instance,faithfulness evaluations view model behavior changes as signals of explanationsquality, with substantial changes reflecting the explanations faithfulness . This type of evaluation often requires replacing pixel values with somebaseline value, which can be highly data-dependent and difficult to tune .Furthermore, it can often be computationally impractical to evaluate all possiblechoices for hyperparameters. Therefore, hyperparameters are usually selectednormatively with the researchers own subjective judgment, frequently drawingon prior studies. Since there is variation in what hyperparameters are being usedin the community , there is some flexibility in selecting hyperparametersfrom an acceptable set of possible choices.In this work, we demonstrate how this flexibility of XAI evaluation can beexploited to manipulate the evaluation outcome. By making seemingly smallchanges to hyperparameters that are widely used in the literature, the outcomeof the faithfulness evaluation can change completely. Tab. 1 illustrates this, where",
  "From Flexibility to Manipulation: The Slippery Slope of XAI Evaluation3": "standard XAI methods are compared with only slight changes in hyperparam-eters but with significant changes in evaluation outcome. We propose to framethe finding of these small changes as an optimization problem that manipulatesthe evaluation, where either the evaluation of a single XAI method is manip-ulated or the evaluation of multiple XAI methods is manipulated jointly. Ourcontributions are:",
  "C4 Towards improving the robustness of quantitative evaluation of XAI, wepropose Mean Resilience Rank, a ranking-based procedure that reduces thesensitivity to hyperparameter manipulation": "Our findings have significant implications for the XAI community. Quantitativeevaluation is crucial to provide objective measurements of explanation quality,which can be used to select an appropriate method for a particular task orfor comparison in method development. If these measurements can be easilyaltered, it reduces the trustworthiness of both method selection and comparison.Therefore, the findings and solutions in this work are of critical importance forthe community both by highlighting the issue of manipulation and by presentingstrategies towards mitigating the issue.",
  "Related Work": "Metric-based Quality Estimation Quantitative analysis of XAI explanation hasimproved considerably in recent years, and researchers now have a vast amount ofevaluation metrics at their disposal . Due to the lack of ground truth expla-nations, researchers try to quantify the quality of an explanation by measuringdesirable properties, which can be categorized into 6 families of properties ;faithfulness , robustness , localisation , complexity , randomisa-tion , and axiomatic . Within each family, a variety of metrics exists. Prior Studies on Hyperparameter Sensitivity in XAI Increasing attention hasbeen given to the influence and potential confounding effects of hyperparam-eters in XAI evaluations . These studies vary in defining dependent versusindependent variables and the hyperparameter space of intervention, be it model,explanation, or evaluation space. Studies have examined the sensitivity of attri-bution methods to explanation hyperparameters like random seed and numberof samples , and the impact of baseline choices in methods like IntegratedGradients on explanation outcomes . Additionally, the sensitivity of ex-planation outcomes concerning model performance variables such as optimizer,",
  "K. Wickstrm et al": "Models and Datasets: We examine several widely used computer vision datasets;MNIST , FashionMNIST , PneumoniaMNIST , and ImageNet ,and two common deep learning architectures: LeNet and ResNet18 . TheLeNet is used for classifying MNIST, FashionMNIST, and PneumoniaMNIST,while the Resnet18 is used for classifying ImageNet. For ImageNet, we ran-domly sample 100 samples to conduct the faithfulness evaluation, for Pneumo-niaMNIST we use 500 samples, and for the remaining datasets we use 1000samples. We choose 100 samples for ImageNet because the larger size of theseimages increases the computational complexity. We choose 500 for Pneumoni-aMNIST as it does not have 1000 samples in its test set. XAI Methods: We investigate the following XAI methods; Layer-wise relevancepropagation (LRP) , Saliency , and KernelSHAP using the captumlibrary . We have picked these three methods as they represent commonchoices in the XAI field, and we have focused on only three methods to providea clear experimental analysis without overloading the reader.",
  "Local explanations Let the input to a black-box classifier f be denoted as x Rd": "and the output of the classifier as f(x) = y. Local explanation methods interpret the decision of f by attributing an importance score to eachcomponent of x. We denote the explanation of f for a given class y as e Rd. Evaluating Explanations Here, we present a generalized formulation of quanti-tative XAI evaluation to illustrate the static input parameters and adjustablehyperparameters. We assuming an evaluation function F R on the form:",
  "F(f, x, e, a, b, c) = s.(1)": "Here, f, x, and e are input parameters provided by the user, while a, b, andc are hyperparameters that must be determined by the user. The output ofthe evaluation is represented by s, which is a scalar indicating the performanceof the particular explanation. Here, we keep the hyperparameters a, b, and ccompletely general for the sake of clarity. But note that there could be more orless hyperparameters and they can take many different forms (e.g. a number ora function), depending on the particular test and the data in questions.",
  "From Flexibility to Manipulation: The Slippery Slope of XAI Evaluation5": "faithfulness evaluation (see Sec. 5 for further details), an im-portant component is perturbing input pixels. There exist numerous methodsfor conducting this perturbation, and it is known that selecting a suitable onecan be challenging . However, evaluating numerous such methods canbe highly computationally demanding, and due to the lack of ground truth ex-planations we cannot decide which method is correct. Therefore, in practice, itis common to consider only a single perturbation method . However, aswe have shown in Tab. 1, even a slight change in the hyperparameter setting canhave a big impact on the evaluation. Those who are aware of this sensitivity canpotentially exploit it, which is the motivation for our manipulation strategy. Intra-manipulation We propose two ways to manipulate XAI evaluation meth-ods. First, we propose to focus on manipulating the evaluation outcome for asingle XAI method, which we refer to as intra-manipulation and is defined as: Definition 1 (Intra-Manipulation).Given an evaluation function F, aninput sample x, an explanation e, hyperparameters a, b, and c, and a feasible setof hyperparameters Aa for the hyperparameter a, the intra-manipulation methodsolves the following optimization problem to determine the hyperparameter a,which maximizes the evaluation score of F:",
  "subject toa Aa": "Definition 1 defines an optimization problem where the goal is to find hyper-parameters that maximize the evaluation outcome, but are constrained to liewithin a feasible set of values (Aa in this case) for the hyperparameters in ques-tions. Determining this feasible set requires a researchers judgment and a goodunderstanding of the particular XAI evaluation method that the user wants tomanipulate. But more deeply, it fundamentally depends on the model: i.e. thefeasible set is and should be dependent on the learned functional response ofthe model. In Sec. 5, we further explain how to determine the feasible set. Ifthe feasible set is large, Definition 1 can be solved through suitable optimizationtechniques. If the feasible set if small, an exhaustive search can be performed.Also note that Definition 1 can be extended to optimize across several hyperpa-rameters, e.g. maximizing both a and b. Inter-manipulation Definition 1 allows for improving the evaluation outcome of asingle XAI method. But in many cases it could be desirable to alter the outcomeof the evaluation of several XAI methods. Our second manipulation approach isto take a holistic view and manipulate the evaluation of several XAI methodsjointly. We refer to this approach as inter-manipulation and define it as: Definition 2 (Inter-Manipulation). Given an evaluation function F, an in-put sample x, a set of explanations {e1, , eM} from M different XAI methods,hyperparameters a, b, and c, and a feasible set of hyperparameters Aa for the",
  "Manipulating Faithfulness Evaluation": "Some types of XAI evaluation methods are more susceptible to manipulationthan others. For instance, localization metrics, which aims to measure if anexplanation is within a region-of-interest, usually only have 1 or even 0 hyper-parameters to select and are therefore harder to manipulate. On the otherhand, faithfulness metrics have at least 3 hyperparameters that must bedetermined, and often more. This is one of the most popular evaluation methodsin XAI and is therefore an important evaluation category tostudy. Therefore, we will focus on manipulating faithfulness metrics. The follow-ing section provides an overview of the fundamental components in faithfulnessevaluation. The fundamental components of faithfulness Faithfulness measures to what ex-tent explanations follow the predictive behavior of the model by iteratively per-turbing the input and monitoring the corresponding change in the output of themodel. Our focus will be on the task of classification, since this is the most com-mon setting in the context of explainability and vision. This section presentsthe mathematical formulation of the general components of most faithfulnessmetrics. Let S denote the set of indices {1, , d} for each element in the inputsample x Rd. Partition S into K sets S1, , SK of equal cardinality C andarranged such that:",
  "iSkei(3)": "Inequality (2) instructs us to rank the indices according to the input features withhighest importance in a descending fashion, and are used to iteratively perturbthe input. Note that some metrics sort the indices in an ascending fashion and some perturb the input randomly , but the general approach infaithfulness metrics is to perturb the inputs according to Equation (2) . Let xS1 denote a perturbed version of x, where all xi for i S1 are replacedby some baseline perturbation function gp. We denote the output of the classifierbased on xS1 as yS1. For xS2, all xi for i S1 S2 are perturbed. In general,xSi will have all have the indices in all sets up to set Si replaced by the baselineperturbation function. 01K5K15K20K25K Number of perturbed input partitions 0.0 0.2 0.4 0.6 0.8 1.0 Prediction score",
  "(c)": ": Example of possible faithfulness curves for digit classification. The leftmostcurve illustrates how an \"intuitive\" faithfulness curve might look, while the remainingcurves show that there is a lot of variation in how these curves can appear. Illustrating the faithfulness curve Based on the K partitions of S, a set of progres-sively more perturbed inputs can be created, i.e. {xS1, , xSK}. Each of the per-turbed inputs are classified, which gives a set of model outputs {yS1, , ySK}.These model outputs are the fundamental components for faithfulness evalua-tion in XAI. The rationale is that a good explanation should remove the essentialparts of an input first, which should lead to a steep drop in the classificationscore. A poor explanation will remove parts that are not important, which willallow the classification score to stay high. a shows an example where theclassifier behaves as expected, with a sharp drop in accuracy when the importantparts of the input are removed. To compare two explanations, one can inspecta plot such as in a and see which explanation has the sharpest dropin classification score. However, such a visual approach has many limitations.First, we generally would like to compare explanations across many samples toget a reliable estimate of how they perform. Inspecting numerous such plotsis cumbersome, and the curves can look different for different visual objects inclassification, which makes comparison challenging. Also, real-world data is not",
  "Hyperparameters in Faithfulness Metrics": "Here, we briefly describe the different hyperparameters that must be determinedby the user to conduct a faithfulness evaluation. It is important to note that manyof these hyperparameters are inherently data dependent, which means that theuser must re-parameterize each metric for their use case, making the resultsnon-comparable across different datasets and potentially models. Size of partition The size of each partition determines how many features are re-moved and replaced in each step of the faithfulness curve. To determine this size,there are several considerations. First, if the size of the partition is very smallthe evaluation will quickly become computationally infeasible, since the numberof forward passes for each sample increases. Furthermore, removing only a singleor a few pixels at a time can lead to adversarial effects . Second, a largepartition size will lead to course faithfulness curves which makes comparisonbetween curves challenging. Therefore, there is a trade-off between computa-tional efficiency and resolution of the faithfulness curves. Some researchers usethe height and width of the image (assuming square images) as the size of thepartition , but other choices are also common . Perturbation Function When a set of features are removed from an image, theyare replaced by some perturbation function. An example of such a perturbationfunction could be Gaussian noise or setting pixel values to zero , but moreadvanced approaches are also available . The type of perturbation functionto apply is highly dependent on the type of images that are being considered. Forexample, replacing pixels with a value of zero can be possible for natural images but would not be a suitable choice for images with a black background, sincethis could potentially not induce a change in the networks output. In general,the choice of perturbation function varies greatly between papers . Aggregation Function Examples in Figures 1b and 1c, demonstrate that it canbe difficult to assess which explanation is superior. Therefore, it is desirable toaggregate the perturbed model outputs into a single score that can be easilyused for comparison using an aggregation function ga. There are two main ap-proaches to aggregate the curves shown in . The first approach is tocalculate the AUC of the faithfulness curve . A low AUC is considereddesirable, since it indicates that the important components of an input are re-moved first. The second approach is to correlate the model outputs with the sumof attributions within each partition . The motivation for this approach isthat when important parts of an object are removed the predictive performanceshould gradually decrease, which will be captured by the correlation functions.Both correlation and AUC are used regularly in the literature .",
  "From Flexibility to Manipulation: The Slippery Slope of XAI Evaluation9": "Normalization Function Attributions produced by different XAI methods canhave a widely different range of values. Therefore, it can be necessary to nor-malize the attributions such that they are comparable across different methods.A simple choice could be to standardize using the mean and standard deviationof the attributions. But choices such as these can influence evaluations andmore sophisticated normalization schemes are also used .",
  "Towards More Reliable Quantitative Evaluation withMean Resilience Rank": "Due to the lack of ground truth explanations, we cannot determine what settingof hyperparameters constitutes the \"correct\" choice. However, we do know thatit is desirable to perform well across all hyperparameter settings. Therefore, ifan XAI method consistently appears among the highest-ranked methods acrossnumerous hyperparameters, it provides an indication of high quality with lesssensitivity to hyperparameters. Thus, to provide robustness towards hyperpa-rameter manipulation, we propose to rank each XAI method for each hyperpa-rameter setting in the feasible set, and average the ranking across the entire set.We will refer to this ranking-approach as Mean Resilience Rank (MRR).Here, we describe mathematically how to perform this ranking. First, assumewe want to evaluate M explanation methods, and that we only have a singlehyperparameter a with a feasible set of values Aa that can be altered. We denoteone element of Aa as ai, such that the evaluation outcome for all M XAI methodscan be collected in the set:",
  "Defining the Feasible Set of Hyperparameters for Faithfulness": "A critical aspect of the manipulation methods outlined in Sec. 4 is to determinethe feasible set of hyperparameters. This requires in-depth knowledge of thefamily of quantitative metrics that we aim to manipulate. In this work, we focuson the faithfulness family of evaluation metrics and the critical hyperparamtersoutlines in Sec. 5.1. We focus on a subset of hyperparameters to provide a clearand understandable evaluation of our manipulation strategies. The feasible setof hyperparameters considered in this work are shown in Tab. 2. This selectionis based on common choices in the literature for partition size , perturbation function , and normalization function . Weconsider the aggregation function fixed as AUC aggregation, which means thata lower faithfulness score is better. Specifically, we compute the AUC of thefaithfulness curve from the set of perturbed model outputs {yS1, , ySK}.",
  ": Intra-results across several datasets and methods. Lower is better": "we call the base set of hyperparameters. The base set of hyperparameters forMNIST, FashionMNIST, and PneumoniaMNIST is a partition size of 28, uni-form noise as perturbations, and no normalization. For ImageNet, the base set ofhyperparameters is a partition size of 224, uniform noise as perturbations, andno normalization. After manipulation using Definition 1 and Definition 2, we willobtain a new set of hyperparameters that we refer to as the manipulated set ofhyperparameters. Our results are centered around comparing the performanceof the base set and the manipulated set.",
  "Intra-Manipulation Results": "Tab. 3 shows the results of performing the intra-manipulation proposed in Defini-tion 1, where base is the score obtained with the selected set of hyperparametersdescribed above and manipulated is the score obtained after manipulation. Theseresults demonstrate that there is much room for changing the evaluation out-come for a single XAI method, in some cases as much as a 130 % improvementfrom the base to the manipulated evaluation outcome. Note that the manipulatedscores are not directly comparable, since the manipulation is performed method-wise and the hyperparameters can be different. Therefore, the inter-manipulationshown in the next section must be used to alter the outcome of an evaluationacross methods.",
  "Inter-Manipulation Results": "Tab. 4, Tab. 5, and Tab. 6 show the results of performing the inter-manipulationproposed in Definition 2, where the scores are manipulated towards LRP, Saliency,and KernelSHAP, respectively. For some tasks, the evaluation outcome can bemanipulated such that most of the three methods achieves the best performance.This is particularly apparent for PneumoniaMNIST, where all XAI methods canachieve the best performance after manipulation. For some datasets there is lessroom for manipulation. This is most clear from the ImageNet results. That said,the evaluation difference between explanation methods can still be reduced andthus make the XAI evaluation findings less conclusive (see e.g. Imagenet resultsin Tab. 6). In Appendix A, we provide a summary of the amount of times eachhyperparameter occurs in the manipulated set.",
  "Towards More Robust Faithfulness Evaluation": "The results in Tab. 3, Tab. 4, Tab. 5, and Tab. 6, demonstrate that the evaluationoutcome can be manipulated and can not be trusted, which reduces the trust-worthiness of the quantitative evaluation. Here, we display the results of usingMRR described in Sec. 6 towards mitigating the potential for manipulation.Tab. 7 displays the results of this ranking procedure, which shows that thetop-performing XAI methods change between datasets. However, if we averagethe ranking across all datasets, LRP comes out as the top-performing methodclosely followed by KernelSHAP, while Saliency seems to be consistently rankedlower. But note that there is notable variation in the scores, which we furtherilluminate in . The benefit of this ranking approach is that there is littleroom for manipulation since the top-performing methods will have to performwell across numerous hyperparameters and datasets. The downside of this rank-ing approach is that it requires a significant amount of computation to calculatethe scores for all methods across all hyperparameters and datasets. Also, whileaveraging across datasets can provide robustness, it can also obfuscate impor-tant insights from a particular dataset. Therefore, it is important to include thedataset-wise ranking such that readers can get an overview of the evaluation.",
  "LRP0.22 0.150.33 0.000.21 0.000.26 0.00 0.29 0.14Saliency0.41 0.260.44 0.310.37 0.310.41 0.330.41 0.30KernelSHAP 0.37 0.330.22 0.310.33 0.270.33 0.060.31 0.31": ": MRR across feasible set for each dataset and across datasets (last column).Lower is better, a rank of 0 is best and 1 is worst. Results show that the top performingmethod can change significantly between datasets, but when averaging across datasetsLRP and KernelSHAP are highlighted as consistently higher ranked than Saliency. shows the faithfulness score for each configuration in the feasible setfor each dataset. This plot illustrates that the average faithfulness score acrossthe feasible set can often be quite close. However, there is large spread in thescores, which is present for all datasets. This spread demonstrates the lack ofrobustness in the faithfulness evaluation and is part of the reason why manipu-lation is possible in this case. But, that alone would not be enough to allow formanipulation, since the different methods could have the same change in scoresfor different set of hyperparameters. However, the large standard deviation inTab. 7 shows that is not the case, since the ranking change between sets of hy-perparameters. In other words, the XAI methods react differently to differentsets of hyperparameters. This, in combination with the variation shown in ,is what allows for manipulation in this study. : Box plot showing faithfulness scores across all hyperparameter configurations inthe feasible set for each dataset. The plot illustrates that the average faithfulness scoreis similar between different XAI methods across datasets. However the high varianceenables a target manipulation. Note that the scores have been normalized dataset-wiseby the highest score to allow for comparison across datasets.",
  "Discussion and Limitations": "The hyperparameters described in Sec. 7.1 could be extended to include otherimportant choices such as the order of perturbation, i.e., descending or ascend-ing and the type of normalization function applied . Also, in all our ex-periments we repeatedly perturb the input until the entire image is perturbed,which is the standard approach in faithfulness analysis. However, when the ma-jority of pixels are removed there is danger of OOD effects (see e.g. ), whichcan influence the evaluation outcome . An alternative approach would be toonly perturb parts of an image to avoid such OOD effects. One example is toperturb until the prediction changes and then stop . But this introduces yetanother hyperparamter, which further increases the scope for manipulation.Our proposed MRR is a simple approach to combat the problem of manipu-lation, but it also has drawbacks. Most prominently, the computational cost risesquickly when more methods and hyperparameters are considered. Also, MMRrequires domain expertise to determine the feasible set of hyperparameters. If theselection of the feasible set is done incorrectly, it might exacerbate the problemof manipulation since it can increase the amount of hyperparameters to choosefrom. MRR is also a ranking-based approach, where the scores depend on theset of explanation methods used in the analysis, including the cardinality of thatset. Since the rankings are relative, they do not allow for meaningful compar-isons across different tasks. To address this, we propose creating an open-sourcedatabase, leveraging tools like Quantus and OpenXAI , to efficiently storeand standardise benchmarking results, thereby supporting researchers with thedevelopment and XAI evaluation. For future work, we further aim to expand theparameter sensitivity analysis to other families of quantitative measures such asrandomisation and robustness which rely on parameters such assegmentation masks and noise perturbation methods, respectively.",
  "Conclusion": "We have presented two general-purpose methods for manipulating the quanti-tative evaluation of explanation methods. Intra-manipulation which increasesthe performance of a single method and inter-manipulation which manipulatesa comparative analysis of XAI methods. The motivation for these methods isbased on the lack of ground truth explanations, which makes the selection of hy-perparameters in quantitative evaluation for XAI challenging. We demonstratethe effectiveness of our manipulation strategies across numerous vision datasetsand XAI methods for faithfulness metrics, with results indicating that there issignificant room for manipulation of the evaluation outcome. This has potentiallybig implications for the XAI community, as it shows that evaluation outcomescannot always be \"taken at face value\" and therefore, trusted. Lastly, we presenta new ranking-based procedure that aims to improve the reliability of quanti-tative evaluation of XAI. We believe that this work highlights the difficulty ofconducting reliable XAI evaluation and emphasizes the importance of a holisticand transparent approach to evaluation in XAI.",
  "From Flexibility to Manipulation: The Slippery Slope of XAI Evaluation15": "1. Adebayo, J., Gilmer, J., Muelly, M., Goodfellow, I., Hardt, M., Kim, B.: Sanitychecks for saliency maps. In: Proceedings of the 32nd International Conference onNeural Information Processing Systems. p. 95259536. NIPS18, Curran AssociatesInc., Red Hook, NY, USA (2018) 2. Agarwal, C., Krishna, S., Saxena, E., Pawelczyk, M., Johnson, N., Puri, I., Zit-nik, M., Lakkaraju, H.: OpenXAI: Towards a transparent evaluation of modelexplanations. In: Thirty-sixth Conference on Neural Information Processing Sys-tems Datasets and Benchmarks Track (2022),",
  ". Arras, L., Osman, A., Samek, W.: Clevr-xai: A benchmark dataset for the groundtruth evaluation of neural network explanations. Information Fusion 81, 1440(2022)": "6. Arya, V., Bellamy, R.K.E., Chen, P., Dhurandhar, A., Hind, M., Hoffman, S.C.,Houde, S., Liao, Q.V., Luss, R., Mojsilovic, A., Mourad, S., Pedemonte, P.,Raghavendra, R., Richards, J.T., Sattigeri, P., Shanmugam, K., Singh, M., Varsh-ney, K.R., Wei, D., Zhang, Y.: One explanation does not fit all: A toolkitand taxonomy of AI explainability techniques. CoRR abs/1909.03012 (2019), 7. Bach, S., Binder, A., Montavon, G., Klauschen, F., Mller, K.R., Samek, W.: Onpixel-wise explanations for non-linear classifier decisions by layer-wise relevancepropagation. PLOS ONE 10(7), e0130140 (Jul 2015). 8. Bansal, N., Agarwal, C., Nguyen, A.: SAM: the sensitivity of attribution methodsto hyperparameters. In: 2020 IEEE/CVF Conference on Computer Vision andPattern Recognition, CVPR Workshops 2020, Seattle, WA, USA, June 14-19, 2020.pp. 1121. Computer Vision Foundation / IEEE (2020) 9. Bhatt, U., Weller, A., Moura, J.M.F.: Evaluating and aggregating feature-basedmodel explanations. In: Bessiere, C. (ed.) Proceedings of the Twenty-Ninth Inter-national Joint Conference on Artificial Intelligence, IJCAI 2020. pp. 30163022.ijcai.org (2020)",
  ". Brocki, L., Chung, N.C.: Evaluation of interpretability methods and perturbationartifacts in deep neural networks. CoRR abs/2203.02928 (2022)": "14. Brunke, L., Agrawal, P., George, N.: Evaluating input perturbation methods forinterpreting CNNs and saliency map comparison. In: Computer Vision ECCV2020 Workshops, pp. 120134. Springer International Publishing (2020) 15. Bykov, K., Hedstrm, A., Nakajima, S., Hhne, M.M.: Noisegrad - enhancing ex-planations by introducing stochasticity to model weights. In: Thirty-Sixth AAAIConference on Artificial Intelligence, AAAI 2022, Thirty-Fourth Conference on In-novative Applications of Artificial Intelligence, IAAI 2022, The Twelveth Sympo-sium on Educational Advances in Artificial Intelligence, EAAI 2022 Virtual Event,February 22 - March 1, 2022. pp. 61326140. AAAI Press (2022) 16. Chalasani, P., Chen, J., Chowdhury, A.R., Wu, X., Jha, S.: Concise explanationsof neural networks using adversarial training. In: III, H.D., Singh, A. (eds.) Pro-ceedings of the 37th International Conference on Machine Learning. Proceedingsof Machine Learning Research, vol. 119, pp. 13831391. PMLR (1318 Jul 2020),",
  ". Fong, R.C., Vedaldi, A.: Interpretable explanations of black boxes by meaningfulperturbation. In: 2017 IEEE International Conference on Computer Vision (ICCV).pp. 34493457 (2017)": "22. Hase, P., Xie, H., Bansal, M.: The out-of-distribution problem in explainability andsearch methods for feature importance explanations. In: Ranzato, M., Beygelzimer,A., Dauphin, Y.N., Liang, P., Vaughan, J.W. (eds.) Advances in Neural Informa-tion Processing Systems 34: Annual Conference on Neural Information ProcessingSystems 2021, NeurIPS 2021, December 6-14, 2021, virtual. pp. 36503666 (2021)",
  "chine Learning Research 24(34), 111 (2023),": "27. Hooker, S., Erhan, D., Kindermans, P.J., Kim, B.: A benchmark for in-terpretability methods in deep neural networks. In: Wallach, H., Larochelle,H., Beygelzimer, A., d'Alch-Buc, F., Fox, E., Garnett, R. (eds.) Advancesin Neural Information Processing Systems. vol. 32. Curran Associates, Inc.(2019), 28. Karimi, A.H., Muandet, K., Kornblith, S., Schlkopf, B., Kim, B.: On the rela-tionship between explanation and prediction: A causal view. In: XAI in Action:Past, Present, and Future Applications (2023), 29. Kermany, D.S., Goldbaum, M., Cai, W., Valentim, C.C., Liang, H., Baxter, S.L.,McKeown, A., Yang, G., Wu, X., Yan, F., Dong, J., Prasadha, M.K., Pei, J., Ting,M.Y., Zhu, J., Li, C., Hewett, S., Dong, J., Ziyar, I., Shi, A., Zhang, R., Zheng,L., Hou, R., Shi, W., Fu, X., Duan, Y., Huu, V.A., Wen, C., Zhang, E.D., Zhang,C.L., Li, O., Wang, X., Singer, M.A., Sun, X., Xu, J., Tafreshi, A., Lewis, M.A.,Xia, H., Zhang, K.: Identifying medical diagnoses and treatable diseases by image-based deep learning. Cell 172(5), 11221131.e9 (2018).",
  ". Rieger, L., Hansen, L.K.: IROF: a low resource evaluation metric for explanationmethods. CoRR abs/2003.08747 (2020),": "41. Rong, Y., Leemann, T., Borisov, V., Kasneci, G., Kasneci, E.: A consistent andefficient evaluation strategy for attribution methods. In: Proceedings of the 39thInternational Conference on Machine Learning. pp. 1877018795. PMLR (2022) 42. Rong, Y., Leemann, T., Borisov, V., Kasneci, G., Kasneci, E.: A consistent andefficient evaluation strategy for attribution methods. In: International Conferenceon Machine Learning. pp. 1877018795 (2022) 43. Samek, W., Binder, A., Montavon, G., Lapuschkin, S., Mller, K.: Evaluatingthe visualization of what a deep neural network has learned. IEEE Trans. NeuralNetworks Learn. Syst. 28(11), 26602673 (2017)",
  ". Sundararajan, M., Taly, A.: A note about: Local explanation methods for deep neu-ral networks lack sensitivity to parameter values. CoRR abs/1806.04205 (2018)": "49. Sundararajan, M., Taly, A., Yan, Q.: Axiomatic attribution for deep networks. In:Precup, D., Teh, Y.W. (eds.) Proceedings of the 34th International Conferenceon Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017.Proceedings of Machine Learning Research, vol. 70, pp. 33193328. PMLR (2017), 50. Sundararajan, M., Taly, A., Yan, Q.: Axiomatic attribution for deep networks. In:Precup, D., Teh, Y.W. (eds.) Proceedings of the 34th International Conferenceon Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017.Proceedings of Machine Learning Research, vol. 70, pp. 33193328. PMLR (2017)",
  ". Theiner, J., Mller-Budack, E., Ewerth, R.: Interpretable semantic photo geolo-calization. CoRR abs/2104.14995 (2021),": "52. Tomsett, R., Harborne, D., Chakraborty, S., Gurram, P., Preece, A.D.: Sanitychecks for saliency metrics. In: The Thirty-Fourth AAAI Conference on ArtificialIntelligence, AAAI 2020, The Thirty-Second Innovative Applications of ArtificialIntelligence Conference, IAAI 2020, The Tenth AAAI Symposium on EducationalAdvances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February7-12, 2020. pp. 60216029. AAAI Press (2020)"
}