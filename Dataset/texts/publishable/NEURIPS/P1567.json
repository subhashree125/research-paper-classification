{
  "Abstract": "Parameter-efficient fine-tuning (PEFT) is an effective method for adapting pre-trained vision models to downstream tasks by tuning a small subset of parameters.Among PEFT methods, sparse tuning achieves superior performance by only ad-justing the weights most relevant to downstream tasks, rather than densely tuningthe whole weight matrix. However, this performance improvement has been ac-companied by increases in memory usage, which stems from two factors, i.e.,the storage of the whole weight matrix as learnable parameters in the optimizerand the additional storage of tunable weight indexes. In this paper, we propose amethod named SNELL (Sparse tuning with kerNELized LoRA) for sparse tuningwith low memory usage. To achieve low memory usage, SNELL decomposesthe tunable matrix for sparsification into two learnable low-rank matrices, savingfrom the costly storage of the whole original matrix. A competition-based spar-sification mechanism is further proposed to avoid the storage of tunable weightindexes. To maintain the effectiveness of sparse tuning with low-rank matrices,we extend the low-rank decomposition by applying nonlinear kernel functionsto the whole-matrix merging. Consequently, we gain an increase in the rank ofthe merged matrix, enhancing the ability of SNELL in adapting the pre-trainedmodels to downstream tasks. Extensive experiments on multiple downstream tasksshow that SNELL achieves state-of-the-art performance with low memory usage,endowing PEFT with sparse tuning to large-scale models. Codes are available at",
  "Introduction": "Fine-tuning has become a predominant way for adapting large pre-trained models to downstream taskswith limited training samples . Nevertheless, fine-tuning all model parameters requiressubstantial memory usage and is susceptible to over-fitting, making it costly and infeasible for large-scale models . To address these limitations, parameter-efficient fine-tuning (PEFT) has been proposed to tune a small subset of parameters while keeping other parametersfrozen. PEFT methods can be categorized into addition-based and reparameterization-based methods.The former attaches additional parameters to a frozen pre-trained backbone, while the latter adjuststhe original parameters in the pre-trained backbone. Addition-based methods have achieved remarkable performance on vision tasks. However,adopting additional parameters incurs extra computational costs during the inference process. Incontrast, reparameterization-based methods directly fine-tune the original parameters. Thesemethods select specific parameters, involving reduced memory usage compared to full-parameterfine-tuning. Based on the granularity of parameter selection, one primary approach focuses on",
  "Tunable Weight Indexes": ": (a) The high memory usage of sparse tuning arises from taking the whole weight matrix aslearnable parameters, in addition to the storage of the tunable weight indexes (typically representedas a binary mask). (b) Our framework (SNELL) only stores the learnable low-rank matrices in theoptimizer. (c) Memory usage comparison on pre-trained models with different depths. specific parameter matrices. For example, Bitfit only adjusts bias to reduce the volume of tunableparameters while Partial-k fine-tunes the last few layers to avoid back-propagation through theentire pre-trained backbone. To further reduce memory usage, LoRA optimizes each selectedweight matrix using two low-rank matrices to achieve memory-efficient fine-tuning. Althoughsufficient in reducing memory usage, these methods usually gain inferior performance compared toaddition-based methods . Recently, SPT and GPS found that combining existing PEFTmethods with sparse tuning, which only adjusts the most task-related weights in a matrix, can achievestate-of-the-art performance on vision tasks. Concurrently, the effectiveness of sparse tuning has alsobeen observed in NLP tasks . By focusing on individual weights in a matrix, sparse tuning allowsfor more precise adjustments, thus achieving good performance and mitigated over-fitting risks . However, the performance gained from sparse tuning has been accompanied by high memory usage,as (a) shows. Although sparse tuning only updates part of weights in the pre-trained weightmatrix, the whole matrix still needs to be stored as learnable parameters in the optimizer and computedfor their corresponding gradients in practice. Additionally, sparse tuning necessitates storing thetunable weight indexes, further aggravating the memory demands. The above observation indicatesthat sparse tuning gains no advantage over full fine-tuning regarding memory usage, especially giventhe increasing parameter volumes in pre-trained models . A sparse tuning method with lowmemory usage is urgently required for applications on large-scale pre-trained models. In this paper, we propose a method that conducts Sparse tuning with kerNELized LoRA (SNELL)shown in (b). SNELL can adapt pre-trained models to downstream tasks with both lowmemory usage and strong performance. To reduce memory usage, we decompose the tunable matrixfor sparsification into low-rank learnable matrices to store fewer parameters in the optimizer anddevelop a competition-based method to avoid storing the tunable weight indexes. To improve theperformance on downstream tasks, we extend LoRA from a kernel perspective and merge low-rankmatrices with nonlinear kernel functions to obtain matrices with higher ranks. Specifically, SNELL updates the pre-trained weight matrix using a sparse low-rank adaptation matrix.This adaptation matrix is first merged with two low-rank learnable matrices and then sparsifiedtoward effective fine-tuning. Compared to storing the whole adaptation matrix, storing low-rankmatrices in the optimizer results in lower memory usage. For the sparsification process, we propose acompetition-based method inspired by the neuron competition phenomenon in neuroscience ,avoiding the storage of the tunable weight indexes that incur additional memory usage. The proposedmethod promotes competition among weights based on their absolute values. Most task-relevantweights are encouraged to have larger absolute values and survive during the fine-tuning process. Bysetting a sparsity ratio as the hyperparameter and determining tunable weights based on their absolutevalues in an end-to-end manner, we can eliminate the storage of the tunable weight indexes. In addition to low memory usage, the performance is also critical for model fine-tuning. However,directly merging two low-rank matrices through the inner product leads to the low-rank structure ofthe adaptation matrix, which narrows the optimization scope of tunable matrices and further limitsthe expressiveness of sparse tuning. To overcome this bottleneck, we draw inspiration from DyN on weight matrix interpretation based on low-dimensional dynamical systems, and reformulate themerging process with nonlinear kernel functions that increase the rank of the merged adaptationmatrix. This new formulation enables a more expressive sparse tuning while maintaining a compactrepresentation with low memory. Extensive experiments are conducted on 24 downstream visual recognition tasks with both plain andhierarchical vision Transformer backbones under supervised and self-supervised pre-training. Resultsshow that SNELL can gain the performance improvement of sparse tuning and the low memoryusage of LoRA concurrently. SNELL obtains the state-of-the-art performance on FGVC (91.8% vs.90.7%) and VTAB-1k (74.6% vs. 74.1%) benchmark with LoRA-level memory usage. Moreover, as(c) shows, the low memory-usage advantage of SNELL becomes increasingly apparent asthe model size grows, enabling sparse tuning on larger models.",
  "Related Work": "Parameter Sparsity. In early work, the parameter sparsity usually serves as an optimization objectivein model pruning . These pruning methods remove the weights from pre-trained modelsirrelevant to a specific task, without significantly degrading model performance. The relevance ofindividual weights can be estimated based on activations , redundancy , per-layer secondderivatives , and energy efficiency . Except for the post-training pruning strategy, sparse net-works directly introduce parameter sparsity into the training process, removing redundantweights more precisely . Motivated by the advantage of parameter sparsity in model optimization,recent studies introduce sparsity to the fine-tuning of pre-trained models and achieve enhanced modelperformance on downstream tasks . The parameter sparsity gives rise to a reduced numberof trainable parameters and serves as a regularization constraint during fine-tuning . Amongsparse tuning, pre-pruning methods adopt model pruning for fine-tuning. These methods sparsify theweight matrix or adapter through pruning metrics to identify learnable parametersfor the fine-tuning process. Other methods select trainable parameters during fine-tuning, includinglearnable mask or diff vectors with sparsity constraints. However, the parameter sparsifica-tion methods need to store the indexes of tunable weights, which incurs additional memory usage. Forsparse tuning under low memory budget, our competition-based mechanism selects weights relevantto downstream tasks in a learnable manner without storing the tunable weight indexes. Parameter-efficient Fine-tuning. Fine-tuning is the most predominant approach for adapting apre-trained model to downstream tasks. However, for large pre-trained models, fine-tuning allparameters is costly and prone to overfit downstream datasets. To tackle these problems, parameter-efficient fine-tuning (PEFT) , which tunes only a tiny portion of parameters, becomesa desirable choice. Following the taxonomy of SPT , PEFT methods can be categorized intoaddition-based and reparameterization-based methods. Addition-based methods attach additional trainable parameters to a frozen pre-trained backbone.Adapters adopt a residual pathway and learn a bottleneck layer including twolinear projections and a non-linear activation. Prompt-tuning methods add trainableparameters to the input and keep the entire pre-trained model unchanged during training. Recentwork attempts to find the optimal configurations to combine multiple addition-based methods.Despite of the popularity and effectiveness of addition-based methods, the additional trainableparameters incur excess computational costs during the inference process . Reparametization-based methods adjust the inherent parameters in the pre-trained backbone toavoid excess computational costs during inference. Early work directly selects parameters with lowmemory usage for fine-tuning, such as the bias terms and the final few layers of the pre-trainedmodel . To further reduce the memory usage of the selected matrices, LoRA optimizeslow-rank matrices that can be reparameterized into the pre-trained weight matrices to reduce memoryusage. Exploring finer-grained parameter selection, some studies propose sparse tuning,which involves selecting and tuning individual weights sparsely within the weight matrices. Recently,SPT combines sparse tuning and LoRA in a hybrid framework that achieves state-of-the-artperformances on visual PEFT tasks. SPT has revealed that optimizing the weights most relevant tothe downstream task through sparse tuning can significantly enhance the performance, which is alsosupported by SAM and GPS . However, existing sparse tuning framework still faces thechallenge of high memory usage brought by sparse tuning. Unlike existing methods, our SNELL",
  "Preliminaries": "Sparse Tuning. Given a downstream training set D = {x(n), y(n)}Nn=1, the objective of sparse tuningis to minimize the models empirical risk on the downstream task, with the sparsity constraints onthe volume of tunable weights in weight matrix W Rmn. The sparsification is usually achievedthrough a binary mask M {0, 1}mn. The objective function can be formulated as",
  "n=1Lf(x(n); W), y(n)(1)": "where f(; ) is a parameterized function over the input (e.g., a neural network), L(, ) is a lossfunction (e.g., cross-entropy), and denotes element-wise multiplication. The binary mask M can beeither a fixed hyperparameter, pre-computed with heuristics such as pre-pruning , or a learnableparameter obtained through end-to-end fine-tuning . All these methods require storing M todetermine the tunable weights, which results in additional memory usage. More importantly, thetunable parameters W M occupy the same amount of memory as the weight matrix W in practice.As a result, the memory usage of sparse tuning is even higher than that of full fine-tuning. LoRA. Given a pre-trained weight matrix W0, LoRA optimizes two low-rank matrices B Rmr, A Rnr to reduce the memory usage during fine-tuning. The low-rank matrices A and Bcan be reparameterized into the pre-trained weight W0,",
  "With r min(m, n), LoRA can achieve high training efficiency and low memory usage by onlyoptimizing the smaller low-rank matrices": "Kernel Trick . In many machine learning tasks, mapping the vectors into higher dimensions isfrequently used to achieve linear separability . However, the explicit mapping process incurssignificant computational costs. To address this problem, the kernel trick is proposed to efficientlymodel data relationships in high-dimensional spaces, without the need to explicitly formulate the space.According to Mercers theorem , a kernel function : Rr Rr R can express an inner productin some space as (x, x) = (x)(x), if and only if is positive semi-definite (Appendix B).x, x Rr, and : Rr Rd is an implicit feature map. By selecting an appropriate kernel function, we can obtain the inner product of two vectors in higher-dimensional space Rd (d r) withoutexplicitly formulating the feature map .",
  "Kernelized LoRA": "We leverage LoRA to reduce the memory usage of sparse tuning in light of its low memory usage. Anintuitive solution is to sparsify the adaptation matrix W composed of the two low-rank matrices.However, the low-rank property of W can lead to the performance degradation of sparse tuning.For the original sparse tuning, the weight matrix W is free of the rank constraint, and weights areindependent of each other. Therefore, we can independently select and optimize weights most relevantto the downstream task. For sparse tuning with LoRA, the adaptation matrix W with rank r isconstrained in Rr(m+nr), a subspace of Rmn. When r min(m, n), the weight optimizationscope of sparse tuning contracts, hindering its performance on downstream tasks.",
  "To achieve sparse tuning with both strong performance and low memory usage, we propose toconstruct a high-rank matrix using low-rank matrices. Inspired by DyN that fits a high-rank": ": Overview of our SNELL strategy. Given two learnable low-rank matrices, we merge themusing a non-linear kernel function (left). This merging process is equivalent to mapping the matricesto higher-rank matrices and then performing matrix multiplication. Then we sparsified this mergedadaptation matrix using a competition-based sparsification mechanism (right). This mechanism zerosout weights with small absolute values based on the specified percentage of s. matrix using the distance matrix of a low-dimension dynamical system, we extend the distancefunction to general kernel functions and investigate LoRA in the kernel perspective. Given twovectors x, x Rr, the kernel function (x, x) can be formulated as an inner product (x)(x)with an implicit feature map : Rr Rd. The merging process of LoRA can be seen as applyinglinear kernel function l(, ) on the rows of the learnable parameters A and B,",
  "Wij = l(Ai,, Bj,) = l(Bj,)l(Ai,) = Bj,Ai,,(3)": "where Ai,, Bj, Rr, l : Rr Rr denotes the identity mapping. By replacing l(, ) with morecomplex non-linear kernel functions, we can approximate relations in higher-dimensional spacesRd and obtain matrices with rank larger than r. The merged adaptation matrix in SNELL can berepresented byW = ((Ai,, Bj,))mn = [(B1,), ..., (Bn,)][(A1,), ..., (An,)] = BA .(4)Note that in practice, explicit computation of A Rnd and B Rmd is unnecessary. W canbe directly derived based on A and B with the kernel function . By extending LoRA in a kernelperspective, SNELL can build high-rank adaptation matrices based on low-rank learnable matrices,empowering strong sparse tuning with low memory usage. We utilize the piecewise linear kernelintroduced in Appendix B without a specific statement.",
  "Competition-based Sparsification Mechanism": "Existing methods store tunable weight indexes M {0, 1}mn for sparsifying the update of theweight matrix W Rmn. The storage of M leads to additional memory usage. Inspired by theneuron competition phenomenon in neuroscience , we design a competition-based parametersparsification mechanism to avoid this additional storage. Instead of determining the learnable weightsin the optimization process based on M, our objective is to encourage the weights to compete basedon their contributions to performance improvement. Weights with stronger contributions survive in thesparsification while the remaining low-contributed weights are zeroed out. The weight contribution isreflected in their absolute values during the end-to-end optimization. During optimization, weightscontributing more to the loss reduction are encouraged to have more significant values, while weightscontributing less approach zero. By retaining higher importance to significant weights and zeroingout the less impactful weights, we can achieve end-to-end tunable parameter selection by solelyrelying on the absolute values of weights, avoiding the storage of M. Specifically, given a merged adaptation matrix W and a sparsity ratio s , we sparsifyweights with a soft-threshold function. To induce weight competition during end-to-end fine-tuning,we propose a dynamic threshold ws, i.e., the weight having the smn-th smallest absolute valuein W. This threshold ensures that only a fixed proportion (s 100%) of weights remain non-zero.Therefore, the weights have to compete with each other to be selected instead of just having a largerabsolute value than a fixed threshold.Wsij = Wij max(|Wij| |ws|, 0),(5) where Ws = (Wsij)mn denotes the sparse matrix with sparsity ratio s. In practice, the sparsityratio s is manually determined regarding specific downstream tasks. Given a sparsity ratio s, thetraining objective in Equation 1 can be reformulated as",
  "Experimental Setup": "Datasets and Metrics. We evaluate our methods on 24 downstream tasks categorized into twogroups following SPT . (i) FGVC is a benchmark for fine-grained image classification. Thisbenchmark includes 5 downstream tasks, which are CUB-200-2011 , NABirds , OxfordFlowers , Stanford Dogs and Stanford Cars . We follow the validation splits in if theofficial validation set is unavailable. (ii) VTAB-1k is a large-scale transfer learning benchmarkconsisting of 19 visual classification tasks. VTAB-1k can be further divided into three groups, i.e.,natural tasks with natural images, specialized tasks with images captured by specialized equipment,and structured tasks with images mostly generated from synthetic environments. We use top-1accuracy averaged within each group as our main metric following . Pre-trained Backbones. We conduct experiments on the plain vision Transformer backbone ViT-B/16 that is pre-trained on ImageNet with different pre-training strategies following ,including supervised pre-training and self-supervised pre-training with MAE and MoCo v3 .We also conduct experiments on the representative hierarchical vision Transformer backbone Swin-B and CNN backbone ConvNeXt-Base under supervised pre-training. In addition, wefine-tune the supervised pre-trained large-scale models (ViT-L/16 , ViT-H/14 ) on VTAB-1kto demonstrate the memory-efficiency and high-performance of SNELL. Competitors. We compare our methods with addition-based methods including MLP-k, VPT-Shallow , VPT-Deep , Adapter-r , and SPT-Adapter . For reparameterization-basedmethods, we compare with Linear, Partial-1, Bias , LoRA-r , SSF , and SPT-LoRA .Here r represents the number of bottleneck dimensions in Adapter-r and the value of rank in LoRA-rand our proposed SNELL-r. Details of the competitors are presented in Appendix A.1. We alsoprovide additional comparisons with other approaches in Appendix C.1. Implementation Details. Following SPT , we use the AdamW optimizer with cosinelearning rate decay. The batch size, learning rate, and weight decay are 32, 1e 3, and 1e 4,respectively. We also follow SPT to implement the standard data augmentation pipeline forVTAB-1K and follow SSF for FGVC as well. SNELL is applied on the pre-trained weightmatrix of all linear layers. For each task, we fine-tune the model with different sparsity ratios s tosearch the optimal volume of tunable parameters for this task. Without specific stating, we adoptthe piecewise linear kernel (introduced in Appendix B) as the kernel function for SNELL. Ablationstudies on different kernel functions are presented in .",
  "Performance on Downstream Tasks": "Performance on Different Benchmarks. Experiments on FGVC and VTAB-1k benchmarks indicatethat SNELL achieves the best performance with supervised pre-trained ViT-B/16 backbone as shownin . SNELL gains large performance improvements over LoRA variants, e.g., SNELL-8surpasses LoRA-8 significantly by 5.5% in terms of mean accuracy on the FGVC benchmark.Moreover, SNELL outperforms the state-of-the-art method SPT-LoRA by a clear margin of 0.5% interms of mean top-1 accuracy on the VTAB-1k benchmark. This stems from the fact that SPT-LoRAonly performs sparse tuning on a portion of the weight matrices while employing LoRA for theremaining part. In contrast, the low memory property of SNELL empowers sparse tuning on all theweight matrices, allowing for more precise adjustments and giving rise to superior performance.",
  "SNELL-8 (ours)68.383.863.571.876.886.063.775.5": "Performance on Different Pre-training Strategies. Experimental results on models pre-trained us-ing different strategies are presented in . SNELL outperforms the state-of-the-art performanceson models pre-trained with MAE (71.8% vs. 69.8%) and MoCo v3 (75.5% vs. 75.3%). Furthermore,SNELL consistently outperforms other PEFT methods on every group of downstream datasets. Thisdemonstrates the general effectiveness of SNELL under different pre-training strategies. Performance on Different Architectures. Following VPT and SPT , we apply SNELL tothe hierarchal vision transformer Swin-B and the CNN architecture ConvNeXt-Base. Experimentalresults are shown in . Results on Swin-B demonstrate that SNELL-8 outperforms existingreparameterization-based PEFT methods by 0.3% and achieves comparable performance to the state-of-the-art addition-based method SPT-Adapter. For ConvNeXt-Base, SNELL achieves a performanceimprovement of 0.4% compared to the best-reported result. These results obtained on differentarchitectures further validate the versatility and effectiveness of our SNELL approach. Memory Usage Comparison. We illustrate the effectiveness of SNELL in terms of memory usageby comparing it with various PEFT methods. (a) shows the accuracy and memory usageof different methods on ViT-B/16. Although some methods achieve satisfactory performance, theirmemory usage is excessively large, even surpassing that of full fine-tuning (e.g. SPT-Adapter andVPT-Deep). In comparison, SNELL achieves superior performance on downstream tasks withmemory usage comparable to memory-efficient methods, including LoRA and Adapter.",
  "(a)(b)(c)": ": (a) The fitting ability of different kernel functions. We fit random sparse matrices bymerging two learnable low-rank matrices with different kernel functions and compute the MSE loss.(b) Performance comparison on groups of datasets in VTAB-1k. (c) Training loss on CIFAR-100dataset in VTAB-1k benchmark of kernelized LoRA with different kernel functions. : The optimal sparsity ratio of SNELL-8 on different downstream tasks (left) and the averageoptimal sparsity ratio within each group (right) in VTAB-1k benchmark. The pre-trained model isthe ConvNeXt-B pre-trained on ImageNet-21k. significantly across different downstream tasks within the same group (e.g., Cifar vs. Sun397, dSpr-loc vs. Clevr-Dist). Furthermore, we can observe that the Natural task group exhibits a higher averageoptimal sparsity ratio compared to the Specialized group, while the Structured group demonstrates thelowest ratio. This observation aligns with the example illustrated in Figure A6, where cross-domainadaptation from a model pre-trained on natural images (ImageNet) to images of Specialized andStructured groups necessitates a larger number of tunable parameters.",
  "LoRA-881.286.653.473.777.984.855.972.9SNELL-882.386.956.675.379.585.156.973.8": ": (a) Performance on VTAB-1k of sparsifying a full-rank matrix, the merged adaptation matrixof LoRA-8 and kernelized LoRA-8 (KLoRA-8) with sparsity ratio s = 0.9. (b) The mean accuracyon VTAB-1k of kernelized LoRA (KLoRA) and SNELL (KLoRA+sparsifying) with different ranksof learnable matrices. Perf. Imp. denotes the performance improvement of SNELL over KLoRA.",
  "Ablation Studies": "Effect of Kernelized LoRA. We explore the effectiveness of kernelized LoRA by comparing theperformance of sparsifying a full-rank matrix, the merged adaptation matrix of LoRA, and the mergedadaptation matrix of kernelized LoRA. Experimental results are presented in (a). We can seethat sparsifying the merged adaptation matrix of LoRA significantly underperforms a full-rank matrix.This reveals that the low-rank property of the merged adaptation matrix in LoRA greatly compromisesthe weight selection scope, leading to performance degradation for sparse tuning. However, when wereplace LoRA with kernelized LoRA, the performance becomes notably comparable to that of thefull-rank matrix under the strong sparsity constraint (s = 0.9). This indicates that kernelized LoRAcan effectively leverage sparse tuning while maintaining a low memory usage. Effect of Sparse Tuning. (b) shows the performance comparison between SNELL andkernelized LoRA to explore the effectiveness of sparse tuning. For kernelized LoRA with differentranks, applying sparse tuning can consistently improve their performance. Moreover, as the rank ofthe learnable matrix increases, the performance of kernelized LoRA decreases while that of SNELLincreases. This difference stems from the model regularization. Similar to sparse regularization, thelow-rank property of LoRA that constrains the dependence between individual weights, can also betaken as a form of regularization. As the rank of the learnable matrix increases, the effect of low-rankregularization diminishes. Consequently, kernelized LoRA becomes more susceptible to over-fittingand encounters performance degradation. In contrast, SNELL employs both low-rank and sparseregularization. Higher ranks enable better sparsification towards downstream tasks, boosting sparseregularization that counteracts the diminished low-rank regularization. Therefore, a higher rank maylead to over-fitting in kernelized LoRA, but it can further enhance performance with sparse tuning. Effect of Different Kernel Function. We investigate the effectiveness of different kernel functions inkernelized LoRA. First, we explore the ability of different kernel functions to fit randomly generatedfull-rank matrices based on low-rank matrices using the gradient descent algorithm (introduced inAppendix A.3). As shown in (a), we explored four kinds of kernel functions. Compared withthe linear kernel function, nonlinear kernel functions can reconstruct the full-rank target matrix moreaccurately based on the low-rank matrices. Subsequently, we explore the performance of kernelizedLoRA with different kernel functions for pre-trained model fine-tuning in (b). We findthat using piecewise linear distance as the kernel function can achieve better results compared tolinear kernel function (LoRA), while using Sigmoid and RBF kernels leads to severe performancedegradation. This is because the complex non-linear kernel functions such as the exponential functionincrease the optimization difficulty in deep networks shown in (c). More comparisonsbetween LoRA and kernelized LoRA (with piecewise linear kernel) are presented in Appendix C.3.2.",
  "Conclusion": "In this work, we proposed a PEFT method named SNELL (Sparse tuning with kerNELized LoRA)to conduct high-performance sparse tuning with low memory usage. To reduce memory usage, wesparsified the adaptation matrix merged with low-rank matrices rather than the pre-trained weightmatrix to reduce the volume of learnable parameters stored in the optimizer. Then we designed acompetition-based sparsification mechanism to avoid the additional memory usage of storing thetunable weight indexes. To reveal the effectiveness of sparse tuning, we utilize nonlinear kernelfunctions to merge the adaptation matrix, increasing the rank of the merged matrix to maintain acompact representation suitable for sparse tuning with low memory usage. Extensive experimentsdemonstrated the ability of SNELL to leverage the high performance of sparse tuning and the lowmemory usage of LoRA. For future work, we will apply SNELL on larger models such as LLMs andimprove its training efficiency. For limitations discussion, please refer to Appendix E.2.",
  "Acknowledgement": "This work was supported in part by the National Key R&D Program of China under Grant2023YFC2508704, in part by the National Natural Science Foundation of China: 62236008,U21B2038, and 61931008, and in part by the Fundamental Research Funds for the Central Universi-ties. The authors would like to thank Zhengqi Pei, Yue Wu, and the anonymous reviewers for theirconstructive comments and suggestions that improved this manuscript. Alan Ansell, Edoardo Ponti, Anna Korhonen, and Ivan Vulic. Composable sparse fine-tuningfor cross-lingual transfer. In Proceedings of the 60th Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers), pages 17781796, Dublin, Ireland, 2022.Association for Computational Linguistics. Yutong Bai, Xinyang Geng, Karttikeya Mangalam, Amir Bar, Alan Yuille, Trevor Darrell,Jitendra Malik, and Alexei A Efros. Sequential modeling enables scalable learning for largevision models. ArXiv preprint, abs/2312.00785, 2023. Ankur Bapna and Orhan Firat. Simple, scalable adaptation for neural machine translation. InProceedings of the 2019 Conference on Empirical Methods in Natural Language Processingand the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),pages 15381548, Hong Kong, China, 2019. Association for Computational Linguistics. Guillaume Bellec, David Kappel, Wolfgang Maass, and Robert A. Legenstein. Deep rewiring:Training very sparse deep networks. In 6th International Conference on Learning Repre-sentations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference TrackProceedings. OpenReview.net, 2018. Elad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel. BitFit: Simple parameter-efficientfine-tuning for transformer-based masked language-models. In Proceedings of the 60th AnnualMeeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 19,Dublin, Ireland, 2022. Association for Computational Linguistics.",
  "Alain Berlinet and Christine Thomas-Agnan. Reproducing kernel Hilbert spaces in probabilityand statistics. Springer Science & Business Media, 2011": "Sergi Caelles, Kevis-Kokitsi Maninis, Jordi Pont-Tuset, Laura Leal-Taix, Daniel Cremers, andLuc Van Gool. One-shot video object segmentation. In 2017 IEEE Conference on ComputerVision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pages53205329. IEEE Computer Society, 2017. Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang, Yibing Song, Jue Wang, and Ping Luo.Adaptformer: Adapting vision transformers for scalable visual recognition. Advances in NeuralInformation Processing Systems, 35:1666416678, 2022. Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple frameworkfor contrastive learning of visual representations. In Proceedings of the 37th InternationalConference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 ofProceedings of Machine Learning Research, pages 15971607. PMLR, 2020. Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervisedvision transformers. In 2021 IEEE/CVF International Conference on Computer Vision, ICCV2021, Montreal, QC, Canada, October 10-17, 2021, pages 96209629. IEEE, 2021. Zihang Dai, Hanxiao Liu, Quoc V. Le, and Mingxing Tan. Coatnet: Marrying convolution andattention for all data sizes. In MarcAurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin,Percy Liang, and Jennifer Wortman Vaughan, editors, Advances in Neural Information Process-ing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS2021, December 6-14, 2021, virtual, pages 39653977, 2021.",
  "E Dataset. Novel datasets for fine-grained image categorization. In First Workshop on FineGrained Visual Categorization, CVPR. Citeseer. Citeseer. Citeseer, 2011": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training ofdeep bidirectional transformers for language understanding. In Proceedings of the 2019 Confer-ence of the North American Chapter of the Association for Computational Linguistics: HumanLanguage Technologies, Volume 1 (Long and Short Papers), pages 41714186, Minneapolis,Minnesota, 2019. Association for Computational Linguistics. Ning Ding, Shengding Hu, Weilin Zhao, Yulin Chen, Zhiyuan Liu, Haitao Zheng, and MaosongSun. OpenPrompt: An open-source framework for prompt-learning. In Proceedings of the60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations,pages 105113, Dublin, Ireland, 2022. Association for Computational Linguistics. Xin Dong, Shangyu Chen, and Sinno Jialin Pan. Learning to prune deep neural networksvia layer-wise optimal brain surgeon. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio,Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors, Advancesin Neural Information Processing Systems 30: Annual Conference on Neural InformationProcessing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 48574867, 2017. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for imagerecognition at scale. In 9th International Conference on Learning Representations, ICLR 2021,Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainableneural networks. In 7th International Conference on Learning Representations, ICLR 2019,New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. Zihao Fu, Haoran Yang, Anthony Man-Cho So, Wai Lam, Lidong Bing, and Nigel Collier. Onthe effectiveness of parameter-efficient fine-tuning. In Proceedings of the AAAI Conference onArtificial Intelligence, volume 37, pages 1279912807, 2023. Timnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, and Li Fei-Fei. Fine-grained car detection for visual census estimation. In Satinder P. Singh and Shaul Markovitch,editors, Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, February4-9, 2017, San Francisco, California, USA, pages 45024508. AAAI Press, 2017. Demi Guo, Alexander Rush, and Yoon Kim. Parameter-efficient transfer learning with diffpruning. In Proceedings of the 59th Annual Meeting of the Association for ComputationalLinguistics and the 11th International Joint Conference on Natural Language Processing(Volume 1: Long Papers), pages 48844896, Online, 2021. Association for ComputationalLinguistics. Song Han, Xingyu Liu, Huizi Mao, Jing Pu, Ardavan Pedram, Mark A Horowitz, and William JDally. Eie: Efficient inference engine on compressed deep neural network. ACM SIGARCHComputer Architecture News, 44(3):243254, 2016. Haoyu He, Jianfei Cai, Jing Zhang, Dacheng Tao, and Bohan Zhuang. Sensitivity-aware visualparameter-efficient fine-tuning. In Proceedings of the IEEE/CVF International Conference onComputer Vision, pages 1182511835, 2023. Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollr, and Ross B. Girshick. Maskedautoencoders are scalable vision learners. In IEEE/CVF Conference on Computer Vision andPattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pages 1597915988. IEEE, 2022. Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross B. Girshick. Momentum contrast forunsupervised visual representation learning. In 2020 IEEE/CVF Conference on Computer Visionand Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages 97269735.IEEE, 2020. Grant Van Horn, Steve Branson, Ryan Farrell, Scott Haber, Jessie Barry, Panos Ipeirotis, PietroPerona, and Serge J. Belongie. Building a bird recognition app and large scale dataset withcitizen scientists: The fine print in fine-grained dataset collection. In IEEE Conference onComputer Vision and Pattern Recognition, CVPR 2015, Boston, MA, USA, June 7-12, 2015,pages 595604. IEEE Computer Society, 2015. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe,Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learningfor NLP. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th",
  "Hengyuan Hu, Rui Peng, Yu-Wing Tai, and Chi-Keung Tang. Network trimming: A data-drivenneuron pruning approach towards efficient deep architectures. ArXiv preprint, abs/1607.03250,2016": "Piotr Indyk and Sandeep Silwal. Faster linear algebra for distance matrices. In S. Koyejo,S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in NeuralInformation Processing Systems, volume 35, pages 3557635589. Curran Associates, Inc.,2022. Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan,and Ser-Nam Lim. Visual prompt tuning. In European Conference on Computer Vision, pages709727. Springer, 2022.",
  "Konstantinos Koutroumbas and Sergios Theodoridis. Pattern recognition. Academic Press,2008": "Wei-Hong Li, Xialei Liu, and Hakan Bilen. Cross-domain few-shot learning with task-specificadapters. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022,New Orleans, LA, USA, June 18-24, 2022, pages 71517160. IEEE, 2022. Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation.In Proceedings of the 59th Annual Meeting of the Association for Computational Linguisticsand the 11th International Joint Conference on Natural Language Processing (Volume 1: LongPapers), pages 45824597, Online, 2021. Association for Computational Linguistics.",
  "Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang Frank Wang,Kwang-Ting Cheng, and Min-Hung Chen. Dora: Weight-decomposed low-rank adaptation.ArXiv preprint, abs/2402.09353, 2024": "Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Lam Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang.P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks.ArXiv preprint, abs/2110.07602, 2021. Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and BainingGuo. Swin transformer: Hierarchical vision transformer using shifted windows. In 2021IEEE/CVF International Conference on Computer Vision, ICCV 2021, Montreal, QC, Canada,October 10-17, 2021, pages 999210002. IEEE, 2021. Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and SainingXie. A convnet for the 2020s. In IEEE/CVF Conference on Computer Vision and PatternRecognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pages 1196611976. IEEE,2022.",
  "Ilya Loshchilov and Frank Hutter. Fixing weight decay regularization in adam. 2018": "Christos Louizos, Max Welling, and Diederik P. Kingma. Learning sparse neural networksthrough l_0 regularization. In 6th International Conference on Learning Representations,ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings.OpenReview.net, 2018. Dmitry Molchanov, Arsenii Ashukha, and Dmitry P. Vetrov. Variational dropout sparsifiesdeep neural networks. In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34thInternational Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11August 2017, volume 70 of Proceedings of Machine Learning Research, pages 24982507.PMLR, 2017. Pavlo Molchanov, Arun Mallya, Stephen Tyree, Iuri Frosio, and Jan Kautz.Importanceestimation for neural network pruning. In IEEE Conference on Computer Vision and PatternRecognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 1126411272.Computer Vision Foundation / IEEE, 2019. Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a largenumber of classes. In 2008 Sixth Indian conference on computer vision, graphics & imageprocessing, pages 722729. IEEE, 2008.",
  "Zhengqi Pei and Shuhui Wang. Dynamics-inspired neuromorphic visual representation learning.In International Conference on Machine Learning, pages 2752127541. PMLR, 2023": "Jonas Pfeiffer, Aishwarya Kamath, Andreas Rckl, Kyunghyun Cho, and Iryna Gurevych.AdapterFusion: Non-destructive task composition for transfer learning. In Proceedings of the16th Conference of the European Chapter of the Association for Computational Linguistics:Main Volume, pages 487503, Online, 2021. Association for Computational Linguistics. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, ZhihengHuang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visualrecognition challenge. International journal of computer vision, 115:211252, 2015. Suraj Srinivas and R. Venkatesh Babu. Data-free parameter pruning for deep neural networks.In Xianghua Xie, Mark W. Jones, and Gary K. L. Tam, editors, Proceedings of the BritishMachine Vision Conference 2015, BMVC 2015, Swansea, UK, September 7-10, 2015, pages31.131.12. BMVA Press, 2015. Xue-Lian Sun, Zhen-Hua Chen, Xize Guo, Jingjing Wang, Mengmeng Ge, Samuel Zheng HaoWong, Ting Wang, Si Li, Mingze Yao, Laura A Johnston, et al. Stem cell competition drivenby the axin2-p53 axis controls brain size during murine development. Developmental Cell,58(9):744759, 2023. Yi-Lin Sung, Jaemin Cho, and Mohit Bansal. Vl-adapter: Parameter-efficient transfer learningfor vision-and-language tasks. In Proceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 52275237, 2022.",
  "Johan AK Suykens and Joos Vandewalle. Chaos control using least-squares support vectormachines. International journal of circuit theory and applications, 27(6):605615, 1999": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Openfoundation and fine-tuned chat models. ArXiv preprint, abs/2307.09288, 2023. Cheng-Hao Tu, Zheda Mai, and Wei-Lun Chao. Visual query tuning: Towards effectiveusage of intermediate representations for parameter and memory efficient transfer learning. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages77257735, 2023. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike vonLuxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and RomanGarnett, editors, Advances in Neural Information Processing Systems 30: Annual Conferenceon Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA,pages 59986008, 2017.",
  "Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. Thecaltech-ucsd birds-200-2011 dataset. 2011": "Runxin Xu, Fuli Luo, Zhiyuan Zhang, Chuanqi Tan, Baobao Chang, Songfang Huang, andFei Huang. Raise a child in large language model: Towards effective and generalizable fine-tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural LanguageProcessing, pages 95149528, Online and Punta Cana, Dominican Republic, 2021. Associationfor Computational Linguistics. Tien-Ju Yang, Yu-Hsin Chen, and Vivienne Sze. Designing energy-efficient convolutionalneural networks using energy-aware pruning. In 2017 IEEE Conference on Computer Visionand Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pages 60716079.IEEE Computer Society, 2017. Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transform-ers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,pages 1210412113, 2022. Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, MarioLucic, Josip Djolonga, Andre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, et al. Alarge-scale study of representation learning with the visual task adaptation benchmark. ArXivpreprint, abs/1910.04867, 2019.",
  "Yuanhan Zhang, Kaiyang Zhou, and Ziwei Liu.Neural prompt search.ArXiv preprint,abs/2206.04673, 2022": "Zhi Zhang, Qizhe Zhang, Zijun Gao, Renrui Zhang, Ekaterina Shutova, Shiji Zhou, andShanghang Zhang. Gradient-based parameter selection for efficient fine-tuning. In IEEE/CVFConference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June16-22, 2024, pages 2856628577. IEEE, 2024. Mengjie Zhao, Tao Lin, Fei Mi, Martin Jaggi, and Hinrich Schtze. Masking as an efficientalternative to finetuning for pretrained language models. In Proceedings of the 2020 Conferenceon Empirical Methods in Natural Language Processing (EMNLP), pages 22262241, Online,2020. Association for Computational Linguistics.",
  "VPT-Deep : freezes the backbone while appending additional trainable prompts to the sequencein the multi-head self-attention layer of each ViT block": "Adapter-r : freezes all the backbone parameters while adding a down projection, a ReLUnon-linearity, and an up projection layer sequentially in the feed-forward network (FFN) of eachvisual Transformer block. We report the performance implemented by for comparison. Lora-r : freezes all the backbone parameters while adding a concurrent branch including twolow-rank matrices to the weight matrices in the multi-head self-attention layers to approximateefficiently updating them. The low-rank matrices can be merged into the backbone weights afterfine-tuning. We report the performance implemented by for comparison. SPT : identifies the tunable parameters for a given task in a data-dependent way, and utilizesLoRA (SPT-LoRA) or Adapter (SPT-Adapter) for weight matrices with a large number of tunableparameters and sparse tuning for weight matrices with a small number of tunable parameters.",
  "Additional-based methods": "MLP-363.8 84.7 62.3 97.4 32.5 49.2 84.7 77.0 88.0 70.2 56.1 47.8 32.8 32.3 58.1 12.9 21.2 15.2 24.8 53.2VPT-Shallow 77.7 86.9 62.6 97.5 74.5 51.2 87.3 78.2 92.0 75.6 72.9 50.5 58.6 40.5 67.1 68.7 36.1 20.2 34.1 64.9VPT-Deep78.8 90.8 65.8 98.0 78.1 49.6 88.3 81.8 96.1 83.4 68.4 68.5 60.0 46.5 72.8 73.6 47.9 32.9 37.8 69.4Adapter-869.2 90.1 68.0 98.8 82.8 54.3 89.9 84.0 94.9 81.9 75.5 80.9 65.3 48.6 78.3 74.8 48.5 29.9 41.6 71.4Adapter-3268.7 92.2 69.8 98.9 84.2 53.0 90.3 83.2 95.4 83.2 74.3 81.9 63.9 48.7 80.6 76.2 47.6 30.8 36.4 71.5NOAH69.6 92.7 70.2 99.1 86.1 53.7 90.4 84.4 95.4 83.9 75.8 82.8 68.9 49.9 81.7 81.8 48.3 32.8 44.2 73.2SPT-Adapter 72.9 93.2 72.5 99.3 88.8 55.8 91.4 86.2 96.1 85.5 75.5 83.0 68.0 51.9 81.2 82.4 51.9 31.7 41.2 74.1",
  "Reparameterized-based methods": "Linear63.4 85.0 63.2 97.0 36.6 51.0 86.3 78.5 87.5 68.6 74.0 34.3 30.6 33.2 55.4 12.5 20.09.619.2 52.9Partial-166.8 85.9 62.5 97.3 37.6 50.6 85.5 78.6 89.8 72.5 73.3 41.5 34.3 33.9 61.0 31.3 32.8 16.3 22.4 56.5Bias72.8 87.0 59.2 97.5 59.9 51.4 85.3 78.7 91.6 72.9 69.8 61.5 55.6 32.4 55.9 66.6 40.0 15.7 25.1 62.0LoRA-867.1 91.4 69.4 98.8 85.3 54.0 90.4 84.9 95.3 84.4 73.6 82.9 69.2 49.8 78.5 75.7 47.1 31.0 44.0 72.3LoRA-1668.1 91.4 69.8 99.0 86.4 53.1 90.5 85.1 95.8 84.7 74.2 83.0 66.9 50.4 81.4 80.2 46.6 32.2 41.1 72.6SPT-LoRA73.5 93.3 72.5 99.3 87.9 55.5 91.5 85.7 96.2 85.9 75.9 84.4 67.6 52.5 82.0 81.0 51.1 30.2 41.3 74.1 SNELL-873.7 92.7 72.4 99.2 89.2 55.4 91.4 84.9 96.1 86.4 75.2 84.0 68.5 53.5 81.0 82.7 49.9 33.9 39.2 74.2SNELL-1674.2 93.4 72.5 99.3 90.2 55.7 91.4 85.7 95.8 86.5 76.3 84.4 68.2 53.0 82.0 82.2 49.6 33.3 40.6 74.4SNELL-3274.5 93.4 73.1 99.3 91.1 55.9 91.5 85.5 96.1 86.5 76.2 83.4 68.6 52.2 81.3 83.2 50.7 35.9 39.0 74.6 We use gradient descent for 1e5 optimization steps, employing the Adam optimizer with a learningrate of 1e 4. We fit 10 randomly generated matrices for each kernel function presented in Table A6and report the average MSE Loss in (a).",
  "holds for all x1, ..., xn Rr, c1, ..., cn R, n N": "Given two vectors x, x Rr, we show the utilized kernel functions in Table A6. We introduceadditional learnable parameters (the e.g. for Sigmoid and RBF kernel, p for piecewise linearkernel) that enable the merged adaptation matrix W to accommodate both positive and negativevalues. The additional parameters select certain elements in the matrix and assign them negativevalues, without compromising the high-rank property of the merged adaptation matrix W. We setP = 2 for the piecewise linear kernel.",
  "C.1More Comparisons with Existing Methods": "Given that some methods do not provide performance or implementation details on both FGVC andVTAB benchmarks, we present a comparison between SNELL and these methods in the appendixrather than in . First, we provide comparisons with GPS on the FGVC benchmark interms of performance and memory usage in Table A8. With comparable performance, SNELL has asignificant advantage over GPS in terms of memory usage. Then, we compare SNELL with VQT on VTAB-1k dataset in Table A9. SNELL significantly outperforms VQT (76.9% vs. 68.8%).",
  "C.2Per-task results on the VTAB-1k benchmark": "We provide the per-tasks results on the VTAB-1k benchmark using ViT-B/16 supervised pre-trainedon ImageNet21K in Table A7. Our SNELL has demonstrated superior performance by achievingSOTA performance on 13 downstream tasks. Additionally, SNELL achieves SOTA performance onthe mean accuracy across all tasks (74.6% vs. 74.1%), indicating its effectiveness in various domains.",
  "C.3.1Comparison between Competition-based Sparsification and Pre-defined Weight Mask": "To verify the effectiveness of the proposed competition-based sparsification mechanism, we comparethe performance on FGVC datasets between kernelized LoRA (KLoRA-8-Fixed) with pre-definedfixed masks, and SNELL in Table A10. The weight masks are generated by SPT . For a faircomparison, we utilize the same data augmentation as SPT. Compared to our dynamic maskingstrategy, pre-defined fixed masking can hardly identify and adjust the most task-relevant weights inan end-to-end fashion, which leads to performance degradation (89.4 vs. 90.3).",
  "C.3.2Comparison between Kernelized LoRA and LoRA": "We compare the performance of LoRA and kernelized LoRA on the VTAB-1k benchmark, where allweight matrices of the pre-trained models are fine-tuned to ensure a fair comparison. The experimentalresults are presented in Table A11. Through experiments with different ranks, we observed thatkernelized consistently outperforms LoRA across various task groups. The replacement of the innerproduct with nonlinear kernel functions leads to stronger expressive ability, which in turn contributesto improved performance on downstream tasks.",
  "C.3.3Additional Memory Usage from Nonlinear Kernel Functions": "In (a), we observe that SNELL requires additional memory usage compared to LoRA dueto the incorporation of nonlinear kernel functions. To explore whether the impact of this additionalusage hinders the usability of SNELL on large models, we compare the memory usage betweenSNELL and LoRA on models as the model size grows (in Table A12). As the model size expands,the incremental memory usage of SNELL becomes negligible.",
  "C.4Experiments on Large Language Models": "We apply SNELL on LLaMA2-7B to adapt to the commonsense reasoning benchmark. AsTable A13 shows, SNELL achieves a better performance than LoRA. This shows the applicability ofSNELL to NLP tasks. Many other vision PEFT approaches lack this capability, as they necessitate afull level of memory usage for fine-tuning as (a) shows.",
  "C.5Training Time Analysis": "Table A14 provides a comparison of training time costs between SNELL and other PEFT methodsusing NVIDIA GeForce RTX 4090 GPU. The training time of SNELL-8 is slightly higher thanLoRA-8 (0.557 vs. 0.443). By further comparing the training time of SNELL-8 and SNELL-8 (savingW), it becomes apparent that the increase in time cost primarily stems from the recomputation",
  "D.1Performance of Different Sparsity Ratio": "Figure A7 depicts the accuracy of different sparsity ratios on datasets in VTAB-1k. Differentdownstream tasks exhibit diverse preferences for the sparsity ratio. For instance, CIFAR-100 tends tofavor a smaller sparsity ratio (0.2), while DTD prefers a larger sparsity ratio (0.8). Both Sun397 andRetinopathy tasks also lean towards a larger sparsity ratio (0.99). This highlights the need to considerthe specific characteristics of each task when determining the optimal sparsity ratio.",
  "D.2Analysis of Tunable Parameters": "We analyze the tunable weights of SNELL for different downstream tasks. In Figure A8, we computethe number of weights in the weight matrix WQ of self-attention selected by multiple tasks.We find that most of the weights are only selected by a single downstream task (Tuned Times=1).Moreover, we find that in blocks of different depths, there will be a small part of weights that areselected by multiple downstream tasks, which indicates that there exists a small number of crucialparameters to improve the models performance on downstream tasks.",
  "E.1Tunable Parameter Volume Computing": "We justify our choice to omit to report the volume of learnable parameters. First, computing thevolume of tunable parameters in SNELL is difficult. In the case of LoRA, the volume correspondsto the shape of the learnable low-rank matrices. Conversely, for sparse tuning, the volume isdetermined by the number of updated weights. However, SNELL employs low-rank matrices aslearnable parameters and achieves additional updated weight reduction by sparsifying the mergedmatrices. When using the parameter volume computation method of LoRA, calculating the reductionin parameters due to sparsification becomes challenging. Conversely, applying the parameter volumecomputation method of sparse tuning would be inherently unfair, given that SNELL is specificallyoptimized using low-rank matrices. Second, the parameter efficiency is a pathway to achieve highperformance and low memory usage rather than an objective for model improvement, becauseperformance improvement and memory usage reduction hold practical value. In our experiments,SNELL has demonstrated its advantages in terms of high performance and low memory usage, whichwe consider more valuable than the pursuit of fewer learnable parameters.",
  "E.2Limitation Discussion": "Despite achieving state-of-the-art performance with low memory usage, SNELL requires moretraining time than LoRA. The additional training time cost comes from the recomputation of themerged matrix W in the backpropagation process presented in Appendix C.5. However, it iscrucial to note that this limitation can be solved. Firstly, given the unique characteristics of thekernel matrix, more efficient methods can be employed to calculate the merged adaptationmatrix W. Secondly, by designing appropriate GPU operators, it is possible to avoid explicitlycalculating W during the fine-tuning process like LoRA and reparameterize the learnablelow-rank matrices into the pre-trained weight matrices after the fine-tuning process.",
  "The answer NA means that the paper has no limitation while the answer No means thatthe paper has limitations, but those are not discussed in the paper": "The authors are encouraged to create a separate \"Limitations\" section in their paper. The paper should point out any strong assumptions and how robust the results are toviolations of these assumptions (e.g., independence assumptions, noiseless settings,model well-specification, asymptotic approximations only holding locally). The authorsshould reflect on how these assumptions might be violated in practice and what theimplications would be. The authors should reflect on the scope of the claims made, e.g., if the approach wasonly tested on a few datasets or with a few runs. In general, empirical results oftendepend on implicit assumptions, which should be articulated. The authors should reflect on the factors that influence the performance of the approach.For example, a facial recognition algorithm may perform poorly when image resolutionis low or images are taken in low lighting. Or a speech-to-text system might not beused reliably to provide closed captions for online lectures because it fails to handletechnical jargon.",
  "If applicable, the authors should discuss possible limitations of their approach toaddress problems of privacy and fairness": "While the authors might fear that complete honesty about limitations might be used byreviewers as grounds for rejection, a worse outcome might be that reviewers discoverlimitations that arent acknowledged in the paper. The authors should use their bestjudgment and recognize that individual actions in favor of transparency play an impor-tant role in developing norms that preserve the integrity of the community. Reviewerswill be specifically instructed to not penalize honesty concerning limitations.",
  ". Experimental Result Reproducibility": "Question: Does the paper fully disclose all the information needed to reproduce the main ex-perimental results of the paper to the extent that it affects the main claims and/or conclusionsof the paper (regardless of whether the code and data are provided or not)?Answer: [Yes]Justification: Please refer to .1, Appendix A and the released codes.Guidelines: The answer NA means that the paper does not include experiments. If the paper includes experiments, a No answer to this question will not be perceivedwell by the reviewers: Making the paper reproducible is important, regardless ofwhether the code and data are provided or not.",
  "If the contribution is a dataset and/or model, the authors should describe the steps takento make their results reproducible or verifiable": "Depending on the contribution, reproducibility can be accomplished in various ways.For example, if the contribution is a novel architecture, describing the architecture fullymight suffice, or if the contribution is a specific model and empirical evaluation, it maybe necessary to either make it possible for others to replicate the model with the samedataset, or provide access to the model. In general. releasing code and data is oftenone good way to accomplish this, but reproducibility can also be provided via detailedinstructions for how to replicate the results, access to a hosted model (e.g., in the caseof a large language model), releasing of a model checkpoint, or other means that areappropriate to the research performed. While NeurIPS does not require releasing code, the conference does require all submis-sions to provide some reasonable avenue for reproducibility, which may depend on thenature of the contribution. For example(a) If the contribution is primarily a new algorithm, the paper should make it clear howto reproduce that algorithm.",
  "(b) If the contribution is primarily a new model architecture, the paper should describethe architecture clearly and fully": "(c) If the contribution is a new model (e.g., a large language model), then there shouldeither be a way to access this model for reproducing the results or a way to reproducethe model (e.g., with an open-source dataset or instructions for how to constructthe dataset). (d) We recognize that reproducibility may be tricky in some cases, in which caseauthors are welcome to describe the particular way they provide for reproducibility.In the case of closed-source models, it may be that access to the model is limited insome way (e.g., to registered users), but it should be possible for other researchersto have some path to reproducing or verifying the results.",
  "Guidelines:": "The answer NA means that the paper does not use existing assets. The authors should cite the original paper that produced the code package or dataset. The authors should state which version of the asset is used and, if possible, include aURL. The name of the license (e.g., CC-BY 4.0) should be included for each asset. For scraped data from a particular source (e.g., website), the copyright and terms ofservice of that source should be provided. If assets are released, the license, copyright information, and terms of use in thepackage should be provided. For popular datasets, paperswithcode.com/datasetshas curated licenses for some datasets. Their licensing guide can help determine thelicense of a dataset.",
  "According to the NeurIPS Code of Ethics, workers involved in data collection, curation,or other labor should be paid at least the minimum wage in the country of the datacollector": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with HumanSubjectsQuestion: Does the paper describe potential risks incurred by study participants, whethersuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)approvals (or an equivalent approval/review based on the requirements of your country orinstitution) were obtained?Answer: [NA]Justification: This paper does not involve crowdsourcing nor research with human subjects.Guidelines:",
  "The answer NA means that the paper does not involve crowdsourcing nor research withhuman subjects": "Depending on the country in which research is conducted, IRB approval (or equivalent)may be required for any human subjects research. If you obtained IRB approval, youshould clearly state this in the paper. We recognize that the procedures for this may vary significantly between institutionsand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and theguidelines for their institution."
}