{
  "Abstract": "Trained on vast corpora of human language, language models demonstrate emer-gent human-like reasoning abilities. Yet they are still far from true intelligence,which opens up intriguing opportunities to explore the parallels of humans andmodel behaviors. In this work, we study the ability to skip steps in reasoningahallmark of human expertise developed through practice. Unlike humans, whomay skip steps to enhance efficiency or to reduce cognitive load, models do notinherently possess such motivations to minimize reasoning steps. To address this,we introduce a controlled framework that stimulates step-skipping behavior byiteratively refining models to generate shorter and accurate reasoning paths. Em-pirical results indicate that models can develop the step skipping ability under ourguidance. Moreover, after fine-tuning on expanded datasets that include both com-plete and skipped reasoning sequences, the models can not only resolve tasks withincreased efficiency without sacrificing accuracy, but also exhibit comparable andeven enhanced generalization capabilities in out-of-domain scenarios. Our workpresents the first exploration into human-like step-skipping ability and providesfresh perspectives on how such cognitive abilities can benefit AI models.",
  "Introduction": "The pursuit of Artificial General Intelligence (AGI) is profoundly influenced and inspired by humanintelligence . Trained extensively on human language, language models not only excel invarious tasks, but also begin to exhibit emergent human-like abilities that are not explicitly engineeredinto them . Among these, reasoning stands out as a core human-like cognitive ability, and hasdemonstrated great potential in a wide range of problem solving scenarios .Despite their advances in displaying human-like cognitive activities, huge gaps remain in how modelsand humans actually behave . These differences bring up interesting questions regardingthe exploration and development of similar capabilities between models and humans. We aim to investigate whether the models exhibit any reasoning abilities unique to human experts,and whether they can evolve from beginners to reasoning experts. When humans learn to reason,beginners typically start with detailed, step-by-step solutions to imitate the gradual process of problemsolving. As practice makes perfect, human experts not only solve problems more swiftly but alsoutilize shorter mental pathways, often skipping steps in their reasoning process . This particularability helps them speed up the reasoning and saves cognitive load for more challenging steps .As demonstrated in , the step-skipping behavior illustrated on the right side is commonlyadopted by human experts during equation simplification.",
  ": Step skipping in equation simplification. We use the specified number of steps in the inputas a stimulation to induce the model to perform skipping by using fewer steps": "In this work, we are curious whether models exhibit mature human-like reasoning ability skippingsteps, and how such abilities can influence the models reasoning behaviors. Unlike humans, modelsdo not inherently possess the intrinsic motivation like time limit or skill maturity that naturallydrives efficiency in cognitive tasks. To induce the skipping step behavior in models, we introduce acontrolled training environment where models are instructed to generate reasoning sequences withina specified number of steps. Our method includes two phases: initialization and iteration. We beginwith a dataset that contains complete stepwise reasoning processes for the questions. In initialization,models are first trained to solve the tasks comprehensively, adhering to the full sequence of reasoningsteps. In , the illustration on the left demonstrates how models are trained to follow aspecified number of steps. Then in the iteration phase, the models are prompted to produce shorteranswers based on the original training data ( right). We then select the shorter reasoning pathsthat still achieve correct answers and mix them with the full-step reasoning paths. This expandeddataset is used to train a new model to have advanced step-skipping capabilities. Each iterationrefines the models ability to identify how steps can be skipped without sacrificing accuracy. Finally,we fine-tune the models using these iteratively generated datasets, including data instances thatdemonstrate successful step-skipping during each iteration. We conduct experiments with three different reasoning datasets, each characterized by clear internalreasoning steps, to evaluate model behaviors. Empirical results demonstrate that models exhibitand develop the ability of skipping steps in our framework - not only solving tasks effectively butalso actively omitting steps to enhance efficiency. Further analysis of model behaviors indicate thatthese skipped reasoning paths act as beneficial enhancements rather than mere biased shortcuts, asevidenced by their maintenance or even improvement of out-of-distribution (OOD) performanceacross various tasks. To the best of our knowledge, this work is the first investigation into the human-like ability of step-skipping in language models, providing empirical evidence that models can indeedskip steps. These preliminary findings provide a fresh perspective on easy-to-hard generalization training models on simpler data comprising both comprehensive and skipped reasoning steps canenhance their ability to generalize to more complex scenarios.",
  "Related Work": "Human-like Abilities in Language ModelsMany of the capabilities widely used in current modelsare inspired by human intelligence. For instance, in-context learning enables models to addressproblems by mimicking the patterns demonstrated in examples . In reasoning tasks, models benefitfrom progressively answer derivations and step-by-step chain-of-thought processes and theirhumanlike enhancements, such as planning , task decomposition , and refinement .Another series of studies explore from the perspectives of cognitive science and psychology . Kosinski reveal that current large language models have demonstrated a certain levelof Theory-of-Mind (ToM) abilities by testing their performance to impute anothers mental statesand perspectives. Further studies provide preliminary evidence of a correlation between theembeddings in LLMs and human brain neurons during ToM tasks, while Ma et al. highlights thelimitation of current ToM evaluations as they target narrow and inadequate aspects of ToM. Apartfrom these cognitive abilities, our work draws inspiration from human problem solving and evaluates language models on these unique step skipping behaviors. Additionally, our workaligns with an expanding field exploring the correlation between System 1 and System 2 reasoning",
  "mechanisms . Rather than removing all reasoning trajectories, our work explores gradualshortening to provide a smoother transition that mirrors natural cognitive processing": "Compositional Generalization ChallengesTransformers have shown limitations in complexcompositional generalization scenarios . Previous work also indicates that models maydevelop biased shortcut, negatively impacting their OOD performance . A growing bodyof research focuses on easy-to-hard generalization , where models improve theirgeneralization ability by learning from easy tasks, without requiring intensive supervision on harderones. Following this line, our work encourages the model to learn from self generated skipping paths,which has been empirically shown to maintain and even enhance OOD generalization capabilities.",
  "Method": "Humans develop the ability to skip steps for several reasons. With practice in specific tasks, theyevolve from novices to experts, optimizing lengthy thought processes into quicker, more efficientreasoning. Additionally, factors such as time constraints or the desire to conserve cognitive resourcescan also prompt humans to skip steps . In contrast, models lack an inherent cognitive signal thatwould drive them to minimize reasoning steps. Rather than attempting to replicate these human-likesignals, we design a training approach to directly control the number of steps in their reasoningprocesses. By restricting the steps in model responses, we can guide the model to self-generate dataincluding skipped steps. Our framework has two phases: initialization and iteration.",
  "Initialization": "We begin with a training dataset D0, which contains detailed full-step reasoning answers to thequestions. Our goal is to train a model that can generate answers by following the specified numberof steps in the input question. Depending on the characteristics of different tasks, there are two designchoices to initialize the framework: cold start and warm start. Cold startIn the cold start approach, we directly fine-tune the model on the original full-steptraining data, i.e., Dinit = D0. The trained model is expected to not only learn to solve the problems,but also adhere to the specified number of steps in the input instructions. Warm startTraining exclusively with full steps does not always guarantee the ability of controllingthe number of steps, especially for the challenging tasks. Therefore, we manually create answers thatcontain skipped steps based on human expertise. Optionally, we can also randomly merge adjacentsteps or simply omit random steps within the rationales to create such skipped-step data. In eitherway, we can expand the original training set with additional data Dskip that can better help models",
  "learn how to solve the problems with fewer steps. Thus, the data for warm start initialization can bedescribes as Dinit = D0 + Dskip": "Using the prepared data, we fine-tune the model to generate the answers with the given numberof steps. For each QA pair in Dinit, the question q is concatenated with the instruction In whichindicates that the reasoning process a(n) should be completed in n steps. Therefore, the resultingmodel in the initialization phase, M0, is described as:",
  "Iteration": "Avg steps 10.38 7.57 5.47 4.444.394.244.254.16 4.39 OOD Hard Avg stepsAccuracy Accuracy (%) 99.8099.90 99.70 99.90100.00100.00 99.8099.80 100.00 Accuracy (%) 98.04 99.22 98.43 99.22 99.80 99.02 98.43 98.63 99.41 Accuracy (%) 4.05 5.71 8.33 12.86 15.95 16.67 14.05 16.91 18.57 : Performance of phi-3-mini across 9 iterations with relaxed step-skipping constraints (up to4 steps) on Analog of Algebra. The figures show the changes in average steps taken (left y-axis) andaccuracy (right y-axis). Continuous iteration improves OOD-hard accuracy and reduces the averagenumber of steps, converging towards stability.",
  "(q,a(n))Dk P(a(n)|q, In)": "The iterative training process described above requires specifying the number of steps in the input,which is impractical in real-world applications because it can be difficult to determine the exactnumber of steps needed for a given question. To be more applicable, we aim to understand howmodels learn from the generated skipped data and what benefits they can derive from it. Therefore,for each intermediate resulting dataset Dk, we train a new model using a standard QA finetuningsetting without specifying the number of steps in the input:",
  "Datasets": "We design three tasks to investigate the models step skipping behavior (). In each task, theintermediate steps needed to solve these problems are explicitly detailed and well-defined, facilitatinga clear analysis of the models predictions. When creating skipped data for warm start, we eitheromit certain steps or heuristically merge two adjacent steps. Details on data creation can be found inAppendix B.1. Analog of AlgebraFollowing Blessing and Anderson , we create an analog of algebra byreplacing the variables and operators with different symbols. As shown in , each variableand standard operator is mapped to a unique, unrelated symbol. The desired result is to isolatethe symbol (i.e., x) on the left side of the symbol (i.e., =). This task is entirely new for themodel, making it an ideal scenario to understand how models develop problem-solving abilities fromscratch. We use a heuristic script to generate the questions along with the stepwise solutions. Aftergenerating the QA pairs, we filter the data based on the number of variables involved in the questionand the steps required to solve it. The training and in-domain test data contains questions with up to",
  ": Illustrations of three different tasks. Each question is accompanied by a comprehensivedetailed step-by-step solution": "7 variables and requiring no more than 5 steps. In addition, we create two out-of-domain datasetsof varying difficulties to evaluate generalization performance: OOD-easy includes variables unseenduring training, with 8 and 9 variables, no limit on steps. OOD-hard is the most challenging setting,including 10 - 14 variables and 9 steps to solve. Both OOD sets contain unseen variables. Multi-digit AdditionAs a basic arithmetic task, multi-digit addition naturally involves detailedstepwise reasoning processes, serving as a suitable task for studying model behaviors in composi-tionality generalization. We utilize step-by-step reasoning processes to perform additionoperation digit by digit, as illustrated in . For training and in-domain test data, we onlyconsider additions involving numbers with up to 3 digits. We introduce two out-of-domain datasetsdepending on the number of digits involved in the addition: OOD-easy includes one number with upto 3 digits and another with 4-7 digits. OOD-hard contains two numbers, both with 4-7 digits. Directional ReasoningWe additionally consider long-form symbolic directional reasoning, whichposes a challenge for direct solution and necessitates continuous reasoning steps to arrive at theanswer. This task provides an initial direction and a list of turning actions. The desired answer is thefinal facing direction. For training and in-domain test set, we consider questions that contain 10actions. OOD-easy includes questions with 11-20 actions and OOD-hard includes questions with21-30 actions. The detailed statistics of three datasets can be found in .",
  "Experiment Setting": "For all our experiments, we use Llama 2 (7B parameters) and phi-3-mini (3.8B parameters, withcontext length of 4K) as our base model. We train the model using a learning rate of 5e-6 for 2epochs with the AdamW optimizer . During inference, we employ greedy decoding. We run ourexperiments with three different random seeds and report the average and standard deviation. Allexperiments are conducted on eight V100 GPUs each with 32GB memory. The total training timerequired to complete one full cycle of five iterations is under six hours.",
  "Can models learn to skip steps?": "To make sure our framework can proceed to iterations smoothly, one crucial factor is the initializedmodels ability to adhere to the specified number of steps in the input. In the cold start setting, we : Step number following ability of the initialized Llama2 models across different tasks. #Skipping represents the number of instances where n i > 0. Step Consistency quantifies thematch between the actual number of steps taken and the number indicated in the input. AnswerAccuracy calculates the percentage of correct final answers out of the # Skipping cases. AverageStep reflects the mean number of steps across all predictions within the dataset.",
  "# Skipping5,3084,1592,8442,1752,0712,049Step Consistency100.0099.19100.00100.0086.2439.19Answer Accuracy8.142.7798.3582.5885.4729.62Average Step2.331.811.901.386.146.66": "train the model exclusively using the full step training data. We then run inference on the trainingset, instructing the model to use n i steps to solve the question, where n denotes the original fullstep number and i . If n i 0, we do not ask the model to try skipping on such cases andinstruct the model to use n steps instead. As shown in , the results demonstrate that the fine-tuned model exhibits good step-numberfollowing ability on the Analog of Algebra over 99 % of the answers follow the given numberof steps. Additionally, when prompted to generate condensed answers with fewer steps, the modelcan produce some correct answers in the specified number of steps, achieving accuracies of 8.14%and 2.77% respectively. Despite this relatively low accuracy, these small amount of correct data canstill assist the model in gradually developing step skipping ability through iterations. Ultimately, themodel manages to produce over 90% of correct skipping data. The trend of the data quantity changecan be found in Appendix B.2. However, this ability varies across different tasks. For the other two tasks, models do not naturallydevelop the capability for active step skipping, leading to near zero step consistency when requiredto provide answers in fewer steps. To address this issue, we employ the warm start setting for thesetasks. presents the results of Multi-digit Addition and Directional Reasoning under the warmstart setting, indicating that this approach enhances the models proficiency with step skipping. Ideally, we aim for models to be initialized through cold start. The benefits of this approach areobvious it allows the model to spontaneously develop step skipping behavior, giving it sufficientfreedom to decide and control which steps to skip. However, our experiments have revealed thatit can be challenging for models to develop such capability in all scenarios. In contrast, the warmstart offers an alternative design choice by providing human-created skipped data. This data includesintuitive and valid skipping steps derived from human expertise, making it more natural and helpingmodels develop human-understandable behaviors. However, it might also introduce human biasesthat constrain the models independent exploration of step skipping. This influence can be mitigatedin the subsequent iteration phase, where the model is given full freedom to develop and amplify itsown step-skipping behavior.",
  "What do models learn from skipping steps?": "Based on this new mixed data including both complete and skipped answers at each iteration, wetrain the standard models to analyze the change of models performance what models can learnfrom the behavior of skipping steps. Models learn to solve problems more effectively with fewer steps.We evaluate the standardmodels on both in-domain and OOD data, with the results presented in . Detailed resultsfrom each iteration of the evaluation can be found in Appendix B.3. Given the simplicity of thetasks, the model is able to overfit on in-domain data, achieving nearly perfect performance. Furtheriterations of skipping steps manage to guide the model to use fewer steps while maintaining theperformance. In two OOD scenarios, we find that the model trained with mixed data performscomparably to the model trained with complete steps on the OOD test sets, and even exhibits superiorgeneralization abilities. Specifically, in Analog of Algebra, Llama2 models of iteration 5 achieves4.76% gain on OOD-easy, while phi-3-mini achieves 7.08% gain on OOD-hard set. In the Multi-digitAddition task, the Llama2 model demonstrates a 13.91% improvement in OOD-easy performance : Performance comparison of models from different phases. Avg steps denotes the averagenumber of steps taken in the prediction. With the skipped step data, models achieve even bettergeneralization performance with fewer steps.",
  ": Comparison of models across different phases relative to question length and complexity.Models achieve near perfect performance on in-domain data but diverge on lengthy OOD data": "and a 4.75% increase in OOD-hard performance. In the OOD-hard dataset for Directional Reasoning,Llama2s performance improvs by 9.2%. These results suggest that not only is the model unaffectedby potential shortcut bias from the skipping steps, but it actually benefits from the mixed trainingdata to gain enhanced task solving abilities. The ablation analysis on various data-mixing approachesare provided in Appendix B.5. Furthermore, we observe that the model uses fewer steps, therebyincreasing problem-solving efficiency.",
  "Analog of Algebra": "(a) presents the performance of Llama2 models across various iterations in the Analog ofAlgebra task, differentiated by the number of steps required in the complete answers. The solid linesrepresent the accuracy of final answers. We perform uniform evaluation on the union of all in-domainand OOD test sets. Initially, all models maintain high accuracy for in-domain problems with up to fivesteps, after which a significant drop is observed as the complexity increases. As the model undergoes iterations, there is a noticeable improvement in its ability to handle longer step lengths (green solidline), particularly in the range of 6 to 10 steps where other models show significant weaknesses.The dashed lines illustrate the proportion of data exhibiting step-skipping in model predictions. Theblue dashed line indicates models initially adopt step-skipping as problems extend in length. Afteriterations, the green dashed line indicates the models consistently employ step skipping in shorterquestions, thereby improving the reasoning efficiency.",
  "Directional Reasoning": "(b) illustrates the comparison of Llama2 models performance across different questionlengths on Directional Reasoning task. We observe that the artificial skipped data has minimal impacton the model, with negligible differences between the cold start and warm start phases. Upon enteringthe iterative phase, the models performance notably declines during the first iteration, particularly inhandling longer problems. This downturn may reflect the models adjustment from manually injectedskipped data to its own step skipping ability. Subsequent iterations show that the model benefitsmore significantly from data generated during the iteration process, as evidenced by the results inIteration 5. The model maintains consistency with the baseline in both in-domain and out-of-domainperformances, and exhibits a slight advantage in solving longer problems. Similar to the previoustask, the Iteration 5 Ratio curve (dashed green line) also shows a significant increase in step-skippingbehavior, suggesting an evolved efficiency in reasoning as the model opts to bypass steps whilemaintaining or even improving accuracy.",
  "Multi-digit Addition": "In , we show a finer-grained evaluation of multi-digit addition tasks on Llama2. Thehorizontal and vertical axes of the matrices represent the number of digits in the two addends for eachquestion in the test set (both in-domain and OOD test data). We utilize the following three metrics:Question-level accuracy assesses whether the final answer is correct for additions involving differentnumbers of digits. Step-level distribution illustrates the distribution of the digit lengths used ineach individual step of the models stepwise reasoning process. Step-level accuracy measures theaccuracy of the single step calculations involving different numbers of digits. In (a), as iterations progress, the model demonstrates improved generalization performanceacross all test datasets. When initialized with a cold start, the model can only learn from the trainingdata involving single-digit addition steps, resulting in overfitting to in-domain test data (digit 3).When augmented with manually created skipped data for a warm start, the model begins to incorporatemulti-digit additions with skipped steps. However, the inconsistency between the manually injecteddata and the models inherent behavior does not significantly enhance the question-level accuracy.As the model is encouraged to explore during the iteration phase, it undertakes broader and bolderattemptsoften combining additions across more digits in skipped steps. With the integration ofthese data, the model trained on this expanded iterative dataset also shows a more pronounced abilityto solve OOD problems. As seen in (b), the model increasingly employs multi-digit additionsin single-step operations. Furthermore, as illustrated in (c), there is an improvement in theaccuracy of these skipped single-step operations. We believe this may be due to the model-generateddata during self-iterations, which are more conducive to enhancing its capability to skip steps, therebybenefiting from this process.",
  "Accuracy of Step-Skipping Answers": "shows the step skipping behavior and accuracy of the standard models at each iteration onthe Analog of Algebra task using Llama-2. The Skipping Ratio measures how often the model skipssteps in the test set, while Accuracy reflects the correctness of these skipping answers. We observe that in the beginning models inherently struggle in OOD scenarios, often producingreasoning steps that are incomplete or shorter than the problem complexity requires. In cold start\"settings, where the model is trained solely with complete steps, it performs well with in-domainquestions but fails to maintain complete reasoning steps and tends to generate shorter responses onOOD sets. Due to its limited generalizability, these skipping or missing steps negatively impactthe performance. However, as the model progressively adapts to step skipping over iterations, theaccuracy of the shorter responses improves, suggesting it gradually develops a more reliable abilityto skip steps when appropriate. Analysis across all tasks can be found in Appendix B.4.",
  "Analysis on the Influence of Training Steps": "Throughout the iterations, as the model progressively generates more successful step skipping data,the size and the quality of the resulting dataset also gradually increases. This can be consideredas a special form of augmentation for answer diversity. To investigate whether the performanceimprovements are primarily due to the model learning from more training steps, we increase thenumber of training epochs during the initialization phase to match the data volume after iterations.The comparison results shown in reveal that increasing the number of training epochs doesnot always lead to performance enhancements; instead, it may cause a performance decline dueto overfitting. In contrast, mixing skip-step data from the iterative process not only maintains orimproves performance in in-domain and OOD-easy tasks but also achieves consistent gains in OOD-hard setting. When the total number of training steps is similar, the integration of skipping data yieldsbetter performance.",
  "Extended Iterative Training": "In this section, we extend the iterative process to allow the model to skip up to 4 steps, rather thanrestricting it to less than 2 steps on Analog of Algebra. The process is continued for a total of 9iterations, and the results are shown in . The model continues to benefit from additionaliterations beyond Iteration 5, which serves as the default cutoff in our main results. Specifically,the accuracy on the OOD-hard set improves steadily, reaching over 18% by the ninth iteration.This increase suggests that even with a greater allowance for step-skipping, the models ability togeneralize to harder out-of-domain samples is enhanced with continued training. Simultaneously, the average number of steps taken decreases across all test sets as iterations progress,suggesting that the model is converging towards fewer steps and becoming increasingly efficient. Bythe ninth iteration, the step count appears to plateau, indicating that the model has likely reacheda stable balance between accuracy and efficiency. We hope our work provides a fresh perspectiveon exploring the connection between System 2 slow reasoning and System 1 fast thinking, and onfacilitating their transformation, paving the way for future research in this direction.",
  "Conclusion": "In this work, we explore the human-like ability of step skipping in language models, providing initialempirical evidence that models can skip steps and benefit from such cognitive behaviors. Addressingthe absence of intrinsic motivation for step skipping in models, we design an approach that not onlyenables models to spontaneously develop the ability but also iteratively encourages models to activelyadopt and enhance this behavior. Through experiments on three tasks, we demonstrate that modelsequipped with step-skipping capabilities can solve tasks more efficiently in fewer steps, withoutsacrificing accuracy. Further empirical results suggest that training on easy data containing both fullsteps and skipped reasoning steps can potentially help models generalize to harder scenarios. Wehope this work offers insights into the relationship and transition between System 1 and System 2thinking and contributes to advancing easy-to-hard generalization in language model reasoning. M. I. Abdin, S. A. Jacobs, A. A. Awan, J. Aneja, A. Awadallah, H. Awadalla, N. Bach,A. Bahree, A. Bakhtiari, H. S. Behl, A. Benhaim, M. Bilenko, J. Bjorck, S. Bubeck, M. Cai,C. C. T. Mendes, W. Chen, V. Chaudhary, P. Chopra, A. D. Giorno, G. de Rosa, M. Dixon,R. Eldan, D. Iter, A. Garg, A. Goswami, S. Gunasekar, E. Haider, J. Hao, R. J. Hewett, J. Huynh,M. Javaheripi, X. Jin, P. Kauffmann, N. Karampatziakis, D. Kim, M. Khademi, L. Kurilenko,J. R. Lee, Y. T. Lee, Y. Li, C. Liang, W. Liu, E. Lin, Z. Lin, P. Madan, A. Mitra, H. Modi,A. Nguyen, B. Norick, B. Patra, D. Perez-Becker, T. Portet, R. Pryzant, H. Qin, M. Radmilac,C. Rosset, S. Roy, O. Ruwase, O. Saarikivi, A. Saied, A. Salim, M. Santacroce, S. Shah,N. Shang, H. Sharma, X. Song, M. Tanaka, X. Wang, R. Ward, G. Wang, P. Witte, M. Wyatt,C. Xu, J. Xu, S. Yadav, F. Yang, Z. Yang, D. Yu, C. Zhang, C. Zhang, J. Zhang, L. L. Zhang,Y. Zhang, Y. Zhang, Y. Zhang, and X. Zhou. Phi-3 technical report: A highly capable languagemodel locally on your phone. CoRR, abs/2404.14219, 2024. doi: 10.48550/ARXIV.2404.14219.URL",
  "S. Blessing and J. R. Anderson. How people learn to skip steps. Journal of ExperimentalPsychology: Learning, Memory and Cognition, 22:576598, 1996.URL": "S. R. Bowman, J. Hyun, E. Perez, E. Chen, C. Pettit, S. Heiner, K. Lukosiute, A. Askell, A. Jones,A. Chen, A. Goldie, A. Mirhoseini, C. McKinnon, C. Olah, D. Amodei, D. Amodei, D. Drain,D. Li, E. Tran-Johnson, J. Kernion, J. Kerr, J. Mueller, J. Ladish, J. Landau, K. Ndousse,L. Lovitt, N. Elhage, N. Schiefer, N. Joseph, N. Mercado, N. DasSarma, R. Larson, S. McCan-dlish, S. Kundu, S. Johnston, S. Kravec, S. E. Showk, S. Fort, T. Telleen-Lawton, T. Brown,T. Henighan, T. Hume, Y. Bai, Z. Hatfield-Dodds, B. Mann, and J. Kaplan.Measuringprogress on scalable oversight for large language models. CoRR, abs/2211.03540, 2022. doi:10.48550/ARXIV.2211.03540. URL T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan,P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child,A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray,B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei.Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M. Bal-can, and H. Lin, editors, Advances in Neural Information Processing Systems 33: AnnualConference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, P. Lee, Y. T. Lee,Y. Li, S. M. Lundberg, H. Nori, H. Palangi, M. T. Ribeiro, and Y. Zhang. Sparks of artificialgeneral intelligence: Early experiments with GPT-4. CoRR, abs/2303.12712, 2023. doi:10.48550/ARXIV.2303.12712. URL C. Burns, P. Izmailov, J. H. Kirchner, B. Baker, L. Gao, L. Aschenbrenner, Y. Chen, A. Ecoffet,M. Joglekar, J. Leike, I. Sutskever, and J. Wu. Weak-to-strong generalization: Eliciting strongcapabilities with weak supervision. CoRR, abs/2312.09390, 2023. doi: 10.48550/ARXIV.2312.09390. URL K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek,J. Hilton, R. Nakano, C. Hesse, and J. Schulman. Training verifiers to solve math wordproblems. CoRR, abs/2110.14168, 2021. URL",
  "I. Dasgupta, A. K. Lampinen, S. C. Y. Chan, A. Creswell, D. Kumaran, J. L. McClelland, andF. Hill. Language models show human-like content effects on reasoning. ArXiv, abs/2207.07051,2022. URL": "W. De Neys. 223C11The Cognitive Unconscious and Dual Process Theories of Reasoning. InThe Cognitive Unconscious: The First Half Century. Oxford University Press, 08 2022. ISBN9780197501573. doi: 10.1093/oso/9780197501573.003.0011. URL Y. Deng, K. Prasad, R. Fernandez, P. Smolensky, V. Chaudhary, and S. M. Shieber. Implicitchain of thought reasoning via knowledge distillation. CoRR, abs/2311.01460, 2023. doi:10.48550/ARXIV.2311.01460. URL",
  "M. Du, F. He, N. Zou, D. Tao, and X. Hu. Shortcut learning of large language models in naturallanguage understanding: A survey. CoRR, abs/2208.11857, 2022. doi: 10.48550/ARXIV.2208.11857. URL": "N. Dziri, X. Lu, M. Sclar, X. L. Li, L. Jiang, B. Y. Lin, S. Welleck, P. West, C. Bha-gavatula, R. L. Bras, J. D. Hwang, S. Sanyal, X. Ren, A. Ettinger, Z. Harchaoui,and Y. Choi.Faith and fate: Limits of transformers on compositionality.In A. Oh,T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advancesin Neural Information Processing Systems 36: Annual Conference on Neural Informa-tion Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10- 16, 2023, 2023.URL S. Hao, Y. Gu, H. Ma, J. J. Hong, Z. Wang, D. Z. Wang, and Z. Hu. Reasoning with languagemodel is planning with world model. In H. Bouamor, J. Pino, and K. Bali, editors, Proceedingsof the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023,Singapore, December 6-10, 2023, pages 81548173. Association for Computational Linguistics,2023. doi: 10.18653/V1/2023.EMNLP-MAIN.507. URL",
  "P. Hase, M. Bansal, P. Clark, and S. Wiegreffe. The unreasonable effectiveness of easy trainingdata for hard tasks. CoRR, abs/2401.06751, 2024. doi: 10.48550/ARXIV.2401.06751. URL": "L. Huang, W. Yu, W. Ma, W. Zhong, Z. Feng, H. Wang, Q. Chen, W. Peng, X. Feng, B. Qin, andT. Liu. A survey on hallucination in large language models: Principles, taxonomy, challenges,and open questions. CoRR, abs/2311.05232, 2023. doi: 10.48550/ARXIV.2311.05232. URL M. Jamali, Z. M. Williams, and J. Cai. Unveiling theory of mind in large language models: Aparallel to single neurons in the human brain. CoRR, abs/2309.01660, 2023. doi: 10.48550/ARXIV.2309.01660. URL Z. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y. Xu, E. Ishii, Y. Bang, A. Madotto, and P. Fung. Surveyof hallucination in natural language generation. ACM Comput. Surv., 55(12):248:1248:38,2023. doi: 10.1145/3571730. URL",
  "N. Lee, K. Sreenivasan, J. D. Lee, K. Lee, and D. Papailiopoulos. Teaching arithmetic tosmall transformers. CoRR, abs/2307.03381, 2023. doi: 10.48550/ARXIV.2307.03381. URL": "B. Liu, J. T. Ash, S. Goel, A. Krishnamurthy, and C. Zhang. Transformers learn shortcuts toautomata. In The Eleventh International Conference on Learning Representations, ICLR 2023,Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL T. Liu, Q. Guo, Y. Yang, X. Hu, Y. Zhang, X. Qiu, and Z. Zhang. Plan, verify and switch:Integrated reasoning with diverse x-of-thoughts. In H. Bouamor, J. Pino, and K. Bali, edi-tors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Pro-cessing, EMNLP 2023, Singapore, December 6-10, 2023, pages 28072822. Associationfor Computational Linguistics, 2023. doi: 10.18653/V1/2023.EMNLP-MAIN.169. URL I. Loshchilov and F. Hutter. Decoupled weight decay regularization. In 7th InternationalConference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.OpenReview.net, 2019. URL P. Lu, B. Peng, H. Cheng, M. Galley, K. Chang, Y. N. Wu, S. Zhu, and J. Gao. Chameleon: Plug-and-play compositional reasoning with large language models. CoRR, abs/2304.09842, 2023.doi: 10.48550/arXiv.2304.09842. URL Z. Ma, J. Sansom, R. Peng, and J. Chai. Towards A holistic landscape of situated theory ofmind in large language models. In H. Bouamor, J. Pino, and K. Bali, editors, Findings of theAssociation for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023,pages 10111031. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.FINDINGS-EMNLP.72. URL A. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao, S. Wiegreffe, U. Alon, N. Dziri,S. Prabhumoye, Y. Yang, S. Welleck, B. P. Majumder, S. Gupta, A. Yazdanbakhsh, and P. Clark.Self-refine: Iterative refinement with self-feedback. CoRR, abs/2303.17651, 2023. doi: 10.48550/arXiv.2303.17651. URL A. Meurer, C. P. Smith, M. Paprocki, O. Certk, S. B. Kirpichev, M. Rocklin, A. Kumar,S. Ivanov, J. K. Moore, S. Singh, T. Rathnayake, S. Vig, B. E. Granger, R. P. Muller, F. Bonazzi,H. Gupta, S. Vats, F. Johansson, F. Pedregosa, M. J. Curry, A. R. Terrel, v. Roucka, A. Saboo,I. Fernando, S. Kulal, R. Cimrman, and A. Scopatz. Sympy: symbolic computing in python.PeerJ Computer Science, 3:e103, Jan. 2017. ISSN 2376-5992. doi: 10.7717/peerj-cs.103. URL",
  "A. Newell, H. A. Simon, et al. Human problem solving, volume 104. Prentice-hall EnglewoodCliffs, NJ, 1972": "L. Pan, A. Albalak, X. Wang, and W. Y. Wang. Logic-lm: Empowering large languagemodels with symbolic solvers for faithful logical reasoning. CoRR, abs/2305.12295, 2023. doi:10.48550/ARXIV.2305.12295. URL L. Pan, M. Saxon, W. Xu, D. Nathani, X. Wang, and W. Y. Wang. Automatically correctinglarge language models: Surveying the landscape of diverse self-correction strategies. CoRR,abs/2308.03188, 2023. doi: 10.48550/ARXIV.2308.03188. URL A. Saparov and H. He. Language models are greedy reasoners: A systematic formal analysis ofchain-of-thought. In The Eleventh International Conference on Learning Representations, ICLR2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL",
  "J. Sweller, R. F. Mawer, and M. R. Ward. Development of expertise in mathematical problemsolving. Journal of Experimental Psychology: General, 112:639661, 1983. URL": "H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra,P. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. Canton-Ferrer, M. Chen, G. Cucurull, D. Es-iobu, J. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn,S. Hosseini, R. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Ko-renev, P. S. Koura, M. Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet,T. Mihaylov, P. Mishra, I. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Sal-adi, A. Schelten, R. Silva, E. M. Smith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor,A. Williams, J. X. Kuan, P. Xu, Z. Yan, I. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang,A. Rodriguez, R. Stojnic, S. Edunov, and T. Scialom. Llama 2: Open foundation and fine-tuned chat models. CoRR, abs/2307.09288, 2023. doi: 10.48550/ARXIV.2307.09288. URL",
  "J. J. Van Merrienboer and J. Sweller. Cognitive load theory and complex learning: Recentdevelopments and future directions. Educational psychology review, 17:147177, 2005": "C. Wang, B. Zheng, Y. Niu, and Y. Zhang. Exploring generalization ability of pretrainedlanguage models on arithmetic and logical reasoning. In L. Wang, Y. Feng, Y. Hong, andR. He, editors, Natural Language Processing and Chinese Computing - 10th CCF InternationalConference, NLPCC 2021, Qingdao, China, October 13-17, 2021, Proceedings, Part I, volume13028 of Lecture Notes in Computer Science, pages 758769. Springer, 2021. doi: 10.1007/978-3-030-88480-2\\_61. URL C. Wang, X. Liu, Y. Yue, X. Tang, T. Zhang, C. Jiayang, Y. Yao, W. Gao, X. Hu, Z. Qi, Y. Wang,L. Yang, J. Wang, X. Xie, Z. Zhang, and Y. Zhang. Survey on factuality in large languagemodels: Knowledge, retrieval and domain-specificity. CoRR, abs/2310.07521, 2023. doi:10.48550/ARXIV.2310.07521. URL",
  "ALimitations": "Our work serves as a preliminary exploration of human-like step skipping capabilities in models,focusing solely on the expansion of problem types in terms of length and compositional complexity,without extending to advanced problem difficulty generalization. We also recognize that ideally thereshould be a clear criterion for determining when to terminate iterations. We observe that the modelcan also perform better in intermediate rounds, which suggests the need for further adjustment of thishyperparameter. Additionally, for the convenience in evaluation, our investigations were confined tothree simple yet representative tasks. While our designed method can be applied to practical tasks,we leave the exploration of scalability to complex reasoning scenarios as future work.",
  "B.1.1Training data creation": "For the Analog of Algebra task, we ensure the quality of the auto-generated dataset by creatingfull-step reasoning data using standard algebraic rules applied to operators. To further verify thevalidity and consistency of the intermediate steps, we utilize the SymPy library. Specifically,we perform SymPy simplification for each intermediate step and ensure that the resulting equationremains algebraically equivalent to the final simplified answer.",
  "B.1.2Manual skipping data for warm start": "We define several heuristic rules to create skipping data for warm start initialization. For the multi-digit addition task, we randomly merge two single-digit addition steps to form a two-digit additionstep. For the directional reasoning task, we incorporate more human expertise by skipping stepsthat involve two adjacent directions that result in no change. For example, adjacent actions such asright-left, left-right, and around-around will not alter the final facing direction, so we manuallyskip these steps. We only manually create one skipped step within a single data.",
  "B.3Detailed results of each iteration": "and show the detailed performance of standard finetuned models from each iterationon Llama2-7B and phi-3-mini respectively. We report the average performance and the standarddeviation across three runs with different random seeds. Analyzing the results from each iteration, we find that the final iteration does not consistently yield thebest performance, highlighting the importance of identifying an optimal stopping point as a directionfor future work. Additionally, significant fluctuations are observed in the test results, particularly inthe OOD settings. Therefore, developing a more stable approach for OOD generalization tasks isanother potential area for further exploration.",
  "Cold start100.00.007.010.0090.000.5315.770.4642.006.2419.390.29Warm start99.970.066.280.0487.205.2114.650.4342.339.2518.022.4": "Iter 1100.00.006.460.0483.005.5714.690.1429.476.5914.242.86Iter 299.970.066.440.0686.473.9314.950.8440.6713.3017.423.23Iter 3100.00.006.490.1388.601.6414.930.4441.537.3017.600.68Iter 499.900.106.360.0689.202.0314.660.3044.336.9917.791.31Iter 5100.00.006.450.0689.331.3614.870.1251.804.2119.490.79 Cold start vs. warm startIn the Multi-digit Addition task, we observe that phi-3-mini achievessatisfactory results with cold start training alone, allowing the model to enter the iteration phasewithout relying on manually provided skipping data. shows the models performance wheninitialized with a cold start in Multi-digit Addition. Compared to the results in , where themodel begins with a warm start, the cold start approach enables the model to independently exploreand develop its skipping behaviors. This leads to a more pronounced improvement in the OODsettings, with accuracy of 25.06% versus 14.98% in Iteration 5 on OOD-hard. Additionally, weobserve that while warm start enables a more immediate reduction in steps, cold start shows a moregradual decrease in the number of steps taken.",
  "B.4Accuracy of step-skipping answers": "In this section, we provide the ratio and the accuracy of the skipping responses across three tasksusing both base models. The results are shown in and . In general, the modelsdemonstrate a progressively enhanced step skipping capability across various test settings for alltasks. In most cases, the model increasingly favors adopting more skipped reasoning steps overiterations, with the accuracy of skipped responses also improving correspondingly. However, weobserve that the proportion of skipped responses fluctuates across different stages of iteration, ratherthan following a strictly monotonic trend. Given that the model autonomously decides whether toemploy skipping, this pattern may indicate the models attempt to find a balance between using step",
  "Cold start99.920.132.860.0035.9312.295.030.225.391.905.440.17Warm start99.970.062.620.0739.083.873.800.355.112.624.060.44": "Iter 1100.00.002.830.0537.4412.735.030.185.210.725.280.17Iter 2100.00.002.780.1538.5028.874.770.574.834.055.000.60Iter 399.900.102.780.0758.789.735.030.209.040.665.270.13Iter 499.930.062.380.2849.1916.524.180.7825.3511.734.950.27Iter 599.830.152.540.2755.473.494.510.3225.066.795.290.13 skipping and providing full-step solutions. Exclusively relying on skipping would not necessarilybe the optimal answering strategy. We also find that a warm start significantly boosts the modelsskipping behavior. Consequently, in models with a warm start, the changes across iterations are lesspronounced, though overall accuracy still improves.",
  ": Skipping ratio and accuracy at each iteration on Llama2-7B": "reasoning paths. presents an ablation study comparing different data mixing strategies withphi-3-mini model on the Analog of Algebra task. The Skipping setting utilizes only the generatedskipping data Dk1 for training the standard model Mk, while w/ Cold Start incorporates boththe original cold-start data and the skipping data, which serves as the default configuration in ourexperiments. The analysis is based on data from Iteration 5, and we report average performance acrossthree runs with different random seeds. Our findings suggest that relying solely on skipping datamay limit the models capacity to address OOD scenarios. Although skipping data provides shorteraverage steps, it lacks the complete reasoning steps essential for a comprehensive understanding of thetask, potentially leading the model to depend on shortcuts that harm generalization. By incorporatinga mixture of cold-start and skipping data, the model is able to learn from both complete and skippedreasoning chains, which enables a more robust understanding, supporting stronger generalizationcapabilities.",
  ": Skipping ratio and accuracy at each iteration on phi-3-mini. On Multi-digit Addition, weillustrate the analysis of the model that is initialized from Cold start": "To investigate the cross-domain generalization of step-skipping capabilities, we conduct a controlledexperiment to assess the impact of step-skipping training data from one task on the models per-formance in others. Specifically, we sampled 2,000 training examples per dataset, including 1,600step-skipping answers in which exactly one step was successfully skipped from these samples, allfrom Iteration 5. This setup ensures an equal balance of full-step and step-skipping data across allthree tasks. We use the phi-3-mini model across three tasks, with the withheld task representing the task thatlacks step-skipping data during training. The All setting contains only full-step answers for alltasks, with no step-skipping data included. The configurations are as follows:",
  "Withheld setting: task1-full + task1-skip + task2-full + task2-skip + task3-full": "summarizes the models performance on each evaluation task. The withheld tasks resultsare compared to those from the All setting, where all tasks are trained with only full-step answers.Our findings reveal that step-skipping data in one or more tasks positively affects the performanceof the withheld task. In most cases, models trained with step-skipping data from other tasks exhibitimproved accuracy and step-skipping performance across datasets, maintaining a comparable numberof steps to the All setting. For example, in the Analog of Algebra task, the average steps remainsimilar, yet accuracy improvements are observed in OOD settings, indicating that training withstep-skipping data promotes a transferable ability to reason efficiently across domains. The overallaccuracy increase suggests that inclusion of step-skipping data in some tasks enables the modelto generalize this ability, even when explicit step-skipping examples are unavailable in the targettask. These results suggest that the step-skipping capability learned in one domain can generalizeacross different tasks, underscoring the potential for enhancing model efficiency through strategicdata composition.",
  "B.7Experiments on GSM8K": "In addition to the synthetic datasets analyzed in the main body of the paper, we conduct experimentson GSM8K to evaluate the applicability of our method to more complicated tasks. To create acontrolled experimental setting, we classify data requiring no more than 4 steps in the annotatedanswers as in-domain data and the remaining as out-of-domain data. provides an overviewof the dataset splits.",
  "Train6,2351,2387,473Test1,0942251,319": "The results across different iterations is presented in . We observe that while the averagenumber of reasoning steps per iteration progressively declines, the accuracy remains stable acrossiterations. Several factors may explain the limited improvement in accuracy. Analysis of the modelsstep-skipping behavior reveals that intermediate steps frequently contain errors, indicating limitationsin the models ability for effective step reduction. Throughout the iterations, the model struggles togenerate responses in fewer steps, as the complexity of the questions often necessitates a completereasoning chain to reach a solution. This aligns with findings by Yu et al. , which suggest thatCoT reasoning is difficult to distill into System 1. We consider further exploration of the gradualtransition between System 1 and System 2 thinking, particularly for complex tasks, as a promisingdirection for future research.",
  ": Frequent skipping pattern in Analog of Algebra (translated into standard algebra)": "shows two frequent cases where models spontaneously develop to skip certain steps initerations. Note that in Analog of Algebra, we employ the cold start setting in initialization. Thebehaviors shown in the figure emerge solely from the model itself, which indicates the model hasdeveloped its own step skipping preference. In addition, we show two cases in from Analogof Algebra and Multi-digit Addition tasks. In these examples, the full step answers exhibit errors intheir reasoning processes. In contrast, the skipped step answers choose to skip steps and arrive at thecorrect answer in fewer steps.",
  "Q: D / G + I + B = A + C * H * F / x": "Full-step answer:A + C * H * F / x = D / G + I + BC * H * F / x = D / G + I + B - AH * F / x = D / G / C + I / C + B - A / CF / x = D / G / C / H + I / C / H + B - A / C / Hx = F / ( D / G / C / H + I / C / H + B - A / C / H )",
  "Q: 3909 + 641": "Full-step answer:1. 0 + 1 (plus carry 0) = 1 (write down 1, no carry over).2. 9 + 4 (plus carry 0) = 13 (write down 3, carry over 1).3. 3 + 6 (plus carry 1) = 10 (write down 0, carry over 1).4. Carry the final carry over 1 to the next place.Thus, the answer is 1031. Skipped-step answer:1. 9 + 1 (plus carry 0) = 10 (write down 0, carry over 1).2. 90 + 64 (plus carry 1) = 155 (write down 55, carry over 1).3. 3 + 0 (plus carry 1) = 4 (write down 4, no carry over).Thus, the answer is 4550."
}