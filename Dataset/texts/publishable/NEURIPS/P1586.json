{
  "Abstract": "Implicit bias describes the phenomenon where optimization-based training algo-rithms, without explicit regularization, show a preference for simple estimatorseven when more complex estimators have equal objective values. Multiple workshave developed the theory of implicit bias for binary classification under the as-sumption that the loss satisfies an exponential tail property. However, there isa noticeable gap in analysis for multiclass classification, with only a handful ofresults which themselves are restricted to the cross-entropy loss. In this work,we employ the framework of Permutation Equivariant and Relative Margin-based(PERM) losses [Wang and Scott, 2024] to introduce a multiclass extension ofthe exponential tail property. This class of losses includes not only cross-entropybut also other losses. Using this framework, we extend the implicit bias result ofSoudry et al. to multiclass classification. Furthermore, our proof techniquesclosely mirror those of the binary case, thus illustrating the power of the PERMframework for bridging the binary-multiclass gap.",
  "Introduction": "Overparameterized models such as neural networks have shown state-of-the-art performance in manyapplications, despite having the potential to overfit. Zhang et al. demonstrate that this potentialis indeed realizable by training real-world models to fit random noise. In recent years, there havebeen several research efforts that aim to understand the impressive performance of overparametrizedmodels despite this ability to overfit. Both the model architecture and the training algorithms forselecting the weights have been investigated in this regard. Work on implicit bias [Soudry et al., 2018, Ji et al., 2020, Vardi, 2022] has focused on the latter factor.Implicit bias is the hypothesis that gradient-based methods have a built-in preference for modelswith low-complexity. This hypothesis is perhaps best understood in the setting of (unregularized)empirical risk minimization for learning a linear model under the assumption of linearly separabledata. Soudry et al. showed that in binary classification, implicit bias holds when the loss hasthe exponential tail property [Soudry et al., 2018, Theorem 3]. The same work also demonstratedimplicit bias in the multiclass setting for the cross-entropy loss, but implicit bias for a more broadlydefined class of losses in the multiclass case is left open. In this work, we extend the notion of theexponential tail property to multiclass losses and prove that the property is sufficient for implicitbias to occur in the multiclass setting. Toward this end, we employ the framework of permutationequivariant and relative margin-based (PERM) losses [Wang and Scott, 2024].",
  "Contributions": "Multiclass extension of the exponential tail property (Definition 2.2)It is unclear how theexponential tail property for binary margin losses should be extended to the multiclass setting. Byusing the PERM framework, we provide a multiclass extension that generalizes the exponential tailproperty to multiclass (Definition 2.2 in .3). We further verify that this property holds forsome common losses. Sufficiency of the exponential tail property for implicit bias (Theorem 3.4)We prove that theproposed multiclass exponential tail property is sufficient for implicit bias. More precisely, we showin Theorem 3.4 that for almost all linearly separable multiclass datasets, given a convex, (-smooth,strictly decreasing) PERM loss satisfying the exponential tail property in Definition 2.2, gradientdescent exhibits directional convergence to the hard-margin multiclass SVM.",
  "Related Work": "Soudry et al. show that gradient descent, applied to unregularized empirical risk minimization,converges to the hard-margin SVM solution at a slow logarithmic rate, provided the loss satisfiesthe exponential tail property (defined below). Nacson et al. improve the convergence rateusing a specific step-size schedule. Ji and Telgarsky extend implicit bias to the setting ofquasi-complete separation [Cands and Sur, 2020], where the two classes are linearly separated butwith a margin of zero. Many works have also considered gradient-based methods beyond gradientdescent. For example, Gunasekar et al. examine the implicit bias effects of mirror descent[Beck and Teboulle, 2003], steepest descent [Boyd and Vandenberghe, 2004], and adaptive gradientdescent [Duchi et al., 2011, Kingma and Ba, 2015]. Cotter et al. , Clarkson et al. , Jiet al. study first order methods that are designed specifically to approach the hard-marginSVM as quickly as possible. Results for the multiclass setting are more scarce, and are always specific to cross-entropy. Soudryet al. establish implicit bias for cross-entropy loss. Lyu and Li focus on homogeneouspredictors and prove convergence of GD on cross-entropy loss to a KKT point of the margin-maximization problem. Lyu and Li proves convergence of gradient flow to a generalized max-margin classifier for multiclass classification with cross-entropy loss using homogeneous models.1 Inthe special case when the model are linear classifiers, the generalized max-margin classifier reducesto the classical hard-margin SVM. Lyu et al. consider two-layer neural networks and proveconvergence of GD on cross-entropy loss to the max-margin solution under an additional assumptionon the data, that both x and its negative counterpart x must belong to the dataset. Wang et al. prove that in certain overparameterized regimes, gradient descent on squared loss leads to anequivalent solution to gradient descent on cross-entropy loss. Beyond work establishing (rate of) convergence to the max-margin classifier, there is also a separateline of work [Shamir, 2021, Schliserman and Koren, 2022, 2023] focusing on the generalizationaspect of implicit bias. These works examine the binary classification setting, with the exception ofSchliserman and Koren who consider cross-entropy.",
  "Notations": "Let K 2 and d 1 denote the number of classes and feature space dimension, respectively. Let[K] := {1, 2, . . . , K}. Vectors are denoted by boldface lowercase letters, e.g., v RK whoseentries are denoted by vj for j [K]. Likewise, matrices are denoted by boldface uppercase letters,e.g., W RdK. The columns of W are denoted w1, . . . , wK. By 0n and 1n we denote then-dimensional vectors of all 0s and all 1s respectively. The n n identity matrix is denoted by In. By v we denote the Euclidean norm of vector v. A2 is the spectral norm of matrix A. Giventwo vectors w, v Rk, we write w v (resp. w v) if wj vj (resp. wj > vj) for all j [k];similarly we write w v (resp. w v) if wj vj (resp. wj < vj) for all j [k]. On the other 1Lyu and Li could be thought of as analyzing losses beyond CE, but the optimization problemwould be non-convex so convergence might not be to a global minimum. See Appendix A for a more detaileddiscussion.",
  "Multiclass Loss Functions": "In multiclass classification, a classifier is typically represented in terms of a class-score functionf = (f1, . . . , fK) : Rd RK, which maps an input x Rd to a vector v := f(x) of class scores.For instance, f may be a feed-forward neural network and v in this context is sometimes referred toas the logits. The label set is [K], and a label is predicted as argmaxjfj(x). A K-ary multiclass lossfunction is a vector-valued function L = (L1, . . . , LK) : RK RK where Ly(f(x)) is the lossincurred for outputting f(x) when the ground truth label is y. In binary classification, a classifier is typically represented using a function g : Rd R. The labelset is {1, 1}, and labels are predicted as x sign(g(x)). A binary margin loss is a function ofthe form : R R where (yg(x)) is the loss incurred for outputting g(x) when the ground truthlabel is y. Margin losses have been central to the development of the theory of binary classification,and the lack of a multiclass counterpart to binary margin losses may have impaired the developmentof corresponding theory for multiclass classification. To address this issue, Wang and Scott introduce PERM losses as a bridge between binary and multiclass classification.",
  "Permutation equivariant and relative margin-based (PERM) losses": "Assume the label set is [K]. Define 2 the matrix D := [IK11K1] R(K1)K. Observe thatDv = (vK v1, vK v2, . . . , vK vK1) for all v RK.Definition 2.1 (PERM loss [Wang and Scott, 2024]). Let K 2 be an integer, and L be a K-arymulticlass loss function. We say that L is",
  ". permutation equivariant if L(Sv) = SL(v) for all v RK and Sym(K),": "2. relative margin-based if for each y [K] there exists a function y : RK1 R so thatLy(v) = y(Dv) = y(vK v1, vK v2, . . . , vK vK1), for all v RK. We refer tothe vector-valued function := (1, . . . , K) as the reduced form of L.",
  ". PERM if L is both permutation equivariant and relative margin-based. In this case, thefunction := K is referred to as the template of L": "Wang and Scott show that PERM losses are characterized by their template . To show this,they introduce the matrix label code, an encoding of labels as matrices. Thus, for each y [K 1],let y be the (K 1) (K 1) identity matrix, but with the y-th column replaced by all 1s.For y = K, let y be the identity matrix. Note that when K = 2, this definition reduces toy = (1)y, the standard encoding of labels in the binary setting. Observe that (after permutation)yDv = (vy v1, vy v2, . . . , vy vK) RK1, where the vy vy = 0 entry is omitted. Pleasesee Wang and Scott [2024, Lemma B.2] for a simple proof.",
  "Conversely, let : RK1 R be a symmetric function. Define a multiclass loss function L =(L1, . . . , Lk) : RK RK according to Eqn. (1). Then L is a PERM loss with template": "Theorem 2.1 shows that a PERM loss is characterized by its template . The right hand side ofEqn. (1) is referred to as the relative margin form of the loss, which extends binary margin losses tomulticlass. As noted by Wang and Scott , an advantage of the relative margin form is that it",
  "Also see [Wang and Scott, 2024, Definition 2]": ": An illustration of the exponential tail property for the cross entropy/multinomial logisticloss when K = 3. Panel a. Plot of (u) = log(1 + exp(u1) + exp(u2)), the template for themultinomial logistic loss. Note that the complement of the positive orthant in the domain R2 is shownin gray. Panel b. and c. Plot of the upper bound (shown in black) and lower bounds (red) of",
  "decouples the labels from the predicted scores, which facilitates analysis. Our results below supportthis understanding": "Many losses in the literature are PERM losses, including the cross-entropy loss whose templateis (u) = log(1 + K1i=1 exp(ui)), the multiclass exponential loss [Mukherjee and Schapire,2013] whose template is (u) = K1i=1 exp(ui), and the PairLogLoss [Wang et al., 2022] whosetemplate is = (u) = K1i=1 log(1 + exp(ui)). See Wang and Scott for other examples.",
  "Multiclass analogue of exponential tail property": "In the binary setting, the exponential tail property defined in prior work (Soudry et al. , Nacsonet al. , Ji et al. ) is assumed to hold for the negative derivative of the loss. Similarly, inthe multiclass setting we are interested in bounding the negative gradient of the PERM loss template.",
  "Main Result": "Consider a dataset {(xn, yn)}Nn=1, with xn Rd and class labels yn [K] := {1, . . . , K}. Theclass score function for class k is fk(x) = wTk x. Define X RdN to be the matrix whose nthcolumn is xn. Define W RdK to be the matrix whose kth column is wk. The learning objectiveis",
  "w (t + 1) = w (t) R (w(t))": "Define the matrix-version of the trajectory W(t) RdK such that w(t) = vec(W(t)). Through-out this work, we frequently work with the risk as a matrix-input scalar-output function R(W), andas a vector-input scalar-output function R(w). These two formulations will each be useful in different situations. For instances, adopting the matrixperspective can facilitate calculation of bounds, e.g., in .2. On the other hand, the vectorizedformulation is easier for defining the Hessian of the risk 2R(w). See Appendix B for detail. We focus on linearly separable datasets:Assumption 3.3. The dataset is linearly separable, i.e. there exists w RdK such that n [N], k [K]\\{yn} : wynxn wk xn + 1. Equivalently, there exists W RdK such thatn [N], ynDWxn 1. 3We note that in the binary case the implicit bias result in [Soudry et al., 2018, Theorem 3] does not requirethe loss to be convex. Closing this binary-multiclass gap is an open question.4Note that multiclass exponential loss (u) does not have a global smoothness constant. However, we showin Appendix C.2.2 that any learning rate < 1/B2R (w(0))is sufficient for the gradient descent iterates toachieve local smoothness, where B =",
  "w = argminw12w2 s.t. n, k = yn : wynxn wk xn + 1.(4)": "Now we state the main result of the paper:Theorem 3.4. For any PERM loss satisfying Assumptions 3.1 and 3.2, for all linearly separabledatasets such that Assumption 4.1 holds, any sufficiently small learning rate 0 < < 212max (X),and any initialization w(0), the iterates of gradient descent will behave asw(t) = w log(t) + (t)where the norm of the residual, (t), is bounded. This implies a directional convergence behavior:",
  "Proof Sketch": "In this section we will overview the proof of the result. Along the way, we prove lemmas that extendto the multiclass setting results from Soudry et al. . The extensions are facilitated by the PERMframework, in particular the relative margin from of the loss. We adopt the notation of Soudry et al. where possible throughout this proof. Recallingthe notation and definitions from the paper: let us define the standard basis ek RK such that(ek)i = ki (where is the Kronecker-delta function), and the d-dimension identity matrix Id. DefineAk RdKd as the Kronecker product between ek and Id, i.e. Ak = ek Id. We can then relatethe original kth-class predictor wk to the long column-vector w as follows: Ak w = wk. Nextdefine xn,k := (Ayn Ak)xn. Using this notation, the multiclass SVM becomes",
  "k=1n,kxn,k1nSk.(6)": "Finally, definer (t) = w (t) log (t) w w(7)where w is a solution tok [K], n Sk : expxn ( wyn wk)= n,k.(8)In Soudry et al. , the existence of w is proven for the binary case for almost all datasets, andassumed in the multiclass case. Here, we also state the existence of w as an additional assumption:Assumption 4.1. Eqn. 8 has a solution, denoted w. We pose the problem of proving Assumption 4.1 for almost all datasets as a conjecture in Appendix H,where we also show experimentally that on a large number (100 instances for each choice ofd {2, 3, 4, 5, 6} and K {3, 4, 5, 6}) of synthetically generated linearly separable datasets,Assumption 4.1 indeed holds.",
  "Lemma 4.2. For any W RdK, we have that R(W) = Ni=1 xiyiDWxi yiD": "This expression involves weight matrix W. However the inequality we set out to prove (Eqn. (10))is in terms of w = vec(W). Throughout our main result proof, these two different forms weightmatrix versus vectorization of that matrix will each be useful in different situations. Thus, to shuttleback and forth between these forms, the following well-known identity is useful:Lemma 4.3. For equally sized matrices M and N, we have vec(M)vec(N) = tr(MN). Now we can prove our inequality of interest, i.e., Eqn. (10).Lemma 4.4. (Multiclass generalization of Soudry et al. [2018, Lemma 1]) For any PERM loss thatis -smooth, strictly decreasing, and non-negative, (Assumption 3.1) and Assumption 3.2, and foralmost all linearly separable datasets (Assumption 3.3), we have wR(w(t)) < 0.",
  "wR(w(t)) = tr( WR(W(t)))(11)": "To see how the PERM framework allows for a simple generalization of binary results, we will compareour multiclass proof side-by-side with the binary proof discussed in Soudry et al. [2018, Lemma 1].In the binary case, we have R (w) = Ni=1 yiwxi= R (w) = Ni=1 yiwxiyixi.Thus wR (w) = Ni=1 yiwxiyi wxi. In the multiclass case, the analogous quantity istr( WR(W(t))) which can be computed as",
  "i=1yiDW(t)xi yiD Wxi": "In the multiclass proof we used the risk gradient from Lemma 4.2 as well as the cyclic property of thetrace operator. Then we dropped the trace because yiDW(t)xi yiD Wxi is a scalar(since () RK1, yiDW(t)xi RK1). For illustrative purpose, we place the rest of theproof, in both the binary and multiclass setting, side-by-side:",
  "r (t + 1) r (t)2 2 R (w (t))2 + w2 t2(12)": "Remark 4.5. Let us see what happens to our proof if we just used the general risk form in Eqn. (2)without the PERM framework. First, we need an expression for the gradient of the risk: R (W) =Ni=1 xiRyiWxi. Proceeding similarly to the binary case, we focus on just the i-th term of",
  "Bounding the Second Term": "In the previous subsection we established a bound on the first term of Eqn. (9). Here we sketch themain arguments required to bound the second term, i.e. (r (t + 1) r (t)) r (t). For more detailsplease refer to Appendix F. We state our final bound below as a lemma:",
  "C1, C2, t1 : t > t1 : (r (t + 1) r (t)) r (t) C1t + C2t2 .(13)": "A remark is in order on the difference of the above result to Soudry et al. [2018, Lemma 20]: on ahigh-level, we are able to generalize the argument of Soudry et al. [2018, Lemma 20] to account forboth binary and multiclass classification, as well as general PERM ET losses beyond just CE.",
  "(yiDW(t)xi)k (1 r[K]\\{yi} exp(xi,rw(t))) exp(xi,kw(t))(19)": "for all k [K] \\ {yi}. We use Definition 2.2s exponential tail bounds by proving that the relativemargins xi,kw(t) that appear in Lemma D.2 eventually become positive. This is true due to thefollowing lemma (see Appendix E for the proof, which again mirrors the binary case): Lemma 4.8. (Multiclass generalization of Soudry et al. [2018, Lemma 1]) Consider any linearlyseparable dataset, and any PERM loss with template that is convex, -smooth, strictly decreasing,and non-negative. For all k {1, ..., K}, let wk(t) be the gradient descent iterates at iteration t forthe kth class. Then i {1, ..., N}, j {1, ..., K}\\{yi} : limt(wyi(t) wj(t))xi . This lemma lets us use the exponential tail bounds with any finite u. To conclude, we apply theupper (18) and lower bounds (19) to the summation in Equation (17), and reduce the problem to thatof Soudry et al. [2018, Appendix E], thereby proving Lemma 4.6. See our Appendix F for details.",
  "t=02 R (w (t))2 + w2 t2": "In the latter inequality we used Eqn. (12). Thus, C is bounded because from Soudry et al. [2018,Lemma 10], we know that t=0 R (w (t))2 < . Here we note that Soudry et al. [2018,Lemma 10] requires the ERM objective R (w) to be -smooth for some positive . It is easy toshow that if the loss is -smooth, then R (w) is 2max (X)-smooth. This explains the learning ratecondition < 2/2max (X)in our theorem. Also, a tp power series converges for any p > 1.",
  "Here we describe some of our works limitations/possible future research directions. We note thatthese questions have been analyzed for the binary classification setting, but not for multiclass": "Non-ET lossesIn our paper we only analyze multiclass implicit bias for losses with the ET property.Another possible line of future work is to analyze the gradient descent dynamics for non-ET losses.Nacson et al. and Ji et al. prove that in the binary setting, ET and well-behavedsuper-polynomial tailed losses ensure convergence to the maximum-margin direction, while otherlosses may converge to a different direction with poor margin. Is such a characterization possible inthe multiclass setting? Other gradient-based methodsThis paper only analyzes vanilla gradient descent. Another lineof work involves exploring implicit bias effects of other gradient-based methods, such as thosecharacterized in Gunasekar et al. . Nacson et al. uses similar proof techniques to proveresults for SGD, which is prevalent in practice and often generalizes better than vanilla GD ([Amiret al., 2021]). Non-asymptotic analysisOur result proves that the gradient descent predictors asymptotically donot overfit. However, in the binary classification case, Shamir goes one step further and provesthat for gradient-based methods, throughout the entire training process (not just asymptotically),both the empirical risk and the generalization error decrease at an essentially optimal rate (or remainoptimally constant). Does the same phenomenon occur in the multiclass setting?",
  "Conclusion": "We use the permutation equivariant and relative margin-based (PERM) loss framework to providean multiclass extension of the binary ET property. On a high level, while the binary ET boundsthe negative derivative of the loss, our multiclass ET bounds each negative partial derivative of thePERM template . We demonstrate our definitions validity for multinomial logistic loss, multiclassexponential loss, and PairLogLoss. We develop new techniques for analyzing multiclass gradientdescent, and apply these to generalize binary implicit bias results to the multiclass setting. Our mainresult is that for almost all linearly separable multiclass datasets and a suitable ET PERM loss, thegradient descent iterates directionally converge towards the hard-margin multiclass SVM solution. Our proof techniques in this paper demonstrate the power of the PERM framework to facilitateextensions of known binary results to multiclass settings and provide a unified treatment of bothbinary and multiclass classification. Thus it is possible that the binary results discussed in theLimitations section can also be extended using the PERM loss framework. In the future we wouldlike to consider more complex settings that have been analyzed primarily for the binary case, such asnon-separable data (Ji and Telgarsky ) and two-layer neural nets (Lyu et al. ).",
  "and Disclosure of Funding": "CS was supported in part by the National Science Foundation under award 2008074, and by theDepartment of Defense, Defense Threat Reduction Agency under award HDTRA1-20-2-0002. Theresearch of DS was funded by the European Union (ERC, A-B-C-Deep, 101039436). Views andopinions expressed are however those of the author only and do not necessarily reflect those ofthe European Union or the European Research Council Executive Agency (ERCEA). Neither theEuropean Union nor the granting authority can be held responsible for them. DS also acknowledgesthe support of the Schmidt Career Advancement Chair in AI. YW was supported in part by the Ericand Wendy Schmidt AI in Science Postdoctoral Fellowship, a Schmidt Futures program.",
  "ADiscussion of Lyu and Li": "Lyu and Li allow the class score functions to be linear classifiers, e.g., wk xi, but alsononlinear, e.g., cubed linear classifier (wk xi)3. By shifting the cubing operation to the loss, wecan view the implicit regularization result of Lyu and Li as a result for losses beyond the crossentropy. This resulting loss is rather exotic and we are not aware of it being used in the literature;it is interesting nevertheless. However, the optimization problem would become non-convex, soconvergence would not necessarily be to a global minimum:",
  "j[K]:j=k{x Rd : (wk x)3 > (wj x)3}": "Let us define Hj := {x Rd : (wk x)3 = (wj x)3}. Note that Hj Rd is the zero set of degree 3polynomials with variables in x, hence, a cubic hypersurface. Now, the set {x Rd : (wk x)3 >(wj x)3} is a subset of the set-theoretic complement of Hj in Rd. Thus, the decision regions arecomplicated geometric objects, compared to the classical hard-margin SVM.",
  "i[m],j[n] .(23)": "We do not define the Hessian of a matrix-input scalar-output function f : Rmn R. Instead, wewill define the Hessian for its vectorization vec(f) : Rmn R.Definition B.1 (Vectorization operator). Let vec denote the vectorization operator by stacking thecolumns of a vector. In other words, if A Rmn is a matrix with columns a1, . . . , an Rm, then",
  "Proof. See [Magnus and Neudecker, 2019, Ch.9-13] for the first identity and [Magnus andNeudecker, 2019, Ch.10-8] for the second identity": "The next two results will be referred to as the gradient formula and the Hessian formula,respectively, for the function g(X) := f(AXB).Lemma B.3. Let f : Rpq R be a matrix-input scalar-output differentiable function with Jacobiandenoted f : Rpq Rpq. Let A Rpn, X Rmn, and B Rmq. Define a functiong : Rmn R by g(X) := f(AXB). Then",
  "g(X) = Xf(AXB) = Bf(AXB)A": "Lemma B.4. Let f : Rp R be a vector-input scalar-output twice differentiable function. LetA Rpn, X Rmn be matrices and b Rm be a (column) vector. Let V be another matrixwith the same shape as X. Let x := vec(X) and v := vec(V). Define g(X) := f(AXb)and let g = vec(g) be the vectorization of g. Then we have the following formula for computingv2g(x)v:v2g(x)v = (AVb)2f(AXb)AVb.",
  "Xij": "Next, write U = [Uk]k[p],[q] in the matrix-comprehension notation. Recall that Uk, i.e., the(k, )-th entry of U, is precisely computed by A[k, :](XB)[:, ] = A[k, :]XB[:, ]. For eachk, [p] [q], we haveUkXij= (A[k, :]XB[:, ]) Xijwhere [k, :] and [:, ] denote taking the k-th row vector and -th column vector, respectively.Now, by Eqn. (71) of the Matrix Cookbook, we have the following expression of the matrix-partialderivative as an outer product",
  "vec(g)(x) = vec(g)(vec(X)) = vec(f)(vec(AXb))": "Note that the last equality is simply vec(f)(vec(AXb)) = f(AXb), but we work in the moregeneral case of a matrix B right now. We will need to simplify vec(AXb). It is more convenientduring the first phase of the proof viewing b as a m 1 matrix and denote it using uppercase letterB. First, applying Lemma B.1 to vec(AXB), we get",
  "Thus, by directly applying Lemma C.1, we obtain2 (u) H": "So now since H is defined to be a diagonal matrix, all thats left to do is bound the diagonal entriesby a positive constant. First note that from the definition of Ci, it follows that 1 + K1k=1 euk =Ci + 1 + eui. Combining this with Eqn. (26), we get:(2Ci + 1) eui",
  "((Ci + 1) + eui)2 =(2Ci + 1)": "((Ci + 1) + eui) ((Ci + 1) eui + 1)We can find a global minimum of the denominator of the above expression and thus arrive at anupper bound for the expression. Differentiating with respect to ui and setting to 0 yields a singlecritical point at ui = log (Ci + 1), which produces a value of 4 (Ci + 1) when substituted in thedenominator (this is a global minimum of the denominator expression). Thus, we get",
  "v2g(x)v = (AVb)2f(AXb)AVb": "We are interested in the special case where X W is a linear classifier (represented as a matrix) andx vec(W) = w is its vectorization as in . Moreover, g(X) represents the risk R(W),viewed as a matrix-input scalar-output function (defined in Appendix B), while g(x) represents thevectorized risk R(w), viewed as a vector-input scalar-output function (defined in Appendix B). Wewill use the formula in Lemma B.4 to calculate v2R(w)v, where we substitute in",
  "Note that v = vec(V) = VF by the definitions of the Frobenius norm and vectorization.Thus,maxvRdK:v=1 v2R(w)v =maxVRdK:VF =1 vec(V)2R(w)vec(V).(29)": "We note that we never defined the Hessian of a matrix-input function, i.e., we do not work with2R(W). Combining the two previous identities, we have provenCorollary C.2. Let R(W) be the risk viewed as a matrix-input scalar-output function defined inEquation (3). Let R(w) be the vectorization of R(W). Then we have",
  "i=1R(W)1(yiDVxi)2 replace each entry of vdiag with risk": "In equality 2 we took trace of a scalar (the expression in equality 1 is a scalar, so taking the trace of itwill not change the value) and used the cyclic property. For equality 3: as per Equation (28), 2 isa diagonal matrix. Finally, in the last inequality, we bound each element of the diagonal vector (i.e.exp (ui) for all i [K 1]). Dropping the R(W) and maxVRdK:VF =1 from the front:",
  "i=1xi R (w) for all aj > 0, M 1,Mi=1 a2j Mi=1 aj": "Gradient descent is a special case of steepest descent with the Euclidean norm [Boyd and Vanden-berghe, 2004]. Thus, we can apply Gunasekar et al. [2018, Lemmas 11 & 12] to see that R (w) 0even for multiclass exponential loss. Elaborating on this: these lemmas from Gunasekar et al. assume a convex risk objective (which we have in the case of multiclass exponential loss). Addition-ally, they assume that R (w) BR (w) and2R (w)2 B2R (w). In the above sectionwe prove these exact results with B as defined in Eqn. 31. Finally, Lemma C.3 below proves thatkDF =",
  "C.3.2-smoothness": "Notice that the partial derivative of the template is exactly the same expression as the derivative ofthe logistic loss (i.e. binary cross-entropy). Thus, the exact same proof as logistic loss can be used toprove -smoothness for the PairLogLoss as well. Thus, from Appendix C.1.2, = 1/4 (logistic lossis simply the K = 2 case for cross-entropy).",
  "DPseudo-index": "Note that yiDR(t)xi produces a (K 1)-dimensional vector with entries of the form of(ryi(t) rk(t)) xi, k [K]\\{yi}. For k [K]\\{yi}, let us represent the corresponding en-try of the vector as yiDR(t)xik. Note that this indexing is not the same as the kth entry of thevectors, since the yith entry (ryi(t) ryi(t)) xi is not present in the vector. Similarly, let us defineyiDW(t)xik to be the corresponding entry of . This section makes this indexing trick rigorous.Lemma D.1. Let W RdK be arbitrary and w := vec(W) be its vectorization. Let (xi, yi) be atraining sample. Then there exists a bijection that depends only on yi that maps the entries of",
  "to the elements of the set of yi-versus-k relative margins, i.e., {xi,kw R : k [K] \\ {yi}}": "The following definition makes the bijection from Lemma D.1 concrete.Definition D.1 (Pseudo-index). In the situation of Lemma D.1, define i,k : RK1 R to bethe coordinate projection such that yiDWxii,k = xi,kw. In other words, i,k selects theyi-versus-k relative margin. When the sample index i is clear from context, we drop i from thesubscript and simply write k. The pseudo-index is useful for working with the exponential tail bounds:Lemma D.2. In the situation of Lemma D.1, consider (yiDWxi) which is the (K 1)-dimensional vector of partial derivatives of the template evaluated at yiDWxi. If satisfiesDefinition 2.2, then(yiDWxi)k exp(xi,kw)and(yiDWxi)k (1",
  "Re-stating the lemma:": "Lemma 4.8. (Multiclass generalization of Soudry et al. [2018, Lemma 1]) Consider any linearlyseparable dataset, and any PERM loss with template that is convex, -smooth, strictly decreasing,and non-negative. For all k {1, ..., K}, let wk(t) be the gradient descent iterates at iteration t forthe kth class. Then i {1, ..., N}, j {1, ..., K}\\{yi} : limt(wyi(t) wj(t))xi .",
  "(since yiDW(t)xij = (wyi(t) wj(t))xi)": "Note that in the binary case, the above convergence-to-infinity condition is for a scalar quanity,where the assumption that the loss be strictly decreasing and non-negative suffices. In the multiclasssetting, we must ensure that all entries of the vector yiDW(t)xi converges to infinity. This is anontrivial result and is addressed by our Proposition G.1.",
  "is the zero vector. Then limt utj = for every j [K 1]": "We prove Proposition G.1 by first proving a structural result (Theorem G.2) concerning symmetricand convex function f : Rn R. The proof of Proposition G.1 will be presented in Appendix G.2as an application of the structural result, where we take f = , the template of a PERM loss, andn = K 1, number of classes minus one.",
  "G.1Proof of Theorem G.2": "Now, to prove the above theorem, we will use induction on m in the following lemma, which issimply a stratification of Theorem G.2 into cases indexed by the parameter m:Lemma G.3. Suppose that f : Rn R is a convex, symmetric, and differentiable function. Letm {0, 1, . . . , n}. Then for any real number C R and any x Rn with the property that|{v val(x) : v < C}| = m, we have",
  "Note that if we have proved Lemma G.3 for each m {0, 1, . . . , n}, then Theorem G.2 holds": "The base step: we prove Lemma G.3 when m = 0 and m = 1. Strictly speaking, the proof-by-induction technique typically only involve only the base case, which would be the m = 0 case in thisinstance. But below, we will see that in the induction step, the m = 1 case is helpful. Note that the m = 0 case holds vacuously, since x C = x. Below, we focus on the m = 1 case,where there exists a unique v val(x) such that v C. Let i argmin(x). Note that we haveidx(v, x) = argmin(x). Using Equation (39) (Fact 1), we have that",
  "Now, since C > v and |argmin(x)| > 0, we must have that [f(x C)]i [f(x)]i 0, asdesired. This proves the base step": "Induction step: Suppose Lemma G.3 holds for every integer m where 0 m < n, we mustshow that Lemma G.3 also holds for m + 1. To this end, let x Rn and C R be such that|{v val(x) : v < C}| = m+1. Let v1, . . . , vm+1 R be all the elements of {v val(x) | v < C}enumerated in increasing order, i.e., v1 < < vm+1. Note by construction, we have that {v val(x) | v < vm+1} = {v1, . . . , vm} and so we immediatelyget that |{v val(x) | v < vm+1}| = m. By the m-th case of Lemma G.3 (i.e., the inductionhypothesis) using vm+1 as C, we get",
  "G.2Application to our setting": "The condition limt utj = by definition means that for every real number M R, there existsT such that for all t T we have utj > M. Thus, suppose that there exists j [K 1] suchthat limt utj = , then there exists a real number M R such that for all T = 1, 2, . . . thereexists some t T such that utj M. Passing to a subsequence, we assume that utj M (and somin(ut) M) for all t = 1, 2, . . .. Note that limt (ut) = 0 continues to hold.",
  ".(50)": "It is more convenient to pool all the class-specific support vectors Sk into a single set: S (n, k) : ( wyn wk) xn = 1. For readability, we linearly order the tuples in S, i.e., we as-sign to each (n, k) S a unique index i {1, ..., |S|}. In other words, we define n(1), . . . , n(|S|)and k(1), . . . , k(|S|) such that",
  "The challenge in proving Condition 2 in the multiclass case is that the column vectors of X mayhave repeats, i.e., it is possible for n(i) = n(i) when i = i. It is easy to generate synthetic linearly": ": Small simulation with N = 10, d = 2 and K = 3. The loss used is the PairLogLoss.Top row. Decision regions of classifiers along the gradient path w(t) at t = 100, 1000, and 100000,respectively from left to right. Bottom row. Decision regions of the hard-margin multiclass SVM.Note that most of the progress is made between iterations 100 and 1000. separable multiclass datasets satisfying this condition. Nonetheless, we observe that even in such acase, the matrix G has rank |S|, i.e., Condition 2 holds. We verify this experimentally in the Pythonnotebook checking_conjecture_in_Appendix_H.ipynb available at In the binary case, linear classifiers are parametrized simply as a single vector, rather than the morecumbersome one-vector-per-class parametrization. Under the one-vector parametrization, the Mmatrix becomes a 1-by-|S| matrix consistings of only 1s, and G reduces to X. Moreover X has norepeats. Thus, Condition 2 holds trivially. In both the multiclass and binary settings, given Condition2, the proof for Condition 1 can proceed exactly as in Lemma 12 from Soudry et al. wheretheir XS is replaced by our G.",
  "The code can be ran on Google Colab with a CPU runtime in under one hour": ": Large simulations with N = 100, d = 10 and K = 3. The loss used is the PairLogLoss.The curves are 10 independent runs with randomly sampled data and random initialization for gradientdescent over 100000 iterations. Note that the convergence in direction of the gradient descent iteratesto the hard-margin SVM slows down in log-log space."
}