{
  "Abstract": "Multi-view data from the same source often exhibit correlation. This is mirrored incorrelation between the latent spaces of separate variational autoencoders (VAEs)trained on each data-view. A multi-view VAE approach is proposed that incorpo-rates a joint prior with a non-zero correlation structure between the latent spaces ofthe VAEs. By enforcing such correlation structure, more strongly correlated latentspaces are uncovered. Using conditional distributions to move between these latentspaces, missing views can be imputed and used for downstream analysis. Learningthis correlation structure involves maintaining validity of the prior distribution, aswell as a successful parameterization that allows end-to-end learning.",
  "Introduction": "Data from multiple sources describing the same subjects arises in a wealth of settings. This can beclinical information of patients alongside genetic information and scan data. Datasets consisting ofmultiple views are referred to as multi-view or multi-modal data. There are instances where not allviews are always available for every realisation. For example, a patient may miss an appointmentor machinery may falter, resulting in no reading for a specific view. Missing data results in smallerusable datasets and reduced statistical power with many methods only applicable to full datasets and can result in reduced performance in downstream analysis . This manuscript presents amulti-view imputation approach, where its aim is to impute the realisations of a missing view byincorporating the information learnt from the other view. The proposed multi-view imputation method, named Joint Prior Variational Autoencoder (JPVAE), isbased on variational autoencoders (VAEs) , with the views connected solely through a joint prioron the VAEs latent embeddings. Standard autoencoders seek to encode a latent representation ofdata, and from this encoding reconstruct the original data via a decoder. Multi-view approaches allowthe latent representation of a missing view to be obtained, from which the reconstruction can be usedas an imputation of the missing view. Several multi-view imputation approaches exist in the literaturebased on autoencoders . However, as variational autoencoders learn a continuous embedding,they provide better interpolation of the latent space than standard autoencoders, making them amore suitable approach for an imputation method. The proposed joint prior in JPVAE incorporates anon-zero correlation structure that is found to increase the observed correlation between views in thelatent spaces. This allows for successful movement between latent spaces, improving the ability toimpute missing views.",
  "arXiv:2411.03097v1 [stat.ML] 5 Nov 2024": "Various approaches have been proposed extending VAEs to the multi view case, including those by, with missing data imputation included as a feature of these methods. Most recently, proposedmethods differ via their method of approximating a joint posterior. Daunhawer et al. discussthe undesirable upper bound these methods put on a lower limit of the log-likelihood used as theobjective function in VAEs. In contrast to existing approaches, which largely assume some jointposterior and/or shared latent space, our work has the novelty that is based on a joint prior betweenthe latent variables. To the best of our knowledge, this is the first attempt made to correlate the latentspaces of VAEs natively through a joint prior. JPVAE is most similar in structure to the private version of Deep Variational Canonical CorrelationAnalysis (VCCA-private), a multi-view VAE approach by Wang et al. . However their modelcontains both private and shared variables in the latent space and doesnt have a suitable approach forimputing missing views. JPVAE can also be seen as an application of the ideas from self-supervisedlearning techniques such as Barlow Twins , where the embeddings from noisy versions of thesame input are driven to be highly correlated via a loss function. illustrates an application of the proposed approach JPVAE, on imputing view 2 of the datathat corresponds to the bottom half of MNIST digits using the top half of the digits data (view 1).Through the proposed approach, reconstructions of missing views can be obtained. This is achievedthrough the conditional distribution between the two latent variables, as illustrated in . Byincorporating a joint prior with a non-zero cross-correlation structure, we observe better qualityresults in imputing view 2. Further realisations of image imputation, including imputation of view 1from view 2, can be found in Appendix A.5.",
  "(b) Correlation learnt natively": ": Additional realisation of an imputation of the top half of MNIST digits (view 1 of thedata) using the bottom half of the image (view 2) on a JPVAE model trained with (a) independentpriors (completely separate VAEs) and (b) a joint prior with learnt correlation structure betweenlatent spaces. The cross entropy loss between true top half of image and imputation is 114.3 in (a)and 104.1 in (b). : Results for [Y 1; Y 2] represent classification accuracy % for model trained on the trainingsplit of [ X1; X2] and tested on the test split of [Y 1; Y 2] (the column wise concatenation of Y 1and Y 2). Accuracy for [X1; X2] with standard deviation in brackets is 98.04% (0.074). Error barspresent +/- one standard deviation. : Results for (Y , Z) represent classification accuracy % for model trained on the trainingsplit of Y and tested on the test split of Z. Accuracies for (X1, X1) and (X2, X2) with standarddeviation in brackets are 93.59% (0.25) and 90.83% (0.23) respectively. Error bars present +/- onestandard deviation.",
  "The code for implementation of JPVAE and the numerical experiments contained in this manuscript can befound at": ": Workflow to obtain reconstruction x1 and imputation x2|1 from input x1 only, on pre-trained JPVAE network. Trapeziums represent encoders/decoders and rectangles represent vectors.The structures for view 1 and 2 are shown in blue and orange respectively. The expectation step isshown in red. takes advantage of this correlation and whilst marginals for each VAE are assumed to follow astandard normal distribution, a joint prior is assumed with a non-zero cross correlation. The imposedcorrelation structure is learnt natively by forcing a stronger correlation in the latent space. presents the proposed method and related theorems that showcase the validity of the proposedmethod. As part of our work, we present and prove theorems that enable the parameterization ofdifferentiable positive definite matrices that allow end-to-end learning. Through our conductedexperiments presented in , learning the correlation structure leads to improved imputationability, lower reconstruction loss and better predictive likelihood. Imputed views from JPVAE can beused for downstream tasks, with improved classification accuracy demonstrated. Lastly, better VAEmodels are learnt with JPVAE preventing posterior collapse, a common problem observed with VAEs.",
  "Notation": "Throughout this paper matrices are denoted by capital letters and column vectors are denoted bylower case letters, both emboldened. X = {xi}ni=1 with vector xi Rc represents an entiredataset in Rnc, with xi an individual realisation. A diagonal matrix with vector a along thediagonal is represented by diag(a). A block diagonal matrix with matrices Ai along the diagonal isrepresented by bdiag(Ai). Vertical concatenation of column vectors a and b is denoted by (a; b)i.e. (a; b) = (aT ; bT )T . The eigenvalues and singular values of matrix A are denoted by i(A) andi(A) respectively, indexed by subscript i in order of decreasing magnitude. I denotes an identitymatrix of relevant dimension.",
  "Multi-view variational autoenconder with a joint prior": "This section presents the novel multi-view VAE approach JPVAE where each view has a separateassociated VAE. Before discussing the proposed approach in detail, the main concepts of VAEs areintroduced. Subsequently the single-view VAE is extended to the multi-view setting and the jointprior is presented. The different components of JPVAE are presented including theoretical work onmatrix parameterization which enables end-to-end learning.",
  "Variational autoencoder": "A standard VAE seeks to encode data X in a probabilistic latent space and decode from this space toreconstructed data X . The goal is for X to be as close as possible to X. As the latent space is aprobabilistic embedding rather than one obtained by a deterministic mapping, it has the advantage that it can be explored fully, including the sampling of new realisations from the same distribution as X.These realisations are assumed to be taken from the same underlying distribution as X, p() = p(|)where is the set of parameters defining the distribution. VAEs are a type of variational Bayesian method that seek to find a lower bound for this marginalprobability, p(X), through a Bayesian framework. The likelihood of x Rc given latent variablez Rd is denoted by p(|z) and the prior for the latent variables, usually taken to be a standardnormal, is denoted by p(z). The encoder and decoder are neural networks that seek to learn theposterior distribution of z given x, p(|x), and the likelihood of x given z respectively, giventhe assumed prior over the latent space. In order to make the posterior learnable, an approximateposterior distribution is used that usually is a multivariate normal distribution. The approximationp(|x) q(|x) is made where denotes the parameters of the probabilistic encoder, q(|x).The encoder maps an input x to a mean vector (x) Rd and to the log of the vector of variances,2(x) Rd. A sample is drawn from the approximate posterior distribution and this is used as aninput to the neural network which acts as the decoder, D. As appears within the distribution of thelatent variables, derivatives cannot simply be taken inside the expectation term. Differentiability of theloss function is required for the loss-driven parameter update steps and therefore a reparamaterizationtrick needs to be implemented . Instead of drawing z directly, N(0, I) is drawn and z = (x) + diag(2(x)) is determined.The distribution over which we take expectation is now independent of the parameters for which wetake derivatives, and derivative update steps may now be implemented. The neural network decoderthen seeks to reconstruct x from the latent variable z. A maximum likelihood principle is then applied to the marginal likelihood of the data p(X) to obtainestimates for the parameters within the encoders and decoders. As the likelihood is intractable a lowerbound on the log-likelihood, known as the Evidence Lower Bound (ELBO) is instead maximised,given by:L(, ) = Ezq(|x) [ln(p(x|z))] DKL(q(|x)||p())(1) where DKL(r|s) denotes the Kullback-Leibler (KL) divergence between distributions r and s .Ezq(|x)() denotes the expectation with respect to the conditional distribution of z given x.Maximising the ELBO is equivalent to finding a balance between an approximate posterior that isclose to the prior and putting weight on the latent variable space that maximises the likelihood of thedata given these same variables, ln(p(x|z)). Without the KL term, the delta function is returned asthe approximate posterior and the autoencoder is recovered. With this VAE formulation the KL term often becomes very small or vanishes leading to theposterior equalling the prior (posterior collapse). This is referred to as KL vanishing and leads toa decoder that is largely independent of the latent variables. See for further discussion of thisproblem. To combat this, we implement KL annealing a procedure where a weight is introducedon the KL term and gradually increased, typically from 0, as learning occurs . The objectivefunction becomes:",
  "L(, ) = Ezq(|x) [ln(p(x|z))] DKL(q(|x)||p()).(2)": "It is now assumed two data views with shared row dimension are present and represented byX = {X1, X2} with Xi RNci. For brevity, the sample subscript is dropped and xi Rcirefers to an individual realisation from view i. Here N is the number of rows in both views andci is the number of columns in view i = 1, 2. This shared row dimension implies the views arepaired, with row j representing the same individual/sample in both views. This allows for meaningfulcorrelation in the latent space. The notation introduced in this section for single view VAEs is usedin the following sections where multi-view VAEs are presented. Subscripts are used to denote therelevant views e.g. the encoder and decoder in view i are represented by Eii and Dii respectively.",
  "Joint prior variational autoencoder": "As the different views in a multi-view dataset are from a common source, correlation exists betweenthe views. This translates to correlation between the latent spaces of each view from independentlytrained VAEs. JPVAE takes advantage of this correlation, enforcing the relationship between thetwo views via a joint prior on the latent variables, as illustrated in . Enforcing the proposedcorrelation structure between the latent spaces ensures we can move from the original space where : Workflow to obtain reconstructions x1 and x2 from realisations x1 and x2 using a learntJPVAE structure. Trapeziums present the encoders and decoders of the two views. Vectors arepresented by the rectangles. The structures for view 1 and 2 are shown in blue and orange respectively.The prior on the latent variables is shown in red. data are highly correlated in a non-linear fashion, to a space where the correlation is linear, andback to the reconstructed space that has a non-linear correlation. The marginal prior on the latentvariables corresponding to each view is a standard normal, as with the traditional VAE. However, across-covariance matrix C is assumed between the latent variables z1 and z2. Having separate VAEstructures for each view assumes conditional independence in both directions: (a) given the data thelatent variables are independent, and (b) given the latent variables the data are independent. Thisallows the unique features of each view to be encoded and decoded. As the latent spaces are linearly correlated, it is possible to move between them via the conditionaldistribution, as illustrated in . This allows for imputation of missing views, obtaining areconstruction of x2 solely from realisation x1 (and vice versa). If a joint prior is not assumed,separate VAEs are trained on each view and there is no correlation enforced between latent spaces.Whilst some correlation exists between the latent spaces, it is not as strong, and therefore may notproduce as accurate reproductions, as illustrated in the numerical experiments of .",
  "q(z|x) = q(1,2)((z1, z2)|(x1, x2)) = q1(z1|x1)q2(z2|x2).(3)": "The individual approximate posteriors qi(|xi) are multivariate Gaussians with mean and covari-ance matrix determined by the output of Eii. For input xi, these are i(xi) and diag(2i (xi))respectively. As the posteriors are assumed to be independent, the joint distribution is multivariateGaussian with mean (1(x1); 2(x2)). The covariance matrix is represented by bdiag(i(xi)) withi(xi) = diag(2i (xi)). Similarly, by assuming the data is independent given the latent variables,the likelihood function can be expressed as:",
  "Joint prior": "There are assumed to be ni latent variables in the VAE for view i, represented by the random vectorzi Rni. Without loss of generality it is that assumed n1 n2. The joint prior distribution of therandom vector z = (z1, z2) Rn1+n2 is assumed to be a multivariate normal with mean 0 andcovariance matrix, C:",
  "(6)": "where C Rn2n1 is the cross-covariance matrix encapsulating the relationship between the twolatent spaces. Ii is the ni ni identity matrix. As C is a covariance matrix, it needs to satisfytwo conditions: symmetry and positive semi-definiteness. With the defined structure it is clearlysymmetric and so it is sufficient to require C to be a positive semi-definite matrix to ensure it is awell-defined covariance matrix. For a covariance matrix with structure as defined in Eq. 6, it is usefulto note that C is positive definite if and only if I1 CT C (or I2 CCT ) is positive definite. Additionally, as (a) a real symmetric matrix is positive (semi) definite if and only if all of itseigenvalues are positive (non-negative) [17, p. 51] and (b) a matrix is invertible if and only if all of itseigenvalues are non-zero, covariance matrix C (and I1 CT C/I2 CCT ) is positive definite ifand only if it is invertible.",
  "As this assumes the existence of 1rand 1s , both covariance matrices must be positive definite": "By utilising the result of Eq. 7 with the prior over z = (z1, z2) set as r = pC, and the approximateposterior distribution as s = q1(|x1)q2(|x2), the KL term of Eq. 2 is obtained. Explicitly,pC = N(0, C) and q1(|x1)q2(|x2) = N((1(x1), 2(x2)), q) with q = bdiag(i). Forthe utilisation of Eq. 7, it is assumed that C is positive definite and the variances of the approximateposterior are positive.",
  "Matrix parameterization": "The matrix C is unknown and therefore must be either chosen apriori or be optimised. In our work,C is updated along with all other parameters at each update step, using the ELBO function as theloss function. By doing this, two challenges were faced and addressed. Firstly, C must be updated insuch a way that C is a valid covariance matrix. Secondly, a differentiable parameterization of C isneeded to allow for end-to-end learning. Conditions for validity of the update step are outlined in thefollowing theorems.",
  "Theorem 2.1. C defined as in Eq. 6 is positive semi-definite if and only if all singular values of Care bounded by 1": "Proof. Due to previous remarks, it is equivalent to show that I1 CT C is positive semi-definite ifand only if the singular values of C are bounded by 1. Further, this is equivalent to the showing thateigenvalues of I1 CT C are non-negative if and only if the eigenvalues of C are bounded by 1.",
  "which combined with the relations above gives k(I1 CT C) = 1 + n1+1k(CT C) =1 2n1+1k(C). Therefore all eigenvalues of I1 CT C are non-negative if and only if all singularvalues of C are bounded by 1": "If the inequality on 1(C) is replaced by a strict inequality then C is guaranteed to be positivedefinite. This is clear as all eigenvalues of I1 CT C are now positive which ensures positivedefiniteness. This guarantees the applicability of Eq. 9. A further restriction, outlined in Theorem2.2, can be made which enables a different parameterization of C which assumes a scaled orthogonalrelationship between the views.",
  "As I1 is positive semi definite, if || 1 then, so too is I1 CT C": "Alternatively, this could be seen as a corollary to Theorem 2.1 - due to the semi-orthogonality ofC, all singular values of C are equal to 1 which means i(C) = with || 1. Again, if thecondition on is replaced with a strict inequality (|| < 1)) this guarantees applicability of Eq. 9. Asimplification of Eq. 9 assuming this orthogonality condition can be found in Appendix A.2. To ensure applicability of Eq. 9, we can either require (a) C = C with C an orthogonal matrixand || < 1 or (b) 1(C) < 1. The latter requires a matrix factorisation of C which includessingular values. The most obvious approach is to use a singular value decomposition (SVD) of C i.e. C = USV T where U and V are orthogonal matrices and S is the matrix of singularvalues i.e. S = diag(k(C)). These singular values can be parameterized by k(C) = (1 exp(k)/(1 + exp(k)) for k R. To fully parameterize C with the singular value constraint,a parameterization of orthogonal matrices U and V is needed. Similarly, a parameterization oforthogonal C is needed for option (a). The following assumes n1 = n2 = n. As discussed by Shepard et al. , there are four popular parameterizations of orthogonal matrices.All methods in their preliminary form parameterize at most a subset of the orthogonal matrices (atmaximum those with determinant +1 or -1). These parameterizations must therefore be extended tomap to the entire orthogonal matrix space. As it is the only one-to-one mapping, the rational Cayleytransform has been chosen for use within JPVAE. In its original form, the Cayley transform tells usthat for all orthogonal matrices with no eigenvalues equal to 1, there exists a unique skew-symmetricmatrix A Rmm such that O = (I +A)(I A)1 . To extend the parameterization to the fullspace of orthogonal matrices, an extra matrix consisting of 1s and 1s along the diagonal is needed. Let J = diag([1mr; 1r]) where r = floor(m/(1 + exp(s))) for s R. Then O =J(I+A)(IA)1 is a mapping onto the space of orthogonal matrices. C is therefore parameterizedin full by (n 1)2/2 + 1 parameters: ({aij R : i < jfori, j {1, , n}} , {s R}).",
  "Imputation": "Once the model has been learned on the training data, missing views can be imputed. Assume data isavailable for view j, but not view i, given xj we can impute the missing value of xi. Data xj is fedinto the encoder for view j, Ej, and latent variables are sampled giving zj = a. An estimate of zican be obtained using the conditional distribution of zi given zj. This is fed through decoder i, Di,to produce an estimate of xi given xj, xi|j. The joint marginal of z1 and z2 is assumed to be a multivariate normal with mean [1; 2] andcovariance matrix . The maximum likelihood estimator for the mean and covariance are obtainedand denoted by [ 1; 2] and respectively. As any subset of variables from a multivariate normalconditioned on a known second subset of variables also follows a multivariate normal distribution,the conditional distribution can be found explicitly. For i = j, the distribution of zi given zj = a isobserved is:zi|zj = a Ni + ij 1jj (a j), ii ij 1jj ji(12) where kl Rnknl is the submatrix of corresponding to the variables associated with view k andview l. The conditional mean E(zi|zj = a) = i + ij 1jj (a j) is then used as an estimate ofthe latent variables in latent space i and fed into Di to obtain xi|j, the imputed value of xi given xj.",
  "Numerical experiments": "Through a series of experiments the performance of JPVAE is explored for both imputation purposesas well as for downstream analyses like classification. As JPVAE enables imputation of missingviews ( Xi|j), this ability is investigated alongside reconstruction of views ( Xi). A multi-view dataset was created from the binary version of the popular MNIST dataset, whichconsists of handwritten digits from 0 to 9 . For each image, the top half was taken as view 1 andthe bottom half was taken as view 2. This dataset of halved images is referred to as hvdMNIST. ThehvdMNIST dataset has the desirable property of having a strong correlation between views. Thedataset contains 50, 000 training images and 10, 000 test images. Experiments are repeated with5 different random seeds and the average and standard deviation reported. Details on the modelarchitecture and training details can be found in Appendices A.3 and A.4 respectively. Two variants of JPVAE are explored, where in each one the validity of C is enforced via differentways. The first variant imposes a bound on singular values (such that 1(C) < 1). The secondvariant enforces a scaled orthogonality where CCT = CT C = 2I, with the value of set as 0.95.The two variants of JPVAE are compared with the C = 0 case, which corresponds to completelydisjoint VAEs for each view. Without explicitly learning a correlation structure, correlation is present between the two generatedlatent spaces. By incorporating a joint prior with a non-zero cross-correlation as presented in Eq. 6 into the loss function, JPVAE increases the correlation between views in the latent space as shown in. illustrates the improvement in the ability to reconstruct view 2 given view 1 andvice versa when correlation structure is learnt. Enforcing the orthogonality restriction improves theperformance of JPVAE compared with simply applying the singular value bound. : Empirical cross-correlation between view 1 and view 2 in the latent spaces. The left plotrepresents empirical cross-correlation for C = 0 and the right shows the same for C learnt withthe orthogonality restriction imposed. The Frobenius norm of these matrices are 1.703 and 3.448respectively.",
  "C = 024.64 (0.37)25.56 (0.24)114.1 (2.3)127.5 (3.2)1(C) < 124.08 (0.44)25.02 (0.21)106.6 (1.4)117.4 (4.1)CCT = CT C = 2I23.41 (0.29)23.98 (0.31)97.25 (1.9)106.6 (1.9)": "The improved performance of the method is enabled by the learnt correlation that prevents posteriorcollapse. Following the work by , the phenomenon of posterior collapse is indicated by thepercentage of active units (AU). The activity of unit u in the latent space is measured by Au =CovxEuq(u|x)[u]. A unit is considered active, i.e. to not have suffered from posterior collapse, ifAu 102. A higher percentage of AUs are preserved when C is learnt, with the best case scenarioobserved with the orthogonality constraint (). Wang et al. proved that posterior collapseis equivalent to latent variable non-identifiability. This indicates that by enforcing the orthogonalityrestriction, we make the latent variable space identifiable.",
  "C = 061 (1.4)1(C) < 166 (1.4)CCT = CT C = 2I98.5 (2.2)": "The imputed views, X1|2 and X2|1, can be used for downstream tasks. If not all views are availablefor an individual, this allows techniques requiring all views to be applied. As an illustration of theperformance of the imputed data in downstream tasks, a basic multi-layer perceptron classifier was trained and tested on different combinations of reconstructed and imputed views. The performanceof a classifier trained on the reconstructed data indicates that when C is learnt, the relevant signalremains present in both the reconstructed and the imputed data. The performance on imputed data isgreatly improved by learning C, and in particular by enforcing the orthogonality constraint (;results with standard deviation can be found in .). : Results for (Y , Z) represent classification accuracy % for model trained on the trainingsplit of Y and tested on the test split of Z. Accuracies for (X1, X1) and (X2, X2) with standarddeviation in brackets are 93.59% (0.25) and 90.83% (0.23) respectively. Best results in bold. Forclarity, results with standard deviation reported can be found in .",
  "C = 089.0747.2177.2285.8851.8378.811(C) < 189.6764.5080.2286.4667.0682.38CCT = CT C = 2I90.3177.2283.5487.8076.5586.30": "Not only does learning correlation structure improve the ability to impute data, the reconstructionloss and classification accuracy for reconstructed data xi is improved compared with those scoresseen when training each view on separate VAEs (). This may be due to the increased use ofthe latent space, as evidenced by the higher percentage of active units. Whilst a classifier trainedon the imputed data from C = 0 demonstrates the retention of signal, the low accuracy seen for aclassifier trained on reconstructed data and high reconstruction loss indicates that it does not retainthe specific signature of the input. For example, taking the top half of a digit 2 as the input, it maycorrectly reconstruct the bottom half of a realisation of the digit 2 but not correctly reconstruct thespecific realisation (as seen in a). Using the joint prior we see that the view with stronger signal (view 1) is bolstering the classificationof the view with weaker signal (view 2). Whilst classification accuracy on x1 is higher than thaton x2, the accuracy on the imputed data X2|1 sees a smaller drop, and is greater than that of X1|2.Notably, the accuracy on imputed data ( X2|1, X2|1) is comparable to that on ( X2, X2) whilst thesame for view 1 experiences a drop in performance.",
  "Conclusions": "A novel multi-view VAE approach has been proposed that natively strengthens the correlation betweenlatent spaces via a joint prior. This is the first time that a connection between multi-view VAEsis made through a joint prior rather than a joint posterior, as has previously been implemented inthe literature. Theoretical guarantees and parameterizations are presented that allow for end-to-endlearning. By simultaneously preventing posterior collapse, JPVAE returns superior models anddemonstrates a promising ability to impute missing data suitable for downstream tasks. Javier E. Flores, Daniel M. Claborne, Zachary D. Weller, Bobbie-Jo M. Webb-Robertson,Katrina M. Waters, and Lisa M. Bramer. Missing data in multi-omics integration: Recentadvances through artificial intelligence. Frontiers in Artificial Intelligence, 6, 2023. ISSN 2624-8212. doi: 10.3389/frai.2023.1098308. URL Dongdong Lin, Jigang Zhang, Jingyao Li, Chao Xu, Hong-Wen deng, and Yu-Ping Wang. Anintegrative imputation method based on multi-omics datasets. BMC Bioinformatics, 17, 062016. doi: 10.1186/s12859-016-1122-6.",
  "Thomas Sutter, Imant Daunhawer, and Julia Vogt. Multimodal generative learning utilizingjensen-shannon-divergence. Advances in neural information processing systems, 33:61006110,2020": "Yuge Shi, N. Siddharth, Brooks Paige, and Philip H. S. Torr. Variational mixture-of-expertsautoencoders for multi-modal deep generative models. In Proceedings of the 33rd InternationalConference on Neural Information Processing Systems, 2019. Mike Wu and Noah Goodman. Multimodal generative models for scalable weakly-supervisedlearning. In Proceedings of the 32nd International Conference on Neural Information ProcessingSystems, page 55805590, 2018.",
  "Weiran Wang, Xinchen Yan, Honglak Lee, and Karen Livescu. Deep variational canonicalcorrelation analysis. arXiv preprint arXiv:1610.03454, 2016": "Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and Stphane Deny. Barlow twins: Self-supervised learning via redundancy reduction. In 38th International Conference on MachineLearning, ICML 2021, pages 1231012320. ML Research Press, 2021. Yixin Wang, David Blei, and John P Cunningham. Posterior collapse and latent variable non-identifiability. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan,editors, Advances in Neural Information Processing Systems, volume 34, pages 54435455,2021.",
  "S. Kullback and R. A. Leibler. On Information and Sufficiency. The Annals of MathematicalStatistics, 22(1):79 86, 1951. doi: 10.1214/aoms/1177729694": "Hao Fu, Chunyuan Li, Xiaodong Liu, Jianfeng Gao, Asli Celikyilmaz, and Lawrence Carin.Cyclical annealing schedule: A simple approach to mitigating KL vanishing. In Proceedingsof the 2019 Conference of the North American Chapter of the Association for ComputationalLinguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 240250. Association for Computational Linguistics, 2019. doi: 10.18653/v1/N19-1021. URL Samuel R. Bowman, Luke Vilnis, Oriol Vinyals, Andrew M. Dai, Rafal Jzefowicz, and SamyBengio. Generating sentences from a continuous space. In Conference on ComputationalNatural Language Learning, 2015.",
  "The Appendix contains additional results and figures to supplement the main body of text": "Theoretical results used within the manuscript are presented in Section A.1. This is followed bya simplification of the KL term when orthogonality constraints are assumed which is outlined inSection A.2. The model architecture and training details implemented in the numerical experimentsare detailed in Sections A.4 and A.3 respectively. Lastly, additional results from the conductednumerical experiments can be found in Section A.5. All results throughout this manuscript are givento 4 significant figures, with standard deviations reported to 2 significant figures.",
  "A.3Model architecture": "All encoders and decoders for the JPVAE, as well as the neural network used for classification, consistof two dense layers with 512 units each. The latent distribution layers consists of two 20 unit denselayers which each parameterize the mean and the log of the variances of 20 normal random variables.The decoders output layer parameterizes the distribution of a Bernouilli random variable for eachpixel. The classifier output layer parameterizes the categorical distribution with 10 outcomes .Allactivation functions were ReLu.",
  "A.4Training details": "The JPVAE used an Adam optimiser with learning rate 0.001, trained for 30 epochs with abatch size of 32 and used binary cross entropy as the reconstruction error. The cyclical KL annealingschedule as introduced by is implemented, with M = 30. The classifier is trained for 50 epochs with a batch size of 32, step size of 0.01, except for on originaldata where it is trained for 15 epochs to prevent overfitting. Cross entropy loss is used as the lossfunction.",
  "Additional results are presented in this section": "illustrates examples of reconstructing view 2 from view 1, with and without learningcorrelation natively, for a model trained with a different random seed to that displayed in .Figures 6 and 7 illustrate examples of reconstructing view 1 from view 2, again with and withoutlearning correlation natively, for models trained with different random seeds. As in , theimputation of the missing view obtained when correlation of the latent spaces is learnt natively is ofhigher quality than when no correlation is learnt. To quantitively evaluate this superior performance,a simple multi-layer perceptron classifier is trained the concatenation of the reconstructed views. It isthen tested on a combination of the reconstructed and imputed views. Explicitly, for datasets wheree.g. view 2 is imputed from view 1, the classifier is trained on (the training split of) [ X1; X2] andtested on (the testing split of) [ X1; X2|1]. Results of this classification task for various test datasetsare displayed in . Results for the classification task described in the main body of the text,and presented in , are illustrated in with standard deviation incorporated via errorbars."
}