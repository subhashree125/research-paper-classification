{
  "Abstract": "The wider application of end-to-end learning methods to embodied decision-makingdomains remains bottlenecked by their reliance on a superabundance of trainingdata representative of the target domain. Meta-reinforcement learning (meta-RL)approaches abandon the aim of zero-shot generalizationthe goal of standardreinforcement learning (RL)in favor of few-shot adaptation, and thus holdpromise for bridging larger generalization gaps. While learning this meta-leveladaptive behavior still requires substantial data, efficient environment simulatorsapproaching real-world complexity are growing in prevalence. Even so, hand-designing sufficiently diverse and numerous simulated training tasks for thesecomplex domains is prohibitively labor-intensive. Domain randomization (DR) andprocedural generation (PG), offered as solutions to this problem, require simulatorsto possess carefully-defined parameters which directly translate to meaningful taskdiversitya similarly prohibitive assumption. In this work, we present DIVA, anevolutionary approach for generating diverse training tasks in such complex, open-ended simulators. Like unsupervised environment design (UED) methods, DIVAcan be applied to arbitrary parameterizations, but can additionally incorporaterealistically-available domain knowledgethus inheriting the flexibility and gener-ality of UED, and the supervised structure embedded in well-designed simulatorsexploited by DR and PG. Our empirical results showcase DIVAs unique ability toovercome complex parameterizations and successfully train adaptive agent behav-ior, far outperforming competitive baselines from prior literature. These findingshighlight the potential of such semi-supervised environment design (SSED) ap-proaches, of which DIVA is the first humble constituent, to enable training inrealistic simulated domains, and produce more robust and capable adaptive agents.Our code is available at",
  "Introduction": "Despite the broadening application of reinforcement learning (RL) methods to real-world problems, generalization to new scenariosones not explicitly supported by the training setremains afundamental challenge . Meta-reinforcement learning (meta-RL), an extension of the RL frame-work, is formulated specifically for training adaptive agents, and is thus well-suited for overcomingthese generalization gaps . One recent work has demonstrated that meta-RL agents can be trainedat scale to achieve adaptation capabilities on par with human subjects . However, learning thishuman-like adaptive behavior naturally requires a large amount of data representative of the down-stream (or target) distribution. For task distributions approaching real-world complexitypreciselythe ones of interestdesigning each scenario by hand is prohibitively expensive.",
  "QD updates": ": Highly structured environment simulators assume access to parameterizations ES()for which random seeds i directly produce meaningfully diverse features (e.g. RACING trackswith challenging turns). Open-ended environments with flexible, unstructured parameterizationsEU()though enabling more complex emergent featureslack direct control over high-levelfeatures of interest. We introduce DIVA, an approach that effectively creates a more workableparameterization EQD() by evolving levels beyond the minimally diverse population from EU().By training on these discovered levels, DIVA enables superior performance on downstream tasks. Prior works have explored the use of domain randomization (DR) and procedural generation (PG)techniques to produce diverse training data for learning agents . Despite eliminating the need forhand-designing each task individually, human labor is still required to carefully design an environmentgenerator that can produce diverse, high-quality tasks. As environments become more complex andopen-ended, the ability to hand-design such a robust generator becomes increasingly infeasible.Some methods, like PLR , attempt to ameliorate this limitation by learning a curriculum overthe generated levels, but these works still operate under the assumption that the generator producesmeaningfully diverse levels with a high probability. Unsupervised environment design (UED) are a broad class of appproaches which use performance-based metrics to adaptively form a curriculum of training levels. ACCEL , a state-of-the-artUED method, uses an evolutionary process to discover more interesting regions of the simulatorsparameter space (i.e. appropriately challenging tasks) than can be found by random sampling. WhileUED approaches are designed to be generally applicable and require little domain knowledge, theyimplicitly require a very constrained environment generatorone in which all axes of difficultycorrespond to meaningful learning potential for the downstream distribution. Moreover, when facedwith complex open-ended environments with arbitrary parameterizations, even ACCEL is not able toefficiently explore the solution space, as it is still bottlenecked by the speed of agent evaluations. In this work, we introduce DIVA, an approach for generating diverse training tasks in open-endedsimulators to train adaptive agents. By using quality diversity (QD) optimization to efficiently explorethe solution space, DIVA bypasses the problem of needing to evaluate agents on all generated levels.QD also enables fine-grained control over the axes of diversity to be captured in the training tasks,allowing the flexible integration of task-related prior knowledge from both domain experts andlearning approaches. We demonstrate that DIVA, with limited supervision in the form of featuresamples from the target distribution, significantly outperforms state of the art UED approachesdespite the UED approaches being provided with significantly more interactions. We further showthat UED techniques can be integrated into DIVA. Preliminary results with this combination (whichwe call DIVA+) are promising, and suggest an exciting avenue for future work.",
  "Density": ": DIVA archive updates on ALCHEMY. The first stage (a) begins with bounds thatencapsulate initial solutions, and the target region. As the first stage progresses (b), and QD discoversmore of the solution space, the sampling region for the emitters gradually shrinks towards the targetregion. The second stage begins by redefining the archive bounds to be the target region and includingsome extra feature dimensions (c). QD fills out just the target region now (d), using sample weightsfrom the target-derived prior (e), the same distribution used to sample levels during meta-training. Each Mi T is a Markov decision process (MDP) defined by a tuple S, A, P, R, , T, whereS is the set of states, A is the set of actions, P(st+1|st, at) is the transition distribution betweenstates given the current state and action, R(st, at) is the reward function, is the discountfactor, and T is the horizon. Meta-training involves sampling tasks Mi T , collecting trajectoriesD = { h}Hh=0where H is the number of episodes in each trial pertaining to the Miandoptimizing policy parameters to maximize the expected discounted returns across all episodes. VariBAD is a context variable-based meta-RL approach which belongs to the wider class of RNN-based methods . While prior methods also use context variables to assist in task adap-tation, VariBAD uniquely learns within a belief-augmented MDP (BAMDP) S, A, Z, P, R, , Twhere the context variables z Z encodes the agents uncertainty about the task, promoting Bayesianexploration. VariBAD utilizes an RNN-based variational autoencoder (VAE) to model a posteriorbelief over possible tasks given the full agent trajectory, permitting efficient updates to prior beliefs. Quality diversity.For a given problem, quality diversity (QD) optimization framework aims togenerate a set of diverse, high-quality solutions. Formally, a problem instance of QD specifiesan objective function J : Rn R and k features fi : Rn R. Let S = f(Rn) be the featurespace formed by the range of f, where f : Rn Rk is the joint feature vector. For each s S,the QD objective is to find a solution Rn where f() = s and J() is maximized. SinceRk is continuous, an algorithm solving the QD problem definition above would require unboundedmemory to store all solutions. QD algorithms in the MAP-Elites family therefore discretizeS via a tessellation method, where G is a tessellation of the continuous feature space S into NGcells. In employing a MAP-Elites algorithm, we relax the QD objective to find a set of solutionsi, i {1, . . . , NG}, such that each i occupies one unique cell in G. We call the occupants i of allM cells, each with its own position f(i) and objective value J(i), the archive of solutions.",
  "Problem setting": "One assumption underlying UED methods is that random parametersor parameter perturbationsfor ACCEL produce meaningfully different levels to justify the expense of computing objectives oneach newly generated level. However, when the genotype is not well-behavedwhen meaningful di-versity is rarely generated through random sampling or mutationsthese algorithms waste significanttime evaluating redundant levels. In our work, we discard the assumption of well-behaved genotypesin favor of making fewer, more realistic assumptions about complex environment generators. Thereare several assumptions we make about the simulated environments DIVA has access to. Genotypes.We assume access to an unstructured environment parameterization function EU(),where each is a genotype (corresponding to the QD solutions i) describing parameters to be fedinto the environment generator. QD algorithms can support both continuous and discrete genotypespaces, and in this work we evaluate on domains with both kinds. Crucially, we make no assumptionof the quality of the training tasks produced by this random generator. We only assume that (1) thereis some nonzero (and for practical purposes, nontrivial) probability that this generator will producea valid level for trainingone in which success is possible and positive rewards are in reach; and(2) that it is computationally feasible to discover meaningful feature diversity through an intelligentsearch over the parameter spacean assumption implicit in all QD applications. Features.We assume access to a pre-defined set of features, S = f(Rn), that capture axes ofdiversity which accurately characterize the diversity to be expected within the downstream taskdistribution. It is also possible to learn or select good environment features from a sample of tasksfrom the downstream distribution, which we discuss in . For the sake of simplicity, we usea grid archive as our tessellation G, where the k dimensions of the discrete archive correspond tothe defined features. The number of bins for each feature is a hyperparameter, and can be learnedor adapted over the course of training. We generally find it to be helpful to use moderately highresolutions to ease the search, since smaller leaps in feature-level diversity are required to uncovernew cells. By default, we use 100 sample feature values across all domains, but demonstrate inablation studies that that significantly fewer may be used (see Appendix C).",
  "DIVA": "DIVA assumes access to a small set of feature samples representative of the target domain. It doesnot, however, require access to the underlying levels themselves. This is a key distinction, as theformer is a significantly weaker assumption. Consider the problem of training in-home assistiverobots in simulation with the objective of adapting to real-world houses. It is more likely we haveaccess to publicly available data describing typical housesdimensions, stylistic features, etc.thanwe have access to corresponding simulator parameters which produce those exact feature values. Feature density estimation.DIVA begins by constructing a QD archive with appropriate boundsand resolution. Given a set of specified features {fi}k and a handful of downstream feature samples,we first infer each features underlying distribution. These can be approximated with kernel densityestimation (KDE), or we can work with certain families of parameterized distributions. For ourexperiments, we assume each feature is either (independently) normally or uniformly distributed. Weuse a statistical test2 to evaluate the fit of each distribution family, and select the best-fitting. Settingthe resolution for discrete feature dimensions is straightforward, and depends only on the range. Forcontinuous features, the resolution should enable enough signal for discovering new cells, whileavoiding practical issues that arise with too many cells3. See for domain-specific details. Two-stage QD updates.Once the feature-specific target distributions are determined, we can usethese to set bounds for each archive dimension. A nave approach would be to set the archive rangesfor each feature based on the confidence bounds of the target distribution. However, random samplesfrom EU may not produce feature values that fall within the target range. We found this to be a majorissue in the ALCHEMY domain (see ), and for some features in RACING. We solve thisproblem by setting the initial archive bounds to include both randomly generated samples from EU, aswell as the full target region. As the updates progress, we gradually update the sample maskwhichis used to inform the sampling of new solutionstowards the target region. We observe empiricallythat updating and applying this mask provides an enormous speed-up in guiding solutions towards thetarget region (see ). After this first stage, solutions are inserted into a new archive definedby the proper target bounds. See Appendix A for more specifics on these two QD update stages.",
  "Overview.DIVA consists of three stages. Stage 1": "(S1) begins by initializing the archive with bounds thatinclude both the downstream feature samples (the targetregion), as well as the initial population generated fromEU(). S1 then proceeds with alternating QD updates,to discover new solutions, and sample mask updates,to guide the population towards the target region. InStage 2 (S2), the archive is reinitialized with existingsolutions, but is now bounded by the target region. QDupdates continue to further diversify the population,now targeting the downstream feature values specifi-cally. The last stage is standard meta-training, wheretraining task parameters are now drawn from PG(), adistribution over the feature space approximated usingthe downstream feature samples, discretized over thearchive cells. See Appendix A for detailed pseudocode. 2We use a KolmogorovSmirnov test for features with continuous values and Chi-squared for discrete.3Memory is one concern; another is that optimizing objectives across all cells is slower with more cells.",
  "Empirical results": "Baselines.We implement the following baselines to evaluate their relative performance to DIVA.ODS is the oracle\" agent trained over the downstream environment distribution ES(), used forevaluation. With this baseline, we are benchmarking the upper bound in performance from theperspective of a learning algorithm that has access to the underlying data distribution.4 DR is themeta-learner trained over a task distribution defined by performing domain randomization over thespace of valid genotypes, , under the training parameterization, EU(). Robust PLR (PLR) is the improved and theoretically grounded version of PLR , where agents performance-basedPLR objectives are evaluated on each level before using them for training. ACCEL is the same asPLR but instead of randomly sampling over the genotype space to generate levels for evaluation,levels are mutated from existing solutions. All baselines use VariBAD as their base meta-learner. Experimental setup.The oracle agent (ODS) is first trained over the each environments down-stream distribution to tune VariBADs hyperparameters. These environment-specific VariBAD settingsare then fixed while hyperparameters for DIVA and the other baselines are tuned. For fairness ofcomparisonsince DIVA is allowed NQD QD update steps to fill its archive before meta-trainingwe allow each UED approach (PLR and ACCEL) to use significantly more environment steps foragent evaluations (details discussed below per environment). All empirical results were run with 5seeds unless otherwise specified, and error bars indicate a 95% confidence region for the metric inquestion. The QD archive parameters were set per environment, and for ALCHEMY and RACING,relied on some hand-tuning to find the right combinations of features and objectives. We leave it tofuture work to perform a deeper analysis on what constitutes good archive design, and how to betterautomate this process.",
  ": Left: A GRIDNAV agent attempt-ing to locate the goal across two episodicrollouts. Right: The marginal probabilityof sampled goals inhabiting each y for dif-ferent complexities k of Ek()": "Our first evaluation domain is a modified version ofGRIDNAV (), originally introduced to motivateand benchmark VariBAD . The agent spawns atthe center of the grid at the start of each episode, andreceives a slight negative reward (r = 0.1) each stepuntil it discovers (inhabits) the goal cell, at which pointit also receives a larger positive reward (r = 1.0). Parameterization.We parameterize the task space(i.e. the goal location) to reduce the likelihood of gen-erating meaningfully diverse goals. Specifically, eachEUk (or Ek) introduces k genes to the solution genotypewhich together define the final y location. Each genej can assume the values j {1, 0, 1}, and the finaly location is determined by summing these values, andperforming a floor division to map the bounds back tothe original range of the grid. As k increases, y values are increasingly biased towards 0, as shownon the right side of . For more details on the GRIDNAV domain, see Appendix B.1. QD updates.We define the archive features to be the x and y coordinates of the goal location.The objective is set to the current iteration, so that newer solutions are prioritized (additionaldetails in Appendix B.1). DIVA is provided NS2 = 8.0 104 (NS1 = 0) QD update iterationsfor filling the archive. To compensate, PLR and ACCEL are each provided with an additional9.6 106 environment steps for evaluating PLR scores, which amounts to three times as many totalinteractionssince all methods are provided NE = 4.8106 interactions for training. If each reset\"call counts as one environment step5, the UED baselines are effectively granted 2.4 more additionalstep data than what DIVA additionally receives through its QD updates (details in Appendix E.1). Results.From a, we see that increasing genotype complexity (i.e. larger k) reduces goaldiversity for DRwhich is expected given the parameterization defined for EU. We can also seethat DIVA, as a result of its QD updates, can effectively capture goal diversity, even as complexity 4Technically, reweighting this distribution (e.g. via PLR) may produce a stronger oracle, but for the purposesof this work, we assume the unaltered downstream distribution can be efficiently trained over, sans curriculum.5In general, rendering the environment (via reset\") is required to compute level features for DIVA.",
  "(d)": ": ALCHEMY environment and results. (a) A visual representation of ALCHEMYs structuredstone latent space. P1 and P2 represent potions acting on stones. Only P1 results in a latent statechange, because P2 would push the stone outside of the valid latent lattice. (b) Marginal featuredistributions for ES (the structured target distribution), DIVA, and EU (the unstructured distributionused directly for DR, and to initialize DIVAs archive). (c) Final episode return curves for DIVA andbaselines. (d) Number of unique genotypes used by each method over the course of meta-training.",
  "ALCHEMY": "ALCHEMY is an artificial chemistry environment with a combinatorially complex task distri-bution. Each task is defined by some latent chemistry, which influences the underlying dynamics,as well as agent observations. To successfully maximize returns over the course of a trial, the agentmust infer and exploit this latent chemistry. At the start of each episode, the agent is provided a newset of (1-12) potions and (1-3) stones, where each stone has a latent state defined by a specific vertexof a cube, i.e. ({0, 1}, {0, 1}, {0, 1}), and each potion has a latent effect, or specific manner in whichit transforms stone latent states (see a). The agent observes only salient artifacts of this latentinformation, and must use interactions to identify the ground-truth mechanics. At each step, the agentcan apply any remaining potion to any remaining stone. Each stones value is maximized the closerits latent state is to (1, 1, 1), and rewards are produced when stones are cast into the cauldron. To make training feasible on academic resources, we perform evaluations on the symbolic versionof ALCHEMY, as opposed to the full Unity-based version. Symbolic ALCHEMY contains the samemechanistic complexity, minus the visuomotor challenges which are irrelevant to this projects aims. Parameterization.ES() is the downstream distribution containing maximal stone diversity. Fortraining, implement EUk where k controls the level of difficulty in generating diverse stones. Specifi-cally, we introduce a larger set of coordinating genes j {0, 1} that together specify the initial stonelatent states, similar to the mechanism we used in GRIDNAV to limit goal diversity. Each stone latentcoordinate is specified with k genes, and only when all k genes are set to 1 is the latent coordinate isset to 1. When any of the genes are 0, the latent coordinate is 0. For our experiments we set k = 8,and henceforth use EU to signify EU8. QD updates.We use features LATENTSTATEDIVERSITY and MANHATTANTOOPTIMAL bothof which target stone latent state diversity from different angles. See Appendix B.2 for more specificson these features and other details surrounding ALCHEMYs archive construction. Like GRIDNAV,the objective is set to bias new solutions. DIVA is provided with NS1 = 8.0 104 and NS2= 3.0 104 QD update iterations. PLR and ACCEL are compensated such that they receive 3.5more additional step data than what DIVA receives via QD updates (see Appendix E.1 for details). Results.Our empirical results demonstrate that DIVA is able to generate latent stone states withdiversity representative of the target distribution. We see this both quantitatively in b, andqualitatively in . In c, we see this diversity translates to significantly better results",
  "Archive after S1": ": ALCHEMY level diversity. Early on in DIVAs QD updates (left), the levels in the archivedo not posses much latent stone diversityall are close to (1, 1, 1). As samples begin populating thetarget region in later QD updates (right), we see stone diversity is significantly increased. on ES over baselines. Despite generating roughly as many unique genotypes as DIVA (d),PLR and ACCEL are unable to generate training stone sets of significant phenotypical diversity toenable success on the downstream distribution.",
  "RACING": "Lastly, we evaluate DIVA on the RACING domain introduced by . In this environment, the agentcontrols a race car via simulated steering and gas pedal mechanisms, and is rewarded for efficientlycompleting the track, Mi T . We adapt this RL environment to the meta-RL setting by loweringthe resolution of the observation space significantly. By increasing the challenge of perception, evencompetent agents benefit from multiple episodes to better understand the underlying track. For all ofour experiments, we use H = 2 episodes per trial, and a flattened 15 15 pixel observation space. Setup.We use three different parameterizations in our experiments: (1) ES() is the downstreamdistribution we use for evaluating all methods, training ODS, and setting archive bounds for DIVA.Parameters are used to seed the random generation of control points which in turn parameterizea sequence of Bzier curves designed to smoothly transition between the control locations. Trackdiversity is further enforced by rejecting levels with control points that possess a standard deviationbelow a certain threshold. (2) EUk() is a reparameterization of ES() that makes track diversityharder to generate, with the difficulty proportional to the value of k N. For our experiments, we usek = 32 (which we will denote simply as EU()), which roughly means that meaningful diversity is32 less likely to randomly occur than when k = 1 (which is equivalent to ES()). This is achievedby defining a small region in the center, 32 (or k, in general) times smaller than the track boundaries,where all points outside the region are projected onto the unit square, and scaled to the track size. (3)EF1() uses as an RNG seed to select between a set of 20 hand-crafted levels official Formula-1tracks , and is used to benchmark DIVAs zero-shot generalization to a new target distribution. QD updates.We define features TOTALANGLECHANGES (TAC) and CENTEROFMASSX (CX) forthe archive dimensions. Levels from EU lack curvature (see ) so TAC, which is defined as thesum of angle changes between track segments, is useful for directly targeting this desired curvature.",
  "(c)": ": RACING features and main results. Left: Marginal feature distributions for ES (targetdistribution), EF1 (human-designed F1 tracks), DIVA, and EU (the unstructured distribution used forDR, the original levels that DIVA evolves)cropped for readability. Center: Final episode returncurves for DIVA and baselines on ES. Right: Track completion rates by method, evaluated on ES.",
  "source (took screenshots from here)": ": RACING level diversity. We see that random EU levels, used by DR, and which formthe initial population of DIVA, are unable to produce qualitatively diverse tracks (left). After thetwo-stage QD-updates, DIVA is able to produce tracks of high qualitative diversity (right). CX, or the average location of the segments, targets diversity in the location of these high-density(high-curvature) regions. We compute an alignment objective over features CENTEROFMASSY andVARIANCEY to further target downstream diversity. See Appendix B.3 for more details relevant tothe archive construction process for RACING. DIVA is provided with 2.5 105 initial QD updateson RACING. PLR and ACCEL are compensated with 4.0 more additional step data than whatDIVA receives through QD updates (see Appendix E.1 for more details). Main results.Results are shown in . DIVA outperforms all baselines, including the UEDapproaches, which have access to three times as many environment interactions. From , wesee that final DIVA levels contain significantly more diversity than randomization over EU.F1 results",
  ": Sample F1 levels (top), andtrack completion rates by methods tar-geting ES, evaluated on EF1 (bottom)": "Transfer to F1 tracks.Next, we evaluate the abilityof these trained policies to zero-shot transfer to human-designed F1 levels , EF1. Though qualitative differ-ences are apparent (see ), from a we canadditionally see how these levels differ quantitatively. Eventhough DIVA uses feature samples from ES to define itsarchive, we see from the results in that DIVA isnot only able to complete many of these tracks, but is alsoable to significantly outperform ODS. This result may seemunlikely, given that DIVA bases its axes of diversity onES. One possible explanation is that while DIVA success-fully matches its TOTALANGLECHANGES distribution toES (see ), because it is less likely for all 12 controlpoints to be mutated to the diversity-enabling region thanjust a few control points with sharp angles, DIVA opts\" forthe latter, and thus produces fewer, sharper angles, which isevidently useful for transferring to (these) human-designedtracks. This hypothesis matches what we see qualitativelyfrom the DIVA-produced levels in . Combining DIVA and UED.While PLR and ACCEL struggle on our evaluation domains,they still have utility of their own, which we hypothesize may be compatible with DIVAs. As apreliminary experiment to evaluate the potential of such a combination, we introduce DIVA+, whichstill uses DIVA to generate diverse training samples via QD, but additionally uses PLR to define",
  ": DIVA+ results compared toDIVA, for (1) misspecified, and (2) well-specified archives, evaluated on ES": "a new distribution over these levels based on approxi-mate learning potential. Instead of randomly samplinglevels from EU, the PLR evaluation mechanism sam-ples levels from the DIVA-induced distribution overthe archive. We perform experiments on two differ-ent archives generated by DIVA: (1) an archive thatis slightly misspecified (see Appendix B.3 for details),and (2) the archive used in our main results. From Fig-ure 10, we see that while performance does not sig-nificantly improve for (2), the combination of DIVAand PLR is able to significantly improve performanceon (1), and even statistically match the original DIVAresults. These results highlight the potential of suchhybrid (QD+UED) semi-supervised environment design(SSED) approaches, a promising area for future work.",
  "Related work": "Meta-reinforcement learning.Meta-reinforcement learning methods range from gradient-basedapproaches (e.g. MAML) , RNN context-based approaches (e.g. RL2), and the slewof emerging works utilizing transformers . We use VariBAD , a state-of-the-artcontext variable-based approach that extends RL2 by using variational inference to incorporate taskuncertainty into its beliefs. HyperX , an extension that uses reward-bonuses, was not found toimprove performance on our domains. In each of these works, the training distribution is given; noneaddress the problem of generating diverse training scenarios in absence of such a distribution. Procedural environment generation.Procedural (content) generation (PCG / PG) is a vastfield. Many RL and meta-RL domains themselves have PG baked-in (e.g. ProcGen , Meta-World,, Alchemy , and XLand ). Each of these works rely on human engineering to producelevels with meaningfully diverse features. A related stream of works apply scenario generation toroboticssome works essentially perform PCG , while others integrate more involved searchmechanics . One prior work defines a formal but generic parameterization forapplying PG to generate meta-RL tasks. It is yet to be shown, however, if such an approach can scaleto domains with vastly different dynamics, and greater complexity. Unsupervised environment design.UED approacheswhich use behavioral metrics to automati-cally define and adapt a curriculum of suitable tasks for agent trainingform the frontier of researchon open-endedness. The recent stream of open-ended agent/environment co-evolution works (e.g.) was kickstarted by the POET algorithm. The UED term itself originated inPAIRED , which uses the performance of an antagonist agent to define the curriculum for themain (protagonist) agent. PLR introduces an approach for weighting training levels based onlearning potential, using various proxy metrics to capture this high-level concept. introducesPLR, which only trains on levels that have been previously evaluated, and thus enabling certaintheoretical robustness guarantees. AdA uses PLR as a cornerstone of their approach for generatingdiverse training levels for adaptive agents in a complex, open-ended task space. ACCEL borrowsPLRs scoring procedure, but the best-performing solutions are instead mutated, so the buffer notonly collects and prioritizes levels of higher learning potential, but evolves them. We use ACCEL asour main baseline because it has demonstrated state-of-the art results on relevant domains, and likeDIVA, evolves a population of levels. The main algorithmic differences between ACCEL and DIVAare that ACCEL (1) performs additional evaluation rollouts to produce scores during training and (2)uses a 1-d buffer instead of DIVAs multi-dimensional archive. PLR serves as a secondary baselinein this work; its non-evolutionary nature makes it a useful comparison to DR. Scenario generation via QD.A number of recent works apply QD to simulated environments inorder to generate diverse scenarios, with distinct aims. Some works, like DSAGE , uses QDto develop diverse levels for the purpose of probing a pretrained agent for interesting behaviors.In another line of work applies QD to human-robot interaction (HRI), and ranges from generating diverse scenarios , to finding failure modes in shared autonomy systems and human-awareplanners . DIVAs application of QD inspired by these approaches, as they produce meaningfullydiverse environment scenarios, but no prior work exists which applies QD to define a task distributionfor agent training, much less adaptive agent training, or overcoming difficult parameterizations inopen-ended environments.",
  "Discussion": "The present work enables adaptive agent training on open-ended environment simulators by in-tegrating the unconstrained nature of unsupervised environment design (UED) approaches, withthe implicit supervision baked into procedural generation (PG) and domain randomization (DR)methods. Unlike PG and DR, which requires domain knowledge to be carefully incorporated intothe environment generation process, DIVA is able to flexibly incorporate domain knowledge, andcan discover new levels representative of the downstream distribution. And instead of relying onbehavioral metrics to infer a general, ungrounded form of learning potential, like UEDwhichbecomes increasingly unconstrained and therefore less useful a signal as environments become morecomplex and open-endedDIVA is able to directly incorporate downstream feature samples to targetspecific, meaningful axes of diversity. With only a handful of downstream feature samples to set theparameters of the QD archive, our experiments () demonstrate DIVAs ability to outperformcompetitive baselines compensated with three times as many environment steps during training. In its current form, the most obvious limitation of DIVA is that, in addition to assuming accessto downstream feature samples, the axes of diversity themselves must be specified. However, weimagine these axes of diversity could be learned automatically from a set of sample levels, or selectedfrom a larger set of candidate features; it may be possible to adapt existing QD works to automatethis process in related settings . The present work also lacks a more thorough analysis of whatconstitutes good archive design. While some amount of heuristic decision-making is unavoidablewhen applying learning algorithms to specific domains, a promising future direction would be tostudy how to approach DIVAs archive design from a more algorithmic perspective. DIVA currently performs QD iterations over the environment parameter space defined by EU(),where each component of the genotype represents some salient input parameter to the simulator.Prior works in other domains (e.g. ) have demonstrated QDs ability to explore the latent space ofgenerative models. One natural direction for future work would therefore be to apply DIVA to neuralenvironment generators (rather than algorithmic generators), where would instead correspond tothe latent input space of the generative model. If the latent space of these models is more convenientto work with than the raw environment parameterse.g. due to greater smoothness with respectto meaningful axes of diversitythis may help QD more efficeintly discover samples within thetarget region. Conversely, DIVAs ability to discover useful regions of the parameter space meansthese neural environment generators do not need to be well-behaved\", or match a specific targetdistribution. Since these generative models are also likely to be differentiable, DIVA can additionallyincorporate gradient-based QD works (e.g. DQD ) to accelerate its search. Preliminary results with DIVA+ demonstrate the additional potential of combining UED and DIVAapproaches. The F1 transfer results (i.e. DIVA outperforming ODS trained directly on ES) furthersuggest that agents benefit from flexible incorporation of downstream knowledge. In future work, wehope to study more principle integrations of UED and DIVA-like approaches, and to more generallyexplore this exciting new area of semi-supervised environment design (SSED). More broadly, now equipped with DIVA, researchers can develop more general-purpose, open-endedsimulators, without concerning themselves with constructing convenient, well-behaved parameteri-zations. Evaluations in this work required constructing our own contrived paramterizations, sincedomains are rarely released without carefully designed parameterizations. It is no longer necessary toaccomodate the assumption made my DR, PG, and UED approachesthat either randomization overthe parameter space should produce meaningful diversity, or that all forms of level difficulty ought tocorrespond to meaningful learning potential. So long as diverse tasks are possible to generate, even ifsparsely distributed within the paramter space, QD may be used to discover these regions, and exploitthem for agent training. Based on the promising empirical results presented in this work, we arehopeful that DIVA will enable future works to tackle even more complicated domains, and assistresearchers in designing more capable and behaviorally interesting adaptive agents.",
  "Reproducibility statement": "The source code, along with thorough documentation for reproducing each result in this paper, ispublicly available on Github6. Even without this code, researchers should be able to fully reproducethe algorithm from the details in the main body, the pseudocode provided in Appendix A, and trainingdetails (hyperparameters and hardware information) provided in Appendix E.",
  "Ethics statement": "Like all fundamental technologies, this work has the potential to be misapplied for malicious purposes.The authors do not believe, however, that the methods introduced in this work present a significant orunique risk for misuse or abuse. The authors intend for DIVA to be applied to use-cases that have thebest interests of humanity (including concern for the earth and other sentient creatures) at heart.",
  "Acknowledgements": "This work was partially supported by NSF CAREER (#2145077) and the DARPA EMHAT project.We thank Tjanaka et al., the developers of pyribs , whose library served as the basis for our QDimplementations. We thank Zintgraf et al., the authors of VariBAD , whose codebase served asthe basis for our meta-RL agent. We thank Jiang et al. and Parker-Holder et al., the authors of PLR and ACCEL , respectively, for their implementations which served as the basis for our UEDbaselines. We specifically thank Minqi Jiang for answering questions related to the PLR codebase inthe early stages of development, and Varun Bhatt for helpful discussion at various stages of this work.",
  "J. Beck, R. Vuorio, E. Z. Liu, Z. Xiong, L. Zintgraf, C. Finn, and S. Whiteson, A survey of meta-reinforcement learning, arXiv [cs.LG], Jan. 2023": "J. Bauer, K. Baumli, F. Behbahani, A. Bhoopchand, N. Bradley-Schmieg, M. Chang, N. Clay, A. Collister,V. Dasagi, L. Gonzalez, K. Gregor, E. Hughes, S. Kashem, M. Loks-Thompson, H. Openshaw, J. Parker-Holder, S. Pathak, N. Perez-Nieves, N. Rakicevic, T. Rocktschel, Y. Schroecker, S. Singh, J. Sygnowski,K. Tuyls, S. York, A. Zacherl, and L. M. Zhang, Human-timescale adaptation in an open-ended taskspace, in Proceedings of the 40th International Conference on Machine Learning (A. Krause, E. Brunskill,K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett, eds.), vol. 202 of Proceedings of Machine LearningResearch, pp. 18871935, PMLR, 2329 Jul 2023.",
  "N. Shaker, J. Togelius, and M. J. Nelson, Procedural Content Generation in Games. ComputationalSynthesis and Creative Systems, Springer, 2016": "M. Jiang, E. Grefenstette, and T. Rocktschel, Prioritized Level Replay, in Proceedings of the 38thInternational Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event (M. Meila andT. Zhang, eds.), vol. 139 of Proceedings of Machine Learning Research, pp. 49404950, PMLR, 2021. M. Dennis, N. Jaques, E. Vinitsky, A. Bayen, S. Russell, A. Critch, and S. Levine, Emergent complexityand zero-shot transfer via unsupervised environment design, Advances in neural information processingsystems, vol. 33, pp. 1304913061, 2020.",
  "Y. Duan, J. Schulman, X. Chen, P. L. Bartlett, I. Sutskever, and P. Abbeel, RL2: Fast ReinforcementLearning via Slow Reinforcement Learning, arXiv:1611.02779 [cs, stat], Nov. 2016": "J. Wang, Z. Kurth-Nelson, H. Soyer, J. Z. Leibo, D. Tirumala, R. Munos, C. Blundell, D. Kumaran,and M. M. Botvinick, Learning to reinforcement learn, in Proceedings of the 39th Annual Meeting ofthe Cognitive Science Society, CogSci 2017, London, UK, 16-29 July 2017 (G. Gunzelmann, A. Howes,T. Tenbrink, and E. J. Davelaar, eds.), cognitivesciencesociety.org, 2017.",
  "L. Zintgraf, K. Shiarli, V. Kurin, K. Hofmann, and S. Whiteson, Fast context adaptation via meta-learning,in International Conference on Machine Learning, pp. 76937702, PMLR, 2019": "K. Rakelly, A. Zhou, C. Finn, S. Levine, and D. Quillen, Efficient Off-policy Meta-reinforcement Learningvia Probabilistic Context Variables, in Proceedings of the 36th International Conference on MachineLearning, ICML 2019, 9-15 June 2019, Long Beach, California, USA (K. Chaudhuri and R. Salakhutdinov,eds.), vol. 97 of Proceedings of Machine Learning Research, pp. 53315340, PMLR, 2019. M. C. Fontaine and S. Nikolaidis, Differentiable Quality Diversity, in Advances in Neural InformationProcessing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS2021, December 6-14, 2021, virtual (M. Ranzato, A. Beygelzimer, Y. N. Dauphin, P. Liang, and J. W.Vaughan, eds.), pp. 1004010052, 2021.",
  "J. Mouret and J. Clune, Illuminating search spaces by mapping elites, CoRR, vol. abs/1504.04909, 2015": "M. Jiang, M. Dennis, J. Parker-Holder, J. N. Foerster, E. Grefenstette, and T. Rocktschel, Replay-guidedAdversarial Environment Design, in Advances in Neural Information Processing Systems 34: AnnualConference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual(M. Ranzato, A. Beygelzimer, Y. N. Dauphin, P. Liang, and J. W. Vaughan, eds.), pp. 18841897, 2021. J. X. Wang, M. King, N. P. M. Porcel, Z. Kurth-Nelson, T. Zhu, C. Deck, P. Choy, M. Cassin, M. Reynolds,H. F. Song, et al., Alchemy: A benchmark and analysis toolkit for meta-reinforcement learning agents, inThirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round2), 2021. C. Finn, P. Abbeel, and S. Levine, Model-agnostic Meta-learning for Fast Adaptation of Deep Networks,in Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW,Australia, 6-11 August 2017 (D. Precup and Y. W. Teh, eds.), vol. 70 of Proceedings of Machine LearningResearch, pp. 11261135, PMLR, 2017. L. C. Melo, Transformers are Meta-Reinforcement Learners, in Proceedings of the 39th InternationalConference on Machine Learning (K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari, G. Niu, and S. Sabato,eds.), vol. 162 of Proceedings of Machine Learning Research, pp. 1534015359, PMLR, 2022.",
  "J. Grigsby, L. Fan, and Y. Zhu, AMAGO: Scalable in-context reinforcement learning for adaptive agents,in The Twelfth International Conference on Learning Representations, 2024": "L. M. Zintgraf, L. Feng, C. Lu, M. Igl, K. Hartikainen, K. Hofmann, and S. Whiteson, Exploration inApproximate Hyper-state Space for Meta Reinforcement Learning, in Proceedings of the 38th InternationalConference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event (M. Meila and T. Zhang,eds.), vol. 139 of Proceedings of Machine Learning Research, pp. 1299113001, PMLR, 2021. K. Cobbe, C. Hesse, J. Hilton, and J. Schulman, Leveraging Procedural Generation to BenchmarkReinforcement Learning, in Proceedings of the 37th International Conference on Machine Learning,ICML 2020, 13-18 July 2020, Virtual Event, vol. 119 of Proceedings of Machine Learning Research,pp. 20482056, PMLR, 2020. T. Yu, D. Quillen, Z. He, R. Julian, K. Hausman, C. Finn, and S. Levine, Meta-World: A Benchmarkand Evaluation for Multi-task and Meta Reinforcement Learning, in 3rd Annual Conference on RobotLearning, CoRL 2019, Osaka, Japan, October 30 - November 1, 2019, Proceedings (L. P. Kaelbling,D. Kragic, and K. Sugiura, eds.), vol. 100 of Proceedings of Machine Learning Research, pp. 10941100,PMLR, 2019. J. Arnold and R. Alexander, Testing autonomous robot control software using procedural content gen-eration, in Lecture Notes in Computer Science, Lecture notes in computer science, pp. 3344, Berlin,Heidelberg: Springer Berlin Heidelberg, 2013. D. J. Fremont, T. Dreossi, S. Ghosh, X. Yue, A. L. Sangiovanni-Vincentelli, and S. A. Seshia, Scenic:a language for scenario specification and scene generation, in Proceedings of the 40th ACM SIGPLANConference on Programming Language Design and Implementation, PLDI 2019, (New York, NY, USA),p. 6378, Association for Computing Machinery, 2019. G. E. Mullins, P. G. Stankiewicz, R. C. Hawthorne, and S. K. Gupta, Adaptive generation of challengingscenarios for testing and evaluation of autonomous vehicles, J. Syst. Softw., vol. 137, pp. 197215, Mar.2018. Y. Abeysirigoonawardena, F. Shkurti, and G. Dudek, Generating adversarial driving scenarios in high-fidelity simulators, in 2019 International Conference on Robotics and Automation (ICRA), pp. 82718277,IEEE, May 2019. A. Gambi, M. Mueller, and G. Fraser, Automatically testing self-driving cars with search-based proceduralcontent generation, in Proceedings of the 28th ACM SIGSOFT International Symposium on SoftwareTesting and Analysis, (New York, NY, USA), ACM, July 2019. Y. Zhou, S. Booth, N. Figueroa, and J. Shah, Rocus: Robot controller understanding via sampling, inProceedings of the 5th Conference on Robot Learning (A. Faust, D. Hsu, and G. Neumann, eds.), vol. 164of Proceedings of Machine Learning Research, pp. 850860, PMLR, 0811 Nov 2022.",
  "D. M. Bossens and D. Tarapore, QED: Using quality-environment-diversity to evolve resilient robotswarms, IEEE Trans. Evol. Comput., vol. 25, pp. 346357, Apr. 2021": "A. Dharna, J. Togelius, and L. B. Soros, Co-generation of game levels and game-playing agents, in Pro-ceedings of the Sixteenth AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment,AIIDE20, AAAI Press, 2020. R. Wang, J. Lehman, J. Clune, and K. O. Stanley, Paired open-ended trailblazer (POET): Endlesslygenerating increasingly complex and diverse learning environments and their solutions, arXiv [cs.NE],Jan. 2019. R. Wang, J. Lehman, A. Rawal, J. Zhi, Y. Li, J. Clune, and K. Stanley, Enhanced POET: Open-endedreinforcement learning through unbounded invention of learning challenges and their solutions, inProceedings of the 37th International Conference on Machine Learning (H. D. III and A. Singh, eds.),vol. 119 of Proceedings of Machine Learning Research, pp. 99409951, PMLR, 1318 Jul 2020.",
  "V. Bhatt, B. Tjanaka, M. Fontaine, and S. Nikolaidis, Deep surrogate assisted generation of environments,Advances in Neural Information Processing Systems, vol. 35, pp. 3776237777, 2022": "D. Gravina, A. Khalifa, A. Liapis, J. Togelius, and G. N. Yannakakis, Procedural content generationthrough quality diversity, in 2019 IEEE Conference on Games (CoG), pp. 18, ieeexplore.ieee.org, Aug.2019. M. C. Fontaine and S. Nikolaidis, A Quality Diversity Approach to Automatically Generating Human-robot Interaction Scenarios in Shared Autonomy, in Robotics: Science and Systems XVII, Virtual Event,July 12-16, 2021 (D. A. Shell, M. Toussaint, and M. A. Hsieh, eds.), 2021. M. C. Fontaine, Y. Hsu, Y. Zhang, B. Tjanaka, and S. Nikolaidis, On the Importance of Environmentsin Human-robot Coordination, in Robotics: Science and Systems XVII, Virtual Event, July 12-16, 2021(D. A. Shell, M. Toussaint, and M. A. Hsieh, eds.), 2021.",
  ":return G": "Details on the two-stage QD updatesHere we provide more details on the process described inSection . Hyperparameters NS1 and NS2 are set to define the number of QD updates toperform in each stage (see Appendix D). In proportion to how many updates in S1 have elapsed, ifthe sample mask is enabled, the mask is moved at a linear pace from encapsulating the full S1 archive,to covering only the target region. We also set a hyperparameter, NSM (see Appendix D), which specifies the minimum number of solutions which must exist within the masks new bounds for it tobe updated. This is to ensure the mask never outpaces the search process. The mask was only foundto be necessary in the ALCHEMY environment. In S1 we sample solutions uniformly from withinthe mask. In S2, we begin sampling from the discretized target density distribution approximatedfrom the downstream feature samples. Two stages are used for RACING as well, since many initialsamples fall outside of the target region, but masking was not found to be necessary. The samplemask has a relatively straightforward implementation for MAP-Elites, which we use for ALCHEMYsdiscrete genotype (and GRIDNAV, where no mask is required). Since MAP-Elite updates entailperforming mutations on solutions directly sampled from the archive, the mask is implemented toonly consider solutions that fall within the mask bounds. However, since the CMA-ES-based emitterwe use for RACING operates by sampling from a parameterized distribution, instead of sampling fromthe archive directly, the mask would need to be applied to these parameters instead of the archive.",
  "NameAbbr.Description": "AREATOLENGTHRATIOATLRThe ratio of enclosed area to curve length.AVERAGECURVATUREACThe average curvature at midpoints of Bezir segments.CENTEROFMASSXCXThe center of mass x position over the curve.CENTEROFMASSYCYThe center of mass y position over the curve.CURVEDISTANCESVARIANCECDVThe variability in distances between successive points.CURVELENGTHCLThe total length of the Bezir curve.ENCLOSEDAREAEAThe area enclosed by the Bezir curve.MEDIANXMXThe median x position over the curve.MEDIANYMYThe median y position over the curve.SIGNIFICANTANGLECHANGESSACThe sum of significant angle changes across the curve.TOTALANGLECHANGESTACThe total change in angle across the curve.TOTALCURVATURETCThe total curvature over each segment and sum them up.VARIANCEXVXThe variance of the x positions over the curve.VARIANCEYVYThe variance of the y positions over the curve.",
  ": ALCHEMY measure covariances": "Archive hyperparameters for ALCHEMY were determined based on some knowledge about the domain,as well as the feature distributions (). We noticed a major deviation between EU and ES inthe feature LATENTSTATEDIVERSITY (LSD), and an even greater on in MANHATTANTOOPTIMAL(MTO). These two constitute the initial dimensions of the archive, and we found the sample maskupdates to be crucial to reach and fill the target region (see ). We use PARITYFIRSTSTONE(PFS) in the second stage to encourage more diversity once the target is reached; it is excluded fromthe first stage, which is focused on simply reaching the target. The archive for the first stage is ofshape , corresponding to LSD, MTO, and PFS. The second stage shape is .We found this archive to produce diverse enough solutions, evidenced by the number and spread in thetarget region, so we used this setting to train our DIVA agents. The only objective we found usefulfor ALCHEMY was a slight bias for newly generated solutions, which we also used for GRIDNAV.We hypothesize this prevents the archive from getting stuck with a suboptimal set of solutions, inabsense of other objectives.",
  "B.3RACING": "RACING features.See for all features defined on RACING. contains thefeature distributions for the structured and unstructured environment parameterizations on RACING,computed over 100 feature samples. shows the covariance between feature values forRACING, computed over 100 samples. Archive hyperparameters for RACING were determined through trial and error, by viewing the samplesproduced by the archives at the end of the QD updates, as well as the target coverage metrics. After afew iterations, it became clear that Total Angle Change TOTALANGLECHANGES (TAC) was the mostuseful feature, and so we tried pairing it with a number of others, prioritizing other features with lowabsolute covariance (see ). The best performing archive used by DIVA on RACING uses TAC and CX as its features, and used ameasure alignment objective over CY and VY. The measure alignment objective rewards solutions forhaving measure values over the specific measures that are similar to the target distribution. We also",
  "found that randomly sampling these objective values according to the target distribution providedsome additional support in covering the target region efficiently": "The slightly misspecified archive was chosen because its solutions generated some diversity, butnot as much as the aforementioned one. This archive uses TAC and ATLR as its features, and uses ameasure diversity objective over just CY. Instead of prioritizing alignment to the target distribution,the diversity objective samples a handful of solutions from the archive, and uses the current solutionsdeviation from these as its objective.",
  "DHyperparameter sensitivity analysis": "Varying QD mutation rate.We perform an ablation on the QD mutation rate, which is theprobability that a given gene will be mutated (for MAP-Elites). We perform this ablation onALCHEMY because the its search is the most challenging of the three environments we consider(it is the sole environment that required a longer S1 and the sample mask trick for accelerating toaccelerate the search). We see from that ALCHEMY results are not very sensitive to thesetting of the mutation rate.",
  "MaskingNo masking": ": ALCHEMY sample mask ablation curves. This specific result is the result of two seedsinstead of five, as we found the variance to be very low for this ablation (validated across otherparameter settings). Varying number of QD updates.We perform a similar ablation to test how robust DIVA is to thenumber of QD updates performed. We see from that ALCHEMY results suffer somewhatfrom fewer updates (e.g. for only 10k in each stage), still significantly outperform baselines ineach case. The trend is clear, however: more QD updates produces more solutions, which generallytranslates to better performance, even if slightly. Varying number of downstream samples.Next we test how robust DIVA is to the number ofdownstream samples used to compute the target distribution. In we see that, despite theerrors increasing with fewer samples, DIVA still significantly outperforms baselines with as few asfive samples.",
  "displays the hyperparameters used for DIVA across all domains": "A note on NTRS computationThe initial QD population (n0) is implemented such that the first setof QD updates simply generates n0 random levels from EU, before performing the actual mutations(for ME) or intelligent sampling (for ES). Thus, the formula we use for computing NTRS, the totalreset steps provided to DIVA (see ), which we use to compare the extra steps we providePLR and ACCEL (discussed in ), does not include n0; it is simply the product of the batchsize and the total number of QD iterations.",
  "# Target Solutions by Mutation Rate": ": Effect of varying QD mutation rate in ALCHEMY. Left: The returns for the finalepisode by mutation rate, after training on archives produced with each mutation rate. Right: Thefinal number of solutions in the archive after performing QD updates with each mutation rate. Thisresult was produced by running three different seeds for each mutation rate.",
  "# Target Solutions by # QD Updates": ": Effect of varying the number of QD updates in ALCHEMY. Left: The returns forthe final episode by number of QD updates in each stage (NS1 = NS2). Right: The final number ofsolutions in the archive after performing each number of QD updates. This result was produced byrunning three different seeds for each setting.",
  "E.4Computational details": "All results were produced on a handful of Titan X or Xp GPUs. Environments were parallelizedacross multiple CPU cores to accelerate training. While the experiment time varies by method andenvironment, most experiments take less than a day to run to completion. PLR and ACCEL take thelongest, as they required twice as many environment steps as the other methodson the two latterdomains, these methods take well over a day to run to completion.",
  "(log scale)": ": Effect of varying number of samples in ALCHEMY. Left: DIVA evaluation returnsfor the final episode by number of downstream samples, after training on archives produced with byusing each number of samples to produce archive bounds and prior. Center/Right: Errors for meanand variance parameters of the normal distribution based on number of samples used for computation;for MANHATTANTOOPTIMAL and LATENTSTATEDIVERSITY. For all plots, five seeds were usedfor each hyperparameter setting.",
  "BVAENumber of trajectories per VAE update25 / 10": "precollect_lenFrames to pre-collect before training5,000num_vae_updatesNumber of VAE update steps per iteration3pretrain_lenNumber of VAE pre-training updates0kl_weightWeight for KL term0.01action_emb_sizeAction embedding size for VAE8state_emb_sizeState embedding size for VAE16rew_emb_sizeReward embedding size for VAE16enc_gru_hidden_sizeGRU hidden size in encoder128latent_dimLatent dimension for VAE10 / 5 rew_loss_coeffReward loss coefficient1.0rew_dec_layersLayers for reward decoderrew_multiheadUse multihead for reward predictionFalserew_pred_typeReward prediction typebernoullikl_to_gaus_priorKL term to Gaussian priorFalserl_loss_thru_encBackprop RL loss through encoderFalsevae_loss_coefVAE loss coefficient1.0",
  ". Claims": "Question: Do the main claims made in the abstract and introduction accurately reflect thepapers contributions and scope?Answer: [Yes]Justification: The two main claims in the abstract are as follows: Our empirical resultsdemonstrate DIVAs unique ability to leverage ill-parameterized simulators to trainadaptive behavior in meta-RL agents, far outperforming competitive baselines.\" (1):This ability is indeed unique, as far as the authors are aware, and is supported by thediscussion of related works in . (2): The empirical results in this paper presented in support the claim that DIVA far outperforms other baselines for training meta-RLagents. Specific pieces of evidence include the GRIDNAV results in , the ALCHEMYresults in , and the results in RACING, contained in , 9, and 10. The finalsentence in the abstract makes a more general, weaker claim about the presented methodspotential: These findings highlight the potential of approaches like DIVA to enable trainingin complex open-ended domains, and to produce more robust and adaptable agents.\" Theauthors believe the same set of evidence supports this claim as well, but is ultimately up tothe readerand more importantly, future workto decide.Guidelines:",
  ". Limitations": "Question: Does the paper discuss the limitations of the work performed by the authors?Answer: [Yes]Justification: The authors discuss limitations of the present work in . This Dis-cussion section covers (1) key takeaways from the paper, (2) limitations, and (3) promisingavenues for future work. The authors combine these sections because the points are allinterrelated, and having them in the same place preserves cohesion and flow of the writing.Guidelines:",
  "The answer NA means that the paper has no limitation while the answer No means thatthe paper has limitations, but those are not discussed in the paper": "The authors are encouraged to create a separate \"Limitations\" section in their paper. The paper should point out any strong assumptions and how robust the results are toviolations of these assumptions (e.g., independence assumptions, noiseless settings,model well-specification, asymptotic approximations only holding locally). The authorsshould reflect on how these assumptions might be violated in practice and what theimplications would be. The authors should reflect on the scope of the claims made, e.g., if the approach wasonly tested on a few datasets or with a few runs. In general, empirical results oftendepend on implicit assumptions, which should be articulated. The authors should reflect on the factors that influence the performance of the approach.For example, a facial recognition algorithm may perform poorly when image resolutionis low or images are taken in low lighting. Or a speech-to-text system might not beused reliably to provide closed captions for online lectures because it fails to handletechnical jargon.",
  "If applicable, the authors should discuss possible limitations of their approach toaddress problems of privacy and fairness": "While the authors might fear that complete honesty about limitations might be used byreviewers as grounds for rejection, a worse outcome might be that reviewers discoverlimitations that arent acknowledged in the paper. The authors should use their bestjudgment and recognize that individual actions in favor of transparency play an impor-tant role in developing norms that preserve the integrity of the community. Reviewerswill be specifically instructed to not penalize honesty concerning limitations.",
  ". Experimental Result Reproducibility": "Question: Does the paper fully disclose all the information needed to reproduce the main ex-perimental results of the paper to the extent that it affects the main claims and/or conclusionsof the paper (regardless of whether the code and data are provided or not)? Answer: [Yes]Justification: In addition to the provided code (see ), which is self-containedand includes documentation for reproducing all empirical results in this paper, the authorsadditionally include all training details, including hyperparameter settings for all methods,in Appendix E.",
  "If the contribution is a dataset and/or model, the authors should describe the steps takento make their results reproducible or verifiable": "Depending on the contribution, reproducibility can be accomplished in various ways.For example, if the contribution is a novel architecture, describing the architecture fullymight suffice, or if the contribution is a specific model and empirical evaluation, it maybe necessary to either make it possible for others to replicate the model with the samedataset, or provide access to the model. In general. releasing code and data is oftenone good way to accomplish this, but reproducibility can also be provided via detailedinstructions for how to replicate the results, access to a hosted model (e.g., in the caseof a large language model), releasing of a model checkpoint, or other means that areappropriate to the research performed. While NeurIPS does not require releasing code, the conference does require all submis-sions to provide some reasonable avenue for reproducibility, which may depend on thenature of the contribution. For example",
  "(b) If the contribution is primarily a new model architecture, the paper should describethe architecture clearly and fully": "(c) If the contribution is a new model (e.g., a large language model), then there shouldeither be a way to access this model for reproducing the results or a way to reproducethe model (e.g., with an open-source dataset or instructions for how to constructthe dataset). (d) We recognize that reproducibility may be tricky in some cases, in which caseauthors are welcome to describe the particular way they provide for reproducibility.In the case of closed-source models, it may be that access to the model is limited insome way (e.g., to registered users), but it should be possible for other researchersto have some path to reproducing or verifying the results.",
  "Answer: [Yes]": "Justification: All results (excepting visualizations, and the trend curve in (a), forwhich they are not needed) are accompanied with error bars that capture the factors ofvariability, mostly due to random seeding (affecting many different random settings of thealgorithm, e.g. initial model weights, sampling, etc.). The significance of all error bars, andnumber of seeds used for each experiment, are detailed in . Other training detailsare available in Appendix E.",
  "Guidelines:": "The answer NA means that the paper does not use existing assets. The authors should cite the original paper that produced the code package or dataset. The authors should state which version of the asset is used and, if possible, include aURL. The name of the license (e.g., CC-BY 4.0) should be included for each asset. For scraped data from a particular source (e.g., website), the copyright and terms ofservice of that source should be provided. If assets are released, the license, copyright information, and terms of use in thepackage should be provided. For popular datasets, paperswithcode.com/datasetshas curated licenses for some datasets. Their licensing guide can help determine thelicense of a dataset.",
  "Answer: [NA]": "Justification: The authors feel that such safeguards are unnecessary for the present work,as the risk for misuse is estimated by the authors to be very lowit is unclear how thiswork and its artifacts can be directly used for malicious purposes. The authors are willing toadjust this position if members of the community feel otherwise.",
  "According to the NeurIPS Code of Ethics, workers involved in data collection, curation,or other labor should be paid at least the minimum wage in the country of the datacollector": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with HumanSubjectsQuestion: Does the paper describe potential risks incurred by study participants, whethersuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)approvals (or an equivalent approval/review based on the requirements of your country orinstitution) were obtained?Answer: [NA]Justification: The paper does not involve crowdsourcing nor research with human subjects.Guidelines:",
  "The answer NA means that the paper does not involve crowdsourcing nor research withhuman subjects": "Depending on the country in which research is conducted, IRB approval (or equivalent)may be required for any human subjects research. If you obtained IRB approval, youshould clearly state this in the paper. We recognize that the procedures for this may vary significantly between institutionsand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and theguidelines for their institution."
}