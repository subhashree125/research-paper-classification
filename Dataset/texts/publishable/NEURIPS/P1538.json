{
  "Abstract": "We develop a hybrid GNN-CNN architecture for the reconstruction of 3-dimensional continuous cosmological matter fields from discrete point clouds,provided by observed galaxy catalogs. Using the CAMELS hydrodynamical cos-mological simulations we demonstrate that the proposed architecture allows foran accurate reconstruction of both the dark matter and electron density given ob-served galaxies and their features. Our approach includes a learned grid assignmentscheme that improves over the traditional cloud-in-cell method. Our method canimprove cosmological analyses in situations where non-luminous (and thus un-observable) continuous fields need to be estimated from luminous (observable)discrete point cloud tracers.",
  "Introduction": "The spatial distribution of cosmological fields, such as densities of dark matter and ionized gas(referred to below as electrons), carries important information about the history of the evolution of theUniverse. For example, knowledge about the distribution of the electrons would increase the qualityof large-scale velocity reconstruction with kinetic Sunyaev-Zeldovich effect or would be helpfulin understanding the nature of dark matter through the resonant conversion . More generally,knowledge of the dark matter map allows for cross-correlation studies with various probes from theelectromagnetic wave spectrum, such as intensity mapping, as well as with gravitational wave sources.While being a rich source of information, continuous density fields are largely unobservable as wecan only directly see the luminous matter - position and properties of galaxies - at a discrete sparseset of points in space. Hence, developing a method to efficiently reconstruct the former from the latter",
  "arXiv:2411.02496v1 [astro-ph.CO] 4 Nov 2024": "is an important task. On large scales, the number density of galaxies and the density of dark matteror electrons are linearly related. On small scales, non-linear gravitational evolution and baryonicinteractions in the interstellar/intergalactic medium become relevant and are usually modelled withnumerical simulations. Simulations can then be used to learn or approximate different aspects ofunderlying physical processes. Significant effort has recently been focused in the direction of both simulations and ML-basedmodeling in cosmology. To name a few, used various AI models, such as generativeadversarial networks and denoising diffusion models, to increase the resolution of the cosmologicalsimulations - a task commonly known as superresolution; developed a field-level emulator ofcosmological large-scale structure; showed that the effects of baryonic physics can be emulatedby a simple transfer function, applied to a (gravity-only) simulated dark matter field. Specifically for our task, recently in it was shown that diffusion models are capable ofreconstructing dark matter fields from observed galaxy fields. However, these works did not treat thediscrete tracers as a point cloud and were only demonstrated in 2-dimensions. A possible approach toour reconstruction problem is to first assign the point objects to pixels and then treat the reconstructionwith a field-to-field machine learning model such as a U-Net, as in . However, the grid assignmentis inherently sub-optimal at finite resolution. The observed late-time galaxy distribution is discreteand non-uniform on small scales, forming structures such as filaments, clusters and voids, collectivelyknown as the cosmic web. Assigning density to a regular grid therefore results in very sparse regionswhere the density is low and the loss of information in regions where the density is higher than thegrid resolution. On the other hand, the distribution of dark matter or electron densities is inherentlycontinuous and can naturally be represented with a structured (regular) grid. The limitations of learning from unstructured discrete objects can be circumvented with Graph NeuralNetworks (GNN). They provide the natural representation of observed galaxy catalogs, which are inthe form of tables with spatial position, redshift, luminosity and other properties. These qualities ofGNNs were recently leveraged to build models capable of inferring cosmological parameters and darkmatter halo masses directly from galaxy catalogs , emulating the late-time dark-matterhalo distribution from N-body simulations , or learning baryonic properties of galaxies . Inthis work, we develop a deterministic hybrid GNN-CNN model that is trained to output the continuouscosmological density fields directly from the galaxy catalogs1. We show that the proposed setup issuccessful in capturing non-linear information in cosmological fields and their correlation functions,both for dark matter and baryons.",
  "Methodology": "X [Mpc/h] Y [Mpc/h] Z [Mpc/h] GalaxiesDark Matter Density : Visualization of the input and output. A graph constructed from the galaxy catalog (beforethe galaxy selection cuts) is depicted on the left (rlink = 3 Mpc/h). The underlying continuous darkmatter density field is shown on the right. The color scheme normalization is logarithmic. Our goal isto estimate the field on the right from the points on the left.",
  "Architecture": "Our proposed architecture consists of three steps and is motivated by the common encoder-decodersetup. The message-passing GNN block is used as an encoder. Then the output of the GNN block isassigned to a regular grid to form proto-density fields which then are passed to the CNN block. Sincethe target fields are represented by a regular spatial grid, convolutional neural nets are the naturalchoice for the decoder. For the GNN block, we use a modification of the graph message-passingscheme that also was used in for cosmological parameter inference from galaxy catalogs.The nodes of the graph are formed with scalar galaxy properties, the edge features are constructedfrom pairwise differences of positions, so that the network preserves translational symmetry. Wedefine connectivity of the graph by providing a minimal linking length, rlink, that is one of thehyperparameters of the model. The graph might also have global features that in our case representcosmological and astrophysical parameters. Given node features wi, edge features eij, and optionallyglobal features u, one message-passing step involves two operations:",
  "jNi eij, (u))": "We parametrize w and e with MLPs. \"\" represents the permutation invariant aggregationoperation that collects messages from all the edges adjacent to the node. The GNN block createsupdated latent node features that we assign to the regular grid at a specified resolution. For this, wedeveloped a grid aggregation layer that is depicted in Fig. (2) and works as follows. The input galaxypoint cloud and the grid are viewed as a bipartite graph. We assign connectivity given radius rlinkwith a modified torch_cluster.radius function to account for the periodic boundaries. Thenmessages from the neighbors are aggregated to the corresponding grid points, with their contributionsscaled according to a learned radial weighting kernel. This weighting kernel is derived by inputting thesquared distances between the galaxy point cloud and the grid locations into a MLP with three fullyconnected layers and two ReLU activation functions. Then the formed fields are passed through theCNN-decoder that we represent with a UNet . We employ skip-connection in the convolutionallayers of the UNet encoder. We also use circular-padded convolutions to account for the periodicstructure of the box.",
  "Dataset": "For training, we use snapshots and SUBFIND subhalo/galaxy catalogs of IllustrisTNG-LH suiteof CAMELS simulation set at redshift z = 0. Each simulation follows the evolutionof 2563 dark matter and 2563 gas particles in a box with a length of 25 Mpc/h. There are 1000simulations in total from this subset. They have variable cosmological parameters m and 8, aswell as astrophysical parameters ASN1,2, AGN1,2 that represent the effects of stellar winds and theinfluence of the active galactic nuclei. We use 27 simulations from the CV subset to evaluate theperformance of the trained models. These simulations have fixed cosmological parameters and differonly by initial random seed.",
  "Every subhalo is assumed to be a separate galaxy": "After applying the selection cuts, we have O(200) galaxies in a 253 (Mpc/h)3 volume. While not afully realistic galaxy data set, our selection is broadly compatible with the data from ongoing andfuture high-density galaxy surveys. The number density of galaxies after the cuts in our data set isapproximately 10 times higher than the one of DESI at z = 1, but somewhat below the expectednumber density of Rubin Observatory . We then generate the graph corresponding to the galaxy catalogue. The graph nodes are generatedfrom scalar features such as stellar mass, or velocity dispersion. We construct translationally invariantgraph edge features from pairwise differences of vector quantities, like positions. The complete set offeatures used and their normalization are listed in Appendix A. We found that it is sufficient to haveone message-passing layer with rlink = 2 Mpc/h. We experimented with larger values of rlink butdidnt find a significant improvement. The targets are the corresponding electron and dark matteroverdensities, e and m, that we put on a downsampled grid of 1283 pixels. We use the AdamW optimizer with the learning rate lr = 2 103 and weight decay wd = 2 102 to minimizeordinary l1 loss (which we found to perform better than l2):",
  "f f true .(1)": "Here, f stands for the output of the model. Out of 1000 simulations of LH suite, we use 850 first onesfor training and keep 150 last ones to track the validation loss. We train two versions of the model, onethat doesnt know about the true simulation parameters and the other that has access to them explicitlyin the form of global features of the graph. More precisely, let us define P(f, |g) - a probability ofthe true density fields f and parameters given the observed galaxy cloud g, represented as a graph.In the first case, we learn the median of the marginalized (over the parameters) probability densityfunctionP(f|g) =dP(f, |g).(2)",
  "Results": "As discussed, to isolate the effects of variable cosmological and astrophysical parameters from theability of the model to learn, we trained two different models with and without known parameters.As a baseline to compare the cross-correlation to, we took a stellar density field that we computedby weighting galaxies by their stellar mass and gridding them with the cloud-in-cell (CIC) procedure.This quantity is directly observable through stellar luminosity and correlates well with the true darkmatter distribution on large scales. If treated as a tracer of dark matter, it would assume that theamount of dark matter is proportional to the mass of the observed galaxy which is located at itsposition. Thats not the case on smaller length scales and we expect our machine learning model toperform better there. Indeed, as we see from Fig. (3) (and Fig. (4) in the Appendix B), the neural k [hMpc1] 0.6 0.7 0.8 0.9 1.0 r2(k) NN, cond NN * k [hMpc1] 0.2 0.4 0.6 0.8 1.0 r2(k) NN, cond NN * : Left: The cross-correlation coefficient Eq. 4 of the reconstructed conditional and marginal-ized dark matter density, NN,cond (blue) and NN (orange) correspondingly, and stellar mass field (green) with a true dark matter density field as a function of wave vector k. Right: The same butfor the electron density field. The shaded region indicates a 1- contour. network is able to reconstruct both dark matter and electron densities from the galaxy features at highfidelity. The model is noticeably better for dark-matter density, though. The reason for this is that thedistribution of galaxies itself is better correlated with the underlying dark matter density. The effectof known vs. unknown cosmological and astrophysical parameters manifests itself in marginallyimproving the mean cross-correlation coefficient for electron density reconstruction. This surprisinglack of improvement (compared to the unconditional case) can be attributed to the fact that the neuralnet is able to learn the cosmological and astrophysical parameters implicitly in the case where we donot provide them. Generally, our results are encouraging for practical applications. For example, forkSZ velocity reconstruction , the expected noise of the observable scales as N r2() where r is the reconstruction coefficient of the baryon density. Comparing to , this can beup to a factor of 2 improvement in the relevant k-range. A visualization of model input and output isshown in and 6 of Appendix C.",
  "Conclusions and Outlook": "Weve designed a hybrid GNN-CNN-based model for the reconstruction of cosmological fieldsdirectly from galaxy catalogs. The benefit of this approach is that there is no need for a pre-processingstep of gridding the catalog. The graph structure allows a more straightforward way to incorporatethe galaxy features directly as node attributes and positional and kinematic variables as edge features.The proposed approach nicely bridges the gap between the discreteness of observable tracers andthe continuous nature of a field-level analysis and opens new possibilities for various simulation- orforward-modeling-based inference schemes directly on the catalog level. There are several interesting future directions. First, its important to consider a more realistic scenario,where we also incorporate the corresponding uncertainties of the observed quantities. One way todo so is to add some noise to the node and edge features. An additional level of realism is to add asurvey mask and redshift evolution. It would be very interesting to forecast the performance of ourmethod for Rubin Observatory. The very high galaxy number density of this experiment could benefitour method, while its photometric redshift errors may decrease the performance. Another importantaspect is robustness. It would be useful to cross-check the performance on other hydrodynamicalsimulations with different subgrid models. Fortunately, for some applications (in particular squeezedlimit observables), baryonic uncertainty in the machine learning model of the small-scale field canbe handled by a bias parameter . One could also perform a probabilistic rather than deterministicreconstruction. The proposed architecture can be used as a conditional encoder for a denoisingdiffusion or a normalizing flow architecture. In that way, one would reconstruct the whole conditionalprobability density of the true field given the observed one. In two dimensions at field level this wasrecently done in . However, in many situations in cosmology, sampling over the conditionalprobability density is computationally prohibitive and a deterministic approach, as presented here, issufficient. AcknowledgmentsWe thank H. Ganjoo for discussions and collaboration on alternative architec-tures for reconstructing cosmological fields. We thank C. K. Jespersen for discussions on graph neuralnetworks. M.M. acknowledges the support by the U.S. Department of Energy, Office of Science,Office of High Energy Physics under Award Number DE-SC-0017647, and by the National ScienceFoundation (NSF) under Grant Number 2307109. M.C.J. is supported by the National Science andEngineering Research Council through a Discovery grant. J.K. acknowledges support from theNatural Sciences and Engineering Research Council of Canada (NSERC) through the Vanier CanadaGraduate Scholarship. Y.K. and M.M. are grateful for the hospitality of Perimeter Institute where apart of this work was done. Research at Perimeter Institute is supported in part by the Government ofCanada through the Department of Innovation, Science and Economic Development Canada and bythe Province of Ontario through the Ministry of Colleges and Universities. Yurii Kvasiuk and Moritz Mnchmeyer. Autodifferentiable likelihood pipeline for the cross-correlation of CMB and large-scale structure due to the kinetic Sunyaev-Zeldovich effect. Phys.Rev. D, 109(8):083515, 2024.",
  "Dalila Prvu, Junwu Huang, and Matthew C. Johnson. Patchy screening of the cmb from darkphotons. Journal of Cosmology and Astroparticle Physics, 2024(01):019, January 2024": "Doogesh Kodi Ramanah, Tom Charnock, Francisco Villaescusa-Navarro, and Benjamin D.Wandelt. Super-resolution emulator of cosmological simulations using deep physical models.Mon. Not. Roy. Astron. Soc., 495:4227, 2020. Yueying Ni, Yin Li, Patrick Lachance, Rupert A. C. Croft, Tiziana Di Matteo, Simeon Bird,and Yu Feng. AI-assisted superresolution cosmological simulations II. Halo substructures,velocities, and higher order statistics. Mon. Not. Roy. Astron. Soc., 507(1):10211033, 2021. Xiaowen Zhang, Patrick Lachance, Yueying Ni, Yin Li, Rupert A. C. Croft, Tiziana Di Matteo,Simeon Bird, and Yu Feng. AI-assisted super-resolution cosmological simulations III: timeevolution. Mon. Not. Roy. Astron. Soc., 528(1):281293, 2024.",
  "Sungwook E. Hong, Donghui Jeong, Ho Seong Hwang, and Juhan Kim. Revealing the localcosmic web from galaxies by deep learning. The Astrophysical Journal, 913(1):76, May 2021": "T Lucas Makinen, Tom Charnock, Pablo Lemos, Natalie Porqueres, Alan F Heavens, andBenjamin D Wandelt. The cosmic graph: Optimal information extraction from large-scalestructure using catalogues. The Open Journal of Astrophysics, 5(1), December 2022. Pablo Villanueva-Domingo, Francisco Villaescusa-Navarro, Daniel Angls-Alczar, Shy Genel,Federico Marinacci, David N. Spergel, Lars Hernquist, Mark Vogelsberger, Romeel Dave, andDesika Narayanan. Inferring halo masses with graph neural networks. The AstrophysicalJournal, 935(1).",
  "John F. Wu and Christian Kragh Jespersen. Learning the galaxy-environment connection withgraph neural networks, 2023": "Peter W. Battaglia, Jessica B. Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zam-baldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner,Caglar Gulcehre, Francis Song, Andrew Ballard, Justin Gilmer, George Dahl, Ashish Vaswani,Kelsey Allen, Charles Nash, Victoria Langston, Chris Dyer, Nicolas Heess, Daan Wierstra,Pushmeet Kohli, Matt Botvinick, Oriol Vinyals, Yujia Li, and Razvan Pascanu. Relationalinductive biases, deep learning, and graph networks, 2018.",
  "Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks forbiomedical image segmentation, 2015": "Francisco Villaescusa-Navarro, Shy Genel, Daniel Angls-Alczar, Lucia A. Perez, PabloVillanueva-Domingo, Digvijay Wadekar, Helen Shao, Faizan G. Mohammad, Sultan Hassan,Emily Moser, Erwin T. Lau, Luis Fernando Machado Poletti Valle, Andrina Nicola, Lean-der Thiele, Yongseok Jo, Oliver H. E. Philcox, Benjamin D. Oppenheimer, Megan Tillman,ChangHoon Hahn, Neerav Kaushal, Alice Pisani, Matthew Gebhardt, Ana Maria Delgado,Joyce Caliendo, Christina Kreisch, Kaze W. K. Wong, William R. Coulton, Michael Eickenberg,Gabriele Parimbelli, Yueying Ni, Ulrich P. Steinwandel, Valentina La Torre, Romeel Dave,Nicholas Battaglia, Daisuke Nagai, David N. Spergel, Lars Hernquist, Blakesley Burkhart,Desika Narayanan, Benjamin Wandelt, Rachel S. Somerville, Greg L. Bryan, Matteo Viel, YinLi, Vid Irsic, Katarina Kraljic, Federico Marinacci, and Mark Vogelsberger. The CAMELSProject: Public Data Release. Astrophys. J. Suppl., 265(2):54, April 2023. Yueying Ni, Shy Genel, Daniel Angls-Alczar, Francisco Villaescusa-Navarro, Yongseok Jo,Simeon Bird, Tiziana Di Matteo, Rupert Croft, Nianyi Chen, Natal S. M. de Santi, MatthewGebhardt, Helen Shao, Shivam Pandey, Lars Hernquist, and Romeel Dave. The CAMELSProject: Expanding the Galaxy Formation Model Space with New ASTRID and 28-parameterTNG and SIMBA Suites. Astrophys. J., 959(2):136, December 2023. Francisco Villaescusa-Navarro, Daniel Angls-Alczar, Shy Genel, David N. Spergel, Rachel S.Somerville, Romeel Dave, Annalisa Pillepich, Lars Hernquist, Dylan Nelson, Paul Torrey,Desika Narayanan, Yin Li, Oliver Philcox, Valentina La Torre, Ana Maria Delgado, Shirley Ho,Sultan Hassan, Blakesley Burkhart, Digvijay Wadekar, Nicholas Battaglia, Gabriella Contardo,and Greg L. Bryan. The CAMELS Project: Cosmology and Astrophysics with Machine-learningSimulations. Astrophys. J., 915(1):71, July 2021. Sirui Wu, Nicola R. Napolitano, Crescenzo Tortora, Rodrigo von Marttens, Luciano Casarini,Rui Li, and Weipeng Lin. Total and dark mass from observations of galaxy centers with machinelearning. Astronomy & Astrophysics, 686:A80, May 2024. Moritz Mnchmeyer, Mathew S. Madhavacheril, Simone Ferraro, Matthew C. Johnson, andKendrick M. Smith. Constraining local non-gaussianities with kinetic sunyaev-zeldovichtomography. Physical Review D, 100(8), October 2019. Boryana Hadzhiyska, Lars Hernquist, Daniel Eisenstein, Ana Maria Delgado, Sownak Bose,Rahul Kannan, Rdiger Pakmor, Volker Springel, Sergio Contreras, Monica Barrera, FulvioFerlito, Csar Hernndez-Aguayo, Simon D M White, and Carlos Frenk. The millenniumtngproject: refining the one-halo model of red and blue galaxies at different redshifts. MonthlyNotices of the Royal Astronomical Society, 524(2):25242538, July 2023.",
  "x - Galaxy position r, g, z - Magnitudes of r,g, and z luminosity bands r1/2 - Stellar half-mass radius M - Stellar mass v - Velocity dispersion": "Coordinates are normalized by the sidelength of the box and vary in the unit range. Magnitudesof the luminosity bands are normalized to have zero mean and unit variance. All other features -stellar mass, stellar half-mass radius, and velocity dispersion - are log-normalized, so that log10 ofthe corresponding quantity has zero mean and the variance of one.",
  "(5)": "Fig. (4) shows the SNR for the reconstructed dark matter (left) and electron densities (right) with boththe marginalized (blue) and conditioned on the cosmological and astrophysical parameters (orange)models. We can see that the effect of global parameters on the mean SNR is more pronounced for thedark matter reconstruction. The conditional model also has smaller variance, as can be seen from thesize of the corresponding 1 regions. k [hMpc1] SNR(k) NN, cond NN k [hMpc1] SNR(k) NN, cond NN"
}