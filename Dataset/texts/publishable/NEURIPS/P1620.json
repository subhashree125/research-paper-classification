{
  "Abstract": "Self-training often falls short under distribution shifts due to an increased discrep-ancy between prediction confidence and actual accuracy. This typically necessitatescomputationally demanding methods such as neighborhood or ensemble-based la-bel corrections. Drawing inspiration from insights on early learning regularization,we develop a principled method to improve self-training under distribution shiftsbased on temporal consistency. Specifically, we build an uncertainty-aware tem-poral ensemble with a simple relative thresholding. Then, this ensemble smoothsnoisy pseudo labels to promote selective temporal consistency. We show that ourtemporal ensemble is asymptotically correct and our label smoothing techniquecan reduce the optimality gap of self-training. Our extensive experiments vali-date that our approach consistently improves self-training performances by 8% to16% across diverse distribution shift scenarios without a computational overhead.Besides, our method exhibits attractive properties, such as improved calibrationperformance and robustness to different hyperparameter choices.",
  "Introduction": "In this work, we address the challenge of adapting pre-trained neural networks at test time underdistribution shifts, a problem known as test-time adaptation (TTA) or source-free domain adaptation(SFDA). Distribution shiftswhere a model trained on one distribution is then tested on a differentoneare ubiquitous in many practical scenarios due to demographic subpopulation shift andchanges in data collection environments . Despite the robust performance of neural networksunder independent and identically distributed (i.i.d.) settings, they often suffer from substantial perfor-mance degradation under such shifts . Recently, TTA and SFDA have proven their effectivenessin resolving these critical issues by effectively leveraging information about the distribution shiftscontained in unlabeled samples given at the test time. Self-training is the basis of many state-of-the-art methods in TTA and SFDA , utilizing pseudolabels generated from the models own predictions to train a model on unlabeled samples . Sincepseudo labels are regarded as true labels in self-training, the success of self-training methods highlydepends on how to filter incorrect pseudo labels to prevent self-confirmation bias . This issuehas been effectively handled via simple confidence-based thresholding in i.i.d. settings .However, the distribution shifts make it hard to filter incorrect pseudo labels due to high noise rateseven under high threshold . Thus, sophisticated methods filter incorrect pseudo labels based on aneighborhood structure of the data and consistency of multiple predictions under differentmodels or augmentations (cf. ), which are computationally intensive by nature.",
  "arXiv:2411.00586v1 [cs.LG] 1 Nov 2024": "previous methods. The temporal consistency regularizer, so called early learning regularization (ELR), was originally developed to address neural networks tendency to learn clean information firstand then gradually memorize noisy labels ; this setting is naturally connected to self-trainingscenarios when we regard the pseudo labels as random noisy labels. However, the impacts of ELR onself-training have not been fully understood. Also, since ELR does not consider unique characteristicsof distribution shifts, we aim to answer the following question: Is there any principled way to improvethe way of memorizing all past predictions tailored self-training under distribution shifts? In this work, we show that the answer is affirmative by proposing Anchored Confidence (AnCon) thatuses confident predictions to support a generalized notion of temporal consistency. Specifically, weconstruct a generalized temporal ensemble, which weighs predictions based on predictive uncertainty,and then use the ensemble as a smoothing vector in label smoothing . Then, through rigoroustheoretical analyses, we show that our simple heuristic for the generalized temporal ensemble isasymptotically correct and that the label smoothing formulation can reduce the optimality gap. Asa result, AnCon can correct wrong pseudo labels without expensive computations and can be easilyapplied to self-training methods by replacing one-hot pseudo labels with smooth pseudo labels, unlikeneighborhood-based or centroid methods. Through extensive experiments, we show that AnConimproves self-training under diverse distribution shift scenarios and posses many attractive properties. Our contribution can be summarized as follows: 1) We develop AnCon, which is the first algorithmthat attempts to improve self-training under distribution shifts by generalizing a notion of temporalconsistency with theoretical guarantees; 2) Without any additional forward passes or neighborhoodsearch, AnCon improves self-training performances 8% and 16% under domain shifts and imagecorruptions, respectively; 3) Remarkably, we also show that AnCon significantly improves calibrationperformance and is robust with respect to model selection methods and hyperparameter choices.",
  "Background": "Notation and setup For an input space X Rd and a label space Y = [K] := {1, 2, , K}, we de-fine X and Y to be random variables of input and output with probability densities pX and pY , respec-tively. We also define a neural network f(; ) : X K1 parameterized by a parameter Rpwhere K1 is the probability simplex with K elements. Our goal is to minimize the cross-entropyloss minRp l() := EXY [H(f(X; ), pY |X)] where H(f(x; ), pY |X=x) := k[K] p(Y =k|x) log fk(x; ). In the SFDA setting, we are given an initial parameter 0 that is trained on adifferent data generating distribution (X, Y ), e.g., 0 arg minRp EXY [H(f(X; ), pY |X)].We can think about this setting as either (1) (X, Y ) being pre-training data and 0 the foundationmodel with the task of fine-tuning the model on unlabeled X or (2) a transfer learning problem with(X, Y ) being the source domain data and X the target domain. Here, we assume only covariateshift without concept shift; that is, pX =D pX but pY |X =D pY |X. Even in this case, we note thatsuboptimality under the distribution shift, i.e., minRp l() l(0), can be large. Self-training In this work, we tackle the distribution shifts by using the self-training method that re-places the true label Y (x) by the pseudo label, i.e., minnew l(new; ) = EX[ log f Y (x;)(x; new)] where Y (x; ) := arg maxk[K] fk(x; ) is a pseudo label under . Specifically, the algorithmicframework of self-training is as follows: given 0, we iteratively find model parameter m+1 form = 0, 1, with m+1 arg minnew l(new; m). For later use, we also define a predictionconfidence c(x; ) := maxk[K] fk(x; ) and 0:m := (0, , m). Early learning regularization In the learning from noisy labels (LFN) scenario, identified an\"early-learning phenomenon\" where neural networks initially learn information contained in cleanlabels before gradually memorizing noisy labels, leading to a performance deterioration as trainingprogresses. To mitigate this issue, ELR penalizes predictions that deviate from earlier predictions bydefining a target network from the past predictions: fELR(x; 0:m) := mj=0(1 ) mjf(x; j).Then, adding an auxiliary loss of LELR(; 0:m) = EX[log(1 f(X; )T fELR(X; 0:m))] tol(; m) can prevent memorization of noisy labels while preserving correct patterns. Notably, this insight has recently been confirmed to be applicable in the SFDA setting by . Thisobservation is appealing because ELR can be efficiently implemented by reusing past predictionswithout additional forward passes or neighborhood searching, unlike dominant methods in SFDA . Nevertheless, given that ELR stems from a general property of the neural network trainingin the i.i.d. setting, herein we aim to step towards a more principled approach to encourage temporalconsistency tailored for distribution shift scenarios.",
  "Anchored confidence": "In this section, we introduce AnCon, which promotes the temporal consistency on selectively chosenpredictions via label smoothing. In .1, we first explain the idea of promoting selectivetemporal consistency based on confident predictions via label smoothing with a temporalensemble. Then, in .2, we explain how to effectively construct temporal ensemble forimproving self-training under distribution shifts. Finally, we theoretically analyze the efficacy ofAnCon by drawing connection between our method and knowledge distillation in .3. 3.1Selective temporal consistency via label smoothingIn this work, we utilize label smoothing to promote the selective temporal consistency insteadof using an auxiliary loss function like ELR. Specifically, given a generalized temporal ensemblef(x; 0:m, w0:m) with w0:m := (w0, , wm) which will be specified in .2, we constructa regularized pseudo label Y (X; 0:m, w0:m) by using f(x; 0:m, w0:m) as a smoothing vector forthe pseudo label Y (x; m). That is, we perform self-training by",
  "(1)where is a coefficient, E1() is the one-hot encoding, and f(x) := ( f1(x), , fK(x)) isthe K-dimensional output of the generalized temporal ensemble (cf. (2))": "Thus, AnCon can control the usage of potentially noisy information in Y (x; m) based on its consis-tency with f(x; 0:m, w0:m). Not only can this approach preserve the early learning phenomenonas in ELR (cf. ), but the label smoothing formulation also significantly stabilizes the self-training performance under different hyperparameter choices due to the fact that the optimal values ofhyperparameters are less problem dependent compared to its equivalent auxiliary regularization .Beyond removing the burden of hyperparameter search, this robustness is a particularly intriguingproperty under distribution shift scenarios where the model selection becomes a challenging task. Further, encouraging the temporal consistency through label smoothing enables us to connect ourmethod with knowledge distillation (KD) , which can provide a wide range of principled tech-niques and theoretical results developed in KD. As a concrete example, we will show when and howAnCon can reduce the optimality gap of self-training, i.e., a case with = 0, in .3. 3.2Constructing an effective generalized temporal ensemble with prediction confidencesNext, we construct the generalized temporal ensemble that makes the selective temporal consistencyin (1) work effectively in self-training under distribution shifts. Specifically, given 0:m and weightsw0:m(x) m for each x X, the prediction by the generalized temporal ensemble is",
  "i=0wi(x) p(y = k|x, i),k [K](2)": "where p(y|x, i) is the prediction made by f(x; i), which can be either soft (p(y = j|x, i) =fj(x; i)) or hard (p(y = j|x, i) = 1 if j = arg maxk[K] fk(x; i) and p(y = j|x, i) = 0otherwise). In this work, we use hard prediction because soft prediction puts more weights on recentpredictions since self-training tends to keep increasing the prediction confidence during training.",
  "where ()m is an exponential moving average (EMA) of prediction confidence with hyperparameter and E[c(X; i)] is a Monte-Carlo approximation of E[c(X; i)] with mini-batch samples": "Intuitively, our weighting mechanism aggregates only relatively confident predictions with a uniformweight. Given the observation that relative ordering of confidence is highly correlated with accuracyeven under distribution shifts , our thresholding rule would tend to put non-zero weights oncorrect predictions. Besides, by employing the relative criterion, the thresholding does not sufferfrom the problems that neglect predictions obtained in the early stage of training. We also remark that AnCon has almost the same computational cost as ELR. Specifically, (2) can beimplemented by fk(x; 0:m, w0:m) = fk(x; 0:m1, w0:m1) + wm(x)p(y = k|x, m) that requiresto store the weighted sum of previous predictions without additional forward passes or storingprevious parameters, which is the same as storing the previous logit vector in ELR. Similarly, (3) canbe efficiently implemented by ()m = ()m1+(1)EX[c(X; m)] that requires constant additionalcomputational costs compared to vanilla self-training with the constant being small. Therefore, AnConshares the same computational benefits as ELR compared to other state-of-the-art methods in SFDA. On optimality of the relative thresholding in (3) From the optimization perspective, the optimalweights w0:m correspond to the weights under which self-training with Y (X; 0:m, w0:m) canminimize the expected loss:",
  "w0:m arg minw0:ml(w0:m),w0:m arg minEX[H(f(X; ), Y (X; 0:m, w0:m))].(4)": "Unfortunately, l(w0:m), or its empirical counterpart, is not available in self-training due to theabsence of labels. Further, even if labels are given, solving (4) is intractable due to non-smoothnessof l(w0:m) with respect to w0:m and the cost of finding w0:m. To circumvent this issue, we show in .3 that (4) can be relaxed to the problem of findingensemble weights that give a maximum likelihood estimation (MLE) solution under certain conditions.As a result, instead of solving the intractable optimization in (4), we find the optimal weights by",
  "w0:m arg maxw0:mEXY [log fY (X; 0:m, w0:m)].(5)": "In the following theorem which is proven in Appendix B.2, we show that the simple relativethresholding in (3) can make f(x; 0:m, w0:m) asymptotically correct for samples where the neuralnetwork tends to be relatively confident during self-training and thus our simple weighting mechanismin (3) to be the solution of (5) in the asymptotic region.Theorem 3.1. Let Ai(c) := {x X|c(x; i) > c}, Q(x; c0:m) := mi=0 1(x Ai(ci)),and p(x; c0:m) =1",
  "Q(x;c0:m)mi=0 EY |X=x[1(Y (x) = Y (x; i))]1(x Ai(ci)) for x such that": "Q(x; c0:m) >0. Let us assume that random events 1(Y (x) = Y (x; i)) and 1(Y (x) = Y (x; j))are conditionally independent given X Ai(c) for j {0, , i 1}, x X, c [0, 1). If x Xsuch that p(x; c0:m) > 1/2, then for the generalized temporal ensemble in (3), it holds that",
  "where (z) := 2z 1 log(2z) is a positive increasing function in z [0.5, 1]": "The result states that as long as the average accuracy for relatively confident predictions over iter-ations exceeds 50%, the error rate of the generalized temporal ensemble monotonically decreasesas Q(x; c0:m) increases. Furthermore, f(x; 0:m, w0:m) is asymptotically correct on x such thatQ(x; c0:m) as m . AnCon aims to achieve these desirable properties through theuncertainty-aware temporal consistency that helps to satisfy the condition p(x; c0:m) > 0.5. Specifi-cally, as shown in a in Appendix, our generalized temporal ensembles accuracy tends tosignificantly increase as the number of confident samples increases, being consistent with our theory.We note that this monotonic improvement would not be the case for the temporal ensemble withoutuncertainty-awareness and vanilla self-training (cf. a). Finally, we emphasize that the assumption p(x; c0:m) > 0.5 applies only to relatively confidentpredictions which are averaged over iterations. This is significantly weaker than requiring a lowerbound of an expected accuracy of each sample for every iteration, which is the case when LFNmethods are directly applied to the self-training scenario. Also, due to its dependency on the choiceof the confidence thresholds c0:m, the assumption can hold by controlling c0:m at the expense of loosening the upper bound in (6) (e.g., selecting 90th-quantile as in b in Appendix).Specifically, increasing the thresholds can improve (p(x; c0:m)) and enhance the chance of satisfyingp(x; c0:m) > 1/2 but reducing Q(x; c0:m). While this trade-off necessitates a proper choice of c0:m,our extensive experiments show that setting the threshold cm by the EMA of prediction confidence,i.e., ()m in (3), works effectively. 3.3Theoretical insights from knowledge distillationIn this section, we present a novel connection between AnCon and KD for addressing intractability of(4). KD is a framework for training a small student network f, e.g., ResNet-50 , with an additionalsupervision from a large teacher network f (t), e.g., ResNet-152. Specifically, a cross-entropy underKD is lKD() = EXYH(f(X; ), (1 KD)E1(Y (X)) + KDf (t)(X))with KD ,which bears a significant similarity with AnCon (cf. (1)). Indeed, AnCon can be understood as aspecial case of KD called self-distillation when f and f (t) have the same architecture, where thegeneralized temporal ensemble f corresponds to the teacher network f (t) with a notable differencethat the pseudo label Y is used instead of the true label Y . Based on this connection, we performa convergence analysis of AnCon by modifying the partial variance reduction theory , as givenbelow. We note that the usage of Y and f results in an inherently biased gradient estimator, whichrequires special treatments for the convergence analysis unlike the typical self-distillation setting.",
  "Setup Following , we assume a linear model fk(x; ) = exp(Tk x)/": "i[K] exp(Ti x) withi Rd for i [K], := Concat(1, , K) RdK, and Concat() is the concatenationoperation. Also, we assume a bounded support for X; that is, x C for all x where pX(x) > 0.Under this setting, we repeat the following steps starting from a given 0 (i.e., m = 0): 1. Outer temporal ensemble update: Update w0:m by (3) to obtain f(x; 0:m, w0:m) (cf. (2)).2. Inner parameter update: With m,0 := m and (0:m, w0:m), solve (1) with stochasticgradient descent m,t+1 = m,t g(m,t)for t {0, 1, , T 1} and set m+1 = m,T .",
  "(8)": "where 2 := E l() 2, 2(0:m,w0:m) := E g(0:m, w0:m) 2, gKL(0:m, w0:m) =EX[DKL(f(X; ) f(X; 0:m, w0:m))] with DKL(p q) is the KL-divergence between p and q,and gC(0:m, w0:m) = EX[ f(X; 0:m, w0:m)] EX[ Y (X; m)] 2. In Theorem 3.2, (7) characterizes the optimality gap in the inner loop optimization. Specifically,the first term is about reducing the initial optimality gap over iterations and motivates why we needadaptation, e.g., by self-training, if the performance deteriorates under severe distribution shifts. Thesecond term is about the bias of the pseudo label and motivates the challenges of self-training underpoorly performing pseudo labels in the case of severe distribution shifts. Crucially, these two termscan be fully characterized by the quality of initial model m and do not depend on w0:m. Thus, weconcentrate on the impacts of w0:m designed in (1)-(3) on N(m; 0:m, w0:m) to show that AnConseffectiveness on improving self-training performance under distribution shifts. First, Theorem 3.2 shows the effectiveness of our label smoothing formulation in (1) under properlychosen m. Specifically, compared to vanilla self-training, AnCon results in the smaller neighborhoodsize of the stochastic gradient descent; N(m; 0:m, w0:m) N(0). That is, the result suggests thatAnCon is at least better than vanilla self-training under the mild regularity conditions. Further, under the additional assumption of a sufficiently good performance temporal ensemble,(8) motivates AnCons weighting mechanism as a relaxed solution of the intractable optimizationproblem in (4). Specifically, if the marginal distribution of the pseudo labels does not change quicklyover outer iterations (which is the case especially for the later training stages as shown in in Appendix), changing w0:m would have only a marginal impact on gC(0:m, w0:m). Thus, theweighting mechanism that minimizes gKL(0:m, w0:m) would minimize the worst-case optimalitygap, which justifies our approach of circumventing intractability of (4) with (5). In this regard,AnCons weighting mechanism in (3) could be thought of as a relaxed solution of (4) as it minimizesgKL(0:m, w0:m) in the asymptotic region (cf. Theorem 3.1).",
  "where p(I( T ) = j) (1 )T ( T 1j) for j {0, , T 1} and T = T 1i=0 (1 )T i": "Corollary 3.2.1 is proved in Appendix B.4 and gives a whole picture of the optimality gap underAnCon. We first remark the trade-off associated with T on the suboptimality E[l( T ) l], whichcharacterize the early-learning phenomenon observed in the biased gradient settings (e.g., self-training and LFN ). Specifically, in (9), increasing T reduces the first term (1)T ( T 1)(l(0)l)but increases the coefficient of the second term T . Therefore, a longer training with a large T maynot enhance the self-training performance especially under a large second term due to inaccuratepseudo labels or the temporal ensemble. Nevertheless, for each outer loop iteration, gE(j) would be smaller under AnCon than its value undervanilla self-training as E[l(j) l] has a tighter upper bound under AnCon due to Theorem 3.2.Therefore, with the guarantee N(j; 0:j, w0:j) N(0), AnCon would achieve a tighter upper boundof (9) than the vanilla self-training method, enabling longer training with smaller value of the secondterm as observed in b. Finally, we remark that this theoretical superiority of AnCon can beextended to the self-training methods with other weighting mechanisms in the asymptotic regionwhen E[l(), g(0:m, w0:m)] 0 (cf. Theorem 3.2).",
  "+ AnCon94.2891.4574.4989.0779.6477.1153.5669.4780.0457.3067.9683.70": "of distribution shifts (Sections 4.1-4.2). We also show that AnCon achieves a better performancethan ELR with ELR {1, 3, 7, 12, 25}, which is the coefficient multiplied to the auxiliary loss, toshow effectiveness of uncertainty aware temporal ensemble. Finally, we assess the integration ofAnCon with generalized cross-entropy (GCE) and neighborhood reciprocity clustering (NRC), which are frequently cited as state-of-the-art methods . We note that AnCon canbe seamlessly applied to GCE and NRC by replacing one-hot pseudo labels by AnCons regularizedpseudo labels Y (X; 0:m, w0:m). For descriptions of the training configurations which we adoptfrom literature, see Appendix D. Evaluation In our self-training settings (SFDA and TTA), we should determine the best checkpointwithout labeled samples, i.e. to select a model 0, 1, , I which is used for evaluation ontest. We use information maximization , IM() = HE(EX[f(X; )]) EX[HE(f(X; ))]when HE is the entropy, which is proven to be effective in unsupervised domain adaptation (UDA)model selection literature . Specifically, we evaluate IM(m) on the hold-out unlabeledsamples at the end of each epoch m for I number of training epochs and select the checkpoint with arg maxm[I] IM(m) as the best one. This is used for final evaluation on the test data.",
  "mi=0 1(c(x;i)>()i). Note that the former with can be": "thought of as the latter with instance-dependent coefficient Q(x; c0:m), which assigns more weighton the generalized temporal ensemble for x with large Q(x; c0:m). In addition, we consider the onlineupdate scenario of pseudo labels, i.e., T = 1. Finally, given the challenging nature of hyperparameterselection in self-training under distribution shifts, we perform all experiments by using a singleconfiguration of hyperparameters of AnCon ( = 0.3 and = 0.9). These hyperparameters result inthe maximum value of maxm[ T ] IM(m) on the hold-out unlabeled samples on Office-31. 4.1Self-training under domain shiftsWe first consider SFDA that adapts a model trained in one domain by performing self-training in thedistribution shifted domain. For the network architecture, we use the modified ResNet used in ,which includes batch normalization after the bottleneck layer and weight normalization inthe last linear layer, which is used in for stabilizing the learning process in SFDA. Datasets We evaluate AnCon on the following datasets: Office-31 with 4,000 images of 31categories from three domains (amazon, dslr, webcam); OfficeHome with 15,000 images of 65categories from four domains (art, clipart, product, real-world); VisDa with 280,000 images of31 categories from two domains (synthetic, real). Results AnCon consistently improves self-training in diverse domain pairs across the three datasets(). Specifically, it reduces the average self-training test error by 5% in Office-31, 6% inOfficeHome, and 13% in VisDa. In addition, compared to ELR with its dataset-dependent optimalhyperparameter value, AnCon shows comparable performance to ELR (2%, 3%, and -3% perfor-mance differences in Office-31, OfficeHome, and VisDa, respectively). Despite its slightly inferiorperformance in VisDa, we note that AnCon achieves significantly better accuracy for low-performingclasses: for Skateboard and Truck classes, AnCon achieves 50% and 16% of accuracies, whileELR achieves 39% and 0.43% in these classes. Thus, AnCon can be as effective as ELR for improvingthe self-training performances under distribution shifts without needing to adjust hyperparametervalues for each dataset unlike ELR.",
  "(c)": ": .2: (a) Test accuracy for each intensity level in ImageNet-C. (b) Performancedegeneration in the defocus blur corruption with intensity 4. .3.1: (c) Maximum performancechanges under different model selection methods. We present performances for individual corruptionsin Appendix. For all boxplots used in the paper, the box represents interquantile range with whiskersas 1.5 interquantile range and the horizontal line inside the box represents the median. Notably, AnCon significantly improves performances of both GCE and NRC via fundamentallydifferent mechanisms for handling noisy pseudo labels (e.g., reducing test accuracy of 3% and 8%on average in OfficeHome, respectively). Specifically, NRC filters incorrect predictions based onlocal consistency, while AnCon uses temporal consistency. Combining NRC and AnCon leveragespseudo labels that are both locally and temporally consistent, resulting in significant performanceimprovements over NRC or Self-Training + AnCon (cf. ). In addition, GCE reduces the impactof wrong pseudo labels rather than finding them. Applying GCE to AnCon minimizes the effects ofpotentially wrong but temporally consistent pseudo labels, which can be implied by the performanceof GCE + AnCon compared to GCE or Self-Training + AnCon (cf. ). Thus, the impressiveperformance gains from AnCon, which would be orthogonal to the gains from state-of-the-art methodsin SFDA, show its significant practical implications. 4.2Self-training under synthetic corruption operationsWhile we have considered the domain shift, e.g., adaptation of a model trained on synthetic images toreal images, in .1, this section examines the self-trainings ability to adapt to distributionshifts by synthetic image corruptions. This setting has been used to measure the robustness of neuralnetworks with respect to a general out-of-distribution setting. To this end, we consider ImageNet-C, which consists of 50,000 images drawn from a validation set of ImageNet where each imageis corrupted by 15 types of synthetic corruptions related to noise, blur, weather and digital. Result Consistent with the findings under the domain shift, AnCon outperforms the average perfor-mances of self-training and ELR under varying levels of corruption intensities (cf. a andTables 7-11 in Appendix), improving the self-training methods accuracy by 16% on average. Further,the gains from AnCon is significant when the distribution shifts are intense (e.g., improving accuraciesby 20% and 52% on average in intensities of 4 and 5) where the initial model trained on the sourcedomain significantly deteriorates. Specifically, for Shot, Impulse, and Gaussian corruptions withthe most extreme shift intensity of 5, where the initial model achieves accuracies of (3.04%, 1.76%,2.12%), AnCon achieves (22.56%, 26.56%, 25.85%) (cf. ). This striking improvement com-pared to vanilla self-training with performances (0.26%, 1.72%, 1.04%) and ELR with performances(8.00%, 14.12%, 16.00%), underscores the importance of the AnCons uncertainty-aware temporalconsistency scheme, as shown in Corollary 3.2.1. We note that this impressive result is also explainedby AnCons ability to prevent the gradual performance degradation during the course of training withthe extremely noisy pseudo labels (cf. b). Combined with previous results in domain shiftscenarios, we expect that AnCon would work effectively in various out-of-distribution settings. 4.3Versatility of AnConIn previous sections, we have shown the universality of AnCon by evaluating it on diverse distributionshift scenarios. In this section, we show versatility of AnCon by analyzing its attractive properties inrobustness and uncertainty representation. 4.3.1Robustness to model selectionThere is no universally agreed model selection criterion, such as cross-validation in the i.i.d. setting,in self-training under distribution shifts. This is partly due to the variety of distribution shift scenarios,where an effective criterion in one may be ineffective or inapplicable in another; for instance, a principled criteria called importance-weighted cross validation in UDA cannot be appliedto SFDA. In this regard, it would be an important characteristic of a self-training method underdistribution shift to be robust with respect to different choices of model selection criteria. Therefore,we evaluate robustness with respect to the following different model selection criteria: InfoMax ,Corr-C , and Ent (see Appendix D.3 for the description).",
  ":Sensitivity analysiswith respect to and on fourdomain pairs (Ar-Pr, Pr-Cl, Rw-Cl, Rw-Pr) in OfficeHome. Here,green triangles are means": "c shows that AnCons maximum performance changedue to different model selection methods is much lower thanthat of other methods, especially under severe distribution shifts.This valuable advantage can be contributed to the property ofAnCon that can prevent performance degeneration (cf. b).Given that, in practice, we barely know when the model collapsehappens and which model selection criteria are the best, the resultshighlight a significant practical value of AnCon. 4.3.2Robustness to the choice of hyperparametersThroughout this paper, we have shown that our single config-uration of parameters ( = 0.3, = 0.9) work well across awide range of benchmark problems. In this section, we aimto show our findings can be preserved when the hyperparam-eter values deviate from the default setting by performing asensitivity analysis for values {0.1, 0.3, 0.5, 0.7, 0.9} and {0.1, 0.3, 0.5, 0.7, 0.9}. We also test two frequently usedannealing schedules that m = m/I and m = min(1, 2m/I),called full and half, respectively. shows that AnCon is sta-ble even under extreme values of hyperparameters. Specifically,for both hyperparameters, the maximum average performancechange is less than 1%, and barely impacts the performanceof AnCon. Indeed, our analysis suggests to increase from ourdefault setting; that is, to put a higher weight on the generaltemporal ensembles prediction. Here, we note that our subop-timal choices of hyperparameters are due to our rigorous andpractical hyperparameter choice. Given the challenging natureof hyperparameter optimization under distribution shifts, the sta-ble performances of AnCon under arbitrary choices of hyperparameters would enable AnCon to beseamlessly applied to diverse practical settings.",
  ": (a) ECEs under five levels ofintensities in ImageNet-C; (b) Accuracyand ECE changes during the course oftraining in VisDa": "We have shown that all self-training methods significantlyimprove the performance of the baseline method afterthe adaptation period. However, it is widely known thatthese noticeable improvements come with the price ofsacrificing an uncertainty representation ability which iscritical in real-world decision-making scenarios .Specifically, the calibration performance, which is the gapbetween the prediction confidence and accuracy, usuallymonotonically increases as self-training keeps reducing theuncertainty for all predictions during the course of training.In this regard, we analyze the calibration performancewith respect to the expected calibration error (ECE; seeAppendix for definition). Here, a lower ECE means alower gap between confidence and accuracy. As shown in a and in Appendix, AnCongives much lower ECE compared to other methods. Con-sidering ELR and GCE both have regularization effects,we conjecture that this phenomenon is due to selectiveregularization in AnCon that increases prediction confi-dences of samples only if the past confident predictionsare consistent with the current prediction. Especially, in b which confirms the accuracy-calibration dilemma in VisDa, AnCon is shown to limit theECE increases during training compared to all other methods. That is, AnCon helps to significantlyreduce the price of the calibration performance we need to pay for improving accuracy, which areboth important measures in practice. 4.4Algorithmic design choicesRecall that we define f(x; 0:m, w0:m) = mi=0 wi(x) p(y|x, i) with our simple design choices:the relative thresholding for weighting scheme wi(x) 1(c(x; i) > ()i) and hard prediction forp(y|x, i). In Appendix C.3, we found that our simple design choices are more appropriate for thedistribution shift settings than several more sophisticated alternatives, which can be summarized asfollows. More sophisticated weighting schemes (e.g., Entropy (wi(x) exp {HE[f(x; i)]}))reduce the self-training performance, despite being a more accurate measure of predictionuncertainty. We conjecture that the poor calibration performance of the neural network inself-training under distribution shifts prevents the sophisticated weighting schemes fromaccurately reflecting the goodness of the prediction. Various soft prediction schemes, which can give more information about the non-leadingentry values, leads to performance reductions. We conjecture that the continuously increasingconfidence in the later stage of self-training would make soft prediction ignore early-stagepredictions which may be valuable to memorize.",
  "Related work": "Filtering incorrect pseudo labels Popular confidence-based thresholding methods fallshort under distribution shifts since even high confident predictions can be highly incorrect. Therefore,recent advances in SFDA and TTA utilize higher order information to filter incorrect pseudo labels.For instance, based on the intuition that true labels of adjacent samples would be same, centroids foreach predicted class can be maintained in the feature space and then the pseudo label for each input iscorrected by the adjacent centroid . The idea of using per-class centroids has been extended toincorporate more general clustering structures . However, the neighborhood structure-basedmethods are computationally demanding due to storage of memory banks in the feature space andnearest neighbors search. Such computational complexity persists in other approaches, which arebased on the consistency of multiple predictions from different augmentations and modelstrained with different loss functions . Compared to these solutions, AnCon can efficiently estimatecorrect labels with only limited extra memory overhead of storing past predictions. Learning from noisy labels Treating pseudo labels as inherently noisy, techniques from theLFN literature have been integrated to self-training. For instance, the LFN literature has proposedrobust loss functions that reduce impacts of random noisy labels , and a recent large-scaleexperimental study shows the applicability of the generalized cross-entropy in the SFDA setting .The effectiveness of ELR on SFDA bears a similar idea because ELR was developed to regularizethe neural networks tendencies to memorize incorrect labels . Despite their effectiveness, bynature, these approaches do not consider important characteristics of the unbounded and instance-dependent noise rates inherent in self-training under distribution shifts, which results in significantsuboptimality in both theory and practice. However, by considering the unique characteristics ofself-training under distribution shift, AnCon relaxes the conditions required to achieve optimality aswell as boosts the self-training performance in diverse scenarios.",
  "Conclusion": "This paper introduces AnCon, which effectively improves self-training performances under diversedistribution shift scenarios by promoting selective temporal consistency based on confident predic-tions. As a result, AnCon effectively mitigates the detrimental effects of noisy pseudo labels withoutmuch computational overhead, unlike the previous methods. We show that AnCon not only advancesour theoretical understanding of a generalized notion of temporal consistency in self-training butalso can be a practical asset as a simple and effective self-training method with attractive properties.In Appendix C.4, we present limitations and future directions, such as adaptive determination of ,combining local and temporal consistency, and extending the selective temporal consistency in thesequential decision making problems.",
  "Dong-Hyun Lee. Pseudo-label: The simple and efficient semi-supervised learning method fordeep neural networks. In Workshop on Challenges in Representation Learning, ICML, 2013": "Eric Arazo, Diego Ortego, Paul Albert, Noel E OConnor, and Kevin McGuinness. Pseudo-labeling and confirmation bias in deep semi-supervised learning. In International Joint Confer-ence on Neural Networks, 2020. Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin A Raf-fel, Ekin Dogus Cubuk, Alexey Kurakin, and Chun-Liang Li. Fixmatch: Simplifying semi-supervised learning with consistency and confidence. In Advances in Neural InformationProcessing Systems, 2020. Bowen Zhang, Yidong Wang, Wenxin Hou, Hao Wu, Jindong Wang, Manabu Okumura, andTakahiro Shinozaki. Flexmatch: Boosting semi-supervised learning with curriculum pseudolabeling. In Advances in Neural Information Processing Systems, 2021. Yidong Wang, Hao Chen, Qiang Heng, Wenxin Hou, Yue Fan, Zhen Wu, Jindong Wang, MariosSavvides, Takahiro Shinozaki, Bhiksha Raj, et al. Freematch: Self-adaptive thresholding forsemi-supervised learning. arXiv preprint arXiv:2205.07246, 2022. Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, David Sculley, Sebastian Nowozin, JoshuaDillon, Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your models uncertainty?evaluating predictive uncertainty under dataset shift. In Advances in Neural InformationProcessing Systems, 2019.",
  "Shiqi Yang, Yaxing Wang, Joost Van De Weijer, Luis Herranz, and Shangling Jui. Generalizedsource-free domain adaptation. In IEEE/CVF International Conference on Computer Vision,2021": "Hao-Wei Yeh, Thomas Westfechtel, Jia-Bin Huang, and Tatsuya Harada. Boosting source-freedomain adaptation via confidence-based subsets feature alignment. In International Conferenceon Pattern Recognition, 2022. Nazmul Karim, Niluthpol Chowdhury Mithun, Abhinav Rajvanshi, Han-pang Chiu, SupunSamarasekera, and Nazanin Rahnavard. C-sfda: A curriculum learning aided self-trainingframework for efficient source free domain adaptation. In IEEE/CVF Conference on ComputerVision and Pattern Recognition, 2023. Sheng Liu, Jonathan Niles-Weed, Narges Razavian, and Carlos Fernandez-Granda. Early-learning regularization prevents memorization of noisy labels. In Advances in Neural Informa-tion Processing Systems, 2020.",
  "Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understandingdeep learning (still) requires rethinking generalization. Communications of the ACM, 64(3):107115, 2021": "Devansh Arpit, Stanisaw Jastrzebski, Nicolas Ballas, David Krueger, Emmanuel Bengio,Maxinder S Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, et al.A closer look at memorization in deep networks. In International Conference on MachineLearning, 2017. Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethink-ing the inception architecture for computer vision. In IEEE Conference on Computer Visionand Pattern Recognition, 2016.",
  "Mher Safaryan, Alexandra Peste, and Dan Alistarh. Knowledge distillation performs partialvariance reduction. In Advances in Neural Information Processing Systems, 2023": "Evgenia Rusak, Steffen Schneider, George Pachitariu, Luisa Eck, Peter Vincent Gehler, OliverBringmann, Wieland Brendel, and Matthias Bethge. If your data distribution shifts, use self-learning. Transactions on Machine Learning Research, 2022. Mingchen Li, Mahdi Soltanolkotabi, and Samet Oymak. Gradient descent with early stoppingis provably robust to label noise for overparameterized neural networks. In InternationalConference on Artificial Intelligence and Statistics, 2020.",
  "l() 2 2(l() l()), Rp.(12)": "We remark that Assumptions B.1 and B.2 can trivially hold under bounded parameter values thatcan be guaranteed by optimizing neural networks with finite iterations under a gradient or weightclipping. Assumption B.3 holds for infinite-width neural networks, i.e., the neural tangent kernel(NTK) regime . Given that the gradient descent training dynamics of neural networks can be wellapproximated by NTK , the PL condition can be generally regarded as a mild assumption.",
  "where (33) holds due to L-smoothness; (34) holds due to g(m,t)= l(m,t)b(m,t)= l(m,t)g(0:m, w0:m); (36) holds due to a, b 1": "4 a 2 + b 2; (37) holds due to the inequality x + y 2 2 x 2 +2 y 2; (38) holds due to the expected smoothness assumption; (39) holdsdue to the condition 14LL; (41) holds due to the law of total expectation with the random event1(Y (X) = Y (X; m)) and then applying the bounded support assumption.",
  "C.3Algorithmic design choices": "Here, we examine effectiveness of our design choicesthe relative thresholding for weighting schemewi(x) and hard prediction for p(y|x, i). To this end, we replace our design choices with severalalternatives, keeping other features of AnCon the same. Then, we analyze the changes in theperformance compared to the default setting of AnCon in four domain pairs of OfficeHome. Sophisticated weighting scheme for wi(x) We compare our relative thresholding with sophisti-cated weighting schemes called Entropy (wi(x) exp {HE[f(x; i)]}) and Maxprob (wi(x) maxk[K] fk(x; i)). These weighting schemes aim to more precisely weight the predictions based onthe uncertainty of the prediction. However, in a, the simple relative thresholding works betterthan these sophisticated weighting schemes. We conjecture that the poor calibration performance of",
  "(b)": ": (a) Counting the number of pseudo labels for each class with 5,000 training samples inImageNet-C over 100 training epochs, which shows that the marginal distribution of pseudo labelsbarely changes during training. (b) Changes in the total variation distance of the marginal distributionsof the pseudo labels for each two consecutive epochs.",
  ": Ablation study of (a) weighting and (b) prediction schemes": "the neural network in self-training under distribution shifts prevents the estimated uncertainty of aprediction from accurately reflecting the goodness of the prediction. Further, the ineffectiveness ofthese sophisticated weighting schemes persists even when we improve the calibration performancewith last-layer Dirichlet and Monte-Carlo Dropout (see Appendix D.3 for detail). Therefore,our relative thresholding is simple yet effective in self-training under distribution shift. Soft prediction for p(y|x, i) We compare hard prediction vs soft prediction with a softmax tempera-ture parameter T {0.5, 0.75, 1.0, 1.25, 1.5} (cf. Appendix D.3 for the definition). By design, softprediction is more informative, giving values in non-leading entries. However, in b, softprediction turns out to underperform hard prediction for various values of T. We conjecture thatthe continuously increasing confidence in the later stage of self-training would make soft predictionignore early-stage predictions which may be valuable to memorize. Thus, we believe that hardprediction would be more appropriate for AnCon.",
  "C.4Limitations and future directions": "Albeit AnCon proving its effectiveness with a fixed value of , Theorem 3.1 suggests that its optimalvalue depends on the temporal ensemble performance. Thus, future works could aim for adaptivelydetermining . In addition, we observe that all self-training based methods fall short withoutsophisticated model designs tailored for distribution shifts (cf. Appendix E). In this regard,it would be valuable to rigorously understand properties of function classes that enable successfulself-training under distribution shifts. Also, while AnCon significantly relaxes the conditions requiredto achieve optimality compared to LFN techniques, the on-average correct prediction condition stillcan be violated in challenging self-training scenarios. Given the compatibility of AnCon and NRC (cf. ), efficiently combining local and temporal consistencies could be a step toward furtherrelaxing the optimality condition. Finally, we remark that the idea of selective temporal consistencyin sequential decision-making scenarios could be a notable future direction of research. ExtendingAnCon to sequential decision-making scenarios presents significant challenges as they involve thefundamentally different mechanisms. In sequential decision-making, leveraging observed rewardsand balancing exploitation and exploration are core aspects (e.g., constructing the upper confidencebound of the reward in the bandit problem), unlike in self-training. Therefore, the main challengesfor applying AnCon to this setting would be defining rewards and incorporating exploration strategiesinto pseudo label generation.",
  "D.1SFDA training configurations": "Our training configuration is based on . Specifically, we train ResNet-50 as a sourceclassifier, which is used as an initial point for SFDA, for 50 epochs, with minibatch stochasticgradient descent with Nesterov momentum by using the initial learning rate of 0.01, the momentumparameter of 0.9, and the minibatch size of 64 in the Office Home dataset. The learning rate isdecayed by (1 + 10 current iteration number / maximum number of iterations)0.75, and for thepre-trained layers we use a 10 times smaller learning rate. For Office-31, we change the number ofepochs by 100 and other configurations are the same as those used in the setting in Office Home.Similarly, we only change the number of epochs to 10, the learning rate to 0.001, and the architectureto ResNet-101 for VisDa. We split dataset into 90% of the training set and 10% of the validation set,and the best model is chosen based on the lowest error rate on the validation set. For adapting thepre-trained model in the target domain, we use the exactly same configurations, except reducing thenumber of epochs to 30 for Office-31 and OfficeHome and to 15 for VisDa.",
  "D.2ImageNet-C training configuration": "For applying self-training, we follow the training configuration used in ; specifically, givenResNet-50 pretrained on ImageNet, we perform self-training without threshold by running thestochastic gradient descent optimizer with the learning rate of 0.001 and the minibatch size of 100 for20 epochs. We remark that other techniques such as momentum and learning rate scheduling are notused.",
  "EResult in UDA": "While SFDA does not assume any information, unlabeled target domain samples as domain shiftinformation can be available during the source domain training process in some practical scenarios,e.g., when labeling target domain samples is costly. In this case, UDA methods are used to minimizethe potential impact of distribution shifts by matching input marginal probability distributions ofsource and target domains. Given the prevalence of UDA scenarios in practice, we aim to showwhether AnCon and other self-training methods can be effective for the UDA methods (conditionaldomain adversarial network (CDAN) and maximum mean discrepancy (MDD) ) in theadaptation stage. However, as shown in , all methods decrease the performance of bothCDAN and MDD for most cases, albeit the reduction rates in AnCon are smaller than self-trainingand ELR. We conjecture that this is because the base methods (CDAN and MDD) do not contain thesophisticated tricks, such as adding weight normalization to the final linear layer, which are used inthe SFDA literature .",
  ". Experimental Result Reproducibility": "Question: Does the paper fully disclose all the information needed to reproduce the main ex-perimental results of the paper to the extent that it affects the main claims and/or conclusionsof the paper (regardless of whether the code and data are provided or not)?Answer: [Yes]Justification: We include training configurations of all experiments in Appendix D.",
  ". Crowdsourcing and Research with Human Subjects": "Question: For crowdsourcing experiments and research with human subjects, does the paperinclude the full text of instructions given to participants and screenshots, if applicable, aswell as details about compensation (if any)?Answer: [NA]Justification: This paper does not include crowdsourcing or research with human subjects. 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with HumanSubjectsQuestion: Does the paper describe potential risks incurred by study participants, whethersuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)approvals (or an equivalent approval/review based on the requirements of your country orinstitution) were obtained?Answer: [NA]Justification: This paper does not include crowdsourcing or research with human subjects."
}