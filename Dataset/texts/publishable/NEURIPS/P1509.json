{
  "Abstract": "A series of recent works by Lyu, Wang, Vadhan, and Zhang (TCC21, NeurIPS 22, STOC 23) showed that composition theorems for non-interactive differentially private mechanisms extend to the concurrentcomposition of interactive differentially private mechanism, when differ-ential privacy is measured using f-DP and the adversary is adaptive. Weextend their work to the continual observation setting, where the datais arriving online in a potentially adaptive manner.More specifically,we show that all composition theorems for non-interactive differentiallyprivate mechanisms extend to the concurrent composition of continualdifferentially private mechanism, where the adversary is adaptive.Weshow this result for f-DP, which also implies the result for pure DP and(, )-DP.",
  "Introduction": "Differential privacy is a popular measure of the privacy protection offered byan algorithm that performs statistical analysis on a sensitive dataset about indi-viduals. While differential private mechanisms for the setting where the datasetis static (i.e., the batch setting) are well studied for a wide variety of problems,the setting where the dataset changes dynamically, i.e., where questions about",
  "InstituteofScienceandTechnology,Klosterneuburg,Austria(email:).InstituteofScienceandTechnology,Klosterneuburg,Austria(email:).Harvard University (email: salil )": "1This project has received funding from the European Research Council(ERC) under the European Unions Horizon 2020 research and innova-tion programme (Grant agreement No. 101019564) and the Austrian Sci-ence Fund (FWF) grant DOI 10.55776/Z422, grant DOI 10.55776/I5982, and grant DOI10.55776/P33775 with additional funding from the netidee SCIENCE Stiftung, 20202024.2This project is supported by NSF grant BCS-2218803, a grant from the Sloan Foundation,and a Simons Investigator Award.",
  "arXiv:2411.03299v1 [cs.DS] 5 Nov 2024": "the dataset and updates of the dataset are arbitrarily interleaved, has only re-cently received more attention. This setting was introduced in 2010 by Dwork,Naor, Pitassi, and Rothblum and the corresponding privacy definition wascalled differential privacy under continual observation. These mechanisms aredata structures (as they can (usually) process an arbitrary number of queriesand updates) that are differentially private under continual observation. Wecall them continual mechanisms for short below.In recent years, mechanisms that are differential private under continual ob-servation have been developed and analyzed for summing a sequence of (binary)numbers , weighted sums , histograms and histogram-basedqueries , set cardinality , various graph properties ,and clustering points in Euclidean space . Some of these works are performedby reduction to another continual observation problem. That is, the continualmechanism M for the new problem uses a continual mechanism M for a pre-viously studied problem mediate all of its access to the dataset.Thus, all steps taken by the mechanism can be seen as post-processing theoutput of M, leading immediately to the guarantee that M is differentiallyprivate under continual observation. However, for some of the above works (seee.g., ), multiple continual mechanisms are used, i.e., M is interactingwith multiple continual mechanisms M , M , . . . in an arbitrary way. Thus, toguarantee privacy for M, the concurrent composition of the involved continualmechanisms needs to be analyzed.A series of recent works by Lyu, Wang, Vadhan, and Zhang (, ,, ) analyzed the concurrent composition of interactive mechanisms inthe setting where the dataset is static and unchanging (but there can be adap-tive queries answered in a potentially stateful manner.) More specifically, theyshowed that all composition theorems for non-interactive differentially privatemechanisms extend to the concurrent composition of interactive differentiallyprivate mechanism. The privacy definition used in their work encompasses f-differential privacy , (, )-differential privacy, and pure differential privacy.This paper analyzes the concurrent composition of continual mechanisms,where there are updates as well as queries, and shows the corresponding result:Composition theorems for non-interactive differentially private mechanisms ex-tend to the concurrent composition of continual differentially private mecha-nism. As our theorem is based on their result, it applies to the same privacydefinitions.",
  "Basic Definitions": "Let X be a family of datasets. A randomized mechanism M : X Y is analgorithm that can be represented by a function that randomly maps a datasetx X to an element of Y (an answer). We call M a non-interactive mechanism(NIM) since it halts immediately after returning an answer.We define a neighboring relation to be a binary relation on the family ofdatasets X. For example, two datasets x, x X are neighboring if they are",
  "M is -differentially private (or -DP for short) if = 0": "To avoid confusion, note that the neighboring relation (on the familyof data-updates sequences) in Definition 3.2 is the actual neighboring relationspecified by the problem definition, while the universal relation on SinitIM isjust for consistency.The definition of is checked and enforced by I andthis is independent of the definition of the neighboring relation on SinitIM inDefinition 3.3. The general definition of in Definition 3.2 enables our resultsto be applied to both event-level and user-level differential privacy.",
  "end procedure": "For b {0, 1}, we write (I M)(b) instead of (I M)((, (b, , ))). To definedifferential privacy for I M, we require a neighboring relation on SinitIM. Forthat, we use the universal relation on SinitIM, where every element of SinitIMis related to every element of SinitIM. Therefore, by Definition 2.3, differentialprivacy for CMs is defined as follows. Definition 3.3 ((, )-Differential Privacy for CMs). For 0 and 0 1,a continual mechanism M is (, )-differentially private (or (, )-DP for short)against a strongly adaptive adversary if for any strongly adaptive adversary Aand every subset V A,IM,",
  "Continual Mechanisms": "In this section, we introduce continual mechanisms (CMs) supporting both (a)queries about the dataset, called question queries, and (b) updates, called data-update queries. We chose these names because our first crucial observation isthat we can define continual mechanisms as a special case of interactive mech-anisms: The reason is that the definition of queries in interactive mechanismsis general enough to allow changes to the dataset. Specifically, the dataset ofa continual mechanism can be encoded as part of its state, which can changeafter each query. Moreover, the definition of interactive mechanisms puts norestrictions on the types of queries they can process, making it flexible enoughto include both question and data-update queries. Definition 3.1 (Continual Mechanism (CM)). A continual mechanism (CM)M is an interactive mechanism whose state includes a dataset.M receivestwo types of messages: question queries and data-update queries, both can haveparameters.Question queries request information about the current dataset,while data-update queries consist of instructions for modifying the dataset. Uponreceiving an invalid message, M terminates the communication. For simplicity,we set the initial state space SinitM to {}. We define a strongly adaptive adversary to be an interactive mechanismthat is supposed to adaptively take one of the following actions on their turn:asking a question query, sending a pair of data-update queries, or requestingto halt the communication. The first and second elements of the pairs of data-update queries must form neighboring sequences, which will be enforced by aninteractive pre-post-processing mechanism, see below.Here, the neighboring relation is a binary relation on the family of sequencesof data-update queries.For example, two sequences could be defined to beneighboring if they are identical except for the absence or presence of a singledata-update query. This is usually called insert/delete event-level privacy.In order to establish communication between a CM and a strongly adaptiveadversary and investigate the privacy of the CM, we introduce an interactive pre-post-processing mechanism called an Identifier. Although we cannot force theadversary to behave correctly, the design of the CM and the Identifier combinedensures that the interaction terminates if the adversary sends an invalid query. Definition 3.2 (Identifier). An Identifier I is an interactive pre-post-processingmechanism with initial state space SinitI= {(0, , ), (1, , )}. For b {0, 1}, wedenote I((b, , )) as I(b). I(b) pre-post-processes a continual mechanism andfilters pairs of data-update queries in a communication with a strongly adaptiveadversary. Specifically, I(b) forwards all messages unchanged except for pairsof data-update queries. For any pair of data-update queries (x0, x1), I(b) onlyforwards xb to the CM. Consider the two sequences formed by the first and secondcomponents of the requested pairs of data-update queries. Let be a neighboringrelation on the family of data-update sequences. Although the CM receives onlyone of these sequences (based on b), I(b) keeps track of both sequences and haltsthe communication if they cease to be neighboring. More details are provided inAlgorithm 2. Recall that a continual mechanism supports single data-update queries, whilea strongly adaptive adversary sends pairs. Thus, without an identifier I in be-tween, a CM and a strongly adaptive adversary cannot communicate. Moreover,as the initial state space of continual mechanisms consists of a single element,the definition of differential privacy for interactive mechanisms (Definition 2.3)implies that any continual mechanism is differentially private. Therefore, wedefine a continual mechanism M to be (, )-differentially private if and only ifthe interactive mechanism I M is (, )-differentially private.By definition, SinitM = {} and SinitI= {(0, , ), (1, , )}, which implies thatthe set of possible initial states for IM is SinitIM = {(, (0, , )), (, (1, , ))}.",
  "Composition": "The composition of non-interactive mechanisms M1, . . . , Mkis a non-interactive mechanism M = Comp[M1, . . . , Mk] that takes a tuple of datasetsx = (x1, . . . , xk) as input and returns M(x) = (M1(x1), . . . , Mk(xk)). Twotuples x = (x1, . . . , xk) and x = (x1, . . . , xk) are neighboring if xi and xi areneighboring for each 1 i k. Differential privacy for Comp[M1, . . . , Mk]follows the standard definition of differential privacy for non-interactive mech-anisms (see Definition 2.1).For the composition of interactive mechanisms, an adaptive adversary asksa query from one of the mechanisms based on the previous answers of all themechanisms, which leads to the definition of concurrent composition. In thiscase, the adversary may either request to terminate the interaction or send amessage in the form of (q, i) or ((x0, x1), i), where q is a question query, (x0, x1)is a pair of data-updates queries, and i is an integer in {1, . . . , k}, indicatingwhich mechanism to query. To formalize this, Vadhan and Wang introducedthe super-mechanism ConComp that runs the IMs to-be-combined internallyand interacts with an adversary. Definition 4.1 (Concurrent Composition of IMs ). Given interactive mech-anisms M1, . . . , Mk with the same set of possible initial states Sinit, the con-current composition of M1, . . . , Mk is an interactive mechanism, denoted byConComp[M1, . . . , Mk], with initial state space (Sinit)k and defined in Algo-rithm 4. Initial states s = (s1, . . . , sk) (Sinit)k and s = (s1, . . . , sk) (Sinit)k",
  "We next define differential privacy for the concurrent composition of inter-active mechanisms": "Definition 4.2 ((, )-Differential Privacy for Concurrent Composition of IMs). Given interactive mechanisms M1, . . . , Mk with the same set of possi-ble initial states and privacy parameters 0 and 0 1, the con-current composition of M1, . . . , Mk is (, )-differentially private (or (, )-DP for short) against an adaptive adversary if the interactive mechanismConComp[M1, . . . , Mk] is (, )-differentially private against an adaptive ad-versary. Similar to Definition 3.3, in order to investigate the privacy of the con-current composition of continual mechanisms M1, . . . , Mk, we need to con-sider the interaction between ConComp[I M1, . . . , I Mk] (instead ofConComp[M1, . . . , Mk]) and the adversary. Note that ConComp[M1, . . . , Mk]is a well-defined interactive mechanism; however, as was the case for a singlecontinual mechanism, the concurrent composition of continual mechanism doesnot have a meaningful interaction with an adversary without the Identifiers.Differential privacy for the concurrent composition of continual mechanismsis now defined as follows. Definition 4.3 ((, )-Differential Privacy for Concurrent Composition ofCMs). Given continual mechanisms M1, . . . , Mk and the privacy parameters 0 and 0 1, the concurrent composition of M1, . . . , Mk is (, )-differentially private (or (, )-DP for short) against a strongly adaptive adver-sary if ConComp[I M1, . . . , I Mk] is (, )-differentially private against astrongly adaptive adversary.",
  "5f-DP and Generalized d-D-DP": "The notion of f-differential privacy (or f-DP) is a generalization of (, )-differential privacy. It takes a statistical point of view and uses trade-off func-tions to measure how indistinguishable the output distributions of a mechanismare for two neighboring datasets.Consider a non-interactive mechanism M : X Y and two neighboringdatasets x0, x1 X. Observing the outcome of M, an analyst A must decidewhether the input dataset was x0 (null hypothesis H0) or x1 (alternative hy-pothesis H1). A rejection rule : Y {0, 1} is a function that takes the answerof M as input and decides whether to reject the null hypothesis or not. Thus,for b {0, 1}, the analyst A chooses dataset xb when returns b.To quantify the indistinguishability between M(x0) and M(x1), f-DP eval-uates the error made by the analyst in selecting the correct hypothesis. Specif-ically, given a rejection rule , the type I error = E[(M(x0))] is the prob-ability of rejecting the null hypothesis H0 while H0 is true (i.e., guessing x1when the dataset is x0). Similarly, the type II error = 1 E[(M(x1))] isthe probability of not rejecting H0 while H1 is true (i.e., guessing x0 when thedataset is x1). Although an analyst could achieve zero type II error by always rejecting thenull hypothesis, when an upper bound on the type I error is set, the minimumachievable type II error reflects the difficulty of distinguishing between M(x0)and M(x1). A trade-off function measures the optimal trade-off between thetype I and type II errors.",
  "T(M(x0), M(x1)) f": "For example, function f,() = max{0, 1 exp(), exp()(1 )}is a trade-off function, and f,-DP is equivalent to (, )-DP .The (meta-)function T in Definition 5.1 is indeed a function that maps a pairof random variables to a trade-off function between these random variables. LetF denote the family of all trade-off functions. We define a partial ordering onF as follows: For f, f F, f f if and only if f() f () for all .Consider two pairs of random variables (X0, X1) and (Y0, Y1), each definedover the same domain. Let fx = T(X0, X1) and fy = T(Y0, Y1). Intuitively,fx fy means that X0 and X1 are closer to each other (harder to distinguish)than Y0 and Y1.Thus, (F, , T) is a tool for measuring the distance (distinguishability) be-tween two random variables. introduced a more general definition of prob-ability distance and showed that (F, , T) satisfies the necessary conditions forsuch a distance.",
  "Any generalized probability distance (D, , D) provides us with a measure ofdistinguishability between distributions of answers, which leads to the definitionof d-D-differential privacy": "Definition 5.4 (d-D-Differential Privacy). Consider a generalized probabilitydistance (D, , D). Given d D, a non-interactive mechanism M : X Yis d-D-differentially private (or d-D-DP for short) if for any two neighboringdatasets x0, x1 X,D(M(x0), M(x1)) d. Similar to (, )-DP, the definitions of f-DP and d-D-DP for non-interactivemechanisms can be extended to interactive and continual mechanisms. We donot repeat the definitions.To state the main theorem, we require defining some properties for general-ized probability distances, which we all take from . Definition 5.5 (Coupling Property). A generalized probability distance (D, , D) satisfies the coupling property if for any d D and any two pairs of randomvariables (X0, X1) and (Y0, Y1) satisfying D(X0, X1) d and D(Y0, Y1) d,there exists a coupling of X0 and Y0 (a joint distribution (X0, Y0)) and a couplingof X1 and Y1 (a joint distribution (X1, Y1)) such that D((X0, Y0), (X1, Y1)) d. Definition 5.6 (Supremum and Complete Poset). In a partially ordered set(D, ), the supremum of a non-empty subset S D, denoted sup(S), is theleast upper bound of S, if it exists. Specifically, s sup(S) for all s S (upperbound), and for any d D satisfying s d for all s S, we have sup(S) d(the least upper bound). The poset (D, ) is called complete if every non-emptysubset of D has a supremum. Definition 5.7 (Continuous Function). Let (D, ) and (E, ) be completeposets, and I be a set of size n.A function f : DI E is continuous ineach variable if for every 1 i n, every d1, . . . , di1, di+1, . . . , dn D, andevery non-empty subset S D,",
  "f(d1, . . . , di1, sup(S), di+1, . . . , dn) = sup{f(d1, . . . , di1, s, di+1, . . . , dn)|s S}": "Definition 5.8 (Chain Rule ). A generalized probability distance (D, , D) satisfies the chain rule property if for every pair of random variablesX0 and X1 with the same domain, there exists a function ChainRuleX0,X1 :Dsupp(X0)supp(X1) D that is continuous in each variable and satisfies thefollowing: for every pair of joint distributions (X0, Y0) and (X1, Y1), where Y0and Y1 are random variables with the same domain,",
  "= DKL(X0||X1) + ExX0[DKL(Y0|X0 = x||Y1|X1 = x)]": "Lemma 5.9 (). Consider the function T from Definition 5.1. Let F be thefamily of trade-off functions. Given trade-off functions f, f F, say f f iff() f () for every . (F, ) is a complete poset, and (F, , T) is ageneralized probability distance satisfying the coupling and chain rule properties.",
  "In this section, we apply the results of on f-DP and d-D-DP concurrentcomposition of interactive mechanisms to continual mechanisms": "Definition 6.1 (Finite Communication for IMs). Consider an interactive mech-anism M with QM = AM = {0, 1}. We say that M has finite communicationif there exists a constant c such that for any adversary A interacting with M,we have that the total length (the number of bits) of Ms answers is bounded byc, and M halts the interaction if A asks a query with length larger than c, or ifthe total number of messages (queries and answers) exceeds c. Theorem 6.2 (). Consider a complete poset (D, ) and a generalized prob-ability distance (D, , D) satisfying the coupling and chain rule properties. Ford1, . . . , dk D, let M1, . . . , Mk be interactive mechanisms such that for each1 i k, QMi = AMi = {0, 1}, Mi is di-D-DP against an adaptive ad-versary, and it has finite communication. If for any k non-interactive mech-anisms N1, . . . , Nk, where Ni is di-D-DP, Comp[N1, . . . , Nk] is d-D-DP, thenConComp[M1, . . . , Mk] is also d-D-DP against an adaptive adversary. 1",
  "A continual mechanisms pre-post-processed by an identifier is a special caseof an interactive mechanism. Thus, the above theorem implies the followingresult for continual mechanisms": "Corollary 6.3. Consider a complete poset (D, ) and a generalized probabil-ity distance (D, , D) satisfying the coupling and chain rule properties.Ford1, . . . , dk D, let M1, . . . , Mk be continual mechanisms such that for every1 i k, QMi = AMi = {0, 1}, Mi is di-D-DP against a strongly adaptiveadversary, and I Mi has finite communication. If for any k non-interactivemechanisms N1, . . . , Nk, where Ni is di-D-DP, Comp[N1, . . . , Nk] is d-D-DP,then ConComp[IM1, . . . , IMk] is also d-D-DP against a strongly adaptiveadversary.",
  "In the original theorem, the state space of M and A (and every IM) is assumed to be{0, 1}; however, this assumption is never used in the proof, and the theorem holds moregenerally": "Combination of Lemma 5.9 and Corollary 6.3 gives the desired result forf-DP, namely that any composition theorem for non-interactive differentiallyprivate mechanisms extend to concurrent composition of continual differentiallyprivate mechanisms. Corollary 6.4. For f1, . . . , fk D, let M1, . . . , Mk be continual mechanismssuch that for every 1 i k, QMi = AMi = {0, 1}, Mi is fi-DP against astrongly adaptive adversary, and I Mi has finite communication. If for any knon-interactive mechanisms N1, . . . , Nk, where Ni is fi-DP, Comp[N1, . . . , Nk]is f-DP, then ConComp[I M1, . . . , I Mk] is also f-DP against a stronglyadaptive adversary.",
  "Pure DP": "The proof of Theorem 6.2 for d-D-DP relies on the fact that in the interactionbetween I M and an analyst A, the number of rounds and possible messagesare bounded by some prefixed constants (finite communication). The followingtheorem presents a stronger result for -DP where the query and answer spacescan be arbitrary sets, and the finite communication assumption is no longerrequired. Theorem 7.1 (). For 1, . . . , k 0, let M1, . . . , Mk be interactive mecha-nisms such that for each 1 i k, Mi is i-DP against an adaptive adversary.For 0 and 0 1, if for any k non-interactive mechanisms N1, . . . , Nk,where Ni is i-DP, Comp[N1, . . . , Nk] is (, )-DP, then ConComp[M1, . . . , Mk]is also (, )-DP against an adaptive adversary.",
  "Consequently, for continual mechanisms we have:": "Corollary 7.2. For 1, . . . , k 0, let M1, . . . , Mk be continual mechanismssuch that for each 1 i k, Mi is i-DP against an adaptive adversary. For 0 and 0 1, if for any k non-interactive mechanisms N1, . . . , Nk, whereNi is i-DP, Comp[N1, . . . , Nk] is (, )-DP, then ConComp[I M1, . . . , I",
  "Mk] is also (, )-DP against an adaptive adversary": "Joel Daniel Andersson and Rasmus Pagh. A smooth binary mechanismfor efficient private continual observation. In Alice Oh, Tristan Naumann,Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors,Advances in Neural Information Processing Systems 36: Annual Confer-ence on Neural Information Processing Systems 2023, NeurIPS 2023, NewOrleans, LA, USA, December 10 - 16, 2023, 2023.",
  "Hendrik Fichtenberger, Monika Henzinger, and Lara Ost.Differentiallyprivate algorithms for graphs under continual observation. In Proc. 29thESA, pages 42:142:16, 2021": "Hendrik Fichtenberger, Monika Henzinger, and Jalaj Upadhyay. Constantmatters: Fine-grained error bound on differentially private continual ob-servation. In Proc. 40th ICML, pages 1007210092. PMLR, 2023. Samuel Haney, Michael Shoemate, Grace Tian, Salil Vadhan, AndrewVyrros, Vicki Xu, and Wanrong Zhang. Concurrent composition for in-teractive differential privacy with adaptive privacy-loss parameters.InProceedings of the 2023 ACM SIGSAC Conference on Computer and Com-munications Security, pages 19491963, 2023.",
  "Monika Henzinger, A. R. Sricharan, and Teresa Anna Steiner. Differen-tially private data structures under continual observation for histogramsand related queries. CoRR, abs/2302.11341, 2023": "Monika Henzinger, A. R. Sricharan, and Teresa Anna Steiner.Privatecounting of distinct elements in the turnstile model and extensions.InAmit Kumar and Noga Ron-Zewi, editors, Approximation, Randomiza-tion, and Combinatorial Optimization. Algorithms and Techniques, AP-PROX/RANDOM 2024, August 28-30, 2024, London School of Economics,London, UK, volume 317 of LIPIcs, pages 40:140:21. Schloss Dagstuhl -Leibniz-Zentrum fur Informatik, 2024. Monika Henzinger, Jalaj Upadhyay, and Sarvagya Upadhyay. A unifyingframework for differentially private sums under continual observation. InDavid P. Woodruff, editor, Proceedings of the 2024 ACM-SIAM Symposiumon Discrete Algorithms, SODA 2024, Alexandria, VA, USA, January 7-10,2024, pages 9951018. SIAM, 2024."
}