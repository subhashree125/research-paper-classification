{
  "Abstract": "With the increasing inference cost of machine learning models, there is a growinginterest in models with fast and efficient inference. Recently, an approach forlearning logic gate networks directly via a differentiable relaxation was proposed.Logic gate networks are faster than conventional neural network approaches be-cause their inference only requires logic gate operators such as NAND, OR, andXOR, which are the underlying building blocks of current hardware and can beefficiently executed. We build on this idea, extending it by deep logic gate treeconvolutions, logical OR pooling, and residual initializations. This allows scalinglogic gate networks up by over one order of magnitude and utilizing the paradigmof convolution. On CIFAR-10, we achieve an accuracy of 86.29% using only 61million logic gates, which improves over the SOTA while being 29 smaller.",
  "Accuracy": "Train Acc. (inference mode)Test Acc. (inference mode)Test Acc. (differentiable train mode) : CIFAR-10 training and test accuracy plot. Thediscretization error, i.e., the difference between the inference(hard) mode accuracy and the differentiable training modeaccuracy is very small during late training. In , we plot the training andtest accuracies for training a convolutionalLGN on CIFAR-10 . Here, we can ob-serve that the discretization error, i.e., thedifference between the inference (hard)mode accuracy and the differentiable train-ing mode accuracy is very small duringlate training. During early training, thediscretization error is more substantial be-cause the method first learns a smoothdifferentiable LGN (with high levels ofuncertainty in the individual logic gatechoices), which later converges to dis-crete choices for each logic gate. Thediscretization step chooses the logic gatewith the highest probability for inferencemode. Accordingly, during early training,the discretization causes larger changes,negatively affecting accuracy, while dur-ing late training, the discretization barely causes any changes and therefore does not considerablyaffect accuracy. We note that there is no noticeable overfitting behavior.",
  "arXiv:2411.04732v1 [cs.LG] 7 Nov 2024": "Differentiable LGNs directly learn the combination of logic gates that have to be executed by thehardware. This differs from other approaches (like BNNs) that require translating an abstraction(like matrix multiplication-based neural networks) into executable logic for inference, an inductivebias that comes with a considerable computational burden. By optimizing the logic directly on thelowest possible level instead of optimizing an abstraction, differentiable LGNs lead to very efficientinference on logic gate-based hardware (e.g., CPU, GPU, FPGA, ASIC). Recently, differentiableLGNs achieved SOTA inference speeds on MNIST , . However, a crucial limitation was therandom choice of connections, preventing LGNs from learning spatial relations, as they arise inimages, which limited performance to an accuracy of only 62% on CIFAR-10 , . To addressthis limitation, we propose to extend differentiable LGNs to convolutions. Specifically, we proposedeep logic gate tree convolutions, i.e., kernels comprised of logic gate trees applied in a convolutionalfashion. Using trees of logic gates, instead of individual gates, increases the expressivity of thearchitecture while minimizing memory accesses, improving accuracy and accelerating training as wellas inference. Further, we adapt pooling operations by representing them with logical or gates (relaxedvia the maximum t-conorm), improving the effectiveness of convolutions in LGNs. Additionally,we propose residual initializations, a novel initialization scheme for differentiable LGNs thatenables scaling them up to deeper networks by providing differentiable residual connections. Theseadvances lead to an accuracy of 86.29% on CIFAR-10 using only 61 million logic gates, leading tocost reductions by 29 compared to SOTAs as displayed in .",
  ": Architecture of a randomly connected LGN. Each nodecorresponds to one logic gate. During training, the distribution overchoices of logic gates (bottom, 16 options) is learned for each node": "Our work builds on and extends dif-ferentiable logic gate networks .To recap,logic gate networks(LGNs) are networks of nodes thatare binary logic gates like AND,NAND, or XOR. LGNs are alsoknown as binary circuits or logicalcircuits, and are the format in whichany digital hardware is implementedon the lowest pre-transistor abstrac-tion level. The function that an LGNcomputes depends on the choices oflogic gates that form its nodes andhow these nodes are connected. Op-timizing an LGN requires choosingthe connections and deciding on agate for each node. A primary chal-lenge when optimizing LGNs is that they are, by default, non-differentiable, preventing gradientdescent-based training, making this problem conventionally a combinatorial problem. However, whenapplied to machine learning problems, solving the combinatorial problem conventionally becomesinfeasible as we require millions of parameters or gates. Thus, a differentiable relaxation of randomlyconnected LGNs has been proposed, which allows training LGNs with gradient descent , over-coming the exponential difficulty of optimizing LGNs. In the remainder of this section, we cover thestructure, relaxation, and training of differentiable LGNs, which we also illustrate in . StructureLGNs follow a layered structure with each layer comprising a number of nodes, eachcomprising one logic gate (3 layers with 4 logic gates each in ). As logic gates are inherentlynon-linear, LGNs do not require any activation functions. Further, LGNs do not have any weights norany biases as they do not rely on matrix multiplications. Due to the binary (i.e., two-input) nature ofthe nodes, LGNs are necessarily sparse and cannot form fully-connected networks. The connectivitybetween nodes has so far been (fully) randomly selected, which works well for easier tasks but canbecome problematic if there is inherent structure in the data as, e.g., in images. During training, theconnections remain fixed and the learning task comprises the choice of logic gate at each node. Differentiable RelaxationTo learn the choices of logic gate for each node with gradient descentrequires the network to be differentiable; however, the LGN is by default not differentiable for tworeasons: (i) Because a logic gate computes a discrete function of its (Boolean) inputs, it is notdifferentiable. (ii) Because the choice of logic gate is not a continuous parameter, but a discrete decision, it is not differentiable. Petersen et al. propose to differentiably relax each logic gate toreal-valued logic via probabilistic logic , . For example, a logical and (a1 a2) is relaxed toa1 a2 and a logical exclusive or (a1 a2) is relaxed to a1 + a2 2 a1 a2, which corresponds tothe output probability when considering two independent Bernoulli variables with coefficients a1, a2.To make the choice of logic gate learnable, Petersen et al. introduce a probability distributionover the 16 possible logic gates (S), which is encoded as the softmax of 16 trainable parameters.For a trainable parameter vector z R16 and all 16 possible logic gate operations as g0, ..., g15, thedifferentiable logic gate as the expectation over its outputs can be computed in closed-form as",
  "With these two ingredients, logic gate networks become end-to-end differentiable": "Initialization, Training, and DiscretizationTraining differentiable logic gate networks corre-sponds to learning the parameters inducing the probability distributions over possible gates. Theparameter vector z for each node has so far been initialized with a standard Gaussian distribution. Theconnections are randomly initialized and remain fixed during training. For classification tasks, eachclass is associated with a set of neurons in the output layer and active neurons in each set are countedcomposing a class score (group sum, right part of ). After dividing them by a temperature ,the class scores are used as logits in a softmax cross-entropy loss. Differentiable LGNs perform bestwhen trained with the Adam optimizer . Empirical evidence showed that the softmax distributionstypically converge to concrete choices of logic gates. Thus, differentiable LGNs can be discretized tohard LGNs for deployment on hardware by selecting the logic gate with the largest probability. Thisdiscretization process incurs only a minimal loss in accuracy compared to the differentiable LGN . LimitationsDifferentiable LGNs have shown significant limitations wrt. the available architecturalcomponents. Previously, they did not provide the option to capture local spatial patterns as they wererandomly connected and only operated on flattened inputs . Further, they previously performed wellonly up to a depth of 6 layers . Thus, more complex relationships between inputs cannot be modeled.Finally, while they provide SOTA performance, differentiable LGNs are very computationallyexpensive to train, e.g., a vanilla 5 million gate network required 90 hours on an A6000 GPU .In the following, we address these limitations by introducing convolutional logic tree layers, logicalor pooling, residual initializations, as well as computational considerations for scaling.",
  "(a)(b)": ": Conventional convolutional neural networks(a) compared to convolutional logic gate networks (b).The images illustrate the first and second to last kernelplacements. The nodes correspond to weighted sums(a), and binary logic gates f1, f2, f3 (b), respectively.The weights / choices of logic gates are shared betweenkernel placements. For visual simplicity, only a singleinput channel and kernel (output channel) is displayed. Convolutional neural networks (CNNs) have ex-perienced tremendous success, being a core con-tributor to the current machine learning ascen-dancy starting with their progress on the Ima-geNet classification challenge in 2012 . Un-derlying CNNs is the discrete convolution of aninput tensor A (e.g., an input image or hiddenactivations) and a linear function / kernel W,denoted as A W. CNNs are especially effec-tive in vision tasks due to the equivariance ofthe convolution, which allows the network togeneralize edge, texture, and shapes in differentlocations by sharing the parameters at all place-ments. However, existing differentiable LGNmethods do not support convolutions. In this work, we propose to convolve activationsA with differentiable binary logic gate trees.While we could convolve A with an individuallogic gate, we observe that actually convolvingA with a (deep) logic gate network or tree leads to substantially better performance as it allows forgreater expressivity of the model. Similar to how the inputs to each logic gate are randomly initializedand remain fixed in conventional differentiable LGNs, we randomly construct the connections in our logic gate tree kernel function. However, we need to put additional restrictions on the connections forlogic gate network kernels. Specifically, we construct each logic gate network kernel as a completebinary tree of depth d with logic gates as nodes and binary input activations as leaves. The output ofthe logic gate operation is then the input to the next higher node, etc. To capture spatial patterns, weselect the inputs / leaves of the tree from the predefined receptive field of the kernel of size sh sw.Based on the depth of the tree, we randomly select as many inputs as necessary. For example, wecould construct a binary tree of depth d = 2, which means that we need to randomly select 2d = 4inputs from our receptive field, e.g., of size 64 3 3, which corresponds to 64 input channels witha kernel size of 3 3. This tree structure allows to capture fixed spatial patterns and correlationsbeyond pair-wise binary inputs. Further, it extends the concept of spacial equivariance to LGNs assuch trees can be used as kernel filters, capturing general patterns in different locations. Using treesof logic gates instead of individual logic gates also has the advantage of reducing memory accessesand improving training and inference efficiency. We remark that, as we apply convolution, the param-eterization of each node is shared between all placements of the kernel (which contrasts convolutionfrom mere local connectivity.) In , we illustrate the difference between conventional CNNmodels and convolutional logic gate networks. During training, the network learns which logic gate operation to choose at each node. Thus, eachlogic tree kernel is parameterized via the choices of each of the 2d 1 logic gates, which are learnable.For a logic kernel of depth 2, we call these logic gates f1, f2, f3 (or more formally fz1, fz2, fz3 forparameter vectors z1, z2, z3 corresponding to Equation 1). Given input activations a1, a2, a3, a4, thekernel is expressed as a binary tree of these logic gates:",
  "(3)": "for k {1, ..., n} where n is the number of tree kernels, i {1, ..., (h sh + 1)}, and j {1, ..., (w sw + 1)} where sh sw is the receptive field size. Note that, in Equation 3, for eachoutput channel k the logic gates f k1 , f k2 , f k3 (or their relaxed form) are chosen and parameterizedindependently. Per convolution, all placements (indexed via i, j) of one kernel share their parameters.",
  "Activation Level": "1: pre or-pooling2: post or-pooling3: no or-pooling : Plot of the density of activations for the sec-ond convolutional block of an or-pooling based convo-lutional LGN. It shows that training implicitly enforcesthat the outputs of the block have the activation levelof a no-pooling network (i.e., with pure stride). In CNNs, max-pooling is a crucial compo-nent selecting the largest possible activationover a predefined receptive field, e.g., for 2 2, max(ai,j, ai,j+1, ai+1,j, ai+1,j+1) . Toadopt this for logic, we propose to use the dis-junction of the binary activations ai,j ai,j+1 ai+1,j ai+1,j+1 via the logical or. Instead ofusing a probabilistic relaxation of the logical or,we can use the maximum t-conorm relaxationof the logical or (max(a, b) = max(a, b)). Bysetting the stride of the pooling operation to thesize of its receptive field, this has a range of cru-cial computational advantages: (i) it is faster tocompute than probabilistic relaxation; (ii) we only need to store the maximum activation and index;(iii) we only need to backpropagate through the maximum activations during training.",
  "CM {1, ..., m}n4 indicates which out of m input channels is selected; CH {1, ..., sh}n4 andCW {1, ..., sw}n4 indicate the selected position inside of the receptive field of size sh sw": "automatic reduction of pre-pooling activations, resolving this potential concern. This phenomenon isshown in . Here, the average activation of a convolutional block of a logic network with 2 2strided or pooling is illustrated. For a random network without pooling, we expect and observe anaverage activation of 50% (dash-dotted). We observe that the post or pooling activations (solid line)for the initialized models is 66.5%, which follows expectation. The pre or pooling activations (dashed)are initialized at 50%, also following expectations. With training, the post or pooling activations(solid) rapidly converge to the average activations of a network without pooling, preventing anyproblematic saturation of activations. We do not introduce any explicit regularization enforcing thisbehavior, but instead found this to be an emerging behavior of training.",
  "Residual Initialization": "The parameters z of existing differentiable LGNs were initialized as random draws from a Gaussiandistribution. Unfortunately, after applying softmax, this leads to rather washed out probabilitydistributions over choices of logic gates. Accordingly, the expected activations, as computed viaEquation 1, are also washed out, quickly converging towards 0.5 in deeper networks. This alsoleads to vanishing gradients in existing differentiable LGNs: With Gaussian initialization, duringbackpropagation, the gradient norm decays at each logic gate by a factor between 0.1 and 0.2 for aninitialized network, exponentially slowing training for deeper networks. In CNNs, a technique for preventing vanishing gradients and preventing loss of information in deepnetworks are residual connections. Residual connections conventionally add the input to a block tothe output of this block . However, when operating in logic, we cannot perform such additions. To prevent the loss of information through washed out activations and reduce vanishing gradientswith a joint strategy, we propose residual initializations. For this, we initialize each logic gate notrandomly but instead to be primarily a feedforwarding logic gate. Here, we choose A as a canonicalchoice and choosing B would be equivalent. In our experiments, we found that initializing theprobability for the logic gate choice A to around 90% and setting all other gates to 0.67% workswell. This corresponds to setting the parameter z3 = 5 and all other zi = 0 for i = 3 in accordanceto Eq. 1. We illustrate an example of residual initializations compared to the existing Gaussianinitializations in .",
  ": Gaussian initialization (a) vs. our residual initialization (b)": "Residual initializationsprevent the loss of infor-mation as well as vanish-ing gradients in deepernetworks. During train-ing, whenever a resid-ual connection is not re-quired, the model learnsto replace the feedforward logic gate choice by an actual operation. Thus, residual initializations areeffectively a differentiable form of residual connections that does not require any hard-wiring. Thisalso means that this form of residuals does not require additional logic gates for residuals. Residualinitializations enable, for the first time, efficient and effective training of LGNs beyond 6 layers.",
  "Computational Training Considerations": "Using trees and pooling allows for substantially improved computational training efficiency andmemory requirement reductions. This is because it allows intermediate activations to be used onlyby the current logic gate tree and because we only need to backpropagate through the maximumactivations during or pooling. For example, using learnable trees with a depth of 2 and or poolingwith a kernel size and stride of 2 2 corresponds to a logic gate tree of depth 2 + 2 = 4 (2 levels arelearnable + 2 from pooling) with 16 inputs and only a single output. For training, it is most efficientto discard all intermediate values and only store the output and information of which path through thepooling was selected, and during backward to recompute only this path, thereby reducing memoryaccesses. The reason for this is that training speed is limited by memory bandwidth and scalability islimited by GPU memory. On average, this strategy reduces memory accesses by 68% and reduces thememory footprint by 90% during training. For using LGNs in hardware designs, trees and poolingimprove the locality of operations and routing, which also leads to more efficient chip layouts. The residual initializations provide a bias towards the feedforward logic gate in trained LGNs. Asfeedforward gates only require a wire and no transistors, this further reduces the necessary transistorcount for hardware implementations of the LGNs, reducing the required chip area. We developed efficient fully-fused low-level CUDA kernels, which, for the first time, enable trainingof convolutional LGNs. The speed of our convolutional layer is up to 200 faster per logic gate thanexisting randomly connected LGN implementations . We will make the code publicly available byincluding it into the difflogic library at github.com/Felix-Petersen/difflogic.",
  "LogicTreeNet Architecture": ": LogicTreeNet architecture. The logical ar-chitectures of the layers / blocks are illustrated on aper neuron basis. Circles indicate a logic gate that canbe learned while the logical ors remain fixed. Duringtraining, for the trainable nodes, we use probabilisticrelaxations of logic gates, which we parameterize viaa softmax distribution over operators (Eq. 1/3). Forthe fixed logical ors, we use the continuous maximumt-conorm relaxation. In the following, we discuss the design of our con-volutional logic gate tree network architectures(LogicTreeNet) for CIFAR-10, which we illus-trate in . We follow the pattern of con-ventional convolutional architectures and designthe architecture by applying convolutional blockswith pooling at the end of each block. Each blockreduces the size by a factor of 22 and we applyblocks until we reach a size of 2 2, increasingthe number of channels in each stage. Followingthis, we apply two randomly connected layers anda group sum as our classification head. This archi-tecture has an overall logical depth of 23 layers,including 4 convolutional blocks (Conv) with treedepths of d = 3, 4 or pooling layers (or-Pool),and 3 randomly connected layers (Rand). 15 ofthese layers are trainable (Conv blocks and Rand),and the pooling layers remain fixed. The archi-tecture is defined in terms of a hyperparameter k,which controls the width of the overall network;we consider k {S 32, M 256, B 512,L 1 024, G 2 048}. In Appendix A.1,we describe LogicTreeNet layer-by-layer and in-clude a LogicTreeNet for MNIST. An additional architecture choice is the connec-tivity for the inputs to a convolutional tree. Whilewe rely on random choices for the inputs, werestrict the choices of channels (CM) such thateach tree observes only 2 (rather than up to 8)input channels. This has the two advantages ofenforcing spatial comparisons of values withinone channel and is more efficient in hardwarecircuit designs. When creating hardware designs,for larger models, routing could become a prob-lem due to congestion when connections betweenchannels follow an arbitrary order. Thus, we re-strict the connections between channels to ensureproper routing: we split the model into k/8 groups, ensuring no cross-connections between thechannels of each group. This restriction as well as similar hardware specific routing restrictions canbe implemented without affecting the accuracy due to the sparsity of the logic gate network model.",
  "Input Processing": "For our smaller CIFAR-10 models (S, M), we use 2 bit precision inputs, and encode them using3 thresholds as in . For our larger CIFAR-10 models (B, L, G), we use 5 bit precision inputs,and process them with low-level feature detectors, in particular, we use edge and curvature detectorkernels with thresholds, converting them into binary encodings, which are converted into LGNs andnot learned. We note that the gates for the input preprocessing are included in each of the gate counts.",
  "Lookup / Truth Table NetworksLookup table networks (aka. truth table networks) are networkscomprised of lookup tables (LUTs) or equivalently (potentially complex) logic gates with n inputs": "There are different approaches for learning or constructing lookup table networks. Chatterjee constructs truth table networks by memorizing training data in an explorative work to considerrelations between memorization and generalization. Wang et al. , replace the multiplicationin BNNs by lookup tables (LUTNet). Benamira et al. transform Heaviside step function activatedCNNs into lookup tables by expressing the binary activation of each neuron via a lookup table thatimplicitly encodes the weight matrix (TTNet). This allows obtaining the binary activation of a neuronby looking up a value from the truth table at a location encoded via the binary inputs of the layer.Benamira et al. use this as an intermediate representation to then convert the truth tables intoLGNs via CNF/DNF (conjunctive / disjunctive normal form) conversion. The resulting LGNs allowfor efficient and effective formal verification. These resulting LGNs differ from the LGNs consideredin this work because they are derived from a conventional CNN and not directly learned, therebyhaving the inductive bias of the neural network architecture (matmul) and its computational overhead,which is similar to BNNs converted into LGNs. We remark that, while TTNets are LGNs, TTNetsare not differentiable LGNs as there is no differentiable representation of LGNs involved. Recently,Bacellar et al. extended differentiable LGNs to learning logic gates with more than two inputs. Binary and Quantized Low-Precision NetworksBNNs and quantized neural networks reduce theprecision of the weight matrices of a neural network. For example, BNNs typically use the weights1 and +1, but variations are possible. For quantized neural networks, a popular choice is 8-bit andother options (such as 4-bit ) are covered in the literature. This leads to substantially reducedstorage requirements of neural networks at the cost of some accuracy. Instead of navely quantizingweights, these approaches typically involve, e.g., quantization-aware fine-tuning . In addition, forsome methods, BNNs and quantized neural networks also reduce the precision of the computationsand activations, leading to speedups during inference , . These approaches typically start witha conventional pre-trained neural network and then convert it into a low-precision representation.BNNs are among the fastest approaches for efficient inference . While BNNs (with binary activations, e.g., XNOR-Net ) are converted into LGNs for inferenceon hardware (e.g., on FPGAs ), the resulting architectures are fundamentally different fromdirectly trained logic gate networks. BNNs have weight matrices and require multiply-accumulate(MAC) operations to express matrix multiplications. Asymptotically, each MAC requires 8 logicgates while at the same time (with only 2 possible states of the weight) this leads to a smallerexpressivity compared to a single learned logic gate (with 16 possible states). We include a technicaldiscussion in the appendix. While it is disadvantageous for inference, for training, BNNs have theadvantage of operating on a higher abstraction level, simplifying training and allowing for translationbetween conventional neural networks and BNNs. We remark that BNNs with binary input activationsand binary weight quantization frequently do not use binary output activations , which meansthat only the multiplications within a matrix multiplication are binary, while the remainder of therespective architectures can require floating precision. In contrast to BNNs, differentiable LGNs arenot parameterized via weight matrices but instead via the choices of logic gates at each node . Sparse Neural NetworksSparse neural networks are networks that are not densely connectedbut instead have only selected connections between layers , , . Conceptually, this meansmultiplying a weight matrix with a binary mask, setting a selection of weights to 0. Sparse nets can beutilized for efficient inference as the sparsity greatly reduces the number of floating-point operationsthat have to be executed. For an overview of sparse neural networks, we refer to Hoefler et al. . Due to the binary (i.e., two-input) nature of logic gates, logic gate networks are intrinsically sparse.Thus, LGNs can be seen as sparse networks; however, sparse neural networks are typically not LGNsand typically operate on real values instead of Boolean values. As differentiable LGNs use randomlyinitialized and fixed connections, it is perhaps important to mention that choosing randomly initializedand fixed connections has been shown to also work well for conventional sparse neural networks .",
  "LogicTreeNet-G86.29%61.0 M": "We train five sizes of LogicTreeNets on theCIFAR-10 data set using the AdamW op-timizer , with a batch size of 128 ata learning rate of 0.02. Additional training de-tails and hyperparameters are in Appendix A.2.We report our main results in and Fig-ure 1. Our primary evaluation is with respectto the number of logic gates (bin. ops), whichcorresponds to the cost in hardware implemen-tations and is proportional to transistor countchip area for ASICs or occupancy on FPGAs. Comparing our model (M) with 3.08 M gatesto the large TTNet model , we can ob-serve that, while the accuracies are similar, ourmodel requires only 1.6% of the number oflogic gates. Increasing the model size, ourmodel (B) matches the accuracy of FINN ,while requiring only 16 M gates compared to901 M gates, a 56 reduction. Consideringan even larger variant of our model (L) with28.9 M gates, we achieve 84.99%. The smallestbaseline model that achieves comparable accu-racy (84.95%) is LUTNet , which requires44.6 as many logic gates. Finally, consid-ering our largest model (G) with 61 M logicgates, we achieve 86.29% test accuracy. We match the accuracy of the Network-in-Network XNOR-Net , while this baseline requires 29 as many gates. Indeed, all networks in the literaturebelow 4 billion gates perform worse than our 61 million gate network.",
  "LogicTreeNet-B80.17%24 ns": "After covering the performance of the trained mod-els, we demonstrate their applicability in hardwaredesigns on a Xilinx FPGA as a proof-of-concept.On CIFAR-10 we limit the hardware developmentup to the base model (B) due to labor cost. In, we report the results. We can observe avery favorable FPGA timing trade-off compared toprevious works. Indeed, using our model (B) weachieve 80.17% accuracy, matching the accuracy ofthe FINN accelerator, but decreasing inference timefrom 45.6 s to 24 ns. In other words, our modelachieves 41.6 million FPS, whereas the previouslyfastest FPGA model achieved 22 thousand FPS(even among all models with 70%). Herein, thelimitation preventing us from reaching around 500million FPS is the transfer speed onto the FPGA.Here, the difference between the smaller models (S& M) and the larger model (B) is that (S & M) receive the input at 2 bit precision whereas (B) receivesthe input at 5 bit precision. We want to remark that substantially accelerated speeds or reduced powerconsumption could be achieved by manufacturing custom hardware such as ASICs; however, this liesout of the scope of this work and is an interesting future research direction. We remark that all accuracies reported in the main paper are from discretized LGNs, and all gatecounts maintain the full convolutional character (no location-based simplifications, e.g., at zero-padding). In Appendix A.4, we include a plot comparing the differentiable training mode accuracyto the discretized inference mode accuracy. Further, we refer to for a comparison ofLogicTreeNet compared to the pareto-front of the state-of-the-art.",
  "LogicTreeNet-L99.35%1.27 M": "We continue our evaluation on MNIST .Here, we use a slightly smaller model ar-chitecture with only 3 (instead of 4) con-volutional blocks due to the input sizeof 28 28.Each convolutional blockhas a depth of 3 and, to maintain validshapes, we use no padding in the first con-volutional block. Each block increasesthe number of channels by a factor of 3.This network architecture is described ingreater detail in Appendix A.1.2. We display the results for MNIST in Ta-ble 3. Here, our models achieves a rangeof new SOTAs: compared to FINN ,we can observe that our small model al-ready improves the accuracy while simul-taneously decreasing the model size bya factor of 36, and reducing inferencetime by a factor of 160. Our mediummodel, with 99.23% test accuracy improves over all BNNs in the literature. When comparingto LowBitNN , a non-binary model, our medium model reduces the inference time by a factor of30 000 while still improving accuracy, increasing throughput from 6 600 FPS to 200 000 FPS. Within the, one-classification-per-cycle regime, comparing to LUTNet , we decrease the errorfrom 1.99% to 0.77%, and we note that the larger FPGA that LUTNet uses should enable placingLogicTreeNet-L (0.65% error) multiple times, enabling multiple classifications per cycle.",
  "S98.21% 0.31%M99.13% 0.11%L99.29% 0.06%": "VariancesFor small models like the small (S) model for MNIST,which has only 16 kernels in the first layer, variance due to the fixedconnectivity can become a significant concern. Thus, for the smallmodels we train multiple models simultaneously, and use a validationset of 10 000 images that we hold-out from the training set (not the testset), and based on which we select the final models. We present thevariations before this selection between individual model in .We can see that with increasing model size, the variance decreases.",
  "input channels83.53%1523": "To demonstrate the importance ofthe provided architectural choices,we provide an ablation study in. Here, we observe that us-ing trees, residual initializations,as well as or pooling are integralto the performance of convolu-tional LGNs. We also provide anablation wrt. model depth. Starting with the model depth ab-lation, in , we can observethat the performance improveswith increasing model depth. Weobserve that decreasing the modeldepth is detrimental to perfor-mance. We note that shallower",
  ": Residual initializations (green) drastically stabi-lize training of the LogicTreeNet compared to Gaussianinitialization (orange)": "Next, we consider the omission of or pool-ing. We can observe that the accuracy dropsby 3.5% when removing or pooling, demon-strating its importance. Setting weight decayto 0 causes a small reduction in accuracy by1%. Allowing each tree to use 8 channels asthe input, rather than just 2, reduces the ac-curacy (1.4%) because it is better to enforcethe ability to perform comparisons within onechannel at different x, y locations in the ker-nel. However, the more important effect ofusing only 2 input channels is the resultingimproved routing in hardware design layouts. Finally, we ablate the proposed residual initializations. We can observe in the table that the accuracydrops by almost 9% without residual initializations. This means the that the Gaussian initializationare almost unusable for such deep networks. In , we display the test accuracy during trainingand observe that, without our residual initializations, training does not converge and is quite unstable. We further ablate the effect of residual initialization on the distribution of gates in . Here,we can observe that residual initializations not only stabilize training, but also lead to the favorableinductive bias of many gates being the A, which is automatically reduced during logic simplification. AB (AB) A (AB) B AB AB (AB) (AB) B AB A AB (AB)",
  "Conclusion": "In this paper, we introduced convolutional differentiable logic gate networks with logic gate treekernels, integrating a range of concepts from machine vision into differentiable logic gate networks. Inparticular, we introduced residual initializations, which not only reduces loss of information in deepernetworks, but also prevents vanishing gradients, enabling training of deeper LGNs than previouslypossible. Further, we introduced logical or pooling, which, combined with logic tree kernels,substantially improved training efficiency. Our proposed CIFAR-10 architecture, LogicTreeNet,decreases model sizes by factors of 29 compared to the SOTA while improving accuracy. Further,our inference stack demonstrates that convolutional LGNs can be efficiently executed on hardware.For example, on MNIST, our model improves accuracy while achieving 160 faster inference speed,and on CIFAR-10, our model improves inference speed by 1900 over the state-of-the-art. Aninteresting direction for future research is applying convolutional differentiable logic gate networksto computer vision tasks with continuous decisions like object localization. We hope that our resultsmotivate the community to adopt convolutional differentiable LGNs, especially for embedded andreal-time applications where inference cost and speed matter most.",
  "and Disclosure of Funding": "This work was supported in part by the Federal Agency for Disruptive Innovation SPRIN-D. CB is sup-ported by the Land Salzburg within the WISS 2025 project IDA-Lab (20102-F1901166-KZP and20204-WISS/225/197-2019). SE is supported by the ARO (W911NF-21-1-0125), the ONR (N00014-23-1-2159), and the CZ Biohub. We thank the reviewers for their supportive and helpful comments.",
  "A. Gholami, S. Kim, Z. Dong, Z. Yao, M. W. Mahoney, and K. Keutzer, A survey ofquantization methods for efficient neural network inference, Computing Research Repository(CoRR) in arXiv, 2021": "T. Hoefler, D. Alistarh, T. Ben-Nun, N. Dryden, and A. Peste, Sparsity in deep learning:Pruning and growth for efficient inference and training in neural networks, The journal ofmachine learning research (JMLR), 2021. S. Liu, T. Chen, X. Chen, L. Shen, D. C. Mocanu, Z. Wang, and M. Pechenizkiy, Theunreasonable effectiveness of random pruning: Return of the most naive baseline for sparsetraining, in International Conference on Learning Representations (ICLR), 2022.",
  "I. Loshchilov and F. Hutter, Decoupled Weight Decay Regularization, in InternationalConference on Learning Representations (ICLR), 2019": "Y. Zhang, J. Pan, X. Liu, H. Chen, D. Chen, and Z. Zhang, Fracbnn: Accurate and fpga-efficient binary neural networks with fractional activations, in The 2021 ACM/SIGDA Interna-tional Symposium on Field-Programmable Gate Arrays, 2021. S. K. Esser, P. A. Merolla, J. V. Arthur, A. S. Cassidy, R. Appuswamy, A. Andreopoulos,D. J. Berg, J. L. McKinstry, T. Melano, D. R. Barch, et al., Convolutional networks for fast,energy-efficient neuromorphic computing, Proceedings of the National Academy of Sciencesof the United States of America, vol. 113, no. 41, p. 11 441, 2016. J. Zhan, X. Zhou, and W. Jiang, Field programmable gate array-based all-layer acceleratorwith quantization neural networks for sustainable cyber-physical systems, Software: Practiceand Experience, 2020. Y. Liu, Y. Chen, W. Ye, and Y. Gui, FPGA-NHAP: A General FPGA-Based NeuromorphicHardware Acceleration Platform With High Speed and Low Power, IEEE Transactions onCircuits and Systems I: Regular Papers, vol. 69, no. 6, pp. 25532566, 2022.",
  "Z. Tu, X. Chen, P. Ren, and Y. Wang, AdaBin: Improving Binary Neural Networks withAdaptive Binary Sets, in Proc. European Conference on Computer Vision (ECCV), 2022": "R. Gong, X. Liu, S. Jiang, T. Li, P. Hu, J. Lin, F. Yu, and J. Yan, Differentiable soft quantization:Bridging full-precision and low-bit neural networks, in Proc. International Conference onComputer Vision (ICCV), 2019. H. Qin, R. Gong, X. Liu, M. Shen, Z. Wei, F. Yu, and J. Song, Forward and Backward Infor-mation Retention for Accurate Binary Neural Networks, in Proc. International Conferenceon Computer Vision and Pattern Recognition (CVPR), 2020. H. Qiu, H. Ma, Z. Zhang, Y. Gao, Y. Zheng, A. Fu, P. Zhou, D. Abbott, and S. F. Al-Sarawi,Rbnn: Memory-efficient reconfigurable deep binary neural network with ip protection forinternet of things, IEEE Transactions on Computer-Aided Design of Integrated Circuits andSystems, 2022.",
  "A.2Training Details": "In , we summarize the hyperparameters for each model architecture configuration. We observethat the hyperparameter that depends the most on the data set is the learning rate . The temperature and thus the range of attainable outputs n/c/ has a minor dependence on the data set. We useweight decay only for the CIFAR-10 models as it does not yield advantages for the smaller MNISTmodels. We note that convergence, when training with weight decay, is generally slightly slower butleads to slightly better models. Models trained wiht weight decay tend to have more gates. : Hyperparameters for each model and data set: softmax temperatures , learning rates , weight decays ,and batch sizes bs. For reference to show the relationship to , we include the number of output neurons in thelast layer per class n/c. The range of attainable class scores is [0, n/c/].",
  "Outputs/classn/c1K8K32K64K80K1K4K8KMax scoren/c/51205117193182158146234": "For the loss, we use different softmax temperatures depending on the model size. We observetwo important relationships for choosing : (i) depends on the number of output neurons (largernumber of output neurons larger ) and (ii) depends on how certain the model will be aftertraining on the respective dataset, i.e., for a hard task with low accuracy, we should choose a larger ,while, for an easier task with a higher accuracy, we should choose a smaller . The reason for thisis that cross-entropy requires smaller logit variances if the model is less certain and requires largerlogit variances if a prediction is certain. A good rule of thumb during scaling is that the optimaltemperature is proportional to the square-root of the number of output gates ( n/c). For the CIFAR-10 B, L, and G models, we use a neural network teacher, supervising the classscores. When using a teacher on the class score level, a good rule of thumb is to increase the softmaxtemperature by a factor of",
  "A.2.1Memory Access Advantages through Fused Trees and Pooling": "Using logic gate trees as convolutional kernels with pooling allows for a substantial speedup duringtraining. For this, we fuse the entire tree as well as pooling into a single CUDA kernel operation.The reason behind this is two-fold, illustrated for the case of depth d = 3 and 2 2 pooling: (i)after reading the necessary 32 inputs, we perform 2 2 = 4 applications of the 7 learnable logicgates comprising the tree. Then, we apply the maximum t-conorm to pool the 4 outputs of the 4 treeapplications to a single output value. Here, we do not need to read the intermediate results frommemory but can instead keep them in registers. This prevents 28 additional memory read operations.(ii) as each set of 4 tree applications only has a single output after pooling, it is sufficient to write onlythis individual output (as well as the index of the pooling operation) to memory, saving 28 memorywrite operations, which are expensive. Further, this also reduces the memory footprint of training by afactor of around 10. This procedure requires recomputing selected intermediate values within eachblock during the backward pass; however, the memory access savings offset this small additionalcomputational cost.",
  "A.2.2Computational Requirements": "The typical training time per epoch for the L model on a single NVIDIA RTX 4090 GPU is 30 seconds.Noteworthy is that d = 3 kernels are typically bottlenecked by CUDA compute cores, whereas d = 2and d = 1 kernels are bottlenecked by memory bandwidth. While we explored d = 4 kernels, they(when fused) are very expensive (> 10) due to register pressure. Generally, but with very limitedexploration, simply going from d = 3 to d = 4 did not improve performance / gate. d = 4 kernelscan also be expressed, without fusing, using 2 logic gate tree layers; however, with this the memoryconsumption during training increases ( 10) which becomes a bottleneck.",
  ": MNIST model (M)": "For efficient hardware implementations, routing is an importantconsideration and the ability to place the LGN without conges-tion is paramount. Due to the sparsity of differentiable logic gatenetworks, limiting the choice of connections to a reasonable de-gree does not negatively affect accuracy. Accordingly, we selectconnections such that the model could be split into k/8 separatedmodels that are only recombined at the stage of output gates afteraccumulation, akin to using grouped convolutions with a con-stant number of groups throughout the network. This preventscongestions without reducing the accuracy. Additional routing re-strictions can straightforwardly be implemented in differentiableLGNs without incurring performance penalties.",
  "We illustrate the placement of our MNIST model (M) on a XilinxXC7Z045 FPGA in": "We developed scalable logic synthesis tools that simplify the logicgate networks after training. For example, for our large MNISTmodel (L), during training, the main network has 3 216 128 gates.After training, in the discrete LGN, many of these gates are trivial(e.g., A or constant) or not connected, and also further simplifications are possible. After logicsynthesis, the number of logic gates was 697 758. Herein, the full convolutional nature of the networkis maintained (and gates that have zero padded input, are still counted as full gates.) For the groupsum operation, we use a tree adder that asymptotically uses 7 gates per output.",
  ": Test accuracy of an MNIST model withdifferent choices of z3 for the residual initialization,in steps of 0.5. Averaged over 5 seeds": "In , we ablate the choice of z3, which isthe hyperparameter that indicates how strong theresidual initialization is applied. We illustrate theablation for an MNIST model. The model per-forms well, when z3 2 (z3 = 1.5 is included butreaches only 13%.) While z3 = 5 is not the optimalchoice for this particular model and training length,we have observed that, for larger models as well asfor longer trainings, larger z3 tend to be favorable.For example, on CIFAR-10, with the greater modeldepth, a z3 of 2 is too small and prevents training,so we generally recommend using z3 = 5.",
  "B.1BMACs in Logic": "When translating a BNN into an LGN for inference on FPGAs, BNNs require BMACs (multiply-accumulate), which are often expressed via an XNOR ((a b)) for the multiplication and a bitcountoperation for the accumulation. In the case of n input bits, the necessary n MAC can be expressedusing O(n) 8 n logic gates: n logic gates are necessary for the XNORs and 7n logic gatesfor the accumulating bitcount. Further, this process adds a delay of O(log n) to the logic circuit. Thismeans that a BMAC is not one logical operation but instead typically requires around 8 binary logicaloperations (not accounting for additional thresholding or potentially batch-normalization operations).",
  "B.2Floats in BNNs": "BNNs typically use 1-bit input activation and 1-bit weight quantization, but no output activationquantization . This means that only the multiplications within a matrix multiplication are binary,while the remainder of the respective architectures is typically non-binary . As in some residualBNN approaches , the residuals are not quantized, they require an additional FLOP (orinteger operation) overhead between the layers; due to the cost of FLOPs in logic gates (>1000binary OPs), effective deployment on a logic level has not been demonstrated for these unquantizedresidual approaches. Quantizing the residuals typically comes at a substantial accuracy penalty asdemonstrated, e.g., by Ghasemzadeh et al. . Thus, as these networks use full-precision residuals,a fair comparison is not applicable. Still, float-residual BNN approaches have important implicationsfor speeding up GPU inference if (i) large matrix multiplications are necessary and (ii) the savingsduring the binary matrix multiplication itself outweigh the costs of converting between bit-wiserepresentations and floats/integer representations; however, float-residual BNNs are not suitable forefficient logic gate based inference in hardware, e.g., on FPGAs or ASICs."
}