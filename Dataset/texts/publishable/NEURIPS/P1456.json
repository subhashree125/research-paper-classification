{
  "Abstract": "We introduce a generative model for protein backbone design utilizing geometricproducts and higher order message passing. In particular, we propose CliffordFrame Attention (CFA), an extension of the invariant point attention (IPA) archi-tecture from AlphaFold2, in which the backbone residue frames and geometricfeatures are represented in the projective geometric algebra. This enables to con-struct geometrically expressive messages between residues, including higher orderterms, using the bilinear operations of the algebra. We evaluate our architecture byincorporating it into the framework of FrameFlow, a state-of-the-art flow matchingmodel for protein backbone generation. The proposed model achieves high des-ignability, diversity and novelty, while also sampling protein backbones that followthe statistical distribution of secondary structure elements found in naturally occur-ring proteins, a property so far only insufficiently achieved by many state-of-the-artgenerative models.",
  "Introduction": "Recent years have shown tremendous progress in applying deep learning to computational chemistry,where applications of learning-based approaches have enabled unprecedented progress across abroad range of problems, such as molecular property prediction , protein-liganddocking , protein structure prediction , and de novo protein design .In case of protein design, state-of-the-art methods typically represent the structure of a protein ofN residues as an element of SE(3)N, i.e. as a collection of N frames, each of which describes theposition and orientation of an individual protein residue. Among the most successful methods arethose based on diffusion models and flow matching , whichmake use of architectures that incorporate the invariant point attention (IPA) of AlphaFold2 . Bymodeling a protein through the frames of its backbone, the task of protein structure generation isreformulated as to model the distribution of a set of frames, an inherently geometric problem. Advances in geometric deep learning have lead to architectures that are equivariant towards rotationsand translations , which can be regarded as a geometric inductive bias that enhancesperformance and data efficiency. Most generative models for protein design achieve equivariance byusing geometric features that are expressed in the canonical local coordinate frames representing theprotein backbone. The coordinates of these features are thus invariant with respect to global rotationsand translations, which allows to apply general layers and non-linearities.",
  "arXiv:2411.05238v1 [cs.LG] 7 Nov 2024": ": (A) Protein backbone residue with three backbone atoms represented by a coordinate frame.(B) In PGA, a frame can be represented via the geometric product of four planes. Two of the planesparameterize the frames rotation around their line of intersection, while the other two encode theframes translation along the separation vector between them. (C) An exemplary protein backbonestructure containing an -helix and a -sheet. Lines (red), planes (violet) and Euclidean frames(blue) can all be embedded as elements of PGA, facilitating a geometric inductive bias for learningrepresentations of the abstract geometry of the protein. Another widely used approach to construct equivariant architectures is to embed internal featuresin symmetry group representations and restrict neural network operations to equivariant functions. Explicit equivariance in these models has usually been limited to theorthogonal group O(3), which contains rotations and reflections, while most models are insteadinvariant towards translations. Only recently, Brehmer et al. proposed an architecture based onthe framework of Ruhe et al. that enables explicit equivariance towards both translations androtations by utilizing the projective geometric algebra (PGA) . Inspired by their work, we demonstrate that apart from its use as E(3)-equivariant formalism, PGAprovides a powerful framework for representing the geometry of protein backbones when incorporatedinto the local frame formulation of protein design architectures. While we utilize PGA to explicitlyrepresent the frames of the protein backbone as elements of the algebra, PGA additionally providesa strong inductive bias as its elements can represent abstract geometric objects like points, linesand planes, which are well suited to capture the geometry of secondary structure elements, such as-helices and -sheets (). Moreover, bilinear operations of the algebra enable to computemany geometric relations between those objects, such as distances, angles, projections and incidences. Main contributions:We introduce Clifford frame attention (CFA), an extension of the invariantpoint attention (IPA) architecture of AlphaFold2 by expressing geometric features as elementsof the projective geometric algebra and using its bilinear operations to construct geometricallyexpressive, higher order messages. We incorporate CFA into an existing flow matching frameworkfor protein backbone generation, FrameFlow , and demonstrate in our experiments that theproposed method achieves state-of-the-art performance in the combination of designability, diversityand novelty of generated protein samples. While, especially for small proteins, other models withhigh designability often over-represent -helices, the proposed method captures the broad distributionof secondary structures of naturally occurring proteins, which we believe is crucial for designingproteins with vast functionalities. Notably, since IPA is widely used throughout the field, CFA maybe readily included in many other protein-related machine learning models.",
  "Related Work": "Geometric (Clifford) algebra in neural networksNeural networks that use the Clifford algebrawere first proposed by Pearson and Bisset , an extension of the multi layer perceptron (MLP) byClifford algebras, which was later studied further by Buchholz and Sommer . More recently, Ruheet al. propose Geometric Clifford Algebra Networks, using geometric (Clifford) algebrasbased on their Clifford Neural Layers , and extend this framework in to E(3)-equivariant",
  "representations. Brehmer et al. propose to use projective geometric algebra, which enablesSE(3)-equivariant feature representations, and the representation of frames as elements of the algebra": "Generative models for protein designWatson et al. propose RFdifffusion, a generativemodel for protein backbone design that utilizes the pre-trained protein structure prediction networkRoseTTAFold . Yim et al. propose FrameDiff, which defines a diffusion model over a setof frames, SE(3)N, and extend this method within the flow matching framework . Bose et al. propose FoldFlow and its variants, diffusion and flow matching models over SE(3)N. Lin andAlQuraishi propose an equivariant encoder and decoder architecture. Wu et al. train atransformer model to predict angles between adjacent residues. Mao et al. propose vector fieldnetworks, which is also an extension of IPA, and employ them in FrameDiff. The main differenceto our approach is that VFN uses virtual atoms as geometric features and vector field operators tomodel interactions, whereas CFA use multivectors and the geometric bilinears of PGA respectively.",
  "Geometric Algebra": "A Clifford algebra over a real vector space, typically referred to as geometric algebra, is a power-ful mathematical framework for describing geometric objects including points, lines, planes andoperations on these objects in an algebraically concise way . In more technical terms, given a vector space V and a quadratic form q : V F from the vectorspace to the underlying field F, we can construct a geometric algebra as the unitary, associative,non-commutative algebra with the property v2 = q(v) for every v V . v2 = vv denotes thegeometric product, the bilinear operation of the algebra, of the vector v with itself. In the geometriccontext, q may be thought of as the metric of the vector space, meaning that for a vector v, q(v) is itssquared norm. Elements of the algebra are called multivectors. They can be constructed by forminggeometric products between basis vectors of V and linearly combining them. In this paper we are mainly interested in the projective geometric algebra (PGA) , which wedenote as G3,0,1. It is the geometric algebra over V = R4 with vector basise0, e1, e2, e3 and q(e1) = q(e2) = q(e3) = 1, q(e0) = 0 .(1)The full algebra has 16 basis elements, which can be grouped in grades according to the number ofvector basis elements they are constructed from. Following Gunn , we can interpret elementsof different grades as geometric objects such as planes, lines and points in 3D space, as listed inTable A.1 and visualized in Figure A.3. Working in a four dimensional space has the advantage thatwe are able to represent lines and planes that do not necessarily include the origin, which is crucialfor the description of translations as shown below. Central to the proposed method is the fact that PGA allows the representation of elements of theEuclidean group E(3) as elements of the algebra. Given a plane p G3,0,1, an arbitrary geometricobject X G3,0,1 can be reflected across the plane via the sandwich product,",
  "X = pXp ,(2)": "where X is the grade involution that flips the sign of elements with odd grade. A vector in PGA maythus be interpreted both as a plane and as a reflection operator. One can extend this idea using theCartan-Dieudonn theorem, which states that any E(3) transformation can be represented by repeatedreflections. Two consecutive reflections through intersecting planes result in a rotation around the lineof intersection and two reflections through two parallel planes correspond to a translation along theseparation vector of the planes. The associated elements of PGA are obtained by taking the geometricproduct of the respective planes. We thus represent each residue frame of a protein by a multivector,a so called motor, corresponding to a rotation followed by a translation as shown in . Similarto eq. 2, a motor M G3,0,1 can be applied to an arbitrary element X G3,0,1 according to",
  "ddtt(x) = vt(t(x)),0(x) = x .(5)": "If the vector field vt is known, one can generate samples from p1 by sampling from the prior andintegrating the flow ODE. Lipman et al. showed that vt can be learned by regressing on a vectorfield ut(x|x1) that is conditioned on a data sample x1 p1, i.e. by minimizing the loss function",
  "t(x0|x1) (1 t)x0 + tx1 ,(7)": "from which ut(x|x1) can be obtained by forming the time derivative. As shown by , samplingfrom pt(xt|x1) in eq. 6 can then be realized by transforming a sample x0 from the prior p0 toxt t(x0|x1). This formulation in terms of conditional distributions allows to learn dynamicoptimal transport (OT) plans from prior to target using minibatch OT as shown by Tong et al. . The framework of CNFs and flow matching can be generalized for sampling from distributions onRiemannian manifolds M, such as SE(3)N, as described by Chen and Lipman . Then, theflow : M M is a diffeomorphism, and a vector field is learned as a smooth maput : M T M to the tangent space T M. A natural choice for the map t(x0|x1) on aRiemannian manifold is the connecting path with minimal length, the geodesic.",
  "Flow Matching for Protein Structures": "For sampling from the distribution of protein backbones, we represent each protein residue by aframe T = (r, x) SE(3) that corresponds to a translation x R3 and rotation r SO(3), thusthe domain for the flow matching process is M SE(3)N, as described in FrameDiff . Theframe for a given residue is defined through the backbone atoms [N, C, C], where the translation xis chosen as the displacement of the C atom relative to the origin, while the rotation r is constructedusing a Gram-Schmidt process on the vectors [C C, N C].",
  "t(T0|T1) = expT0(t logT0(T1)) .(8)": "We calculate the time derivative of the flow as described in FrameFlow and regress on it asin eq. 6. As prior p0, we choose a 3N-dimensional normal distribution with unit variance N(0, I)for translations and, for rotations, adopt the heuristic trick of using IGSO(3) during training andU(SO(3)) for inference from Yim et al. . Additionally, we use minibatch optimal transport with the equivariant cost function from .",
  "Ideally, the main goal of any generative model should be the creation of structurally diverse andnovel proteins, thereby maximizing the access to unseen structures and possibly functions. Since": "the functionality of proteins is grounded in their structure, it is crucial to extensively cover thebroad range of secondary structure elements and topologies found in natural proteins. Typically, theproperties of generated proteins are assessed by the scores of diversity and novelty, indicating howsimilar generated backbones are to each other, and to known native proteins, respectively. Whileexplicit analyses of secondary structure elements of generated proteins are often overlooked, suggest that diffusion models commonly generate redundant protein structures composed mostly of-helices, which raises the question of how much functionality can be hypothetically encoded inthese proteins. A key quality check for generated proteins is their biophysical consistency in the sense that theirsequence, indeed, folds into the intended structure . In the case of backbone generation, this istypically evaluated by predicting a sequence from the generated structure with the inverse foldingmodel ProteinMPNN . The obtained sequence is re-folded with ESMFold and if the resultingstructure aligns well with the original backbone, the latter is called designable . Thus, thedesignability metric measures consistency between well-established sequence and structure predictionmodels, which can be seen as a proxy for biophysical validity and practical realizability .",
  "Geometric Algebra Flow Matching for Protein Backbone Generation": "We propose a geometric algebra-based neural network architecture to predict the vector fields in a flowmatching process on SE(3)N and call this approach Geometric Algebra Flow Matching (GAFL). Theproposed neural network architecture is an extension of the one introduced in FrameDiff , wherewe replace its central component, the invariant point attention (IPA) block from AlphaFold2 byClifford Frame Attention (CFA), which we explain in more detail in the next paragraph. Input features are the noised frames Tt, pairwise spatial distances, positional encodings of absoluteand relative sequence positions, and the flow matching time t. As FrameDiff, we use self-conditioning.The network relies on a series of six blocks, in which the frames are updated consecutively to predictthe denoised backbone structure and from this the respective conditional vector fields. Each blockuses CFA to perform message passing between protein residues, in particular processing geometricinformation as given by geometric node features and the current set of frames. The SE(3) invariantoutput of CFA is fed through an MLP and a transformer and then used to predict frame updates.For a detailed explanation of the architecture, we refer to Appendix A.2.1.",
  "Clifford Frame Attention": "The original IPA mechanism uses geometric node features in the form of 3D points for the calculationof attention scores and as queries, keys and values, as described in Appendix A.2.2. The featuresare expressed in local coordinate frames, which allows the use of arbitrary layers without breakingequivariance. Messages between nodes are constructed as a linear combination of attention values,weighted by attention scores. While other generative models for protein design incorporate theoriginal version of IPA directly , we propose to enhance its geometric expressivity byperforming the following modifications. PGA features:We replace the point-valued attention values by multivectors, which can encodepoints, lines, planes and Euclidean frames as shown in . At the same time we decrease thenumber of channels to retain approximately the same number of parameters. Geometric messages:Instead of linearly combining geometric features in the message passingstep, we construct messages utilizing the geometric bilinear layer that was introduced as part of theGeometric Algebra Transformer in Brehmer et al. , effectively calculating geometric product andjoin, another bilinear operation of PGA (see Definition A.10), between the node features Vi and Vj(see Algorithm 4). These two operations are able to compute many geometric relations as detailed inAppendix A.1.1 and . The messages mh,pijfrom node i to node j can then be written as",
  "The operation is thus performed on a node level and scales linearly with the number of nodes": "Higher order message passing:We also incorporate higher order messages into the proposedarchitecture, that is, messages that depend on more than two nodes and are thus capable of describingrelationships beyond pairwise interaction. As in , we use bilinearity of geometric product and jointo construct higher order messages by multiplying aggregated two-body messages m and m, e.g.",
  "jkm(3)ijk(11)": "can be seen as sum of three-body messages m(3)ijk. In practice, we use a learnable linear combinationof versions of eq. 11 projected onto different multivector grades using the geometric product and thejoin, as described in algorithm 5. Frames as features:Since we embed both residue frames and geometric node features in G3,0,1,it is a natural idea to combine them on a node level such that they can interact via the geometricbilinears during message passing. To this end we compute relative frame transformations for all pairsand aggregate them with the attention weights,",
  "jaijT1i Tj .(12)": "We concatenate these with the remaining geometric features before the construction of geometricmessages and also pass them directly to the backbone update block using a residual connection. Thefull CFA algorithm is provided in Appendix A.2.3. Although PGA would in principle allow to construct a fully equivariant architecture () withoutusing local frames, we decide to keep the local frame formulation of IPA, since it allows to use moregeneral layers and non-linearities. Moreover, the common problem of ambiguous local frame choicesthat other architectures suffer from is not apparent in our case since backbone residues providea canonical, geometrically meaningful choice for the local frames. We discuss the equivariance ofGAFL in Appendix A.2.4.",
  "Experiments": "We train GAFL2 on a subset of the Protein Data Bank (PDB) dataset comprised of monomericprotein structures with up to 512 residues and perform extensive ablations on the smaller, curatedSCOPe dataset filtered by proteins with length of up to 128 residues (SCOPe-128) asin . A representative selection of designable protein backbones generated by GAFL trainedon the PDB dataset is illustrated in .",
  "Checkpoint Selection": "With the aim of finding a good balance between designability and secondary structure content inmind, we introduce a checkpointing criterion that takes secondary structure into account. We firsttrain the model for Ntrain epochs. Then, from epoch Ntrain to Ntrain + Nselect, we calculate the relativeoccurrence of -helices and -strands r and r of 100 generated proteins after each epoch. Wekeep the top k checkpoints in terms of secondary structure content deviation, which we define asdc |r r(ref)| + |r r(ref)| , with the training set as reference. Among those k checkpoints,we choose the checkpoint with the highest designability and filter by a threshold dc dmax. Thehyperparameters Ntrain, Nselect, dmax and k for the different training runs are listed in Appendix A.3.8.",
  "Metrics": "To assess the performance of a given model, we follow a well-established pipeline of self-consistencyevaluation . For each generated backbone, we design 8 candidate sequences with Protein-MPNN , which are subsequently refolded with ESMfold , and define scRMSD as the smallestRMSD between our generated backbone and the 8 refolded, aligned candidates. As in , wedefine designability as the fraction of generated samples with scRMSD < 2.0 . We also reportdiversity and novelty of the designable backbones as average TM-similarity scores within the setof generated backbones and with respect to the PDB correspondingly (see Appendix A.3.1 and ).To evaluate how well the secondary structure distribution of the training set is captured, we calculatethe average helix and strand content of all designable backbones using the DSSP algorithm .",
  "Baselines": "At the task of generating backbones of up to 300 residues, we compare GAFL trained on the PDBto the diffusion models RFdiffusion and FrameDiff and to the flow matching modelsFoldFlow and FrameFlow . At the time of submitting the paper, FrameFlow had notbeen trained on the PDB yet, however, we include it in our results as contemporary work. Further,we perform an ablation study for generating smaller backbones, where we compare GAFL modelsto the originally published FrameFlow model, which was trained on the SCOPe dataset withbackbones of up to 128 residues. While all of the baselines above incorporate the original IPA architecture, we also compare GAFL with VFN , in which an alternative modification of IPA isproposed. For all models considered, we generate backbones using published model weights and therespective default inference settings (Appendix A.3.2).",
  "Results": "We train GAFL for 15 days on two NVIDIA A100-80GB GPUs on the dataset used in FrameDiff,which comprises monomeric structures from the Protein Data Bank (PDB), filtered by a maximumlength of 512 and a maximum coil content of 50%, resulting in a total of around 25,000 backbones .As in FoldFlow, we evaluate GAFL and the respective baselines on the task of generating backbonesof lengths {100, 150, 200, 250, 300}. For GAFL, we use 200 inference timesteps (Figure A.9). In, we report the metrics described in .2 for each model together with the time neededto generate a single backbone of length 100 on an NVIDIA A100 GPU without batching.",
  "GAFL has state-of-the-art performance": "We find that GAFL can reliably generate designable, diverse and novel backbones while capturing thestatistical distribution of secondary structure elements of natural proteins. In all metrics considered,GAFL outperforms both variants of FoldFlow and is better or as good as FrameDiff. GAFL alsooutperforms FrameFlow, which was trained on the same dataset, in terms of designability and, atthe same time, achieves better secondary structure content. GAFLs designability is only matchedby RFdiffusion, which is not directly comparable since it relies on pre-trained model weights fromthe folding model RoseTTAFold and has around three times more parameters. For diversity,novelty and helix content, GAFL performs better than RFdiffusion. We also observe that GAFL andFrameFlow can generate backbones around three times faster than the other evaluated models. : (A) Performance of evaluated models in terms of designability and secondary structurecontent as a function of backbone length. 200 backbones were generated for each model at eachlength {60, 80, 100, 150, 200, 250, 300}. (B) Comparison of the secondary structure distributionsof backbones generated by GAFL and RFdiffusion from (A) to the PDB dataset filtered by therespective protein lengths along with the Wasserstein distance (WD) between the distributions. (C)Examples of designable backbones generated by GAFL for lengths 450 and 500. We also report TMscores of the backbones to the closest hit in the PDB database computed with FoldSeek. : Performance of GAFL and baseline models for the generation of 200 protein backbones foreach length in {100, 150, 200, 250, 300}. We report the metrics from .2, including standarderrors obtained by bootstrapping, and the time needed to generate a backbone of length 100. The bestvalues and values within the respective margin of error are bold.",
  "GAFL generates proteins with diverse secondary structures at various lengths": "Although protein design campaigns span a wide range of protein sizes , the primarygoal remains to encode maximum functionality into the smallest protein possible, driven by thegrowing costs of synthesis as protein size increases. Thus, we further assess designability andsecondary structure content of generated backbones as a function of their length (A). Forall lengths considered, backbones generated by GAFL, FrameFlow and RFdiffusion are highlydesignable while FrameDiff and FoldFlow struggle with the generation of long proteins. The lengthdependence of designability and secondary structure content for GAFL and FrameFlow is qualitativelysimilar; however, GAFL achieves overall better results (, Figure A.8). Crucially, we find thatfor generating proteins with less than 150 residues, GAFL is well suited as it is capable of generating highly designable backbones with a similar amount of -strands as naturally occurring proteins, whileRFdiffusion over-represents -helices. This is also reflected by the Wasserstein distances betweenthe secondary structure distribution of naturally occurring backbones and those generated by GAFLand RFdiffusion, respectively: Purely -helical proteins are over-represented by RFdiffusion, leadingto a higher Wasserstein distance of 0.35 compared to 0.25 for GAFL (B).",
  "GAFL can generate large proteins": "To compare GAFL with VFN , we evaluate it at generating five backbones for each lengthin {100, 105, . . . , 500}, some of which are portrayed in C. We find that, with a value of0.74, GAFL outperforms not only VFN (0.44) but also FrameFlow (0.64) and RFdiffusion (0.71) indesignability. However, GAFL and FrameFlow over-represent helices for large proteins (Table A.3).",
  "Ablation of CFA": "In order to investigate the effect of the proposed architectural changes, we conduct an ablation studyon the semi-manually curated SCOPe dataset , which clusters proteins by their sequence andstructural similarities ensuring that its entries are evolutionary and structurally non-redundant. Wefurther filter SCOPe by the length of up to 128 residues, which results in 3938 proteins. : Helix content and designabilities of90 model checkpoints sampled during three train-ing runs on the PDB dataset for GAFL and re-trained FrameFlow, respectively. For each check-point, we sample 40 backbones per length in{100, 150, . . . , 300}. We compare GAFL with FrameFlow and modelsfor which we leave out all proposed architecturalchanges or only higher order message passing,respectively, while scaling the width of the lay-ers such that the number of parameters remainsthe same. All models are trained with three dif-ferent random seeds for 6500 epochs on oneNVIDIA A100-80GB GPU, which takes around6 days, and evaluated by generating backbonesfor lengths between 60 and 128 as in . We find that GAFLs designability of 90.5% is8 percentage points higher than that of the pub-lished FrameFlow model trained on SCOPe(81.2%) as depicted in . If retrainedwith GAFLs training procedure, FrameFlowsarchitecture with original IPA achieves a des-ignability of 88.2%. Using PGA-valued featuresand higher order message passing as proposedin CFA increases the designability by 1.4 and2.3 percentage points, respectively. While thetraining procedure has a larger effect on the des-ignability, the improvement due to CFA can be considered to be significant since it persists acrossdifferent random seeds. This can also be observed in the distribution of designabilities of 30 check-points sampled during the training procedure (Figure A.10) described in .1.",
  "*Published model weights": "We find that GAFL generates the highest fraction of designable backbones, while also yieldingthe highest diversity. At the same time RFdiffusion has the best novelty and generates structureswith a secondary structure content which is closest to the PDB. The trend that GAFL outperformsRFdiffusion with respect to secondary structure content on small proteins but performs worse onlonger proteins is also already apparent from .",
  "Discussion": "Our results suggest that GAFL is one of the current state-of-the-art models for unconditional proteinstructure generation. GAFL outperforms the widely used, pre-trained model RFdiffusion in terms ofdiversity, novelty and inference time and is on par at designability. Remarkably, GAFL can achievethis performance without requiring pre-trained model weights while other non-pre-trained modelsoften lack designability or show a mode-collapse towards generating helical structures. Especially for generating small, highly designable backbones with distinct secondary structures,GAFL performs well, in particular better than RFdiffusion. Since in most protein design campaignsthe goal is to incorporate the desired functionality into the smallest protein possible while exploringa large structural space, we consider this advantage of GAFL to be highly relevant for futuredevelopments and real-world applications of generative models for proteins. Our ablation studies provide evidence that achieving high designability without over-representing-helices can be attributed to the replacement of IPA by the proposed CFA architecture. Since IPA isused in many current state-of-the-art architectures for backbone structure, CFA has the potential toenable improvements in many protein-related tasks. LimitationsWhile the proposed method GAFL achieves state-of-the-art performance in proteinbackbone generation, there is still room for improvement. We note that achieving high designabilitywithout compromising the diversity of secondary structures remains a grand challenge. GAFL doesnot perfectly capture the secondary structure distribution of natural proteins, especially for largeproteins. Further, unconditional protein generation can be regarded as suitable benchmarking taskfor protein design models, however, most applications require conditional sampling. GAFL can bereadily incorporated into existing frameworks for conditioning, e.g. on motifs or symmetry .",
  "Conclusion": "We introduced Geometric Algebra Flow Matching (GAFL), a flow matching model for proteindesign based on FrameFlow . GAFL relies on the proposed Clifford Frame Attention (CFA),an extension of the invariant point attention block from AlphaFold2 , by representing residueframes and geometric features of a protein in the projective geometric algebra. This enables the usageof the bilinear operations in the algebra to construct geometrically expressive messages betweenresidues. The experiments demonstrate that the resulting model is state-of-the-art in the combinationof the established metrics designability, diversity and novelty and performs notably well at generatingdesignable small backbones with distinct secondary structures, containing -strands in particular.Given the promising results of the extension of invariant point attention with geometric algebrapresented in this work, we look forward to exploring its benefits for other protein-related tasks. AcknowledgmentsThis study received funding from the Klaus Tschira Stiftung gGmbH (HITSLab). We acknowledge the National Academic Infrastructure for Supercomputing in Sweden (NAISS),partially funded by the Swedish Research Council through grant agreement no. 2021-29 for awardingthis project access to the Berzelius resource provided by the Knut and Alice Wallenberg Foundationat the National Supercomputer Centre. The authors acknowledge support by the state of Baden-Wrttemberg through bwHPC and the German Research Foundation (DFG) through grant INST35/1597-1 FUGG.",
  "David Baker and George Church. Protein design meets biosecurity. Science, 383(6681):349349, 2024": "David Baker, Susana Vzquez Torres, Melisa Benard Valle, Stephen P. Mackessy, Stefanie Men-zies, Nicholas R. Casewell, Shirin Ahmadi, Nick J. Burlet, Edin Muratspahic, Isaac Sappington,Max Overath, Esperanza Rivera de Torre, Jann Ledergerber, Andreas Hougaard Laustsen, KimBoddum, Asim K. Bera, Alex Kang, Evans Brackenbrough, Iara Cardoso, Edouard Crittenden,Rebecca Edge, Justin Decarreau, Robert J. Ragotte, Arvind Pillai, Mohamad H. Abedi, HannahHan, Stacey R. Gerben, Analisa Murray, Rebecca Skotheim, Lynda Stuart, Lance Stewart,Thomas Fryer, and Timothy Patrick Jenkins. De novo designed proteins neutralize lethal snakevenom toxins. Research Square, 2024. Ilyes Batatia, David P Kovacs, Gregor Simm, Christoph Ortner, and Gbor Csnyi. Mace:Higher order equivariant message passing neural networks for fast and accurate force fields.Advances in Neural Information Processing Systems, 35:1142311436, 2022. Simon Batzner, Albert Musaelian, Lixin Sun, Mario Geiger, Jonathan P Mailoa, MordechaiKornbluth, Nicola Molinari, Tess E Smidt, and Boris Kozinsky. E (3)-equivariant graph neuralnetworks for data-efficient and accurate interatomic potentials. Nature communications, 13(1):2453, 2022. Nathaniel R Bennett, Brian Coventry, Inna Goreshnik, Buwei Huang, Aza Allen, DionneVafeados, Ying Po Peng, Justas Dauparas, Minkyung Baek, Lance Stewart, et al. Improving denovo protein binder design with deep learning. Nature Communications, 14(1):2625, 2023.",
  "Helen M Berman, John Westbrook, Zukang Feng, Gary Gilliland, Talapady N Bhat, HelgeWeissig, Ilya N Shindyalov, and Philip E Bourne. The protein data bank. Nucleic acids research,28(1):235242, 2000": "Joey Bose, Tara Akhound-Sadegh, Guillaume Huguet, Kilian FATRAS, Jarrid Rector-Brooks,Cheng-Hao Liu, Andrei Cristian Nica, Maksym Korablyov, Michael M. Bronstein, and Alexan-der Tong. SE(3)-stochastic flow matching for protein backbone generation. In The TwelfthInternational Conference on Learning Representations, 2024. Johannes Brandstetter, Rob Hesselink, Elise van der Pol, Erik J Bekkers, and Max Welling.Geometric and physical quantities improve e(3) equivariant message passing. In InternationalConference on Learning Representations, 2022.",
  "Sven Buchholz and Gerald Sommer. On clifford neurons and clifford multi-layer perceptrons.Neural Networks, 21(7):925935, 2008": "Longxing Cao, Inna Goreshnik, Brian Coventry, James Brett Case, Lauren Miller, Lisa Kozodoy,Rita E Chen, Lauren Carter, Alexandra C Walls, Young-Jun Park, et al. De novo design ofpicomolar sars-cov-2 miniprotein inhibitors. Science, 370(6515):426431, 2020. John-Marc Chandonia, Lindsey Guan, Shiangyi Lin, Changhua Yu, Naomi K Fox, and Steven EBrenner. Scope: improvements to the structural classification of proteinsextended databaseto facilitate variant interpretation and machine learning. Nucleic acids research, 50(D1):D553D559, 2022.",
  "Taco Cohen. Equivariant convolutional networks. PhD thesis, Taco Cohen, 2021": "Gabriele Corso, Bowen Jing, Regina Barzilay, Tommi Jaakkola, et al. Diffdock: Diffusionsteps, twists, and turns for molecular docking. In International Conference on LearningRepresentations (ICLR 2023), 2023. J. Dauparas, I. Anishchenko, N. Bennett, H. Bai, R. J. Ragotte, L. F. Milles, B. I. M. Wicky,A. Courbet, R. J. de Haas, N. Bethel, P. J. Y. Leung, T. F. Huddy, S. Pellock, D. Tischer, F. Chan,B. Koepnick, H. Nguyen, A. Kang, B. Sankaran, A. K. Bera, N. P. King, and D. Baker. Robustdeep learning-based protein sequence design using ProteinMPNN. Science, 378(6615):4956,2022.",
  "Leo Dorst and Steven De Keninck. A guided tour to the plane-based geometric algebra pga.2020. URL": "Leo Dorst, Daniel Fontijne, and Stephen Mann. Geometric Algebra for Computer Science: AnObject-Oriented Approach to Geometry. Morgan Kaufmann Publishers Inc., San Francisco, CA,USA, 1st edition, 2007. ISBN 0123694655. Naomi K Fox, Steven E Brenner, and John-Marc Chandonia. Scope: Structural classificationof proteinsextended, integrating scop and astral data and classification of new structures.Nucleic acids research, 42(D1):D304D309, 2014.",
  "Charles G. Gunn. Projective geometric algebra: A new framework for doing euclidean geometry.arXiv preprint arXiv:1901.05873, 2020": "Charles R. Harris, K. Jarrod Millman, Stfan J. van der Walt, Ralf Gommers, Pauli Virtanen,David Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J. Smith, Robert Kern,Matti Picus, Stephan Hoyer, Marten H. van Kerkwijk, Matthew Brett, Allan Haldane, Jaime Fer-nndez del Ro, Mark Wiebe, Pearu Peterson, Pierre Grard-Marchant, Kevin Sheppard, TylerReddy, Warren Weckesser, Hameer Abbasi, Christoph Gohlke, and Travis E. Oliphant. Arrayprogramming with NumPy. Nature, 585(7825):357362, September 2020.",
  "David Hestenes and Garret Sobczyk. Clifford algebra to geometric calculus: a unified languagefor mathematics and physics, volume 5. Springer Science & Business Media, 2012": "John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ron-neberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin dek, Anna Potapenko, AlexBridgland, Clemens Meyer, Simon A A Kohl, Andrew J Ballard, Andrew Cowie, BernardinoRomera-Paredes, Stanislav Nikolov, Rishub Jain, Jonas Adler, Trevor Back, Stig Petersen,David Reiman, Ellen Clancy, Michal Zielinski, Martin Steinegger, Michalina Pacholska, TamasBerghammer, Sebastian Bodenstein, David Silver, Oriol Vinyals, Andrew W Senior, KorayKavukcuoglu, Pushmeet Kohli, and Demis Hassabis. Highly accurate protein structure predic-tion with AlphaFold. Nature, 596(7873):583589, August 2021. Wolfgang Kabsch and Christian Sander. Dictionary of protein secondary structure: patternrecognition of hydrogen-bonded and geometrical features. Biopolymers: Original Research onBiomolecules, 22(12):25772637, 1983. Leon Klein, Andreas Krmer, and Frank Noe. Equivariant flow matching. In A. Oh, T. Naumann,A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural InformationProcessing Systems, volume 36, pages 5988659910. Curran Associates, Inc., 2023. Andrew Leaver-Fay, Michael Tyka, Steven M Lewis, Oliver F Lange, James Thompson, RonJacak, Kristian W Kaufman, P Douglas Renfrew, Colin A Smith, Will Sheffler, et al. Rosetta3:an object-oriented software suite for the simulation and design of macromolecules. In Methodsin enzymology, volume 487, pages 545574. Elsevier, 2011.",
  "Sergey Ovchinnikov and Po-Ssu Huang. Structure-based protein design with deep learning.Current opinion in chemical biology, 65:136144, 2021": "Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, AndreasKopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style,high-performance deep learning library. In Advances in Neural Information Processing Systems.2019.",
  "David Ruhe, Johannes Brandstetter, and Patrick Forr. Clifford group equivariant neuralnetworks. In Thirty-seventh Conference on Neural Information Processing Systems, 2023": "David Ruhe, Jayesh K Gupta, Steven De Keninck, Max Welling, and Johannes Brandstetter.Geometric clifford algebra networks. In International Conference on Machine Learning, pages2930629337. PMLR, 2023. Vctor Garcia Satorras, Emiel Hoogeboom, and Max Welling. E(n) equivariant graph neuralnetworks. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th InternationalConference on Machine Learning, volume 139 of Proceedings of Machine Learning Research,pages 93239332. PMLR, 1824 Jul 2021.",
  "Joost Schymkowitz, Jesper Borg, Francois Stricher, Robby Nys, Frederic Rousseau, and LuisSerrano. The foldx web server: an online force field. Nucleic acids research, 33(suppl_2):W382W388, 2005": "Guillem Simeon and Gianni De Fabritiis. Tensornet: Cartesian tensor representations forefficient learning of molecular potentials. In A. Oh, T. Naumann, A. Globerson, K. Saenko,M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, vol-ume 36, pages 3733437353. Curran Associates, Inc., 2023. Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsuper-vised learning using nonequilibrium thermodynamics. In International conference on machinelearning, pages 22562265. PMLR, 2015. Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, andBen Poole. Score-based generative modeling through stochastic differential equations. arXivpreprint arXiv:2011.13456, 2020. Nathaniel Thomas, Tess Smidt, Steven Kearnes, Lusann Yang, Li Li, Kai Kohlhoff, and PatrickRiley. Tensor field networks: Rotation- and translation-equivariant neural networks for 3d pointclouds. arXiv preprint arXiv:1802.08219, 2018. Alexander Tong, Kilian Fatras, Nikolay Malkin, Guillaume Huguet, Yanlei Zhang, Jarrid Rector-Brooks, Guy Wolf, and Yoshua Bengio. Improving and generalizing flow-based generativemodels with minibatch optimal transport. arXiv preprint arXiv:2302.00482, 2024. Brian L Trippe, Jason Yim, Doug Tischer, David Baker, Tamara Broderick, Regina Barzilay,and Tommi Jaakkola. Diffusion probabilistic modeling of protein backbones in 3d for themotif-scaffolding problem. arXiv preprint arXiv:2206.04119, 2022. Michel Van Kempen, Stephanie S Kim, Charlotte Tumescheit, Milot Mirdita, Jeongjae Lee,Cameron LM Gilchrist, Johannes Sding, and Martin Steinegger. Fast and accurate proteinstructure search with foldseek. Nature Biotechnology, 42(2):243246, 2024.",
  "Guido Van Rossum and Fred L Drake Jr. Python reference manual. Centrum voor Wiskunde enInformatica Amsterdam, 1995": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural informationprocessing systems, 30, 2017. Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, DavidCournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, Stfan J.van der Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, AndrewR. J. Nelson, Eric Jones, Robert Kern, Eric Larson, C J Carey, Ilhan Polat, Yu Feng, Eric W.Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A.Quintero, Charles R. Harris, Anne M. Archibald, Antnio H. Ribeiro, Fabian Pedregosa, Paulvan Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for ScientificComputing in Python. Nature Methods, 17:261272, 2020.",
  "Omry Yadan. Hydra - a framework for elegantly configuring complex applications. Github,2019. URL": "Jason Yim, Andrew Campbell, Andrew YK Foong, Michael Gastegger, Jos Jimnez-Luna,Sarah Lewis, Victor Garcia Satorras, Bastiaan S Veeling, Regina Barzilay, Tommi Jaakkola, et al.Fast protein backbone generation with se (3) flow matching. arXiv preprint arXiv:2310.05297,2023. Jason Yim, Brian L Trippe, Valentin De Bortoli, Emile Mathieu, Arnaud Doucet, ReginaBarzilay, and Tommi Jaakkola. Se (3) diffusion model with application to protein backbonegeneration. arXiv preprint arXiv:2302.02277, 2023. Jason Yim, Andrew Campbell, Emile Mathieu, Andrew Y. K. Foong, Michael Gastegger,Jose Jimenez-Luna, Sarah Lewis, Victor Garcia Satorras, Bastiaan S. Veeling, Frank Noe,Regina Barzilay, and Tommi Jaakkola. Improved motif-scaffolding with SE(3) flow matching.Transactions on Machine Learning Research, 2024. ISSN 2835-8856.",
  "General construction of Clifford algebras": "A Clifford algebra can be constructed from a vector space V by extending it with an additionalbilinear operation called the geometric product. We write this algebra as G(V ). Elements of thealgebra A G(V ) are called multivectors and are written in bold. In cases where we want tohighlight that an algebra element coincides with a vector in V , we write it with a (bold) lower caseletter. Other algebra elements are generally denoted by upper case letters. The geometric product hasto fulfill the following properties :",
  "{1, e1, . . . , en, e1e2, e1e3, . . . , e1 . . . en}": "In general, a geometric algebra over an n dimensional vector space will have 2n basis vectors. Ageneral multivector can be written as a linear combination of these basis vectors. The basis elementscan be further categorized into so called grades according to the number of basis vectors they contain,i.e. 1 will be of grade 0, ei of grade 1, eiej of grade 2 and so on. With this definition it is possible todefine the projection of an arbitrary multivector onto a specific grade .",
  "which selects the k-th grade of a given multivector": "Multivectors that contain only elements of one grade receive specific names according to their grade.Elements of grade 1 are vectors, elements of grade 2 are bivectors, elements of grade 3 are trivectorsand so on. The metricSo far we have only specified that the square of a vector should yield a scalar. In orderto uniquely define a geometric algebra it is important to specify the exact values of these scalars,which will define a metric on the algebra. The choice of metric is crucial for the properties of thealgebra and the geometric interpretation of its elements. For an algebra over an n-dimensional vectorspace we may choose n scalars, one for each basis vector. It is common to work with a metric thatassigns 1 or -1 to all non-zero values. This convention leaves {1, 0, 1} as possible choices for thescalars. One defines Gn,m,l as the geometric algebra with n basis vectors squaring to 1, m basis vectors squaring to 1 and l basis vectors squaring to 0. Having fixed a metric for the vectors of thealgebra we can go on to discuss the construction of a norm for general multivectors. The definition ofthe norm in geometric algebra uses the concept of reversion which is defined as follows:",
  "The Euclidean geometric algebra G3": "One of the most prominent examples for a geometric algebra is the Euclidean geometric algebraG3, which is the geometric algebra over R3 with the standard Euclidean metric, i.e. a vector basis{e1, e2, e3} fulfilling e21 = e22 = e23 = 1. Apart from the geometric product we can define twoadditional operations that will be useful for further analysis.",
  "(a + b)2 a2 b2 = a b.(18)": "Since all terms on the right side are scalars due to the third property in Definition A.1, we canconclude that the inner product of two vectors is a scalar itself. In fact, 18 corresponds to the wellknown polarization identity from linear algebra, and since we defined the norm of vectors to bethe standard Euclidean norm, the inner product from Definition A.5 is the standard Euclidean innerproduct.",
  "always assume an orthogonal basis. As a consequence of 17 for i = j basis elements anticommuteeiej = ejei and the geometric product is equal to the outer product eiej = ei ej": "Geometric interpretation of the algebra elementsAs the name geometric algebra suggests, itis possible to assign geometric meaning to the algebra elements with grade greater than 0. Gradeone elements or vectors keep their usual geometric interpretation as directed line segments. To findan interpretation for higher grade elements, one can observe that the outer product of two vectorsA = a b defines a homogeneous subspace in the sense that for every vector in the span of a and bthe following equation holds",
  "x A = span {a, b} x A = 0,(19)": "as shown in . Additionally A has a magnitude according to Definition A.4 and also an orientationdue to the antisymmetry of the outer product. For example e1 e2 has opposite orientation comparedto e2 e1 = e1 e2 as indicated by the relative minus sign. 2-blades, that is multivectors whichcan be written purely as the outer product of two vectors, can thus be interpreted as oriented areas,which lie within the subspace spanned by their generating vectors and area equal to the norm of theblade. Analogously 3-blades, multivectors equal to the outer product of three vectors, correspond tooriented volumes. Notably like vectors, 2-blades and 3-blades neither have a position in space nor dothey have a specified shape. The only fixed properties are the magnitude of their area/volume as wellas their orientation. The geometric interpretation of algebra elements is visualized in A.1. Grades of the same dimensionality are closely related by a relation called duality. Graphicallyspeaking, a plane in 3D space for example can be represented both as the plane itself or by the normalvector that is orthogonal to it. In algebraic terms this translates to e.g. e3 representing the normalvector of the plane described by the bivector e1e2. One says that these two elements are dual toeach other. In the same way, scalars are dual to trivectors since trivectors, like scalars, have only onedegree of freedom in three dimensions and are thus also sometimes called pseudoscalars. Formallyduality in G3 can be defined as follows:",
  "a b2 = ab| sin()|,(22)": "where is the angle enclosed by a and b. This is exactly the area of the parallelogram spanned bythe two vectors. Similar calculations can be performed for the case of 3-blades to show that theirmagnitude is equal to the volume of the parallelepiped spanned by their three generating vectors.These findings further support the idea of interpreting bivectors and trivectors as area and volumeelements respectively. Orthogonal transformationsAnother important class of operations which geometric algebra candescribe in an efficient way are orthogonal transformations, that is transformations T : G3 G3which preserve the inner product between vectors. In group theoretic terms, these transformationsform the group O(3). In order to construct such transformations we first look at how reflections arehandled in G3.",
  "V.(30)": "The property that the outer product of the transformed vectors is equal to the transformed outer productmakes any orthogonal transformation a so called outermorphism. As a consequence, orthogonaltransformations preserve the grade of blades, which geometrically translates to the fact that vectorsare transformed to vectors, bivectors to bivectors and so on, which is what we would expect from ageometric transformation (a vector will not become a volume when rotated). A formal proof of thisproperty can be found in . The multivector V in 29 is called a versor. Versors together with the geometric product form a groupon their own, the so called Pin(3) group. This group is said to be the double cover of O(3) (see). 30 is both a representation of O(3) and Pin(3). One can generalize this definition to arbitrarygeometric algebras in the following way: Definition A.8: The Pin(n,m,l) and Spin(n,m,l) groupsLet V = {v Gn,m,l | v = 1} be the set of versors . Then V together with the geometricproduct forms the group Pin(n, m, l).Let V = {v G+n,m,l | v = 1} be the set of even versors, i.e. versors that only involveeven grades. Then V together with the geometric product forms the group Spin(n, m, l). The projective geometric algebra G3,0,1The Euclidean geometric algebra G3 allows for a powerful description of geometric objects in 3Dspace. However, it lacks the ability to describe absolute positions. For example, planes parametrizedby 2-blades are restricted to go through the origin, and vectors only describe a relative displacement,not points in space. These problems are solved by the projective geometric algebra (PGA), which isthe geometric algebra G3,0,1 with three Euclidean basis vectors squaring to 1, e21 = e22 = e23 = 1and one null vector squaring to 0, e20 = 0. We thus use an algebra based on a four dimensionalvector space to describe 3D space, similarly to what one does with homogeneous coordinates. In thefollowing we will denote PGA vectors which only contain a Euclidean part by a. There are multiple ways to interpret the different algebraic elements geometrically. Here, we willfocus on the plane-based approach as described in , since this yields the most useful descriptionof Euclidean motions as orthogonal transformations. Elements of grade 1 are interpreted not as pointsbut as planes, i.e. a vector of the form",
  "{1}scalar1{e0, e1, e2, e3}planes2{e0e1, e0e2, e0e3, e1e2, e1e3, e2e3}lines, vanishing lines3{e0e1e2, e0e1e3, e0e2e3, e1e2e3}points, vanishing points4{e0e1e2e3}pseudoscalar": "corresponds to a plane with (Euclidean) normal vector n and a distance from the origin. In turn, theEuclidean vectors e1, e2, e3 correspond to basis planes through the origin, whereas e0 is interpretedas the plane at infinity. Notably any multiple of 31 of the form (n + e0), R represents thesame plane. Similarly to the case of G3, we can make use of 19 to obtain a geometric interpretation for blades ofhigher grade. For 2-blades of the form L = a b, the subspace spanned by a and b corresponds toall planes which contain the line of intersection between the two original planes. It is thus sensibleto take L as representation of this line. Similarly, a 3-blade X = a b c corresponds to thesubspace spanned by all planes which contain the point of intersection of a, b, and c, and thus3-blades represent points in PGA. The logic behind this construction is visualized in A.3.",
  "PGA also contains additional classes of geometric objects which e.g. result from taking the outerproduct of parallel planes. For a further discussion of these, we refer to": "Meet and JoinWe have seen that the outer product in PGA can be used to calculate the intersectionof geometric objects. In this context it is thus also known as the so called meet. There is also anopposite operation called the join3, which, as the name suggests, e.g. maps two points to the linewhich passes through both of them. In order to properly define the join we first extend the concept ofduality from G3 to G3,0,1.",
  "As explained in , this definition is slightly different compared to Definition A.6 since in PGAe0 does not have a multiplicative inverse. However, the overall idea to map between the different": "3In some literature the roles of meet and join are actually reversed, i.e. the outer product takes the role of thejoin. This seeming contradiction can be explained by the fact that we follow the approach of interpreting vectorsas planes and not as points. subspaces of equal dimensionality is still the same. In practice the hodge dual maps a basis elementsimply to the element which contains all the basis vectors not present in the initial multivector (insome situations with an additional minus sign), e.g. e0e3 = e1e2 (confer for more details).Using the hodge dual one can define the join as follows.",
  "where 1 is the inverse hodge dual, which differs from the usual hodge dual by a sign forsome elements": "As mentioned earlier, the join can be viewed as being dual to the meet, linking geometric objectstogether instead of finding their incidence. In that way, the join of two points is the line connectingboth of them and the join of three points results in a plane containing all three points. In PGA a norm for multivectors can be defined analogously to Definition A.4. However, since PGAcontains the null vector e0, only half of the components of a general multivector contribute to thevalue of the norm. It is thus useful to define a second norm which depends on all of the componentswhich contain e0.",
  "p.(36)": "The crucial difference in comparison to G3 is that the reflecting plane is no longer restricted to gothrough the origin. This allows one to do consecutive reflections not only in intersecting planes, butalso in parallel planes. Two important special cases are the reflections in two intersecting planes andthe reflecions in two parallel planes as shwon in A.4. The first case corresponds to a rotation aroundthe line of intersection by an angle which is twice the angle enclosed by the planes. The second caseresults in a translation along the distance vector by an amount equal to twice the distance between theplanes (see ). Compositions of these two types of transformations make up the special Euclidean group SE(3),which can be seen as the group of rigid body motions. This means that every Euclidean transformationwhich does not involve reflections can be embedded as an element of the even subalgebra G+3,0,1 ={A G3,0,1 | Ak = 0, k odd}, as stated in the following theorem:",
  "X = MXM,(37)with an arbitrary multivector X G3,0,1, is a representation of T on G3,0,1": "We emphasize that the expression in 37 can be applied to any geometric object which is representablein the algebra, i.e. it does not matter if X is a point, a vector, or a plane; the correct equation totransform it always takes the above form. This remarkable property can also be found for otherimportant operations in PGA. In fact, we have already mentioned the meet operation which allows tocalculate intersections of arbitrary geometric objects A, B G3,0,1 via the universal formula A B,as well as the join which can likewise be applied to pairs of arbitrary objects. Metric RelationsWe have already seen how to transform objects, find incidences between them,and join them together. Furthermore the basic operations of PGA allow to calculate a host of differentmetric relations between its elements. Below we provide a non-complete list of examples.",
  "Edge Update": "Figure A.5: High level overview over the GAFL architecture. The architecture was adapted fromFrameDiff/FrameFlow . We replaced invariant point attention (IPA) with Clifford frameattention (CFA) and added geometric node features G. We adapted the GAFL architecture from FrameDiff/FrameFlow and replaced invariant pointattention (IPA) with Clifford frame attention (CFA) (see Algorithm 3). Furthermore we introducedgeometric node features Gi that are used in the prediction of geometric attention values and backboneframe updates. An overview over the architecture is shown in Figure A.5. The backbone update blockis presented in Algorithm 6. For details on the edge update we refer to .",
  "In the following we provide a short overview over the IPA architecture": "Invariant point attention uses local frames to construct the messages between nodes, where thecoordinate frames are given by the frames of the individual residues. The full procedure is presentedin algorithm 1 and visualized in A.6. The part of IPA that processes geometric information (red nodesin A.6) can be summarized as follows: 1. Learn a certain number of local vector valued queries, keys and values qi,ki, vi.2. In the calculation of attention scores, transform the local queries and keys into the globalframe via Ti qi and calculate the squared distance between them. Importantly they shouldbe thought of as points in 3D space rather than vectors, meaning that they change under",
  ". Finally, concatenate the output vectors along with their norm with the remaining scalaroutput features and put them through a final linear layer": "The update of the backbone frames, which is used in conjunction with IPA e.g. in , isaccomplished by learning an SE(3) transformation per frame which is concatenated with the currentframe representation. To this end, the network predicts a quaternion for the rotation and a translationvector from the scalar node features si as shown in algorithm 2",
  "siGi": "Figure A.7: Overview of Clifford frame attention. Blue nodes represent layers which process scalarinformation, red nodes represent layers which process point valued information and orange nodesrepresent layers which process features in the PGA. The central innovation is the novel constructionof geometric messages, summarized in the grey box. To retain readability we omit the calculation ofrelative frame transformations in the chart.",
  "Although we do not need this property for enforcing equivariance, we use it for its parameterefficiency and geometric inductive bias": "4. We construct implicit higher body messages inspired by MACE . Bilinearity of thegeometric product and the join means that repeated products of aggregated two-bodymessages correspond to a sum of N-body messages as shown in eq. 11. In algorithm 5, weuse the geometric product and the join to construct three-body messages from aggregatedtwo-body messages, stored in node features A.",
  ". In addition to the Euclidean norm we also compute the infinity norm (see Definition A.11)of multivector features after message passing": "6. In the Backbone update step (Algorithm 6), we concatenate scalar and geometric featuresalong the feature dimension and pass them through an MLP to predict multivectors R and Sthat parameterize a rotation and translation respectively. These are then multiplied with thecurrent frames to compute frame updates. The main difference to the backbone update asused in is the inclusion of geometric features and the use of the MLP. The predictionof rotor and translator instead of a rotation matrix and translation vector is just a matter ofrepresentation.",
  "T 1i T 1global Tglobal Tj vhpj= T 1i Tj vhpj": "In GAFL, we do not modify the calculation of attention scores from IPA, which means that theirinvariance remains ensured. During message passing, we use the same construction as above (see line11 of Algorithm 3), but use a different representation of SE(3), namely multivector features insteadof point features. The choice of representation, however, does not influence the invariance of thewhole expression. Also the relative frame transformations T1i Tj we compute are invariant. Allsubsequent layers including the GeometricBilinear layer and the ManyBodyProduct layer operateexclusively on invariant node features, hence overall equivariance is retained throughout those layers.",
  "i=1maxjTM(xi, xj)": "with n the number of all designable samples and xj being the sample from the PDB database withthe highest TM score to the sample xiSince diversity and novelty are defined as averaged similarity scores, small values are desirable forthe generation of structurally distinct and de novo proteins. Table A.2: Performance of different models when generating 10 backbones for each length in{60, 61, . . . , 128}. For strand and helix content, we choose the PDB dataset filtered for lengths 60 to128 as reference. FrameFlow denotes the model trained on the PDB published in .",
  "A.3.2Baselines used for comparison with GAFL": "We generate backbones using RFdiffusion relying on publicly available weights with default settings(noise_scale_ca: 1) (GitHub RFdiff). For Genie, we use the model with the published weightstrained either on the SCOPe or SwissProt datasets containing proteins of up to 128 or 256 residueslong(GitHub Genie), and generate backbones with sampling for 1000 time steps, as it has demon-strated the best performance.FrameDiff was used with the newest published model weights(best_weights.pth) with sampling for 500 timesteps (GitHub FrameDiff). FrameFlow was usedwith the published weights trained on the same dataset as GAFL (GitHub FrameFlow). The originallymodel, denoted as FrameFlow can be found at (GitHub FrameFlow (legacy)). We note that theGitHub repository supporting FrameFlow contains an implementation of minibatch OT , whichwe use for retraining FrameFlow. For FoldFlow we use the optimal transport model (foldflow-ot.pth)with inference annealing as suggested by and sampling for 500 timesteps (GitHub FoldFlow).",
  "A.3.3Results for small proteins": "In order to evaluate GAFLs performance for small proteins in particular, we evaluate a range ofmodels in the setting from the original FrameFlow paper, where 10 backbones per length in{60, 61, . . . , 128} are generated. Apart form the models in , we also evaluate Genie ,which was trained on backbones smaller than 300 residues and were therefore excluded from theevaluation in . We find that GAFL and FrameFlow are the only models capable of generating highly designablebackbones with diverse secondary structures for the considered length range, where GAFL outper-forms FrameFlow in designability and secondary structure content. While FoldFlow and RFdiffusion,like GAFL, achieve designabilities over 90%, they generate backbones with 0% and 7% -strandcontent, respectively, indicating a mode-collapse towards generating -helical structures. GAFLalso outperforms both RFdiffusion and FoldFlow in terms of diversity and novelty. All other modelsconsidered have significantly lower designability. For Genie in particular, we observe good noveltybut it has the lowest designability among the models considered and also under-represents betastrands.",
  "A.3.4Results for long proteins": "We also evaluate the performance of GAFL for generating proteins up to length 500, which enables tocompare GAFL against vector field networks (VFN) , which at the time of writing have no pub-lished model weights. We thus evaluate GAFL and other baselines trained on the PDB with the sameinference settings as in , sampling 5 protein backbones for each length in {100, 105, . . . , 500}.We report the results in Table A.3. For VFN, we can only report on designability since havedifferent definitions for diversity and novelty respectively and do not calculate the secondary structurecontent of generated structures.",
  "A.3.5Ablation for CFA on the PDB": "As described in .5, we retrained FrameFlow in the same setup as GAFL with defaulthyperparameters from the repository in A.3.2. For both FrameFlow and GAFL, we performedthree training runs with the hyperparameters reported in Appendix A.3.8. For all runs we selectedcheckpoints using our checkpointing criterion, described in .1. In , we report theperformance of the best checkpoints per run. As extension of , we directly compare the designability of GAFL and the published Frame-Flow model by length in Figure A.8. We find that GAFL has higher designability for lengths smallerthan 200 and greater than 250. Since we only generate 200 backbones for each length, the margins oferror of the individual designabilities overlap, except for length 300. The total designability of GAFL,averaged over all 1000 backbones, however, is significantly better than that of FrameFlow (). Figure A.8: Designability of GAFL and the published FrameFlow model as a function of length withstandard errors obtained by bootstrapping the set of generated samples. 200 backbones are generatedfor each length, as in .",
  "A.3.6Timestep analysis of GAFL": "We perform a timestep analysis, evaluating the best GAFL checkpoint with a different number oftimesteps taken during inference, in order to judge its impact on the designability metric. To thisend we generate 100 backbones for each length in {100, 150, . . . , 300}, for different numbers oftimesteps. We see that the performance beyond 200 timesteps stays almost constant, suggesting that200 timesteps represents a good tradeoff between performance and inference speed. Figure A.9: Designability as a function of timesteps used during inference. 100 backbones for eachlength in {100, 150, . . . , 300} were sampled per each data point. Vertical lines denote the standarddeviation.",
  "A.3.8Training on PDB": "On the PDB dataset, we train both GAFL and FrameFlow for 5200 epochs, where one epoch isdefined as one iteration over all 4776 clusters, which we define as in FrameDiff by 30% sequencesimilarity. The learning rate is increased in 50 warmup steps to 0.0002 and then kept constant for3500 epochs. From there we use a cosine-annealing schedule to decrease the learning rate to 0.0001at epoch 5000. From epoch Ntrain = 5000 to Ntrain + Nselect = 5200 we employ our checkpointingcriterion described in .1 evaluating secondary structures and storing checkpoints everysecond epoch. We keep the k = 30 best checkpoints and filter them for checkpoints with a secondarystructure content deviation of less than dmax < 0.2.In order to select the best checkpoint we evaluate all filtered checkpoints by sampling 40 backbones foreach length in {100, 150, 200, 250, 300} and choosing the checkpoint with the highest designability.We then run new and independent inference runs for all experiments we conduct on the selectedcheckpoint.",
  "A.3.9Training on SCOPe (Ablation)": "On SCOPe, we train both GAFL and FrameFlow for 6500 epochs. We use a constant learningrate of 0.0001 for the whole procedure. From epoch Ntrain = 4000 to Ntrain + Nselect = 6500 weuse our ceckpointing criterion, storing checkpoints every 25 epochs. We keep the k = 10 bestcheckpoints also filtering them for checkpoints with a secondary structure content deviation of lessthan dmax < 0.2. We evaluate each filtered checkpoint by sampling 10 backbones for each length in{60,61,...,128} and visualize the resulting designability distribution in Figure A.10. Figure A.10: Left: Total train loss averaged over flow matching times 0.5 to 0.75 on SCOPe forGAFL and FrameFlow. Right: Designabilities of 30 checkpoints sampled during three training runson SCOPe for GAFL and retrained FrameFlow, respectively and filtered for a secondary structurecontent deviation of less than dmax < 0.2. For each checkpoint, we sample 10 backbones per lengthin {60,61,...,128}.",
  ". Claims": "Question: Do the main claims made in the abstract and introduction accurately reflect thepapers contributions and scope?Answer: [Yes]Justification: In the abstract and introduction we made the following claims, which accuratelyreflect the papers contributions and scope:(a) Represent frames as elements of the projective geometric algebra(b) Geometrically more expressive bilinear products of the geometric algebra(c) Higher order message passing(d) High designability and diversity of sampled protein backbones(e) The proposed method closely captures the distribution of protein secondary structures,while other models with high designability often over-represent alpha helices",
  "The answer NA means that the paper has no limitation while the answer No means thatthe paper has limitations, but those are not discussed in the paper": "The authors are encouraged to create a separate \"Limitations\" section in their paper. The paper should point out any strong assumptions and how robust the results are toviolations of these assumptions (e.g., independence assumptions, noiseless settings,model well-specification, asymptotic approximations only holding locally). The authorsshould reflect on how these assumptions might be violated in practice and what theimplications would be. The authors should reflect on the scope of the claims made, e.g., if the approach wasonly tested on a few datasets or with a few runs. In general, empirical results oftendepend on implicit assumptions, which should be articulated. The authors should reflect on the factors that influence the performance of the approach.For example, a facial recognition algorithm may perform poorly when image resolutionis low or images are taken in low lighting. Or a speech-to-text system might not beused reliably to provide closed captions for online lectures because it fails to handletechnical jargon.",
  "If applicable, the authors should discuss possible limitations of their approach toaddress problems of privacy and fairness": "While the authors might fear that complete honesty about limitations might be used byreviewers as grounds for rejection, a worse outcome might be that reviewers discoverlimitations that arent acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an impor-tant role in developing norms that preserve the integrity of the community. Reviewerswill be specifically instructed to not penalize honesty concerning limitations.",
  ". Experimental Result Reproducibility": "Question: Does the paper fully disclose all the information needed to reproduce the main ex-perimental results of the paper to the extent that it affects the main claims and/or conclusionsof the paper (regardless of whether the code and data are provided or not)?Answer: [Yes]Justification: Full explanation of the experiments is provided in and Ap-pendix A.3.2. The source code of the implementation will be published together withthe camera ready version of the paper.Guidelines: The answer NA means that the paper does not include experiments. If the paper includes experiments, a No answer to this question will not be perceivedwell by the reviewers: Making the paper reproducible is important, regardless ofwhether the code and data are provided or not.",
  "If the contribution is a dataset and/or model, the authors should describe the steps takento make their results reproducible or verifiable": "Depending on the contribution, reproducibility can be accomplished in various ways.For example, if the contribution is a novel architecture, describing the architecture fullymight suffice, or if the contribution is a specific model and empirical evaluation, it maybe necessary to either make it possible for others to replicate the model with the samedataset, or provide access to the model. In general. releasing code and data is oftenone good way to accomplish this, but reproducibility can also be provided via detailedinstructions for how to replicate the results, access to a hosted model (e.g., in the caseof a large language model), releasing of a model checkpoint, or other means that areappropriate to the research performed. While NeurIPS does not require releasing code, the conference does require all submis-sions to provide some reasonable avenue for reproducibility, which may depend on thenature of the contribution. For example(a) If the contribution is primarily a new algorithm, the paper should make it clear howto reproduce that algorithm.",
  "(b) If the contribution is primarily a new model architecture, the paper should describethe architecture clearly and fully": "(c) If the contribution is a new model (e.g., a large language model), then there shouldeither be a way to access this model for reproducing the results or a way to reproducethe model (e.g., with an open-source dataset or instructions for how to constructthe dataset). (d) We recognize that reproducibility may be tricky in some cases, in which caseauthors are welcome to describe the particular way they provide for reproducibility.In the case of closed-source models, it may be that access to the model is limited insome way (e.g., to registered users), but it should be possible for other researchersto have some path to reproducing or verifying the results.",
  "Answer: [Yes]": "Justification: No human subjects or data with privacy concerns was used. Training was onlyperformed on publicly available standard academic datasets. Societal impact is consideredmostly positive, as de novo protein design holds the promise to develop drugs againstdiseases and personalized therapies against cancer, which out-weights potential risks, whileof course security concerns remain, see e.g. Baker and Church(2024). The presentedmethod does not enable the design of proteins with a certain function, so risk of misuse isconsiderably low.",
  "Guidelines:": "The answer NA means that the paper does not use existing assets. The authors should cite the original paper that produced the code package or dataset. The authors should state which version of the asset is used and, if possible, include aURL. The name of the license (e.g., CC-BY 4.0) should be included for each asset. For scraped data from a particular source (e.g., website), the copyright and terms ofservice of that source should be provided. If assets are released, the license, copyright information, and terms of use in thepackage should be provided. For popular datasets, paperswithcode.com/datasetshas curated licenses for some datasets. Their licensing guide can help determine thelicense of a dataset.",
  ". Broader Impacts": "Question: Does the paper discuss both potential positive societal impacts and negativesocietal impacts of the work performed?Answer: [Yes]Justification: Societal impact is briefly discussed in Appendix A.4.1, while we refer thereader to e.g Baker and Church (2024) for recent discussions of potential risks. The presentedmethod does not enable the design of proteins with a certain function, so risk of misuse isconsiderably low.Guidelines: The answer NA means that there is no societal impact of the work performed. If the authors answer NA or No, they should explain why their work has no societalimpact or why the paper does not address societal impact. Examples of negative societal impacts include potential malicious or unintended uses(e.g., disinformation, generating fake profiles, surveillance), fairness considerations(e.g., deployment of technologies that could make decisions that unfairly impact specificgroups), privacy considerations, and security considerations. The conference expects that many papers will be foundational research and not tiedto particular applications, let alone deployments. However, if there is a direct path toany negative applications, the authors should point it out. For example, it is legitimateto point out that an improvement in the quality of generative models could be used togenerate deepfakes for disinformation. On the other hand, it is not needed to point outthat a generic algorithm for optimizing neural networks could enable people to trainmodels that generate Deepfakes faster. The authors should consider possible harms that could arise when the technology isbeing used as intended and functioning correctly, harms that could arise when thetechnology is being used as intended but gives incorrect results, and harms followingfrom (intentional or unintentional) misuse of the technology. If there are negative societal impacts, the authors could also discuss possible mitigationstrategies (e.g., gated release of models, providing defenses in addition to attacks,mechanisms for monitoring misuse, mechanisms to monitor how a system learns fromfeedback over time, improving the efficiency and accessibility of ML).",
  ". Safeguards": "Question: Does the paper describe safeguards that have been put in place for responsiblerelease of data or models that have a high risk for misuse (e.g., pretrained language models,image generators, or scraped datasets)?Answer: [NA]Justification: The presented method does not enable the design of proteins with a certainfunction, so risk of misuse is considerably low.Guidelines: The answer NA means that the paper poses no such risks. Released models that have a high risk for misuse or dual-use should be released withnecessary safeguards to allow for controlled use of the model, for example by requiringthat users adhere to usage guidelines or restrictions to access the model or implementingsafety filters.",
  "According to the NeurIPS Code of Ethics, workers involved in data collection, curation,or other labor should be paid at least the minimum wage in the country of the datacollector": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with HumanSubjectsQuestion: Does the paper describe potential risks incurred by study participants, whethersuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)approvals (or an equivalent approval/review based on the requirements of your country orinstitution) were obtained?Answer: [NA]Justification: No insitutional review board approval required.Guidelines:",
  "The answer NA means that the paper does not involve crowdsourcing nor research withhuman subjects": "Depending on the country in which research is conducted, IRB approval (or equivalent)may be required for any human subjects research. If you obtained IRB approval, youshould clearly state this in the paper. We recognize that the procedures for this may vary significantly between institutionsand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and theguidelines for their institution."
}