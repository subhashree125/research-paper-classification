{
  "Abstract": "Predicting low-energy molecular conformations given a molecular graph is animportant but challenging task in computational drug discovery. Existing state-of-the-art approaches either resort to large scale transformer-based models thatdiffuse over conformer fields, or use computationally expensive methods to gen-erate initial structures and diffuse over torsion angles. In this work, we introduceEquivariant Transformer Flow (ET-Flow). We showcase that a well-designedflow matching approach with equivariance and harmonic prior alleviates the needfor complex internal geometry calculations and large architectures, contrary tothe prevailing methods in the field. Our approach results in a straightforwardand scalable method that directly operates on all-atom coordinates with minimalassumptions. With the advantages of equivariance and flow matching, ET-Flowsignificantly increases the precision and physical validity of the generated con-formers, while being a lighter model and faster at inference. Code is available",
  "Introduction": "Generating low-energy 3D representations of molecules, called conformers, from the molecular graphis a fundamental task in computational chemistry as the 3D structure of a molecule is responsible forseveral biological, chemical and physical properties (Guimares et al., 2012; Schtt et al., 2018, 2021;Gasteiger et al., 2020; Axelrod and Gomez-Bombarelli, 2023). Conventional approaches to molecularconformer generation consist of stochastic and systematic methods. While stochastic methods suchas Molecular Dynamics (MD) accurately generate conformations, they can be slow, cost-intensive,and have low sample diversity (Shim and MacKerell Jr, 2011; Ballard et al., 2015; De Vivo et al.,2016; Hawkins, 2017; Pracht et al., 2020). Systematic (rule-based) methods (Hawkins et al., 2010;Bolton et al., 2011; Li et al., 2007; Miteva et al., 2010; Cole et al., 2018; Lagorce et al., 2009) thatrely on torsional profiles and knowledge base of fragments are much faster but become less accurate",
  "(b)": ": (a) Overview of ET-Flow. The model predicts a conditional vector field v using interpolatedpositions (xt), molecular structure (G), and time-step (t). Samples are drawn from the harmonicprior (x0 p0) and then rotationally aligned with the samples from data (x1 p1). A conditionalprobability path is constructed between pairs of x0 and x1, and xt is then sampled from this pathat a random time t. (b) The ET-Flow architecture consists of a representation module based on theTorchMD-NET architecture (Thlke and De Fabritiis, 2022) and an equivariant vector output module.For detailed architecture and input preprocessing information, see Section A.1.",
  "with larger molecules. Therefore, there has been an increasing interest in developing scalable andaccurate generative modeling methods in molecular conformer generation": "Existing machine learning based approaches use diffusion models (Ho et al., 2020; Song and Ermon,2019) to sample diverse and high quality samples given access to low-energy conformations. Priormethods typically fall into two categories: diffusing the atomic coordinates in the Cartesian space (Xuet al., 2022; Wang et al., 2024) or diffusing along the internal geometry such as pairwise distances,bond angles, and torsion angles (Ganea et al., 2021; Jing et al., 2022). Early approaches based on diffusion (Shi et al., 2021; Luo et al., 2021; Xu et al., 2022) facedchallenges such as lengthy inference and training times as well as having lower accuracy comparedto cheminformatics methods. Torsional Diffusion (Jing et al., 2022) was the first to outperformcheminformatics methods by diffusing only on torsion angles after producing an initial conformerwith the chemoinformatics tool RDKiT. This reliance on RDKiT structures instead of employing anend-to-end approach comes with several limitations, such as restricting the tool to applications wherethe local structures produced by RDKiT are of sufficient accuracy. Unlike prior approaches, thecurrent state-of-the-art MCF (Wang et al., 2024) proposes a domain-agnostic approach by learning todiffuse over functions by scaling transformers and learning soft inductive bias from the data (Zhuanget al., 2022). Consequently, it comes with drawbacks such as high computational demands due tolarge number of parameters, limited sample efficiency from a lack of inductive biases like euclideansymmetries, and potential difficulties in scenarios with sparse data a common challenge in thisfield. In this paper, we propose Equivariant Transformer Flow (ET-Flow), a simple yet powerful flow-matching model designed to generate low-energy 3D structures of small molecules with minimalassumptions. We utilize flow matching (Lipman et al., 2022; Albergo et al., 2023; Liu et al., 2022),which enables the learning of arbitrary probability paths beyond diffusion paths, enhancing bothtraining and inference efficiency compared to conventional diffusion generative models. Departingfrom traditional equivariant architectures like EGNN (Satorras et al., 2021), we adopt an EquivariantTransformer (Thlke and De Fabritiis, 2022) to better capture geometric features. Additionally, ourmethod integrates a Harmonic Prior (Jing et al., 2023; Stark et al., 2023), leveraging the inductive biasthat atoms connected by a bond should be in close proximity. We further optimize our flow matchingobjective by initially conducting rotational alignment on the harmonic prior, thereby constructingshorter probability paths between source and target distributions at minimal computational cost.",
  "Background": "Diffusion Generative Models. Diffusion models (Song and Ermon, 2019; Song et al., 2020; Hoet al., 2020) enables a high-quality and diverse sampling from an unknown data distribution byapproximating the Stochastic Differential Equation(SDE) that maps a simple density i.e. Gaussianto the unknown data density. Concretely, it involves training a neural network to learn the score,represented as x log pt(x) of the diffused data. During inference, the model generates sample byiteratively solving the reverse SDE. However, diffusion models have inherent drawbacks, as they (i)require on longer training times (ii) are restricted to specific probability paths and (iii) depend on theuse of complicated tricks to speed up sampling (Song et al., 2020; Zhang and Chen, 2022). Flow Matching. Flow Matching (Albergo et al., 2023; Lipman et al., 2022; Liu et al., 2022)provides a general framework to learn Continuous normalizing flows (CNFs) while improving upondiffusion models in simplicity, generality, and inference speed in several applications. Through simpleregression against the vector field reminiscent of the score-matching objective in diffusion models,Flow matching has enabled a fast, simulation-free training of CNFs. Several subsequent studies havethen expanded the scope of flow matching objective to manifolds (Chen and Lipman, 2024), arbitrarysources (Pooladian et al., 2023), and conditional flow matching with arbitrary transport maps andoptimal couplings between source and target samples (Tong et al., 2023). Molecular Conformer Generation. Various machine learning (ML) based approaches (Kingma andWelling, 2013; Liberti et al., 2014; Dinh et al., 2016; Simm and Hernndez-Lobato, 2019; Shi et al.,2021; Luo et al., 2021; Xu et al., 2021; Ganea et al., 2021; Xu et al., 2022; Jing et al., 2022; Wanget al., 2024) have been developed to improve upon the limitations of conventional methods, amongwhich the most advanced are TorsionDiff (Jing et al., 2022) and Molecular Conformer Fields (MCF)(Wang et al., 2024). TorsionDiff designs a diffusion model on the torsion angles while incorporatingthe local structure from RDKiT ETKDG (Riniker and Landrum, 2015). MCF trains a diffusion modelover functions that map elements from the molecular graph to points in 3D space. Equivariant Architectures for Atomistic Systems. Inductive biases play an important role ingeneralization and sample efficiency. In the case of 3D atomistic modelling, one example of a usefulinductive bias is the euclidean group SO(3) which represents rotation equivariance in 3D space.Recently, various equivariant architectures (Duval et al., 2023) have been developed that act on bothCartesian (Satorras et al., 2021; Thlke and De Fabritiis, 2022; Simeon and De Fabritiis, 2024; Duet al., 2022; Frank et al., 2022) and spherical basis (Musaelian et al., 2023; Batatia et al., 2022;Fuchs et al., 2020; Liao et al., 2023; Passaro and Zitnick, 2023; Anderson et al., 2019; Thomas et al.,2018). For molecular conformer generation, initial methods like ConfGF, DGSM utilize invariantnetworks as they act upon inter-atomic distances, whereas the use of equivariant GNNs have beenused in GeoDiff (Xu et al., 2022) and Torsional Diffusion (Jing et al., 2022). GeoDiff utilizes EGNN(Satorras et al., 2021), a Cartesian basis equivariant architecture while Torsional Diffusion uses TensorField Networks (Thomas et al., 2018) to output pseudoscalars.",
  "Method": "We design ET-Flow, a scalable equivariant model that generates energy-minimized conformers givena molecular graph. In this section, we layout the framework to achieve this objective by detailingthe generative process in flow matching, the rotation alignment between distributions, stochasticsampling, and finally the architecture details. Preliminaries We define notation that we use throughout this paper. Inputs are continuous atompositions x RN3 where N is the number of atoms. We use the notation vt(x) interchangeablywith v(t, x) for vector field.",
  "Flow Matching": "The aim is to learn a time-dependent vector field vt(x) : RN3 RN3 associated with thetransport map Xt : RN3 RN3 that pushes forward samples from a base distribution0, often an easy-to-sample distribution, to samples from a more complex target distribution 1, thelow-energy conformations of a molecule. This can be defined as an ordinary differential equation(ODE),Xt(x) = vt(Xt(x)),Xt=0 = x0,(1)where x0 0. We can construct the vt via a time-differentiable interpolation between samplesfrom 0 and 1 that gives rise to a probability path t that we can easily sample (Lipman et al., 2022;Liu et al., 2022; Albergo and Vanden-Eijnden, 2023; Tong et al., 2023). The general interpolationbetween samples x0 0 and x1 1 can be defined as:",
  "L = EtU(0,1),xt(x0,x1)v(t, x) v(t, x)2.(5)": "For training, we sample (i) x0 0, x1 1, and t U(0, 1), (ii) interpolate according toEquation 2, (iii) add noise from a standard Gaussian, and (iv) minimize the loss defined in Equation 5.For sampling, we sample x0 0 and integrate from t = 0 to t = 1 using the Eulers method. Ateach time-step, the Euler solver iteratively predicts the vector field for xt and updates its positionxt+t = xt + v(t, x)t. More details on the training and sampling algorithms are provided inAppendix B.",
  "Alignment": "Several previous works (Tong et al., 2023; Klein et al., 2024; Jing et al., 2024; Song et al., 2024)demonstrate that constructing a straighter path between base distribution 0 and target distribution1 minimizes the transport costs and improves performance. In our work, we reduce the transportcosts between samples from the harmonic prior 0 and samples from the data distribution 1 byrotationally aligning them using the Kabsch algorithm (Kabsch, 1976) similar to (Klein et al., 2024;Jing et al., 2024). This approach leads to faster convergence and reduces the path length betweenatoms by leveraging the similarity in \"shape\" of the samples as seen in a without incurringhigh computational cost.",
  "Stochastic Sampling": "We employ a variant of the stochastic sampling technique inspired by (Karras et al., 2022). Specifically,we inject noise at each time step to construct an intermediate state, evaluate the vector field fromthe intermediate state, and then perform the deterministic ODE step from the noisy state. Theoriginal method utilizes a second-order integration, which averages the denoiser output at the noisyintermediate state and the state at the next time step after integration. : Stochastic sampling pro-cedure used in inference. Noise isadded to the positions xt indicatedby the purple line, resulting in xt.Then, the model predicts the vec-tor field vt from xt instead of xtindicted by the yellow line and up-dates xt using vt to get xt+1. In our experiment, we use the stochastic sampler without thissecond-order correction term, which empirically provided aperformance boost comparable to the second-order method. Weapply stochastic sampling only during the final part of the inte-gration steps, specifically within the range t [0.8, 1.0]. Thishelps prevent drifting towards overpopulated density regionsand improves the quality of the samples (Karras et al., 2022).Stochastic sampling has improved both diversity and accuracyof the generated conformers, measured by Coverage and Aver-age Minimum RMSD (AMR) respectively as shown in .Detailed information on the stochastic sampling algorithm isprovided in algorithm B.",
  "Chirality Correction": "While generating conformations, it is necessary to take accountof the stereochemistry of atoms bonded to four distinct groupsalso referred to as tetrahedral chiral centers. To generate con-formations with the correct chirality, we propose a simple posthoc trick as done in GeoMol (Ganea et al., 2021). We comparethe oriented volume (OV) (Equation 6) of the generated conformation and the required orientationwith the RDKit tags. In the case of a mismatch, we simply flip the conformation against the z-axis.This correction step can be efficiently performed as a batched operation since it involves a simplecomparison with the required RDKit tags and an inversion of position if necessary.",
  ".(6)": "We also consider an alternative approach for chirality correction. Instead of using the post hoccorrection with our O(3) equivariant architecture, we slightly tweak our architecture to make itSO(3) equivariant by introducing a cross product term in the update layers. We compare thesemethods on both the GEOM-DRUGS and GEOM-QM9 dataset in and . Our basemethod (ET-Flow) corresponds to using the post hoc correction whereas the SO(3) variant is referredby ET-Flow-SO(3). We empirically observe that using an additional chirality correction step isnot only computationally efficient, but also performs better. We provide details on the architecturalmodification and proof of SO(3) equivariance in Section A.1 and Section C.1 respectively.",
  "Architecture": "ET-Flow (b) consists of two main components: (1) a representation module based on theequivariant transformer architecture from TorchMD-NET (Thlke and De Fabritiis, 2022) and (2)the equivariant vector output module. In the representation module, an embedding layer encodesthe inputs (atomic positions, atomic numbers, atom features, bond features and the time-step) into aset of invariant features. Initial equivariant features are constructed using normalized edge vectorswhere the edges are constructed using a radius graph of 10 angstrom and the bonds from the 2Dmolecular graph. Then, a series of equivariant attention-based layers update both the invariant andequivariant features using a multi-head attention mechanism. Finally, the vector field is produced bythe output layer, which updates the equivariant features using gated equivariant blocks (Schtt et al.,2018). Given that TorchMD-NET was originally designed for modeling neural network potentials,we implement several modifications to its architecture to better suit generative modeling, as detailedin Section A.1.",
  "Experimental Setup": "Dataset: We conduct our experiments on the GEOM dataset (Axelrod and Gomez-Bombarelli, 2022),which offers curated conformer ensembles produced through meta-dynamics in CREST (Prachtet al., 2024). Our primary focus is on GEOM-DRUGS, the most extensive and pharmacologicallyrelevant subset comprising 304k drug-like molecules, each with an average of 44 atoms. We use atrain/validation/test (243473/30433/1000) split as provided in (Ganea et al., 2021) Additionally, wetrain and test model on GEOM-QM9, a subset of smaller molecules with an average of 11 atoms.Finally, in order to assess the models ability to generalize to larger molecules, we evaluate the modeltrained on GEOM-DRUGS on a GEOM-XL dataset, a subset of large molecules with more than 100atoms. The results for GEOM-QM9 and GEOM-XL can be found in the Appendix D. Evaluation: Our evaluation methodology is similar to that of (Jing et al., 2022). First, we look atRMSD based metrics like Coverage and Average Minimum RMSD (AMR) between generated andground truth conformer ensembles. For this, we generate 2K conformers for a molecule with Kground truth conformers. Second, we look at chemical similarity using properties like Energy (E),dipole moment (), HOMO-LUMO gap () and the minimum energy (Emin) calculated using xTB(Bannwarth et al., 2019). Baselines: We benchmark ET-Flow against leading approaches outlined in . Specifically,we assess the performance of GeoMol (Ganea et al., 2021), GeoDiff (Xu et al., 2022), TorsionalDiffusion (Jing et al., 2022), and MCF (Wang et al., 2024). Notably, the most recent among these,MCF, has demonstrated superior performance across evaluation metrics compared to its predecessors.Its worth mentioning that GeoDiff initially utilized a limited subset of the GEOM-DRUGS dataset;thus, for a fair comparison, we consider its re-evaluated performance as presented in (Jing et al.,2022).",
  "Ensemble RMSD": "As shown in and , ET-Flow outperforms all preceding methodologies and demonstratescompetitive performance with the previous state-of-the-art, MCF (Wang et al., 2024). Despite beingsignificantly smaller with only 8.3M parameters, ET-Flow shows a substantial improvement in thequality of generated conformers, as evidenced by superior Precision metrics across all MCF models,including the largest MCF-L. When compared to MCF-S, which is closer in size, ET-Flow achievesmarkedly better Precision while the impact on Recall is less significant and limited to Recall Coverage.Notably, our Recall AMR remains competitive with much bigger MCF-B, underscoring the inherentadvantage of our method in accurately predicting overall structures. : Molecule conformer generation results on GEOM-QM9 ( = 0.5). ET-Flow - SO(3) is ET-Flow using the SO(3) architecture for chirality correction. For both ET-Flow and ET-Flow-SO(3),we sample conformations over 50 time-steps.",
  "Coverage Threshold Plots": "We compare the coverage metrics of ET-Flow against Torsional diffusion (Jing et al., 2022) andMCF (Wang et al., 2024) against a wide range of thresholds on the GEOM DRUGS dataset in. ET-Flow consistently outperforms previous methods in precision-based metrics. In terms ofrecall, our approach demonstrates better performance than Torsional Diffusion across all thresholds.Despite MCF performing better at higher thresholds, ET-Flow outperforms in the lower thresholds,underscoring its proficiency in generating accurate conformer predictions. : Recall and Precision Coverage result on GEOM-DRUGS as a function of the thresholddistance. ET-Flow outperforms TorsionDiff by a large margin especially in a lower threshold region.We emphasize the better performance of ET-Flow at lower thresholds in both Recall and Precisionmetrics.",
  "Ensemble Properties": "RMSD provides a geometric measure for assessing ensemble quality, but it is also essential to considerthe chemical similarity between generated and ground truth ensembles. For a random 100-moleculesubset of the test set of GEOM-DRUGS, if a molecule has K ground truth conformers, we generatea minimum of 2K and a maximum of 32 conformers per molecule. These conformers are thenrelaxed using GFN2-xTB (Bannwarth et al., 2019), and the Boltzmann-weighted properties of thegenerated and ground truth ensembles are compared. Specifically, using xTB (Bannwarth et al., 2019), we compute properties such as energy (E), dipole moment (), HOMO-LUMO gap (), andthe minimum energy (Emin). illustrates the median errors for ET-Flow and the baselines,highlighting our methods capability to produce chemically accurate ensembles. Notably, we achievesignificant improvements over both TorsionDiff and MCF across all evaluated properties.",
  "Inference Steps Ablation": "In , our sampling process with ET-Flow utilizes 50 inference steps. To evaluate the methodsperformance under constrained computational resources, we conducted an ablation study by progres-sively reducing the number of inference steps. Specifically, we sample for 5, 10 and 20 time-steps.The results on GEOM-DRUGS are presented in . We observed minimal performance degra-dation with a decrease in the number of steps. Notably, ET-Flow demonstrates high efficiency,maintaining performance across all precision and recall metrics even with as few as 5 inference steps.Interestingly, ET-Flow with 5 steps still achieves superior precision metrics compared to all existingmethods. This underscores ET-Flows ability to generate high-quality conformations while operatingwithin limited computational budgets.",
  ": Sampling efficiency as a measure of the quality of Inference time with respect to the numberof time steps on GEOM-DRUGS": "We demonstrate the ability of ET-Flow to generate samples efficiently. We evaluate the inferencetime per molecule over varying number of time steps and report the average time across 1000 randomsamples from the test set of GEOM-DRUGS. shows that ET-Flow outperforms Torsionaldiffusion (Jing et al., 2022) in inference across all time steps. While ET-Flow may not achievethe fastest raw inference times (potentially due to MCF variants benefiting from optimized CUDAkernels for attention), it maintains competitive speeds while ensuring higher precision. We suspectthat concurrent work on improving equivariant operations with optimized CUDA kernels (Lee et al.,2024) should lead to similar efficiency gains as seen in transformer-based architectures. : Ablation over number of inference steps on GEOM-DRUGS ( = 0.75). Performance ofET-Flow at 5 steps is competent across all metrics while also retaining state-of-the-art performanceon precision metrics when compared with previous methods.",
  "meanmedianmeanmedianmeanmedianmeanmedian": "ET-Flow (5 Steps)77.8482.210.4760.44374.0380.80.550.474ET-Flow (10 Steps)79.0584.000.4510.41574.6481.380.5330.457ET-Flow (20 Steps)79.2984.040.4490.41374.8981.320.5310.454ET-Flow (50 Steps)79.5384.570.4520.41974.3881.040.5410.470 ET-Flow effectively balances performance and speed, making it ideal for tasks that require highsample quality with efficient computation. With the ability to generate high-quality samples in fewertime steps, e.g., 5 time steps, as indicated in , ET-Flow is well-suited for scenarios demandinga large number of samples, as fewer steps lead to lower inference time per molecule. Additionally,we encountered difficulties running MCF-L for 20 and 50 steps, so those results have not beenincluded. In summary, ET-Flow demonstrates efficient sampling, balancing precision and speed,making it highly effective for generating high-quality molecular samples while remaining competitivein inference time.",
  "Conclusion": "In this paper, we present our simple and scalable method ET-Flow, which utilizes an equivariant trans-former with flow matching to achieve state-of-the-art performance on multiple molecular conformergeneration benchmarks. By incorporating inductive biases, such as equivariance, and enhancingprobability paths with a harmonic prior and RMSD alignment, we significantly improve the precisionof the generated molecules, and consequently generate more physically plausible molecules. Impor-tantly, our approach maintains parameter and speed efficiency, making it not only effective but alsoaccessible for practical high-throughput applications.",
  "Limitations And Future Works": "While ET-Flow demonstrates competitive performance in molecular conformer generation, there areareas where it can be enhanced. One such area is the recall metrics, which capture the diversity ofgenerated conformations. Another area is the use of an additional chirality correction step that isused to predict conformations with the desired chirality. Moreover, although our performance on theGEOM-XL dataset is comparable to MCF-S and TorsionDiff, there is still room for improvement. We propose three future directions here. First, we observe during experiments that a well-designedsampling process incorporating stochasticity can enhance the quality and diversity of generatedsamples. An extension of our current approach could involve using Stochastic Differential Equations(SDEs), which utilize both vector field and score in the integration process, potentially improving thediversity of samples. Second, we propose to scale the number of parameters of ET-Flow, which hasnot only been shown to be useful across different domains of deep learning, but has also shown to beuseful in molecular conformer generation for MCF (Wang et al., 2024). Third, to better handle thechirality problem, we aim to explore alternatives for incorporating SO(3)-equivariance into the modelin the future.",
  "Acknowledgements": "The authors sincerely thank Cristian Gabellini, Jiarui Ding, and the NeurIPS reviewers for theinsightful discussions and feedback. Resources used in completing this research were provided byValence Labs. Furthermore, we acknowledge a grant for student supervision received by Mila -Quebecs AI institute - and financed by the Quebec ministry of Economy.",
  "Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprintarXiv:1607.06450, 2016": "Andrew J Ballard, Stefano Martiniani, Jacob D Stevenson, Sandeep Somani, and David J Wales.Exploiting the potential energy landscape to sample free energy. Wiley Interdisciplinary Reviews:Computational Molecular Science, 5(3):273289, 2015. Christoph Bannwarth, Sebastian Ehlert, and Stefan Grimme. Gfn2-xtban accurate and broadlyparametrized self-consistent tight-binding quantum chemical method with multipole electrostaticsand density-dependent dispersion contributions. Journal of Chemical Theory and Computation, 15(3):16521671, 2019. doi: 10.1021/acs.jctc.8b01176. URL PMID: 30741547. Ilyes Batatia, David P Kovacs, Gregor Simm, Christoph Ortner, and Gbor Csnyi. Mace: Higherorder equivariant message passing neural networks for fast and accurate force fields. Advances inNeural Information Processing Systems, 35:1142311436, 2022.",
  "Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. arXivpreprint arXiv:1605.08803, 2016": "Weitao Du, He Zhang, Yuanqi Du, Qi Meng, Wei Chen, Nanning Zheng, Bin Shao, and Tie-Yan Liu.Se (3) equivariant graph neural networks with complete local frames. In International Conferenceon Machine Learning, pages 55835608. PMLR, 2022. Alexandre Duval, Simon V Mathis, Chaitanya K Joshi, Victor Schmidt, Santiago Miret, Fragkiskos DMalliaros, Taco Cohen, Pietro Li, Yoshua Bengio, and Michael Bronstein. A hitchhikers guideto geometric gnns for 3d atomic systems. arXiv preprint arXiv:2312.07511, 2023. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Mller, Harry Saini, YamLevi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers forhigh-resolution image synthesis. arXiv preprint arXiv:2403.03206, 2024. Thorben Frank, Oliver Unke, and Klaus-Robert Mller. So3krates: Equivariant attention for interac-tions on arbitrary length-scales in molecular systems. Advances in Neural Information ProcessingSystems, 35:2940029413, 2022. Fabian Fuchs, Daniel Worrall, Volker Fischer, and Max Welling. Se (3)-transformers: 3d roto-translation equivariant attention networks. Advances in neural information processing systems, 33:19701981, 2020. Octavian Ganea, Lagnajit Pattanaik, Connor Coley, Regina Barzilay, Klavs Jensen, William Green,and Tommi Jaakkola.Geomol: Torsional geometric generation of molecular 3d conformerensembles. Advances in Neural Information Processing Systems, 34:1375713769, 2021.",
  "Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution.Advances in neural information processing systems, 32, 2019": "Yuxuan Song, Jingjing Gong, Minkai Xu, Ziyao Cao, Yanyan Lan, Stefano Ermon, Hao Zhou,and Wei-Ying Ma. Equivariant flow matching with hybrid probability transport for 3d moleculegeneration. Advances in Neural Information Processing Systems, 36, 2024. Hannes Stark, Bowen Jing, Regina Barzilay, and Tommi Jaakkola. Harmonic prior self-conditionedflow matching for multi-ligand docking and binding site design. In NeurIPS 2023 AI for ScienceWorkshop, 2023.",
  "Philipp Thlke and Gianni De Fabritiis. Torchmd-net: Equivariant transformers for neural networkbased molecular potentials. arXiv preprint arXiv:2202.02541, 2022": "Nathaniel Thomas, Tess Smidt, Steven Kearnes, Lusann Yang, Li Li, Kai Kohlhoff, and Patrick Riley.Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point clouds.arXiv preprint arXiv:1802.08219, 2018. Alexander Tong, Nikolay Malkin, Guillaume Huguet, Yanlei Zhang, Jarrid Rector-Brooks, KilianFatras, Guy Wolf, and Yoshua Bengio. Conditional flow matching: Simulation-free dynamicoptimal transport. arXiv preprint arXiv:2302.00482, 2(3), 2023.",
  "A.1Architecture": "The ET-Flow architecture () consists of 2 major components, a representation layer and an output layer.For the representation layer, we use a modified version of the embedding and equivariant attention-based updatelayers from the equivariant transformer architecture of TorchMD-NET (Thlke and De Fabritiis, 2022). Theoutput layer utilizes the gated equivariant blocks from (Schtt et al., 2018). We highlight our modifications overthe original TorchMD-NET architecture with this color. These modifications enable stabilized training sincewe use a larger network than the one proposed in the TorchMD-NET (Thlke and De Fabritiis, 2022) paper.Additionally, since our input structures are interpolations between structures sampled from a prior and actualconformations, it is important to ensure our network is numerically stable when the interpolations contain twoatoms very close to each other. Embedding Layer: The embedding layer maps each atoms physical and chemical properties into a learnedrepresentation space, capturing both local atomic features and geometric neighborhood information. For the i-thatom in a molecule with N atoms, we compute an invariant embedding xi through the following process:",
  "where t represents the time-step, and denotes concatenation. The resulting embedding xi Rd serves asinput to subsequent layers of the network": "Attention Mechanism: The multi-head dot-product attention operation uses atom features xi, atom attributeshi, time-step t and inter-atomic distances dij to compute attention weights. The input atom-level features xi aremixed with the atom attributes hi and the time-step t using an MLP and then normalized using a LayerNorm (Baet al., 2016). To compute the attention matrix, the inter-atomic distances dij are projected into two dimensionalfilters DK and DV as:",
  "DV = W DV eRBF (dij) + bDV (13)": "The atom level features are then linearly projected along with a LayerNorm operation to derive the query Q andkey K vectors. The value vector V is computed with only the linear projection of atom-level features. ApplyingLayerNorm on Q, K vectors (also referred to as QK-Norm) has proven to stabilize un-normalized values in theattention matrix (Dehghani et al., 2023; Esser et al., 2024) when scaling networks to large number of parameters.The Q and K vectors are then used along with the distance filter DK for a dot-product operation over the featuredimension:",
  "scalar filters s1ij and s2ij (edge-level features)": "Update Layer: The update layer computes interactions between atoms in the attention block and uses theoutputs to update the scalar feature xi and the vector feature vi. First, the scalar feature output yi from theattention mechanism is split into three features (q1i , q2i , q3i ), out of which q1i and q2i are used for the scalar featureupdate as,xi = q1i + q2i U1vi U2vi,(18)where U1vi U2vi is the inner product between linear projections of vector features vi with matrices U1, U2. The edge vector update consists of two components. First, we compute a vector wi, which for each atom iscomputed as a weighted sum of vector features and a clamped-norm of the edge vectors over all neighbors:",
  "max(ri rj, ),(19)": "vi = wi + q3i U3vi(20)where U1 and U3 are projection matrices over the feature dimension of the vector feature vi. In this layer, weclamp the minimum value of the norm (to = 0.01) to prevent numerically large values in cases where positionsof two atoms are sampled too close from the prior.",
  "NameDescriptionRange": "chiralityChirality Tag{unspecified, tetrahedral CW & CCW, other}degreeNumber of bonded neighbors{x : 0 x 10, x Z}chargeFormal charge of atom{x : 5 x 5, x Z}num_HTotal Number of Hydrogens{x : 0 x 8, x Z}number_radical_eNumber of Radical Electrons{x : 0 x 4, x Z}hybrizationHybrization type{sp, sp2, sp3, sp3d, sp3d2, other}aromaticWhether on a aromatic ring{True, False}in_ringWhether in a ring{True, False}",
  "A.3Evaluation Metrics": "Following the approaches of (Ganea et al., 2021; Xu et al., 2022; Jing et al., 2022), we utilize Average MinimumRMSD (AMR) and Coverage (COV) to assess the performance of molecular conformer generation. Here,Cg denotes the set of generated conformations, and Cr denotes the set of reference conformations. For bothAMR and COV, we calculate and report Recall (R) and Precision (P). Recall measures the extent to which thegenerated conformers capture the ground-truth conformers, while Precision indicates the proportion of generatedconformers that are accurate. The specific formulations for these metrics are detailed in the following equations:",
  "A.4Training Details and Hyperparameters": "For GEOM-DRUGS, we train ET-Flow for a fixed 250 epochs with a batch size of 64 and 5000 training batchesper epoch per GPU on 8 A100 GPUs. For the learning rate, we use the Adam Optimizer with a cosine annealinglearning rate which goes from a maximum of 103 to a minimum 107 over 250 epochs with a weight decay of1010. For GEOM-QM9, we train ET-Flow for 200 epochs with a batch size of 128, and use all of the trainingdataset per epoch on 4 A100 GPUs. We use the cosine annealing learning rate schedule with maximum of8 104 to minimum of 107 over 100 epochs, post which the maximum is reduced by a factor of 0.05. Weselect checkpoints based on the lowest validation error.",
  "C.1Designing SO(3) Equivariance": "We show that we can modify the architecture in Section A.1 (Equation 18) to produce a final vector output thatsatisfies rotation equivariance and reflection asymmetry. Let v1 and v2 be linearly independent non-zero vectorsv1 > 0, v2 > 0, and s be a scalar. We implement SO(3) equivariance by adding a vector with a crossproduct. We show that vector v = v1 + s(v1 v2), where v1 v2 denotes cross product of v1 and v2, satisfiesanti-symmetry while maintaining rotation equivariance as follows,",
  "D.1Design Choice Ablations": "We conduct a series of ablation studies to assess the influence of each component in the ET-Flow. Particularly,we re-run the experiments with (1) O(3) equivariance without chirality correction, (2) Absence of Alignment,(3) Gaussian Prior as a base distribution. We demonstrate that improving probability paths and utilizing anexpressive equivariant architecture with correct symmetries are key components for ET-Flow to achieve stateof the art performance. The ablations were ran with reduced settings (50 epochs; lr = 1e 4; 4 A100 gpus).Results are shown in Table D.1.",
  "Tor. Diff.1.931.862.842.7177MCF - S2.021.872.92.6977MCF - B1.711.612.692.4477MCF - L1.641.512.572.2677ET-Flow (ours)2.001.802.962.6375": "provided by Torsional Diffusion, we encountered 27 failed cases for generation likely due to RDKit failures,similar to the observations in MCF albeit with slightly different exact numbers. In both experiments involving all102 molecules and a subset of 75 molecules, ET-Flow achieves performance comparable to Torsional Diffusionand MCF-S, but falls short of matching the performance of MCF-B and MCF-L. Its worth noting that MCF-Band MCF-L are significantly larger models, potentially affording them an advantage in generalization tasks. Aspart of our future work, we plan to scale up our model and conduct further tests to explore its performance inthis regard.",
  "ET-Flow (DRUGS RS)79.5384.570.4520.41974.3881.040.5410.470ET-Flow (DRUGS SS)76.0680.650.6440.54567.8374.190.5110.473": "To further evaluate the generalization performance of ET-Flow, we conduct two more out-of-distributionexperiments in addition to GEOM-XL. First, we test the model on scaffold-based splits of the GEOM-QM9 andGEOM-DRUGS dataset, which offers a more challenging alternative to the standard random split. We split thedatasets based on Murcko scaffolds of the molecules into an 80:10:10 ratio for train, validation, and test sets. Weevaluate our method on 1000 randomly sampled molecules from the resulting test set. The second experimentinvolves training the model on GEOM-DRUGS and assessing its performance on GEOM-QM9, a dataset withsignificantly smaller molecules. This experiment complements the generalization task to larger molecules in GEOM-XL by assessing ability for ET-Flow to generalize to smaller molecules. The results, presented in ,indicate that the models performance degrades only marginally on the scaffold-based split. Furthermore, themodel demonstrates robust performance on GEOM-QM9 even when trained on GEOM-DRUGS."
}