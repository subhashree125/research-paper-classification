{
  "Abstract": "Research in auditory, visual, and audiovisual speech recognition (ASR, VSR, andAVSR, respectively) has traditionally been conducted independently. Even recentself-supervised studies addressing two or all three tasks simultaneously tend to yieldseparate models, leading to disjoint inference pipelines with increased memoryrequirements and redundancies. This paper proposes unified training strategiesfor these systems. We demonstrate that training a single model for all three tasksenhances VSR and AVSR performance, overcoming typical optimisation challengeswhen training from scratch. Moreover, we introduce a greedy pseudo-labellingapproach to more effectively leverage unlabelled samples, addressing shortcomingsin related self-supervised methods. Finally, we develop a self-supervised pre-training method within our framework, proving its effectiveness alongside oursemi-supervised approach. Despite using a single model for all tasks, our unifiedapproach achieves state-of-the-art performance compared to recent methods onLRS3 and LRS2 for ASR, VSR, and AVSR, as well as on the newly releasedWildVSR dataset. Code and models are available at",
  "Introduction": "Speech recognition can be achieved using auditory signals (known as auditory/automatic speechrecognition; ASR) , visual cues from lip movements (visual speech recognition; VSR) ,or both (audiovisual speech recognition; AVSR) . Audio typically offers the most relevantinformation in videos of talking faces, but lipreading can greatly enhance recognition, especiallywhen the audio is noisy or wholly unavailable . Despite the similarities between ASR, VSR, andAVSR, research in these fields has largely developed independently . The Transformer architectures versatility has spurred efforts to unify speech recognitionby pre-training a single model on various unlabelled inputs (visual, auditory, and audiovisual) throughself-supervision . However, these methods often require separate fine-tuning stages forASR, VSR, and AVSR, leading to separate models for each task, which increases computational loadand complexity. u-HuBERT shows that a single pre-trained model can be fine-tuned for all threetasks, yet does not reach the performance of separately fine-tuned models . In this paper, we delve deeper into strategies for unified speech recognition (USR) by training a singlemodel to perform ASR, VSR, and AVSR. We find that training such a model from scratch on theLRS3 dataset achieves competitive performance on all tasks. This is notable given the known",
  "arXiv:2411.02256v1 [cs.CV] 4 Nov 2024": "optimisation difficulties in VSR training, which previously required self-supervised pre-training ,supervised feature extractor pre-training , or curriculum learning strategies . Our findingssuggest that including audio improves the optimisation landscape for VSR and AVSR supervisedtraining, as observed in a different context by . Furthermore, we propose a semi-supervised pseudo-labelling approach to leverage unlabelled audiovi-sual data, addressing shortcomings of standard fine-tuning in self-supervised methods .Fine-tuning often leads to overfitting due to using fewer samples than pre-training, requiring varioustricks to reach optimal performance . This issue is particularly pronounced in encoder-decoder architectures where usually only the encoder is pre-trained, and attempts to pre-train thedecoder have yielded inconsistent results . Our semi-supervised approach generates pseudo-labels via an encoder-decoder momentum-based teacher to leverage unlabelled samples through-out training, effectively mitigating overfitting. Training on all three modalities simultaneously helpsalleviate the computational cost of pseudo-labelling as the cost is amortised across the inputs. Lastly, inspired by recent self-supervised works, we design a pre-training method within our unifiedframework. We combine pre-training with pseudo-labelling and show that our semi-supervisedapproach is complementary to self-supervision. Our final unified models achieve state-of-the-artresults across multiple settings, surpassing existing methods that use separate models for each task.",
  "Related Work": "Audiovisual self-supervised speech representation learning.Recent interest in audiovisual self-supervised learning for speech recognition has focused on leveraging the correspondence betweenaudio waveforms and silent lip movements to capture shared semantic content across the modali-ties . These methods employ cross-modal learning and masked prediction to develop contextualised representations from large unlabelled datasets, which are more readilyavailable than transcribed datasets. After pre-training, a randomly initialised decoder is appendedto the encoder, often with an optional CTC layer . The system is then fine-tuned on a smallerset of labelled samples for tasks such as ASR, VSR, and AVSR, usually resulting in different mod-els for each task . However, these methods may fail to leverage unlabelled samples fullysince the pretext tasks are not directly aligned with speech recognition. Furthermore, the decoder,trained on limited data during fine-tuning, is highly susceptible to overfitting, necessitating strategiessuch as freezing encoder layers or employing variable learning rates across layers to optimiseperformance . Pseudo-labelling for speech recognition.Pseudo-labelling has been explored in audiovisualspeech recognition literature, with methods such as offline pseudo-labelling and frame-wisedistillation using frozen teacher models . While these approaches rely on frozen external ASRmodels trained on large-scale datasets , our USR method eliminates this dependency using arandomly initialised teacher model that improves throughout training. Iterative pseudo-labelling has shown promise for ASR. Some employ multiple rounds of pseudo-labelling using costly beam search and filtering strategies , while others continuouslyand efficiently update pseudo-labels using a CTC-only loss . However, eliminating filteringand attention losses can impact training due to low-quality pseudo-labels, as observed in a recentmethod that aims to apply these approaches for ASR, VSR, and AVSR but lags behind thestate-of-the-art (see Appendix J). In contrast, USR uses an encoder-decoder architecture to generateCTC and attention pseudo-labels at each iteration through a greedy approach, while pseudo-labelquality is maintained via a token-wise filtering mechanism inspired by the semi-supervised FixMatchtechnique in image recognition. We note that sharing the same pseudo-labels across auditory,visual, and audiovisual inputs amortises generation costs, leading to efficient CTC-attention training. Single model for multiple modalities.An earlier study trained a single recurrent neuralnetwork for ASR, VSR, and AVSR, but noted significant performance differences comparedto modality-specific models. Recent works have shown that the Transformer architecture canhandle multiple modalities using the same weights, with minimal performance degradation .In speech recognition, some use the same Transformer encoder for auditory, visual, andaudiovisual inputs during pre-training, but then separately fine-tune the parameters for ASR, VSR, andAVSR, resulting in separate models during deployment. u-HuBERT uses the same weights for all",
  "TS": ": Unified Speech Recognition. Our USR method combines self-supervised pre-trainingwith semi-supervised fine-tuning. For semi-supervised training, pseudo-labels are generated fromunmasked audiovisual features using an EMA (exponential moving average)-based teacher. Thestudent, intaking masked inputs, predicts pseudo-labels for unlabelled data and ground-truth labelsfor labelled data. To obtain the pseudo-labels, an argmax operation is applied to the CTC andattention teacher output probabilities; the tokens with predicted probability below a fixed thresholdare discarded. For self-supervised pre-training, a student encoder processes masked visual, auditory,and audiovisual samples and predicts targets, generated by an EMA-based teacher intaking unmaskedaudiovisual samples, via a shallow predictor. The targets are the average outputs of the teacher blocks.The resulting student weights are used to initialise the student and teacher in semi-supervised fine-tuning. Feature extraction is achieved through modality-specific feature extractors, whose featuresare concatenated along the channel dimension to produce the audiovisual inputs. The auditory, visual,and audiovisual student inputs are batched together for training efficiency. modalities when fine-tuning a pre-trained AV-HuBERT backbone , demonstrating the viability ofa unified model. However, it encounters limitations common to other self-supervised approaches, suchas proneness to overfitting during supervised fine-tuning. Our proposed semi-supervised approachleverages unlabelled samples during the fine-tuning stage, significantly alleviating these concerns.",
  "Unified Speech Recognition": "Our unified method trains a pre-LN Transformer encoder-decoder model for ASR, VSR, andAVSR. .1 describes the task of unified speech recognition using supervised training, wherewe have ground-truth annotation for each audio-visual pair. Sections 3.2 and 3.3 then introduce ourproposed idea, which employs semi-supervised training and self-supervised pre-training to effectivelyutilise unlabelled samples. An overview of USRs components is depicted in .",
  "Unified Supervised Training": "Inputs.Let {(vb, ab, yb) : b [1, B]} be a batch of B labelled samples, where vb denotes a Tv-frame video of lip movements, ab denotes the corresponding (raw) audio waveform of Ta = 640Tvframes1, and yb denotes the label sequence of length Tl. Following , vb and ab are zero-maskedwith a maximum duration of 0.4 and 0.6 seconds for each second of video and audio, respectively.",
  "We assume the video is sampled at 25 frames per second and the audio at 16,000kHz": "Multi-modal feature extraction.The raw video and audio are fed into ResNet-18 architectures:a 2D version with a 3D stem for video and a 1D version for audio, sub-sampling the audio tomatch the videos sampling rate . Linear layers follow the feature extractors to produce the visualand auditory features. The audiovisual features are formed by concatenating the feature extractoroutputs along the channel dimension and applying a linear transformation. Finally, the features fromthe three modalities are concatenated along the batch dimension for efficient processing. We providethe model with all three input types, enabling it to perform well on ASR, VSR, and AVSR.",
  "Inputs.In addition to the labelled batch from .1, we now also have Bu unlabelled videoand audio samples {(vub, aub) : b [1, Bu]}. The student inputs are masked as before": "Pseudo-labels.The teacher, sharing the same architecture as the student, generates pseudo-labelsfor unlabelled samples. The student is optimised as usual, but no gradients are passed to the teacher.Instead, the teachers weights t are updated at each iteration via an exponential moving average(EMA) of the students weights s : t t +(1)s, where increases throughout trainingfrom 0.999 to 1 using a cosine scheduler. For an unmasked audiovisual sample, let cb and ab denote the CTC probabilities from the teacherencoder and the attention probabilities from the teacher decoder, respectively. The CTC and attentionpseudo-labels are given by arg max(cb) and arg max(ab), respectively, where arg max is appliedtoken-wise. Hence, the pseudo-labels correspond to units with the maximum probability across thevocabulary for each input/output time-step. The attention targets are generated auto-regressively byselecting, at each time-step, the most likely unit as the input for the next time-step, without using acostly beam search strategy. Our greedy approach allows for efficient label generation. Filtering.The teacher may not consistently generate high-quality predictions, especially early intraining. We propose a straightforward token-wise filtering mechanism, creating masks 1(max(cb) ) and 1(max(ab) ), where the operations are applied token-wise. We thus discard a pseudo-label for a given time-step if its confidence falls below a certain threshold . This mechanism drawsinspiration from image recognition literature and is adapted to sequences.",
  "Unified Self-supervised Pre-training": "examines the main properties of our self-supervised method (see .3). We fine-tunepre-trained models with different hyperparameters using our semi-supervised approach (.2).We use the LRS3 low-resource setting, as in .2. See Appendix C.6 for training details. Target modality.In a, we evaluate our method with targets derived from the differentinput modalities. Across all cases, pre-training outperforms training from scratch, highlighting thecomplementarity of semi- and self-supervised training. Visual targets enhance VSR but diminishASR/AVSR performance compared to auditory targets; overall, audiovisual targets consistentlyperform best. These results suggest that cross-modal-only pre-training may lose crucial modality-specific information, reducing generalisation when fine-tuning on all data (including unlabelledsamples), i.e., via pseudo-labelling. Our observations are in contrast to previous findings with",
  "Main Properties": "In this section, we investigate the behaviour of our unified model. For all experiments, we use a12-block Base model with hidden size of 512 (see Appendix C.4 for model details). We report test setword error rates (WER) for direct comparison with the main results. Note that we used the validationset from in the exploration stage to avoid overfitting to the test set.",
  "In , we investigate properties of training our unified model from scratch on the full LRS3dataset (see .1). Training details are provided in Section C.5": "Sharing weights.a studies the impact of weight sharing versus separate models per task(ASR, VSR, AVSR). While using only auditory inputs yields strong performance, training VSRand AVSR models from scratch encounters optimisation challenges, in line with prior research . Interestingly, these hurdles are overcome with weight sharing, resulting in robust VSR andAVSR performance without self-supervised pre-training or training techniques like curriculumlearning . This is likely due to audio containing denser verbal information than video, enhancingthe optimisation landscape for visual modalities . Modality sampling.We employ a weighted average to combine the per-modality losses (see Eq. 4).In contrast, other methods randomly sample, at each iteration, input types with differentprobabilities, which may vary during training. b shows that our approach performs similarlywith random sampling when training the latter for 3 more epochs. Our approach offers benefitssuch as sharing computational costs among feature extractor forward passes and amortising the costof pseudo-label generation across input types (see .2), as all modalities use the same targets. Input type weight.c studies the effect of using different weights for the visual modality.We observe that using a higher v for the VSR loss improves VSR but worsens ASR/AVSR. Wechoose v = 0.3 as our default setting, striking a balance in performance among the different tasks.",
  "Unified Semi-supervised Training": "In , we ablate various components to better understand our unified semi-supervised framework(see .3). We adopt the common low-resource setting : the 30-hour trainval partition ofLRS3 serves as our labelled dataset, while the remaining portion of LRS3 (without labels) providesour unlabelled samples. See Appendix C.5 for training details. Filtering predicted tokens. investigates the impact of the threshold parameter {0, 0.8, 1}. We plot (from left to right) (1) the proportion of tokens exceeding , (2) the validationattention accuracy of the decoder using teacher forcing, and (3) the CTC loss, as a function of theepoch number. We also show the final WER. We observe that = 1, where only labelled samplescontribute to training, results in poor attention accuracy, high CTC loss, and high WER across inputtypes. Conversely, = 0, implying no filtering (i.e., all tokens are considered regardless of confidencelevel), yields competitive performance, suggesting some robustness to low-quality pseudo-labels.Finally, for = 0.8, the proportion of tokens with confidence over begins at a low level and steadilyincreases throughout training as the teacher network improves. This yields improved performance interms of attention accuracy, CTC loss, and final WER, demonstrating the efficacy of filtering via asimple confidence threshold. A more fine-grained analysis of values are given in Section D.1.",
  "CTC-att37.84.03.9": "Quantity/quality trade-off.Pseudo-labels tend to be abundant but noisy, while ground-truthtranscriptions are scarce yet high-quality. The balance between quantity and quality is adjustable viathe hyperparameters v and a in Eq. 7. a explores different values for v and a, revealingbetter performance when a > v. Noisy pseudo-labels generated from audiovisual samples maysuffice for VSR, which often performs worse than ASR/AVSR and benefits from data abundance.Conversely, ASR/AVSR is less prone to overfitting and may suffer with excessive reliance on low-quality pseudo-labels, requiring a higher relative weight on labelled losses. Momentum.b shows the effect of updating the teachers weight via EMA ( = 0.999)compared to simply copying the students weights at every iteration ( = 0). Using EMA results inbetter performance, yet good results are achieved even without it. Loss types.CTC and attention-based encoder-decoder frameworks are dominant approaches inspeech recognition. While attention typically outperforms CTC, it may struggle with proper alignmentprediction, requiring tuning of various decoding hyperparameters . To address these challenges,we adopt a CTC-attention hybrid framework , as in . The costly auto-regressiveattention pseudo-label generation is made computationally feasible via our greedy strategy and multi-modal feature extraction (which amortises pseudo-label generation costs). c demonstrates asignificant improvement in results by using both CTC and attention compared to CTC alone.",
  "Comparisons with Self-supervised Methods": "compares our approach on LRS3 with self-supervised methods under similar model sizesand data settings. We combine pre-training (.3) with standard fine-tuning (.1) whenusing identical pre-training and fine-tuning data, and with semi-supervised fine-tuning (.2)when using extra unlabelled data. In addition to the low-resource labelled data setting outlined in.2, we test in a high-resource setting using the full 433-hour LRS3 dataset for fine-tuning.Our pre-training employs either LRS3 alone or combined with a 1,326-hour English-only version ofVoxCeleb2 . We experiment with Base, Base+, and Large Transformers (see Appendix C.4). Low-resource.Using the Base model and LRS3 for pre-training, our approach significantly exceedsthe previous state-of-the-art across VSR, ASR, and AVSR, when fine-tuning on 30 hours. Increasingthe pre-training data and model size enhances performance, demonstrating our methods scalability.With the Large model and LRS3+Vox2 as pre-training data, we achieve 26.9% WER for VSR and2.4% WER for both ASR and AVSR, matching BRAVEn on ASR and surpassing it on VSR. Unlikeother methods, which use separate models for each task, USR employs a single model for all tasks. High-resource.In the high-resource setting, our results are comparable to modality-specific modelsfor ASR/AVSR and superior for VSR across all settings. Our top model obtains 22.3% WER forVSR, 1.2% WER for ASR, and 1.1% WER for VSR, significantly outperforming u-HuBERT, whichalso uses a single model for all modalities. Furthermore, USRs low-resource VSR performance issuperior to u-HuBERTs high-resource VSR result.",
  "Comparisons with the State-of-the-Art": "LRS3.In , we compare our best model against the state-of-the-art on LRS3. We presentour USR results with a language model incorporated via shallow fusion , improving VSRperformance from 22.3% to 21.5%. Despite using a shared model for all tasks, our performanceexceeds multiple supervised methods and approaches top results , which use significantlymore labelled data. USR surpasses Auto-AVSR on VSR (21.5% vs. 23.5%) despite the latter usingmore total data and external ASR models for transcription. Finally, we outperform self-supervised : Comparisons with self-supervised methods. LRS3 results for the low-resource (LR) andhigh-resource (HR) labelled data settings, with 30 and 433 hours of labelled data, respectively. Bestresults in bold, second-best underlined.",
  "USR2231,75915.41.91.9": "methods using self-training that require a costly beam search strategy combining CTC,attention, and language model scores. Our simpler, greedy approach is effective, and we aim toexplore additional offline pseudo-labelling for USR in future work. LRS2.We also compare with the state-of-the-art on the LRS2 dataset (see ). We trainour model using the same hyperparameters as for the high-resource LRS3 setting. Consistent withour LRS3 results from , USR surpasses all other self-supervised methods across ASR, VSR,and AVSR, and outperforms strong supervised methods trained with > 4 more labelled data(433 vs. 1,759 hours). Results on the WildVSR dataset are in Appendix E.",
  "Conclusion": "Despite their similarities, research in VSR, ASR, and AVSR has typically focused on developing sep-arate models for each task. In this paper, we propose unified training strategies that use a single modelto address all three tasks simultaneously. Our USR approach combines self-supervised learning witha greedy pseudo-labelling semi-supervised technique to achieve state-of-the-art results, surpassingrelated methods that use separate models for each task. Future work could explore alternative encoderarchitectures, strategies to improve pseudo-label quality, and methods to incorporate extra audio-onlydata. We hope to inspire further efforts towards consolidating ASR, VSR, and AVSR systems.",
  "Only Imperial College co-authors downloaded, accessed, and used the datasets. Imperial Collegeauthors conducted all of the dataset pre-processing at Imperial College": "O. Abdel-Hamid, A.-r. Mohamed, H. Jiang, L. Deng, G. Penn, and D. Yu, Convolutional neural networksfor speech recognition, IEEE/ACM Transactions on audio, speech, and language processing, vol. 22,no. 10, pp. 15331545, 2014. C.-C. Chiu, T. N. Sainath, Y. Wu, R. Prabhavalkar, P. Nguyen, Z. Chen, A. Kannan, R. J. Weiss, K. Rao,E. Gonina et al., State-of-the-art speech recognition with sequence-to-sequence models, in 2018 IEEEinternational conference on acoustics, speech and signal processing (ICASSP).IEEE, 2018, pp. 47744778.",
  "Y. M. Assael, B. Shillingford, S. Whiteson, and N. De Freitas, Lipnet: End-to-end sentence-levellipreading, arXiv preprint arXiv:1611.01599, 2016": "B. Martinez, P. Ma, S. Petridis, and M. Pantic, Lipreading using temporal convolutional networks, inICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP).IEEE, 2020, pp. 63196323. S. Petridis, T. Stafylakis, P. Ma, F. Cai, G. Tzimiropoulos, and M. Pantic, End-to-end audiovisual speechrecognition, in 2018 IEEE international conference on acoustics, speech and signal processing (ICASSP).IEEE, 2018, pp. 65486552. P. Ma, S. Petridis, and M. Pantic, End-to-end audio-visual speech recognition with conformers, inICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP).IEEE, 2021, pp. 76137617. A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, wav2vec 2.0: A framework for self-supervised learningof speech representations, Advances in neural information processing systems, vol. 33, pp. 12 44912 460,2020. W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdinov, and A. Mohamed, Hubert: Self-supervised speech representation learning by masked prediction of hidden units, IEEE/ACM Transactionson Audio, Speech, and Language Processing, vol. 29, pp. 34513460, 2021.",
  "A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, . Kaiser, and I. Polosukhin,Attention is all you need, Advances in neural information processing systems, vol. 30, 2017": "H. Akbari, L. Yuan, R. Qian, W.-H. Chuang, S.-F. Chang, Y. Cui, and B. Gong, Vatt: Transformers formultimodal self-supervised learning from raw video, audio and text, Advances in Neural InformationProcessing Systems, vol. 34, pp. 24 20624 221, 2021. R. Girdhar, M. Singh, N. Ravi, L. van der Maaten, A. Joulin, and I. Misra, Omnivore: A single model formany visual modalities, in Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition, 2022, pp. 16 10216 112.",
  "B. Shi, W.-N. Hsu, K. Lakhotia, and A. Mohamed, Learning audio-visual speech representation by maskedmultimodal cluster prediction, arXiv preprint arXiv:2201.02184, 2022": "Q. Zhu, L. Zhou, Z. Zhang, S. Liu, B. Jiao, J. Zhang, L. Dai, D. Jiang, J. Li, and F. Wei, Vatlm:Visual-audio-text pre-training with unified masked prediction for speech representation learning, IEEETransactions on Multimedia, 2023. J. Lian, A. Baevski, W.-N. Hsu, and M. Auli, Av-data2vec: Self-supervised learning of audio-visualspeech representations with contextualized target representations, arXiv preprint arXiv:2302.06419, 2023.",
  "T. Afouras, J. S. Chung, and A. Zisserman, Lrs3-ted: a large-scale dataset for visual speech recognition,arXiv preprint arXiv:1809.00496, 2018": "Y. A. D. Djilali, S. Narayan, H. Boussaid, E. Almazrouei, and M. Debbah, Lip2vec: Efficient and robustvisual speech recognition via latent-to-latent visual to audio representation mapping, in Proceedings ofthe IEEE/CVF International Conference on Computer Vision, 2023, pp. 13 79013 801. J. Ao, Z. Zhang, L. Zhou, S. Liu, H. Li, T. Ko, L. Dai, J. Li, Y. Qian, and F. Wei, Pre-training transformerdecoder for end-to-end asr model with unpaired speech data, arXiv preprint arXiv:2203.17113, 2022. A. Elkahky, W.-N. Hsu, P. Tomasello, T.-A. Nguyen, R. Algayres, Y. Adi, J. Copet, E. Dupoux, andA. Mohamed, Do coarser units benefit cluster prediction-based speech pre-training? in ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP).IEEE, 2023,pp. 15. A. Tarvainen and H. Valpola, Mean teachers are better role models: Weight-averaged consistency targetsimprove semi-supervised deep learning results, Advances in neural information processing systems,vol. 30, 2017.",
  "K. Clark, M.-T. Luong, Q. V. Le, and C. D. Manning, Electra: Pre-training text encoders as discriminatorsrather than generators, arXiv preprint arXiv:2003.10555, 2020": "P. Ma, A. Haliassos, A. Fernandez-Lopez, H. Chen, S. Petridis, and M. Pantic, Auto-avsr: Audio-visualspeech recognition with automatic labels, in ICASSP 2023-2023 IEEE International Conference onAcoustics, Speech and Signal Processing (ICASSP).IEEE, 2023, pp. 15. T. Afouras, J. S. Chung, and A. Zisserman, Asr is all you need: Cross-modal distillation for lip reading, inICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP).IEEE, 2020, pp. 21432147. V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, Librispeech: an asr corpus based on public domainaudio books, in 2015 IEEE international conference on acoustics, speech and signal processing (ICASSP).IEEE, 2015, pp. 52065210.",
  "A. Rouditchenko, R. Collobert, and T. Likhomanenko, Av-cpl: Continuous pseudo-labeling for audio-visual speech recognition, arXiv preprint arXiv:2309.17395, 2023": "K. Sohn, D. Berthelot, N. Carlini, Z. Zhang, H. Zhang, C. A. Raffel, E. D. Cubuk, A. Kurakin, and C.-L. Li,Fixmatch: Simplifying semi-supervised learning with consistency and confidence, Advances in neuralinformation processing systems, vol. 33, pp. 596608, 2020. T. Makino, H. Liao, Y. Assael, B. Shillingford, B. Garcia, O. Braga, and O. Siohan, Recurrent neuralnetwork transducer for audio-visual speech recognition, in 2019 IEEE automatic speech recognition andunderstanding workshop (ASRU).IEEE, 2019, pp. 905912.",
  "R. J. Williams and D. Zipser, A learning algorithm for continually running fully recurrent neural networks,Neural computation, vol. 1, no. 2, pp. 270280, 1989": "J.-B. Grill, F. Strub, F. Altch, C. Tallec, P. Richemond, E. Buchatskaya, C. Doersch, B. Avila Pires,Z. Guo, M. Gheshlaghi Azar et al., Bootstrap your own latent-a new approach to self-supervised learning,Advances in neural information processing systems, vol. 33, pp. 21 27121 284, 2020. K. He, X. Chen, S. Xie, Y. Li, P. Dollr, and R. Girshick, Masked autoencoders are scalable visionlearners, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022,pp. 16 00016 009. M. Caron, H. Touvron, I. Misra, H. Jgou, J. Mairal, P. Bojanowski, and A. Joulin, Emerging propertiesin self-supervised vision transformers, in Proceedings of the IEEE/CVF international conference oncomputer vision, 2021, pp. 96509660.",
  "D. Serdyuk, O. Braga, and O. Siohan, Transformer-based video front-ends for audio-visual speechrecognition for single and multi-person video, arXiv preprint arXiv:2201.10439, 2022": "X. Liu, E. Lakomkin, K. Vougioukas, P. Ma, H. Chen, R. Xie, M. Doulaty, N. Moritz, J. Kolar, S. Petridiset al., Synthvsr: Scaling up visual speech recognition with synthetic supervision, in Proceedings of theIEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 18 80618 815. O. Chang, H. Liao, D. Serdyuk, A. Shahy, and O. Siohan, Conformer is all you need for visual speechrecognition, in ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and SignalProcessing (ICASSP).IEEE, 2024, pp. 10 13610 140.",
  "J. Son Chung, A. Senior, O. Vinyals, and A. Zisserman, Lip reading sentences in the wild, in Proceedingsof the IEEE conference on computer vision and pattern recognition, 2017, pp. 64476456": "A. Haliassos, K. Vougioukas, S. Petridis, and M. Pantic, Lips dont lie: A generalisable and robustapproach to face forgery detection, in Proceedings of the IEEE/CVF conference on computer vision andpattern recognition, 2021, pp. 50395049. Y. A. D. Djilali, S. Narayan, E. LeBihan, H. Boussaid, E. Almazrouei, and M. Debbah, Do vsr modelsgeneralize beyond lrs3? in Proceedings of the IEEE/CVF Winter Conference on Applications of ComputerVision, 2024, pp. 66356644.",
  "I. Loshchilov and F. Hutter, Sgdr: Stochastic gradient descent with warm restarts, arXiv preprintarXiv:1608.03983, 2016": "S. Watanabe, T. Hori, S. Karita, T. Hayashi, J. Nishitoba, Y. Unno, N. Enrique Yalta Soplin, J. Heymann,M. Wiesner, N. Chen, A. Renduchintala, and T. Ochiai, ESPnet: End-to-end speech processing toolkit,in Proceedings of the 19th Annual Conference of International Speech Communication Association(INTERSPEECH), 2018, pp. 22072211. S. Watanabe, T. Hori, S. Kim, J. R. Hershey, and T. Hayashi, Hybrid ctc/attention architecture for end-to-end speech recognition, IEEE Journal of Selected Topics in Signal Processing, vol. 11, no. 8, pp.12401253, 2017. A. Varga and H. J. Steeneken, Assessment for automatic speech recognition: Ii. noisex-92: A database andan experiment to study the effect of additive noise on speech recognition systems, Speech communication,vol. 12, no. 3, pp. 247251, 1993.",
  "ALimitations": "USR uses unlabelled samples during fine-tuning via pseudo-labelling, which is more computationallyintensive than standard supervised fine-tuning due to (1) the increased data volume and (2) thehigh cost of pseudo-labelling. However, our semi-supervised approach without pre-training stilloutperforms state-of-the-art self-supervised methods (37.8% vs. 43.4% WER in the LRS3low-resource setting). Additionally, our approach efficiently generates pseudo-labels using a simplethresholding mechanism. Despite this, higher-quality labels are known to improve speech recognition,often enhanced by techniques like beam search, language modelling, and combining CTC andattention scores. We do not explore alternative filtering mechanisms, which we defer to future work.",
  "BSocietal Impact": "Speech recognition technology can greatly benefit people with disabilities who may struggle tointeract with devices using traditional input methods like keyboards. Visual speech recognition canassist individuals with aphonia, who cannot produce voiced speech. It has also been shown thatmodels trained for visual speech recognition can also aid in detecting fake videos by understandingnatural mouth movements . However, speech recognition technology also poses societal risks. It can be exploited for surveillancethrough, e.g., CCTV, necessitating appropriate government regulations. As in other machine learningapplications, there may be biases in the datasets used to train the models. Biases related to gender,age, or ethnic background can lead to reduced performance for underrepresented groups. Addressingthis requires training models on balanced data or employing bias-reduction techniques.",
  "C.1Dataset Details": "LRS3.The LRS3 dataset is the largest publicly accessible audio-visual dataset for continuousspeech recognition with transcriptions. It includes approximately 430 hours of spoken sentences fromTED Talks and features a vocabulary of over 50,000 words spoken by thousands of different speakers.The dataset is collected by automatically tracking faces, synchronising the video/audio streams, andsplitting the videos into individual sentences. The test set comprises roughly 1 hour of utterancesfrom speakers not included in the training set. LRS2.The LRS2 dataset , totalling 223 hours of footage from BBC programs, is the second-largest transcribed audio-visual dataset available for continuous speech recognition. The test set isaround 0.5 hours long. Like LRS3, LRS2 features an unrestricted vocabulary and includes thousandsof diverse speakers. However, LRS3 tends to contain videos of more variable quality, making it amore challenging dataset for VSR. WildVSR.WildVSR is a recent VSR dataset, created by closely following the LRS3 datasetcuration processes. The VSR dataset contains more challenging samples compared with LRS3,leading to significant drops in the VSR performance of models evaluated on WildVSR. The test setcontains around 5 hours of footage. VoxCeleb2.VoxCeleb2 is a large-scale audio-visual dataset containing talking faces of celebri-ties, with about 6,000 speakers and over 2,400 hours of footage. The dataset includes elementslike laughter, cross-talk, music, and other interference, with an unconstrained vocabulary. SinceVoxCeleb2 is multilingual, we use an English-only version curated by , which consists of 1,323hours of footage.",
  "C.3Pre-processing": "We follow the video pre-processing protocol from related works . We remove motionjitter from the videos, crop a 96 96 region centred around the mouth for each frame, and apply agrayscale transformation. We note that raw audio is used without pre-processing. As in ,we tokenise the targets using SentencePiece subword units with a vocabulary size of 1,000.",
  "C.4Model Configurations": "Following , we use three model sizes: Base, Base+, and Large. While the Transformer encodersand decoders vary in size, the feature extractors remain unchanged, consistent with (whichuse the same feature extractors). The configuration of the models is summarised in . Base+corresponds to the Base models used in similar works . We train our Base, Base+, andLarge models on 32, 64, and 128 A100 40GB GPUs, respectively.",
  "C.5Supervised/Semi-supervised Training Settings": "We use consistent settings across supervised training (.1) and semi-supervised training(.2). We train our models using AdamW for 75 epochs with a 20-epoch linearwarmup and a cosine learning rate decay . We use gradient clipping and drop path for regularisation. In addition to the masking discussed in the main text, we also perform randomspatial cropping (size 88 88) and horizontal flipping (probability 0.5) on the videos in a temporallyconsistent manner, as in . The hyperparameter details are presented in . We fix the seedto 42. It takes approximately 12 hours to train the Base model on the labelled data (32 GPUs). It takesaround one, four, and six days to train the Base (32 GPUs), Base+ (64 GPUs), and Large (128 GPUs)models, respectively. Note that Base is trained on LRS3, and Base+ and Large on LRS3+Vox2.",
  "C.6Pre-training Settings": "The pre-training settings are similar. We use a longer schedule (in terms of number of epochs) forLRS3 with 150 total training epochs and 40 warmup epochs. We also use a higher learning rate of5 103. The full settings are given in . It takes approximately two days to pre-train allmodels.",
  "S = Sctc + (1 )Satt + Slm,(10)": "where Sctc and Satt are scores from the CTC and attention branches, respectively, and Slm is theoptional score from a pre-trained language model, which is incorporated through shallow fusion .Following , we set = 0.1 for all experiments. When using a language model, we select from {0.1, 0.2, 0.3, 0.4} based on the validation set.",
  "remains consistent across a range of different thresholds, with no clear improvement when usingseparate thresholds": "Hard versus soft sampling.Our greedy attention pseudo-labelling strategy involves choosing ateach generation step the most likely pseudo-label according to the probability distribution given bythe decoder. For comparison, we consider an alternative soft sampling approach as well. We useweighted sampling at each generation step, drawing a label based on the entire distribution given bythe decoder. Each label has a chance of being selected proportional to its estimated probability. Thisapproach increases the variety of pseudo-labels but may reduce their quality since low-probabilitypseudo-labels are more frequently used. In b we compare the two approaches. We observe that hard sampling outperforms softsampling for all three modalities. Future work can explore alternative methods to effectively increasepseudo-label variety.",
  "D.2Self-supervised ablations": "Mask probability.In a, we compare different mask probabilities for pre-training. A lowmask probability can result in a trivial learning task, whereas a high probability can make the taskoverly challenging. We find that a probability between 0.4 and 0.6 achieves a good balance. Combining targets.During pre-training, targets are generated from audio-visual input and pre-dicted by students using masked auditory, visual, and audio-visual inputs. We explore predicting thecombined targets from all input types by summing the corresponding outputs from the teacher, but asshown in b, this does not yield improvements over simply predicting the audio-visual targets.",
  "In , we closely evaluate the differences between supervised and semi-supervised fine-tuning": "Supervised fine-tuning with few labelled samples is prone to overfitting, necessitating various trainingtricks to improve performance. For example, use a smaller decoder for the low-resourcesetting, different learning rates for the encoder and decoder, and layer-wise learning rate decay .We use our Base model and the low-resource setting to evaluate supervised and semi-supervised (ourdefault) fine-tuning, with and without these strategies. As shown in a, while these trickssignificantly benefit supervised training (consistent with ), they actually hurt semi-supervisedfine-tuning. This suggests that semi-supervised training is less prone to overfitting, making these",
  "regularisation methods unnecessary. In general, we noticed that using semi-supervised fine-tuningresults in less sensitivity to pre-training hyperparameters (e.g., compare Tables 13b and 3a)": "In a, we observed that our semi-supervised fine-tuning benefits most from audiovisual targets.Here, we fine-tune the same pre-trained model using only labeled data to assess the influence oftarget type on supervised fine-tuning. b shows that audio-only targets perform best forsupervised fine-tuning, consistent with findings from other works . As discussed in the maintext, semi-supervised fine-tuning allows the model to leverage the rich and diverse information inaudiovisual targets, which supervised fine-tuning struggles to achieve. : Experiments with auditory noise. We compare USR with the modality-specific BRAVEnmethod on LRS3 with different signal-to-noise-ratio (SNR) levels. We use Base models trained underthe low-resource setting.",
  "GExperiments with Auditory Noise": "We have demonstrated that AVSR slightly outperforms ASR on the clean LRS3 test set. However, itis in the presence of auditory noise that AVSR truly excels, as visual cues help clarify ambiguousutterances. presents ASR and AVSR results under varying levels of audio babble noise fromthe NOISEX dataset . We employ our Base model under the low-resource setting with LRS3 asthe pre-training dataset. Notably, the noise is added to the LRS3 test set, and the model is not trainedon noisy data. We observe that as noise levels increase (and the signal-to-noise ratio decreases), theperformance gap between AVSR and ASR widens. Interestingly, this gap is more pronounced forUSR compared to the modality-specific BRAVEn.",
  "HError Bars": "Due to high computational demands and in line with previous studies , we donot include error bars for our main results. To assess the variability of our method across multipletraining runs, presents the mean and standard deviation over five runs with different randomseeds for our low- and high-resource settings, using our Base model with LRS3 as the pre-trainingdataset. We observe that the results are consistently stable around the mean.",
  "I.1Comparisons with RAVEn/BRAVEn": "RAVEn and BRAVEn pre-train separate Transformer encoders for visual and auditory inputs, whichare then fine-tuned for ASR and VSR. AVSR can be performed through shallow fusion of visualand auditory features. In contrast, USR pre-trains a single student Transformer encoder for auditory,visual, and audiovisual inputs, significantly reducing training and inference costs. We adopt the approach of using a shallow Transformer encoder as a predictor, which has been shownto improve representation learning . However, while RAVEn and BRAVEn use separate predictorsfor visual and auditory features (with BRAVEn also using differently-sized predictors), we use asingle predictor for all modalities, simplifying the architectural design.",
  "I.2Comparisons with AV-data2vec": "AV-data2vec also unifies pre-training by using a single Transformer encoder for all modalities.However, while AV-data2vec employs random modality sampling, we compute all per-modalitylosses at each iteration, amortising the cost of target generation (see .1). AV-data2vecsuse of a scheduler for modality probabilities increases the complexity of the pre-training process.Furthermore, AV-data2vec uses audio-only targets, whereas we use audiovisual targets, which areshown to perform best for our semi-supervised fine-tuning (see .2).",
  "JComparison with AV-CPL": "As mentioned in , the recent AV-CPL method uses pseudo-labelling to train a singlemodel for ASR, VSR, and AVSR, similar to our semi-supervised approach described in .2. compares USR with AV-CPL on the low- and high-resource labelled data settings using theLarge model and LRS3+Vox2 as the pre-training dataset. We observe dramatic WER differencesbetween the two methods, which we attribute to USRs use of CTC-attention training, self-supervisedpre-training, and pseudo-label filtering, among other design choices studied in .",
  "KSummary of the Impact of Semi- and Self-supervised Training": "Sections 4.2, 4.3, and Appendix F demonstrate the impact of self- and semi-supervised learningon speech recognition performance. summarizes the contributions of each component.Self-supervised pre-training on the full LRS3 dataset, followed by supervised fine-tuning on 30hours of LRS3 (see Appendix F), outperforms supervised training on the same 30 hours alone, asexpected. Additionally, semi-supervised training (without pre-training) significantly surpasses theself-supervised baseline. Combining self-supervised pre-training with semi-supervised fine-tuningyields the best results.",
  "LFailure Cases": "presents some failure cases from the LRS3 test set. We evaluated our Large model trained ina high-resource setting with LRS3 and VoxCeleb2. While VSR tends to produce more errors thanASR and AVSR, these errors are often related to phonetically similar sounds, such as this vs. theseor disguised vs. denies. Additionally, using both auditory and visual modalities (AVSR) canimprove the models ability to distinguish challenging samples, such as Mali Wear vs. malware.",
  "The answer NA means that the paper has no limitation while the answer No means thatthe paper has limitations, but those are not discussed in the paper": "The authors are encouraged to create a separate \"Limitations\" section in their paper. The paper should point out any strong assumptions and how robust the results are toviolations of these assumptions (e.g., independence assumptions, noiseless settings,model well-specification, asymptotic approximations only holding locally). The authorsshould reflect on how these assumptions might be violated in practice and what theimplications would be. The authors should reflect on the scope of the claims made, e.g., if the approach wasonly tested on a few datasets or with a few runs. In general, empirical results oftendepend on implicit assumptions, which should be articulated. The authors should reflect on the factors that influence the performance of the approach.For example, a facial recognition algorithm may perform poorly when image resolutionis low or images are taken in low lighting. Or a speech-to-text system might not beused reliably to provide closed captions for online lectures because it fails to handletechnical jargon.",
  "If applicable, the authors should discuss possible limitations of their approach toaddress problems of privacy and fairness": "While the authors might fear that complete honesty about limitations might be used byreviewers as grounds for rejection, a worse outcome might be that reviewers discoverlimitations that arent acknowledged in the paper. The authors should use their bestjudgment and recognize that individual actions in favor of transparency play an impor-tant role in developing norms that preserve the integrity of the community. Reviewerswill be specifically instructed to not penalize honesty concerning limitations.",
  "If the contribution is a dataset and/or model, the authors should describe the steps takento make their results reproducible or verifiable": "Depending on the contribution, reproducibility can be accomplished in various ways.For example, if the contribution is a novel architecture, describing the architecture fullymight suffice, or if the contribution is a specific model and empirical evaluation, it maybe necessary to either make it possible for others to replicate the model with the samedataset, or provide access to the model. In general. releasing code and data is oftenone good way to accomplish this, but reproducibility can also be provided via detailedinstructions for how to replicate the results, access to a hosted model (e.g., in the caseof a large language model), releasing of a model checkpoint, or other means that areappropriate to the research performed. While NeurIPS does not require releasing code, the conference does require all submis-sions to provide some reasonable avenue for reproducibility, which may depend on thenature of the contribution. For example(a) If the contribution is primarily a new algorithm, the paper should make it clear howto reproduce that algorithm.",
  "(b) If the contribution is primarily a new model architecture, the paper should describethe architecture clearly and fully": "(c) If the contribution is a new model (e.g., a large language model), then there shouldeither be a way to access this model for reproducing the results or a way to reproducethe model (e.g., with an open-source dataset or instructions for how to constructthe dataset). (d) We recognize that reproducibility may be tricky in some cases, in which caseauthors are welcome to describe the particular way they provide for reproducibility.In the case of closed-source models, it may be that access to the model is limited insome way (e.g., to registered users), but it should be possible for other researchersto have some path to reproducing or verifying the results.",
  "Guidelines:": "The answer NA means that the paper does not use existing assets. The authors should cite the original paper that produced the code package or dataset. The authors should state which version of the asset is used and, if possible, include aURL. The name of the license (e.g., CC-BY 4.0) should be included for each asset. For scraped data from a particular source (e.g., website), the copyright and terms ofservice of that source should be provided. If assets are released, the license, copyright information, and terms of use in thepackage should be provided. For popular datasets, paperswithcode.com/datasetshas curated licenses for some datasets. Their licensing guide can help determine thelicense of a dataset.",
  "Question: Does the paper discuss both potential positive societal impacts and negativesocietal impacts of the work performed?Answer: [Yes]Justification: See Appendix B.Guidelines:": "The answer NA means that there is no societal impact of the work performed. If the authors answer NA or No, they should explain why their work has no societalimpact or why the paper does not address societal impact. Examples of negative societal impacts include potential malicious or unintended uses(e.g., disinformation, generating fake profiles, surveillance), fairness considerations(e.g., deployment of technologies that could make decisions that unfairly impact specificgroups), privacy considerations, and security considerations. The conference expects that many papers will be foundational research and not tiedto particular applications, let alone deployments. However, if there is a direct path toany negative applications, the authors should point it out. For example, it is legitimateto point out that an improvement in the quality of generative models could be used togenerate deepfakes for disinformation. On the other hand, it is not needed to point outthat a generic algorithm for optimizing neural networks could enable people to trainmodels that generate Deepfakes faster. The authors should consider possible harms that could arise when the technology isbeing used as intended and functioning correctly, harms that could arise when thetechnology is being used as intended but gives incorrect results, and harms followingfrom (intentional or unintentional) misuse of the technology. If there are negative societal impacts, the authors could also discuss possible mitigationstrategies (e.g., gated release of models, providing defenses in addition to attacks,mechanisms for monitoring misuse, mechanisms to monitor how a system learns fromfeedback over time, improving the efficiency and accessibility of ML).",
  ". Safeguards": "Question: Does the paper describe safeguards that have been put in place for responsiblerelease of data or models that have a high risk for misuse (e.g., pretrained language models,image generators, or scraped datasets)?Answer: [NA]Justification: The paper poses no such risks.Guidelines: The answer NA means that the paper poses no such risks. Released models that have a high risk for misuse or dual-use should be released withnecessary safeguards to allow for controlled use of the model, for example by requiringthat users adhere to usage guidelines or restrictions to access the model or implementingsafety filters.",
  ". Institutional Review Board (IRB) Approvals or Equivalent for Research with HumanSubjects": "Question: Does the paper describe potential risks incurred by study participants, whethersuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)approvals (or an equivalent approval/review based on the requirements of your country orinstitution) were obtained?Answer: [NA]Justification: The paper does not involve crowdsourcing or research with human subjects.Guidelines:",
  "The answer NA means that the paper does not involve crowdsourcing nor research withhuman subjects": "Depending on the country in which research is conducted, IRB approval (or equivalent)may be required for any human subjects research. If you obtained IRB approval, youshould clearly state this in the paper. We recognize that the procedures for this may vary significantly between institutionsand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and theguidelines for their institution."
}