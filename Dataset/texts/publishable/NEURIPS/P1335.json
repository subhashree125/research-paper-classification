{
  "Abstract": "Understanding what defines a good representation in large language models(LLMs) is fundamental to both theoretical understanding and practical applications.In this paper, we investigate the quality of intermediate representations in variousLLM architectures, including Transformers and State Space Models (SSMs). Wefind that intermediate layers often yield more informative representations fordownstream tasks than the final layers. To measure the representation quality,we adapt and apply a suite of metricssuch as prompt entropy, curvature, andaugmentation-invarianceoriginally proposed in other contexts. Our empiricalstudy reveals significant architectural differences, how representations evolvethroughout training, and how factors like input randomness and prompt lengthaffect each layer. Notably, we observe a bimodal pattern in the entropy of someintermediate layers and consider potential explanations tied to training data. Overall,our results illuminate the internal mechanics of LLMs and guide strategies forarchitectural optimization and training.",
  "Introduction": "Large Language Models (LLMs) have revolutionized natural language processing by achievingremarkable performance across a wide range of tasks (Muennighoff et al., 2022; Hendrycks et al.,2021). Despite their success, understanding what constitutes a good representation within thesemodels remains an open question. Specifically, how do representations at different layers contributeto downstream task performance, and how can we quantify their quality? However, most previous studies have focused primarily on final-layer representations, often over-looking the potential of intermediate layers. Recent work suggests that intermediate layers may offerricher or more generalizable features for certain tasks (Bordes et al., 2023; Gurnee & Tegmark, 2023;Fan et al., 2024). These observations prompt a deeper investigation into the layer-wise behavior ofLLMs. In this paper, we explore the quality of representations across different layers of LLMs in varioussettings, including different model architectures (Transformers (Vaswani et al., 2017) vs. State SpaceModels (SSMs) (Gu & Dao, 2024)), training checkpoints, input randomness, and prompt length. Ourmain contributions are:",
  "LLM2Vec 8B (Transformer)100%64.7%66.8%Pythia 410M (Transformer)96.6%49.8%53.3%Mamba 130M (SSM)100%46.9%50.9%": "Furthermore, we uncover significant differences in the behavior of these metrics between Transformersand SSMs. Notably, we observe a bimodal distribution in entropy within intermediate layers andinvestigate potential causes, such as the influence of training data examples. Ultimately, our findings provide a deeper understanding of how internal representations developin LLMs and offer practical guidance for model optimization. By illuminating the intricacies ofintermediate layers, we pave the way for improved architectures, better training strategies, and moreefficient utilization of LLM representations.",
  "Related Work": "Understanding representations in neural networks has been a topic of extensive research. Alain& Bengio (2017) analyzed hidden representations to interpret neural networks learning processes.Raghu et al. (2017) introduced Singular Vector Canonical Correlation Analysis (SVCCA) to comparerepresentations across layers and networks, providing insights into learning dynamics. In the contextof Transformers, Liu et al. (2019) studied the linguistic knowledge captured at different layers,finding that lower layers encode more syntactic information while higher layers capture semanticfeatures. Similarly, Jin et al. (2024) showed that semantic concepts are learned in intermediate layersand proposed a layer-wise probing technique to identify the specific layers where these conceptsare formed. On the other hand, state-space models have been less explored in this regard. Gu &Dao (2024) introduced Mamba, an SSM architecture capable of handling long sequences efficiently.However, comparative studies between SSMs and Transformers at the representation level remainscarce. Metrics like entropy and curvature have been used in other contexts to analyze representations.Shwartz-Ziv & Tishby (2019); Shwartz-Ziv (2022) discussed the Information Bottleneck principle,suggesting that networks learn to compress representations. Hosseini & Fedorenko (2023) introducedcurvature as a measure of representational dynamics in recurrent networks. Several works in thevision domain have proposed unsupervised representation quality metrics that are strongly correlatedwith accuracy on downstream tasks (Garrido et al., 2023; Agrawal et al., 2022; Thilak et al., 2024).Notably, the RankMe measure from Garrido et al. (2023) can be shown to be a measure of entropyknown as matrix-based entropy, which we use in our analysis.",
  "Notation": "We consider a batch of N samples, each represented by a D-dimensional vector. Let Z RND bethe matrix of representations, where zi denotes the i-th row of Z. For a matrix M, we use i(M) todenote its i-th largest eigenvalue, and tr(M) to denote its trace. When dealing with sequences, we letx RLd represent the input sequence and y RLd the output sequence, where L is the sequencelength and d is the feature dimension.",
  "We compare two main types of architectures: Transformer-based models (Vaswani et al., 2017) andState Space Models (SSMs) (Gu & Dao, 2024)": "Transformers: Transformers use self-attention layers to capture long-range dependencies within theinput. By computing attention weights between tokens, they can integrate global context at everylayer and scale effectively to large inputs. State Space Models (SSMs): SSMs represent sequence processing using linear state transitionscombined with gating mechanisms. They offer efficient handling of long sequences with linear timeand memory complexity, making them a promising alternative to Transformers.",
  "Token Embedding Diversity Metrics": "Token embedding diversity metrics evaluate the variability and richness of the representations at thetoken level within a single sequence. These metrics are designed to capture how distinctively eachtoken is represented within the context of the entire prompt, providing insight into how effectivelythe model encodes information and differentiates between different parts of the input. Prompt Entropy:Following Wei et al. (2024), we use the -order matrix-based entropy (Giraldoet al., 2014) as a surrogate for Rnyi entropy. For a sequence of token representations Z RLd, theGram matrix is KZ = ZZ. The entropy is:",
  ".(1)": "In this context, prompt entropy quantifies the degree of diversity and dispersion in token embeddingswithin a single sequence. Higher entropy values indicate that the model preserves more nuanced andvaried token-level information. Conversely, lower entropy suggests that the model compresses theinput representations into fewer dimensions or patterns. As such, prompt entropy provides a usefulmeasure of how well the model maintains complexity and richness in its intermediate representations. Unless otherwise specified, we use the limit case = 1 in our calculations. At this limit, the metric isequivalent to the RankMe measure defined in Garrido et al. (2023). We explore the effects of different values in Appendix D. For a more in-depth examination of prompt entropy, refer to Appendix C. CurvatureAs introduced by Hosseini & Fedorenko (2023), curvature measures how rapidlythe direction between two adjacent token embedding vectors changes. Define their difference asvk = zk+1 zk. The average curvature of a prompt is:",
  "Augmentation Invariance Metrics": "These metrics measure how consistently a model represents a prompt when it is perturbed oraugmented. Because augmentations may change prompt length, we average all token embeddings toform a single vector per prompt. Because augmentation may change the prompt length, the token embedding diversity metrics de-scribed in 3.3.1 are no longer suitable. Instead, we average all token embeddings to form a singlevector per prompt and use the metrics described below to measure the similarity between twoaugmentations of the same prompt.",
  "Let Z1 RND and Z2 RND represent two augmented sets of N prompts, where the i-throw in both corresponds to the same original prompt. Details on the augmentation process are inAppendix F": "InfoNCEInfoNCE (Oord et al., 2018) provides a mutual information lower bound between pairedaugmentations. Lower InfoNCE loss suggests that augmentations of the same prompt map to similarrepresentations, indicating invariance to perturbations. This loss is widely used to train augmentation-invariant networks in self-supervised learning for vision and is well-suited to capturing the semanticsimilarity underlying the augmented prompts (Chen et al., 2020a,b; Shwartz Ziv & LeCun, 2024;Ben-Shaul et al., 2023). DiMEDiME (Skean et al., 2023) compares the alignment of paired samples to that of randomlypaired samples. Similar to InfoNCE, it is used to estimate the mutual information between twoaugmented sets of prompts. DiME is grounded in the matrix-based entropy defined in Eq. 1. Inessence, it quantifies how closely the pairings in (Z1, Z2) resemble each other, compared to pairingsof (Z1, Z2) for a permutation matrix . Higher DiME values imply that correct augmentation pairsyield representations that are significantly more similar than random pairings, indicating strongeraugmentation invariance. LiDARLiDAR (Thilak et al., 2024) employs a linear discriminant analysis (LDA) frameworkto assess how well augmentations of a single prompt cluster together. Each prompt is considereda separate class, with its augmentations serving as class samples. By examining the variancesof the linear discriminant components, LiDAR quantifies the tightness of these clusters. HigherLiDAR scores indicate that augmentations belonging to the same prompt form more coherent groups,reflecting stronger invariance. To compute the LDA matrix, LiDAR uses augmentations to construct the class scatter matrix. Inour setup, we use N classes (one for each prompt) and J = 16 samples per class. This is a largersample size than the J = 2 used in DiME or InfoNCE, reflecting the more complex requirements ofcomputing the LDA matrix.",
  "Intermediate Layers Provide Better Representations for Downstream Embedding Tasks": "We begin by evaluating representations at each model layer on a suite of downstream tasks from theMassive Text Embedding Benchmark (MTEB) (Muennighoff et al., 2022). MTEB is designed to testthe performance of LLMs on various embedded tasks. We chose 32 tasks covering classification,clustering, and re-ranking. We use three models: Pythia 410M, Mamba 130M, and LLM2Vec-unsup-simcse (BehnamGhader et al., 2024). Our findings indicate that intermediate layers consistently outperform the final layer across allthree architectures (). Selecting the best-performing intermediate layer yields at least a 2%improvement in average accuracy compared to using the last layer. While prior work (Fan et al., 2024)noted similar trends for generation tasks, our results extend this observation to embedding-basedevaluations.",
  "Downstream Performance and Entropy Are Negatively Correlated": "We next examine how prompt entropy relates to downstream performance on the Massive MultitaskLanguage Understanding (MMLU) benchmark (Hendrycks et al., 2021), which tests comprehensiveknowledge across 57 diverse subjects, covering topics from elementary mathematics to professionallaw. We compare two similarly sized models, Llama3-8B and Mamba2-8B. Despite having the sameparameter count, Llama3 achieves 63.85 0.38% accuracy, far surpassing Mamba2s 26.76 0.37%.We hypothesize that Llama3s intermediate layers compress information more effectively, helping itdiscard irrelevant details and focus on task-relevant features. As shown in , the correlationbetween intermediate-layer entropy and MMLU performance in Llama3 is strongly negative (-0.43between the second and later layers) (). In contrast, Mamba2 shows no such relationship, norevidence of similar compression ().",
  "Experimental Setup for Evaluating Representation Quality": "We now apply the metrics from .3 to quantify representation quality layer-by-layer. Ourexperiments span both Transformers, SSMs, and Pythia (Biderman et al., 2023), including variousscales. We utilize two datasets: WikiText-103 (Merity et al., 2017), representing general text, and aninstruction-based medical dataset (Vsevolodovna, 2024) for more specialized content. This setupallows us to probe how architectural choices and input complexity affect internal representations.",
  "Architectural Differences": "Our analysis reveals notable differences in representation quality between Transformer-based archi-tectures (e.g., Pythia) and SSMs (e.g., Mamba). compares entropy, InfoNCE, LiDAR, andDiME metrics as a function of model depth, normalized to allow fair comparisons across models withdifferent numbers of layers. For entropy and LiDAR, Pythia shows a pronounced decrease at intermediate layers, suggestinggreater information compression and consolidation. In contrast, Mamba maintains more stablevalues, indicating less compression in its intermediate representations. Meanwhile, Mamba exhibitslower DiME and InfoNCE values than Pythia, implying reduced variability in its intermediate-layerrepresentations. Overall, these metric shifts are more pronounced in Pythia than in Mamba, suggesting that Pythiaundergoes stronger representational transformations at intermediate depths. By comparison, Mambasrepresentations remain more uniform across layers. These differences may influence how each modelencodes and leverages information for downstream tasks.",
  "(f) DiME divided by Pr. Ent": ": Training effects are most pronounced in the intermediate layers. Representation metricsacross layers at different training checkpoints (steps 1 to 143k). The x-axis is the depth percentage ofthe model, showing how training influences different layers, particularly those at intermediate depths. The results show that the most significant changes occur in the intermediate layers. As trainingprogresses, prompt entropy in these layers decreases, indicating that the model is learning to compressand abstract input information more efficiently. In contrast, the InfoNCE metric peaks in theintermediate layers, suggesting that the representations become more distinct. Meanwhile, LiDARand DiME values both decline, reflecting a reduction in variability along certain representationaldimensions. Interestingly, the metrics for the earliest layers remain relatively stable throughout training. Thisobservation aligns with the detokenization hypothesis proposed by (Lad et al., 2024), which positsthat initial layers primarily handle the mapping of raw input tokens into an initial embedding space.Their roles appear to solidify early on, exhibiting less ongoing change than the intermediate layers.",
  "Prompt Entropy under Extreme Input Conditions": "To gain deeper insights into how prompt entropy behaves under various input perturbations, weinvestigate the impact of extreme prompt modifications on the models internal representations.Specifically, we analyze how prompt entropy evolves across different layers of the Pythia 410Mmodel when subjected to high levels of token repetition, randomness, or increased prompt length.",
  "We design three types of extreme prompts:": "1. Prompts with Increasing Token Repetition: We select 1,000 standard prompts from theWikiText dataset and randomly replace tokens with a fixed token from the prompt at varyingprobabilities p. As p increases, the amount of repetition in the prompt increases. 2. Prompts with Increasing Token Randomness: We introduce randomness by randomlysubstituting tokens in the prompts with arbitrary tokens from the vocabulary at varyingprobabilities p. Higher values of p correspond to greater randomness in the prompts.",
  "(c) Random Prompt Length": ": Prompt entropy across layers of Pythia 410M under various extreme input conditions.(a) Increasing token repetition leads to decreased entropy in intermediate layers. (b) Increasing tokenrandomness results in higher entropy, especially in initial layers. (c) Unnormalized prompt entropyincreases with prompt length due to the larger number of tokens. These results demonstrate how themodels internal representations adapt to different types of input perturbations.",
  "displays both normalized and unnormalized prompt entropy across different layers for eachtype of extreme prompt. The key observations from this analysis are:": "1. Increasing token repetition reduces entropy in intermediate layers. As the probability p oftoken repetition rises, the model compresses redundant information, leading to lower entropy valuesin the middle layers. This compression indicates that the model effectively recognizes and encodesrepetitive patterns within the input. 2. Increasing token randomness elevates entropy, particularly in initial layers. Introducingrandom tokens enhances the diversity of token representations, resulting in higher entropy values.The initial layers exhibit the most significant increases, suggesting that these layers are more sensitiveto input noise and variability. 3. Prompt length influences entropy in Both normalized and unnormalized Forms. Unnormalizedentropy naturally grows with prompt length due to the increased number of tokens. Although notdisplayed, normalized entropy demonstrates sublinear growth, implying that each additional tokencontributes progressively less to the overall diversity as the prompt lengthens. These findings illustrate that extreme input conditions distinctly affect the models internal representa-tions, especially within intermediate layers. The varying compression and encoding behaviors basedon the nature of input perturbations provide valuable insights into the models processing mechanismsand its capacity to maintain or reduce information complexity under different scenarios.",
  "Bimodal Behavior in Prompt Entropy": "During our analysis of average prompt entropy across different layers, we identified an intriguingphenomenon: a distinct bimodal distribution of entropy values in certain layers of Transformermodels, which was absent in SSMs. presents the entropy distributions for both the WikiTextand AI-Medical-Chatbot datasets (Vsevolodovna, 2024). Notably, the AI-Medical-Chatbot datasetexhibits a pronounced bimodal distribution in the middle layers of Transformer models. This suggeststhat the model processes some prompts in fundamentally different ways at these intermediate stages.To investigate the underlying causes of this bimodality, we conducted several experiments detailedin Appendix A. Our findings indicate that factors such as prompt length, semantic complexity, andoverlap with training data do not account for this behavior. Consequently, the root cause of thebimodal entropy distribution remains an open question.",
  "Discussion and Conclusion": "In this work, we explored the representation quality of intermediate layers in LLMs, providing insightsinto their critical role in downstream task performance. By applying a diverse set of evaluation metrics,including prompt entropy, curvature, InfoNCE, LiDAR, and DiME, we highlighted distinct behaviorsin Transformer-based architectures and SSMs. Our findings demonstrate that intermediate layers",
  "often outperform final layers in representation quality, underscoring their significance for featureextraction and transfer learning": "Transformers exhibited greater representational variability and information compression withinintermediate layers, whereas SSMs displayed more stable and consistent representations. Thissuggests differing strategies in encoding information, with Transformers excelling in adaptability andSSMs prioritizing robustness. Furthermore, the training analysis revealed that the most substantialimprovements in representation quality occur in intermediate layers, reinforcing their importance inlearning dynamics. Our investigation into extreme input conditions revealed that intermediate layers play a pivotal rolein adapting to diverse input scenarios, with distinct responses to token repetition, randomness, andprompt length. Additionally, the observation of bimodal entropy distributions in intermediate layersof Transformer models remains an open question, offering avenues for further research. In conclusion, our research advances the understanding of internal representation dynamics inLLMs, highlighting the pivotal role of intermediate layers and the distinct behaviors of differentarchitectures. These findings not only enhance the theoretical knowledge of model representations butalso provide practical guidance for optimizing model design, training, and application. Future workshould delve deeper into the causes of phenomena such as bimodal entropy distributions and explorethe development of new metrics specifically tailored to LLMs to further enhance representationevaluation.",
  "(c) Llama3 8B": ": Bimodal distribution of prompt entropies observed in intermediate layers. Thedistributions of prompt entropies for WikiText and ai-medical-chatbot datasets are shown for Pythia,Mamba, and Llama3 models. The middle column highlights the layer with the highest Dip Test score(Hartigan & Hartigan, 1985), which measures the degree of multimodality in the entropy distribution.",
  "AInvestigation into Bimodal Distribution of Entropies": "To determine the underlying cause of this bimodal distribution of prompt entropies, we conductedseveral experiments to see if specific properties of the dataset could explain this phenomenon. Ourgoal was to understand whether the bimodality was related to characteristics such as prompt length,semantic complexity, or overlap with training data. Effect of Prompt LengthInitially, we hypothesized that the bimodality might be caused byvariations in prompt length. If one mode corresponded to shorter prompts and the other to longerprompts, it could indicate different processing strategies. However, since the entropy values werenormalized and theoretically invariant to length, this was unlikely. Upon further analysis, weconfirmed that prompt length did not significantly correlate with the observed bimodality. Manual Examination of PromptsWe then manually examined prompts from each mode of thedistribution to identify any distinguishing features, such as difficulty or specific types of medicalterminology. Despite this effort, we found no significant differences between the prompts in eithermode. Both modes contained a similar range of medical complexity and varied use of terminology,",
  "suggesting that the models entropy was not merely a reflection of the difficulty or specificity of theinput": "Training Set OverlapNext, we investigated whether the low entropy mode might be associatedwith prompts that were very similar to samples seen during training. Given that both the ai-medical-chatbot dataset and PILE (Gao et al., 2020) (which Mamba, Pythia, and possibly Llama3 were trainedon) contained medical articles from PubMed, we hypothesized that overlap with training data couldlead to more confident, lower-entropy representations. To test this, we implemented a BM25 index(L, 2024) to quickly search for identical or highly similar articles between the two datasets. While we did find identical articles between the ai-medical-chatbot dataset and PILE, these articleswere evenly distributed across both modes of the bimodal entropy distribution. This suggests thatthe presence of training set overlap does not explain the bimodal behavior, and the underlying causeremains an open question.",
  "The first measure of token embedding diversity we call prompt entropy. This entropy is measured onthe intermediate tokens and captures how diverse the token representations are": "We follow the work of Wei et al. (2024) and use -order matrix-based entropy Giraldo et al. (2014);Skean et al. (2023, 2024), which serves as a tractable surrogate for traditional Rnyis -order entropyRnyi (1961). The quantity is calculated using a similarity kernel on a batch of samples drawnfrom a distribution, without making explicit assumptions on what the true distribution is. The choiceof kernel is flexible and can be any infinitely divisible kernel such as the Gaussian kernel, linearkernel, or Laplacian kernel, among others. For this work, we restrict ourselves to the linear kernel(a, b) = abT . This choice is motivated by the linear representation hypothesis Park et al. (2024)which finds that large language model representations encode high-level concepts such as truth Burnset al. (2022), honesty Mallen & Belrose (2024), and part-of-speech Mamou et al. (2020) in linearlyseparable manifolds. The equation for matrix-based entropy was previously defined in Eq. 1. One way to interpret Eq. 1 isas the -order Rnyi entropy of the Gram matrix eigenvalues1. Notice how each eigenvalue is dividedby tr(KZ) before being raised to the power. This is so that the eigenvalues of KZ sum to one(because tr() = ni=1 i()), which is a necessary condition to treat the eigenvalues as a probabilitydistribution. Futhermore, each eigenvalue of KZ signifies the variance of samples in a particularprincipal component direction Scholkopf & Smola (2018). If entropy is low, then the eigenvalues forma heavy-tail distribution which implies that a few components dominate the variance of samples in Z.On the other hand, at maximum entropy, the eigenvalues form a uniform distribution and samples arespread equally in all directions. Matrix-based entropy is reminiscent of the LogDet entropy whichuses the determinant of KZ to capture how much \"volume\" a dataset occupies Shwartz-Ziv et al.(2023); Zhouyin & Liu (2021). The LogDet entropy is given by SLogDet(Z) = log det(KZ) log 2.One can use Jensens inequality to show that the LogDet entropy is a lower bound of Eq 1 whenlim1 (Appendix J.4 of Shwartz-Ziv et al. (2023)). Depending on the choice of , several special cases of matrix-based entropy can be recovered. Inparticular, when lim1 it equals Shannon entropy (also referred to as von Neumann entropy inquantum information theory Bach (2022); Boes et al. (2019)), and when = 2 it equals collisionentropy. Interestingly, the case of = 2 can be calculated without explicit eigendecomposition Skeanet al. (2024). We show in the Appendix how varying values of affect the matrix-basedentropy of Gram matrices with eigenvalues distributed with a -power law such that i = i. It isshown that for larger values of , smaller eigenvalues contribute more to the entropy.",
  "DBehavior of Matrix-based Entropy for different choices of": "Depending on the choice of , several special cases of matrix-based entropy can be recovered. Inparticular, when lim1 it equals Shannon entropy (also referred to as von Neumann entropy inquantum information theory Bach (2022); Boes et al. (2019)), and when = 2 it equals collisionentropy. Interestingly, the case of = 2 can be calculated without explicit eigendecomposition Skeanet al. (2024). We show in the Appendix how varying values of affects the matrix-basedentropy of Gram matrices with eigenvalues distributed with a -power law such that i = i. It isshown that for larger values of , smaller eigenvalues contribute more to the entropy.",
  "E.1Wikitext Dataset": "We used the wikitext dataset Merity et al. (2017) for the majority of our experiments in .3.This was downloaded from Salesforce/wikitext on huggingface. The dataset consists of 100 milliontokens scraped from the Featured articles on wikipedia. We filtered out prompts which were less than30 tokens or were wikipedia section headings. 1The non-zero eigenvalues of the Gram matrix ZZT are equivalent to those of the covariance matrix ZT Z.Using the covariance matrix instead of the Gram matrix in Eq. 1 makes no difference and is more computationallyefficient if D < N.",
  "The SplitAug augmentation randomly splits words into two parts by adding a space. The RandomCharAug augmentation randomly inserts, substitutes, swaps, or deletes charac-ters": "The Keyboard augmentation randomly substitutes characters with other characters that areat a distance of one as measured on a QWERTY keyboard. For instance, the character \"k\"may be replaced with \"i\", \"l\", \"m\", or \"j\". We use the pseudocode below to do our augmentations using three types of augmentations, usingthe default library settings for each type. When computing augmentation-invariance metrics likeinfoNCE or DiME, we use the two augmented prompts rather than using one augmented promptalongside the original prompt. Note that these augmentations may change the token length T of aprompt.",
  "G.1Increasing Repetition": "We take regular prompts from the wikitext dataset, tokenize them, and then for each token werandomly replace it with probability p. We draw replacements tokens by sampling a random tokenfrom within the prompt. We show examples below for varying levels of p. (p = 0) Mint records indicate the first gold dollars were produced on May 7... (p = 0.1) Mint records indicate the first gold dollars were Mint Mint May 7... (p = 0.5) Mint records Mint Mint Mint gold dollars were Mint Mint Mint 7... (p = 1.0) Mint Mint Mint Mint Mint Mint Mint Mint Mint Mint Mint Mint Mint...",
  "G.2Increasing Randomness": "We take regular prompts from the wikitext dataset, tokenize them, and then for each token we ran-domly replace it with probability p. We draw replacements uniformly from the tokenizer distribution.We show examples below for varying levels of p. Unlike the character-level random noise added toprompts in Section with random noise discussed in Appendix F which might change the number oftokens T of the prompt, the token-level random noise used here does not do so. (p = 0) Mint records indicate the first gold dollars were produced on May 7... (p = 0.1) Mint records indicate salivary first gold dollars were produced on May NaCl... (p = 0.5) Mint records Dallas actively first dollars persufors on Mayder129 18... (p = 1.0) arf emulsion minorensteinorianmega_TOStack potsRecip Installifykeeping...",
  "G.3Random Prompts with Certain Length": "To make a random prompt of a specific length T, we sample T tokens uniformly from the Pythiatokenizer distribution. Such a prompt may look like the following for T = 16: \"PropositionSequencespecific Exp fibers brows Club overviewNos toss Thinking traderMulti indoorlis\". We show how random prompt representations evolve over Pythia training checkpoints in .The random prompts we use are of length 512 tokens. It is readily observed that the prompt entropyis flat across layers in the beginning of training. As training progresses, the model compresses moreand more near the final layers. 0.850.90 0.4 0.5 0.6 0.7 0.8 0.9",
  "Accuracy": "Corr: -0.16 Layer 60 0.750.80 0.15 0.20 0.25 0.30 0.35 0.40 Corr: -0.12 Layer 61 0.750.80 0.15 0.20 0.25 0.30 0.35 0.40 Corr: -0.08 Layer 62 0.700.75 0.15 0.20 0.25 0.30 0.35 0.40 Corr: -0.10 Layer 63 0.60.7 0.15 0.20 0.25 0.30 0.35 0.40 Corr: -0.10 Layer 64 0.350.400.45 0.15 0.20 0.25 0.30 0.35 0.40"
}