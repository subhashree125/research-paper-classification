{
  "Abstract": "Offline-to-online (O2O) reinforcement learning (RL) provides an effective meansof leveraging an offline pre-trained policy as initialization to improve performancerapidly with limited online interactions. Recent studies often design fine-tuningstrategies for a specific offline RL method and cannot perform general O2O learn-ing from any offline method. To deal with this problem, we disclose that there areevaluation and improvement mismatches between the offline dataset and the onlineenvironment, which hinders the direct application of pre-trained policies to onlinefine-tuning. In this paper, we propose to handle these two mismatches simultane-ously, which aims to achieve general O2O learning from any offline method toany online method. Before online fine-tuning, we re-evaluate the pessimistic critictrained on the offline dataset in an optimistic way and then calibrate the misalignedcritic with the reliable offline actor to avoid erroneous update. After obtaining anoptimistic and and aligned critic, we perform constrained fine-tuning to combatdistribution shift during online learning. We show empirically that the proposedmethod can achieve stable and efficient performance improvement on multiple sim-ulated tasks when compared to the state-of-the-art methods. The implementation isavailable at",
  "Introduction": "Offline reinforcement learning (RL) aims to learn a policy from a fixed dataset without additionalinteractions with the environment. This characteristic makes it particularly promising for criticalapplications such as healthcare decision-making , human-AI coordination and autonomousdriving . Generally, the performance of the learned policy relies on the quality of the dataset.Given that the offline data is limited, fine-tuning the policy through interactions with the environmentis still necessary to achieve favorable performance. Consequently, offline-to-online (O2O) RL tendsto achieve faster performance improvements based on better initializations. To effectively fine-tune offline policies, O2O methods are typically designed based on specific offlineRL algorithms. Existing methods can be roughly divided into two groups according to the base offlinemethods they use. The first group relies on policy constraint methods. These approaches aim toimprove online performance by either adaptively adjusting constraints or directly applyingoffline algorithms to online fine-tuning . Unfortunately, these methods often suffer frominefficient performance improvement due to restricted action exploration caused by policy constraints.The second group builds on value regularization methods. These methods aim to prevent excessively",
  "arXiv:2412.18855v1 [cs.LG] 25 Dec 2024": "low Q-values resulting from pessimistic evaluations, such as those in CQL . The goal is toenhance the generalization of the value function and mitigate potential performance declines .Some other methods adopt ensemble Q-learning address these issues. However, thesemethods often face high computational costs due to the need to train multiple Q-networks. Considering that the aforementioned methods develop online fine-tuning algorithms based on specificoffline methods, they often struggle to be applied to other offline methods. To establish a generalO2O framework, it is essential to address the core issues associated with transitioning from offline toonline environments. Inspired of the recent work , which highlights the misalignment betweenthe actor and the critic in an explicit policy constraint method, we identify two mismatches in generalO2O RL: evaluation mismatches and improvement mismatches. Evaluation mismatches primarilyoccur in value regularization methods. These refer to the differences in policy evaluation methodsbetween online and offline settings, which cause severe fluctuations in Q-value estimation duringthe initial stages of fine-tuning. Improvement mismatches, on the other hand, are prevalent in policyconstraint methods. They are often caused by differences in objectives for updating the policy, leadingto a misalignment between the probabilities of actions and their Q-values. Thanks to another recentwork by Xu et al , which connects value regularization and policy constraint methods, we bridgethese two types of mismatches within a unified framework for general RL-based offline algorithms. In this paper, we propose a general O2O framework designed to address both evaluation and im-provement mismatches simultaneously, aiming for stable and favorable fine-tuning performance fromany offline method to representative online methods. To address the evaluation mismatch in valueregularization methods, we propose re-evaluating the offline policy in an optimistic manner usingan off-policy evaluation method. This approach allows us to obtain optimistic Q-value estimates,preventing the dramatic fluctuations in Q-values that could potentially cause the policy to collapse.Although the re-evaluated critic can estimate Q-values optimistically, it suffers from the misalignmentwith the offline policy, causing the improvement mismatch. To handle the improvement mismatch inthe re-evaluated critics and policy constraint methods, we introduce value alignment to calibrate thecritic so that it aligns with the probabilities predicted by the policy. Our approach involves using theQ-value of the most likely action as an anchor and then calibrating other Q-values by either exploitingthe correlation between Q-values of different state-action pairs or modeling Q-values as a Gaussiandistribution. Finally, we propose a constrained fine-tuning framework to guide the policy update byadding a regularization term, with the target of mitigating the negative impact of data shift. Extensiveexperimental results on multiple benchmark environments validate that the proposed methods canachieve better or comparable performance when compared to state-of-the-art methods.",
  "Our contributions can be summarized as follows:": "We systemically study that there exist evaluation and improvement mismatches for offlineRL methods from the perspective of online RL. We show that resolving these two types ofmismatches is essential for achieving general O2O RL. We develop two techniques to address these mismatches. Policy re-evaluation aims toachieve optimistic Q-value estimates, preventing instability in Q-value estimation. Valuealignment calibrates the critic to align with the policy, ensuring consistency between actionprobabilities and their corresponding Q-values. We introduce a constrained fine-tuning framework that incorporates a regularization terminto the policy objective, combating the inevitable distribution shift and ensuring stable andoptimal performance when fine-tuning the policy in online environments.",
  "In this section, we introduce necessary preliminaries about RL, including Markov decision process,and three target online RL methods": "Markov Decision Process (MDP)A Markov Decision Process M is defined by the tuple(S,A,R,P,,) , where S is the state space, A is the action space, P : S A (S) is thetransition function, R : S A R is the reward function, is the initial state distribution, and is adiscount factor. The goal is to learn a policy that maximizes the expected return as",
  "Evaluation and Improvement Mismatches": "In this section, we focus on the differences between online and offline RL and study how thesedifferences impact the performance of online fine-tuning. In general, we summarize these differencesas two types of mismatches, evaluation mismatch and improvement mismatch. The former arises fromthe changes in policy evaluation functions during the transition from offline to online environments;while the latter represents the inconsistency in the objectives for policy updates between offline andonline RL. Most offline RL methods suffer from one or both of these issues, which underscores theimportance of addressing these mismatches to achieve stable and effective online fine-tuning. Evaluation mismatch often occurs in the value regularization methods. For example, in CQL , arepresentative offline method, the policy evaluation function transitions from a pessimistic estimationinherent to offline learning to a more optimistic estimation during online training. This shift frequentlyresults in a sharp increase in Q-values at the beginning of online fine-tuning, which can hinder stableperformance improvements. To address this problem, several attempts have been made to mitigate theexcessive underestimation of out-of-the-distribution (OOD) actions during offline learning or to initialize a pessimistic Q-ensemble to maintain pessimism during online fine-tuning .Unfortunately, these approaches are predominantly tailored to the transition from CQL to SAC,limiting their applicability to other offline algorithms. Improvement mismatch is commonly found in policy constraint methods. Typical examples includeTD3+BC and AWAC , where the objective of actor updates differs significantly from typicalonline methods. In these models, updates to the actor are not solely reliant on the critics evaluation.Consequently, actions that have high Q-values may not automatically translate to high probabilities ofbeing selected, and vice versa. This divergence often misguides the update of policy at the beginningstage of online fine-tuning, resulting in unfavourable performance. Besides, One-step RL and non-iterative methods, e.g., IQL, exhibit both evaluation and improvementmismatches. During the policy evaluation stage, these methods estimate the Q-values based on thebehavior policy or an unknown policy instead of the target policy as in online methods; duringthe improvement stage, because they impose additional constraints on policy updates, they alsoencounter the same issue as discussed above. This can lead to discrepancies in both the assessmentof action values and the subsequent policy optimization, making it hard to achieve effective policyimprovement. Thanks to the recent work , which presents a unified framework for understanding offline RL,we bridge these two mismatches for general RL-based offline algorithms. Formally, the offline RLproblem can be defined by the behavior-regularized MDP problem via maximizing thefollowing objective:",
  "(9)where f() is a regularization function and is the behavior policy": "From Eq. (1) and Eq. (9), we observe a significant divergence in the relation between the actor andthe critic in online versus offline scenarios. Unlike in online cases where the policy update is solelydependent on the Q-function, in offline scenarios, it also critically depends on the data distribution,as indicated by the regularization term in Eq. (9). This distinction highlights why offline methodsoften face evaluation and improvement mismatches when applied to online fine-tuning. Using offlineactor and critic trained by Eq. (9) for initialization in online fine-tuning introduces both types ofmismatches, resulting in unstable and inefficient updates.",
  "Method": "In this section, we introduce our proposed O2O method for handling the mismatches discussed in and the distribution shift problem. To address the pessimistic or unreliable evaluation,such as in CQL and IQL, we develop a policy re-evaluation technique. This technique optimisticallyre-evaluates the well-trained offline policy using an off-policy evaluation method. Although thisre-evaluation helps the critic achieve more optimistic Q-value estimates, unavoidable factors such asfunction approximation errors and partial data coverage can still lead to a misalignment between thecritic and the offline policy. This misalignment means that the action with the highest probabilitypredicted by the policy does not necessarily have the highest Q-value, leading to what we have termedimprovement mismatch. As discussed in in , both the value regularization (after re-evaluation) and policy constraintmethods exhibit improvement mismatch, though the reasons for the mismatch differ between thesetwo approaches. To address this issue, we propose value alignment, which aims to align the criticsestimates with the policys action probabilities, effectively tackling the improvement mismatch inboth types of methods. Finally, to deal with the inevitable distribution shift between offline and onlineenvironments, we develop a constrained fine-tuning framework. This framework ensures that thepolicy consistently updates in the optimal direction by incorporating a regularization term into thepolicy objective. We propose methods for various representative online RL algorithms within a unified framework,including SAC , TD3 , and PPO . These algorithms represent the mainstream approachesin online RL, and are targeted respectively for the off-policy approach with stochastic policies, theoff-policy approach with deterministic policies, and the on-policy approach.",
  "Policy Re-evaluation": "The critic trained on an offline dataset typically maintains pessimistic estimates of Q-values. Whenusing online evaluation methods to fine-tune this critic without any value regularization, the Q-values can experience a dramatic jump, especially for OOD actions, leading to inaccurate Q-valueestimations. To mitigate this problem, we propose re-evaluating the offline policy to acquire a newcritic by employing an off-policy evaluation (OPE) method. The goal is to enable the critic to haveoptimistic estimates of Q-values that more closely approximate the true values. However, directly applying OPE methods for policy evaluation on offline datasets often leads to largeextrapolation errors, as discussed in the previous work . These errors arise due to absent dataand training mismatches. Thanks to the pessimism in offline RL, it is reasonable to assume that awell-trained policy is close enough to the behavior policy or even captures the support set of thebehavior policy. A common assumption is single-policy concentrability , which demonstrateshow concentrated a learned policy is within the given dataset and can be defined as follows.",
  "max(s,a)SAd (s, a)d (s, a) C": "where d (s, a) is the occupancy measure of and C is a constant. Single-policy concentrabilitymeasures the degree to which the state-action distribution induced by the learned policy is coveredby the dataset used for training. By ensuring that the learned policy does not deviate significantlyfrom the behavior policy, the extrapolation error can be greatly reduced . When using arepresentative OPE method called fitted Q-evaluation (FQE), based on Assumption 4.1 and Theorem4.2 in , the upper bound of extrapolation error can be obtained.",
  ": The results of actorsupdated with different critics": "With a powerful neural network and sufficient data, the inherentBellman evaluation error could be tiny. Accordingly, with a largetraining step K, the error will be bounded by an acceptable value.This implies that, given sufficient data, one can achieve a criticwith optimistic property and minor extrapolation error throughpolicy re-evaluation. In practical implementation, for off-policymethods SAC and TD3, we can directly use Eq. (3) and Eq.(6) to re-evaluate the offline policy. For the on-policy methodPPO, we train a critic by fitting the returns of offline trajectories.Considering that the critic can only approximate V (s) ratherthan the true value function, we propose a regularization term in.2, which ensures the critics estimates are reliable andconducive to effective policy improvement.",
  "Value Alignment": "Although the critic after policy re-evaluation possesses the opti-mistic property needed in the online environment, it often does notalign well with the offline policy due to factors such as functionapproximation errors, generalization errors of neural networks,and partial data coverage. Moreover, as discussed in ,policy constraint offline methods also suffer from misalignmentbetween the critic and policy. The misalignment means that theaction with the highest Q-value does not necessarily have the highest probability, often leading tomisleading updates of the policy. To verify this observation, we trained the policy using SAC and TD3 with three different critics: the fixed re-evaluated critic, the iterative re-evaluated critic (which updateswith the policy), and our aligned critic. From , the performance of re-evaluated critics sharplydeclines at the initial stage and does not recover in the subsequent training; while our aligned criticachieves stable and favorable performance. These results indicates that the misalignment between there-evaluated critic and the offline policy can make it difficult for the policy to optimize in a correctdirection, leading to an irreversible decline in performance. Given that the well-trained offline policyis reliable, the desirable critic should not only have optimistic Q-value estimates but also maintainalignment with the offline policy. To achieve this, we propose performing value alignment to calibratethe critic. The main idea is to use the Q-values of the offline policy actions as anchors, keeping themunaltered, and to suppress any overestimated Q-value that exceed these anchors, thereby aligning thecritic with the offline policy. Below, we will discuss how to implement value alignment for differentonline methods.",
  "Q(s, a) = V (s) + log (a|s)(11)": "One intuition behind our method is that actions with higher probabilities typically have more accurateQ-value estimates because these actions are closer to the dataset, and there is sufficient data nearby toobtain a precise estimate. This motivates us to use the Q-value Q(s, a) of the optimal action a tocalibrate any other overestimated action. Assuming that off is the offline policy, we perform valuealignment for any state-action pair (s, a) as follows:",
  "where Qfqe(s, a) are the low Q-values after policy re-evaluation, which do not require calibration,Vfqe(s)=Qfqe(s, a) log (a|s) and Va(s)=Q(s, a) log (a|s)": "During value alignment, we update the policy with Eq. (2) simultaneously for sampling the overes-timated actions. The whole process iterates Eq. (2) and Eq. (13) to obtain a policy that performsequivalently to the offline policy and a critic aligned with it. We denote the policy as on and thealigned critic as Qon for sequential training. Note that Qon, which is modified from the critic obtainedin the policy re-evaluation, does not depend on specific offline critics. This flexibility allows us toimplement the transition to SAC from different offline algorithms.",
  "Unfortunately, in TD3, the actor is modeled as a deterministic policy, lacking an explicit expressionfor Q-values, which prevents us from directly aligning Q-values with the policy": "To solve this problem, our main idea is to model the distribution of Q-values around the policy actiona as a Gaussian distribution. Specifically, in Eq. (5), when we use the offline policy as , thegradient of the policy is only related to the the gradient of Q(s, a) with respect to a. It is easy to seethat the gradient around a should tend to 0 as a is the optimal action selected by the offline policy.Due to the use of smoothing regularization in the critics update in TD3, the Q-values of actions neara differ only slightly from the Q-value of a itself. This enables us to assume that normalized Q-valuesaround a follow a Gaussian distribution Q(s, a)/Q(s, a) N(a, ). Formally, we can calibrate theQ-values of other actions as (see Appendix G.2 for detailed derivation)",
  "(15)": "where k is a constant, which is set as 1 across all tasks, and d(a, a) is a distance measure, whichis defined as the euclidean distance divided by the square root of the action dimension in ourimplementation. From Eq. (15), after calibration, the Q-values of the actions around a are only slightly lower thanQ(s, a), which ensures that a is the output action of the policy while maintaining a smoothing andoptimistic property of Q-values. Moreover, for the actions that differ greatly from a, we limit themaximum of the distance measure d(a, a) in Eq. (15) to the policy noise used in Eq. (6) to avoidsevere underestimation of their Q-values. Formally, we define the objective loss of value alignmentfor O2TD3 asLalignQ(i) = EsR",
  "where Q (s, a) = min(Q(s, a), Q(s, a)) and a is a perturbed action defined in Eq (6)": "O2PPOIn PPO, only the critic V (s) is used to estimate the advantages for policy update. Duringthe re-evaluation process, we train a critic by fitting the returns of offline trajectories as mentioned in.1. This indicates that the re-evaluated critic only approximate V (s) instead of the true one,misguiding the update of the policy. To mitigate this problem, we propose an auxiliary advantagefunction to correct erroneous updates. Generally, a desirable auxiliary advantage function should satisfy the following two conditions: 1) itenables the policy to update in a reliable region; 2) its value must be zero at the beginning of onlinefine-tuning to enable the policy to transition smoothly from offline to online. Considering that thewell-trained offline policy is reliable, we define the auxiliary advantage function as",
  "A(s, a) = log off(a|s) + H(off(|s))(17)": "where H is the entropy of action probabilities predicted by off. It is easy to verify the secondcondition that A(s, a) = 0 at the beginning of offline fine-tuning. To verify the first condition, wederive the following proposition. Its proof can be found in Appendix F.Proposition 4.4. With A(s, a) in Eq. (17), the policy update is regularized by the cross-entropyloss about the offline policy, thereby constraining the policy update in a reliable region.",
  ": Performance curves on D4RL MuJoCo locomotion tasks during online fine-tuning": "with the optimism in online RL, encountering out-of-distribution (OOD) states and actions becomesinevitable, potentially leading to significant performance fluctuations. Especially for OOD states thatare absent in the offline dataset, even though the policy was trained well in the offline phase, it maystill fail to output favourable actions, thereby causing erroneous policy update. Considering that the critic maintains an optimistic nature after undergoing policy re-evaluation andvalue alignment, with the optimistic update way during online fine-tuning, it typically overestimatesthe Q-values of OOD state-action pairs, which can mislead the update of the policy. Inspired ofCMDP , we develop constrained fine-tuning (CFT) to introduce a regularization term toconstrain the current actor and critic, which prevents the policy update from being severely misguided. Specifically, we impose a constraint term f(, ref) on the policy objective to ensure that it updateswithin the credible region of ref, where f(, ) is a divergence measurement and ref is the optimalhistorical policy during online evaluations. Formally, we define the policy objective of CFT as",
  "(20)": "We provide a theoretical guarantee for the proposed CFT framework.Corollary 4.5. With the penalty f(, ref) defined before and appropriate learning rates, algorithmof Eq. (20) almost surely to a fixed point (, , ), where = 0, and are corresponded to and Q, which are optimal in the MDP without constraint.",
  ": The fine-tuning performance achieved by transferring to three online algorithms from theirheterogeneous offline algorithms": "ability to constrain the policy update, we only need to replace off with ref during online fine-tuning. Note that in our methods, at the beginning of online fine-tuning, the regularization functionf((a|s), on(a|s)) is zero for any state, which guarantees no destruction of the alignment for theactor and the critic obtained in .2.",
  "Experiments": "In this section, we perform experiments to validate the effectiveness of the proposed method onD4RL MuJoCo and AntMaze tasks, including HalfCheetah, Hopper, Walker2d and AntMazeenvironments. Specifically, we compare our methods with AWAC , IQL , PEX , Off2On, Cal-QL and ACA . For all methods except for O2PPO, we run 100,000 interactionsteps with the environment and evaluate the policy per 1000 steps, as they all use off-policy methodsfor online fine-tuning. For our O2PPO method, due to the low efficiency of the on-policy method, werun 250,000 interaction steps to validate its performance, with 2500 steps as the evaluation interval.We run all methods with five random seeds and report their averaging results. Due to the spacelimitation, more experimental results can be found in Appendix B and C.",
  "Comparison with State-of-the-Art Methods": "We respectively initialize the policies of O2SAC, O2TD3 and O2PPO from the results of CQL,TD3+BC and IQL. shows the performance curves of our methods and comparing methodson Mujoco tasks. Similar to the previous works , for O2SAC and O2TD3, we use CQL andTD3+BC for offline policy pre-training. From the figure, we can see that our method can convergemore stably and rapidly than other methods and achieve the optimal performance in most cases.Although the ensemble method Off2On can achieve better final performance than our methods insome cases, it often suffers from a dramatic drop in performance at the beginning of fine-tuning,which is often unacceptable in the O2O problems. We report the results of O2PPO in Appendix Abecause of its different number of interaction steps. Although PPO is an on-policy method with lowefficiency, from the results in , it shows significant superiority on sparse reward tasks, eventhough with equal interactions.",
  "Study on Transferability": "In this section, we perform experiments to verify the powerful transferability of the proposed method.As mentioned earlier, one of the advantages of our method is that it imposes no requirements on offline algorithms. This means we can achieve the transfer from any offline RL algorithm to three online RLalgorithms. illustrates the fine-tuning performance of three online algorithms transferringfrom their heterogeneous offline algorithms. From (a), pre-trained with TD3+BC, O2SACoutperforms SAC trained from scratch with a larger margin. Although Online Decision Transformer(ODT) achieves favourable performance in the offline environment, it converges slowly during onlinefine-tuning due to the architecture of the transformer. Our O2TD3 and O2PPO significantly enhancesits performance through online fine-tuning. These results convincingly verify that our method showthe strong transferability from various offline methods.",
  "Conclusion": "In this paper, we disclose there exist two types of mismatches when online fine-tuning offline RLmethods. To address these two mismatches in O2O RL, we proposed optimistic critic reconstructionto re-evaluate an optimistic critic and align it with the offline actor before online fine-tuning, ensuringthe stable performance improvement at the beginning stage and potentially better efficiency dueto the reconstructed optimism consistent with online RL. Furthermore, to combat the inevitabledistribution shift that can hinder the stable performance improvement, we introduce constrainedfine-tuning to constrain the divergence of current policy and the best foregoing policy to maintain thestability of online fine-tuning. These two components form a versatile O2O framework, allowingthe transition from any offline algorithms to three state-of-the-art online algorithms. Experimentsshow our framework can converge to optimal performance without affecting the aligned critic at thebeginning of online fine-tuning and achieve strong empirical performance.",
  "Divyansh Garg, Joey Hejna, Matthieu Geist, and Stefano Ermon. Extreme q-learning: Maxentrl without entropy. arXiv preprint arXiv:2301.02328, 2023": "Cong GUAN, Ke XUE, Chunpeng FAN, Feng CHEN, Lichao ZHANG, Lei YUAN, ChaoQIAN, and Yang YU. Open and real-world human-ai coordination by heterogeneous trainingwith communication. Frontiers of Computer Science, 19(4):194314, 2025. Siyuan Guo, Yanchao Sun, Jifeng Hu, Sili Huang, Hechang Chen, Haiyin Piao, Lichao Sun, andYi Chang. A simple unified uncertainty-guided framework for offline-to-online reinforcementlearning. arXiv preprint arXiv:2306.07541, 2023.",
  "Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learningwith deep energy-based policies. In International conference on machine learning, pages13521361. PMLR, 2017": "Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In Internationalconference on machine learning, pages 18611870. PMLR, 2018. Philippe Hansen-Estruch, Ilya Kostrikov, Michael Janner, Jakub Grudzien Kuba, and SergeyLevine. Idql: Implicit q-learning as an actor-critic method with diffusion policies. arXiv preprintarXiv:2304.10573, 2023.",
  "Hoang Le, Cameron Voloshin, and Yisong Yue. Batch policy learning under constraints. InInternational Conference on Machine Learning, pages 37033712. PMLR, 2019": "Seunghyun Lee, Younggyo Seo, Kimin Lee, Pieter Abbeel, and Jinwoo Shin. Offline-to-onlinereinforcement learning via balanced replay and pessimistic q-ensemble. In Conference on RobotLearning, pages 17021712. PMLR, 2022. Kun Lei, Zhengmao He, Chenhao Lu, Kaizhe Hu, Yang Gao, and Huazhe Xu. Uni-o4: Unifyingonline and offline deep reinforcement learning with multi-step on-policy optimization. arXivpreprint arXiv:2311.03351, 2023.",
  "Ashvin Nair, Abhishek Gupta, Murtaza Dalal, and Sergey Levine. Awac: Accelerating onlinereinforcement learning with offline datasets. arXiv preprint arXiv:2006.09359, 2020": "Mitsuhiko Nakamoto, Yuexiang Zhai, Anikait Singh, Max Sobol Mark, Yi Ma, Chelsea Finn,Aviral Kumar, and Sergey Levine. Cal-ql: Calibrated offline rl pre-training for efficient onlinefine-tuning. arXiv preprint arXiv:2303.05479, 2023. Paria Rashidinejad, Banghua Zhu, Cong Ma, Jiantao Jiao, and Stuart Russell. Bridging offlinereinforcement learning and imitation learning: A tale of pessimism. Advances in NeuralInformation Processing Systems, 34:1170211716, 2021.",
  "RichardS. Sutton and AndrewG. Barto. Reinforcement learning: An introduction. IEEETransactions on Neural Networks, page 285286, Jan 2005": "Shengpu Tang, Maggie Makar, Michael Sjoding, Finale Doshi-Velez, and Jenna Wiens. Lever-aging factored action spaces for efficient offline reinforcement learning in healthcare. Advancesin Neural Information Processing Systems, 35:3427234286, 2022. Denis Tarasov, Alexander Nikulin, Dmitry Akimov, Vladislav Kurenkov, and Sergey Kolesnikov.CORL: Research-oriented deep offline reinforcement learning library.In 3rd Offline RLWorkshop: Offline RL as a Launchpad, 2022.",
  "Chen Tessler, Daniel J Mankowitz, and Shie Mannor. Reward constrained policy optimization.arXiv preprint arXiv:1805.11074, 2018": "Ikechukwu Uchendu, Ted Xiao, Yao Lu, Banghua Zhu, Mengyuan Yan, Josphine Simon,Matthew Bennice, Chuyuan Fu, Cong Ma, Jiantao Jiao, et al. Jump-start reinforcement learning.In International Conference on Machine Learning, pages 3455634583. PMLR, 2023. Shenzhi Wang, Qisen Yang, Jiawei Gao, Matthieu Gaetan Lin, HAO CHEN, Liwei Wu, NingJia, Shiji Song, and Gao Huang. Train once, get a family: State-adaptive balances for offline-to-online reinforcement learning. In Thirty-seventh Conference on Neural Information ProcessingSystems, 2023. Jialong Wu, Haixu Wu, Zihan Qiu, Jianmin Wang, and Mingsheng Long. Supported policyoptimization for offline reinforcement learning. Advances in Neural Information ProcessingSystems, 35:3127831291, 2022. Tengyang Xie, Nan Jiang, Huan Wang, Caiming Xiong, and Yu Bai. Policy finetuning: Bridgingsample-efficient offline and online reinforcement learning. Advances in neural informationprocessing systems, 34:2739527407, 2021.",
  "Total1083.74(31.44)1037.15(18.53)1046.45(27.83)1126.48(138.58)": "Compared with the baseline methods, our methods outperform much in D4RL Mujoco locomotiontasks, as showned in . We can compare the fine-tuning performance in groups based ononline update way. Off2On, Cal-QL, ACA and our O2SAC are updated in the SAC way duringonline fine-tuning. Although there is a lack of the baseline methods that update the policy in theTD3 way, we compare our O2TD3 with a recent O2O method PROTO+TD3 in Appendix C.4 todemonstrate the superiority of our methods. And we compare our O2PPO with the policy constraintmethods AWAC, IQL and PEX, that are implemented in the IQL way, since the idea of PPO is similarto a kind of policy constraint. With such groups for comparison, our methods get the best performance improvements respectively.Note that in our implementation, Off2On achieves much better performance than the original paper and the implementations in other papers and . However, our O2SAC still outperforms itwith less computational cost during online fine-tuning and less requirements for offline policy. In",
  ": Performance of our O2PPO and direct PPO from IQL on D4RL MuJoCo locomotiontasks during online fine-tuning. The solid lines and shaded regions represent mean and standarddeviation": "Due to the low sample efficiency of PPO, we run 250,000 interaction steps for the implementation ofO2PPO, which makes it unreasonable to compare the results of different O2O methods in one graph.So we demonstrate the results of O2PPO without any baseline method, but compare our method withthe direct way of fine-tuning the offline policy of IQL in the PPO way in the online phase. Note thatour purpose is to achieve stable performance improvement, the efficiency depends mainly on theonline algorithm, so in some environments, such as walker2d-medium-replay-v2, the performanceimprovement may be less than the direct fine-tuning. This phenomenon could arise due to the criticscapability to approximate the genuine values effectively, thereby facilitating accurate evaluation ofthe policy, obviating the necessity for corrective adjustments in the policy update direction. Andour O2PPO uses an auxiliary advantage function to constrain the policy update, resulting in slowerperformance improvement. However, since we cannot judge when the critic is reliable, it is necessaryto use O2PPO to achieve stable performance improvement, and there is no remarkable discrepancybetween O2PPO and the direct way, although in the above scenarios. In general, our O2PPO canachieve stable and efficient performance improvement with limited interactions.",
  "Before the analysis of ablation studies, we need to clarify the roles and applicability of the differentcomponents of our methods for different offline methods": "The benefit and applicability of policy re-evaluation As talked before, the purpose of policy re-evaluation is to get optimistic Q-value functions, avoiding the drop of Q-value due to underestimatedQ-values in offline. Many previous works discussed such a problem and solve it by alleviatingexcessive pessimism. Our methods do not need initialize critic from offline results, but re-evaluatethe policy in the online way to get optimistic Q-value functions, which is applicable to differentalgorithms and avoids the performance drop caused by the drastic drop of Q-value. Note thatoptimistic Q-value functions obtained by policy re-evaluation may be unreasonable, especially forOOD actions, hence value alignment is needed for redress overestimated values. In some policy constraint methods , only the policy is limited to update in a region closeto the dataset, but the critic is obtained in the same way as the online update. Therefore, for suchoffline methods, the online critic can be initialized directly by the offline one, and just need to alignthe values with the offline policy. In addition, policy re-evaluation can be applied to the scenarioswhere only the offline policy is provided, such as where the offline policy is obtained in the style ofsequence modeling or non-standard RL . The necessity of value alignment With the optimistic property, Q-value functions obtained in policyre-evaluation will overestimate OOD actions and induce the policy to take such actions, resulting inperformance degradation. For the transition from those policy constraint methods that do not requirere-evaluation of the policy, this case still holds. The purpose of value alignment is to approximate theoptimistic property and suppress the Q-values of OOD actions to a reasonable estimation. For thoseoffline algorithms where the actor and the critic are misaligned like explicit policy constraint methods,if the constraint is removed during online fine-tuning, the critic will induce actor to align with it.However, the actor is reliable but the critic is not, so such process leads to unknown performancechanges that are most likely to be worse. In addition, for algorithms induced by behavior-regularizedMDP, such problem still exists because the update way between offline and online changes, leadingto drastic variation of the critic. Therefore, for O2O RL, it is necessary to align the critic with theactor instead of aligning the actor with the critic. The necessity of constrained fine-tuning Although we keep the optimistic property for Q-valuefunctions and align the critic with actor, it is still challenging to achieve stable online fine-tuning.In general, most of the current offline algorithms focus on how to avoid OOD actions and train areliable policy on the states of the dataset. Due to the optimism in online RL, OOD states and actionsare inevitable, may leading to drastic performance fluctuations, which is undesirable for importantscenarios especially for high-risk scenarios. Especially for OOD states, even trained well in the offlinephase, the policy still fails to output favourable actions, that may causes erroneous policy update.Therefore, for stable O2O RL, online fine-tuning with the constraint of ensuring safe exploration isnecessary.",
  "(f) Ablation results of O2PPO": ": Ablation results of our methods, PR=Policy re-evaluation, VA=Value Alignment,CF=Constrained Fine-tuning. For O2PPO, VA means the use of the auxiliary advantage, andCF means the update of the reference policy. The results of show that even with a narrow dataset e.g. hopper-expert-v2, our methods stillachieve stable online fine-tuning. For O2SAC, the fine-tuning process without the optimistic criticreconstruction can lead a sudden performance drop at the beginning phase, e.g. in (a), asthe offline critic may be severely pessimistic. Due to the alignment of the offline critic and the actorin CQL, using constrained fine-tuning alone can result in overall stable performance improvement, but O2SAC demonstrates a more efficient result. As for O2TD3, fine-tuning from offline directlysuffers from severe performance degradation due to the mismatch of the actor and the critic in offlinetraining, such as in (c). Thanks to value alignment, we can address the mismatch, therebyachieving stable performance improvement at the beginning stage. However, due to OOD statesduring online fine-tuning, constrained fine-tuning is necessary for the long-term stability. At last, PPOis an on-policy method that has a high data quality requirement, so it cannot improve performance ifthe value is incorrectly evaluated, e.g. in (e). Through the reliable auxiliary function, ourO2PPO can improve performance stably even with imperfect value evaluation. Although our methods are universal for any offline algorithm to SAC, TD3 and PPO, some stepscan be omitted according to the type of the offline algorithm. For example, for value regularizationmethod e.g. CQL, we can only use constrained fine-tuning for stable and efficient O2O RL if theconstraint term with a low coefficient has little effect on the critic, as the offline actor and criticare aligned, constrained fine-tuning is enough to combat the problem caused by the drastic jump ofQ-values. And for policy constraint methods with explicit constraint e.g. TD3+BC and AWAC, astalker above, the critic is evaluated in the same way as online way, so policy re-evaluation can beomitted. For other offline methods with the update way that does not match the online way, such asIQL , IVR and -QL , and methods induced by behavior-regularized MDP, and withspecial policy form decision transformer, all steps of our methods are needed for stable O2O RL.",
  "C.1Difference in policy performance caused by optimistic critic reconstruction": "Note that we use on as the initialization of the actor at the beginning of online fine-tuning. Inaccordance with our analysis, the performance of the initial policy on is expected to align with thecharacteristics of the actual offline policy. As depicted in from our empirical experiments, theresults substantiate our analysis, revealing minimal disparities in the performance of on compared tothe actual offline policy. In most environments, on even demonstrates superior performance than theoffline policy. We attribute this to the effectiveness of our min operator, which sensibly tightens thepolicy distribution.",
  "Note that here we run O2PPO with 250,000 environments steps and run O2SAC with 200,000environments steps, while in .1, we run O2PPO with 200,000 environments steps": "Note that as some previous work, in our O2SAC implementation on AntMaze tasks, we do not use thedouble Q networks trick to avoid overly underestimation, and the threshold reaches the maximumin step 100,000. Since there is a lack of hyper-parameters for these tasks in many related work , wedo not compare the results of our methods with other methods. However, according to our simplereproduction, our methods achieve competitive results. O2SAC with optimal reference policyO2SAC with a fixed update intervalO2TD3 with optimal reference policyO2TD3 with a fixed update interval",
  "C.3Comparisons with the results of updating the reference policy update at a fixed interval": "We set the reference policy as the optimal historical policy during online evaluations in our precedingexperiments, and it indeed introduces additional testing information. However, for most of thescenarios that offline RL focus on, this would be acceptable as a well-learned policy can be deployeddirectly without serious consequence due to its decent performance, e.g. in autonomous driving. In addition, thanks to the abundant data, another way that does not require practical evaluation is totrain a transition model that can be used to evaluate the policy by generating synthetic rollouts as theinitial states are provided. Since such a methods is similar to model-based methods, we will leave itfor future work. If evaluation is forbidden, we can replace ref with a recent policy at a fixed interaction interval. TheCorollary 4.3. still holds because the policy performance will be almost certainly improved with along interaction interval, especially compared to the last reference policy. Considering that the policyperformance is most likely to fluctuate in the beginning stage during online fine-tuning, we can set alarge update interval for this stage and a small update interval for the subsequent stage, or reduce thethreshold change range. We experiment our methods with a given update interval on D4RL Mujoco locomotion tasks andcompare the results with those of the methods with the optimal reference policy, shown as and . Without special hyper-parameter optimization, we just update the reference policy per1000 steps for most environments, but for hopper-medium-expert-v2 and hopper-expert-v2, we updatethe reference policy per 10000 steps, and as well as for walker2d-expert-v2 in O2TD3, because thesedatasets are narrow, making it easy for the policy to suffer from OOD states and actions. With anappropriate interval, the policy performance can achieve competitive improvement when comparedwith the methods with the optimal reference policy. Policy re-evaluation and Value alignmentguarantee the smoothing and stable performance improvement at the beginning stage, and constrainedfine-tuning is necessary for the stability of the subsequent stage. Although in hopper-medium-v2,O2SAC with a fixed update interval suffers from the performance degradation at the latter stage,because the threshold is large and the Lagrange multiplier is low, the lack of constraint on the policyupdate in the reliable region. This phenomenon can easily be amended by reducing the thresholdchange range. O2PPO with optimal reference policyO2PPO with a fixed update interval",
  "C.4Comparisons with PROTO": "In the absence of O2O methods on the deterministic policy, we compare our methods with PROTO, a recent O2O method that supports fine-tuning from an offline policy for both the stochasticand the deterministic policy. We reproduce the code of PROTO in Pytorch for the initialization ofthe policies derived from CQL and TD3+BC, and set all hyper-parameters as in the official paper. As shown in , our methods demonstrate significant superiority in both stability andeffectiveness, especially for the deterministic policy.",
  "C.5Comparisons on initialization of different offline methods": "We conducted some experiments for O2SAC with the initialization of different offline methods,including CQL, IQL and ODT. The results are shown in . The initial performance ofO2SAC initialized from ODT is lower than others since the simple behavior cloning (we directlymaximize the likelihood of the actions output by offline ODT while keeping an appropriate entropy)could harm the performance, as discussed in Appendix I. But in hopper-medium-v2, the performanceimproves quickly. We analyze that by the constraint, the policy can recover the offline performance(about 97 normalized score of ODT), as the output of the cloned policy is near the ODT policy. Theresults demonstrate that our methods are suitable for any offline algorithm, even the policies areheterogeneous.",
  "C.6Accelerate learning with sample-efficient RL": "An additional benefit of our methods is their straightforward compatibility with sample-efficientonline RL algorithms. Since we only add a constraint that can be considered as part of the reward, thepolicy iteration process remains consistent with the normal online approach, which makes it feasibleto incorporate techniques from advanced efficient RL algorithms. Drawing from , we conductedsome experiments using a high UTD ratio of 10 (but still update the lagrangian multiplier once perstep) and achieved better performance improvements, as shown in . O2SACO2TD3PROTOPROTO_TD3",
  ": The performance with a high Update-To-Data ratio of O2SAC. The solid lines and shadedregions represent mean and standard deviation": "In policy re-evaluation, since the policy is fixed and the re-evaluation of the critic is straightforward,the computational cost of re-evaluation is significantly lower than that of offline learning. The timecost of value alignment is somewhat higher but still less than that of the offline phase. In fact, thetime cost is approximately proportional to the offline phase according to the alignment steps, sinceboth the actor and critic are updated in the value alignment phase. However, it is worth noting that although we set the training steps for value alignment at 500k, insome environments, only a few alignment steps are needed to calibrate the critic with the offlineactor, as shown in and . Only in antmaze environments, where it is hard for the critic tocapture the sparse reward signal, more alignment steps are necessary. Additionally, in constrainedfine-tuning, since only the lagrangian multiplier is added to be updated and the interaction costdominates, the time cost increases very little. Moreover, in O2O RL, we are typically not concerned about the time cost in the offline process, asdifferent offline methods take different amounts of time. Instead, we prioritize the cost of interactionsduring online fine-tuning. Our method re-evaluates and aligns the critic with the offline actor solelywithin the offline dataset, making the time cost less critical.",
  "DRelated Work": "Unified algorithms for offline-to-online Many offline algorithms are unified across phases in O2ORL , who share the common philosophy of designing an RL algorithm that issuitable for both offline and online phases and then the network parameters trained in the offline phasecan be reused for further learning in the online phase. Most of them are policy constraint methodswhich learn policy without querying OOD samples or penalize action probabilities of themby an explicit estimation of behavior policy , which is beneficial for offline performance butlimits efficient performance improvements in online fine-tuning, as talked about in . For explicitpolicy constraint, recent works focus on how to make the constraint adaptive or loosen itgradually . Although such algorithms achieve great performance in offline phase, there is a lack ofresearch on from such algorithms to more efficient online algorithms. Efficient online fine-tuning Some recent works aim to online fine-tune efficiently with optimisticexploration. propose a unified uncertainty-guided framework to explore optimistically in onlinefine-tuning phase and keep the offline constraint for OOD actions to avoid erroneous update. and utilize ensemble Q-learning to alleviate distribution shift, and implement optimistic explorationby some approaches about ensemble in online RL. For model-based O2O RL, explores regionswith high uncertainty and returns in learned model. and concatenate different offline andonline algorithms by resetting a new online policy learning from the offline policy gradually, whereoptimistic exploration is implemented by the new policy. With a given policy and dataset, focushow to recover the performance of the policy rapidly, whose setting is suitable for our method too. Stable online fine-tuning As offline RL works for some important scenarios with high risk, thestable performance is considerable for O2O RL. After put forward distribution shift problem inO2O RL which is alleviated by pessimistic Q-ensemble and balanced replay proposed by them, manymethods are proposed to combat this problem. From offline to SAC, mitigates the penalty for OOD actions in CQL and calibrates Q-values of them by Monte Carlo returns, while reconstructsQ-functions aligned with offline policy. also induces KL divergence to policy objective, whichis about current policy and the policy at the last iteration to perform a trust-region-style update, butthe performance will drop suddenly at the beginning of online fine-tuning. and utilizeenvironment dynamics model ensemble to obtain uncertainty to penalty OOD actions.",
  "EDetailed Discussions about Mismatch in SOTA Offline Algorithms": "CQL For common implementation of CQL, do not specify exactly how k is updated but onlyprovide properties on the policy exp(fk(s, a)/Z(s)) like SAC. In this way, the actor of CQL is onlyrelated to the critic, and the action probability is directly proportional to Q(s, a) like online RL.However, as the conservative policy evaluation operator used to update the Q-function (accordingto Appendix C, equation 13), the policy is not only related to rewards, but also related to thedistribution of behavior policy. The conservative policy evaluation operator in CQL is:",
  "(at|st) 1])](25)": "which is in according with behavior-regularized MDP proposed by at f(x) = x 1. Theconservative policy objective causes pessimistic Q-values estimation, hence in online fine-tuning, thechange of policy objective leads to drastic Q-values jump and performance degeneration. IQL As expectile regression used to estimate expectiles of the state value function with respectto random actions, Q(s, a) and V (s) are not induced by the policy , and we denote them asQ(s, a) and V (s), where represents a special policy related to the expectile regression . Withinitialization of such Q(s, a) and V (s) in online fine-tuning phase, if we update the actor and thecritic in online algorithm, the actor update will tends to at the beginning, which causes unknownperformance change because the performance of is unknown. And usually the performance willdeteriorate significantly the probability of a satisfactory policy is low. On the other hand, the work of reveals that the optimal critic of IQL can be derived frombehavior-regularized MDP at f(x) = log(x). Meanwhile, the update of actor is derived from returnmaximization with a KL divergence constraint, it is easily to discovery that the learned policy isrelevant to dataset. Therefore, the O2O RL for IQL still suffers from the problem of policy objectivechange. And compared with value regularization methods, there is not only pessimistic estimationproblem but also initial misalignment problem, which means offline actor cannot be derived fromoffline critic in online update way. In fact, in offline RL, the policy update function is to maximize Eq.(9), which is highly related to behavior policy as is a fixed hyper-parameter. If 1 is directly used tofine-tune policy, the policy will tends to be only proportional to Q(s, a) of offline critic, hence thepolicy performance will drop sharply with a great probability. TD3+BC With a explicit constraint of policy update, the improvement mismatch is obvious: the criticupdate follows the Bellman backup, but the actor update does not only follow Q-values maximization,that constrains the actor only update in a region close to the dataset. A little different from the mismatch in IQL, TD3+BC does not suffer from pessimistic estimation since the critic is updatedin an optimistic way as the online way, but such improvement mismatch still leads to performancedrop. The probability of action with the largest Q-value may be not largest, as talked about in ,which is not in according with online RL and leads to erroneous update to destroy performance at thebeginning of online fine-tuning. Such improvement mismatch exists in almost all explicit constraintmethods, but their critics are optimistic because the update ways is the same as online RL, so valuealignment is necessary for O2O RL from such offline methods.",
  "(31)": "Proof of Proposition 4.3 We define Qfqe(s, a) as the state-action values for some actions after theprocess of policy re-evaluation and lower than the calibrated value, i.e. Qfqe(s, a) Q(s, a) (log off (a|s) log off (a|s)). With the min operator in Eq. (12), we can divide actions into twocategories, calibrated actions or standard actions. The former refer to those actions with Q-values tobe calibrated, and the latter refer to those actions with unchanged Q-values since they are lower thanthe calibrated values. We denote them as acal and afqe respectively.",
  "= CELoss(, ref) + C(35)": "Therefore, the term of the auxiliary advantage function regularizes the policy update near the referencepolicy, thereby ensuring reliable update even with inaccurate value estimation. And at the beginningof online fine-tuning, the reference policy is initialized from off. Proof of Corollary 4.5 Since our constrained fine-tuning method solves a constrained MDPproblem by the method in , Theorem 2 in still holds in our method, which can be de-scribed as With the constraint satisfied, RCPO algorithm converges almost surely to a fixed point((, ), (), ).",
  "Assumption According to the convergence of RCPO, we can assume the convergent point((, ), (), ) with = 0": "Derivation As the constraint is E[f((at|st), ref(at|st))] < and ref is the best one among oldpolicies during online evaluations, ref = when the algorithm converges, so the constraint termtends to . According to the update function Eq. (20) of , tends to be reduced, which is incontradiction to the Assumption. Therefore, = 0 when the algorithm converges. Meanwhile, when = 0, the constraint is removed, hence the and Q are the same with theoptimal results in the MDP without constraint, which means our method converges the optimal pointlike online RL but keeps stable improvement.",
  ": Policy performance during valuealignment with different": "Although is generally smaller than 1 after offlinetraining, as we use the energy policy to align criticwith actor, a small has no influence on the recoveryof offline policy but leads to a wide distribution forpolicy, which means the policy modeled by Gaussiandistribution has a large standard deviation, which isharmful for online exploration. If the standard devi-ation is large, some OOD actions may be taken anddangerous state may arise during online exploration,which dose not suit the setting of high risk scenariosand affects favourable performance. Therefore wechoose a suitable which is smaller than 1 but notexcessively small. For Mujoco locomotion tasks,we determine that alpha is 0.2 for the dataset withmedium quality, and is 0.5 for the dataset with expertquality, because small standard deviation induced by a large is advantageous for online explorationof well trained policy. For AntMaze navigation tasks, we set alpha as 0.5 for medium and largedatasets and 0.2 for umaze datasets. Moreover, as we use min operator for value alignment, the limit for is not so strict. If Q-valuesinduced by policy re-evaluation is smaller than alignment objective, as a Gaussian distribution is usedto fit energy policy, such Q-values will leads to a smaller standard deviation.",
  "policy re-evaluation As talked about .1, with a given , online policy evaluation of SACEq. (3) can be used to policy re-evaluation to obtain an optimistic critic": "value alignment In .2, the value alignment method to O2SAC has been described in detail.In brief, we use online policy improvement Eq. (2) to get the actions which have overestimatedQ-values, and use Eq. (13) to inhibit their Q-values to a reliable value calculated by maximumentropy RL. According to the min operator, there is no change if Q-values induced by policy re-evaluation issmaller than alignment objective, so our value alignment method not only aligns Q-values with actorbut also is as consistent with the results of online evaluation as possible, which reduces Bellman errorin online fine-tuning to keep Q-values more stable.",
  "= min0 EsR,a(|s)[log (a|s) log ref(a|s) ](39)": "A constraint related to offline policy is necessary for online fine-tuning, because OOD states willappear surely during online exploration, especially in narrow dataset, which may lead to erroneousoverestimation and destroy old policy. Such constraint can avoid overestimation of actions far awayfrom current policy, hence trust-region style update guarantees stable performance improvement. Compared with the constraint of current policy and offline policy, our proposed constraint guaranteesmore optimal update because the policy updated in trust-region of offline policy generally has similarperformance to offline policy, which leads to not much performance improvement. And compared with the tight constraint of current policy and the policy at last iteration, our con-straint guarantees rapid recovery when some erroneous update occurs which results in performancedegradation, because the Lagrange multiplier will be larger and make the Q-values of OOD actionslower, hence the policy tends to be close to ref. In such situation, our method can recover a similarperformance to ref for current policy rapidly, but the tight constraint needs to take much time to dothat as it constrains the current policy close to the poor policy at last iteration.",
  "policy re-evaluation Similar to O2SAC, online policy evaluation of TD3 Eq. (6) can be used directlyto evaluate an optimistic critic, and in TD3, there is no need for": "value alignment As TD3 models a deterministic policy, value alignment method can not be deriveddirectly from the relationship of the actor and the critic like O2SAC. However, from Eq. (5), thegradient of policy is only related to the the gradient of Q(s, a) to a if we fix the policy as offline policy. So, we have two insights for the actor in TD3: Q(s, (s)) is the maximal in Q(s, ) and the gradientaround (s) should tend to 0, and the latter also corresponds to the idea of policy smoothing updatein TD3.",
  "+ 22 Q(s, a)(43)": "In our implementation, we replace Eq. (43) to Eq. (15), where k is used to control penalty foroverestimated values, and as action range is from -1 to 1, the alignment objective avoids severeunderestimation because the minimum Q does not tends to 0 if we set a small k, which alsoreduces Bellman error during online update as the same thing as min operator does. And we usethe euclidean distance divided by the square root of action dimension to calculate , which is(a, a) = (ai ai)2/|A|. Note that in O2TD3, we use the same trick of reward normalization as TD3+BC, which means wesubtract 1 from all rewards, so we need to consider the situation of negative rewards. In Eq. (43), weset Q(s, a)/Q(s, a) N(a, ) when rewards are positive, so it is natural to set Q(s, a)/Q(s, a) N(a, ) at the situation of negative rewards. Therefore, for the environment with negative Q-values,we use following way to redress critic:",
  "G.3O2PPO": "policy re-evaluation Different from O2SAC and O2TD3, as Eq. (8) is not related to offline policy, inpractice we can only obtain V (s) through fitting the returns. Advantages Aoff(s, a) computed bysuch V (s) may be awfully incorrect, which sequentially causes error update. However, in order tofollow the update way of PPO (only V (s) is used to update), we stick to update state value functionby fitting the returns, and we introduce a modification to advantages used to update.",
  "value alignment As V off(s) obtained in policy re-evaluation is actually V (s), which is incorrectto compute advantages A(s, a) for on-policy update, we propose an auxiliary advantage to redress": "erroneous update. Let us think about the property of the auxiliary advantage. First, as an advantagefunction, its mathematical expectation of offline policy off should be 0. Second, the function shouldbe able to output positive values and negative values for different actions to distinguish the quality ofactions. Last, better actions should correspond higher values. Drawing from the policy form of SAC,we propose the auxiliary advantage as:",
  "A(s, a) = log off(a|s) + H(off(|s))H(off(|s)) = Eaoff(|s)[log(off(a|s))](48)": "When the policy is modeled as Gaussian distribution, it satisfies: (1) Eaoff(|s)[A(s, a)] = 0. (2)A(s, a) > 0 when a 2 < . (3) A(s, a) log off(a|s). And such properties are in accordwith requirements of auxiliary advantage function. It is notable that here is an implicit assumption that the actions with higher probability for off arebetter, and for a well trained offline policy, such assumption should be valid, at least for the beginningof online fine-tuning. With such auxiliary advantage function, we can redress advantages computed byincorrect V (s) to ensure the stable performance improvement during online fine-tuning, especiallyfor the beginning stage. constrained fine-tuning As the auxiliary advantage function has the ability to constrain policy toupdate in a reliable region near off, that means ref = off, we just need to replace ref as the optimalpolicy during online evaluations to constrain online fine-tuning. Meanwhile, with the increase ofinteractions, the critic gradually becomes accurate, approximating V . And on-policy method isnaturally stable to improve performance by updating in a reliable region, therefore the constraintfactor anneals to 0 from 1 during online fine-tuning.",
  "TD3+BC Similar to above, we reproduce the results of TD3+BC by the code from CORL": "Online fine-tuning results For offline-to-online algorithms, for most methods, we reproduce theresults according to their official implementations. For AWAC , IQL and Cal-QL , wereproduce the results by the code from CORL . For ACA and PEX , we reproducethe results by the official open-source code Although PROTO focus on online fine-tuning, but initialize the policy from other offline algorithmsin the official implementation. For the sake of fairness, we rewrite the code according to the paperand official code ( of PROTO in Pytorch, and we use theoffline results of CQL and TD3+BC for initialization respectively. In addition, in the official implementation of Off2On , the policy update 1,000 times per1,000 environment steps, that is different from the common implementation.Therefore, wereproduce by using all parts that related to the prioritized replay in the official code from and we use offline results of CQL for initialization withthe ensemble size of 5, that is the same as the official paper and implementation.",
  "H.2General implementation of our methods": "Network Architecture As we need to re-evaluate policy, which means we only need offline policyand do not use the offline critic, we can modify the critic network architecture for stable onlinefine-tuning. In our implementation, we adopt the same network architecture as offline phase but applyLayer Normalization (LayerNorm) for the output of hidden layers after activation function. indicate that in offline RL, LayerNorm is a good solution to effectively avoid divergence withoutintroducing detrimental bias, leading to superior performance. Moreover, find that LayerNormis favourable for efficient online RL with offline data. Therefore, for stable evaluation and futureconsideration, we decide to apply LayerNorm to online critic network. Initialization of online replay buffer We test our method by initializing the online replay buffer withfour different types: (1) Initialize the buffer with the entire offline dataset akin to . (2) Conduct aseparate online buffer and sample symmetrically from both offline dataset and online buffer akin to. (3) Initialize the buffer with a small number of offline data with high quality akin to . (4)Initialize the buffer without any offline data. For simplicity, we denote them as All, Half, Part, Nullrespectively.",
  "Total scores163.08225.54220.26201.63": "As the results shown in 6, there is little difference among different ways of initialization, except forall, as the the quality of the offline dataset may be low. Although the initialization ways of part andnull also show the competitive results, however, to be consistent with and future efficient update,we decide to adopt Half as our implementation way. It is notable that online policy is still favourableeven if we adopt Null initialization, thanks to our adaptive constrained fine-tuning. loss weight for update Since in Eq. (20) is a variable applied in all states, it may decrease greatlyin a batch. In order to considering more about the situation that needs to be constrained, we modifythe term about in the loss Eq. (20) with a weight.",
  "where = |0.7 I((f((a|s), ref(a|s))) > )| gives a large weight to negative coefficients,thereby constraining the abrupt decrease of": "The choice of the initial value of As is the Lagrange multiplier which is adaptive to the constraint,there is no need to design the initial value of specially. For medium and large datasets of Antmazetasks, we set the initial value as 2.0 since the initial performance is low, and we set the initial value as2.0 for other tasks.",
  "H.3O2SAC implementation": "The choice of constraint threshold As we consider the constraint in O2SAC is KL divergence, andwe model policy as a squashed Gaussian distribution, we can directly the KL divergence of Gaussiandistribution to approximate the constraint. Since ref is the best one among old policies during onlineevaluations and it will be close to with the performance improvement, we can assume that thestandard deviation of ref is the same as the one of . Therefore, according to the KL divergence ofGaussian distribution, we can get:",
  "(50)": "Therefore, we can determine constraint threshold based on the magnitude of change in the mean.For medium dataset, we constrain that | b| < , hence = 0.5. However, due to differentstandard deviations, this threshold may not accurate, and it is an intuitive idea to loosen the constraintat a later stage. So we set as a linearly increasing variable from 0.125 to 2.0 for medium andmedium-replay datasets, which means the allowable range of policy distribution mean is from /2 to",
  ". And for medium-expert and expert datasets, we set it from 0.005 to 0.125 for safe update, whichmeans the allowable range of policy distribution mean is from /10 to /2": "Clipped log likelihood Due to the limit of precision of Pytorch and the squashed Gaussian policyused in O2SAC, when the output action a is near 1.0, like 0.99999999, the log likelihood log (a|s)is will be severely low as the value of action will be computed as 1.0, which will occurs when thelog likelihood is need to be computed by given a action in value alignment phase and constrainedfine-tuning phase. Therefore, for simple calculation, we set the minimum value of log likelihood is-50, a enough small number, which has litter influence on results.",
  "H.4O2TD3 implementation": "The choice of constraint threshold As TD3 models deterministic policy, it is unable to determinethe constraint threshold according to the standard deviation. However, when we inspect the interactionprocess of TD3, we can find that exploration noise is akin to the standard deviation. So we determine from the exploration noise. First we set exploration noise as 0.1 for medium dataset, which is thesame as TD3 learning from scratch. And for expert dataset, we set exploration noise as 0.05 becauseoffline policy is well trained in such dataset, and a lower exploration noise is favourable to avoid pooraction samples. After determination of exploration noise, with the idea of the equivalence of the standard deviationand the exploration noise, we set tau similar to O2SAC. For medium and medium-replay datasets,tau grows linearly from 0.0025 to 0.01, which means the allowable range of policy distributionmean is from /2 to 2, when we consider the exploration noise is equal to the standard deviation.Similarly, for medium-expert and expert datasets, we set it from 0.000025 to 0.000625.",
  "We implement our O2PPO basically according to the code of Uni-O4": "The decay rate of In our implementation for O2PPO, we set the interaction steps as 250,000 sincePPO is an on-policy method which improves performance more slowly than off-policy methods.Since we keep the idea that in medium-expert and expert dataset, the offline policy is well learned,we decay from 1 to 0 linearly in 500,000 steps, which means in the end of our fine-tuning, = 0.5.And for policies trained in other datasets, including AntMaze tasks and medium and medium-replaydatasets of Mujoco tasks, we decay from 1 to 0 linearly during 250,000 steps to reserve morepotential of performance improvement. Clipped standard deviation As shown in 14, the policy trained by IQL algorithm has a large standarddeviation which makes poor exploration performance, especially in hopper-medium-expert-v2 andhopper-expert-v2. Therefore, we set the maximum value of the standard deviations of policies trainedin the two datasets as 0.05, which corresponds to the set of exploration noise in O2TD3, because fora deterministic policy, exploration noise and standard deviation of a stochastic policy are equivalentduring taking actions in exploration.",
  "Shaped and weighted auxiliary advantage For one-dimensional Gaussian distribution f =N(, 2), the entropy is log(": "2e2), which is equal to log(f( )), hence the maximumvalue of A(s, a) in (48) is 1/2. However, the minimum value of it may be much low, which leads tounstable training. Therefore, we use SoftPlus activation function to clip the range of A(s, a). Foreach dimensional, we clip A(s, a) as follows:",
  "Clip(A(s, a)) := SoftPlus(A(s, a) + 4) 4(51)": "By such clipping operator, the range of A(s, a) is (4, 1/2) approximately. Note that when thecurrent policy is close to the reference policy, the actions with overly low logarithmic values are rare,thereby having a little effect on the results. Note that in the implementation of PPO, the normalization of advantages is needed. In order tomake A(s, a) and A(s, a) the same order of magnitude, we reweight A(s, a) by multiplying acoefficient that is the double value of the standard deviation of A(s, a). Therefore, the maximumvalue of A(s, a) is the standard deviation of the batch of A(s, a). Environment Steps(5e3)",
  "IConnect Different Offline and Online RL Methods": "Our methods permit connecting different offline and online RL methods as only offline policy isneeded in our methods. Therefore, for those RL-based methods, which means the policy is modeledas a stochastic policy or deterministic policy in traditional RL way, it is easy to implement stableonline fine-tuning by our methods. For those methods which model policy with different form,such as decision transformer (DT) , our methods can be applied easily if a stochastic policy ordeterministic policy is used to clone the offline policy.",
  "minE(sD,aoff(|s))[ log((a|s)) [ H((|s)]](52)": "The purpose to maximize the entropy is to avoid an excessively narrow distribution, which may causedrastic jump of Q-values in O2SAC as will decay to zero quickly. However, in our experiment, sucha way of behavior cloning will leads to performance degeneration, which is a question for imitationlearning, It is more easy for the behavior cloning of the deterministic policy, MSE loss can be used toupdate the policy for the states in offline dataset. It is notable that degree of coverage has an important influence on behavior cloning, which is still anintractable challenge in imitation learning. Our methods provide the opportunity for the application ofadvanced imitation learning to O2O RL. In addition, even though the policy initialized with imperfectperformance, our methods achieve better online performance than the online algorithm learning fromscratch."
}