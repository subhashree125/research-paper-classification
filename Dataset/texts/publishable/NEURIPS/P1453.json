{
  "Abstract": "We present a novel dataset of simulations of the photodissociation region (PDR)in the Orion Bar and provide benchmarks of emulators for the dataset. Numeri-cal models of PDRs are computationally expensive since the modeling of thesechanging regions requires resolving the thermal balance and chemical compositionalong a line-of-sight into an interstellar cloud. This often makes it a bottleneckfor 3D simulations of these regions. In this work, we provide a dataset of 8192models with different initial conditions simulated with 3D-PDR. We then bench-mark different architectures, focusing on Augmented Neural Ordinary DifferentialEquation (ANODE) based models 2. Obtaining fast and robust emulators thatcan be included as preconditioners of classical codes or full emulators into 3Dsimulations of PDRs.",
  "Introduction": "At the edges of interstellar clouds there exist PDR regions that are dominated by UV photon chemistry.Computational models of these regions are expensive, since both the physical conditions and chemicalcomposition of these regions change as we move deeper into the cloud. In order to simulate thisaccurately, we need an iterative method that solves for heating, cooling and chemistry, creating avisual extinction based problem. The visual extinction is a measure for the decrease in radiationas we move into an astronomical object, and is related to the amount of hydrogen along a line ofsight [Gver and zel, 2009]. By solving the differential equations describing these processes alonga line of sight, we obtain the temperatures and abundances. Having to solve along lines of sightmakes comprehensive 3D simulations of these regions computationally expensive, raising the needfor surrogates that provide approximations of the chemistry, freeing up budget for hydrodynamicsand radiative transport. In this article we provide a dataset inspired by the Orion Bar, a region within the Orion nebulae, thatis currently being thoroughly investigated with the James Webb Space Telescope [Andree-Labschet al., 2017, Habart et al., 2023, Peeters et al., 2024]. The dataset is thus derived from typicalconditions in the Orion Bar, with radiation fields 101 GUV(Draine) 104, number densities102 nH(cm3) 107 and lastly the cosmic ray ionisations 1017 (s1) 1015. We usethese parameters as initial condition for the simulation with 3D-PDR, the resulting 8192 modelstogether we call the 3D-PDR Orion dataset3.[Vermarin and Viti, 2024].",
  "arXiv:2412.00758v1 [astro-ph.GA] 1 Dec 2024": "So far several machine learning based methods have been proposed to tackle problems in astrochem-istry. Emulators have been trained to predict equilibrium chemistry [de Mijolla et al., 2019], timeseries based chemistry [Grassi et al., 2021, Holdship et al., 2021, Tang and Turk, 2022, Branca andPallottini, 2022, Sulzer and Buck, 2023, Branca and Pallottini, 2024] and position based chemistry[Maes et al., 2024]. But most of these have limited accuracy, only generalize to a small part of thephysical parameter space or fail to include the interaction between temperature and chemistry. We then focus on benchmarking surrogate models (emulator) that work for large physical parameterspaces and are effective at emulating a selection of molecules that have complex formation pathways,without having to include the entire chemical network. To this end, we use Augmented NeuralOrdinary equations (ANODE), where the augmented part of the NODE are auxiliary features thatrelate to the physical conditions of 3D-PDR models.",
  "Augmented Neural Ordinary Differential Equations with auxiliary parameters": "The concept of the Neural Ordinary Differential Equations [Chen et al., 2019, Kidger, 2022] isthat instead of defining the right hand side (RHS) of an ordinary differential equation with expertchemistry and physics knowledge, we instead define an approximator f. In this case we use a neuralnetwork, that uses data to generate a nonlinear RHS that can approximate the series. This provides anintegral of the form:",
  "x1f(x, y, e)dx(1)": "where x is the independent variable, y the dependent variables and e auxiliary parameters. Theaddition of auxiliary parameters e, allows us to train a model that generalizes over many differentphysical models with different physical parameters. The usage of parameters to find more expressiveNeuralODEs has been coined as augmented ODEs [Dupont et al., 2019] and parameterized ODEs[Lee and Parish], in this article we employ the term auxiliary parameters distinguish them fromthe physical parameters of the dataset and prevent confusion. Consequently, the acronym of theemployed architecture, ANODE, still fits. These neural differential equations can be combined with encoder and decoder models [Kramer,1991], allowing one to construct a lower dimensional latent ODE, which can be solved at, typically,a lower cost [Grathwohl et al., 2018, Rubanova et al.]. This latent ODE can be defined by a smalldummy chemical network [Grassi et al., 2021], constant terms [Sulzer and Buck, 2023] or a tensorexpression akin to a larger chemical network [Maes et al., 2024].",
  "The 3D-PDR Orion dataset": "The 3D-PDR code [Bisbas et al., 2012] is a flexible code that can simulate photodissociation regionsin both 1D and 3D. For the purpose of this paper, we generated a dataset of 1D models, each with adifferent initial condition of external radiation field GUV,0, density nH,0 and cosmic ray ionisation0, inspired by the Orion cloud. We call this physical parameter space P R3. From this physicalparameter space, we generate a total of 8192 models using Sobol sampling; these models are thendivided into a training, validation and test set with a 0.70, 0.15 and 0.15 split respectively. For each sample in p P, we obtain one series consisting of 215 relative abundances xi(AV ) =ni(AV )/nH,0 as a function of 300 monotonously increasing visual extinctions (AV ), illustrated inappendix A. This gives us a series Xp R300215 for each point in the physical parameter space,p P, where each vector of abundances is linked to one visual depth. Besides the abundances of themolecules, 3D-PDR provides us with outputs such as the extinction AV , gas temperature Tgas(AV ),dust temperature Tdust (AV ), GUV, nH,0 and 0, giving the auxiliary parameters Ep R3006, withthe last two parameters constants. We combine these aforementioned series into the dataset by pickingthe abundances of: e, H, H2, O, CO, H2O, CH, O2, CN, HCO+, NH, HCN, C2, HCO, H2CO, CO+,CS and CH3OH, adding the auxiliary parameters Ep and two of the constant physical parameters,namely nH,0and 0. This results in the complete dataset, with 8192 series of D R30025.",
  "On training neural differential equations": "In this paper, we investigate two different types of architectures. Firstly the we explore the directapplication of ANODE on the data, mapping from Di directly to the next visual extinction Di+1,hereafter called the evolve (e) model. The second architecture uses an additional encoder anddecoder, adapted from the encoder-decoder with bottleneck and latent ODE model from Sulzerand Buck , which applies the same mapping but now with the ANODE in the latent space,hereafter called the encoder-evolve-decode (eed) model. To the latter, we also add the auxiliaryparameters Ep into the encoder with a latent bottleneck of size 5 (eed-a), adding them in the latentspace giving it size 5+4 (eed-b) and adding them into the encoder but enlarging the bottleneck sizeto 9 (eed-c). All these models can be found at In order to effectively train the evolve models, we use several methods. Firstly we use weightdecay [Loshchilov and Hutter, 2017] to penalize large weights in the model, since they can lead toexpensive to evaluate and instable differential equations. Additionally we initialize the weights forthe ANODE with a truncated normal distribution and scale it with a factor b/n [He et al., 2015]with b a hyperparameter and n the width of the inputs into the layer. This ensures the RHS of thedifferential starts out small and improves training. Additionally, we use a learning rate scheduler,namely a cosine delay schedule with a linear warmup phase [Loshchilov and Hutter, 2016].We alsopropose to introduce the series in fractions, first training on the first part, then adding the second partsand so forth. This is combined with the learning rate scheduler, restarting the cosine delay scheduleeach time after introducing a new fraction. In summary for the direct models, we choose, after hyperparameter tuning, a neural network of 592neurons wide, 3 layers deep, a softplus activation function between all layers, a peak learning rateof 1.1 103, a weight scale b = 1.3 with no truncation between -10 and 10, a weight decay of1.2 104, a batch size of 48 and the dataset split into three equal fractions, with each fraction beingexpanded after 50 epochs, with the full dataset being trained on for an extra 50 epochs, resulting in200 epochs in total. We then perform an ablation study to compare the complete model with all thesefeatures, model (e-a), to a model without the weight decay (e-b), a model without the learning rateschedule (e-c), a model with normal weight initialization (e-d), a model without introducing thefractions of training data (e-e), and lastly a model with none of the above (e-f).",
  "Experiments and results": "We find that both the encoder-evolve-decoder and the evolve models can emulate 3D-PDR in asatisfactory fashion. We firstly investigate the MSE over all test samples per visual extinction indexin figure 1. This figure shows that the first four evolve models (e-a through e-d) perform best ingeneral, with every single ablation not performing significantly worse, (e-f) with all features ablated,shows a significantly worse performance. It is not clear which of the improvements to the trainingprocess we proposed is exactly responsible for the improvement in performance, but altogether theyresult in a well trained model. The family of eed models does not perform as well as the evolvemodels for most features, which could be attributed to the small bottleneck size. Only for the AV , theeed models do better, which is counter-intuitive since the AV prediction is reused as an input featurefor the next evolution of the model, but apparently so, the inaccurate AV predictions are sufficient. We show the predictions for a selection molecules, the temperatures and the auxiliary features inferredwith the best model, (e-a) in figure 2, with all individual series in appendix C. The model showsgood agreement for most features, with the molecules at high abundances hard to discern from theoriginal data. Especially the high abundance, and therefore easily observable by telescope, moleculesshow a good agreement between the data and the predictions. This is also well within the underlyinguncertainties of the underlying parameters as specified in the chemical databases [Millar et al., 2024].The molecules at low abundances have many fluctuations in the data, the neuralODE however fits a MSE log(xi) log(T) log(Av) log(Av) index MSE log(nH) log(Av) index log(GUV) log(Av) index log( 0) e-ae-be-ce-de-ee-feed-aeed-beed-c",
  ": The result for one sample as inferred by the (e-a) emulator as a function of the log-AV .The MSE in the feature space for this sample is listed in parenthesis behind each feature": "smooth function to it, providing a desirable smooth interpolation in this regime. This is preferred tooverfitting the jumps in the data since they are artifacts of the 3D-PDR solver, caused by the iterativenature of solving the temperature and chemistry balance. The temperature is fitted well, with the gastemperature having a small deviation at the point where the trend in temperature changes rapidly.Both the changing and constant auxiliary features are reproduced well, with the constant featureshaving the lowest errors of all. The original dataset of 8192 samples was generated on an Intel(R) Core(TM) i9-13900 CPU systemwith 16 cores, taking 432 hours. The training happens on a system with a NVIDIA RTX 2080,taking 2.5 hours for 200 epochs. Inference then takes 6 seconds for 1228 samples including loading,compiling and saving the data. This critical speed up is exactly what we need for simulating PDRs.",
  "Conclusions": "In this paper we introduced the 3D-PDR Orion dataset, which provides 8192 models that capturethe chemistry of the Orion Bar. The emulators that we train on this dataset can be used for higherspatial and temporal resolution simulations, enabling us to resolve PDR regions in more detail. Weshow that the benchmark ANODE architectures can provide high fidelity predictions at a fractionof the computational cost, especially with the addition of the weight decay, learning rate scheduler,small weight initialization and weight decay. With these emulators, we enable fast computation of thechemistry of PDR regions, either by replacing the classical code altogether, or by using the emulatoras a preconditioner for the classical code. In future work, the dataset will be expanded upon to containactual line traces of 3D simulations of different regions, with variable density, visual extinction andcosmic ray attenuation profiles along the line of sight [Gaches et al., 2019].",
  "and Disclosure of Funding": "G.V., R.R. and S.V. acknowledge support from the European Research Council (ERC) Advanced grantMOPPEX 833460. T.G.B. acknowledges support from the Leading Innovation and EntrepreneurshipTeam of Zhejiang Province of China (Grant No. 2023R01008). The authors declare not competinginterests. The ANODEs were implemented using diffrax[Kidger, 2022] and jax[Bradbury et al., 2018].Plots were made using matplotlib [Hunter, 2007]. The dataset was serialized into its final formatusing h5py [Collette, 2013]. S. Andree-Labsch, V. Ossenkopf-Okada, and M. Rllig. Modelling clumpy photon-dominated regions in 3D.Understanding the Orion Bar stratification. Astronomy and Astrophysics, 598:A2, February 2017. ISSN0004-6361. doi: 10.1051/0004-6361/201424287. T. G. Bisbas, T. A. Bell, S. Viti, J. Yates, and M. J. Barlow. 3D-PDR: A new three-dimensional astrochemistrycode for treating Photodissociation Regions. Monthly Notices of the Royal Astronomical Society, 427(3):21002118, December 2012. ISSN 00358711, 13652966. doi: 10.1111/j.1365-2966.2012.22077.x. James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, GeorgeNecula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: Composabletransformations of Python+NumPy programs, 2018. L Branca and A Pallottini. Neural networks: Solving the chemistry of the interstellar medium. Monthly Noticesof the Royal Astronomical Society, 518(4):57185733, December 2022. ISSN 0035-8711, 1365-2966. doi:10.1093/mnras/stac3512.",
  "Will Grathwohl, Ricky T. Q. Chen, Jesse Bettencourt, Ilya Sutskever, and David Duvenaud. FFJORD: Free-formContinuous Dynamics for Scalable Reversible Generative Models, October 2018": "Tolga Gver and Feryal zel. The relation between optical extinction and hydrogen column density in theGalaxy. Monthly Notices of the Royal Astronomical Society, 400(4):20502053, December 2009. ISSN0035-8711. doi: 10.1111/j.1365-2966.2009.15598.x. Emilie Habart, Romane Le Gal, Carlos Alvarez, Els Peeters, Olivier Bern, Mark G. Wolfire, Javier R.Goicoechea, Thibaut Schirmer, Emeric Bron, and Markus Rllig. High-angular-resolution NIR viewof the Orion Bar revealed by Keck/NIRC2. Astronomy & Astrophysics, 673:A149, May 2023. ISSN0004-6361, 1432-0746. doi: 10.1051/0004-6361/202244034.",
  "S. Maes, F. De Ceuster, M. Van de Sande, and L. Decin. MACE: A Machine learning Approach to ChemistryEmulation, May 2024": "T. J. Millar, C. Walsh, M. Van De Sande, and A. J. Markwick. The UMIST Database for Astrochemistry 2022.Astronomy & Astrophysics, 682:A109, February 2024. ISSN 0004-6361, 1432-0746. doi: 10.1051/0004-6361/202346908. Els Peeters, Emilie Habart, Olivier Bern, Ameek Sidhu, Ryan Chown, Dries Van De Putte, Boris Trahin, IlaneSchroetter, Amlie Canin, Felipe Alarcn, Bethany Schefter, Baria Khan, Sofia Pasquini, Alexander G. G. M.Tielens, Mark G. Wolfire, Emmanuel Dartois, Javier R. Goicoechea, Alexandros Maragkoudakis, TakashiOnaka, Marc W. Pound, Slvia Vicente, Alain Abergel, Edwin A. Bergin, Jeronimo Bernard-Salas, ChristiaanBoersma, Emeric Bron, Jan Cami, Sara Cuadrado, Daniel Dicken, Meriem Elyajouri, Asuncin Fuente,Karl D. Gordon, Lina Issa, Christine Joblin, Olga Kannavou, Ozan Lacinbala, David Languignon, Romane LeGal, Raphael Meshaka, Yoko Okada, Massimo Robberto, Markus Rllig, Thibaut Schirmer, Benoit Tabone,Marion Zannese, Isabel Aleman, Louis Allamandola, Rebecca Auchettl, Giuseppe Antonio Baratta, SalmaBejaoui, Partha P. Bera, John H. Black, Francois Boulanger, Jordy Bouwman, Bernhard Brandl, PhilippeBrechignac, Sandra Brnken, Mridusmita Buragohain, Andrew Burkhardt, Alessandra Candian, StphanieCazaux, Jose Cernicharo, Marin Chabot, Shubhadip Chakraborty, Jason Champion, Sean W. J. Colgan,Ilsa R. Cooke, Audrey Coutens, Nick L. J. Cox, Karine Demyk, Jennifer Donovan Meyer, Sacha Foschino,Pedro Garca-Lario, Maryvonne Gerin, Carl A. Gottlieb, Pierre Guillard, Antoine Gusdorf, Patrick Hartigan,Jinhua He, Eric Herbst, Liv Hornekaer, Cornelia Jger, Eduardo Janot-Pacheco, Michael Kaufman, SarahKendrew, Maria S. Kirsanova, Pamela Klaassen, Sun Kwok, lvaro Labiano, Thomas S.-Y. Lai, Timothy J.Lee, Bertrand Lefloch, Franck Le Petit, Aigen Li, Hendrik Linz, Cameron J. Mackie, Suzanne C. Madden,Jolle Mascetti, Brett A. McGuire, Pablo Merino, Elisabetta R. Micelotta, Karl Misselt, Jon A. Morse,Giacomo Mulas, Naslim Neelamkodan, Ryou Ohsawa, Roberta Paladini, Maria Elisabetta Palumbo, AmitPathak, Yvonne J. Pendleton, Annemieke Petrignani, Thomas Pino, Elena Puga, Naseem Rangwala, MathiasRapacioli, Alessandra Ricca, Julia Roman-Duval, Joseph Roser, Evelyne Roueff, Gal Rouill, Farid Salama,Dinalva A. Sales, Karin Sandstrom, Peter Sarre, Ella Sciamma-OBrien, Kris Sellgren, Sachindev S. Shenoy,David Teyssier, Richard D. Thomas, Aditya Togi, Laurent Verstraete, Adolf N. Witt, Alwyn Wootten, NathalieYsard, Henning Zettergren, Yong Zhang, Ziwei E. Zhang, and Junfeng Zhen. PDRs4All - III. JWSTs NIRspectroscopic view of the Orion Bar. Astronomy & Astrophysics, 685:A74, May 2024. ISSN 0004-6361,1432-0746. doi: 10.1051/0004-6361/202348244."
}