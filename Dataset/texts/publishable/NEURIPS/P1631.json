{
  "Abstract": "In question-answering scenarios, humans can assess whether the available in-formation is sufficient and seek additional information if necessary, rather thanproviding a forced answer. In contrast, Vision Language Models (VLMs) typi-cally generate direct, one-shot responses without evaluating the sufficiency of theinformation. To investigate this gap, we identify a critical and challenging taskin the Visual Question Answering (VQA) scenario: can VLMs indicate how toadjust an image when the visual information is insufficient to answer a question?This capability is especially valuable for assisting visually impaired individualswho often need guidance to capture images correctly. To evaluate this capa-bility of current VLMs, we introduce a human-labeled dataset as a benchmarkfor this task. Additionally, we present an automated framework that generatessynthetic training data by simulating where to know scenarios. Our empir-ical results show significant performance improvements in mainstream VLMswhen fine-tuned with this synthetic data. This study demonstrates the potential tonarrow the gap between information assessment and acquisition in VLMs, bring-ing their performance closer to humans. Our dataset and code are available at:",
  "Introduction": "In recent years, Vision-Language Models (VLMs) have made significant strides in general multimodaltasks such as visual recognition and Visual Question Answering (VQA) . This progress hasopened up a vast potential for various applications, including enhancing visual accessibility forvisually impaired individuals , supporting decision-making in autonomous systems ,enabling interactive technologies , etc. Despite these advances, VLMs still fall short of humancapabilities. Humans can intuitively assess whether the available information is sufficient to answer aquestion and seek additional details when necessary . In contrast, VLMs typically tend toprovide direct, single-response outputs even when information is insufficient to answer the questionaccurately. This limitation reduces their effectiveness in real-world applications . To address thisissue, recent studies have explored ways to teach VLMs to assess information sufficiency . Thesestudies aim to have VLMs either provide concrete answers or label questions as unanswerable, usingbenchmark datasets from real user questions like VizWiz . However, a significant gap remains in handling unanswerable cases: deciding what actions to takewhen VLMs identify a question to be unanswerable. Humans naturally possesses the ability toseek additional details when faced with unanswerable questions a challenge often encountered inreal-world VQA tasks due to poor image quality, ambiguous questions, or loss of context . To",
  ": The examples of the Directional Guidance task. The model utilizes self-knowledge todistinguish between known and unknown information and provides guidance on where to find moreinformation": "the best of our knowledge, no existing benchmarks focused on what to do after the model identifiesinformation insufficiency. This active process of information acquisition, fundamental to humancognition, has not been replicated in VLMs and remains largely unexplored. To narrow the gap between VLMs and human intelligence, we suggest going beyond improvingaccuracy on answer generation or merely deciding on information sufficiency. Instead, we focus onenhancing the models capability to provide constructive feedback when encountering unanswerablequestions. In response to this challenge, we introduce a novel VQA task aimed at providing Direc-tional Guidance, which aligns with real-world needs, particularly for visually impaired individuals.As indicated in previous studies , a common issue is that many images taken by visually impairedusers are ill-framed. Our task aims to guide users on how to reframe their images during the interactiveVQA process. This task evaluates the models ability to understand visual direction and determine apotential direction to obtain more relevant information. Moreover, to empower VLM with such guiding capability, we propose an automatic VQA dataaugmentation framework. This framework begins by prompting a pretrained VLM to filter a set ofanswerable questions from the given VQA dataset. The corresponding images are then perturbedusing predefined rules that crop relevant visual information, making it more challenging for the modelto answer the questions correctly. Finally, the VLM is fine-tuned using this augmented dataset, withthe task of providing Directional Guidance on resolving the predefined perturbations. This approachsimulates information inadequacy scenarios and holds promising potential for enhancing the modelsability to guide users in acquiring relevant information. To validate the effectiveness of the approach, we contribute a manually labeled test set containing theDirectional Guidance for real-world unanswerable datasets with images taken by visually impairedindividuals. Our experiments on three popular open-source VLMs show significant improvementsin the models performance on the Directional Guidance task after fine-tuning with our synthetictraining data. Notably, the best-performing model outperforms GPT-4o (CoT) by 3% accuracyscore.",
  "Related Work": "Directional Visual Understanding. Many studies have identified that current VLMs struggle tointerpret and understand spatial relationships within an input image, especially on fundamental visualconcepts like relative directions . This ability is important for interactive VQA applications like autonomous agents , visual navigation, and assistive technologies designed for visuallyimpaired individuals . To enhance VLMs capability to understand directional relationships,researchers construct extensive training data , add assistive visual prompt , or includecollaborative VLMs to communicate and ensemble their decisions . However, these methods oftenrequire heavy data collection or introduce additional models. Previous studies have investigatedvisual learning through simulation , but they rely on virtual interactive environments that maynot accurately reflect real-world scenarios. Another trend involves generating training data byasking questions about directional relationships in existing images , but this also requiresadditional involvement of advanced models. In our study, we aim to improve the models directionalunderstanding with simple data augmentation methods, using images collected from real users. Assistive technology for visually impaired individuals. Over the past decade, applications likeVizWiz and Be My Eyes have used real-time video connections to enhance visual accessibility.VLMs present a more accessible and responsive solution to satisfy the users needs, as they canprovide immediate responses when given a photo-query pair. However, as noted in , visuallyimpaired users often face challenges in capturing clear images. In real-world conditions, many imagessuffer from quality issues such as blurriness, obstructions, and improper exposure, making themdifficult to recognize. These issues often result in divergence in human annotations . Moreover,even when the images are clear, the questions may still be difficult to answer due to the off-framingof the target objects . Addressing these challenges typically requires multiple rounds of queriesand adjustments to properly frame the key object. For VLMs, these difficulties may be amplifiedbecause their training data typically lack examples of unrecognizable images. Additionally, to alignwith the multiple adjustment interaction offered by human operators in Be My Eyes applications,VLMs need to offer honest and effective guidance to navigate to target objects. Self-knowledge. Self-knowledge refers to the models ability to recognize what is known andunknown . When confronted with unanswerable questions due to ambiguity or insufficientinformation, VLMs/LLMs often generate hallucinated responses . Previous research hasintroduced methods to help LLMs understand limitations regarding unknowns . Subse-quent studies, such as , have explored explaining unanswerability by constructing known andunknown datasets through data augmentation and refining base models with a self-curation method.For VLMs, presents a robust visual instruction tuning dataset that includes negative instructionsat different semantic levels, i.e. nonexistent object manipulation, existent object manipulation, andknowledge manipulation, all implemented by GPT-4. Although these studies validate the benefits ofdata augmentation, they have focused on generating negative or unknown data primarily within thelanguage modality. Instead, our study extends this exploration into multimodal data by incorporatingthe vision modality.",
  "The Cognitive Question: From Whats Unknown and Where to Know": "To understand how a statistical model conceptualizes the world, one effective approach is to draw ananalogy to human cognition. In the meta task in NLP (i.e., Question-Answering as other core NLPtasks can be transformed into QA), human cognitive processes in problem-solving and learning aremultifaceted, involving not just the retrieval of stored information but also the recognition of onesknowledge boundaries and the strategic acquisition of new knowledge. To simulate these processes,we propose a hierarchical cognitive process pattern comprising three levels: 1. Response Generation (knowing whats known): At the foundational level, the modelutilizes its existing knowledge base and basic analysis capabilities to generate responses toqueries. This process mirrors the human cognitive function of retrieving known informationfrom memory, akin to recall or recognition tasks in cognitive psychology. It reflectsthe models ability to combine available information into coherent answers. 2. Awareness of Knowledge Limits (knowing whats unknown): The second level reflectsthe models metacognitive ability to evaluate its own knowledge state, recognizing when itlacks sufficient information to answer a question accurately . This awareness iscrucial for intellectual honesty and mirrors the human cognitive process of monitoring andevaluating ones understanding and capabilities, a key aspect of metacognition .",
  ". Knowledge Acquisition Direction (knowing where to know the unknown): At the mostadvanced level, the model identifies pathways for acquiring new knowledge when existing": "information is insufficient. This ability to seek out and engage in learning opportunitiesmirrors the human cognitive strategies for addressing knowledge gaps, such as identifyingresources, formulating questions, or modifying learning strategies. It signifies the modelscapacity for self-guided learning and adaptation, similar to strategic learning and problem-solving in human cognition. As mentioned in , most existing works on VLMs cognitive questions are focused on the firsttwo levels , and the third level is mostly under-explored. We argue that the challenges lie inthe difficulty of collecting suitable data for benchmark and training data: there are few VQA samplesthat exhibit both awareness of knowledge limits and knowledge acquisition direction. Therefore, inour study, we focus on benchmark dataset curation and training data generation.",
  "Directional Guidance Task": "We define our Directional Guidance task as follows: in the context of VQA, given an image-questionpair < I, Q >, the model M should determine whether the image needs to be reframed. To bespecific, if the target object is only partially visible and not sufficient to answer the question, themodel should give clear guidance for the reframing direction (left, right, up, or down). Otherwise,the model should inform whether the question is already answerable (no need to change) or remainsunanswerable even with potential reframing (none of the other options). This task mirrors real-worldscenarios where visually impaired individuals need guidance to position their cameras correctlythrough many attempts. Although the target object might be only partially visible on each attempt,with continuous adjustments under guidance, the user can always capture a better view and finallyhave a better chance to get the question answered. This task goes beyond simply detecting theill-framing issue of the image: it assesses whether the framing issue impacts the models ability toanswer the specific question posed. For example, reframing may not be necessary if the question canbe directly answered with the available visual information even if the image is ill-framed. We regardthese guide responses as an additional output that complements the original VQA answering process. This task exemplifies three levels of the hierarchical cognitive pattern discussed in . Insteadof a binary classification of answerable/unanswerable as proposed in , this task emphasizes themodels ability to effectively utilize available visual information. It requires the model to assesswhat is known and determine where to acquire extra information, standing in the transition fromunanswerable to answerable.",
  "Directional Guidance Dataset": "Benchmark dataset. To evaluate model performance in our task setting, we created a benchmarkdataset derived from VizWiz dataset families . The VizWiz dataset consists of real VQA queriescollected from visually impaired individuals . From this dataset, we used all the unanswerablesamples (1.4k) from the validation set as the training set may have potential leakage issues duringthe pre-training process of VLMs. We invited 26 human annotators to identify ill-framed photosand label the most promising direction to move the camera, by which the reframing action couldpotentially help to answer the question (more details are available in Appendix A.1). After cleaningand re-evaluation, we collected 291 samples where reframing could potentially lead to an answer, and230 samples unlikely to be answered even with reframing. The rest samples are where the humanannotators have disagreements. The details of the data collection are presented in the SupplementaryMaterials. To ensure a balanced distribution in the test set, we randomly selected 300 samplesfrom the VizWiz-grounding test set and simulated the case where the current image already hassufficient information to answer the question, under the assumption that the visual evidence could betheoretically grounded in the image. Combining those three groups, we get a high-quality DirectionalGuidance benchmark dataset including 821 samples. Despite the size of the dataset being relativelysmall, this reflects the inherent challenge of the task, where ill-framed images are rare in standardVQA datasets but commonly seen in real-world scenarios. Furthermore, our datasets diversity andcomprehensiveness make it suitable for evaluating model performance on the target task, providing avaluable foundation for future studies.",
  ": The training set generation framework": "Training dataset. We propose a data augmentation process to simulate the ill-framed samples,instead of collecting ad-hoc images that suit the task. Initially, we take all the training samples from adataset pool - the validation set of VizWiz-grounding dataset as it includes the visual groundingsfor each answerable VQA query. With that visual grounding information, manual perturbations havebeen applied to simulate ill-framing. Specifically, we identify the bounding box surrounding thetarget object and divide it into 10 zones, horizontally and vertically. We then choose a specific zonefor cropping, resulting in an image that has some missing information while retaining a part of thetarget object. With a series of perturbations, we observe the consistency of the models response to theinitial VQA query and capture the cases where an ill-framing issue impacts the question-answering.As the VizWiz is an open-ended task, we use precision as the evaluation:",
  "w |P(w)|(1)": "P(w) and T(w) denote a word from the model prediction and from the ground-true answer. Precisioncalculates how many words in the predictions also appear in the ground-truth answer, and we set athreshold e to identify the correctness. Following , only non-stop words have been taken intoconsideration. and algorithm 1 outline the process of generating training data with guidancelabels. Another crucial case in the benchmark test set involves samples that remain unanswered even afteradjusting the camera. One more data argumentation technique has been placed: we mismatchthe questions and images from the same dataset pool to create new pairs with different semanticinformation. Most questions in the original dataset pool are generic, as a highly frequent question isWhat is this? without semantic information. Correspondingly, our GPT-4 enabled argumentationhelped rephrase the paired question and answer. For example, given an image Ii with the questionQi What is this? and an answer Ai laptop, the new question Qi will be rephrased to Whatsthe color of this laptop?. Then, we mismatch the Qi with another irrelevant image Ij to form a newpair. This augmentation generates complex, real-world queries where straightforward answers areinfeasible, compelling models to learn deeper semantic information.",
  "Experiment settings": "Model Selection. To verify the feasibility and effectiveness of our approach for different modelarchitectures and sizes, we analyze the experiments of four mainstream open-source large modelswith different sizes, including: LLaVA-1.5 , InstructBlip , GPT-4o , and CLIP . First,we benchmark the test set on the LLaVA-1.5, InstructBlip, and GPT-4o on the zero-shot setting. Aseries of prompts has been designed to test their zero-shot performances, serving as our baselines.Next, we generate a training dataset using algorithm 1 and apply LoRA fine-tuning on theopen-sourced models. We anticipate the effectiveness of our proposed training framework will bereflected by the improvement of model performance compared with the zero-shot baseline. Task format. To quantitatively analyze the models ability to provide guidance, we format the taskwith a basic VQA multiple choice template: <image>{Original_Question} To improve theimage and answer the question, how should the camera be moved?A.Leave itunchanged.B.Left.C.Right.D.Up.E.Down.F.None of the other options.Each option reflects the models decision of Directional Guidance: The leave it unchangedoption indicates that the current image contains all the necessary information to answer the question.The four directional options suggest that the relevant object is only partially visible, and furtherimage adjustment is needed. The None of the other options implies that moving the camerawill not help because the question is inherently unanswerable, i.e. due to the ambiguity, or therelevant object is absent from the current image. We use the F1 score and accuracy as the evaluationmetrics and also analyze the confusion matrix of the different options. Zero-shot prompt setting. For the zero-shot baseline, we enhance the basic template with addi-tional instructions and explanations tailored for each model. We designed two prompt settings toaccommodate their varying capabilities. The first setting is a single-round query where the modelmakes predictions from six options directly. The second setting is a two-round prompt, following theChain-of-Thought process. This two-round prompt decomposes the tasks and works as follows:Initially, we prompt the model to determine if the target object is fully present in the image, partiallyvisible, or if the question is unanswerable. The corresponding options are: leave it unchanged,reframe, and none of the other options. If the model indicates that the target object is onlypartially visible, we then ask it to decide a specific direction for movement: left, right, up, ordown. To ensure reproducibility, we include all prompts we used in Supplementary Materials A.5. Fine-tune setting. In our training framework, we utilize data augmentation to generate potentialsamples with guidance labels. We assess the consistency of the models predictions before and afterperturbations and categorize the samples into two groups. Samples where the model fails to predictpost-perturbation are considered positive, and their Directional Guidance labels are assigned one offour directions: left, right, up, or down. Conversely, samples where the model maintains correctpredictions are labeled as negative, with the Directional Guidance label set to leave it unchanged.Upon analyzing these groups, we observed that negative samples predominated the generated trainingset. To ensure a balanced distribution within the training dataset, we under-sampled the negativesamples to align with the average count of the four directional categories. We also adjusted thenumber of None of the other options samples to achieve an even distribution across the entiretraining set. After generating the training set, we format the new pairs into a standardized instruction fine-tuninglayout: each sample, comprising < I, question >, is supplemented with option choices andinstructions. Since the task requires models to respond with a single letter, the prediction processis equivalent to a classification task. Following the settings in , the loss is only computed onthe token for the chosen letter and the < eos > (end of the sentence) token. Also, to prevent themodel from memorizing the letter distribution, we randomly shuffle the association between lettersand options, ensuring each letter (from A to F) is paired with an option randomly in each trainingsample. When fine-tuning each model, most training configurations follow the officially suggestedsettings, and more training details are presented in Supplementary Materials A.2. None-generative Models. Since the task has been simplified to a classification problem, we alsoinvestigate whether a non-generative model with a simpler architecture could suffice. Accordingly, weadd a linear probe layer onto CLIP and perform a classification head, using a vision encoder (CLIP-ViT-L-336px) aligned with LLaVA-1.5 and a text encoder in the default setting. We concatenate theimage feature and the text feature as the input for the classification head. Since the CLIP model cannot generate open-ended answers, we use the synthetic training dataset generated by LLaVA-1.5 13b.",
  "Directional Guidance benchmark dataset and baseline performance": "(a) shows the distribution of four directions in our benchmark dataset. The horizontal directionsare the most common, with left at 38.5% and right at 29.6%. The figure displays four typical samplesfrom each direction. We also identified a frequent scenario where users need to take another photoand attempt a different question, as shown in (e). This pattern reveals a common challenge forvisually impaired individuals: without continuous guidance, the user and assistant can easily lose thecontext of the original VQA. These findings emphasize the importance of providing clear, sequentialdialogue-based guidance for effectively adjusting the camera position. As mentioned in .3, we use different prompt settings for each group of models that suittheir capabilities. For the 7b models, we use the two-round prompt because these models benefitfrom a more structured, step-by-step approach, which helps them handle the task more effectively.In contrast, we tested the LLaVA-1.5 13b and GPT-4o model with a single-round of prompting tosee if they were capable of this task. The prediction results are presented in , from (a1) - (a4).We observe that three open-sourced models (LLaVA-1.5 7b/13b, and Instructblip 7b) show similarbehaviors: these models tend to avoid predicting the reframing cases and mistakenly categorize them as either leave it unchanged or none of the other options. With the two-round prompt,LLaVA-1.5 7b and InstructBlip 7b make some correct predictions in the left and right categories.However, the correct and incorrect predictions are nearly balanced, with frequent misclassifications inthe opposite direction. For example, the number of true left predictions is equivalent to the numberof erroneous left predictions that were intended to be right. For GPT-4o, there are fewer errors incategorizing reframing cases into the wrong categories, and more cases within the reframing categoryare correctly predicted. However, contradictory predictions also occur frequently within the reframingcases. The result demonstrates that all the models are generally incapable of accurately predictingthe reframing cases under zero-shot prompt settings. However, every baseline model performswell in the none of the other options category. We present the examples from each model inthe Supplementary material A.4 to visualize the models pretrained capability on the DirectionalGuidance Task.",
  "Models performance after fine-tuning": "We present the heat maps of the fine-tuned model predictions in . By comparing the predictionresults between the zero-shot baselines and the fine-tuned models, we observe significant and consis-tent improvements in prediction performance, demonstrating the effectiveness and generalizabilityof our proposed method. Although there is considerable potential to improve the overall accuracy,the fine-tuned models reduce confusion between reframing, leaving it unchanged(O), andnone of the other options(X). The fine-tuned models are more likely to provide directionalguidance on the reframing cases. Moreover, the predictions within reframing cases show noticeableimprovement, as indicated by the clear diagonal line in the heat map. Another interesting finding isthe substantial reduction in wrong predictions with opposite directions (e.g., predicting an up case asdown). This clarity is meaningful, as it lowers the chance of users receiving conflicting guidance,thereby enhancing safety and efficiency in real-world applications. Overall, the fine-tuned modelsreduce errors across all options, showing significant improvement in both cross-category and withinreframing predictions. To evaluate our training frameworks sensitivity to different settings, we conducted groups of com-prehensive experiments. We used three metrics to quantitatively assess the models performance:overall F1 score, overall Accuracy, and Accuracy on the reframing cases denoted as ACC(F). Themetrics for the baseline models are presented in . In some baseline experiments, we found thatthe zero-shot setting did not always ensure a standard output format. In such cases, we performedpost-processing and excluded samples with predictions that did not fall within our options. The totalnumber of excluded samples was fewer than 10, and this only occurred in the zero-shot baselinemodels. The results, as shown in , include a combination of different settings from twoaspects: varying perturbation ranges and the impact of shuffling letters and options in the trainingdata. Regarding the choice to shuffle, we observed that randomly mixing letters and options does notconsistently enhance performance. For instance, with a perturbation range of 0.1-0.9, the unshuffledapproach often outperformed the shuffled version, while with a perturbation range of 0.3-0.7, shufflinggenerally resulted in inferior performance. The CLIP with linear probing method achieves comparable performance with the zero-shot perfor-mance of InstructBlip, but its still not able to provide informative guidance (the accuracy for randomchoice for a six classification task is 16.6%). This suggests that simple CLIP-based encoders, lackingintegration with a language model, may not be sufficient for this task. While the task largely relieson the models perception of salient features, the contextual information within questions is alsoessential. For instance, the VizWiz dataset includes many generic questions such as What is the colorof this? Many of these questions are answerable even though the photos are heavily ill-framed. Sincethese questions do not concern spatial details, the appropriate guidance is to leave it unchanged.This underlines a key distinction between our task and other image quality detection tasks, especiallythose focusing on ill-framing solely on image modality.",
  "Discussion": "In this section, we analyze the effect of different settings, including perturbation range and shufflingoperations, on the generation of training data. A detailed heatmap of the models predictions ispresented in Supplementary Materials with . The perturbation range determines the crop : Models performance with different settings. F1 and ACC denote the F1 score and accuracyscore, respectively. ACC(F) refers to the accuracy of reframing directions, excluding the categoriesLeave it unchanged and None of the other options.N/A indicates not applicable experiments due tolimitations on the models accessibility or incompatibility with the experiment design.",
  "GPT-4o0.560.600.19": "ratios used to generate the training samples, ranging from 0.1 (minimal crop) to 0.9 (maximum crop).We observed that positive Guidance samples tend to cluster at high crop ratios, while lower ratiosoften correspond to negative samples (where the Guidance is leave it unchanged). Therefore,a range of 0.3-0.7 leads to a more balanced selection of training data, while a range of 0.1-0.9provides a more comprehensive and varied dataset. This setting can affect the models performancedue to the balance between the diversity and complexity of the generated training samples, andthe trade-off works as follows: when the perturbation becomes more severe (e.g., at a ratio of 0.9),images are aggressively corrupted. This increases the chance of obtaining positive Guidance samples,as the model is more likely to fail in predicting these heavily perturbed samples, which it couldhave predicted accurately without perturbation. However, it also results in significant informationloss and greater challenges in detecting objects, making it a harder sample to learn. Conversely, amoderate perturbation ratio results in less aggressive cropping, allowing the model to access moreinformation and better respond to the original question. However, this can lead to fewer positiveGuidance samples, as the perturbation does not sufficiently challenge the predictions. The differenceswith the shuffling settings could be attributed to the regularization effect, which prevents the modelfrom memorizing fixed patterns and increases training difficulty, especially when training data isscarce. In scenarios with less training data, shuffling acts as a form of data augmentation, increasingthe diversity of training examples and making the model more robust. However, shuffling might addunnecessary complexity to a larger training dataset derived from a wider perturbation range, makingit harder for the model to learn effectively. In such cases, the unshuffled approach allows the modelto quickly identify and leverage consistent patterns, facilitating faster and more efficient learningprocesses. Ablation StudyTo gain a more comprehensive understanding of how different perturbation rangesaffect model performance, we conducted a more fine-grained ablation study with LLaVA1.5-7b.Specifically, we evaluated perturbation ranges of 0.1-0.3, 0.3-0.5, 0.5-0.7, and 0.7-0.9, alongsideour main experiments of 0.1-0.9 and 0.3-0.7. The results, presented in , reveal that as theperturbation range increases from 0.1-0.3 to 0.5-0.7, both overall F1 scores and overall accuracyshow substantial improvements, stabilizing around 0.49. However, at the highest perturbation rangeof 0.7-0.9, we observe a slight decrease in the ACC(F) metric, suggesting that overly aggressiveperturbations may introduce excessive complexity, hindering the models ability to accurately identifyrelevant objects. Notably, the broader range of 0.1-0.9 achieves the highest overall F1 and accuracyscores, suggesting that a wide perturbation range strikes an effective balance between data diversityand sample complexity. Additionally, all perturbation ranges demonstrate improvements in ACC(F),with enhanced reframing direction performance as perturbation increases, except at the highest range.These findings support our initial discussion by emphasizing the trade-off between data diversity andthe complexity of perturbed samples. Future work could explore optimized strategies for selectingperturbation ranges, potentially employing dynamic or adaptive methods to further improve modelperformance based on specific dataset characteristics.",
  "Limitation and Future Work": "In this study, we focused specifically on guiding image reframing directions as a proof of concept.Although reframing is one of the most common needs when assisting visually impaired individuals,some other aspects that impact the VQA process could also be explored, such as orientation, exposure,and focus. Our data augmentation framework can be extended to these aspects and generate trainingdata in a similar way, and we plan to explore this in future work. Second, the directional guidancehas been simplified to a classification task on directions, which may not fully capture the complexityof real-world scenarios. For example, effective reframing might require combining multiple direc-tionssuch as moving both up and leftor even zooming out. A more informative guidance inpractice would also consider additional parameters like the magnitude of the reframing action. Thosecomplexities can confuse the model, leading to inaccurate evaluations. To enhance clarity and reduceambiguity in our benchmark dataset, we included only cases that received consistent annotationsfrom multiple annotators, which resulted in a limited size in our benchmark test set. Additionally,our preliminary experiments are designed to validate the effectiveness of our proposed frameworkrather than to maximize the models performance. Consequently, the current method cannot fullyguarantee the reliability of the models prediction, and it still requires cautious deployment in high-risk scenarios. Moving forward, we aim to refine the task design and data generation framework,adapting more effectively to complex, real-world applications. In addition, theoretically, our guidanceframework can be extended to more general and quantitative scenarios. By simulating spatial drift,we can customize the ratio of drift and produce synthetic training data with quantitative values. Thispotential extension would allow models to identify not just the direction but also the extent of cameramovement required. This makes such quantitative guidance particularly meaningful in applicationssuch as robotics, where precise tracking of target objects is crucial, for example, in calculating theground truth for the extent of movement needed . Furthermore, we expect our unsupervised datageneration framework to alleviate the pressing data needs of studies exploring LLM or VLM forspatial or temporal reasoning tasks .",
  "Conclusion": "In this paper, we introduced a novel task and benchmark dataset within the context of Visual QuestionAnswering (VQA) aimed at improving the self-knowledge of Vision-Language Models (VLMs).Our task specifically evaluates how well VLMs can assess the sufficiency of visual information anddetermine the necessary actions to reframe an image to obtain additional information. To addressthe challenge of limited training data, we proposed an automated framework that generates syntheticdata by simulating unanswerable scenarios through perturbations applied to answerable cases. Ourresults show that current high-performing VLMs, including LLaVA and GPT-4o, struggle with thistask, revealing a gap in their ability to handle incomplete or ambiguous visual inputs. However, whenfine-tuned with the synthetic training data generated by our framework, the models significantlyoutperformed the zero-shot baseline on real-world data. This study highlights the importance of self-knowledge in VLMs, particularly their ability to rec-ognize the boundaries of available information and take appropriate actions when confronted withincomplete or misleading data. By mimicking human cognitive processes, our approach presents apromising solution for enhancing models self-knowledge and robustness, especially in real-worldapplications that require accurate and adaptive responses. This is particularly relevant for assistivetechnologies, such as those designed for visually impaired individuals, where providing timely andeffective guidance is essential. As VLMs continue to evolve, the ability to recognize their knowl-edge boundaries and make informed decisions will be critical for their successful deployment indynamic environments. Future work can explore additional strategies for refining this self-knowledge,potentially leading to more robust models capable of learning in complex, uncertain scenarios.",
  "This material is based upon work supported by the Air Force Office of Scientific Research underaward number FA9550-24-1-0149": "Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. LawrenceZitnick, and Devi Parikh. VQA: Visual Question Answering. In International Conference onComputer Vision (ICCV), 2015. Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-rag: Learn-ing to retrieve, generate, and critique through self-reflection. In The Twelfth InternationalConference on Learning Representations, 2023.",
  "Yuhang Cao, Pan Zhang, Xiaoyi Dong, Dahua Lin, and Jiaqi Wang. Dualfocus: Integrat-ing macro and micro perspectives in multi-modal large language models. arXiv preprintarXiv:2402.14767, 2024": "Boyuan Chen, Zhuo Xu, Sean Kirmani, Brain Ichter, Dorsa Sadigh, Leonidas Guibas, and FeiXia. Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages1445514465, 2024. Chongyan Chen, Samreen Anjum, and Danna Gurari. Grounding answers for visual questionsasked by visually impaired people. In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition (CVPR), pages 1909819107, June 2022. Liangyu Chen, Bo Li, Sheng Shen, Jingkang Yang, Chunyuan Li, Kurt Keutzer, Trevor Darrell,and Ziwei Liu. Large language models are visual reasoning coordinators. Advances in NeuralInformation Processing Systems, 36, 2024. Tai-Yin Chiu, Yinan Zhao, and Danna Gurari. Assessing image quality issues for real-worldproblems. In proceedings of the IEEE/CVF conference on computer vision and pattern recogni-tion, pages 36463656, 2020. Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, WeishengWang, Boyang Li, Pascale N Fung, and Steven Hoi. Instructblip: Towards general-purposevision-language models with instruction tuning. Advances in Neural Information ProcessingSystems, 36, 2024.",
  "John H Flavell.Metacognition and cognitive monitoring:A new area of cognitivedevelopmental inquiry. American psychologist, 34(10):906, 1979": "Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah ASmith, Wei-Chiu Ma, and Ranjay Krishna. Blink: Multimodal large language models can seebut not perceive. arXiv preprint arXiv:2404.12390, 2024. Yunhao Ge, Yihe Tang, Jiashu Xu, Cem Gokmen, Chengshu Li, Wensi Ai, Benjamin JoseMartinez, Arman Aydin, Mona Anvari, Ayush K Chakravarthy, et al. Behavior vision suite:Customizable dataset generation via simulation. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 2240122412, 2024.",
  "Akshay Gopalkrishnan, Ross Greer, and Mohan Trivedi. Multi-frame, lightweight & effi-cient vision-language models for question answering in autonomous driving. arXiv preprintarXiv:2403.19838, 2024": "Daniel Gordon, Aniruddha Kembhavi, Mohammad Rastegari, Joseph Redmon, Dieter Fox, andAli Farhadi. Iqa: Visual question answering in interactive environments. In Proceedings of theIEEE conference on computer vision and pattern recognition, pages 40894098, 2018. Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the Vin VQA matter: Elevating the role of image understanding in Visual Question Answering. InConference on Computer Vision and Pattern Recognition (CVPR), 2017. Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo,and Jeffrey P Bigham. Vizwiz grand challenge: Answering visual questions from blind people.In Proceedings of the IEEE conference on computer vision and pattern recognition, pages36083617, 2018.",
  "Fangyu Liu, Guy Emerson, and Nigel Collier. Visual spatial reasoning. Transactions of theAssociation for Computational Linguistics, 11:635651, 2023": "Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. Mitigat-ing hallucination in large multi-modal models via robust instruction tuning. In The TwelfthInternational Conference on Learning Representations, 2023. Hanchao Liu, Wenyuan Xue, Yifei Chen, Dapeng Chen, Xiutian Zhao, Ke Wang, Liping Hou,Rongjun Li, and Wei Peng. A survey on hallucination in large vision-language models. arXivpreprint arXiv:2402.00253, 2024.",
  "George Mandler. Recognizing: The judgment of previous occurrence. Psychological Review,87:252271, 1980": "Ishan Misra, Ross Girshick, Rob Fergus, Martial Hebert, Abhinav Gupta, and Laurens VanDer Maaten. Learning by asking questions. In Proceedings of the IEEE Conference on ComputerVision and Pattern Recognition, pages 1120, 2018. Soroush Nasiriany, Fei Xia, Wenhao Yu, Ted Xiao, Jacky Liang, Ishita Dasgupta, Annie Xie,Danny Driess, Ayzaan Wahid, Zhuo Xu, et al. Pivot: Iterative visual prompting elicits actionableknowledge for vlms. arXiv preprint arXiv:2402.07872, 2024. OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. SungYeon Park, MinJae Lee, JiHyuk Kang, Hahyeon Choi, Yoonah Park, Juhwan Cho, AdamLee, and DongKyu Kim. Vlaad: Vision and language assistant for autonomous driving. InProceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages980987, 2024. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visualmodels from natural language supervision. In International conference on machine learning,pages 87488763. PMLR, 2021. Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you dont know: Unanswerable ques-tions for squad. In Proceedings of the 56th Annual Meeting of the Association for ComputationalLinguistics (Volume 2: Short Papers), pages 784789, 2018.",
  "Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wideshut? exploring the visual shortcomings of multimodal llms. arXiv preprint arXiv:2401.06209,2024": "Yile Wang, Peng Li, Maosong Sun, and Yang Liu. Self-knowledge guided retrieval augmentationfor large language models. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Findings ofthe Association for Computational Linguistics: EMNLP 2023, pages 1030310315, Singapore,December 2023. Association for Computational Linguistics. Yuhao Wang, Yusheng Liao, Heyang Liu, Hongcheng Liu, Yu Wang, and Yanfeng Wang. Mm-sap: A comprehensive benchmark for assessing self-awareness of multimodal large languagemodels in perception. arXiv preprint arXiv:2401.07529, 2024. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le,Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models.Advances in neural information processing systems, 35:2482424837, 2022. Haoning Wu, Zicheng Zhang, Erli Zhang, Chaofeng Chen, Liang Liao, Annan Wang, KaixinXu, Chunyi Li, Jingwen Hou, Guangtao Zhai, et al. Q-instruct: Improving low-level visualabilities for multi-modality foundation models. arXiv preprint arXiv:2311.06783, 2023.",
  "Lingfeng Yang, Yueze Wang, Xiang Li, Xinlong Wang, and Jian Yang. Fine-grained visualprompting. Advances in Neural Information Processing Systems, 36, 2024": "Zhangyue Yin, Qiushi Sun, Qipeng Guo, Jiawen Wu, Xipeng Qiu, and Xuan-Jing Huang.Do large language models know what they dont know? In Findings of the Association forComputational Linguistics: ACL 2023, pages 86538665, 2023. Peng Zhang, Yash Goyal, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Yin andYang: Balancing and answering binary visual questions. In Conference on Computer Visionand Pattern Recognition (CVPR), 2016. Xingcheng Zhou, Mingyu Liu, Bare Luka Zagar, Ekim Yurtsever, and Alois C Knoll. Visionlanguage models in autonomous driving and intelligent transportation systems. arXiv preprintarXiv:2310.14414, 2023.",
  "A.1Benchmark Dataset Collection": "We invited 26 human annotators to label the 1.4k unanswerable samples extracted from the validationset of VizWiz VQA dataset. We divided the dataset evenly, assigning each annotator a segment towork on. Each sample has an image and corresponding question from the original dataset. We includeseveral sub-tasks for each VQA question to categorize how the images could be improved to betteranswer the question: annotators were required to choose options from the following categoriesReframing, Other Actions, or No Way to Answer. In the Reframing category, annotatorsspecified the direction left, right, up, downfor a reframing action that might reveal an answer.The Other Actions includes the actions that are beyond the previous four actions, such as zoomin/out, rotation, and adjusting exposure. The No Way to Answer category is used to indicate caseswhere there is unlikely to yield additional information by taking any actions. We also asked theannotators to summarize their selected options with one sentence, which can be open-ended andqualitative comments. This is used for sanity checks and helps our further cleaning. After the initialround of annotations, we engaged four additional annotators (validators) to review the labels, notingany errors or disagreements. These validators conducted two rounds of evaluation and discussionbefore finalizing the benchmark dataset. After the validation, we select the Reframing and No Wayto Answer categories to join our benchmark dataset. The instructions given to annotators are:",
  "Task Overview": "Thank you for joining this task! In Visual-Question-Answering (VQA), some image-question pairsare marked \"unanswerable\" due to insufficient information in the image. Our goal is to determine ifspecific camera adjustments or other actions could potentially make these questions answerable. For each image-question pair, your task is to identify if any adjustments or guidance could potentiallymake the question answerable. Please select the most applicable option from the drop-down cells andprovide a brief explanation in the summary column. Below is a description of each option. Reframing: Choose \"Left, Right, Up\", or \"Down\" if moving the camera in a specific directioncould reveal information to answer the question. Choose \"Leave it unchanged\" if the currentimage already contains all the information needed to answer the question.",
  "GPT4: SINGLE-ROUND PROMPT -": "You are an assistive technology specializing in visual question answering, i.e., the task of providing a natural language answer to a questionabout a given image. To better answer the question {QUESTION} based on the given image, please choose one of the six options (A. Leave itunchanged, B. Up, C. Left, D. Right, E. Down, F. None of the other options) on the camera framing. The definitions of each of theoptions are given below: - A: Leave it unchanged - The question can be answered based on the given image without the need for changing camera framing and there isvisible text or complete object in the image to answer the question. The image is clear and shows the object in question without any truncation orneed for reframing. The entire object is visible and identifiable. - B: None of the other options - The question cannot be answered based on the given image, even with a change in camera framing, or thequestion seems to be unrelated to the content of the image provided, or the question is incomplete or the question is unrelated to the image orthere is no visible text on the image to answer the question. If the answer to the question, i.e., visible text or object is partially visible in the image or the specific text content is not clear due to the angleand quality of the image or the piece of text is partially obscured, and the necessary details to answer the question are not discernible, pleasechoose one of the below camera framing accordingly.",
  "Round 1": "You are an assistive technology specializing in visual question answering, i.e., the task of providing a natural language answer to a questionabout a given image. To better answer the question {QUESTION} based on the given image, please choose one of the given options (A. Leaveit unchanged, B. None of the other options, C. Move camera) on the camera framing. The definitions of each of the options are given below: - A: Leave it unchanged - The question can be answered based on the given image without the need for changing camera framing and there isvisible text or complete object in the image to answer the question. The image is clear and shows the object in question without any truncation orneed for reframing. The entire object is visible and identifiable. - B: None of the other options - The question cannot be answered based on the given image, even with a change in camera framing, or thequestion seems to be unrelated to the content of the image provided, or the question is incomplete or the question is unrelated to the image orthere is no visible text on the image to answer the question. - C: Move camera - If the answer to the question i.e., visible text or object is partially visible in the image or the specific text content is notclear due to the angle and quality of the image or the piece of text is partially obscured, and the necessary details to answer the question are notdiscernible.",
  "Round 2": "You are an assistive technology specializing in visual question answering, i.e., the task of providing a natural language answer to a question abouta given image. To better answer the question {QUESTION} based on the given image or previous context {RESULT}, please choose one of thegiven four options (A. Up, B. Left, C. Down, D. Right) on the camera framing. The definitions of each of the options are given below: If the answer to the question i.e., visible text or object is partially visible in the image or the specific text content is not clear due to the angle andquality of the image or the piece of text is partially obscured, and the necessary details to answer the question are not discernible, please chooseone of the below camera framing accordingly.",
  "{QUESTION}": "In the previous setting, the result is framing which means that the image has part of the needed information for answering, and the camera canmove to the corresponding direction to better answer the question. Please choose the most suitable one of the four options for moving camera forbetter answering the question by the directions: left, right, up, down. The definitions are given below:"
}