{
  "Abstract": "Given the ubiquitous presence of time series data across various domains, pre-cise forecasting of time series holds significant importance and finds widespreadreal-world applications such as energy, weather, healthcare, etc. While numer-ous forecasters have been proposed using different network architectures, theTransformer-based models have state-of-the-art performance in time series fore-casting. However, forecasters based on Transformers are still suffering fromvulnerability to high-frequency signals, efficiency in computation, and bottleneckin full-spectrum utilization, which essentially are the cornerstones for accuratelypredicting time series with thousands of points. In this paper, we explore a novelperspective of enlightening signal processing for deep time series forecasting.Inspired by the filtering process, we introduce one simple yet effective network,namely FilterNet, built upon our proposed learnable frequency filters to extract keyinformative temporal patterns by selectively passing or attenuating certain compo-nents of time series signals. Concretely, we propose two kinds of learnable filtersin the FilterNet: (i) Plain shaping filter, that adopts a universal frequency kernel forsignal filtering and temporal modeling; (ii) Contextual shaping filter, that utilizesfiltered frequencies examined in terms of its compatibility with input signals fordependency learning. Equipped with the two filters, FilterNet can approximatelysurrogate the linear and attention mappings widely adopted in time series literature,while enjoying superb abilities in handling high-frequency noises and utilizing thewhole frequency spectrum that is beneficial for forecasting. Finally, we conduct ex-tensive experiments on eight time series forecasting benchmarks, and experimentalresults have demonstrated our superior performance in terms of both effectivenessand efficiency compared with state-of-the-art methods. Our code is available at 1.",
  "Introduction": "Time series forecasting has been playing a pivotal role across a multitude of contemporary applications,spanning diverse domains such as climate analysis , energy production , traffic flow patterns ,financial markets , and various industrial systems . The ubiquity and profound significance oftime series data has recently garnered substantial research efforts, culminating in a plethora of deeplearning forecasting models that have significantly enhanced the domain of time series forecasting.",
  ": Performance of Mean Squared Error (MSE) on a simple synthetic multi-frequency signal.More details about the experimental settings can be found in Appendix C.4": "Convolution Neural Network-based methods including TCN , SCINet , etc. Recently, however,with the continuing advancement of deep learning, two branches of methods that received particularlymore attention have been steering the development of time series forecasting, i.e., Multilayer Percep-tron (MLP)-based methods, such as N-BEATS , DLinear , and FreTS , and Transformer-based methods, such as Informer , Autoformer , PatchTST , and iTransformer .While MLP-based models are capable of providing accurate forecasts, Transformer-based modelscontinue to achieve state-of-the-art time series forecasting performance. However, forecasters based on Transformers are still suffering from vulnerability to high-frequencysignals, efficiency in computation, and bottleneck in full-spectrum utilization, which essentially are thecornerstones for accurately predicting time series composed of thousands of timesteps. In designinga very simple simulation experiment on the synthetic data only composed of a low-, middle- andhigh-frequency signal respectively (see (a)), we find the state-of-the-art iTransformer model performs much worse in forecasting ((b) and (c)). This observation showsthat state-of-the-art Transformer-based model cannot utilize the full spectrum information, even for anaive signal of three different frequency components. In contrast, in the field of signal processing, afrequency filter enjoys many good properties such as frequency selectivity, signal conditioning, andmulti-rate processing. These could have great potential in advancing the models ability to extractkey informative frequency patterns in time series forecasting. Thus, inspired by the filtering process in signal processing, in this paper, we introduce one simpleyet effective framework, namely FilterNet, for effective time series forecasting. Specifically, westart by proposing two kinds of learnable filters as the key units in our framework: (i) Plain shapingfilter, which makes the naive but universal frequency kernel learnable for signal filtering and temporalmodeling, and (ii) Contextual shaping filter, which utilizes filtered frequencies examined in terms ofits compatibility with input signals for dependency learning. The plain shaping filter is more likelyto be adopted in predefined conditions and efficient in handling simple time series structures, whilethe contextual filter can adaptively weight the filtering process based on the changing conditions ofinput and thus have more flexibility in facing more complex situations. Besides, these two filters asthe built-in functions of the FilterNet can also approximately surrogate the widely adopted linearmappings and attention mappings in time series literature . This also illustrates theeffectiveness of our FilterNet in forecasting by selectively passing or attenuating certain signalcomponents while capturing the core time series structure with adequate learning expressiveness.Moreover, since filters are better fit in the stationary frequency domain, we let filters wrapped bytwo reversible transformations, i.e., instance normalization and fast Fourier transform toreduce the influence of non-stationarity and accomplish the domain transformation of time seriesrespectively. In summary, our contributions can be listed as follows: In studying state-of-the-art deep Transformer-based time series forecasting models, an interestingobservation from a simple simulation experiment motivates us to explore a novel perspective ofenlightening signal processing techniques for deep time series forecasting. Inspired by the filtering process in signal processing, we introduce a simple yet effective network,FilterNet, built upon our proposed two learnable frequency filters to extract key informativetemporal patterns by selectively passing or attenuating certain components of time series signals,thereby enhancing the forecasting performance. We conduct extensive experiments on eight time series forecasting benchmarks, and the resultshave demonstrated that our model achieves superior performance compared with state-of-the-artforecasting algorithms in terms of effectiveness and efficiency.",
  "Related Work": "Deep Learning-based Time Series ForecastingIn recent years, deep learning-based methodshave gained prominence in time series forecasting due to their ability to capture nonlinear andcomplex correlations . These methods have employed various network architectures to learntemporal dependencies, such as RNN , TCN , etc. Notably, MLP- and Transformer-based methods have achieved competitive performance, emerging as state-of-the-art approaches.N-HiTS integrates multi-rate input sampling and hierarchical interpolation with MLPs to enhanceunivariate forecasting. DLinear introduces a simple approach using a single-layer linear modelto capture temporal relationships between input and output time series data. RLinear utilizeslinear mapping to model periodic features, demonstrating robustness across diverse periods withincreasing input length. In contrast to the simple structure of MLPs, Transformers advanced attentionmechanisms empower the models to capture intricate dependencies and long-range interactions. PatchTST segments time series into patches as input tokens to the Transformerand maintaining channel independence. iTransformer inverts the Transformers structure bytreating independent series as variate tokens to capture multivariate correlations through attention. Time Series Modeling with Frequency LearningIn recent developments, frequency technologyhas been increasingly integrated into deep learning models, significantly improving state-of-the-art accuracy and efficiency in time series analysis . These models leverage the benefits offrequency technology, such as high efficiency and energy compaction , to enhanceforecasting capabilities. Concretely, Autoformer introduces the auto-correlation mechanism,improving self-attention implemented with Fast Fourier Transforms (FFT). FEDformer enhancesattention with a FFT-based frequency approach, determining attentive weights from query and keyspectrums and conducting weighted summation in the frequency domain. DEPTS utilizesDiscrete Fourier Transform (DFT) to extract periodic patterns and contribute them to forecasting.FiLM employs Fourier analysis to retain historical information while filtering out noisy signals.FreTS introduces a frequency-domain Multi-Layer Perceptrons (MLPs) to learn channel andtemporal dependencies. FourierGNN transfers the operations of graph neural networks (GNNs)from the time domain to the frequency domain. FITS applies a low pass filter to the input datafollowed by complex-valued linear mapping in the frequency domain. Unlike these methods, in this paper we propose a simple yet effective model FilterNet developed froma signal processing perspective, and apply all-pass frequency filters to design the network directly,rather than incorporating them into other network architectures, such as Transformers, MLPs, orGNNs, or utilizing them as low-pass filters, as done in FITS and FiLM .",
  "Preliminaries": "Frequency FiltersFrequency filters are mathematical operators designed to modify thespectral content of signals. Specifically, given an input time series signal x[n] with its correspondingFourier transform X[k], a frequency filter H[k] is applied to the signal to produce an output signaly[n] with its corresponding Fourier transform Y[k] = X[k]H[k]. The frequency filter H[k] alters theamplitude and phase of specific frequency components in the input time series signal x[n] accordingto its frequency response characteristics, thereby shaping the spectral content of the output signal. According to the Convolution Theorem , the point-wise multiplication in the frequency domaincorresponds to the circular convolution operation between two corresponding signals in the timedomain. Consequently, the frequency filter process can be expressed in the time domain as:",
  "Y[k] = H[k]X[k] y[n] = h[n] x[n],(1)": "where h[n] is the inverse Fourier transform of H[k] and denotes the circular convolution operation.This formulation underscores the intrinsic connections between the frequency filter process and thecircular convolution in the time domain, and it indicates that the frequency filter process can efficientlyperform circular convolution operations. In time series forecasting, Transformer-based methods haveachieved state-of-the-art performance, largely due to the self-attention mechanism ,which can be interpreted as a form of global circular convolution . This perspective opens upthe possibility of integrating frequency filter technologies, which are well-known for their ability toisolate and enhance specific signal components, to further improve time series forecasting models.",
  "-1": ": The overall architecture of FilterNet. (i) Instance normalization is employed to addressthe non-stationarity among time series data; (ii) The frequency filter block is applied to capture thetemporal patterns, which has two different implementations, i.e., plain shaping filter and contextualshaping filter; (iii) Feed-forward network is adopted to project the temporal patterns extracted byfrequency filter block back onto the time series data and make predictions.",
  "Methodology": "As aforementioned, frequency filters enjoy numerous advantageous properties for time series fore-casting, functioning equivalently to circular convolution operations in the time domain. Therefore,we design the time series forecaster from the perspective of frequency filters. In this regard, wepropose FilterNet, a forecasting architecture grounded in frequency filters. First, we introduce theoverall architecture of FilterNet in .1, which primarily comprises the basic blocks and thefrequency filter block. Second, we delve into the details of two types of frequency filter blocks: theplain shaping filter presented in .2 and the contextual shaping filter discussed in .3.",
  "Overview": "The overall architecture of FilterNet is depicted in , which mainly consists of the instancenormalization part, the frequency filter block, and the feed-forward network. Specifically, for a giventime series input X = [X1:L1, X1:L2, ..., X1:LN ] RNL with the number of variables N and thelookback window length L, where X1:LN RL denotes the N-th variable, we employ FilterNet topredict the future time steps Y = [XL+1:L+1, XL+1:L+2, ..., XL+1:L+N] RN. We providefurther analysis about the architecture design of FilterNet in Appendix A. Instance NormalizationNon-stationarity is widely existing in time series data and poses a crucialchallenge for accurate forecasting . Considering that time series data are typically collectedover a long duration, these non-stationary sequences inevitably expose forecasting models to dis-tribution shifts over time. Such shifts can result in performance degradation during testing due tothe covariate shift or the conditional shift . To address this problem, we utilize an instancenormalization method, denoted as Norm, on the time series input X, which can be formulated as:",
  "(b) Contextual Shaping Filter": ": The structure of frequency filters. (a) Plain shaping filter: the plain shaping filter is initializedrandomly with channel-shared (left) or channel-unique (right) parameters, and then performs circularconvolution (i.e., the symbol ) with the input time series; (b) Contextual shaping filter: the contextualshaping filter firstly learns a data-dependent filter and then conducts multiplication (i.e., the symbol) with the frequency representation of the input time series. Frequency Filter BlockPrevious representative works primarily leverage MLP architectures (e.g.,DLinear , RLinear ) or Transformer architectures (e.g., PatchTST , iTransformer ) tomodel the temporal dependencies among time series data. As mentioned earlier, time series forecasterscan be implemented through performing a frequency filter process in the frequency domain, and thuswe propose to directly apply the frequency filter in the frequency domain, denoted as FilterBlock, toreplace the aforementioned methods for modeling corresponding temporal dependencies, such as:",
  "where F is Fourier transform, F1 is inverse Fourier transform and Hfilter is the frequency filter": "Inspired by MLP that randomly initializes a learnable weight parameters and Transformer that learnsthe data-dependent attention scores from data (further explanations are provided in Appendix B), weintroduce two types of frequency filters, i.e., plain shaping filter (PaiFilter) and contextual shapingfilter (TexFilter). PaiFilter applies a random initialized learnable weight H to instantiate thefrequency filter Hfilter, and then the frequency filter process is reformulated as:",
  "FilterBlock(Z) = F1(F(Z)H(F(Z))).(6)": "Feed-forward NetworkThe frequency filter block has captured temporal dependencies amongtime series data, and then we employ a feed-forward network (FFN) to project them back ontothe time series data and make predictions for the future time steps. As the output P of FFN areinstance-normalized values, we conduct an inverse instance normalization operation (InverseNorm)on them and obtain the final predictions Y. The entire process can be formulated as follows:",
  "Plain Shaping Filter": "PaiFilter instantiates the frequency filter by randomly initializing learnable parameters and thenperforming multiplication with the input time series. In general, for multivariate time series data, thechannel-independence strategy in channel modeling has proven to be more effective compared to thechannel-mixing strategy . Following this principle, we also adopt the channel-independencestrategy for designing the frequency filter. Specifically, we propose two types of plain shaping filters:the universal type, where parameters are shared across different channels, and the individual type,where parameters are unique to each channel, as illustrated in (a).",
  "S = F1(S),": "where F is Fourier transform, F1 is inverse Fourier transform, L denotes the element-wise productalong L dimension, H(Uni) C1L is the universal plain shaping filter, H(Ind) CNL is theindividual plain shaping filter, and S RNL is the output of PaiFilter. We further compare andanalyze the two types of PaiFilter in .3.",
  "Contextual Shaping Filter": "In contrast to PaiFilter, which randomly initializes the parameters of frequency filters and fixesthem after training, TexFilter learns the parameters generated from the input data, allowing forbetter adaptation to the data. Consequently, we devise a neural network H that flexibly adjusts thefrequency filter in response to the input data, as depicted in (b). Given the time series input Z RNL and its corresponding Fourier transform denoted as Z =F(Z) CNL, the network H is utilized to derive the contextual shaping filter, expressed asH : CNL CND. First, it embeds the raw data by a linear dense operation : CL CDto improve the capability of modeling complex data. Then, it applies a series of complex-valuemultiplication with K learnable parameters W1:K C1D yielding ((Z) W1:K) where isthe activation function, and finally outputs H(Z). Then we apply TexFilter by:",
  "Experimental Setup": "DatasetsWe conduct empirical analyses on diverse datasets spanning multiple domains, includingtraffic, energy, and weather, among others. Specifically, we utilize datasets such as ETT datasets ,Exchange , Traffic , Electricity , and Weather , consistent with prior studies on longtime series forecasting . We preprocess all datasets according to the methods outlinedin , and normalize them with the standard normalization method. We split the datasets intotraining, validation, and test sets in a 7:2:1 ratio. More dataset details are in Appendix C.1. BaselinesWe compare our proposed FilterNet with the representative and state-of-the-art modelsto evaluate their effectiveness for time series forecasting. We choose the baseline methods fromfour categories: (1) Frequency-based models, including FreTS and FITS ; (2) TCN-basedmodels, such as MICN and TimesNet ; (3) MLP-based models, namely DLinear andRLinear ; and (4) Transformer-based models, which include Informer , Autoformer ,Pyraformer , FEDformer , PatchTST , and the more recent iTransformer for compar-ison. Further details about the baselines can be found in Appendix C.2. Implementation DetailsAll experiments are implemented using Pytorch 1.8 and conductedon a single NVIDIA RTX 3080 10GB GPU. We employ MSE (Mean Squared Error) as the lossfunction and present MAE (Mean Absolute Errors) and MSE (Mean Squared Errors) results as theevaluation metrics. For further implementation details, please refer to Appendix C.3. : Forecasting results for prediction lengths {96, 192, 336, 720} with lookback windowlength L = 96. The best results are in red and the second best are blue. Due to space limit, additionalresults with other baselines and under different lookback length are provided in Tables 4 and 5.",
  "Main Results": "We present the forecasting results of our FilterNet compared to several representative baselineson eight benchmarks with various prediction lengths in . Additional results with differentlookback window lengths are reported in Appendix F and G.3. demonstrates that our modelconsistently outperforms other baselines across different benchmarks. The average improvement ofFilterNet over all baseline models is statistically significant at the confidence of 95%. Specifically,PaiFilter performs well on small datasets (e.g., ETTh1), while TexFilter excels on large datasets(e.g., Electricity) due to the ability to model the more complex and contextual correlations present inlarger datasets. Also, the prediction performance of iTransformer , which achieves the best resultson the Traffic dataset (862 variables) but not on smaller datasets, suggests that simpler structuresmay be more suitable for smaller datasets, while larger datasets require more contextual structuresdue to their complex relationships. Compared with FITS built on low-pass filters, our modeloutperforms it validating an all-pass filter is more effective. Since PaiFilter is simple yet effective,the following FilterNet in the experimental section refer to PaiFilter unless otherwise stated. 5.3Model AnalysisIn this part, we conduct experiments to delve into a thorough exploration of frequency filters, includingtheir modeling capabilities, comparisons among different types of frequency filters, and the variousfactors impacting their performance. Detailed experimental settings are provided in Appendix C.5. Modeling Capability of Frequency FiltersDespite the simplicity of frequency filter architecture, demonstrates that this architecture can also achieve competitive performance. Hence, in thispart, we perform experiments to explore the modeling capability of frequency filters. Particularly,given the significance of trend and seasonal signals in time series forecasting, we investigate theefficacy of simple filters in modeling these aspects. We generate a trend signal and a multi-periodsignal with noise, and then we leverage the frequency filters (i.e., PaiFilter) to perform trainingon the two signals respectively. Subsequently, we produce prediction values based on the trainedfrequency filters. Specifically, (a) and 4(b) show that the filter can effectively model trendand periodic signals respectively even compared with state-of-the-art iTransformer when the datacontains noise. These results illustrate that the filter has excellent and robust modeling capabilitiesfor trend and periodic signals which are important components for time series. This can also explainthe effectiveness of FilterNet since the filters can perform well in such settings.",
  "H(Ind)0.3820.4020.4300.4290.4720.4510.4810.4730.0910.2110.1860.3050.3800.4490.8960.712": "Shared vs. Unique Filters Among ChannelsTo analyze the different channel strategies of filters,we further conduct experiments on the ETTh and Exchange datasets. Specifically, we compareforecasting performance under different prediction lengths between two different types of frequencyfilters, i.e., H(Uni)and H(Ind). In H(Uni), filters are shared across different channels, whereas H(Ind)signifies filters unique to each channel. The evaluation results are presented in . Itdemonstrates that filters shared among different channels consistently outperform across all predictionlengths. In addition, we visualize the prediction values predicted on the ETTh1 dataset by the twodifferent types of filters, as illustrated in (see Appendix G.1). The visualization reveals thatthe prediction values generated by filters shared among different channels exhibit a better fit thanthe unique filters. Therefore, the strategy of channel sharing seems to be better suited for time seriesforecasting and filter designs, which is also validated in DLinear and PatchTST . Visualization of PredictionWe present a prediction showcase on ETTh1 dataset, as shown in. We select iTransformer , PatchTST as the representative compared methods.Comparing with these different state-of-the-art models, we can observe FilterNet delivers the mostaccurate predictions of future series variations, which has demonstrated superior performance. Inaddition, we include more visualization cases and please refer to Appendix G.3.",
  ": Visualization of prediction on the ETTh1 dataset with lookback and horizon length as 96": "Visualization of Frequency FiltersTo provide a comprehensive overview of the frequency re-sponse characteristics of frequency filters, we conduct visualization experiments on the Weather,ETTh, and Traffic datasets with the lookback window length of 96 and the prediction length of 96.The frequency response characteristics of learned filters are visualized in . From Figures 7(a)and 7(b), we can observe that compared with Transformer-based approaches (e.g., iTransformer ,PatchTST ) tend to attenuate high-frequency components and preserve low-frequency infor-mation, FilterNet exhibits a more nuanced and adaptive filtering behavior that can be capable ofattending to all frequency components. (c) demonstrates that the main patterns of the Trafficdataset primarily resides in the low-frequency range. This observation also explains why iTransformerperforms well on the Traffic dataset, despite its low-frequency nature. Overall, demonstrates",
  ": Spectrum visualizations of filters learned on ETTm1 with different prediction lengths": "that FilterNet possesses comprehensive processing capabilities. Moreover, visualization experimentsconducted on the ETTm1 dataset across various prediction lengths, as shown in , furtherillustrate the extensive processing abilities of FilterNet. Additional results conducted on differentlookback window lengths and prediction lengths can be found in Appendix G.2. Efficiency AnalysisThe complexity of FilterNet is O(Log L) where L is the input length. Tocomprehensively assess efficiency, we evaluate it based on two dimensions: memory usage andtraining time. Specifically, we choose two different sizes of datasets: the Exchange (8 variables, 7588timestamps) and Electricity datasets (321 variables, 26304 timestamps). We compare the efficiency ofour FilterNet with the representative Transformer- and MLP-based methods under the same settings(lookback window length of 96 and prediction length of 96), and the results are shown in . Ithighlights that FilterNet surpasses other Transformer models, regardless of dataset size. While ourapproach exhibits similar efficiency to DLinear, our effective results outperform its performance. InAppendix E, we further conduct ablation studies to validate the rationale of FilterNet designs.",
  "Conclusion Remarks": "In this paper, we explore an interesting direction from a signal processing perspective and make anew attempt to apply frequency filters directly for time series forecasting. We propose a simple yeteffective architecture, FilterNet, built upon our proposed two kinds of frequency filters to accomplishthe forecasting. Our comprehensive empirical experiments on eight benchmarks have validated thesuperiority of our proposed method in terms of effectiveness and efficiency. We also include manycareful and in-depth model analyses of FilterNet and the internal filters, which demonstrate manygood properties. We hope this work can facilitate more future research integrating signal processingtechniques or filtering processes with deep learning on time series modeling and accurate forecasting.",
  "Hui He, Qi Zhang, Simeng Bai, Kun Yi, and Zhendong Niu. CATN: cross attentive tree-awarenetwork for multivariate time series forecasting. In AAAI, pages 40304038. AAAI Press, 2022": "Adebiyi A Ariyo, Adewumi O Adewumi, and Charles K Ayo. Stock price prediction using thearima model. In 2014 UKSim-AMSS 16th international conference on computer modelling andsimulation, pages 106112. IEEE, 2014. Nijat Mehdiyev, Johannes Lahann, Andreas Emrich, David Enke, Peter Fettke, and Peter Loos.Time series classification using deep learning for process planning: A case from the processindustry. Procedia Computer Science, 114:242249, 2017.",
  "Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. Are transformers effective for time seriesforecasting? In AAAI, pages 1112111128. AAAI Press, 2023": "Kun Yi, Qi Zhang, Wei Fan, Shoujin Wang, Pengyang Wang, Hui He, Ning An, Defu Lian,Longbing Cao, and Zhendong Niu. Frequency-domain mlps are more effective learners in timeseries forecasting. In NeurIPS, 2023. Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and WancaiZhang. Informer: Beyond efficient transformer for long sequence time-series forecasting. InAAAI, pages 1110611115, 2021.",
  "Pierre Duhamel and Martin Vetterli. Fast fourier transforms: a tutorial review and a state of theart. Signal processing, 19(4):259299, 1990": "Bryan Lim and Stefan Zohren. Time-series forecasting with deep learning: a survey. Philosoph-ical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences,379(2194):20200209, feb 2021. Cristian Challu, Kin G. Olivares, Boris N. Oreshkin, Federico Garza, Max Mergenthaler, andArtur Dubrawski. N-hits: Neural hierarchical interpolation for time series forecasting. CoRR,abs/2201.12886, 2022.",
  "Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. FEDformer:Frequency enhanced decomposed transformer for long-term series forecasting. In ICML, 2022": "Shizhan Liu, Hang Yu, Cong Liao, Jianguo Li, Weiyao Lin, Alex X Liu, and Schahram Dustdar.Pyraformer: Low-complexity pyramidal attention for long-range time series modeling andforecasting. In ICLR, 2021. Kun Yi, Qi Zhang, Longbing Cao, Shoujin Wang, Guodong Long, Liang Hu, Hui He, ZhendongNiu, Wei Fan, and Hui Xiong. A survey on deep learning based time series analysis withfrequency transformation. CoRR, abs/2302.02173, 2023.",
  "Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng Long. Timesnet:Temporal 2d-variation modeling for general time series analysis. In ICLR. OpenReview.net,2023": "Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, AndreasKpf, Edward Z. Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style,high-performance deep learning library. In NeurIPS, pages 80248035, 2019.",
  "According the above equation, we can find that the mean value 1": "NN1n=0 x[n] in the time domain isequal to the zero frequency component X in the frequency domain. Similarly, we can also viewthe standard deviation from the frequency perspective, and it is related to the power spectral density. According to these analysis, the instance normalization is analogous to a form of data preprocessing.Given that filters are primarily crafted to discern particular patterns within input data, while instancenormalization aims to normalize each instance in a dataset, a function distinct from the conventionalrole of filters, we treat instance normalization as a distinct block within our FilterNet architecture. Frequency Filter BlockRecently, Transformer- and MLP-based methods have emerged as the twomain paradigms for time series forecasting, exhibiting competitive performance compared to othermodel architectures. Building on prior work that conceptualizes self-attention and MLP architecturesas forms of global convolution , it becomes apparent that frequency filters hold promisefor time series forecasting tasks. Just as self-attention mechanisms capture global dependenciesand MLPs learn to convolve features across the entire input space, frequency filters offer a meansto extract and emphasize specific temporal patterns and trends from time series data. By applyingfrequency filters to time series data, we can learn recurring patterns, trends, and periodic behaviorsthat are essential for forecasting future time series data and making accurate predictions. Feed-forward NetworkIncorporating a feed-forward network within the FilterNet architectureis essential for enhancing the models capacity to capture complex relationships and non-linearpatterns within the data. While frequency filters excel at extracting specific frequency componentsand temporal patterns from time series data, they may not fully capture the intricate dependenciesand higher-order interactions present in real-world datasets . By integrating a feed-forwardnetwork, the model gains the ability to learn hierarchical representations and abstract patterns fromthe input data, allowing it to capture more nuanced relationships and make more accurate predictions.This combination of frequency filters and a feed-forward network leverages the strengths of bothapproaches, enabling the model to effectively process and analyze time series data across differentfrequency bands while accommodating the diverse and often nonlinear nature of temporal dynamics.Overall, the inclusion of a feed-forward network enriches the expressive power of FilterNet, leadingto improved forecasting performance and robustness across various domains.",
  "where W are the learned weights and b are the learned biases. Once trained, the weights W andbiases b remain static, meaning that they do not dynamically change in response to new data inputs": "MLPs are straightforward, less data-dependent models that apply fixed transformations to the inputdata, making them suitable for tasks with static relationships between the input data. In contrast,self-attention mechanisms are highly data-dependent, dynamically computing attention scores basedon the input data to capture complex, context-specific dependencies, making them ideal for tasksrequiring an understanding of sequential or structured data. Inspired by the two paradigms, FilterNet designs two corresponding types of filters: plain shapingfilters and contextual shaping filters. Plain shaping filters offer stability and efficiency, making themsuitable for tasks with static relationships. In contrast, contextual shaping filters provide the flexibilityto capture dynamic dependencies, excelling in tasks that require context-sensitive analysis. This dualapproach allows FilterNet to effectively handle a wide range of data types and forecasting scenarios,combining the best aspects of both paradigms to achieve superior performance.",
  "C.1Datasets": "We evaluate the performance of our proposed FilterNet on eight popular datasets, including Ex-change, Weather, Traffic, Electricity, and ETT datasets. In detail, the Exchange2 dataset collectsdaily exchange rates of eight different countries including Singapore, Australia, British, Canada,Switzerland, China, Japan, and New Zealand ranging from 1990 to 2016. The Weather3 dataset,including 21 meteorological indicators such as air temperature and humidity, is collected every 10minutes from the Weather Station of the Max Planck Institute for Biogeochemistry in 2020. TheTraffic dataset contains hourly traffic data measured by 862 sensors on San Francisco Bay areafreeways, which has been collected since January 1, 2015. The Electricity4 dataset collects thehourly electricity consumption of 321 clients from 2012 to 2014. The ETT5 (Electricity TransformerTemperature) datasets contain two visions of the sub-dataset: ETTh and ETTm, collected fromelectricity transformers every 15 minutes and 1 hour between July 2016 and July 2018. Thus, in totalwe have 4 ETT datasets (ETTm1, ETTm2, ETTh1, and ETTh2) recording 7 features such as load andoil temperature. The details about these datasets are summarized in .",
  "C.2Baselines": "We choose twelve well-acknowledged and state-of-the-art models for comparison to evaluate theeffectiveness of our proposed FilterNet for time series forecasting, including Frequency-based models,TCN-based models, MLP-based models, and Transformer-based models. We introduce these modelsas follows: FreTS introduces a novel direction of applying MLPs in the frequency domain to effectivelycapture the underlying patterns of time series, benefiting from global view and energy compaction.The official implementation is available at To ensure fairand objective comparison, the results showed in are obtained using instance normalizationinstead of min-max normalization in the original code. FITS performs time series analysis through interpolation in the complex frequency domain,enjoying low cost with 10k parameters. The official implementation is available at Because its original paper doesnt provide the forecasting results with the fixedlookback length L = 96, we report the performance of FITS with lookback length L = 96 under fiveruns in . MICN employs multi-scale branch structure to model different potential patterns separatelywith linear complexity. It combines local features and global correlations to capture the overall viewof time series. The official implementation is available at experimental results showed in follow its original paper. TimesNet transforms 1D time series into a set of 2D tensors based on multiple periods to analysetemporal variations. The above transformation allows the 2D-variations to be easily captured by 2Dkernels with encoding the intraperiod- and interperiod-variations into the columns and rows of the2D tensors respectively. The official implementation is available at The results showed in follow iTransformer and the results showed in follow RLinear . DLinear utilizes a simple yet effective one-layer linear model to capture temporal relationshipsbetween input and output sequences. The official implementation is available at We report the performance of DLinear with lookback length L {96, 336} under five runs in and 5. RLinear uses linear mapping to model periodic features in multivariate time series with robust-ness for diverse periods when increasing the input length. It applies RevIN (reversible normalization)and CI (Channel Independent) to improve overall forecasting performance by simplifying learningabout periodicity. The official implementation is available at The experimental results showed in follow iTransformer . Informer enhances Transformer with KL-divergence based ProbSparse attention for O(L log L)complexity, efficiently encoding dependencies among variables and introducing a novel architecturewith a DMS forecasting strategy. The official implementation is available at and the experimental results showed in follow FEDformer . Autoformer employs a deep decomposition architecture with auto-correlation mechanism toextract seasonal and trend components from input series, embedding the series decomposition blockas an inner operator. The official implementation is available at and the experimental results showed in follow FEDformer . Pyraformer introduces pyramidal attention module (PAM) with an O(L) time and memorycomplexity where the inter-scale tree structure summarizes features at different resolutions andthe intra-scale neighboring connections model the temporal dependencies of different ranges. Theofficial implementation is available at andthe experimental results showed in follow DLinear . FEDformer implements sparse attention with low-rank approximation in frequency domain,enjoying linear computational complexity and memory cost. And it proposes mixture of expertsdecomposition to control the distribution shifting. The official implementation is available at and the experimental results showed in followiTransformer . PatchTST divides time series data into subseries-level patches to extract local semantic in-formation and adopts channel-independence strategy where each channel shares the same embed-ding and Transformer weights across all the series. The official implementation is available at The experimental results showed in followiTransformer . And because iTransformer doesnt provide the prediction results with lookbacklength L = 336, we report the performance of PatchTST with lookback length L = 336 under fiveruns in . iTransformer inverts the structure of Transformer without modifying any existing modules byencoding individual series into variate tokens. These tokens are utilized by the attention mechanismto capture multivariate correlations and FFNs are adopted for each variate token to learn nonlin-ear representations. The official implementation is available at The experimental results showed in follow its original paper. And becauseiTransformer doesnt provide the prediction results with lookback length L = 336, we report theperformance of iTransformer with lookback length L = 336 under five runs in .",
  "C.3Implementation Details": "The architecture of our FilterNet is very simple and has two main hyperparameters, i.e., the bandwidthof filters and the hidden size of FFN. As shown in , the bandwidth of the filters corresponds tothe lookback window length, so we select the lookback window length as the bandwidth accordingly.For the hidden size of FFN, we carefully tune the size over {64, 128, 256, 512}. Following theprevious methods , we use RevIN as our instance normalization block. Besides, wecarefully tune the hyperparameters including the batch size and learning rate on the validation set,and we choose the settings with the best performance. We tune the batch size over {4, 8, 16, 32} andtune the learning rate over {0.01, 0.05, 0.001, 0.005, 0.0001, 0.0005}.",
  "C.5Experimental Settings for Filters Analysis": "Modeling capability of frequency filtersWe generate two signals: a trend signal with Gaussiannoise and a multi-periodic signal with Gaussian noise. We then apply PaiFilter to these signals witha lookback window length of 96 and a prediction length of 96. The results are displayed in . Visualization of Frequency FiltersGiven a filter H R1L, where L is its bandwidth, wevisualize the frequency response characteristic of the filter by plotting the values in R1L. First, weperform a Fourier transform on these values to obtain the spectrum, which includes the frequency andits corresponding amplitude. Finally, we visualize the spectrum, as shown in Figures 7, 8, and 12.",
  "DStudy of the Bandwidth of Frequency Filters": "The bandwidth parameter (i.e., L in Equation (8) and D in Equation (9)) holds significant importancein the functionality of filters. In this part, we conduct experiments on the Weather dataset to delveinto the impact of bandwidth on forecasting performance. We explore a range of bandwidth valueswithin the set {96, 128, 192, 256, 320, 386, 448, 512} while keeping the lookback window lengthand prediction length constant. Specifically, we conduct experiments to evaluate the impact underthree different combinations of lookback window length and prediction length, i.e., 96 96,96 192, and 192 192, and the results are represented in . We observe clear trends in therelationship between bandwidth settings and lookback window length. (a) and (b)show that increasing the bandwidth results in minimal changes in forecasting performance. (c) demonstrates that while forecasting performance fluctuates with increasing bandwidth, it isoptimal when the bandwidth equals the lookback window length. These results indicate that usingthe lookback window length as the bandwidth is sufficient since the filters can effectively model thedata at this setting, and it also results in lower complexity.",
  ": Ablation Studies on the ETTh1, ETTm1, and Electricity datasets": "by eliminating the particular component of the FilterNet architecture. The evaluation results arepresent in . In the figure, W/O Norm indicates that instance normalization and inverseinstance normalization have been removed from FilterNet. W/O Filter signifies the removal of thefilter block, and W/O FFN denotes the exclusion of the feed-forward network. The experimentsare conducted with lookback window length of 96 and output length of 96. From the figure, we canconclude that each block is indispensable, as the removal of any component results in a noticeabledecrease in performance. This highlights the critical role each block plays in the overall architectureof FilterNet, contributing to its effectiveness in time series forecasting.",
  "FAdditional Results": "compares the performance of various methods with our FilterNet, demonstrating that ourmodel consistently outperforms the others. To further assess the performance of FilterNet underdifferent lookback window lengths, we conducted experiments on the ETTh1, ETTm1, Exchange,Weather, and Electricity datasets with the lookback window length of 336. The results, shown in, indicate that our model achieves the best performance across these datasets.",
  "G.1Visualization of Channel-shared vs Channel-unique Filters": "To further compare the channel-shared and channel-unique filters, we visualize the prediction valuesby the corresponding filters. The results are shown in . The figure demonstrates that thevalues predicted by channel-shared filters closely align with the ground truth compared to thosepredicted by channel-unique filters. This observation is consistent with the findings presented in, indicating the superiority of channel-shared filters.",
  "G.2Visualization of Frequency Filters": "We further conduct visualization experiments to explore the learnable filters under different lookbackwindow lengths and prediction lengths. The experiments are performed on the Electricity dataset, andthe results are illustrated in . These figures illustrate that FilterNet possesses full spectrumlearning capability, as the learnable filters exhibit values across the entire spectrum. Besides, weobserve that the frequency primarily concentrates in the low and middle ranges which explains thatsome works based on low-pass filters can also achieve good performance.",
  "G.3Visualizations of Predictions": "To further offer an evident comparison of our model with the state-of-the-art models, we presentsupplementary prediction showcases on ETTm1 dataset, and the results are shown in 13. We choosethe following representative models, including iTransformer , PatchTST , and DLinear ,as the baselines. Comparing with these different types of state-of-the-art models, FilterNet deliversthe most accurate predictions of future series variations, demonstrating superior performance."
}