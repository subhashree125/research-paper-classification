{
  "Abstract": "Recent advances in 3D object detection leveraging multi-view cameras have demon-strated their practical and economical value in various challenging vision tasks.However, typical supervised learning approaches face challenges in achieving satis-factory adaptation toward unseen and unlabeled target datasets (i.e., direct transfer)due to the inevitable geometric misalignment between the source and target do-mains. In practice, we also encounter constraints on resources for training modelsand collecting annotations for the successful deployment of 3D object detectors. Inthis paper, we propose Unified Domain Generalization and Adaptation (UDGA),a practical solution to mitigate those drawbacks. We first propose Multi-viewOverlap Depth Constraint that leverages the strong association between multi-view,significantly alleviating geometric gaps due to perspective view changes. Then, wepresent a Label-Efficient Domain Adaptation approach to handle unfamiliar targetswith significantly fewer amounts of labels (i.e., 1% and 5%), while preservingwell-defined source knowledge for training efficiency. Overall, UDGA frameworkenables stable detection performance in both source and target domains, effectivelybridging inevitable domain gaps, while demanding fewer annotations. We demon-strate the robustness of UDGA with large-scale benchmarks: nuScenes, Lyft, andWaymo, where our framework outperforms the current state-of-the-art methods.",
  "Introduction": "3D Object Detection (3DOD) is a pivotal computer vision task in various real-world applications suchas autonomous driving and robotics. Recent progress in 3DOD have showcased remarkableadvancements, primarily due to the large-scale benchmark datasets and the introduction ofmultiple computer vision sensors (e.g., LiDAR, multi-view cameras, and RADAR). Among these,camera-based multi-view 3DOD has drawn significant attention for its cost-efficiency andrich semantic information. However, a significant challenge remains largely unexplored: accuratelydetecting the location and category of objects in the presence of distributional shifts between thesource and target domains (i.e., data distributional gaps between the training and the testing datasets). To successfully develop and deploy Multi-view 3DOD models, we need to solve two practicalproblems: (1) the geometric distributional shift across different sensor configurations, and (2) thelimited amount of resources (e.g., insufficient computing resources, expensive data annotations). Thefirst problem poses a challenge in learning transferable knowledge for robust generalization in novel",
  "Multi-view 3D Object Detection": "3D object detection is a fundamental aspect of computer vision tasks in the real world.Especially, Multi-view 3D Object Detection leveraging Birds Eye View (BEV) representations have rapidly expanded. We observe that this paradigm is divided into two categories: (i) LSS-based , and (ii) Query-based . The former adopts explicit methods leveragingdepth estimation network, and the latter concentrates on implicit methods utilizing the attentionmechanism of Transformer . Recently, these methods significantly benefit fromimproved geometric understanding leveraging temporal inputs. Also, methods that directlyguide the model using the LiDAR teacher model significantly encourage BEV spatial details. Inparticular, this approach is being adopted to gradually replace LiDAR in real-world scenarios; however,it still suffers from poor generalizability due to drastic domain shifts (e.g., weather, country, andsensor). To mitigate these issues, we present a novel paradigm, Unsupervised Domain Generalizationand Adaptation (UDGA), that effectively addresses geometric issues leveraging multi-view triangularclues and smoothly bridge differenet domains without forgetting previously learned knowledge.",
  "Bridging the Domain Gap for 3D Object Detection": "Due to the expensive cost of sophisticated sensor configurations and accurate 3D annotations forautonomous driving scenes, existing works strive to generalize 3D perception models in various datadistributions. Specifically, they often fail to address the covariate shift between the training and testsplits. To bridge the domain gap, existing approaches have introduced noteworthy solutions as below. LiDAR-based. Wang et al. introduced Statistical Normalization (SN) to mitigate the differencesin object size distribution across various datasets. ST3D leveraged domain knowledge throughrandom object scale augmentation, and their self-training pipeline refined the pseudo-labels. SPG aims to capture the spatial shape, generating the missing points. 3D-CoCo contrastively adjustthe domain boundary between source and target to extract robust features. LiDAR Distillation generates pseudo sparse point sets in spherical coordinates and aligns the knowledge between sourceand pseudo target. STAL3D effectively extended ST3D by incorporating adversarial learning.DTS randomly re-sample the beam and aim to capture the cross-density between student andteacher models. CMDA aim to learn rich-semantic knowledge from camera BEV features andadversarially guide seen sources and unseen targets, achieving state-of-the-art UDA capacity. Camera-based. While various groundbreaking methods based on LiDAR have been researched,camera-based approaches are still limited. Due to the elaborate 2D-3D alignment, not only areLiDAR-based approaches not directly applicable, but conventional 2D visual approaches cannot be adopted either. To mitigate these issues, STMono3D self-supervise the monocular3D detection network in a teacher-student manner. DG-BEV adversarially guide the networkfrom perspective augmented multi-view images. PD-BEV explicitly supervise models by theRenderNet with pseudo labels. However, camera domain generalization methods cannot meet theperformance required for the safety, struggling to address the practical domain shift in the perspectivechange. To narrow the gap, we introduce a Unified Domain Generalization and Adaptation (UDGA)framework that effectively promotes depth-scale consistency by leveraging occluded clues betweenadjacent views and then seamlessly transfers the models potential along with a few novel labels.",
  "Parameter Efficient Fine-Tuning": "Recent NLP works fully benefit from general-purpose Large-language Models (LLM). Additionally,they have proposed Parameter-Efficient Fine-Tuning (PEFT) to effectively transferLLM power to various downstream tasks. Specifically, PEFT preserves and exploits previouslylearned universal information, fine-tuning only additional parameters with a few downstream labels.This paradigm enables to notably reduce extensive computational resources, and large amounts oftask-specific data and also effectively address challenging domain shifts in various downstream tasksas reported by . Inspired by this motivation, to address drastic perspective shifts between sourceand target domains, we design Label-Efficient Domain Adaptation that fully transfers generalizedsource potentials to target domains by fine-tuning only our extra modules with few-shot target data.",
  "(a) Translated perspective views": ": (a) An illustration of multi-view installation translation difference. The first (i.e., source)and second (i.e., target) rows are two perspective views of the same scene captured from differentinstallation points. The translation gap between these views is substantial, approximately 30%. (b)Source trained network shows poor perception capability in target domain, primarily due to extrinsicshifts. In Height, mAP and NDS have dropped up to -67% compared to source. Note that wesimulate the camera extrinsic shift leveraging CARLA (refer to Appendix A for further details).",
  "Preliminary": "Multi-view 3D Object Detection is a fundamental computer vision task that involves safely localizingand categorizing objects in a 3D space exploiting 2D visual information from multiple camera views.Especially, recent landmark Multi-view 3D Object Detection models are formulatedas follow; arg min L(Y, D(V(I, K, T)), where Y represents the size (l, w, h), centerness (cx, cy, cz),and rotation of each 3D object. Also, I = {i1, i2, ..., in} RNHW 3, K, and T = [R|t]denotes multi-view images, intrinsic and extrinsic parameters. Specifically, these models, which fullybenefit from view transformation modules V, encode 2D visual features alongside the 3D spatialenvironment into a birds eye view (BEV) representation. First, these works adopts explicit methods(BEV view transformation P as shown in Eq. 1) exploiting depth estimation network. Subsequently,Detector Head modules D supervises BEV features with 3D labels Y in a three-dimensional manner.",
  "Domain Shifts in Multi-view 3D Object Detection": "In this section, we analyze and report de facto domain shift problems arising in the AutonomousDriving system. As shown in 3.1, recent works adopt camera parameters K and T as extra inputsin addition to multi-view image I. As reported by , assuming that the conditional distributionof outputs for given inputs, is the same across domains, it is explained that shifts in the domaindistribution are caused by inconsistent marginal distributions of inputs. To mitigate these issues,recent generalization approaches often focus on covariate shift in geometricfeature representation mainly due to optical changes (i.e., Focal length, Field-of-View, and pixel size). This is the only part of a story. We experience drastic performance drops (up to -54% / -67%performance drop in NDS and mAP, respectively, as shown in Fig 2 (b)) from non-intrinsic factors(i.e., only extrinsic shifts). Especially, we capture a phenomenon wherein the actual depth scale froman ego-vehicles visual sensor to an object (Fig 2 (a) red boxes) varies depending on the sensorsinstallation location. Followed by Pythagorean theorem, as the height difference h increases, thedepth scale difference d also increases accordingly. Note that this is not limited to height solely;any shifts in deployment translation (e.g., along the x, y, or z axis) lead to changes in actual depthscale. As a result, perspective view differences significantly hinder the models three-dimensionalgeometric understanding by causing depth inconsistency. To address above drawbacks, we introducea novel penalising strategy that effectively boost depth consistency in various camera geometry shifts.",
  "Backbone": ": An overview of our proposed methodologies. Our proposed methods comprise two majorparts: (i) Multi-view Overlap Depth Constraint and (ii) Label-Efficient Domain Adaptation (LEDA).In addition, our framework employs two phases (i.e., pre-training, and then fine-tuning). Note thatwe adopt our proposed depth constraint in both phases, and LEDA only in the fine-tuning phase.",
  "Multi-view Overlap Depth Constraint": "Motivation. Recently, previous efforts augment multi-view images to generalizechallenging perspective view gaps. However, these strategies suffer from poor generalizabilityin cross-domain scenarios, primarily due to the underestimated extent of view change betweendifferent sensor deployments as reported in section 3.2. To alleviate perspective gaps, we introduceMulti-view Overlap Depth Constraint, effectively encouraging perspective view-invariant learning.Here, we start from three key assumptions: First, perspective shifts between adjacent cameras inmulti-view modalities are non-trivial and varied, closely akin to those observed in cross-domains(e.g., nuScenes Lyft). Second, visual odometry techniques such as Structure from Motion (SfM)and Simultaneous Localization and Mapping (SLAM) often benefit from improved depth consistencythrough relationships between adjacent views (e.g., relative pose estimation). Third, in multi-viewmodalities, overlap regions serve as strong geometric triangular clues, seamlessly bridging betweenadjacent views. However, under conditions where camera parameters are input, off-the-shelf poseestimation leads to ambiguity in learning precise geometry. To mitigate these issues, weintroduce a novel depth constraint ( (i)) with overlap regions between adjacent cameras. Approach. To achieve generalized BEV extraction, we directly constrain depth estimation networkfrom adjacent overlap regions between multi-view cameras. Also, we advocate that multi-frameimage inputs substantially complement geometric understanding in dynamic scenes with speedytranslation and rotation shifts. To this end, we formulate corresponding depth D leveraging spatialand temporal adjacent views. First, we calculate overlap transformation matrices Tij from Eq. 2.",
  "(i,j)pe(IjKj, Pj, IjKj, Tij, P ij),(4)": "where P represents point clouds generated by D, and pe is photometric error by SSIM . Also, denotes bilinear sampling on RGB images. Concretely, we take two advantages leveraging Lpin narrow occluded regions; First, Lp effectively mitigates the triangular misalignment. Second,Lp potentially supports insufficiently scaled Lov. Ultimately, we alleviate perspective view gaps bydirectly constraining the corresponding depth and the photometric matching between adjacent views.",
  "Label-Efficient Domain Adaptation": "Motivation. There exist practical challenges in developing and deploying multi-view 3D objectdetectors for safety-critical self-driving vehicles. Each vehicle and each sensor requires its ownmodel that can successfully operate in various conditions (e.g., dynamic weather, location, and time).Furthermore, while collecting large-scale labels in diverse environments is highly recommended, it isextremely expensive, inefficient and time-consuming. Among those, we are particularly motivated totackle the following: (i) Stable performance, (ii) Efficiency of training, (iii) Preventing catastrophicforgetting, and (iv) Minimizing labeling cost. To satisfy these practical requirements, we carefullydesign an efficient and effective learning strategy, Label-Efficient Domain Adaptation (LEDA) thatseamlessly transferring and preserving their own potentials leveraging a few annotated labels. Approach. In this paper, we propose Label-Efficient Domain Adaptation, a novel strategy toseamlessly bridge domain gaps leveraging a small amount of target data. To this end, we add extraparameters A consisting of bottleneck structures (i.e., projection down down and up up layers).",
  "y = B(x) + A(x),(6)": "Firstly, we feed x into down to compress its shape to [H/r, W/r], where r is the rescale ratio, andthen utilize up to restore it to [H, W]. Secondly, we fuse each outputs from B, and Adapter byexploiting skip-connections that directly link between the downsampling and upsampling paths. Bydoing so, these extensible modules allow to capture high-resolution spatial details while reducingnetwork and computational complexity. Plus, it notes worthy that they are initialized by a near-identityfunction to preserve previously updated weights. Finally, our frameworks lead to stable recognitionin both source and target domains, incrementally adapting without forgetting pre-trained knowledge.",
  "Experimental Results": "In this section, we showcase the overall performance of our methodologies on landmark datasetsfor 3D Object Detection: Waymo , Lyft , and nuScenes . The three datasets have differentspecifications; thus, we convert them to a unified detection range and coordinates for accuratecomparison. We also adopt only seven parameters to achieve consistent training results under thesame conditions: the location of centerness (x, y, z), the size of box (l, w, h), and heading angle .Additionally, we summarize 3D Object Detection datasets and implementation details in Appendix A.",
  "mTPTP(1 min(1, mTP))](8)": "We reconstruct the unified category for Unified Domain Generalization and Adaptation as follows:the car for nuScenes and Lyft, and the vehicle for Waymo. Furthermore, we only validateperformance in the range of x, y axis from -50m to 50m. Note that we offer an empirical lower boundDirect Transfer (i.e., directly evaluating the model pre-trained in the source domain only), and anempirical upper bound Oracle (i.e., evaluating the model fully supervised in the target domain). Wereport Full F.T. (i.e., fine-tuning all parameters from the pre-trained source model) and Adapter (i.e.,parameter efficient fine-tuning without our proposed depth constraint methods from the pre-trainedsource model) Furthermore, we formulate Closed Gap-representing the hypothetical closed gap by",
  "Experiment Results": "Performance Comparison in Domain Generalization. As shown in Tab. 1, we showcase fourchallenging generalization scenarios, and quantitatively compare our proposed methodology withexisting state-of-the-art methods, which include CAM-Conv , Single-DGOD , DG-BEV ,and PD-BEV . Here, we observe that these methods still struggle to fully pilot geometric shiftsfrom perspective changes in cross-domain scenarios. Importantly, in Lyft nuScenes, existingmethods suffer from the orientation error mainly due to significantly different ground truth directions(i.e., only recovering 0.198 mAOE). In nuScenes Waymo (i.e., one of the most challenging : Comparison of UDGA performance on BEVDepth with various PEFT modules, SSF ,and Adapter . We construct six different target data splits from 1% to 100%. Additionally, #Params denote the number of parameters for training. Note that represents Do not support.",
  "Oracle51.7M0.587 / 0.475": "Full FT51.7M0.476 / 0.3690.515 / 0.4340.547 / 0.4340.577 / 0.4640.590 / 0.4830.610 / 0.506SSF 1M0.245 / 0.0790.294 / 0.1120.360 / 0.2560.374 / 0.2660.421 / 0.3270.439 / 0.275Adapter-B21.3M0.465 / 0.2830.481 / 0.3650.511 / 0.3840.558 / 0.4440.569 / 0.4600.581 / 0.473Adapter-S8.8M0.326 / 0.1340.372 / 0.1610.444 / 0.2550.465 / 0.2830.509 / 0.3900.538 / 0.443",
  "Oracle51.7M0.684 / 0.602": "Full FT51.7M0.531 / 0.3900.594 / 0.4730.623 / 0.5130.650 / 0.5490.678 / 0.5870.700 / 0.615SSF1M0.316 / 0.1150.355 / 0.1450.386 / 0.1850.420 / 0.2300.447 / 0.2690.470 / 0.300Adapter-B21.3M0.499 / 0.3280.556 / 0.4650.584 / 0.4750.633 / 0.5320.670 / 0.5640.684 / 0.596Adapter-S8.8M0.420 / 0.2300.463 / 0.3250.500 / 0.3560.537 / 0.4000.561 / 0.4260.573 / 0.442",
  ": Performance relative to training parameters. The Domain Generalization task is representedin blue, while the Domain Adaptation task is divided into two stages: 1% in gray and 100% in red": "scenarios due to the rear camera drop), previous approaches still show a significant gap compared toOracle (i.e., -49.7% Closed Gap). In this paper, our novel depth constraint notably addresses theseissues, outperforming existing SOTAs (especially, up to +4.7% NDS and +12.6% Closed Gap betterthan DG-BEV in Lyft nuScenes). Especially, leveraging triangular clues to explicitly superviseoccluded depth contributes significantly to improving geometric consistency compared to priorapproaches . Overall, we demonstrate that our novel approaches significantly enhanceperspective-invariance, featuring strong association in occluded regions between multi-views. Performance Comparison in UDGA. In Tab. 2, we show that our proposed Unified Domain Gener-alization and Adaptation performance compared with various PEFT approaches (i.e., SSF , andAdapter ). SSF directly scale and shift the deep features extracted by pre-trained operation blocks,leveraging additional normalization parameters. Adapter represents sole module performance withoutour proposed constraint; Adapter-B, and Adapter-S denotes base, and small version, respectively. Existing PEFT paradigms benefit from fine-tuning only extra parameters, retaining previously updatedweights. However, we observe that these paradigms do not successfully adapt to the covariate shiftsoriginated by challenging geometric differences as reported in section 3.2. More specifically, SSF andAdapter-S, which exploit a small number of parameters, begin to capture transferable representationsand then marginally adapt at the 10% data split. Also, Adapter-B leveraging 21.3M parametersprovide poor adaptation capability (i.e., inferior to Scratch and Full FT in Lyft nuScenes 100%). However, our proposed strategy seamlessly adapt to target domains in 1%, and 5%, effectively bridgeperspective gaps. Furthermore, our proposed strategy show superior performance gain (outperformingScratch in Lyft nuScenes 50%, and Full FT in both Lyft nuScenes, and nuScenes Lyft 100%),effectively adapting to novel targets. It is noteworthy that the most effective adaptation is achievedby updating extra parameters (less than 20% of the total), which demonstrates the practicality andefficiency of our novel UDGA strategy as shown in . In addition, unlike Full FT, it proves thatour UDGA framework stably adapts to the target without forgetting previously learned knowledge as",
  "Ablation studies": "Exploring the Synergy Between Modules. To better understand the role of each module, wepresent ablation studies of UDGA in this experiment (Tab. 3). Precisely, we aim to analyze the prosand cons in both training steps (i.e., pre-train B and fine-tune A), with the objective of effectivelyelucidating the plausibility of UDGA. First, the strategy trained from scratch leveraging our depthconstraint significantly recovers performance drop from the sensor deployment shift (up to +20.8%NDS). However, this strategy finds it difficult to provide a practical solution for Multi-view 3DOD,mainly due to unsatisfying generalizability. Additionally, although LEDA without Lov and Lp yieldsimproved performance, it fails to transfer its previously learned potential, resulting in only +2.3%NDS compared to our individual depth constraint. To tackle these issues, we concentrate on bridgingtwo distinct domains by capturing generalized perspective features. Especially, our depth constraint(only trained during pre-training B) significantly encourages understanding of the target in LEDAduring fine-tuning A with a 10% split, addressing the geometric covariate shift (+30.3% NDS).Furthermore, UDGA strategy using Lov and Lp in both phases learns the transferable knowledge andshows impressive improvement (+42.5% NDS). Finally, UDGA successfully presents an effectiveand efficient paradigm for Multi-view 3DOD, highlighting notable recovery in novel target scenarios. Effect of Overlap Depth Constraint. In Tab. 4, we carefully evaluate our depth constraint compo-nents in various cross-domain environments. Here, Ld denotes depth supervision by LiDAR. Also,we design ext aug that globally rotate ground truths with randomly initialized angle to releasethe direction shift. More importantly, we observe that perspective view shifts from different sensordeployments lead to severe translation and orientation errors. To tackle these issues, we advocate thatLov, which leverages strong relationships between adjacent views, effectively alleviating perspectivegaps compared to Ld (recovering up to +19% NDS in Lyft nuScenes). Lp relieves slight mis-alignment, encouraging depth-scale consistency. Additionally, our ext aug substantially boost stablegeneralization, suppressing orientation errors (up to +1.4% additional NDS gain). Consequently, ournovel objectives (Lov and Lp) demonstrate their effectiveness, significantly tackling geometric errors.",
  "Qualitative Analysis": "To qualitatively analyze the effectiveness of Multi-view Overlap Depth Constraint, we presentadditional visualized results in . For accurate comparison, we conduct binary masking leveraginggiven sparse depth ground truths. In middle row, BEVDepth fail to perceive hard samples (e.g.,far distant and occluded objects) in yellow boxes, mainly due to different extent of deformationrelative to perspective as reported in section 3.2. We aim to tackle this problem, explicitly bridgingadjacent views in various dynamic scenes. Precisely, in bottom row, we showcase distinguishableresults in yellow boxes, capturing semantic details from various view deformation. As as results,we qualitatively demonstrate that our proposed method effectively encourage depth consistency anddetection robustness, significantly improving geometric understanding in cross-domain scenarios.",
  "Conclusion": "Limitations. While our work significantly improves the adaptability of 3D object detection, it cannotguarantee seamless adaptation due to several limitations, including: (1) The performance does notmatch that of 3D object detection models using LiDAR point clouds. (2) Our Multi-view OverlapDepth Constraint relies on the presence of overlapping regions between images. (3) Achieving fullydomain-agnostic approaches without any target labels remains challenging. As a result, it is essentialto incorporate a fallback plan when deploying the framework in safety-critical real-world scenarios. Summary. Multi-View 3DOD models often face challenges in expanding appropriately to unfamiliardatasets due to inevitable domain shifts (i.e., changes in the distribution of data between the trainingand testing phases). Especially, the limited resource (e.g., excessive computational overhead andtaxing expensive and taxing data cost) leads to hinder the successful deployment of Multi-View3DOD. To mitigate above drawbacks, we carefully design Unified Domain Generalization andAdaptation (UDGA), a practical solution for developing Multi-View 3DOD. We first introduceMulti-view Overlap Depth Constraint that advocates strong triangular clues between adjacent views,significantly bridging perspective gaps. Additionally, we present a Label-Efficient Domain Adaptationapproach that enables practical adaptation to novel targets with largely limited labels (i.e., 1% and5%) without forgetting well-aligned source potential. Our UDGA paradigm efficiently fine-tuneadditional parameters leveraging significantly fewer annotations by effectively transferring from thesource to target domain. In summary, our extensive experiments in various landmark datasets(e.g.,nuScenes, Lyft and Waymo) show that our novel paradigm, UDGA, provide a practical solution,outperforming current state-of-the-art models on Multi-view 3D object detection.",
  "and Disclosure of Funding": "This work was primarily supported by Samsung Advanced Institute of Technology (SAIT) (85%),partially supported by Institute of Information & communications Technology Planning & Evaluation(IITP) grant funded by the Korea government (MSIT) (No. 2019-0-00079, Artificial IntelligenceGraduate School Program (Korea University), 5%), and Culture, Sports and Tourism R&D Pro-gram through the Korea Creative Content Agency grant funded by the Ministry of Culture, Sportsand Tourism in 2024(International Collaborative Research and Global Talent Development for theDevelopment of Copyright Management and Protection Technologies for Generative AI, RS-2024-00345025, 5%; Development of technology for dataset copyright of multimodal generative AI model,RS-2024-00333068, 5%). Xuyang Bai, Zeyu Hu, Xinge Zhu, Qingqiu Huang, Yilun Chen, Hongbo Fu, and Chiew-LanTai. Transfusion: Robust lidar-camera fusion for 3d object detection with transformers. InProceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages10901099, 2022. Gyusam Chang, Wonseok Roh, Sujin Jang, Dongwook Lee, Daehyun Ji, Gyeongrok Oh, JinsunPark, Jinkyu Kim, and Sangpil Kim. Cmda: Cross-modal and domain adversarial adaptation forlidar-based 3d object detection. In Proceedings of the AAAI Conference on Artificial Intelligence,volume 38, pages 972980, 2024. Tingting Liang, Hongwei Xie, Kaicheng Yu, Zhongyu Xia, Zhiwei Lin, Yongtao Wang, TaoTang, Bing Wang, and Zhi Tang. Bevfusion: A simple and robust lidar-camera fusion framework.Advances in Neural Information Processing Systems, 35:1042110434, 2022. Zhijian Liu, Haotian Tang, Alexander Amini, Xinyu Yang, Huizi Mao, Daniela L Rus, and SongHan. Bevfusion: Multi-task multi-sensor fusion with unified birds-eye view representation.In 2023 IEEE international conference on robotics and automation (ICRA), pages 27742781.IEEE, 2023. Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, PaulTsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, et al. Scalability in perception forautonomous driving: Waymo open dataset. In Proceedings of the IEEE/CVF conference oncomputer vision and pattern recognition, pages 24462454, 2020. Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu,Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: A multimodaldataset for autonomous driving. In Proceedings of the IEEE/CVF conference on computervision and pattern recognition, pages 1162111631, 2020. John Houston, Guido Zuidhof, Luca Bergamini, Yawei Ye, Long Chen, Ashesh Jain, SammyOmari, Vladimir Iglovikov, and Peter Ondruska. One thousand and one hours: Self-drivingmotion prediction dataset. In Conference on Robot Learning, pages 409418. PMLR, 2021. Yue Wang, Vitor Campagnolo Guizilini, Tianyuan Zhang, Yilun Wang, Hang Zhao, and JustinSolomon. Detr3d: 3d object detection from multi-view images via 3d-to-2d queries. InConference on Robot Learning, pages 180191. PMLR, 2022. Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chonghao Sima, Tong Lu, Yu Qiao, andJifeng Dai. Bevformer: Learning birds-eye-view representation from multi-camera images viaspatiotemporal transformers. In European conference on computer vision, pages 118. Springer,2022. Wonseok Roh, Gyusam Chang, Seokha Moon, Giljoo Nam, Chanyoung Kim, Younghyun Kim,Jinkyu Kim, and Sangpil Kim. Ora3d: Overlap region aware multi-view 3d object detection.arXiv preprint arXiv:2207.00865, 2022.",
  "Junjie Huang, Guan Huang, Zheng Zhu, Yun Ye, and Dalong Du. Bevdet: High-performancemulti-camera 3d object detection in bird-eye-view. arXiv preprint arXiv:2112.11790, 2021": "Yinhao Li, Zheng Ge, Guanyi Yu, Jinrong Yang, Zengran Wang, Yukang Shi, Jianjian Sun, andZeming Li. Bevdepth: Acquisition of reliable depth for multi-view 3d object detection. InProceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 14771485,2023. Jose M Facil, Benjamin Ummenhofer, Huizhong Zhou, Luis Montesano, Thomas Brox, andJavier Civera. Cam-convs: Camera-aware multi-scale convolutions for single-view depth. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages1182611835, 2019. Shuo Wang, Xinhai Zhao, Hai-Ming Xu, Zehui Chen, Dameng Yu, Jiahao Chang, Zhen Yang,and Feng Zhao. Towards domain generalization for multi-view 3d object detection in bird-eye-view. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,pages 1333313342, 2023.",
  "Garrick Brazil and Xiaoming Liu. M3d-rpn: Monocular 3d region proposal network for objectdetection. In Proceedings of the IEEE/CVF International Conference on Computer Vision,pages 92879296, 2019": "Zechen Liu, Zizhang Wu, and Roland Tth. Smoke: Single-stage monocular 3d object detectionvia keypoint estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition Workshops, pages 996997, 2020. Yin Zhou and Oncel Tuzel. Voxelnet: End-to-end learning for point cloud based 3d objectdetection. In Proceedings of the IEEE conference on computer vision and pattern recognition,pages 44904499, 2018. Alex H Lang, Sourabh Vora, Holger Caesar, Lubing Zhou, Jiong Yang, and Oscar Beijbom.Pointpillars: Fast encoders for object detection from point clouds. In Proceedings of theIEEE/CVF conference on computer vision and pattern recognition, pages 1269712705, 2019. Jonah Philion and Sanja Fidler. Lift, splat, shoot: Encoding images from arbitrary camera rigsby implicitly unprojecting to 3d. In Computer VisionECCV 2020: 16th European Conference,Glasgow, UK, August 2328, 2020, Proceedings, Part XIV 16, pages 194210. Springer, 2020. Yingfei Liu, Tiancai Wang, Xiangyu Zhang, and Jian Sun. Petr: Position embedding transfor-mation for multi-view 3d object detection. In European Conference on Computer Vision, pages531548. Springer, 2022.",
  "A Vaswani. Attention is all you need. Advances in Neural Information Processing Systems,2017": "Jinhyung Park, Chenfeng Xu, Shijia Yang, Kurt Keutzer, Kris M Kitani, Masayoshi Tomizuka,and Wei Zhan. Time will tell: New outlooks and a baseline for temporal multi-view 3d objectdetection. In The Eleventh International Conference on Learning Representations, 2022. Chenyu Yang, Yuntao Chen, Hao Tian, Chenxin Tao, Xizhou Zhu, Zhaoxiang Zhang, GaoHuang, Hongyang Li, Yu Qiao, Lewei Lu, et al. Bevformer v2: Adapting modern imagebackbones to birds-eye-view recognition via perspective supervision. In Proceedings of theIEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1783017839, 2023.",
  "Zehui Chen, Zhenyu Li, Shiquan Zhang, Liangji Fang, Qinhong Jiang, and Feng Zhao.Bevdistill: Cross-modal bev distillation for multi-view 3d object detection. arXiv preprintarXiv:2211.09386, 2022": "Sujin Jang, Dae Ung Jo, Sung Ju Hwang, Dongwook Lee, and Daehyun Ji. Stxd: Structuraland temporal cross-modal distillation for multi-view 3d object detection. Advances in NeuralInformation Processing Systems, 36, 2024. Yan Wang, Xiangyu Chen, Yurong You, Li Erran Li, Bharath Hariharan, Mark Campbell,Kilian Q Weinberger, and Wei-Lun Chao. Train in germany, test in the usa: Making 3d objectdetectors generalize. In Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 1171311723, 2020. Jihan Yang, Shaoshuai Shi, Zhe Wang, Hongsheng Li, and Xiaojuan Qi. St3d: Self-trainingfor unsupervised domain adaptation on 3d object detection. In Proceedings of the IEEE/CVFconference on computer vision and pattern recognition, pages 1036810378, 2021. Qiangeng Xu, Yin Zhou, Weiyue Wang, Charles R Qi, and Dragomir Anguelov. Spg: Unsuper-vised domain adaptation for 3d object detection via semantic point generation. In Proceedingsof the IEEE/CVF International Conference on Computer Vision, pages 1544615456, 2021. Zeng Yihan, Chunwei Wang, Yunbo Wang, Hang Xu, Chaoqiang Ye, Zhen Yang, and Chao Ma.Learning transferable features for point cloud detection via 3d contrastive co-training. Advancesin Neural Information Processing Systems, 34:2149321504, 2021. Yi Wei, Zibu Wei, Yongming Rao, Jiaxin Li, Jie Zhou, and Jiwen Lu. Lidar distillation: Bridgingthe beam-induced domain gap for 3d object detection. In European Conference on ComputerVision, pages 179195. Springer, 2022. Yanan Zhang, Chao Zhou, and Di Huang. Stal3d: Unsupervised domain adaptation for 3dobject detection via collaborating self-training and adversarial learning. IEEE Transactions onIntelligent Vehicles, 2024. Qianjiang Hu, Daizong Liu, and Wei Hu. Density-insensitive unsupervised domain adaptionon 3d object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 1755617566, 2023. Vidit Vidit, Martin Engilberge, and Mathieu Salzmann. Clip the gap: A single domain gen-eralization approach for object detection. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, pages 32193229, 2023. Aming Wu and Cheng Deng. Single-domain generalized object detection in urban scene viacyclic-disentangled self-distillation. In Proceedings of the IEEE/CVF Conference on computervision and pattern recognition, pages 847856, 2022. Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollr, and Ross Girshick. Maskedautoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference oncomputer vision and pattern recognition, pages 1600016009, 2022. Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple frameworkfor contrastive learning of visual representations. In International conference on machinelearning, pages 15971607. PMLR, 2020. Zhenyu Li, Zehui Chen, Ang Li, Liangji Fang, Qinhong Jiang, Xianming Liu, and JunjunJiang. Unsupervised domain adaptation for monocular 3d object detection via self-training. InEuropean conference on computer vision, pages 245262. Springer, 2022. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe,Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learningfor nlp. In International conference on machine learning, pages 27902799. PMLR, 2019.",
  "Dongze Lian, Daquan Zhou, Jiashi Feng, and Xinchao Wang. Scaling & shifting your features:A new baseline for efficient model tuning. Advances in Neural Information Processing Systems,35:109123, 2022": "Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, andColin A Raffel. Few-shot parameter-efficient fine-tuning is better and cheaper than in-contextlearning. Advances in Neural Information Processing Systems, 35:19501965, 2022. Yue-Jiang Dong, Yuan-Chen Guo, Ying-Tian Liu, Fang-Lue Zhang, and Song-Hai Zhang.Ppea-depth: Progressive parameter-efficient adaptation for self-supervised monocular depthestimation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages16091617, 2024. Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen Koltun.CARLA: An open urban driving simulator. In Proceedings of the 1st Annual Conference onRobot Learning, pages 116, 2017. Qiqi Gu, Qianyu Zhou, Minghao Xu, Zhengyang Feng, Guangliang Cheng, Xuequan Lu,Jianping Shi, and Lizhuang Ma. Pit: Position-invariant transform for cross-fov domain adapta-tion. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages87618770, 2021. Tzofi Klinghoffer, Jonah Philion, Wenzheng Chen, Or Litany, Zan Gojcic, Jungseock Joo,Ramesh Raskar, Sanja Fidler, and Jose M Alvarez. Towards viewpoint robustness in birds eyeview segmentation. In Proceedings of the IEEE/CVF International Conference on ComputerVision, pages 85158524, 2023. Yunhan Zhao, Shu Kong, and Charless Fowlkes. Camera pose matters: Improving depthprediction by mitigating pose distribution bias. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, pages 1575915768, 2021.",
  "Ke Wang, Bin Fang, Jiye Qian, Su Yang, Xin Zhou, and Jie Zhou. Perspective transformationdata augmentation for object detection. IEEE Access, 8:49354943, 2019": "Clment Godard, Oisin Mac Aodha, Michael Firman, and Gabriel J Brostow. Digging intoself-supervised monocular depth estimation. In Proceedings of the IEEE/CVF internationalconference on computer vision, pages 38283838, 2019. Wenjing Bian, Zirui Wang, Kejie Li, Jia-Wang Bian, and Victor Adrian Prisacariu. Nope-nerf: Optimising neural radiance field with no pose prior. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition, pages 41604169, 2023. Xiaoyang Lyu, Liang Liu, Mengmeng Wang, Xin Kong, Lina Liu, Yong Liu, Xinxin Chen, andYi Yuan. Hr-depth: High resolution self-supervised monocular depth estimation. In Proceedingsof the AAAI conference on artificial intelligence, volume 35, pages 22942301, 2021.",
  "CARLA6128-beam102.0M2,500< 100mCarla Town10---": "We evaluate overall performance on landmark datasets for 3D Object Detection: Waymo , Lyft ,and nuScenes . The three datasets have different point cloud ranges and specifications. Hence,we convert them to a unified range and coordinates for accurate comparison. We also adopt onlyseven parameters to achieve consistent training results under the same conditions: center locations(x, y, z), box size (l, w, h), and heading angle . Additionally, to estimate practical degradation dueto changes in camera positioning, we conducted a proof of concept by generating data similar to thenuScenes using the CARLA simulation. The details are as follows: Waymo The Waymo dataset consists of high-quality and large-scale data with 230K frames fromall 1,150 scenes using multiple LiDAR scanners and cameras. Furthermore, for the generalizationpurpose, Waymo is recorded at diverse cities, weather conditions, and times. For object detection in2D or 3D, Waymo provides point cloud-annotated 3D bounding boxes as 3D data pairs and RGBimage-annotated 2D bounding boxes as 2D data pairs. nuScenes The nuScenes dataset uses 6 cameras that cover a full 360-degree range of view anda single LiDAR sensor to obtain 40K frames from 20-second-long 1,000 video sequences, whichare fully annotated with 3D bounding boxes for 10 object classes. The nuScenes dataset covers 28kannotated samples for training. Also, validation and test contain 6k scenes each. The nuScenesframes are captured in the same manner as Waymo dataset for the data diversity. But unlike Waymo,nuScenes provides labels only for the point cloud data with 23 classes of 3D bounding boxes. Lyft The Lyft dataset is motivated by the impact of large-scale datasets on Machine Learningand consists of over 1,000 hours of data. This was collected by a fleet of 20 autonomous vehiclesalong a fixed route in Palo Alto, California, over a four-month period. It consists of 170,000 scenes(each scene is 25 seconds long) and contains 3D bounding boxes with the precise positions of nearbyvehicles, cyclists, and pedestrians over time. In addition, the Lyft dataset includes a high-definitionsemantic map with 15,242 labelled elements and a high-definition aerial view over the area. CARLA To quantify the performance drop resulting from camera shifts, we employed an autonomousdriving simulation powered by CARLA 0.9.14 and Unreal Engine 4.26. We collected 24Kframes for training and 1K frames for each evaluation, driving through Town10 under cloudlessweather conditions between sunrise and sunset times. This dataset includes over 100 vehicles and30 pedestrians in random locations. In , the Source utilizes 6 nuScenes-like cameras and 6LiDARs, while the Target has perturbed sensors. From the Source sensors, the Height increases by0.65m and the Pitch increases by 5 degrees. The All synthetically moves the x, y, z-coordinates by-0.12m, 0.65m, and -0.2m/+0.2m, respectively, and rotates the yaw by -5/+5 degrees, depending ontheir directions. Each target sets is collected simultaneously with the Source. : Comparison of Unified Domain Generalization and Adaptation performance with state-of-the-art techniques. We validate our proposed methods with the same baseline model, namedBEVDepth, on Cross-domain. The bold values indicate the best performance. Also, denotes Donot support.",
  "BImplementation Details": "To validate the effectiveness of our proposed methods, we adopt BEVDepth and BEVFormer as our base detectors. Both detectors utilize ResNet50 backbone that initialized from ImageNet-1K. Also, we construct BEV representations within a perception range of [-50.0m, 50.0m] for both theX and Y axes. In BEVDepth, we reshape multi-view input image resolutions as follow: fornuScenes, for Lyft, for Waymo. As following DG-BEV , we train 24 epochswith AdamW optimizer by learning rate 2e-4 in pre-training phase. The training takes approximately18 hours using one A100 GPU. In fine-tuning phase, we conduct an extensive grid search to determinethe optimal learning rate proportional to the number of learnable parameters. Note that we extensivelyaugment various image conditions as detailed in .",
  "CAdditional Experiments": "In this appendix, we present additional experiments to validate the effectiveness of our proposedparadigm. First, Tab. 6 summarizes the overall results of our work from the perspective of domainshift. We also analyze how changes in camera positioning worsen the performance and evaluatewhether existing augmentation methods can mitigate the deterioration. Additionally, we conductablation studies to enhance the LEDA structure, including comparisons with formal adapters. Finally,we present the comparison results with the transformer-based detector. The qualitative analysis of themulti-view results from our proposed paradigm is included towards the end of this chapter. Performance across domains. In this section, we compare our proposed UDGA with existingsolutions (i.e., DG, UDA) in various cross-domain conditions (see Tab. 6). We aim to practicallymitigate perspective shifts without hindering well-defined source knowledge. Our DG branch achievestop performance, surpassing Direct Transfer in the Source domain. The UDGA, which follows DG,improves Target accuracy without compromising Source performance. Especially, we advocate thatUDGA enables efficient adaptation with significantly down-scaled data split (i.e., 1% and 5%). Also,it is noteworthy that UDGA do not forget previously learned potentials, fully transferring to targetdomains (up to +14.2% NDS gain in LyftnuScenes). Overall, UDGA provide a practical solutionto address perspective view changes, efficiently adapting with only tiny split.",
  "Ours0.4210.2810.4870.324": "Practical domain shift analysis. We analyze the impact of changes in camera geometry on 3D objectestimation. The experimental model is trained using only the source dataset on ResNet50-basedBEVDet and then evaluated on three sets of (source, target) to analyze performance differences. InTab. 7, the performance of the source is similar in all test sets. On the other hand, the performanceof the target decreases significantly in all cases. Since this experiment is conducted in the sameenvironment with the same camera sensors, it demonstrates how much performance degradationis caused by the position of the camera. The set with the largest performance drop in the target isHeight, where the mATE value increased significantly. The target All exhibits the worst mASE andmAOE, while the other measures also deteriorated by a similar amount as Height. Conventional augmentation methods enhance the robustness of the model. We evaluate some ofthem in Tab. 8. GT sampling and CBGS are techniques designed to balance ground truths. 2Daugmentation directly augment multi-view inputs (i.e., image resize, crop and paste, contrast andbrightness distortion). 3D and extrinsic methods are global augmentations that address both input andground truths, and ground truths only, respectively. These methods enhance geometric understandingfrom input noises. However, in dynamic view changes (i.e., cross-domain), they still suffer fromgeometric inconsistency and show poor generalization capability. Moreover, various 2D approachesdo not guarantee geometric alignments between 2D images and 3D ground truths and relevant studieshave not been explored well, as reported in and . Searching adapter structures. We explore various modules and structures to find a suitable adapterarchitecture. Tab. 9, 10 show which structures and locations affects the models performance. Foradapter locations, performance is optimal when adapters are attached to all modules, gradually",
  "OursConv.Linear8.8M0.5730.4570.6380.537": "improving with the addition of more. Exceptionally, attaching adapters only at the Detection Headleads to a decline in LyftnuScenes. In addition, Tab. 10 represents the performance of variousadapter structures. The combination of Convolution and Linear layer respectively for Project Downand Up shows the best performance in both tasks. Note that training with fewer parameters(8.8M) ismore effective. However, we suggest that large-scale parameters may require a larger dataset or moretraining, as we only trained on 10% of the target dataset for less than 20 epochs in this experiment. : Comparison of UDGA performance on BEVFormer. We train with two different data splits50%, and 100%. Additionally, # Params denote the number of parameters for training. The boldvalues indicate the best performance. denotes Do not support.",
  "V(I, K, T) = CrossAttn(q : Pxyz, k v : F2d),(10)": "where q, k and v represents query, value and key in Transformer, and then Pxyz denotes pre-definedanchor BEV positions by K, and T. Here, Query-based module benefits from CrossAttn with sparsequery sets, implicitly learning geometric information. Thus, we reconstruct our UDGA paradigmwithout explicit depth constraints. First, we adopt linear-based bottleneck structures with LayerNormalization in Eq. 11. up and down denote the projection up and down layer.",
  "y = ffn(x) + up((down(LN(x)))),(11)": "where ffn denotes feed-forward networks, and LN represents Layer Normalization. We conductexperiments by plugging these extra modules, which accounts for 36% of the total parameters, intoBEVFormer. As a result, we achieve significant adaptation performance with the 50% data split.Notably, we demonstrate effectiveness, achieving parity with Full FT in the 100% data split. Additional qualitative analysis. In this section, we further visualize our depth quality in variousscenarios (i.e., Lyft, and nuScenes). Not only our overlap depth constraint significantly improve depth",
  "DBroader Impacts": "Our framework is a practical AI algorithm that enhances its generalization ability to handle domainchanges robustly, enabling us to effectively reduce data costs and computing resources required foradaptation. Practically, our method makes it suitable for deployment in mass-produced vehicles,where the algorithm can inherit the knowledge of well-trained pretrained weights while self-learningto adapt to each fleet environment. The adaptation learning process is also simplified, making it easierto transfer improved pretrained networks. Furthermore, by demonstrating superior performancecompared to previous methods that relied on LiDAR for auxiliary depth networks, our approachreduces the dependency on lidar modality. This suggests the feasibility of excluding expensiveLiDAR sensors from future autonomous vehicles.",
  "(b) nuScenes": ": Multi-view visualization of the depth estimation of BEVDepth and Ours for (a)Lyft and(b)nuScenes samples. In general, our depth consistency was better in the Lyft dataset, while it wasdifficult to make a quantitative comparison in the case of nuScenes due to the sparseness of theLiDAR point clouds. The depth range is from 1m to 60m. Best viewed in color."
}