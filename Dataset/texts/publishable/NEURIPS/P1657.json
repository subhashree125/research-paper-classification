{
  "Abstract": "Dense Associative Memories are high storage capacity variants of the Hopfieldnetworks that are capable of storing a large number of memory patterns in theweights of the network of a given size. Their common formulations typicallyrequire storing each pattern in a separate set of synaptic weights, which leads tothe increase of the number of synaptic weights when new patterns are introduced.In this work we propose an alternative formulation of this class of models usingrandom features, commonly used in kernel methods. In this formulation the numberof networks parameters remains fixed. At the same time, new memories can beadded to the network by modifying existing weights. We show that this novelnetwork closely approximates the energy function and dynamics of conventionalDense Associative Memories and shares their desirable computational properties.",
  "Introduction": "Hopfield network of associative memory is an elegant mathematical model that makes it possible tostore a set of memory patterns in the synaptic weights of the neural network . For a given prompti(t = 0), which serves as the initial state of that network, the neural update equations drive thedynamical flow towards one of the stored memories. For a system of K memory patterns in the D-dimensional binary space the networks dynamics can be described by the temporal trajectory i(t),which descends the energy function",
  "i=1i i2(1)": "Here i (index = 1...K, and index i = 1...D) represent memory vectors. The neural dynamicalequations describe the energy descent on this landscape. In this formulation, which we call thememory representation, the geometry of the energy landscape is encoded in the weights of thenetwork i , which coincide with the memorised patterns. Thus, in situations when the set of thememories needs to be expanded by introducing new patterns one must introduce additional weights. Alternatively, one could rewrite the above energy in a different form, which is more commonly usedin the literature. Specifically, the sum over the memories can be computed upfront and the energy canbe written as",
  "arXiv:2410.24153v1 [cs.LG] 31 Oct 2024": ": The Distributed Representation for Dense Associative Memory (DrDAM) approximates boththe energy and fixed-point dynamics of the traditional Memory Representation for Dense AssociativeMemory (MrDAM) while having a parameter space of constant size. A) Diagram of DrDAM using a basis function parameterized by random features (e.g., see eq. (8)). In the distributed representation,adding new memories does not change the size of the memory tensor. B) Comparing energy descentdynamics between DrDAM and MrDAM on 3x64x64 images from Tiny Imagenet . Both modelsare initialized on queries where the bottom two-thirds of pixels are occluded with zeros; dynamics arerun while clamping the visible pixels and their collective energy traces shown. DrDAM achieves thesame fixed points as MrDAM, and these final fixed points have the same energy. The energy decreaseswith time for both MrDAM and DrDAM, although the dependence of the energy relaxation towards thefixed point is sometimes different between the two representations. Experimental setup is describedin appendix D. In this form one can think about weights of the network being the symmetric tensor Tij instead ofi . One advantage of formulating the model this way is that the tensor Tij does not require addingadditional parameters when new memories are introduced. Additional memories are stored in thealready existing set of weights by redistributing the information about new memories across thealready existing network parameters. We refer to this formulation as distributed representation. A known problem of the network (eqs. (1) and (2)) is that it has a small memory storage capacity,which scales at best linearly as the size of the network D is increased . This limitation has beenresolved with the introduction of Dense Associative Memories (DenseAMs), also known as ModernHopfield Networks . This is achieved by strengthening the non-linearities (interpreted as neuralactivation functions) in eq. (1), which can lead to the super-linear and even exponentially largememory storage capacity . Using continuous variables x RD, the energy is defined as1",
  "=1FS, g(x),(3)": "where the function g : RD RD is a vector function (e.g., a sigmoid, a linear function, or alayernorm), the function F() is a rapidly growing separation function (e.g., power F() = ()n orexponent), S[x, x] is a similarity function (e.g., a dot product or a Euclidean distance), and Q is ascalar monotone function (e.g., linear or logarithm). For instance, in order to describe the classicalHopfield network with binary variables (eq. (1)) one could take: linear Q, quadratic F() = ()2,dot product S, and a sign function for gi = sign(xi) = i. There are many possible combinationsof various functions g, F(), S(, ) that lead to different models from the DenseAM family ;many of the resulting models have proven useful for various problems in AI and neuroscience .Diffusion models have been linked to even more sophisticated forms of the energy landscape . From the perspective of the information storage capacity DenseAMs are significantly superiorcompared to the classical Hopfield networks. At the same time, most2 of the models from theDenseAM family are typically formulated using the memory representation, and for this reason 1Throughout the paper we use bold symbols for denoting vectors and tensors, e.g., is a D-dimensionalvector in the space of neurons for each value of index . Individual elements of those vectors and tensors aredenoted with the same symbol, but with plain font. In the example above, these individual elements have anexplicit vector index, e.g., i . Same applies to vectors in the feature space introduced later.2This is true for all DenseAMs with the exception of the power model of Krotov and Hopfield , which canbe written using n-index tensors Ti1,i2,...,in in analogy with the 2-tensor Tij as in eq. (2). : DrDAM achieves parameter compression over MrDAM, successfully storing 20 different64x64x3 images from TinyImagenet and retrieving them when occluding the lower 40% of eachquery. The memory matrix of MrDAM is of shape (20, 12288) while the memory tensor of DrDAM isof shape Y = 2 105, a 20% reduction in the number of parameters compared to MrDAM; all otherconfigurations for this experiment match those in appendix D. Further compression can be achievedwith a higher tolerance for DrDAMs retrieval error, smaller , and fewer occluded pixels, see 4.Top: Occluded query images. Middle: Fixed-point retrievals from DrDAM. Bottom: (ground truth)Fixed-point retrievals of MrDAM. require introduction of new weights when additional memory patterns are added to the network. Themain question that we ask in our paper is: how can we combine superior memory storage propertiesof DenseAMs with the distributed (across synaptic weights) formulation of these models in the spiritof classical Hopfield networks (eq. (2))? If such a formulation is found, it would allow us to addmemories to the existing network by simply recomputing already existing synaptic weights, withoutadding new parameters. A possible answer to this question is offered by the theory of random features and kernel machines.Given an input domain X, kernel machines leverage a positive definite Mercer kernel functionk : X X R+ that measures the similarity between pairs of inputs. The renowned kernel trickallows one to compute the inner-product",
  "=1(x)(x)(4)": "between two inputs x, x X in a rich feature space defined by the feature map (x) without everexplicitly realizing the feature map (x). Various machine learning models (such as support vectormachines , logistic regression, and various others ) can be learned with just access topairwise inner-products, and thus, the kernel trick allows one to learn such models in an extremelyexpressive feature space. Kernel functions have been developed for various input domains beyondthe Euclidean space such as images, documents, strings (such as protein sequences ), graphs(molecules , brain neuron activation paths) and time series (music, financial data) . Commonkernels for Euclidean data are the radial basis function or RBF kernel k(x, x) = exp(x x22)and the polynomial kernel k(x, x) = (x, x + b)p. To appreciate the expressivity of these kernelmachines, note that, for input domain RD, the RBF kernel corresponds to an infinite dimensionalfeature space (Y = ) and the polynomial kernel to a O(Dp) dimensional feature space. Interpreting the composition of the separation and similarity functions in eq. (3) as the left handside of the kernel trick eq. (4) we can map the energy into the feature space, using appropriatelychosen feature maps. Subsequently, the order of the summations over memories and features can beswapped, and the sum over memories can be computed explicitly. This makes it possible to encodeall the memories in a tensor T, which we introduce in section 3, that contains all the necessaryinformation about the memories. The energy function then becomes defined in terms of this tensoronly, as opposed to individual memories. This functionality is summarized in fig. 1. Additionally,we show examples of retrieved Tiny ImageNet images that have been memorised using the originalDenseAM model, which we call MrDAM, and the featurized version of the same model, which wecall DrDAM (please see the explanations of these names in the caption to fig. 1). These examplesvisually illustrate that mapping the problem into the feature space preserves most of the desirablecomputational properties of DenseAMs, which normally are defined in the kernel space.",
  "Contributions:": "We propose a novel approximation of a DenseAM network utilizing random features commonlyused in kernel machines. This novel architecture does not require the storage of the originalmemories, and can incorporate new memories without increasing the size of the network. We precisely characterize the approximation introduced in the energy descent dynamics by thisarchitecture, highlighting the different critical factors that drive the difference between the exactenergy descent and the proposed approximate one.",
  "We validate our theoretical guarantee with empirical evaluations": "In the past, kernel trick has been used for optimizing complexity of the attention mechanism inTransformers , and those results have been recently applied to associative memory , given thevarious connections between Transformers and DenseAMs . Existing studies focus onsettings when attention operation or associative memory retrieval is done in a single step update. Thisis different from our goals here, which is to study the recurrent dynamics of the associative memoryupdates and convergence of that dynamics to the attractor fixed points. Iatropoulos et al. proposekernel memory networks which are a recurrent form of a kernel support vector machine, and highlightthat DenseAM networks are special cases of these kernel memory networks. Making a connectionbetween nonparametric kernel regression and associative memory, Hu et al. propose a familyof provably efficient sparse Hopfield networks , where the dynamics of any given input areexplicitly driven by a subset of the memories due to various entropic regularizations on the energy.DenseAMs have been also used for sequences . To reduce the complexity of computingall the pairs of F(S[, x]) for a given set of memories and queries, Hu et al. leverage a low-rankapproximation of this separation-similarity matrix using polynomial expansions. The kernel trick hasalso recently been used to increase separation between memories (with an additional learning stageto learn the kernel), thereby improving memory capacity . There are also very recent theoreticalanalysis of the random feature Hopfield networks , where their focus in on the construction ofmemories using random features. Kernels are also related to density estimation , and recent workshave leveraged a connection between mixtures of Gaussians and DenseAMs for clustering .Lastly, random features have been used for biological implementations of both Transformers andDenseAMs . To the best of our knowledge there is no rigorous theoretical and empirical comparison of DenseAMsand their distributed (featurized) variants in recurrent memory storage and retrieval settings, as wellas results pertaining to the recovery of the fixed points of the energy descent dynamics. This is themain focus of our work.",
  "Technical background": "Given the energy function in eq. (3), a variable x is updated in the forward pass through the layersof this recurrent model such that its energy decreases with each update. If the energy is bounded frombelow, this ensures that the input will (approximately) converge to a local minimum. This can beachieved by performing a gradient descent in the energy landscape. Considering the continuousdynamics, updating the input x over time with dx/dt, we need to ensure that dE/dt < 0. This canbe achieved by setting dx/dt xE.",
  "x(t) x(t1) (t1)xE(t1),(5)": "where (t) is a (step dependent) step-size for the energy gradient descent, E(t) is the energy of theinput after the t-th layer, and the input to the first layer x(0) x. The final output of the associativememory network after L layers is x(L). DenseAMs significantly improve the memory capacity of the associative memory network by utiliz-ing rapidly growing nonlinearity-based separation-similarity compositions such as F(S[x, ]) =exp( x, ) or F(S[x, ]) = exp(/2x2) or F(S[x, ]) = (x, )p , p > 2, amongother choices, with > 0 corresponding to the inverse temperature that controls how rapidly theseparation-similarity function grows. However, these separation-similarity compositions do not allowfor the straightforward simplifications as in eq. (2), except for the power composition. For a generalsimilarity function, the update based on gradient descent over the energy in eq. (3) is given by:",
  "dx. (7)": "This form does not directly admit itself to a distributed storage of memories as in eq. (2), and thus,in order to perform the gradient descent on the energy, it is necessary to keep all the memories intheir original form. We will try to address this issue by taking inspiration from the area of kernelmachines .",
  "Random Features for Kernel Machines": "The expressivity of kernel learning usually comes with increased computational complexity both dur-ing training and inference, taking time quadratic and linear in the size of the training set respectively.The groundbreaking work of Rahimi and Recht introduced random features to generate explicitfeature maps : RD RY for the RBF and other shift-invariant kernels4 that approximate the truekernel function that is (x), (x) k(x, x). Various such random maps have been developedfor shift-invariant kernels and polynomials kernels . For the RBF kernel and the exponentiated dot-product or EDP kernel k(x, x) = exp(x, x), thereare usually two classes of random features trigonometric features and exponential features. For theRBF kernel k(x, x) = exp(x x22/2), the trigonometric features are given on the left andthe exponential features are on the right:",
  ",(8)": "where N(0, ID) {1, . . . , Y } are the random projection vectors.5 A random feature map for the RBF kernel can be used for the EDP kernel by scaling (x) with exp(x22/2). Whilethe trigonometric features ensure that k(x, x) = (x), (x) = 1, the exponential features ensurethat (x) R2Y+ , which is essential in certain applications as in transformers . Furthermore,while the random samples N(0, ID) are supposed to be independent, Choromanski et al. show that the {1, . . . , Y } can be entangled to be exactly orthogonal to further reduce thevariance of the approximation while maintaining unbiasedness. In general, the approximation ofthe random feature map is O( D/Y ), implying that a feature space with Y O(D/2) randomfeatures will ensure, with high probability, for any x, x RD, |k(x, x) (x), (x) | .Scaling in the kernel functions such as exp(x x22/2) or exp( x, x) can be handled withthe aforementioned random feature maps by applying them to x with(x), (x)exp(x x22/2).",
  ".(9)": "3We are eliding the dg(x)/dx = (1/x2)[ID (1/x3/22)xx] term for the ease of exposition.4Kernel functions that only depend on (x x) and not individually on x and x.5A technical detail here is that while we are using Y random samples, we are actually developing a 2Y -dimensional feature map : RD R2Y we can get a Y dimensional feature map by dropping the sin()terms in the trigonometric features (and add a random rotation term b, Y to the cos(, x + b)term), and the exp() term in the exponential features. This modification (using 2Y features instead of Y )reduces the variance of the kernel function approximation [18, Lemma 1, 2].",
  "dx(10)": "where d(x)/dx RY D is the gradient of the feature map with respect to its input. In the presenceof such an explicit map , we can distribute the memory in a MrDAM into the single Y -dimensionalvector T, and be able to apply the update in eq. (10). We can then use the random feature basedenergy gradient x E(x) instead of the true energy gradient xE(x) in the energy gradient descentstep in eq. (5).6 We name this scheme Distributed representation for Dense Associative Memoryor DrDAM, and we compare the computational costs of DrDAM with the Memory representation ofDense Associative Memory or MrDAM in the following: Proposition 1. With access to the K memories { RD, K}, MrDAM takes O(LKD) timeand O(KD) peak memory for L energy gradient descent steps (or layers) as defined in eq. (5) withthe true energy gradient xE(x).",
  "Compute q dQ(s)/ds|s=T,preturn qz": "Naively, the random feature based DrDAM would requireO(DY ) memory to store the random vectors and thex(x) matrix. However, we can show that we cangenerate the random vectors on demand to reduce theoverall peak memory to just O(Y ). The various proce-dures in DrDAM are detailed in Algorithm 1. The RFsubroutine generates the random feature for any memoryor input. The ProcMems subroutine consolidates all thememories into a single T RY vector. The GradCompsubroutine compute the gradient x E. The followingare the computational complexities of these procedures: Proposition 2. The RF subroutine in Algorithm 1 takesO(DY ) time and O(D + Y ) peak memory.Proposition 3. ProcMems in Algorithm 1 takesO(DY K) time and O(D + Y ) peak memory.Proposition 4. GradComp in Algorithm 1 takesO(D(Y + D)) time and O(D + Y ) peak memory. Thus, the computational complexities of DrDAM neuraldynamics are (see appendix F.1 for proof and discus-sions):Theorem 1. With a random feature map utiliz-ing Y random projections {, {1, . . . , Y }}and K memories { RD, {1, . . . , K}}, the random-feature based DrDAM takesO (D (Y K + L(Y + D))) time and O(Y + D) peak memory for L energy gradient descent steps(or layers) as defined in eq. (5) with the random feature based approximation gradient x E(x) de-fined in eq. (10). However, note that the memory encoding only needs to be done once, while the same T can be utilizedfor L steps of energy gradient steps for multiple input, and the cost of ProcMems is amortized overthese multiple inputs. We also show that the computational costs of the inclusion of a new memories :",
  "Proposition 5. The inclusion of a new memory RD to a DrDAM with K memories distributed inT RY takes O(DY ) time and O(D + Y ) peak memory": "The above result shows that inclusion of new memories correspond to constant time and memoryirrespective of the number of memories in the current DenseAM. Next, we study the divergencebetween the output of a L-layered MrDAM using the energy descent in eq. (5) with the true gradientin eq. (6) and that of DrDAM using the random feature based gradient in eq. (10).",
  "there is a constant C1 > 0 such thatexp(x x22/2) (x), (x) C1": "D/Y . Given aninput x X, let x(L) be the output of the MrDAM defined by the energy function in eq. (11) usingthe true energy gradient in eq. (6) and x(L) be the output of DrDAM with approximate gradient ineq. (10) using the random feature map using a constant step-size of > 0 in (5). Thenx(L) x(L)2 2LC1KeE(x)",
  "(12)": "Assumption (A1) just ensures that all the memories and inputs have bounded norm, and can beachieved via translating and scaling the memories and inputs. Assumption (A2) pertains to theapproximation introduced in the kernel function evaluation with the random feature map, and issatisfied (with high probability) based on results such as Rahimi and Recht [37, Claim 1] andChoromanski et al. [18, Theorem 4]. The above result precisely characterizes the effect on thedivergence x(L) x(L) of the (i) initial energy of the input E(x) lower is better, (ii) the inversetemperature lower is better, (iii) the number of memories K lower is better, (iv) the ambientdata dimensionality D lower is better, (v) the number of random features Y higher is better,and (vi) the number of layers L lower is better. The proof and further discussion are provided inappendix F.2. Note that theorem 2 analyzes the discretized system, but as the step-size 0, weapproach the fully contracting continuous model. An appropriate choice for the energy descent step-size simplifies the above result, bounding the divergence to O(",
  "(D1) for the same query, DrDAM must predict similar energies and energy gradients as MrDAM; and(D2) for the same initial query, DrDAM must retrieve similar fixed points as MrDAM": "However, in our experiments we observed that the approximation quality of DrDAM is stronglyaffected by the choice of and that the approximation quality decreases the further the query patternsare from the stored memory patterns, as predicted by theorem 2. We characterize this behavior inthe following experiments using the trigonometric SinCos basis function, which performed best inour ablation experiments (see appendix C), but note that the choice of the random features do play asignificant role in the interpretations of these results.",
  "(D1) How accurate are the energies and gradients of DrDAM?": "evaluates how well DrDAM, configured at different feature sizes Y , approximates the energyand energy gradients of MrDAM configured with different inverse temperatures and storing randombinary patterns of dimension D. The experimental setup is described below. : DrDAM produces better approximations to the energies and gradients of MrDAM whenthe queries are closer to the stored patterns. Approximation quality improves with larger featuredimension Y , but decreases with higher and higher pattern dimension D. Approximation error iscomputed on 500 stored binary patterns normalized between {0,1 D}. The Mean ApproximationErrors (MAE, eq. (14)) is taken over 500 queries initialized: at stored patterns (i.e., queries equalthe stored patterns), near stored patterns (i.e., queries equal the stored patterns where 10% of thebits have been flipped), and randomly (i.e., queries are random and far from stored patterns). Errorbars represent the standard error of the mean but are visible only at poor approximations. Redhorizontal lines represent the expected error of random energies and gradients. The theoretical errorupper bounds of eq. (13) (dark curves on the gradient errors in the right plot only) show a tight fit toempirical results at low and D and are only shown if predictions are better than random. Theshaded area shows the difference between the theoretical bound and the empirical results.",
  "D}D, K, where D is a hyperparameter controlled by the experiment. For a given , thememory matrix is converted into the featurized memory vector T :=": "() from eq. (9), where 2Y . The remaining patterns are treated as the random queries xbfar, b K (i.e., queriesthat are far from the stored patterns). Finally, in addition to evaluating the energy at these randomqueries and at the stored patterns, we also want to evaluate the energy at queries xbnear that are nearthe stored patterns; thus, we take each stored pattern and perform bit-flips on 0.1D of its entries. For each set of queries xb {b, xbnear, xbfar}, b K, and choice of , Y , and D, we compute theMean Approximation Error (MAE) between MrDAMs energy Eb := E(xb; , ) (whose gradientmatrix is denoted xEb) and DrDAMs energy Eb := E(xb; , T) (whose gradient is denoted x Eb).",
  "xEb x Eb2(14)": "We found it useful to visualize the results using log-scale and to compare the errors against theexpected error of a random guess of the energy/gradients (horizontal red dashed line in each plotof fig. 3). The random guess error was empirically computed by sampling a new set of randomqueries xbguess, b K (independent of the reference queries) and computing the MAE between thestandard energy on the reference queries vs. the approximate energies on the random queries. Thiserror was averaged across Y for each ; the highest average error across all s is plotted.Observation 1: DrDAM approximations are best for queries near stored patternsDrDAMapproximations for both the energy and energy gradients are better the closer the query patterns areto the stored patterns. In this regime, approximation accuracy predictably improves when increasing : A) Retrieval errors predictably follow the approximation quality of fig. 3. Error is lowestat/near stored patterns but is completely random when energy and gradient approximations are poor,i.e., at high values of and D. Note that error improves across Y but follows a different (andnoisier) trace than the corresponding approximations for energy and gradient in fig. 3 due to erroraccumulating over multiple update steps. B) DrDAMs approximation quality improves as Y increases(visible at low ), but larger Y s are needed for good approximations to the DAMs fixed pointsat higher s. (Left) The same corrupted query from CIFAR-10 where bottom 50% is masked ispresented to DAMs with different s. (Middle) The fixed points of DrDAM for each at differentsizes Y of the feature space. (Right) The ground truth fixed point of MrDAM. The top 50% of pixelsare clamped throughout the dynamics.",
  "the value for Y within reasonable values (i.e., values corresponding into sizes of featurized queriesand memories that can operate within 46GB of GPU memory)": "Observation 2: DrDAM approximations worsen as inverse temperature increasesAcrossnearly all experiments, DrDAM approximations worsen as increases. At queries near the storedpatterns, = 50 has an energy error approximately 10 that of = 30 and 100 that of = 10across all Y . At high D and when queries are far from the patterns, the error of = 50 approaches1000 the error of = 10. This observation similarly holds for the errors of corresponding gradients,corroborating the statement of theorem 2. Observation 3: DrDAM approximations break at sufficiently high values of D and In general,DrDAMs approximation errors remain the same across choices for D, especially when the queriesare near the stored patterns. However, when both and D are sufficiently large (e.g., 40 andD 100 in fig. 3), increasing the value of Y does not improve the approximation quality: DrDAMcontinues to return almost random gradients and energies. We explore this phenomenon more in 4.2in the context of the retrievability of stored patterns.",
  "(D2) How accurate are the memory retrievals using DrDAM?": "Memory retrieval is the process by which an initial query x(0) descends the energy function and istransformed into a fixed point of the energy dynamics. This process can be described by the discreteupdate rule in eq. (5), where E can represent either MrDAMs energy or the approximate energy ofDrDAM. A memory is said to be retrieved when |E(x(L)) E(x(L1))| < for some small > 0,at which point x(L1) x(L) =: x is declared to be the retrieved memory after L iterations becausex lives at a local minimum of the energy function E.",
  "Quantifying retrieval errorGiven the same initial queries x(0) {0,1": "D}D, we want to quantifythe difference between the fixed points x retrieved by descending DrDAMs approximate energy andthe fixed points x retrieved by descending the energy of MrDAM. We follow the experimental setupof 4.1, only this time we run full memory retrieval dynamics until convergence. Note that since energy uses an L2-similarity kernel, memory retrieval is not guaranteed to returnbinary values. Thus, we binarize x by assigning each entry to its nearest binary value beforecomputing the normalized Hamming approximation error H, i.e.,",
  "D)": "A shows the results of this experiment. Many observations from 4.1 translate to theseexperiments: we notice that retrieval is random at high and D, and that retrievals are of generallyhigher accuracy nearer the stored patterns. However, we notice that high values can retrieve betterapproximations than lower values of when the queries are at or near stored patterns. Additionally,for sufficiently high (e.g., see D = 1000, = 50 near stored patterns), this accompanies aninteresting thresholding behavior for Y where retrieval error starts to improve rapidly once Yreaches a minimal threshold. This behavior is corroborated in the high D regime in fig. 4B. Visualizing retrieval errorB shows what retrieval errors look like qualitatively. We storedK = 10 random images from CIFAR10 into the memory matrix of MrDAM, resulting in patternsof size D = 3 32 32 = 3072, and compared retrievals using s that produced meaningful imageresults with MrDAM. To keep values consistent with our previous experiments, each pixel wasnormalized to the continuous range between 0 and1",
  "D], with K and i D": "From 4.1 and fig. 4A, we know that approximate retrievals are inaccurate at high and high D ifthe query is far from the stored patterns. However, this is exactly the regime we test when retrievingimages in fig. 4B. The visible pixels (top half of the image) are clamped while running the dynamicsuntil convergence. Retrieved memories at different configurations for DrDAM are plotted against theircorresponding MrDAM retrievals in fig. 4B. As increases, insufficiently large values of Y fail to retrieve meaningful approximations to thedynamics of MrDAM. We observe that image completions generally become less noisy as Y increases,but with diminishing improvement in perceptible quality after some threshold where DrDAM goesfrom predicting noise to predicting meaningful image completions.",
  "Conclusion": "Our study is explicitly designed to characterize where DrDAM is a good approximation to the energiesand dynamics of MrDAM. In pushing the limits of the distributed representation, we discovered thatDrDAM is most accurate when: (1) query patterns are nearer to the stored patterns; (2) is lower; and(3) Y is large. Error bounds for these situations are explicitly derived in theorem 2 and empiricallytested in 4. We have explored the use of distributed representations via random feature maps in DenseAMs. Wehave demonstrated how this can be done efficiently, and we precisely characterized how it performs theneural dynamics relative to the memory representation DenseAMs. Our theoretical results highlightthe factors playing a role in the approximation introduced by the distributed representations, andour experiments validate these theoretical insights. As future work, we intend to explore how suchdistributed representations can be leveraged in hierarchical associative memory networks ,which can have useful inductive biases (e.g., convolutions, attention), and allow extensions withmultiple hidden layers.",
  "Karsten M Borgwardt, Cheng Soon Ong, Stefan Schnauer, SVN Vishwanathan, Alex JSmola, and Hans-Peter Kriegel.Protein function prediction via graph kernels.Bioin-formatics, 21(suppl_1):i47i56, 2005.URL": "K-R Mller, Alexander J Smola, Gunnar Rtsch, Bernhard Schlkopf, Jens Kohlmorgen, andVladimir Vapnik. Predicting time series with support vector machines. In Internationalconference on artificial neural networks, pages 9991004. Springer, 1997.URL Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane,Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinkingattention with performers. Proceedings of ICLR, 2020. URL Deepali Jain, Krzysztof Marcin Choromanski, Kumar Avinava Dubey, Sumeet Singh, VikasSindhwani, Tingnan Zhang, and Jie Tan. Mnemosyne: Learning to train transformers withtransformers. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.URL Benjamin Hoover, Yuchen Liang, Bao Pham, Rameswar Panda, Hendrik Strobelt, Duen HorngChau, Mohammed Zaki, and Dmitry Krotov. Energy transformer. Advances in Neural Infor-mation Processing Systems, 36, 2024. URL",
  "Hamza Chaudhry, Jacob Zavatone-Veth, Dmitry Krotov, and Cengiz Pehlevan.Longsequence hopfield memory.Advances in Neural Information Processing Systems, 36,2024.URL": "Dennis Wu, Jerry Yao-Chieh Hu, Weijian Li, Bo-Yu Chen, and Han Liu. STanhop: Sparse tan-dem hopfield model for memory-enhanced time series prediction. In The Twelfth InternationalConference on Learning Representations, 2024. URL Jerry Yao-Chieh Hu, Thomas Lin, Zhao Song, and Han Liu. On computational limits of modernhopfield models: A fine-grained complexity analysis. In Proceedings of the 41st InternationalConference on Machine Learning, volume 235 of Proceedings of Machine Learning Research,pages 1932719343. PMLR, 2127 Jul 2024. URL",
  "Dennis Wu, Jerry Yao-Chieh Hu, Teng-Yun Hsiao, and Han Liu. Uniform memory retrievalwith larger capacity for modern hopfield models. arXiv preprint arXiv:2404.03900, 2024. URL": "Matteo Negri, Clarissa Lauditi, Gabriele Perugini, Carlo Lucibello, and Enrico Maria Malatesta.Random feature hopfield networks generalize retrieval to previously unseen examples. InAssociative Memory & Hopfield Networks in 2023, 2023. Matteo Negri, Clarissa Lauditi, Gabriele Perugini, Carlo Lucibello, and Enrico Malatesta.Storage and learning phase transitions in the random-features hopfield model. Physical ReviewLetters, 131(25):257301, 2023.",
  "Bernard W Silverman.Density estimation for statistics and data analysis.Chapman &Hall/CRC, 1998": "Bishwajit Saha, Dmitry Krotov, Mohammed J Zaki, and Parikshit Ram. End-to-end differen-tiable clustering with associative memories. In Proceedings of the 40th International Confer-ence on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages2964929670. PMLR, 2329 Jul 2023. URL Rylan Schaeffer, Nika Zahedi, Mikail Khona, Dhruv Pai, Sang Truong, Yilun Du, MitchellOstrow, Sarthak Chandra, Andres Carranza, Ila Rani Fiete, Andrey Gromov, and Sanmi Koyejo.Bridging associative memory and probabilistic modeling, 2024. URL",
  "Peter W Frey and David J Slate. Letter recognition using holland-style adaptive classifiers.Machine learning, 6:161182, 1991": "James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, DougalMaclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, andQiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL Yingcong Li, Muhammed Emrullah Ildiz, Dimitris Papailiopoulos, and Samet Oymak. Trans-formers as algorithms: Generalization and stability in in-context learning. In Andreas Krause,Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett,editors, Proceedings of the 40th International Conference on Machine Learning, volume 202of Proceedings of Machine Learning Research, pages 1956519594. PMLR, 2329 Jul 2023.URL",
  "ALimitations": "In this paper, we have explored the use of distributed representations via random feature maps inDenseAMs. However, we are only scratching the surface of opportunities that such distributedrepresentations bring to DenseAMs. There are various aspects we do not cover: (i) We do not coverthe ability of these distributed representations to provide (probably lossy) compression. (ii) We donot study the properties of DrDAM relative to MrDAM when DrDAM is allowed to have different stepsizes and number of layers than MrDAM. A further limitation of our work is the limited number ofdatasets on which we have characterized the performance of DrDAM.",
  "DrDAM": "4.1 validated eq. (13), confirming that approximation error decreases as the number of randomfeatures Y increases under constant number of stored patterns K. We can also consider a relatedbut different question: under constant number of random features Y , how does approximation errorbehave when increasing the number of stored patterns K? Intuitively, DrDAMs approximation shouldbe good when a small number of patterns are stored in the network, and this approximation shouldworsen as we increase the number of stored patterns. validates this intuition empirically, with the caveat that random queries generally improvein accuracy because the probability of being near a stored patterns (a regime that generally leads tohigher accuracy of retrievals, see 4) increases as we store more patterns into the network. For thisexperiment, Y = 2e5 was held constant across all experiments and each plotted approximation erroris averaged over a number of queries equal to the number of stored patterns K. The experimentaldesign otherwise exactly replicates that of fig. 3. : Mean Approximation Error (MAE, eq. (14)) increases as the number of stored patterns Kincreases (except at random starting positions, where more stored patterns increases the probabilitythat a random query is closer to a memory, a regime that leads to higher accuracy of the retrievals,see fig. 3), keeping Y = 2e5 constant across all experiments.",
  "where N(0, ID), Y are the random projection vectors and b U(0, 2) are randombiases or shifts in the basis function": "shows how well the above basis functions approximated the true energy and energy gradientat different values for and size of feature dimension Y . Specifically, given the Letter dataset which consists of 16-dimensional continuous vectors whose values were normalized to be between[0,1 D], we randomly selected 900 unique data points, storing 500 patterns into the memory andchoosing the remaining 400 to serve as new patterns. We then compared how well the energy andenergy gradients of the chosen basis function approximates the predictions of the original DAM. We observe that the trigonometric basis functions (i.e., either Cos or SinCos) provide the mostaccurate approximations for the energy and gradients of the standard MrDAM, especially in the regimeof high which is required for the high memory storage capacity of DenseAMs. Surprisingly, thePositive Random Features (PRFs) of do not perform well in the dense (high ) regime; in general,trigonometric features always provide better approximations than the PRFs. We conclude that the SinCos basis function is the best approximation for use in the experimentsreported in the main paper, as this choice consistently produces the best approximations for the energygradients across all values of .",
  "DTinyImagenet Experimental Details": "In performing the qualitative reconstructions shown in fig. 1, we used a standard MrDAM energy(eq. (7)) configured with inverse temperature = 60. We approximated this energy in a DrDAMusing the trigonometric SinCos basis function shown in eq. (8) configured with feature dimensionY = 1.8e5. The four images shown were selected from the Tiny Imagenet dataset, rasterizedinto a vector, and stored in the memory matrix a MrDAM, resulting in a memory of shape (4, 12288).Energy descent for both MrDAM and DrDAM used standard gradient descent at a step size of 0.1 untilthe dynamics of all images converged (for fig. 1 after 300 steps, see energy traces). Visible pixels areclamped throughout the duration of the dynamics by zeroing out the energy gradients on the visibletop one-third of the image. : Trigonometric basis functions significantly outperform Positive Random Features, especiallyin the regime of large . We end up choosing the SinCos function to analyze in the main paper,as this choice of basis function always produced the best approximations to the energy gradient.Experiments performed on the 16-dimensional Letters dataset . In MrDAM, the memory matrix necessarily grows linearly when storing new patterns . However,the distributed memory tensor T of DrDAM does not grow when new patterns are stored. This meansit is possible to compress the memories into a smaller tensor T where Y < NumPixels, providedthat we operate in a regime that allows larger approximation errors in the retrieval and smaller initialocclusions. shows a variation of the setting of fig. 1 where stored patterns are actuallycompressed into DrDAMs memory tensor, successfully storing 20 12288 pixels from a distributedtensor of size Y = 2e5 and retrieving the memories with 40% initial occlusion of the queries, a20% reduction in the number of parameters compared to MrDAM. All other hyperparameters are thesame as was used to generate fig. 1, and convergence on all images occurs after 1000 steps.",
  "EDetails on Computational Environment for the Experiments": "All experiments are performed on a single L40s GPU equipped with 46GB VRAM. Experimentswere written and performed using the JAX library for tensor manipulations. Unless otherwisenoted, gradient computation was performed using JAXs powerful autograd mechanism. Experimen-tal code with instructions to replicate the results in this paper are made available at this GitHub repos-itory ( complete with instructions to setup thecoding environment and run all experiments.",
  "F.1.2Comparing computational complexities of MrDAM and DrDAM": "Note that, comparing the computational complexities of MrDAM in proposition 1 to that of DrDAMin theorem 1 does not directly provide any computational improvements as it would depend on thechoices of D, K, L, Y . The main point of these results is to highlight, that once the memories areprocessed via ProcMems, the energy descent with DrDAM requires computation and memory thatonly depends on D and Y . And together with theorem 2 and corollary 1, we characterize situationswhere the energy descent divergence between MrDAM and DrDAM can be bounded with a choice of Ythat only depends on D (and other parameters in the energy function) but not K. While we do not claim or highlight computational gains over MrDAM, note that the peak memorycomplexity of MrDAM is O(KD) compared to O(Y + D) for DrDAM. Given that in the interestingregime of Y O(D/2) which upperbounds the energy descent divergence between DrDAM andMrDAM in corollary 1 to at most some > 0, DrDAM is more memory efficient than MrDAM if thenumber of memories K > C/2 for some sufficiently large positive constant C. Ignoring the timerequired to encode the memories into the distributed representation in DrDAM using ProcMems, theruntime complexities are O(LKD) for MrDAM compared to O(LD(Y + D)) for DrDAM. Again,considering the interesting regime of Y O(D/2), DrDAM will be computationally more efficientthan MrDAM if the number of memories K > CD/2 for some sufficiently large positive constant C.",
  ",implying that exp(E(x)) exp(/2)/K, and we can replace this in the upper bound and removethe dependence on E(x)": "However, an important aspect of our analysis is that the bound is input specific, and depends on theinitial energy E(x). As discussed above, this can be upper bounded uniformly, but our bound is moreadaptive to the input x. For example, if the input is initialized near one of the memories, while being sufficiently far fromthe remaining (K 1) memories, then exp(E(x)) term can be relatively small. More precisely,with all memories and queries lying in a ball of diameter 1, let the query be at a distance r < 1 toits closest memories, and as far as possible from the remaining (K 1) memories. In this case, theinitial energy E(x) (1/) log(exp(r/2) + (K 1) exp(/2)), implying that"
}