{
  "Abstract": "Energy-based models (EBMs) offer a flexible framework for probabilistic mod-elling across various data domains. However, training EBMs on data in discreteor mixed state spaces poses significant challenges due to the lack of robust andfast sampling methods. In this work, we propose to train discrete EBMs withEnergy Discrepancy, a loss function which only requires the evaluation of theenergy function at data points and their perturbed counterparts, thus eliminatingthe need for Markov chain Monte Carlo. We introduce perturbations of the datadistribution by simulating a diffusion process on the discrete state space endowedwith a graph structure. This allows us to inform the choice of perturbation from thestructure of the modelled discrete variable, while the continuous time parameterenables fine-grained control of the perturbation. Empirically, we demonstrate theefficacy of the proposed approaches in a wide range of applications, including theestimation of discrete densities with non-binary vocabulary and binary image mod-elling. Finally, we train EBMs on tabular data sets with applications in syntheticdata generation and calibrated classification.",
  "Introduction": "Discrete structures are intrinsic to most types of data such as text, graphs, and images. Estimating thedata generating distribution pdata of discrete data sets with a probabilistic model can contribute greatlyto downstream inference and generation tasks, and plays a key role in synthetic data generation oftabular, textual or network data (Raghunathan, 2021). Energy-based models (EBMs) are probabilisticgenerative models of the form pebm exp(U), where the flexible choice of the energy function Uallows great control in the modelling of different data structures. However, energy-based models are,by definition, unnormalised models and notoriously difficult to train due to the intractability of theirnormalisation, especially in discrete spaces. Energy-based models are typically trained with the contrastive divergence (CD) algorithm (Hinton,2002) which performs approximate maximum likelihood estimation by approximating the gradient ofthe log-likelihood with Markov Chain Monte Carlo (MCMC) techniques. This method motivatedrich research results on sampling from discrete distributions to enable fast and accurate estimation ofenergy-based models (Zanella, 2020; Grathwohl et al., 2021; Zhang et al., 2022b; Sun et al., 2022b,a,2023a). However, the training of energy-based models with CD remains challenging as it relies on",
  "arXiv:2412.01019v1 [stat.ML] 2 Dec 2024": "sufficiently fast mixing of Markov chains. Since accurate sampling from the EBM typically cannotbe achieved, contrastive divergence lacks theoretical guarantees (Carreira-Perpinan & Hinton, 2005)and leads to biased estimates of the energy landscape (Nijkamp et al., 2019). For mixed data types,energy-based models have only been applied to combinations of numerical features with a singlecategorical label (Grathwohl et al., 2020). The recently introduced Energy Discrepancy (ED) (Schrder et al., 2023) is a new type of contrastiveloss functional that, by definition, depends on neither gradients nor MCMC methods. Instead, thedefinition of ED only requires the evaluation of the energy function on positive and contrasting,negative samples which are generated by perturbing the data distribution. However, the work inSchrder et al. (2023) is currently limited to Gaussian perturbations on continuous spaces and doesnot explores strategies to choose perturbations on discrete spaces, especially when these discretespaces exhibit some additional structure. In this work, we propose a framework to train energy-based models with energy discrepancy ondiscrete data, making the following contributions: 1) We explore a method to define discrete diffusionprocesses on structured discrete spaces through a heat equation on the underlying graph and investigatethe effect of geometry and time parameter on the diffusion. 2) Based on the discrete diffusion process,we extend energy discrepancy to discrete spaces in a systematic way, thus introducing a MCMC-free method for the training of energy-based models that requires little tuning. 3) We extend ourmethodology to mixed state spaces and establish to the best of our knowledge the first robust trainingmethod of energy-based models on tabular data sets. We demonstrate promising performance ondownstream tasks like synthetic data generation and calibrated prediction, thus unlocking a new toolfor generative modelling on tabular data.",
  "xX exp(U(x)),(1)": "where U is the energy function parameterised by and Z denotes the normalisation constant.Given a set of i.i.d. samples {xi}Ni=1 from an unknown data distribution pdata(x) we aim to learn anapproximation p(x) of pdata(x). The de facto standard approach for finding such is to minimisethe negative log-likelihood of p under the data distribution via gradient decent",
  "Epdata(x)[log p(x)] = Expdata[U(x)] Exp[U(x)].(2)": "The intuition behind this update is to decrease the energy of positive data samples x pdata(x)and to increase the energy of negative samples x p(x). However, the exact computation ofgradient in (2) is known to be NP-hard in general (Jerrum & Sinclair, 1993) and quickly becomesprohibitive even on relatively simple data sets. Consequently, existing approaches resort to samplingfrom the model p to approximate the gradient of log-likelihood via Monte Carlo estimation. Indiscrete settings, the most popular sampling methods include the locally informed sampler (Zanella,2020), Gibbs with gradients (GwG) (Grathwohl et al., 2021), discrete Langevin (Zhang et al., 2022b),and generative flow networks (GFlowNet) (Zhang et al., 2022a). Despite their established successin discrete energy-based modelling, these methods necessitate a trade-off that hampers scalability:running the sampler for an extended duration rapidly increases the cost of maximum likelihoodtraining, while shorter sampler runs yield inaccurate approximations of the likelihood gradient andintroduce biases into the learned energy. Energy Discrepancy (Schrder et al., 2023) is a recently proposed method to train energy-basedmodels without the need for an extensive sampling process. Instead, it constructs negative samplesby perturbing the data, thus bypassing the sampling step while still yielding a valid training objective.To elucidate, the energy discrepancy is formally defined as follows:",
  "EDq(pdata, U) := Epdata(x)[U(x)] Epdata(x)Eq(y|x)[Uq(y)].(4)": "We will refer to q as the perturbation. The validity of this loss functional was proven in Schrderet al. (2023) in large generality: In particular, it is sufficient for U = argmin EDq(pdata, U) exp(U ) pdata that any two points x, y X are q-equivalent, i.e. there exists a chain of states(zi)Ti=1 X with z1 = x, zT = y such that q(zi+1|zi) > 0 for all i = 1, . . . , T 1. Energy discrepancy can also be understood from seeing it as a type of Kullback-Leibler divergence.Specifically, the loss function defined in (4) is equivalent to the expected Kullback-Leibler divergence",
  "Energy Discrepancies for Discrete Data": "For this work we will first consider a state space for the data distribution that can be written as theproduct of d discrete variables with Sk classes each, i.e. X = dk=1{1, . . . , Sk}. Examples forspaces of this type are the categorical entries of a data table for which d denotes the number of features,or binary image data sets for which we typically write X = {0, 1}d. To define energy discrepancy onsuch spaces we need to specify a perturbation process under the following considerations: 1) Thenegative samples obtained through q are informative for training the EBM when only finite amountsof data are available. 2) The contrastive potential Uq(y) has a numerically tractable approximation. Let us consider one component X = {1, . . . , S}, only. Inspired from previous works on diffusionmodelling for discrete data (Campbell et al., 2022; Sun et al., 2023b; Lou et al., 2024; Campbellet al., 2024) we model the perturbation as a continuous time Markov chain (CTMC) with transitionprobabilityqt(y = b|x = a) = exp (tR)ba ,a, b {1, 2, . . . , S} where R RSS is the so-called rate matrix which satisfies Rbb = Sa=b Rba and exp(tR) is thematrix exponential. For a given rate matrix R, this approach then leaves us with a single tunablehyperparameter t characterising the magnitude of perturbation applied. We first analyse how thechoice of rate matrix and time parameter affect the statistical properties of the energy discrepancyloss. In fact, under weak conditions, the energy discrepancy loss converges to maximum likelihoodestimation for t , thus achieving the same loss function implemented by contrastive divergence:Theorem 1. Let qt(|x) be a Markov transition density defined by the rate matrix R with eigenvalues0 = 1(R) 2(R) S(R) and uniform stationary distribution. Then, there exists aconstant zt independent of such that energy-discrepancy converges to the maximum-likelihood loss",
  "with the loss of maximum-likelihood estimation LMLE() := Epdata(x)log p(x)": "Here, zt is a constant independent of , so the optimisation landscapes of energy discrepancyestimation and maximum likelihood estimation in align at an exponential rate, except for a shiftby zt which does not affect the optimisation. This result improves the linear convergence rate inSchrder et al. (2023) and relates it to the spectral gap |2(R)| of the rate matrix. Such a resultis meaningful as the maximum-likelihood estimator is generally statistically preferable with bettersample efficiency, and Theorem 1 suggests that energy discrepancy estimation can approximatemaximum likelihood estimation without resorting to MCMC like in classical EBM training methods.The proof is given in Appendix A.1. 4On discrete spaces dx is assumed to be a counting measure. On continuous spaces X, the appearing sumsand expectations turn into integrals with respect to the Lebesgue measure5With a slight abuse of notations, we represent the contrastive potential induced by distribution q as Uq anddenote the energy function as U with or without the subscript .",
  "Heat Equation in Structured Discrete Spaces": "In principle, Theorem 1 establishes that energy discrepancy converges to the loss of maximumlikelihood estimation in the limit t for any choice of rate matrix with spectral gap. In practice,however, large perturbations of data can produce high-variance parameter gradients and provide littletraining signal. Instead, it is desirable to construct perturbations that allow a fine-grained trade-offbetween the statistical properties of the loss function and the variance of the gradients. For this reason,we investigate the perturbation for small t which, as we will see, can be informed by the assumedgraph structure of the underlying discrete space.",
  "qt(y = b|x = a) (b, a) + tRba": "with (b, a) = 1 a = b and zero otherwise. To model the relationship between values a, b {1, . . . , S} we endow the space with a graph structure with adjacency matrix A and out degree matrixDout and model the rate matrix as the graph Laplacian R := (A Dout). By definition, the rowsof the graph Laplacian matrix sum to zero Sa=1 Rba = 0. The smallest possible perturbation isthen characterised as the transition to an adjacent neighbour. The characterisation of the CTMC interms of the graph Laplacian is implicitly assumed in previous work. Campbell et al. (2022) describea diffusion via a uniform perturbation which corresponds to a fully connected graph and Lou et al.(2024) describe the rate matrix associated to a star graph with absorbing (masking) state:",
  "Estimating the Energy Discrepancy Loss": "We now discuss how discrete energy discrepancy can be estimated. We will typically assume thateach dimension of the data point is perturbed independently, i.e. the perturbation q(y|x) is modelledas the product of component-wise perturbations. On Euclidean data, we resort to the implementationin Schrder et al. (2023) and obtain perturbed samples by adding isotropic Gaussian noise to thesamples. We are now left with the heat equation on discrete space.",
  "k=1(y, k) .(7)": "Practically speaking, this perturbation remains in its state with probability et and samples uniformlyfrom the state space otherwise. The case of the cyclical and ordinal structure is more delicate.We first note that the heat equation can be solved in terms of its eigenvalue expansion exp(Rt) =V exp(t)V, where is the matrix with the eigenvalues p along its diagonal and V is a matrixof orthogonal eigenvectors with each column containing the corresponding eigenvector vp. Theperturbation for Rcyc and Rord can then be computed by means of a discrete Fourier transform:",
  "where cycp= (p 1)/S and ordp= (p 1)/2S, respectively, and zp = (2, 1, . . . , 1)": "For the derivation, see Appendix A.2. Due to this result, the heat equation can be efficiently solvedin parallel without requiring any sequential operations like multiple Euler steps. In addition, thetransition matrices can be computed and saved in advance, thus reducing the computational complexityto the matrix multiplication with a batch of one-hot encoded data points. Gaussian limit and choice of time parameter.For tabular data sets the cardinality S changesbetween different dimensions which raises the question how t should be scaled with S. To answerthis question we observe the following scaling limit of the perturbation: Theorem 2 (Scaling limit). Let yt qt(|x = S) with {1/S, 2/S, . . . , 1} where qt is eitherthe transition density of the cyclical or ordinal perturbation. Let : R (0, 1], where for all n Zand x (0, 1] cyc(n + x) = x and ord(2n + x) = x, ord(2n + 1 + x) = x. Then,",
  "yS2t/SS (t)witht N(, 2t)": "Consequently, under the rescaling of time and space prescribed, the perturbation behaves indepen-dently of the state space size like a Gaussian with variance 2t that is reflected or periodically continuedat the boundary states of (0, 1]. The phenomenon is visualised in . Based on this scalinglimit we typically choose a quadratic rule t = S2tbase. Alternatively, we may choose a linear rulet = Stbase in which case the limit becomes a regular Gaussian on R+, thus recovering the Euclideancase from Schrder et al. (2023). The theorem is proven in Appendix A.3. As a byproduct of this result we can also approximate the perturbation with discretised rescaledsamples from a standard normal distribution and applying either periodic or reflecting mappings onperturbed states outside the domain. This may be computationally favourable for spaces of the form{1, . . . , S}d where the vocabulary size S and dimension of the state space d grow very large. Localisation to random grid.For unstructured categorical variables the uniform perturbation mayintroduce too much noise to inform the EBM about the correlations in the data set. In these cases, itcan be beneficial to sample a random dimension k {1, . . . , d} and apply a larger perturbation inthis dimension, only. This effectively means to replace the product of categorical distributions with amixture perturbation",
  "k=1qt(yk|xk)": "In our experiments we only consider the case of perturbing the randomly chosen dimension uniformly.We call this grid perturbation due to connections with concrete score matching (Meng et al., 2022).The resulting loss can be understood as a variation of pseudo-likelihood estimation. Special case of binary state space.In the special case of X = {0, 1}d, the structures of the cyclical,ordinal, and uniform graph coincide, and the perturbation qt(y|x) becomes the product of identicalBernoulli distributions with parameter = 0.5(1 et). We also explore the grid perturbation whichassumes that a dimension is selected at random and the entry is flipped deterministically from zero toone or one to zero. For details, see Appendix B.1.",
  "Estimation of the Contrastive Potential": "The final challenge in turning energy discrepancy into a practical loss function lies in the estimationof the contrastive potential Uq. We use the fact that for a symmetric rate matrix R, the inducedperturbation is symmetric as well, i.e. qt(y|x) = qt(x|y). Thus, we first write the contrastive poten-tial as an expectation Uq(y) = log xX exp(U(x))q(y|x) = log Eq(x|y) [exp(U(x))]and subsequently approximate the energy discrepancy loss as in Schrder et al. (2023) asLq,M,w(U) :=1NNi=1 logw + Mj=1 exp(U(xi) U(xi,j )with xi pdata, yi qt(|xi),",
  "Related Work": "Contrastive loss functions. Our work is based on energy discrepancies first introduced in (Schrderet al., 2023). Energy discrepancies are equivalent to certain types of KL contraction divergenceswhose theory was studied in Lyu (2011), however, without proposing a training algorithm for EBMs.On Euclidean data, ED is related to diffusion recovery-likelihood (Gao et al., 2021) which usesa CD-type training algorithm. For a masking perturbation, ED estimation can be understood as aMonte-Carlo approximation of pseudo-likelihood (Besag, 1975). Furthermore, the structure of thestabilised energy discrepancy loss shares similarities with other contrastive losses such as Ceylan &Gutmann (2018); Gutmann & Hyvrinen (2010); Oord et al. (2018); Foster et al. (2020) due to theirclose connection to the Kullback-Leibler divergence. Discrete diffusion models. We extend the continuous time Markov chain framework introduced anddeveloped in Campbell et al. (2022, 2024); Lou et al. (2024) and provides a geometric interpretationthereof. Similar to us, Kotelnikov et al. (2023) defines a flow on mixed state spaces as the product ofa Gaussian and a categorical flow, utilising multinomial flows (Hoogeboom et al., 2021). Our workhas connections to concrete score matching (Meng et al., 2022) through the usage of neighbourhoodstructures to define a replacement of the continuous score function. Contrastive divergence and sampling. Contrastive divergence (CD) is commonly utilised fortraining energy-based models in continuous spaces with Langevin dynamics (Xie et al., 2016, 2018,2022; Du et al., 2021; Xiao et al., 2020). In discrete spaces, EBM training heavily relies on CDmethods as well, which is a major driver for the development of discrete sampling strategies. Thestandard Gibbs method was improved by Zanella (2020) through locally informed proposals. Thismethod was extended to include gradient information (Grathwohl et al., 2021) to drastically reducethe computational complexity of flipping bits in several places (Sun et al., 2022b; Emami et al., 2023;Sun et al., 2022a). Moreover, a discrete version of Langevin sampling was introduced based on thisidea (Zhang et al., 2022b; Rhodes & Gutmann, 2022; Sun et al., 2023a). Consequently, most currentimplementations of contrastive divergence use multiple steps of a gradient-based discrete sampler.Alternatively, EBMs can be trained using generative flow networks which learn a Markov chain thatconstruct data optimising the energy as reward function (Zhang et al., 2022a).",
  ": Comparison of energy discrepancy and contrastive divergence on the dataset with 16 dimen-sions and 5 states. Rows 1 and 2 show the estimated density and synthesised samples, respectively": "Other training methods and applications of EBMs for discrete and mixed data. A sampling-freeapproach for training binary discrete EBMs is ratio matching (Hyvrinen, 2007; Lyu, 2009; Liu et al.,2023). Dai et al. (2020) propose to apply variational approaches to train discrete EBMs instead ofMCMC. Eikema et al. (2022) replace the widely-used Gibbs algorithms with quasi-rejection samplingto trade off the efficiency and accuracy of the sampling procedure. The perturb-and-map (Papandreou& Yuille, 2011) is also recently utilised to sample and learn in discrete EBMs (Lazaro-Gredilla et al.,2021). Tran et al. (2011) introduce mixed-variate restricted Boltzmann machines for energy-basedmodelling on mixed state spaces. Deep architectures, on the other hand, have been mostly limitedto a single categorical target variable which is modelled via a classifier (Grathwohl et al., 2020).Moreover, Ou et al. (2022) apply discrete EBMs on set function learning, in which the discrete energyfunction is learned with approximate marginal inference (Domke, 2013).",
  "Discrete Density Estimation": "We first demonstrate the effectiveness of energy discrepancy on density estimation using syntheticdiscrete data. Following Dai et al. (2020), we initially generate 2D floating-point data from severaltwo-dimensional distributions. Each dimension of the data is then converted into a 16-bit Gray code,resulting in a dataset with 32 dimensions and 2 states. To construct datasets beyond binary cases, wefollow Zhang et al. (2024) and transform each dimension into 8-bit 5-base code and 6-bit decimalcode. This process creates two additional datasets: one with 16 dimensions and 5 states, and anotherwith 12 dimensions and 10 states. The experimental details are given in Appendix D.1. illustrates the estimated energies as well as samples that are synthesised with Gibbs samplingfor energy discrepancy (ED) and contrastive divergence (CD) on the dataset with 16 dimensionsand 5 states. It can be seen that ED excels at capturing the multi-modal nature of the distribution,consistently learning sharper energy landscape in the data support compared to CD. This coincideswith the previous observations in continuous spaces (Schrder et al., 2023), suggesting EDs advantagein handling complex data structures. For more results of additional datasets with 5 and 10 states, wedeferred them to Figures 7 and 8, respectively. For binary cases with 2 states, we compare our approaches to three baselines: PCD (Tieleman, 2008),ALOE+ (Dai et al., 2020), and EB-GFN (Zhang et al., 2022a). In Tables 3 and 4, we quantitativelyevaluate different methods by evaluating the negative log-likelihood (NLL) and the exponentialHamming MMD (Gretton et al., 2012), respectively. We observe that energy discrepancy outperformsthe baseline methods in most settings, but without relying on MCMC simulations (as in PCD) or thetraining of additional variational networks (as in ALOE and EB-GFN). This performance gain islikely explained by the good theoretical guarantees of energy discrepancy for well-posed estimationtasks. In contrast, the baselines introduce biases due to their reliance on variational proposals andshort-run MCMC sampling that may not have converged.",
  "Tabular Data Synthesising": "In this experiment, we assess our methods on synthesising tabular data, which presents a challengedue to its mix of numerical and categorical features, making it more difficult to model comparedto conventional data formats. To demonstrate the efficacy of energy discrepancies, we first conductexperiments on synthetic examples before proceeding to real-world tabular data. Additional detailsregarding the experimental setup are deferred to Appendix D.2.",
  ": Comparison of the energy dis-crepancy and contrastive divergence onthe synthetic tabular datasets": "Synthetic Dataset.We first showcase the effectivenessof our methods on mixed data types by learning EBMson a synthetic ring dataset. The dataset consists of fourcolumns, with the first two columns indicating numericalcoordinates of data points. The third column categorizesdata points into four circles whereas the last column speci-fies the 16 colours each data point could be classified into.Therefore, each row in the tabular contains 2 numericalfeatures and 2 categorical features. To train an EBM on a dataset comprising mixed types of data, we employ either contrastive divergenceor energy discrepancy. For CD, we adopt a strategy involving a replay buffer in conjunction with ashort-run MCMC using 20 steps. Specifically, we utilise Langevin dynamics and Gibbs samplingfor numerical and categorical features, respectively. In the case of ED, we perturb the numericalfeatures with a Gaussian perturbation and the categorical features with grid perturbation. illustrates the results of synthesised samples generated from the learned energy using Gibbs sampling.These findings align with those depicted in , where CD struggles to capture a faithful energylandscape, leading to synthesized samples potentially lying outside the data distribution support.Instead, by leveraging a combination of perturbation techniques tailored to the data types present, EDoffers a more robust and reliable framework for training EBMs in mixed state spaces. Real-world Dataset.We then evaluate our methods by benchmarking them against various baselinesacross 6 real-world datasets. Following Xu et al. (2019), we first split the real datasets into trainingand testing sets. The generative models are then learned on the real training set, from whichsynthetic samples of equal size are generated. This synthetic dataset is subsequently used to train aclassification/regression XGBoost model, which is evaluated using the real test set. We compare the performance, as measured by the AUC score for classification tasks and RMSE forregression tasks, against CTGAN, TVAE, (Xu et al., 2019) and TabDDPM (Kotelnikov et al., 2023)baselines which utilise generative adversarial networks, variational autoencoders, and denoisingdiffusion probabilistic models, respectively. The results are reported in . Here, TabED-Strrefers to an ED loss for which the perturbation was chosen with prior knowledge about the structureof the modelled feature, i.e. ordinal and cyclical features were hand-picked. We do not report resultsfor TabED-Str on the Cardio, Churn, and Mushroom datasets, since the state spaces only consist : Experimental results for discrete image modelling. We report the negative log-likelihood(NLL) on the test set for different models. The results of Gibbs, GWG, and DULA are taken fromZhang et al. (2022b), and the result of EB-GFN is from Zhang et al. (2022a).",
  "of unstructured features. To compute the average ranking we use the rank of TabED-Uni on thesedatasets since on unstructured features TabED-Uni and TabED-Str coincide": "The variants of ED show promising results on diverse datasets, thus demonstrating the suitability of EDfor EBM training on mixed-state spaces. While TabDDPM outperforms the other approaches, TabEDshows comparable performance to the CTGAN and TVAE baselines and outperforms both in averageranking. Furthermore, the contrastive divergence approach performs poorly which highlights itslimitations in accurately modelling distributions on mixed state spaces. Surprisingly, the unstructuredperturbation TabED-Uni performs slightly better than the structured approaches. This may partiallybe attributed to the fact that the state spaces of the discrete features are relatively small. Consequently,the uniform perturbation might be a good approximation of maximum likelihood estimation inagreement with Theorem 1, while not producing high-variance gradients on these specific datasets.",
  ": Calibration results comparison between the base-line (left) and energy discrepancy (right) on the adult dataset": "ImprovingCalibration.Despitethe improving accuracy of neural-network-based classifiers in recentyears, they are also becoming increas-ingly recognised for their tendency toexhibit poor calibration due to over-confident outputs (Guo et al., 2017;Mukhoti et al., 2020). Since energy-based model on mixed state spacescan capture the likelihood of tuplesof features and target labels, they im-plicitely quantify the confidence in aprediction and can be adapted into classifiers with better calibration than deterministic methods. Thisopens up a new avenue for applying EBMs in deterministic tabular data modelling methods. Let y and x be the target label and the rest features in the tabular data, an EBM U(x, y) learnedon the joint probability pdata(x, y) can be transformed into a deterministic classifier: pEBM(y|x) exp(U(x, y)). As a baseline for comparison, we additionally train a classifier pCLF(y|x) with thesame architecture by maximising the conditional likelihood: Epdata[log pCLF(y|x)]. Results on theadult dataset can be seen in . We find that the EBM and the baseline exhibit comparableaccuracy. However, the baseline model is less calibrated, generating over-confident predictions. Incontrast, the EBM learned through ED achieves better calibration, as evidenced by lower expectedcalibration error (Guo et al., 2017). Further details and results are provided in Appendix D.2.",
  "Discrete Image Modelling": "In this experiment, we evaluate our methods on high-dimensional binary spaces. Following thesettings in Grathwohl et al. (2021), we conduct experiments on various image datasets and compareagainst contrastive divergence using various sampling methods, namely vanilla Gibbs sampling,Gibbs-With-Gradient (Grathwohl et al., 2021, GWG), Generative-Flow-Network (Zhang et al., 2022a,GFN), and Discrete-Unadjusted-Langevin-Algorithm (Zhang et al., 2022b, DULA). The trainingdetails are provided in Appendix D.3. After training, annealed importance sampling (Neal, 2001) isemployed to estimate the negative log-likelihood (NLL). displays the NLLs on the test dataset. It is evident that energy discrepancy achieves com-parable performance to the baseline methods on the Omniglot dataset. Despite the performancegap compared to the contrastive divergence methods on the MNIST dataset, energy discrepancystands out for its efficiency, requiring only M evaluations of the energy function in parallel (see for the comparison of running time complexity). This represents a significant computationalreduction compared to contrastive divergence, which lacks the advantage of parallelisation andinvolves simulating multiple MCMC steps. Additionally, our methods show superiority over CD-1 bya substantial margin, as demonstrated in , affirming the effectiveness of our approach. Forfurther insights, we provide visualisations of the generated samples in .",
  "Conclusion and Limitations": "In this paper we extend the training of energy-based models with energy discrepancy to discrete andmixed state spaces in a systematic way. We show that the energy-based model can be learned jointlyon continuous and discrete variables and how prior assumptions about the geometry of the underlyingdiscrete space can be utilised in the construction of the loss. Our method achieves promising resultson a wide range of discrete modelling applications at a significantly lower computational cost thanMCMC-based approaches. To the best of our knowledge, our approach is also the first workingtraining method for energy-based models on tabular data sets, unlocking a wide range of inferenceapplications for tabular data sets beyond the scope of classical joint energy-based models. Limitations: Similar to prior work on energy discrepancy in continuous spaces (Schrder et al.,2023), our training method is sensitive to the assumption that the data distribution is positive onthe whole state space. While our method scales to high-dimensional datasets like binary imagedata, where the positiveness of the data distribution is assumed to be violated due to the manifoldhypothesis, the large difference between intrinsic and ambient dimensionality poses challenges toour approach and may explain why energy discrepancy cannot match the performance of contrastivedivergence with a large number of MCMC steps on binary image data.",
  "Broader Impact: In principle, our method can be used for imputation and prediction in tabular datasets and can thus have discriminating or excluding effects if used irresponsibly": "Outlook: For future work, we are interested in extensions to highly structured types of data such asmolecules, text, or data arising from networks. So far, our work only considers cyclical and ordinalstructures on the discrete space, while incorporating more complex structures as prior information intothe rate function may be beneficial. Furthermore, interesting downstream applications ranging fromtable imputation with confidence bounds, simulation-based inference involving discrete variables, orreweighting of language models with residual EBMs have been left unexplored in this work.",
  "Author Contributions": "TS and ZO conceived the project idea to use an ED loss for the training of EBMs on discrete andmixed data. TS devised the main conceptual ideas, developed the theory, conducted the proofs andimplemented the ED loss. ZO contributed to the conceptual ideas and designed and carried outthe experiments. ABD supervised the conceptualisation and execution of the research project andcontributed proof ideas. ZO, YL, and ABD checked derivations and proofs. TS and ZO equallycontributed to the writing under the supervision of YL and ABD.",
  "Besag, J. Statistical analysis of non-lattice data. Journal of the Royal Statistical Society. Series D(The Statistician), 24(3):179195, 1975": "Campbell, A., Benton, J., De Bortoli, V., Rainforth, T., Deligiannidis, G., and Doucet, A. Acontinuous time framework for discrete denoising models. Advances in Neural InformationProcessing Systems, 35:2826628279, 2022. Campbell, A., Yim, J., Barzilay, R., Rainforth, T., and Jaakkola, T. Generative flows on discretestate-spaces: Enabling multimodal flows with applications to protein co-design. InternationalConference on Machine Learning, 41, 2024.",
  "Carreira-Perpinan, M. A. and Hinton, G. On contrastive divergence learning. In Internationalworkshop on artificial intelligence and statistics, pp. 3340. PMLR, 2005": "Ceylan, C. and Gutmann, M. U. Conditional noise-contrastive estimation of unnormalised models. InProceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedingsof Machine Learning Research. PMLR, 2018. Dai, H., Singh, R., Dai, B., Sutton, C., and Schuurmans, D. Learning discrete energy-based modelsvia auxiliary-variable local exploration. Advances in Neural Information Processing Systems, 33:1044310455, 2020.",
  "Gao, R., Song, Y., Poole, B., Wu, Y. N., and Kingma, D. P. Learning energy-based models bydiffusion recovery likelihood. International Conference on Learning Representations, 2021": "Grathwohl, W., Wang, K.-C., Jacobsen, J.-H., Duvenaud, D., Norouzi, M., and Swersky, K. Yourclassifier is secretly an energy based model and you should treat it like one.InternationalConference on Learning Representations, 2020. Grathwohl, W., Swersky, K., Hashemi, M., Duvenaud, D., and Maddison, C. J. Oops I took a gradient:Scalable sampling for discrete distributions. In International Conference on Machine Learning,volume 38, 2021.",
  "Xie, J., Lu, Y., Zhu, S.-C., and Wu, Y. A theory of generative convnet. In International Conferenceon Machine Learning, pp. 26352644. PMLR, 2016": "Xie, J., Lu, Y., Gao, R., and Wu, Y. N. Cooperative learning of energy-based model and latent variablemodel via MCMC teaching. In Proceedings of the AAAI Conference on Artificial Intelligence,volume 32, 2018. Xie, J., Zhu, Y., Li, J., and Li, P. A tale of two flows: Cooperative learning of langevin flowand normalizing flow toward energy-based model. In International Conference on LearningRepresentations, 2022.",
  "A.2Eigenvalue Decomposition of Rate Matrices for Proposition 1": "The rate matrices for the cyclical and for the ordinal graph structures have a similar structureand are referred to as circulant and tridiagonal matrices. The easiest method for deriving theeigenvalue decompositions consists in deriving recurrence relations for the characteristic polynomial.A systematic study of block circulant matrices can be found in Tee (2007) and a study of tridiagonalmatrices was given in Losonczi (1992); Yueh (2005). These more general results may be helpfulwhen constructing perturbations for spaces with a more complex structure than the ones introducedin this work. We take already existing results and check that the desired results hold.",
  "A.3Proof of Scaling limit in Theorem 2": "It is a typical generalisation of the central limit theorem that random walks attain Brownian motionas a universal scaling limit. We reproduce similar arguments for the law of the continuous timeMarkov chain. Without loss of generality we shift the state space by one and consider the state space{0, 1, . . . , S 1} with cyclical and ordinal structure and let yt qt(|x = S), where we alwaysassume that the process is initialised at state S. Furthermore, we introduce the process zt whichis an unconstrained continuous time Markov chain on Z with rate matrix Raa = 2, Ra,a+1 =1, Ra,a1 = 1. The constrained process can then be described in terms of the unconstrained one:Let S : Z {0, 1, . . . , S 1} with S(2nS + p) = p and S((2n + 1)S + p) = p for forp {0, 1, . . . , S 1} and n Z. Then, S reflects the unconstrained process zt at the boundaries 0and S 1, i.e. yordt= S(zt) and ycyct= zt mod S. For the unconstrained process we define theholding times, i.e. the random time intervals in which the process does not change state",
  "ha = inft0{t : zt = x = a}(21)": "and the jump process Nt := ht (zh = zh), where zh = limsh zs which counts the numberof state transitions up to time t. It is a standard result that the holding times are exponentiallydistributed (Anderson, 2012, Proposition 2.8) ha Exp(Raa). Furthermore, since all holdingtimes are identically distributed and Raa = 2, the resulting jump process has Poisson distribution",
  "Nt Poisson(2t) .(22)": "With these definitions, we can now first proof the Gaussian limit of zt and then derive the limit of yt.Theorem 2 (Scaling limit). Let yt qt(|x = S) with {1/S, 2/S, . . . , 1} where qt is eitherthe transition density of the cyclical or ordinal perturbation. Let : R (0, 1], where for all n Zand x (0, 1] cyc(n + x) = x and ord(2n + x) = x, ord(2n + 1 + x) = x. Then,",
  "S2t": "where we used that cos(x) = 1/2(exp(ix) + exp(ix)) in the third step, the Poisson distribution ofNt in the fourth step, and the series expansion of the exponential function in the final step. Sincecos(x) 1 1/2x2, we now have point-wise for any s and due to the fact that s/S 0",
  "S(s)S exp(is ts2)(25)": "which is the characteristic function of a Gaussian with variance 2t and mean . This proves theconvergence in distribution of the rescaled unconstrained process zt. Furthermore, for ord and cycit holds S(zt)/S = (zt/S) for all S N. Furthermore, ord, cyc are continuous maps from Rto [0, 1) with reflecting or periodic boundary conditions. We thus have by the continuous mappingtheoremyS2t",
  "xXq(y|x) exp(U(x))(28)": "While in theory energy discrepancy yields a valid training functional for energy-based models foralmost any choice of conditional distribution q, the conditional distribution also needs to allow anestimation of Uq with low variance. This is particularly easy when q is symmetric, i.e. q(y|x) =q(x|y) in which case the contrastive potential can be expressed as an expectation which can readilybe approximated from samples. This leads to",
  "log(M)(29)": "with xi pdata, yi qt(|xi), and xi,j qt(|yi), where the offset w stabilises the loss approxima-tion as discussed in Schrder et al. (2023). The interpretation of w is that contributions from negativesamples with U(xi,j ) > U(xi) are exponentially suppressed as contributions to the loss functional,thus avoiding the energies of negative samples to explode. 0.250.500.751.00 t = 0.01 S = 5 0.000.250.500.751.00 S = 20 0.000.250.500.751.00 S = 50 0.000.250.500.751.00 S = 100 0.000.250.500.751.00 S = 500 0.250.500.751.00 t = 0.05 0.000.250.500.751.000.000.250.500.751.000.000.250.500.751.000.000.250.500.751.00 0.250.500.751.00 t = 0.01 S = 5 0.000.250.500.751.00 S = 20 0.000.250.500.751.00 S = 50 0.000.250.500.751.00 S = 100 0.000.250.500.751.00 S = 500 0.250.500.751.00 t = 0.05 0.000.250.500.751.000.000.250.500.751.000.000.250.500.751.000.000.250.500.751.00 0.50.00.51.01.5 t = 0.1 S = 5 S = 20 S = 50 2.50.02.55.07.5 S = 100 S = 500 1.00.50.00.51.0 t = 0.5 101220242.50.02.55.07.55051015 0.50.00.51.01.5 t = 0.1 S = 5 S = 20 S = 50 0.02.55.07.5 S = 100 S = 500 0.50.00.51.01.5 t = 0.5 101230240.02.55.07.55051015 : Scaling limit of the introduced perturbations. Top: Convergence of rescaled cyclical andordinal perturbations yS2t/S for base time parameters t = 0.01 and t = 0.05 to Gaussian [0, 1)with non-trivial boundary conditions. One can see that the perturbation converges to a fixed shapeon the normalised state space. Bottom: Convergence of rescaled cyclical and ordinal perturbation(ySt E[ySt])/ S for base time parameters t = 0.1 and t = 0.5 to Gaussian on R (red line). Theorange mark indicates the initial state. One can see that the perturbation remains non-trivial as thestate space grows to infinity at rate",
  "On binary spaces, the construction of perturbations is particularly simple. We give some details inthis subsection": "Bernoulli perturbation. As proposed previously in (Schrder et al., 2023, Appendix B.3), q canbe defined as a Bernoulli distribution. Specifically, for Bernoulli()d, (0, 1) define theBernoulli perturbed data point as y = x + mod 2. This induces a symmetric transition densityq(y|x) on {0, 1}d. The Bernoulli random variable k emulates an indicator function, signifying ineach dimension whether to flip the entry of x. The value of controls the information loss induced bythe perturbation. In theory, larger values of lead to a more data-efficient loss, while smaller valuesof may be more practical as they contribute to improved training stability.",
  "thus relating the continuous time Markov chain framework on {0, 1} to a Bernoulli perturbation withparameter 0.5 (1 e2t)": "Neighbourhood-based perturbation and grid perturbation. Inspired by concrete score matching(Meng et al., 2022), one can introduce a perturbation scheme based on neighbourhood maps: x N(x), which assigns each data point x X a set of neighbours N(x). In this case, the forwardtransition density is given by the uniform distribution over the set of neighbours, i.e., q(y|x) =1",
  "Ngrid(x) = {y {0, 1}d : y x = ek, k = 1, 2, . . . , d},(31)": "where ek is a vector of zeros with a one in the k-th entry. Notably, this neighbourhood structure alsoexhibits symmetry, i.e., N 1grid(x) = Ngrid(x). The same perturbation can be derived from an Eulerdiscretisation of the continuous time Markov chain. On a binary space we have for t = 1 for thesame rate matrix as in Equation (30)",
  "s{1,2,...,Sk} exp(U(x1, . . . , xk = s, . . . , xd)) = log p(xk|xk)": "Hence, this specific ED loss function is indeed a Monte Carlo approximation of pseudo-likelihood.Energy discrepancy offers additional flexibility through the tunable choice of t and M, thus makingED adaptable to the structure of the underlying space and more efficient in practice, since thenormalisation of the pseudo-likelihood is only computed from M samples and does not require theintegration along an entire state space dimension.",
  "CTabular Data Synthesising with Energy-Based Models": "In this section, we introduce how to use energy discrepancy for training an energy-based modelon tabular data. Let dnum and dcat be the number of numerical columns and categorical columns,respectively. Each row in the table is a data point represented as a vector of numerical featuresand categorical features x = [xnum, xcat], where xnum Rdnum and xcat dcatk=1{1, . . . , Sk}. Totrain an EBM with energy discrepancy, one should define the perturbation methods, which can bedone by solving the differential equation in (6). For the numerical features, we choose the Gaussianperturbation as in Schrder et al. (2023), which has the transition probability in the form of",
  "D.1Discrete Density Estimation": "Experimental Details. This experiment keeps a consistent setting with Dai et al. (2020). Wefirst generate 2D floating-points from a continuous distribution p which lacks a closed form butcan be easily sampled. Then, each sample x := [x1, x2] R2 is converted to a discrete datapoint x {0, 1}32 using Gray code. To be specific, given x p, we quantise both x1 and x2into 16-bits binary representations via Gray code (Gray, 1953), and concatenate them togetherto obtain a 32-bits vector x. As a result, the probabilistic mass function in the discrete space isp(x) p ([GrayToFloat(x1:16), GrayToFloat(x17:32)]). To extend datasets beyond binary cases,we adhere to the same protocol but utilise base transformation instead. This transformation enablesthe conversion of floating-point coordinates into discrete variables with different state sizes. It isimportant to highlight that learning EBMs in such discrete spaces presents challenges due to thehighly non-linear characteristics of both the Gray code and base transformation. We parameterise the energy function using a 4 layer MLP with 256 hidden dimensions and Swish(Ramachandran et al., 2017) activation. To train the EBM, we adopt the Adam optimiser with alearning rate of 0.0001 and a batch size of 128 to update the parameter for 105 steps. For the energydiscrepancy, we choose w = 1, M = 32 and the grid perturbation for all variants. For contrastivedivergence, we employ short-run MCMC using Gibbs sampling with 10 rounds (i.e., 10 S steps). After training, we quantitatively evaluate all methods using the negative log-likelihood (NLL) andthe maximum mean discrepancy (MMD). To be specific, the NLL metric is computed based on4, 000 samples drawn from the data distribution, and the normalisation constant is estimated usingimportance sampling with 1, 000, 000 samples drawn from a variational Bernoulli distribution withp = 0.5. For the MMD metric, we follow the setting in Zhang et al. (2022a), which adopts theexponential Hamming kernel with 0.1 bandwidth. Moreover, the reported performances are averagedover 10 repeated estimations, each with 4, 000 samples, which are drawn from the learned energyfunction via Gibbs sampling. Qualitative Results. In Figures 7 and 8, we present additional qualitative results of the learned energyon datasets with 5 and 10 states. We see that ED consistently yields more accurate energy landscapescompared to CD. Notably, we only showcase results using grid perturbation with the uniform ratematrix, as qualitative findings are consistent across different perturbation methods. Additionally, we",
  ": Comparison of calibration results between the baseline (top) and energy discrepancy(bottom) on varying datasets. Left to right: Bank, Cardio, Churn, Mushroom": "empirically observe that gradient-based Gibbs sampling methods (Grathwohl et al., 2021; Zhanget al., 2022b) tend to generate samples outside the data support more readily. In this regard, we onlydisplay the results of CD methods using vanilla Gibbs sampling. Quantitative Results. The quantitative results are illustrated in Tables 3 and 4, indicating thesuperior performance of our approaches in most scenarios. Notably, previous studies on discreteEBM modelling exclusively focus on binary cases. As a result, we only present the quantitativecomparison for the dataset with 2 states.",
  "D.2Tabular Data Synthesising": "Experimental Details for the Synthetic Dataset. For the synthetic dataset, we parametrise theenergy function using three MLP layers with 256 hidden states and Swish activation. To handlemixed data types, we transform each categorical feature into a 4-dimensional embedding using alinear layer, and then concatenate these embeddings with the numerical features as input. To train theEBM, we apply the Adam optimiser with a learning rate of 0.0001 and a batch size of 128. We updatethe parameters over 1, 000 epochs, with each epoch consisting of 100 update iterations. For ED, weset w = 1, M = 32, using Gaussian perturbation for the numerical features and grid perturbationfor the categorical features. For CD, we incorporate the replay buffer strategy and employ Langevindynamics and Gibbs sampling with 50 rounds (totalling 50S steps) for the numerical and categoricalfeatures, respectively. Experimental Details for the Real-world Dataset. summarises the statistical propertiesof the datasets. To parameterise the energy function and handle mixed data types, we use the sameapproach but with 1024 hidden units instead of 256. We train the model using the AdamW optimiser(Loshchilov & Hutter, 2019) with a learning rate of 0.0001 and a weight decay rate of 0.0005. Themodel is trained for 20, 000 update steps with a batch size of 4096. For ED, Gaussian perturbation isemployed for numerical features, while categorical features undergo different perturbation methods.Specifically, TabED-Uni and TabED-Grid use uniform and grid perturbations with t = 0.1, respec-tively. For TabED-Cyc and TabED-Ord, corresponding to cyclical and ordinal perturbations, quadraticscaling is applied with t chosen from the best performance in {0.01, 0.005, 0.001}. Moreover, CD",
  "utilises the same algorithm as in the synthetic dataset, but with 10 steps for short-run MCMC. Thereported results are averaged over 10 randomly sampled synthetic data": "Experimental Details for Calibration. Let y and x be the target label and the rest features in thetabular data, we can transform a learned EBM U(x, y) into a deterministic classifier: pEBM(y|x) exp(U(x, y)). As a baseline for comparison, we additionally train a classifier pCLF(y|x) with thesame architecture by maximising the conditional likelihood: Epdata[log pCLF(y|x)]. In particular, weutilise the Adam optimiser with a learning rate of 0.001 and a batch size of 4096 to train the classifierpCLF. The model undergoes training for 50 epochs. Additional Results for Calibration. presents additional calibration results across differentdatasets. It shows that the energy-based classifier learned by energy discrepancy exhibits superiorcalibration compared to the deterministic classifier, except for the Mushroom dataset, where thedeterministic classifier achieves 100% accuracy, resulting in low calibration error. Additional Results with Other Metrics. We evaluate our methods against baselines using twoadditional metrics: single-column density similarity and pair-wise correlation similarity. Thesemetrics assess the similarity in the empirical distribution of individual columns and the correlationsbetween pairs of columns in the generated versus real tables. Both metrics can be computed usingthe open-source SDMetrics API. As shown in , the result shows that the proposed ED-basedapproaches either outperform or achieve comparable performance to the baselines across mostdatasets.",
  "D.3Discrete Image Modelling": "Experimental Details. In this experiment, we parametrise the energy function using ResNet (Heet al., 2016) following the settings in Grathwohl et al. (2021); Zhang et al. (2022b), where the networkhas 8 residual blocks with 64 feature maps. Each residual block has 2 convolutional layers and usesSwish activation function (Ramachandran et al., 2017). We choose M = 32, w = 1 for all variants ofenergy discrepancy, = 0.001 in Bernoulli perturbations. Note that here we choose a relatively small since we empirically find that the loss of energy discrepancy converges to a constant rapidly with",
  "Static MNIST182.53 130.94 102.7098.0788.1396.1190.61Dynamic MNIST 157.14 130.5697.5091.0084.1697.1290.19Omniglotnan.161.96 142.91 149.68 146.1197.5793.94": "larger , which can not provide meaningful gradient information to update the parameters. All modelsare trained with Adam optimiser with a learning rate of 0.0001 and a batch size of 100 for 50, 000iterations. We perform model evaluation every 5, 000 iteration by conducting Annealed ImportanceSampling (AIS) with the GWG (Grathwohl et al., 2021) sampler for 10, 000 steps. The reportedresults are obtained from the model that achieves the best performance on the validation set. Aftertraining, we finally report the negative log-likelihood by running 300, 000 iterations of AIS. Qualitative Results. To qualitatively assess the validity of the learned EBM, this study presentsgenerated samples from the dynamic MNIST dataset. We first train an EBM using ED-Grid and thensynthesise samples by employing various sampling methods, including: i) GWG (Grathwohl et al.,2021) with 1000 steps; ii) GFlowNet with the same architecture and training procedure as per Zhanget al. (2022a); and iii) GFlowNet followed by GWG with 100 steps. Empirically, we find that the quality of generated samples can be improved with more advancedsampling approaches. As depicted in , the GWG sampler suffers from mode collapse,leading to samples with similar patterns. In other hands, GFlowNet enhances the quality to someextent, but it produces noisy images. To address this issue, we apply GWG with 100 steps followingthe GFlowNet. It can be seen that the resulting GFlowNet+GWG sampler yields the highest qualitywith clear digits. These observations validate the capability of our energy discrepancies to accuratelylearn the energy landscape from high-dimensional datasets. We leave the development of a moreadvanced sampler in future work to further improve the quality of generated images using our energydiscrepancy approaches. Time Complexity Comparison for Energy Discrepancy and Contrastive Divergence. Energydiscrepancy offers greater training efficiency than contrastive divergence, as it does not rely onMCMC sampling. In this experiment, we evaluate the running time per iteration and epoch for energydiscrepancy and contrastive divergence in training a discrete EBM on the static MNIST dataset.The experiments include contrastive divergence with varying MCMC steps and variants of energydiscrepancy with a fixed value of M = 32. The results, presented in , highlight that ED-Bernand ED-Grid are the fastest options, as they do not involve gradient computations during training. Comparison to Contrastive Divergence with Different MCMC Steps. Considering the greatertraining efficiency of energy discrepancy over contrastive divergence, this study comprehensivelycompares these two methods with varying MCMC steps in contrastive divergence. Specifically,we utilise the officially open-sourced implementation6 of DULA to conduct contrastive divergencetraining. As depicted in , we find that energy discrepancy significantly outperforms contrastivedivergence when employing a single MCMC step, and achieves performance comparable to CD-10.We attribute this superiority to the fact that CD-1 involves a biased estimation of the log-likelihoodgradient due to inherent issues with non-convergent MCMC processes. In contrast, energy discrepancydoes not suffer from this issue due to its consistent approximation. The Efficacy of the Number of Negative Samples.In all experiments, we choose thenumber of negative samples as M=32 irrespective of the dimension of the prob-lem, to maximise computational efficiency within the constraints of our GPU capacity.",
  "NLL90.1390.3789.1490.61": "To investigate the impact of the number of nega-tive samples on performance, we conduct exper-iments by training energy-based models on thestatic MNIST dataset with ED-Grid for differentvalues of M. As detailed in , our resultsmaintain comparable quality even as the numberof negative samples is decreased. Notably, ourapproach offers greater parallelisation potential compared to the sequentially computed MCMC ofcontrastive divergence.",
  "p(x) exp(xT Jx), x {1, 1}D,": "where J = AD with R and AD being theadjacency matrix of a D D grid. FollowingGrathwohl et al. (2021); Zhang et al. (2022b,a),we generate training data through Gibbs sampling and use the generated data to fit a symmetric matrixJ via energy discrepancy. Note that the training algorithms do not have access to the data-generatingmatrix J, only to the collection of samples. Experimental Details. As in Grathwohl et al. (2021); Zhang et al. (2022a,b), we train a learnableconnectivity matrix J to estimate the true matrix J in the Ising model. To generate the training data,we simulate Gibbs sampling with 1, 000, 000 steps for each instance to construct a dataset of 2, 000samples. For energy discrepancy, we choose w = 1, M = 32 for all variants, = 0.1 in Bernoulliperturbations. The parameter J is learned by the Adam (Kingma & Ba, 2015) optimiser with alearning rate of 0.0001 and a batch size of 256. Following Zhang et al. (2022a), all models are trainedwith an l1 regularisation with a coefficient in {100, 50, 10, 5, 1, 0.1, 0.01} to encourage sparsity. Theother setting is basically the same as Section F.2 in Grathwohl et al. (2021). We report the best resultfor each setting using the same hyperparameter searching protocol for all methods. Qualitative Results. In , we consider D = 10 10 grids with = 0.2 and illustrate thelearned matrix J using a heatmap. It can be seen that the variants of energy discrepancy can identifythe pattern of the ground truth, confirming the effectiveness of our methods. : Mean negative log-RMSE (higher is better) be-tween the learned connectivity matrix J and the true matrixJ for different values of D and . The results of baselinesare directly taken from Zhang et al. (2022a).",
  "ED-Bern5.14.02.92.52.35.14.3ED-Grid4.64.03.12.62.34.54.0": "Quantitative Results. In the quan-titative comparison to the baselines,we consider D = 10 10 grids with = 0.1, 0.2, . . . , 0.5 and D = 9 9grids with = 0.1, 0.2.Themethods are evaluated by computingthe negative log-RMSE between theestimated J and the true matrix J.As shown in , our methodsdemonstrate comparable results to thebaselines and, in certain settings, evenoutperform Gibbs and GWG, indicat-ing that energy discrepancy is ableto discover the underlying structurewithin the data.",
  ": Visualisation of the training data and samples drawn from the energy-based models learnedby the variants of our approaches on the Ego-small dataset": "et al., 2008). We consider the following baselines7 in graph generation, including GraphVAE(Simonovsky & Komodakis, 2018), DeepGMG (Li et al., 2018), GraphRNN (You et al., 2018), GNF(Liu et al., 2019), GrappAF (Shi et al., 2020), GraphDF (Luo et al., 2021), EDP-GNN (Niu et al.,2020), RMwGGIS (Liu et al., 2023), and contrastive divergence with GWG sampler (Grathwohlet al., 2021).",
  "ED-Bern0.0630.0540.0140.044ED-Grid0.0360.0500.0190.035": "Experimental Details. Following the setup in Youet al. (2018), we split the Ego-small dataset, allo-cating 80% for training and the remaining 20% fortesting. To provide better insight into this task, weillustrate a subset of training data in a. No-tably, these training data examples closely resemblerealistic one-hop ego graphs. For a fair comparison, we parametrise the energyfunction via a 5-layer GCN (Kipf & Welling, 2017)with the ReLU activation and 16 hidden states for allenergy-based approaches. For hyperparameters, wechoose M = 32, w = 1 for all variants of energy dis-crepancy and = 0.1 for the Bernoulli perturbation.Following the configuration in Liu et al. (2023), weapply the advanced version of RMwGGIS with thenumber of samples s = 50 (Liu et al., 2023, Equation11). Regarding the EBM (GWG) baseline, we train it using persistent contrastive divergence with abuffer size of 200 samples and the MCMC steps being 50. To train the models, we use the Adamoptimiser with a learning rate of 0.0001 and a batch size of 200. After training, we generate newgraphs by first sampling N, which is the number of nodes to be generated, from the empiricaldistribution of the number of nodes in the training dataset, and then applying the GWG sampler(Grathwohl et al., 2021) with 50 MCMC steps from a randomly initialised Bernoulli noise. To assessthe quality of these samples, we employ the MMD metric, evaluating it across three graph statistics,i.e., degrees, clustering coefficients, and orbit counts. Following the evaluation scheme in Liu et al.(2019), We trained 5 separate models of each type and performed 3 trials per model, then averagedthe result over 15 runs. Qualitative Results. We provide a visualisation of generated graphs from variants of our methods inFigures 12b and 12c. Notably, the majority of these generated graphs resemble one-hop ego graphs,illustrating their adherence to the graph characteristics in the training data. Quantitative Results. In , we compare our methods to various baselines. It can be seen thatour methods outperform most baselines in terms of the average of the three MMD metrics, indicatingthe faithful energy landscapes learned by the energy discrepancy approaches. 7There is insufficient information to reproduce EBM (GwG) and RMwGGIS precisely from Liu et al. (2023).We reran these two baselines with controlled hyperparameters for a fair comparison, while other baseline resultswere taken from their original papers.",
  "ENaming Conventions and Parameters of Introduced Methods": "This table summarises the naming conventions and available tuning parameters for all introducedmethods. The structured perturbation TabED-Str uses different perturbations depending on the statespace structure: On unstructured data, the uniform perturbation with tuning hyper-parameter tcatis used, while on ordinally and cyclically structured data the ordinal perturbations and cyclicalperturbations are used, respectively, with tuning parameter tbase.",
  "NameSpace (Discrete component)Perturbation (Discrete component)Tuning Parameter": "ED-Bern{0, 1}ddk=1 Bern() = 0.5(1 e2t)ED-Grid{0, 1}ddk=11d|ykxk|=1NoneTabED-Unidk=1{1, . . . , Sk}dk=1 exp(tRunif)ykxk (Equation (7))t > 0TabED-Griddk=1{1, . . . , Sk}dk=11d(yk, )(yk, xk)NoneTabED-Cycdk=1{1, . . . , Sk}dk=1 exp(tkRcyc)ykxk (Proposition 1)tk = S2ktbaseTabED-Orddk=1{1, . . . , Sk}dk=1 exp(tkRord)ykxk (Proposition 1)tk = S2ktbaseTabED-Strdk=1{1, . . . , Sk}dk=1 exp(tkRk)ykxk (Mixed)Mixed"
}