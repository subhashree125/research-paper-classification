{
  "Abstract": "Significant advances have been achieved in leveraging foundation models, suchas large language models (LLMs), to accelerate complex scientific workflows. Inthis work we introduce FoamPilot, a proof-of-concept LLM agent designed toenhance the usability of FireFOAM, a specialized solver for fire dynamics and firesuppression simulations built using OpenFOAM, a popular open-source toolbox forcomputational fluid dynamics (CFD). FoamPilot provides three core functionalities:code insight, case configuration, and simulation execution. Code Insight is an alter-native to traditional keyword searching leveraging retrieval-augmented generation(RAG) and aims to enable efficient navigation and summarization of the FireFOAMsource code for developers and experienced users. For case configuration, the agentinterprets user requests in natural language and aims to modify existing simula-tion setups accordingly to support intermediate users. FoamPilots job executionfunctionality seeks to manage the submission and execution of simulations in high-performance computing (HPC) environments and provide preliminary analysisof simulation results to support less experienced users. Promising results wereachieved for each functionality, particularly for simple tasks, and opportunitieswere identified for significant further improvement for more complex tasks. Theintegration of these functionalities into a single LLM agent is a step aimed at accel-erating the simulation workflow for engineers and scientists employing FireFOAMfor complex simulations critical for improving fire safety.",
  "Introduction": "Computational fluid dynamics (CFD) is an indispensable tool in the study and simulation of firedynamics and combustion phenomena (Ren et al. , Nmira and Consalvi ). Amongthe various CFD solvers, FireFOAM3 (Wang et al. ), a solver developed specifically for firedynamics simulations, stands out for its ability to model complex fire and fire suppression physics,including buoyancy-driven turbulence, gas-phase chemical reactions, solid-phase heat transfer andpyrolysis, liquid film flow and spray transport. FireFOAM is built using OpenFOAM4, a popular C++-based open-source toolbox for CFD. A rendered image of a typical FireFOAM simulation of a large-scale fire suppression scenario in a warehouse is shown in a. Despite its advanced capabilities,the process of setting up, running and post-processing FireFOAM simulations remains a challengingand time-consuming task, especially on high-performance computing (HPC) architectures. Usersmust navigate a multitude of keywords, configurations, and code parameters to achieve accurate andreliable results, creating a steep learning curve for entry-level users and remaining time-consumingeven for experienced users. A high-level overview of a typical FireFOAM case structure is shown in",
  "b. This complexity necessitates a level of expertise not only in fire science, but also in CFD andHPC, creating a significant barrier for many researchers and engineers": "The development of foundation models, especially large language models (LLMs), has revolution-ized various scientific and technical fields by offering advanced capabilities in understanding andgenerating human-like text and code. LLMs, such as OpenAIs GPT series, Metas LLaMA andAnthropics Claude, and specialized models like OpenAIs earlier Codex project (OpenAI ) andMetas Code LLaMA (Meta ), have demonstrated substantial capabilities in natural languageprocessing, code generation, and automated problem-solving by interpreting natural language queriesto generate corresponding code and thereby aiding software development (Brown , Roziereet al. , Feng et al. , Achiam et al. ). While LLMs can generate code from natural language descriptions, it is typically a static output,namely a string of text. The LLM lacks the ability to interact with its computational environment,adapt to changing requirements, or handle unexpected errors autonomously (Jiang et al. , Wanget al. ). To overcome these limitations, an LLM-based agent system is neededone that notonly generates code, but also iteratively tests, debugs, optimizes, and integrates it within a broadersoftware ecosystem (Liu et al. , Hong et al. , Qian et al. , Bouzenia et al. ).Such systems combine the strengths of LLMs with additional tools for automated code execution,validation, and refinement, enabling a more comprehensive approach to software development. Previous works have leveraged LLM agents to automate scientific work (Xia et al. ). Indeed,some works on supporting scientific and engineering simulations, such as Chen et al. , haveshown success in modifying parameters of existing simulation configurations. Parameter modification,and more generally simulation case setup, is an important aspect of the scientific simulation workflow.Considered more broadly, the scientific simulation workflow typically involves problem identificationand parametrization, simulation case setup, simulation execution, post-processing of output data andextraction of the desired physical insights. In this work, we introduce FoamPilot, an LLM agentdesigned to address three core functionalities within the scientific simulation workflow for both newand experienced human users of FireFOAM: Code Insight, Case Configuration, and Job Execution.",
  "The FoamPilot agent": "An overview of the FoamPilot agent is illustrated in . The agent is implemented using theLangChain/LangGraph 0.2 framework (Chase ) and is designed to be agnostic to the choice oflarge language model (LLM). This design allows for flexibility in using local or cloud-hosted, open-source or closed-source LLMs, enabling the agent to leverage the ongoing performance improvementsin these models, such as those trained for complex reasoning tasks (e.g., o1 from OpenAI [2024a],still in limited preview at the time of writing). The agent structure follows a graph consisting of three nodes, as shown in : user, LLM, andtools, with edges connecting them to facilitate message transfer. The tool node provides access toutilities, which in the present work consist of a Shell Command Tool, a Python Interpreter Tool, anda Retrieval-Augmented Generation (RAG) tool. These tools were chosen due to their direct utilityfor the agents desired functionalities. The Shell Command Tool executes Linux commands, thePython Interpreter Tool runs Python scripts generated by the LLM, and the RAG Tool identifies theembeddings closest to the query to retrieve information from the database, which, in this work, is avector store of the FireFOAM source code. Further tools could have been added in the design of the",
  "agent, but this was decided against to avoid giving the agent redundant or unnecessary capabilitieswhich could result in mistaken tool choices and accordingly diminished performance": "The LLM processes user queries in natural language augmented with information regarding availabletools. The agent then dynamically and iteratively produces a structured output flow to engage with theavailable tools to achieve the users desired functionalities. In our implementation, a static conditionaledge, containing \"if\" statements to direct the flow to different nodes based on the LLMs structuredoutput, is used to invoke the appropriate tools from the tool node and to determine when to terminatethe process. The tools cannot interact directly, but can be called by the LLM in the same query.The LLM must orchestrate their use through potentially multiple loops of query and feedback. Theprocess continues until the task is successfully completed, the LLM gives up, or the context windowlimit is approached, at which point a summary is provided to the user for progress tracking andfeedback purposes.",
  "Code Insight": "A user of a simulation code, particularly a developer-user, will often need to refer to the source codeto understand the details of sub-model implementations, as well as how to invoke those sub-modelsin the simulation configuration files. Traditional methods for navigating and understanding sourcecode often rely on basic text search tools like grep and find. These methods can be inefficient andcumbersome because variable names and in-line comments depend on the code authors personal style. In large open-source projects like OpenFOAM and FireFOAM, this can turn code navigationinto a time-consuming task. Furthermore, translating classes and functions back to their originalmathematical representations, which is often necessary when seeking to understand complex codethat is sparsely commented and documented, can also be time-consuming. RAG is a natural language processing (NLP) approach that enhances text generation by integratingexternal knowledge retrieval to improve factual accuracy, coherence, and context relevance. UsingRAG with an embedding space search based on cosine similarity offers a significant advantage overtraditional user-driven keyword-based searches like grep and find for navigating FireFOAMs C++source code. Unlike manual keyword matching, the RAG method captures semantic relationshipsand contextual relevance, enabling it to identify conceptually related code fragments even if they donot share exact terms. This typically leads to more meaningful search results, allowing users to findrelevant code more efficiently. In preprocessing the FireFOAM source code for use with the RAG Tool, we have developed a methodto address the challenges posed by the separation of C++ header files (.H) and source files (.C).Header files primarily contain variable declarations and function prototypes, lacking details on theiractual implementation, which are provided in the source files. If these files were indexed separately, aretrieval model might mostly return header files, providing limited value for the downstream LLM.To improve the usefulness of the retrieved content, we combine the header and source files intoa single document before passing them to an embedding model. This approach ensures that boththe declarations and the corresponding implementations are available in the same context, makingthe retrieved information more valuable and comprehensive. It is important to note that popularembedding models typically have smaller maximum input sizes than the context windows of themost performant LLMs. The standard practice is to split or chunk large documents into smallerpieces for embedding. However, most of FireFOAMs header and source files do not exceed themaximum context window of the embedding model in this work (8192 tokens). Additionally, placingthe header file first in the combined context ensures that the embedded vectors always include criticalinformation from key terms. Thus, even if some of the source code exceeds the embedding modelsmaximum input size, it can simply be truncated and the similarity search in embedding space stillperforms well. The correct file is returned to the LLM, the entirety of which comfortably fits withinthe LLMs context window. This allows for better continuation during complex task execution, sinceeven after multiple tool calls, the source code files will still be present within the context window.This also demonstrates the importance of large context windows for keeping track of the details oflong chains of LLM-tool interactions. Additional preprocessing steps included prepending the relative file path to each combined file,providing contextual cues that can be leveraged by the LLM upon retrieval to better understand theorganization of the codebase. To optimize token usage and remove unnecessary content, we strippedout boilerplate headers and license information, as these elements do not contribute meaningfulinformation for code understanding or search tasks. Comments in the headers and throughout thecode were retained, as these provide useful contextual information. After these steps, the preprocesseddata were embedded, with similarity search and retrieval accomplished using a vector store. Thepreprocessing and retrieval steps are illustrated in . Compared to the manual keyword searchapproach, this method is contextually aware, improving the effectiveness of code search within theFireFOAM codebase.",
  "Case Configuration": "FoamPilot interprets user requests for case configurations expressed in natural language. Currently, itis capable of modifying an existing simulation case provided by the user. Initially, we sought onlyto point the agent to the location of an existing case, allowing it to autonomously find and choosethe necessary configuration files to modify to meet the users request. We found that this was aninefficient approach, with the agent issuing calls to the Shell Tool to individually check every filewithin the case folder. The case folder for a FireFOAM simulation contains many individual files, asshown in b. To address this, we applied an approach similar in spirit to the RAG Tool described in .1:we stripped out all boilerplate headers and license information, prepended the relative file path toeach configuration file, then compressed the entire case folder into a single long string. We thenprovided the case configuration string to the agent within the prompt. For FireFOAM simulations, the",
  ": Illustration of the process by which the source code is embedded in a vector database andretrieved by the RAG Tool": "total token usage for this case configuration string was approximately 20k tokens using the GPT-4otokenizer, well below that models context window limit of 128k. The case configuration stringprovides the agent with a comprehensive overview of the case, allowing it to efficiently identify therelevant parameters and locations within the configuration files. The case configuration prompt isshown in , where user_request is the users description of their desired modification to thecase they have provided. We contend that the case configuration functionality makes FireFOAM more accessible to non-expertusers, by reducing the complexity and associated time required to correctly configure simulations.Our approach assumes the user possesses a case that they wish to modify according to some request,but does not require the user to provide additional example cases. This is a typical situation for ajunior scientist, who may inherit a simulation case from a senior scientist and then be required tomodify it according to some request, but may not have access to a database of example cases relevantto the specific task at hand.",
  "Job Execution": "Running FireFOAM simulations in a Linux environment can be challenging for entry-level users,particularly since large-scale simulations are often conducted in an HPC environment using a jobscheduler like SLURM (Yoo et al. ), which they are unlikely to have previously used. Here, wesought to develop a functionality that would allow FoamPilot to handle the execution of simulations.We sought to be able to execute without a scheduler, such as on a local machine or on the headnode of an HPC system as a serial job, as this is a common configuration for debugging and testingpurposes. We also sought to be able to execute using a job scheduler to run large simulations onnumerous multi-core HPC nodes. When running a FireFOAM simulation locally or on the head node, the agent must prepare the meshand execute the simulation directly on the command line. If requested by the user, the agent canalso provide a preliminary analysis of the results once the simulation has completed. When runningon an HPC system with a job scheduler like SLURM, the agent must first execute commands likescontrol to identify available resources. It must then prepare the mesh and determine the size ofthe mesh. Then, it must perform domain decomposition based on the number of nodes available andthe size of the mesh, and finally write a job submission script and submit the job to the queue. The Job Execution functionality was achieved by providing detailed instructions through prompting,which cause the agent to use the Shell Tool to fulfill the request. The prompts used for serial andHPC jobs are provided in . For the HPC job, as an additional challenge, the selection of thenumber of nodes and cores based on the mesh size was left to the agent.",
  "User Query": "Prompt for serial job: I have a FireFOAM simulation case located at {case_path}. Takea look at the case directory. Mesh the case using the provided script, and then run thesimulation in serial on the command line by invoking fireFoam. Write the output to a log file.After the simulation is finished, plot the results of volumetric heat release rate and save themin the case directory. Remember to load environment variables from {OF_bashrc_path}. Prompt for HPC job: I have a FireFOAM simulation case located at {case_path}. Determinewhat SLURM queues you have access to. Mesh the case using the provided script. Based onthe mesh size and the resources you have available, choose how many nodes to use. Use allphysical cores on each node you use. Configure the number of subdomains in the case basedon the number of physical cores and decompose the domain. Prepare a SLURM script for thequeue and core count, then submit the job. Remember to always load environment variablesfrom {OF_bashrc_path} before any FOAM command, both in the command line and in theSLURM script. Always read the contents of a file before modifying it. I have compressed theentire case directory, including the README file, into a single long string for you to viewand understand my request, as follows: {case_contents}",
  "Experimental results and discussion": "To ensure consistency across all our experiments, we utilized the same LLM, version, and temperaturesetting: Azure/OpenAIs GPT-4o, version 2024-05-13, with a temperature setting of 0.0. The LLMwas chosen due to its performance on reasoning tasks and its apparent familiarity with some aspectsof the OpenFOAM toolbox. Despite efforts to maintain reproducibility, the agent still exhibits somevariability in success rates when completing tasks with the same user query under identical conditionsa temperature setting of 0.0 does not guarantee deterministic results. Therefore, each experiment wasrepeated five times to assess stability. A single user prompt was used in each experiment, and thesame prompt was used in each repeat. All of the experiments considered are unambiguous and have asingle correct outcome, where a successful outcome is determined by comparing the agents actionsto that of an experienced FireFOAM user. The results are summarized in . A system prompt was used to define the models role, behavior, and objectives, and to inform it whattools it has access to, thereby guiding it to produce relevant, accurate, and safe responses. It alsohelped eliminate redundant content from user queries. We implemented a system prompt inspired byChase , shown in . Given the exploratory nature of this project, and given the Shell Tools ability to execute arbitrarycommands, there existed a risk of causing damage to the system on which it was run during ourexperimentation. Thus, all experiments were conducted on a dedicated AWS EC2 instance, and HPCjobs were submitted to a dedicated AWS parallelCluster using the SLURM job scheduler. FoamPilotis a small Python-based code, and we employed a cloud-hosted LLM in our experiments, thus thesystem requirements for the instances were driven entirely by the FireFOAM code. We note that while our implementation does include a mechanism to allow the user to verify shell commandsbefore their execution, this may still be challenging for users unfamiliar with the Linux commandline to judge and manage effectively. We would therefore strongly recommend that the testing ofagents with shell command execution authority should only be carried out within a secure sandboxenvironment.",
  "System prompt": "You are an assistant whose job is to help fire scientists navigate and summarize the sourcecode, modify the simulation case configuration files, and run simulation jobs. Respond to thehuman scientist as helpfully and accurately as possible. You have access to the following tools:{tool_names}. Use a JSON blob to specify a tool by providing an action key (tool name) andan action_input key (tool input). Valid \"action\" values: \"Final Answer\" or {tool_names}.Provide only ONE action per $JSON_BLOB, as shown: ```{{\"action\": $TOOL_NAME,\"action_input\": $INPUT}}```Follow this format:Question: input question to answerThought: consider previous and subsequent stepswhile requests is not finished, doAction:```$JSON_BLOB```Observation: action resultendAfter the problem is solved, give a final thought to summarize.",
  ": System prompt, inspired by Chase": "Two experiments were considered for the Code Insight functionality, in both of which the agentis prompted to return the mathematical form of an equation that is expressed in the source code.The first case, shown in , seeks to obtain the energy equation used in the solid phase, and thesecond seeks to obtain the form of the non-spherical particle drag force. The agent was consistentlysuccessful in both cases, as shown in . The accuracy of the Code Insight functionality depends heavily on whether the RAG Tool retrievesthe relevant code files, which is strongly influenced by the precision of the retrieval process. Thechoice of embedding model plays a critical role in determining retrieval accuracy. We tested severalopen-source and closed-source embedding models currently available, and some models demonstratedpoor performance due to their limited input sizes, resulting in unsatisfactory results. We ultimatelychose to produce embeddings using the text-embedding-ada-002 model from OpenAI [2024b]and created the vector store using the FAISS library (Douze et al. ) with LangChain. We note that while cosine similarity search in embedding space offers significant improvementsover manual and automated keyword-based searching, ambiguous user queries can still lead tothe retrieval of incorrect code files, causing the agent to produce inaccurate responses. Popularself-correcting RAG systems are typically based on a multi-agent approach (Yan et al. , Asaiet al. ). However, to date no LLM has been trained on the entire source code libraries ofOpenFOAM and FireFOAM. This raises doubts about whether a secondary LLM in such systemscan effectively assess the correctness of the retrieved information. This limitation primarily arisesfrom the current dependence of LLMs on labeled data for training. Acquiring such labeled data,such as well-commented source code for a large-scale open-source simulation software package, isprohibitively expensive. Still, agent performance could likely be significantly enhanced if it were",
  "+ (qr nMagSf)This equation is solved in the solveEnergy function within the reactingOneDimFF class,located in the reactingOneDimFF.C source file": ": Code Insight example: Summarizing the 1D solid-phase energy equation based on Fire-FOAM source code. Note that the appearance of apparently canceling diffusion terms on theleft-hand-side is correct; the terms appear this way in the code due to details of the numericalimplementation. Considering the Case Configuration functionality, two examples were tested based on the tutorialcases provided with FireFOAM: modification of the size of a burner, and modification of the thicknessof cardboard boxes. The result of the former case is shown in , where we see that the agentcorrectly identifies both files requiring modification and modifies them accurately. In the latter case,the agent only needed to modify one file, but the task was more difficult since cardboard thicknessis not a keyword present anywhere in the case files. In both cases, we see in that the agentwas consistently successful. We note that only a limited number of parameters need to be modifiedbetween the two examples, making them relatively simple cases. In our experiments, we observedthat case modifications significantly more complex than those shown failed consistently. We alsonote that intermediate and advanced users may be able to detect errors in the configured cases that donot cause FireFOAM to crash, but rather cause unintended outputs; however, less experienced userswould struggle to do so. In the case of the Job Execution functionality, we ran two series of tests: running a serial job onthe head node, and submitting a job to a scheduler. We observed in general that the Job Executionfunctionality was mostly unsuccessful unless given a highly detailed prompt as in . In thecase of the serial job, the agent was successful in three of five runs, requiring seven agent-tool loopsto complete the task as prescribed in . In one case, the agent incorrectly called the RAGTool instead of the Shell Tool, which polluted the context window with a significant amount ofirrelevant information. Still, the agent recovered and successfully ran the serial job with the Shell",
  ": Case Configuration example: Modifying burner size in FireFOAMs poolFireMcCaffreytutorial case from 0.3m to 0.6m": "Tool and plotted the results using the Python Interpreter Tool in 17 loops. In the failing run, the agenthallucinated the existence of a number of files in the case directory, and ultimately failed to recover. The agent performed much more poorly in the HPC tests, only succeeding once in nine agent-toolloops. In one failing case, the agent failed to correctly identify the HPC environment and issuedincorrect SLURM commands. In the other failing cases, the agent performed almost all sub-taskscorrectly per the system prompt, but failed to correctly setup the environment in its job submissionscript. We observed that the agent consistently gave reasonable estimations for the number of coresto use for the simulation based on the mesh it generated. A more challenging task involves handling multi-functionality queries. These tasks typically requirethe agent to iteratively call multiple tools based on the users query. An example of such a task wouldbe modifying a simulation case file based on information retrieved from the source code. Comparedto simply perturbing parameter values, the agent is expected to review the C++ code retrieved by theRAG Tool and, based on this, identify the correct case files in which to replace specific keywords oreven functions. We report one such test we conducted, where we prompted the agent to change thedrag force model on the droplets in FireFOAMs burningBoxSuppression tutorial case withoutspecifying how such a modification should be achieved, thereby combining Code Insight and CaseConfiguration functionalities. The agent was successful in two runs, requiring only three agent-toolloops to use both the RAG Tool and the Shell Tool to learn what modifications needed to be made andmake the necessary modifications correctly. In one run, the agents RAG Tool query was incorrectand returned irrelevant information, from which it did not recover. In the other two runs, the agentsuccessfully used the RAG Tool to retrieve relevant files, but failed to glean the necessary informationfrom them to make the requested modification to the simulation configuration, and instead madeincorrect modifications. We note that on multi-functionality tests that were more complex than therelatively simple one presented here, the agent failed consistently.",
  "Conclusions and future work": "An LLM agent, FoamPilot, was developed as a proof-of-concept with the aim of reducing thecomplexity and time required for source code navigation, simulation setup and simulation execution,thereby making fire dynamics simulations more accessible and efficient for both new and experiencedusers. In our exploration, we found that the agent was consistently successful for tasks of lowcomplexity, but that its success rate dropped precipitously with increasing task complexity. Important functionalities that were not addressed in this work include the ability to run simulationsasynchronously, which requires the LLM agent to save and recover its state between sessions. Thiswould be particularly useful to users that are less familiar with the HPC environment and workflow. Inaddition, a robust ability for optional human feedback during FoamPilots operations was not achieveddue to technical challenges with the chosen agentization framework. A request for human approval ofany tool usage was included as a necessary safety precaution during testing; however, challenges arosein the LangChain/LangGraph framework with respect to the solicitation and inclusion of substantialhuman feedback between tool usages by the LLM agent. Our experiments considered only GPT-4o for the agents LLM, and it is plausible that there areLLMs available at the time of writing that would have achieved better results in our experiments.We anticipate greater success for case configuration tasks when reliably-structured LLM outputs arecombined synergistically with non-AI case configuration tools that embed expert knowledge. Weexpect that near-future LLMs specialized for reasoning tasks will perform better on the multi-step,complex tasks required for setting up and executing simulations. Furthermore, we expect that presentand future LLM-enhanced developer tools, such as GitHub CoPilot and Cursor AI Code Editor, willcertainly outperform our implementation of the Code Insight functionality, since this functionality isuseful for all software developers, not just FireFOAM developers. Indeed, the challenges identifiedand the solution approaches developed in this work are not unique to FireFOAM, they are applicableto many scientific simulation workflows. The development of generalized frameworks for LLMagents to interact with and control simulation software will be beneficial. Looking ahead, we note that an increasing number of large language models are becoming multimodal,capable of processing image data as input. We believe this capability could improve the agentsunderstanding of simulation configurations if the geometry and mesh are presented visually to providefurther context regarding the simulation at hand. Lastly, we found that the limited domain-specific knowledge of general-purpose LLMs reducesFoamPilots ability to handle complex tasks using FireFOAM, particularly those involving multiplefunctionalities. We note that FireFOAMs codebase is itself around 1M tokens, whereas the Open-FOAM toolbox on which it is built is approximately 10x larger. Thus, FireFOAM, or large parts ofit, may fit in the expanded context windows of future LLMs. Additionally, continued pre-trainingon the OpenFOAM or combined FireFOAM/OpenFOAM codebase may potentially allow accuratezero-shot prompting for analyzing and supporting further code developments. Ning Ren, Jaap de Vries, Xiangyang Zhou, Marcos Chaos, Karl V. Meredith, and Yi Wang. Large-scale fire suppression modeling of corrugated cardboard boxes on wood pallets in rack-storageconfigurations. Fire Safety Journal, 91:695704, 2017. ISSN 0379-7112. Fire Safety Science:Proceedings of the 12th International Symposium. Fatiha Nmira and Jean-Louis Consalvi. Local contributions of resolved and subgrid turbulence-radiation interaction in les/presumed fdf modelling of large-scale methanol pool fires. InternationalJournal of Heat and Mass Transfer, 190:122746, 2022.",
  "Tom B Brown. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020": "Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, YossiAdi, Jingyu Liu, Tal Remez, Jrmy Rapin, et al. Code llama: Open foundation models for code.arXiv preprint arXiv:2308.12950, 2023. Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, BingQin, Ting Liu, Daxin Jiang, et al. Codebert: A pre-trained model for programming and naturallanguages. arXiv preprint arXiv:2002.08155, 2020. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman,Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report.arXiv preprint arXiv:2303.08774, 2023.",
  "Juyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim. A survey on large languagemodels for code generation. arXiv preprint arXiv:2406.00515, 2024": "Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, JiakaiTang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, and Jirong Wen. A survey on largelanguage model based autonomous agents. Frontiers of Computer Science, 18(6), 2024. Junwei Liu, Kaixin Wang, Yixuan Chen, Xin Peng, Zhenpeng Chen, Lingming Zhang, and YilingLou. Large language model-based agents for software engineering: A survey. arXiv preprintarXiv:2409.02977, 2024. Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang,Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, et al. Metagpt: Meta programming for multi-agentcollaborative framework. arXiv preprint arXiv:2308.00352, 2023. Chen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang, Jiahao Li, Cheng Yang, Weize Chen,Yusheng Su, Xin Cong, et al. Chatdev: Communicative agents for software development. InProceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume1: Long Papers), pages 1517415186, 2024."
}