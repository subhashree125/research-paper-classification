{
  "Abstract": "Knowledge graphs (KGs) are valuable for representing structured, interconnectedinformation across domains, enabling tasks like semantic search, recommendationsystems and inference. A pertinent challenge with KGs, however, is that manyentities (i.e., heads, tails) or relationships are unknown. Knowledge Graph Com-pletion (KGC) addresses this by predicting these missing nodes or links, enhancingthe graphs informational depth and utility. Traditional methods like TransE andComplEx predict tail entities but struggle with unseen entities. Textual-basedmodels leverage additional semantics but come with high computational costs,semantic inconsistencies, and data imbalance issues. Recent LLM-based modelsshow improvement but overlook contextual information and rely heavily on entitydescriptions. In this study, we introduce a contextualized BERT model for KGCthat overcomes these limitations by utilizing the contextual information from neigh-bouring entities and relationships to predict tail entities. Our model eliminates theneed for entity descriptions and negative triplet sampling, reducing computationaldemands while improving performance. Our model outperforms state-of-the-artmethods on standard datasets, improving Hit@1 by 5.3% and 4.88% on FB15k-237and WN18RR respectively, setting a new benchmark in KGC.",
  "Introduction": "A knowledge graph (KG) is a structured representation of entities (as nodes) and relationships (aslinks) that supports search, recommendation and other downstream reasoning tasks. However, KGsare often incomplete, with many entities (heads/tails) or relationships missing, limiting their utility inreal-world applications Ding and Jia . Consequently, Knowledge Graph Completion (KGC)predicting a missing tail entity (h, r, ?), head entity (?, r, t), or relationship (h, ?, t) in a triplethasbecome a critical research objective, with numerous methodologies proposed to tackle this issue. Embedding-based methods, for instance, learn vector embeddings for entities and relationships fromtraining data, but these methods struggle to generalize to unseen entities or relationships, impairingperformance in tail prediction during testing Xie et al. . Recently, large language model(LLM)-based approaches for KGC have shown potential in overcoming this limitation by leveragingLLMs trained on extensive datasets to capture complex semantic relationships and generalize betterto unseen entities Yao et al. , Wei et al. , Zhang et al. , Wei et al. . Despite",
  "arXiv:2412.11016v1 [cs.CL] 15 Dec 2024": "these strengths, LLM-based models are computationally demanding, often overlook relation context,and depend heavily on entity descriptions and negative sampling. More recent LLM prompting-basedapproaches encode KGs into promptsWei et al. , but injecting all relevant facts from a KG intoprompts is labor-intensive, and generic LLMs often struggle with domain-specific KGs. Additionally,textual information-based methods like NN-KGC and Sim-KGC utilize neighborhood informationfor KGC, but they often require entity descriptions, which may not be available in many datasets,and add computational overhead Wang et al. , Li et al. . To address these limitations,we propose a Context-Aware BERT for Knowledge Graph Completion (CAB-KGC) that extractscontextual information associated with the operational relationship, its neighboring entities, andrelationships associated with the head entity. This context is then integrated with the BERT model toenhance the prediction of tail entities. To summarise, this study makes the following contributions tothe KG domain: We introduce the CAB-KGC approach to address the KGC problem, leveraging graphfeatures of head entity context and relationship context and the BERT model. The CAB-KGC approach outperforms SOTA KGC methods.",
  "Methodology": "Problem Formulation (see for notations): Consider a knowledge graph G(E, R) as a collectionof triplets (h, r, t), where h E is the head entity, t E is the tail entity, and r R represents therelationship between them, our CAB-KGC model predicts a missing tail t (represented by ?) givenan incomplete triple (h , r , ?).",
  "NotationDescriptionNotationDescription": "eentity or noderrelationshiphhead entity nodettail entity nodeEEntities SetRRelationships SetHcHead (h) or Entity contextRcRelationship contextR(h), E(h)relation and entities associated to head hNTTotal number of tripletsp(ti | hi, ri)Tail ti probability given head hi and relationship rirankiRank of the true tail entity ti in the predictionResults: Lower is betterResults: Higher is better : A concise view of the CAB-KGC Method. Box on the left shows head context Hccalculation; the middle one shows relationship context Rc calculation. Hc and Rc are then fed intothe model pipeline shown on the right side. provides an overview of the CAB-KGC model. It predicts the tail entity t given a head hand a relationship r, in the following steps:",
  ". Extract Head Context Hc: To extract the contextual information for the head i.e. Hc, wefirst identify the relationships r that are associated with the head entity h, i.e., R(h). If k": "relationships are associated with the head h from the set R of all relationships ri in thegraph G, then:R(h) = Aki=1 ({ri | (h, ri, ej) T, ej E})(1)Next, we find the entities e that are neighbours (have a direct connection) with the headentity h, i.e., E(h) using the identified relationships R(h).These neighbour entities can bemathematically expressed as:",
  "Hc = (R(h)) (E(h))(3)": "2. Extract Relationship Context Rc: To acquire the relationship context Rc, we identify allthe entities associated with the operational relationship r in the knowledge Graph G. Rc isgiven as:Rc = Ali,j=1 ({ei, ej | (ei, r, ej) T})(4) 3. Prepare Input Sequence for BERT Classifier: The contextual information extracted in theabove steps forms the input to BERT. Specifically, the input sequence contains h, Hc fromEquation 3, r, and Rc from Equation 4, as shown below:",
  "Input Sequence = [CLS] h, Hc [SEP] r, Rc(5)where [CLS] is BERTs classifier token and [SEP] is the separator token": "4. Predict and train with BERT Classifier: A classification layer is added on top of the BERTmodel, which aims to classify the tail entity (h , r , ?). Once the BERT classifier receivesthe input, it processes it through various transformer layers, provides a contextualizedrepresentation of each token and uses that to classify the input. The classifier model predictsthe tail entity by employing a softmax function over the output embedding to calculate theprobability for all the available tail entities. The input-output description of the model isgiven as:P(t | h, r) = softmax(W BERT(Input Sequence))(6) Where W is a learned weight matrix. Putting the above equations together, the CAB-KGCmodel can be expressed as:CAB-KGC(t | h, r) = softmax(W BERT(h, Hc, r, Rc))(7)The CAB-KGC model is trained using cross-entropy loss, which compares the probabilitydistribution of the predicted label with the true label for the tail entity. The cross-entropyloss is given by:L =",
  "Experiments SetupDatasets: We assessed the proposed CAB-KGC model on various commonly used KG datasets.These datasets are briefly explained here:": "FB15k-237 Bollacker et al. is an updated version subset of the FB15k dataset, wherethe inverse triplets have been removed to increase the difficulty of the KGC. It has 14541unique entities and 237 relationships. WN18RR Miller is the subset of WN18, where the reverse triplets are removed,making it more complex for the models to incorporate the problem of KGC.Hyperparameters: The experiments used a batch size of 16 and a learning rate of 5e-5, Adam as theoptimizer and cross-entropy as the loss function. The experiments were accomplished on an NVIDIAGeForce RTX 3090 GPU with 24 GB of memory. Training for the CAB-KGC model was halted onceevaluation metrics stabilized to the third decimal place.Evaluation: Various standard evaluation metrics in KGC, as given in Equation 9, such as MRR,and Hit@k, are utilized to assess the performance of the proposed method and other state-of-the-artapproaches.",
  "Results": "Our CAB-KGC approach shows superior results on the FB15k-237 dataset. CAB-KGCs significantperformance is its Hits@1 score of 0.322, which improves SOTA by almost 5.3%, showing a superiorability to rank accurate entities in the first place. It obtains a Hits@3 score of 0.399 and improvesby 0.5%, notably above other models, indicating that CAB-KGC reliably predicts relevant entitieswithin the top 3 ranks. The CAB-KGC method performed well on the WN18RR dataset, gettingan MRR of 0.685, which is an improvement of 1.2% over SOTA models and a Hits@1 of 0.637,an improvement of 4.88%, outperforming state-of-the-art methods. All the results are reported in. Note that we have excluded results from KICGPTWei et al. during comparison forreasons: (a) the model is prompt-based and not trainable, (b) its performance is highly dependenton the underlying LLMs knowledge base and (c) large KGs cannot be injected as prompts to thismodel. Furthermore, injecting all relevant facts from different KGs into prompts is labor-intensive,and underlying LLM will oftern struggle with domain-specific KGs when it does not contain enoughrelevant knowledge. : Comparison of the proposed and baseline methods on the datasets FB15k-237 and WN18RR. Theoptimal outcome for each metric is highlighted in bold, while the second-best result is underlined. The circlesymbol denotes that the results have been extracted from the study conducted by Wei et al. Wei et al. ,while the symbol indicates that the results have been extracted from the study conducted by Yao et al. in Yaoet al. .",
  "MethodsMRR Hits@1 Hits@3 MRR Hits@1 Hits@3": "Embedding-Based MethodsRESCAL Nickel et al. 0.3560.2660.3900.4670.4390.478TransE Bordes et al. 0.2790.1980.3760.2430.0430.441DistMult Yang et al. 0.2410.1550.2630.4300.3900.440ComplEx Trouillon et al. 0.2470.1580.2750.4400.4100.460RotatE Sun et al. 0.3380.2410.3750.4760.4280.492TuckER Wang et al. 0.3580.2660.3940.4700.4430.482CompGCN Vashishth et al. 0.3550.2640.3900.4790.4430.494HittER Chen et al. 0.3440.2460.3800.4960.4490.514HAKE Zhang et al. 0.3460.2500.3810.4970.4520.516 Text-and Description-Based MethodsPretrain-KGE Zhang et al. 0.332--0.235--StAR Wang et al. 0.2630.1710.2870.3640.2220.436MEM-KGC (w/o EP) Choi et al. 0.3390.2490.3720.5330.4730.570MEM-KGC (w/ EP) Choi et al. 0.3460.2530.3810.5570.4750.604SimKGC Wang et al. 0.3330.2460.3630.6710.5850.731NNKGC Li and Yang 0.3380.2520.3650.6740.5960.722",
  "The proposed CAB-KGC approach exploits the contexual information to outperform existing methodsin MRR and Hit@k measures, with improvements of 5.3% and 4.88% over FB15k-237 and WN18RR,respectively": "K. Bollacker, C. Evans, P. Paritosh, T. Sturge, and J. Taylor. Freebase: a collaboratively createdgraph database for structuring human knowledge. In Proceedings of the 2008 ACM SIGMODInternational Conference on Management of Data, pages 12471250, 2008. A. Bordes, N. Usunier, A. Garcia-Duran, J. Weston, and O. Yakhnenko. Translating embeddingsfor modeling multi-relational data. In C.J. Burges, L. Bottou, M. Welling, Z. Ghahramani, andK.Q. Weinberger, editors, Advances in Neural Information Processing Systems, volume 26. CurranAssociates, Inc., 2013."
}