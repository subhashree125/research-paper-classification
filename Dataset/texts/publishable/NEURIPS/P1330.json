{
  "Abstract": "Many algorithms have been developed to estimate probability distributions subject to differentialprivacy (DP): such an algorithm takes as input independent samples from a distribution and estimatesthe density function in a way that is insensitive to any one sample. A recent line of work, initiatedby Raskhodnikova et al. (Neurips 21), explores a weaker objective: a differentially private algorithmthat approximates a single sample from the distribution. Raskhodnikova et al. studied the samplecomplexity of DP single-sampling i.e., the minimum number of samples needed to perform this task.They showed that the sample complexity of DP single-sampling is less than the sample complexity ofDP learning for certain distribution classes. We dene two variants of multi-sampling, where the goal isto privately approximate m > 1 samples. This better models the realistic scenario where synthetic datais needed for exploratory data analysis.A baseline solution to multi-sampling is to invoke a single-sampling algorithm m times on indepen-dently drawn datasets of samples. When the data comes from a nite domain, we improve over thebaseline by a factor of m in the sample complexity. When the data comes from a Gaussian, Ghazi etal. (Neurips 23) show that single-sampling can be performed under approximate differential privacy;we show it is possible to single- and multi-sample Gaussians with known covariance subject to pure DP. Oursolution uses a variant of the Laplace mechanism that is of independent interest.We also give sample complexity lower bounds, one for strong multi-sampling of nite distributionsand another for weak multi-sampling of bounded-covariance Gaussians.",
  "Introduction": "Much research effort has been focused on estimating the parameters of data distributions under thedifferential privacy (DP) constraint (Dwork et al., 2006). DP algorithms assure any outlier that the choiceto contribute their data does not greatly increase risk of privacy harms. This is powerful given thatthe data can concern sensitive personal attributes such as medical history, location history, or internethabits. If we assume data are independent samples from a distribution D, it is natural to estimatethe expectation; Karwa and Vadhan (2018) give DP condence intervals for the Gaussian mean whileKamath et al. (2020) estimate the mean of heavy tailed distributions. Other algorithms perform themore ambitious task of estimating the distributions density function itself; for example, Kamath et al.(2019) and Bie et al. (2022) show how to estimate Gaussians accurately in statistical distance.But we do not always require private estimates of the distribution or its parameters. Exploratorydata analysis, for example, only requires synthetic samples that in some sense resemble the original dis-tribution. To rigorously dene what it means to make such a summary, this work builds on the workof Raskhodnikova et al. (2021). Under their denition, a DP sampling algorithm ingests independentsamples from a distribution D like prior work but it only emits one random variable that is meant to",
  "*Google. Email: University. Email:": "approximate one fresh sample from D up to statistical distance . This denition is also adopted byGhazi et al. (2023), who provide approximate DP algorithms for sampling from Gaussians. We empha-size that each algorithm by these two sets of authors only produces an approximation of a single sample,so we name their objective single-sampling. The main motivation of our work is broadening the objectiveto generating multiple samples under DP.We note that multiple formalizations of the DP objective exist. Pure DP algorithms, for example,bound the participation risk in terms of a single parameter . Meanwhile, approximate DP algorithmspermit a > 0 chance that the bound does not hold. Other variants of DP like zero concentrated DP(zCDP) interpolate between those two extremes. Our algorithms span these three DP variants.",
  "Our work extends the prior work along a number of directions": "1. Novel Denitions. We formalize what it means to produce m > 1 samples from a distributionsubject to DP in . Specically, we provide strong and weak variants of multi-sampling inDenitions 6 and 7. Both variants require that the output random variables are mutually indepen-dent and identically distributed. The weak variant only requires that the marginal distribution ofthe output is close to that of the input, while the strong variant requires that the product distribu-tions are close. 2. New Pure DP Single-samplers. For distributions over the set [k], we show how to performpure DP single-sampling with a sample complexity that has a smaller leading constant than theprior work by Raskhodnikova et al. (2021) in Theorem 12. We achieve this via amplication-by-subsampling. For multivariate Gaussian distributions with known covariance, we provide a pureDP single-sampler that builds upon the approximate DP single-sampler from Ghazi et al. (2023) inTheorems 20 and 21. We achieve this with a novel variant of the Laplace distribution, which is ofindependent interest. 3. Multi-samplers. We describe baseline techniques to create multi-sampling algorithms from onesdesigned for single-sampling (see Lemmas 8 & 9 and Corollary 10). In particular, if we applyCorollary 10 to our single samplers, we obtain strong multi-samplers under pure DP for k-ary andGaussian distributions. The sample complexities grow by a factor of m2: one factor of m comesfrom repeatedly calling the single-sampler, another from a union bound. But under approx. DPwe show that one factor of m is sufcient. In the k-ary case, amplication-by-shufing lets usavoid paying the cost of repetitions. In the Gaussian case, Ghazi et al. (2023)s single-sampler hasa sample complexity depending only logarithmically on , mitigating the union bounds effect. 4. Lower Bounds for Multi-Sampling. For strong multi-sampling, Theorem 24 formalizes the fol-lowing intuition: if there are m i.i.d. samples from D and m i.i.d. samples from D such that thejoint random variable is close (i.e., we have a strong multi-sampler) then the individual variablesmust be even closer (i.e., we have single-sampler with smaller ). For weak multi-sampling, wenote that we can treat the m outputs of a DP sampler as if they were fresh samples without privacyconstraints. We formalize the intuition that overly-large m would circumvent DP estimation lowerbounds, in particular those for Gaussians (Theorem 30).",
  "Under approximate DP, we are able to avoid a multiplicative factor of m in the sample complexityof strong multi-sampling from k-ary distributions. But is approximate DP necessary?": "Is it possible to perform weak multi-sampling of Gaussians without a multiplicative factor of m,as with k-ary distributions? Our use of amplication by shufing is limited, as the known resultsfor local additive noise are weaker than the one for randomized response. Perhaps most interestingly, we can conceive of an alternative denition of weak multi-sampling:we could design an algorithm that bounds the distance between the joint distribution of its outputand m i.i.d. samples from D, but makes no promises about independence of the members of itsoutput. We call this qualitatively weak multi-sampling, as opposed to our quantitatively weak multi-sampling.1 Any strong multi-sampler, like the ones in our work, satisfy both denitions of weak",
  "Measuring Closeness of Distributions": "We briey review the various ways we will measure how far or close distributions are from eachother. Note that we mildly abuse notation and dene distances and divergences for distributions andrandom variables interchangeably.We will measure the error of our sampling algorithms according to total variation (TV) distance, alsoknown as statistical distance. The TV distance between a pair of distributions D,D is",
  "Differential Privacy": "Our privacy objective is differential privacy (DP). There are multiple variants but all rely on a neigh-boring relation: two inputs X,X each containing data from the same n users are neighbors if they differon the value contributed by one user. This denition of neighbor is sometimes referred to as thereplacement or bounded neighboring relation, to contrast with add-remove or unbounded variant. In the for-mer, neighboring datasets have the same public size n. In the latter, one dataset includes a users datawhile the other does not (so n is not public knowledge). We prefer the replacement relation becauseguarantees made for add-remove neighbors make the implicit assumption that n is not public.",
  "Denitions of Sampling Tasks": "We now dene the tasks of interest under DP. Let D be a family of distributions (e.g. Gaussians ordistributions over [k]). We will use n to denote sample complexity, m to denote the number of desiredsamples, and to denote an error tolerance. Denition 5 (Single-Sampling Raskhodnikova et al. (2021)). An algorithm A performs -sampling fora class D of distributions with sample complexity n Z if the following holds for any D D: algorithmA consumes n i.i.d. samples from D in order to produce one sample from some distribution D where||D D||TV . The randomness of D comes from both the n samples of D and the coins of algorithm A. Denition 6 (Strong Multi-sampling). An algorithm A performs strong (m,)-sampling for D with sam-ple complexity n if the following holds for any D D: when A consumes n independent samples fromD, it produces m independent samples from some distribution D where ||Dm Dm||TV . Denition 7 (Weak Multi-sampling). An algorithm A performs weak (m,)-sampling for D with samplecomplexity n if the following holds for any D D: algorithm A consumes n i.i.d. samples from D in orderto produce m i.i.d. samples from some distribution D where ||D D||TV . We briey illustrate the difference between strong and weak multi-sampling. Consider an algorithmthat consumes m samples from D and produces some analysis of D (e.g. quantile). If it were giventhe output of a strong multi-sampler instead of m samples from D, the analysis will remain -close toits non-private counterpart. Meanwhile, consuming the output of a weak multi-sampler would lead toconclusions about a distribution close to D.",
  "Baseline Techniques for Multi-sampling": "Here, we describe baseline techniques to perform weak and strong multi-sampling. We show that itis possible to use a single-sampler for weak multi-sampling and that it is possible to use a weak multi-sampler for strong multi-sampling. Lemma 8 (From Single to Weak Multi-sampling). If A performs -sampling for D with sample complexity n,then there is an algorithm A that performs weak (m,)-sampling for D with sample complexity mn. Specically,A executes A on disjoint sets of n samples. A has the same differential privacy guarantee as A. Lemma 9 (From Weak to Strong Multi-sampling). If A performs weak (m,)-sampling for D with samplecomplexity n(), then there is an algorithm A that performs strong (m,)-sampling for D with sample complexityn(/m). Specically, A executes A with a small enough error tolerance to apply a union bound. A has the samedifferential privacy guarantee as A.",
  "Families of Distributions": "For any positive integer k, the family of k-ary distributions consists of all distributions over the set[k] := {1,...,k}. We will sometimes use nite-domain distributions to refer to them.For any positive integer d and positive reals ,R, we use Nd( R, ) to denote the Gaussian distri-butions that have mean Rd and covariance matrix Rdd satisfying2 R and I I. Wewill call this a family of bounded-mean and bounded-covariance Gaussians. We drop d when it is clearfrom context.We note special cases of the above. The symbol N( , ) (resp. N( R, )) refers to all Gaussianswith -bounded covariance but unbounded mean (resp. R-bounded mean but unbounded covariance).The symbol N( R,) refers to all Gaussians with R-bounded mean and covariance matching . Ghazi et al. (2023) in their paper show that a result by Ghazi et al. (2020, Theorem 6) implies thatDP algorithms that perform sampling for Nd( R, ) can be turned ones that perform sampling forNd( , ).",
  "DP Sampling Algorithms for Finite-Domain Distributions": "In this section we describe DP sampling algorithms for k-ary distributions. Our analysis is basedupon privacy amplication: we develop a local randomizer with a large privacy parameter 0, then ar-gue random sampling or shufing shrinks the effective privacy parameter . We formalize the intuitionthat the large local 0 means that the samples are not too polluted by DP noise.We note that Appendix B will detail extensions when we have hints about the shape of the distribu-tion.",
  "Single-Sampling": "As a warm up to multi-sampling, we rst show how to perform DP single-sampling by picking arandom sample and then performing randomized response. We can express the output distributionas a mixture between the input distribution and the uniform distribution; we bound the weight onthe uniform distribution by . The sample complexity bound given by Raskhodnikova et al. (2021)has a leading constant of 2, while our constant is 1. Both algorithms are asymptotically optimal, asRaskhodnikova et al. (2021) proved a lower bound of (k/).",
  "To bound the second summand, we recall that randomized response is 0-local DP: the above is boundedby 1 + 1": "n exp(0) This in turn is bounded by 1 + exp() by virtue of our choice of 0.We now argue that the output of SubRR, X, will have TV distance from D when run on independentsamples from D. Observe that the random variable obtained by executing randomized response on anyx [k] is distributed as the mixture",
  "Note that the ratio between our algorithms sample complexity and mthe average cost to generateeach of the m samplesis O(1 +k": "m), a function of m that approaches 1 from above. For comparison,the naive approach of repeatedly executing a DP single-sampler has average sample cost (k/).Our work relies upon the technical result by Feldman et al. (2021), which expresses the privacy pa-rameter of the shufed output as a function of 0 and target :",
  "DP Sampling Algorithms for Gaussian Distributions": "Ghazi et al. (2023) gave the rst approximate DP algorithms for single sampling from Gaussianswhere the covariance is either known, bounded or unknown. Weak and strong multi-sampling algo-rithms can be derived for all of these cases using Lemmas 8 and 9. We build upon the algorithms ofGhazi et al. to present the rst pure DP algorithms for the known covariance case. Furthermore, wedemonstrate that one of their algorithms satises the stricter privacy notion of zCDP. Additionally, weexamine Gaussians with bounded covariance in appendix C. Remark 17. The sample complexity of the approximate DP -sampling algorithms from Ghazi et al. (2023, ) only has a logarithmic dependence on 1/, thus applying Lemma 9 leads to a factor of m and not m2 in thesample complexity for strong (m,) multi-sampling as seen in Tables 2 and 3.",
  "Known Covariance, Pure Differential Privacy": "In this section, we describe pure DP algorithms for single-sampling from Gaussians with known co-variance matrix . It is without loss of generality to assume it is the identity matrix I since we can applythe transform 1/2. Weak and strong multi-sampling algorithms can be derived by way of Lemmas 8and 9. It is an interesting open question whether we can avoid a factor of m as with k-ary distributions.",
  "Building-block: The Euclidean-Laplace mechanism": "A core building block of this section is a generalization of the Laplace distribution to d-dimensionalspace (and a corresponding variant of the Laplace mechanism). The generalization that is most commonin the DP literature is comprised of one independent scalar Laplace random variable per dimension. Itis enough to ensure pure DP for 1-sensitive functions. Here, we instead calibrate to 2-sensitivity bydening a density function in terms of the Euclidean distance.Let ELap(b) denote the distribution over Rd with one2 parameter b > 0 and density function",
  "(4)": "Proving that the above function integrates to 1 is an exercise we defer to Appendix D. Note that the ex-ponential term is a generalization of the exponential term of the scalar Laplace distribution: the absolutevalue simply became the Euclidean norm. We call this distribution as the Euclidean-Laplace distribution.It will be very useful to bound the norm of a Euclidean-Laplace random variable:",
  ")": "Proof. Let M be the present mechanism. By closure of DP under post-processing and the fact that weensured all the inputs have norm at most B, M is -DP. So it simply remains to argue that, when giveni.i.d. samples from N(,I), this mechanisms output closely resembles one sample from N(,I).",
  "Consider the alternative mechanism M where is deterministically set to the 0 vector. Note that M": "is found in the prior work by Ghazi et al. (2023) (their Algorithm 1). We write N(,I)n as shorthand forn independent samples from N(,I). We will we show M(N(,I)n) ,2 M(N(,I)n); via Lemma 1 andthe fact that (0,1), this means ||M(N(,I)n) M(N(,I)n)||TV 6. Ghazi et al. (2023) show that ||M(N(,I)n) N(,I)||T V in TV-distance (their Theorem 4.2). Viathe triangle inequality, we have that the TV distance between M(N(,I)n) and N(,I) is 7.Thus, it remains to prove M(N(,I)n) ,2 M(N(,I)n). To do so, we will bound the norm of ourEuclidean-Laplace noise vector then invoke the differential privacy offered by Gaussian noise. Moreprecisely, Lemma 18 implies that our sample from the Euclidean-Laplace distribution has Euclideannorm at most O(dBlog(d/)/) except with probability . Because Z is a Gaussian with covariance n1",
  "A Generic Recipe for Weak Multi-Sampling Lower Bounds": "Thus far, we have assumed that DP algorithms ensure privacy for all inputs. But there exist algo-rithms A which consume inputs for whom DP is not enforced. Specically, there are n inputs labeledprivate and m inputs labeled public such that A is insensitive to any change in any one of the pri-vate inputs. We refer to m as the public sample complexity and n as the private sample complexity.We will use such semi-private 3 algorithms to derive lower bounds.Given a task T concerning distribution D D, suppose that the two statements below are true",
  "The term comes from the work by Beimel et al. (2013)": "If we had a weak (m = m,)-sampler for D with sample complexity n < n, we can treat the generatedvariables as the public samples required by the algorithm in (2) to do T. But overall we have onlyconsumed < private samples from D, which means we have arrived at a contradiction.Via Lemma 8, our recipe also applies to single-sampling: if we had a single-sampler with samplecomplexity n < n/m, there is a weak (m,)-sampler with sample complexity < n.",
  "log": "is a lower bound on the sample complexity. If it were not the case, we can use s private samples fromD to generate d + 1 samples from a Gaussian that is -close to D (under zCDP). Then we can use Bie etal.s algorithm to learn D by feeding it n < s of the private samples and all d + 1 generated samples.Now, for sufciently large R and ,",
  "But this contradicts the lower bound on zCDP learning without public data": "Amos Beimel, Kobbi Nissim, and Uri Stemmer.Private learning and sanitization:Pure vs. ap-proximate differential privacy.In Prasad Raghavendra, Sofya Raskhodnikova, Klaus Jansen, andJose D. P. Rolim, editors, Approximation, Randomization, and Combinatorial Optimization. Algorithmsand Techniques - 16th International Workshop, APPROX 2013, and 17th International Workshop, RAN-DOM 2013, Berkeley, CA, USA, August 21-23, 2013. Proceedings, volume 8096 of Lecture Notesin Computer Science, pages 363378. Springer, 2013.doi: 10.1007/978-3-642-40328-6\\",
  ".URL": "Alex Bie, Gautam Kamath, and Vikrant Singhal.Private estimation with public data.In SanmiKoyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, Advances inNeural Information Processing Systems 35: Annual Conference on Neural Information Processing Sys-tems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022.URL Mark Bun, Kobbi Nissim, and Uri Stemmer. Simultaneous private learning of multiple concepts. InMadhu Sudan, editor, Proceedings of the 2016 ACM Conference on Innovations in Theoretical ComputerScience, Cambridge, MA, USA, January 14-16, 2016, pages 369380. ACM, 2016. doi: 10.1145/2840728.2840747. URL Cynthia Dwork and Jing Lei.Differential privacy and robust statistics.In Michael Mitzenmacher,editor, Proceedings of the 41st Annual ACM Symposium on Theory of Computing, STOC 2009, Bethesda,MD, USA, May 31 - June 2, 2009, pages 371380. ACM, 2009. doi: 10.1145/1536414.1536466. URL",
  "CynthiaDworkandAaronRoth.Thealgorithmicfoundationsofdifferentialprivacy.Found. Trends Theor. Comput. Sci.,9(3-4):211407, 2014.doi:10.1561/0400000042.URL": "Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam D. Smith. Calibrating noise to sensitivityin private data analysis. In Shai Halevi and Tal Rabin, editors, Theory of Cryptography, Third Theoryof Cryptography Conference, TCC 2006, New York, NY, USA, March 4-7, 2006, Proceedings, volume 3876of Lecture Notes in Computer Science, pages 265284. Springer, 2006. doi: 10.1007/11681878\\",
  ". URL": "Vitaly Feldman, Audra McMillan, and Kunal Talwar. Hiding among the clones: A simple and nearlyoptimal analysis of privacy amplication by shufing. In 62nd IEEE Annual Symposium on Foundationsof Computer Science, FOCS 2021, Denver, CO, USA, February 7-10, 2022, pages 954964. IEEE, 2021. doi:10.1109/FOCS52979.2021.00096. URL Badih Ghazi, Ravi Kumar, and Pasin Manurangsi. Differentially private clustering: Tight approxima-tion ratios.In Hugo Larochelle, MarcAurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, andHsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference onNeural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.URL Badih Ghazi, Xiao Hu, Ravi Kumar, and Pasin Manurangsi.On differentially private sam-pling from gaussian and product distributions.In Alice Oh,Tristan Naumann,AmirGloberson,Kate Saenko,MoritzHardt,and Sergey Levine,editors,Advances inNeuralInformation ProcessingSystems 36:AnnualConferenceonNeural Information ProcessingSys-tems 2023,NeurIPS 2023,New Orleans,LA, USA, December 10 - 16,2023,2023.URL Gautam Kamath, Jerry Li, Vikrant Singhal, and Jonathan R. Ullman.Privately learning high-dimensional distributions.In Alina Beygelzimer and Daniel Hsu, editors, Conference on LearningTheory, COLT 2019, 25-28 June 2019, Phoenix, AZ, USA, volume 99 of Proceedings of Machine Learning Re-search, pages 18531902. PMLR, 2019. URL Gautam Kamath, Vikrant Singhal, and Jonathan R. Ullman. Private mean estimation of heavy-tailed dis-tributions. In Jacob D. Abernethy and Shivani Agarwal, editors, Conference on Learning Theory, COLT2020, 9-12 July 2020, Virtual Event [Graz, Austria], volume 125 of Proceedings of Machine Learning Re-search, pages 22042235. PMLR, 2020. URL Vishesh Karwa and Salil P. Vadhan.Finite sample differentially private condence intervals.In Anna R. Karlin,editor,9th Innovations in Theoretical Computer Science Conference,ITCS2018, January 11-14, 2018, Cambridge, MA, USA, volume 94 of LIPIcs, pages 44:144:9. SchlossDagstuhl - Leibniz-Zentrum fur Informatik, 2018.doi:10.4230/LIPICS.ITCS.2018.44.URL",
  "The same holds for both variants of (m,)-sampling": "Proof Sketch of Lemma 11. Accuracy Analysis. The DensestBall algorithm by Ghazi et al. (2020, Theorem6) is the main ingredient of this reduction. If there is a ball of radius r that contains a majority of thedataset, then DensestBall given r as input, outputs a ball of radius O(r) that also contains a majority ofthe dataset. Since I, we can use a concentration inequality for Gaussians to set r = O( d). Letc be the center of the ball that DensestBall outputs. This algorithm has a guarantee that || c|| r holdswith high probability. Shifting each input sample points, X to X c, reduces to the case where |||| ri.e., the mean is bounded. Privacy Analysis. The DensestBall algorithm has both pure DP and approximate DP variants. Let usassume we run the pure DP (approximate DP) variant with privacy budget (or (,) respectively). Weare given that (,)-DP algorithm for the bounded mean case exists. Using basic composition gives usthat the algorithm for the unbounded mean case is (2,)-DP (or 2, + )-DP respectively.The pure DP variant of DensestBall has a sample complexity of O(d log(d)/) whereas the approximateDP variant has a sample complexity of O(",
  "BDP Weak Multi-Sampling for Finite Distributions, with Hints": "Previously, we assumed that all distributions in consideration D D have support S where S [k].Here, we consider cases where other information about S is known. We effectively reduce to the knownsupport case by using established algorithms to identify a superset of the support.The central idea is that as long as we can nd an interval which contains more than 1 fraction ofthe probability mass of the distribution D, we can run ShuRR on that interval.",
  "B.1Contiguous and Bounded Support": "Some contiguous interval of width k contains S. In this case, we use the classic stability-based his-togram algorithm of Bun et al. (2016)which we call noise-and-thresholdto nd the mode v of D andthen run ShuRR assuming [v k,v + k] is the support. The number of samples to nd v is O( k",
  "B.2Bounded Support": "We assume only |S| k.Option 1 (non-adaptive): Run the noise-and-threshold algorithm to identify a subset E of the support,such that the items that are missing each have mass at most /k so that the distribution conditioned onE is within of the original. The number of samples to do this is O( k log(k/))Option 2 (adaptive): Find an element v from the support of D using the noise-and-threshold algorithm,then run the above-threshold algorithm to nd h such that [vh,v+h] contains 1O() fraction of samples,then run ShuRR assuming [v h,v +h] is the support. The number of samples to nd v is O( k",
  "B.4Sub-Gaussian distribution": "In this case, the distribution D is a discrete sub-Gaussian distribution over Z with known varianceproxy .Due to the sub-Gaussian property of D, most of the probability mass is concentrated in an interval Iof size 2 centered at the true mean of D. In our algorithm, we divide Z into bins of size O(). Theinterval I mentioned above will intersect at most 2 bins. Running the noise-and-threshold using these binswill help us identify these two bins giving us a rough estimate of where lies. Now, we can extend thisrough estimate of where lies on both sides by O( log1/) to create a new interval which contains atleast 1 fraction of the probability mass of D. We then run ShuRR using this new interval.",
  "log 1": ") i.i.d samples from a distribution D with support size k, theprobability that the noise-and-threshold algorithm run with privacy parameters ,/k nds every heavy elementof D with non-zero frequency is at least 1 2. Proof. Consider any arbitrary heavy element e in the support of D. For noise-and-threshold to report e,the noisy count of e must be non-zero. We know an element will be dropped if its count is less than thethreshold of t = O( 1",
  "log k": ")Notice that, since e is a heavy element, its probability mass is at least /k. Using the same logic as inthe previous lemma, we can use a multiplicative Chernoff bound to show that our choice of n ensuresthat e will have count which exceeds 2t in X. except with probability /k. We also know that addingLaplace noise to 2t will result in a value less than t, except with probability /k. A union bound overthese two events, tells us that noise-and-threshold will nd e, except with probability 2/k.There can be at most k heavy elements in the support, as the support size k. Union bounding overat most k heavy elements completes the proof.",
  "CGaussians with Bounded Covariance": "In this section, we show the existence of single sampling algorithms for Gaussians with boundedcovariance i.e. Nd( , ) under zCDP . We show that an approximate DP algorithm proposed byGhazi et al. (2023, Algorithm 3) satises zCDP. Weak and strong multi-sampling algorithms can be de-rived by way of Lemmas 8 and 9.",
  "D.2Sampling algorithm": "Lemma 36. Suppose there exists a Gaussian oracle that, when given > 0, produces a sample from N(0,2).Also suppose there exists a Gamma oracle that, when given shape parameter k > 0 and scale parameter > 0,produces a sample from (k,). Then there exists an algorithm that, when given b > 0, produces a sample fromd-dimensional ELap(b) by making 1 call to the Gamma oracle and d calls to the Gaussian oracle.",
  "Lemma 37. Let Rd such that ELap(b), then2 Gamma(d,1/b)": "Proof. FELap(b)() depends on the norm of i.e. ||||2 and is independent of the direction . Thus theprobability density of the norm ||||2 at distance r from the origin, denoted by F(r), can be found byintegrating FELap(b)() over all that lie on the surface of d 1 dimensional sphere of radius r denotedby Sd1(r). Since the pdf FELap(b) is constant over the surface of the sphere Sd1(r), F(r) is equal to theprobability density of where ||||2 = r times the surface area of Sd1(r).",
  "k": "Proof. We know from fact 38 that X can be decomposed into independent and identically distributedrandom variables X1,...Xk such that each Xi Exp(). For X to be greater than the threshold t, at leastone of the k Xis has to be greater than t/k by a simple averaging argument. Thus,"
}