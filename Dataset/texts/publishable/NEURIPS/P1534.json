{
  "Abstract": "Large-scale cell microscopy screens are used in drug discovery and molecularbiology research to study the effects of millions of chemical and genetic pertur-bations on cells. To use these images in downstream analysis, we need modelsthat can map each image into a feature space that represents diverse biologicalphenotypes consistently, in the sense that perturbations with similar biologicaleffects have similar representations. In this work, we present the largest founda-tion model for cell microscopy data to date, a new 1.9 billion-parameter ViT-G/8MAE trained on over 8 billion microscopy image crops. Compared to a previ-ous published ViT-L/8 MAE, our new model achieves a 60% improvement inlinear separability of genetic perturbations and obtains the best overall perfor-mance on whole-genome biological relationship recall and replicate consistencybenchmarks. Beyond scaling, we developed two key methods that improve perfor-mance: (1) training on a curated and diverse dataset; and, (2) using biologicallymotivated linear probing tasks to search across each transformer block for thebest candidate representation of whole-genome screens. We find that many self-supervised vision transformers, pretrained on either natural or microscopy images,yield significantly more biologically meaningful representations of microscopyimages in their intermediate blocks than in their typically used final blocks. Morebroadly, our approach and results provide insights toward a general strategy forsuccessfully building foundation models for large-scale biological data.1",
  "Introduction": "Large-scale cell microscopy assays are used to discover previously unknown biological processes(Przybyla & Gilbert, 2022; Bock et al., 2022; Rood et al., 2024) and identify novel drug candi-dates and targets (Vincent et al., 2022). Labs are now able to achieve extremely high throughput byleveraging high content screening (HCS) systems that combine automated microscopy with roboticliquid handling (Boutros et al., 2015). Extracting meaningful features from microscopy images inlarge-scale screens has become increasingly difficult as this scale has increased. Public datasetslike RxRx3 (Fay et al., 2023) and JUMP-CP (Chandrasekaran et al., 2023) now include millions ofcellular images across 100,000s of unique chemical and genetic perturbations. In addition to limita-tions in expressiveness of the features that can be derived from them, traditional methods relying oncustomized pipelines for segmentation, feature extraction, and downstream analysis (Caicedo et al.,2017) struggle to handle this scale effectively (Chandrasekaran et al., 2021; Carpenter et al., 2006).",
  "arXiv:2411.02572v1 [cs.LG] 4 Nov 2024": "The size and complexity of large-scale microscopy data demands image models that can extract richbiological features and do so consistently across experimental replicates, both of which are crucialfor downstream biomedical applications. Rich, biologically meaningful representations reveal rela-tionships between genes or compounds to drive the discovery of novel targets and drug candidates,while consistency in features extracted across replicates ensures that findings are reproducible andreliable for therapeutic development. Foundation models have been developed for representing high-dimensional unstructured biologicaldata such as protein structures (Jumper et al., 2021) and transcriptomics (Hao et al., 2024), but thescale and dimensionality of large-scale microscopy data present unique challenges for generatingrepresentations that are both biologically informative and consistent across replicates. HCS datasetsare often confounded by complex noise known as batch effects (Caicedo et al., 2017), stemmingfrom differences between experimental batches and biological variability. These batch effects including natural variation in cell populations obscure the biological effects of perturbations andmake it challenging to isolate the specific effects of the perturbations applied (Yang et al., 2019).Overcoming these obstacles with a model capable of generating robust, biologically meaningfulrepresentations can empower HCS to systematically interrogate gene function and identify noveldrug candidates (Rood et al., 2024). State-of-the-art (SOTA) deep learning methods for microscopy leverage Vision Transformers(ViT) (Dosovitskiy et al., 2020) trained with self-supervised learning (SSL) techniques (Balestrieroet al., 2023) to learn unbiased representations from large-scale screens (Doron et al., 2023; Kim et al.,2023; Bourriez et al., 2024). Recent studies have demonstrated that ViTs trained as Masked Autoen-coders (MAEs) (He et al., 2022) can effectively scale beyond previous approaches and outperformvarious supervised and smaller SSL models in capturing biologically informative representations ofcell images (Kraus et al., 2024). However, the level of consistency found in these representationsacross a large number of experimental replicates was not previously reported. Furthermore, com-pared to recent multi-billion parameter transformers developed for natural images (Dehghani et al.,2023) and natural language (Llama3, 2024), model scale in microscopy lags behind (Kraus et al.,2024; Chen et al., 2023a) despite the existence of massive datasets. In this work, we developed the largest foundation model to date for cell microscopy images, achiev-ing SOTA results in both replicate consistency and biological recall of known gene-gene relation-ships. Specifically our work offers the following contributions: We demonstrate that training on a curated microscopy dataset of statistically significantpositive samples, named Phenoprints-16M, improves both recall of known gene-gene re-lationships and consistency of embeddings for gene knockout perturbations (A).We describe components of this curation strategy that can be generalized to other scientificdatasets ( 3.1).",
  "We present a new foundation model, MAE-G/8, a 1.86 billion parameter ViT-G/8 MAEtrained on Phenoprints-16M over 48,000 H100 GPU hours on more than 8 billion samplesfrom the curated dataset (A, 3.2)": "We propose new set of biological linear probing tasks to evaluate representations learnedby intermediate ViTs blocks for microscopy data ( 4). Performance on these linear probingtasks are strongly correlated with performance on important whole-genome scale evalua-tion metrics while requiring significantly less resources to compute (). We find that using intermediate layers leads to better performance on these down-stream whole-genome benchmarks at a lower computational inference cost, across SSLViTs trained on microscopy or natural images. By taking advantage of our linear probingproxy task, we are able to cheaply find the best performing intermediate block (Eq. 1). Our results indicate that the biological scaling properties first identified by Kraus et al. (2023) extendto the multi-billion parameter regime ( A.9). We show that our MAE-G/8 model produces a nearly60% more phenotypically linearly separable latent space compared to previous approaches usingthe final block of MAE-L/8 (), correlating with significant improvements in both recall andreplicate consistency when benchmarking across the whole genome (B).",
  "Replicate": "consistency AB Curated dataModel scalingBlock search Proxy task Block position 307M 1.9B MAE L/8 MAE G/8 : (A) Overview of performance gain from different foundation model pretraining and infer-ence strategies. (B) Example whole-genome results for replicate consistency and biological rela-tionship recall on StringDB for models trained with different combinations of strategies, by modelname and dataset (left to right): MAE-L/8 (RPI-93M, block b = 24), MAE-L/8 trimmed to blockb = 15, MAE-L/8 (Phenoprints-16M, block b = 20), MAE-G/8 (Phenoprints-16M, b = 38),where b is the optimal block according to linear probes as defined in Equation 1.",
  "Related work": "Evaluating Representations for Drug Discovery.Evaluating the quality of biological represen-tation learning methods for drug discovery remains challenging, as ground truth data is sparse, noisy,biased to well-studied diseases and pathways, and poorly annotated. Metrics have been proposedthat use mean average precision (Kalinin et al., 2024) or AUC ROC (Sivanandan et al., 2023) toassesses how similar related samples are represented, including replicates of the same perturbationor different perturbations with similar annotated biological activities. Recently, Celik et al. (2024)introduced terminology for describing perturbative maps of biology, in which representations ofperturbations in HCS data can be placed in unified, relatable embedding spaces allowing for thegeneration of genome-scale sets of pairwise comparisons. Here we leverage the biological relation-ship recall benchmark proposed by Celik et al. (2024), which assess how well known relationshipsbetween pairs of perturbations are recalled among the most similar or dissimilar embeddings. Com-puting reliable versions of these relationship benchmarks with HCS data is particularly expensive asthey require genome-wide embeddings to be inferred for hundreds of millions of image crops fromthe genome-wide RxRx3 microscopy screen (Fay et al., 2023). Dataset Curation for Foundation Models.Dataset curation is crucial for enhancing the effi-ciency of foundation models, especially in large-scale contexts. Usual approaches to dataset con-struction are inspired by the image retrieval community (Weinzaepfel et al., 2022; Radenovic et al.,2018; Berman et al., 2019). Existing methods often utilize pre-trained models for filtering and prun-ing, such as vision-language models to discard irrelevant pairs (Schuhmann et al., 2021), semanticdeduplication to remove redundancy (Abbas et al., 2023), and prototypicality-based approaches toretain representative data (Sorscher et al., 2022). However, these techniques are less effective forHCS, where redundancy, variability, and subtle morphological differences make conventional filter- ing challenging. Our work addresses these limitations by building on Celik et al. (2024)s perturba-tion consistency framework to curate a balanced dataset of images across semantic classes, which isvital for effective learning under the masked objectives (Zhang et al., 2022). Layer-wise Analysis of Deep Neural networks.Recent work suggests that intermediate layers(or, blocks) in large ViTs may achieve superior performance on certain linear probing tasks com-pared to the final encoder layer (Evci et al., 2022; Dehghani et al., 2023). For example, Alkinet al. (2024) reported that intermediate layers in large MAE-ViTs (ViT-L, ViT-H) have superiorImageNet-1K k-NN accuracy. They attributed this property to the later encoder layers becomingmore optimized for the reconstruction task.",
  "Training Dataset Curation": "Many academic and industry labs have adopted the Cell Painting imaging protocol (Bray et al.,2016), which multiplexes fluorescent dyes to reveal eight broadly relevant cellular components.The datasets used here contain a six-channel implementation of Cell Painting (), as wellas brightfield images, spanning 100,000s of chemical and genetic perturbations applied to dozensof cell types (Kraus et al., 2024). In these datasets, cells that look like unperturbed cells tend tobe very over-represented because many perturbations do no induce a morphological change. Somemorphological changes are also far more common (e.g. many perturbations will kill cells, resultingin a relatively high proportion of dead cell morphological phenotype). This results in significantimbalance in the morphological phenotypes that the models learn to reconstruct. To address this, we constructed an aggressively curated training dataset ( A.1). To learn an initialrepresentation, we began by reproducing the MAE-L/8 model of Kraus et al. (2024) on a dataset ofsimilar size consisting of 93 million HCS images. Using this representation, we first filtered pertur-bations that did not induce consistent morphological changes to cells. To perform this filtering, weutilized Celik et al. (2024)s non-parametric perturbation consistency test ( A.3) Typical VariationNormalization (Ando et al., 2017a; Kraus et al., 2024). This test was applied within each experimentfor computational efficiency, and we restricted the analysis to wells containing single perturbations.This consistency was computed for CRISPR guides, siRNAs, and particular concentrations of smallmolecules across replicates of the same perturbation. P-values were computed for each gene andeach (perturbation, concentration) pair. When multiple experiments existed for the same condition,we combined p-values using the Cauchy Combination test (Liu & Xie, 2018). We repeated this procedure with a weakly supervised learning (WSL) model trained on RxRx1(Sypetkowski et al., 2023) and filtered to perturbations where any condition had a p-value < 0.01in either the MAE-L/8 or WSL model. This process reduced our original dataset of 93M samplesto 16M, which we refer to as Phenoprints-16M. While some redundancy remains when distinctperturbations have the same effect, the proportion of samples with that differ from negative controlsincreased substantially with little decrease in overall diversity. We believe that iteratively repeatingthis process with the best models from previous iterations to guide data selection for subsequentmodels may be a viable strategy.",
  "Models": "Baselines.We compare to several non-finetuned baseline ViT image encoders: three differentDino-v2 backbones (Oquab et al., 2024) (with 4 register tokens (Darcet et al., 2024)) trained on acurated non-biological natural image dataset; a weakly supervised (WSL) classifier ViT-L/16 trainedon Imagenet-21k (Ridnik et al., 2021); a MAE ViT-L/16 trained on Imagenet-21k (He et al., 2022);and an untrained ViT-S/16. Preliminary investigations found that channel-wise self-standardizationworked best as the image normalization preprocessing for these baselines, and that the class tokenwas slightly better than the global pool of the patch tokens (except for MAE). Convolutional weights",
  "in the patch embedding layer were repeated to embed 6 channel images when using models trainedon RGB datasets (Wightman, 2019)": "Prior work.Our primary point of comparison is with respect to the best pretrained foundationmodel presented by Kraus et al. (2024), the MAE-ViT-L/8+ trained on RPI-93Mrained for approxi-mately 40 epochs, learning from over 3.5 billion image crops, using the L2 mean squared error lossfunction plus an additional Fourier domain reconstruction loss term. CA-MAE-S/16 trained on RxRx3.We trained a new channel-agnostic MAE (Kraus et al., 2024)ViT-S/16 on the RxRx3 dataset (Fay et al., 2023) for 100 epochs. Channel-agnostic ViTs tokenizeeach image channel separately with shared patch embedding weights and leverage the dynamicsequence length of transformers with repeated positional encodings to train ViTs that can processimages with varying numbers of channels (Bao et al., 2024; Bourriez et al., 2024; Kraus et al., 2024).Kraus et al. (2024) demonstrate that the large MAEs with 8x8 patch size perform either better or thesame as the 16x16 channel-agnostic variants for consistently 6-channel data, so we opted to trainstandard MAEs for the following two new models since they require fewer tokens at inference time. MAE-L/8 trained on Phenoprints-16M.Holding the model backbone constant compared to theMAE-ViT-L/8 by Kraus et al. (2024), we assess the impact of our curated dataset in contrast to the93M dataset by training a new ViT-L/8 MAE for 500 epochs on Phenoprints-16M. MAE-G/8 trained on Phenoprints-16M.Holding the dataset constant compared to MAE-L/8above, we assess the impact of increased model scale in terms of parameters by training a newViT-Gigantic MAE with nearly 1.9 billion parameters for 500 epochs on Phenoprints-16M. Trainingthis model required 256 H100 GPUs running in parallel for over 1 week. See A.2 for otherhyperparameter settings we used for model training.",
  "Linear probing representation learning across ViT blocks": "We improve the quality of our learned image representations by leveraging previous findings thatsuggest intermediate blocks within an encoder can provide better representation compared to thefinal block (Alkin et al., 2024). Unfortunately, it is infeasible to search for the best block by sim-ply performing whole-genome evaluation on each block of a large model because the evaluationis extremely time-consuming and resource intensive. For example, evaluating the final block ofMAE-G/8 required 4,000 L4 GPU hours just for inference ( 5). We demonstrate that using block-wise linear probes provides insights into the quality of biological features extracted by these modelsin their intermediate blocks, allowing us to trim the model to an earlier block to both reduce infer-ence costs and improve representation quality.",
  "(b) Anax functional gene group classification": ": Block-wise validation set linear probe results comparing ViT models pretrained on cellmicroscopy images (left) versus natural images (right). (a) 1139-class RxRx1 SiRNA knockdownclassification (Sypetkowski et al., 2023); (b) 40-class Anax functional gene group classification onHUVEC cell images from RxRx3 CRISPR knockouts (Fay et al., 2023). group that the gene belongs to, and test performance on held-out experiments ( A.4). We definethe optimal block b for a probing task as the block whose output features achieve the highest testbalanced accuracy when trained on the probing task, across all N blocks of the encoder,",
  "where z(b) are output features from block b of a ViT. Performance on our linear probing tasks canbe viewed as a measure of linear separability of a feature space across experimental batches": "RxRx1 1139-class siRNA genetic perturbation classification.We expect high quality represen-tations of cell images to generate similar embeddings for cells with the same perturbation, hencea simple linear probe should be able to predict gene perturbation from these representation reason-ably well. We train linear probes on the publicly-available RxRx1 dataset in Sypetkowski et al.(2023) which consists of 125,510 high-resolution fluorescence microscopy images of human cellsunder 1,138 siRNA-induced gene knockdowns (plus unperturbed controls) across four cell types(HEPG2, HUVEC, U2OS, RPE). These gene knockdowns produce strong phenotypes which makesthe prediction task more feasible. We found that, for MAE-G/8 , the best features came from intermediate block b = 38 (out of 48) ofthe encoder, achieving a balanced accuracy (0.51) that is 8.5% greater compared to its final blocksoutput features (a, left). Additionally, these features achieved 60% greater accuracy than .36 .38 .40 .42 .44 .46 .48 .50 Whole-genome StringDB Recall Pearson r = 0.95Spearman = 0.97 Pearson r = 0.89Spearman = 0.81 .18.20.22.24.26.28.30 Validation accuracy on Anax linear probe (40 classes) .30 .35 .40 .45 .50 .55 .60 .65 Whole-genome Replicate Consistency (KS) Pearson r = 0.97Spearman = 0.91 .10.15.20.25.30.35.40.45.50 Validation accuracy on RxRx1 linear probe (1139 classes) Pearson r = 0.92Spearman = 0.71 CA-MAE-S/16 (RxRx3), b * = 12MAE-L/8 (RPI-93M), b = 24 MAE-L/8 (RPI-93M), b * = 15MAE-L/8 (PP-16M), b = 24 MAE-L/8 (PP-16M), b * = 20 MAE-G/8 (PP-16M), b * = 38MAE-G/8 (PP-16M), b = 48ViT-L/16 MAE, b = 24 ViT-L/16 MAE, b * = 11Dino-S/14, b = 12 Dino-S/14, b * = 5Dino-L/14, b = 24 Dino-L/14, b * = 12Dino-G/14, b = 40 Dino-G/14, b * = 16line of best fit : Correlations between validation set linear probing () on Anax and RxRx1 for bestand last blocks (Eq. 1) compared to downstream whole-genome benchmarks () for biologicalrelationship recall on StringDB at 0.05-0.95 threshold and replicate consistency KS statistic. the typically used final block of MAE-L/8+ (Kraus et al., 2024). We observed similar trends forViT models pretrained on natural images. For example, DINO-G/14 and ViT-L/16 MAE trainedon non-biological natural image data have their best features at blocks that are positioned withinthe first half of the encoder. For ViT-L/16 MAE, the performance of the best block is 27% highercompared to its final block output features that are typically used for downstream tasks. The higherperformance observed for intermediate blocks does not appear to be an intrinsic feature of the ViTarchitecture as an untrained ViT did not exhibit such a parabolic trend (a, right). Anax 40-class functional gene group classification.Biologically meaningful representation ofmicroscopy images of genetically perturbed cells should capture functional relationships betweengenes, hence a simple linear probe should be able to predict functional gene groups when trainedon these representations. We curated a small subset of 80,000 wells from RxRx3 (Fay et al., 2023)to evaluate linear probes on functional group prediction. We also evaluated similar whole genomeknockout screens with ARPE-19 and an additional population of HUVEC cells with soluble TNF-added to all wells. We manually curated Anax, a set of 40 functionally-diverse gene groups con-taining 348 genes, with details provided in ( A.8). Examples of groups include major proteincomplexes (e.g. proteasome, ribosome-small/large), metabolic pathways (e.g. Krebs cycle) and sig-naling pathways (e.g. calcium signaling) (). These groups span broad biological processesthat are conserved across cell types linear separability of these groups would likely indicate thatrepresentations are biologically meaningful regardless of cell type. As shown in b, MAE-G/8 significantly outperforms other models in Anax group linear probeclassification. The best representations once again are obtained from an intermediate block, achiev-ing a balanced accuracy (0.32) that is 5% greater compared to its final blocks output features. We : Multivariate known biological relationship recall and univariate replicate consistencybenchmarks by model, encoding block b, benchmark database, and test statistic. The trimmed mod-els used linear probes to select an earlier block as the feature encoder (). Results are computedover all whole-genome CRISPR knockout perturbation images in RxRx3, after applying TVN andchromosome arm bias correction. For relationship recall, we report results over four databases (Re-act stands for Reactome-PPI (Gillespie et al., 2021)). Best overall result is in bold.",
  "observed similar trends for ViT models pretrained on natural images and representations computedfrom microscopy images of other cell types/conditions ( A.5, )": "In , we observe that performance on this novel linear probing task correlates strongly withdownstream whole-genome benchmarks across all models (), whether they are trained on mi-croscopy data or natural images, achieving an overall rank correlation = 0.97 with whole-genomeStringDB recall and = 0.91 with whole-genome replicate consistency. This strong correlation iscrucial as it allows us to trim our model to the block with the best linear probe performance as a wayto improve the quality of our representations for the whole-genome ().",
  "Whole-genome benchmarking": "presents our benchmarks computed across the whole-genome. These evaluate the genomicrepresentations obtained for each model by aggregating millions of embeddings of cell images span-ning >100,000 of genetic knockout perturbations (17,063 genes 6 single guide RNAs each) onHUVEC cells from RxRx3 (Fay et al., 2023). Computing these benchmarks for HCS screens typi-cally requires inferring 140 million crops from the genome-wide RxRx3 microscopy screen (Krauset al., 2023) (64 tiled crops per each of the 2.2 million wells), but, to reduce compute costs, wediscard the outer ring of crops, leaving the 36 center non-edge crops for each well. This requires80 million forward passes to comprehensively evaluate a new encoder. After inference, we use typ-ical variation normalization (Ando et al., 2017b) and chromosome arm bias correction (Lazar et al.,2023) to post-process the embeddings and aggregate them to the gene-level. We present the multivariate biological relationship recall benchmarks proposed by Celik et al.(2024) and originally evaluated for MAEs by Kraus et al. (2023, 2024). These metrics evaluatehow many annotated pair-wise relationships are recalled from public databases (CORUM, hu.MAP,Reactome-PPI, StringDB) in the extremities of a ranked list of cosine similarities of all pair-wisepost-processed embeddings (details in A.6). To ensure embeddings represent technical replicatesof perturbations consistently, we also evaluate model performance on replicate consistency based 0.00.20.40.60.8",
  "Cosine Similarity": "=0.00 =0.17 Kolmogorov-Smirnov test (KS) = 0.63Cramer Von-Mises test (CVM) = 18.2 MAE ViT-G/8 (b * = 38), pretrained on microscopy images Different perturbation pairs (Null)Same perturbation pairs (Query)Mean (Null)Mean (Query) : Replicate consistency whole-genome results between perturbations from cosine similaritydistributions on RxRx3 post-TVN. Comparing baseline Dino-V2 ViT-G/14 at its typically used finalblock (left) versus MAE-G/8 at the best block found via linear probing (right). on the experimental design used in the RxRx3 dataset. Specifically, we compare the similarity of theembedding for corresponding wells across different experiments via a non-parametric statistical test.The test statistic measures the difference between the perturbation replicates similarity distributionand an empirical null distribution, with larger values indicating greater consistency (details in A.7). In order to compare models, we summarize the resulting statistics over all technical replicates inRxRx3 by taking their median, as reported in columns KS and CVM in , and visualized in. MAEs pretrained on microscopy data show improved performance compared the baselinemodels. Furthermore, training on the Phenoprints-16M dataset improves the performance of theMAEs significantly and trimmed MAE-G/8 achieves the best overall performance. Our linear probing analysis () allowed us to trim our models to better encoding blocks.Comparing models on their best respective blocks, MAE-G/8 improves on MAE-L/8 with a 16%improvement in Anax functional gene group classification (.27.31) and a 24% improvement inRxRx1 perturbation classification (.41.51). Compared to the best published result for whole-genome benchmarks (MAE-L/8 trained on RPI-93M (Kraus et al., 2023)), MAE-G/8 obtains a 20%improvement in replicate consistency KS (.52.63) and 4.3% improvement in StringDB recall(.472.492). When using our linear probes to select outputs from block b = 15 (Equation 1)from that MAE-L/8, the gain for MAE-G/8 changes to 9.2% in KS and 3.5% in StringDB recall. Similarly, linear probing to select optimal ViT blocks led to significant improvements even whenapplied to frozen Dino-V2 based models pretrained on natural images. Dino-V2 ViT-G obtains anearly 20% improvement CORUM recall (.44.53) by using the embeddings extracted at b = 16(chosen by linear probes) rather than the final embedding from b = 40 (which performs worse thana random untrained ViT-S). Dino-V2 ViT-S also observes improvements by using b = 5 rather thanb = 12 and outperforms Dino-V2 ViT-G in replicate consistency.",
  "Discussion and Conclusions": "This work demonstrates that: (1) within the context of biological imaging, trimming many ViTs to anearlier block leads to stronger biological linearity and improved performance on downstream tasksin addition to cheaper inference costs (); (2) linear probing performance on a subset of ge-netic perturbations correlates strongly with downstream performance on whole-genome benchmarksand can be used to optimize which block is selected for representing the whole-genome ();(3) the most scaled model, MAE-G/8 , obtains the overall best performance across all benchmarksand linear probes, providing further evidence for the scaling hypothesis in biological image data (Ta-ble 2). This demonstrates that intentionally scaling training compute and parameters of SSL modelsfor microscopy can benefit downstream biological relationship recall, whole-genome replicate con-sistency, and biological linear separability on smaller datasets (see A.9 for scaling plots). More broadly, this work proposes a reusable recipe for training and extracting optimal representa-tions from fully self-supervised models trained on experimental data. The pattern we use can beapplied to other domains that contain data from repeated experiments but without accurate groundtruth labels. Specifically, we recommend: (1) curating the training set by identifying diverse sets ofsamples that are represented consistently, e.g., by using a pre-existing model to select such samples;(2) training a scaled transformer-based model using a self-supervised learning technique, such as",
  "D. Michael Ando, Cory Y. McLean, and Marc Berndl. Improving Phenotypic Measurements inHigh-Content Imaging Screens. bioRxiv, pp. 161422, 2017b. doi: 10.1101/161422": "Randall Balestriero, Mark Ibrahim, Vlad Sobal, Ari Morcos, Shashank Shekhar, Tom Goldstein, Flo-rian Bordes, Adrien Bardes, Gregoire Mialon, Yuandong Tian, Avi Schwarzschild, Andrew Gor-don Wilson, Jonas Geiping, Quentin Garrido, Pierre Fernandez, Amir Bar, Hamed Pirsiavash,Yann LeCun, and Micah Goldblum. A Cookbook of Self-Supervised Learning. arXiv, 2023. doi:10.48550/arxiv.2304.12210. Yujia Bao, Srinivasan Sivanandan, and Theofanis Karaletsos. Channel vision transformers: Animage is worth 1 x 16 x 16 words.In The Twelfth International Conference on LearningRepresentations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL",
  "Maxim Berman, Herve Jegou, Andrea Vedaldi, Iasonas Kokkinos, and Matthijs Douze. Multigrain:a unified image embedding for classes and instances, 2019. URL": "Christoph Bock, Paul Datlinger, Florence Chardon, Matthew A. Coelho, Matthew B. Dong, Keith A.Lawson, Tian Lu, Laetitia Maroc, Thomas M. Norman, Bicna Song, Geoff Stanley, Sidi Chen,Mathew Garnett, Wei Li, Jason Moffat, Lei S. Qi, Rebecca S. Shapiro, Jay Shendure, Jonathan S.Weissman, and Xiaowei Zhuang. High-content CRISPR screening. Nature Reviews MethodsPrimers, 2(1):8, 2022. doi: 10.1038/s43586-021-00093-4. Nicolas Bourriez, Ihab Bendidi, Ethan Cohen, Gabriel Watkinson, Maxime Sanchez, GuillaumeBollot, and Auguste Genovesio. Chada-vit : Channel adaptive attention for joint representationlearning of heterogeneous microscopy images. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, 2024.",
  "Michael Boutros, Florian Heigwer, and Christina Laufer. Microscopy-Based High-Content Screen-ing. Cell, 163(6):13141325, 2015. ISSN 0092-8674. doi: 10.1016/j.cell.2015.11.007": "Mark-Anthony Bray, Shantanu Singh, Han Han, Chadwick T Davis, Blake Borgeson, Cathy Hart-land, Maria Kost-Alimova, Sigrun M Gustafsdottir, Christopher C Gibson, and Anne E Carpen-ter. Cell Painting, a high-content image-based assay for morphological profiling using multi-plexed fluorescent dyes.Nature Protocols, 11(9):17571774, 2016.ISSN 1754-2189.doi:10.1038/nprot.2016.105. Juan C Caicedo, Sam Cooper, Florian Heigwer, Scott Warchal, Peng Qiu, Csaba Molnar, Aliak-sei S Vasilevich, Joseph D Barry, Harmanjit Singh Bansal, Oren Kraus, Mathias Wawer, LassiPaavolainen, Markus D Herrmann, Mohammad Rohban, Jane Hung, Holger Hennig, John Con-cannon, Ian Smith, Paul A Clemons, Shantanu Singh, Paul Rees, Peter Horvath, Roger G Lin-ington, and Anne E Carpenter. Data-analysis strategies for image-based cell profiling. NatureMethods, 14(9):849863, 2017. ISSN 1548-7091. doi: 10.1038/nmeth.4397. Anne E Carpenter, Thouis R Jones, Michael R Lamprecht, Colin Clarke, In Han Kang, OlaFriman, David A Guertin, Joo Han Chang, Robert A Lindquist, Jason Moffat, Polina Golland,and David M Sabatini.CellProfiler: image analysis software for identifying and quantify-ing cell phenotypes.Genome Biology, 7(10):R100, 2006.ISSN 1465-6906.doi: 10.1186/gb-2006-7-10-r100. Safiye Celik, Jan-Christian Hutter, Sandra Melo Carlos, Nathan H. Lazar, Rahul Mohan, Conor Till-inghast, Tommaso Biancalani, Marta M. Fay, Berton A. Earnshaw, and Imran S. Haque. Build-ing, benchmarking, and exploring perturbative maps of transcriptional and morphological data.PLOS Computational Biology, 20(10):124, 10 2024. doi: 10.1371/journal.pcbi.1012463. URL Srinivas Niranj Chandrasekaran, Hugo Ceulemans, Justin D. Boyd, and Anne E. Carpenter. Image-based profiling for drug discovery: due for a machine-learning upgrade? Nature Reviews DrugDiscovery, 20(2):145159, 2021. ISSN 1474-1776. doi: 10.1038/s41573-020-00117-w. Srinivas Niranj Chandrasekaran, Jeanelle Ackerman, Eric Alix, D Michael Ando, John Arevalo,Melissa Bennion, Nicolas Boisseau, Adriana Borowa, Justin D Boyd, Laurent Brino, et al.Jump cell painting dataset: morphological impact of 136,000 chemical and genetic perturbations.bioRxiv, pp. 202303, 2023. Richard J Chen, Tong Ding, Ming Y Lu, Drew F K Williamson, Guillaume Jaume, Bowen Chen, An-drew Zhang, Daniel Shao, Andrew H Song, Muhammad Shaban, Mane Williams, Anurag Vaidya,Sharifa Sahai, Lukas Oldenburg, Luca L Weishaupt, Judy J Wang, Walt Williams, Long Phi Le,Georg Gerber, and Faisal Mahmood. A General-Purpose Self-Supervised Model for Computa-tional Pathology. arXiv, 2023a. doi: 10.48550/arxiv.2308.15474. Xiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Yao Liu, Hieu Pham, Xu-anyi Dong, Thang Luong, Cho-Jui Hsieh, et al. Symbolic discovery of optimization algorithms.arXiv preprint arXiv:2302.06675, 2023b.",
  "Timothee Darcet, Maxime Oquab, Julien Mairal, and Piotr Bojanowski. Vision transformers needregisters, 2024. URL": "Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer,Andreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Scalingvision transformers to 22 billion parameters. In International Conference on Machine Learning,pp. 74807512. PMLR, 2023. Michael Doron, Theo Moutakanni, Zitong S. Chen, Nikita Moshkov, Mathilde Caron, Hugo Tou-vron, Piotr Bojanowski, Wolfgang M. Pernice, and Juan C. Caicedo. Unbiased single-cell mor-phology with self-supervised vision transformers.bioRxiv, 2023.doi: 10.1101/2023.06.16.545359. URL Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, ThomasUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An im-age is worth 16x16 words: Transformers for image recognition at scale. In International Confer-ence on Learning Representations (ICLR), 2020. Kevin Drew, Chanjae Lee, Ryan L Huizar, Fan Tu, Blake Borgeson, Claire D McWhite, Yun Ma,John B Wallingford, and Edward M Marcotte. Integration of over 9,000 mass spectrometry ex-periments builds a global map of human protein complexes. Molecular Systems Biology, 13(6):932, 2017. ISSN 1744-4292. doi: 10.15252/msb.20167490. Utku Evci, Vincent Dumoulin, Hugo Larochelle, and Michael C Mozer. Head2toe: Utilizing in-termediate representations for better transfer learning. In International Conference on MachineLearning, pp. 60096033. PMLR, 2022. Marta M Fay, Oren Kraus, Mason Victors, Lakshmanan Arumugam, Kamal Vuggumudi, John Ur-banik, Kyle Hansen, Safiye Celik, Nico Cernek, Ganesh Jagannathan, et al. Rxrx3: Phenomicsmap of biology. bioRxiv, pp. 202302, 2023. Marc Gillespie, Bijay Jassal, Ralf Stephan, Marija Milacic, Karen Rothfels, Andrea Senff-Ribeiro,Johannes Griss, Cristoffer Sevilla, Lisa Matthews, Chuqiao Gong, Chuan Deng, Thawfeek Varu-sai, Eliot Ragueneau, Yusra Haider, Bruce May, Veronica Shamovsky, Joel Weiser, Timothy Brun-son, Nasim Sanati, Liam Beckman, Xiang Shao, Antonio Fabregat, Konstantinos Sidiropoulos,Julieth Murillo, Guilherme Viteri, Justin Cook, Solomon Shorser, Gary Bader, Emek Demir, ChrisSander, Robin Haw, Guanming Wu, Lincoln Stein, Henning Hermjakob, and Peter DEustachio.The reactome pathway knowledgebase 2022. Nucleic Acids Research, 50(D1):D687D692, 2021.ISSN 0305-1048. doi: 10.1093/nar/gkab1028. Madalina Giurgiu, Julian Reinhard, Barbara Brauner, Irmtraud Dunger-Kaltenbach, Gisela Fobo,Goar Frishman, Corinna Montrone, and Andreas Ruepp. CORUM: the comprehensive resourceof mammalian protein complexes2019. Nucleic Acids Research, 47(Database issue):D559D563, 2019. ISSN 0305-1048. doi: 10.1093/nar/gky973. Minsheng Hao, Jing Gong, Xin Zeng, Chiming Liu, Yucheng Guo, Xingyi Cheng, Taifeng Wang,Jianzhu Ma, Xuegong Zhang, and Le Song. Large-scale foundation model on single-cell tran-scriptomics. Nature Methods, pp. 111, 2024. Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked au-toencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computervision and pattern recognition, pp. 1600016009, 2022. John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger,Kathryn Tunyasuvunakool, Russ Bates, Augustin Zdek, Anna Potapenko, et al. Highly accurateprotein structure prediction with alphafold. nature, 596(7873):583589, 2021. Alexandr A. Kalinin, John Arevalo, Loan Vulliard, Erik Serrano, Hillary Tsang, Michael Bornholdt,Bartek Rajwa, Anne E. Carpenter, Gregory P. Way, and Shantanu Singh. A versatile informationretrieval framework for evaluating profile strength and similarity. bioRxiv, pp. 2024.04.01.587631,4 2024. doi: 10.1101/2024.04.01.587631. Vladislav Kim, Nikolaos Adaloglou, Marc Osterland, Flavio M Morelli, and Paula A Marin Zapata.Self-supervision advances morphological profiling by unlocking powerful image representations.bioRxiv, 2023. doi: 10.1101/2023.04.28.538691. Oren Kraus, Kian Kenyon-Dean, Saber Saberian, Maryam Fallah, Peter McLean, Jess Leung, Va-sudev Sharma, Ayla Khan, Jia Balakrishnan, Safiye Celik, et al. Masked autoencoders are scalablelearners of cellular morphology. In Neural Information Processing Systems Workshop on Gener-ative AI and Biology (NeurIPS GenBio), 2023. Oren Kraus, Kian Kenyon-Dean, Saber Saberian, Maryam Fallah, Peter McLean, Jess Leung, Va-sudev Sharma, Ayla Khan, Jia Balakrishnan, Safiye Celik, et al. Masked autoencoders for mi-croscopy are scalable learners of cellular biology. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pp. 1175711768, 2024. Nathan H Lazar, Safiye Celik, Lu Chen, Marta Fay, Jonathan C Irish, James Jensen, Conor A Till-inghast, John Urbanik, William P Bone, Genevieve HL Roberts, et al. High-resolution genome-wide mapping of chromosome-arm-scale truncations induced by crispr-cas9 editing. bioRxiv, pp.202304, 2023. Yaowu Liu and Jun Xie. Cauchy combination test: A powerful test with analytic p-value calculationunder arbitrary dependency structures. Journal of the American Statistical Association, 115:393 402, 2018. URL",
  "Team Llama3. The Llama 3 Herd of Models. arXiv, 2024. doi: 10.48550/arxiv.2407.21783": "Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov,Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nico-las Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, MichaelRabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herve Jegou, Julien Mairal, Patrick Labatut, Ar-mand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision,2024. URL",
  "Jennifer E Rood, Anna Hupalowska, and Aviv Regev. Toward a foundation model of causal cell andtissue biology with a perturbation cell and tissue atlas. Cell, 187(17):45204545, 2024": "Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis,Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset ofclip-filtered 400 million image-text pairs. ArXiv, abs/2111.02114, 2021. URL Srinivasan Sivanandan, Bobby Leitmann, Eric Lubeck, Mohammad Muneeb Sultan, PanagiotisStanitsas, Navpreet Ranu, Alexis Ewer, Jordan E. Mancuso, Zachary F Phillips, Albert Kim,John W. Bisognano, John Cesarek, Fiorella Ruggiu, David Feldman, Daphne Koller, EilonSharon, Ajamete Kaykas, Max R. Salick, and Ci Chu. A Pooled Cell Painting CRISPR Screen-ing Platform Enables de novo Inference of Gene Function by Self-supervised Deep Learning.bioRxiv, pp. 2023.08.13.553051, 2023. doi: 10.1101/2023.08.13.553051.",
  "Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, and Ari S. Morcos. Beyond neuralscaling laws: beating power law scaling via data pruning. ArXiv, abs/2206.14486, 2022. URL": "Maciej Sypetkowski, Morteza Rezanejad, Saber Saberian, Oren Kraus, John Urbanik, James Tay-lor, Ben Mabey, Mason Victors, Jason Yosinski, Alborz Rezazadeh Sereshkeh, et al. Rxrx1: Adataset for evaluating experimental batch correction methods. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition, pp. 42844293, 2023. Damian Szklarczyk, Annika L Gable, Katerina C Nastou, David Lyon, Rebecca Kirsch, SampoPyysalo, Nadezhda T Doncheva, Marc Legeay, Tao Fang, Peer Bork, Lars J Jensen, and Chris-tian von Mering. The STRING database in 2021: customizable proteinprotein networks, andfunctional characterization of user-uploaded gene/measurement sets. Nucleic Acids Research, 49(D1):D605D612, 2020. ISSN 0305-1048. doi: 10.1093/nar/gkaa1074. Fabien Vincent, Arsenio Nueda, Jonathan Lee, Monica Schenone, Marco Prunotto, and MarkMercola.Phenotypic drug discovery: recent successes, lessons learned and new directions.Nature Reviews Drug Discovery, 21(12):899914, 2022.ISSN 1474-1776.doi: 10.1038/s41573-022-00472-w.",
  "RossWightman.Pytorchimagemodels. 2019": "Samuel J. Yang, Scott L. Lipnick, Nina R. Makhortova, Subhashini Venugopalan, Minjie Fan,Zan Armstrong, Thorsten M. Schlaeger, Liyong Deng, Wendy K. Chung, Liadan OCallaghan,Anton Geraschenko, Dosh Whye, Marc Berndl, Jon Hazard, Brian Williams, ArunachalamNarayanaswamy, D. Michael Ando, Philip Nelson, and Lee L. Rubin. Applying Deep NeuralNetwork Analysis to High-Content Image-Based Assays. Slas Discovery, 24(8):829841, 2019.ISSN 2472-5552. doi: 10.1177/2472555219857715. Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers.In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.1210412113, 2022.",
  ". Filtering out data with missing information about the perturbations applied, data with morethan 3 perturbations applied, and data of unusual size (in the image dimension or numberof channels)": "3. Filtering out perturbation conditions that had been in less than 3 distinct experiments or 20distinct wells so as to capture a variety of batch effects and have a broad sample of positivesper class. 4. Under-sampling perturbation conditions that were clearly over-represented in the dataset.Our experiment designs contain positive controls, negative controls, and wells without per-turbation within each experiment. At this step, we keep 10% of positive controls and wellswithout any perturbation, 30% of negative controls, and all other perturbation conditions.",
  "A.2Training hyperparameters": ": Training hyperparameters for the new models presented in this work. Each used a one-cycle cosine learning rate decay schedule with 10% warm-up using the Lion optimizer from Chenet al. (2023b) with betas (0.9, 0.95) and weight decay of 0.05, with additional ViT settings such asLayerScale as proposed by Dehghani et al. (2023). Note that MAE-G/8 had multiple restarts duringtraining due to challenges associated with massive model training on large-scale shared distributedcompute clusters.",
  "HyperparameterCA-MAE-S/16MAE-L/8MAE-G/8": "Vision transformer backboneViT-SViT-LViT-G (Zhai et al., 2022)Pretraining DataRxRx3Phenoprints-16MPhenoprints-16MTraining epochs100500500Learning rate1e-43e-53e-5Global batch size2048163848192Stochastic depth0.10.30.6# GPUs16 A100s128 H100s256 H100s# GPU-hours40015,36048,000 provides the hyperparameters used for training the new vision transformers presented in thiswork. Each model was trained using a 75% mask ratio and the standard decoder architecture forMAEs (He et al., 2022). Each model was trained with the standard L2 MAE loss and the Fourier-space loss function implemented by Kraus et al. (2024) with a weight of = 0.01. We note,however, that the details presented by Kraus et al. (2024) do not precisely correspond with the im-plementation provided in their Github repository; when reshaping the tokens to a shape compatiblewith the 2D Fourier transform, the permute operation resulted in adjacent pixels being from differentchannels of the input, resulting in the high frequency components of the loss being a function of therelationships between input channels. An initial investigation with a ViT-L/8 showed that changingthe implementation to the one described in the paper did not dramatically change probing results.As such, we used the implementation as-is and leave additional analysis of loss function design forMAEs to future work.",
  "A.3Perturbation Consistency": "In order to assess the consistency of the induced morphology on the cells by the perturbations, weused a non-parametric perturbation consistency test similar to the one introduced in Celik et al.(2024). Let xg,1, xg,2, , xg,n be the embeddings for replicates of perturbation xg on experiment(batch) e. As the test statistic for perturbation consistency, seg is defined as the mean of the cosinesimilarities across all pairs of replicates of xg.",
  "A.4Training linear probes": "In this section, we provide details about the training process and preprocessing steps used in ourlogistic regression models. These models were trained on output features derived from variousVision Transformer (ViT) blocks. The data was split by experiments, ensuring that the test data originated from experiments distinctfrom those used for training. This approach helps to validate the generalization performance of ourmodels across different experimental conditions. For both RxRx1 gene prediction and Anax group prediction, we apply StandardScaler from thescikit-learn library as the only preprocessing step to standardize the features prior to training linearprobes. StandardScaler transformation was fitted on data from the train split. We trainedthe logistic regression models using scikit-learns LogisticRegression class. The followingparameters and settings were used during model optimization:",
  "Class Weight: balanced": "For RxRx1 gene prediction, we trained logistic regression models to predict one of 1139 possibleperturbation labels (1138 genetic perturbation and non-perturbed control). For Anax group pre-diction, we trained logistic regression models to predict one of 40 possible function group labels( A.8). We report the balanced test accuracy as the main evaluation metric for all linear probingexperiments.",
  "A.5Anax classification for other cell lines/treatment conditions: ARPE19 and HUVEC withTNF-alpha background": "We performed linear probing on imaging data obtained for a retinal pigment epithelia (RPE) cell line,ARPE19, and HUVEC cells treated with an inflammatory cytokine, TNF. We similarly observedthat intermediate blocks often have the most linearly separate features compared to the final block. 0.00.20.40.60.81.0 Normalized block position 0.15 0.20 0.25 0.30 Balanced accuracy ARPE19 MAE-G/8MAE-L/8 (PP-16M)MAE-L/8 (RPI-93M)CA-MAE-S/16 0.00.20.40.60.81.0 Normalized block position 0.10 0.15 0.20 0.25 Balanced accuracy HUVEC + TNF MAE-G/8MAE-L/8 (PP-16M)MAE-L/8 (RPI-93M)CA-MAE-S/16 : Layerwise validation set linear probe performance on Anax functional gene group classi-fication beyond RxRx3: CRISPR knockouts in the ARPE-19 immortalized epithelial cell-line (left),and in HUVEC cells with a TNF- background (right).",
  "A.6Biological Relationship Recall": "A valuable use of large-scale HCS experiments is to perform large-scale inference of biological rela-tionships between genetic perturbations. We evaluate each models ability to recall known relation-ships by using the biological relationship recall benchmark described in Celik et al. (2024). First,we correct for batch effects using Typical Variation Normalization (TVN) (Ando et al., 2017b), andalso correct for possible chromosome arm biases known to exist in CRISPR-Cas9 HCS data (Lazaret al., 2023). To infer biological relationships, we compute the aggregate embedding of each per-turbation by taking the spherical mean over its replicate embeddings across experiments. We usethe cosine similarity of a pair of perturbation representations as the relationship metric, setting theorigin of the space to the mean of negative controls. We compare these similarities with the rela-tionships found in the following public databases: CORUM (Giurgiu et al., 2019), hu.MAP (Drewet al., 2017), Reactome (Gillespie et al., 2021), and StringDB Szklarczyk et al. (2020) (with >95%combined score). reports the recall of known relationships amongst the top and bottom 5%of all cosine similarities between CRISPR knockout representations in RxRx3 (Fay et al., 2023).",
  "A.7Replicate Consistency": "In order to assess the reproducibility of the perturbations across their technical replicates, we com-pare the distributions of the similarities for same perturbations across replicates against an empiricalnull distribution. Specifically, for technical replicate experiments eia and eib, we calculate the cosinesimilarity between the embeddings of perturbation xj in them, denoted as sxj.The query distributionqei is constructed by computing the cosine similarities for all perturbations that have a matching wellon experiments eia and eib. An empirical null distribution of identical cardinality is created by com-puting cosine similarity, rxk,xl, between random pairs from eia and eib such that no pair correspondsto the same perturbation, pei0 . Using non-parametric statistical tests, namely Kolmogorov-Smirnov(KS) and Cramer Von-Mises (CVM), we can evaluate the hypothesis that qei and pei0 are drawnfrom the same distribution. Formally, let Qei(x) and P ei0 (x) be the cumulative distribution func-tions for qei and pei0 respectively, then the KS statistic for the two-sample case of technical replicateexperiments eia and eib is defined as:",
  "V-ATPaseATP6V1A, ATP6V, ATP6V1D, ATP6V1E1, ATP6V1F, ATP6V1H": "Since the pairs are randomly selected for pei0 , the embeddings would be mostly orthogonal thus thedistribution would be centered around 0, similar to what illustrates. Given that not allCRISPR knockouts would induce a morphological change in the cells, its plausible for distributionqei to exhibit a peak around 0. As the model approaches the precision of an oracle, we wouldanticipate the mass situated around this peak to shift towards higher cosine similarity values.",
  "A.8Anax Group Prediction Details": "The Anax probing task introduced in this paper is intended to balance capturing a diverse range ofbiology that is broadly conserved between cell types with a reduced cost of execution. The nameAnax is a reference to Anaximander, the 6th century B.C. philosopher credited with making thefirst world map. In curating these genes, we analyzed the sources listed in A.6 as well as internal gene expressiondata to produce functional groups corresponding to biological processes, cellular components, andmolecular functions. Not all genes within each group are expected to have the same knockout phe-notype, but are classified by humans as having related function linear separability of these geneswould indicate that a model has learned similar concepts to those deemed significant by biologists.",
  "The gene groups we use for the 40-class Anax group classification task ( A.4) are listed in": "FLOPS (Log Scale) 0.55 0.56 0.57 0.58 0.59 0.60 0.61 0.62 CORUM recall FLOPS vs CORUM recall (Pearson: 0.99) FLOPS (Log Scale) 0.38 0.39 0.40 0.41 0.42 0.43 0.44 hu.MAP recall FLOPS vs hu.MAP recall (Pearson: 0.98) FLOPS (Log Scale) 0.23 0.24 0.25 0.26 Reactome PPI recall FLOPS vs Reactome PPI recall (Pearson: 0.99) FLOPS (Log Scale) 0.43 0.44 0.45 0.46 0.47 0.48 0.49 StringDB recall FLOPS vs StringDB recall (Pearson: 1.00) FLOPS (Log Scale) 0.475 0.500 0.525 0.550 0.575 0.600 0.625 Replicate Consistency (KS) FLOPS vs Replicate Consistency (KS) (Pearson: 1.00) FLOPS (Log Scale) Replicate Consistency (CVM) FLOPS vs Replicate Consistency (CVM) (Pearson: 1.00) FLOPS (Log Scale) 0.22 0.24 0.26 0.28 0.30 Anax linear probe accuracy FLOPS vs Anax linear probe accuracy (Pearson: 0.94) FLOPS (Log Scale) 0.2 0.3 0.4 0.5 RxRx1 linear probe accuracy FLOPS vs RxRx1 linear probe accuracy (Pearson: 1.00) CA-MAE-S/16 (b * = 12, RxRx3)MAE-L/8 (b * = 15, RPI-93M)MAE-L/8 (b * = 20, PP-16M)MAE-G/8 (b * = 38, PP-16M)",
  "A.9Correlation between model scale and benchmark results": "In we show the correlations between training FLOps (floating point operations) and down-stream results. Over all benchmarks we observe a very strong consistent linear trend where scalingtraining FLOps improves overall pwerformance. This work provides the next log step in scale aswe enter into the billion-parameter model regime with MAE-G/8. These results therefore provideadditional evidence that the trend initially discovered by Kraus et al. (2023) between FLOps andrelationship recall actually extends both to billion-parameter models and even moreso for other bio-logically meaningful benchmarks pertaining to linear probes on small experiments and to replicateconsistency on the whole-genome."
}