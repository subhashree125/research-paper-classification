{
  "Abstract": "Advances in large language models (LLMs) have spurred research into enhancingtheir reasoning capabilities, particularly in math-rich STEM documents. WhileLLMs can generate equations or solve math-related queries, their ability to fullyunderstand and interpret abstract mathematical symbols in long, math-rich docu-ments remains limited. In this paper, we introduce STEM-POM, a comprehensivebenchmark dataset designed to evaluate LLMs reasoning abilities on math symbolswithin contextual scientific text. The dataset, sourced from real-world ArXiv docu-ments, contains over 2K math symbols classified as main attributes of variables,constants, operators, and unit descriptors, with additional sub-attributes includingscalar/vector/matrix for variables and local/global/discipline-specific labels forboth constants and operators. Our extensive experiments show that state-of-the-artLLMs achieve an average of 20-60% accuracy under in-context learning and 50-60% accuracy with fine-tuning, revealing a significant gap in their mathematicalreasoning capabilities. STEM-POM fuels future research of developing advancedMath-AI models that can robustly handle math symbols.",
  "Introduction": "Large language models (LLMs) have demonstrated exceptional reasoning abilities across numerousfields . With the increasing shift towards applying LLMs to complex tasks, the need for supplementary data beyond the general pre-trained datasets has becomeincreasingly important. Among these, mathematical reasoning tasks have recently drawn theattention of several researchers (see Backgrounds in Appendix B). In particular, Part-of-Math Tagging , the mathematical analog to part-of-speech tagging where mathematicaltokens are classified according to a given taxonomy of attributes, continues to gain interest but lacksthe foundational datasets that can support advanced NLP tasks . In addition, integratingmathematical language into NLP models remains a substantial challenge , especially in therealm of document parsing . Traditional semantic parsing methods like LateXML orarXMLiv often fall short when applied to math-rich documents, where precision and structuredsyntax are paramount . These methods struggle to accurately perform pattern matchingbetween abstract mathematical symbols and their corresponding XML tag notations. Similarly,recent advanced LLMs, such as ChatGPT , also face difficulties in understanding and reasoningwith abstract mathematical symbols due to their contextual polymorphism (as shown).",
  "arXiv:2411.00387v1 [cs.CL] 1 Nov 2024": ": The overall pipeline for constructing the STEM-POM dataset. We extract math symbolswith corresponding text information to formulate the dataset. Each math symbol is initially classifiedinto one of four primary categories based on its definition. Then, the symbol is further categorizedinto secondary categories by the context in which it appears or by the symbols dimensionality. AnLLM is evaluated via the first-level and second-level classification tasks. For example, in the linear equation: y = mx + p, y is categorized as a variable. Whereas in thecross-entropy loss function: L(x, y) = Ni=1 xi log(yi), the symbol y represents the fixed targetlabels, which is considered a constant for a given dataset. Without the corresponding contextualinformation of a mathematical symbol, LLMs are unable to distinguish between different attributesof the symbol and cannot effectively process related mathematical reasoning tasks. Thus, taggingmath symbols within domain-specific contexts is essential for language models. In this paper, we introduce a novel benchmark dataset, STEM-POM, designed to evaluate thereasoning capabilities of language models on mathematical symbols across different domains. TheSTEM-POM dataset consists of 2,109 instances extracted from a random sampling of over 10,000arXiv manuscripts, which are math-rich documents spanning domains such as Mathematics, Physics,Chemistry, and more. We provide a mathematical symbol for each dataset instance, its order inthe document, its main and sub-level attributes, and the corresponding text or expression from theoriginal arXiv paper containing the symbol. Each mathematical symbol in the dataset is classifiedaccording to two levels of attributes . The first-level attribute categorizes the symbol as variable,constant, operator, or unit descriptor. The second-level attribute further classifies the symbol into oneof six types based on its first-level category: scalar, vector, matrix, local, global, or discipline-specific. illustrates the datasets category distribution. To further enrich the STEM-POM datasetwith additional arXiv manuscripts and other math-rich document resources, we also design theSTEM-PoM Labeler, a feasible method for assisting dataset generation by automatically searching,extracting, and recording hand-labeled mathematical symbols and their corresponding context fromlong-text documents. We conduct thorough experiments on the STEM-POM dataset to assess the mathematical reasoningabilities of seven open- and closed-source language models, including LSTM , Mixtral-8x7B, Llama2-13B , Llama3-80B , Claude-3.5-sonnet , GPT-3.5, and GPT-4o withvarious prompting and fine-tuning strategies. The experimental results indicate that STEM-POMdistinguishes the performance levels across different LLMs, offering insights into the mathematicalsymbol reasoning abilities of these models. In addition, we investigate and analyze the influence ofcontext length on the ability of language models to understand mathematical symbols.",
  ": Discipline Distribution from Source ArXiv": "from the mathematical token definition extraction benchmark, MTDE . 2. Raw ArXiv papers, where we apply the STEM-PoM Labeler to identify and extract mathematical symbols from theArXiv dataset. We include a detailed description of each source dataset in Appendix A.2. Dataset Construction. After obtaining the mathematical symbols, we categorize each symbolinto different attributes and assign corresponding information to construct the STEM-POM dataset.Specifically, we first extract the file name and symbol order for each mathematical symbol. Then,for each symbol, we extract the contexts in which the symbol appears, using several predefinedlengths. Following this, we manually classify each symbol into four primary categories: Variable,Constant, Operator, and Unit Descriptor. For Variable, Constant, and Operator, we further categorizethem into sub-level categories. The variable is classified as Vector, Scalar, or Matrix, while Constantand Operator are categorized as Local, Global, or Discipline-Specific. outlines the overalldataset structure. We manually examine each entry of the dataset thoroughly to ensure its robustnessand correctness. We provide a detailed explanation of the dataset structure in Appendix A.3 andthe definitions of each levels attributes in Appendix A.4. Additionally, the STEM-PoM Labeler isdescribed in Appendix A.5.",
  "Dataset Statistics": "We summarize the key statistics of our dataset in this section. presents the categorical statis-tics, including the math symbols along with their first- and second-level attributes. The distributionof Variables, Constants, Operators, and Unit Descriptors is 58.5%, 18.2%, 17.2%, and 6.1%, respec-tively. In addition, illustrates the discipline distribution of the source arXiv papers. Ourdataset covers mathematical symbols from various fields, including Mathematics, Physics, Chemistry,Economics, Computer Science, etc.",
  "Evaluation Metrics. We apply the Precision Accuracy as our metric for the mathematical symbolclassification task, the metric can be formulated as: Precision Accuracy = Number of correct predictions": "Total number of samplesTraining & Inference Details. We evaluate several models under both pre-training and fine-tuningsettings. Specifically, we train an LSTM model with varying layers and apply the LoRA method, a PEFT technique, to GPT-3.5. We evaluate other models in the in-context learning setting.Appendix C provides the training and model parameter details.",
  "First-Level Classification Results": "presents the accuracy results for different models across varying context lengths. Theresult shows that the small-parameter-size language model such as the LSTM struggles with loweraccuracy, achieving between 18.7% and 22.6%. In contrast, larger models, such as Llama2-13Band Mistral-8x7B, show marked improvements as context length increases, with Mistral-8x7Breaching up to 53.6% on the full manuscript. In addition, Claude3.5-Sonnet achieves comparableperformance with GPT-4o across all context lengths, with accuracy consistently above 63.7% andup to 66.7%. GPT-based models exhibit stronger performance overall, with GPT-3.5 achievingbetween 56.8% and 60.6%. GPT-4o further improves across all context lengths, outperforming othermodels with an overall accuracy of 68.5% with the full manuscript input. The performance gapbetween models remains consistent as context length increases. For instance, GPT-4o outperformsLlama3-80B by 16.0%, 14.4%, and 16.8% for context lengths of one sentence, ten sentences, andthe full manuscript, respectively. This consistent performance gap suggests that larger models withmore pre-trained knowledge, such as GPT-4o and Claude3.5-Sonnet, exhibit superior scalability withlonger contexts. These models can more effectively leverage extended context lengths to distinguishbetween mathematical symbols and other nuanced elements in the input prompts. On the other hand,the overall performance gain from increasing context length is more pronounced in smaller models,such as Llama2-13B and Mistral-8x7B, which have less pre-trained knowledge. These modelsbenefit more from extended context as they rely on additional information to compensate for theirlimited pre-training. Larger models like GPT-4o and Claude3.5-Sonnet, which come with extensivepre-trained knowledge, show relatively smaller performance gains as context length increases.",
  "Second-Level Classification Results": "shows second-level classification accuracy with full manuscript input. In this experiment,we assume that the model got the first-level classification correct. From the results, LSTM performspoorly, with an accuracy as low as 11.3% for predicting the DS. Larger models, like Llama2-13B and Mistral-8x7B, improve performance, especially in classifying Constants (up to 37.8%).Llama3-80B shows moderate improvements, with 40.2% accuracy for Global Operators, indicatingreasonable capabilities in operator classification tasks. Claude3.5-Sonnet and GPT-3.5 show furtherimprovements, particularly in Global Constants and Operators classification. GPT-3.5 achieves48.5% accuracy for Local Constants and 49.7% for Global Operators. Lastly, GPT-4o provides thehighest accuracy overall, reaching 60.5% for Local Operators and 58.6% for Matrix classification.By horizontally comparing the same model performance on different sub-attribute classifications, wefind that the attribute Constants are generally easier to classify compared to Variables and Operatorsacross all sizes of models, as seen by the overall higher accuracy in Constant-related tasks. However,Matrix and DS classification continue to present challenges, even for the largest models, indicatingthat certain structures or content types within manuscripts remain difficult to categorize accurately atthe sub-attribute level. Overall, performance across all models on both first-level and second-level classification tasks showsa clear trend of improvement with increasing context length, highlighting the importance of contextfor accurately classifying mathematical symbols. Additionally, both small and large-size languagemodels show a relatively higher accuracy in identifying Unit Descriptors and Operators comparedto Variables and Constants, indicating that symbols with more distinct contextual or syntacticalpatterns are easier for models to classify. Through the above results, we aim to gain insights into theextent to which different category attributes of mathematical symbols influence LLMs understandingof math-rich documents by correctly classifying the symbols in real-world scenarios. We leaveadditional experiments in Appendix D.",
  "Conclusion": "In this paper, we introduce STEM-POM, a comprehensive benchmark for evaluating languagemodels mathematical reasoning abilities to classify math symbols from scientific texts. The datasetincludes over 2,000 math instances sourced from ArXiv papers. Extensive experiments show thatthe best-performing model, achieves only 73.8% and 60.5% for first and second-level Part-of-Math Tagging classification accuracy, highlighting the challenge of extracting and categorizingmathematical symbols from large text corpora. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia LeoniAleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4technical report. arXiv preprint arXiv:2303.08774, 2023.",
  "Alex Graves and Alex Graves. Long short-term memory. Supervised sequence labelling withrecurrent neural networks, pages 3745, 2012": "Muhammad Usman Hadi, Qasem Al Tashi, Abbas Shah, Rizwan Qureshi, Amgad Muneer,Muhammad Irfan, Anas Zafar, Muhammad Bilal Shaikh, Naveed Akhtar, Jia Wu, et al. Largelanguage models: a comprehensive survey of its applications, challenges, limitations, and futureprospects. Authorea Preprints, 2024. Muhammad Usman Hadi, Rizwan Qureshi, Abbas Shah, Muhammad Irfan, Anas Zafar, Muham-mad Bilal Shaikh, Naveed Akhtar, Jia Wu, Seyedali Mirjalili, et al. A survey on large languagemodels: Applications, challenges, limitations, and practical usage. Authorea Preprints, 2023. Emma Hamel, Hongbo Zheng, and Nickvash Kani. An evaluation of nlp methods to ex-tract mathematical token descriptors. In International Conference on Intelligent ComputerMathematics, pages 329343. Springer, 2022. Xinyi He, Jiaru Zou, Yun Lin, Mengyu Zhou, Shi Han, Zejian Yuan, and Dongmei Zhang.Conline: Complex code generation and refinement with online searching and correctness testing.arXiv preprint arXiv:2403.13583, 2024.",
  "Tak Cheung Lam, Jianxun Jason Ding, and Jyh-Charn Liu. Xml document parsing: Operationaland performance characteristics. Computer, 41(9):3037, 2008": "Yuanchun Li, Hao Wen, Weijun Wang, Xiangyu Li, Yizhen Yuan, Guohong Liu, Jiacheng Liu,Wenxing Xu, Xiang Wang, Yi Sun, et al. Personal llm agents: Insights and survey about thecapability, efficiency and security. arXiv preprint arXiv:2401.05459, 2024. Yiheng Liu, Tianle Han, Siyuan Ma, Jiayue Zhang, Yuanyuan Yang, Jiaming Tian, Hao He,Antong Li, Mengshen He, Zhengliang Liu, et al. Summary of chatgpt-related research andperspective towards the future of large language models. Meta-Radiology, page 100017, 2023.",
  "Helmut Schmid. Part-of-speech tagging with neural networks. arXiv preprint cmp-lg/9410018,1994": "Ruocheng Shan and Abdou Youssef. Towards math terms disambiguation using machinelearning. In Intelligent Computer Mathematics: 14th International Conference, CICM 2021,Timisoara, Romania, July 2631, 2021, Proceedings 14, pages 90106. Springer, 2021. Ruocheng Shan and Abdou Youssef. Using large language models to automate annotation andpart-of-math tagging of math equations. In International Conference on Intelligent ComputerMathematics, pages 320. Springer, 2024. Yuan Sui, Jiaru Zou, Mengyu Zhou, Xinyi He, Lun Du, Shi Han, and Dongmei Zhang. Tap4llm:Table provider on sampling, augmenting, and packing semi-structured data for large languagemodel reasoning. arXiv preprint arXiv:2312.09039, 2023. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Openfoundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam, Arjun RLoomba, Shichang Zhang, Yizhou Sun, and Wei Wang. Scibench: Evaluating college-levelscientific problem-solving abilities of large language models. arXiv preprint arXiv:2307.10635,2023.",
  "Abdou Youssef. Part-of-math tagging and applications. In International Conference on Intelli-gent Computer Mathematics, pages 356374. Springer, 2017": "Dongxiang Zhang, Lei Wang, Luming Zhang, Bing Tian Dai, and Heng Tao Shen. The gap ofsemantic parsing: A survey on automatic math word problem solvers. IEEE transactions onpattern analysis and machine intelligence, 42(9):22872305, 2019. Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, AojunZhou, Pan Lu, Kai-Wei Chang, Peng Gao, et al. Mathverse: Does your multi-modal llm trulysee the diagrams in visual math problems? arXiv preprint arXiv:2403.14624, 2024. Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min,Beichen Zhang, Junjie Zhang, Zican Dong, et al. A survey of large language models. arXivpreprint arXiv:2303.18223, 2023. Jiaru Zou, Mengyu Zhou, Tao Li, Shi Han, and Dongmei Zhang. Promptintern: Saving inferencecosts by internalizing recurrent prompt during large language model fine-tuning. arXiv preprintarXiv:2407.02211, 2024.",
  "A.2Source Dataset": "MTDE contains approximately 10,000 entries of mathematical symbol names along with theirdefined contexts. Each entry includes a short definition and a long definition. A short definition is asingle-word definition, while a long definition consists of one or more words. The data was collectedthrough random sampling from mathematical and scientific arXiv preprint manuscripts, coveringa broad range of disciplines such as Physics, Computer Science, and Biology. For pre-processing,we ensured that the candidate data was generated via a corpus crawler and subsequently pruned andcleaned manually. ArXiv Paper Dataset contains 1.7 million arXiv articles, spanning a wide range of disciplines,including Mathematics, Physics, Chemistry, Economics, and Computer Science. We randomlysample 10,000 articles from this raw dataset and manually ensure that each manuscript is math-rich,containing numerous mathematical expressions and symbols. For pre-processing, we utilize theSTEM-PoM Labeler to extract these symbols along with their surrounding context, ensuring that thedata is representative of real-world mathematical usage across various scientific fields. Additionally,",
  "File Name: This attribute serves as a reference point, indicating the source of the file. Specifically, itdenotes the arXiv article from which the dataset extracts its contents": "Symbol Order: This component records the sequence in which mathematical symbols appear withinthe article. By capturing the ordinal position of each symbol, we facilitate a structured analysis of thesymbols progression and their contextual relationships within the document. Symbols: This field encapsulates the mathematical symbols themselves, predominantly consisting ofGreek letters, albeit inclusive of additional characters. The precise documentation of these symbols isparamount for the subsequent analytical processes. Main and Sub Attributes: These attributes categorize each mathematical symbol into specificclasses, delineating a hierarchical structure within the dataset. This classification scheme is vital forunderstanding the symbols roles and relationships within the mathematical discourse. Related Contents: This segment comprises the words or sentences surrounding each symbol,embodying a critical resource for our model training. The contextual information surrounding eachsymbol is indispensable, as it imbues our models with a deeper understanding of each symbolsapplication and significance within the mathematical narrative.",
  "A.5STEM-PoM Labeler": "During the dataset construction, a pivotal step involves the meticulous annotation of each mathematicalsymbol with corresponding tags. This process, inherently labor-intensive and repetitive, necessitatesa systematic approach to mitigate the workload and facilitate collaboration among the research teammembers. To address these challenges, we developed a labeling pipeline designed to streamline thedataset construction process. The UI design is shown in figure 4. The functionalities are delineatedbelow:",
  ": The UI Design of STEM-PoM Labeler": "File Reading: We initiate the data importing operation progress by importing files from the desig-nated arXiv folder, ensuring a structured and accessible repository of mathematical documents forsubsequent processing. Symbol Identification and Contextualization: For each file, we enumerate and display essentialinformation: the current file being processed, the total number of symbols within, the sequencenumber of the current symbol, the graphical representation of the symbol, and the contextual contentsurrounding the symbol. This feature aids in providing a comprehensive overview and facilitatesaccurate symbol annotation. Annotation Interface: We then present a user-friendly interface offering a set of predefined taggingoptions for each symbol. Through the designed interface, we easily select the most appropriate tagfrom these options, standardizing the labeling process and enhancing the consistency of the dataset. Data Recording: Upon the selection of a tag for a symbol, We record this association by appendinga new line to the dataset, capturing the symbol along with its assigned tag. This systematic datarecording ensures the integrity and scalability of the MTCE dataset. Dataset Evaluation: After constructing the dataset, we manually evaluate the quality and applicabilityof the annotated data. Specifically, we process the evaluation process through the following steps:Consistency Check, Inter-annotator Agreement, Statistical Analysis, and Benchmark Testing.",
  "BBackgrounds": "Part-of-Math (PoM) Tagging : The part-of-math tagging task draws inspiration from similartagging tasks such as part-of-speech tagging . In the PoM context, the goal is to label individualmathematical tokens or expressions in math formulas with their corresponding mathematical roles.Such a task is essential for enabling a deeper semantic understanding of mathematical content bymachines. Several datasets or benchmarks have been developed for the part-of-tagging task, butseveral challenges are still remaining. Abdou collects mathematical content, such as formularepresentation and tagging for specific mathematical formula translations and verifications, includingconverting formulae into semantic LaTeX or testing with tools like CAS (Computer Algebra Systems).However, this focus on structured and narrow formula translations does not align with the broader,more diverse text-based tasks required to assess NLP models, due to the lack of scalability featuresin the collected math symbols. Ruocheng recently evaluated the potential of leveraging LLMs for automated annotation and Part-of-Math tagging of math symbols. However, their PoMtagging was conducted on the Digital Library of Mathematical Functions (DLMF) . Since thesource of math symbols is only one manuscript, the mathematical tokens collected only have a singleclassification type and are self-consistent. In contrast, our dataset incorporates the inherent messinessof published literature across several STEM subjects, where these domain-specific math symbols canhave multiple classifications or meanings depending on the discipline and related context information. Large Language Models: Pre-trained large language models (LLMs) have become a cornerstone inmodern NLP . These models, which assign probabilities to word sequences by decomposingthe probability of a sequence into the product of conditional probabilities of subsequent tokens,have evolved significantly over time. Early approaches were based on N-gram models, but with theadvent of distributed word embeddings , neural language models gained prominence. Thescalability and performance improvements introduced by these models, along with the availabilityof vast textual data, have enabled the unsupervised pre-training of LLMs. These models, oftenreferred to as foundation models , can then be fine-tuned on smaller, task-specific datasetsto adapt them for various downstream applications. For STEM-POM, we apply one traditionalsequence-based NLP model, LSTM , and several recent LLMs for our dataset evaluation.",
  "CAdditional Experiment Setups": "Training Details In our experiments, we train an LSTM with varying numbers of layers for themathematical symbol classification tasks. For LLMs, we choose GPT-3.5 and apply a commonparameter-efficient fine-tuning (PEFT) method, LoRA , to evaluate the model precision perfor-mance. We split our dataset into 80%/10%/10% for training/validation/testing sets. Model Parameters For the LSTM model, we use different layer sizes from {128, 256, 512, 1024}.The hidden state size is set to 256, the learning rate is set from {0.1, 0.01, 0.001}, the training epochis 5, and the batch size is 16. We utilize the Adam optimizer . For GPT-3.5 fine-tuning, we usethe GPT-3.5-turbo-0125 model version and set the training epoch to 3. For LoRA fine-tuning, we setthe LoRA rank to 32, batch size to 32, weight decay to 0.01, dropout to 0.1, and learning rate to 1e4.",
  ": First-level classification with various context lengths on GPT-3.5 and fine-tuned GPT-3.5": "shows the comparison result on main attributes between fine-tuned and directly vanilla-referenced GPT3.5. Notably, the fine-tuned GPT-3.5 model achieves an accuracy of 67.4% in theone-sentence context. However, its performance diminishes as the context length increases, with anoticeable drop to 66.9% for ten sentences and further down to 62.2% for full manuscript-lengthcontext. The decreasing trend suggests that while the fine-tuning process improves performance forshorter contexts, the models ability to handle longer contexts is hindered. The vanilla inference results also show a similar pattern of improvement with context length, butthe gap between fine-tuned and vanilla inference narrows as the context length grows. For instance,the difference in overall accuracy between fine-tuned and vanilla models is 10.6% for one-sentencecontexts but only 1.6% for full manuscripts. Overall, the diminishing return for fine-tuned models with longer contexts indicates that fine-tuningamplifies sensitivity to the introduction of noisy or less relevant information when longer contexts areinvolved. The observation also could point to challenges in the fine-tuning process for long-contextLLMs, which require more refined techniques to handle context length effectively.",
  ": LSTM first-level classification accuracy based on different input context lengths": "Model Performance vs Model Size presents the classification accuracy of an LSTM modelfor first-level classification across different model sizes, ranging from 128 to 1024 layers. Note thatwe set the input context length to be one sentence. The results show a clear positive correlationbetween the model size and classification accuracy across all four categories. For the smallest model(128 layers), the accuracy ranges from 10.3% for the Operator class to 27.1% for the Unit Descriptorclass. As the model size increases, the performance improves notably, with the largest model (1024layers) achieving a relatively high-performance gain in accuracy, ranging from 35.9% for the Constantclass to 51.3% for the Unit Descriptor class. The most substantial improvements are observed in theOperator category, where accuracy increases from 10.3% for 128 layers to 44.2% for 1024 layers.These results suggest that larger model sizes are more effective in capturing complex patterns. Model Performance vs Data Input Lengths displays the classification accuracy of anLSTM model across varying input context lengths across four categories. A trend of increasingaccuracy can be observed as the input length increases. For instance, in the Variable category, theaccuracy increases from 24.5% for one sentence to 28.1% for ten sentences. Similarly, for theConstant category, accuracy rises from 13.2% for one sentence to 16.8% for ten sentences. TheOperator category shows a modest increase from 10.3% to 15.5% as the input length expands. Finally,for the Unit Descriptor category, accuracy grows from 27.1% to 30.2%. These results suggest thatlonger input data contributes to improved classification accuracy."
}