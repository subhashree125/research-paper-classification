{
  "Abstract": "Recent deep learning workloads exhibit dynamic characteristics, leading to therising adoption of dynamic shape compilers. These compilers can generate efficientkernels for dynamic shape graphs characterized by a fixed graph topology anduncertain tensor shapes. However, memory optimization, although particularlycrucial in this large model era, remains relatively underexplored for dynamic shapegraphs. The fundamental challenge lies in the lack of precise tensor shapes whichare essential in conventional methods such as operation scheduling(op scheduling)and rematerialization. To address this challenge, we propose op scheduling and re-materialization approaches based on symbolic shapes and developed BladeDISC++.Besides, since rematerialization decisions cannot be made solely at compile timewhen tensor shapes are unknown, BladeDISC++ employs a compilation-runtimecombined strategy to optimally address shape dynamics. Evaluations indicatethat BladeDISC++ effectively reduces memory usage for dynamic shape graphs,achieving memory consumption comparable to optimizations using precise shapes,thereby promoting the broader adoption of dynamic shape compilers.",
  "Introduction": "Dynamic shape compilers are becoming increasingly prevalent due to their ability to optimizedeep learning workloads with dynamic characteristics. While systems like TorchInductor andModular have made significant strides in kernel generation, memory optimization still remainsunderexplored. Conventional methods like op scheduling and rematerialization(recomputation and offloading included) rely on exact tensor shape to assess the memory impact ofops or rematerialization subgraphs, and make optimization decisions at compile time. However, inthe absence of shape values, these methods become unfeasible. BladeDISC++, built upon a dynamic shape compiler BladeDISC, leverages symbolicshapes to tackle the above challenges. With symbolic shapes, BladeDISC++ is able to derivecomparative memory impacts of different op sequences and find the optimum scheduling order. Forrematerialization, symbolic shapes are utilized to search for optimum recomputation subgraph atcompile time and assist to conduct final rematerialization decisions at runtime. Our evaluations demonstrate that BladeDISC++ can effectively reduce memory usage for training withdynamic shape graphs compared to BladeDISC. Additionally, BladeDISC++ achieves comparablememory consumption with static shape training while alleviating the overhead of recompilation andtensor padding.",
  "Memory optimizations based on symbolic shapes": "As shown in , given a dynamic shape computation graph, BladeDISC++ first performs sym-bolic shape analysis to create a global symbolic shape graph that describes the algebraic relationshipsbetween shape symbols(in section 2.1). Then, the symbolic shape graph, together with the computa-tion graph, undergoes optimization passes including op fusion, op scheduling, and rematerializationfor memory optimization. As BladeDISCs prior work has tackled the op fusion problem, this paper focuses on opscheduling (in section 2.2) and rematerialization (in section 2.3). In particular, with the symbolicshape graph instead of exact tensor shape, BladeDISC++ can still compare memory impacts ofdifferent op sequences, and determine whether a recomputation subgraph would benefit memoryconsumption. Additionally, because a dynamic shape graph might have varying memory footprintsacross different runs, it is impractical to make rematerialization decisions, such as how much memoryto evict, solely at compile time. Therefore, BladeDISC++ explores all rematerialization candidatesand searches their corresponding regeneration subgraphs and conduct final rematerialization decisionsat runtime.",
  "Listing 1: Example of a dynamic shape graph and its symbolic shape graph": "As illustrated in Listing 1, BladeDISC++ introduces a SymbolicDim op to define a symbolic value,bond to a dimension of a tensor shape in the dynamic shape graph as op attributes, exemplified by ten-sor<?x?, [@S0, @S1]>. For instance, the equation @S0 = 12 * @S1 stems from DynamicReshapeOpthat its input and output tensor have the same number of elements. Comparison between memory sizes of tensors is critical to op scheduling and rematerialization.BladeDISC++ introduces SymbolicExpr to express algebraic representations of symbolic dimensions,allowing for comparative evaluations with a best-effort strategy. For example, the element number oftensor %1084 and %1085 can be represented by SymbolicExprs expr1 = 11008 * @S1 and expr2 =1024 * @S0 respectively. As its already derived from DynamicReshapeOp that @S0 = 12 * @S1,exp1 can be simplified to 132096 * @S0, thus BladeDISC++ can infer that expr1 is less than expr2.",
  ": OpScheduler algorithm main loop": "Op scheduling tries to find a memory-efficientop sequence from the original computationgraph. Existing scheduling algorithms oftentraverse the computation graph and select an opfrom a ReadySet (including ops whose predeces-sors have already been scheduled) at each step.The selection is mainly based on comparing dif-ferent ops memory impact, which is determinedby the difference between bytes freed and allo-cated after scheduling a specific op. BladeDISC++ adopts a similar methodology, emphasizing thecomputation and comparison of memory impact among different ops with the absence of exact tensorshapes in dynamic shape graphs. Specifically, in BladeDISC++, the memory impact for each op iscalculated using symbolic shapes and thus expressed as a SymbolicExpr. These SymbolicExprs arethen compared to each other with the help of symbolic shape graph. In Listing 1, for example, the DynamicReshapeOp and DotOp appear in the ReadySet at a specificstep. DotOp, as the last consumer of %2 and producer of %3, has a memory impact of 10996 *@S1 due to the allocation for %3 and deallocation for %2. The DynamicReshapeOps memoryimpact, on the other hand, is 4096 * @S0 because scheduling it only involves allocation for %1. Tocompare two SymbolicExprs containing different sets of symbols, we first simplify the SymbolicExprof DynamicReshapeOps memory impact based on @S1 to 49152 * @S1 with the same proceduredescribed in 2.1, then it can be determined that the DynamicReshapeOp has a higher memory impactthan the DotOp.",
  "Rematerialization": "Conventional rematerialization methodsinvolve algorithms to determine which tensors to beevicted earlier to alleviate memory pressure, as well as how to perform subsequent regeneration, eitherthrough reloading or recomputation. These methods also include a search process to identify optimalrecomputation subgraphs by evaluating their memory impacts. Notably, tensor rematerializationmay negatively affect end-to-end performance, so it should only be employed when the graphsexecution risks exceeding the memory limit. However, a dynamic shape graph, with undeterminedtensor shapes, can exhibit varying peak memory usage across different runs. Some runs may not needrematerialization since they remain within memory limits, while others may need. Its impractical tomake all decisions solely during compilation. Furthermore, the lack of exact shapes raises challengesin assessing the memory impacts of potential recomputation subgraphs. To address these issues, BladeDISC++ utilizes a combined compilation-runtime strategy based onsymbolic shapes to best manage shape dynamics across graph runs. During compilation, it exploresall potential rematerialization candidates and identifies their corresponding regeneration subgraphs,which are then inserted into the original computation graph as different execution branches. Finaldecisions regarding which tensor to evict and the associated regeneration method are made at runtime. During compile time, as illustrated in , BladeDISC++ inserts a Remat::EvictOp after eachop, checking if any active tensors at that point need to be evicted to alleviate memory pressure. Foreach candidate tensor, regeneration subgraphs, including those for reload and recomputation, are alsogenerated. While reloading only involves a host-to-device (H2D) instruction and is memory-neutral,searching for recomputation subgraphs requires careful evaluation since sub-optimal choices may",
  "even increase peak memory usage. BladeDISC++ uses a standard search process but assesses memoryimpact of potential subgraphs based on SymbolicExpr": "Taking recomputation subgraph searching for %4 in Listing 1 as an example. Starting from theReduceOp, BladeDISC++ determines the memory impacts: -11007 * @S1 for just the ReduceOp,-11 * @S1 with the addition of the DotOp, and 1 * @S1 when the DynamicReshapeOp is included.Although exact shape values are unknown, BladeDISC++ can still ascertain that the last recomputationsubgraph is memory-efficient, whereas the others are not. Then, BladeDISC++ inserts Remat::RegenerateOps, along with the corresponding regenerationsubgraphs (both reload and recompute), before each candidate tensors subsequent consumers. TheRemat::RegenerateOp checks whether a candidate tensor is evicted and its regeneration method, At runtime, BladeDISC++ monitors memory usage throughout kernel execution. Each time anEvictOp is triggered, BladeDISC++ checks the current memory usage and performs an on-the-flyanalysis of all candidate tensors provided by the EvictOp when the memory limit is about to besurpassed. The final decisions on which tensor from the above candidates needs to be evicted as wellas the corresponding regeneration method are made by considering factors such as memory savingsand end-to-end performance impact, following a similar approach as outlined in . SubsequentRemat::RegenerateOps then query these decisions and determine which regeneration subgraphs needto be triggered.",
  "Evaluation": "For our evaluation, we conducted experiments on the supervised fine-tuning of Llama-2-1b, a tailoredmodel from the official Llama-2-7b with the only change that decreasing num_hidden_layersfrom 32 to 4, on an Alibaba Cloud ecs.gn7-c12g1.3xlarge instance(with 40GB GPU RAM) usingthe CodeAlpaca-20K dataset . CodeAlpaca-20K contains samples with text lengths ranging fromapproximately 100 to 3000 characters. In each training iteration, a fixed number of randomly selectedsamples are assembled into a batch, resulting in variable batch shapes across different iterations. To assess the effectiveness of BladeDISC++, we compared memory usage and end-to-end performancein dynamic shape training using BladeDISC++ against both dynamic and static shape training usingBladeDISC. For static shape training, following common practice, input sequences are padded tonearest power of 2 in length to balance redundant computation and compilation overhead. Besides,in our experiments, we deliberately set the largest bucket size equal to the longest sequence lengthin the dataset to investigate whether we can achieve comparable memory optimization results usingsymbolic shapes instead of exact shapes. The experimental results indicate that BladeDISC++ can effectively reduce peak memory consump-tion for dynamic shape training. Furthermore, BladeDISC++ demonstrates comparable memoryconsumption to static shape training while also improving end-to-end performance by alleviating theoverhead of recompilation and input bucketing.",
  "Conclusion": "This paper shares our industry experience in optimizing memory for dynamic shape graphs . Weproposed op scheduling and rematerialization based on symbolic shapes and developed BladeDISC++.Evaluations show that BladeDISC++ can effectively reduce memory usage for dynamic shape trainingand can achieve comparable memory optimization results to static shape training. As far as we know,this is a pioneering effort in this area, and we aspire that it will support the compiler community inmanaging dynamic shape workloads and promote wider use of dynamic shape compilers.",
  "Codealpaca-20k dataset, 2024. Accessed: December 24, 2024": "Renze Chen, Zijian Ding, Size Zheng, Chengrui Zhang, Jingwen Leng, Xuanzhe Liu, and Yun Liang.Magis: Memory optimization via coordinated graph transformation and scheduling for dnn. In Proceedingsof the 29th ACM International Conference on Architectural Support for Programming Languages andOperating Systems, Volume 3, ASPLOS 24, page 607621, New York, NY, USA, 2024. Association forComputing Machinery.",
  "Alibaba Cloud. Alibaba cloud ecs.gn7-c12g1.3xlarge instance, 2024. Accessed: December 24, 2024": "Chien-Chin Huang, Gu Jin, and Jinyang Li. Swapadvisor: Pushing deep learning beyond the gpu memorylimit via smart swapping. In Proceedings of the Twenty-Fifth International Conference on ArchitecturalSupport for Programming Languages and Operating Systems, ASPLOS 20, page 13411355, New York,NY, USA, 2020. Association for Computing Machinery. Paras Jain, Ajay Jain, Aniruddha Nrusimha, Amir Gholami, Pieter Abbeel, Kurt Keutzer, Ion Stoica, andJoseph E. Gonzalez. Checkmate: Breaking the memory wall with optimal tensor rematerialization, 2020.",
  "Pytorch Team. Dynamic shape in pytorch, 2023. Accessed: December 24, 2024": "Zihan Wang, Chengcheng Wan, Yuting Chen, Ziyi Lin, He Jiang, and Lei Qiao. Hierarchical memory-constrained operator scheduling of neural architecture search networks. In Proceedings of the 59thACM/IEEE Design Automation Conference, DAC 22, page 493498, New York, NY, USA, 2022. Associa-tion for Computing Machinery. Zhen Zheng, Zaifeng Pan, Dalin Wang, Kai Zhu, Wenyi Zhao, Tianyou Guo, Xiafei Qiu, Minmin Sun,Junjie Bai, Feng Zhang, et al. Bladedisc: Optimizing dynamic shape machine learning workloads viacompiler approach. Proceedings of the ACM on Management of Data, 1(3):129, 2023. Kai Zhu, WY Zhao, Zhen Zheng, TY Guo, PZ Zhao, JJ Bai, Jun Yang, XY Liu, LS Diao, and Wei Lin.Disc: A dynamic shape compiler for machine learning workloads. In Proceedings of the 1st Workshop onMachine Learning and Systems, pages 8995, 2021."
}