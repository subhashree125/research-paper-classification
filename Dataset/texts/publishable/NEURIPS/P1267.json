{
  "Abstract": "Efficient data selection is essential for improving the training efficiency of deepneural networks and reducing the associated annotation costs. However, traditionalmethods tend to be computationally expensive, limiting their scalability and real-world applicability. We introduce PruneFuse, a novel method that combines pruningand network fusion to enhance data selection and accelerate network training. InPruneFuse, the original dense network is pruned to generate a smaller surrogatemodel that efficiently selects the most informative samples from the dataset. Oncethis iterative data selection selects sufficient samples, the insights learned from thepruned model are seamlessly integrated with the dense model through networkfusion, providing an optimized initialization that accelerates training. Extensiveexperimentation on various datasets demonstrates that PruneFuse significantlyreduces computational costs for data selection, achieves better performance thanbaselines, and accelerates the overall training process.",
  "Introduction": "Deep learning models have achieved remarkable success across various domains, ranging from imagerecognition to natural language processing . However, the performance of models heavily relieson the access of large amounts of labeled data for training . In real-world applications, manuallyannotating massive datasets can be prohibitively expensive and time-consuming. Data selectiontechniques such as Active Learning (AL) offer a promising solution to address this challenge byiteratively selecting the most informative samples from the unlabeled dataset for annotation. Thegoal of AL is to reduce labeling costs while maintaining or improving model performance. However,as data and modal complexity grow, traditional AL techniques that require iterative model trainingbecome computationally expensive, limiting scalability in resource-constrained settings. In this paper, we propose PruneFuse, a novel strategy for efficient data selection in active learningsetting that overcomes the limitations of traditional approaches. Our approach is based on modelpruning, which reduces the complexity of neural networks. By utilizing small pruned networksfor data selection, we eliminate the need to train large models during the data selection phase,thus significantly reducing computational demands. Additionally after the data selection phase,we utilize the learning of these pruned networks to train the final model through a fusion process,which harnesses the insights from the trained networks to accelerate convergence and improve thegeneralization of the final model. Contributions. Our key contribution is to introduce an efficient and rapid data selection techniquethat leverages pruned networks. By employing pruned networks as data selectors, PruneFuse ensurescomputationally efficient selection of informative samples leading to overall superior generalization.",
  "(1 )": "PruningFusion : Overview of the PruneFuse Method: (1) An untrained neural network is initially pruned to form astructured, pruned network p. (2) This pruned network p queries the dataset to select prime candidates forannotation, similar to active learning techniques. (3) p is then trained on these labeled samples to form thetrained pruned network p. (4) The trained pruned network p is fused with the base model , resulting in afused model. (5) The fused model is further trained on a selected subset of the data, incorporating knowledgedistillation from p.",
  "Background and Related Works": "Subset Selection Framework. Active Learning (AL) is widely utilized iterative approach tailoredfor situations with abundant unlabeled data. Given a classification task with C classes and a largepool of unlabeled samples U, AL revolves around selectively querying the most informative samplesfrom U for labeling. The process commences with an initial set of randomly sampled data s0 from U,which is subsequently labeled. In subsequent rounds, AL augments the labeled set L by adding newlyidentified informative samples. This cycle repeats until a predefined number of labeled samples b areselected. Data Selection. Approaches such as aim to select informative samples using techniques likediversity maximization and Bayesian uncertainty estimation. Parallelly, the domain of active learninghas unveiled strategies, such as , which prioritize samples that can maximizeinformation gain, thereby enhancing model performance with minimal labeling effort. While thesemethods achieve efficient data selection, they still require training large models for the selectionprocess, resulting in significant computational overhead. Other strategies such as optimizethis selection process by matching the gradients of subset with training or validation set based onorthogonal matching algorithm and performs meta-learning based approach for online dataselection. SubSelNet proposes to approximate a model that can be used to select the subsetfor various architectures without retraining the target model, hence reducing the overall overhead.However, it involves pre-training routine which is very costly and needed again for any changein data or model distribution. SVP introduces to use small proxy models for data selectionbut discards these proxies before training the target model. Additionally, structural discrepanciesbetween the proxy and target models may result in sub-optimal data selections. Our approach alsobuilds on this foundation of using small model (which in our case is a pruned model) but it enablesdirect integration with the target model through the fusion process. This ensures that the knowledgeacquired during data selection is retained and actively contributes to the training of the original model.Also, the architectural coherence between the pruned and the target model provides a more seamlessand effective mechanism for data selection, enhancing overall model performance and efficiency. Efficient Deep Learning. Methods such as have been proposed to reduce model sizeand computational requirements. Neural Network pruning has been extensively investigated as atechnique to reduce the complexity of deep neural networks . Pruning strategies can be broadlydivided into Unstructured Pruning and Structured Pruning based on the granularity and regularity of the pruning scheme. Unstructured pruning often yields a superior accuracy-sizetrade-off whereas structured pruning offers practical speedup and compression without necessitatingspecialized hardware . While pruning literature suggests pruning after training or duringtraining , recent research explore the viability of pruning at initialization . Inour work, we leverage the benefits of pruning at initialization to create a small representative modelfor efficient data selection.",
  "PruneFuse": "In this section, we delineate the PruneFuse methodology, illustrated in (and Algorithm 1provided in Appendix). The procedure begins with network pruning at initialization, offering astreamlined model for data selection. Upon attaining the desired data subset, the pruned model under-goes a fusion process with the original network, leveraging the structural coherence between them.The fused model is subsequently refined through knowledge distillation, enhancing its performance.We framed the problem as, let sp be the subset selected using a pruned model p and s be the subsetselected using the original model . We want to minimize:",
  "E(x,y)sp[l(x, y; , p)] E(x,y)D[l(x, y; )](1)": "Where E(x,y)sp[l(x, y; , p)] is the expected loss on subset sp (selected using p) when eval-uated using the original model and E(x,y)D[l(x, y; )] is the expected loss on full datasetD when trained using the original model . Furthermore, the subset can be defined as sp ={(xi, yi) D | score(xi, yi; p) } where score(xi, yi; p) represents the score assigned to eachsample selected using p. The score function can be based on various strategies such as LeastConfidence, Entropy, or Greedy k-centers. defines the threshold used in the score-based selectionmethods (Least Confidence or Entropy) to determine the inclusion of a sample in sp. The goal of the optimization problem is to select sp such that when is trained on it, the performanceis as close as possible to training on the full dataset D. The key insight is that the subset sp selectedusing the pruned model p is sufficiently representative and informative for training the originalmodel . This is because p maintains a structure that is essentially identical to , although withsome nodes pruned. As a result, there is a strong correlation between and p, ensuring that theselection made by p effectively minimizes the loss when is trained on sp. By leveraging thissurrogate p, which is both computationally efficient and structurally coherent with , we can selectmost representative data out of D to train .",
  "Pruning at Initialization": "Pruning at initialization shows potential in training time reduction, and enhanced model general-ization. In our methodology, we employ structured pruning due to its benefits such as maintaining thearchitectural coherence of the network, enabling more predictable resource savings, and often leadingto better-compressed models in practice. Consider an untrained neural network, represented as .Let each layer of this network have feature maps or channels denoted by c, with {1, . . . , L}.Channel pruning results in binary masks m {0, 1}d for every layer, where d represents the totalnumber of channels in layer . The pruned subnetwork, p, retains channels described by c m,where symbolizes the element-wise product. The sparsity p of the subnetwork illustratesthe proportion of channels that are pruned: p = 1",
  "m/": "d. To reduce the model complexity,we employ channel pruning procedure prune(C, p). This prunes to a sparsity p via two primaryfunctions: i) score(C): This operation assigns scores z Rd to every channel in the networkcontingent on their magnitude (using the L2 norm). The channels C are represented as (c1, . . . , cL).and ii) remove(Z, p): This process takes the magnitude scores Z = (z1, . . . , zL) and translates theminto masks m such that the cumulative sparsity of the network, in terms of channels, is p. We employa one-shot channel pruning that scores all the channels simultaneously based on their magnitude andprunes the network from 0% sparsity to p% sparsity in one cohesive step. Although previous workssuggest re-initializing the network to ensure proper variance . However, since the performanceincrement is marginal, we retain the weights of the pruned network before training.",
  "Data Selection via Pruned Model": "We begin by randomly selecting a small subset of data samples, denoted as s0, from the unla-beled pool U = {xi}i[n] where [n] = {1, ..., n}.These samples are then annotated.Thepruned model p is trained on this labeled subset s0, resulting in the trained pruned model p.With p as our tool, we venture into the larger unlabeled dataset U to identify samples thatare prime candidates for annotation. Regardless of the scenario, our method employs three dis-tinct criteria for data selection: Least Confidence (LC) , Entropy , and Greedy k-centers. LC based selection gravitates towards samples where the pruned model exhibits the leastconfidence in its predictions. Thus, the uncertainty score for a given sample xi is defined asscore(xi; p)LC = 1 maxy P(y|xi; p). The entropy-based selection focuses on samples with highprediction entropy, computed as score(xi; p)Entropy = y P(y|xi; p) log P(y|xi; p), highlight-ing uncertainty. Subsequently, we select the top-k samples exhibiting the highest uncertainty scores,proposing them as prime candidates for annotation. The Greedy k-centers aims to cherry-pick kcenters from the dataset such that the maximum distance of any sample from its nearest center isminimized. The selection is mathematically represented as x = arg maxxU minccenters d(x, c)where centers is the current set of chosen centers and d(x, c) is the distance between point x andcenter c. While various metrics can be employed to compute this distance, we opt for the Euclideandistance since it is widely used in this context.",
  "Training of Pruned Model": "Once we have selected the samples from U, they are annotated to obtain their respective labels.These freshly labeled samples are assimilated into the labeled dataset L. At the start of eachtraining cycle, a fresh p is generated. Training from scratch in every iteration is vital to prevent themodel from developing spurious correlations or overfitting to specific samples . This fresh startensures that the model learns genuine patterns in the updated labeled dataset without carrying overpotential biases from previous iterations. The training process adheres to a typical deep learningparadigm. Given the dataset L with samples (xi, yi), the aim is to minimize the loss function:L(p, L) =1|L||L|i=1 Li(p, xi, yi), where Li denotes the individual loss for the sample xi. Trainingunfolds over multiple iterations (or epochs). In each iteration, the weights of p are updated usingbackpropagation with an optimization algorithm like stochastic gradient descent (SGD). This processis inherently iterative as in AL. After each round of training, new samples are chosen, annotated, andthe model is reinitialized and retrained from scratch. This cycle persists until certain stopping criteria,e.g. labeling budget or desired performance, are met. With the incorporation of new labeled samplesat every stage, p progressively refines its performance, becoming better suited for the subsequentdata selection phase.",
  "(c) F with a refined trajectory due to fusion": ": Evolution of training trajectories. Pruning to p tailors the loss landscape from 2a to 2b, allowingp to converge on an optimal configuration, denoted asp. This model, p, is later fused with the original ,which provides better initialization and offers superiortrajectory for F to follow, as depicted in 2c. After achieving the predetermined budget, thenext phase is to integrate the insights from thetrained pruned model p into the untrained orig-inal model . This step is crucial, as it amal-gamates the learned knowledge from p withthe expansive architecture of the original model,aiming to harness the best of both worlds. Rationale for Fusion. Traditional pruning andfine-tuning methods often involve training alarge model, pruning it down, and then fine-tuning the smaller model. While this is effective,it does not fully exploit the potential benefits ofthe larger, untrained model. The primary reasonis that the pruning process might discard usefulstructures and connections within the original model that were not yet leveraged during initial training.By fusing the trained pruned model with the untrained original model, we aim to create a model thatcombines the learned knowledge by p with the broader, unexplored model .",
  "PruneFuse (p = 0.8)0.0380.1187.5890.5092.4293.3236.4950.9858.5362.8765.851.0218.3437.8647.1551.7755.18": ": Performance Comparison of Baseline and PruneFuse on CIFAR-10, CIFAR-100 and Tiny ImageNet-200. This table summarizes the test accuracy of final models (original in case of AL and Fused in PruneFuse)for various pruning ratios (p) and labeling budgets(b). Least Confidence is used as a metric for subset selectionand different architectures (ResNet-56 for CIFAR-10 and CIFAR-100 while ResNet-50 for Tiny-ImageNet) areutilized. The Fusion Process. Fusion is executed by transferring the weights from the trained pruned modelsweight matrix p to the corresponding locations within the weight matrix of the untrained originalmodel . This results in a new, fused weight matrix: F = Fuse(, p). Lets represent a model as a sequence of layers, where each layer L consists of filters (for CNNs). We can denote the ithfilter of layer j in model as F i,j. Given: is the original untrained model and p is the trainedpruned model. For a specific layer j, has a set of n filters {F 1,j, F 2,j, ...F n,j} and p has a set of",
  "Where F Fi,j is the ith filter of layer j in the fused model F": "Advantages of Retaining Unaltered Weights: By copying weights from the trained pruned modelp into their corresponding locations within the untrained original model , and leaving the remainingweights of yet to be trained, we create a unique blend. The weights from p encapsulate theknowledge acquired during training, providing a foundation. Meanwhile, the rest of the untrainedweights in still have their initial values, offering an element of randomness. This duality fosters aricher exploration of the loss landscape during subsequent training. illustrates the transforma-tion in training trajectories resulting from the fusion process. The trained weights of p provides abetter initialization, while the unaltered weights serve as gateways to unexplored regions in the losslandscape. This strategic combination in the fused model F enables the discovery of potentiallysuperior solutions that neither the pruned nor the original model might have discovered on their own.",
  "Refinement via Knowledge Distillation": "After the fusion process, our resultant model, F , embodies a synthesis of insights from both thetrained pruned model p and the original model . Although PruneFuse outperforms baseline AL(results are provided in Appendix), we further optimize and enhance F using Knowledge Distillation(KD). KD enables F to learn from p (the teacher model), enriching its training. During the fine-tuning phase, we use two losses: i) Cross-Entropy Loss, which quantifies the divergence between thepredictions of F and the actual labels in dataset L, and ii) Distillation Loss, which measures the dif-ference in the softened logits of F and p. These softened logits are derived by tempering logits of p,which in our case is the teacher model, with a temperature parameter before applying the softmax func-tion. The composite loss is formulated as a weighted average of both losses. The iterative enhancementof F is governed by: (t+1)F= (t)F (t)F LCross Entropy((t)F , L) + (1 )LDistillation((t)F , p).Here represents the learning rate, while functions as a coefficient to balance the contributions ofthe two losses. By incorporating KD in the fine-tuning phase, we aim to ensure that the fused modelF not only retains the trained weights of pruned model but also reinforce this knowledge iteratively,optimizing the performance of F in subsequent tasks.",
  "(a)(b)(c)(d)": ": Computation Comparison of PruneFuse and Baseline (Active Learning): This figure illustrates thetotal number of FLOPs utilized by PruneFuse, compared to the baseline Active Learning method, for selectingsubsets with specific labeling budgets b = 10%, 30%, 50%. The experiments are conducted on the CIFAR-10dataset using the ResNet-56 architecture. Subfigures (a), (b), (c), and (d) correspond to different pruning ratios(0.5, 0.6, 0.7, and 0.8, respectively).",
  ": Performance Comparison of SVP and PruneFuse across various labeling budgets b for efficient trainingof Target Model (ResNet-56)": "ResNet-56 and ResNet-164 architecture in our experiments. We pruned these architectures using theTorch-Prunnig library for different pruning ratios p = 0.5, 0.6, 0.7, and 0.8 to get the prunedarchitectures. We trained the model for 181 epochs using the mini-batch of 128 for CIFAR-10and CIFAR-100 and 100 epochs using the mini-batch of 256 for TinyImageNet-200. For all theexperiments SGD is used as an optimizer. We took AL as a baseline for the proposed technique andinitially, we started by randomly selecting 2% of the data. For the first round, we added 8% fromthe unlabeled set, then 10% in each subsequent round, until reaching the label budget, b. After eachround, we retrained the models from scratch, as described in the methodology. All experiments arecarried out independently 3 times and then the average is reported.",
  "Results and Discussions": "Main Experiments. summarizes the generalization performance of baseline and differentvariants of PruneFuse on different datasets (detailed results on different architectures and dataselection metrics are provided in Appendix). All variants of PruneFuse achieve higher accuracycompared to the baseline, demonstrating the effectiveness of superior data selection performanceand fusion. (a), (b), (c), and (d) illustrates the computational complexity of the baseline andPruneFuse variants in terms of Floating Point Operations (FLOPs) for different labeling budgets.The FLOPs are computed for the whole training duration of the pruned network and the selectionprocess. Different variants of PruneFuse p = 0.5, 0.6, 0.7, and 0.8 provide the flexibility that theuser can choose the variant of PruneFuse depending on their computation resources e.g. PruneFuse(p = 0.8) requires very low computation resources compared to others while achieving good accuracyperformance. Comparison with Selection-via-Proxy. delineates a comparison of PruneFuse and the SVP, performance metrics show that PruneFuse consistently outperforms SVP across all labelingbudgets for the efficient training of a Target Model (ResNet-56). SVP employs a ResNet-20 as its dataselector, with a model size of 0.26 M. In contrast, PruneFuse uses a 50% pruned ResNet-56, reducingits data selector size to 0.21 M. Notably, while the data selector of PruneFuse achieves a loweraccuracy of 90.31% at b = 50% compared to SVPs 91.61%, the target model utilizing PruneFuse-selected data attains a superior accuracy of 93.69%, relative to 92.95% for the SVP-selected data.This disparity underscores the distinct operational focus of the data selectors: PruneFuses selector isoptimized for enhancing the target models performance, rather than its own. Ablation Studies. demonstrates the effect of Knowledge Distillation (KD) on the PruneFusetechnique relative to the baseline method across various data selection matrices and label budgets onCIFAR-100 datasets, using ResNet-56 architecture. The results indicate that PruneFuse consistentlyoutperforms the baseline method, both with and without incorporating KD from a trained pruned",
  ": Ablation Study of Knowledge Distillation on PruneFuse for CIFAR-100 datasets on Resnet-56": "model. This superior performance is attributed to the innovative fusion strategy inherent to PruneFuse.The proposed approach gives the fused model an optimized starting point, enhancing its ability tolearn more efficiently and generalize better. The impact of this strategy is evident across differentlabel budgets and architectures, demonstrating its effectiveness and robustness.",
  "(a) p = 0.5, b = 30%(b) p = 0.6, b = 30%": ": Impact of Model Fusion on PruneFusePerformance: This figure compares the accuracy overepochs between fused and non-fused training approacheswithin the PruneFuse framework, both utilizing subset(with labeling budget b) selected by the pruned model.Experiments are conducted using the ResNet-56 on theCIFAR-10. Subfigures (a) and (b) correspond to pruningratios p = 0.5 and 0.6, respectively. demonstrates the effect of fusion acrossvarious pruning ratios, the models trained withfusion in-place perform better than those trainedwithout fusion, achieving higher accuracy lev-els at an accelerated pace. The rapid conver-gence is most notable in initial training phases,where fusion model benefits from the initializa-tion provided by the integration of weights froma trained pruned model p with an untrainedmodel . The strategic retention of untrainedweights introduces a beneficial stochastic com-ponent to the training process, enhancing themodels ability to explore new regions of the pa-rameter space. This dual capability of exploitingprior knowledge and exploring new configura-tions enables the proposed technique to consis-tently outperform, making it particularly beneficial in scenarios with sparse label data.",
  "Conclusion": "We introduce PruneFuse, a novel approach combining pruning and network fusion to optimize dataselection in deep learning. PruneFuse leverages a small pruned model for data selection, which thenseamlessly fuses with the original model, providing fast and better generalization while significantlyreducing computational costs. Extensive evaluations on CIFAR-10, CIFAR-100, and Tiny-ImageNet-200 show that PruneFuse outperforms existing baselines, establishing its efficiency and efficacy.PruneFuse offers a scalable, practical, and flexible solution to enhance the training efficiency ofneural networks, particularly in resource-constrained settings.",
  "Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-timeobject detection with region proposal networks. Advances in neural information processingsystems, 28, 2015": "Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for se-mantic segmentation. In Proceedings of the IEEE conference on computer vision and patternrecognition, pages 34313440, 2015. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for imagerecognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,pages 770778, 2016. Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonableeffectiveness of data in deep learning era. In Proceedings of the IEEE international conferenceon computer vision, pages 843852, 2017. Yarin Gal, Riashat Islam, and Zoubin Ghahramani. Deep bayesian active learning with imagedata. In Proceedings of the 34th International Conference on Machine Learning-Volume 70,pages 11831192. JMLR. org, 2017.",
  "Ozan Sener and Silvio Savarese. Active learning for convolutional neural networks: A core-setapproach. In International Conference on Learning Representations, 2018. URL": "Alexander Freytag, Erik Rodner, and Joachim Denzler. Selecting influential examples: Activelearning with expected model output changes. In Computer VisionECCV 2014: 13th EuropeanConference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part IV 13, pages 562577. Springer, 2014. Christoph Kding, Erik Rodner, Alexander Freytag, and Joachim Denzler. Active and continuousexploration with deep neural networks and expected model output changes. arXiv preprintarXiv:1612.06129, 2016. Krishnateja Killamsetty, Sivasubramanian Durga, Ganesh Ramakrishnan, Abir De, and RishabhIyer. Grad-match: Gradient matching based data subset selection for efficient deep modeltraining. In International Conference on Machine Learning, pages 54645474. PMLR, 2021.",
  "Krishnateja Killamsetty, Durga Subramanian, Ganesh Ramakrishnan, and Rishabh Iyer. Glister:A generalization based data selection framework for efficient and robust learning. AAAI, 2021": "Eeshaan Jain, Tushar Nandy, Gaurav Aggarwal, Ashish Tendulkar, Rishabh Iyer, and Abir De.Efficient data subset selection to generalize training across models: Transductive and inductivenetworks. Advances in Neural Information Processing Systems, 36, 2024. Cody Coleman, Christopher Yeh, Stephen Mussmann, Baharan Mirzasoleiman, Peter Bailis,Percy Liang, Jure Leskovec, and Matei Zaharia. Selection via proxy: Efficient data selection fordeep learning. arXiv preprint arXiv:1906.11829, 2019.",
  "Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural net-works with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149,2015": "Zhen Dong, Zhewei Yao, Daiyaan Arfeen, Amir Gholami, Michael W Mahoney, and KurtKeutzer. Hawq-v2: Hessian aware trace-weighted quantization of neural networks. Advances inneural information processing systems, 33:1851818529, 2020. Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard,Hartwig Adam, and Dmitry Kalenichenko. Quantization and training of neural networks forefficient integer-arithmetic-only inference. In Proceedings of the IEEE conference on computervision and pattern recognition, pages 27042713, 2018. Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou. Dorefa-net:Training low bitwidth convolutional neural networks with low bitwidth gradients. arXiv preprintarXiv:1606.06160, 2016.",
  "Yihui He, Xiangyu Zhang, and Jian Sun. Channel pruning for accelerating very deep neuralnetworks. In Proceedings of the IEEE international conference on computer vision, pages13891397, 2017": "Zhonghui You, Kun Yan, Jinmian Ye, Meng Ma, and Ping Wang. Gate decorator: Globalfilter pruning method for accelerating deep convolutional neural networks. Advances in neuralinformation processing systems, 32, 2019. Xiaohan Ding, Guiguang Ding, Yuchen Guo, and Jungong Han. Centripetal sgd for pruningvery deep convolutional networks with complicated structure. In Proceedings of the IEEE/CVFconference on computer vision and pattern recognition, pages 49434953, 2019. Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, and Changshui Zhang.Learning efficient convolutional networks through network slimming. In Proceedings of theIEEE international conference on computer vision, pages 27362744, 2017.",
  "Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M Roy, and Michael Carbin. Pruning neu-ral networks at initialization: Why are we missing the mark? arXiv preprint arXiv:2009.08576,2020": "Hidenori Tanaka, Daniel Kunin, Daniel L Yamins, and Surya Ganguli. Pruning neural networkswithout any data by iteratively conserving synaptic flow. Advances in neural informationprocessing systems, 33:63776389, 2020. Yulong Wang, Xiaolu Zhang, Lingxi Xie, Jun Zhou, Hang Su, Bo Zhang, and Xiaolin Hu.Pruning from scratch.In Proceedings of the AAAI Conference on Artificial Intelligence,volume 34, pages 1227312280, 2020.",
  "A.1Performance Comparison with different Datasets, Selection Metrics, and Architectures": "To comprehensively evaluate the effectiveness of PruneFuse, we conducted additional experimentscomparing its performance with baseline utilizing other data selection metrics such as Least Confi-dence, Entropy, and Greedy k-centers. Results are shown in Tables 4 and 5 for various architecturesand labeling budgets. In all cases, our results demonstrate that PruneFuse mostly outperforms thebaseline using these traditional metrics across various datasets and model architectures, highlightingthe robustness of PruneFuse in selecting the most informative samples efficiently.",
  ": Comparison of SVP and PruneFuse on SmallModels": "demonstrates the performance compar-ison of PruneFuse and SVP for small modelarchitecture ResNet-20 on CIFAR-10.SVPachieves 91.88% performance accuracy by uti-lizing the data selector having 0.074 M param-eters whereas PruneFuse outperforms SVP byachieving 92.29% accuracy with a data selectorof 0.066 M parameters. Number of Parameters of Data Selector104 90.8 90.9 91.1 91.2 91.3 91.4 91.5 Accuracy (%) Accuracy vs Model Size (Parameters) R8 R14(p=0.5) R14(p=0.6) ProposedSVP Number of Parameters of Data Selector104 91.7 91.8 91.9 92.1 92.2 92.3 92.4 Accuracy (%) Accuracy vs Model Size (Parameters) R8 R20(p=0.5) R20(p=0.6) ProposedSVP",
  "(a) Target Model = ResNet-14(b) Target Model = ResNet-20": ": Comparison of PruneFuse with SVP. Scatterplot shows final accuracy on target model against the model size fordifferent ResNet models on CIFAR-10 dataset with labeling budget b =50%. (a) shows for the target network ResNet-14, ResNet-14 (withp = 0.5 and p = 0.6) and ResNet-8 models are used as data selectorsfor PruneFuse and SVP, respectively. While in (b), PruneFuse utilizesResNet20 (i.e. p = 0.5 and p = 0.6) and SVP utilizes ResNet-8models for data selection when the target model is ResNet-20. (a) and (b) show that target models whentrained with the data selectors of the PruneFuseachieve significantly higher accuracy while us-ing significantly less number of parameters com-pared to SVP. These results indicate that thePruneFuse does not require an additional archi-tecture for designing the data selector; it solelyneeds the target model. In contrast, SVP neces-sitates both the target model (ResNet-14) anda smaller model (ResNet-8) that functions as adata selector.",
  "A.3Ablation Study of Fusion": "The fusion process is a critical component of the PruneFuse methodology, designed to integratethe knowledge gained by the pruned model into the original network. Our experiments reveal thatmodels trained with the fusion process exhibit significantly better performance and faster convergencecompared to those trained without fusion. By initializing the original model with the weightsfrom the trained pruned model, the fused model benefits from an optimized starting point, whichenhances its learning efficiency and generalization capability. illustrates the training trajectoriesand accuracy improvements when fusion takes places, demonstrating the tangible benefits of thisinitialization. These results underscore the importance of the fusion step in maximizing the overallperformance of the PruneFuse framework.",
  "A.4Ablation Study of Knowledge Distillation in PruneFuse": "demonstrates the effect of Knowledge Distillation on the PruneFuse technique relative tothe baseline Active Learning (AL) method across various experimental configurations and labelbudgets on CIFAR-10 and CIFAR-100 datasets, using ResNet-56 architecture. The results indicatethat PruneFuse consistently outperforms the baseline method, both with and without incorporatingKnowledge Distillation (KD) from a trained pruned model. This superior performance is attributed tothe innovative fusion strategy inherent to PruneFuse, where the original model is initialized usingweights from a previously trained pruned model. The proposed approach gives the fused model anoptimized starting point, enhancing its ability to learn more efficiently and generalize better. Theimpact of this strategy is evident across different label budgets and architectures, demonstrating itseffectiveness and robustness.",
  "A.5Ablation of Different Selection Metrics": "The impact of different selection metrics is presented in demonstrating clear differences inperformance across both the Baseline and PruneFuse methods on CIFAR-100 using ResNet-164architecture. It is evident that across both the baseline and PruneFuse methods, the Least Confidencemetric surfaces as particularly effective in optimizing label utilization and model performance.The results further reinforce that regardless of the label budget, from 10% to 50%, PruneFusedemonstrates a consistent superiority in performance with different data selection metrics (LeastConfidence, Entropy, Random, and Greedy k-centres) compared to Baseline.",
  "BAlgorithmic Details": "In this section, we provide a detailed explanation of the PruneFuse algorithm given in Algorithm 1.The PruneFuse methodology begins with structured pruning an untrained neural network, , to createa smaller model, p. This pruning step reduces complexity while retaining the networks essentialstructure, allowing p to efficiently select informative samples from the unlabeled dataset, U. Thealgorithm proceeds as follows. First, the original model is randomly initialized and pruned to obtainp. The pruned model p is then trained on an initial labeled dataset s0 to produce p. This trainingequips p with preliminary knowledge for data selection. The labeled dataset L is initially set to s0. Adata selection loop runs until the labeled dataset L reaches the maximum budget b. In each iteration,p is retrained on L to keep the model updated with new samples. Uncertainty scores for all samplesin U are computed using the trained p on the available labelel subset, and the top-k samples with thehighest scores are selected as Dk. These samples are labeled and added to L. Once the budget b ismet, the final trained pruned model p is fused with the original model to create the fused model F .This fusion transfers the weights from p to , ensuring the pruned models knowledge is retained.Finally, F is trained on L using knowledge distillation from p, refining the models performance byleveraging the pruned models learned insights. In summary, PruneFuse strategically adapts pruningin data selection problem and to enhance both data selection efficiency and model performance. Algorithm 1 Efficient Data Selection using Pruned NetworksInput: Unlabeled dataset U, Initial labeled dataset s0, labeled dataset L, original model , prunemodel p, fuse model F , maximum budget b, pruning ratio p."
}