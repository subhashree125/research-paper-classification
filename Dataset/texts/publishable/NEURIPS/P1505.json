{
  "Abstract": "Estimating causal quantities from observational data is crucial for understandingthe safety and effectiveness of medical treatments. However, to make reliable infer-ences, medical practitioners require not only estimating averaged causal quantities,such as the conditional average treatment effect, but also understanding the random-ness of the treatment effect as a random variable. This randomness is referred to asaleatoric uncertainty and is necessary for understanding the probability of benefitfrom treatment or quantiles of the treatment effect. Yet, the aleatoric uncertainty ofthe treatment effect has received surprisingly little attention in the causal machinelearning community. To fill this gap, we aim to quantify the aleatoric uncertaintyof the treatment effect at the covariate-conditional level, namely, the conditionaldistribution of the treatment effect (CDTE). Unlike average causal quantities, theCDTE is not point identifiable without strong additional assumptions. As a remedy,we employ partial identification to obtain sharp bounds on the CDTE and therebyquantify the aleatoric uncertainty of the treatment effect. We then develop a novel,orthogonal learner for the bounds on the CDTE, which we call AU-learner. Wefurther show that our AU-learner has several strengths in that it satisfies Neyman-orthogonality and is doubly robust. Finally, we propose a fully-parametric deeplearning instantiation of our AU-learner.",
  "Introduction": "Estimating causal quantities from observational data is crucial for decision-making in medicine . For example, medical practitioners are interested in estimating the effect of chemotherapyvs. immunotherapy on patient survival from electronic health records to understand the best treatmentstrategies in cancer care. Here, common estimation targets are averaged causal quantities such as theaverage treatment effect (ATE) and the conditional average treatment effect (CATE), yet averagedcausal quantities do not allow for understanding the variability of the treatment effect. What is needed for the reliability of causal quantities in medicine? To obtain reliable causal quantities,one often needs to move beyond the mean and consider the inherent randomness in thetreatment effect as a random variable. This randomness is referred to as aleatoric uncertainty . Quantifying the aleatoric uncertainty of the treatment effect is relevant in medical practice tounderstand the probability of benefit from treatment and the quantiles and variance of thetreatment effect . As an example, averaged quantities such as the CATE wouldsimply suggest a positive effect for some patients, while the probability of benefit from treatmentcan inform patients about the odds of being negatively affected by the treatment. Hence, aleatoricuncertainty of the treatment effect promises additional, fine-grained insights beyond simple averages.",
  "Target": "estimand XX2 scaled one-step bias correction DR-learnerAU-learner (CRPS)AU-learner ( ) Continuous ranked probability score (CRPS) Squared Wasserstein-2 distance ( ) ? XX3 or : Identification and estimation of the conditional distribution of the treatment effect (CDTE)(=our setting) compared to the (well-studied) identification and estimation of the CATE. In this paper,we focus specifically on the CDF of the CDTE, P(Y Y | x), shown in orange. Our maincontribution relates to the estimation, shown in yellow. However, moving from CATE identificationand estimation to our setting comes with important challenges:1 CATE (shown in green) is pointidentifiable but the CDTE is not (shown in blue); 2 there is no closed-form expression of the targetestimand in terms of nuisance functions and, because of that, CATE learners cannot be directlyadapted for estimation; and 3 CATE is an unconstrained target estimand whereas Makarov bounds(shown in gray ) are monotonous and contained in the interval . effect estimation was primarily focused on estimating averaged causal quantities . Some research aims to quantify the epistemic uncertainty in treatment effectestimation or the total uncertainty (but without distinguishing the types of uncertainty) . Other works focused on the aleatoric uncertainty of the potential outcomes 1or on contrasts between distributions of potential outcomes (also known as distributional treatmenteffects) .2 However, to the best of our knowledge, there is no comprehensivemeta-learning theory for the estimation of the aleatoric uncertainty in the treatment effect. In this paper, we aim to quantify the aleatoric uncertainty of the treatment effect at the covariate-conditional level in the form of a conditional distribution of the treatment effect (CDTE). Knowingthe CDTE would automatically allow one to compute the above-mentioned quantities of aleatoricuncertainty, namely, the probability of benefit from treatment and the quantiles and variance of thetreatment effect, at both population and covariate-conditional levels.",
  "Yet, the identification and estimation of the CDTE in contrast to CATE come with three challengesas follows (see ):": "Challenge 1 is that the CDTE does not allow for point identifiable, neither in the potential outcomesframework nor in randomized control trials due to the fundamental problem of causal inferenceas counterfactual outcomes can not be observed . We thus employ partial identification to obtain bounds on the CDTE and thereby quantify the aleatoric uncertainty of the treatmenteffect. Specifically, we focus on Makarov bounds that give sharp bounds for both thecumulative distribution function (CDF) and the quantiles of the CDTE. Challenge 2 is that there is no closed-form expression of the target estimand in terms of nuisancefunctions. Because of this, existing CATE learners cannot be directly adapted to our task of estimatingMakarov bounds. For example, there are no orthogonal learners in the general setting, and existing 1In the Neyman-Rubin potential outcomes framework , a potential outcome Y [a] refers to the value anoutcome variable would take for an individual under a specific treatment or intervention a. Each individual hasmultiple potential outcomes one for each possible treatment condition but only one of these outcomes isobserved.2Notably, (a) the distributional treatment effects and (b) the distribution of the treatment effect (our setting)are both different interpretationally and inferentially. That is, (a) provide contrasts between distributions ofpotential outcomes and are point identifiable, and (b) work on the distribution of the difference of potentialoutcomes and are only partially identifiable. See Appendix A for further details.",
  "approaches only use nave plug-in estimators/learners. Furthermore, even the derivation of theorthogonal loss is non-trivial as there is no efficient influence function at hand for the Makarovbounds": "Challenge3 is that CATE is an unconstrained target estimand whereas Makarov bounds aremonotonous and contained in the interval . Notably, any constraints of the target estimandcould be violated by orthogonal learners . Therefore, an orthogonal learner for Makarovbounds needs to be carefully adapted, especially to perform well in low-sample settings.",
  "Epistemic": "uncertainty Treatment effect, Patient characteristic, ? ? Aleatoric uncertainty? : Total uncertainty of the treat-ment effect can have different sources.Both upper and lower plots have thesame total uncertainty but vastly dif-ferent aleatoric and epistemic compo-nents. Yet, aleatoric uncertainty is non-identifiable (see Challenge 1 ). In this paper, we develop a novel, orthogonal learner forestimating Makarov bounds which we call AU-learner,which allows to quantify the aleatoric uncertainty of thetreatment effect. Our AU-learner addresses all of theabove-mentioned challenges1 3 . Further, our AU-learner has several useful theoretical properties, such assatisfying Neyman-orthogonality and double robustness.Finally, we propose a flexible, fully-parametric deep learn-ing instantiation of our AU-learner. For this, we makeuse of conditional normalizing flows and call our methodAU-CNFs.",
  "Related Work": "In the following, we briefly summarize the existing works on uncertainty quantification in the potentialoutcomes framework; on the identification of the CDTE; and on the estimation of Makarov bounds.For a more detailed overview of literature, we refer to Appendix A. Uncertainty quantification in the potential outcomes framework. The (total) uncertainty of apredictive model in machine learning is generally split into (a) epistemic and (b) aleatoric uncertainty.4 This split is important, as it informs a decision-maker about the source of uncertainty (see), especially in the context of the potential outcomes framework. (a) Epistemic uncertainty wasstudied for predictive models targeting at identifiable averaged causal quantities, such as conditionalaverage potential outcomes (CAPOs) and CATE . (b) Aleatoric uncertainty, on the otherhand, is only identifiable for potential outcomes . Prominent methods focus on interventional(counterfactual) quantities such as: (i) CDF/quantiles estimation ; (ii) densityestimation ; and (iii) distributional distances (also known as distributionaltreatment effects) estimation . Yet, our work differs substantially from the above,as we aim at inferring the aleatoric uncertainty of the treatment effect, which is only partiallyidentifiable. 3Code is available at uncertainty relates to the uncertainty of fitting a model on finite data and reduces to zero asdata size grows. In contrast, aleatoric uncertainty originates from the inherent randomness of the outcome and isirreducible wrt. data size.",
  "(A-)IPTW: (augmented) inverse propensity of treatment weighted; DR: doubly robust": "Identification of the distribution of the treatment effect. Point identification of the distribution ofthe treatment effect (or, equivalently, a joint distribution of potential outcomes) is only possible underadditional assumptions on the data-generating mechanism. A common example is, e. g., invertibilityof latent outcome noise . Other works have rather focused on partial identification . Forexample, proposed sharp bounds for the distribution of the treatment effect under a monotonicityassumption. Later, assumption-free sharp bounds were proposed for both the joint CDF of potentialoutcomes and for the variance of treatment effect , both known as Frchet-Hoeffding bounds. Finally, proposed sharp bounds on the CDF/quantiles of the treatment effectwithout any additional assumptions, so-called Makarov bounds . Makarov bounds werefurther generalized and applied to other settings but different from ours. Estimation of Makarov bounds. provides a comparison of key methods for estimatingMakarov bounds, at both covariate-conditional and population levels. Existing methods build mainlyupon plug-in (single-stage) estimators/learners. Examples are methods tailored for randomizedcontrolled trials and for potential outcomes framework . Crucially, thesemethods are not orthogonal and, thus, are sensitive to the misspecification of the nuisance functions.Nevertheless, we include the latter methods as baselines for our experiments as they usea highly flexible CDF estimator based on kernel density estimators. Some works also developedefficient estimators for Makarov bounds at the population level (analogous to the two-stage orthogonallearners at the covariate-conditional level) but only in highly restricted settings. In particular, is restricted to binary outcomes, assumed a known propensity score, and made specialoptimization assumptions.5 In addition, all three works suggest fixing a value of /,which the CDF/quantiles of the treatment effect are evaluated at; when our work suggests targetingat several values of / at once. Therefore, the previous methods are not applicable to our generalsetting of estimating covariate-conditional level Makarov bounds.",
  "Identification of Distribution of Treatment Effect": "Notation. Let capital letters X, A, Y, denote random variables and small letters x, a, y, theirrealizations from domains X, A, Y, . Let P(Z) denote a distribution of some random variable Z,and let P(Z = z) be the corresponding density or probability mass function. Furthermore, (x) =P(A = 1 | X = x) is propensity score, a(x) = E(Y | X = x, A = a) are conditional expectations,and Fa(y | x) = P(Y y | x, a) is a conditional outcome CDF. For other conditional quantitiesor distributions, we use short forms whenever possible; e. g., E(Y | x) = E(Y | X = x). Further,Pn{f(Z)} = 1 nni=1 f(zi) is a sample average of a random f(Z), where n is the sample size. Wedenote linear rectifier functions as [x]+ = max(x, 0) and [x] = min(x, 0), and sup/inf convolutionsof two functions f1( | x), f2( | x) as (f1 f2)Y( | x) = supyY{f1(y | x) f2(y | x)}and (f1 f2)Y( | x) = infyY{f1(y | x) f2(y | x)}. Problem setup. We consider the standard setting of the NeymanRubin potential outcomes frame-work . That is, we have an observational dataset D with a binary treatment A A = {0, 1},potentially high-dimensional covariates X X Rdx and a continuous outcome Y Y R. Forinstance, a typical scenario is in cancer therapy, where the outcome is tumor growth, the treatment iswhether chemotherapy is given, and the covariates include patient details like age and sex. We definea joint random variable Z = (X, A, Y ). D = {xi, ai, yi}ni=1 is sampled i.i.d. from the observational 5The work in assumes the possibility of finding feasible Kantorovich dual functions to a target functional(e. g., CDF of the treatment effect). By doing so, the authors are able to infer valid partial identification boundson very general functionals; yet, the sharpness can not be practically guaranteed. distribution P(Z) = P(X, Y, A), where n is the sample size. The potential outcomes frameworkthen makes three (causal) assumptions, i. e., (1) consistency: if A = a, then Y [a] = Y ; (2) overlap:P(0 (X) 1) = 1; and (3) exchangeability: A (Y , Y ) | X. Treatment effect distribution. In this paper, we refer to the treatment effect = Y Y as arandom variable.6 The CATE is given by (x) = E( | x), which is identifiable as 1(x) 0(x)under the causal assumptions (1)(3). We are interested in identifying a conditional distribution ofthe treatment effect (CDTE), specifically, its CDF or quantiles:",
  "F1( | x) = inf{ | F( | x)}, .(2)": "The CDF and the quantiles of the CDTE are point non-identifiable due to the fundamental problemof causal inference, i. e., that the counterfactual outcome, Y [1 A], is never observed. This isillustrated in , where both conditional potential outcome distributions are identifiable asP(Y [a] | x) = P(Y | x, a); but a conditional joint distribution, P(Y , Y | x), and the CDTE,P( | x), are not. Partial identification of the CDTE. Fan et al. proposed pointwise sharp bounds on the CDFand the quantiles of the CDTE, so-called Makarov bounds . Given that the outcome Yis continuous, the Makarov bounds for the CDF of the CDTE are given by linearly rectified sup/infconvolutions of conditional CDFs of potential outcomes:",
  "(F11 F10 )( 1 | x),if = 1,": "F11 (1 | x) F10 (0 | x),if = 1,(4)where , and F1a (u | x) are the quantiles of P(Y [a] | x). Then, under the causalassumptions (1)(3), conditional distributions of potential outcomes coincide with observed ones,i. e., P(Y [a]) = P(Y | x, a). Notably, Makarov bounds on the CDFs are CDFs themselves, but theseCDFs do not correspond to the solution of the partial identification task (which implies pointwisesharpness). We refer to Appendix B for more illustrations about the inference of the Makarov bounds,the explanation of pointwise sharpness, and Makarov bounds for categorical/mixed-type outcomes.",
  "An AU-learner for estimating Makarov bounds": "In the following, we develop a theory of orthogonal learning for Makarov bounds, which then givesrise to our AU-learner. For this, we first review the plug-in learner and its shortcomings. Motivated bythis, we then derive two-stage learners. Here, we first present a novel CA-learner in an intermediatestep and finally our AU-learner. Note that both are novel but we frame our contributions around theAU-learner because of favorable theoretical properties. For notation, we use over- and underlines asin, e. g., F( | x) to refer to the upper and/or lower bound.",
  "Also known as an individual treatment effect. The term individual should not be confused with the termcovariate-conditional, which refers to the causal quantity and not the random variable itself": "Challenge 1 : An example showing point non-identifiability of the distribution of the treatment effectbased on the i = 7-th instance of the semi-synthetic IHDP100 dataset . Shown are two data-generation models, indistinguishable in potential outcomes framework or RCTs, i. e., a monotone,Mm, and an antitone, Ma. For both models we also plot (a) conditional densities of potentialoutcomes, P(Y [a] = y | x7) and conditional joint laws of potential outcomes, P(Y , Y | x7);and (b) corresponding CDFs of the CDTE (shown in blue), F( | x7) = P(Y Y | x7),together with Makarov bounds (shown in gray ) and point identifiable CATE (shown in green),(x7) = E( | x7) 2.342. Non-identifiability of the CDTE is easy to see: Both data-generationmodels have the same conditional distributions of potential outcomes but different conditional jointlaws and, thus, different CDTEs. The latter figures, (b), also demonstrate the bounds on the probabilityof benefit from treatment (a special case of Makarov bounds), P(Y Y 0 | x7) [0, 0.242].Hence, Makarov bounds are informative almost everywhere (except = (x7)). Shortcomings. The plug-in learner suffers from two important shortcomings . (a) The plug-inlearner does not account for the selection bias, meaning that F1 is estimated better for the treatedpopulation and F0 for the untreated. Hence, it could be necessary to re-weight the loss wrt. to thepropensity score. A remedy is to employ an inverse propensity of treatment weighted (IPTW) learnerfor both F0 and F1 (see Appendix C). (b) The plug-in learner does not target the Makarovbounds directly but rather the conditional outcome distributions. Therefore, it is unclear how toincorporate an inductive bias that the Makarov bounds are less heterogeneous than either of theconditional outcome CDFs (i. e., the Makarov bounds can depend on a subset of covariates X). Thesecond shortcoming thus motivates our derivation of two-stage learners.",
  "Two-stage learners for Makarov bounds": "In order to address the above shortcomings of the plug-in learners, a two-stage learning theory wasproposed . Yet, the two-staged learning theory is primarily built for simple targetestimands (e. g., CATE) and therefore requires non-trivial adaptations by us to extend to Makarovbounds, which we do in the following. Working model & target risk. Our two-stage learners seek to find the best approximation of theground-truth Makarov bounds, functional target estimands, by a (parametric) working model G.Formally, the working model is given G = {g(, x) | g : X ; g(, x)is non-decreasing}.The best approximation g is then obtained by minimizing a (population) target risk via g =arg mingG L(g).7 In our setting, L is chosen as some distributional distance between the targetestimands and the working model. Specifically, we use continuous ranked probability score (CRPS) as a target risk for learning Makarov bounds on the CDF via",
  "F1( | X) g1(, X)2 d.(7)": "7By postulating a restricted working model class, G, we might compromise on the sharpness and end uphaving looser bounds. Yet, this looseness bias is only relevant in the infinite data regime. In the finite-sampleregime, the feasibility of the low-error estimation is a much more important problem. The target risks in Eq. (6) and (7) cannot be directly minimized as we do not observe the ground-truthMakarov bounds. Yet, due to the identifiability results in Sec. 3, the Makarov bounds depend on thenuisance functions, Fa(y | x), which can be estimated from the observational data. We perform thatin the following.",
  ": Comparison of learners forestimating Makarov bounds": "From now on, we denote the ground-truth nuisance functionsas and their estimates as . Also, we make the dependenceon the target risks of the nuisance functions explicit; that is,we write the target risk as L(g, ) and the target estimandsas F( | X; ) and F1( | X; ). Covariate-adjusted learner. A straightforward way to es-timate and then minimize the target risk is to plug-in the esti-mates of conditional outcome CDFs, Fa(y | x), into Eq. (6)and (7), respectively. This yields the so-called covariate-adjusted (CA) learner, which aims at minimizing the follow-ing losses (empirical risks):",
  "where we call both FPI( | x; ) = FPI( | x) and FPI1( | x; ) = FPI1( | x) pseudo-CDFs andpseudo-quantiles, respectively, and where both can be obtained from Eq. (5)": "The CA-learner addresses the shortcoming (b) of the plug-in learner from above in that loss mini-mization in the equations above targets directly at Makarov bounds. However, the shortcoming (a)of the selection bias still persists. Furthermore, a new shortcoming (c) now emerges: The lossescan be highly sensitive to badly estimated nuisance functions so that LPI(g, ) and LPI(g, ) differsignificantly. Next, we develop an orthogonal learner that addresses all of the shortcomings. One-step bias correction. In order to address the before-mentioned shortcomings of the CA-learner,we employ the concept of (Neyman-)orthogonal losses . Informally, orthogonal losses arefirst-order insensitive to the misspecification of the nuisance functions, which introduces manyfavorable properties such as double robustness . The CA-learner losses in Eq. (8) and (9) can bemade orthogonal by performing a one-step bias correction . The one-step bias correctionrequires the knowledge of an efficient influence function, which has not yet been derived for Makarovbounds ( Challenge 2 ). Hence, the following theorem presents one of our main theoretical results.Theorem 1 (Efficient influence function for Makarov bounds). Let P denotes P(Z) = P(X, A, Y ),and let yY( | x) and u ( | x) be argmax/argmin sets of the convolutions (F1 F0)Y( | x) and(F11 F10 )( 0 | x), respectively. Then, under mild conditions on the conditional outcomedistributions and for almost all values of and for all values of (0, 1) (see Appendix D),average Makarov bounds are pathwise differentiable. Further, the corresponding efficient influencefunctions, , are as follows:",
  "Proof. See Appendix D": "In the above theorem, we use red color to show the nuisance functions of P that are influencing thetarget estimand, i. e., averaged Makarov bounds. Therein, we also provide a Corollary 1, where wederive efficient influence functions for the target risks from Eq. (6) and (7), namely (L(g); P). Note on infinite argmax/argmin sets. When argmax/argmin sets of the sup/inf-convolutions areinfinite, average Makarov bounds are pathwise non-differentiable, and, thus, one-step bias correctionis not possible as statistical inference becomes non-regular . This result also holds for othercausal quantities that contain sup/inf operators (e. g., for the policy value of the optimal treatmentstrategy ). Although, there exist approaches to perform inference in the non-regular setting, we focus solely on the regular setting where pathwise differentiability holds (see Appendix Dfor a discussion on the generality of such a setting). Orthogonal leaner (AU-learner). Given the derived efficient influence function for the target risks,we perform a -scaled one-step bias correction of the CA-learner losses, namely, LPI(g, ) +Pn(L(g); P). The latter then yields our novel orthogonal AU-learner (see Corollary 2 inAppendix D). Our AU-learner effectively resolves all the above-mentioned shortcomings (see acomparison in ). Formally, it aims at minimizing one of the following losses:",
  "FDR(, Z; , ) = FPI( | X; ) + C(, Z; )andFDR1(, Z; , ) = FPI1( | X; ) + C1(, Z; ),": "where C(, Z; ) and C1(, Z; ) are given by Eq. (11) and (12), respectively; and (0, 1] is ascaling hyperparameter. We present a meta-algorithm of our AU-learner (with the CRPS target risk)based on cross-fitting in Algorithm 1 (AU-learner with the W 22 target risk follows analogously). Scaling hyperparameter. The scaling hyperparameter is introduced to tackle Challenge 3 fromabove, namely, that the pseudo-CDF term, FDR(, Z; , ), is not guaranteed to be a valid CDF for > 0 (both monotonicity wrt. and -constraint can be violated).8 The same happens with thepseudo-quantiles of the AU-learner, FDR1(, Z; , ), which could be non-monotonous wrt. . Themain intuition behind scaling is that it interpolates between the full AU-learner ( = 1), that hasfavorable theoretical properties; and the CA-learner ( = 0), for which the pseudo-CDFs and pseudo-quantiles are valid CDFs and quantiles, respectively (we refer to Appendix E with visual examples).Hence, scaling mimics a learning rate of a Newton-Raphson method (usually considered as an analogyto the one-step bias correction ). We found fixed values for the scaling hyperparameter to workwell in all of our experiments and to improve the low-sample performance of our AU-learner.",
  "g = arg mingG LAU, CRPS(g, )": "In the following, we formulate our sec-ond main theoretical result. For the re-sults to hold, the nuisance functions =(, F0, F1)/ = (, F10 , F11 ) need to beestimated independently from the secondstage model g/g1. This could be done byeither assuming a not-too-flexible class ofmodels (such as a Donsker class of estima-tors and fitting all the models on the samedataset D) or by using a generic approachof cross-fitting . Theorem 2 (Neyman-orthogonality ofAU-learner (informal)). Under the assumptions of the Theorem 1, the following holds for AU-learnerfrom Algorithm 1 with the scaling hyperparameter = 1:1. Neyman-orthogonality. Losses in Eq. (13) and Eq. (14) are first-order insensitive wrt. to themisspecification of the nuisance functions.",
  ". Rate double robustness. The bias from the misspecification of the nuisance functions is of secondorder and vanishes at the same rate as the fastest estimated nuisance functions": "8The issue of pseudo-outcomes violating constraints was also raised wrt. DR-learner for CATE when the outcome space is bounded (e. g., Y = ) and for the CDFs of potential outcomes . Yet, theoptimal solution remains an open research question. We refer to the Appendix D for the detailed formulation of the theorem and the proof. Notably, therate double robustness has two important implications. (i) If at least one of the nuisance functions isestimated consistently, the target estimands are also estimated consistently. (ii) It allows to achieve aso-called quasi-oracle property . This means that our AU-learner with (sufficiently fast) estimatednuisance functions performs nearly identical to the AU-learner with the ground-truth nuisancefunctions.",
  "Neural instantiation with AU-CNFs": "We now introduce a flexible fully-parametric instantiation of our AU-learner, which we call AU-CNFs.Therein, we employ conditional normalizing flows (CNFs) as main backbone for ourAU-learner. CNFs are a flexible neural probabilistic method with tractable conditional densities,CDFs, and quantiles. Importantly, all three attributes of the CNFs (densities, CDFs, and quantiles)can be used for both training via back-propagation and inference , which makes them a perfectmodel for both stages of our AU-learner. Architecture. The architecture of our AU-CNFs is inspired by interventional normalizing flows(INFs) (a two-stage model for efficient estimation of potential outcomes densities). Our AU-CNFs consist of several CNFs corresponding to the two stages of learning, namely a nuisance CNF,which fits the nuisance functions, (, F0, F1) or, equivalently, (, F10 , F11 ); and two target CNFs,which implement second stage working models for upper and lower bounds, G and G, respectively. Training & implementation. At the first stage of AU-CNFs learning, the nuisance CNF aims atmaximizing the conditional log-likelihood and minimizing a binary cross-entropy via a joint loss.Then, we generate the pseudo-CDFs and pseudo-pseudo quantiles, as described in Algorithm 1.Therein, we set the /-grid size to n = n = 50 and discretize the Y-space/-interval to inferthe argmax/argmin values, y/u. Then, we proceed with the second stage of AU-CNFs learning,where we set = 0.25 for the CRPS loss and = 0.01 for the W 22 loss. We found the fixed valuesof to work well in all of the synthetic and semi-synthetic experiments (except for the IHDP100dataset, where the overlap assumption is violated). We use the same training data for two stages oflearning, as (regularized) CNFs as neural networks belong to the Donsker class of estimators .We refer to Appendix F for more details on our AU-CNFs.",
  "Experiments": "We now evaluate our AU-learner. For this, we use (semi-)synthetic benchmarks with the ground-truthconditional CDFs/quantiles of potential outcomes, i. e., Fa(y | x)/F1a (u | x). In this way, we caninfer the ground-truth Makarov bounds and use them for evaluation. Evaluation metric. We use evaluation metrics based on the target risks (as introduced in Sec 4.2).Specifically, we report root continuous ranked probability score (rCRPS) and Wasserstein-2 distance(W2) based on training data (in-sample) and test data (out-sample). Baselines. We compare the proposed hierarchy of learners from Sec. 4 with CNFs as backbones.These are the plug-in learner (Plug-in CNF), IPTW-learner (IPTW-CNF), CA-learners (CA-CNFs(CRPS / W 22 )), and AU-learners (AU-CNFs (CRPS / W 22 )). The only relevant baseline found in theliterature is a plug-in learner based on kernel density estimation .9 For this, we useddistributional kernel mean embeddings (Plug-in DKME), a standard conditional kernel densityestimation method. Details on the baselines are in Appendix G. Synthetic data. We adapt the synthetic data generator (dx = 2) from by creating threesettings with different conditional outcome distributions: normal, multi-modal, and exponential (seedata generation details in Appendix H). In the synthetic data, the ground-truth Makarov boundsare less heterogeneous than the potential outcomes, and, hence, two-stage learners are expectedto perform the best. We sample ntrain {100; 250; 500; 750; 1000} training and ntest = 1000 testdatapoints. The out-sample results are in . Here, our AU-CNFs perform the best wrt. rCRPS inthe majority of settings and different sizes of training data. We also report the results wrt. W 22 inAppendix I.",
  ": Results for synthetic experiments with varying sizeof training data, ntrain, in 3 settings: normal, multi-modal, andexponential. Reported: mean out-sample rCRPS over 20 runs": "HC-MNIST dataset. HC-MNIST is a high-dimensional semi-synthetic dataset (dx = 785), builton top of the MNIST dataset (see details in Appendix H). Here, the heterogeneity of Makarovbounds is also smaller than that of potential outcomes, reflecting inductive biases in the real world.We report the out-sample performance of different methods in (the Plug-in DKME is omitteddue to a too-long runtime). Therein, our AU-CNFs (CRPS) achieve the best performance and, thus,scale well with the dataset size and the dimensionality of covariates. Further, our AU-CNFs (W 22 )improve the performance of CA-CNFs (W 22 ). In general, we observe that the loss based on the CDFdistance (i. e., CRPS) has a lower variance and is easier to fit. In Appendix I, we additionally reportthe results for another popular semi-synthetic benchmark, IHDP100 . Case study. In Appendix J, we provide a real-world case study based on the observational datasetfrom . Therein, we demonstrate how our AU-learner (AU-CNFs) can be used to estimate theeffectiveness of lockdowns during the COVID-19 pandemic. We estimate the probability of benefitfrom intervention (a special case of Makarov bounds with = 0). As expected, we observe a drop inthe incidence rate is highly probable after the implementation of a strict lockdown.",
  "Discussion": "Low-sample & asymptotic performance. In several experiments, especially in low-sample settings,the CA-learner or even the plug-in approach are performing nearly as well or even sometimes betterthan the AU-learner. This can be expected, as the best low-sample learner and the asymptoticallybest learner can, in general, be different , and there is no single one-fits-all data-driven solutionto choose the former one . This can be explained by too small dataset sizes or the severe overlapviolations (as is the case with the IHDP100 dataset; see Appendix I). Yet, only our doubly robustAU-learner offers asymptotic properties in the sense that it is asymptotically closest to the oracle(see ). We thus argue for a pragmatic choice in practice (i. e., in the absence of ground-truthcounterfactuals or additional RCT data) where our AU-learner should be the preferred method for thecovariate-conditional Makarov bounds even in low-sample data. Future work. Our work sets a foundation for several extensions to estimate covariate-conditionalMakarov bounds. For example, the estimation of the interval probabilities (see Appendix B) of thetreatment effect can provide a connection with the existing works on total uncertainty with conformalprediction . Additionally, one might want to study possible extensions of Makarov bounds tailoredto high-dimensional outcomes. Limitations & broader impact. Our work is subject to the standard assumptions of the potentialoutcomes framework. We further make assumptions on the outcome distribution, though these arevery mild. Nevertheless, we expect our work to have a positive impact, as it will help to improve thereliability of decision-making in medicine and other safety-critical fields. Conclusion. We are the first to offer a theory of orthogonal learning to quantify the aleatoricuncertainty of the treatment effect at the covariate-conditional level and present flexible neuralinstantiation. This paper is supported by the DAAD program Konrad Zuse Schools of Excellence in ArtificialIntelligence, sponsored by the Federal Ministry of Education and Research. Additionally, the authorswould like to thank Lars van der Laan, Dennis Frauen, and Alicia Curth for their helpful remarksand comments on the content of this paper. SF also acknowledges funding from the Swiss NationalScience Foundation (SNSF) via Grant 186932.",
  "Nicolas Banholzer et al. Estimating the effects of non-pharmaceutical interventions on thenumber of new infections with COVID-19 during the first epidemic wave. In: PLoS one16.6 (2021), e0252827": "Elias Bareinboim et al. On Pearls hierarchy and the foundations of causal inference. In:Probabilistic and Causal Inference: The Works of Judea Pearl. Association for ComputingMachinery, 2022, pp. 507556. Ioana Bica et al. From real-world patient data to individualized treatment effects usingmachine learning: Current and future methods to address underlying challenges. In: ClinicalPharmacology & Therapeutics 109 (2021), pp. 87100.",
  "Alicia Curth, Ahmed M. Alaa, and Mihaela van der Schaar. Estimating structural target func-tions using machine learning and influence functions. In: arXiv preprint arXiv:2008.06461(2020)": "Alicia Curth and Mihaela van der Schaar. In search of insights, not magic bullets: Towardsdemystification of the model selection dilemma in heterogeneous treatment effect estimation.In: International Conference on Machine Learning. 2023. Alicia Curth and Mihaela van der Schaar. Nonparametric estimation of heterogeneoustreatment effects: From theory to learning algorithms. In: International Conference onArtificial Intelligence and Statistics. 2021.",
  "Andrew Ying. A geometric perspective on double robustness by semiparametric theory andinformation geometry. In: arXiv preprint arXiv:2404.13960 (2024)": "Yao Zhang, Alexis Bellot, and Mihaela van der Schaar. Learning overlapping representationsfor the estimation of individualized treatment effects. In: International Conference onArtificial Intelligence and Statistics. 2020. Zhehao Zhang and Thomas S. Richardson. Bounds on the distribution of a sum of two ran-dom variables: Revisiting a problem of Kolmogorov with application to individual treatmenteffects. In: arXiv preprint arXiv:2405.08806 (2024).",
  "AExtended Related Work": "Partial identification and sensitivity models in the potential outcomes framework. Interventionalquantities (such as CAPOs and distributions of potential outcomes) and some counterfactual quantities(e. g., CATE) are point identifiable in potential outcomes framework (see ). However, relaxationof the unconfoundedness assumption renders those quantities partially identifiable or even non-identifiable identifiable . In this case, additional sensitivity models can be employed. Examplesare the marginal sensitivity model and outcome sensitivity model. Other approaches suggest using instrumental variables or noisy proxy variables. Counterfactual quantities, on the other hand, are inherently non-identifiable (e. g., jointdistribution of potential outcomes , expected counterfactual outcomes of (un)treated , anddistribution of treatment effect ). There is no general approach for deriving sharp bounds forpartially identifiable causal quantities, therefore the above-mentioned works are not relevant in oursetting.",
  "Interventional": "Ladder of causation Potentialoutcomesframework, RCTsconditional distribution of outcome conditional average potential outcome conditional distribution of treatment effect conditional average treatment effect Example of causal diagrams conditional average outcome conditional distribution of potential outcome Additivity ofexpectation : Pearls ladder of causation containing different observational, interventional,and counterfactual quantities related to the potential outcomes framework. Here, X are covariates,A is a binary treatment, and Y is a continuous outcome. We also plot three exemplar causaldiagrams, satisfying the assumptions of the potential outcomes framework, for each layer of causation,correspondingly. Quantities with the light gray background can be expressed with the quantitiesfrom lower layers (e. g., conditional average treatment effect) and quantities with yellow backgroundrequire the information from the same layer. In this paper, we are interested in the CDTE, P(Y Y | x), shown in orange. Notably, point non-identifiability of the CDTE can be seen with aparallel worlds network (the causal diagram of the counterfactual layer). Distributional treatment effects and the distribution of the treatment effect. There are twoseemingly similar notions in the existing literature: (a) distributional treatment effects and (b) the distribution of the treatment effect . However, (a) and (b) have twoimportant differences: (i) Interpretation. Distributional treatment effects represent the differences between differentdistributional aspects of the potential outcomes (e.g., Wasserstein distances, KL-divergence, orthe quantile differences) . Hence, they can answer questions like How are 10% of theworst-possible outcomes with treatment different from the worst 10% of the outcomes withouttreatment?. Here, the two groups (treated and untreated) of the worst 10% contain, in general,different individuals. This is problematic in many applications like clinical decision supportand drug approval. Here, the aim is not to compare individuals from treated vs. untreatedgroups (where the groups may differ due to various, unobserved reasons). Instead, the aim is toaccurately quantify the treatment response for each individual and allow for quantification of thepersonalized uncertainty of the treatment effect. The latter is captured in the distribution of thetreatment effect, which allows us to answer the question about the CDF/quantiles of the treatmenteffect. For example, we would aim to answer a question like What are the worst 10% of valuesof the treatment effect?. Here, we focus on the treatment effect for every single individual.The latter is more complex because we reason about the difference of two potential outcomessimultaneously. Hence, in natural situations when the potential outcomes are non-deterministic,both (a) the distributional treatment effect and (b) the distribution of the treatment effect willlend to very different interpretations, especially in medical practice. In particular, the distributionof the treatment effect (which we study in our paper) is important in medicine, where it allowsquantifying the amount of harm/benefit after the treatment . This may warn doctors about",
  "situations where the averaged treatment effects are positive but where the probability of thenegative treatment effect is still large": "(ii) Inference. The efficient inference of the distributional treatment effects only requires the esti-mation of the relevant distributional aspects of the conditional outcome distributions (e. g., quan-tiles) and the propensity score . However, in our setting of the bounds on the CDF/quantilesof the treatment effect, we also need to perform sup/inf convolutions of the CDF/quantiles of theconditional outcomes distributions. Hence, while the definitions of (a) the distributional treatmenteffects and (b) the distribution of the treatment effect appear related, their estimation is verydifferent.",
  "The interval probabilities, P(1 Y Y 2) . The sharp bounds on the intervalprobabilities are only defined implicitly and are also, in general, different from Makarov bounds(see Appendix B)": "We are not aware of other bounds for measuring aleatoric uncertainty (e. g., kurtosis, skewness,or entropy). Importantly, the above-mentioned bounds on different measures of uncertainty areorthogonal to our work, and we focus on the bounds on the CDF/quantiles of the treatment effect. Efficient estimators and orthogonal learners. Efficient estimation of causal quantities (targetestimands) was studied in the scope of (i) semi-parametric efficient estimation theory and, moregeneral, (ii) orthogonal learning theory. Both theories (i) and (ii) rely on the concept of influencefunctions (pathwise derivatives) , which allow performing a one-step bias correction for(i) a plug-in estimator of the target estimand or (ii) the population risk containing target estimand,respectively. (i) Semi-parametric efficient estimation theory provides asymptot-ically efficient estimators for finite-dimensional target estimands (e. g., average treatment effect (ATE)and average potential outcomes (APOs)). On the other hand, (ii) orthogonal learning theory (ordebiased ML) was developed for infinitely-dimensional (functional) estimands, like,CATE and CAPO. Orthogonal learning theory (or orthogonal learners) estimates target estimandsby minimizing (Neyman-)orthogonal losses that are first-order insensitive to the misspecification ofthe nuisance functions. Specific examples of orthogonal learners for identifiable quantities includeCAPO learners and CATE learners . Estimation of partially identifiable quantities. Orthogonal learners were also proposed for boundson partially identifiable causal quantities. Examples include different interventional quantities such,e. g., in marginal sensitivity model , in instrumental variables setting , in noisy proxy variablessetting . Examples from counterfactual quantities are, e. g., the variance of treatment effectand joint CDF of potential outcomes (Frchet-Hoeffding bounds) . More generally, estimation andlearning for partially identifiable quantities was studied in econometrics as estimation of intersectionbounds . In this paper, we construct a novel orthogonal learner targeting at Makarovbounds on the CDF/quantiles of the CDTE. Total uncertainty quantification. Total uncertainty in potential outcomes framework can be generallyquantified with two approaches: (i) Bayesian methods (e. g., Gaussian processes) and (ii) conformalprediction framework. (i) Bayesian methods allow to infer posterior predictive distributionsor credible intervals and for both potential outcomes and treatment effect, but under additional identi-fiability assumptions (e. g., an assumption of additive latent outcome noise, which renders treatmenteffect distribution identifiable). (ii) Conformal prediction aims at providing a valid predictive intervaland was applied to quantify total uncertainty of predicting potential outcomes andtreatment effect . The latter works either make a similar additive latent outcomenoise assumption or hide non-identifiable treatment effect into the predictive interval (however,oracle predictive interval could never be reached in this case). We argue that total uncertainty needsto be split into epistemic and aleatoric explicitly to provide insights on the origin of uncertainty, especially in our setting where aleatoric uncertainty of the treatment effect itself is partiallyidentifiable (see ).",
  "B.1Bounds Construction": "In the following, we provide an additional intuition on how Makarov bounds are inferred fromconditional distributions of the potential outcomes, P(Y [a] | x). For example, Makarov bounds forthe CDF of the CDTE are a composition of the sup/inf convolutions, applied to the conditional CDFsof the potential outcomes, and the rectifier functions (see ). Makarov bounds can be inferred (i) analytically or (ii) numerically. (i) Analytic formulas wereproposed for very simple distributions (e. g., a normal distribution ). At the same time, the(ii) numerical approach is more flexible. For example, we can discretize the Y-space or -intervaland perform maximization/minimization on that grid. Notably, in our experiments, we infer theground-truth bounds with the approach (i), when the ground-truth conditional potential outcomesdistributions are normal; and (ii) otherwise. : A example of the inference of Makarov bounds on the CDF of the CDTE based on i = 7-thinstance of the semi-synthetic IHDP100 dataset . The construction of the upper bound is shownin (a) and (b); and the lower bound corresponds to subfigures (c) and (d). The subfigures on theleft, (a) and (c), contain the conditional CDFs of both potential outcomes, namely, F0( | x7) andF1( | x7). Therein, the conditional CDF F0( | x7) is shifted wrt. four values of . The figures onthe right, (b) and (d), then demonstrate the corresponding Makarov bounds values for the same fourvalues of (shown in red). We also plot the full Makarov bounds with a gray color.",
  "B.2Pointwise and Uniformly Sharp Bounds": "The sharpness of Makarov bounds proposed in has one important characteristic. Specifically,although the Makarov bounds on the CDFs, F( | X), are CDFs themselves, they are not validsolutions to the partial identification task. This can be easily checked, i. e., the expectation wrt. to theMakarov bounds does not coincide with the point-identifiable CATE, (x) = E( | x):",
  "(15)Therefore, there is an important distinction between so-called pointwise sharpness and a uniformsharpness": "In the case of uniform sharpness, a sharp bound coincides with the solution to the partial identificationtask. This implies that if a bound is uniformly sharp, then the joint bound on the set of quantities, evaluated in two (or more) points of or , is also sharp. Many known bounds (e. g., Frchet-Hoeffding bounds , and marginal sensitivity model bounds ) are uniformlysharp. Yet, Makarov bounds are only pointwise sharp. Recent works developed uniformly sharp bounds on the CDF of the CDTE . However, theirinference requires a special computational routine for every value of / wrt. the CDF/quantilesof the CDTE. Their usage is further complicated by the fact that the CDFs of the uniformly sharpbounds correspond to mixed-type discrete/continuous random variables. We show an example of theuniformly sharp bounds on the CDF for = 0 in . The uniformly sharp bounds may be usefulfor more complex aleatoric uncertainty quantities (e. g., interval quantities like P(1 Y Y 2 | x)) or simultaneous bounds on the variance and the CDF of the CDTE. Nevertheless, in manypractical applications, pointwise sharp bounds (Makarov bounds) are enough and we focus on thosein our paper.",
  "F0( | x7)": ": Comparison of pointwise (Makarov) and uniformly sharp bounds on the CDF of the CDTEbased on i = 7-th instance of the semi-synthetic IHDP100 dataset . Pointwise (Makarov) boundsare shown in gray . Also, we show uniformly sharp bounds inferred for F(0 | x7) = P(Y Y 0 | x7), namely F0( | x7) (shown in red). We display the lower bound in (a) and the upper boundsin (b). Notably, the expectation wrt. F0( | x7) coincides with the CATE, (x7) = E( | x7).",
  "CIPTW-learner": "In the following, we develop an improved single-stage learner, namely, an inverse propensity oftreatment weighted (IPTW)-learner. First, we revisit the plug-in learner by defining its estimationobjective. Then, we introduce the IPTW-learner, which addresses one of the shortcomings of theplug-in learner. At the end, we mention a surprising property of the IPTW-learner, namely, theorthogonality wrt. to target risk aiming at the potential outcome distributions.",
  "LS/T,a( = Fa) = Pn1{A = a} lY, Fa( | X),(28)": "where l(, ) > 0 is a probabilistic loss (e. g., negative log-likelihood, check score, or CRPS withan empirical CDF). Here, the plug-in learner has two possible variants, namely, S- and T-learner,depending on whether the conditional outcome distribution is learned by a single model or twomodels . IPTW-learner. The IPTW-learner addresses the selection bias of the plug-in learner. For this, itadditionally employs an estimated propensity score, . The estimated propensity score is used tore-weight the original probabilistic loss, l(, ), in the following way:",
  "La(g) = ElY [a], g(, X),(30)": "where g(, x) G is a working model defined as in Sec. 4.2. This target risk aims to find the bestapproximation of the conditional potential outcome distribution, P(Y [a] | x), with the workingmodel, g G. The orthogonality of the IPTW-learner wrt. the target risk aiming at the potential outcome distributionswas formally proved in . Therein, the authors notice that the target estimand (e. g., the CDF ofone of the potential outcomes) coincides with one of the nuisance functions (i. e., Fa). Informally, theorthogonality follows from the fact that the working model, g, simultaneously fits the target estimandand the nuisance function. The orthogonality of the IPTW-learner can also be seen by (1) performing a one-step bias-correctionof Eq. (28) and (2) setting the same estimator for the working model and the nuisance func-tion, i. e., g = Fa. For example, if the probabilistic loss is the CRPS with the empirical CDF,l(Y, g(, X)) =",
  ",(32)": "where a(x) = a (x) + (1 a) (1 (x)), and P is an estimator of the conditional density ofthe outcome. Again, after (2) setting g = P, it is easy to see that the minimization of one-step biascorrected loss in Eq. (32) is equivalent to the minimization of the IPTW-learners objective in Eq. (29)(where P is used in place of Fa). This is possible due to two facts: (1) both entropy terms in Eq. (32), Y log g(y, X) g(y, X) dy, only require the minimization wrt. to the working model g under thelogarithm; and (2) these entropies are minimal as the cross-entropy for any distribution is minimalwhen evaluated with itself.",
  "DProofs": "In the following, we provide the main theoretical results of our paper. We use the following additionalnotation: {} is a Dirac delta function, a b means there exists C such that a C b. Also, in thefollowing theorems, we use red color to show the nuisance functions of P that are influencing thetarget estimand.",
  "D.1Efficient influence functions": "We start with deriving the efficient influence functions for the average Makarov bounds and, af-terwards, for the target risks. For that, we make two mild assumptions: (1) one the conditionaloutcome distributions and (2) another on the set of where linear rectifiers are differentiable. Theseassumptions allow us to (1) handle sup-/inf-convolutions as max-/min-convolutions with a finitenumber of argmax/argmin values and to (2) get a derivative of the linear rectifiers. Assumption 1 (Finite argument sets). We assume that the outcome space Y is compact. Also, weassume that conditional outcome CDFs, Fa(y | x), are continuously differentiable and consist of afinite number of strictly concave / convex regions. Assumption 1 implies that sup-/inf-convolutions are achieved by some finite set of values in Y.Furthermore, Assumption 1 is a special case of the margin assumption (Assumption 3.2) from .Yet, we find our version to be more interpretable and many regular distributions satisfy it, e. g., anexponential family, finite mixtures of normal distributions, etc.",
  "Theorem 1 (Efficient influence function for Makarov bounds). Let P denotes P(Z) = P(X, A, Y )and let yY( | x) and u": "( | x) be argmax/argmin sets of the convolutions (F1 F0)Y( | x) and(F11 F10 )( 0 | x), respectively. Then, under the mild assumption of the finite argument sets(Assumption 1), average Makarov bounds are pathwise differentiable for values of that satisfythe differentiability of linear rectifiers (Assumption 2) and for all values of (0, 1). Further, thecorresponding efficient influence functions, , are as follows:",
  "= IF(y)P(Y = y | x, A = 1) P(Y = y | x, A = 0)= 0,(51)": "where the last equality holds due to the properties of the argmax, y yY( | x). Namely, underthe necessary condition for a local maximum, we haveddy(F1(y | x) F0(y | x)) = P(Y = y |x, A = 1) P(Y = y | x, A = 0) = 0. In the context of the efficient influence functions, this means that the Makarov bounds are first-order insensitive to the misspecification of argmax/argmin.Interestingly, a similar result was demonstrated for the efficient influence functions of the policyvalues of the optimal policies .",
  "t1...tk F(f + t1h1 + +tkhk)|t1==tk=0 are pathwise derivatives , g = arg mingG L(g, ), and is the ground-truthnuisance function": "Informally, this definition means that the risk is first-order insensitive wrt. to the misspecification ofthe nuisance functions. Notably, the pathwise derivative in the direction of the Dirac delta distributioncoincides with the efficient influence function , i. e., DPF(P)[{Z } P(Z = )] =(F(P); P), where P(Z = ) is the PDF of the P(Z).Theorem 2 (Neyman-orthogonality of AU-learner). Under the assumptions of the Theorem 1, thefollowing holds for AU-learner from Algorithm 1 with the scaling hyperparameter = 1:",
  "Y, u, 1, +1}. Further-": "more, if the nuisance functions are estimated sufficiently fast, i. e., L2 = o(n1/4) or bothF1 y F1 yCRPS = o(n1/4) andF0 (y ) F0 (y )CRPS = o(n1/4),then the AU-learner (CRPS) achieves the quasi-oracle property (analogous result holds forAU-learner (W 22 )). This means that the estimation error of the second stage with the estimatednuisance functions behaves in the same way as if the ground-truth nuisance functions were used. Proof. 1. Neyman-orthogonality. The Neyman-orthogonality follows by the construction of theAU-learner as a one-step bias-corrected estimator. Specifically, it is easy to verify that the pathwisecross-derivative from Eq. (78) is equal to zero.",
  "Y( | X); y": "Y( | X) are the argmax/argmin sets of the convolutions(F1 + t(F1 F1) F0)Y( | X); and () = 0 follows from the the same considerations as inEq. (51). Analogously, the pathwise derivative wrt. F0 can be shown to be equal to zero. We refer tothe appendices of for more details.",
  "(F11+ t(F11 F11 ) F10 )( 0 | X). The cross-derivatives for the upper bound wrt. F11and for both upper and lower bounds wrt. F10follow similarly": "2. Rate double robustness. The result is a direct application of Theorem 1 in : It is easy to seethat the Assumptions 14 from hold for the population versions of the empirical risks of ourAU-learner (i.e., CRPS and W 22 ). In the following, we provide the derivation of the rate doublerobustness property for the population version of CRPS loss (the derivation is similar to one in ).",
  "FDR( | zi; , = 0.25)": ": Comparison of estimated pseudo-CDFs based on i = {1, . . . , 50} instances of the semi-synthetic IHDP100 dataset . Here, we compare CA-learners pseudo-CDFs (first column) withtwo variants of AU-learner: w/o scaling ( = 1, second column), and w/ scaling ( = 0.25, thirdcolumn). The scaling hyperparameter = 0.25 facilitates pseudo-CDFs to better comply with-boundedness and monotonicity constraints.",
  "Legend": ": Overview of our AU-CNFs. AU-CNFs combine several conditional normalizing flows(CNFs), which we call a nuisance CNF and upper/lower target CNFs. The nuisance CNF is a firststage model and aims at estimating the nuisance functions, i. e., the propensity score, a(x) =a(x) + (1 a)(x); and the conditional outcome CDFs, Fa(y | x). Upper/lower target CNFs arethe second stage working models, G and G, respectively. They aim at minimizing one of the losses of",
  "AU-learner, LAU, CRPS/W 22": "Our AU-CNFs allow us to implement the Algorithm 1 of our AU-learner (see ) by combiningseveral conditional normalizing flows (CNFs) . It consists of a (i) nuisance CNF and(ii) two target CNFs (upper and lower). (1) The nuisance CNF aims to fit the nuisance functions,(, F0, F1) or, equivalently, (, F10 , F11 ). (2) Upper and lower target CNFs constitute the secondstage working models, namely, G and G, and minimize the loss of our AU-learner. (1) Nuisance CNF. The nuisance CNF has three components, similarly to . These are twofully-connected subnetworks (FC1 and FC2) and a CNF, parametrized by . The two subnetworksFC1 and FC2 form a hypernetwork, which outputs the conditional parameters, = (X, A). Thisallows us to flexibly model the conditional outcome distribution. The nuisance CNF has the following joint loss for the nuisance functions: LN = LNLL + L. Here,LNLL is a conditional negative log-likelihood loss, L is a binary cross-entropy, and > 0 is ahyperparameter. We additionally employed noise regularization to regularize the conditional negativelog-likelihood loss . (2) Upper and lower target CNFs. The upper and lower target CNFs use the pseudo-CDFs /pseudo-quantiles, generated by the nuisance CNF, and then implement a second stage loss of ourAU-learner. Both target CNFs have the same structure. Specifically, they have a fully-connectedsubnetwork, FC3, and a CNF, parametrized by . Analogously, FC3 serves as a hypernetwork so thatthe parameters can be conditioned on X: = (X). To fit the target CNFs, we use a second stage loss of our AU-learner, namely, Eq. (13) or Eq. (14).For that, we discretize the Y-space or the -interval of u into nd values and infer argmin/argmaxvalues based on those grids. Then, to approximate the integrals, we do the same for the -space andthe -interval of . The later creates a /-grid with n/n points. Those grids are later used fora rectangle quadrature integration. Furthermore, we also regularize the target CNFs by applying thenoise regularization .",
  "F.2Implementation": "Implementation. We implemented our AU-CNFs using PyTorch and Pyro. For the CNFs of bothstages of learning, we used neural spline flows with a standard normal distribution as a basedistribution. Neural spline flows build an invertible transformation based on invertible rational-quadratic splines and, thus, allow the direct inference of the (conditional) log-probability, CDF, andquantiles. Neural spline flows are characterized by two main hyperparameters, namely, a numberof knots nknots and a span of the transformation interval, [B, B]. The number of knots, nknots,controls the expressiveness of the flow. The span B defines the support of the transformation. In",
  "our experiments, we tune the number of knots nknots and set the span B via a heuristic depending onsample max/min values (as Y is assumed to be compact)": "Training. To train our AU-CNFs, we make use of Algorithm 1. However, both first and secondstage models are fit on the same training data D without cross-fitting as (regularized) CNFs asneural networks belong to the Donsker class of estimators . Training of our AU-CNFs proceedsas follows: (1) we fit the nuisance CNF; (2) we freeze the nuisance CNF and generate pseudo-CDFs/pseudo-quantiles; and (3) we train the upper and lower target CNFs. The hyperparameters arethen as follows: 1. First stage. We used stochastic gradient descent (SGD) with a minibatch size bN, ne,N = 200epochs and a learning rate N. Furthermore, we set the loss coefficient to = 1. Both thenumber of hidden units of FC1/FC2 and the size of the output of the FC1 are set to 10. Thenuisance CNF has the number of knots nknots, N and the span B = maxi(yi) mini(yi) + 5,where yi are standard normalized outcomes yi. The intensities of the noise regularizationfor the input and the output are set to 2x and 2y, respectively. 2. Intermediate stage. We set nd = 200 and n = n = 50. Furthermore, we set the scalinghyperparameters = 0.25 for the CRPS loss and = 0.01 for the W 22 loss. We clipped toolow propensity scores (lower than 0.05). 3. Second stage. The upper and lower target CNFs are also fit via SGD with the minibatchsize bT = 64, ne,T = 200 epochs, and the learning rate T = 0.005. The intensities of thenoise regularization for the input are the same as for the nuisance CNF, 2x. The number ofhidden units of FC3 is also set to 10. The target CNFs have the number of knots twice largerthan the nuisance flow, nknots, T = 2 nknots, N, and the span B = maxi(ai yi) mini(ai yi) +maxi((1 ai) yi) mini((1 ai) yi) + 5, where yi are standard normalized outcomes yi.To further stabilize the training of the target CNFs, we employed an exponential movingaverage (EMA) of the target CNFs parameters with a smoothing hyperparameter = 0.995.",
  "We demonstrate the detailed training procedure of our AU-CNFs (CRPS) in Algorithm 2 (AU-CNFs(W 22 ) follow analogously)": "Hyperparameter tuning. We performed extensive hyperparameter tuning only for the nuisance CNF.The following hyperparameters are subjects to tuning: the minibatch size nb,N, the learning rate N,the number of knots nknots, N, and the intensities of the noise regularization, 2x and 2y. Further detailsof hyperparameter tuning are provided in Appendix G. The hyperparameters of the target CNFs forall the experiments are either kept fixed or are inherited from the nuisance CNF.",
  "H.1Synthetic data": "Our synthetic data generator is adapted from . Although the original synthetic benchmarkcontains hidden confounding, we include the confounder as the second observed covariate. Wecreated three settings with different conditional outcome distributions: (1) normal, (2) multi-modaland (3) exponential. Specifically, synthetic covariates, X1, X2, a treatment, A, and an outcome, Y ,are sampled from the following data generating mechanisms:",
  "where N(, 2) is the normal distribution and where Exp() is the exponential distribution": "The synthetic benchmark allows us to infer or approximate the ground-truth Makarov bounds. Forthe (1) normal distribution, they are given by the analytical solution , namely, the CDFs of half-normal distributions. However, in the settings (2) and (3), they need to be approximated numerically.Thus, for the (2) multi-modal distribution, we only infer the Makarov bounds on the CDF, as thequantiles are not directly available for the mixture distribution. For the (3) exponential distribution,however, we can infer both the Makarov bounds on the CDF and the quantiles.",
  "H.2HC-MNIST dataset": "HC-MNIST dataset was introduced as a high-dimensional, semi-synthetic dataset based onthe MNIST image dataset . The HC-MNIST dataset builds on ntrain = 60, 000 train andntest = 10, 000 test images. HC-MNIST takes original high-dimensional images and maps them ontoa one-dimensional manifold, where potential outcomes depend in a complex way on the averageintensity of light and the label of an image. The treatment also uses this one-dimensional summary, ,together with an additional (hidden) synthetic confounder, U (we consider this hidden confounder asanother observed covariate). HC-MNIST is then defined by the following data-generating mechanism:",
  "(135)": "where c is a label of the digit from the sampled image X; Nx is the average intensity of the sampledimage; c and c are the mean and standard deviation of the average intensities of the images withthe label c; and Minc = 2 +410c, Maxc = 2 +410(c + 1). The parameter defines whatfactor influences the treatment assignment to a larger extent, i.e., the additional confounder or theone-dimensional summary. We set = exp(1). For further details, we refer to . Similarly to the synthetic data with the normal distribution, the ground-truth Makarov bounds for theHC-MNIST dataset are given by the analytical solution, namely, the CDFs of half-normal distributions.",
  "H.3IHDP100 dataset": "The Infant Health and Development Program (IHDP100) is a standard semi-syntheticbenchmark for treatment effect estimation. It contains 100 train/test splits with ntrain = 672, ntest =75, and dx = 25. Yet, this dataset contains severe overlap violations, which makes the methods usingpropensity re-weighting unstable . The IHDP100 dataset samples synthetic outcomes from the conditional normal distribution, N(i, 1),where i are CAPOs provided in the dataset. Therefore, the ground-truth Makarov bounds are givenby the half-normal distributions .",
  "I.1Synthetic data": "We provide additional results of our synthetic benchmark in . Therein, the out-of-sampleperformance is reported wrt. W2 evaluation score for two settings, i.e., normal and exponentialsetting.10 Our AU-CNFs achieve superior performance in the normal setting and perform well in theexponential setting. Notably, the IPTW-CNF also perform well in the exponential setting, mainly dueto the orthogonality wrt. potential outcome distributions.",
  "JCase study: Lockdown effectiveness": "In the following, we provide a case study, where we apply our AU-learner to a real-world problem.Here, we want to study the effectiveness of lockdowns during the COVID-19 pandemic by using theobservational data collected in the first half-year of 2020 . Specifically, we aim to estimate theprobability that the incidence falls after the implementation of the strict lockdown, i. e., a probabilityof individual benefit from treatment (intervention) (PITB).",
  "J.1Dataset": "We used multi-county data provided by .11 The outcome Y is defined as the relativecase growth per week (in log), namely, the number of new cases divided by the number of cumulativecases. Then, the treatment A {0, 1} is taken as an implementation of the strict lockdown one weekbefore. We also choose three (dx = 3) pre-treatment covariates X: the relative case growths from theprevious week, the relative case growth from two weeks ago, and the implementation of the strictlockdown from two weeks ago. We assume that the data is i.i.d. and that the causal assumptions(1)(3) are satisfied. We filtered out observations where the number of cumulative cases is fewer than20. As a result, we ended up with n = n0 + n1 = 152 + 112 treated and untreated observations,respectively.",
  "J.2Results": "We present the results for our case study in . Therein, we report two quantities: the estimatedbounds on the probability of the individual treatment benefit (PITB), P(Y Y 0 | x), for 20countries during week 15 of 2020. Additionally, we show bounds on a population analogue of PITB,namely, a probability of the population treatment benefit, P(Y Y 0). We estimated thebounds on the PITB with both AU-CNFs (CRPS) and AU-CNFs (W 22 ), and both methods producedvery similar results (which implies a robustness of our AU-learner). For the population analogue ofthe PITB, we first efficiently estimated the distributions of the potential outcomes with interventionalnormalizing flows (INFs) and then used them to infer the Makarov bounds at the populationlevel.",
  "Week: 15": ": Results for the real-world case study analyzing the effectiveness of lockdowns during theCOVID-19 pandemic. Reported: in-sample estimated bounds on the probability of the individualtreatment benefit (PITB), P(Y Y 0 | x), over 20 runs for 20 countries during week 15 of2020. Also, we show bounds on a probability of the population treatment benefit, i.e., P(Y Y 0). These are displayed in the small figure on the left. Each estimated bound is shown as two boxplots;hence, we also display the epistemic uncertainty. There are two important takeaways: (1) The bounds on the PITB are more shifted towards 1,suggesting the drop in the incidence is highly probable after the implementation of the strict lockdownin all the studied countries. (2) The bounds on PITB are much tighter than their population analogue(e. g., average upper-lower bound width is 0.66 for AU-CNFs (CRPS) and 0.88 for INFs). The latterimplies that individualization enhances decision-making and makes the Makarov bounds on thealeatoric uncertainty tighter and, thus, more informative."
}