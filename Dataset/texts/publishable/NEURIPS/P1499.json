{
  "Abstract": "Despite the successful application of Temporal Graph Networks (TGNs) for taskssuch as dynamic node classification and link prediction, they still perform poorlyon the task of dynamic node affinity prediction where the goal is to predict howmuch two nodes will interact in the future. In fact, simple heuristic approachessuch as persistent forecasts and moving averages over ground-truth labels sig-nificantly and consistently outperform TGNs. Building on this observation, wefind that computing heuristics over messages is an equally competitive approach,outperforming TGN and all current temporal graph (TG) models on dynamic nodeaffinity prediction. In this paper, we prove that no formulation of TGN can rep-resent persistent forecasting or moving averages over messages, and propose toenhance the expressivity of TGNs by adding source-target identification to eachinteraction event message. We show that this modification is required to representpersistent forecasting, moving averages, and the broader class of autoregressivemodels over messages. Our proposed method, TGNv2, significantly outperformsTGN and all current TG models on all Temporal Graph Benchmark (TGB) dynamicnode affinity prediction datasets.",
  "Introduction": "Temporal Graph (TG) models have become increasingly popular in recent years (Xue et al., 2021;Skarding et al., 2021; Feng et al., 2024) due to their suitability for modeling a range of real-worldsystems as dynamic graphs that evolve through time, e.g. social networks, traffic networks, andphysical systems (Deng et al., 2019; Song et al., 2019; Zhao et al., 2020; Guo et al., 2019; Sanchez-Gonzalez et al., 2020; Pfaff et al., 2021). Unlike static graphs, dynamic graphs allow the addition ofnodes and edges, and graph features to change over time. Despite the successes of current TG modelsfor dynamic node classification and link prediction, they have been shown to struggle in dynamicnode affinity prediction, being significantly outperformed by simple heuristics such as persistentforecasting and moving average over ground-truth labels (Huang et al., 2023; Yu, 2023). In dynamic node affinity prediction, the task is to predict a nodes future affinity for other nodesgiven the temporal evolution of the graph. Informally, the affinity of a node x towards a node yover some time interval [t, t + ] refers to how much x has interacted with y over that interval. Forexample, if node A sends 10 identical messages to node B and 100 of the same messages to nodeC over some time interval [t, t + ], then A has a higher affinity for C over that interval. Thisformulation is useful in settings such as recommender systems, e.g. predicting a users future songpreferences given their past listening history (Huang et al., 2023). A concrete example from theTemporal Graph Benchmark (TGB) (Huang et al., 2023) is tgbn-trade, where nodes representnations and edges represent the amount of goods exchanged in a single trade. In this case, the goal ofdynamic node affinity prediction is to predict the amount of trade one nation would have with anothernation in the next year, given the past evolution of global trading patterns.",
  "arXiv:2411.03596v3 [cs.LG] 28 Nov 2024": "Contributions.This work is based on the assumption that considering past messages betweentwo nodes is important to predict their affinity at a future time. We start by empirically validatingthis assumption by demonstrating that a moving average computed over a nodes past messages toanother node is a powerful heuristic, beating all existing TG models. Armed with this result, we askwhether Temporal Graph Networks (TGNs) (Rossi et al., 2020), a popular TG model, can representmoving averages over messages. Surprisingly, we find that no formulation of TGN can representmoving averages of any order k. This result implies that TGNs are unable to represent persistentforecasting (i.e. the simple heuristic of outputting the most recent message between a pair of nodes),indicating a substantial weakness in its design. To remedy this, we propose to modify TGN by addingsource-target identification to each interaction event message. We prove that our method, TGNv2, isstrictly more expressive than TGN as it is able to represent persistent forecasting, moving averages,and autoregressive models. Further, we show that TGNv2 significantly outperforms all current TGmodels on all TGB datasets on dynamic node affinity prediction.",
  "The Hidden Limitation of Temporal Graph Networks": "This work is motivated by our observation that computing moving averages over past messages,despite still lagging behind moving average over ground-truth labels, is a competitive heuristic thatoutperforms all current TG models on every node affinity prediction dataset (). Given an orderk N+, the moving average heuristic over past messages for node affinity prediction is defined as:",
  "tM(u,v,t,k)euv(t)": "where M(u, v, t, k) returns k ordered timestamps that constitute the k most-recent messages sentfrom node u to node v up to time t and euv(t) is the scalar event message passed from node u tonode v during their interaction at time t. Given this observation, we focus on TGNs and study ifthere exists a formulation of TGN that can exactly represent a moving average of order k. Our firstimportant result is proving that this cannot be the case:Theorem 1. No formulation of TGN can represent a moving average of order k N+ for anytemporal graph with a bounded number of vertices. We prove Theorem 1 in Appendix B.1. In short, the proof constructs a minimal example of two nodesin two graphs sending different messages. We show that TGNs cannot distinguish the two nodes,leading them to compute the same moving average for both nodes. This is a direct consequence ofthe permutation-invariance of TGNs, which renders them unable to discriminate between senders andreceivers of messages, and in turn incapable of capturing important functions. Since the above theorem holds for any k and persistent forecasting is equivalent to a moving averagewhen k = 1, it follows that TGNs cannot represent persistent forecasting. Proceeding similarly toour proof for Theorem 1, we can show that, more generally, TGNs cannot represent the class ofautoregressive functions (proof in Appendix B.2):Corollary 1. No formulation of TGN can represent an autoregressive model of order k N+ forany temporal graph with a bounded number of vertices.",
  "TGNv2: Increasing the expressive power of TGNs": "The main problem with TGN lies in the construction of the messages when an event occurs. In TGN,for every interaction between nodes i and j, two messages are constructed:mi(t) = msgs(si(t), sj(t), (t), eij(t));mj(t) = msgd(sj(t), si(t), (t), eij(t))If we look closely, however, we can see that each message does not contain the source or thedestination of the message. Not only does this make it impossible for the memory vectors to have animprint of past interactions, but this also renders TGNs to be invariant to the identities of the sendersand receivers of messagesa property that is undesirable for dynamic node affinity prediction. Toaddress this issue, we introduce TGNv2, where we modify the message construction of TGNs toinclude source-target identification:mi(t) = msgs(si(t), sj(t), t(t), eij(t), n(i), n(j))",
  "mj(t) = msgd(sj(t), si(t), t(t), eij(t), n(j), n(i))": "Here, we map all nodes to an arbitrary, but fixed node index, and n R Rd is an encoderfunction for node indices, similar to t. Incoming nodes that have not been encountered before areassigned to fresh, unused node indices as the graph evolves. This modification is a way to break thepermutation-invariance of TGN, which is necessary to compute moving averages and autoregressivemodels. We are now able to prove:Theorem 2. There exists a formulation of TGNv2 that can represent persistent forecasting, movingaverage of order k N+, or any autoregressive model of order k N+ for any temporal graph witha bounded number of vertices. Our proof of Theorem 2 (Appendix B.3) leverages the existence of the node identification to indexinto the memory vector to store information. From this, it follows that TGNv2 is strictly moreexpressive than TGN, as TGN is a special case of TGNv2.",
  "Experiments": "shows our experimental results on the TGB benchmark. The top 3 rows are simple heuristicsover ground-truth labels / ground-truth messages. Persistent Frcst (L) and Moving Average (L)refer to persistent forecasting and moving average over ground-truth labels respectively; whileMoving Avg (M) is a moving average over messages. The rest of the rows correspond to TG models.We describe the experimental details in Appendix C. Evidently from , Moving Average (M) is a competitive method that outperforms all TGmodels. TGN (tuned) denotes the TGN that we trained using the same set of hyperparametersfor TGNv2. Though TGN enjoys a performance boost with this set of hyperparameters, TGNv2significantly outperforms TGN and all TG models on all datasets. Further, we can see that TGNv2performs comparably to Moving Avg (M) on tgbn-trade, tgbn-genre, tgbn-reddit whileTGN is beaten by Moving Avg (M) on all datasets.",
  "Related Work": "We believe this work is the first to address the limitations of TG models in dynamic node affinityprediction. Huang et al. (2023) were the first to point out this problem, highlighting that TGN andDyRep (Trivedi et al., 2018) are outperformed by heuristics over ground-truth labels. Yu (2023)extended this work and found that a suite of other TG models (JODIE (Kumar et al., 2019), TGAT(Xu et al., 2020), CAWN (Wang et al., 2022), TCL (Wang et al., 2021), GraphMixer (Cong et al.,2023), and DyGFormer (Yu et al., 2023)) all underperform in dynamic node affinity prediction.Despite still lagging behind heuristics over ground-truth labels, TGNv2 significantly outperforms allof the methods above, constituting what we believe to be the first positive result in improving TGmodels for dynamic node affinity prediction. Our method of augmenting TGNs with source-targetidentification to increase expressivity is most similar to the work of Sato et al. (2019), where they increased the expressivity of static GNNs via port numbering. Relatedly, other works demonstratedthat breaking the permutation-invariance of static GNNs (e.g. by using RNNs to aggregate messages)led to empirical benefits (Xu and Velickovic, 2024; Hamilton et al., 2018).",
  "Conclusion": "In this paper, we proposed to augment TGN with source-target identification. We proved that TGNv2is strictly more expressive than TGN and consequently showed that TGNv2 achieves significantlyhigher performance than current TG models across all dynamic node affinity prediction datasets fromTGB. In the future, we would like to close the remaining empirical gap between TGNv2 and theheuristics approaches. We believe this is because we formulated our message aggregator to outputthe last message, which was necessary to compare our results fairly with prior TGN experiments(Appendix C). To address this, we hope to explore more expressive aggregation functions. Moreover,we would like to further develop our work by studying TGNv2 on other TG tasks, such as dynamiclink prediction.",
  "Y. Shi, Z. Huang, S. Feng, H. Zhong, W. Wang, and Y. Sun. Masked label prediction: Unifiedmessage passing model for semi-supervised classification, 2021. URL": "J. Skarding, B. Gabrys, and K. Musial. Foundations and modeling of dynamic networks using dynamicgraph neural networks: A survey. IEEE Access, 9:7914379168, 2021. ISSN 2169-3536. doi:10.1109/access.2021.3082932. URL W. Song, Z. Xiao, Y. Wang, L. Charlin, M. Zhang, and J. Tang. Session-based social recommendationvia dynamic graph attention networks. In Proceedings of the Twelfth ACM International Conferenceon Web Search and Data Mining, WSDM 19. ACM, Jan. 2019. doi: 10.1145/3289600.3290989.URL R. Trivedi, M. Farajtabar, P. Biswal, and H. Zha. Representation learning over dynamic graphs, 2018.L. Wang, X. Chang, S. Li, Y. Chu, H. Li, W. Zhang, X. He, L. Song, J. Zhou, and H. Yang.Tcl: Transformer-based dynamic graph modelling via contrastive learning, 2021. URL",
  "zi(t) = gh(si(t), sj(t), eij, vi(t), vj(t)) : j N Li ([0, t])": "Here, h is a learnable function, g is a permutation-invariant function such as a sum or mean, and Lcorresponds to the number of layers used for temporal message passing. We note that our formulationof the embedding layer is a more general version than in the TGN paper, and we can recover theoriginal formulation by setting g to be a sum.",
  "Theorem 1. No formulation of TGN can represent a moving average of order k N+ for anytemporal graph with a bounded number of vertices": "Proof. The main idea of the proof is that TGNs are unable to distinguish nodes whose messages areidentical in every form but have different senders and/or recipients. To show this, we construct atemporal graph G with 3 nodes (Node 1, 2, and 3) and flip it in such a way to yield a G such thatNode 1 in G is sending different messages when compared to Node 1 in G but is indistinguishablefrom Node 1 in G from the point of view of TGNs (Appendix B.1). We proceed by way of contradiction. Assume that there exists a particular formulation of TGNthat can implement a moving average of order k for any temporal graph with a bounded number ofvertices. We initialise all memory vectors to be the zero vector, as per TGNs original formulation.Now, consider the following sequence of events that implicitly define the temporal graph G:",
  "where t1 < < tn < tn+1 < < tn+m, n k, and m k. Let = nk+1++n": "k, themoving average of order k of 1, ..., n. Similarly, define to be the moving average of order k of1, ..., m. Suppose we compute Node 1s embedding at time tT where tT > tn+m. We assume thatno node updates are done, and all v1(t) = v2(t) = v3(t) for all t. Node 1 receives n messages fromits interactions with Node 2:",
  "We can do the same set of calculations for Node 2 and Node 3:": "m2(ti) = msgd(0, 0, ti, i)i [1, . . . , n]m3(tn+i) = msgd(0, 0, tn+i, i)i [1, . . . , m]m2(tT ) = agg(m2(t1), . . . , m2(tn))m3(tT ) = agg(m3(tn+1), . . . , m3(tn+m))s2(tT ) = mem( m2(tT ), 0)s3(tT ) = mem( m3(tT ), 0)",
  "Corollary 2. No formulation of TGN can represent an autoregressive model of order k N+ forany temporal graph with a bounded number of vertices": "Proof. Our proof for Theorem 2 proceeds very similarly to Theorem 1. Notice that in our proof ofTheorem 1, we did not make use of the fact that and are moving averages. Therefore, if ourauto-regressive model has weights w1, . . . , wk, then we can define and to be:",
  "B.3Proof for Theorem 3": "Theorem 2. There exists a formulation of TGNv2 that can represent persistent forecasting, movingaverage of order k N+, or any autoregressive model of order k N+ for any temporal graph witha bounded number of vertices. Proof. We first prove the theorem for the case of moving averages of order k, and extend that to applyto persistent forecasting and autoregressive models. Let the maximum number of nodes encounteredin the temporal graph be n, and assign each node an identifier such that each node is uniquelyidentified by an i [0, . . . , n 1]. We initialise all memory vectors si to be 0 Rnk. Next, denote e(l)ij be the feature of the l-th message that i sends to j, and let M(i, j, t) return theindex of the most recent message that i sent to j at time t. We assume that the batch size is 1, whichmeans that as soon as a message is sent, we update the memory vectors. Further, for the time being,we ignore the formulation of msgd, and assume that all the messages received by the aggregator aremessages constructed by msgs.",
  "Our goal is to find a formulation of TGNv2 such that, for each timestamp t, it computes zi(t)[j] =1kk1x=0 e(M(i,j,t)x)ij. We assume that the moving average is defined for all values of t by letting": "e(l)ij = 0 for all negative l. We now concretely define the formulation of TGNv2, and subsequentlyshow that it computes the moving average. Now, suppose some node i sends j a message at time t.Let msgs be formulated as: msgs(si(t), sj(t), t(t), eij(t), n(i), n(j)) = [eij(t), j]i.e. msgs simply outputs a 2-element vector with the feature of the event message and the index ofthe destination node. Since our batch size is 1, the aggregator only receives at most one message. Welet the aggregator be the identity function. The main idea of the proof is in the formulation of thememory module, which takes advantage of the node index of the destination to store the newestfeature message between i and j.",
  ". . .110. . .001. . .0............0. . .10": "In order to compute the moving average, then each time we receive a message, we need to makeroom in our memory vector to store the message. We introduce the shift matrix S, which is a k kmatrix that is obtained by taking the (k 1) (k 1) identity matrix and sufficiently padding thetop row and rightmost column with zeroes which, when applied to a vector v Rk, keeps the topk 1 elements and discards the last element:",
  "S0. . .00I. . .0............0. . .0I": "Similarly, define the generator vector y Rnk = [1, 0, . . . , 0]T . Next, define f(j) = (P)jX(PT )j,which cyclically shifts the block matrices in X a total number of j times, and p(j) = Qjy, whichcyclically shifts the elements of y a total number of j times. Now, let the memory module be:",
  "which is the moving average of order k, as multiplying A with si(t) has the effect of summing thek most recent messages for each node and multiplying the sum by 1": "k. From this, we can see thatthe theorem holds for persistent forecasting as persistent forecasting is moving average with k = 1.Subsequently, we can adapt our proof above to hold for autoregressive models of any order k byformulating the aggregator matrix A to have the autoregressive weights wk, . . . , w1 in entries where1 is present. In our constructions above, we assumed that our batch size is 1 and we ignored the fact that nodesare receiving messages from msgd. To adapt the proof for an arbitrary batch size, we can define theaggregator module to concatenate all incoming messages, and then during the memory update, weunpack this concatenation and apply our logic above for each message. Finally, to handle messagesfrom msgd, we can expand the size of the message vector by 1 to include a tag that is nonzero ifand only if the message originates from msgd. Then, the aggregator module can drop messages frommsgd by inspecting this tag, leaving us with messages from msgs which we have shown how tohandle.",
  "The code to reproduce our experiments can be found at": "For both TGN and TGNv2, we utilise the same choices of core modules where applicable and usethe same hyperparameters in order to make the results as comparable as possible. We repeat eachexperiment run three times with three different random seeds, each time picking the best-performingmodel on the validation set, and reporting the mean and standard deviation NDCG @ 10 on both thevalidation and test set. For our experiments, our choice of core module largely follows the choicesmade in TGBs experiments with TGN (Huang et al., 2023) for dynamic node affinity prediction:",
  "where si Rdmemory": "EmbeddingWe set the embedding module to be one layer of TransformerConv (Shi et al., 2021)with 2 heads and a dropout value of 0.1. For efficiency, we only use the last x neighbours for temporalmessage passing. We describe the value of x for each experiment in . We set zi(t) to have adimension of dembedding. DecoderThe decoder takes in the embedding for each node and outputs the node affinities forall other nodes. For our decoder, we chose an MLP with 2 layers + ReLU. Both layers havedimensionality ddecoder.",
  "Learning Rate1e-31e-41e-41e-4Batch Size200200200200Epochs750505050d7847847841024No. of temporal neighbours x25303010": "HyperparametersFor moving average over the ground-truth labels (Moving Average (L)), we setk = 7. For moving average over messages (Moving Average (M)), we set k = 2048 for tgbn-trade,tgbn-genre, tgbn-reddit, and k = 512 for tgbn-token due to memory issues. We use a constantlearning rate schedule for all experiments, except for tgbn-trade, where we decay the learning rateby 0.5 every 250 epochs. We use the Adam optimiser (Kingma and Ba, 2017) to train our models.We set a global hidden dimension d, that is used in all places where we need to select a dimension,i.e. d = dmemory = dembedding = ddecoder = dtime = dnode. The hyperparameters that we chosecan be found in ."
}