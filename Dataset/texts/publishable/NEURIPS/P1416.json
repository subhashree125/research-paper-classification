{
  "ABSTRACT": "We study the performance of the Thompson Sampling algorithm for logistic bandit problems,where the agent receives binary rewards with probabilities determined by a logistic functionexp(a, )/(1 + exp(a, )). We focus on the setting where the action a and parameter lie within the d-dimensional unit ball with the action space encompassing the parame-ter space. Adopting the information-theoretic framework introduced by Russo and Van Roy(2015), we analyze the information ratio, which is dened as the ratio of the expected squareddifference between the optimal and actual rewards to the mutual information between theoptimal action and the reward. Improving upon previous results, we establish that the infor-mation ratio is bounded by 9",
  "Introduction": "This paper studies the logistic bandit problem, where for T time steps, an agent selects an action and receives abinary reward with probabilities determined by the logistic function exp(a, )/(1+exp(a, )) with slopeparameter > 0. In this setting, both the action a and the parameter vector lie within the d-dimensional unitball. The logistic bandit applies to various scenarios, for instance, in personalized advertisement systems, wherea platform suggests content to users who provide binary feedback, such as like or dislike (Chapelle and Li,2011; Russo and Van Roy, 2017). The performance, or regret, of algorithms for logistic bandits, has been extensively studied, with signicant con-tributions including analyses of Upper-Condence-Bound (UCB) algorithms by Filippi et al. (2010), Li et al.(2017) and Faury et al. (2020) as well as the study of Thompson Sampling (TS) by Russo and Van Roy (2014)and Abeille and Lazaric (2017). However, nearly all existing regret bounds exhibit an exponential dependenceon the parameter . This dependence is highly unsatisfactory because, in practice, as increases, it is fasterto identify the optimal actions, as the distinction between near-optimal and suboptimal actions becomes morepronounced. In this work, we focus on the TS algorithm (Thompson, 1933), which, despite its simplicity, has proven tobe highly effective across a wide range of problems (Russo et al., 2018; Chapelle and Li, 2011). To ana-lyze the regret of TS, Russo and Van Roy (2015) introduced the concept of the information ratio, a statisticthat quanties the trade-off between the information gained about the parameter and the immediate regret in-curred. Dong and Van Roy (2018) conjectured that the information ratio for TS in logistic bandits could be",
  "Recently, Neu et al. (2022) derived a regret bound of O(": "dT |A| log(T )) on the logistic bandit problem, buttheir result relies on a worst case TS information ratio bound scaling with the cardinality of the action space |A|.Dong et al. (2019) provided a bound of 100d on the information ratio for TS when < 2. They also suggested,through numerical computations, that this bound holds for larger values of . However, their work has two keylimitations. First, they do not provide a rigorous proof for generalizing to larger values. Second, and morecritically, their regret analysis is incomplete as it relies on the rate-distortion bound from Dong and Van Roy(2018), which specically requires a bound on the one-step compressed TS information ratio; a fundamen-tally different quantity from the TS information ratio they studied. Notably, their techniques to bound the TSinformation ratio do not apply to the one-step compressed TS information ratio.",
  "+ exp(a, )": "Here, is a known scale parameter, and a, denotes the inner product of the action vector a A andthe unknown parameter O. The logistic function, sometimes referred to as the link function, is denoted(a, ). In this setting, the action a lies within the d-dimensional Euclidean unit ball, Bd(0, 1), and theparameter vector on the d-dimensional Euclidean unit sphere, Sd(0, 1). Additionally, we assume the actionspace A encompasses the parameter space O, that is O A. Note that this ensures that, for each O, thereexist an action a A equal to , such that a, = 1. Following the Bayesian framework, we assume the parameter vector is sampled from a known prior distribu-tion P. As the reward distribution depends only on the selected action and the parameter, it can be written asRt = R(At, ). The agents history at time t is denoted by Ht = {A1, R1, . . . , At1, Rt1}, representing allpast actions and rewards observed up to time t.",
  ",": "where A is the optimal action for the parameter .We construct the optimal mapping ():=argmaxaAE[R(a, )] and dene A = (). The expectation is taken over the randomness of the actionselection, the reward distribution, and the prior distribution of the parameter . Since the -algebras of the history are often used in conditioning, we introduce the notations Et[] := E[|Ht]and Pt[] := P[|Ht] to denote the conditional expectation and probability given Ht. Additionally, we de-ne It(A; Rt|At) := Et[DKL(PRt|Ht,A,AtPRt|Ht,At)] as the disintegrated conditional mutual informationbetween the optimal action A and the reward Rt conditioned on the action At, given the history Ht.",
  "Thompson Sampling, Information ratio, and Quantization": "An elegant algorithm for solving bandit problems is the Thompson Sampling algorithm. It works by randomlyselecting actions according to their posterior probability of being optimal. More specically, at each time stept {1, . . ., T }, the agent samples a parameter estimate t from the posterior distribution conditioned onthe history Ht and selects the action that is optimal for the sampled parameter estimate, At = (t). Thepseudocode for TS is given in Algorithm 1.",
  "It(A; R(At, ), At)": "This ratio measures the trade-off between minimizing the current squared regret and gathering informationabout the optimal action. Russo and Van Roy use this concept to provide a general regret bound that dependson the time horizon T , the entropy of the prior distribution of A, and an algorithm- and problem-dependentupper bound on the average expected information ratio (Russo and Van Roy, 2015, Proposition 1). A limitation of this approach is that the prior entropy of the optimal action, H(A), can grow arbitrarily largewith the number of actions or get innite if the action space is continuous. We address this issue with Theorem 1,where we propose a regret bound that depends instead on the entropy of , a quantized version of the parameter. The quantized parameter, dened in Denition 1, is obtained by setting as the closest approximation for on an -net for the metric space (O, ).",
  "Main Results": "This section presents our main results on the regret of TS for logistic bandits. In Theorem 1, we start by leverag-ing the previously introduced concepts to derive an information-theoreticregret bound that holds for continuousand innite parameter spaces. Following this, we state in Proposition 1 our principal contribution, where weprove a bound of 9",
  "T (H() + T )": "Notably, the above theorem holds for continuous action and parameter spaces and works with bounds onthe average expected information ratio of the standard TS, instead of the one-step compressed TS asin (Dong and Van Roy, 2018, Theorem 1). This distinction is crucial for effectively controlling the informationratio. We explore this difference in more detail in Appendix B.",
  "arXiv TemplateA PREPRINT": "The proof of Theorem 1 relies on an approximation of the conditional mutual information I(; Rt|At, Ht) asI(; Rt|At, Ht) + exploiting the fact that, for all a A and O, the log-likelihood of R(a, ) is-Lipschitz with respect to . The proof is presented in Appendix A. In the following proposition, we present our main contribution, a bound on the information ratio of TS thatdepends only on the dimension d of the problem. This result conrms, under the considered setting, and up toa multiplicative factor of 9, the conjecture of Dong and Van Roy (2018).",
  "2d": "Under the logistic bandit setting with link function , the reward R(At, ) is given by a Bernoulli randomvariable with associated probability (At, ). We use the notation Bern((At, )) to make the settingmore explicit. Since we assumed the action space A encompasses the parameter space O and both are subsetsof a d-dimensional unit sphere, the optimal action is to select the action that aligns with the parameter, () =. We can therefore write R(A, ) = R((), ) equivalently as Bern((, )) and similarly, writeR(At, ) as Bern((t, )). To alleviate the writing, we will omit the subscript t notation for the rest ofthe section.",
  "Sketch of proof After combining Theorem 1 with Proposition 1, we upper bound the entropy H() bythe cardinality of the -net to get a regret bound of 3": "d/2T (log(||) + T ). As the parameter spaceO is within the Euclidean unit ball, we can use Lemma 8 to control the covering number as log(||) d log(1 + 2/). Finally, setting = d/(T ) and rearranging terms inside the logarithm yields the desiredresult. To the best of our knowledge, this is the rst regret bound for logistic bandits that scales only logarithmicallywith the logistic functions parameter while remaining independent of the number of actions. We note that itis within a factor of O(",
  "Conclusion and Future Work": "In this paper, we studied the Bayesian regret of the Thompson Sampling algorithm for sequential decision-making problems under uncertainty, focusing on logistic bandit problems with action and parameter spacesin the d-dimensional unit ball. We improved the state-of-the-art bounds, proving that when the action spaceA encompasses the parameter space O, the information ratio of TS is bounded by 9",
  "T log(T/d))": "A natural direction for future work is to extend our bounds to settings where the action space does not fully en-compass the parameter space. This extension requires careful analysis of how well the action space aligns withthe parameter space, a property closely related to the fragility dimension, (A, O), introduced by Dong et al.(2019). This quantity is crucial for the analysis of logistic bandits, as Dong et al. (2019) demonstrated thatthere cannot be an (A, O)-independent upper bound that is both polynomial in d and sub-linear in T . In caseswhere the action space does encompass the parameter space, this fragility dimension is minimal, equal to d + 1.However, in problems where this relation is not satised and with dimension d 3, the fragility dimensioncan grow signicantly and become as large as the cardinality of the action set. Future research should take thischallenge into consideration to develop regret bounds applicable to more general settings.",
  "We would like to thank Benjamin Van Roy and Yifan Zhu for the insightful conversations": "D. Russo and B. Van Roy, An Information-Theoretic Analysis of Thompson Sampling, Jun. 2015, number:arXiv:1403.5341 arXiv:1403.5341 [cs]. [Online]. Available: Chapelle and L. Li, An Empirical Evaluation of Thompson Sampling, in Advances in NeuralInformation Processing Systems,vol. 24.Curran Associates,Inc.,2011. [Online]. Available: Russo and B. Van Roy, Learning to Optimize via Information-Directed Sampling, Jul. 2017,arXiv:1403.5556 [cs]. [Online]. Available: Filippi, O. Cappe, A. Garivier, and C. Szepesvri, Parametric Bandits: The Generalized Linear Case, inAdvances in Neural Information Processing Systems, vol. 23. Curran Associates, Inc., 2010. [Online]. Avail-able: Li, Y. Lu, and D. Zhou, Provably Optimal Algorithms for Generalized Linear Contextual Bandits, inProceedings of the 34th International Conference on Machine Learning.PMLR, Jul. 2017, pp. 20712080,iSSN: 2640-3498. [Online]. Available: Faury, M. Abeille, C. Calauznes, and O. Fercoq, Improved Optimistic Algorithms for Logistic Bandits,Jun. 2020, arXiv:2002.07530 [cs, stat]. [Online]. Available: Russo and B. Van Roy, Learning to Optimize via Information-Directed Sampling, in Advances inNeural Information Processing Systems, vol. 27.Curran Associates, Inc., 2014. [Online]. Available: Abeille and A. Lazaric, Linear Thompson Sampling Revisited, in Proceedings of the 20th InternationalConference on Articial Intelligence and Statistics.PMLR, Apr. 2017, pp. 176184, iSSN: 2640-3498.[Online]. Available: R. Thompson, On the likelihood that one unknown probability exceeds another in view of theevidence of two samples, Biometrika, vol. 25, no. 3-4, pp. 285294, Dec. 1933. [Online]. Available: J. Russo, B. V. Roy, A. Kazerouni, I. Osband, and Z. Wen, A Tutorial on Thompson Sampling,Foundations and Trends in Machine Learning, vol. 11, no. 1, pp. 196, Jul. 2018, publisher: NowPublishers, Inc. [Online]. Available: Dong and B. Van Roy, An Information-Theoretic Analysis for Thompson Sampling with Many Actions,May 2018, arXiv:1805.11845 [cs, math, stat]. [Online]. Available: AnalysisofThompsonSamplingforContextualBandits,AdvancesinNeural Information Processing Systems, vol. 35, pp. 94869498, Dec. 2022. [Online]. Available: Dong, T. Ma, and B. V. Roy, On the Performance of Thompson Sampling on Logistic Bandits, Conferenceon Learning Theory, 2019.G. Neu, G. K. Dziugaite, M. Haghifam, and D. M. Roy, Information-Theoretic Generalization Boundsfor Stochastic Gradient Descent, arXiv:2102.00931 [cs, stat], Aug. 2021, arXiv: 2102.00931. [Online].Available: Gouverneur, B. Rodrguez-Glvez, T. J. Oechtering, and M. Skoglund, Thompson Sampling RegretBounds for Contextual Bandits with sub-Gaussian rewards, Apr. 2023, arXiv:2304.13593 [cs, stat].[Online]. Available: Dani, T. P. Hayes, and S. M. Kakade, Stochastic Linear Optimization under Bandit Feedback, 21st AnnualConference on Learning Theory, vol. 21st Annual Conference on Learning Theory, pp. 355366, 2008.Y. W. Yury Polyanskiy, Information Theory - From Coding to Learning, 1st ed.Cambridge University Press,Oct. 2022.J.Duchi,Lecturenotesforstatistics311/electricalengineering377,URL: notes. pdf. Last visited on, vol. 2, p. 23, 2016.R. van Handel, Probability in High Dimension.Princeton University, Dec. 2016, vol. APC 550 Lecture Notes.",
  "since fRt|Ht,At, = fRt|At, a.s. by the conditional Markov chain Rt At Ht |": "Since the derivative of log((x)) is equal to /(1 + exp(x)), it is bounded by and is therefore -Lipschitz.As for all a A and all O, the inner product a, 1, we conclude that log(fRt|At,=(1)) is -Lipschitz with respect to . Similarlyd dx log(1 (x)) = exp(x)/(1 + exp(x)) is also bounded by and is therefore -Lipschitz. We get that log(fRt|At,=(0)) is -Lipschitz with respect to . We conclude that| log fRt|At,=(Rt) log fRt|At,=q()(Rt)| (, q()) . Then, dening the random variable :=q(), we note that the second term in (1) is equal toI(; Rt|Ht, At). Summing the T mutual information I(; Rt|Ht, At) and applying the chain rule (see(Yury Polyanskiy, 2022, Theorem 3.7.e)), we obtain",
  "I(; Bern((, )), )": "Our proof can be broadly divided into three key components: establishing a lower bound on the mutual infor-mation, deriving an upper bound on the squared expected regret, and obtaining an upper bound on a ratio ofexpected variances by analyzing the limit case as . A crucial element in our analysis is the expectedvariance of (, ) conditioned on , expressed as E[V[(, )|]]. It appears in the lower bound onmutual information and a related quantity is used to upper bound the squared expected regret.",
  "(k)= E[I(, ); Bern(, )| = ],": "where (i) follows as and are independent conditioned on the history; (j) follows as and are iden-tically distributed conditioned on the history; and (k) is obtained by taking the expectation over and us-ing (Yury Polyanskiy, 2022, Theorem 3.2.d) as (, x) is a one-to-one mapping. Finally, applying Lemma 1yields the desired result.",
  "V[U]": "The function (x) satises the rst two requirements from Lemma 4: applied on the difference of innerproducts , , , it is a mapping from to and (0) = (1) (1 0) = 0. However,it fails to satisfy the third requirement; (x)/x increases initially, reaching a maximum between 1 and 2before decreasing (see in Remark 1). This issue leads to the introduction of a modied function, which we callthe logistic surrogate, as the tightest upper bound (x) on that satises the last requirement from Lemma 4.",
  "B.3Bounding the ratio of expected variances over the functions and": "By denition, the function and its surrogate are equal for x and then diverge linearly at a rateof ()/. We observe, in Remark 1, that is a decreasing function of and that ()/ strictlyincreases with .This observation suggests that studying the case could provide a general upper bound.Indeed, taking the limit case , the domain where the two functions differ is maximized, and the rate atwhich they differ is the largest. We show in Lemma 7 that under some simple transformations, increasing thevalue of does lead to a larger ratio of expected variances, and therefore, the case tending to can serve",
  ". These transformations arechosen to ensure that they can only increase the ratio of expected variances": "By denition, the function and its surrogate are identical for x [0, ) and then diverge linearly at arate of ()/ on the interval x . We illustrate this on . Focusing on the domain wherethe two functions coincide, we observe that the transformation f(x) = max(x, (1)) reduces the expectedvariance for both and . However, since (x) is less than or equal to (x) for all x , andboth functions exceed (1) on the interval , the transformation f proportionally reduces the expectedvariance of more than that of . As a result, the transformation increases the ratio of expected variancesbetween the two functions. As and are strictly increasing functions, the resulting functions, illustratedon , can be written as",
  "()= 1 + (x )": "and as is a decreasing function of (see Remark 1), the ratio (x )/ is a increasing function of for all x ], 2]. This fact ensures that the expected variance of g(f((x))) cannot increase proportionallymore than the expected variance of f((x)). We can therefore study the ratio of expected variances betweenf() and g(f()). Thelastoperationweapplyismerelyaconvenientshiftingandscaling,h(x)=(x g(f((1))))/(g(f((2))) g(f((1)))). Applied on both g(f()) and f() these operations donot affect the ratio of expected variances. The resulting functions are illustrated on .",
  "+ exp((1 x))": "We have to distinguish between three cases for (x 1): negative, zero, or positive. For values of x ]1, 2], wehave that(1 x) < 0 and that lim (x) = 1, if x = 1, we have that lim (x) = 1/2 and forvalues of x [0, 1[, we have that(1 x) > 0 and that lim (x) = 0. We can then write",
  "dT/2 (H() + T ),": "where is dened for some > 0 as in Denition 1. To dene , we can set O as the -net of small-est cardinality. The entropy H() is upper bounded by the logarithm of the cardinality of the space O(see (Yury Polyanskiy, 2022, Theorem 1.4.b)), which corresponds to the logarithm of the covering numberN(O, , ). As O Bd(0, 1) and is the Euclidean distance, we can apply Lemma 8 and upper bound the TSregret as"
}