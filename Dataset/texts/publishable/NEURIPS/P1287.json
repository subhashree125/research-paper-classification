{
  "Abstract": "Disparate impact doctrine offers an important legal apparatus for targetingunfair data-driven algorithmic decisions.A recent body of work has focusedon conceptualizing and operationalizing one particular construct from thisdoctrine the less discriminatory alternative, an alternative policy that reducesdisparities while meeting the same business needs of a status quo or baselinepolicy. This paper puts forward four fundamental results, which each representlimits to searching for and using less discriminatory algorithms (LDAs).(1)Statistically, although LDAs are almost always identifiable in retrospect on fixedpopulations, making conclusions about how alternative classifiers perform on anunobserved distribution is more difficult. (2) Mathematically, a classifier can onlyexhibit certain combinations of accuracy and selection rate disparity betweengroups, given the size of each group and the base rate of the property or outcomeof interest in each group. (3) Computationally, a search for a lower-disparityclassifier at some baseline level of utility is NP-hard. (4) From a modeling andconsumer welfare perspective, defining an LDA only in terms of business needscan lead to LDAs that leave consumers strictly worse off, including members ofthe disadvantaged group. These findings, which may seem on their face to givefirms strong defenses against discrimination claims, only tell part of the story.For each of our negative results limiting what is attainable in this setting, we offerpositive results demonstrating that there exist effective and low-cost strategiesthat are remarkably effective at identifying viable lower-disparity policies.",
  "Introduction": "Most scholarship on algorithmic discrimination over the past decade has focused on disparate im-pact doctrine, which allows plaintiffs to challenge facially neutral policies that nevertheless havean unjustified or avoidable disparate impact on legally protected groups. Disparate impact casesare initiated by a plaintiff who puts forward evidence that the decision-making process at issue hasresulted in a disparate impact along the lines of a legally protected characteristic (e.g., race, gen-der, age, etc.). For example, a loan applicant must first demonstrate that there is a disparity in theapproval rate for applicants according to their gender. The defendant can then put forward what isknown as a business necessity defense, where they claim that the observed disparities are requiredto meet some legitimate business goal for example, that a lenders credit scoring model predictsdefault to some reasonable degree of accuracy. Finally, the plaintiff can rebut the firms justificationby pointing to what is commonly known as a less discriminatory alternative: an alternative decision-making process that would serve the firms goals equally well, but with less disparate impact. If arejected credit applicant can furnish a credit scoring model of equivalent accuracy that has a smallergap in selection rates across gender groups, then the firm can be found liable for discrimination.",
  "arXiv:2412.18138v1 [cs.CY] 24 Dec 2024": "The final step in disparate impact doctrine and the notion of a less discriminatory algorithm (LDA)has attracted significant scholarly attention in recent years, as research suggests that there may bereadily available less discriminatory alternatives to existing algorithms used in many contexts. Oneidea in particular that is attracting growing interest in both the technical and legal communities isthe phenomenon of model multiplicity: it is often possible to develop many different models that allexhibit the same accuracy but make quite different predictions on individual points (i.e., a specificperson) and groups of points (i.e., specific groups of people) . Two models might be equallyaccurate, but make opposite predictions for a specific person. Likewise, the first model might selectmembers from one group more than members of another group, while the second model mightselect members from both groups at a similar rate, even though both models are equally accurate.Multiplicity therefore suggests that there is not always an inevitable trade-off between accuracy andfairness. Instead, practitioners will often be able to develop a large set of equally accurate modelsand then choose among these the one that happens to have less disparate impact across groups. Model multiplicity speaks directly to the question of less discriminatory alternatives because it tellsus that there will often be many equally accurate ways to select individuals based on some outcomeof interest (e.g., loan default), some of which will have more of a disparate impact than others. Inlegal proceedings, less discriminatory alternatives are generally understood to serve as evidence ofan avoidable disparate impact and thus of illegal discrimination and the burden for identifyingthem is today commonly thought to rest with the plaintiff . Identifying a less discriminatoryalternative is mainly a means for a plaintiff to hold a firm accountable for its past discrimination that is, for the avoidable adverse impact already experienced by the plaintiff. But there is alsogrowing belief that the phenomenon of multiplicity justifies placing the burden of searching forless discriminatory alternatives on firms themselves.1 Given the fact of multiplicity, there is noreason to simply accept an algorithm that has a disparate impact without first asking whether afirm has attempted to find a less discriminatory alternative . Unless a firm has specifically takendisparate impact into account while developing its algorithms, it is extremely unlikely to land onthe least discriminatory means of achieving its goals . Thus, a failure to even attempt to identifyalternatives can be treated as a decision to accept an avoidable and thus unjustified disparateimpact. Multiplicity thus puts pressure on firms to proactively test for disparate impact and, if found,to explore whether they can identify a less discriminatory means of achieving their goal equally well.This move positions LDAs as a way to prevent avoidable disparate impacts from occurring in thefirst place, not simply a way to establish that they have occurred in the past. Legal scholars and computer scientists have worked together to capitalize on the promise of mul-tiplicity for dealing with algorithmic discrimination, exploring the full implications of multiplicityfor disparate impact doctrine and proposing specific techniques for finding the least discrimina-tory alternative . Civil society organizations have likewise sought to leverage this insight, call-ing on firms to take affirmative steps to find less discriminatory alternatives when developing theirdecision-making algorithms and asking regulatory agencies to clarify their expectations of regulatedfirms . And a range of federal and state regulators have now issued statements instructingfirms to search for less discriminatory alternatives while developing algorithms . Despite this apparent enthusiasm, there remains considerable uncertainty around how much can beachieved by exploiting multiplicity and how easily this can be achieved . Even the scholarshipchampioning multiplicity recognized that it is generally not possible to find the least discriminatorypossible algorithm and that the process of finding LDAs in practice is not always self-evidentand may require non-trivial effort and resources . Attempts to operationalize or automate the search for an LDA must grapple with fundamental ques-tions about disparate impact law: what is the purpose of the LDA and what are the appropriateway(s) it may be used or invoked? One view, for instance, might hold the LDA as a piece of evi-dence to be used by a plaintiff in court for establishing past discriminatory treatment. An alternativeview holds the LDA search as a proactive, forward-looking process that can allow decision-makersto arrive at better policies for future populations of decision subjects. Depending on the use andmeaning ascribed to the LDA, courts might apply different standards and burdens on those whosearch for them. Our findings suggest that, whether the LDA is held as evidence or as a proactiveprocess for improving decision policies, there exist fundamental challenges to finding an LDA that 1Plaintiffs also generally lack the necessary information to be able to recognize when firms practices areproducing a disparate impact, let along the necessary information, expertise, and resources to identify a lessdiscriminatory alternative that would actually serve firms goals equally well.",
  "has generalizable, improved fairness and performance properties. Best-practices informed by thesechallenges, however, can yield LDA searches that are reasonable and effective": "In this paper, we attempt to provide a more precise technical characterization of the limits of LDAsand we explore what these limits might mean for the law and for the practice of searching forLDAs. In so doing, we attempt to provide greater clarity about what might be reasonably expectedof firms in light of multiplicity. This paper is organized by a slate of fundamental (negative) results,summarized below, which each represent limits to searching for and using LDAs. 1. Statistical Limits (): Firms often design algorithms before accessing all relevantinformation about the particular population subjected to an algorithm, so higher perfor-mance on a fixed dataset may not mean theres an LDA that generalizes to new populations. 2. Mathematical Limits (): We show that there are bounds on how much one canclose the gap in selection rates between groups at a certain levels of accuracy, given thesize of each group and the base rate of the property or outcome of interest in each group.We further show that given the full joint distribution of data, group and outcome (X,G,Y)and an initial classifier, determining whether an LDA exists is computationally intractablein general (i.e., NP-complete). 3. Welfare Limits (): We observe that LDAs narrow focus on the welfare of firms(i.e., whether they can achieve their goals equally well) fails to account for the fact thatcertain LDAs might leave consumers strictly worse off, even if they narrow disparities inselection rates across groups. Each of these results might seem to provide firms with reasons to reject calls to search for LDAsor with arguments to defend themselves in a disparate impact case. However, these claims only tellpart of the story. For each of our negative results limiting what is attainable in this setting, we offerpositive results demonstrating that there exist effective and low-cost strategies that are remarkablypowerful, if not perfect. These strategies enable firms to reliably unearth less discriminatory modelsthat generalize to new populations, even with very simple search methods. In , we provideresults from empirical tests of a particular group of methods that randomly generate alternativemodels in the same model class. In some observed instances, we find that these methods can reducedisparity out-of-sample, sometimes at no cost to utility.",
  "Here we provide an overview of related literature on less discriminatory alternatives, accuracy andfairness, and multiplicity": "Less discriminatory alternatives and statistical approaches. The applicability of discriminationlaw to data-driven algorithmic decisions has received much recent attention . Attempts todefine and operationalize normatively or legally useful notions of fairness abound . Inrecent work, Gillis et al. put forward a definition formalizing the notion of a less discriminatoryalternative. In their formulation, optimization occurs over a fixed dataset and both false positive rateand false negative rate are constrained. The authors use a mixed-integer program to identify LDAsin the case of linear classifiers. Black et al. point out the issue that fairness improvements maynot generalize, and requiring the reporting of fairness improvements on training data could lead tomanipulations and faulty results which the authors term D-hacking. Auerbach et al. , inspiredby disparate impact law, develop a statistical method for comparing the fairness-improvability of analternative classifier to a baseline. Accuracy-fairness trade-offs and welfare. Fair machine learning (ML) is a vast area of researchand finding satisfactory notions compatible with commonly held normative intuitions has provedcuriously difficult . Scholars have developed impossibility results, trade-offs andwelfare notions to frame the terms and achievable goals of defining algorithmic fairness .Liang et al. put forward a model for understanding the achievable performance across twogroups error rate and overall accuracy. Pinzon et al. analyze the feasible set of realized equalopportunity and accuracy and provide conditions where these are incompatible. Pleiss et al. ,focused on trade-offs between fairness notions, find that satisfying welfare defined over differenterror rates is equivalent to treating some population members randomly. Some empirical work hasleveraged findings in this space to develop fairness or fairness-accuracy improvements in real-worldsystems . Multiplicity. There is a growing interest in the idea that many classifiers can be similarly accuratebut have different performance along other desirable attributes. This notion is referred to commonlyas model multiplicity , the Rashomon effect , or under-specification . Semenovaet al. find that the size of the set of good models can serve as a measure of model-class sim-plicity. Cooper et al. find that variance in the design of classification algorithms can lead toindividual people receiving different treatments depending on which model is chosen. In the contextof disparate impact cases, Black et al. argue that the onus should be placed on the firm ratherthan the plaintiff to conduct a reasonable search for a less discriminatory alternative in light ofthe flexibility afforded by multiplicity .",
  "Statistical Limits": "The reality of ML-driven (predictive) policies is that they are designed and deployed without per-fect knowledge about the population subjected to them. Even if model A is observed to performbetter than model B on a training dataset, one cannot guarantee this performance out-of-sample onunseen data. These comparisons are particularly hard when A and B are sampled from differentdistributions (e.g., come from different model classes). The statistical task of understanding whatinferences can be made from a limited dataset is a fundamental challenge for ML research. As wewill describe, this challenge is similarly present for inferences about less discriminatory alternatives.In what follows, we present two imperfect LDA definitions that illustrate the challenges introducedby statistical limits. What is knowable when?To what statistical standards should a purported LDA be held? An LDAmust, of course, be (nearly) as good from the firms perspective as the model in question, and it mustalso exhibit lower disparities. But on what data distributions should we evaluate these claims? Ingeneral, we can only expect to evaluate performance of a model on pre-deployment data collectedby the firm, since post-deployment data typically lacks ground truth outcomes for those rejected bya model . On the other hand, we can evaluate the disparity produced by a candidate LDA onpost-deployment data, since this measure does not require access to ground truth labels; indeed, thisis one of the reasons to use selection rate disparities as an indicator for discrimination . It mightseem that this is the natural way to evaluate an LDA: it must offer (1) comparable utility as measuredon pre-deployment data, and (2) lower disparity as measured on post-deployment data. Unfortunately, this definition introduces a critical asymmetry between the plaintiff and the defendantthat renders it nearly vacuous. Whereas the plaintiffs search is entirely in-sample they observeboth pre-deployment and post-deployment data when looking for an LDA2 the defendants searchis out-of-sample. The defendant must (by definition) commit to a model before observing post-deployment data. Even if we restrict our attention to a set of models that achieve a certain accuracy,there is no guarantee that the lowest-disparity model on pre-deployment data will continue to exhibitminimal disparity on post-deployment data. In other words, the plaintiff will often be able to identifysuch an LDA after the fact, even if a defendant could never have done so in advance. To illustrate this point, we can consider the simpler task of increasing accuracy. Given labeled data inhindsight, producing the perfect-accuracy decision rule is easy (just use the outcome labels!). Sim-ilarly, producing a fair classifier in hindsight is intuitively an easier task than designing a classifierthat must be fair on unseen data. Optimizing for the future.Recognizing the mistake of evaluating a model on post-deploymentdata, we might revise our LDA definition as follows. As before, an LDA should (1) maintain com-parable utility when evaluated on pre-deployment data. However, instead of evaluating its post-deployment disparity, we will instead require that (2) it has lower disparity as measured on the pre-deployment data. We no longer allow for a temporal asymmetry between plaintiffs and defendants;all evaluations are conducted solely on information the firm has at the time of model development. While it is in principle feasible for a firm to meet this standard, doing so would be undesirable. Afirms goal is not (and should not be) to optimize for performance on some pre-deployment dataset;they instead seek to perform well out-of-sample when the model is deployed. A model that overfits",
  "Recall that plaintiffs initiate a disparate impact claim by using post-deployment data to establish that apolicy has resulted in a disparity in selection rates": "to the pre-deployment data (e.g., by only assigning positive classification to feature values withpositive labels in the pre-deployment data) is unlikely to generalize well. Similarly, while a firm canachieve any utility-disparity combination shown in in-sample, we would not expect thesemetrics to generalize out-of-sample . Instead, firms will in practice restrict the complexity of themodels they train to trade variance for bias. Models trained this way will in general be far away fromthe boundaries of the feasible polygon (see ). Under the LDA definition discussed here, theplaintiff could effectively choose a model with (near-)perfect pre-deployment performance. But itwould be undesirable and unreasonable to expect the firm to deploy such a model, since it wouldalmost certainly have poor post-deployment performance. Reasonable and unreasonable searches.If neither of these definitions captures the principlebehind LDAs, what could we do instead? Our proposal builds on Black et al. to put fortha standard of a reasonable search.While we cannot provide a comprehensive definition ofreasonble in this context, we offer several concrete criteria. First, when sampling multiple modelsfrom the same distribution (e.g., changing the random seed for a random split of the dataset orinitialization of a randomized training procedure), it is reasonable for a firm to choose the modelthat optimizes its stated utility-disparity trade-off as measured pre-deployment. Intuitively, whilein-sample performance does not provide an unbiased estimate for out-of-sample performance,classical results from learning theory tell us that empirical risk minimization yields good mod-els . Second, a reasonable search cannot require the defendant to know about the realizationof data post-deployment. A plaintiff may, however, question whether the firms pre-deploymentdata was collected so as to be distributed similarly to the post-deployment data if, for example,the defendant trained models on data from one country and deployed in another, the plaintiff mightquestion whether the firms model selection process was reasonable. Finally, the defendantschoice of model class is necessarily based on heuristics. Without knowing the true data distribution,there is no right model class. A plaintiff might well question whether, e.g., the defendant couldhave reduced disparities while preserving utility by using a more complex model class; suchquestions must be litigated on a case-by-case basis.",
  "Mathematical Limits": "As discussed in the prior section, in most cases, a firm can only access a single, finite dataset.This implies that conclusions about disparate impact must arise from information contained in arealized sample of dataeven when we hope these conclusions hold more generally. Setting asidethe statistical challenge of drawing generalizable conclusions about disparity out-of-sample, herewe limit our analysis to what is attainable on already realized data or fully-known distributions. Inother words, even when we remove from consideration the statistical challenges, we find certainfundamental limits and challenges still remain. In this section, we reason about the bounds of what is mathematically possible or impossible for anLDA search process to achieve. Our strategy is as follows: By strictly expanding the information thepolicy designer can access, if we can show a certain algorithm or strategy is impossible or complex,then we know such shortcomings hold in the (strictly more challenging) case where the firm accessesless information. A natural starting question one might ask in this setting is, what accuracy and fairness measuresare (im)possible to jointly achieve in the fixed sample? In other words, assuming we have accessto group belonging, labels, and outcome information Y , what can we conclude from moving errorsaround freely, caring only about performance in-sample? This question is considered in .2. This inquiry uncovers a fundamental limit to the efficacy of any search for a less discriminatory al-ternative: at certain levels of accuracy, finding a 0-disparity classifier is impossible. In the remainderof this section, we will show that 0- or near-0 disparity classifiers are possible to achieve as longas the utility achieved by the starting classifier remains below a certain cutoff, which depends onlyon the sizes of the positive group 1 (n1,+), negative group 1 (n1,), positive group 2 (n2,+), andnegative group 2 (n2,) populations. Another question we will ask is, given access to the full distribution over discrete data, what is thecomputational burden of finding whether there exists a less discriminatory classifier? This questionis considered in .3. We know that achieving highly performant LDA classifiers on a fixed and labeled dataset or aknown data distribution is much easier than guaranteeing any accuracy or disparity property out-of-sample from a finite sample. This suggests that limits and impossibility results in the easier casehold in the realistic setting, which is only harder. To make these claims more precise, we introducea working formalism before stating our results in formal terms.",
  "A Working Formalism": "Here we put forward a mathematical formalism for our setting, in which a firm makes a selectionamong a population. Our aim here is to introduce notation that will support inferences about whatis attainable and what is not attainable in the search for a less discriminatory algorithm. Population. Following convention , our population is described by the joint distributionX, G, Y, where each member (e.g., a loan applicant) is described by features x X, belongs to(categorical) group g G, and has an underlying true label y Y. In settings with binary labels,y {0, 1} (interchangeably, we may refer to these class labels as negative and positive +). Afinite dataset, drawn from the distribution X, is denoted X, with corresponding group membershipvector Y and group belonging vector G. Finally, we call the size of the population n = |X| and mayrefer to subsets of the population using subscript: ng,y. Probability distributions. Though the joint distribution of X, G, Y captures the relevant informa-tion about the population in full generality, there are a few particular probability distributions thatwill be useful to define. First, we define (x) := P[y = 1 | x], the probability that an individualwith data x has a positive outcome class. For simplicity, we assume that this unobserved outcomeclass Y that the firm wishes to base their selection on is independent of protected group status condi-tional on X, but we note that our results do not require this assumption (i.e., they hold in settings thatexhibit differential prediction). Second, we define g(x) := P[x | g], the group-specific probabilitydistribution over the data features. If the set of features X is finite, then g(x) represents the fractionof members of group g with data values x. Classifiers. We define a classifier h(x) : X {0, 1} as a mapping from features to binary labels.We use H to denote the set of all possible classifiers. The particular classifier that a firm commits tois h0(x), and a candidate alternative classifier (which may or may not be a LDA) is h(x). Selection rates and errors.For a given classifier h, denote the selection rate SR(h):=PxX [h(x) = 1] = ExX [h(x)]. For a particular group g, the group-specific selection rate wouldbe SRg(h) := ExX [h(x) | g]. Finally, because the firm is hoping to design their classifier h(x)to mimic the value y, we define the standard notions of false positive rate, true positive rate, falsenegative rate, and true negative rate. We will refer to them using their 3-letter abbreviations. Forexample, the false positive rate is defined as FPR := PxX [h(x) = 1 y = 0], i.e. the total por-tion of the population that is positively classified but has y = 0. The group-specific false positiverate FPRg is accordingly defined as the FPR conditional on group membership g. We use analogousnotation for TPR, FNR and TNR. Disparity and Utility. We define the demographic disparity of a classifier h to be (h) := SR1(h)SR2(h). We define this disparity assuming members of the population can be split into two groups,i.e. G {1, 2}. If we assume group g = 1 to be advantaged and group g = 2 to be disadvantaged,then the demographic disparity would be non-negative. Sometimes, well be interested in absolutedisparity, which we define as the absolute value of the demographic disparity. Finally, here weoffer a broad notion of utility for a given classifier h, allowing entities (firms or consumers) to haveparticular weightings of preferences over true positive and false positive outcomes. We define utilityas U(h; ) := TPRFPR where the value of R+ suggests the relative benefit of a true positivecompared to the cost of increasing the probability of false positive classification. This particularformulation is discussed further in .",
  "Polygon of Possibilities": "Here we consider the set of all attainable accuracy and disparity values for a binary selection rule.The set of attainable accuracy and fairness values can be surmised given access to a dataset X withgroup membership G and outcomes Y . This should be no surprise: if we can peek at the outcomescorresponding to every data point in a dataset, then we can easily produce decisions that bound theattainable accuracy and/or disparity performance. For example, the perfectly accurate decision rule",
  "(c) Feasible , U (deterministic)": ": Consider a given, finite population broken down by group belonging and outcomes (a; left). Ifrandomized decision rules are feasible, then a polygon depicts the convex set of feasible, in-sample utility anddisparity values (b; center). If solutions are restricted to deterministic classifiers over the dataset (c; right), thepolygon bounds the achievable values. could be attained by simply using the outcome data as classification labels. The perfectly biaseddecision rule would only select based on protected attribute. The highest-accuracy, fair decision rulecould be identified by starting with the perfect-accuracy classifier and swapping either 1) positive-labeled advantaged group members or 2) negatively-labeled disadvantaged group members untilselection rates equalize (choose whichever swap optimally trades off utility for disparity reductions). A representation of the feasible set of all decisions on the accuracy-disparity plane is provided in. The blue shape in (b) is the region encompassing all achievable accuracy (i.e.,utility) and disparity values. Note, however, that as long as the data and decision are discrete, not allvalues within the polygons are achievable. With discrete data, the set of achievable decision rules isnot connected. Instead, it is a dense lattice of points. Moving from one point to a neighboring pointcorresponds to switching the label on a single individual in the population. This lattice is depicted in(c). For exposition, in much of the remainder our analysis, we allow randomized decisions,though the results should hold for the discrete case. Recall that in this setting, the achievable accuracy and disparity values are computed on a singleinstance of a dataset with true outcome labels Y and group belonging G. This information allowsus to make certain claims about what performance a classifier can achieve on a single dataset. Forinstance, any reasonable classifier should be in the upper right or upper left quadrant of this plane.If two of our goals are to maximize utility and minimize disparity, then an ideal classifier should beas far north as possible, without veering too far west or east. Notice that the most accurate classifierrequires some non-zero level of disparity: if a classifier exhibits perfect accuracy on a dataset, thenits disparity is the difference in group-specific base-rates (this point is plotted as a yellow pyramid).Among the set of perfectly fair classifiers, the most accurate is plotted as a green point. There are anumber of questions we can ask about this feasible set. For example, how high does accuracy haveto be such that there does not exist an alternative classifier with 0-disparity and the same (or higher)firm utility? In the remainder of this section, we make formal claims about the properties of theaccuracy-disparity polygon, and discuss the reasonableness of conclusions based on it.",
  "(h) (h0)": "The proof of the above finding is provided in Appendix B. Intuition for this result can be found in(b). The classifiers that optimally trade off between utility and disparity can be found alonga (small) line segment between the perfect-accuracy classifier and the optimal, 0-disparity classifier.As long as a starting classifier has utility less than the optimal, zero-disparity classifier, then, thereexists a zero-disparity LDA with equal or greater utility.",
  "The fact that the accuracy-optimal classifier may, in many cases, have non-zero disparity couldraise questions about whether a 0-disparity classifier is possible to achieve at a certain (starting)": "level of accuracy. In other words, a firm defending against charges of discrimination might claimthat there is an inevitable trade-off between disparity and accuracy, and that therefore, favoring theadvantaged group serves a business need. We find, however that the fundamental trade-off betweenthese values on a fixed dataset does not kick in unless the accuracy of the starting classifier isabove a certain cutoff, and even still, there is room to maneuver to a floor on disparity. Further, asdepicted in , and as observed on empirical datasets in , this fundamental bound isvacuous except at the most (often unreasonably) high levels of accuracy, so likely would not offera sound defense in many realistic settings. So far we have claimed that a selection policy with 0 disparity and the same level of accuracy exists(in sample) in many cases. But, in practice, we often want deterministic classifiers, and we cannotcleanly separate members of the dataset according to their group belonging and outcome label. Ifwe stop allowing an algorithm designer to navigate the polygon in these ways, there may be no 0-disparity, deterministic classifier with limited data. Thus, in the next section, we ask when the firmcan find the best deterministic classifier.",
  "Hardness Result": "If it were easy to determine whether better alternatives exist, then the law would not have to rely onplaintiffs to tirelessly search for them. Hypothetically, an auditor could simply verify that any lend-ing policy or hiring policy is the least discriminatory possible, given the set of alternative policiesthat might perform a similar function (at least as effectively) for the business. Better yet, if this wereeasy to do, the business itself could simply employ the procedure to find a minimally-discriminatorypolicy that performs at least as well as their existing policy. However, the task of arriving at lessdiscriminatory alternatives is not so simple. In this section, we offer findings on the computational complexity and algorithmic opportunities forcertifying whether (reasonable) less discriminatory policies are possible. We show that certifyingwhether there exists a less discriminatory alternative algorithm is NP-hard. We demonstrate thisfinding even under special conditions where the firm accesses information and capabilities that, inrealistic settings, further steps would be needed to estimate or execute. However, in these settings,we show that certain approximation and relaxation strategies enable a firm to arrive at an LDA aslong as some de minimis difference in treatment is acceptable. These results suggest that defensiveclaims about the computational burden of identifying the least discriminatory policy are credible,but do not imply that reasonable search procedures are impossible. A starting definition. Because we treat the problem of finding an LDA with some formality in thissection, it is worth providing a technical definition for the sake of analysis. However, as we willquickly see, there are a number of ways to define this concept, and many intricacies and challengesarise depending on the definition used. Here we provide a starting formalism. Definition 1 (Full-information LDA). Given a data distribution X, a Bayes-optimal predictor (x),a group-specific probability density function g(x), a utility function U(h; ), and a baseline classi-fier h0(x), a full information LDA is a classifier h with utility at least U(h0) and absolute disparityless than |(h0)|. We call this definition the full-information case because we assume that we have full access tothe the probability distribution X and the Bayes-optimal predictor (x), which represents the trueprobability that members of our dataset with attributes x have positive outcome variable. We makethis assumption for the sake of proving impossibility and hardness results: if we can show that aproblem is NP-hard or contains certain inherent impossibilities even in the case where our capabili-ties are unrealistically broad, then we can infer that these results hold in the (harder) scenario wherewe also have to predict labels. In future sections we consider cases where the firm does not haveperfect access to . Finally, we note that Gillis et al. offer an alternative formulation wherethe business need condition is represented by upper bounds on both the FPR and FNR. Our utilitynotion is somewhat more flexible because it offers a weighting over errors, and focuses simply onthe relative value of true positives compared to the cost of false positives.",
  "C.2 contains a proof of the theorem involving a reduction from the Subset Sum Problem": "Meaning and strategies. How should we interpret this result? On its face, it seems to provide firmswith a natural defense: it is too computationally burdensome to find an LDA. A closer look suggeststhat NP-hardness may not be as much of an obstacle as it might seem. In Appendix C.3, we give a(1 + )-approximation algorithm for the full-information LDA problem. While it may be difficultto find the least discriminatory alternative, given full information, we can efficiently find a closeapproximation. We noted in that mathematical constraints, while present, were unlikely tobe binding in practicethe same is true here. If a plaintiff finds an LDA that a firm failed to use, itis unlikely to be because finding that LDA was intractable. Instead, it is more likely the firm lacked(or failed to collect) enough information. We turn next to the question of information and its limits.",
  "Consumer Welfare and Modeling Limits": "So far, we have focused on methods for arriving at an LDA with a single utility function, param-eterized by . One might reasonably assume this utility function represents the interests of thefirm, since disparate impact law harps on finding an alternative policy that meets the firms businessneeds. However, it is well known that firm and consumers can have divergent interests . Whenfirms select whom to offer a loan, their policies aim to avoid granting credit to people who are likelyto default, because the firm is unlikely to turn a profit in these cases. Granting risky loans could havedeleterious consequences for the consumer, too, because they can worsen the consumers financialoutlook in the long-term . In this section, we explore the possibility that consumers and firmsmight have different preferences over outcomes. In certain cases where the utilities of consumersand firms differ, we show that there exist alternative classifiers that jointly serve business needs andlower selection rate disparities but that leave consumers strictly worse off. In other words, thereexist classifiers satisfying our prior definitions of LDA that harm the consumers, including thosebelonging to the disadvantaged group. However, we show that just because someone can identify the existence of a consumer-harmingLDA, this does not necessarily mean that consumer-benefiting LDAs do not exist. Additionally, wefind that requiring that consumers should not be harmed does not impose significant computationalburden or effort on the LDA search. For any number of additional utility considerations beyond 2,or in settings where the set of utility considerations is unknown, the optimization is easily reducedto the case where || = 2.",
  "Consumer-harming LDAs exist. Here, we show that if firm and consumer welfare differ (as spec-ified further in Appendix D.1), it is possible to identify an LDA that strictly harms consumers": "Claim 1. There exists a full-information LDA setting X, , g with two utility parameters f = c,and a pair of classifiers h0, h such that (h) < (h0) and Uf(h) Uf(h0), but h strictlyharms consumers: Uc(h) < Uc(h0). The above finding (proven in Appendix D.2) suggests that there are certain instances where it is onlypossible to achieve consumer-benefiting or firm-benefiting alternatives, but not both simultaneously.An LDA that serves the firms interest will not, necessarily, benefit consumers, and vice versa. The existence of consumer-harming LDAs points to the fact that simplistic definitions of this con-cept can lead firms to arrive at classifiers that do not meaningfully improve conditions for con-sumers. It also suggests that an LDA might reasonably be refuted, if it fails to meet the conditionthat consumers (especially, those from the disadvantaged group) should be better off as the result ofa proposed LDA. Thankfully, as we will see, adding stipulations and considerations for consumerwelfare does not pose a significant computational challenge as it has nice convexity properties. Utility behaves nicely. In practice, how should a firm account for consumer utility, particularlywhen consumers dont necessarily know their utility functions and firms dont have the ability toask at scale? Here we show that as long as an LDA is able to satisfy two utility requirements,defined by two values 1, 2, it satisfies all utility values for lambda in the interval between them. Claim 2. If a firm is able to identify a region such that the utility consideration(s) are withinthis region, an LDA that improves welfare for each of the endpoints of the region also improvesutility for every utility function in that region. A formal proof is provided in Appendix D.3. Another way of thinking about the above result is thatthe utility as weve defined it is convex, so it isnt too burdensome to add constraints that requireadditional players or utility functions are satisfied in the LDA search. Further, in a world wherefirms do not know, exactly, what consumer welfare is, they should still be able to search for LDAsthat satisfy a range of (likely) utility formulations. Broadly, even though it is possible that navely-defined LDAs leave consumers worse off, it is easy to stipulate that an LDA should satisfy additionalutility considerations, and this is unlikely to considerably increase the firms computational burden.",
  "Empirical Results": "Here we describe a set of empirical experiments. The aim of these tests is to better understand theeffectiveness and characteristics of certain simple heuristic search methods for finding an alternativeclassifier with lower disparity and higher utility performance on new data. We do not expect thesemethods to identify a Pareto-optimal, fair-and-accurate classifier on unseen data an algorithmdesigner would need to be unfathomably lucky to arrive at a classifier of that sort, even on relativelysmall datasets. Instead, the methods we test search for alternative classifiers that are similar to thestarting classifier, by changing the random seed or randomly sampling and re-training. After pro-ducing, say, 100 similar models, the methods we test evaluate the level of disparity using availabledata (i.e., an evaluation set) and select the minimally-disparate alternative. Although these methods only search a narrow slice of possible classifiers, they have some desirableproperties that make them easy to work with and analyze. First of all, they do not explicitly modeldisparity and they produce a selection policy that does not, explicitly, use the protected attribute.In regulated industries like lending, discrimination based on these features is illegal. Second, themethods produce a set of alternative models that are in the same model class as the original classifier.This provides a convincing argument for meeting the business need for generalizable performance:if a model falls within the same class of models as the starting classifier, it does not introducenew complexity or expressiveness that could exhibit good performance on training data that doesnot translate, in reality, to new data. Third and finally, because the methods we use produce a setof similar models from the same model class, their disparity and utility performance on held-outevaluation data will represent identical and independent draws from a single distribution. This givesthe algorithm designer good reason to select the minimum disparity classifier on evaluation dataas a best-guess for the best-performing model in general. So-called disparate learning processes attempts to produce group-independent classifiers using a group-aware learning process havebeen analyzed at length, and there is reason to believe other methods may more effectively find lessdiscriminatory models . Data and models. For this test, we use two datasets, Adult and German Credit. Both datasets arecollected in financial settings. We use the Adult dataset to perform a classification where the taskis to predict which individuals make over $50,000 in income per year. We use the German Creditdataset to perform a classification where the task is to predict credit-worthiness (as defined in thedatasets meta-data). We test three starting classifiers: a Logistic Regression, a Random Forest, anda Decision Tree classifier. We implement these classifiers in Python using sklearn, typically withpre-set default hyper-parameters (though we do set max depth=5 for the random forest classifier).Though the datasets do not have balanced class weights, we train our models using balanced weights.A more detailed description of the datasets and models is provided in Appendix E. Search processes. We conduct two search types to identify proximate models from the same modelclass. The first is sampling and re-training we take a random sample, with replacement, the sizeof the training data and re-train the same model on this new sample of data. The second methodis testing different random seeds. We only use this second method for the Random Forest modelbecause it is an ensemble model that explicitly uses pseudorandomness and the model changessignificantly when we perturb the random seed parameter. Experiment procedure. First, we split the data into train, evaluation, and test sets. Using thetraining data, for each search process and model type, we train 10,000 models (generated at random)and record their performance and qualities in a dataset. The utility, disparity, and selection rate ismeasured on training, evaluation and testing data for every model, and recorded in a dataset. Totest the procedure of searching over n alternative models, we draw from our dataset n times forn {1, ..., 100}. Equipped with a subset of n models, we select the model with lowest disparity : Results from a simple randomized search for a less discriminatory alternative algorithm on the Adultdataset. We search by randomly sampling with replacement and re-training a Random Forest classifier n timesfor n {2, ..., 100} on a training dataset, and selecting the minimum-disparity classifier on an evaluation set.As n increases within this range, we find disparity decreases, on average, on a held out (out-of-sample) testdata (a; left). In this case, we also find that utility does not diminish from this procedure (b; center). However,as the number of random draws increases, the probability of having selected the optimally-performing modelout-of-sample decreases (c; right) suggesting that conducting an effective search might require passing overmodels that end up having lower disparity. on the evaluation data, and record its utility and disparity performance on test data. The averagelift in utility or reduction in disparity is attained by comparing this selected model to the averageperformance over the n models. For each value of n, we perform this sub-sampling procedure 2000times, and record the mean, 2.5th and 97.5th percentiles to produce point estimates, lower-bounds,and upper-bounds, respectively. Finally, we also track the rate at which the procedure tested produces a perfect guess of the out-of-sample disparity-minimizing model, over the set of models tested. That is, if we conduct ourprocedure with n = 100, we measure the frequency that the lowest-disparity model according toevaluation data is also the lowest-disparity model on the test dataset. We note that we do not claim to have the best or optimal procedure for searching, nor do we claimthat the method we put forward is reasonable in the legal interpretation as advocated by Black et al.. Instead, we aim to explore whether, in certain instances, simple methods can attain generalizablereductions in discrimination at no cost to accuracy (utility). We also wish to test whether thesemethods consistently arrive at the least discriminatory classifiers out-of-sample, or whether theyopen up the possibility that firms test and reject alternatives that end up with lower disparity inhindsight.",
  "GCDecision TreeSample-0.0019-0.00080.0105Logistic Reg.Sample-0.0136-0.00610.0175Random ForestSeed0.0015-0.00320.0040Sample-0.00610.00340.0125": "Results. The main results are reported in . The tests provide evidence that certain searchmethods can reveal the existence of lower-disparity alternative classifiers whose performance gen-eralizes out-of-sample. The asterisk * signifies that the results are statistically significant accordingto the boostrapped 95% confidence intervals, attained by repeating the search procedure 2000 times.In other words, the asterisk tests whether the directional change in disparity or utility was observedin at least 95% of the 2000 times the procedure was tested. Disparity reductions are statistically sig-nificant according to this procedure on the Adult dataset for all models and search methods tested. :Empirically observed achievable polygon using the evaluation data sets for the Adult andGerman Credit. In the case of Adult (left), the Random Forest starting classifier exhibits wide disparitiesand randomly sampled alternatives uncovers LDAs with significant (though small) gains on the training set. Inthe case of German Credit (right), the starting classifier does not exhibit wide disparities, leaving a narrowregion for LDA improvement. No statistically significant disparity reduction is observed on training data.",
  "The disparity effects on the German Credit dataset are not significant, which is explained both bythe much smaller sample size and the minuscule starting disparity level": "The Random Forest model has the highest utility measure on both datasets compared to the othermodels tested. It also had the highest absolute disparity on both datasets. These results are depictedin . Performing an LDA search procedure on the RF increases utility in our Adult test,meaning in this case, utility increases and disparity decreases statistically no trade-off is observed.The observed differences in these values is visualized in . In other cases, with other models,however, the utility decreases. This makes sense, given nothing about our procedure guarantees ortries to maximize directly the utility. One feature of the procedure we test is that it checks, and rejects, 99 models in favor of the modelthat has lowest disparity on an evaluation dataset. We find this method, even when it consistentlyreduces disparity out-of-sample, can increase the likelihood of passing over the actually disparity-optimal model out-of-sample. The frequency that the selected model is disparity-optimal of thoseconsidered is displayed in the rightmost plot in . The methods tested are not the only way to systematically search for less discriminatory algorithms.We could imagine altering the loss function of an algorithm directly so that the algorithm optimizessome weighted combination of disparity reduction and utility. We could similarly imagine encodingknowledge about the structure of disparity if certain attributes are likely proxies for the protectedgroup belonging, using more bespoke modeling assumptions could better target de-biasing interven-tions. Some may find the methods we test desirable, however, because they do not use pre-existingknowledge about groups or disparity, as they simply draw from the set of plausible good modelsgiven a pre-existing model class.",
  "Conclusion": "This paper is concerned with a legal concept that predates the era of pervasive machine learning.The less discriminatory alternative has emerged as an enticing notion because it could offer a wayfor old rules to apply to new tools. We find, however, that simple and elegant operationalizationsof this concept run up against fundamental limits, trade-offs, and impossibilities that circumscribethe attainable properties and attributes of an LDA search. Mathematically, a classifier can onlyexhibit certain combinations of accuracy and disparity a near-perfect-accuracy model cannothave 0 disparity unless a dataset already exhibits equal base rates. Computationally, a search forthe least-discriminatory-possible model at some baseline level of utility is generally intractablein polynomial time. Statistically, although LDAs are almost always identifiable in retrospect onfixed populations, making conclusions about how alternative classifiers perform on an unobserveddistribution is much more difficult. Finally, from a modeling and consumer welfare perspective, defining an LDA only in terms of business needs can lead to LDAs that leave consumers strictlyworse off. Each of these results seems to offer ammunition to firms defending their (potentiallyunfair) practices against accusations of discrimination. However, they only tell part of the story.While each of these results do fundamentally limit the search, they do not preclude firms andplaintiffs from adopting available, low-cost, and effective methods for identifying LDAs.",
  "A Feder Cooper and Ellen Abrams.Emergent unfairness in algorithmic fairness-accuracytrade-off research.In Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, andSociety, pages 4654, 2021": "A Feder Cooper, Katherine Lee, Solon Barocas, Christopher De Sa, Siddhartha Sen, andBaobao Zhang. Is my prediction arbitrary? measuring self-consistency in fair classification.arXiv preprint arXiv:2301.11562, 2023. Amanda Coston, Ashesh Rambachan, and Alexandra Chouldechova. Characterizing fairnessover the set of good models under selective labels. In International Conference on MachineLearning, pages 21442155. PMLR, 2021. Alexander DAmour, Katherine Heller, Dan Moldovan, Ben Adlam, Babak Alipanahi, AlexBeutel, Christina Chen, Jonathan Deaton, Jacob Eisenstein, Matthew D Hoffman, et al. Un-derspecification presents challenges for credibility in modern machine learning. Journal ofMachine Learning Research, 23(226):161, 2022. Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairnessthrough awareness. In Proceedings of the 3rd innovations in theoretical computer scienceconference, pages 214226, 2012.",
  "Jon Kleinberg, Sendhil Mullainathan, and Manish Raghavan. Inherent trade-offs in the fairdetermination of risk scores. arXiv preprint arXiv:1609.05807, 2016": "Himabindu Lakkaraju, Jon Kleinberg, Jure Leskovec, Jens Ludwig, and Sendhil Mullainathan.The selective labels problem: Evaluating algorithmic predictions in the presence of unobserv-ables. In Proceedings of the 23rd ACM SIGKDD International Conference on KnowledgeDiscovery and Data Mining, pages 275284, 2017. Benjamin Laufer, Sameer Jain, A Feder Cooper, Jon Kleinberg, and Hoda Heidari. Four yearsof facct: A reflexive, mixed-methods analysis of research contributions, shortcomings, andfuture prospects. In Proceedings of the 2022 ACM Conference on Fairness, Accountability,and Transparency, pages 401426, 2022. Benjamin Laufer, Thomas Gilbert, and Helen Nissenbaum. Optimizations neglected norma-tive commitments. In Proceedings of the 2023 ACM Conference on Fairness, Accountability,and Transparency, pages 5063, 2023. Benjamin Laufer, Jon Kleinberg, Karen Levy, and Helen Nissenbaum. Strategic evaluation.In Proceedings of the 3rd ACM Conference on Equity and Access in Algorithms, Mechanisms,and Optimization, pages 112, 2023. Jinsook Lee, Emma Harvey, Joyce Zhou, Nikhil Garg, Thorsten Joachims, and Rene F Kizil-cec. Ending affirmative action harms diversity without improving academic merit. In Pro-ceedings of the 4th ACM Conference on Equity and Access in Algorithms, Mechanisms, andOptimization, pages 117, 2024.",
  "Zhi Liu and Nikhil Garg. Redesigning service level agreements: Equity and efficiency in citygovernment operations. arXiv preprint arXiv:2410.14825, 2024": "Michael Lohaus, Matthaus Kleindessner, Krishnaram Kenthapadi, Francesco Locatello, andChris Russell. Are two heads the same as one? identifying disparate treatment in fair neuralnetworks. Advances in Neural Information Processing Systems, 35:1654816562, 2022. Charles Marx, Flavio Calmon, and Berk Ustun. Predictive multiplicity in classification. InHal Daume III and Aarti Singh, editors, Proceedings of the 37th International Conference onMachine Learning, volume 119 of Proceedings of Machine Learning Research, pages 67656774. PMLR, 1318 Jul 2020. URL John Merrill, Mark Jones, Mark Eberstein, Kareem Saleh, Dana Lockwood, Lusine Pet-rosyan, and Michael Akinwum. Improving mortgage underwriting and pricing outcomes forprotected classes through distribution matching. National Fair Housing Alliance and Fair-Play, 2024.URL",
  "Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory toalgorithms. Cambridge university press, 2014": "Jamelle Watson-Daniels, Solon Barocas, Jake M Hofman, and Alexandra Chouldechova.Multi-target multiplicity: Flexibility and fairness in target specification under resource con-straints. In Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Trans-parency, pages 297311, 2023. Jamelle Watson-Daniels, David C Parkes, and Berk Ustun. Predictive multiplicity in proba-bilistic classification. In Proceedings of the AAAI Conference on Artificial Intelligence, vol-ume 37, pages 1030610314, 2023.",
  "AFurther Related Work": "The so-called fairness-accuracy trade-off has been discussed by a number of scholars, and there areinstances where improving both is attainable algorithmically . The extent to whichsome form of disparate treatment is required to achieve demographic parity on datasets with base ratedifferences is discussed by . Reasoning about what notions of fairness (e.g. selection rates)and utility are attainable relate to, broadly, the opportunities and limits of phrasing the LDA conceptas an optimization problem, and face constraints about what is easily measurable and accessible(see e.g., ). Fuster et al. have explored the distributional effects between-group ofintroducing machine learning to credit markets.",
  "B.1Proof of Theorem 1": "Proof. The proof relies on reasoning about the Pareto-efficient region of achievable U and values.We say an alternative classifier Pareto-dominates a starting classifier if 1) the alternatives perfor-mance measures (e.g., utility and disparity) are greater than or equal to the starting classifiers, and 2)at least one of the alternatives performance measures is strictly greater than the starting classifiers.We define the set of Pareto-efficient classifiers as the set of classifiers that are not Pareto-dominated. A classifier in our setting is specified by four quantities: the proportion of the population of type(g, y) selected, for g {1, 2} and y {+, }. Phrased this way, we can use the fact that weresearching over a convex decision space (a cube representing four proportion values) and any pertur-bation or swap affects disparity and utility as a function of the g and y value of the swapped data,as well as the fixed population sizes ng,y.",
  "Observe that the perfect classifier, h(x) = y, is in the Pareto-efficient set because it uniquelymaximizes utility. We can use this classifier as an anchor point and find the alternative classifiers": "that optimally trade off utility and disparity. Notice there are exactly four possible ways to perturbh: a) Decrease the proportion of (1, +) members with positive label, b) decrease the proportionof (2, +) members with positive label, c) increase the proportion of (1, ) members with positivelabel, or d) increase the proportion of (2, ) members with positive label. We can immediately ruleout two of these perturbations by noticing they strictly harm both accuracy and disparity: (b) and(c) introduce errors that strictly leave group 2 with fewer selections or group 1 with more selections(respectively). This leaves two possible candidates for introducing errors that optimally trade-offaccuracy and disparity. Two claims are necessary for the remainder of the proof. First, observe that there are sufficientswaps of each type (a) and (d) to achieve zero-disparity using only one of these types. This obser-vations follows directly from our definitions of demographic disparity and advantaged group. Ifgroup 1 is advantaged (given), then SR1(h) > SR2(h)",
  "x h(x|g=2)": "n2 0. However, based on the fea-sible set of classifiers, we know that by exhausting all of swap (a) wed have SR1 = 0 without anychange to SR2. Similarly, if we exhaust all swaps of type (d) wed have SR2 = 1 at no change to SR1.Because there are sufficient swaps to change the ordering of the group selection rates, we know thatthere are enough of each swap type to equalize selection rates across groups.",
  "n": "Taken together, the above two claims suggest that a constant (real, not necessarily integer) numberof swaps draw the Pareto-efficient region trading off between utility and disparity, and this regionis a line segment defined by two points in utility-disparity space: utility-optimal classifier h andthe utility-optimal classifier with zero disparity hf, representing by (, U) = (0, uf), where uf =",
  "C.1Intuition for Hardness Result": "Here, we provide some intuition for the proof of the computational complexity of the full informationLDA. Consider the specific case where our dataset is made up of discrete and finite data values X ={x1, x2, ..., x|X|}. At its core, the mathematical task of finding the LDA is to grab (i.e., positivelylabel) a subset of data values in X that meet certain utility and disparity requirements. Intuitively, wecan imagine our data set as a collection of points. Every point xi X has a disparity value di and autility value ui. We are looking to find the subset S X such that the classifier h(x) = 1[x S] minimizes disparity (h) =iS di subject to a single constraint U(h) = iS ui U(h0),where we can think of u(h0) as some constant utility cutoff or threshold that constrains the search.Notice that if we switch the sign on utility, we can think of each point as having a cost ci := ui,and our utility constraint can be thought of as a budget B := U(h0) that constrains how much weare able to spend.",
  "i=0ci1[xi S] B": "If there exists a solution to the above optimization problem, then it would qualify as an LDA andsolve the problem put forward in Definition 1. However, solving this problem may be computation-ally onerous. Notice the optimization above looks almost identical to the 0-1 Knapsack Problem,which is known to be NP-hard. A noticeable difference in our case, however, is that we take anabsolute value over the objective function. The full proof (in Appendix C.2) involves a reductionfrom a related problem, the Subset Sum Problem.",
  "Definition 2 (Subset sum problem). Given a set W of integers {w1, w2, ..., w|W|}, find a subsetwhose values sum to 0": "Suppose we have a black box that computes any LDA problem. Were given an instance W ofthe subset sum problem. Our goal is to build an instance of the LDA problem where the solutionenables us to solve the subset sum problem efficiently. To start the proof, we will build a population of people that collectively compose our dataset. Wecan construct this population however we want, and at the end, well have an LDA search over thispopulation where the LDA classifier (if one exists) will precisely identify the subset of integers thatsolve subset sum! The people will be split cleanly into categories, denoted by a categorical datavalue x X, which takes values X {1, 2, ...}. We construct the population as follows: cyclethrough the values in our subset sum problem {w1, ..., w|W|}. For each element i, if the integer wiis positive, add 2wi people of group 1 to our population, each of type x = i. Otherwise, if integerwi is negative, add 2|wi| people of group 2 to our population, each of type x = i. We end up with apopulation that looks something like this:",
  "112w1120210222|w2|.........|W|12w|W||W|20": "In the above example, the first subset sum value w1 is positive, the second is negative, and the last ispositive, which means that the people are assigned to groups 1, 2, and 1, respectively. We use ng(x)to refer to the number of people of data type x and group g. To complete our population, we add two more data values, x and x. The first data value, x,contains a single person of group 1. The second data value, x, will equalize the total number ofpeople of each group: we take the absolute difference between all the people weve created in group1 and all the people weve created in group 2, which is equal to |2 i wi + 1|. Well add this manypeople with data type x to whichever group has fewer people to equalize the overall number ofpeople in each group. Finally, if we want, we can add additional people to the population, but theymust have data type x and an equal number must be added from each group, to preserve balance.",
  "i wi + 1| + 2": "Now, our population is fully specified. Since we know the total number of people (N), we are nowable to specify the probability density function of our population, both overall and within group the distribution is specified by g(x). Our last step is to specify the underlying base rates for eachdata point in our population. For this, we simply assign (x) = 1 for all x = x, and (x) = 0.",
  "i wi + 1 + 20": "Based on the way weve set up this population, we want the LDA search to select exactly the valuesin X that correspond to the indices of W that solve the subset sum instance (if and only if a solutionexists). Therefore, we want the LDA search to never select our slack parameters x, x. Thefirst slack parameter will never be selected because the density of the group-1 population in x is2N , which is half the minimum difference between any two densities of group 2, so a LDA withdisparity 0 could never include data x. The second slack parameter will never be selected as longas the utility of selecting it is negative, with greater magnitude than the sum of all other utilities in theentire search, since in that case, including it would always yield a utility less than 0 < U(h0) = 1",
  "i|wi| + 1": "The point here is, our hardness result does not depend on any particular value of . We can completethe reduction for any feasible value > 0, as long as we set to be sufficiently large. Now were ina position to state the following Lemma, which represents the rest of whats needed for our proof: Lemma 1. Consider the LDA problem X, g, , h0 specified above, for any given value > 0. Ifan LDA does not exist, there is no solution to subset sum problem W. If an LDA h does exist, theindices of the data for which h(xi) = 1 are the subset {wi} W which sums to 0.",
  "N , 12": "N , ... Now, notice the disparity of our startingclassifier h0 is1N . This value is strictly between 0 and the minimum non-zero disparity given oursetup,4N . Thus, any less-discriminatory classifier that is attainable from the non-slack variables(x {1, 2, ..., |W|}) must have disparity values that sum to 0. Finally, weve already shown thatno LDA solution will include a positive label for the slack variables x, x, so this concludes theproof.",
  "This concludes the reduction": "Weve thus shown the full-information LDA problem is at least as hard as Subset Sum. Showingthat the full-information LDA problem is NP-complete further requires that the problem is in NP.We know this is true because, given a candidate solution and a problem statement, we can simplycheck whether the solution has lower demographic disparity and greater or equal utility (which bothrequire only taking a sum over the population).",
  "C.3A (1 + ) Approximation": "Weve shown that the task of finding a less discriminatory alternative classifier is NP-hard, even incases where we can access the true probability of a positive outcome for every data point. However,hardness does not imply impossibility, and there remain strategies for identifying LDAs. Here, weask, what if were okay with identifying a less discriminatory alternative so long as the startingclassifier h0 is at least some minimum distance from the least discriminatory alternative classifier? Hypothetically, consider a case where the difference in disparity between a classifier h0 and the leastdiscriminatory alternative (which we can call h) is 0.01. We may decide that such small differencesare legally and ethically de minimus, i.e., too trivial or small to merit consideration. Equippedwith some minimal value of this sort, denoted , perhaps wed be satisfied if we can reliably and efficiently identify an alternative classifier whose disparity is within a factor of at least (1+) of theleast discriminatory possible alternative. If we have this approximation, then the only case wherewe might fail to find an LDA where one exists is when the starting classifier h0 is exceedingly closeto the least discriminatory alternative h. In cases where the starting classifier is outside of a factorof (1 + ) of h, we can guarantee that we will find a less discriminatory alternative.Definition 3 (Full-information approximate LDA). Given a set of data X, a Bayes-optimal predictor(x), a group-specific probability density function g(x), a utility function U(h; ), a baselineclassifier h0(x), and > 0, a full-information -approximate LDA is a classifier h with utility atleast U(h0) and absolute disparity less than |(h0)|(1",
  "C.4Empirical Demonstration of Approximation Algorithm": "To demonstrate the effectiveness of the approximation derived above empirically, we im-plement both exact and approximation algorithms to solve any LDA problem, specified byg(x), (x), h0(x), for some discrete number of values x X. Equipped with these algorithms,we can evaluate their performance on instances of the LDA problem. These findings corroboratethe formal results put forward in this section: that although the worst-case runtime explodes for ageneral instances of the LDA, there are efficient (i.e., polynomial-time) approximation algorithmsthat identify LDAs with high accuracy. Simulating LDA problems. To simulate instances of the LDA problem, we first specify the numberof discrete data values, noptions := |X|, and the maximum number of digits per density specification,maxdigits. Given these specifications, a simulator generates two sets of uniform random numbers,each of size noptions, and then performs a manipulation so that each set adds to 1 and contains floatvalues with at most maxdigits digits after the decimal place. These sets are used as values 1 and2, the group-specific probability densities over the data. Finally, using the identical procedure, asimulator establishes the true probability values for each data option, (x), this time using at most2 digits after the decimal place. Finally, to create variation in the total size of the input, we simulateinstances with varying values for noptions {4, 5} and maxdigits {1, 2, 3, 4}. In total, wegenerate 202 instances of LDA problems, of which 164 contain an identifiable less discriminatoryalternative and 86 contain no LDA. In this simulation, the input sizes range from 14 to 47 totaldecimal digits.",
  "The runtime and accuracy performances are displayed in": ": Runtime (left, center) and accuracy performance (right) of exact and approximate algorithms forcomputing the least discriminatory alternative classifier, given oracle access to X, G, and the Bayes-optimalpredictor (x). A total of 250 instances of the LDA problem were randomly generated for varying numbersof data values, starting classifiers, and the number of digits used to specify probability densities. For everygenerated instance, all four algorithms were run and runtime and accuracy were recorded. As the size ofthe input increases (measured as the total number of digits beyond the decimal places used to specify (x)and g(x)), the worst-case runtime complexity for the exact algorithm explodes. However, polynomial-timeapproximations achieve high hit rate.",
  "D.1Firm and Consumer Utility Differ": "The LDA is a less discriminatory policy with some welfare guarantee. Part of what makes it attrac-tive, conceptually, is that it seems to be a way to achieve a desirable goal (lowering disparity) withoutharming the decision maker (the firm). However, the firm is not the only stakeholder whose interestsmatter. The decision subjects welfare is also important as part of the broader set of societal con-siderations , and indeed protecting the interests of disadvantaged consumers is the underlyingmotivation for fair lending interventions and regulations, including those that use the LDA. Why might consumer welfare be left out of discussions of LDA? There are a number of reasons whythis consideration isnt made explicit, or is paid only lip service. First, it can be enticing to makethe simplifying assumption that firm and consumer welfare are aligned. Firms dont want to loan toconsumers if itll bury them in debt they arent able to repay, and similarly, consumers are harmedby these sorts of predatory practices. At the same time, firms want to provide loans to applicantswho are likely to repay a loan, because they profit from interest payments, and these applicantsbenefit from access to capital. Second, consumer welfare is hard to directly observe, comparedto selection rates. Consumer welfare the actual preferences of consumers over loan grantingdecision outcomes is heterogeneous across the consumer population, depending on a host offactors including the true probability of default and circumstantial and contextual factors motivatingthe present need for cash, none of which can be observed by a regulator or a firm. Selection rates,on the other hand, are more easily and immediately observed. Using lending rates and disparities as a proxy for the underlying goal (consumer welfare) begs thequestion: how good of a proxy is it? Though the answer to this question depends largely on context,we can make some strides towards answering the question with the help of modeling. In controlledenvironments where we can either observe ground truth labels or simulate them, perhaps we canbegin to answer the question of when LDA searches based on selection rates align with consumerutility, and when they do not. Formal Utility Model. Recall that we define utility to be U = TPRFPR. This definition of utilitymight be applicable for certain contexts. For example, in lending, there are benefits associated withbeing offered a loan, and costs associated when credit is awarded even though the borrower is likelyto default. We note, however, that there are other functions (outside this function family) that mightbetter represent welfare interests, especially in other contexts such as hiring, where a consumermight benefit from being offered a job whether or not they are deserving. For the sake of thissections analysis, we say that instead of a single real value, can be a vector of real values, denoted Rd. We will most often consider the case where d = 2, and refer to the two utility values ofinterest as f and c, or the utility of the firm and consumer, respectively. Later in this section, wewill motivate why we only consider two welfare values, because any vector of welfare considerationsof length greater than 2 can be reduced to an LDA search with only two welfare considerations.",
  "ESupplementary Materials on Empirics": "Data and Models (further information). Every row in the Adult dataset represents features of anindividual, and the outcome variable is an indicator representing whether they make over $50,000in annual income. The data is on American adults from census information in 1994. We use atotal of four variables maritalstatus (a categorical variable representing whether the individualis single/divorced/married/etc), hoursperweek (a numerical variable representing the number ofhours the individual works per week), education (a categorical variable specifying the level ofeducation achieved, e.g. college degree), and workclass (a categorical variable representing thetype of work). The protected attribute is the gender (given as a binary M/F). The dataset contains atotal of 32,561 rows. The German Credit dataset contains loan decisions in Germany, and was accessed viathe UCI machine learning database.The dataset contains a total of 1,000 rows withcategorical and numerical features related to individuals credit, financial status, employ-ment, and loan application.The following five features were used to train the models:credit history category (categorical variable representing information about credit history),credit amount (numerical variable representing amount of credit requested by loan applicant),unemployment category (categorical variable with information about whether the individual isunemployed), installment rate percentage income (numerical feature representing the ratiobetween loan amount and income), and present residence duration (a numerical variable rep-resenting the amount of time the applicant has resided in their current residence). The protectedattribute is the gender (given as a binary M/F).",
  "FAcknowledgments": "The authors would like to thank Emily Black, Mingwei Hsu, Pauline Kim, Jon Kleinberg, LoganKoepke, Helen Nissenbaum, and the members of the Fairness, Accountability, Transparency, andEthics group (FATE) at Microsoft Research, the AI, Policy and Practice group (AIPP) at CornellUniversity, and the Digital Life Initiative (DLI) at Cornell Tech for providing feedback on this work.The work is supported in part by a grant from the John D. and Catherine T. MacArthur Foundation.Much of the work was completed while Laufer was an intern at Miscrosoft Research. He is addi-tionally supported by a LinkedIn-Bowers CIS PhD Fellowship, a doctoral fellowship from DLI, anda SaTC NSF grant CNS-1704527. Any opinions, findings, conclusions, or recommendations ex-pressed in this material are those of the authors and do not reflect the views of NSF or other fundingagencies."
}