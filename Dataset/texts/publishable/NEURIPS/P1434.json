{
  "Abstract": "In many settings, machine learning models may be used to inform decisions thatimpact individuals or entities who interact with the model. Such entities, or agents,may game model decisions by manipulating their inputs to the model to obtainbetter outcomes and maximize some utility. We consider a multi-agent settingwhere the goal is to identify the worst offenders: agents that are gaming mostaggressively. However, identifying such agents is difficult without being ableto evaluate their utility function. Thus, we introduce a framework featuring agaming deterrence parameter, a scalar that quantifies an agents (un)willingnessto game. We show that this gaming parameter is only partially identifiable. Byrecasting the problem as a causal effect estimation problem where different agentsrepresent different treatments, we prove that a ranking of all agents by theirgaming parameters is identifiable. We present empirical results in a synthetic datastudy validating the usage of causal effect estimation for gaming detection andshow in a case study of diagnosis coding behavior in the U.S. that our approachhighlights features associated with gaming.",
  "Introduction": "Machine learning (ML) models often guide decisions that impact individuals or entities. Attributesdescribing an individual or entity are often inputs to such models. In response, such entities maymodify their attributes to obtain a more desirable outcome. But changing ones attributes may be costlydue to the difficulty of generating supporting evidence, or penalties for fraud. This behavior is calledgaming or strategic adaptation . Strategic adaptation frames gaming as utility maximization:agents change their attributes to maximize a payout, but incur a cost for modifying attributes. As an illustrative example, we turn to the health insurance industry. In the United States (U.S.),contracted health insurance companies report their enrollees diagnoses to the government, whichcalculates a payout based on reported diagnoses via a publicly available model . The payout isintended to support care of the enrollee in relation to the diagnosis. Companies may attempt tomaximize payouts by reporting extraneous diagnoses, an illegal practice known as upcoding .Despite increasing awareness of upcoding , upcoding costs U.S. taxpayers over $12B U.S.dollars annually , even with substantial investment in audits ($100.7M U.S. dollars, 2023 )and payout changes to adjust for gaming . Since audits may not scale and often overlookfraud , tools for flagging gaming-prone agents could help target audits. Beyond healthinsurance, gaming emerges in responses to credit-scoring algorithms and driver responses torider allocation algorithms in ride-sharing apps .",
  "an unknown utility function": ": Left: Two agents with gaming deterrence parameters 1 = 30 (purple) and 2 = 50 (blue)maximize utility (reward R - cost c) with respect to diagnosis rate. Gaming costs increase in (),and lower an agents optimal diagnosis rate (stars). Center: Agents observed decisions reflectutility-maximizing behavior. Right: A decision-maker computes a payout based on agent decisions. In this work, we study how one can identify agents with the highest propensity to strategicallymanipulate their inputs given a dataset of agents and their observed model inputs. A supervisedapproach is infeasible since fraud/gaming labels are unavailable in our setting. A common paradigmfor fraud/gaming detection is unsupervised anomaly detection. However, gamed attributes may notbe outlier-like. The perspective of gaming as utility-maximizing behavior in strategic classifica-tion provides an alternative to existing fraud/gaming detection methods. Many works in strategicclassification (e.g., ) assume known utility functions and identical feature manipulation costsacross agents, which assists in identifying an agents optimal gaming behavior. However, suchassumptions may not always apply. For example, in U.S. Medicare, due to the rarity of penaltiesfor upcoding, it is unclear how to quantify the cost of fraud. Furthermore, due to the large numberof companies contracted with U.S. Medicare, with distinct incentives (e.g., for-profit vs. non-profitgroups) and disjoint populations of patients, there may be heterogeneity in feature manipulation costs. To bridge this gap, we propose a novel framework for modeling agent utilities by introducing agaming deterrence parameter, which scales the perceived cost (to the agent) of gaming. First, weshow that directly estimating the gaming parameter is not possible: the best we can do is a lowerbound on the gaming deterrence parameter. However, by re-casting gaming detection as a causal effectestimation problem, where each agent represents a treatment, we prove that a ranking of agentsbased on their gaming deterrence parameter is recoverable. Thus, we propose a causally-motivatedranking algorithm that produces a ranking of agents. Practically, agents most likely to game underour ranking could be flagged for further monitoring/auditing. While our framework is inspired byhealth insurance fraud, it applies more broadly to instances of gaming where multiple agents aregaming an ML-guided decision. We evaluate the performance of causal effect estimators for gaming detection in a synthetic dataset.Empirically, causal approaches rank the worst offenders higher than existing non-causal approachesthat screen based on payouts/randomly, as well as anomaly detection methods. We then verify in areal-world U.S. Medicare claims dataset that causal effect estimation yields rankings correlated withthe prevalence of for-profit healthcare providers, a suspected driver of gaming . In summary: we 1) extend strategic classification to model differences in gaming behavior acrossagents (), 2) prove that point-identifying an agents gaming parameter is impossible withoutstrong assumptions (), 3) show that, by recasting gaming detection as causal effect estimation,one can recover a ranking of agents based on their gaming deterrence parameters (), 4)demonstrate empirically that our framework identifies the worst offenders with fewer audits thanbaselines (), and 5) show in a real-data case study that our approach yields rankingscorrelated with suspected drivers of gaming (). Code to replicate our experiments will bemade publicly available at",
  "Related works": "Machine learning-based anomaly detection.Fraud detection is often framed as an unsupervisedanomaly/outlier detection problem . Such approaches assume gamedmodel inputs are outliers in some distribution. However, this assumption may be incorrect if gaming is common in the data, or if gaming results in only small changes to observed agent attributes. Inborrowing from strategic adaptation, we frame gaming as utility maximization, rather than makingdistributional assumptions about gamed agent attributes. Strategic classification & gaming in machine learning.A large body of work in strategic classi-fication aims to design incentives to mitigate gaming/strategic behavior by agents. However, suchworks assume that feature manipulations costs are known/can be estimated across agents, or areidentical . In contrast, in our setting, feature manipulation costs areunknown and may differ across agents, but exhibit some shared structure that facilitates comparisonsacross agents. Shao et al. assumes that agent gaming capacities may differ by placing bounds onmanipulation, which may be unrealistic. Closest to our work is that of Dong et al. , which alsoassumes unknown and differing agent costs, but does not leverage similarities in gaming across agents.Our work supplements the strategic classification literature by studying a related yet fundamentallydistinct problem: rather than designing incentives to mitigate strategic behavior, which is infeasibleunder unknown manipulation costs, we leverage differences in costs across agents to identify agentsmore likely to game.",
  "(d) arg maxdDR( d; f) g( d, d)(1)": "where R : D R is the payout of changing d to d, and g : D D R+ is the cost of manipulatingd. When D R, g is often assumed to be separable; i.e., for some function c, g( d, d) = c( d d).() describes how an agent manipulates d to obtain a higher payout from f. This behavior iscalled strategic adaptation or gaming. For simplicity, we assume R = f; i.e., the model f directlydetermines the payout. Modeling agent variation in gaming. To extend strategic adaptation to multiple agents, we add anon-negative gaming deterrence parameter p R+ to Eq. 1. Consider an observational datasetDp {(xi, di)}Mpi=1 of an agent ps decisions di {0, 1} given some information xi X. Forsimplicity, we assume di is binary, though the proposed framework generalizes to non-binary decisionsand arbitrary numbers of independent decisions. For example, a health insurance plan p chooseswhether to report that an enrollee has a diagnosis di given enrollee characteristics xi. Agentassignment is mutually exclusive (e.g., individuals are enrolled in one health insurance plan).",
  "P(di = 1 | p) p(dp) arg maxdR( d) pc( d dp)(2)": "where R : R, c : R R+, and dp is the ground truth value of d given the xi seen by agentp. Thus, p(dp) is the gamed/observed decision rate of agent p in a population where the groundtruth decision rate is dp. Although d since it is a proportion, our framework applies to d onarbitrary intervals. This formulation assumes that each xi is equally likely to be gamed, that xi aretruthfully observed, and that any difference between p(dp) and dp is due to gaming. We focus on the gaming deterrence parameter p, which scales the cost of manipulation c(). p isnon-negative and represents an agents aversion to gaming. Lower values of p mean that agentp is more willing to game. Thus, identifying agents most willing to game means finding agentswith the lowest p. We summarize multi-agent strategic adaptation in . Next, we introduceassumptions on the reward function R, cost function c, and ground truth dp. Assumption 1 (Shared rewards & costs). Reward (R) and cost (c) functions are shared across agents.Sharing R encodes the assumption that all agents are reacting to the same payout model, while sharingc across agents encodes the belief agents must take similarly-costly actions to manipulate features(e.g., if insurance plans must follow/fraudulently report specific procedures to justify a diagnosis).Many works in strategic classification implicitly make a similar assumption (e.g., ).Assumption 2 (Increasing rewards). The reward function R is strictly increasing in d.Increasing rewards formalizes the agents incentive to perturb its decisions (i.e., inputs to the payoutmodel) from di = 0 to 1 in Eq. 2.Assumption 3 (Cost convexity). The cost function c is strictly convex and minimized at 0 such thatc(0) = 0 and c(0) = 0, and increases for all agents p for any d dp.One possible c is c(x) = x2. Strict convexity ensures a unique cost-minimizing action, and c(0) = 0ensures that dp is ground truth, such that increasing d incurs greater cost (e.g., , left). Assumption 4 (Diminishing or linear returns). The reward function R is concave in di.For example, R may be a log or affine function. Assumption 3 (strictly convex c) and 4 ensure that Rcannot grow fast enough to offset manipulation costs. Furthermore, due to Assumptions 2- 4:Remark 1 (Gaming is utility-maximizing). Given any agent p and dp, we have that p(dp) dp.Equivalently, optimal gaming entails increasing di from the ground truth. Note that Assumptions 2- 4are more general versions of assumptions placed on rewards/costs in the strategic classificationliterature (e.g., ).Assumption 5 (Non-strategic behavior is feasible). dp is a constant depending solely on xi.Due to Assumption 5, ground truth dp may vary by agent due to differences in xi (e.g., healthinsurance plans serve populations with varying levels of health).",
  "and the bound is sharp": "Intuitively, different values of the unknown dp yield different estimates of p consistent with theobserved p(dp). Thus, uncertainty in dp results in uncertainty in p. Equivalently, point-identifyingp requires perfect knowledge of dp. Thus, without further assumptions, p is only partiallyidentifiable. The lower bound is attained for dp = 0 (all di = 1 are manipulated), while p as p(dp) dp (no manipulation). Intuitively, increases in p further disincentivize increases top(dp), such that p(dp) gets closer to dp. A nave approach to gaming detection would be to rank individuals using the above bound. To seewhy this is problematic, consider an Agent 1 (1 = 10) and Agent 2 (2 = 30), and let R(x) = xand c(x) = x2. Suppose Agent 1 is a health insurance plan serving a relatively healthy population(d1 = 0.05), while Agent 2 serves a population with a higher burden of illness (d2 = 0.12). ViaEq. 2, we have 1(d1) = 0.10, while 2(d2) 0.14. Substitution into Eq. 3 yields 1 5 and2 3.66, flipping the true ranking of p. Thus, acting on this bound may incorrectly penalizeagents when a high p(dp) is appropriate; e.g., insurance plans serving sicker populations.",
  "(repeat for & other agent pairs)": ": Causally-motivated gaming detection. Left: First, we impute counterfactual decisions foreach agent. Middle: The imputed counterfactuals yield average treatment effects (ATEs) across pairsof agents. Right: Using ATE estimates to rank agents yields a ranking of the gaming parameter p.We show one direction of comparison across agents for simplicity. In practice, we impute decisionsfor both directions of comparison and average (given blue agents observations, impute purple agentsdecisions).",
  "Identifying a ranking of the gaming parameter": "Since we showed that point-identifying p is impossible without further assumptions, we relaxgaming detection to a ranking problem. Intuitively, differences in agent behavior under similarconditions may indicate different gaming capacities, from which the proposed approach follows. Ranking p by estimating counterfactuals. Recall that we aim to find agents with the lowest p.Thus, it suffices to rank agents by p, which can be done as follows:Theorem 1. Under Assumptions 1- 5, and p(dp) defined as",
  "Theorem 1 tells us that estimation of p(dp) and p(dp) can be used to rank p vs. p. Eq. 4": "differs from Eq. 2: while R, c, and dp are the same, p changes to p. While subtle, the distinctionis key to gaming detection: p(dp) denotes what agent p would have done in a population withground truth dp, as opposed to what agent p actually did in their population (ground truth dp). To build a ranking strategy, first consider ranking two agents p and p. Define Dp,p {(xi, di, pi) |i = 1, . . . , n and pi {p, p}}, where pi is an indicator for the agent that observed example i.Following the Neyman-Rubin potential outcomes framework , let di(p) be the value of di ifpi was set to p (i.e. had agent p been compelled to make a decision). Such variables are calledcounterfactuals. (left) shows a toy dataset with counterfactuals di(p), di(p), where ? areunobserved decisions di. Dropping unobserved data, the average di(p) is p(dp) by definition. Thus,if di(p) and di(p) are fully observed, one could estimate p(dp) and p(dp) as follows:",
  "return ranking": ": Pseudocode for causally-motivated gaming detection. Causal effect estimators take pairsof agents (p, p) and their observations x() as inputs, and output the mean difference in predicteddecision rate. where np is the number of observations by agent p, and use these estimates to rank p as per Theo-rem 1. However, only one of di(p) or di(p) is ever observed. The need to estimate a counterfactual(namely, p(dp)) suggests a causal inference approach, which proceeds assuming the following :",
  "Assumption 7 (Consistency). For all i, di(pi) = di": "Assumption 8 (Positivity/overlap). For any agent p and x X, 0 < P[p | x] < 1.Via Assumptions 6- 8, we have that E[di(p) | xi] = E[di | xi, p] and E[di(p) | xi] = E[di |xi, p] . Hence, the causal effect is identifiable. This estimand corresponds to a three-variablecausal graph, where xi are confounders, the agent indicator pi is a treatment, and the agentsdecision di is the outcome (, right).1 We proceed by estimating the effect of swapping agents:",
  "Corollary 1. Define (p, p) Exi[E[di(p) | xi]] Exi[E[di(p) | xi]]. Then, given Assumptions 1-8, (p, p) > 0 if and only if p < p": "Since E[di(p) | xi] = E[di | xi, p], it is an unbiased estimator of p(dp) by definition, andsubstitution into Theorem 1 yields the result. Thus, one can rank p by estimating the effect ofswitching from agent p to p on the observed decision rate for all pairs of agents p, p. Each effectestimate is a pairwise comparison of agents, from which a ranking of () follows. We summarizecausally-motivated gaming detection in , with pseudocode in . Obtaining a well-ordered ranking. Since we aim to rank p, our chosen estimator should yield awell-ordered ranking. Via Corollary 1, the oracle treatment effect suffices, but must generally beestimated. We show that a sufficiently accurate estimate also yields the desired result: Proposition 2. Let () be the oracle treatment effect function as defined in Corollary 1, and be itssample estimate. Given Assumptions 1- 8, for any > 0, if sup |(p, p) (p, p)| , then, forall p, p such that minp,p |(p, p)| > , (p, p) > 0 if and only if p < p. The result is immediate: sufficiently low estimation error in (i.e., ) cannot flip any pairwiserankings where > . Thus, any consistent estimate of yields (asymptotically) a well-ordering ofp. We defer to past asymptotic analyses of causal effect estimation for further discussion .",
  "Empirical results & discussion": "We aim to demonstrate that causal inference can be used for gaming detection. First, we discuss oursetup (.1). Then, we show in synthetic data (.2) that causal methods require feweraudits than existing non-causal methods to catch the worst offenders. Finally, in real-world case study(.3), we find that causal methods yield rankings correlated with suspected drivers of gaming.",
  "We describe the datasets, evaluation method, and gaming detection methods under consideration": "Datasets. In real datasets, ground truth gaming rankings are often unavailable. Thus, we validate ourframework in a synthetic dataset. We hand-select 20 p values (one per agent; see Appendix C.1 forraw p values) and simulate confounding by generating covariates from agent-specific Gaussianswith means p R2. To control confounding strength, we set p = g(log(p)), where g is anaffine transformation such that the range of p across agents equals a chosen range parameterR {0, 0.1, . . . , 1.0}. . Smaller R implies less confounding, since () varies less across agents.We generate 500 observations x(i) R2 and ground-truth d(i) per agent via",
  "d(i) Ber((i)p );(i)p= arg maxdplog( dp) p( dp d(i))2": "Recall that the decisions d(i) are also agent inputs to a payout model. We generate 10 datasets (eachN = 10, 000; 20 agents 500 observations) for all 11 levels of confounding (as measured by therange of means R). Causal inference assumptions hold in the synthetic data: all confounders x(i)are observed (Assump. 6), consistency holds by construction (Assump. 7), and overlap holds since allx(i) | p are supported on R2 (Assump. 8). Full synthetic data generation details are in Appendix C.1. To benchmark causal effect estimation in a more realistic setting, we apply causal methods to gamingdetection in U.S. Medicare claims. Medicare is the public health insurance system in the U.S. forresidents aged 65 and over. Since private insurance claims data is not widely available, we conducta gaming case study in healthcare providers. In Medicare, the U.S. government pays healthcareproviders on a per-service basis . Thus, providers may be incentivized to label enrollees with asmany diagnoses as possible to secure extra payment from the government. We select a 0.2% sampleof all Medicare enrollees with a claim in 2018; i.e., those who utilized a service covered directly byMedicare (N = 37, 893). We use demographic information and diagnoses in 2018 as covariates andselect the rate of uncomplicated diabetes diagnosis in 2019 as the outcome. Given differences inhealthcare policy and access across U.S. states, we pool data at the U.S. state level and treat eachstate as an agent. Additional cohort details are in Appendix C.2. Evaluating rankings. Given an observational dataset of the form {(xi, di, pi)}Ni=1, with covariatesxi, observed decisions di, and agent indicators pi, gaming detection algorithms output an ordinalagent ranking in terms of the gaming parameter p. We aim to measure the efficiency of a predictedranking given some level of resources committed by a decision-maker (i.e., # of agents audited). Ground truth rankings are available in synthetic data. Thus, we measure the top-5 sensitivity at k (Sk),the % of top-5 worst offenders in the predicted top-k, and the discounted cumulative gain (DCG) atk, a weighted sum of ground-truth relevance scores for the top-k predicted agents, across auditintensities k {1, . . . , 20}. For a K-agent dataset, we define the relevance score as K + 1 minus theground-truth rank (e.g., true rank 1 = relevance K, true rank 2 = relevance K 1, etc.). Concretely,for ri defined as the ith ranked agent in a predicted ranking, and rank() as the function returning theground-truth ordinal rank with respect to p, our ranking evaluation metrics are computed as follows:",
  "K rank(ri)log2(i + 1) .(6)": "Note that sensitivity is an all-or-nothing measure of audit quality given a fixed audit intensity k.However, DCG rewards higher predicted rankings for top-k worst offenders, regardless of the absoluteranking position. Furthermore, DCG weights decrease with predicted rank (Eq. 6), which prioritizes 2For ease of implementation, x(i) is generated conditioned on p(i). This remains consistent with thecausal DAG (, right), since the DAG factorizes as P(D | P, X)P(P | X)P(X), or equivalently,P(D | P, X)P(X | P)P(P), as in our data-generation process. # of agents audited 0.0 0.5 1.0 Top-5 sens. Top-5 sensitivity () # of agents audited DCG Disc. cumulative gain (DCG, )",
  "correctly ranking the worst offenders over correctly ranking agents unlikely to game. To summarizeaudit efficiency across k, we also report area under the top-5 sensitivity curve (AUSC) across k.3": "In contrast, ground truth gaming rankings are unavailable in the Medicare cohort. As an exploratoryanalysis, we compare an estimated gaming ranking to 104 state-level healthcare statistics fromthe 2003-2017 National Neighborhood Data Archive and 2018 Medicare Provider of Servicefiles relating to healthcare access and hospital information (e.g., ownership and size). We reportthe top five statistics most positively and negatively correlated with our predicted rankings in terms ofSpearman rank-correlation. State-level summary statistics used are enumerated in Appendix C.2. Models. To demonstrate the utility of causal effect estimation for gaming detection, we compare non-causal baselines to causal effect estimators. Non-causal approaches include a payout-only ranking(based on P(di = 1) per agent) and random ranking. We also compare to existing approachesin anomaly detection, which do not make causal assumptions, but assume that gamed decisionsare outlier-like. We test k-nearest neighbor outlier detection (KNN) , empirical-cumulative-distribution-based outlier detection (ECOD) , and deep isolation forests (DIF) . Thesemethods use (xi, di) as inputs to an anomaly score model. We use average within-agent anomalyscores as a ranking. We discuss other works in algorithmic anomaly/fraud detection in Appendix A. We implement the following causal effect estimators. PSM fits a propensity score model to matchpoints in one agents population to its nearest neighbor in the other agents population with respect topropensity score estimates . The S-learner trains one outcome prediction model for all agents,while the T-learner trains one model per agent . DragonNet jointly models the outcome (oneprediction head per agent) and propensity score to control for confounding . S+IPW fitsthe S-learner with estimated sample weights that reweight the observed distribution to resemble anunconfounded distribution (randomized treatment assignment) . The R-learner fits an outcomeand propensity model, then regresses the residuals on one another to obtain a final unconfoundedestimator . Hyperparameters for all baselines and causal effect estimators are in Appendix E. Implementation details. We use neural networks for all modeling (causal effect estimators + DIF).We use one-hot encoding for treatments (agent indicators). Matching across pairs of agents occurredwithout replacement (one-to-one), dropping unmatched individuals. We use generalizations ofIPW and R-learners to multiple treatments, namely permutation weighting and the structuredintervention network , respectively. We perform a 7:3 dataset train-test split, training all modelson the larger split. All rankings are computed on the test split. Early stopping is performed on a 20%validation split randomly sampled from the training set. Full modeling and training details, includingthe architecture and hyperparameters used, are in Appendix E.",
  "shows the top-5 sensitivity and DCG of rankings produced by causal vs. non-causal gaming detectionapproaches at high confounding (mean range: 0.9). Since the S-, T-learner, and DragonNet perform": "3AUSC is similar to the area under the ROC curve (AUROC) with the top-k worst offenders defined as thepositive class, except that the x-axis is the # of audits rather than false positive rate. Unlike AUROC, randomperformance is less than 0.5, but tends to 0.5 as the number of agents K (Appendix B.6). 0.00.20.40.60.81.0 Confounding level (range of means across agent-specific covariate distributions) 0.3 0.5 0.7 0.9 AUSC Area under the sensitivity curve (AUSC, ) vs. confounding strength RandomPayout-onlyECODDIFKNNDragonNetR-learnerS+IPW : Area under the sensitivity curve (AUSC) for causal vs. non-causal methods across levels ofconfounding, with error. As confounding increases, a payout-only ranking degrades. Anomalydetection performance does not vary across confounding strength, maintaining slightly better thanrandom rankings. Causal methods generally maintain higher mean AUSC than baselines acrossconfounding levels. : nave baselines. : anomaly detectors. : causal effect estimators. similarly (Appendix D.2), we show only DragonNet here. Since PSM underperforms due to challengeswith multi-treatment confounding control, we also defer results for PSM to Appendix D.2. Acrossaudit intensities, causal approaches outperform non-causal methods in terms of sensitivity (e.g., at 7audits, , right; S+IPW: 0.8600.135 vs. KNN: 0.5200.215) and DCG (S+IPW: 56.15.11 vs.KNN: 42.310.8).4 Trends are similar at other levels of confounding (Appendix D.1), though thegain in ranking performance of causal approaches over non-causal methods diminish at lower levelsof confounding (; S+IPW AUSC: 0.6140.096 vs. KNN: 0.6480.129, mean range 0.0). 0.00.20.40.60.81.0 Confounding strength (mean range) 0.6 0.8 AUSC S+IPW vs. KNN, AUSC () S+IPWKNN",
  ": AUSC of S+IPW (causal) vs.KNN (non-causal) across confoundingstrength with error. The advantageof S+IPW over KNN decreases as con-founding diminishes": "A payout-only approach yields worse than random rankingwith sufficient confounding between covariates and agentdecisions (payout-only AUSC: 0.2990.067 vs. random:0.4730.101, , mean range 1.0; e.g., if healthierpatients are enrolled in more gaming-prone plans). Anomalydetection methods (KNN, ECOD, DIF) are ill-suited for de-tecting gaming in dense regions of covariate space by design,while causal approaches would excel due to improved over-lap. If outliers are more likely to be manipulated (e.g., ifpopulations with lower ground-truth dp are more likely to begamed), an anomaly detection method would identify gam-ing in such points. Indeed, anomaly detectors empiricallyoutperform random ranking but lag causal methods. Wefurther discuss why anomaly detection methods underperform causal approaches in Appendix D.1. Trends in ranking performance vary across causal effect estimators. DragonNet and the R-learnerdegrade slightly as confounding increases (DragonNet AUSC: 0.7550.096 0.6960.083; R-learner: 0.7280.114 0.6350.111; mean range 0.0 1.0), likely due to slightly worse overlap.However, S+IPW improves as confounding increases (AUSC: 0.6140.096 0.8370.067; meanrange 0.0 1.0). This is likely since the oracle IPW weights deviate from a uniform weightingas confounding increases. In such settings, estimation error in IPW weights may be less likely toincorrectly up-weight points that an oracle propensity score would down-weight, and vice versa.While such error would bias the pointwise treatment effect estimate, for the purposes of ranking,causal effect estimators only need to correctly estimate the sign of the treatment effect. Thus, wehypothesize that our ranking may be less affected by such errors in the propensity score estimate. Weleave formal analyses of the properties of IPW with respect to the sign of causal effect estimates tofuture work. A sensitivity analysis of all causal approaches is in Appendix D.2. Takeaways.In a synthetic dataset, causal effect estimation approaches identified gaming moreefficiently than non-causal baselines across levels of confounding. The empirical results provideproof-of-concept for causal effect estimation as a gaming detection method.",
  "State-level healthcare system statisticSpearman corr.p-value": "% of OP PT/SP providers that are for-profit0.2940.036ratio of for-profit to non-profit hospitals0.2720.053% of hospice providers that are for-profit0.2320.101% providers that are ambulatory surgery centers0.2220.118% of hospitals that are for-profit0.2220.118 Upcoding is correlated with for-profit provider prevalence. Four of the top five features mostpositively associated with our predicted ranking reflect a greater state-level prevalence of for-profithealthcare providers. This matches the intuition that for-profit providers may game more aggressivelydue to stronger profit motives. Notably, the feature 2nd-most positively correlated with our rankings(ratio of for-profit to non-profit hospitals) is a suspected driver of upcoding in Medicare ,with the idea that competition from for-profit providers drives non-profit providers towards gaming.Note that many of the correlations are not statistically significant, and unmeasured factors such ashealthcare quality could explain differences in diagnosis coding, rather than gaming. Despite thelimitations, causal effect estimation shows promise as a practical approach to gaming detection.Takeaways. In an exploratory case study of gaming in U.S. Medicare, causal effect estimation yields",
  "Conclusion": "We propose a causally-motivated framework for ranking agents by gaming propensity in the contextof strategic adaptation. We show that the gaming parameter is only partially identifiable, but a rankingof a set of agents based on the gaming deterrence parameter is identifiable via causal inference. Wedemonstrate the utility of causal effect estimation for gaming detection on synthetic data and a casestudy of upcoding in Medicare. Limitations & broader impact. We assume agents always increase di with respect to ground truth,and that gaming explains all differences in agent behaviors, ignoring factors such as agent quality(e.g., quality of care). Utility-maximizing behavior and conditional exchangeability are strongassumptions, but are statistically unverifiable. Many works in game theory and causal inference sharethese limitations. We caution that policies informed by our framework could reinforce imbalancedpower dynamics (i.e., are individual citizens or more powerful entities gaming a model) viaextraneous or weaponized accusations of gaming, since not all entities have equal capacity to respondto such claims. In particular, gaming by individuals may potentially reflect structural inequitiesrather than inherently pathological behavior. To mitigate such risks, we suggest shadowing studies(decisions visible, but not acted upon) alongside existing audit mechanisms before adoption.",
  "Acknowledgements": "We thank (in alphabetical order) Amanda Kowalski, Daniel Shenfield, Donna Tjandra, Divya Shan-mugan, Dylan Zapzalka, Ezekiel Emanuel, Jung Min Lee, Meera Krishnamoorthy, Sarah Jabbour,and Serafina Kamp for helpful conversations and feedback. Special thanks to Dexiong Chen, MichaelIto, Shengpu Tang, Stephanie Shepard, and Winston Chen for their comments on drafts of this work,to Kathryn Ashbaugh, Michael Shafir, and the Advanced Research Computing team at the Universityof Michigan for assistance with data access and usage, and Matt Guido for coordinating our meetings.The authors are supported by a grant from Schmidt Futures (Award No. 70960). The funders hadno role in the study design, analysis of results, decision to publish, or preparation of the manuscript.This study was deemed exempt and not regulated by the University of Michigan institutional reviewboard (IRBMED; HUM00230364). Moritz Hardt, Nimrod Megiddo, Christos Papadimitriou, and Mary Wootters. Strategic classi-fication. In Proceedings of the 2016 ACM conference on innovations in theoretical computerscience, pages 111122, 2016. Gregory C Pope, John Kautter, Randall P Ellis, Arlene S Ash, John Z Ayanian, Lisa I Iezzoni,Melvin J Ingber, Jesse M Levy, and John Robst. Risk adjustment of medicare capitationpayments using the cms-hcc model. Health care financing review, 25(4):119, 2004.",
  "Elaine Silverman and Jonathan S Skinner. Are for-profit hospitals really different? medicareupcoding and market structure, 2001": "Sridhar Ramaswamy, Rajeev Rastogi, and Kyuseok Shim. Efficient algorithms for miningoutliers from large data sets. In Proceedings of the 2000 ACM SIGMOD international conferenceon Management of data, pages 427438, 2000. Daniel De Roux, Boris Perez, Andrs Moreno, Maria del Pilar Villamil, and Csar Figueroa.Tax fraud detection for under-reporting declarations using an unsupervised machine learningapproach. In Proceedings of the 24th ACM SIGKDD International Conference on KnowledgeDiscovery & Data Mining, pages 215222, 2018.",
  "Han Shao, Avrim Blum, and Omar Montasser. Strategic classification under unknown personal-ized manipulation. Advances in Neural Information Processing Systems, 36, 2024": "Jinshuo Dong, Aaron Roth, Zachary Schutzman, Bo Waggoner, and Zhiwei Steven Wu. Strategicclassification from revealed preferences. In Proceedings of the 2018 ACM Conference onEconomics and Computation, pages 5570, 2018. Yahav Bechavod, Katrina Ligett, Steven Wu, and Juba Ziani. Gaming helps! learning fromstrategic interactions in natural dynamics. In International Conference on Artificial Intelligenceand Statistics, pages 12341242. PMLR, 2021.",
  "Marian Tietz, Thomas J. Fan, Daniel Nouri, Benjamin Bossan, and skorch Developers. skorch:A scikit-learn compatible neural network library that wraps PyTorch, July 2017": "F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel,P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher,M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of MachineLearning Research, 12:28252830, 2011. Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, DavidCournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, Stfan J.van der Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, AndrewR. J. Nelson, Eric Jones, Robert Kern, Eric Larson, C J Carey, Ilhan Polat, Yu Feng, Eric W.Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A.Quintero, Charles R. Harris, Anne M. Archibald, Antnio H. Ribeiro, Fabian Pedregosa, Paulvan Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for ScientificComputing in Python. Nature Methods, 17:261272, 2020.",
  "Steven Diamond and Stephen Boyd. CVXPY: A Python-embedded modeling language forconvex optimization. Journal of Machine Learning Research, 17(83):15, 2016": "Suresh Bolusani, Mathieu Besanon, Ksenia Bestuzheva, Antonia Chmiela, Joo Dionsio, TimDonkiewicz, Jasper van Doornmalen, Leon Eifler, Mohammed Ghannam, Ambros Gleixner,Christoph Graczyk, Katrin Halbig, Ivo Hedtke, Alexander Hoen, Christopher Hojny, Rolfvan der Hulst, Dominik Kamp, Thorsten Koch, Kevin Kofler, Jurgen Lentz, Julian Manns, GioniMexi, Erik Mhmer, Marc E. Pfetsch, Franziska Schlsser, Felipe Serrano, Yuji Shinano, MarkTurner, Stefan Vigerske, Dieter Weninger, and Lixing Xu. The SCIP Optimization Suite 9.0.Technical report, Optimization Online, February 2024. Charles R. Harris, K. Jarrod Millman, Stfan J. van der Walt, Ralf Gommers, Pauli Virtanen,David Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J. Smith, Robert Kern,Matti Picus, Stephan Hoyer, Marten H. van Kerkwijk, Matthew Brett, Allan Haldane, Jaime Fer-nndez del Ro, Mark Wiebe, Pearu Peterson, Pierre Grard-Marchant, Kevin Sheppard, TylerReddy, Warren Weckesser, Hameer Abbasi, Christoph Gohlke, and Travis E. Oliphant. Arrayprogramming with NumPy. Nature, 585(7825):357362, September 2020.",
  "AAdditional related works": "Algorithmic anomaly/fraud detection.Our framework can be understood as an algorithmic audit-ing method for fraud/anomaly detection. Many approaches assume that ground-truth fraud/gaminglabels are available, reducing gaming detection to supervised learning ). We do notassume access to such labels. Unsupervised approaches for anomaly/fraud detection generally assume that anomalies are outliers with respect to some distribution. Mixture-modelingapproaches similarly assume that fraudulent/non-fraudulent decisions correspond to distributionslearnable under restrictive parametric assumptions . Instead of distributional assumptions, wemake behavioral assumptions about agents following strategic classification. Existing models ofagent behavior in the context of fraud detection make domain-specific assumptions about featurespredictive of fraud , agent utility (e.g., constant penalties ), or access to audit labels . Wegeneralize past work by making looser assumptions on agent utility and does not assume access toauxiliary information (e.g., audit labels), and circumvents the need for ground-truth labels by makingassumptions about gaming.",
  "desired bounds": "A causal DAG-based perspective on non-identifiability of p.In the context of the causal graphof strategic adaptation (, right), the non-identifiability of p is immediate. Since it is aproxy for the treatment effect of plan (p) on reported diagnosis rate (d), without accounting forx and d, conditional exchangeability (Assumption 6) does not hold. The utility-maximizationformulation of gaming borrowed from strategic classification provides additional information notencoded in the causal graph. Namely, we have that R(p(dp))/c(p(dp) dp) is monotonic in dpand dp [0, p(dp)], provided that R and c satisfy Assumptions 1- 4. These observations allow usto further bound the range of p.",
  "Otherwise, p [p, p] [(p; ), (p; )] is non-empty, and we cannot definitively ruleout or prove the existence of -gaming": "However, this algorithm may be of purely technical interest: it is doubtful that the requisite assump-tions are satisfied in our motivating setting (upcoding detection), especially Assumption 9, and itis similarly unclear whether such assumptions apply in other settings. In addition, if agents gamesimilarly (i.e., have similar values of p), it may be difficult to rule out/prove -gaming for the vastmajority of agents.",
  "Corollary. Define (p, p) as above. Then, given Assumptions 1- 8, (p, p) > 0 if and only ifp < p": "Proof. It is sufficient to show that (p, p) > 0 if and only if p(dp) < p(dp), from whichTheorem 1 yields the desired result. We can do so by showing (without loss of generality) thatE[E[di | p, xi]] is an unbiased estimate of p(dp) (and the case for p(dp) proceeds symmetrically).Since p(dp) is equivalent to P[di = 1 | p] (Eq. 2), the result is immediate:",
  "B.4Proposition 2": "Proposition. Let () be the oracle treatment effect function, and be some sample estimate of .Given Assumptions 1- 8, for any > 0, if sup |(p, p) (p, p)| , then, for all p, p such thatinfp,p |(p, p)| > , (p, p) > 0 if and only if p < p.",
  "Proof. Choose , , and some arbitrary as specified in the theorem statement. It suffices to showthat for all p, p such that infp,p |(p, p)| > , it holds that (p, p) > 0 if and only if (p, p) > 0": "( = ) By contradiction; suppose that (p, p) > 0 but (p, p) < 0. Then, by assumption,(p, p) < . But sup |(p, p) (p, p)| , so (p, p) < 0, yielding a contradiction. Thus,(p, p) > 0=(p, p) > 0. The reverse direction ( = ) proceeds identically. Thus, theproposition is true. Connections to robustness to non-rational actors.We assume throughout that agents behaverationally; i.e., always perfectly maximize the utility function given by Eq. 2. However, the rationalactor assumption may not always hold; e.g., if agents do not have the resources to carry out theresource maximizing action, or incorrectly estimate their costs. The former is particularly salientfor our motivating problem of Medicare upcoding, in which representatives of certain plans mayshedule a home visit with a healthcare professional to generate diagnosis codes a potentiallyresource-intensive process. Our ranking formulation can afford some robustness to violations of therational actor assumption:Remark 2 (Robustness to bounded rationality violations). Let p(dp) be the utility-maximizingaction, and let p(dp) be an agents observed action, such that |p(dp) p(dp)| < 1/2 for some1 > 0. Let (p, p) be some sample estimate of , fitted via (). Then:",
  "B.5Identifiability result": "For completeness, we show the derivation of the standard causal effect identifiability result for ourproblem setting (i.e., as in ). We note that this is a direct application of a known result in theliterature.Proposition. Given Assumptions 6- 8, E[di(p) | xi] = E[di | xi, p] and E[di(p) | xi] = E[di |xi, p].",
  "B.6What is the expected AUSC?": "For completeness, we also analyze the AUSC metric and provide a closed-form expression for itsexpected value under a random ranking. First, consider a population of K N agents, where we areinterested in top-k sensitivity for some k {1, . . . , K}. Suppose that we conduct m random audits,for some m {1, . . . , K}. The number of ground truth top-k agents that are in the set of m audited agents can be modeled as ahypergeometric random variable nm Hyp(k, K, m), which is a variable describing the number ofsuccesses observed by a random draw from a population of K objects with k success states in mdraws without replacement. By standard properties of the hypergeometric distribution, we know thatE[nk] = km/K. Thus, by linearity of expectation, the average (across audit intensities m) numberof agents identified by random auditing is given by",
  "C.1Fully synthetic data": "Overview.Our general fully synthetic data-generation pipeline is as follows. First, for each agent,we draw some value of p R2. We draw individual observations x(i) R2 for each agent from aGaussian with mean p and some fixed variance 2. Given the x(i), we simulate a ground-truthdecision d(i). Note that d(i) represents the ground truth decision rate. Then, for each agent p, wesimulate a gamed (i.e., strategically perturbed) version of d(i) via a version of the agent utilityfunction in Eq. 2. We now formally describe the data-generation process. Generating a population for each agent.To generate a population upon which each agentmakes decisions, we randomly draw a mean vector specifying a Gaussian distribution for each agent.Given P agents, the mean depends on p as follows:",
  "i=1p,(31)": "where a > 0 is a parameter controlling the range of agent-specific means, and b R is some constnatoffset. In other words, we log-transform the p values, then apply min-max scaling and a constantshift. This design allows us to control the level of confounding in the synthetic data by changing R.We use p to generate populations for each agent as described below. Concretely, we consider () [0.001, 0.003, 0.005, 0.007, 0.009, 0.01, 0.015, 0.02, 0.025, 0.03,0.035, 0.04, 0.045, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.2, 0.3] (20 agents), and set b = 1. In practice,this means that p [1, R 1]. We manually chose () values to set the difficulty of rankingagents by such that a payout-only approach would succeed under low confounding, but fail underhigh levels of confounding.",
  "where () is generated for each agent p(i) as described previously, w is a randomly generated positivevector, and bd R is some offset": "The p(i)s are assigned deterministically; i.e., we sequentially draw a fixed number of x(i) fromeach agent-specific distribution and concatenate the results. For realism, inspired by the healthinsurance setting, and the fact that diagnosis rates for most conditions are relatively low, we set bd tologit(0.05) E[wx] such that (i) is relatively low, where E[] denotes the sample mean.",
  "C.2Medicare cohort": "We select a 0.2% pseudo-random sample of all U.S. Medicare beneficiaries using the last charactersof encrypted beneficiary IDs for inclusion in the cohort. As covariates, we choose age, racial category,biological sex, and diagnosis code categories (defined using the 2018 version of the HierarchicalCondition Category [HCC] schema released by the Center for Medicare Services). This yields atotal of 97 features.",
  ": Mean top-5 sensitivity (left) and DCG (right) across # of agents audited at mean range 0.0,with error. : nave baseline. : anomaly detection method. : causal effect estimator": "as dual-eligible beneficiaries. Dual-eligibility refers to individuals simultaneously eligible for U.S.Medicare and U.S. Medicaid. While eligibility for Medicare is primarily based on age, eligibilityfor Medicaid is primarily based on disability status. Dual-eligible beneficiaries are excluded sincethe U.S. government uses a different payout model for dual-eligible vs. non-dual-eligible enrollees,potentially violating Assumption 1 (shared rewards), since agents may not be reacting to the samepayout model for all enrollees.",
  "Licensing.Our cohort is drawn from a 20% sample of all U.S. Medicare beneficiaries provided tothe authors under a data usage agreement with the Center for Medicare & Medicaid Services": "State-level healthcare statistics.We use a mix of raw and engineered features from the CMS(Center for Medicare & Medicaid Services) Provider of Service file (license: Public Use File) 5and the National Neighborhood Data Archive (NANDA; CC-BY 4.0) . In NANDA, data is alreadyaggregated at the state level (including the District of Columbia). We keep statistics pertaining toper-capita or per-sq. mi. healthcare provider density (50 features), filtering to providers with non-zerosales. From the CMS Provider of Service file, data is reported at the provider level. First, we filterout providers ineligible for Medicare participation, and providers that are no longer active. Wethen manually code ownership information following the provided data dictionaries (for-profit vs.non-profit vs. publicly owned). Next, we compute at the state level the prevalence of for-profit,non-profit, and publicly-owned providers of each type (as defined by CMS) by state, as well as theratio of for-profit to non-profit providers. Additionally, we extract the average per-hospital bed count,physician count, medical school affiliation rate, and compliance rate as certified by CMS, for a total of54 features. All aggregations for the Provider of Service file are unweighted (i.e., all hospitals/otherhealthcare facilities contribute equally). This yields a total of 104 state-level health indicators. We report the statistics derived from the provider of service file () and NANDA (), withsummary statistics across states. Summary statistics are computed excluding infinite and NaN values(i.e., ratios of 0/0 or 1/0). When computing correlations, we exclude states with NaN values for thatfeature (i.e., ratio of 0/0). Proportions of publicly-owned, non-profit, and for-profit providers do notsum to 1, because providers reporting unknown or other ownership are excluded. For furtherinformation on the feature definitions, consult the data dictionaries for the provider of service6 andNANDA7 files.",
  "Feature nameMeans.d": "ambulatory health care specialists per 1000 people4.4292.745ambulatory health care specialists per sq. mi.15.48016.928physicians per 1000 people1.7531.196physicians per sq. mi.6.4777.593physicians (excluding mental health) per 1000 people1.6561.090physicians (excluding mental health) per sq. mi.6.1377.195mental health physicians per 10000.0960.112mental health physicians per sq. mi.0.3400.412dentists per 1000 people0.6390.242dentists per sq. mi.2.5532.945all other health practitioners (besides the above) per 1000 people1.4831.039all other health practitioners (besides the above) per sq. mi.4.9495.264chiropractors per 1000 people0.2820.257chiropractors per sq. mi.0.8400.999optometrists per 1000 people0.1000.026optometrists per sq. mi.0.3070.240non-physician mental health practitioners per 10000.1760.390non-physician mental health practitioners per square0.6870.929P/O/ST and audiologists per 1000 people0.1330.054P/O/ST and audiologists per sq. mi.0.4740.657other health practitioners (besides the above) per 1000 people0.7920.610other health practitioners (besides the above) per sq. mi.2.6402.742outpatient care centers per 1000 people0.1810.133outpatient care centers per sq. mi.0.5560.516diagnostic labs per 1000 people0.0750.114diagnostic labs per sq. mi.0.2290.230home health services per 1000 people0.1730.112home health services per sq. mi.0.5170.441other ambulatory care services (besides the above) per 1000 people0.1250.327other ambulatory care services (besides the above) per sq. mi.0.1980.176all nursing and residential care facilities per 1000 people0.3340.324all nursing and residential care facilities per sq. mi.0.8360.650nursing care facilities per 1000 people0.2240.330nursing care facilities per sq. mi.0.4730.308residential IDD care facilities per 1000 people0.0260.014residential IDD care facilities per sq. mi.0.1020.174inpatient facilities for care of individuals with IDDs per 1000 people0.0120.010inpatient facilities for care of individuals with IDDs per sq. mi.0.0390.068inpatient facilities providing MHSA care per 1000 people0.0140.007inpatient facilities providing MHSA care per sq. mi.0.0630.109continuing care and assisted living facilities per 1000 people0.0230.011continuing care and assisted living per sq. mi.0.0700.062other residential care facilities (besides the above) per 1000 people0.0610.031other residential care facilities (besides the above) per sq. mi.0.1910.179pharmacies and drug stores per 1000 people0.3140.669pharmacies and drug stores per sq. mi.0.8531.336optical goods stores per 1000 people0.0760.025optical goods stores per sq. mi.0.2820.360misc. other health and personal care stores per 1000 people0.0540.014misc. other health and personal care stores per sq. mi.0.1490.090 : Features chosen for analysis derived from NANDA, with mean and standard deviationacross states. All chosen summary statistics are for providers with > $0 U.S. dollars in sales. S.d.:standard deviation. Sq. mi.: square mile. P/O/ST: physical, occupational, and speech therapists. IDD:intellectual and developmental disabilities. MHSA: mental health and substance abuse. # of agents audited 0.0 0.5 1.0 Top-5 sens. Top-5 sensitivity () # of agents audited DCG Discounted cumulative gain (DCG, ) Ranking performance across auditing thresholds RandomPayout-onlyECODDIFKNNDragonNetR-learnerS+IPW",
  ": Area under the sensitivity curve (AUSC) for all methods tested across levels of confounding(mean range R > 0.5). : nave baseline. : anomaly detection method. : causal effect estimator": "We summarize the main trends for non-causal approaches and defer discussion of causal methodsto the sensitivity analysis of all causal effect estimators. For convenience, we also plot the AUSCfor all approaches across levels of confounding in Figures 19 (R 0.5) and 20 (R > 0.5). Forreadability, in contrast to our other figures, causal effect estimators are marked with insteadof .",
  "The payout-only approach can degrade to worse-than-random ranking due to confounding.The random auditing method performs similarly across all levels of confounding, as expected": "However, as confounding increases, the payout-only ranking degrades toward random, then worsethan random. The latter can occur if confounding is so strong that the relationship between gamingand observed diagnosis rates flips; e.g., if (in the health insurance setting) very dishonest plans tendto serve relatively healthy populations compared to more gaming-averse plans. In the synthetic dataset, anomaly detection methods use the variance of the observed decision(V[di]) as a gaming signature.Most anomaly detection methods perform near-random, or slightlybetter than random. While this is due to the properties of the synthetic dataset, the results highlightpotentially interesting characteristics of anomaly detection methods for gaming detection. Recallthat the anomaly detection methods take covariates and agent decisions (xi, di) as input. V[xi] isidentical across agents by design, and all agents see the same number of observations. However,V[di] may vary across agents. Thus, KNN uses V[di] as a signal of gaming, which has utility underlow confounding, but is less useful as confounding increases. To see this, recall that di is a binary decision, and let pd = P(di = 1) for some agent. V[di] isproportional to pd (1 pd), and is concave in pd with maximizer pd = 0.5. Since P(di) is generallylow in simulation (i.e., 0.5), agents with higher observed P(di) rates will generally have higherV[di] as well, yielding a higher anomaly score. Under no confounding, these are precisely the agentsthat are gaming more. Indeed, KNN performs slightly better than random, but its advantage overrandom performance diminishes slightly with confounding as the utility of V[di] as a signaturefor gaming. However, even at low confounding, KNN and related anomaly detection methods areinherently unable to detect gaming in non-outlier points, which occur in denser regions of covariatespace. In these regions, causal methods enjoy an advantage over anomaly detection approaches dueto improved overlap (Assumption 8). Thus, KNN does not exceed the ranking performance of thebest causal methods. Note that this argument assumes that V[xi] is similar across agents, which holdsin the synthetic dataset. More advanced anomaly detection methods can also achieve slightly better than random performance(e.g., DIF), but since these approaches transform the covariate space in highly non-linear ways (e.g.,via random projections as in DIF, or via feature-wise CDFs as in ECOD), they may destroy outlierinformation useful for gaming detection. Ultimately, anomaly detection methods have inherentlylimited utility for gaming detection, since some gamed decisions may appear distributionally close toother gamed decisions.",
  ": Sensitivity analysis of all causal methods tested, mean range 1.0. : nave baseline. :causal effect estimator": "Note that, even absent confounding, causal approaches outperform the payout-only model. This isbecause causal approaches explicitly incorporate the covariates into modeling, while the payout-onlymodel directly uses the marginal outcome distribution. Incorporating covariates into regressionmodels for treatment effect estimation can decrease estimator variance (but is not guaranteed todo so), consistent with the empirical results. Propensity score matching (PSM) also performs poorly across all levels of confounding. Since thematching approaches conduct matching pairwise across observations seen by agent pairs, the causaleffect estimates are computed in a subset of similar observations across one pair of agents, but notthe subset of similar observations across all distributions agent observations. This suggests thatcontrolling for confounding simultaneously across all levels of treatment is potentially important forapplying causal effect estimators to gaming detection. Ultimately, matching approaches may notscale to large numbers of treatments, such as those expected in multi-agent strategic adaptation. Non-optimal matching approaches (e.g., greedy matching without replacement) are a potential workaround,but we leave the adaptation of matching methods to large numbers of treatments to future work. We note that, at low levels of confounding, the difference between the S-learner and T-learner may bedataset-dependent: empirically, S-learners often regularize causal effect estimates towards zero, whileT-learners thrive when causal effects are non-zero and heterogeneous , as in our syntheticdataset. Thus, per-agent modeling, as done by the T-learner and DragonNet, can better capture thecomplex treatment effects in our dataset. Furthermore, the R-learner and S+IPW both perform poorly at low levels of confounding, but improveat high levels of confounding. Since both the R-learner and S+IPW fit a nuisance propensity scoreestimator, this suggests that difficulties in propensity score estimation at low levels of confoundingcould potentially explain the observed trends. The underperformance of the R-learner may be surprising given its doubly-robust properties, but thehigh-dimensional generalization requires restrictive conditions for convergence. Formally, theR-learner fits four models m, g, h, e of the form",
  "d m(x) + g(x)(h(p) e(x)),(32)": "where m is fit independently, and g, h, e are fitted using alternating optimization. The final treatmenteffect estimate of swapping from agent p to p is given by g(x)h(p) g(x)h(p), but the oraclerepresentation of h() is unknown and must be fitted. Convergence of the nuisance parameter estimateof e(x) to the oracle value of h(p) is necessary for convergence of the overall treatment effectestimate.",
  "E.1Model architectures": "All approaches that fit a model are built based on a fully-connected neural network with two hiddenlayers and 300 neurons per layer plus ReLU activations. The output of the neural network eitherhas size two with a softmax non-linearity (for classification; i.e., predicting agent decisions), or noactivation and a pre-specified output size (i.e., for generating feature maps in the high-dimensionalR-learner).",
  "We describe approach-specific modifications to the architectures as follows:": "S+IPW.The estimated weights are used as a sample weight when training the S-learner. Atinference time, weights are also computed for test examples based on the propensity model fitted onthe training set to take an inverse propensity-weighted average of the S-learner estimates. DragonNet.We changed the propensity prediction head from a binary classification (as in theoriginal paper ) to a multi-class classification head, since there are multiple treatments in oursetting. Furthermore, we introduce a new outcome modeling head per agent. Targeted regularizationis omitted due to the multi-treatment setup. R-Learner.Feature maps for all nuisance parameters have dimensionality 10. The generalized R-learner uses alternating optimization to fit some nuisance parameters, where two models representing aproduct decomposition of the response function are updated K times for every update of a propensityfeature model. We set K = 10; we defer to , page 6 for more details about the training procedurefor the generalized R-learner, from which we designed our implementation.",
  "E.2Anomaly detection hyperparameters": "For KNN, we keep the neighborhood size at 5, the default value. DIF uses neural network-basedrandom projections to compute an anomaly score. Thus, we use the same architecture for DIF asused for causal approaches, with an ensemble of 50 representations. Each representation is used asinput into an isolation forest of size 6 .8 ECOD does not take hyperparameters .",
  "E.3Dataset splits": "We use a seeded random development-test split (7:3) for all datasets. The development split isreserved for all model fitting, while all causal effect estimates are reported on the test split. Thedevelopment split is further randomly split into a training and a validation set. All model selectiontechniques (e.g., early stopping) are performed with respect to evaluation metrics on the validationset.",
  "Optimizer: SGD with learning rate 102 and weight decay 103": "8An isolation forest is a separate anomaly detection method, in which the anomaly score is related to thenumber of random splits with respect to a single randomly-selected covariate (i.e., a threshold of the formxi r) needed to isolate a point. This method assumes that outliers are easier to isolate, and require fewersplits. Multiple random-splitting routines (isolation trees) are ensembled to form an isolation forest.",
  "F.1Software": "All code was written in Python 3.10.4 (license: PSF). All non-causal anomaly detection approacheswere implemented using PyOD (license: BSD 2-clause) . All neural networks were implementedin PyTorch 2.2.0 (license: Custom BSD-style9) , using Skorch 0.15.0 (license: BSD 3-clause) as a wrapper. Metrics were computed using both Scikit-Learn 1.3.2 (license: BSD3-clause) and Scipy 1.11.4 (license: BSD 3-clause) . For the fully synthetic data generationprocess, CVXPY 1.4.2 (license: Apache 2.0) was used to solve each agents utility maximizationproblem, and used in tandem with SCIP 9.0 (pyscipopt 5.0.0; license: Apache 2.0) for the matchingapproaches (formulated as mixed-integer programs) . Numpy 1.22.3 (license: BSD-style) 10and Pandas 2.0.3 (license: BSD 3-clause) were used for data manipulation. Matplotlib 3.8.2(empirical results; license: PSF-style)11 and Adobe Illustrator 2023 (overview figures; license:commercial, Named User Licensing12) were used for figure generation. For the Medicare cohorts,we generated HCC (Hierarchical Condition Categories; used by the Center for Medicare Services)codes from raw diagnosis codes reported in claims data via HCCPy 0.1.9 (license: Apache 2.0)13. Other dependencies include tqdm 4.66.2 for rendering progress bars (license: MPL 2.0 and MIT),gitpython 3.1.43 for bookkeeping (license: BSD 3-clause), pandarallel 1.6.5 for parallel dataprocessing (license: BSD 3-clause), and ruamel 0.18.6 (license: MIT) for configuration file manage-ment. All software excepting Adobe Illustrator is open-source and free for use.",
  "F.2Hardware": "All experiments were run on either one Titan V or V100 GPU using 12.9GB of RAM as managedvia a Slurm job submission system. Computing nodes had two 2.10GHz Intel Broadwell (XeonE5-2620V4) processors each (16 cores total). Execution time was limited to six hours per run, but alltraining runs (one model type on 10 datasets) lasted under one hour due to the relatively small size ofthe architectures and datasets under consideration."
}