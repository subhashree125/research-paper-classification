{
  "Abstract": "Originally formalized with symbolic representations, syntactic trees may also beeffectively represented in the activations of large language models (LLMs). Indeed,a Structural Probe can find a subspace of neural activations, where syntactically-related words are relatively close to one-another. However, this syntactic coderemains incomplete: the distance between the Structural Probe word embeddingscan represent the existence but not the type and direction of syntactic relations. Here,we hypothesize that syntactic relations are, in fact, coded by the relative directionbetween nearby embeddings. To test this hypothesis, we introduce a Polar Probetrained to read syntactic relations from both the distance and the direction betweenword embeddings. Our approach reveals three main findings. First, our Polar Probesuccessfully recovers the type and direction of syntactic relations, and substantiallyoutperforms the Structural Probe by nearly two folds. Second, we confirm that thispolar coordinate system exists in a low-dimensional subspace of the intermediatelayers of many LLMs and becomes increasingly precise in the latest frontier models.Third, we demonstrate with a new benchmark that similar syntactic relations arecoded similarly across the nested levels of syntactic trees. Overall, this work showsthat LLMs spontaneously learn a geometry of neural activations that explicitlyrepresents the main symbolic structures of linguistic theory.",
  "Introduction": "Human languages have long been proposed to systematically follow tree-like structures (Chomsky,1957; Tesnire, 1953). In a sentence, words that are far apart can be syntactically linked. For example\"cats\" is the subject of \"chase\" in the sentence The cats in cities chase the mice. In dependencygrammar, the edges of such trees are directed and labelled to indicate the type of syntactic relationbetween words (cats is the subject of chase, B). Despite their conceptual soundness and alignment with human behavior (Robins, 2013), syntactic treeshave long been the crux of a core challenge in cognitive science (Smolensky, 1987): trees are symbolicrepresentations, which can superficially appear incompatible with the vectorial representations ofneural networks. This opposition between symbols and vectors has been a major challenge to the",
  "unification of linguistic theories on the one hand, and neuroscience and connectionist AI on the otherhand": "Recently, Hewitt and Manning (Hewitt and Manning, 2019) proposed an important concept for thisissue, by suggesting that the existence of syntactic link between two words may be represented bythe distance between their corresponding embeddings. Specifically, their Structural Probe consistsin finding a subspace of contextualized word embeddings such that the squared euclidean distancebetween words represents their distance in the dependency tree. They showed that the StructuralProbe is most powerful in the intermediate layers of language models: these layers contain a subspacewhere syntactically-related words are closer together. This Structural Probe, however, can only reveal one aspect of dependency trees: namely, the existenceof syntactic relations, between word pairs. However, whether and how the direction and the type ofsyntactic relations are represented in language models remains unknown. Here, we hypothesize that syntactic relations are represented by a polar coordinate system, wherethe existence and type of syntactic relations are represented by distances and direction, respectively(). To test this hypothesis, we introduce a Polar Probe: a linear transformation trained suchthat pairs of words linked by the same dependency type are collinear, while remaining orthogonal todifferent dependency types.",
  "case": ": Dependency trees hypothesized in linguistics and in neural networks. A. According to thedependency grammar framework, the sentences can be described as linear sequences of words connected by anacyclic graph. B. More precisely, such acyclic graph is both labeled and directed, where each edge has a direction,representing the hierarchy of the syntactic relation, and a label, representing the type of syntactic relation. C.The Structural Probe (Hewitt and Manning, 2019) finds a a linear transform (gray plane) of the language modelsactivations (here simplified as a 3D space), such that the distance between word embeddings is predicted bytheir dependency tree. In the Structural Probe subspace, however, it is not possible to distinguish whether\"The cat chases the mouse\" or \"The mouse chases the cat.\" D. Our Polar Probe finds a linear transformationwhere the angle between syntactically-related word additionally represents the type and direction of thesesyntactic relations, and the distance codes its presence. The colored arrows indicate orthogonal directions in thePolar-Probe subspace.",
  "u : wi, wj {wi, wj} indicate the head word (and thus direction) of the syntactic relationbetween directly-connected words": "Language models based on neural networks represent sentences as sequences of word vectors(a.k.a word embeddings)1. As these embeddings propagate through the layers of the network,they incorporate information about the sentence they belong within, becoming contextualized wordembeddings. Let hi Rk be the ith contextualized word embedding of wi obtained at the output of layer of aneural network. For simplicity, we will drop the layer index in what follows, keeping in mind thatdifferent layers yield different representations.",
  "d(hi, hj) d(wi, wj),t(hi, hj) t(wi, wj),u(hi, hj) u(wi, wj),": "While d, t and u could be any complex functions, the goal of the present work is to identify a simple,interpretable code of how syntactic trees may be represented in vectorial systems. Following theclassic definition of a representation as a linearly readable information, we focus on linear operators(DiCarlo and Cox, 2007; Kriegeskorte and Bandettini, 2007; King and Dehaene, 2014).",
  "Structural Probe": "In (Hewitt and Manning, 2019), the authors propose to solve d as a distance in a subspace of thecontextualized word embeddings. Their Structural Probe is a linear transform, BS : Rk Rk,which projects word embeddings such that their relative distances correspond to their distances in thesyntactic tree. Formally, if si,j Rk is the (directed) edge embedding between words wi and wj:",
  "(wi,wj)S|d(wi, wj) d(wi, wj)|(3)": "Following the development of the Structural Probe, squared Euclidean distances between probedword embeddings are not designed to represent both the presence of dependency relations and theirtypes and directions simultaneously. (Hewitt and Manning, 2019) thus only propose a representationalsystem to solve d, but not t and u. Whether and how the directed and labeled syntactic tree is encodedin neural networks, thus remains unknown. 1Sequences are built from tokens, which sometimes correspond to subwords. When this is the case, onecan simply average the subword embeddings to obtain word embeddings (Hewitt and Manning, 2019).2See (Chen et al., 2021) for an explanation of how hyperbolic spaces are best measured through squareEuclidean distances.",
  "Here, we hypothesize that neural networks use the orientation of the relations formed by connectedword pairs to represent the type and direction of their syntactic dependency": "To test this hypothesis, we first introduce an Angular Probe consisting of a linear transformBA : Rk Rk. By abuse of notation, we denote as t(si,j) t(wi, wj) the syntactic type of thecorresponding edge. For the Structural Probe, the function d to be recovered is defined on all pairsof words; here the function t to be recovered is only defined on pairs of syntactically linked words,hence we only consider word pairs (wi, wj) which are indeed syntactically linked. We use contrastive learning to align relations of the same type and ensure that different types arepointing to different directions. This approach is designed such that a linear readout could explicitlycategorize dependency types. Specifically, the objective for the Angular Probe is to ensure that given two edge embeddings s and sof syntactic types c = t(s) and c = t(s), the linear transforms BAs and BAs are colinear if c = c,and orthogonal if c = c.",
  "Data": "Natural dataset.We consider natural sentences extracted from the English Web Treebank dataset(Silveira et al., 2014)3. This corpus contains 254,820 words from 16,622 sentences, sourced froma diverse array of web media genres, including weblogs, newsgroups, emails or reviews. All thesentences in the dataset are manually annotated according to the Universal Dependencies frame-work (Nivre et al., 2017), where each word is a node, each syntactic link is a labelled directed edge,and the syntactic tree is acyclic. Sentences containing email or web addresses are excluded from the dataset. Such filter removes noisysentences not interesting from a syntactic point of view. We follow the default splitting provided bythe English Web TreeBank resulting in a total of 11827 sentences for training, 1851 for validation,and 1869 for testing. Controlled dataset.To precisely evaluate our approach on well-controlled sentences, we designeda dataset, extending previous work (Lakretz et al., 2021b), comprising 100 sentences built witha long-nested structure (e.g., The book that the boy besides the car reads fascinates my teacher).In these sentences, a subordinate clause branches off the main phrase. There is also a constituentbesides the car that forms a branch inside the subordinate clause, adding further complexity to thesyntactic structure. This controlled dataset is designed to clarify how the Polar Probe reconstructs the syntactic tree incomplex conditions, but conditions well theorized in linguistics. In particular, we can create severalvariations of these long-nested sentences.",
  "Training": "We train the Polar Probe on the neural activations of Mistral-7B-v0.1 and Llama-2-7b-hf (Touvronet al., 2023; Jiang et al., 2023), in response to sentences of the Natural Dataset described above.Both models are Auto-Regressive Language Models, they aim to identify future words from inputsentences. Furthermore, these models read all the words simultaneously, building representationswhich depend on the whole sequence of tokens. We trained the Polar Probes with gradient descent, using the Adam optimizer (Kingma and Ba, 2014)with a learning rate of 0.005, and a batch size of 200 sentences. The duration of the training is30 epochs, we perform model selection using the validation set. Hyperparameter is set to 10.0,ensuring an optimal balance between the Angular and Structural objective.",
  "obj": ": The Polar Probe reliably identifies dependency types. A. PCA visualization of edges linearly readby the Polar Probe. The color of each edge corresponds to one of three different dependency types (nsubj,obj, det): the linear readouts point in systematic directions. B. AUC and Balanced Accuracy metrics obtainedfor dependency type classification. C. Pairwise cosine similarity (0=orthogonal, 1=collinear) matrices obtainedwithout a probe (left) the Structural Probe (middle) and the Polar Probe (right).",
  "Evaluation": "We evaluate each probe either on its ability to faithfully represent (i) the unlabelled and undirecteddependency tree (structure), (ii) the type and direction of dependencies and (iii) both of theseelements. Dependency structure.Following (Hewitt and Manning, 2019), we evaluate whether the probesaccurately predicts the existence of each syntactic relation by using the Undirected UnlabeledAttachment Score (UUAS). UUAS quantifies the proportion of dependency relations (directly con-nected words) in the dependency tree that are correctly identified by the probe, irrespective of theirdependency types and direction.",
  "Dependency type.To evaluate the accuracy of the predicted dependency types, we use three distinctmetrics: Area Under the Curve (AUC), Dependency Type Accuracy and Dependency Type BalancedAccuracy": "For AUC, we compute the cosine similarity between each edge si and all other edges sj, that areeither from the same dependency type, or not. This procedure ends with a distance vector x Rm ofm edge pairs and a binary vector of y 1m. We can finally input these two vectors into scikit-learnsroc_auc_score (Pedregosa et al., 2011). For Accuracy and Balanced Accuracy, we classify dependency types by comparing relations toprototypical relations. For this, from the training set, we pool 10,000 relations and define a prototypesk for each dependency type k, by computing the centroid of all relations belonging to the same typek. Then, we predict dependency types from the cosine similarity between each relation si and eachprototype Vc, using scikit-learns KNeighborsClassifier.(Pedregosa et al., 2011). Finally, weuse scikit-learn accuracy_score or balanced_accuracy_score to limit the effect of imbalancebetween dependency types. Combined dependency type and structure.Finally, to provide a metric which evaluates bothdependency structures and dependency types, we compute the Labeled Attachment Score (LAS).LAS is defined as the proportion of correctly predicted labeled and directed edges in a sentence. Baselines.We compare the Polar Probe to a variety of baselines: (i) Structural Probe, (ii) PolarProbe Random LLM: a Polar Probe trained on top of a random language model and (iii) No Probe:the raw activations of the Language Model without any transformation.",
  "Results": "Reliable coding of dependency types.We first analyze the Polar Probe on the 16th layer of Llama-2-7b-hf, Mistral-7B-v0.1 and BERT-large (Touvron et al., 2023; Jiang et al., 2023; Devlin et al., 2019)on the English Web Treebank (EWT) sentences (Silveira et al., 2014) annotated with dependencytrees. We evaluate, on an independent test set with 10000 relations, whether pairs of words linked bysimilar syntactic relations point towards similar orientations in the probes representational space(). .A shows a Principal Component Analysis (PCA) projection of the dependency relationembeddings from the test set, once linearly read by the Polar Probe. For readability, we restrictourselves to three of the most common types of dependencies in the dataset. As expected, the threetypes of dependency consistently point in different directions. We then compute the cosine similarity between all pairs of edge embeddings probed with the PolarProbe (.C (right)), indeed showing that relations of the same types are collinear, while relationsof different types are orthogonal. This is much clearer for the Polar Probe than for baselines (.C).",
  "Comparison with baselines.In .B, we summarize with AUC and Balanced Accuracy theextent to which the orientations of these edge embeddings reliably represent the syntactic types": "On average across dependency types, the Polar Probe reaches a AUC score of 95%, well above theStructural Probe (AUC=74%), the No Probe (AUC=80%) and the Polar Probe trained on a RandomlyInitialized LLM (AUC=50%). The same relative results across probes are conserved for the BalancedAccuracy score. Importantly, these results confirm that the Polar Probe outperforms the StructuralProbe in predicting dependency types from the probe embeddings. The latter is therefore somethingnot emergent in the Structural Probe. Unexpectedly, No Probe predicts syntactic types well above chance, and significantly better thanthe Structural. This hints to the fact that syntactic types are already represented in the raw activations.A likely explanation for this is that words belonging to the same part-of-speech (such as verbs, nouns)are clustered in the embedding space, thus partially guiding the inference of syntactic dependencies. Moreover, training a Polar Probe on a random initialization of a language model does not accomplishthe contrastive objective better than chance. Resulting in near chance-level Balanced Accuracy andAUC scores. This confirms that the linear probe needs a rich underlying representation space to work,and cannot learn to cluster the different syntactic types on its own. Layer-wise analysis.To evaluate how the Angular and Structural performance interact in thePolar Probe, and whether the mechanism generalizes to both Mistral-7B-v0.1, Llama-2-7b-hf andBERT-large, we evaluate the layers of the three models on Labeled Attachment Score (LAS) ().(See Supplementary for BERT-large and Mistral-7B-v0.1) Interestingly, BERT-large, Mistral-7B-v0.1 and Llama-2-7b-hf all peak at layer 16, which is the samelayer reported in (Hewitt and Manning, 2019). At layer 16, the models achieve a LAS on the test setof 70.2, 60.6 and 62.9 respectively. As recently reported in (Eisape et al., 2022) for the StructuralProbe, these results suggest that the Polar Probe also works best with Masked Language Models.The Polar Probe, despite its conceptual simplicity matches performance with a more intricate andmodular labeled probe (Mller-Eberstein et al., 2022). Structural evaluation.The Polar Probe is optimized with both a Structural and an Angularobjective. This means that the optimization of such probe might affect the original performance ofthe Structural Probe. To verify that the gap in performance, we compute the UUAS between the predicted tree and the annotated tree for both the Structural and Polar probe (). The resultsconfirm that the Polar Probe preserves (but does not improve) the syntactic distances between thelinear readout of the probed word embeddings.",
  "Layer": "Existence and Type Score ABC : The Polar Probe outperforms the Structural Probe at identifying labeled and directed dependen-cies. A. For dependency existence, the Polar Probe matches the UUAS performance of the Structural probe,peaking at layer 16. B. For dependency type, the Polar Probe outperforms in Label Accuracy the Structural(LAS) Probe by around 80% accross the different layers of Llama-2-7b-hf. C. For both dependency existenceand type, the Polar Probe outperforms in LAS the Structural Probe by around 90% accross the different layers ofLlama-2-7b-hf. Dimensionality analysis.How many dimensions are necessary to successfully represent the fullsyntactic tree with the proposed polar coordinate system? To address this question, we varied k,namely the dimensionality of the space of the Polar Probe (). Analogously to the rank analysisof Structural Probe (Hewitt and Manning, 2019), we observe a peak around k = 128. Contraryto theoretical predictions (Smolensky, 1987), these results suggest that the space representing thecomplete syntactic tree needs not be unreasonably large. For dependency types, this phenomenoncould be relatively intuitive, as the unit circle (i.e. only 2 dimensions) can easily separate manydifferent dependency types, such that a weakly non-linear readout would isolate these categories.",
  "Polar Probe Dimension": "Existence and Type Score ABC : The optimal dimensionality for the Polar Probe is an order of magnitude small than modelslayer size. Polar Probe performance as a function of dimensionality, measured by A. UUAS, B. DependencyType Accuracy and C. LAS for Llama-2-7b-hf as a function of k, the dimensionality of the probes space. Theoptimal dimensionality for the Polar Probe is 128, achieving the highest LAS. Controlled sentences.Natural sentences are highly variable in structure and content. To verifymore precisely the behavior of the Polar Probe, we evaluate it on the Controlled Dataset and itsdifferent sentence levels: Short, Relative Clause and Long-Nested. First, we observe that in the space of the Polar Probe, the representation of dependency trees appearsto be consistent across sentences length and substructures. For example, as shown in , thePCA visualization of Polar Probe word embeddings belonging to the main phrase is virtually identicalwhether it is attached to a relative clause or to a long-nested structure. This invariance supports thenotion that dependency trees are represented by a systematic coordinate system that can be recoveredwith the Polar Probe.",
  "det nsubj obj nmod acl case": ": Visualization of the dependency tree uncovered by the Polar Probe on a set of sentences withincreasingly complex hierarchical structures. A. We display a PCA visualization of the distributions of wordembeddings (once linearly read out by the Polar Probe), for the different syntactic levels in the ControlledDataset. Each individual distribution corresponds to a specific role of the word in the sentence. The centroidsare linked with colored lines, displaying the true syntactic tree of the corresponding sentence. B. Most frequentsyntactic tree prediction by the Polar Probe for the different syntactic levels. The relations between words arecolor coded according to the type of syntactic dependency. The incorrectly predicted relations are representedwith dashed arrows. That is, either a dependency relation existence (no arrow), or a dependency type (witharrow) was erroneously identified.",
  "Discussion": "Summary.We show that within the activation space of language models, there exists a subspace,where syntactic trees are fully represented by a polar coordinate system. There, the presence andtype of a syntactic relation between two words is represented by their distance and relative direction,respectively. Importantly, the Polar Probe preserves the structural properties of the Structural Probe(Hewitt and Manning, 2019), but better represents the type of syntactic relations. Limitations.The present work presents four main limitations. First, we only investigate the Englishlanguage. Yet, human languages use different grammatical rules, and may, consequently, be structuredaccording to different types of trees. Interestingly, as language models become increasingly able toprocess a wide spectrum of languages (Costa-juss et al., 2022), the present framework opens theexciting possibility to explore universal (or divergent) grammatical representations in artificial neuralnetworks, following (Mller-Eberstein et al., 2022; Chi et al., 2020). Second, syntactic structures are not necessarily restricted to the description of relations betweenwords. In particular, morphology predicts that words themselves may be represented as trees ofmorphemes. Consequently, whether and how the present framework generalizes to the different scalesof linguistic structures remains to be further investigated.",
  "Third, like the Structural Probe, the Polar Probe is based on a supervised task: we optimize a lineartransformation that maximally retrieves a known syntactic structure from the neural activations": "Developing an unsupervised probe would be important to help discovers unsuspected syntacticstructures. In addition, we here focused on dependency structures. Yet, other formalisms, based onphrase structures (Chomsky, 1957; Joshi and Schabes, 1997; Cinque and Rizzi, 2009; Chomsky,2014) could offer alternative trees, and could be equally probed through the present framework. Thisapproach could thus offer the possibility of experimentally testing which of these linguistic theoriesbest account for the representations of human languages in neural networks. Finally, we assume that syntactic trees can be best read out using Euclidean probes. However,alternative assumptions, such as hyperbolic representations, have been a fruitful tool to interpret deeplearning models representations in both text and image modalities (Dhingra et al., 2018; Nickel andKiela, 2017; Desai et al., 2023). We speculate that this direction could provide a valuable avenue forextending the current work.",
  "Related work": "Syntax in artificial neural networks.Overall, this study complements previous research on syntaxin artificial neural networks. Originally, (Smolensky, 1987) demonstrated that vectorial systems could,in principle, represent symbolic structures with tensor products but did not provide an empiricaldemonstration that neural networks did, in fact, demonstrate this property. More recently, languagemodels were tested on their capacity to process syntactic structures by evaluating their behavioron grammatical and ungrammatical sentences (Lakretz et al., 2020, 2021a; Hewitt and Manning,2019; Hale et al., 2022; Evanson et al., 2023; Linzen et al., 2016). Finally, several groups exploredhow this capacity was instantiated in the neural activations (Huang et al., 2017; Palangi et al., 2017;Soulos et al., 2019; Lakretz et al., 2019), culminating in (Hewitt and Manning, 2019)s StructuralProbe. Since the discovery of the Structural Probe different adaptations have been developed, notablyincluding hyperbolic (Chen et al., 2021), orthoghonal (Limisiewicz and Marecek, 2021), nonlinear(Eisape et al., 2022; White et al., 2021) variants. The present work completes this long effort byshowing how an interpretable syntactic code based on both distances and orientations spontaneouslyemerges in language models. Syntax in biological neural networks.This link between linguistics and artificial neural networksholds significant potential for neuroscience. In particular, until the latest rise of large languagemodels, many experimental neuroimaging studies aimed to identify the neural bases of syntax in thehuman brain (Hale et al., 2022). For example, (Pallier et al., 2011) showed with functional MagneticResonance Imaging (fMRI) that several regions of the superior temporal lobe and prefrontal cortexresponded proportionally to constituent size. Critically, language models are now becoming standardbases to predict and explain the brain responses to natural language processing: The activations ofthese artificial neural networks have indeed been shown to linearly map onto fMRI, intracranial andMEG recordings of the brain in responses to the same words and sentences (Jain and Huth, 2018;Caucheteux and King, 2022; Reddy and Wehbe, 2021; Caucheteux et al., 2021; Pasquiou et al., 2022,2023). However, this mapping remains difficult to interpret, and the neural code for syntax in thebrain remains a major unknown. The present work thus provides a testable hypothesis to understandhow syntactic trees may be explicitly represented in the brain. Broader impact.Combined with the works outlined above, our results open the exciting possibilitythat the polar coordinate system may, in fact, explain how syntax is encoded in the human brain. Criti-cally, this framework may generalize beyond syntactic tree structures, and apply to any compositionalproblem, including compositional semantics, object-feature binding in vision, and representation ofknowledge graphs. Above all, while many have long opposed symbolic and connectionist formalisms,this work contributes to show how these two systems of representations may be largely compatiblewith one another, as long predicted (Smolensky, 1990; Smolensky et al., 2022). This reconciliationthus holds great promises to understand the brain mechanisms of language and composition.",
  "Cinque, G. and Rizzi, L. (2009). The cartography of syntactic structures": "Costa-juss, M. R., Cross, J., elebi, O., Elbayad, M., Heafield, K., Heffernan, K., Kalbassi, E., Lam,J., Licht, D., Maillard, J., et al. (2022). No language left behind: Scaling human-centered machinetranslation. arXiv preprint arXiv:2207.04672. Desai, K., Nickel, M., Rajpurohit, T., Johnson, J., and Vedantam, S. R. (2023). Hyperbolic image-textrepresentations. In Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S., and Scarlett, J.,editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 ofProceedings of Machine Learning Research, pages 76947731. PMLR.",
  "Lakretz, Y., Kruszewski, G., Desbordes, T., Hupkes, D., Dehaene, S., and Baroni, M. (2019). Theemergence of number and syntax units in. pages 1120. Association for Computational Linguistics": "Limisiewicz, T. and Marecek, D. (2021). Introducing orthogonal constraint in structural probes. InZong, C., Xia, F., Li, W., and Navigli, R., editors, Proceedings of the 59th Annual Meeting ofthe Association for Computational Linguistics and the 11th International Joint Conference onNatural Language Processing (Volume 1: Long Papers), pages 428442, Online. Association forComputational Linguistics.",
  "Linzen, T., Dupoux, E., and Goldberg, Y. (2016). Assessing the ability of lstms to learn syntax-sensitive dependencies. Transactions of the Association for Computational Linguistics, 4:521535": "Mller-Eberstein, M., van der Goot, R., and Plank, B. (2022). Probing for labeled dependency trees.In Muresan, S., Nakov, P., and Villavicencio, A., editors, Proceedings of the 60th Annual Meetingof the Association for Computational Linguistics (Volume 1: Long Papers), pages 77117726,Dublin, Ireland. Association for Computational Linguistics. Nickel, M. and Kiela, D. (2017). Poincar embeddings for learning hierarchical representations. InGuyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R.,editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.",
  "Pallier, C., Devauchelle, A.-D., and Dehaene, S. (2011). Cortical representation of the constituentstructure of sentences. Proceedings of the National Academy of Sciences, 108:25222527": "Pasquiou, A., Lakretz, Y., Hale, J., Thirion, B., and Pallier, C. (2022). Neural language models arenot born equal to fit brain data, but training helps. In ICML 2022-39th International Conferenceon Machine Learning, page 18. Pasquiou, A., Lakretz, Y., Thirion, B., and Pallier, C. (2023). Information-restricted neural languagemodels reveal different brain regions sensitivity to semantics, syntax, and context. Neurobiologyof Language, 4(4):611636. Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M.,Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher,M., Perrot, M., and Duchesnay, E. (2011). Scikit-learn: Machine learning in Python. Journal ofMachine Learning Research, 12:28252830.",
  "Tesnire, L. (1953). Esquisse dune syntaxe structurale. Klincksieck": "Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S.,Bhargava, P., Bhosale, S., Bikel, D., Blecher, L., Ferrer, C. C., Chen, M., Cucurull, G., Esiobu, D.,Fernandes, J., Fu, J., Fu, W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A., Hosseini,S., Hou, R., Inan, H., Kardas, M., Kerkez, V., Khabsa, M., Kloumann, I., Korenev, A., Koura, P. S.,Lachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y., Mao, Y., Martinet, X., Mihaylov, T.,Mishra, P., Molybog, I., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Saladi, K., Schelten, A.,Silva, R., Smith, E. M., Subramanian, R., Tan, X. E., Tang, B., Taylor, R., Williams, A., Kuan,J. X., Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur, M., Narang, S., Rodriguez, A.,Stojnic, R., Edunov, S., and Scialom, T. (2023). Llama 2: Open foundation and fine-tuned chatmodels. White, J. C., Pimentel, T., Saphra, N., and Cotterell, R. (2021). A non-linear structural probe.In Toutanova, K., Rumshisky, A., Zettlemoyer, L., Hakkani-Tur, D., Beltagy, I., Bethard, S.,Cotterell, R., Chakraborty, T., and Zhou, Y., editors, Proceedings of the 2021 Conference of theNorth American Chapter of the Association for Computational Linguistics: Human LanguageTechnologies, pages 132138, Online. Association for Computational Linguistics. Parameter Count (Million parameters) Structural and Label Score gpt2-large gpt2-mediumgpt2 gpt2-xl pythia-70m pythia-160m pythia-410m pythia-1b pythia-1.4b pythia-2.8b bert-large bert-base mistral llama-2 llama-3",
  ": Polar Probe performance on the EN-EWT dataset for Language Models with different families andsizes": "Sentence length Structural and Label Score Ranked model size Sentence depth Structural and Label Score Ranked model size : Comparative analysis of Polar Probe performance on the EN-EWT dataset as a function of sentencelength (left) and sentence depth (right). The scores are shown across various model sizes (ranked by model size),with darker lines indicating larger models. ShortRelative clauseLong-nested Accuracy (%) UUASLASnsubjobjnmoddet : Polar Probe performance across different sentence structures and dependency types in a controlleddataset. The three categories (Short, Relative clause, and Long-nested) show the performance breakdown byUnlabeled Attachment Score (UUAS), Labeled Attachment Score (LAS), and specific dependency relations inthe main phrase. Error bars represent the standard error across relations."
}