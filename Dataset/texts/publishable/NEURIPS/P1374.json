{
  "Abstract": "We explore the Iterative Inference Hypothesis (IIH) within the context oftransformer-based language models, aiming to understand how a models latentrepresentations are progressively refined and whether observable differences arepresent between correct and incorrect generations. Our findings provide empiricalsupport for the IIH, showing that the nth token embedding in the residual streamfollows a trajectory of decreasing loss. Additionally, we observe that the rate atwhich residual embeddings converge to a stable output representation reflects un-certainty in the token generation process. Finally, we introduce a method utilizingcross-entropy to detect this uncertainty and demonstrate its potential to distinguishbetween correct and incorrect token generations on a dataset of idioms.",
  "Introduction and Related Work": "Transformer-based architectures currently dominate artificial intelligence applications andserve as the underlying architecture for most Large Language Models (LLMs). While LLMsshow impressive emergent abilities, these models exhibit limitations such as hallucinations andbiased outputs which pose significant societal challenges . Inaccurate outputs can misleadusers, while malicious actors can exploit AI models to create deceptive images, videos, and text torepresent fictional occurrences as truth. Mitigating harms caused by model misuse, biased outputs,or misalignment with human values is a primary motivation behind research and policy decisionsrelated to AI interpretability . In this work we investigate a novel method for detecting uncertainty during the token generationprocess of transformer-based language models. One framework that has emerged for understandingthe feed-forward behavior of residual models, such as the transformer, is the Iterative InferenceHypothesis (IIH) . This hypothesis posits that predictions are formed in the residual stream,and that each block in a residual architecture incrementally updates these predictions in a direction ofdecreasing loss . A related line of research on in-context learning has suggested that transformerstrained on autoregressive tasks are closely related to formulations of iterative optimization algorithms,namely gradient descent . We combine these two threads of research, framing transformer inference as an optimization processthat iteratively updates the nth input embedding (i.e. the last word in the input sequence) to convergetoward the most likely next-token embedding given the context and model weights. We proposemethods for evaluating how input embeddings evolve towards output embeddings and find preliminary",
  "Methods": "Here we define methods that offer insight into how representations in the residual stream evolveduring inference. The transformer architecture, excluding token embedding and unembedding,can be succinctly represented by the following recurrence relation: ri+1 = ri + Li+1(ri), whereri = [ei1, ..., ein] represents the set of token embeddings in the residual stream after an update fromlayer Li. Each layer contains attention and feed-forward sublayers. With r0 as the set of input tokenembeddings plus positional encodings, the residual stream is the sequence (r0, r1, ..., rk) for a modelwith k layers. In this work we are particularly interested in tracking the evolution of the embeddingof the nth input token, ein, as shown in red in . Its residual representation after the final layerupdate, ekn, is used to predict the next token in an autoregressive framework.",
  ": The transformer as a recurrence relation, iteratively refining a prediction for the next token": "In the convolutional architecture of the original residual network (ResNet), the residual streamcontained shortcut connections that transformed the basis of latent space in order to reduce itsdimensionality at regular intervals . However, in the transformer, the residual stream has anentirely linear structure that preserves the basis of the embedding space . Each layer adds anoutput to each residual embedding, corresponding to translations in token embedding space. Thusthe residual stream can be viewed as a path through token embedding space, and each point alongthe path can be mapped back to a distribution over tokens via methods like logit lens . To getthe intermediate distribution predicted by the residual stream after layer Li, we pass ein through theoutput layer norm and then to the models output head to obtain logits over the vocabulary. We referto these logits as residual predictions. This framework forms the basis of our methods and analysis.",
  "Residual Cross-Entropy": "We present a method by which cross-entropy can be used to measure the evolution of residualpredictions during token generation. The IIH posits that the output of each layer updates the residualprediction in a direction of decreasing loss. We thus examine per-layer changes in cross-entropy, theloss function used to train transformers on autoregressive language tasks , including the specificmodel we examine here (GPT-2 XL) . Cross-entropy is a measure of dissimilarity, requiring a candidate and a target distribution. We use theresidual predictions described above as the candidate, and for the target distribution we examine twochoices: (1) the one-hot distribution of the token sampled deterministically by taking the argmax themodels predicted probabilities, denoted y, and (2) the one-hot distribution of the ground truth nexttoken given by the dataset, denoted y. These measures are equivalent to the negative log likelihood ofthe sampled and target tokens respectively, as elaborated in A.1. When the sampled output is correct,or y = y, these two measures are the same. Examining how layer updates affect cross-entropy with respect to the training objective y is necessary for our evaluation of the IIH, but the ground truth nexttoken may be unavailable or ambiguous at inference time, making y more practical for analyzingthe residual stream in an online setting. Some existing works have examined Kullback-Leiblerdivergence between residual predictions and the final logits output by the model , rather than yas we do here. Our approach implicitly penalizes generations that have high output entropy, which isadvantageous for our study of uncertainty during generation. We provide a comparison between thetwo in A.3.",
  "Model": "To investigate how representations evolve in the residual stream we examine GPT-2 XL , whichhas 48 layers and 1.5 billion parameters. We chose this model as it has low compute requirementswhile still being representative of frontier model architectures. In addition, it has widely used opensource implementations and official weights made publicly available by OpenAI on HuggingFace1.",
  "Inference Data": "For our preliminary study of the IIH and model uncertainty, we chose a simple dataset with a widevariety of generation difficulty: English idiom completion. An idiom completion task providesa relatively clear-cut, single token \"correct\" and \"incorrect\" answer, making it straightforward toevaluate. This dataset is intentionally challenging, where some answers would be hard or nearlyimpossible to guess given the input context or brevity of the idiom. The idiom dataset consists of 330 static idioms taken from the EPIE Dataset . To build ourdataset, we split each static idiom so that the final word serves as the \"correct\" output for the model.To guide the model in completing the idiom, we added instructions to the start of the idiom phrase.The instructions read: \"The following prompt is the beginning of a popular English idiom, pleaserespond with a single word to complete the phrase.\" Thus, each prompt in this dataset consists of theinstructions + the first words of an idiom. We excluded 29 idioms from the EPIE dataset because thetarget outputs were represented by more than one token in the vocabulary, such as [\"help\", \"ful\"].",
  "Results": "In this section, we examine the evolution of residual representations in GPT-2 XL on the idiominference dataset by analyzing the per-layer change in cross-entropy. Our main objectives are, first,to evaluate whether there is evidence supporting the IIH, and second, to determine if these metricsreveal a noticeable difference between correct and incorrect output distributions that could aid indeveloping a measure of uncertainty for a models predictions.",
  "(b) Cross-Entropy (layer, y)": ": Plots showing the cross-entropy between the residual prediction at each layer and a targetdistribution. The median, inter-quartile ranges, and outliers of correct and incorrect generations areplotted for 330 samples. (Left) The token predicted by the model y is used as the target. (Right) Theground-truth token y from the dataset is used as the target.",
  "Official weights for GPT-2 XL hosted on Hugging Face:": "The cross-entropy per model layer for the idiom dataset is presented in . To generatethese results, we first feed a prompt into the model, recording the nth residual embedding beforeand after the update from each layer. We then calculate the cross-entropy between these residualpredictions and a target. For a we take the target to be the token predicted by the model, y, asdescribed in .1. For b, we let the target be the ground-truth next token, y, from theidiom dataset. Distributions of correct generations (y = y) for each figure are plotted in blue andincorrect generations are plotted in red. This approach allows us to clearly observe the evolution ofrepresentations that ultimately lead to correct predictions during the inference process. In other words, a displays how the embeddings in the residual stream evolve towards anarbitrary output representation, while b shows how it evolves towards the most likely nexttoken according to the dataset. A distinct separation is observable between the correct and incorrectdistributions for both cross-entropy plots, suggesting these measures may be useful for understandingthe certainty of a models output as it is being generated. The separation is more pronounced whenthe target is the ground truth, as a result of the incorrect generations failing to converge to the correctrepresentation in embedding space. b demonstrates clear evidence for the IIH, with themedian layer update decreasing loss with respect to the ground truth nearly monotonically throughoutthe model for both correct and incorrect generations. We provide a table with the average decrease incross-entropy per layer for (b) in A.4.",
  "(b) Output Cross-Entropy ROC Curve": ": (Left) Distributions of correct and incorrect generations according to final layer cross-entropy with target y. (Right) The corresponding ROC curve. As indicated by the AUC of 0.92, theoutput cross-entropy is a strong predictor of correct generations for the idiom dataset. In a we show the cross-entropy distributions of the models output logits with respect to y,corresponding to the vertical slice over the last layer in a. We observe that the distributionof correct generations is exponential with a mean of 0.43, indicating that correct generations tendto converge more closely to a one-hot distribution and thus implying their output distributions havelower entropy. The incorrect predictions are normally distributed with a mean of 1.91 and a standarddeviation of 0.86, indicating that final predictions tend to be further in distribution from y and thushave higher entropy. By visual inspection, any generations resulting in a output cross-entropy greaterthan 1.5 are very likely to be incorrect for this dataset. In b we plot a receiver operatingcharacteristic (ROC) curve for the output cross-entropy and observe an area under the curve (AUC)of 0.9239, indicating that output cross-entropy is a strong predictor of correct vs incorrect generationon the idiom dataset. This was corroborated with a Mann-Whitney U test yielding a statistic withthe same value. A potential application of this metric is visualized in , measuring the output cross-entropyper token on an open ended generation task. Again, we measure the cross-entropy distributions of themodels output logits with respect to y since the model has access to this information at inferencetime. This generation was produced using a seed of 42, a temperature of 0.8, and the prompt \"AlanTuring\".",
  ": Output cross-entropy per generated token given the open-ended prompt \"Alan Turing\"": "In response, GPT-2 XL generates a number of erroneous historical facts, each of which is observedto have a spike in cross-entropy relative to the rest of the sequence. For reference, Alan Turingwas born in 1912 in London, England, attended Kings College in Cambridge, and died on June 7,1954. The cross-entropy values are lowest when the language heavily implies the next token, such aswith generated tokens 9, 10, 16, 24, 37, and 44. Here the space of possible next tokens is heavilyconstrained, in contrast to open-ended portions of the sequence encountered when generating tokens8, 22, 33, 40, and 41. Large cross-entropy values in this sequence are also observed on for tokensrepresenting dates, locations, and other facts - see tokens 1, 22, 26, 35, and 42. This example concisely illustrates trends observed over numerous generation studies, but it aloneis not sufficient for general claims. Anecdotally, the cross-entropy measure appears to capture bothuncertainty inherent in the prompt and the uncertainty of the model. It appears to be insufficientto disambiguate between these two sources of uncertainty. Additionally, if a model is confidentlyincorrect, this metric would not capture any uncertainty to indicate a potential mistake a limitationhumans face as well. Output cross-entropy can be computed cheaply at inference time and these results indicate that itcan potentially be used by a nave classifier to indicate the likelihood of an incorrect generation.Future work will examine if this result holds across models and datasets. In the appendix A.2, wepresent a table showing the intermediate predictions for the highest and lowest cross-entropy results ina. These samples further show that output cross-entropy appears to correspond with promptopen-endedness and the models uncertainty given the prompt. These combined results indicate thatthe correctness and perhaps amount of certainty for the next generated token can be measured by therate and degree to which embeddings converge to stable output representations in the residual streamof transformers.",
  "Conclusion": "In this work we investigate the Iterative Inference Hypothesis as applied to the transformer architectureon autoregressive language modeling problems. We provide a mechanism for investigating theevolution of predictions in the residual stream and find empirical evidence to support the hypothesis.In addition, we propose a novel method for observing uncertainty during the token generationprocess by measuring the cross-entropy between the model output logits and a one-hot distributionrepresenting the deterministically sampled token y. Using this metric we find distinct differencesbetween distributions of correct and incorrect generations on an idiom dataset, and we observe thatthis output cross-entropy appears to correspond with model uncertainty given a prompt. Future work will aim to address limitations of this preliminary study by expanding our analysis to abroad range of datasets and language models of varying sizes. We will additionally extend our studyto multi-token generations and explore the use of output cross-entropy as an uncertainty measure andpotential flag for hallucinations. Finally, we intend to explore additional convergence metrics thatmay better predict correct versus incorrect generations, then examine how broadly applicable theyare across models and datasets. The ultimate goal of this research is to develop methods for assuringthe quality of language model output with minimal computational cost.",
  "and Disclosure of Funding": "We thank Cash Costello and Patrick Emmanuel for providing helpful feedback. Funding was providedvia an internal research grant from Johns Hopkins University Applied Physics Lab. Compute hourswere provided on Oak Ridge National Laboratorys Summit supercomputer as a part of an awardfor this project from the National Science Foundations National Artificial Intelligence ResearchResources (NAIRR) Pilot Program. Guy Dar, Mor Geva, Ankit Gupta, and Jonathan Berant. Analyzing transformers in embeddingspace. In Proceedings of the 61st Annual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers), pages 1612416170, Toronto, Canada, July 2023. LAYING DOWN and INTELLIGENCE ACT. Proposal for a regulation of the europeanparliament and of the council laying down harmonised rules on artificial intelligence (artificialintelligence act) and amending certain union legislative acts, 2021. Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann,Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, DeepGanguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt,Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, andChris Olah. A mathematical framework for transformer circuits. Transformer Circuits Thread,2021.",
  "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for imagerecognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),pages 770778, 2016": "Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson,Heewoo Jun, Tom B. Brown, Prafulla Dhariwal, Scott Gray, Chris Hallacy, Benjamin Mann,Alec Radford, Aditya Ramesh, Nick Ryder, Daniel M. Ziegler, John Schulman, Dario Amodei,and Sam McCandlish. Scaling laws for autoregressive generative modeling, 2020. Stanisaw Jastrzebski, Devansh Arpit, Nicolas Ballas, Vikas Verma, Tong Che, and YoshuaBengio. Residual connections encourage iterative inference. In International Conference onLearning Representations, 2018.",
  "Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Languagemodels are unsupervised multitask learners. OpenAI, 2019": "Prateek Saxena and Soma Paul. Epie dataset: A corpus for possible idiomatic expressions. InText, Speech, and Dialogue: 23rd International Conference, TSD 2020, Brno, Czech Republic,September 811, 2020, Proceedings, page 8794, Berlin, Heidelberg, 2020. Springer-Verlag. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Informa-tion Processing Systems, volume 30. Curran Associates, Inc., 2017. Johannes Von Oswald, Eyvind Niklasson, Ettore Randazzo, Joao Sacramento, AlexanderMordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context bygradient descent. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt,Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conferenceon Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages3515135174. PMLR, 2329 Jul 2023.",
  "xXp(x) log q(x)": "In our case, X represents the vocabulary of tokens, q represents the probabilities predicted by themodel over X, and p represents some target distribution over X. For our methods, we take p to be aone-hot encoding representing either the sampled token y or the ground truth token y. With a one-hotencoding for p, the above summation yields only one non-zero term",
  "A.2Best and Worst Case Sample Generations": "We present the residual predictions for the generations with the highest and lowest output cross-entropy scores vs model predictions y on the idiom dataset in . These correspond to left- andright-most samples on the cross-entropy axis in a. We use the logit lens technique to recovertoken predictions from the residual stream after each layer update as described in 2. Many of the idiom prompts are one word and are extremely difficult to complete, even for humans.This open-endedness lends itself to high uncertainty, which is captured by the cross-entropy as shownhere. The samples with the lowest cross-entropy have prompts with multiple words that heavily implya specific next-token, severely constraining the set of possible next-tokens. In contrast, samples withthe highest cross-entropy have short prompts with very common words that are could have manyvalid next tokens, resulting in a very open-ended generation task. We observe output cross-entropyreflecting the open-endedness of the prompt via the corresponding uncertainty from the model.",
  "A.3Choice of Divergence Target": "Here we compare KL divergence between residual predictions and model output logits versusresidual predictions and a one-hot encoding of the top predicted logit. Note that KL divergence andcross-entropy between two distributions differs only by a constant. By definition, the final residualprediction is equivalent to the model output logits, thus the KL divergence approaches zero for allgenerations, as observed in a below. If the output logits of the model exhibit high entropy,then they will have a higher divergence when measured against to a one-hot representation of the toppredicted logit. This can be observed in b. We find this bias useful for distinguishing correctand incorrect generations.",
  "A.4Idiom Dataset Loss Table": "below shows average change in loss after each layer update to predictions in the residualstream for the idiom dataset. White cells denote no change, blue cells denote a decrease, and red cellsdenote an increase. Nearly all layer updates move the residual prediction in a direction of decreasingloss, supporting the IIH. Layers 20-38 appear to contribute the most to reducing residual predictionloss for correct generations. : A look into the residual stream for the idiom generations with highest and lowest outputcross-entropy. The token corresponding to the highest logit in the residual prediction after each layeris displayed to show how the path through token space.",
  "(b) KL Divergence (layer, y one-hot)": ": Computing KL divergence with respect to (a) the model output logits versus (b) the one-hotdistribution representing the top logit in the model output. It can be observed that (b) implicitlypenalizes outputs with high entropy. : The change in cross-entropy loss between the prediction in the residual stream and theground truth y after each layer update, averaged across all prompts in the idiom dataset. Nearly allupdates reduce the loss on average across all groupings."
}