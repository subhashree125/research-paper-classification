{
  "Abstract": "Multi-object 3D Grounding involves locating 3D boxes based on a given queryphrase from a point cloud. It is a challenging and significant task with numerousapplications in visual understanding, human-computer interaction, and robotics.To tackle this challenge, we introduce D-LISA, a two-stage approach incorporatingthree innovations. First, a dynamic vision module that enables a variable and learn-able number of box proposals. Second, a dynamic camera positioning that extractsfeatures for each proposal. Third, a language-informed spatial attention modulethat better reasons over the proposals to output the final prediction. Empirically,experiments show that our method outperforms the state-of-the-art methods onmulti-object 3D grounding by 12.8% (absolute) and is competitive in single-object3D grounding.1",
  "Introduction": "Building agents that can operate in real-world environments with humans has been a fundamentalgoal of artificial intelligence. Importantly, the agent would need to understand the 3D scene andnatural language to take instructions from humans. To benchmark these capabilities, there is anincreasing amount of interest in the task of object grounding in 3D .Recently, the task of multi-object 3D grounding has been proposed, i.e., given a text descriptionand a 3D scene localize all objects referred by the description. Along with the benchmark, Zhang et al. proposes, M3DRef-CLIP, a two-stage approach thatfirst detects all the potential objects (capped at a maximum number) from the 3D scene, and thenreasons about which of the objects are relevant to the text description by extracting features for eachof the objects. Specifically, they leverage both 3D features from the point cloud, and 2D featuresextracted from renderings of the detected objects at fixed camera poses. These object features alongwith the text embedding are passed into a Transformer to make the final prediction. Model training isformulated as multi-output classification, where each potential object is classified based on whether itis referred to by the text. In this work, we identify several directions in which M3DRef-CLIP could be improved. First, thegeneration of object proposals is based on a fixed maximum. Prior work points out the dilemmaof deciding the number of boxes in the 3D grounding task under the two-stage detection-and-selectiondiagram. Excessive proposals may increase complexity and lead to redundant computations whilesparse proposals may miss critical information in the scene. Second, the camera poses of the rendererare fixed to hand-selected viewpoints, which seems unlikely to be ideal given the variability in objectsizes. Third, the fusion module does not effectively reason over the spatial relationship of the objectsbased on the text description.",
  "arXiv:2410.22306v2 [cs.CV] 20 Dec 2024": "To address these shortcomings, we propose D-LISA, a two-stage approach that incorporates threeinnovative modules. First, instead of using all detected objects, we use a dynamic proposal module toselect the key box proposals. Second, we incorporate a dynamic multi-view renderer module thatoptimizes the viewing angles tailored to a specific scene. Third, we introduce a language-informedspatial fusion module that uses textual description to guide reasoning based on spatial relations. To evaluate our proposed method, we conduct experiments on the Multi3DRefer benchmark formulti-object 3D grounding and achieve a substantial 12.8% absolute increase over the existingbaseline M3DRef-CLIP. We also validate the effectiveness of our method by achieving the state-of-the-art performance on ScanRefer benchmark and competitive results on Nr3D benchmark forsingle-object 3D grounding. Our contributions are summarized as follows: We introduce a dynamic box proposal module that automatically determines the key box proposalsfor the later reasoning stage, which could potentially replace the fixed object proposals prevalentin existing two-stage grounding pipelines. Also, we learn the camera pose for 2D renderingdynamically based on the scene, enhancing the quality of auxiliary object features in uncertainenvironments. We propose a language-informed spatial fusion module that dynamically captures the spatialrelations among objects, significantly improving the models contextual understanding andperformance in the multi-object 3D grounding task. We conduct thorough experiments to validate the proposed framework. The proposed approachnot only significantly outperforms the state-of-the-art model in multi-object 3D grounding, butalso maintains robust performance in the single-object 3D grounding task.",
  "Related Work": "2D grounding aims to identify the target object in a 2D image based on a natural language description.The conventional detection-and-selection two-stage pipeline first extracts the visual features forthe proposals and language features for the description then employs the attention mechanism toeffectively align the visual features and language features . Alternatively, one-stagemethods directly regress the target boxes by integrating object detection and language understanding. While relational graphs have been used to explicitly model the object relations in2D images , extending the modeling to 3D is challenging due to larger number of objectsand more complex spatial relations. 3D grounding. Similar to 2D grounding, 3D grounding aims to target the language-referred object ina 3D scene. There have been a variety of datasets and approaches to tacklethis challenging problem. M3DRef-CLIP is the pioneered work to explore targeting multipleobjects that match the language description. Other than the one-stage methods that directly identifythe target box , two-stage methods like M3DRef-CLIP following the detection-and-selectiondiagram are facing the issue of determining the number of boxes from the detection stage. We proposea module that dynamically selects the key box proposals from object candidates. 2D features have been widely used to assist with 3D grounding as well as other 3Dtasks . However, most studies rely on fixed camera poses to generate these 2Dimage features, which is sub-optimal given the varying object sizes across different 3D scenes. Incontrast, we propose to learn scene-conditioned camera poses for object rendering. Many works have studied how to model the object relations in complex 3D scenes . For example, 3DVG-Trans and M3DRef-CLIP model the spatial relationsbased on distances. ViL3DRef and CORE-3DVG incorporate language and hand-selectedfeatures to guide the spatial relations. Differently, we propose a simple yet effective language-informed balancing strategy to explicitly reason over the spatial relation that solely depends ondistances.",
  "Approach": "Given a 3D point cloud of a scene S, and a text description T , the task of multi-object 3D groundingaims to predict the set of bounding boxes P for objects that are referred to in the text description.Our proposed Multi-Object 3D Grounding with Dynamic Modules and Language Informed Spatial",
  "A black armchair sits in the center among other chairs near a white board": ": Illustration of the overall pipeline. Our D-LISA processes the 3D point cloud through thedynamic visual module (Sec. 3.1) and encodes the text description through a text encoder. The visualand word features are fused through a language informed spatial fusion module (Sec. 3.2). Attention (D-LISA) follows the detection-and-selection paradigm for multi-object 3D groundingtask . This paradigm involves three components: (i) a text encoder to extract text features; (ii) avision module to detect object proposals and extract corresponding features given a point cloud; (iii)a fusion module that combines the text and object features to select the final referred bounding-boxes.Specifically, our D-LISA is designed with a novel vision module that allows for a dynamic number ofproposal boxes and extracts features from dynamic viewpoints (Sec. 3.1) per scene. Furthermore,we propose a fusion model that is spatially aware with explicit language conditioning (Sec. 3.2). Anoverview of our approach is illustrated in .",
  "Dynamic Vision Module": "Our dynamic vision module takes a 3D scene point cloud S as the input and generates a set of boxproposals B with corresponding visual features F. As in prior work , we adopt the backbonedetector of PointGroup to obtain a fixed number of M box candidates C, i.e., |C| = M. Toeliminate irrelevant detected objects, we employ a dynamic box proposal module with non-maximumsuppression (NMS). This module dynamically selects a subset with variable sizes, from the Mcandidates, to form the set of box proposals B, which are then used by the fusion model.",
  "where bm denotes the 3D box of the mth object": "We then use non-maximum suppression (NMS) to remove overlapping boxes from the boxproposal candidates B and finalize the box proposals B. First, the proposal probabilities i aresorted in descending order. Then we sequentially select the candidate with the highest probabilityas a box proposal and remove other box proposal candidates that have an Intersection over Union(IoU) greater than a threshold NMS. The NMS module ensures the box proposals B do not includeduplicated boxes for the same object.",
  "This loss encourages the model to use as few box proposals as possible while maintaining thegrounding performance": "Object proposal feature extraction. Given the N box proposals B, i.e., |B| = N, we extract visualfeatures F that is a concatenation of both the 3D features F3D from the detector and 2D features F2Dfrom our dynamic multi-view renderer. 3D feature from detector backbone. Each box bn in the box proposals B has a corresponding 3Dfeature f 3Dithat can be extracted from the detector backbone. Next, to ensure that the proposalprobability m reflect the quality of the box bm, we weight the 3D features with the probability, i.e.,",
  "F3D = {1 f 3D1 , 2 f 3D2 , . . . , N f 3DN }.(4)": "2D feature from Dynamic multi-view renderer. The dynamic multi-view renderer takes as input thebox proposals B and generates the corresponding 2D features F2D. Instead of using fixed cameraposes for rendering all objects across different scenes, we learn scene-conditioned camera poses forrendering. We predefined V base camera poses dcamjfor j = 1, 2, . . . , V . Next, we calculate theaverage size of all boxes denoted as q R3 with the average length, width, and height respectively.We use a Multi-Layer Perceptron (MLP) to learn the camera pose offset for each view j based on theaverage box size q:pcamj= MLPj(q).(5)",
  "Language-Informed Spatial Fusion Module": "Given the visual features F from the dynamic vision module and the word features W from CLIPstext encoder, the language-informed spatial fusion module predicts a probability pn on whether theobject in box bn is targeted in the text description. The module consists of a stack of transformerlayers followed by an MLP grounding head. To better capture the spatial relationship among objects, we introduce the language-informed spatialattention (LISA) block that balances the visual attention weights and the spatial relations using thesentence feature g, a weighted sum over all word features. Each transformer layer comprises alanguage-informed spatial attention block and a cross-attention block, as illustrated in . Finally,we only predicted a box if the associated probability pn exceeds a threshold pred, i.e., the predictedbox set isP = {bn | pn > pred}.(8)",
  "where 1N is an all-ones matrix and softmax normalizes along each row. We now describe B and D": "Spatial scores B. Given a variety of objects in a complex scene, we want the model to dynamicallylearn whether an object should pay more attention to the spatial relationship based on text description.For the ith object in the box proposal, we predict the normalized score i by concatenating the visualfeature fi and the sentence feature g, followed by a linear projection. To align with the attentionweights, we construct the spatial scores B RNN as",
  "M3DRef-CLIP w/NMS79.040.567.640.049.149.3D-LISA82.443.767.142.551.051.2": "Detection loss. We use Pointgroup as our detector backbone and adopt their training losses.The detection loss Ldet consists of four components: a) a semantic segmentation loss, b) an offsetregression loss, c) an offset direction loss, and d) a proposal score loss. loss. For multi-object 3D grounding, we adopt the binary cross-entropy loss over thedetected objects as the reference loss Lref. We apply the Hungarian algorithm to find an optimalmatch based on the pairwise IoU between the detected objects and ground truth. A detected box issuccessfully grounded if it matches one ground truth box in the Hungarian solution and the pairwiseIoU is greater than a threshold train. For single-object 3D grounding, we use the cross-entropy loss.We identify the highest IoU between the detected boxes and the ground truth box and consider it asuccess if this maximal IoU is greater than the threshold train. Contrastive loss. We apply a symmetric contrastive loss Lctr between the object features and the wordfeatures. A positive pair is formed if the object features and the word features come from the samescene-instruction pair, while a negative pair is formed if they come from different scene-instructionpairs. For computing efficiency, we only identify the positive and negative pairs within a single batch.This loss has been proven effective for learning better multi-modal embeddings .",
  "Multi-object 3D grounding": "Dataset and evaluation metric. Multi3DRefer is a dataset based on ScanRefer . It contains61,926 descriptions of 11,609 objects, with each text description potentially referencing zero, single,or multiple target objects. Using the standard evaluation protocol , we report the F1 score at the intersection over union (IoU)threshold of 0.5 over five different categories: a) zero target without distractors of the same semanticclass (ZT w/o D); b) zero target with distractors (ZT w/D); c) single target without distractors (STw/o D); d) single target with distractors (ST w/D); and e) multiple targets (MT). The average overthese categories is reported as an overall score. Baselines. Following prior work , we consider two-stage methods that perform well on theScanRefer dataset as baselines; including, 3DVG-Trans , D3Net , 3DJCG and M3DRef-CLIP . We also report the performance of M3DRef-CLIP with NMS after the first-stage detectorfor a fair comparison. Implementation details. We train our model on a single NVIDIA A100 GPU. We set the batchsize to 4 with the AdamW optimizer using a learning rate of 5e4. We follow the same train/val setsplit as the baselines . For the PointGroup detector, we use the same pre-trained PointGroup",
  "M3DRef-CLIPw/NMS": ": Qualitative examples of Multi3DRefer val set. For each scene-text pair, we visualizethe predictions of M3DRef-CLIP, M3DRef-CLIP w/NMS, D-LISA and ground truth labels inmagenta/blue/green/red separately. module following Zhang et al. with the same loss coefficients. We set the dynamic proposal losscoefficient dyn to 5. We set the train to 0.25 and search for the optimal value of pred over {0.05, 0.1,0.15, 0.2, 0.25} during evaluation for M3DRef-CLIP w/NMS and our model. Results. We compare the metric of our model and state-of-the-art baselines on Multi3DReferval set in Tab. 1. Our D-LISA achieves a 12.8% absolute increase in the overall score overM3DRef-CLIP. Comparing M3DRef-CLIP and M3DRef-CLIP w/NMS, we observe that NMS isa key factor in the final F1 score, successfully removing duplicate predictions leading to improvedrecall. Next, D-LISA achieves a better overall F1 score, especially for multiple targets and sub-categorieswhere the distractors of the same semantic class exist. We further provide qualitative results over ourmethod and the baselines in . The top two rows are examples from multiple target categories.Our D-LISA successfully identifies more objects that match the text description. The last row showsan example of a single target with distractors. Our D-LISA accurately identifies the object while thebaselines are affected by the distractors and predict additional incorrect targets.",
  "Single-object 3D grounding": "Dataset and evaluation metric. We evaluate the single-object 3D grounding performance on theScanRefer and the Nr3D datasets. The ScanRefer dataset contains 51,583 human-written sentencesfor 800 scenes in ScanNet . ScanRefer divides scenes into Unique and Multiple subsetsbased on whether the semantic class of the target object is unique in the scene. The Nr3D dataset consists of 41,503 human-annotated text descriptions across 707 indoor scenesfrom ScanNet. Nr3D divides scenes into Easy and Hard subsets based on whether there exist thedistractors of the same semantic class, and into View-dependent and View-independent subsetsbased on whether a specific viewpoint is required to identify the target. Both ScanRefer and Nr3Dare annotated for single-object grounding. Different from ScanRefer, Nr3D assumes perfect objectproposals are provided. Following prior work , for the ScanRefer dataset we report on both val and test setsover different subsets. The number represents the proportion of predicted target boxes that have anIoU value greater than 0.5 compared to the ground truth box. For the Nr3D dataset, we report theaccuracy of selecting the target bounding box among all candidate proposals on the test set overdifferent subsets.",
  "D-LISA60.246.244.357.453.1": "Baselines. We focus on comparing the two-stage methods designed for the situation where the groundtruth box proposals are not provided. For the ScanRefer dataset, we compare with the baselines:TGNN , FFL-3DOG , InstanceRefer , 3DVG-Trans , 3DJCG , D3Net , UniT3D, HAM , CORE-3DVG and M3DRef-CLIP . For joint captioning and groundingmodels 3DJCG, D3Net, and UniT3D, we compare their best grounding performance with extracaptioning training data. For the Nr3D dataset, we compare with the above baselines which reportedthe performance in their paper. Implementation details. We follow the multi-object setting to adapt to the single-object setting.Differently, we let the fusion module return the most likely box among all the proposal boxes insteadof using a threshold. For the Nr3D dataset, we follow the prior work to directly crop the boxfeatures from the detector backbone based on the ground truth bounding boxes. We follow the sametrain/val/test set split for both datasets as the baselines. Results. We report the of different methods on the ScanRefer val set and test set in Tab. 2.Comparing M3DRef-CLIP and M3DRef-CLIP w/NMS, we could see that non-maximum suppressionslightly improves the performance. Our D-LISA outperforms all existing baselines on both theScanRefer val set and test set, especially for the subsets where there are multiple objects with thesemantic class of the target object in the scene. Next, we report the grounding accuracy of different methods on the Nr3D test set in Tab. 3. OurD-LISA outperforms all baselines on the Nr3D test set over all subsets. For more comparison withother methods on the ScanRefer and the Nr3D datasets, see Sec. A2 in the Appendix. Limitations: As with other two-stage methods, the grounding performance of our designed two-stagemodel is upper bounded by the detector quality. From Tab. 1 and Tab. 2, we can see that our modelachieves better performance for complex scenarios but sacrifice some performance for the simplersingle-object settings.",
  "Ablation studies": "We conduct ablation studies on the proposed modules to validate their effectiveness under the multi-object grounding setting on the M3DRef dataset. The ablations follow the same experiment settingsfor the multi-object grounding in Sec. 4.1. The baseline Row #1 shows the result of M3DRef-CLIPw/NMS. Dynamic box proposal. In Tab. 4, comparing Row #3 with baseline Row #1, we validate theeffectiveness of the dynamic box proposal module. We also validate the number of box candidates inthe reasoning stage after using the dynamic box proposal module. For our complete model Row #5,an average of 30.5 boxes are selected for the fusion stage on the M3DRefer val set. This is a muchsmaller number of boxes compared to the 62.4 boxes used in baseline Row #1. Dynamic multi-view renderer. In Tab. 4, comparing Row #2 with baseline Row #1, we validate theeffectiveness of the dynamic multi-view renderer module. We provide the qualitative results for thedynamic multi-view renderer in . Instead of using fixed camera poses, the dynamic rendereradapts different camera poses from scene to scene, enhancing the quality of 2D object features. Language informed spatial fusion. In Tab. 4, comparing Row #4 with baseline Row #1, we validatethe effectiveness of the language-informed spatial fusion module, especially for the sub-categorieswhere distractors exist (ZT w/D and ST w/D). For more ablation results on the language-informedspatial fusion module, please refer to Appendix Sec. A3. Computational cost. We report the FLOPs and inference time of each proposed module and acomparison with the baseline model M3DRef-CLIP in Tab. 5. All experiments are conducted onMulti3DRefer validation set on a single NVIDIA A100 GPU. The reported FLOPs and inferencetime are the average over the validation set. We observe that the dynamic box proposal module",
  "Conclusion": "In this paper, we present D-LISA, a two-stage pipeline for multi-object 3D grounding, featuring threenovel components. Our dynamic box proposal module dynamically selects the key box proposalsfrom detected objects. We enhance the 2D features through optimized scene-conditioned renderingposes using a dynamic multi-view renderer. Furthermore, our language-informed spatial fusionmodule facilitates explicit reasoning over the object spatial relations. Our proposed approach notonly outperforms the state-of-the-art model in multi-object 3D grounding but also is competitive insingle-object 3D grounding.",
  "The appendix is organized as follows:": "In Sec. A1, we provide additional results on the Multi3DRefer dataset for multi-object grounding. In Sec. A2, we provide additional comparisons with state-of-the-art methods on ScanRefer andNr3D datasets for single-object grounding. In Sec. A3, we provide additional comparisons and ablation results for our proposed LISA block. In Sec. A4, we provide additional details for D-LISA. In Sec. A5, we provide additional qualitative results.",
  "M3DRef-CLIP81.839.453.534.643.642.8M3DRef-CLIP w/NMS79.040.576.946.857.056.3D-LISA82.443.775.549.358.457.8": "Additional ablation results on question types. Additional ablations for different query types,including queries with spatial, color, texture, and shape information are reported in Tab. A2. Weobserve that each proposed module effectively improves the performance for the queries that containspatial, color, and shape information, and is competitive with the baseline for queries with textureinformation. The overall model achieves better grounding performance across all query types thanthe baseline. Table A2: Ablation studies on question types on Multi3DRefer dataset. LIS., DBP. and DMR.stands for Language informed spatial fusion, Dynamic box proposal, and Dynamic multi-viewrenderer respectively. results are reported.",
  "A2Additional single-object grounding comparisons": "We provide additional comparisons with state-of-the-art methods on ScanRefer and Nr3D datasets forsingle-object grounding. These methods do not follow the detection-and-selection two-stage diagram.Different from ScanRefer, Nr3D assumes perfect object proposals are provided. We focus on thegrounding performance on the ScanRefer dataset as the task setting is more realistic. We report thegrounding performance on both ScanRefer and Nr3D for completeness.",
  "D-LISA75.540.046.969.039.746.3": "ScanRefer dataset. We provide additional comparisons with other state-of-the-art methods on theScanRefer dataset in Tab. A5. For the methods using object proposals as input instead of the 3Dscene, typically a separate pre-trained detector is used to pre-process the scene .Our D-LISA outperforms all existing methods and achieves the best grounding accuracy on both thevalidation set and test set, which further validates the effectiveness of our proposed modules.",
  "A3Additional results for LISA": "We provide more experimental results on our designed language informed spatial attention (LISA)module. We show the ablation results on the design choice and compare our module with otherlanguage-guided attention modules. Design choice. We analyze the factors that affect the spatial score and report the metricon Multi3DRefer dataset in Tab. A7. The result shows using both the sentence feature and objectfeature to predict the spatial score yields the best grounding performance. Additional comparison.We compare our designed LISA with the spatial self-attention inViL3DRef , which also models the object relations guided by language. ViL3DRef pre-definesobject relations through hand-crafted features. These hand-selected features work with ground truthobject proposals but lead to worse performance when the object proposals are predicted, i.e. noisy.As is shown in Tab. A5 and Tab. A6, though ViL3DRel works well on the Nr3D benchmarkwhich provides ground truth box proposals, the performance is much worse when validating on theScanRefer benchmark where no ground truth proposals are provided.",
  "We provide additional architecture details for our D-LISA and additional implementation details forthe experiment setup": "Cross-attention. In the language informed fusion module, a language informed spatial attention blockis followed by a cross-attention block (Sec. 3.2). The cross-attention block takes the spatially enhancedvisual features F s from LISA and word features after a self-attention block as input and generateslanguage-informed visual features F c. We follow the standard cross-attention mechanism as describedin Vaswani et al. . We formulate the word feature matrix input as FT = [t1, t2, . . . , tL]T Rdd,where tj Rd is the corresponding feature for wj W after self-attention. Given F s and FT ,queries Qc, keys Kc and values Vc correspond to:",
  "where softmax is the softmax normalization along rows": "Additional implementation details. Following the prior work , we take point coordinates, pointnormals, and per-point multi-view features S RH(3+3+128) as scene input, where H denotes thetotal number of points in the scene. For NMS process, we set the threshold NMS to be 0.4. For CLIP,we use a frozen pre-trained CLIP with ViT-B/32. For loss coefficient terms in Eq. (12), we set det,ref and ctr to 1 and dyn to 5. We initialize the camera baseline poses following the fixed cameraposes in prior work , where for each view the rendering camera is set to be 1 meter away from theobject, with an elevation angle of 45. For the fusion module, we follow the same settings in terms ofdimension size, layer number, and head size as used for the baseline ."
}