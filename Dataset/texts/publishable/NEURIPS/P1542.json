{
  "Abstract": "We study the problem of learning mixtures of Gaussians with approximate dierential privacy.We prove that roughly kd2 + k1.5d1.75 + k2d samples suce to learn a mixture of k arbitraryd-dimensional Gaussians up to low total variation distance, with dierential privacy. Our workimproves over the previous best result [AAL24b] (which required roughly k2d4 samples) and isprovably optimal when d is much larger than k2. Moreover, we give the rst optimal boundfor privately learning mixtures of k univariate (i.e., 1-dimensional) Gaussians.Importantly,we show that the sample complexity for privately learning mixtures of univariate Gaussians islinear in the number of components k, whereas the previous best sample complexity [AAL21]was quadratic in k. Our algorithms utilize various techniques, including the inverse sensitivitymechanism [AD20b, AD20a, HKMN23], sample compression for distributions [ABDH+20], andmethods for bounding volumes of sumsets.",
  "Introduction": "Learning Gaussian Mixture Models (GMMs) is one of the most fundamental problems in algo-rithmic statistics. Gaussianity is a common data assumption, and the setting of Gaussian mixturemodels is motivated by heterogeneous data that can be split into numerous clusters, where each clus-ter follows a Gaussian distribution. Learning mixture models is among the most important problemsin machine learning [Bis06], and is at the heart of several unsupervised and semi-supervised machinelearning models. The study of Gaussian mixture models has had numerous scientic applicationsdating back to the 1890s [Pea94], and is a crucial tool in modern data analysis techniques in a va-riety of elds, including bioinformatics [LKWB22], anomaly detection [ZSM+18], and handwritinganalysis [Bis06].In this work, we study the problem of learning a GMM from samples. We focus on the densityestimation setting, where the goal is to learn the overall mixture distribution up to low total variationdistance. Unlike the parameter estimation setting for GMMs, density estimation can be done evenwithout any boundedness or separation assumptions on the parameters of the components. In fact,it is known that mixtures of k Gaussians in d-dimensions can be learned up to total variationdistance using O(kd2/2) samples [ABH+18].",
  "Work done as a student at Carnegie Mellon UniversityWork done as a student at MIT": "Ensuring data privacy has emerged as an increasingly important challenge in modern dataanalysis and statistics. Dierential privacy (DP) [DMNS06] is a rigorous way of dening privacy,and is considered to be the gold standard both in theory and practice, with deployments by Apple[Tea17], Google [EPK14], Microsoft [DKY17], and the US Census Bureau [DLS+17]. As is the casefor many data analysis tasks, standard algorithms for learning GMMs leak potentially sensitiveinformation about the individuals who contributed data. This raises the question of whether wecan do density estimation for GMMs under the constraint of dierential privacy.Private density estimation for GMMs with unrestricted Gaussian components is a challengingtask. In fact, privately learning a single unrestricted Gaussian has been the subject of multiplerecent studies [AAK21, KMS+22b, AL22, KMV22, AKT+23, HKMN23]. Private learning of GMMsis signicantly more challenging, because even without privacy constraints, parameter estimationfor GMMs requires exponentially many samples in terms of the number of components [MV10].Therefore, it is not clear how to use the typical recipe of adding noise to the estimated parameters orprivately choosing from the nite-dimensional space of parameters. Consequently, the only knownsample complexity bounds for privately learning unrestricted GMMs are loose [AAL24b, AAL21].Let us rst formally dene the problem of learning GMMs.We represent a GMM D =ki=1 wiN(i, i) by its parameters, namely {(wi, i, i)}ki=1, where wi 0, i wi = 1, i Rd,and i is a positive denite matrix. In the following, a GMM learning algorithm A receives a set ofdata points in Rd and outputs a (representation of) a GMM. The total variation distance betweentwo distributions is dTV( D, D) = 1",
  "to accuracy and failure probability if for every GMM D, given samples X1, . . . , Xni.i.d. D, itoutputs (a representation of) a GMM D such that dTV( D, D) with probability at least 1": "and are called the accuracy and failure probability, respectively. For clarity of presentation,we will typically x the value of (e.g., = 1/3).The above denition does not enforce theconstraint of dierential privacy.The following denitions formalizes (approximate) dierentialprivacy. Denition 1.2 (Dierential Privacy (DP) [DMNS06, DKM+06]). Let , 0.A randomizedalgorithm A : X n O is said to be (, )-dierentially private ((, )-DP) if for any two neighboringdatasets X, X X n and any measurable subset O O,",
  "If the GMM learner A of Denition 1.1 is (, )-DP, we say that A privately learns GMMs.Formally, we have the following denition": "Denition 1.3 (Privately learning GMMs). Fix the number of samples n, dimension d, and numberof mixture components k. For , (0, 1) and , 0, a randomized algorithm A, that takes asinput X1, . . . , Xn Rd, (, )-privately learns GMMs up to accuracy and failure probability , if: 1. For any GMM D that is a mixture of up to k Gaussians in d dimensions, if X = {X1, . . . , Xn} i.i.d.D, A(X1, . . . , Xn) outputs a GMM D such that dTV( D, D) with probability at least 1(over the randomness of the data X and the algorithm A).",
  "+ k2d2": "k2d log(1/)3/2/(2) is known [AAL21]. These are the only known results even for privately learn-ing (unbounded) univariate GMMs. In other words, the best known upper bound for sample com-plexity of privately learning univariate GMMs has quadratic dependence on k.In the related public-private setting [BKS22, BBC+23], it is assumed that the learner has accessto some public data. In this setting, [BBC+23] show that unrestricted GMMs can be learned witha moderate amount of public and private data.Assuming the parameters of the Gaussian components (and the condition numbers of the co-variance matrices) are bounded, one can create a cover for GMMs and use private hypothesisselection [BSKW19] or the private minimum distance estimator [AAK21] to learn the GMM. Onthe ip side, [ASZ21] prove a lower bound on the sample complexity of learning GMMs, thoughtheir lower bound is weaker than ours and is only against pure-DP algorithms.The focus of our work is on density estimation. A related problem is learning the parametersa GMM, which has received extensive attention in the (non-private) literature (e.g., [Das99, MV10,BS10, LM21, BDJ+22, LL22] among many other papers). To avoid identiability issues, one hasto assume that the Gaussian components are suciently separated and have large-enough weights.In the private setting, the early work of [NRS07] demonstrated a privatized version of [VW04] forlearning GMMs with xed (known) covariance matrices.The strong separation assumption (of(k1/4)) between the Gaussian components in [NRS07] was later relaxed to a weaker separationassumption [CCAd+23]. A substantially more general result for privately learning GMMs with un-known covariance matrices was established in [KSSU19], based on a privatized version of [AM05].Yet, this approach also requires a polynomial separation (in terms of k) between the components,as well as a bound on the spectrum of the covariance matrices. [CKM+21] weakened the separationassumption of [KSSU19] and improved over their sample complexity.This result is based on ageneric method that learns a GMM using a private learner for Gaussians and a non-private cluster-ing method for GMMs. Finally, [AAL23] designed an ecient reduction from private learning ofGMMs to its non-private counterpart, removing the boundedness assumptions on the parametersand achieving minimal separation (e.g., by reducing to [MV10]). Nevertheless, unlike density esti-mation, parameter estimation for unrestricted GMMs requires exponentially many samples in termsof k [MV10].A nal important question is that of ecient algorithms for learning GMMs.Much of thework on learning GMM parameters focuses on computational eciency (e.g,. [MV10, BS10, LM21,BDJ+22, LL22]), as does some work on density estimation (e.g., [CDSS14, ADLS17]). However,under some standard hardness assumptions, it is known that even non-privately learning mixturesof k d-dimensional Gaussians with respect to total variation distance cannot be done in polynomialtime as a function of k and d [DKS17, BRST21, GVV22]. Addendum.In a concurrent submission, [AAL24a] extended the result of [AAL24b] for learningunrestricted GMMs to the agnostic (i.e., robust) setting. In contrast, our algorithm works only inthe realizable (non-robust) setting. Moreover, [AAL24a] slightly improved the sample complexityresult of [AAL24b] from O(log(1/)k2d4/(4)) to O(log(1/)k2d4/(2)). The sample complexityof our approach is still signicantly better than [AAL24a] in terms of all parameterssimilar to theway it improved over [AAL24b].",
  "For privately learning mixtures of univariate Gaussians, the previous best-known result forarbitrary Gaussians required Ok2 log3/2(1/)": "2samples [AAL21]. Importantly, we are the rst paperto show that the sample complexity can be linear in the number of components.Our work purely focuses on sample complexity, and as noted in Theorems 1.4 and 1.5, theydo not have polynomial time algorithms. We note that the previous works of [AAL21, AAL24b]also do not run in polynomial time. Indeed, there is reason to believe that even non-privately, it isimpossible to learn GMMs in polynomial time (in terms of the optimal sample complexity) [DKS17,BRST21, GVV22].Finally, we prove the following lower bound for learning GMMs in any xed dimension d. Theorem 1.6. Fix any dimension d 1 number of components k 2, any , at most a sucientlysmall constant c, and (/d)O(1). Then, any (, )-DP algorithm that can learn a mixtureof k arbitrary full-dimensional Gaussians in d dimensions up to total variation distance , withprobability at least 2/3, requires at least the following number of samples:",
  "Related work": "In the non-private setting, the sample complexity of learning unrestricted GMMs with respect tototal variation distance (a.k.a. density estimation) is known to be (kd2/2) [ABM18, ABH+18],where the upper bound is obtained by the so-called distributional compression schemes.In the private setting, the only known sample complexity upper bound for unrestricted GMMs[AAL24b] is roughly k2d4 log(1/)/(4), which exhibits sub-optimal dependence on various pa-rameters4.This bound is achieved by running multiple non-private list-decoders and then pri-vately aggregating the results.For the special case of axis-aligned GMMs, an upper bound of",
  "i, as we can nish the procedure with hypothesis selection, as discussed above": "Birds eye view:Say we are given a dataset X = {X1, . . . , Xn}: note that every Xj R since weare dealing with univariate Gaussians. The main insight is to sort the data in increasing order (i.e.,reorder so that X1 X2 Xn) and consider the unordered multiset of successive dierences{X2 X1, X3 X2, . . . , Xn Xn1}. One can show that if a single datapoint Xj is changed, thenthe set of consecutive dierences (up to permutation) does not change in more than 3 locations (seeLemma F.5 for a formal proof).We then apply a standard private histogram approach. Namely, for each integer a Z, we createa corresponding bucket Ba, and map each Xj+1 Xj into Ba if 2a Xj+1 Xj < 2a+1. If somemixture component i had variance i = 2i , we should expect a signicant number of Xj+1 Xjto at least be crudely close to i, such as for the Xj drawn from the ith mixture component. So,some corresponding bucket should be reasonably large. Finally, by adding noise to the count ofeach bucket and taking the largest noisy counts, we will successfully nd an approximation to allvariances.",
  "In more detail:For each (possibly negative) integer a let ca be the number of indices i such that2a Xj+1 Xj < 2a+1. We will prove that, if the weight of the ith component in the mixture is": "wi and there are n points, then we should expect at least (wi n) indices j to be in a bucket awith 2a within a poly(n) multiplicative factor of the standard deviation i (see Lemma F.3). Thepoint of this observation is that there are at most O(log n) buckets Ba with 2a betweeni",
  "log nfor some a withi": "poly(n) 2a O(i).Moreover, we know that if we change a single data point Xj, the set of consecutive dierences{Xj+1 Xj} after sorting changes by at most 3. So, if we change a single Xj, at most 3 of thecounts ca can change, each by at most 3.Now, for every integer a, draw a noise value from the Truncated Laplace distribution (seeDenition A.2 and Lemma A.3), and add it to ca to get a noisy count ca.The details of thenoise distribution are not important, but the idea is that this distribution is always bounded byO 1",
  "log 1": ". Moreover, the Truncated Laplace distribution preserves (, )-DP. This means that thecounts {ca}aZ will have (O(), O())-DP, because the true counts ca only change minimally acrossadjacent datasets.Our crude approximation to the set of standard deviations will be the set of 2a such that caexceeds some threshold which is a large multiple of 1",
  ". So, for each i k withweight at least /k, some corresponding a withi": "poly(n) 2a O(i) will have count ca signicantlyexceeding the threshold, and thus noisy count ca exceeding the threshold. This will be enough tocrudely approximate the values i coming from large enough weight. We can ignore any componentwith weight less than /k, as even if all but one of components have such small weight, togetherthey only contribute weight. So, we can ignore them and it will only cost us in total variationdistance, which we can aord.",
  "+ k log n": ",which is sucient to solve our desired problem in the univariate setting.Note that this proof relies heavily on the use of private histograms and the order of the datapoints in the real line. Therefore, it cannot be extended to the high-dimensional setting. We willuse a completely dierent approach to prove Theorem 1.4.",
  "Overview of Theorem 1.4 for general GMMs": "As in the univariate case, we only need rough approximations of the covariances. We will learn thecovariances one at a time: in each iteration, we privately identify a single covariance i which crudelyapproximates some covariance i in the mixture, with (/ k log(1/), /k)-DP. Using the well-known advanced composition theorem (see Theorem A.1), we will get an overall privacy guaranteeof (, )-DP. However, to keep things simple, we will usually aim for (, )-DP when learning eachcovariance, and we can replace with /",
  "k log(1/) and with /k at the end.A natural approach for parameter estimation, rather than learn i one at a time, is to learn": "all of the covariances together. However, we believe that this approach would cause the samplecomplexity to multiply by a factor of k, compared to learning a single covariance. The advantage oflearning the covariances one at a time is that we can apply advanced composition: this will causethe sample complexity to multiply by roughly",
  "k log(1/) instead.In the rest of this subsection, our main focus is to identify a single crude approximation i": "Applying robustness-to-privacy:The rst main insight is to use the robustness-to-privacyconversion of Hopkins et al. [HKMN23] (see also [AUZ23]). Hopkins et al. prove a black-box (butnot computationally ecient) approach that can convert robust algorithms into dierentially privatealgorithms, using the exponential mechanism and a well-designed score function. This reductiononly works for nite dimensional parameter estimation and therefore cannot be applied directlyto density estimation.On the other hand, parameter estimation for arbitrary GMMs requiresexponentially many samples in the number of components [MV10]. However, we will demonstratethat this lower bound does not hold when we only need a very crude estimation of the parameters.The idea is the following. For a dataset X = {X1, . . . , Xn}, let S = S(, X) be a score function,which takes in a dataset X of size n and some candidate covariance , and outputs some non-negative integer. At a high level, the score function S(, X) will be the smallest number of datapoints t that we should change in X to get to some new data set X with a specic desired property:X should look like a sample generated from a mixture distribution with being the covarianceof one of its components namely, a component with a signicant mixing weight. One can denelooks like in dierent ways, and we will adjust the precise denition later. We remark that thisnotion of score roughly characterizes robustness, because if the samples in X were truly drawn froma Gaussian mixture model with covariances {i}ki=1, we should expect S(i, X) to be 0 (since Xshould already satisfy the desired property), but if we altered k data points, the score should be atmost k. The high-level choice of score function is somewhat inspired by a version of the exponentialmechanism called the inverse sensitivity mechanism [AD20b, AD20a], though the precise way ofdening the score function requires signicant care and is an important contribution of this paper.The robustness-to-privacy framework of [HKMN23], tailored to learning a single covariance,implies the following general result, which holds for any score function S following the blueprintabove. For now, we state an informal (and slightly incorrect) version. Theorem 2.1 (informal - see Theorem C.3 for the formal statement). For any [0, 1), and anydataset X of size n, dene the value V(X) to be the volume (i.e., Lebesgue measure) of the setof covariance matrices (where the covariance can be viewed as a vector by attening), such thatS(, X) n.Fix a parameter < 0.1, and suppose that for any dataset X of size n such that V/2(X) isstrictly positive, the ratio of volumes V(X)/V/2(X) is at most some K (which doesnt depend onX). Then, if n log K",
  "low score (i.e., where S(, X) n) using n samples": "Note that Theorem 2.1 does not seem to say anything about whether X comes from a mixtureof Gaussians. However, we aim to instantiate this theorem with a score function that is carefullydesigned for GMMs. Recall that we want S(, X) to capture the number of points in X that needto be altered to make it look like a data set that was generated from a mixture, with being thecovariance of one of the Gaussian components. In other words, S(, X) should be small if (andhopefully only if) a mildly corrupted version of X includes a subset of points that are generated from a Gaussian with covariance . At the heart of designing such a score function, one needs tocome up with a form of robust Gaussianity tester that tells whether a given set of data pointsare generated from a Gaussian distribution. Aside from this challenge, the volume ratio associatedwith the chosen score function needs to be small for every dataset X otherwise the above theoremwould require a large n (i.e., number of samples). These two challenges are, however, related. If therobust Gaussianity tester has high specicityi.e., rejects most of the sets that are not generatedfrom a (corrupted) Gaussianthen the volume ratio is likely to be small (i.e., a smaller number ofcandidates would receive a good/low score). First Attempt:We rst try an approach which resembles that of [HKMN23] for privately learninga single Gaussian. We dene S(, X) as the smallest integer t satisfying the following property:there exists a subset Y X of size n/k, such that we can change t data points from Y to get to Y,where Y looks like i.i.d. samples from a Gaussian with some covariance that is similar to .The choice of Y having size n/k is motivated by the fact that each mixture component, on average,has n/k data points in X.The notions of looks like and similar to of course need to be formally dened. We say issimilar to (or ) if they are spectrally close, i.e., 0.5 2. We say that Y lookslike samples from a Gaussian with covariance if some covariance estimation algorithm predictsthat Y came from a Gaussian with covariance . The choice of covariance estimation algorithmwill be quite nontrivial and ends up being a key ingredient in proving Theorem 1.4.To apply Theorem 2.1, we rst set = c/k for some small constant c. We cannot set a largervalue , because if we change t n/k data points, we could in fact create a totally arbitrary newGaussian component with large weight. Since there is no bound on the eigenvalues of the covariancematrix, this could cause the volume V(X) to be innite. The main question we must answer is howto bound the volume ratio V(X)/V/2(X). To answer this question, we rst need to understandwhat it means for S(, X) n. If S(, X) n, then there exists a corresponding set Y Xof size n/k, and we can change n = c |Y| points from Y to get to some Y which looks likesamples from a Gaussian with covariance . Thus, Y looks like c-corrupted samples from sucha Gaussian (i.e., a c fraction of the data is corrupted). This motivates using a robust covarianceestimation algorithm: indeed, robust algorithms can still approximately learn even if a smallconstant fraction of data is corrupted, so for any Y X, we expect that no matter how we changea c fraction of Y to obtain Y, the robust algorithms covariance estimate should not change much.So, for any xed Y, the set of possible , and thus the set of possible , should not be that large.In summary, to bound V(X) versus V/2(X), there are at most nn/kchoices for Y X in theformer case, and at least 1 choice in the latter case (since we assume V/2(X) > 0 in Theorem 2.1).Moreover, for any such Y, the volume of corresponding should be exponential in d2 (either forV(X) or V/2(X)), since the dimensionality of the covariance is roughly d2. So, this suggests thatthe overall volume ratio is at most nn/k eO(d2). Since log nn/k (n/k) log k, if we plug into",
  "Theorem 2.1 it suces to have n (n/k) log k+d2": "(c/k). Unfortunately this is impossible unless log k.These ideas will serve as a good starting point, though we need to improve the volume ratioanalysis. To do so, we also modify the robust algorithm, by strengthening the assumptions on whatit means for samples to look like they came from a Gaussian. Improvement via Sample Compression:To improve the volume ratio, we draw inspirationfrom a technique called sample compression, which has been used in previous work on non-privatedensity estimation for mixtures of Gaussians [ABH+18, ABDH+20]. The idea behind sample com-pression is that one does not need the full set Y to do robust covariance estimation; instead, welook at a smaller set of samples. For instance, if Y X looks like c-corrupted samples from aGaussian of covariance , we expect that a random subset Z of Y also looks like c-corruptedsamples from such a Gaussian. Moreover, as long as one uses m O(d) corrupted samples from aGaussian, we can still (ineciently) approximate the covariance. This motivates us to modify therobust algorithm as follows: rather than just checking whether Y looks like c-corrupted samplesfrom a Gaussian of covariance roughly , we also test whether an average subset Z Y of size mdoes as well. Therefore, if has low score, there exists a corresponding set Z X of size m, andthere are onlynm em log n choices for Z. So, now it suces to have n m log n+d2",
  "(c/k), which will give": "us a bound of O(d2k/), as long as m O(d2). Importantly, we still check the robust algorithm onY of size roughly n/k, which allows us to keep the robustness threshold at roughly c/k.There is one important caveat that for each subset Z, there is a distinct corresponding covariance, and the volume of can change drastically as changes. (For instance, the volume of T is T (d2) times as large as the volume of . Since we have no bounds on thepossible covariances, T could be unbounded.) For our volume ratio to actually be bounded by aboutnm eO(d2), we want the volume of to stay invariant with respect to . This can be done bydening a normalized volume where the normalization is inversely proportional to the determinant(see Appendix C.1 for more details). The robustness-to-privacy conversion (Theorem 2.1) will stillhold.While the bound of O(d2k/) seems good, we recall that this bound is merely the samplecomplexity for (, )-DP crude approximation of a single Gaussian component. As discussed at thebeginning of this subsection, to learn all k Gaussians, we actually need (/ k log(1/), /k)-DP,rather than (, )-DP, when crudely approximating a single component. This will still end up leadingto a signicant improvement over previous work [AAL24b], but we can improve the volume ratioeven further and thus obtain even better sample complexity. Improving Dimension Dependence:Previously, we used the fact that the volume of candidate (corresponding to a xed Z) was roughly exponential in d2 for either V/2(X) or V(X), so theratio should also be roughly exponential in d2. Here, we improve this ratio, which will improve theoverall volume ratio.First, we return to understanding the guarantees of the robust algorithm. It is known that,given m O(d) samples from a Gaussian of covariance , we can provide an estimate such that(1 O(",
  "d/m)to 1 O(c +": "d/m), for now let us ignore the additional c factor.If V/2(X) > 0, then there is some covariance and some set Y of size n/k, where the robustalgorithm thinks Y looks like (possibly corrupted) Gaussian samples of covariance . So, every such that 0.5 2 has score of at most /2 n. This gives us a lower bound on V/2(X). Wenow want to upper bound V(X). If S(, X) < n, we still have that the robust algorithm thinkssome Y looks like Gaussian samples of covariance , where 0.5 2. But now, we use theadditional fact that for some Z Y of size m, the robust algorithm on Z nds a covariance . By",
  "d/m. However, the robust algorithm can do better than justestimating up to spectral error c +": "d/m: it can also get an improved Frobenius error. While wewill not formally state the guarantees on the robust algorithm here (see Theorem B.3 for the formalstatement), the main high-level observation is that if the robust estimator can be 1 c times aslarge as the true covariance in only O(1) directions then for an average direction the ratio of to will be 1 O( d/m). We can utilize this observation to bound the volume ratio, using somecareful -net arguments (this is executed in Appendix C.3). Our dimension dependence of d5/3 willincrease to d7/4, though this still improves over the previous d2 bound.We will formally dene the score function S(, X) in Appendix E.1 and fully analyze the appli-cation of the robustness-to-privacy conversion, as outlined here, in Appendix E.2. Accuracy:One important nal step that we have so far neglected is ensuring that any of lowscore must be crudely close to some i, if X is actually drawn from a GMM. We will just focus onthe case where S(, X) = 0, so some Y X of size n/k looks like a set of samples from a Gaussianwith covariance . If the samples Y all came from the i-th component of the GMM, then it willnot be dicult to show is similar to i. The more dicult case is when data point in Y aregenerated from several components.However, if n 20k2d, then |Y| 20kd, which means that by the Pigeonhole Principle, atleast 20d points in Y come from the same mixture component (i, i). We are able to prove that,with high probability over samples drawn from a single Gaussian component N(i, i), that theempirical covariance of any subset of size at least 20d is crudely close to i (see Corollary B.5). Asa result, when verifying that a subset Y looks like i.i.d. Gaussian samples with covariance , wecan ensure that the empirical covariance of every subset Z Y of size 20d is crudely close to .Thus, if the score S(, X) = 0, is close to , which is crudely close to some i.We also formally analyze the accuracy in Appendix E.2. Putting everything together:To crudely learn a single Gaussian component with (, )-DP,we will need n O(d7/4k/) samples to nd some covariance with low score, and we also needn O(k2d) so that a covariance of low score is actually a crude approximation to one of the realmixture components. To crudely approximate all components, we learn each Gaussian componentwith (/",
  "+ d2k": ". Combining these terms will givethe nal sample complexity.We remark that the sample complexity obtained above is actually better than the complexityin Theorem 1.4. There are two reasons for this. The rst is that we have been assuming eachcomponent has weight 1/k, meaning it contributes about n/k data points. In reality, the weightsmay be arbitrary and thus some components may have much fewer data points. However, it turnsout that one can actually ignore any component with less than /k weight, if we want to solvedensity estimation up to total variation distance .This will multiply the sample complexity",
  "Roadmap": "In Appendix A, we note some general preliminary results. In Appendix B, we note some additionalpreliminaries on robust learning of a single Gaussian. In Appendix C, we discuss the robustness-to-privacy conversion and prove some volume arguments needed for Theorem 1.4. In Appendix D,we explain how to reduce to the crude approximation setting, using private hypothesis selection.In Appendix E, we design and analyze the algorithm for multivariate Gaussians, and prove Theo-rem 1.4. In Appendix F, we design and analyze the algorithm for univariate Gaussians, and proveTheorem 1.5. In Appendix G, we prove Theorem 1.6. Finally, Appendix H proves some auxiliaryresults that we state in Appendices B and C.",
  "[AAL24b]Mohammad Afzali, Hassan Ashtiani, and Christopher Liaw. Mixtures of gaussians areprivately learnable with a polynomial number of samples. In Algorithmic LearningTheory, pages 127. PMLR, 2024": "[ABDH+20] Hassan Ashtiani, Shai Ben-David, Nicholas JA Harvey, Christopher Liaw, AbbasMehrabian, and Yaniv Plan. Near-optimal sample complexity bounds for robust learn-ing of gaussian mixtures via compression schemes.Journal of the ACM (JACM),67(6):142, 2020. [ABH+18]Hassan Ashtiani, Shai Ben-David, Nicholas Harvey, Christopher Liaw, Abbas Mehra-bian, and Yaniv Plan. Nearly tight sample complexity bounds for learning mixtures ofgaussians via sample compression schemes. Advances in Neural Information ProcessingSystems, 31, 2018.",
  "[AD20b]Hilal Asi and John C Duchi. Near instance-optimality in dierential privacy. CoRR,abs/2005.10630, 2020": "[ADLS17]Jayadev Acharya, Ilias Diakonikolas, Jerry Li, and Ludwig Schmidt. Sample-optimaldensity estimation in nearly-linear time. In Proceedings of the Twenty-Eighth AnnualACM-SIAM Symposium on Discrete Algorithms, pages 12781289. SIAM, 2017. [AKT+23]Daniel Alabi, Pravesh K Kothari, Pranay Tankala, Prayaag Venkat, and Fred Zhang.Privately estimating a gaussian: Ecient, robust, and optimal. In Proceedings of the55th Annual ACM Symposium on Theory of Computing, pages 483496, 2023.",
  "[AUZ23]Hilal Asi, Jonathan Ullman, and Lydia Zakynthinou. From robustness to privacy andback.In International Conference on Machine Learning, pages 11211146. PMLR,2023": "[BBC+23]Shai Ben-David, Alex Bie, Clment L. Canonne, Gautam Kamath, and Vikrant Singhal.Private distribution learning with public data: The view from sample compression. InAdvances in Neural Information Processing Systems (NeurIPS), 2023. [BDJ+22]Ainesh Bakshi, Ilias Diakonikolas, He Jia, Daniel M Kane, Pravesh K Kothari, andSantosh S Vempala. Robustly learning mixtures of k arbitrary gaussians. In Proceedingsof the 54th Annual ACM SIGACT Symposium on Theory of Computing, pages 12341247, 2022.",
  "[BSKW19]Mark Bun, Thomas Steinke, Gautam Kamath, and Zhiwei Steven Wu. Private hypoth-esis selection. Advances in Neural Information Processing Systems, 32, 2019": "[BUV14]Mark Bun, Jonathan Ullman, and Salil Vadhan. Fingerprinting codes and the priceof approximate dierential privacy.In Proceedings of the forty-sixth annual ACMsymposium on Theory of computing, pages 110, 2014. [CCAd+23] Hongjie Chen, Vincent Cohen-Addad, Tommaso dOrsi, Alessandro Epasto, JacobImola, David Steurer, and Stefan Tiegel. Private estimation algorithms for stochasticblock models and mixture models. arXiv preprint arXiv:2301.04822, 2023. [CDSS14]Siu-On Chan, Ilias Diakonikolas, Rocco A Servedio, and Xiaorui Sun. Ecient densityestimation via piecewise polynomial approximation. In Proceedings of the forty-sixthannual ACM symposium on Theory of computing, pages 604613, 2014. [CKM+21]Edith Cohen, Haim Kaplan, Yishay Mansour, Uri Stemmer, and Eliad Tsfadia.Dierentially-private clustering of easy instances. In International Conference on Ma-chine Learning, pages 20492059. PMLR, 2021.",
  "[DK22]Ilias Diakonikolas and Daniel M. Kane. Algorithmic High-Dimensional Robust Statis-tics. Cambridge University Press, 2022": "[DKM+06]Cynthia Dwork, Krishnaram Kenthapadi, Frank McSherry, Ilya Mironov, and MoniNaor. Our data, ourselves: Privacy via distributed noise generation. In Advances inCryptology-EUROCRYPT 2006: 24th Annual International Conference on the Theoryand Applications of Cryptographic Techniques, St. Petersburg, Russia, May 28-June 1,2006. Proceedings 25, pages 486503. Springer, 2006. [DKS17]Ilias Diakonikolas, Daniel Kane, and Alistair Stewart. Statistical query lower boundsfor robust estimation of high-dimensional gaussians and gaussian mixtures. In 58thIEEE Annual Symposium on Foundations of Computer Science, FOCS 17, pages 7384, 2017.",
  "[DKY17]Bolin Ding, Janardhan Kulkarni, and Sergey Yekhanin.Collecting telemetry dataprivately. In Advances in Neural Information Processing Systems (NeurIPS), pages35713580, 2017": "[DLS+17]Aref N. Dajani, Amy D. Lauger, Phyllis E. Singer, Daniel Kifer, Jerome P. Reiter, Ash-win Machanavajjhala, Simson L. Garnkel, Scot A. Dahl, Matthew Graham, VisheshKarwa, Hang Kim, Philip Lelerc, Ian M. Schmutte, William N. Sexton, Lars Vilhuber,and John M. Abowd. The modernization of statistical disclosure limitation at the u.s.Census Bureau, 2017. In the September 2017 meeting of the Census Scientic AdvisoryCommittee, 2017. [DMNS06]Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise tosensitivity in private data analysis. In Theory of Cryptography: Third Theory of Cryp-tography Conference, TCC 2006, New York, NY, USA, March 4-7, 2006. Proceedings3, pages 265284. Springer, 2006.",
  "[DR14]Cynthia Dwork and Aaron Roth. The algorithmic foundations of dierential privacy.Foundations and Trends in Theoretical Computer Science, 9(34):211407, 2014": "[EPK14]lfar Erlingsson, Vasyl Pihur, and Aleksandra Korolova. RAPPOR: randomized aggre-gatable privacy-preserving ordinal response. In Proceedings of the 2014 ACM SIGSACConference on Computer and Communications Security (CCS), pages 10541067, 2014. [GDGK20]Quan Geng, Wei Ding, Ruiqi Guo, and Sanjiv Kumar.Tight analysis of privacyand utility tradeo in approximate dierential privacy. In International Conference onArticial Intelligence and Statistics (AISTATS), volume 108 of Proceedings of MachineLearning Research, pages 8999. PMLR, 2020. [GVV22]Aparna Gupte, Neekon Vafa, and Vinod Vaikuntanathan. Continuous LWE is as hardas LWE & applications to learning gaussian mixtures. In 63rd IEEE Annual Symposiumon Foundations of Computer Science, FOCS 2022, Denver, CO, USA, October 31 -November 3, 2022, pages 11621173. IEEE, 2022.",
  "[HKMN23]Samuel B Hopkins, Gautam Kamath, Mahbod Majid, and Shyam Narayanan. Robust-ness implies privacy in statistical estimation. In Symposium on Theory of Computing,pages 497506, 2023": "[KMS22a]Gautam Kamath, Argyris Mouzakis, and Vikrant Singhal.New lower bounds forprivate estimation and a generalized ngerprinting lemma.In Advances in NeuralInformation Processing Systems 35, NeurIPS 22, 2022. [KMS+22b] Gautam Kamath, Argyris Mouzakis, Vikrant Singhal, Thomas Steinke, and JonathanUllman. A private and computationally-ecient estimator for unbounded gaussians.In Conference on Learning Theory, pages 544572. PMLR, 2022.",
  "[KMV22]Pravesh Kothari, Pasin Manurangsi, and Ameya Velingker. Private robust estimationby stabilizing convex relaxations. In Conference on Learning Theory, pages 723777.PMLR, 2022": "[KSSU19]Gautam Kamath, Or Sheet, Vikrant Singhal, and Jonathan Ullman. Dierentiallyprivate algorithms for learning mixtures of separated gaussians. Advances in NeuralInformation Processing Systems, 32, 2019. [KV18]Vishesh Karwa and Salil Vadhan. Finite sample dierentially private condence inter-vals. In Proceedings of the 9th Conference on Innovations in Theoretical Computer Sci-ence, ITCS 18, pages 44:144:9, Dagstuhl, Germany, 2018. Schloss DagstuhlLeibniz-Zentrum fuer Informatik.",
  "2k log 1": "+ k (e 1), k + -DP.Moreover, this holds even if the algorithms Ai are adaptive. By this, we mean that for all i 2the algorithm Ai is allowed to depend on A1(X), . . . , Ai1(X). However, privacy must still hold forAi, conditioned on the previous outputs A1(X), . . . , Ai1(X).",
  "d,, i.e., there is no additional Frobenius normcondition beyond what is already imposed by the operator norm condition": "Although ,, is neither symmetric nor transitive, symmetry and transitivity happen in anapproximate sense. Namely, the following result holds: we defer the proof to Appendix H. Weremark that similar results are known (e.g., [AL22, Lemma 3.2], [AAL23, Lemma 4.1]). Proposition B.2 (Approximate Symmetry and Transitivity). Fix any , , > 0 such that 0.1. Then, for any (1, 1) ,, (2, 2), we have that (2, 2) 2,2,2 (1, 1), and for any(1, 1) ,, (2, 2) and (2, 2) ,, (3, 3), we have that (1, 1) 4,4,4 (3, 3). We note the following theorem about the accuracy of robustly learning a single Gaussian. Whilethis result will roughly follow from known results and techniques in the literature, we were unableto nd a formal statement of the theorem, so we prove it in Appendix H.",
  "B.1Directional bounds": "In this section, we note that, when given i.i.d. samples from a Gaussian, with high probability onecannot choose a reasonably large subset with an extremely dierent empirical mean or covariance.First, we note the following proposition, for which the proof follows by an -net type argument. Proposition B.4. Let n n 20d, and L C1n2 for a suciently large constant C1. Supposethat some data points X = {X1, . . . , Xn} are i.i.d. sampled from N(0, I). Then, with probability atleast 1 n(n), the following both hold.",
  "L for all Xi Y": "Proof. If Xi2 L, then XMX L2 for M the dd identity matrix I. However, E[XMX] = d.So, by Lemma A.7, the probability of this event is at most 2ecmin(n4/d,n2) 2ecn2 n(n).Next, we bound the probability that the rst item holds but the second item doesnt hold. First,we can create a1",
  "L.In other words, if the rst event holds and the second does not, there exists v, r in the net andY X of size n, such that |Xi, v r| 3": "L for all Xi Y. We can now bound this via a unionbound. To perform the union bound, we rst bound the probability of this event holding for somexed v, r, Y.For any xed v, r, Y, note that Xi, vXiY is just n i.i.d. copies of N(0, 1). Lets call thesevalues z1, . . . , zn. If there is some r such that |zi r| 3",
  ". For any xed , , with probability at least 1 O() over Y1, . . . , Ym N(, ), A(Y) =andA(Y) C0,C0,C0 (, )": "3. Fix an integer k 1, and additionally suppose that m 40d k.Suppose that X ={X1, . . . , Xn} are drawn i.i.d. from some mixture of k Gaussians (i, i). Then, with probabil-ity at least 1 O() over X, for every Y X of size m and every Y with dH(Y, Y) m/2,either A(Y) =, or A(Y) = (, ), where there is some i [k] such that1",
  "As in Theorem B.3, A has knowledge of , , , , but not or": "Proof. The algorithm A works as follows. Start o by computing A0(Y) = (, ), where A0 is thealgorithm of Theorem B.3. We can run the algorithm on any arbitrary dataset, even if it didntcome from Gaussian samples, and we can assume that the algorithm A0 always outputs some mean-covariance pair (for instance, by having it output (0, I) by default instead of ). Next, A, on adataset Y, checks if any of the following three conditions hold.",
  "If any of these conditions hold, the output is A(Y) =. Otherwise, the output is A(Y) = A0(Y)": "We now verify that the algorithm satises the required properties. Property 1 clearly holds,because if there exist datasets Y, Y with dH(Y, Y) m, A(Y) = and A(Y) =, andA(Y) 8C0,8C0,8C0 A(Y), then we would have in fact set A(Y) to .For Property 2, note that with probability 1 O(), A0(Y) satises the desired property byTheorem B.3, since ed so d + log 1/ = O(d). So, we just need to make sure that we dont setA(Y) to be . Since Y1, . . . , Ym N(, ), then with probability 1 O(), both Y and any Y with dH(Y, Y) n satisfy the conditions of Theorem B.3. In other words, A0(Y) C0,C0,C0and A0(Y) C0,C0,C0. So, by Proposition B.2, A0(Y) 8C0,8C0,8C0 A0(Y) for any Y withdH(Y, Y) n. Thus, condition a) is not met.Moreover, by Corollary B.5, with failure probability at most m(20d) , 1/2(Yi)2 Lfor all Yi Y, and for all Z Y of size 20d, there does not exist a real r and a nonzero vectorv such that |Yi, v r| 1",
  "L 1/2v2, then |Yi, v r| 1": "L 1/2v2, for all Yi Y. Thus, conditionsb) and c) are also not met, so Property 2 holds.For Property 3, note that if m 40d k, then |Y X| 20d k, i.e., Y contains at least 20dkuncorrupted points. So, by the Pigeonhole Principle, for every possible Y, there is some indexi [k] such that at least 20d points in Y X come from the ith Gaussian component.Now, let us condition on the event that Corollary B.5 holds for X, where n = 20d, whichhappens with at least 1 n(n) 1 probability. We claim that for every possible Y, andfor an i [k] such that 20d points Z Y X came from the ith Gaussian component, then eitherA(Y) = or if A(Y) = (, ) then1",
  "L": "viv which is impossible by Part 2 ofCorollary B.5Next, we verify that 1/2i( i)2 10L3.By Triangle inequality, for every Yi Z,1/2i(Yi )2 + 1/2i(Yi i)2 1/2i(i )2. But, 1/2i(Yi i)2 L by Part 1of Corollary B.5, and 1/2i(Yi )2 3L2 1/2(Yi )2 3L2 3L 9L3, because we justproved that 9L4i and assuming we do not reject due to condition b). Thus, 1/2i(i)2 9L3 + L 10L3.",
  "C.1Normalized Volume": "Given a pair (, ), where Rd and Rdd is positive denite, we will dene Proj(, ) Rd(d+3)/2 to represent the coordinates of along with the upper-diagonal coordinates of . Forany set of mean-covariance pairs (, ), dene Proj() := {Proj(, ) : (, ) }. Because is symmetric, Proj(, ) fully encodes the information about and . We also dene vol() to bethe Lebesgue measure of Proj(), i.e., the d(d+3)",
  "Lemma C.1.[MH08, Theorem 11.1.5] For J as dened above, det(J) = det(A)d+1": "Now, for any xed Rd and positive denite Rdd, consider the transformation (, ) (1/2 1/2, 1/2 + ), viewed as a linear map g from Proj(, ) Proj(1/2 + , 1/2 1/2).By setting A := 1/2, note that the map g behaves like f on the last d(d+1) 2coordinates, and onthe rst d coordinates, it is simply an ane map 1/2 + . Therefore, the overall linear mapg has determinant det(1/2) det(1/2)d+1 = (det )(d+2)/2.From this, we can infer the following.",
  "p()d, where p 0 is some nonnegative Lebesgue-measurablefunction and the integration is Lebesgue integration.In our application, we will think of =Proj(, ), and p() = (det )(d+2)/2, to match with (1)": "Theorem C.3. Restatement of [HKMN23, Lemma 4.2]Let 0 < < 0.1 and 10 1 bexed parameters. Also, x privacy parameters 0, 0 < 1 and condence parameter 0 < 1. Let S =S(, X) R0 be a score function that takes as input a dataset X = {X1, . . . , Xn} and a parameter RD. For any dataset X and any 0 1, let V(X) be the D-dimensional normalizedvolume of points with score at most n, i.e., V(X) = voln({ : S(, X) n}).Suppose the following properties hold:",
  ", then M(X)": "outputs some of score at most 2n with probability 1 0.We remark that the algorithm M is allowed prior knowledge of n, D, , , 0, 0, 0, as well asthe domain , function p() that dictates the normalized volume, and score function S. We remark that the original result in [HKMN23] assumes the volumes are unnormalized, butthe result immediately generalizes to normalized volumes. To see why, consider a modied domain RD+1, where = (, z) if and only if and 0 z p(). Also, consider a modiedscore function S acting on , where S((, z), X) := S(, X) for for any (, z) .Then, note that the unnormalized volume of , by Fubinis theorem, is precisely",
  "p()d, which is precisely the normalized volume of corresponding to the new precisely match": "the normalized volumes corresponding to the old . A similar calculation will give us that the un-normalized volume of points (, z) in with S((, z), X) t equals the normalized volume ofpoints with S(, X) t, for any t.Thus, to privately nd a parameter of low score with respect to S (assuming the normalizedvolume constraints hold), can create and S, and apply the unnormalized version of Theorem C.3to obtain some (, z). Also, if S((, z), X) 2n, then S() = S((, z), X) 2n, and if outputting(, z) is (, )-DP, then so is simply outputting .",
  "d10 -net (in Euclidean distance) of size eO(d log d).Let B0 be this net. The net B will be the set of matrices i=1 iwiwi , where every wi B0, everyi is an integral multiple of1": "d10 , and every |i| . The cardinality of B is at most |B0| (d10) eO(d log d).Now, we show that B is actually a net. For any matrix M R3, we can write M = di=1 ivivi ,where i, vi are the eigenvalues and eigenvectors, respectively, of M. Assume the eigenvalues aresorted so that |i| are in decreasing order. Now, since di=1 2i 2, which means that |i|",
  "Proof. For now, we also assume that = 0 and = I.Let 1 d be decided later, and let B be the net from Lemma C.6, where we set = 2and = 2. Let B1 be a1": "d10 -net (in Euclidean distance) over the d-dimensional unit ball, i.e.,over with 2 1. Note that |B1| eO(d log d), and recall that |B| eO(d log d). Now, for any(2, 2) T2(0, I), we can write (2, 2) 1,1 (1, 1), where 1 Iop 2, 1 IF 2,and 12 2. Now, we can choose some B1 with 1 2 d10 and B B such that(1 I) Bop 22",
  ". This implies that (2, 2) 1+102/": ",61 (, I + B), for some B1and B B. Note that this holds for any (2, 2) T2(0, I).Therefore, recalling that |B1| eO(d log d), and |B| eO(d log d) we can cover T2(0, I) withat most eO(d log d) regions, each of which is the set of (2, 2) 1+102/",
  "2d2 6d eO(d5/3 log d2/32/21)": "Finally, we show how to remove the assumption that = 0 and = I.First, note thatfor any general , , (1, 1) T1(0, I) if and only if (1/21 + , 1/211/2) T1(, ), byLemma C.7. So, by Lemma C.2, voln(T1(, )) = voln(T1(0, I)). Next, (2, 2) 1,1 (1, 1) and(1, 1) 2,2,2 (0, I) if and only if (1/22 + , 1/221/2) 1,1 (1/21 + , 1/211/2) and(1/21 + , 1/211/2) 2,2,2 (, ), by two applications of Lemma C.7. So, by Lemma C.2,voln(T2(, )) = voln(T2(0, I)). Thus, the volume ratios stay the same as well.",
  "DFine Approximation via Hypothesis Selection": "In this section, we prove that if we know a very crude approximation to all of the Gaussian mixturecomponents with suciently large weight, we can privately learn the full density of the Gaussianmixture model up to low total variation distance. This will be useful for proving both Theorem 1.4and Theorem 1.5.We start with the following auxiliary lemma.",
  "Proof. First, assume that = I and = 0. Now, let us consider the set U := U0,I = {(, ) :2 G, 1": "G I GI}. Let G G be a parameter that we will set later. Now, we can look atthe un-normalized volume of U viewed in Rd(d+3)/2 (i.e., we are projecting to the upper-triangularpart of ). First, we claim (using a standard volume argument) that there is a cover B U ofsize (2G)d(d+3), such that for every (, ) U, there is (, ) B with 2 1",
  "G , iop 1": "2G } are disjoint, and are all contained in the set S := {(, ) : 2 2G, op 2G}. S is just a shifting and scaling of Si by a factor of 4(G)2, and thus vol(S) =(4(G)2)d(d+3)/2 vol(Si) = (2G)d(d+3) vol(Si). Because every Si is disjoint and contained in S, thenumber of such indices i is at most (2G)d(d+3).Next, consider any (, ) U and (, ) B with 2 1",
  "G, and = 1/20 1/2, where 1": "G I 0 GI. So, if we consider the map f : (0, 0) (1/20 + , 1/20 1/2), this bijectively maps U to U,. We can also consider the cover B, =f(B), where B is the cover constructed for U. Then, by Lemma C.7, (0, 0) G/G,G/G (0, 0) ifand only if f(0, 0) G/G,G/G f(0, 0).Hence, regardless of the choice of , , for every (, ) U,, there exists (, ) B, with(, ) G/G,G/G (, ). Moreover, |B,| = |B| (2G)d(d+3). Moreover, if (, ) G/G,G/G",
  "Next, we note the following result about dierentially private hypothesis selection": "Theorem D.2.[BSKW19]Let H = {H1, . . . , HM} be a set of probability distributions oversome domain D. There exists an -dierentially private algorithm (with respect to a dataset X ={X1, . . . , Xn}) which has following guarantees.Let D be an unknown probability distribution over D, and suppose there exists a distributionH H such that dTV(D, H) . If n Olog M",
  "Proof. Let =": "k . For each (j, j), let Bj,j be as in Lemma D.1. We dene B = jk Bj,j.Let H be the set of hypotheses consisting of mixtures of up to k Gaussians N(i, i) withweights wi, with every (i, i) B and with every wi an integral multiple of . Note that thenumber of hypothesis M = |H| is at most | B|O(k) (1/)O(k) = (k G",
  "E.1Algorithm": "High-Level Approach:Suppose that the unknown distribution is a GMM with representation{(wi, i, i)}ki=1.Dene to be the set of all feasible mean-covariance pairs (, ), i.e., where Rd and Rdd is positive denite. We also start o with a set = , which will roughly characterizethe region of remaining mean-covariance pairs.At a high level, our algorithm proceeds as follows. We will rst learn a very crude approximation",
  "-DP, we will learn some": "(1, 1) which is vaguely close to some (i, i). We will then remove every (, ) that isvaguely close to this (, ) from our , and then attempt to repeat this process. We will repeat it upto k times, in hopes that every (i, i) is vaguely close to some (j, j). By advanced composition,the full set of {(j, j)} is still (, )-DP.At this point, the remainder of the algorithm is quite simple. Namely, we have some crudeestimate for every (i, i) (it is vaguely close to some (j, j)). Moreover, one can create a nenet of roughly e O(d2) (, )-pairs that cover all mean-covariance pairs vaguely close to each (j, j),since the dimension of (, ) is O(d2). Moreover, the weight vector (w1, . . . , wk) has a ne net ofsize roughly e O(k). As a result, we can reduce the problem to a small set of hypotheses: there areroughly e O(d2) choices for each (i, i), and e O(k) choices for w, for a total of e O(kd2) choices for themixture of Gaussians. We can then apply known results on private hypothesis selection [BSKW19],which will suce.The main diculty in the algorithm is in privately learning some (, ) which is vaguelyclose to some (i, i).We accomplish this task by applying the robustness-to-privacy conver-sion of [HKMN23], along with a carefully constructed score function, which we will describe laterin this section.",
  "C0 and =": "8C0 . We will consider the (deterministic) algorithm A of Lemma B.6, whereA is given parameters , , , 0, that acts on a dataset Z and outputs either or some (, ).Next, for some domain of feasible mean-covariance pairs (, ), we will dene a functionf, which takes as input a dataset Y of size N and a mean-covariance pair (, ) , and outputs",
  "dH(X, X) = t, Y X, |Y| = N, (, ) , (, ), f(Y, , ) = 1.(3)": "Note that (, ) does not need to be in , but it must satisfy (, ) , (, ) for some (, ) .Also, note that it is possible that no matter how one changes the data points, the conditions arenever met (for instance if (, ) , (, ) for any (, ) ). This is not a problem: we willsimply set the score to be + if this occurs.",
  "that (, ) is viewed as n(n+3)": "2-dimensional by only considering the upper-triangular part of . Wealso assume the domain is and the normalized volume is as in (1).Given this algorithm M (which implicitly depends on ), the algorithm works as follows. Wewill actually draw a total of n + n samples (for n as above, and n to be dened later), though westart by just looking at the rst n samples X = {X1, . . . , Xn}. We initially set = , and run thefollowing procedure up to k times, or until the algorithm outputs . For the jth iteration, we useM(X) to compute some prediction (j, j) (or possibly we output ). Assuming we do not output, we remove from every (, ) such that n12 j n12 j and 1/2j( j)2 n12.At the end, we have computed some {(j, j)}kj=1 where 0 k k. If k = 0 we simply output",
  "E.2Analysis of Crude approximation": "In this section, we analyze Lines 717 of Algorithm 2. The main goal is to show that the algorithmis private, and that for samples drawn from a mixture of Gaussians, every component (i, i) withlarge enough weight wi is vaguely close to some (j, j) computed by the algorithm.First, we show that when the samples are actually drawn as a Gaussian mixture, then undersome reasonable conditions, any (, ) close to a true mean-covariance pair has low score. Proposition E.1. Suppose X = {X1, . . . , Xn} is drawn from a Gaussian mixture model, withrepresentation {(wi, i, i)}ki=1. Then, with 1 O(k 0) probability over X, for any set of mean-covariance pairs, for all i [k] such that (i, i) and wi /k, and for all (, ) , (i, i),S((, ), X) = 0. Proof. Fix any i [k] with wi /k. Set = i and = i. Also, let X = X, and Y be arandom subset of size N among the samples in X actually drawn from N(i, i). Note that thenumber of such samples has distribution Bin(n, wi), which for wi",
  "k and n = 2k": "N, is at least Nwith e(N) 0 failure probability.Then, Y is just N i.i.d. samples from N(i, i). So, if we draw a random subset Z of sizem of Y, it has the same distribution as m i.i.d.samples from N(i, i). We apply Part 2 ofLemma B.6 (where we use parameters , , in the application). Note that m Od",
  ", so": "with probability at least 1 O(0) over Z, A(Z) 8C0,8C0,8C0 (i, i). By our setting of , ,this means that A(Z) ,, (i, i).In other words, for each index i Nmand corresponding subset Z of Y, if we let Wi be theindicator that A(Z) ,, (i, i), then P(Wi = 1) O(0). While the values of Wi are notnecessarily independent, by linearity of expectation we have that E[ Wi] Nm O(0), so theprobability that E[ Wi] 1",
  "Proposition E.2. Suppose that t = min, S((, ), X). Then, there exists some , suchthat for all (, ) , (, ), we have that S((, ), X) = t": "Proof. Fix , so that S((, ), X) = t. Then, there is some (, ) and some X, Y suchthat dH(X, X) = t, Y X, |Y| = N, and f(Y, , ) = 1. Thus, for any (, ) , (, ), bydenition, we have that S((, ), X) t. But since t is the minimum possible score, we in facthave equality: S((, ), X) = t for all such (, ).",
  "Next, we want to show that regardless of the dataset (even if not drawn from any Gaussianmixture distribution), the set of points of low score cant have that much volume": "Proposition E.3. Fix a dataset X of size n. Then, the set of , such that S((, ), X) ncan be partitioned into at mostnmregions Si, which is indexed by some (i, i). Moreover, for all(, ) Si, there exists (, ) such that (, ) , (, ) and (, ) 8,8,8 (i, i). Proof. Pick any (, ) with score at most n. Let X, Y, , be such that dH(X, X) n,Y X, |Y| = N, (, ) , (, ), and f(Y, , ) = 1. If we dene Y X of size N to bethe corresponding subset as Y is to X, then dH(Y, Y) n = c2",
  "m andA(Z) ,, (, ).Now, for any xed Z X of size m, if we look at any sets Z, Z of size m and Hammingdistance at most c2": "2 m from Z, then dH(Z, Z) c2 m. So, by Property 1 of Lemma B.6, for everysuch Z and Z with A(Z), A(Z) =, we must have A(Z) ,, A(Z).To complete the proof, we order the subsets Z1, Z2, . . . , Z( nm) of size m in X. For each i nm, if",
  "there exists some Z such that dH(Zi, Z) c2": "2 m and A(Z) =, choose an arbitrary such Z and let(i, i) := A(Z). Otherwise, we do not dene (i, i). Then, for any (, ) of score at most n,there exists (, ), a subset Zi X of size m, and a set Zi of size m such that (, ) , (, ),dH(Zi, Zi) c2",
  "Proof. We apply Part 3 of Lemma B.6, though we use N to replace the value m in Lemma B.6. Weare assuming N 40d k. So, by denition of score, (, ) , (, ) where f(Y, , ) = 1, for": "some Y which can be generated by taking a subset of X of size at most N and altering at mostN/2 elements. Since f(Y, , ) = 1, this means that if (, ) = A(Y) then (, ) ,, (, ).So, by Proposition B.2, (, ) 8,8 (, ). Assuming = = c2 is suciently small, this meansthat (, ) 1/2,1/2 (, ).By Part 3 of Lemma B.6, with probability at least 1 O(0), there exists i [k] such that1",
  "There exists (, ) such that (, ) , (, )": "Proof. We will apply Theorem C.3. We rst need to check the necessary conditions. It follows almostimmediately from the denition of S that S((, ), ) has the bounded sensitivity property withrespect to neighboring datasets, for any xed , , .To check the volume condition, note that for any dataset X of size n, if min, S((, ), X) 0.7n, then by Proposition E.2, there exists , such that S((, ), X) 0.8n for all (, ) ,(, ). Conversely, for any X, by Proposition E.3, the set of (, ) of score at most n can bepartitioned into nmregions Si, indexed by (i, i). Moreover, each Si is contained in the set of(, ) such that there exists (, ) such that (, ) , (, ) 8,8,8 (i, i).Let us use the notation of Lemma C.8, where 1 = , 2 = 8, and 2 = 8. Then, we have thatthe set of points with score at most 0.8n contains T1(, ) for some (, ), but the set of pointswith score at most n is contained in the union of T2(i, i) for at mostnmchoices of (i, i).So, as long as min, S((, ), X) 0.7n, we can apply Lemma C.8 to obtain",
  "we have that M is (0, 0)-DP, by Theorem C.3. This holds by our parameter settings, assumingK is suciently large": "Next, we prove accuracy. Assume that X is regular. We again adopt the notation of Lemma C.8,(with 1 = , 2 = 8, and 2 = 8). By the rst condition of regularity, for all i [k] such that(i, i) and wi /k, every (, ) T1(i, i) satises S((, ), X) = 0. We still have thatthe set of points of score at most n is contained in the union of at mostnmsets T2(i, i). Thus,",
  "k , there exists j such that n12 j i n12 j and 1/2j( j)2 n12": "Proof. The privacy guarantee is immediate by (adaptive) advanced composition. Indeed, the algo-rithm M at each step is (0, 0)-DP, and only depends on X and , which is determined only bythe output of all previous runs of M.Now, lets say that X = {X1, . . . , Xn} is regular, and we run Algorithm 2 on X. Now, after j 0steps of the loop, dene Pj to be the set of indices i [k] such that wi",
  "k and (i, i) j (wherej refers to the set after j steps of the loop are completed). Likewise, dene Qj to be the set ofindices i such that there exists (, ) j with1": "n10 i n10 i and 1/2i( i)2 n10.Note that P0 Q0 = [k], and that Pj Qj always. Moreover, because only shrinks, Pj+1 Pjand Qj+1 Qj always.Now, after some step j < k, we claim that if Pj = , then |Qj+1| |Qj| 1 with failureprobability at most O(0), i.e., Q decreases in size for the next step if P isnt currently empty. Thefailure probability is only over the randomness of M at each step, and holds for any regular datasetX. A union bound says the total failure probability is O(k 0).To see why this holds, note that if some index i Pj, then (i, i) j. So, we can applyLemma E.5. At step j + 1, we nd some (j+1, j+1), with the following properties. First, thereexists an index i [k] with1",
  "O(n8) j+1 i O(n8) j+1 n10 j+1.Also,": "1/2i(j+1 i)2 O(n6), and since i n10 j+1, this means 1/2j+1 (i j+1)2 O(n11) n12. Thus, the algorithm will remove (i, i) from at step j + 1, so i Qj+1.Thus, either Pj is empty, or Qj decreases in size to Qj+1 for each 0 j k + 1. This impliesthat Pk is empty, i.e., for every i [k] such that wi",
  "E.3Summary and Completion of Analysis": "We rst quickly summarize how to put everything together to prove Theorem 1.4. To prove our mainresult, need to ensure that our algorithm is private, uses few enough samples, and accurately learnsthe mixture. Privacy will be simple, as we have shown in Lemma E.6 that the crude approximationis private, and the hypothesis selection is private as shown in Appendix D. The sample complexitywill come from the settings of n and n in lines 3 and 18 of the algorithm, and from our settingof parameters.Finally, we showed in Lemma E.6 that we have found a set of (j, j), of atmost k mean-covariance pairs, such that every true (i, i) of suciently large weight is crudelyapproximated by some (j, j). Indeed, this is exactly the situation for which we can apply theresult on private hypothesis selection (Lemma D.3, based on [BSKW19]).We are now ready to complete the analysis and prove Theorem 1.4. Proof of Theorem 1.4. By Lemma E.6, note that the algorithm up to Line 17 is (, )-DP withrespect to X1, . . . , Xn, and does not depend on Xn+1, . . . , Xn+n. Assuming {(j, j)} from theselines is xed, Lines 1820 are (, )-DP with respect to Xn+1, . . . , Xn+n, by Lemma D.3, and donot depend on X1, . . . , Xn. So, the overall algorithm is (, )-DP.Next, we verify accuracy. Note that by Lemma E.6, and for G = n12, the sets (j, j) thatwe nd satisfy the conditions for Lemma D.3, with failure probability at most O(k 0).Thisprobability is at most 0.1, assuming ed is signicantly larger than k. So, it suces for n, the numberof samples used in Line 18 of Algorithm 2, to satisfy n Od2klog(Gk",
  "F.1Algorithm": "As in the algorithm in Appendix E, we will actually draw a total of n + n samples. We starto by only considering the rst n data points X = {X1, . . . , Xn}, where each Xj R. Now, letY = {Y1, . . . , Yn} be the sorted version of X, i.e., Y1 Y2 Yn, and (X1, . . . , Xn) and(Y1, . . . , Yn) are the same up to permutation. Finally, for each j n1, dene Zj to be the orderedpair (Yj, Yj+1 Yj), and dene Z = Z(X) to be the unordered multiset of Zjs. (Note that Zdepends deterministically on X.)The algorithm now works as follows. Suppose we have data X = {X1, . . . , Xn}. After sortingto obtain Y1, . . . , Yn, let rj = Yj, sj = Yj+1 Yj, and Zj = (rj, sj). So, Z(X) = {(rj, sj)}1jn1,viewed as an unordered set. Note that every sj 0, and the rjs are in nondecreasing order. Wewill create a set of buckets that can be bijected onto Z2. For each Zj = (rj, sj), if sj > 0 we assignZj to the bucket labeled (a, b) Z2 if 2a sj < 2a+1 and b n5 2a rj < (b + 1) n5 2a. If sj = 0,we do not assign Zj to any bucket.For each element e Z2, we keep track of the number of indices j [n 1] with Zj sent to e.In other words, for e = (a, b), we dene the count ce = #{j : 2a sj < 2a+1, b n5 2a rj <(b + 1) n5 2a}. For each ce, we sample an independent draw ge TLap(1, /10, /10), and denece = ce + ge. Finally, we let S = {(i, i)} be the the set of pairs (b n5 2a, 22a) where e = (a, b)satises ce > 100",
  "We now prove that the mechanism (until Line 19) is dierentially private. First, we note thefollowing auxiliary claim": "Lemma F.5. Let X, X be adjacent datasets of size n (i.e., only diering on a single element).Then, the corresponding sets Z = Z(X) and Z = Z(X) dier in distance at most 3, where bydistance we mean that there exists a permutation of the elements in Z and Z, respectively, such thatat most three indices i n 1 satisfy Zi = Zi.",
  "Proof. Note that for the sorted versions Y, Y of X, X, respectively, we can convert from Y to Y": "by removing one data point and adding one more data point, without aecting the order of anyother data points.Suppose we remove some Yj from Y . If j = 1, then this just removes Z1, and if j = n, then thisjust removes Zn1. If j 2, this modies Zj1 and removes Zj. Likewise, if we add some new Yj,this will either add one new ordered pair to Z (if Yj is either the smallest or largest element), orreplace one ordered pair in Z with two new pairs. Therefore, if we remove a Yj and then add a Yj,this will change at most 3 of the ordered pairs in Z.",
  "Lemma F.6. The set S = {(j, j)} of candidate mean-covariance pairs is (, )-DP with respectto X = {X1, . . . , Xn}": "Proof. Let X, X be adjacent datasets of size n. Then, the corresponding sets Z, Z (after a possiblepermutation) dier in 3 elements. Therefore, if we let {ce}eZ2 be the counts for Z and {ce}eZ2 bethe counts for Z, we have that ce ce1 6. Because changing a single count ce by 1 leads to(/10, /10)-DP, overall, the counts {ce} will satisfy (, )-DP. Finally, S is a deterministic functionof the noisy counts ce, and therefore must also be (, )-DP.",
  "Finally, we can incorporate the ne approximation (i.e., Lines 2022 of Algorithm 3) and proveTheorem 1.5": "Proof of Theorem 1.5. By Lemma F.6, the algorithm up to Line 19 is (, )-DP with respect toX1, . . . , Xn, and does not depend on Xn+1, . . . , Xn+n. Assuming S = {(i, i)} from these linesis xed, Lines 2022 are (, )-DP with respect to Xn+1, . . . , Xn+n, by Lemma D.3, and do notdepend on X1, . . . , Xn. So, the overall algorithm is (, )-DP.Next, we verify accuracy. Note that by Lemma F.4, and for G = n10, the sets (i, i) thatwe nd satisfy the conditions for Lemma D.3, with failure probability at most 0.01.Moreover,the size of S = {(i, i)} is at most n, by Lemma F.2. So, because d = 1, it suces for n, thenumber of samples used in Line 20 of Algorithm 3, to satisfy n Oklog(Gkn/)",
  "is already known see [ABH+18].The lower bound of kd2": "will follow from Theorem G.2. To explain how, we consider k distinctGaussians N(i, i), where the means i are known and very far away from each other, and I i 2I are unknown. The overall mixture that we will try to learn is the uniform mixture overN(i, i), i.e., every weight wi = 1/k. By making them very far away from each other, we aremaking learning the full mixture equivalent to learning each component (on average). Namely, evenif we are given the information of which Gaussian each sample comes from, we will need to learn atleast 2/3 of the Gaussians up to total variation distance O(), to learn the full mixture up to total",
  "The lower bound of k log(1/)": "will follow from Theorem G.1. Note that it suces to prove thelower bound in the univariate case. We plant k distinct Gaussians N(i, 1), where the i are veryfar away from each other, i.e., pairwise |i j| (1/)10C. We also assume that 1 = 0 is known,and the remaining i are unknown but we are promised the value of each i up to error (1/)C.The overall mixture will have the rst Gaussian N(0, 1) of weight w1 = 1/c, and the remainingGaussians N(i, 1) each have weight wi = /(c (k 1)). Even if we are given the informationof which Gaussian component each sample comes from, to learn the overall mixture up to error ,we need to learn at least 2/3 of the small-weight components, each up to total variation distanceO(c). Hence, we will need log(1/)",
  "Therefore, G :=": "Rd g(x) dx = 1 .Now two cases are possible either G < 1, or G > 1, otherwise we are done. If G < 1, let h(x) bethe density function corresponding to the following distribution: with probability G take a samplefrom g(x)/G, and with probability 1 G select 0. Then",
  "as desired": "Lemma G.4. Suppose w (0, 1) is xed. Suppose an (, ) dierentially private algorithm existsthat takes n samples from the mixture D = wD1+(1w)D2, and learns D1 in total variation distanceup to error , with success probability 1 . Moreover, assume while sampling it is known whichcomponent the sample is sampled from. Then there exists an (, ) dierentially private algorithmA that takes nw/ samples from D1 and outputs an estimate of D1 up to total variation distance, with success probability 1 . Proof. We can view the sampling procedure of the mixture as sampling a random variable t Bin(n, w), and taking t samples from D1 and nt samples from D2. In order to make an algorithmusing nw/ samples we can take that many samples from D1, and then sample t from Bin(n, w),and run A on n data points constructed as follows: If t is smaller than nw/, use t of the samplestaken from D1 and set the rest to 0, If t is larger than nw/, just run A on all zeroes. Finallyoutput the output of A on this input. Clearly, this would be (, ) dierentially private. We knowthe Algorithm succeeds with probability 1 , over the random coins of the algorithm and therandomness of sampling, if the input is sampled from wD1 + (1 w)D2. From Markovs inequality,we know P[t nw/] 1. Therefore, our constructed sample is drawn i.i.d from wD1+(1w)D2with probability at least 1 , where D2 is the xed 0 distribution. Therefore, there exists an (, )dierentially private algorithm that takes nw/ many samples from D1 and outputs an estimate ofD1 up to total variation distance , with success probability 1 , as desired.",
  "must also satisfy n = ( d2": ").Let is be k distinct vectors in Rd each having 2 distance M from each other, for M to beset later. To see why such a set exists, we can take i = Mie1, where e1 is the rst unit vector.Now consider the following set of Gaussians: Di = N(i, i), where is are known and constructedas above and i unknown. Consider the uniform mixture D over these Gaussians, with weightswi = 1/k. We also assume that when sampling from this distribution we know that which componentthe samples came from. Consider an (, ) dierentially private algorithm A that takes n samplesfrom D and outputs a distribution D such that dTV(D, D) with probability 2/3.Now consider a sample from Di, from standard Gaussian tail bounds we know that at least1 exp(M2/800) fraction of the mass of Di is contained within a ball of radius M/10, around i.Let Bi denote this ball, and note that Bis are disjoint.Let f, fi, f be the probability density functions corresponding to D, Di, D respectively. Assum-ing, we are in the success regime, we can write",
  "Rd\\Bifi(x) dx 300 + 100/k 400": "Now we may apply Lemma G.3, and deduce that given D we can construct probability densityfunctions and distributions Dis such that dTV( Di, Di) 800, for all i G. To recap, so far wehave shown that given an (, ) dierentially private algorithm that takes as inputs samples fromour constructed mixture of Gaussians and outputs a density D that has total variation distance atmost , from the ground truth distribution with success probability 2/3, we can use D to constructdensities Di such that dTV( Di, Di) 800, for 0.99k of the indices i. This implies that there existsa xed index i for which the component Di is learned up to error 800 with success probability2/3 0.01 0.65.Applying Lemma G.4, implies that there must exist an (, ) dierentiallyprivate algorithm that takes 100n/k samples from Di and estimates its density up to total variationdistance 800, with success probability at least 0.6. Therefore, applying Theorem G.2, we concludethat n = (kd2",
  ").Now lets prove the last term k log(1/)": ". We aim to apply Theorem G.1. Let is be k distinctvalues in R, each having distance M from each other, for M (1/)10C to be set later, where 1 = 0.It is easy to see such a set exists. Now consider the following set of Gaussians: Di = N(i, 1),where is are known up to log(1/)C, and 1 = 0 is also known. Consider the mixture D overthese Gaussians, with weights w1 = 1 /c, and wi = /(c(k 1)). We also assume that whensampling from the mixture we know which component each sample comes from. Consider an (, )dierentially private algorithm A that takes n samples from D and outputs a distribution D suchthat dTV(D, D) with probability 2/3.Now consider a sample from Di, from standard Gaussian tail bounds we know that at least1 exp(M2/200) fraction of the mass of Di is contained within a ball of radius M/10, around i.Let Bi denote this ball, and note that Bis are disjoint.Let f, fi, f be the probability density functions corresponding to D, Di, D respectively. Assum-ing, we are in the success regime, similar to the proof of the previous term, we can show that there ex-ists a set G of indices such that |G| 0.99k, and i G :",
  "k 400c": "Now we may apply Lemma G.3, and deduce that given D, we can construct probability densityfunctions and distributions Dis such that dTV( Di, Di) 800c, for all i G. To recap, so far wehave shown that given an (, ) dierentially private algorithm that takes as inputs samples from ourconstructed mixture of Gaussians and outputs density D that has total variation distance at most from the ground truth distribution with success probability 2/3, we can use D to construct densitiesDi such that dTV( Di, Di) 800c, for 0.99k of the indices i. This implies that there exists a xedindex i = 1, for which the component Di is learned up to error 800c, with success probability2/3 0.01 0.65. Applying Lemma G.4, implies that there must exist an (, ) dierentiallyprivate algorithm that takes 100nwi samples from Di and estimates its density up to total variationdistance 800c, with success probability 0.6. Therefore, applying Theorem G.1, and noting thatwi = /(c(k 1)) we conclude that n = (k log(1/)",
  "We now prove Proposition B.2": "Proof. Let J1 := 1/221/21and J2 := 1/231/22.First, assume (1, 1) ,, (2, 2). This means J1J1 Iop and J1J1 IF .We then have that 1/2121/21= J11 (J11 ) = (J1 J1)1.Now, note that J1J1and J1 J1are both symmetric and have the same eigenvalues. If we call these eigenvalues 1, . . . , d, thenthe eigenvalues of 1/2121/21are 11 , . . . , 1d . Now, our assumption (1, 1) ,, (2, 2)implies that 1 i 1 + and (1 i)2 2.This means that, assuming 0.1,1 2 1",
  "1 + 2, and 1 1i2 (1 i)2 2i 2 (1 i)2 = 22": "This means that 1/2121/21op 2 and 1/2121/21F 2.Finally, 1/21(1 2) = J11 1/22(1 2).Because 1/22(1 2) has magnitude atmost by our assumption, J11 1/22(1 2) has magnitude at most the maximum singularvalue of J11times .But every singular value of J11is some 1/2iwhich is at most 2, so1/21(2 1)2 = J11 1/22(1 2)2 2. Next, assume (1, 1) ,, (2, 2) and (2, 2) ,, (3, 3). First, note that 1/2311/23=J2J1J1 J2 = (J2J1)(J2J1). If the eigenvalues of J1J1 are {i} and the eigenvalues of J2J2 are{i}, then the singular values of J1 and J2 are {",
  "i": "1 + . Thus, the singular valuesof J2J1 are between 1 and 1+, which means that the eigenvalues of (J2J1)(J2J1) are between(1 )2 and (1 + )2. Hence, for 0.1, J2J1J1 J2 Iop 4.Assume that i, i are in decreasing order. We now consider the kth largest singular valueof J2J1. If k :=",
  "k is the kth largest singular value of J1 and k :=": "k is the kth largestsingular value of J2, by Corollary A.5 there exist subspaces Vk, V k of dimension d k + 1 suchthat J1v2 kv2 for all v Vk and J2v2 kv2 for all v V k. Therefore, for everyv Vk J11 V k (note that J1 is invertible since J1J1 has all eigenvalues between 1 and 1+) wehave that J2J1v2 kJ1v2 kkv2. Because Vk and J11 V k both have dimension d k + 1,their intersection has dimension at least d 2k + 2. So, there is a subspace of dimension at leastd 2k + 2 such that every v in the subspace has J2J1v2 kk v2.Thus, the (2k 1)th largest singular value of J2J1 is at most kk, so the (2k 1)th largesteigenvalue of J2J1J1 J2 is at most kk. The same argument, looking at the smallest singular values,tells us that the (2k1)th smallest eigenvalue of J2J1J1 J2 is at least dk+1dk+1. Thus, for any t,the tth largest eigenvalue of J2J1J1 J2 is at most (t+1)/2(t+1)/2 and at least (d+t)/2(d+t)/2.Overall, this means that",
  "(J1J1 I2F + J2J2 I2F ) 162,": "where the fourth line uses Fact H.1. Thus, 1/2311/23 IF 4.Finally, note that 1/23(1 3)2 1/23(1 2)2 +1/23(2 3)2 = J21/22(1 2)2 +1/23(2 3)2. By our assumptions, both 1/22(1 2)2 and 1/23(2 3)2 areat most , and J2 has operator norm at most 1.1 2, which means that 1/23(13)2 3.",
  "First, we note a series of known results that will be key to proving the theorem. We start with thebound for robust covariance estimation in spectral error": "Lemma H.2 (e.g., [DK22, Exercise 4.3]). Fix any (0, 0), where 0 < 0.01 is a small universalconstant, and x any (0, 1).There is a (deterministic, inecient) algorithm A1 with thefollowing property. Let Rdd be any covariance matrix, and let X = {X1, . . . , Xn} N(0, ), where n O((d+log(1/))/2). Then, with probability at least 1 over the randomness of X, forany -corruption X = {X1, . . . , Xn} of X, A1(X) outputs 1 such that 1/2 11/2 Iop O(). Importantly, A1 may have knowledge of and , but does not have knowledge of X or .",
  "Next, we prove how to robustly estimate the covariance up to Frobenius error. We start withthe following structural lemma": "Lemma H.3. There exists a universal constant c (0, 0.1) with the following property. Fix any1 k d, and let n O(dk) be a suciently large (i.e., n dk (C3 log(dk))C4 for some absoluteconstants C3, C4). If we sample i.i.d. X = {X1, . . . , Xn} N(0, I), then with probability at least1 ecn over X, for all symmetric matrices P Rdd of rank at most k and Frobenius norm 1,and for all subsets S [n] of size at least (1 c) n, 1",
  "niSXiXi I, P 0.1": "Proof. Consider any xed P Rdd of rank k and Frobenius norm 1, and x any integer m. Forany data points X1, . . . , Xmi.i.d.N(0, I), let X = (X1, . . . , Xm) Rmd be the concatenation ofX1, . . . , Xm, and let Q R(md)(md) be the block matrix",
  "2ec1m": "Now, we draw X1, . . . , Xn N(0, I), and take a union bound over all subsets S [n] of size atleast (1 c)n (with m = |S|) and a union bound over a net of possible matrices P. The numberof options for S is at most icnni (e/c)cn. For P, we can choose a 1/n10-sized net over theFrobenius norm metric (i.e., the distance between two matrices P1, P2 is P1 P2F ) for each ofthe k nonzero eigenvalues and eigenvectors in the unit d-dimensional sphere, which has size at mostn100dk. Therefore, by a union bound, the probability that 1 niSXiXi I, P 0.01 for every|S| (1 c)n and every P in the net is at least 1 (2e/c)cn n100dk ec1n/2.Finally, we consider P outside of the net. For any symmetric P of rank k and Frobenius norm 1,it has Frobenius distance at most 1/n8 from some P in the net. Let us consider the event that everyXi22 10n, which for n d occurs with failure probability at most 2n ec1n by Hanson-Wright.Under this event, XiXi I, P P XiXi IF P P F (10n +",
  "Lemma H.4. Fix any 2": "d. Let k = 4d/2, and let n O(d k) and c (0, 0.1) beas in Lemma H.3. Fix any covariance matrix and let X = {X1 . . . , Xn} N(0, ). Then, withprobability at least 1ecn, for every such that 0.95 1.05 and 1/21/2IF ,for every symmetric matrix P Rdd of rank at most k and Frobenius norm 1, and for every S [n]of size at least (1 c)n,1",
  "n JJ I, P": "Now, note that by our assumptions, 1/21/2 Iop 0.05 and 1/21/2 IF .Thus, by Proposition B.2, 1/21/2 Iop 0.1, and by Proposition B.2 again, applied thereverse direction this time, 1/21/2 IF /2. We just showed JJ Iop 0.1, so byProposition A.6, JPJF 2PF 2. Therefore, 1 niSYiY i I, JPJ 0.2.Conversely, we just showed JJ IF /2.So, if we order the eigenvalues of JJ as1, 2, . . . , d (and the corresponding unit eigenvectors v1, . . . , vd) such that |i1| are in decreasingorder, then di=1(i 1)2 2/4, which means that 4d/2i=1 (i 1)2 1. So, if we choose P to be4d/2i=1 (i 1)21/2 4d/2i=1 (i 1)vivi , we have that PF = 1 and",
  "-corruption X = {X1, . . . , Xn} of X, A2(X) outputs 2 such that 1/2 21/2 IF O().Importantly, A2 may have knowledge of , , and , but does not have knowledge of X or": "Proof. In the case that 2, the claim follows immediately from known results (for instance, it isimplicit from [HKMN23]).Alternatively, assume that 2. In this case, the algorithm works as follows. Assume 0 c/2, where c is the constant in Lemma H.3. First, compute 1 based on Lemma H.2. Notethat (1 O()) 1 (1 + O()) with 1 probability, since the number of samples issuciently large. Now, nd any positive denite and a set T [n] of size at least (1 )n, suchthat:",
  "niS1/2XiXi 1/2 I, P 0.2": "First, we note that = is a feasible choice of . Indeed, the rst condition trivially holds.For the second condition, let T be the subset of uncorrupted data points (i.e., Xi = Xi). Then,for any S T, the data points Xi for i S are the same as Xi, so by Lemma H.3, with 1 probability, for every such S, 1 niS1/2XiXi 1/2 I, P 0.1.Next, we show that every with 1/21/2 IF is infeasible. First, we may assumethat 0.95 1.05, as otherwise, we cannot simultaneously satisfy (1 O()) 1 (1 + O()) 1 and (1 O()) 1 (1 + O()) , assuming c/2 is suciently small.Hence, we just have to verify the infeasibility for every such that 1/21/2 IF and 0.95 1.05. Indeed, for any subset T of size at least (1)n, let S be the uncorruptedpoints in T. Because there are at most n uncorrupted points, |S| (1 2)n. So by Lemma H.4,with 1 probability, for every such S, 1",
  "Given a dataset X with these properties, call it (, )-stable with respect to": "Lemma H.7 (implicit from [DK22, ]). Fix suciently small and = O(). There isa deterministic algorithm A3 that, on a dataset X, outputs such that 2 O(), for any-corruption X of any X that is (, )-mean stable with respect to any Rd. Importantly, A3does not require knowledge of X or .",
  "}n/2i=1, and X = {(X2i1 X2i)/": "2}n/2i=1. Note that X are i.i.d. samples fromN(0, ), and X is at most 2-corrupted.Now, because , X is at most 2 corrupted, so Lemma H.2 on X (replacing with 2)gives us some 1 such that 1/2 11/2 Iop O(), by our assumed bound on the numberof samples. Next, Lemma H.5 on X gives us some 2 such that 1/2 21/2 IF O(),by our assumed bound on the number of samples. So, we can set to be any covariance suchthat 1/2 1 1/2 Iop O() and 1/2 2 1/2 IF O().Note that satisesthese properties, and any that satises these properties must satisfy 1/2 1/2 IF O() and 1/2 1/2 IF O(), by the approximate symmetry and transitivity properties(Proposition B.2).Now, we estimate the mean . Taking the original data X, we compute {1/2Xi}. By stability(Lemma H.7), we know that with 1 probability, {1/2Xi} is (, )-stable with respect to (where we are using the uncorrupted data and the true covariance ). Letting J = 1/21/2, weknow that J has all singular values between 1O() and 1+O(), and that {J1 1/2Xi} is (, )-stable with respect to . Moreover, note that we can write v, 1/2(Xi) = Jv, J1 1/2(Xi), and that 1 O() Jv2 1 + O(). Therefore, {1/2Xi} is (, O())-stable with respectto 1/2, which means that by Lemma H.7, A3 on {1/2Xi} outputs some value such that 1/22 O(). Thus, by setting := 1/2 , we have that 1/2()2 O(), whichmeans that 1/2( )2 = J1 1/2( )2 (1 + O()) 1/2( )2 O().",
  "(1 2)12 (1 2).(5)": "Now, consider replacing 1 with 3 := 1/211/2, 2 with 4 := 1/221/2, 1 with 3 :=1/21 + , and 2 with 4 := 1/22 + . Again, since M being PSD implies AMA is PSD (andvice versa), we have that (1)2 1 (1+)2 if and only if (1)4 3 (1+)4.Moreover, note that"
}