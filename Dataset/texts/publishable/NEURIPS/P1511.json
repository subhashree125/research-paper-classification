{
  "Abstract": "Large-scale training of latent diffusion models (LDMs) has enabled unprecedentedquality in image generation. However, the key components of the best performingLDM training recipes are oftentimes not available to the research community,preventing apple-to-apple comparisons and hindering the validation of progressin the field. In this work, we perform an in-depth study of LDM training recipesfocusing on the performance of models and their training efficiency. To ensureapple-to-apple comparisons, we re-implement five previously published modelswith their corresponding recipes. Through our study, we explore the effects of(i) the mechanisms used to condition the generative model on semantic information(e.g., text prompt) and control metadata (e.g., crop size, random flip flag, etc.) on themodel performance, and (ii) the transfer of the representations learned on smallerand lower-resolution datasets to larger ones on the training efficiency and modelperformance. We then propose a novel conditioning mechanism that disentanglessemantic and control metadata conditionings and sets a new state-of-the-art in class-conditional generation on the ImageNet-1k dataset with FID improvements of7% on 256 and 8% on 512 resolutions as well as text-to-image generation on theCC12M dataset with FID improvements of 8% on 256 and 23% on 512 resolution.",
  "Introduction": "Diffusion models have emerged as a powerful class of generative models and demonstrated unprece-dented ability at generating high-quality and realistic images. Their superior performance is evidentacross a spectrum of applications, encompassing image and video synthesis ,denoising , super-resolution and layout-to-image synthesis . The fundamental principleunderpinning diffusion models is the iterative denoising of an initial sample from a trivial prior distri-bution, that progressively transforms it to a sample from the target distribution. The popularity ofdiffusion models can be attributed to several factors. First, they offer a simple yet effective approachfor generative modeling, often outperforming traditional approaches such as Generative AdversarialNetworks (GANs) and Variational Autoencoders (VAEs) in terms of visualfidelity and sample diversity. Second, diffusion models are generally more stable and less prone tomode collapse compared to GANs, which are notoriously difficult to stabilize without careful tuningof hyperparameters and training procedures .",
  ": Qualitative examples. Images generated using our model trained on CC12M at 512 resolution": "Despite the success of diffusion models, training such models at scale remains computationallychallenging, leading to a lack of insights on the most effective training strategies. Training recipesof large-scale models are often closed (e.g., DALL-E, Imagen, Midjourney), and only a few studieshave analyzed training dynamics in detail . Moreover, evaluation often involves humanstudies which are easily biased and hard to replicate . Due to the high computational costs,the research community mostly focused on the finetuning of large text-to-image models for differentdownstream tasks and efficient sampling techniques . However, there hasbeen less focus on ablating different mechanisms to condition on user inputs such as text prompts,and strategies to pre-train using datasets of smaller resolution and/or data size. The benefits ofconditioning mechanisms are two-fold: allowing users to have better control over the content thatis being generated, and unlocking training on augmented or lower quality data by for exampleconditioning on the original image size and other metadata of the data augmentation. Improvingpre-training strategies, on the other hand, can allow for big cuts in the training cost of diffusionmodels by significantly reducing the number of iterations necessary for convergence. Our work aims to disambiguate some of these design choices, and provide a set of guidelines thatenable the scaling of the training of diffusion models in an efficient and effective manner. Beyondthe main architectural choices (e.g., Unet vs. ViT), we focus on two other important aspects forgenerative performance and efficiency of training. First, we enhance conditioning by decouplingdifferent conditionings based on their type: control metadata conditioning (e.g., crop size, randomflip, etc.), semantic-level conditioning based on class names or text-prompts. In this manner, wedisentangle the contribution of each conditioning and avoid undesired interference among them.Second, we optimize the scaling strategy to larger dataset sizes and higher resolution by studyingthe influence of the initialization of the model with weights from models pre-trained on smallerdatasets and resolutions. Here, we propose three improvements needed to seamlessly transition acrossresolutions: interpolation of the positional embeddings, scaling of the noise schedule, and using amore aggressive data augmentation strategy. In our experiments we evaluate models at 256 and 512 resolution on ImageNet-1k and ConceptualCaptions (CC12M), and also present results for ImageNet-22k at 256 resolution. We study thefollowing five architectures: Unet/LDM-G4 , DiT-XL2 w/ LN , mDT-v2-XL/2 w/ LN ,PixArt--XL/2, and mmDiT-XL/2 (SD3) . We find that among the studied base architectures,mmDiT-XL/2 (SD3) performs the best. Our improved conditioning approach further boosts theperformance of the best model consistently across metrics, resolutions, and datasets. In particular,we improve the previous state-of-the-art DiT result of 3.04 FID on ImageNet-1k at 512 resolutionto 2.76. For CC12M at 512 resolution, we improve FID of 11.24 to 8.64 when using our improvedconditioning, while also obtaining a (small) improvement in CLIPscore from 26.01 to 26.17. See for qualitative examples of our model trained on CC12M. 641282565121024: Influence of control condi-tions. Images generated using the samelatent sample. Top: Model trained withconstant weighting of the size condition-ing as used in SDXL , introducing un-desirable correlations between image con-tent and size condition. Bottom: Modeltrained using our cosine weighting of low-level conditioning, disentangling the sizecondition from the image content. In summary, our contributions are the following: We present a systematic study of five different diffusion architectures, which we train from scratchusing face-blurred ImageNet and CC12M datasets at 256 and 512 resolutions.",
  "Conditioning mechanisms": "Background. To control the generated content, diffusion models are usually conditioned on classlabels or text prompts. Adaptive layer norm is a lightweight solution to condition on class labels, usedfor both UNets and DiT models . Cross-attention is used to allow more fine-grainedconditioning on textual prompts, where particular regions of the sampled image are affected only bypart of the prompt, see e.g. . More recently, another attention based conditioning was proposedin SD3 within a transformer-based architecture that evolves both the visual and textual tokensacross layers. It concatenates the image and text tokens across the sequence dimension, and thenperforms a self-attention operation on the combined sequence. Because of the difference between thetwo modalities, the keys and queries are normalized using RMSNorm , which stabilizes training.This enables complex interactions between the two modalities in one attention block instead of usingboth self-attention and cross-attention blocks. Moreover, since generative models aim to learn the distribution of the training data, data quality isimportant when training generative models. Having low quality training samples, such as the onesthat are poorly cropped or have unnatural aspect ratios, can result in low quality generations. Previouswork tackles this problem by careful data curation and fine-tuning on high quality data, see e.g. .However, strictly filtering the training data may deprive the model from large portions of the availabledata , and collecting high-quality data is not trivial. Rather than treating them as nuisance factors,SDXL proposes an alternative solution where a UNet-based model is conditioned on parameterscorresponding to image size and crop parameters during training. In this manner, the model is awareof these parameters and can account for them during training, while also offering users control overthese parameters during inference. These control conditions are transformed and additively combinedwith the timestep embedding before feeding them to the diffusion model. Disentangled control conditions. Straightforward implementation of control conditions in DiT maycause interference between the time-step, class-level and control conditions if their correspondingembeddings are additively combined in the adaptive layer norm conditioning, e.g. causing changes inhigh-level content of the generated image when modifying its resolution, see . To disentanglethe different conditions, we propose two modifications. First, we move the class embedding to be fedthrough the attention layers present in the DiT blocks. Second, to ensure that the control embedding",
  ": Weighting of low-levelcontrol conditions. The weightis zeroed out early on when im-age semantics are defined, and in-creased later when adding details": "Control conditions can be used for different types of data augmenta-tions: (i) high-level augmentations (h) that affect the image compo-sition e.g. flipping, cropping and aspect ratio , and (ii) low-levelaugmentations (l) that affect low-level details e.g. image resolu-tion and color. Intuitively, high-level augmentations should impactthe image formation process early on, while low-level augmenta-tions should enter the process only once sufficient image details arepresent. We achieve this by scaling the contribution of the low-levelaugmentations, l, to the control embedding using a cosine schedulethat downweights earlier contributions:",
  "where the embedding functions Eh, El are made of sinusoidal em-beddings followed by a 2-layer MLP with SiLU activation, andwhere c is the cosine schedule illustrated in": "Improved text conditioning. Most commonly used text encoders, like CLIP , output a constantnumber of tokens T that are fed to the denoising model (usually T = 77). Consequently, whenthe prompt has less than T tokens, the remaining positions are filled by zero-padding, but remainaccessible via cross-attention to the denoising network. To make better use of the conditioning vector,we propose a noisy replicate padding mechanism where the padding tokens are replaced with copiesof the text tokens, thereby pushing the subsequent cross-attention layers to attend to all the tokens inits inputs. As this might lead to redundant token embeddings, we improve the diversity of the featurerepresentation across the sequence dimension, by perturbing the embeddings with additive Gaussiannoise with a small variance txt. To ensure enough diversity in the token embeddings, we scale theadditive noise by (txt)m 1, where m is the number of token replications needed for padding,and (txt) is the per-channel standard deviation in the token embeddings. Integrating classifier-free guidance. Classifier-free guidance (CFG) allows for training con-ditional models by combining the output of the uncoditional generation with the output of theconditional generation. Formally, given a latent diffusion model trained to predict the noise , CFGreads as: = s + (1 ) , where is the uncoditional noise prediction, s is the noise pre-diction conditioned on the semantic conditioning s (e.g., text prompt), and is the hyper-parameter,known as guidance scale, which regulates the strength of the conditioning. Importantly, duringtraining is set alternatively to 0 or 1, while at inference time it is arbitrarily changed in order to steerthe generation to be more or less consistent with the conditioning. In our case, we propose the controlconditioning to be an auxiliary guidance term, in order to separately regulate the strength of theconditioning on the control variables c and semantic conditioning s at inference time. In particular,we define the guided noise estimate as:",
  "On transferring models pre-trained on different datasets and resolutions": "Background.Transfer learning has been a pillar of the deep learning community, enablinggeneralization to different domains and the emergence of foundational models such as DINO and CLIP . Large pre-trained text-to-image diffusion models have also been re-purposed fordifferent tasks, including image compression and spatially-guided image generation . Here,we are interested in understanding to which extent pre-training on other datasets and resolutionscan be leveraged to achieve a more efficient training of large text-to-image models. Indeed, trainingdiffusion models directly to generate high resolution images is computationally demanding, therefore,it is common to either couple them with super-resolution models, see e.g. , or fine-tune themwith high resolution data, see e.g. . Although most models can directly operate at a higherresolution than the one used for training, fine-tuning is important to adjust the model to the differentstatistics of high-resolution images. In particular, we find that the different statistics influence thepositional embedding of patches, the noise schedule, and the optimal guidance scale. Therefore,we focus on improving the transferability of these components. Positional Embedding. Adapting to a higher resolution can be done in different ways. Interpolationscales the most often learnable embeddings according to the new resolution . Extrapolationsimply replicates the embeddings of the original resolution to higher resolutions as illustrated in ,resulting in a mismatch between the positional embeddings and the image features when switching todifferent resolutions. Most methods that use interpolation of learnable positional embeddings, e.g., adopt either bicubic or bilinear interpolation to avoid the norm reduction associated withthe interpolation. In our case, we take advantage of the fact that our embeddings are sinusoidal andsimply adjust the sampling grid to have constants limit under every resolution, see App. C.",
  ": Interpolation and extrapo-lation of positional embeddings": "Scaling the noise schedule. At higher resolution, the amountof noise necessary to mask objects at the same rate changes . If we observe a spatial patch at low resolution under agiven uncertainty, upscaling the image by a factor s createss2 observations of this patch of the form y(i)t= xt + t(i) assuming the value of the patch is constant across the patch.This increase in the number of observations reduces the un-certainty around the value of that token, resulting in a highersignal-to-noise (SNR) ratio than expected. This issue gets further accentuated when the schedulerdoes not reach a terminal state with pure noise during training, i.e., a zero SNR , as the mismatchbetween the non-zero SNR seen during training and the purely Gaussian initial state of the samplingphase becomes significant. To resolve this, we scale the noise scheduler in order to recover the sameuncertainty for the same timestep.Proposition 1. When going from a scale of s to a scale s, we update the scheduler according tothe following rule",
  ": Low-resolution pre-training. Crop size used for pre-training impacts finetuning": "Pre-training cropping strategies. When pre-training and finetuningat different resolutions, we can either first crop and then resize thecrops according to the training resolution, or directly take differentlysized crops from the training images. Using a different resizingduring pre-training and finetuning may introduce some distributionshift, while using crops of different sizes may be detrimental tolow-resolution training as the model will learn the distribution ofsmaller crops rather than full images, see . We experimentallyinvestigate which strategy is more effective for low-resolution pre-training of high-resolution models. Guidance scale. We discover that the optimal guidance scale for both FID and CLIPScore varieswith the resolution of images. In App. D, we present a proof revealing that under certain conditions,the optimal guidance scale adheres to a scaling law with respect to the resolution, as(s) = 1 + s ( 1).(4)",
  "Experimental setup": "Datasets. In our study, we train models on three datasets. To train class-conditional models, we useImageNet-1k , which has 1.3M images spanning 1,000 classes, as well as ImageNet-22k ,which contains 14.2M images spanning 21,841 classes. Additionally, we train text-to-image modelsusing Conceptual 12M (CC12M) , which contains 12M images with accompanying manuallygenerated textual descriptions. We pre-process both datasets by blurring all faces. Differentlyfrom , we use the original captions for the CC12M dataset.",
  "Evaluation. For image quality, we evaluate our models using the common FID metric. Wefollow the standard evaluation protocol on ImageNet to have a fair comparison with the relevant": ": Comparison between different model architectures. We compare results reported in the literature(top, reporting available numbers) with our reimplementations of existing architectures (middle), and to our bestresults obtained using architectural refinements and improved training. For 512 resolution, we trained models byfine-tuning models pre-trained at 256 resolution. In each column, we bold the best results among those in thefirst two blocks, and also those in the last row when they are equivalent or superior. denotes that numbersare unavailable in the original papers, or architectures are incompatible with text-to-image generation in ourexperiments. indicates diverged runs. * is used for Esser et al. pre-trained on CC12M to denote thatFID is computed differently and some details about their evaluation are unclear.",
  "Our improved architecture and trainingmmDiT-XL/2 (ours)1.592.766.7926.606.276.6926.17": "literature . Specifically, we compute the FID between the full training set and 50ksynthetic samples generated using 250 DDIM sampling steps. For image-text alignment, we computethe CLIP score similarly to . We measure conditional diversity, either using class-levelor text prompt conditioning, using LPIPS . LPIPS is measured pairwise and averaged amongten generations obtained with the same random seed, prompt, and initial noise, but different sizeconditioning (we exclude sizes smaller than the target resolution); then we report the average over10k prompts. In addition to ImageNet and CC12M evaluations, we provide FID and CLIPScore onthe COCO validation set, which contains approximately 40k images with associated captions.For COCO evaluation , we follow the same setting as for computing the CLIP score, using25 sampling steps and a guidance scale of 5.0. Training. To train our models we use the Adam optimizer, with a learning rate of 104 and1, 2 = 0.9, 0.999. When training at 256256 resolution, we use a batch size of 2, 048 images, aconstant learning rate of 10 104, train our models on two machines with eight A100 GPUs each.In preliminary experiments with the DiT architecture we found that the FID metric on ImageNet-1kat 256 resolution consistently improved with larger batches and learning rate, but that increasing thelearning rate by another factor of two led to diverging runs. We report these results in supplementary.When training models at 512512 resolution, we use the same approach but with a batch size of 384distributed over 16 A100 GPUs. We train our ImageNet-1k models for 500k to 1M iterations and for300k to 500k iterations for CC12M. Model architectures. We train different diffusion architectures under the same setting to provide a faircomparison between model architectures. Specifically, we re-implement a UNet-based architecturefollowing Stable Diffusion XL (SDXL) 1 and several transformer-based architectures: vanillaDiT , masked DiT (mDiT-v2) , PixArt DiT (PixArt-) , and multimodal DiT (mmDiT) asin Stable Diffusion 3 . For vanilla DiT, which only supports class-conditional generation, weexplore two variants one incorporating the class conditioning within LayerNorm and another onewithin the attention layer. Also, for text-conditioned models, we use the text encoder and tokenizerof CLIP (ViT-L/14) having a maximum sequence length of T = 77. The final models sharesimilar number parameters, e.g. for DiTs we inspect the XL/2 variant , for UNet (SDXL) weadopt similar size to the original LDM . Similar to , we found the training of DiT with",
  "Evaluation of model architectures and comparison with the state of the art": "In Tab. 1, we report results for models with different architectures trained at both 256 and 512resolutions for ImageNet and CC12M, and compare our results (2nd block of rows) with thosereported in the literature, where available (1st block of rows). Where direct comparison is possible,we notice that our re-implementation outperforms the one of existing references. Overall, we found themmDiT architecture to perform best or second best in all settings compared to other alternatives.For this reason, we apply our conditioning improvements on top of this architecture (last row of thetable), boosting the results as measured with FID and CLIPScore in all settings. Below, we analysethe improvements due to our conditioning mechanisms and pre-training strategies.",
  "Control conditioning": "Scheduling rate of control conditioning. In Tab. 2a we consider the effect of controlling theconditioning on low-level augmentations via a cosine schedule for different decay rates . Wecompare to baselines (first two rows) with constant weighting (as in SDXL ) and without controlconditioning. We find that our cosine weighting schedule significantly reduces the dependencebetween size control and image semantics as it drastically improves the instance specific LPIPS (0.33vs. 0.04) in comparison to uniform weighting. In terms of FID, we observe a small gap with thebaseline (3.04 vs. 3.08), which increases (3.80 vs. 5.04) when computing FID by randomly samplingthe size conditioning in the range , see Tab. 2b. Finally, the improved disentanglingbetween semantics and low-level conditioning is clearly visible in the qualitative samples in . Crop and random-flip control conditioning. A potential issue of horizontal flip data augmentationsis that it can create misalignment between the text prompt and corresponding image. For example theprompt \"A teddy bear holding a baseball bat in their right arm\" will no longer be accurate whenan image is flipped showing a teddy bear holding the bat in their left arm. Similarly, croppingimages can remove details mentioned in the corresponding caption. In Tab. 2c we evaluate modelstrained on CC12M@256 with and without horizontal flip conditioning, and find that adding thisconditioning leads to slight improvements in both FID and CLIP as compared to using only cropconditioing. We depict qualitative comparison in , where we observe that flip conditioningimproves prompt-layout consistency. Inference-time control conditioning of image size. High-level augmentations (h) may affectthe image semantics. As a result they influence the learned distribution and modify the generationdiversity. For example, aspect ratio conditioning can harm the quality of generated images, whenimages of a particular class or text prompt are unlikely to appear with a given aspect ratio. In Tab. 2bwe compare of different image size conditionings for inference. We find that conditioning on the samesize distribution as encountered during the training of the model yields a significant boost in FID",
  "zero7.1926.25replicate06.9326.47replicate0.026.7926.60replicate0.056.8226.58replicate0.17.0126.47replicate0.27.0226.41": "Control conditioning and guidance. To understand how controlcondition impacts the generation process, we investigate the influ-ence of control guidance (introduced in Sec. 2.1 ) on FID andreport the results in . We find that a higher control guidancescale results in improved FID scores. However, note that this im-provement comes at the cost of compute due to the additional controlterm c,s. Replication text padding.We compare our noisy replicationpadding to the baseline zero-padding in Tab. 3. We observe thatusing a replication padding improves both FID and CLIP score, andthat adding scaled perturbations further improves the results 0.35point improvement in CLIP score and 0.4 point improvement in FID.",
  "Transferring weights between datasets and resolutions": "Dataset shift. We evaluate the effect of pre-training on ImageNet-1k (at 256 resolution) whentraining the models on CC12M or ImageNet-22k (at 512 resolution) by the time needed to achievethe same performance as a model trained from scratch. In Tab. 4a, when comparing models trainedfrom scratch to ImageNet-1k pre-trained models (600k iterations) we observe two benefits: improvedtraining convergence and performance boosts. For CC12M, we find that after only 100k iterations,both FID and CLIP scores improve over the baseline model trained with more than six times the : Effect of pre-training across datasets and res-olutions. Number of (pre-)training iterations given inthousands (k) per row. Relative improvements in FID andCLIP score given as percentage in parenthesis.",
  "(c) Influence of pretraining scaleon convergence": "amount of training iterations. For ImageNet-22k, which is closer in distribution to ImageNet-1k thanCC12M, the gains are even more significant, the finetuned model achieves an FID lower by 0.5 pointafter only 80k training iterations. In Tab. 4b, we study the relative importance of pre-training vsfinetuning when the datasets have dissimilar distributions but similar sample sizes. We fix a trainingbudget in terms of number of training iterations N, we first train our model on ImageNet-22k for Kiterations before continuing the training on CC12M for the remaining N K iterations. We see thatthe model pretrained for 200k iterations and finetuned for 150k performs better than the one spendingthe bulk of the training during pretraining phase. This validates the importance of domain specifictraining and demonstrates that the bulk of the gains from the pretrained checkpoint come from therepresentation learned during earlier stages. Resolution change. We compare the performance boost obtained from training from scratch at 512resolution vs. resuming from a 256-resolution trained model. According to our results in Tab. 4c,pretraining on low resolution significantly boosts the performance at higher resolutions, both forUNet and mmDiT, we find that higher resolution finetuning for short periods of time outperformshigh resolution training from scratch by a large margin ( 25%). These performance gains might inpart be due to the increased batch size when pre-training the 256 resolution model, which allows themodel to see more images as compared to training from-scratch at 512 resolution. Positional Embedding. In a, we compare the influence of the adjustment mechanism forthe positional embedding. We find that our grid resampling approach outperforms the defaultextrapolation approach, resulting in 0.2 point difference in FID after 130k training iterations. Scaling the noise schedule. We conducted an evaluation to ascertain the influence of the noiseschedule by refining our mmDiT model post its low resolution training and report the results inb. Remarkably, the application of the rectified schedule, for 40k iterations, resulted in animprovement of 0.7 FID points demonstrating its efficacy at higher resolutions. Pre-training cropping strategies. During pretraining, the model sees objects that are smallerthan what it sees during fine tuning, see . We aim to reduce this discrepancy by adoptingmore agressive cropping during the pretraining phase. We experiment with three cropping ratiosfor training: 0.9 1 (global), 0.4 0.6 (local), 0.4 1 (mix). We report the results in c. OnImageNet1K@256, the pretraining FID scores are 2.36, 245.55 and 2.21 for the local, global andmixed strategies respectively. During training at 512 resolution, we observe that the global and mixcropping strategies both outperform the local strategy. However, as reported in c, the localstrategy provides benefits at higher resolutions. Overall, training with the global strategy performsthe best at 256 resolution but lags behind for higher resolution adaptation. While local croppingunderperforms at lower resolutions, because it does not see any images in their totality, it outperformsthe other methods at higher resolutions an improvement of almost 0.2 FID points is consistent afterthe first 50k training steps at higher resolution.",
  "Conclusion": "In this paper, we explored various approaches to enhance the conditional training of diffusionmodels. Our empirical findings revealed significant improvements in the quality and control overgenerated images when incorporating different coditioning mechanisms. Moreover, we conducted a comprehensive study on the transferability of these models across diverse datasets and resolutions.Our results demonstrated that leveraging pretrained representations is a powerful tool to improve themodel performance while also cutting down the training costs. Furthermore, we provided valuableinsights into efficiently scaling up the training process for these models without compromisingperformance. By adapting the schedulers and positional embeddings when scaling up the resolution,we achieved substantial reductions in training time while boosting the quality of the generated images.Additional experiments unveil the expected gains from different transfer strategies, making it easierfor researchers to explore new ideas and applications in this domain. In Appendix B we discusssocietal impact and limitations of our work. Omri Avrahami, Thomas Hayes, Oran Gafni, Sonal Gupta, Yaniv Taigman, Devi Parikh, Dani Lischinski,Ohad Fried, and Xi Yin. Spatext: Spatio-textual representation for controllable image generation. In CVPR,2023. Lucas Beyer, Pavel Izmailov, Alexander Kolesnikov, Mathilde Caron, Simon Kornblith, Xiaohua Zhai,Matthias Minderer, Michael Tschannen, Ibrahim Alabdulmohsin, and Filip Pavetic. FlexiViT: One modelfor all patch sizes. In CVPR, 2023.",
  "Jaemin Cho, Abhay Zala, and Mohit Bansal. DALL-Eval: Probing the reasoning skills and social biases oftext-to-image generation models. In ICCV, 2023": "Xiaoliang Dai, Ji Hou, Chih-Yao Ma, Sam Tsai, Jialiang Wang, Rui Wang, Peizhao Zhang, SimonVandenhende, Xiaofang Wang, Abhimanyu Dubey, Matthew Yu, Abhishek Kadian, Filip Radenovic, DhruvMahajan, Kunpeng Li, Yue Zhao, Vladan Petrovic, Mitesh Kumar Singh, Simran Motwani, Yi Wen, YiwenSong, Roshan Sumbaly, Vignesh Ramanathan, Zijian He, Peter Vajda, and Devi Parikh. Emu: Enhancingimage generation models using photogenic needles in a haystack. arXiv preprint, 2309.15807, 2023.",
  "Prafulla Dhariwal and Alex Nichol. Diffusion models beat GANs on image synthesis. In NeurIPS, 2021": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, ThomasUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, andNeil Houlsby. An image is worth 1616 words: Transformers for image recognition at scale. In ICLR,2021. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Mller, Harry Saini, Yam Levi,Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey,Alex Goodwin, Yannik Marek, and Robin Rombach. Scaling rectified flow transformers for high-resolutionimage synthesis. In ICML, 2024.",
  "William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, 2023": "Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Mller, Joe Penna,and Robin Rombach. SDXL: Improving latent diffusion models for high-resolution image synthesis. InICLR, 2024. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, GirishSastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learningtransferable visual models from natural language supervision. In ICML, 2021.",
  "Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net: Convolutional networks for biomedicalimage segmentation. In Medical Image Computing and Computer-Assisted Intervention, 2015": "Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet LargeScale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3):211252,2015. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar SeyedGhasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho,David J Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep languageunderstanding. In NeurIPS, 2022.",
  "ARelated work": "Diffusion Models. Diffusion models have gained significant attention in recent years due to theirability to model complex stochastic processes and generate high-quality samples. These models havebeen successfully applied to a wide range of applications, including image generation ,video generation , music generation , and text generation . One of the earliest diffusionmodels was proposed in , which introduced denoising diffusion probabilistic models (DDPMs)for image generation. This work demonstrated that DDPMs can generate high-quality images thatcompetitive with state-of-the-art generative models such as GANs . Following this work, severalvariants of DDPMs were proposed, including score-based diffusion models , conditional diffusionmodels , and implicit diffusion models . Overall, diffusion models have shown promisingresults in various applications due to their ability to model complex stochastic processes and generatehigh-quality samples . Despite their effectiveness, diffusion models also have somelimitations, including the need for a large amount of training data and the required computationalresources. Some works have studied and analysed the training dynamics of diffusion models,but most of this work considers the pixel-based models and small-scale settings with limited imageresolution and dataset size. In our work we focus on the more scalable class of latent diffusionmodels , and consider image resolutions up to 512 pixels, and 14M training images. Model architectures. Early work on diffusion models adopted the widely popular UNet arcchitec-ture . The UNet is an encoder-decoder architecture where the encoder is made of residualblocks that produce progressively smaller feature maps, and the decoder progressively upsamples thefeature maps and refines them using skip connections with the encoder . For diffusion, UNetsare also equipped with cross attention blocks for cross-modality conditioning and adaptive layernormalization that conditions the outputs of the model on the timestep . More recently, visiontransformer architectures were shown to scale more favourably than UNets for diffusion modelswith the DiT architecture . Numerous improvements have been proposed to the DiT in orderto have more efficient and stable training, see e.g. . In order to reduce the computationalcomplexity of the model and train at larger scales, windowed attention has been proposed . Latentmasking during training has been proposed to encourage better semantic understanding of inputsin . Others improved the conditioning mechanism by evolving the text tokens through the layersof the transformer and replacing the usual cross-attention used for text conditioning with a variantthat concatenates the tokens from both the image and text modalities . Large scale diffusion training. Latent diffusion models unlocked training diffusion models athigher resolutions and from more data by learning the diffusion model in the reduced latent spaceof a (pre-trained) image autoencoder rather than directly in the image pixel space. Follow-up workhas proposed improved scaling of the architecture and data . More recently, attention-basedarchitectures have been adapted for large scale training, showing even more improvementsby scaling the model size further and achieving state-of-the-art performance on datasets such as timestep 0.0 0.2 0.4 0.6 0.8 1.0 t(s) s=1s=2s=3s=4 timestep 0.0 0.2 0.4 0.6 0.8 1.0 t(s) 1 +t(s)2 s=1s=2s=3s=4 Figure A1: Noise schedulescaling law. At higher resolu-tions, keeping the same uncer-tainty means spending moretime at higher noise levels,thereby counteracting the un-certainty reduction from the in-crease in the observations forthe same patch.",
  "BSocietal impact and limitations": "Our research investigates the training of generative image models, which are widely employed togenerate content for creative or education and accessibility purposes. However, together with thesebeneficial applications, these models are usually associated with privacy concerns (e.g., deepfakegeneration) and misinformation spread. In our paper, we deepen the understanding of the trainingdynamics of these modes, providing the community with additional knowledge that can be lever-aged for safety mitigation. Moreover, we promote a safe and transparent/reproducible research byemploying only publicly available data, which we further mitigate by blurring human faces. Our work is mostly focused on training dynamics, and to facilitate reproducibility, we used publiclyavailable datasets and benchmarks, without applying any data filtering. We chose datasets relying onthe filtering/mitigation done by their respective original authors. In general, before releasing modelslike the ones described in our paper to the public, we recommend conducting proper evaluation ofmodels trained using our method for bias, fairness and discrimination risks. For example, geographicaldisparities due to stereotypical generations could be revealed with methods described by Hall et al., and social biases regarding gender and ethnicity could be captured with methods from Luccioniet al. and Cho et al. . While our study provides valuable insights into control conditioning and the effectiveness of repre-sentation transfer in diffusion models, there are several limitations that should be acknowledged. (i)There are cases where these improvements can be less pronounced. For example, noisy replicatesfor the text embeddings can become less pertinent if the model is trained exclusively with longprompts. (ii) While low resolution pretraining with local crops on ImageNet resulted in better FID at512 resolution (see c), it might not be necessary if pretraining on much larger datasets (e.g.>100M samples, which we did not experiment in this work). Similarly, flip conditioning is onlypertinent if the training dataset contains position sensitive information (left vs. right in captions, orrendered text in images), otherwise this condition will not provide any useful signal. (iii) We did notinvestigate the impact of data quality on training dynamics, which could have implications for thegeneralizability of our findings to datasets with varying levels of quality and diversity. (iv) As ouranalysis primarily focused on control conditioning, other forms of conditioning such as timestep andprompt conditioning were not explored in as much depth. Further research is needed to determinethe extent to which these conditionings interact with control conditioning and how that impacts thequality of the models. (v) Our work did not include studies on other parts that are involved in thetraining and sampling of diffusion models, such as the different sampling mechanisms and trainingparadigms. This could potentially yield additional gains in performance and uncover new insightsabout the state-space of diffusion models.",
  "FLOPS@256 (G)95.38275.14237.93237.93259.08237.4FLOPS@512 (G)3901, 2001, 0501, 0501, 1401, 050Params. (M)401.75611.7679.09792.44748.07679.09": "Computational costs.In Tab. A1 we compare the model size and computational costs of thedifferent architectures studied in the main paper. All model architectures are based on the originalimplemetations, but transposed to our codebase. Mainly, we use bfloat16 mixed precision andmemory-efficient attention 2 from PyTorch. Experimental details.To ensure efficient training of our models, we benchmark the most widelyused hyperparameters and report the results of these experiments, which consist of the choice ofthe optimizer and its hyperparameters, the learning rate and the batch size. We then transpose theoptimal settings to our other experiments. For FID evaluation, we use a guidance scale of 1.5 for 256resolution and 2.0 for resolution 512. For evaluation on ImageNet-22k, we compute the FID scorebetween 50k generated images and a subset of 200k images from the training set. Training paradigm.We use the EDM abstraction to train our models for epsilon predictionfollowing DDPM paradigm. Differently from , we do not use learned sigmas but followa standard schedule. Specifically, we use a quadratic beta schedule with start = 0.00085 andend = 0.012. In DDPM , a noising step is formulated as follows, with x0 being the data sampleand t [[0, T]] a timestep:",
  "and the loss is weighted by the inverse SNR12t": "Scaling the training.A recurrent question when training deep networks is the coupling betweenthe learning rate and the batch size. When multiplying the batch size by a factor , some worksrecommend scaling the learning rate by the square root of , while others scale the learning rate bythe factor itself. In the following we experiment with training a class-conditional DiT model withdifferent batch sizes and learning rates and report results after the same number of iterations.",
  "w/ DiT6459.26104.5525639.5337.0886.5151222.4823.6183.6102412.0317.1376.15204810.1912.3682.62": "Table A2: Influence of learning rate and batchsize on convergence. Training is performed onImageNet-1k@256. Results are reported on themodel without EMA after 70k training steps. FIDis computed using 250 sampling steps w.r.t. thetraining set of ImageNet-1k@256. : diverged.For almost all learning rates, the optimal batchsize is the highest possible. The best performanceis obtained when using the highest learning ratethat does not diverge with biggest batch size pos-sible.",
  "Number of tokens": "Figure A3: Contribution of Padding to-kens. For short prompts, a large number oftext tokens do not contain useful information,but may still contribute to the cross-attention,as illustrated by the non-zero gradients w.r.t.tokens after the ones coding the prompt (in-dicated by the dashed vertical line). Text padding mechanism.In order to train at a largescale, most commonly used text encoders output a constantnumber of tokens T that are fed to the denoising model(usually T = 77). Consequently, when the prompt has lessthan T words, the remaining tokens are padding tokensthat do not contain useful information, but can still con-tribute in the cross-attention, see Figure A3. This raisesthe question of whether better use can be made of paddingtext tokens to improve training performance and efficiency.One common mitigation involves using recaptioning meth-ods that provide longer captions. However, this creates aninconsistency between training and sampling as users aremore likely to provide shorter prompts. Thus, to make bet-ter use of the conditioning vector, we explore alternativepadding mechanisms for the text encoder. We explore areplicate padding mechanism where the padding tokensare replaced with copies of the text tokens, thereby push-ing the subsequent cross-attention layers to attend to allthe tokens in its inputs. To improve the diversity of the feature representation across the sequencedimension, we perturb the embeddings with additive Gaussian noise with a small variance txt. Forshorter prompts with a high number of repeats m, we scale the additive noise by m 1 to accountfor the reduction in posterior uncertainty induced by these repetitions:",
  "(c) Noisy replicate padding": "Figure A4: Illustration of the attention matrix under different padding mechanisms. With the zero paddingmechanism, a significant part of the attention can be used on dead padding tokens, potentially de-focusingfrom the relevant information. Using a replicate padding instead results in redundant information. Noisy replicatepadding increases the diversity in the text token representations and therefore acts as a regularizer fostering themodel to be robust to local variations in the latent space of the conditioning, e.g., akin to data augmentation. where ch is the standard deviation of the feature text embeddings over the feature dimension. SeeFigure A4 for an illustration comparing zero-padding, replication padding, and our noisy replicationpadding.",
  "Qualitative results.We provide additional qualitative examples on ImageNet-1k in Fig. A5": "Summary of findings.In Table A3 we summarize the improvements w.r.t. the DiT baselineobtained by the changes to the model architecture and training. In Table A4 we compare our modelarchitecture and training recipe to that of SDXL and SD3. In Table A5, we provide a synopsis ofthe research questions addressed in our study alongside a respective recommendation based on ourfindings.",
  "zero.(t 1.0)3.150.090.046zero.(t 2.0)3.120.140.062zero.(t 6.0)3.090.260.170": "Effectiveness of the power cosine scheduleWe experiment with different function profiles forcontrolling the conditioning on low-level augmentations. Specifically, we compare the power-cosineprofile with a linear and a piecewise constant profile. While the linear schedule manages an acceptableperformance in terms of reducing LPIPS (although still higher than the power-cosine profile), it stillachieves a higher FID than all the configurations with the cosine schedule. For the piecewise constantprofiles, they achieve a higher LPIPS while also having a higher FID. In conclusion, the proposedpower-cosine profile outperforms these simpler schedules in both FID and LPIPS, improving imagequality while better removing the unwanted distribution shift induced from choosing different samplesduring training."
}