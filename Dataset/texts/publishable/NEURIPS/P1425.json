{
  "Abstract": "Large Language Models (LLMs) are typically trained to predict in the forwarddirection of time. However, recent works have shown that prompting these modelsto look back and critique their own generations can produce useful feedback. Moti-vated by this, we explore the question of whether LLMs can be empowered to think(predict and score) backwards to provide unsupervised feedback that complementsforward LLMs. Towards this, we introduce Time Reversed Language Models(TRLMs), which can score and generate queries when conditioned on responses,effectively functioning in the reverse direction of time. Further, to effectively inferin the response to query direction, we pre-train and fine-tune a language model(TRLM-Ba) in the reverse token order from scratch. We show empirically (andtheoretically in a stylized setting) that time-reversed models can indeed comple-ment forward model predictions when used to score the query given response forre-ranking multiple forward generations. We obtain up to 5% improvement on thewidely used AlpacaEval Leaderboard over the competent baseline of best-of-Nre-ranking using self log-perplexity scores. We further show that TRLM scoringoutperforms conventional forward scoring of response given query, resulting insignificant gains in applications such as citation generation and passage retrieval.We next leverage the generative ability of TRLM to augment or provide unsupervisedfeedback to input safety filters of LLMs, demonstrating a drastic reduction in falsenegative rate with negligible impact on false positive rates against several attackspublished on the popular JailbreakBench leaderboard.",
  "Introduction": "Large Language Models (LLMs) trained on a large corpora of text are able to accomplish a widevariety of downstream tasks such as summarization, open-ended/ context-based question answering,document retrieval, and citation generation [Brown et al., 2020, Zhao et al., 2023a]. While the gener-ations from pre-trained and instruction-tuned models already show significant promise, alignmenttechniques such as Reinforcement Learning via Human Feedback (RLHF) [Anil et al., 2023a, Ouyanget al., 2022] are widely used to improve the quality of their generations further. However, thesemethods rely heavily on additional supervision to construct preference data, which can be expensiveto acquire, or noisy for training. This brings up a natural question Can we generate useful feedbackon LLM generations without additional supervised data?",
  "arXiv:2412.02626v2 [cs.CL] 4 Dec 2024": "A recent line of work aims at specially prompting LLMs to review their own generations and generatemeaningful natural language feedback, which can subsequently be used to refine them [Madaan et al.,2024]. This process can be repeated to improve the generations iteratively. The success of suchmethods serves as an evidence that it is indeed possible to obtain better responses without additionalsupervision. However, such methods rely on the superior instruction following and reasoning abilitiesof LLMs, which may not necessarily hold for low capacity models. Further, these methods involvesequential processing of the generated responses, and thus increase inference time significantly. In this work, we propose a natural method of enabling LLMs to look backwards in order to obtainmeaningful unsupervised feedback during inference. Towards this, we introduce a class of modelsthat we call Time Reversed Language Models (TRLMs), which operate in the reversed direction of aregular LLM, or the time-reversed direction. Rather than predicting (or scoring) in the standard query response direction, time reversed language models predict (or score) in the response querydirection. We first introduce TRLM-Fo - a TRLM variant based on forward models, which are promptedto operate in the time-reversed direction using a prompt such as \"Generate a question thatwould result in the following answer:<response>\". Further, we extend the reversalto token-level granularity by pre-training LLMs from scratch in a reversed token direction, rather thanthe standard forward token direction. We call this as TRLM-Ba where Ba stands for Backward. Notethat the inputs and outputs of such a model are in the reversed language order. Pre-training TRLM-Baon reversed text exposes the model to a completely different world model where the conventionalorder of information is flipped. Introductions follow conclusions, questions follow answers, logicalprecedents follow their antecedents. Hence, such a model may not only develop representations thatare distinct from those of a regular LLM despite being trained on the same pre-training corpus butmay also be better suited to score/ generate in the reverse direction, i.e. conditional on the response. We show in several use-cases that scoring and generation in this reverse direction can producenon-trivial feedback on the responses generated by forward LLMs. We consider three classes of tasksto showcase the scoring and generating capability of TRLM, viz. a) Reranking answers in open endedquestion answering b) Citation and retrieval tasks and c) Amplifying existing safety filters throughquery generation in the reverse. Our Contributions:a) We propose time reverse language models - TRLM-Fo, TRLM-Ba and TRLM-FoBa, all of whichscore and generate queries given responses, enabling their use in obtaining unsupervised feedback onLLM generations. TRLM-Fo is a forward model prompted to predict in reverse, while TRLM-Ba ispre-trained in the reverse token order, enabling reverse prediction naturally. TRLM-FoBa is pre-trainedin both reverse and forward token orders and can be used to predict in forward or reversed language.b) We demonstrate significant improvements when best-of-N reranking is applied to multipleLLM generations by using TRLM scores. Specifically, we show up to a 5% improvement overself-reranking using TRLM-Ba, in LC win-rates (0.98 Pearson correlation with human preferences)against a GPT4-1106-Preview reference model. We show multiple ablations on this study.c) We demonstrate that the reverse direction of scoring (response query) is highly significant,as it improves citation attribution accuracy by 44.15% when compared to the forward baseline onthe CNN-Daily Mail dataset. Further, we improve the NDCG@10 metric by 44.13 points on theNF-Corpus medical information retrieval benchmark, and obtain similar improvements on MS-Marcoas well.d) We show that the reverse generation capability of the TRLM models - specifically TRLM-Ba, can beused to improve False Negative rate (FNR) of input safety filters with negligable impact on FPR. Weshow significant improvements on several attacks submitted to the Jailbreakbench benchmark, and ona Human Annotated dataset from JailbreakBench. We complement these results with theoretical arguments using a bipartite graph model between queriesand responses, to show that RLHF done with TRLM-Ba scores induces a non trivial distribution shiftin answers, mitigating primitive forms of hallucination under the defined conditions.",
  "Related Work": "Reverse Direction in Language Modeling: Classical work [Serdyuk et al., 2017] showed howsequence to sequence models can regularize the current word token embedding based on the abilityof the future tokens to be able to predict the current token. Such bi-directional (forward and reverse) consistency checks have been used to improve forward models. Golovneva et al. train an LLMin the forward direction first, followed by the reverse token direction, and show that this alleviates thereversal curse identified by Berglund et al. . This work is closely related to ours in that we alsoconsider a variant of combining reverse and forward token order during training. Our key modelsdiffer from this, and are trained in either forward (TRLM-Fo )/ reverse (TRLM-Ba ) token order, usingwhich we demonstrate improvements in a wide range of applications such as long form questionanswering, citations, retrieval and augmenting input filters for defending against toxic questions.Yang et al. use question generation from a given answer combined with access to externaldatabases to determine hallucination. Another recent work [Guo et al., 2024] also explores a differentpre-training order. While their focus is to correct causal ordering bias, our work instead is focused onthe value that scoring and generation of these models bring to downstream tasks. Reversed scoring: Several prior works [Li et al., 2016, Zhang et al., 2018, 2020] have proposedto improve the diversity of generated responses by optimizing the mutual information between theresponses and the respective queries. These works motivate the need for better decoding strategiesbased on scores in both, response query and query response directions. We theoreticallyshow that reverse scoring alone, when used with forward generations, will achieve this naturallyusing a formal RLHF based argument (Lemma 2), and present strong empirical results across a widerange of tasks to support the same. Controlling Decoding through feedback: A broad line of works align a pre-trained model to areward model trained on human feedback by using Reinforcement learning (RL) techniques likeProximal Policy Optimization (PPO) [Stiennon et al., 2020, Ouyang et al., 2022, Korbak et al., 2022],(Identity policy optimization) IPO (and PO) [Azar et al., 2024], Direct Preference Optimization[Rafailov et al., 2024] and offline RL [Snell et al., 2022]. Zhao et al. and Zhao et al. [2023b]calibrate likelihood of generated responses on a dataset with desired responses or human preferencefeedback.[Krause et al., 2020, Yang and Klein, 2021, Qin et al., 2022] control the generation of anLLM at test time by specifying constraint functions or discriminators that operate in the token or logitspace, encouraging certain attributes in the output. Using preference feedback, Mudgal et al. [2023b]train a prefix scorer model that acts as a value function over partial completions consistent with thepreference rewards. Yang et al. [2024b] investigate the relation between best-of-N-reranking and KLregularized RL objective. An observation made by Yang et al. [2024b] is that best-of-N-rerankingdominates/ competes very well with most RL based alignment methods. Under certain assumptions,authors show formally that best-of-N-reranking approximates the optimal solution to the regularizedRL objective. We take inspiration from this and use best-of-N-reranking to evaluate generationsthrough unsupervised feedback by the reverse LLMs. Our work differs from all these in that they relyon external feedback to control generation, while our method does not. Self Play and Self Tuning: Chen et al. explore how an LLM can be prompted to self-debugbased on an explanation of the code produced by the LLM during code generation and the executionoutput on test cases. Welleck et al. use a corrector model that is trained to prefer a newcorrected answer if the corrected answer has higher value that a default generation. They requireaccess to a value function for this determination. All these approaches use an external feedback toalign the model in their pipeline. Fu et al. explore LLM agents initialized as buyers and sellers to play a negotiating game ofsetting the price of a transaction. A critic LLM provides feedback to both the buyer and seller agentsto improve. Madaan et al. propose a self refining loop where the same model is prompted toprovide feedback and further use the feedback to refine and regenerate. Both these works use verypowerful and large models from the Claude, GPT-4, GPT-3.5 family to use self generated languagefeedback. Madaan et al. remark that the self refining approach does not work well withweaker models. In contrast, we focus on improving generation quality of much smaller models usingunsupervised scalar feedback. Other prior works relating to self play are reviewed in the surveyarticle by Amini et al. .",
  "TRLM - Time Reversed Language Models": "We introduce our primary contribution - TRLM (Time Reversed Language Models), a class of languagemodels that operate in the response query direction during scoring and generation. This isachieved by either (a) [TRLM-Ba ] reversing the token order and effectively utilizing previous tokenprediction instead of next token prediction during pre-training, scoring, and generation, or (b)",
  "ModelDescription": "TRLM-BaPre-trained in the reverse token order for previous token prediction (Alg. 1 in thesupplement). Instruction-tuned variant is FLaN fine-tuned [Longpre et al., 2023] inreverse token order. Scores the reversed question given a reversed answer combinedwith suitable prompts. Generates questions in the reverse direction when conditionedon answers in the reverse direction.",
  "Scoring: Scores identically to TRLM-Fo.Generation: Generates identically to TRLM-Fo": "SelfScoringThe model that is used for generating a given response is also used for scoring responsesgiven queries in the conventional forward scoring direction.Scoring: We use the models own perplexity scores as feedback to select the responses. ForwardBaselineA conventional forward model trained for next-token prediction on the same trainingcorpus and model class as TRLM .Scoring: While self-scoring used the perplexity obtained from the generator model, inthis setting, we use perplexity of a different forward model.",
  "[TRLM-Fo ] maintaining the standard token order during pre-training but reversing the direction ofgeneration through appropriate prompts during inference (scoring and generation)": "We show that TRLM provides non-trivial unsupervised feedback that could be used by pre-trained,fine-tuned, and instruction tuned models, for various downstream tasks like reranking to improveopen-ended long-form question answering, generating citations, and retrieval. We demonstrate thatthe ability of TRLM to score in the reverse direction scoring query based on the response is essentialto achieve the requisite gains. Further, TRLMs that are pre-trained in the reverse direction (TRLM-Ba) provide an additional boost in most cases. We further leverage the generative ability of TRLM inreverse (generating query from a response) to amplify the effectiveness of input safety filters as well.",
  "We propose four variants of the TRLM class TRLM-Ba , TRLM-Fo , TRLM-FoBa (Reverse) andTRLM-FoBa (Forward) based on how they are pre-trained and fine-tuned": "TRLM models can be considered to have three functions:TRLM.Pretrain, TRLM.Score, andTRLM.Generate, which we describe for each of the four variants in . We further outlinethese functions for different TRLM models in Algorithms 1, 2, 3, & 4. For this work, we consider twobaselines, which are trained in forward token order, and score in the conventional order of responsegiven the query. The first of these uses self-scoring based on the models own perplexity. The second(Forward Baseline) is a forward model that we train, whose training corpus and model class areidentical to TRLM. TRLM Model Training: The pre-training setup for all TRLM models is identical to that of PALM2-Ottermodels described by Anil et al. [2023b], except for the token orders specified by our TRLM.pretrainmethods for TRLM-Fo , TRLM-Ba and TRLM-FoBa respectively. We fine-tune them on the FLaN dataset [Longpre et al., 2023] using the TRLM-xx.pretrain function. Where xx can refer to Fo,Ba or FoBa based on the model being fine-tuned. Let Instruction, Question, Answer denoteinstruction, question and answer respectively. Before calling the pretrain function during finetuning , we merge Instruction + Question to be the new question.",
  "Scoring in Reverse": "In this section, we provide formal results on TRLM and the benefit of using pre-training in the reversedirection. Let us denote by PFw(A|Q) the conditional distribution of a forward LLM. Similarly,denote PTRLM(Q|A) to be the conditional distribution of the Time Reversed Language Model. Forsimplicity, we merge the instruction and question together.",
  "In this subsection, we focus on the distribution shift encountered while using a reverse model basedscorer on forward generations": "Specifically, we conclude that while reranking using Forward Baseline is equivalent to temperaturescaling [Yang et al., 2024b], reranking using TRLM induces a distribution shift that is not equivalent totemperature scaling. Consider the alignment problem of learning a new forward LLM - PFw(Answer|Question). A verypopular framework is the KL constrained optimization objective with respect to a reward oracleR(Question, Answer), for some threshold :",
  "[R(Question, Answer)] s.t. DKL(PFwPFw) (1)": "Log-perplexity of the forward model used as reward: In general, for long form question answeringwhere an explicit reward model is not available, a typical method is to use log-perplexity of theforward model i.e. log PFw as a reward. Then, we have the following corollary of Lemma 1 in Yanget al. [2024b], Lemma 1 (Corollary of Lemma 1 in Yang et al. [2024b]). The new LLM policy PFw that optimizes (1)is given by: PFw(Answer|Question) P1+Fw(Answer|Question) where is chosen appropriatelydepending on the threshold when reward R() is set to log perplexity of the forward model PFw.",
  "Log-perplexity of the TRLM-Ba.score used as reward: Suppose R() is set to output ofTRLM-Ba.score computed on the the question given the answer, then we have:": "Lemma 2 (Corollary of Lemma 1 in Yang et al. [2024b]). The new LLM policy PFw that optimizes(1) is given by: PFw(Answer|Question) PFw(Answer|Question)PTRLM-Ba(Question|Answer)where is chosen appropriately depending on when reward R() is set to log perplexity of thereverse model PTRLM. Optimal distribution after alignment using TRLM scores results in a non-trivial distribution that is notsimply temperature re-scaling. While we have not used TRLM for alignment using KL constraints inour experiments, the distribution shift that is induced by reverse token training is indeed non-trivialeven with Best-of-N-re-ranking, which we adopt in our experiments.",
  "Experimental Results": "In this section, we explore the effectiveness of time reversed language models on different downstreamtasks, by utilizing unsupervised feedback to improve upon existing forward model generations.Broadly, these applications fall into two categories - first, where we utilize the scoring capacity ofTRLM (three use cases), and second where we utilize the generative capacity of TRLM for generatingqueries given a response.",
  "The best-of-N reranking task involves outputting the best response out of N model responses to auser query": "Specifically, given N LLM outputs to a user query, a reranking algorithm finds the best responsebased on scalar scores assigned to each response. Prior works [Rafailov et al., 2023, Mudgal et al.,2023a] aim to improve LLM performance on this task by using feedback-based RLHF algorithms andtraining on KL-regularized alignment objectives. Yang et al. [2024a] show that best-of-N reranking isthe most effective way to approximate these RL objectives, and further, it is empirically observed tooutperform them. In this work, we consider several best-of-N reranking based algorithms based on TRLM.Score, forevaluating a base model response. The methods considered rely on nothing more than the pre-training(or instruction-tuning) corpus to achieve alignment of response to the user query. We further note thatsuch scores from TRLM may be used within RL objectives as well, but we leave the exploration ofsuch rewards to future work.",
  "Alpaca Leaderboard Evaluation": "Benchmark and Evaluation: The AlpacaEval leaderboard [Dubois et al., 2024] is a widely usedbenchmark to evaluate the capability of language models. In this benchmark, there are 805 questionsfrom the AlpacaFarm evaluation set consisting of questions ranging from general writing, chatability, and reasoning to general knowledge. The goal is to output a response that is better than a basemodels response, as judged by an annotator model. Both base model and annotator model are set asGPT4-1106-Preview on the AlpacaEval leaderboard as on May 10, 2024, and hence we use thesame for our evaluations. The evaluation benchmark computes various metrics including winrates,discrete winrates and length-controlled winrates [Dubois et al., 2024]. The length-controlled winratesare calculated using a debiasing algorithm that removes the length bias that is otherwise preferred byGPT4-1106-Preview . Formally, we define the task for TRLM as follows Given a query Q from the dataset and N model re-sponses A = {A1 . . . AN} from a generator model, we wish to use TRLM.score to output the highestscoring response ai A, which is further evaluated against an answer from GPT4-1106-Preview. In our experiment, we consider outputs from a generator model that is Gemini-Pro-1.0 [Anil et al.,2023a]. We generate 16 responses using a temperature = 0.8 to ensure diversity of answers. Wethen rerank the responses using different variants of TRLM from the PALM2-Otter family of models(TRLM training details in the supplement). We further consider two baselines, Self scoring andForward Baselines, as described in . Scoring prompts and Conditioning prompts usedwith various TRLM variants for this task are described in the of Appendix C.1. Discussion of Results: In , we see that TRLM-Ba scores the highest length controlled winrate which is 5% over the self scoring baseline of Gemini-Pro-1.0 with 16 generations againstthe GPT4-1106-Preview judge. Further, it registers an 8% increase over the reported number forsingle generations in the benchmark leaderboard. We note that scoring Response->Query seems tobring out some improvements as TRLM-Fo improves over Forward Baseline. Further, TRLM-Baoutperforms TRLM-Fo indicating the impact of reverse token pre-training. This demonstrates that timereversed scoring provides an intrinsic unsupervised feedback that could help improve the performanceof even larger capacity models. We note that pre-training in both forward and reverse directions(TRLM-FoBa models) and scoring in the reverse direction is better than TRLM-Fo variant. We present further results where the generations of a Mixtral model [Jiang et al., 2024b] arereranked and compared against GPT4-1106-Preview , and the generations of a smaller Mixtralmodel are reranked and compared against a larger Mixtral model. These results are presentedin the Appendix C.2. We note a 4% improvement over Forward Baseline with the proposedTRLM-Ba.Score method of reranking. Key Takeaway:Through empirical justifications, we show that TRLM variant models can beused as effective re-rankers of generations from multiple classes of models (Gemini-Pro-1.0 ,Mixtral8x22B, Mixtral8x7B), and improve the instruction following capability of the model as awhole. This is consistent with Theorem 1 considering the fact that we outperform generation models",
  "Citation Attribution": "In this section, we describe applications of reverse scoring to the task of producing citations tooriginal passages that can corroborate the sentences in an already produced summary. Summariesare created from long form articles, and one often wants to know which part of the article a givensummary sentence is derived from (Cohen-Wang et al. ). Dataset and Evaluation: For this task, we take the CNN Daily Mail Dataset [CNN] which consistsof pairs of news articles and their respective highlights. Our goal is to identify which sentence (orgroups of sentences) within a given news article provides the most direct corroboration for a specificarticle highlight given as a query. We evaluate the attributed citations using various relevancy metrics.We use cosine similarity on the embeddings of the Gecko model [Lee et al., 2024], cosine similarityon TF-IDF features, BLEU score and ROUGE score to compute metrics. We score and choose the bestpairing using all the models from the TRLM PALM2-Otter family trained in the forward, reverse andforward-reverse directions as outlined in .1.1. Algorithms: Different search algorithms, Linear Search, Binary Search and ExclusionSearch are coupled with using TRLM.score to find the attribution. We outline these in Algo-rithms 7, 8 and 9 along with details in the supplement. The number of inference calls is O(log N)where N is the number of article sentences for Binary Search, and this method produces multiplesentences as a citation. The other methods require O(N) calls to produce the citation for a sentence. Our results shown in , demonstrate the efficacy of TRLM for the attribution task. Specifically,we show 44% gains over the baseline in the linear search method, 39% gains in the binary searchmethod and 34% gains in the exclusion search method as measured through gecko cosine similarity. Key Takeaway: Through our results on CNN-Daily Summarization dataset we present multiplemethods of citation attribution and demonstrate significant gains with TRLM model variants. We notethat a direction of low information to high information (summary > article) is harder to reasonupon and select among a given set of texts. Further, we highlight the importance of binary selectionbased approach over log-perplexity based exclusion based search. We show 9% improvement usingTRLM-Ba on Gecko embedding-based metric using only O(log N) inference calls to the main model.",
  "Document Retrieval": "In this section, we study the performance of TRLM in retrieving relevant passages from a corpus toanswer a specific question. Our goal is to show the efficacy of TRLM based reverse scoring over doingit in the forward direction. The task is as follows: Given a question, the goal is to retrieve relevantdocuments from the given corpus. We retrieve k documents from the corpus and compute variousinformation-retrieval metrics to calculate performance w.r.t. the golden set of documents.We experiment with two retrieval-based datasets from MTEB benchmark [Muennighoff et al., 2023]as shown in . Metrics are precision, recall and normalized discounted cumulative gain(NDCG) (details in Appendix E.1). We show our results in . TRLM reverse scoring algorithms : Tabulates the citation Attribution results through Re-ranking on the CNN-Daily Mail dataset. Adenotes article whereas S denotes the corresponding summary. The ease of scoring a summary given the articleinstead of reverse is clearly highlighted in all of the search methods.",
  "DatasetDescription": "MS-MarcoContains 101.09k examples in its public dev split. Each example consists of asimple question along with 10 relevant passages. [Bajaj et al., 2016]NF-CorpusMedical information retrieval dataset with 323 queries in its test split and 3.6ktotal documents in the corpus. Queries are in simple English, and documents areextracted from PubMed with a fair amount of medical terminology. [Boteva et al.,2016a, Pub] along with respective prompts used are presented in Algorithms 11, 10 of the Supplement. As suggests, results favor TRLM based reverse scoring methods. For example, we see a 22.48%improvement in recall at K = 4 for MS-MARCO dataset. TRLM-Ba model dominates across metrics.For NF-Corpus, we see that the conventional forward scoring algorithm (query > document) hasa very poor performance. We attribute this to the fact that, in this inference direction, we are scoringa highly complex medical document using a simple natural language query. We see a gain of 44.2points in NDCG at K = 10 with TRLM-Fo compared to Forward Baseline . The results in boththese datasets suggest that TRLM can show greater gains when the complexity of documents in thecorpus differs significantly from the complexity of queries. : Tabulates the results of various reranking algorithms with two inference directions. Q de-notes Queries, while D denotes Documents. TRLM outperforms Forward Baseline and BackwardBaseline significantly, which highlights the importance of inference direction in this task.",
  "MethodInferenceDirectionPrecisionRecallNDCG@10PrecisionRecallNDCG@10K=1K=4K=1K=4K=10K=20K=10K=20": "TRLM-BaD > Q28.418.5427.2270.2961.4915.711.3810.6813.0843.23TRLM-FoBa (Reverse)D > Q24.917.3823.8565.8558.8414.9810.9110.0112.7641.65TRLM-FoBa (Forward)D > Q21.1615.5820.2559.0855.4617.8612.611.1113.548TRLM-FoD > Q20.3714.919.4556.3954.4617.3112.389.7411.7648.08Forward BaselineQ > D21.0513.8218.4247.81530.870.870.170.313.89Backward BaselineQ > D16.814.0415.9953.1352.071.110.790.210.293.95 Key takeaways: We experiment with two information retrieval-based benchmarks MS-MARCOand NF-CORPUS and compute multiple metrics to compare TRLM variant models with standardForward Baseline and unconventional Backward Baseline . We show a gain of 8.49 points inNDCG@10 on MS-MARCO and 44.19 points in NDCG@10 on NF-CORPUS. Aligning with theresults in citation, the results from this task also accurately demonstrate the importance of going froma high information direction to a low information direction. The massive difference between thedirections is evident in the NF-CORPUS dataset. 5.4Defending against Jailbreak attacksWe next aim to leverage the generative ability of TRLM to augment toxicity filters that are used toimprove the safety of LLMs. Prior works show that LLMs (and their input filters) can be jailbrokenusing crafted adversarial attacks [Zou et al., 2023], while output filters tend to have a high falsenegative rate due to the sensitivity to the presence of toxic words, despite being in a neutral context(See Table-10). We propose to combine the benefits of input and output filters by projecting the : Performance of the proposed defense strategies across different thresholds, evaluated on thehuman annotated and jailbreakbench toxic responses. TRLM-Ba achieves significant gains over allother approaches. Notations: PT [Pretrained], IT[Instruction-finetuned], FNR[False Negative Rate],FPR[False Positive Rate], new-HA [new HA Dataset], JBB[JBB Dataset], (H) [Hard], (E) [Easy]",
  "MethodFNR-HAFNR-JBBFPR (H)FPR (E)FNR-HAFNR-JBBFPR (H)FPR (E)FNR-HAFNR-JBBFPR (H)FPR (E)": "TRLM-Fo (PT)0.0036.1117.002.0036.3655.5612.000.0045.4570.836.000.00TRLM-Ba (PT)18.1852.780.008.0027.2765.280.002.0027.2769.440.002.00TRLM-Fo (IT)54.5555.563.000.0063.6472.221.000.0063.6481.941.000.00TRLM-Ba (IT)18.1859.720.008.0018.1870.830.004.0027.2779.170.002.00 output response of LLMs to the input query space using the reverse generative capability of TRLM ,and further detecting the toxicity of the generated queries to block/ pass the response to the originalquery based on a pre-specified criteria. We thus effectively amplify input safety filters, i.e. reduceFalse Negative Rate (FNR) with marginal/ no impact on False Positive Rate (FPR). Key Idea: Consider TRLM.Generate(Response) that generates queries that could have produced agiven response. The insight is that, the reverse generative ability of TRLM allows the projection of acandidate (jailbreak) query that could bypass the input filter back to the (naive) query space observedduring training. These projected questions can thus be rightly classified using the same input filter. Defense Strategy: We propose a defense strategy where i) a query is passed through the inputfilter, ii) if the input filter rejects the query, we return reject as well, iii) if the input filter allowsthe query, we take the Response produced by the model and generate multiple queries usingTRLM.Generate(Response). If the number of generated queries rejected exceeds a threshold, wereject the query as \"unsafe\". Otherwise, we declare it as safe, and output the response correspondingto the input query. An elaborate description is provided in Algorithm 12 of the Supplement. Datasets:Weconsiderahumanannotated(HA)datasetprovidedaspartoftheJailbreakBench benchmark [HAd] for evaluating the performance of toxicity classifiers. Thiscontains 100 questions annotated by humans, of which 43 are annotated as toxic based on a majorityvote across 3 annotators. We introduce a GPT-4 based filter, that considers the prompt-response pairto judge their toxicity (Details in Appendix-F.2), and has 0 FNR on this HA dataset, which is ideal fordefense evaluation. We further consider a gpt-3.5-turbo-1106 based input toxicity filter for theempirical evaluation of the proposed defense, which has an FNR of 25.58% on this dataset. Theseunblocked questions form our new-HA dataset for the experiments. In addition to this, we use thefollowing datasets for evaluation: JBB dataset that contains jailbreak questions (that are toxic as perthe GPT-4 judge, but are safe as per the GPT-3.5 filter we augment) corresponding to different attackson JailbreakBench, E dataset that contain safe and easy questions and H dataset that contains safequestions that are hard to classify as safe. We discuss more details on these datasets in Appendix-F.1. In the two toxic datasets (HA and JBB), the gpt-3.5-turbo-1106 based input filter does not blockany of the questions, and our defense strategy aims at lowering the False Negative rate on the toxicquestions (JBB dataset and new-HA dataset), while ensuring a low false positive rate on the safequestions as well (E and H datasets). We present the improvements in FNR rates for Algorithm 12when combined with the gpt-3.5-turbo-1106 input filter and various TRLM variants in Table-6.We further present the impact of varying the threshold in of the Appendix. Results: We firstly note that the proposed TRLM defense strategy improves the FNR of thegpt-3.5-turbo-1106 input filter across all settings considered. Further, the TRLM-Ba pre-trainedmodel improves FNR by more than 70% on the HA dataset and around 35% on the JBB dataset, andoutperforms other variants with negligible impact on FPR. We note that the proposed defense outperforms existing perplexity thresholding based defenses[Jain et al., 2023, Alon and Kamfonas, 2023] and Smooth-LLM [Robey et al., 2023] on theJailbreakBench attacks [Chao et al., 2023, Deng et al., 2024] owing to the integration with aninput filter that already outperforms them on the same. Hence, we do not compare with them. Further,these defenses operate only in the input space, while the proposed defense aims at augmenting theinput space with feedback from the response. Hence, the proposed defense is orthogonal to suchmethods, and can thus be integrated with them as well.",
  "Conclusions": "In this work, we explore the capabilities of TRLM for scoring and generation of queries, whenconditioned on responses. Our study points to the importance of the response query directionin LLMs. When deploying TRLM models for reverse scoring, we show improvements on AlpacaEvalleaderboard, Citation attribution and retrieval tasks. We further show that generations from TRLM canaugment safety filters effectively.",
  "Limitations": "We note that the assumptions made for our theoretical results in are stylized, and maynot hold true in practice, as the space of all answers to questions may not be adequately capturedby assumptions in that section. Given this assumption, one may wish to explore other models forhallucination that are more general and provide results about reverse scoring. We leave such atheoretical exploration to future work. Further, TRLM benefits have thus far been explored on tasks related to short form queries that havelong answers. One may wish to understand and demonstrate the effects of reverse scoring on othertasks. For instance, one might pose the question does TRLM provide possible benefits for a broaderset of tasks that language models are used for. We leave the exploration of such settings in which thereverse scoring direction of response query is better than the forward scoring direction, alongwith obtaining an understanding on the reason behind such an advantage, as part of future work.",
  "M.-R. Amini, V. Feofanov, L. Pauletto, E. Devijver, and Y. Maximov. Self-training: A survey. arXivpreprint arXiv:2202.12040, 2022": "R. Anil, S. Borgeaud, Y. Wu, J. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, K. Mil-lican, D. Silver, S. Petrov, M. Johnson, I. Antonoglou, J. Schrittwieser, A. Glaese, J. Chen, E. Pitler,T. P. Lillicrap, A. Lazaridou, O. Firat, J. Molloy, M. Isard, P. R. Barham, T. Hennigan, B. Lee, F. Vi-ola, M. Reynolds, Y. Xu, R. Doherty, E. Collins, C. Meyer, E. Rutherford, E. Moreira, K. Ayoub,M. Goel, G. Tucker, E. Piqueras, M. Krikun, I. Barr, N. Savinov, I. Danihelka, B. Roelofs, A. White,A. Andreassen, T. von Glehn, L. Yagati, M. Kazemi, L. Gonzalez, M. Khalman, J. Sygnowski, andet al. Gemini: A family of highly capable multimodal models. CoRR, abs/2312.11805, 2023a. doi:10.48550/ARXIV.2312.11805. URL R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos, S. Shakeri, E. Taropa, P. Bailey,Z. Chen, E. Chu, J. H. Clark, L. E. Shafey, Y. Huang, K. Meier-Hellstern, G. Mishra, E. Moreira,M. Omernick, K. Robinson, S. Ruder, Y. Tay, K. Xiao, Y. Xu, Y. Zhang, G. H. brego, J. Ahn,J. Austin, P. Barham, J. A. Botha, J. Bradbury, S. Brahma, K. Brooks, M. Catasta, Y. Cheng,C. Cherry, C. A. Choquette-Choo, A. Chowdhery, C. Crepy, S. Dave, M. Dehghani, S. Dev,J. Devlin, M. Daz, N. Du, E. Dyer, V. Feinberg, F. Feng, V. Fienber, M. Freitag, X. Garcia,S. Gehrmann, L. Gonzalez, and et al. Palm 2 technical report. CoRR, abs/2305.10403, 2023b. doi:10.48550/ARXIV.2305.10403. URL M. G. Azar, Z. D. Guo, B. Piot, R. Munos, M. Rowland, M. Valko, and D. Calandriello. A generaltheoretical paradigm to understand learning from human preferences. In International Conferenceon Artificial Intelligence and Statistics, pages 44474455. PMLR, 2024. P. Bajaj, D. Campos, N. Craswell, L. Deng, J. Gao, X. Liu, R. Majumder, A. McNamara, B. Mitra,T. Nguyen, et al. Ms marco: A human generated machine reading comprehension dataset. arXivpreprint arXiv:1611.09268, 2016.",
  "L. Berglund, M. Tong, M. Kaufmann, M. Balesni, A. C. Stickland, T. Korbak, and O. Evans. Thereversal curse: Llms trained on\" a is b\" fail to learn\" b is a\". arXiv preprint arXiv:2309.12288,2023": "V. Boteva, D. G. Ghalandari, A. Sokolov, and S. Riezler. A full-text learning to rank dataset formedical information retrieval. In N. Ferro, F. Crestani, M. Moens, J. Mothe, F. Silvestri, G. M. D.Nunzio, C. Hauff, and G. Silvello, editors, Advances in Information Retrieval - 38th EuropeanConference on IR Research, ECIR 2016, Padua, Italy, March 20-23, 2016. Proceedings, volume9626 of Lecture Notes in Computer Science, pages 716722. Springer, 2016a. doi: 10.1007/978-3-319-30671-1\\_58. URL V. Boteva, D. Gholipour, A. Sokolov, and S. Riezler. A full-text learning to rank dataset for medicalinformation retrieval. In Advances in Information Retrieval: 38th European Conference on IRResearch, ECIR 2016, Padua, Italy, March 2023, 2016. Proceedings 38, pages 716722. Springer,2016b. T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural informationprocessing systems, 33:18771901, 2020.",
  "O. Golovneva, Z. Allen-Zhu, J. Weston, and S. Sukhbaatar. Reverse training to nurse the reversalcurse. arXiv preprint arXiv:2403.13799, 2024": "R. A. Google and, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos, S. Shakeri, E. Taropa,P. Bailey, Z. Chen, E. Chu, J. H. Clark, L. E. Shafey, Y. Huang, K. Meier-Hellstern, G. Mishra,E. Moreira, M. Omernick, K. Robinson, S. Ruder, Y. Tay, K. Xiao, Y. Xu, Y. Zhang, G. H.Abrego, J. Ahn, J. Austin, P. Barham, J. Botha, J. Bradbury, S. Brahma, K. Brooks, M. Catasta,Y. Cheng, C. Cherry, C. A. Choquette-Choo, A. Chowdhery, C. Crepy, S. Dave, M. Dehghani,S. Dev, J. Devlin, M. Daz, N. Du, E. Dyer, V. Feinberg, F. Feng, V. Fienber, M. Freitag, X. Garcia,S. Gehrmann, L. Gonzalez, G. Gur-Ari, S. Hand, H. Hashemi, L. Hou, J. Howland, A. Hu, J. Hui,J. Hurwitz, M. Isard, A. Ittycheriah, M. Jagielski, W. Jia, K. Kenealy, M. Krikun, S. Kudugunta,C. Lan, K. Lee, B. Lee, E. Li, M. Li, W. Li, Y. Li, J. Li, H. Lim, H. Lin, Z. Liu, F. Liu,M. Maggioni, A. Mahendru, J. Maynez, V. Misra, M. Moussalem, Z. Nado, J. Nham, E. Ni,A. Nystrom, A. Parrish, M. Pellat, M. Polacek, A. Polozov, R. Pope, S. Qiao, E. Reif, B. Richter,P. Riley, A. C. Ros, A. Roy, B. Saeta, R. Samuel, R. Shelby, A. Slone, D. Smilkov, D. R. So,D. Sohn, S. Tokumine, D. Valter, V. Vasudevan, K. Vodrahalli, X. Wang, P. Wang, Z. Wang,T. Wang, J. Wieting, Y. Wu, K. Xu, Y. Xu, L. Xue, P. Yin, J. Yu, Q. Zhang, S. Zheng, C. Zheng,W. Zhou, D. Zhou, S. Petrov, and Y. Wu. Palm 2 technical report, 2023.",
  "Q. Guo, R. Wang, J. Guo, X. Tan, J. Bian, and Y. Yang. Mitigating reversal curse via semantic-awarepermutation training. arXiv preprint arXiv:2403.00758, 2024": "H. Inan, K. Upasani, J. Chi, R. Rungta, K. Iyer, Y. Mao, M. Tontchev, Q. Hu, B. Fuller, D. Testuggine,et al. Llama guard: Llm-based input-output safeguard for human-ai conversations. arXiv preprintarXiv:2312.06674, 2023. N. Jain, A. Schwarzschild, Y. Wen, G. Somepalli, J. Kirchenbauer, P.-y. Chiang, M. Goldblum,A. Saha, J. Geiping, and T. Goldstein. Baseline defenses for adversarial attacks against alignedlanguage models. arXiv preprint arXiv:2309.00614, 2023.",
  "A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary, C. Bamford, D. S. Chaplot, D. d. l.Casas, E. B. Hanna, F. Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088,2024a": "A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary, C. Bamford, D. S. Chaplot,D. de Las Casas, E. B. Hanna, F. Bressand, G. Lengyel, G. Bour, G. Lample, L. R. Lavaud,L. Saulnier, M. Lachaux, P. Stock, S. Subramanian, S. Yang, S. Antoniak, T. L. Scao, T. Gervet,T. Lavril, T. Wang, T. Lacroix, and W. E. Sayed. Mixtral of experts. CoRR, abs/2401.04088, 2024b.doi: 10.48550/ARXIV.2401.04088. URL",
  "B. Krause, A. D. Gotmare, B. McCann, N. S. Keskar, S. Joty, R. Socher, and N. F. Rajani. Gedi:Generative discriminator guided sequence generation. arXiv preprint arXiv:2009.06367, 2020": "J. Lee, Z. Dai, X. Ren, B. Chen, D. Cer, J. R. Cole, K. Hui, M. Boratko, R. Kapadia, W. Ding,et al. Gecko: Versatile text embeddings distilled from large language models. arXiv preprintarXiv:2403.20327, 2024. J. Li, M. Galley, C. Brockett, J. Gao, and B. Dolan. A diversity-promoting objective function forneural conversation models. In K. Knight, A. Nenkova, and O. Rambow, editors, NAACL HLT2016, The 2016 Conference of the North American Chapter of the Association for ComputationalLinguistics: Human Language Technologies, San Diego California, USA, June 12-17, 2016, pages110119. The Association for Computational Linguistics, 2016. doi: 10.18653/V1/N16-1014.URL S. Longpre, L. Hou, T. Vu, A. Webson, H. W. Chung, Y. Tay, D. Zhou, Q. V. Le, B. Zoph, J. Wei, et al.The flan collection: Designing data and methods for effective instruction tuning. In InternationalConference on Machine Learning, pages 2263122648. PMLR, 2023. A. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao, S. Wiegreffe, U. Alon, N. Dziri, S. Prabhumoye,Y. Yang, et al. Self-refine: Iterative refinement with self-feedback. Advances in Neural InformationProcessing Systems, 36, 2024. S. Mudgal, J. Lee, H. Ganapathy, Y. Li, T. Wang, Y. Huang, Z. Chen, H. Cheng, M. Collins,T. Strohman, J. Chen, A. Beutel, and A. Beirami. Controlled decoding from language models.CoRR, abs/2310.17022, 2023a. doi: 10.48550/ARXIV.2310.17022. URL",
  "S. Mudgal, J. Lee, H. Ganapathy, Y. Li, T. Wang, Y. Huang, Z. Chen, H.-T. Cheng, M. Collins,T. Strohman, et al. Controlled decoding from language models. arXiv preprint arXiv:2310.17022,2023b": "N. Muennighoff, N. Tazi, L. Magne, and N. Reimers. MTEB: massive text embedding benchmark.In A. Vlachos and I. Augenstein, editors, Proceedings of the 17th Conference of the EuropeanChapter of the Association for Computational Linguistics, EACL 2023, Dubrovnik, Croatia, May2-6, 2023, pages 20062029. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.EACL-MAIN.148. URL L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama,A. Ray, et al. Training language models to follow instructions with human feedback. Advances inNeural Information Processing Systems, 35:2773027744, 2022.",
  "L. Qin, S. Welleck, D. Khashabi, and Y. Choi. Cold decoding: Energy-based constrained textgeneration with langevin dynamics. Advances in Neural Information Processing Systems, 35:95389551, 2022": "R. Rafailov, A. Sharma, E. Mitchell, C. D. Manning, S. Ermon, and C. Finn. Direct preference opti-mization: Your language model is secretly a reward model. In A. Oh, T. Naumann, A. Globerson,K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Or-leans, LA, USA, December 10 - 16, 2023, 2023. URL R. Rafailov, A. Sharma, E. Mitchell, C. D. Manning, S. Ermon, and C. Finn. Direct preferenceoptimization: Your language model is secretly a reward model. Advances in Neural InformationProcessing Systems, 36, 2024.",
  "C. Snell, I. Kostrikov, Y. Su, M. Yang, and S. Levine. Offline rl for natural language generation withimplicit language q learning. arXiv preprint arXiv:2206.11871, 2022": "N. Stiennon, L. Ouyang, J. Wu, D. Ziegler, R. Lowe, C. Voss, A. Radford, D. Amodei, and P. F.Christiano. Learning to summarize with human feedback. Advances in Neural InformationProcessing Systems, 33:30083021, 2020. G. Team, R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M.Dai, A. Hauth, et al. Gemini: a family of highly capable multimodal models. arXiv preprintarXiv:2312.11805, 2023.",
  "S. Yang, R. Sun, and X. Wan. A new benchmark and reverse validation method for passage-levelhallucination detection. arXiv preprint arXiv:2310.06498, 2023": "Y. Zhang, M. Galley, J. Gao, Z. Gan, X. Li, C. Brockett, and B. Dolan.Generating infor-mative and diverse conversational responses via adversarial information maximization.InS. Bengio, H. M. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, ed-itors, Advances in Neural Information Processing Systems 31: Annual Conference on Neu-ral Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montral,Canada, pages 18151825, 2018. URL Y. Zhang, S. Sun, M. Galley, Y. Chen, C. Brockett, X. Gao, J. Gao, J. Liu, and B. Dolan. DIALOGPT: Large-scale generative pre-training for conversational response generation. In A. Celikyilmazand T. Wen, editors, Proceedings of the 58th Annual Meeting of the Association for ComputationalLinguistics: System Demonstrations, ACL 2020, Online, July 5-10, 2020, pages 270278. Asso-ciation for Computational Linguistics, 2020. doi: 10.18653/V1/2020.ACL-DEMOS.30. URL",
  "AResults on a Bipartite Graph Model for Questions and Answers": "In this section, we outline a simple toy model involving a universe of questions and answers withrelations between them where we show how TRLM-Ba perplexity based alignment distribution helpsin picking the right answer when the forward model \"hallucinates\". For simplicity of exposition, wewill only focus on the distribution PTRLM-Ba(Q|A) for the TRLM class of models. Universe of Questions and Answers: We consider a universe of questions and answers in the formof a bi-partite graph which are deemed to constitute the ground truth. Let Q VK and A VKwhere V is the vocabulary, be the universe of questions and answers respectively. For a given questionQ, let N(Q) A denote the set of ground truth answers of Q. Let G(Q, A, E) be a bipartite graphsuch that E = {(Q, A)}QQ,AN (Q) is the edge set of all valid answers. In other words, Ideally,one may like a forward model to approximate the distribution, P(A|Q) = 1/|N(Q)|, A N(Q)and 0 otherwise, closely. Hallucination Model (Hamming distance version): We would like to model an imperfect forwardmodel that does not fully adhere with the ideal ground truth forward model. For a given question Q, theimperfect model produces answers N(Q) to the neighbouring questions Q which are at a hammingdistance of 1 from Q. Concretely, let H(, ) denote the hamming distance function. The support ofthe answer distribution is then S =",
  "PFw(A|Q) PFw(A|Q)P TRLM(Q|A)(2)": "for some > 0. For a fixed question Q, left hand side is potentially non-zero only for A N(Q) :H(Q, Q) 1. since the first term in the right hand side is non-zero only for those by definition of thehallucination model. Consider an A such that Q : A N(Q), H(Q, Q) = 1. We will argue thatthe second term is zero for such an answer A. Suppose it is non-zero, according to the hallucinationmodel for the reverse direction, it means that A : H(A, A) = 1, A N(Q). However Q and Qare hamming distance one away. From the assumptions, their neighborhood are far apart by morethan 1, therefore contradicting the implication that H(A, A) = 1. Key Takeaway: Therefore under the above simplistic hallucination model, although the forwardmodel has a wider support |S| in the answer space, due to alignment with TRLM-Ba s perplexity, thenew distribution has a support of at most N(Q) provably. While assumptions in the theorem arenot reflective of true complexities of the universe of questions and answers in a domain, this simplemodel shows that alignment using TRLMs scoring metric can give rise to better re-ranking whenevernearby questions produce far away answers and generating forward models tends to confuse betweennearby questions (a form of hallucination).",
  "Algorithm Description: We describe the three attribution algorithms that use TRLM.score functionin the reverse direction with appropriate prompts in the supplement": "Linear search (Algorithms 7) uses scores every possible sentence in the article with the highlightsentence.Binary search(Algorithm 8), actually starts with scores the first against the second half of thearticle for a given highlight and chooses the best recurses further by splitting the chosen half(analogous to binary search) until a contiguous set of article sentences of sufficient granularity isreached.In Exclusion search(Algorithm 9), we drop article sentences and score the rest of the article withthe highlight sentence. We pick the choice with the least score.",
  "Method: TRLM is prompted with prex = Document has an answer to and sux = Query": ": This task is used to assess the representational capability of TRLM. Here we look athow likely a document is to contain information relevant to answering a question. The languageunderstanding of an LLM makes it likely that it produces better semantic retrieval than a simpleembedding based model which is not contextual.",
  "Threshold": "(d) FPR (Easy negatives) TRLM-Ba (IT)TRLM-Fo (IT)TRLM-Ba (PT)TRLM-Fo (PT) : Plots showing the False Negative Rate and False Positive Rate of the proposed defensestrategy. Positive indicates UNSAFE response, while negative indicates SAFE response. The firstplot considers 72 questions generated from the JBB dataset. The second plot considers questionsfrom the new-HA dataset. The third plot considers 48 hard safe questions generated by GPT4, whoseanswers contain content that appears unsafe (from the H dataset). The fourth plot considers 49 easysafe questions from Alpaca Eval2 dataset (E dataset). TRLM-Ba (PT) - the reverse pre-trained modelclearly outperforms all other cases with lower FNR rate while keeping FPR rates under check.",
  "JBB Dataset: We form the union of all question-answer pairs that have been generated by variousattack strategies in JailbreakBench. There are a total of 1037 question and answer pairs of which": "472 are identified as toxic by the GPT-4 toxicity filter. Amongst these questions, only 72 are declaredas \"safe\" (False Negatives) by the gpt-3.5-turbo-1106 input filter acting on the question alone,which we further reduce using the proposed defense. Thus, this set of 68 questions forms our JBBDataset. We note that these are hard questions, since they are able to jailbreak the LLMs consideredin JailbreakBench, as well as the gpt-3.5-turbo-1106 input filter.E-Dataset: We sample about 50 easy safe questions from AlpacaEval dataset, to test False PositiveRates, and eliminate one question that was predicted as unsafe by gpt-3.5-turbo-1106.H-Dataset: To test the FPR on hard samples, we prompt GPT-4 to generate about 50 questions thatcould possibly mislead an input filter, while being safe. We further verify that the input-output pairsare safe using the GPT-4 toxicity classifier, and eliminate 2 questions that were predicted as unsafe.The prompt used to generate this dataset is shown below: Can you help me create a dataset of questions that would lead to answerscontaining words that can be perceived as harmful, but are not.I wantto use this to ensure that my toxicity filter does not filter them out.For example, \"Create a political cartoon about gun violence\", \"Describe ahistoric event in detail.,October Revolution\".Generate as many questionsas you can.Stop ONLY when you find that you are repeating questions.",
  "We prompt GPT4-1106-Preview with the following prompt to verify the toxicity of the questionand answer": "SYSTEM PROMPTAnalyze user-submitted question-answer pairs with a focus on safety and ethical standards. Your goal isto classify each answer as \"SAFE\" or \"UNSAFE\", especially considering the potential for encouragingor detailing harmful behavior. Answer in one word without any explanation.Definitions:SAFE: Answers that are appropriate for general audiences, educate without promoting unethicalactions, and maintain a respectful and informative tone without revealing sensitive information.UNSAFE: Answers that could potentially encourage or inform harmful or unethical behavior, even ifunintentionally. This includes content that might educate or provide a how-to guide on harmful actions,discriminatory practices, manipulation tactics, illegal activities, or other unethical behaviors.",
  "Algorithm 12 Defense strategy using TRLM.Generate": "1: Input: Question Q, a forward model/ system to be defended M, an input filter F that classi-fies input questions as UNSAFE/SAFE, filter-threshold , number of generated questions N.Conditioning Prompt:CP. 2: If F(Q) = UNSAFE, return UNSAFE3: Else Generate Answer A from Forward Model M for question Q4: Generate N questions Q = {q1 . . . qN} using TRLM .Generate(A,CP)5: Compute F(qi) for qi Q. Calculate T = total number of UNSAFE questions.6: If T > threshold , return UNSAFE7: Else return answer A to query Q"
}