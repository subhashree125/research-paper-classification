{
  "Abstract": "In ill-posed imaging inverse problems, there can exist many hypotheses that fitboth the observed measurements and prior knowledge of the true image. Ratherthan returning just one hypothesis of that image, posterior samplers aim to ex-plore the full solution space by generating many probable hypotheses, which canlater be used to quantify uncertainty or construct recoveries that appropriatelynavigate the perception/distortion trade-off. In this work, we propose a fast andaccurate posterior-sampling conditional generative adversarial network (cGAN)that, through a novel form of regularization, aims for correctness in the posteriormean as well as the trace and K principal components of the posterior covariancematrix. Numerical experiments demonstrate that our method outperforms contem-porary cGANs and diffusion models in imaging inverse problems like denoising,large-scale inpainting, and accelerated MRI recovery. The code for our model canbe found here:",
  "Introduction": "In image recovery, the goal is to recover the true image x from noisy/distorted/incomplete measure-ments y = M(x). This arises in linear inverse problems such as denoising, deblurring, inpainting,and magnetic resonance imaging (MRI), as well as in non-linear inverse problems like phase-retrievaland image-to-image translation. For all such problems, it is impossible to perfectly recover x from y. In much of the literature, image recovery is posed as point estimation, where the goal is to returna single best estimate x. However, there are several shortcomings of this approach. First, its notclear how to define best, since L2- or L1-minimizing x are often regarded as too blurry, whileefforts to make x perceptually pleasing can sacrifice agreement with the true image x and causehallucinations , as we show later. Also, many applications demand not only a recovery x butalso some quantification of uncertainty in that recovery . As an alternative to point estimation, posterior-sampling-based image recovery aims togenerate P 1 samples {xi}Pi=1 from the posterior distribution px|y(|y). Posterior samplingfacilitates numerous strategies to quantify the uncertainty in estimating x, or any function of x,from y . It also can help with visualizing uncertainty and increasing robustness to adversarialattacks . That said, the design of accurate and computationally-efficient posterior samplersremains an open problem. Given a training dataset of image/measurement pairs {(xt, yt)}Tt=1, our goal is to build a generatorG that, for a given y, maps random code vectors z N(0, I) to samples of the posterior,i.e., x = G(z, y) px|y(, y). There are many ways to approximate the ideal generator. Therecent literature has focused on conditional variational autoencoders (cVAEs) , conditional",
  "arXiv:2411.00605v1 [eess.IV] 1 Nov 2024": "normalizing flows (cNFs) , conditional generative adversarial networks (cGANs) ,and Langevin/score/diffusion-based generative models . Although diffusion models havegarnered a great share of recent attention, they tend to generate samples several orders-of-magnitudeslower than their cNF, cVAE, and cGAN counterparts. With the goal of fast and accurate posterior sampling, recent progress in cGAN training has beenmade through regularization. For example, Ohayon et al. proposed to enforce correctness inthe generated y-conditional mean using a novel form of L2 regularization. Later, Bendel et al. proposed to enforce correctness in the generated y-conditional mean and trace-covariance usingL1 regularization plus a correctly weighted standard-deviation (SD) reward, and gave evidence thattheir rcGAN competes with contemporary diffusion samplers on MRI and inpainting tasks. Soonafter, Man et al. showed that L2 regularization and a variance reward are effective when trainingcGANs for JPEG decoding. More details are given in . In a separate line of work, Nehme et al. trained a Neural Posterior Principal Components (NPPC)network to directly estimate the eigenvectors and eigenvalues of the y-conditional covariance matrix,which allows powerful insights into the nature of uncertainty in an inverse problem. Our contributions. Inspired by regularized cGANs and NPPC, we propose a novel pcaGAN thatencourages correctness in the K principal components of the y-conditional covariance matrix, as wellas the y-conditional mean and trace covariance, when sampling from the posterior. We demonstratethat our pcaGAN outperforms existing cGANs and diffusion models on posterior sampling tasks like denoising, large-scale inpainting, and accelerated MRI recovery,while sampling orders-of-magnitude faster than those diffusion models. We also demonstrate thatpcaGAN recovers the principal components more accurately than NPPC using approximately thesame runtime.",
  "Background": "Our approach builds on the rcGAN regularization framework from , which itself builds on thecGAN framework from , both of which we now summarize. Let X, Y, and Z denote the domainsof x, z, and y, respectively. The goal is to design a generator G : Z Y X where, for a giveny, the random x = G(z, y) induced by the code vector z pz (with z independent of y) has adistribution that is close to the true posterior px|y(, y) in the Wasserstein-1 distance, given by",
  "Ladv(, ) Ex,z,y{D(x, y) D(G(z, y), y)}(3)": "over and minimizing Ladv(, ) + Lgp() over , where Lgp() is a gradient penalty thatencourages D L1 . In practice, the expectation in (3) is approximated by a sample averageover the training data {(xt, yt)}. In the typical case that the training data includes only a single image xt for each measurement vectoryt, minimizing Ladv(, ) alone does not encourage the generator to produce diverse samples. Rather,it leads to a form of mode collapse where the code z is ignored. In , Adler and Oktem proposeda two-sample discriminator that encourages diverse generator outputs without compromising theWasserstein objective (2). In , Bendel et al. instead proposed to regularize Ladv(, ) in a waythat encourages correct posterior means and trace-covariances, i.e.,",
  "PPi=1 xi.(9)": "The reward weight SD in (6) is automatically adjusted to accomplish (5) during training .We note that the L1,P () regularization proposed in is closely related to the regularizationL2,P () Ex,z1,...,zP,y{xx(P )22} proposed earlier by Ohayon et al. in . A detailed discussionof the advantages and disadvantages of various cGAN regularizations can be found in .",
  "Leval() EyEx,z1,...,zP|y Kk=11 k/k2y.(12)": "Here, {(vk, k)}Kk=1 denote the principal eigenvectors and eigenvalues of the -dependent generatedcovariance matrix x|y and {(vk, k)}Kk=1 denote the principal eigenvectors and eigenvalues of thetrue covariance matrix x|y. Because (11) is the classical PCA objective , minimizing Levec()over will drive the generated principal eigenvector vk towards the true principal eigenvector vkfor each k = 1, . . . , K. Likewise, minimizing Leval() over will drive the generated principaleigenvalue k towards the true principal eigenvalue k for each k = 1, . . . , K. Based on ourexperiments, putting k in the denominator works better than the numerator and the squared error in(12) works better than an absolute value. In practice, the expectations in (11)-(12) are replaced by sample averages over the training data. Inthe typical case that the training data includes only a single image xt for each measurement vectoryt, the quantities x|y and {k} in (11)-(12) are unknown and non-trivial to estimate for each yt.Hence, when training the pcaGAN, we approximate them with learned quantities. This must be donecarefully, however. For example, if x|y in (11) was simply replaced by the -dependent quantityx|y, then minimizing Levec() over would encourage x|y to become overly large in order to driveLevec() to a large negative value. Algorithm 1 details our proposed approach to training the pcaGAN. In particular, it describes thesteps used to perform a single update of the generator parameters based on the training batch{(xb, yb)}Bb=1. Before diving into the details, we offer a brief summary of Algorithm 1. For the initialepochs, the rcGAN objective LrcGAN alone is optimized, which allows the generated posterior meanx|y to converge to the vicinity of x|y. Starting at Eevec epochs, the Levec() regularization from(11) is added, but with x|y approximated as StopGrad(x|y). The use of StopGrad forces Levec()to be minimized by manipulating the eigenvectors {vk}Kk=1 and not the generated posterior meanx|y. These eigenvectors are computed using an SVD of centered approximate-posterior samples.To reduce the computational burden imposed by this SVD, a lazy regularization approachis adopted, which computes Levec() only once every M training steps. Training proceeds in thismanner until the eigenvectors {vk} converge. Starting at Eeval epochs, the Leval() regularizationfrom (12) is added, but with k approximated as",
  "Algorithm 1 pcaGAN generator-training iteration": "Require: number of estimated eigen-components K, number of samples used for eigenvector andeigenvalue regularization Ppca, number of samples used for rcGAN regularization Prc, epochat which to involve eigenvector regularization Eevec, epoch at which to involve eigenvalueregularization Eeval, lazy update period M, adversarial loss weight adv, std regularizationweight SD, eigenvector and eigenvalue regularization weight pca, training batch {(xb, yb)}Bb=1,current model parameters , current training epoch e, the current training step s",
  ": end for25:26: Adam(, L())": "where StopGrad is used so that the optimization focuses on {k}. The rational behind (13) is that,when vk = vk and x|y = x|y, the terms [vk (xb x|y)]2 and [vk (xj x|y)]2 j all equal kin yb-conditional expectation. This expectation is approximated using a (1 + Ppca)-term sampleaverage in (13) via the squared norm. The eigenvalues {k} in (12) are computed using the previouslydescribed SVD and the regularization schedule is again M-lazy. We now provide additional details on Algorithm 1. After the loss is initialized in line 1, the followingsteps are executed for each measurement vector yb in the batch. First, approximate posterior samples{xi}Prci=1 are generated in line 5, where Prc = 2 as per the suggestion in . Using these samples,the adversarial component of the loss is added in line 6 and the rcGAN regularization is added inline 8. Starting at epoch Eevec, lines 11-16 are executed whenever the training iteration is a multipleof M. Nominally, Eevec is set where the validation PSNR of (an empirical approximation ofx|y) stabilizes and M = 100. Within those lines, samples {xj}Ppcaj=1 are generated in line 12 (wherenominally Ppca = 10K), their sample mean is computed in line 13, and the SVD of the centeredsamples is computed in line 14. The top K right singular vectors are then extracted in line 15 in orderto construct the Levec() regularization, which is added to the overall generator loss L() in line 16.Starting at epoch Eeval, where nominally Eeval = Eevec + 25, lines 20-22 are executed whenever thetraining iteration is a multiple of M. In line 20, the top K eigenvalues {k} are constructed fromthe previously computed singular values and, in line 22, the Leval() regularization is constructed",
  "(c) W2(px|y, px|y)/d vs. d": ": Gaussian experiment. Wasserstein-2 distance versus (a) lazy update period M for pcaGANwith d = 100 = K, (b) estimated eigen-components K for pcaGAN with d = 100 and M = 100,and (c) problem dimension d for all methods under test with K = d and M = 100. and added to the overall training loss. The construction of Leval() was previously described around(13). Finally, once the losses for all batch elements have been incorporated, the gradient L() iscomputed using back-propagation and a gradient-descent step of is performed using the Adamoptimizer in line 26.",
  "Recovering synthetic Gaussian data": "Here our goal is to recover x N(x, x) Rd from y = Mx + w Rd, where M masks xat even indices and noise w N(0, 2I) is independent of x with 2 = 0.001. Since x and yare jointly Gaussian, the posterior is Gaussian posterior with x|y = x + xy1y (y y) andx|y = x xy1y yx, where x, y, x, y are marginal and xy, yx are joint statistics. We generate random x N(0, I) and x with half-normal eigenvalues k (see additional detailsin App. A), and consider a sequence of problem sizes d = 10, 20, 30, . . . , 100. For each d, wegenerate 70 000 training, 20 000 validation, and 10 000 test samples. The generator and discriminatorare simple multilayer perceptrons (see App. D.1) trained for 100 epochs with K = d, Eevec = 10,adv = 105, and pca = 102. Competitors. We compare the proposed pcaGAN to rcGAN and NPPC . rcGAN usesthe same generator and discriminator architectures as pcaGAN and is trained according to (6) withadv = 105 and Prc = 2. For NPPC, we use the authors implementation with K = d and someminor modifications to work with vector data. To evaluate performance, we use the Wasserstein-2(W2) distance between px|y and px|y, which in the Gaussian case reduces to",
  "For the cGANs, we compute x|y and x|y empirically from 10d samples, while for NPPC we usethe conditional mean, eigenvalues, and eigenvectors returned by the approach": "Results. a examines the impact of the lazy update period M on pcaGANs W2 distance atd = 100 with K = d. Based on this figure, to balance performance with training overhead, we setM = 100 for all future experiments. b examines the impact of K on W2 distance for thepcaGAN with d = 100. It shows that using K < d causes a relatively mild increase in W2 distance,as expected due to the half-normal distribution on the true eigenvalues k. c shows that theproposed pcaGAN outperforms rcGAN and NPPC in W2 distance for all problem sizes d.",
  ": For (a) pcaGAN and (b) NPPC, this figure shows the true image x, noisy measurements y,the conditional mean x|y, principal eigenvectors {vk}, and two perturbations of x|y": "validation images, and we use the entire MNIST fold set for testing. For pcaGAN and rcGAN, weuse a UNet generator and the encoder portion of the same UNet followed by one dense layer asthe discriminator. pcaGAN was trained for 125 epochs with Eevec = 25, adv = 105, pca = 101,and K {5, 10}. Competitors. We again compare the proposed pcaGAN to rcGAN and NPPC. For rcGAN we usedthe same generator and discriminator architectures as pcaGAN and trained according to (6) withadv = 105 and Prc = 2. For NPPC, we used the authors MNIST implementation from .",
  "Following the NPPC paper , we evaluate performance using root MSE (rMSE) Ex,y{x x|y2}": "and residual error magnitude (REM5) Ex,y(I V 5 V 5)e2, where e = x x|y and V 5 is an282 5 matrix whose kth column equals the kth principal eigenvector vk. For the cGANs, we usex|y = x(P ) and compute {vk} from the SVD of a matrix of centered samples {xi}Pi=1, both withP = 100. For NPPC, we use the conditional means and eigenvectors returned by the approach. Forperformance evaluation, we also consider Conditional Frechet Inception Distance (CFID) withInceptionV3 features. CFID is analogous to Frechet Inception Distance (FID) but applies toconditional distributions (see App. B for more details). Results. shows rMSE, REM5, CFID, and the reconstruction time for a batch of 128 imageson the test fold. (NPPC does not generate image samples and so CFID does not apply.) The tableshows that the proposed pcaGAN wins in all metrics, except for rMSE where NPPC wins. This isnot surprising because NPPC computes x|y using a dedicated network trained to minimize MSEloss. NPPC also generates its eigenvectors slightly quicker than pcaGAN generates samples. also shows that pcaGAN performance improves as K increases from 5 to 10, despite the fact thatREM5 uses only the top 5 eigenvectors. Figure E.1 shows examples of the 5 principal eigenvectorsand posterior mean learned by pcaGAN and NPPC. The eigenvectors of pcaGAN are more structuredand less noisy than those of NPPC. Figure E.1 also shows x|y+vk for and k {1, 4}.Additional figures can be found in App. E.1.",
  "ModelCFID1CFID2CFID3FIDAPSDTime (4)CFID1CFID2CFID3FIDAPSDTime (4)": "E2E-VarNet (Sriram et al. )16.0813.0710.2638.880.0310ms36.8629.9023.8244.040.0316msLangevin (Jalal et al. )33.05--31.435.9e-614 min48.59--52.627.6e-614 mincGAN (Adler & Oktem )19.0012.057.0029.773.9e-6217 ms59.9440.2426.1031.817.7e-6217 mspscGAN (Ohayon et al. )13.7410.567.5337.287.2e-8217 ms39.6731.8124.0643.397.7e-7217 msrcGAN (Bendel et al. )9.715.271.6925.623.8e-6217 ms24.0413.203.8328.437.6e-6217 mspcaGAN (Ours)8.784.481.2925.024.4e-6217 ms21.6511.473.2128.356.5e-6217 ms brain volumes with at least 8 coils, crops to 384 384 pixels, and compresses to 8 virtual coils .This yields 12 200 training, 2 376 testing, and 784 validation images. To create each yt, we transformxt to the k-space, subsample using the Cartesian GRO mask at accelerations R = 4 and R = 8,and transform the zero-filled k-space measurements back to the image domain.",
  "We train pcaGAN for 100 epochs with K = 1, Eevec = 25, adv = 105, and pca = 102 andselect the final model using validation CFID computed with VGG-16 features": "Competitors. We compare the proposed pcaGAN to rcGAN , pscGAN , Adler & OktemscGAN , the Langevin approach , and the E2E-VarNet . All cGANs use the generatorand discriminator architectures from and enforce data-consistency . For rcGAN and theLangevin approach, we did not modify the authors implementations from and except to usethe GRO sampling mask. For E2E-VarNet, we use the GRO mask, hyperparameters, and trainingprocedure from . Following , we convert the multicoil outputs xi to complex-valued images using SENSE-basedcoil combining with ESPIRiT-estimated coil sensitivity maps, and compute performanceon magnitude images. All feature-based metrics (CFID, FID, LPIPS, DISTS) were computed withAlexNet features to show that pcaGAN does not overfit to the VGG-16 features used for validation.It was shown in that image-quality metrics computed using ImageNet-trained feature generatorslike AlexNet and VGG-16 perform comparably to metrics computed using MRI-trained featuregenerators in terms of correlation with radiologists scores.",
  "Results. shows CFID, FID, APSD ( 1": "PPi=11N x(P )xi2)1/2, and 4-sample generationtime for the methods under test. Due to its slow sample-generation time, we evaluate the CFID, FID,and APSD of the Langevin technique using the 72-image test from . But due to the bias ofCFID at small sample sizes , we evaluate the other methods using all 2 376 test images (CFID2)and again using all 14 576 training and test images (CFID3). shows that pcaGAN yieldsbetter CFID and FID than the competitors. All cGANs generated samples 34 orders-of-magnitudefaster than the Langevin approach .",
  "ModelCFIDFIDLPIPSTime (40 samples)": "DPS (Chung et al. )7.262.000.124514 minDDNM (Wang et al. )11.303.630.140930 sDDRM (Kawar et al. )13.175.360.15875 spscGAN (Ohayon et al. )18.448.400.1716325 msCoModGAN (Zhao et al. )7.852.230.1290325 msrcGAN (Bendel et al. )7.512.120.1262325 mspcaGAN (Ours)7.081.980.1230325 ms shows PSNR, SSIM, LPIPS , and DISTS for the P-sample average x(P ) at P {1, 2, 4, 8, 16, 32} and R = 8. (See App. C for R = 4.) It has been shown that DISTS correlatesparticularly well with radiologist scores . The E2E-VarNet achieves the best PSNR, but theproposed cGAN achieves the best LPIPS and DISTS when P = 2 and the best SSIM when P = 8.This P-dependence is related to the perception-distortion trade-off and consistent with thatreported in . shows zoomed versions of two recoveries xi and the sample average x(P ) with P = 32.Appendices E.2 and E.3 show additional plots of x(P ) that visually demonstrate the perception-distortion trade-off.",
  "Large-scale inpainting": "Our final goal is to inpaint a face image with a large randomly generated masked region. For thistask, we use 256 256 FFHQ face images and the mask generation procedure from . Werandomly split the FFHQ training fold into 45 000 training and 5 000 validation images, and we usethe remaining 20 000 images for testing.",
  "For pcaGAN, we use CoModGANs generator and discriminator architecture and train for 100epochs using K = 2, Eevec = 25, adv = 5 103, and pca = 103": "Competitors. We compare with CoModGAN , pscGAN , rcGAN , and state-of-the-art diffusion methods DDRM (20 NFEs) , DDNM (100 NFEs) , and DPS (1000 NFEs). CoModGAN, pscGAN, and rcGAN differ from pcaGAN only in generator regularization andCoModGANs use of discriminator MBSD . For rcGAN, DDNM, DDRM, and DPS, we use theauthors implementations from , , , and with mask generation from . FID andCFID were evaluated on our 20 000 image test set with P =1. Results. shows test CFID, FID, LPIPS, and 40-sample generation time. The table shows thatthe proposed pcaGAN wins in CFID, FID and LPIPS, and that the four cGANs generate samples34 orders-of-magnitude faster than DPS. shows five generated samples for each methodunder test, along with the true and masked image. pcaGAN shows better subjective quality than thecompetitors, as well as good diversity. Additional figures can be found in App. E.4.",
  "Discussion": "When training a cGAN, the overall goal is that the samples {xi} generated from a particular yaccurately represent the true posterior px|y(|y). Achieving this goal is challenging when trainingfrom paired data {(xt, yt)}, because such datasets provide only one example of x for each given y.Early methods like focused on providing some variation among {xi}, but did not aim forthe correct variation. The rcGAN from focused on providing the correct amount of variation byenforcing tr(x|y) = tr(x|y), and the proposed pcaGAN goes farther by encouraging x|y and x|yto agree along K principal directions. Our experiments demonstrate that pcaGAN yields a notableimprovement over rcGAN and outperforms contemporary diffusion approaches like DPS . PCA principles have also been used in unconditional GANs, where the goal is to train a generator Gthat turns codes z N(0, I) into outputs x = G(z) that match the true marginal distribution pxfrom which the training samples {xt} are drawn. For example, the eigenGAN from aims to trainin such a way that semantic attributes are learned (without supervision) and can be independentlycontrolled by manipulating individual entries of z. But their goal is clearly different from ours. Limitations.We acknowledge several limitations of our work. First, generating Ppca = 10Ksamples during training can impose a burden on memory when x is high dimensional. In the multicoilMRI experiment, x Rd for d = 2.4e6, which limited us to K = 1 at batch size 2. Second, althoughour focus is on designing a fast and accurate posterior sampler, more work is needed on how to bestuse the generated samples across different applications. Using them to compute rigorous uncertainty",
  "Conclusion": "In this work, we proposed pcaGAN, a novel image-recovery cGAN that enforces correctness in theK principal components of the conditional covariance matrix x|y, as well as in the conditionalmean x|y and trace-covariance tr(x|y). Experiments with synthetic Gaussian data showed pcaGANoutperforming both rcGAN and NPPC in Wasserstein-2 distance across a range of problemsizes. Experiments on MNIST denoising, accelerated multicoil MRI, and large-scale image inpaintingshowed pcaGAN outperforming several other cGANs and diffusion models in CFID, FID, PSNR,SSIM, LPIPS, and DISTS metrics. Furthermore, pcaGAN generates samples 34 orders-of-magnitudefaster than the tested diffusion models. The proposed pcaGAN thus provides fast and accurateposterior sampling for image recovery problems, which enables uncertainty quantification, fairness inrecovery, and easy navigation of the perception/distortion trade-off.",
  "N. M. Gottschling, V. Antun, A. C. Hansen, and B. Adcock, The troublesome kernelOn hallucinations,no free lunches and the accuracy-stability trade-off in inverse problems, arXiv:2001.01258, 2023. 1": "M. Abdar, F. Pourpanah, S. Hussain, D. Rezazadegan, L. Liu, M. Ghavamzadeh, P. Fieguth, X. Cao,A. Khosravi, U. R. Acharya, et al., A review of uncertainty quantification in deep learning: Techniques,applications and challenges, Info. Fusion, vol. 76, pp. 243297, 2021. 1 B. Lambert, F. Forbes, S. Doyle, H. Dehaene, and M. Dojat, Trustworthy clinical AI solutions: Aunified review of uncertainty quantification in deep learning models for medical image analysis, ArtificialIntelligence in Medicine, p. 102830, 2024. 1",
  "Y. Wang, J. Yu, and J. Zhang, Zero-shot image restoration using denoising diffusion null-space model, inProc. Intl. Conf. Learn. Rep., 2023. 1, 2, 8": "G. Ohayon, T. J. Adrai, M. Elad, and T. Michaeli, Reasons for the superiority of stochastic estimatorsover deterministic ones: Robustness, consistency and perceptual quality, in Proc. Intl. Conf. Mach. Learn.,pp. 2647426494, 2023. 1 G. Ohayon, T. Adrai, G. Vaksman, M. Elad, and P. Milanfar, High perceptual quality image denoising witha posterior sampling CGAN, in Proc. IEEE Intl. Conf. Comput. Vis. Workshops, vol. 10, pp. 18051813,2021. 2, 3, 7, 8, 14",
  "M. Soloveitchik, T. Diskin, E. Morin, and A. Wiesel, Conditional Frechet inception distance,arXiv:2103.11521, 2021. 6, 7, 13": "M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter, GANs trained by a two time-scaleupdate rule converge to a local Nash equilibrium, in Proc. Neural Info. Process. Syst. Conf., vol. 30, 2017.6, 13 J. Zbontar, F. Knoll, A. Sriram, M. J. Muckley, M. Bruno, A. Defazio, M. Parente, K. J. Geras, J. Katsnelson,H. Chandarana, Z. Zhang, M. Drozdzal, A. Romero, M. Rabbat, P. Vincent, J. Pinkerton, D. Wang,N. Yakubova, E. Owens, C. L. Zitnick, M. P. Recht, D. K. Sodickson, and Y. W. Lui, fastMRI: An opendataset and benchmarks for accelerated MRI, arXiv:1811.08839, 2018. 6",
  "K. P. Pruessmann, M. Weiger, M. B. Scheidegger, and P. Boesiger, SENSE: Sensitivity encoding for fastMRI, Magn. Reson. Med., vol. 42, no. 5, pp. 952962, 1999. 7": "M. Uecker, P. Lai, M. J. Murphy, P. Virtue, M. Elad, J. M. Pauly, S. S. Vasanawala, and M. Lustig,ESPIRiTan eigenvalue approach to autocalibrating parallel MRI: Where SENSE meets GRAPPA, Magn.Reson. Med., vol. 71, no. 3, pp. 9901001, 2014. 7 P. M. Adamson, A. D. Desai, J. Dominic, C. Bluethgen, J. P. Wood, A. B. Syed, R. D. Boutin, K. J. Stevens,S. Vasanawala, J. M. Pauly, A. S. Chaudhari, and B. Gunel, Using deep feature distances for evaluatingMR image reconstruction quality, in Proc. Neural Info. Process. Syst. Workshop, 2023. 7 R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang, The unreasonable effectiveness of deepfeatures as a perceptual metric, in Proc. IEEE Conf. Comp. Vision Pattern Recog., pp. 586595, 2018. 8,13",
  ": end for": "We begin with dimension d = 100, generating random mean (100)x R100 and eigenvalues{(100)k}100k=1. To construct the eigenvectors {v(100)k}100k=1, we perform a QR decomposition on a100100 matrix with i.i.d. N(0, 1) entries and set v(100)kas the kth column of Q. For each remainingd {90, 80, . . . , 10}, we construct (d)x , {(d)k }, and u(d)kby truncating the previous quantities toensure some level of continuity across d.",
  "W2(pa, pb) minpa,b(pa,pb) Ea,b{a b22},(B.2)": "where (pa, pb) pa,b : pa =pa,b db and pb =pa,b dadenotes the set of joint distributionspa,b with marginal distributions pa and pb. Similarly to how FID is computed for marginaldistributions, CFID approximates (B.1) for conditional distributions. In particular, the random vectorsx, x, and y are replaced by low-dimensional embeddings x, x, and y, generated by the convolutionallayers of a deep network (e.g., InceptionV3 or VGG-16 or AlexNet), and the embedding distributionspx|y and px|y are approximated by multivariate Gaussians. We compute CFID using the code from ,which is described in .",
  "CAdditional MRI results for acceleration R = 4": "shows PSNR, SSIM, LPIPS , and DISTS for the P-sample average x(P ) at P {1, 2, 4, 8, 16, 32} for R = 4. In this case, the E2E-VarNet attains the best PSNR and SSIM, whilepcaGAN performs best in LPIPS and DISTS when P = 2 and P = 1, respectively. These results,",
  "D.1Synthetic Gaussian recovery": "cGAN training. We choose adv = 105, nbatch = 64, Prc = 2, and train for 100 epochs for bothrcGAN and pcaGAN. Running PyTorch on a server with 4 Tesla A100 GPUs, each with 82 GB ofmemory, the cGAN training for d = 100 takes approximately 8 hours, with training time decreasingwith smaller d. For pcaGAN, we choose K = d for each d in this experiment (unless otherwisenoted) and pca = 102. cGAN architecture. We exploit the Gaussian nature of the problem, constructing G with twodense layers; one which takes in y as input and one which takes in z as input. The output of eachlayer is added, yielding x. Similarly, D is comprised of a single dense layer which takes in theconcatenation of x / x and y and outputs a scalar. We use this architecture for both rcGAN andpcaGAN. Note that there is no listed license for rcGAN. NPPC. For NPPC, we use the suggested hyperparameters from and opt to train the MMSEnetwork before training NPPC. We use the suggested architectures from their Gaussian denoisingexperiment and train for 100 epochs with nbatch = 64. We leverage the authors implementationin , modifying it for this Gaussian problem. There is no listed license for NPPC.",
  "The MNIST dataset is available under the GNU general public license, which we respect through ouruse": "cGAN training. We choose adv = 105, nbatch = 64, Prc = 2, and train for 125 epochs for bothrcGAN and pcaGAN. Running PyTorch on a server with 4 Tesla A100 GPUs, each with 82 GB ofmemory, the cGAN training for d = 100 takes approximately 8 hours, with training time decreasingwith smaller d. For pcaGAN, we train two models, one with K = 5 and one with K = 10. In bothcases, pca = 101, Eevec = 25, and Eeval = 50. cGAN architecture.For both cGANs, the generator is the standard UNet which takes in theconcatenation of y and code z. The network consists of 3 pooling layers and 32 initial channels. Theconvolutions use a kernel of size 3 3, instance normalization, and leaky ReLU activations with anegative slope of 0.1. For the encoder portion of the UNet, we use max pooling with a kernel sizeof 2 2 to reduce spatial resolution by a factor of 2. For the decoder portion of the UNet, we usenearest-neighbor interpolation to increase spatial resolution by a factor of 2. The discriminator issimply the encoder portion of the UNet with an additional dense layer appended to map the UNetslatent space to a scalar output. Note that there is no listed license for rcGAN.",
  "For our MRI experiments, we use the fastMRI dataset which is available under the MIT license,which we respect through our use": "cGAN training. We choose adv = 105, pca = 102, nbatch = 2, Prc = 2, K = 1, Eevec = 25,and Eeval = 50 for pcaGAN. For rcGAN, pscGAN , and Adlers cGAN, we use the hyperparametersand training procedure described in . All models were trained for 100 epochs using the Adamoptimizer with a learning rate of 103, 1 = 0, and 2 = 0.99, as in . Running PyTorch ona server with 4 Tesla A100 GPUs, each with 82 GB of memory, the training of each MRI cGAN tookapproximately 1 day. Note that there is no listed license for rcGAN, pscGAN, or Adler and OktemscGAN. cGAN architecture.All four cGANs used the same generator and discriminator architecturesdescribed in , except that Adler and Oktems discriminator used extra input channels to facilitatethe 3-input loss. E2E-VarNet. For the Sriram et al.s E2E-VarNet , we use the same training procedure andhyperparameters outlined in with modification to the sampling pattern as in . As in ,we use the SENSE-based coil-combined image as the ground truth instead of the RSS image. TheE2E-VarNet is available under the MIT license, which we respect. Langevin approach. For Jalal et al.s MRI approach , we do not modify the authors implemen-tation from other than replacing the default sampling pattern with the GRO sampling mask. Weborrow both generated samples and results from . The authors code is available under the MITlicense, which we respect.",
  "For our inpainting experiment, we use the FFHQ dataset which is available under the CreativeCommons BY-NC-SA 4.0 license, which we respect through our use": "cGAN training.We choose adv = 5 103, pca = 103, nbatch = 5, Prc = 2, K = 2,Eevec = 25, and Eeval = 50 for pcaGAN. Running PyTorch on a server with 4 Tesla A100 GPUs,each with 82 GB of memory, the training of our cGAN took approximately 1.5 days.",
  "cGAN architecture. As in , we use the CoModGAN networks from which extend theStyleGAN2 network. The StyleGAN2 architecture is available under the NVIDIA Source CodeLicense, which we respect": "rcGAN. We follow the training procedure outlined in , only modifying the inpainting mask tobe random. The total training time on a server with 4 NVIDIA A100 GPUs, each with 82 GB ofmemory, is roughly 1 day. There is no listed license for rcGAN. pscGAN. We use the same training procedure outlined in , modifying the inpainting masks to berandom and using the L2,P objective described briefly in Sec. 2 with P =8. The total training timeon a server with 4 NVIDIA A100 GPUs, each with 82 GB of memory, is roughly 1.5 days. There isno listed license for pscGAN. CoModGAN. We use the PyTorch implementation of CoModGAN from and train the model.The total training time on a server with 4 NVIDIA A100 GPUs, each with 82 GB of memory, isroughly 1 day. There is no listed license for CoModGAN, beyond the NVIDIA Source Code License.",
  "For all three diffusion methods, we use the pretrained weights from": "DPS. We use the suggested settings for the 256 256 FFHQ dataset and the code from the officialPyTorch implementation . We found the LPIPS-minimizing step-size via grid search overa 1000 image validation set. We generate 1 sample for all 20 000 images in our test set, using abatch-size of 1 and 1000 NFEs. The total generation time on a server with 4 NVIDIA A100 GPUs,each with 82 GB of memory, is roughly 9 days. There is no listed license for DPS. DDNM. We use the code from the official PyTorch implementation . We generate 1 sample forall 20 000 images in our test set, using a batch-size of 1 and 100 NFEs. The total generation time ona server with 4 NVIDIA A100 GPUs, each with 82 GB of memory, is roughly 1.5 days. There is nolisted license for DDNM. DDRM. We use the code from the official PyTorch implementation . We generate 1 sample forall 20 000 images in our test set, using a batch-size of 1 and 20 NFEs. The total generation time on aserver with 4 NVIDIA A100 GPUs, each with 82 GB of memory, is roughly 5.5 hours. There is nolisted license for DDRM.",
  "SampleSampleAverage(P=2)Average(P=4)Average(P=32)Std. Dev.Map": "Figure E.7: Example R = 8 MRI reconstruction. Row one: pixel-wise SD with P = 32, Row two:x(P ) with P = 32, Row three: x(P ) with P = 4, Row four: x(P ) with P = 2, Rows five and six:posterior samples. The arrows indicate regions of meaningful variation across posterior samples."
}