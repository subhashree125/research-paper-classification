{
  "Abstract": "Vision transformers dominate image processing tasks due to their superior perfor-mance. However, the quadratic complexity of self-attention limits the scalability ofthese systems and their deployment on resource-constrained devices. State SpaceModels (SSMs) have emerged as a solution by introducing a linear recurrencemechanism, which reduces the complexity of sequence modeling from quadraticto linear. Recently, SSMs have been extended to high-resolution vision tasks.Nonetheless, the linear recurrence mechanism struggles to fully utilize matrixmultiplication units on modern hardware, resulting in a computational bottleneck.We address this issue by introducing VMeanba, a training-free compression methodthat eliminates the channel dimension in SSMs using mean operations. Our keyobservation is that the output activations of SSM blocks exhibit low variancesacross channels. Our VMeanba leverages this property to optimize computation byaveraging activation maps across the channel to reduce the computational overheadwithout compromising accuracy. Evaluations on image classification and semanticsegmentation tasks demonstrate that VMeanba achieves up to a 1.12x speedup withless than a 3% accuracy loss. When combined with 40% unstructured pruning, theaccuracy drop remains under 3%.",
  "Introduction": "Computer vision has advanced significantly due to deep learning and the availability of large-scaledatasets. Convolutional Neural Networks (CNNs) have become foundational for tasks such as imageclassification and object detection . However, CNNs struggle to capture long-range dependencies. Vision Transformers (ViTs) which utilize self-attention mechanisms,effectively address this limitation but suffer from high computational costs due to quadratic complexity.To mitigate these costs, research has focused on reducing ViT complexity , applyingmodel compression techniques , and exploring alternative architectures likeRWKV and State Space Models (SSMs) .",
  "arXiv:2412.16602v1 [cs.CV] 21 Dec 2024": "various tasks . For example, VMamba achievesd 82.6% top-1 accuracy onImageNet-1k , surpassing Swin Transformer by 1.3% with comparable FLOPs. However,despite reducing computational complexity, SSMs still fail to fully utilize matrix multiplication unitson GPUs, creating a bottleneck in vision-based SSM models. : The GPU kernel time of each operationin a VMamba block. The latency is measured usingfeature maps with an input resolution of 224224.We rank the kernels by their latency and highlightsthe top-5 time-consuming kernels on the bar chart.The selective scan operation is one of the majorcontributors in the VMamba block. To this end, we first analyze the latency break-down of VMamba and identify the selectivescan operation as one of the key bottlenecksin inference. shows that the selectivescan accounts for 14.3% of the total kernel timein a VMamba block. While optimizing selectivescan operation is critical for enhancing SSM effi-ciency, few research works address this problemand optimizing the efficiency of SSMs remainsunexplored. In this paper, we propose VMeanba, a novelactivation compression method designed to op-timize the selective scan operation in VMambablocks. The high-level overview of VMeanbais presented in . The key idea is to re-duce the input tensors channel dimensions inthe associate scan operation by applying a meanoperation. Through analysis of the weight and activation distributions in the trained VMamba model,we identified a smooth pattern with small variances that allows for dimensional reduction. Basedon this observation, we developed the VMeanba block to exploit this pattern, resulting in a moreefficient associate scan operation without compromising accuracy. Experimental results demonstratethat VMeanba achieves up to a 1.12x speedup with less than a 3% accuracy loss. To the best of ourknowledge, this is the first work optimizing of the selective scan operation in VMamba. : Overview of the VMeanba block. VMeanba reduced the channel dimension of the inputs tothe associated scan operation by applying a transform T, thereby simplifying the computation. Theproposed VMeanba components are highlighted in red, while the original selective scan componentsare shown in blue and green, with the green block indicating the main area of optimization.",
  "Distribution Analysis of VMamba": "We conduct an in-depth investigation into the characteristics of each layers output within the Mambablock of VMamba. The output is denoted as ylayer RBDL, where B is the batch size, D is theinner channel dimension utilized by the scan algorithm within the Mamba block, and L is 4x of thefeature map size HW. Our analysis revealed that for each ylayer, the distribution of values across theinner channel dimension is remarkably consistent across different data points, as illustrated in figure3. This observation raised a critical question: Is the full dimensionality D necessary for each ylayer? : The figure illustrates the distribution of inner dimension values of ylayer across variousdata points as a function of sequence length. Notably, the distribution remains remarkably consistentacross different data points for identical l values, as indicated by the arrows. The distribution forl = 195 , shown on the right, provides further evidence of this concentration.",
  "d, d [1, D], d = d(1)": "Given that the scan algorithm in the Mamba block performs a linear transformation, this uniqueproperty of ylayer can be attributed to the inputs A, But and C to the SSM system. Consequently,we propose that a reduced set of inputs ( Areduce, ( But)reduce, Creduce), referred to collectively asIbasis, can effectively represent the original inputs ( A, But, C). By leveraging these reduced inputs,we can optimize the computational efficiency of each Mamba block.",
  "VMeanba": "Building on the findings from section 2.1, we indroduce a new model inference efficiency optimizationmethod called VMeanba, which computes Ibasis for each Mamba block using mean operators. Wefurther design a pipeline to select which layers in the model will undergo this optimization.VMeanba block. The Ibasis is derived by having a transform function T that maps the originalinputs ( A, But, C) to reduced dimension inputs. After processing by the original Mamba block, theoutput is recovered using an inverse transform function T 1. This entire process can be expressed asequation (2).ylayer = T 1(Mamba(T( A, But, C)))(2)In this process, T is defined as the mean operator applied along the inner channel dimensionaxis, and T 1 is defined as the broadcast operator. While the mean transform may lead to a lossof information, it significantly reduces the dimensionality of the inputs from D to 1, with ourexperiments demonstrating that model performance is maintained. The computational complexityanalysis is provided in B.Layer Selection. We developed a pipeline to replace K Mamba blocks with VMeanba blocks. Wetreat the choices of layers as a hyperparameter, determined using the validation set. Specifically, wecalculate the layer impact score Slayer for each layer, and select the layers with the K smallest scoresto apply the VMeanba optimization. The impact score is defined by equation (3):",
  "Results on image classification and semantic segmentation": "Accuracy versus K Analysis. We applied the VMeanba method to VMamba backbone models forboth image classification and semantic segmentation tasks, varying the parameter K, as shown in. Our results indicate that the model accuracy remains largely unaffected when an appropriateK value is chosen. However, there is a trade-off exists: increasing K reduces inference time but leadsto a more pronounced accuracy decline, as indicated by the arrows in the figure. Striking an optimalbalance between accuracy and K is essential. For example, selecting K = 10 for the base model inimage classification and semantic segmentation appears reasonable. In cases where accuracy drop isdeemed unacceptable, one could still opt for a larger K and retrain the model to recover performance.Since this study focuses on a training-free approach, retraining strategies are left for future work.",
  ": Accuracy comparison ofVMeanba with pruning on Linearand Conv2D layers using the basebackbone": "We demonstrated that our VMeanba method can be seam-lessly integrated with other optimization techniques to en-hance model efficiency. Specifically, we explored the effective-ness of combining VMeanba with unstructured pruning on theVMamba base model for the image classification task usingvalue K = 8. The results are summarized in . Pruningwas applied to weight of linear layer or convolution 2D layerusing the l1 norm, with a consistent pruning ratio of 40%. Ourfindings indicate that the VMeanba method is orthogonal topruning, as it enhances efficiency while maintaining compa-rable accuracy, demonstrating that the two techniques can becombined without interference.",
  "Conclusion": "In this work, we introduced VMeanba, a novel, training-freemodel compression technique that reduces the inference time of the Mamba block in VMamba byapplying a mean operation to reduce the dimensionality of input channel tensors in the associatescan operation. Our experimental results demonstrate that VMeanba enhances inference speed andthroughput while maintaining competitive accuracy in VMamba. This work contributes to the field by introducing a practical method for improving VMambasefficiency and suggests future exploration of the dimensionality of input channel tensors and the kernelfusion of the discretization and selective scan operations to improve GPU utilization. Additionally,we envision extending VMeanba to other computer vision tasks to evaluate its broader applicabilityand scalability.",
  "In this section, we introduce some preliminaries of the State Space Model, SSM , and two recentlyproposed methods using SSM, mainly selective state space model (Mamba) and VMamba": "State Space Model (SSM).The SSM is a mathematical model that represents the evolution of asystem over time. The model is specified as a set of equations that relate the state of the system to theobservations at each time step. The most general form of the SSM is called continuous-time lineardynamical system, which is defined as equation (4).",
  "h(t) = A(t)h(t) + B(t)u(t)y(t) = C(t)h(t) + D(t)u(t)(4)": "h(t) Rn is the state variable at time step t R, or usually called hidden variable in recentmachine learning literature, u(t) Rm is the input, y(t) Rp is the output, and A(t) Rnn,B(t) Rnm, C(t) Rpm, D(t) Rpm are the system matrices at each time step. Note that inthe following context, we treat u(t) and y(t) as scalars, i.e., m = p = 1. The above continuous-timelinear dynamical system can lead to a linear time-invariant (LTI) system when the system matricesA(t), B(t), C(t), D(t) are all time-invariant. This LTI SSM then can be written as equation (5). Itcan be discretized into a discrete-time linear dynamical system, which is defined as equation (6). Oneof the frequent ways for this transformation utilized in the literature related to SSM is zero-order hold(ZOH) discretization, which is defined as equation (7). Besides, it can further written as a convolutionform (8).h(t) = Ah(t) + Bu(t)y(t) = Ch(t) + Du(t)(5)",
  "y = x K(8)": "Selective State Space Model (Mamba). Mamba is the discrete-time linear dynamical system with atimescale parameter that transforms the continuous variables A, B to discrete variables A, B. Inaddition to discretization, Mamba also relax the time-invariant constraint of the system matrices byintroducing selection mechanism, which simply makes several parameters , B, C to be time-varyingby functions s of the input u. Specifically defined as equation (9).",
  "(9)": "The Lineard is a parameterized linear projection to dimension d, and the = softplus. As theselection mechanism loses the equivalence to convolution form (7), to avoid the sequential recurrence,Mamba further incorporates a work-efficient parallel algorithm, associate scan, into its GPU kernelimplementation to facilitate parallel computation of the system. VMambaThe original Mamba block is designed for 1-dimensional input and output, which isnot suitable for computer vision tasks. VMamba proposed a new module called 2D-Selective-Scan(SS2D) for adapting Mamba to 2D input and output. The SS2D module is composed of three steps:cross-scan, selective scan (Mamba block), and cross merge. The cross-scan unfold the input featuremap along four directions, forming 4 sets of 1D sequences. Then the selective scan processes each 1Dsequence in parallel. The cross-merge finally merges the 4 sets of 1D sequences back to 2D featuremap. The cross-scan and cross-merge are called Cross Scan Module (CSM) together, and by this way,the model can have a global receptive field. VMamba further stack multiple SS2D blocks in a layer,and then stack layers to form the whole model.",
  "BComputation Complexity": "Complexity of SSMThe computational complexity of the associated scan operation in Mambablock, measured in floating-point operations (FLOPs), is derived from processing a sequence oflength L, which requires 2L operations. Furthermore, the input to the scan operation incurs anadditional cost of 3 FLOPs, leading to a total of 3 2BLD, where B is the batch size, L is thesequence length, and D is the inner dimension. In the context of the SSM system, computations involve multiplications for But and Cht, whichamount to 2BLD, and additions for Cht and D, totaling BLD FLOPs. Consequently, the overallFLOPs for the SSM system is 3BLD. The total FLOPs for the Mamba block, therefore, aggregate to3 2BLD + 3BLD. Complexity of reduced SSMThe reduction in FLOPs can be achieved by employing the Ibasis,which consists of 9BLd FLOPs, and additional FLOPs for the reduce operation and broadcastoperation. The total reduction in FLOPs is summarized by the equation (10):",
  "DExperiments Setup": "Datasets. The datasets we use for our VMeanba experiments are the ImageNet-1k dataset for image classification and the ADE20k dataset for semantic segmentation. We only usethe validation set of them for the experiments. The ImageNet-1k dataset contains 50k validationimages from 1k classes, and the ADE20k dataset contains 2k images for validation, with pixel-levelannotations.Models. We use the VMamba pre-trained backbone models for both tasks. The backbonemodels is first trained on the ImageNet-1k training dataset. It is then used as the pre-trained backbonemodels for downstream task. The segmentation task use the UperNet on top of the VMambapre-trained backbone models, and trained on the ADE20k training dataset. The VMamba backbonemodels have three different versions: tiny, small, and base. There are two mainly differences betweenthese versions: the number of layers and the dimension of the L and D in the SS2D block. All of thebackbone models have four layers and the tiny version is stack as , while the other twoversions are stack as . The dimension of the L and D is different across two tasks, both of them remain the same inside each layer. However, the dimension of the D grows by a factor of 2, andthe dimension of the L scale down by a factor of 4 along the layers.Kernel Implementation. The original CUDA kernel for the Mamba block includes both thediscretization and scan operations, dividing the GPU multiprocessor into a 2D grid blocks based onthe batch size and inner dimension. In this configuration, multiple threads within the block handlethe scan operation. However, since the discretization process is not the focus of this study, and theoriginal approach of dividing the inner dimension across blocks is not compatible with our VMeanbamethod, we developed a new CUDA kernel. This new kernel exclusively handles the scan operation,with the discretization process executed outside the kernel. All experiments conducted in this paperare based on this optimized kernel. Future work includes integrating the discretization and scanoperations into a single kernel for further optimization.Additional Information. The evaluation metric for the image classification task is top-1 accuracy,while for the semantic segmentation task, we utilized all pixel accuracy (aAcc). The batch size forthe image classification task is set to 128, whereas for the semantic segmentation task, it is limited to1 due to the dynamic input size present in the validation set. All experiments were conducted on asingle NVIDIA RTX A6000 GPU with 48GB of memory. The profiling was performed using NVTXAPI, Nvidia Nsight Systems, and Nvidia Nsight Compute tools.",
  "G / 823.5M6.4M / 1.3M10247841.6G / 411.9M1.6M / 14.5K2048196822.1M / 207.5M412.4K / 5.8K409649411.1M / 107.5M108.5K / 0": "Kernel Analysis We analyzed GPU kernel speedup and memory usage when applying VMeanbaacross varying scan sequence lengths and inner dimensions, as shown in Tables 2 and 3. Optimizedkernel times, consistently around 0.02 ms, are excluded from . The VMeanba method achievesup to 293x speedup, particularly for longer scan sequences, aligning with the O(DL) complexitydiscussed in B. Additionally, memory transfer between global and shared memory is significantlyreduced, enabling longer scan sequences and larger batch sizes for improved throughput.",
  "%5271x283.7%5191.02x483.3%5151.02x882.6%5081.04x": "Batch Inference Time Analysis. We compared batch inference times of VMamba models with andwithout the proposed VMeanba method across three backbone models on an image classification task(). The application of VMeanba reduced inference times, increasingly so as the value of Kincreased due to time savings from applying the mean operation to more layers. Notably, the basemodel exhibited less speedup compared to the small and tiny models, likely due to its larger innerdimension size incurring greater time consumption during discretization and the mean operation.",
  "preprint arXiv:2004.05150, 2020": "J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchicalimage database. In 2009 IEEE conference on computer vision and pattern recognition, pages248255. Ieee, 2009. A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani,M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby. An image is worth 16x16words: Transformers for image recognition at scale. In ICLR, 2021."
}