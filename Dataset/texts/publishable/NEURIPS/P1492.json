{
  "Abstract": "The Privacy Preserving Federated Learning Document VQA (PFL-DocVQA) com-petition challenged the community to develop provably private and communication-efficient solutions in a federated setting for a real-life use case: invoice processing.The competition introduced a dataset of real invoice documents, along with associ-ated questions and answers requiring information extraction and reasoning over thedocument images. Thereby, it brings together researchers and expertise from thedocument analysis, privacy, and federated learning communities. Participants fine-tuned a pre-trained, state-of-the-art Document Visual Question Answering modelprovided by the organizers for this new domain, mimicking a typical federatedinvoice processing setup. The base model is a multi-modal generative languagemodel, and sensitive information could be exposed through either the visual ortextual input modality. Participants proposed elegant solutions to reduce commu-nication costs while maintaining a minimum utility threshold in track 1 and toprotect all information from each document provider using differential privacyin track 2. The competition served as a new testbed for developing and testingprivate federated learning methods, simultaneously raising awareness about privacywithin the document image analysis and recognition community. Ultimately, thecompetition analysis provides best practices and recommendations for successfullyrunning privacy-focused federated learning challenges in the future.",
  "Introduction": "Automatic document image processing has become a highly active research field in recent years [Ap-palaraju et al., 2024, Lee et al., 2023, Tito et al., 2023], with invoices being one of the most frequentlyprocessed document types [imsa et al., 2023]. In a typical real-life invoicing scenario, business sup-pliers produce invoices for their services and send them to their customers. These documents containsensitive information, such as consumer/purchaser identity, transaction details, purpose, date, phonenumbers, amount paid, account information for payment, etc. The customers (document users) needto extract this information and take the corresponding actions (i.e. reject, or make a payment againstthe invoice). In automated pipelines, these documents would be sent to AI technology providers,",
  "typically offered in the form of cloud services2, which automatically extract all required informationfrom the documents, and return it to the document users": "A generic approach to extract information from invoices is DocVQA [Mathew et al., 2020]. Theextraction is done by asking questions in a natural language form to get specific information asanswers, using a deep learning model. However, training an accurate DocVQA model requires aconsiderable amount of data, that is rarely held by a single entity. One solution is to train this modelcollaboratively by aggregating and centralizing data from a set of clients that face the same problem.But, documents often cannot be freely exchanged due to the sensitive information they contain.Federated Learning (FL) is a learning paradigm that purports to solve this problem [McMahan et al.,2017b]. Rather than exchanging privately-held data, participating entities (known as clients) trainmodels on their data in a decentralized fashion, exchanging only the local model updates with acentral server. However, even though FL is more private than the centralized approach, a significantamount of information can still be inferred from the updates shared during training, or from theparameters of the resulting trained model, whether by an adversarial server, client, or downstreamuser [Sikandar et al., 2023]. Differential Privacy (DP) [Dwork et al., 2016] is considered the gold standard in terms of privacypreservation and can be used to provide provable privacy guarantees. DP formally quantifies themaximum information leakage from the inclusion of any one individual record in a dataset. Deeplearning models can be trained under DP by clipping parameter updates and adding noise to them [Ra-jkumar and Agarwal, 2012, Song et al., 2013, Abadi et al., 2016]. However, this introduces a trade-offbetween privacy and utility. Stronger privacy guarantees require introducing more noise, whichproportionately degrades model accuracy. Another drawback of FL is the high communication cost [Kairouz et al., 2021]. At each federatedround, the global model is transmitted by the server to selected clients (downstream step) to be trainedon their local data, and then the update of this model is sent by these selected entities back to the server(upstream step). For models with millions or even billions of parameters, this requires significantbandwidth, multiplied by the number of federated rounds required to reach model convergence. In this paper, we present an analysis of the NeurIPS 2023 competition on privacy preserving FLDocVQA that we designed to expose the above challenges and invite the community to design novelcreative solutions for this real-life use case. It brought together researchers and expertise from thedocument analysis, privacy, and FL communities. Additionally, it added a realistic use case forprivacy and FL researchers as well as expanding the scope of document analysis to DP solutions.",
  "Related Work": "Document Visual Question Answering (DocVQA) DocVQA has been an evolving field duringthe last few years. This is due to the emerging datasets that address different document domains.For instance, industry documents [Mathew et al., 2020, 2021, Tito et al., 2021b, 2023], infograph-ics [Mathew et al., 2022], multidomain [Landeghem et al., 2023a,b], open-ended questions [Tanakaet al., 2021], multilingual [Qi et al., 2022], multipage [Tito et al., 2023] or collections of docu-ments [Tito et al., 2021a]. However, these datasets are often small and highly domain-specific, whichlimits generalizability. Federated Learning (FL) FL [Shokri and Shmatikov, 2015, McMahan et al., 2017b] addressesthis issue, and has seen practical use in both research and industrial applications [Li et al., 2020],particularly in domains where sensitive data is common, such as medicine [Dayan et al., 2021] andfinance [Long et al., 2020]. FL carries a trade-off between model utility, data privacy, and communi-cation efficiency [Zhang et al., 2023], and requires specific consideration of client data heterogeneity,scalability, and fault tolerance. Much recent work in FL focuses on mitigating these problems,primarily through developments in aggregation algorithms [Moshawrab et al., 2023, Elkordy andAvestimehr, 2022, So et al., 2022, Nguyen et al., 2022], but also in parameter compression [Tanget al., 2019] and quantization [Xu et al., 2022].",
  "Automatic document processing services offered by large corporations (AWS Intelligent Document Process-ing, Google Cloud Document AI, Microsoft Azure Form Recognizer, etc) or specialized providers": "potentially act as adversaries. Gradient updates in FL have the potential to disclose information aboutthe training data, making them susceptible to \"gradient inversion attacks\" [Zhu et al., 2019, Zhaoet al., 2020, Fu et al., 2022, Wainakh et al., 2022, Li et al., 2022b, Geiping et al., 2020, Melis et al.,2019, Li et al., 2022d], which enable accurate data reconstruction. Moreover, adversaries can execute\"membership inference attacks\" [Nasr et al., 2019, Melis et al., 2019, Suri et al., 2022, Shokri et al.,2017, Choquette-Choo et al., 2021, Li and Zhang, 2021, Hu et al., 2022b] to infer the inclusion ofspecific data points in other participants datasets, as well as \"property inference attacks\" [Melis et al.,2019] to deduce subgroup statistics despite secure aggregation [Kerkouche et al., 2023, Pej andBiczk, 2023]. FL inherently lacks protection against these threats, necessitating explicit mitigationstrategies to safeguard client data from adversaries. Differential Privacy (DP) (, )-DP [Dwork et al., 2006] has a privacy budget consisting of 0and , where smaller values correspond to a stronger privacy guarantee. Especially relevantto our setting is group-level DP, which preserves privacy leakage from the inclusion or exclusionof groups of datapoints [Galli et al., 2023, Marathe and Kanani, 2022], such as multiple recordsassociated with a specific user. We refer to Dwork and Roth for a comprehensive intro to DP. High utility models under DP Currently, many works improve the utility-privacy trade-off throughtransfer learning [Yosinski et al., 2014] assuming the availability of non-sensitive public data forpre-training and only utilizing DP to protect sensitive downstream data during fine-tuning. Wewould like to refer to Tramr et al. [2022a] for a discussion on the drawbacks of these assumptions.Transfer learning is highly effective for both language [Li et al., 2022c, Yu et al., 2022a] and visiontasks [Cattan et al., 2022, De et al., 2022, Kurakin et al., 2022, Tobaben et al., 2023]. In particular,parameter-efficient fine-tuning [Houlsby et al., 2019] with adaptation methods such as LoRA [Huet al., 2022a] have been demonstrated to yield improved utility-privacy trade-offs for DP, as havequantization [Youn et al., 2023] or compression of model updates [Kerkouche et al., 2021a,b, Miaoet al., 2022]. All these methods reduce the size of the updates, and thereby reduce the amount ofnoise addition required. The same strategies often yield competitive performance for FL.",
  "PFL-DocVQA Dataset": "For this competition, we created PFL-DocVQA [Tito et al., 2024], the first dataset for privatefederated DocVQA. The dataset is created using invoice document images gathered from the DocILEdataset [imsa et al., 2023]. For every image, we provide the OCR transcription and form a setof question/answer pairs. The competitions version of PFL-DocVQA contains a total of 336,842question-answer pairs framed on 117,661 pages of 37,669 documents from 6,574 different invoiceproviders. PFL-DocVQA is designed to be used in two tasks, and so is divided into two subsets. Forthe first task of training and evaluating machine learning privacy-preserving solutions on DocVQA ina FL fashion, a base subset of PFL-DocVQA called the BLUE data is used. In the second task,membership inference attacks are designed to assess the privacy guarantees of the DocVQA modelsthat were trained with the base data. These attacking approaches are to utilize a second subset calledthe RED data. In this competition, we focus on the first task, thus, we use only the BLUE data.For more details on the full PFL-DocVQA datasets, refer to Tito et al. . PFL-DocVQA aims to train and evaluate DocVQA systems that protect sensitive document infor-mation. In our scenario, sensitive information encompasses all information originating from eachinvoice provider. Therefore, an effective model must prevent the disclosure of any details associatedwith these providers (such as provider names, emails, addresses, logos, etc.) across diverse federatedclients. Following this, the base data used in this competition consists of a training set divided amongN clients (we use N = 10), a validation set and a test set. (See Figure A.1). The training set of eachof the N clients contains invoices sampled from a different subset of providers, resulting in a highlynon-i.i.d. distribution. In the validation and test sets, we include documents both from the providersthat were seen during training, and from a set of providers that were not seen, to better evaluate thegeneralizability of the models.",
  "In the PFL-DocVQA Competition three main aspects are evaluated: The models utility, the commu-nication cost during training and the DP privacy budget spent through training the model": "Utility To evaluate the visual question answering performance of the participants methods we useaccuracy and ANLS (Average Normalized Levenshtein Similarity), a standard soft version of accuracyextensively used in most of the text-based VQA tasks [Biten et al., 2019a,b, Mathew et al., 2020, Titoet al., 2021b, Mathew et al., 2021, Tito et al., 2021a, Mathew et al., 2022, Tito et al., 2023, Landeghemet al., 2023b,a]. This metric is based on the normalized Levenshtein Distance [Levenshtein, 1966]between the predicted answer and the ground truth, allowing us to assess the methods reasoningcapabilities while smoothly penalizing OCR errors. Communication cost We measure the efficiency of the communications as the total amount ofinformation transmitted between the server and the clients in Gigabytes (GB) in both directions. Theinitial transmission of the pre-trained model to the clients is not included in the communication cost. Privacy The methods of track 2 are required to comply with a DP privacy budget of no more thana pre-defined {1, 4, 8} at = 105. We provided a script within the starter kit detailed in.4 to compute the required noise multiplier given the target (, ). Participants may need toadjust the script to their algorithms. Moreover, we required the participants to upload a theoreticalprivacy proof of their methods, which was manually reviewed by the competition organizers.",
  "Pre-trained Model": "The participants were asked to implement their solutions starting from the same pre-trained model.The architecture chosen is Visual T5 (VT5), it is a multimodal generative network consisting ofa simplified version of Hi-VT5 [Tito et al., 2023], which was originally proposed for multi-pageDocVQA. VT5 exploits the image and text modalities, which is beneficial to perform the DocVQAtask. However, this dual-modality approach also presents a more complex challenge: safeguardingprivate information across both modalities, compared to handling just one. Moreover, VT5 is agenerative model based on the T5 [Raffel et al., 2020] language model. Language models can sufferhallucinations [Rawte et al., 2023], leading to the potential leakage of private information. The architecture VT5 consists of an encoder-decoder model based on T5. The input of the modelis the question, the OCR tokens of the document (text and spatial information), and the encodeddocument image using the DiT [Li et al., 2022a] vision transformer model. These three inputs areconcatenated and fed to the VT5 to output the answer following the autoregressive mechanism. We also provide pre-trained weights for VT5. First, the language backbone T5 is initialized with thepre-trained weights on the C4 dataset [Raffel et al., 2020], and the visual DiT with the pre-trainedweights on the document classification task. After that, the full model is fine-tuned on the single-pageDocVQA task, using the SP-DocVQA dataset [Mathew et al., 2020, 2021] for 10 epochs.",
  "Starter Kit": "The starter kit includes the pre-trained model checkpoint, the fine-tuning dataset, code for running thebaselines and instructions on how to run and modify the code. The code itself is based on establishedlibraries such as PyTorch [Paszke et al., 2019] and the FL framework Flower [Beutel et al., 2020].Besides the training code, the starter kit includes functions for computing the privacy parametersbased on the hyperparameters and for logging the communication between server and clients. Wetested the installation and execution of the baseline on various clusters across different institutionsand provided support to participants if they encountered any difficulties. The starter kit is openlyavailable:",
  "Task Formulation": "The objective of track 1 is to reduce the communication used (# bytes), while achieving a comparableutility (ANLS) with the organizers baseline. The baseline achieved a validation ANLS of 0.8676and we define a comparable utility to the baseline as 0.8242 ANLS (5% w.r.t. the baseline). Anysubmission that achieves at least that ANLS is valid, thus the deciding factor for winning thecompetition is the communication efficiency, which is measured using a single metric. We optedfor scoring using a single metric as the trade-off between utility and communication is not linear.Furthermore, in real world applications less communication efficiency will lead to higher monetarycosts or longer training times that need to be considered in contrast to changes in model utility. Participants are required to use the VT5 baseline model with the initial pre-trained weights and utilizeonly the PFL-DocVQA dataset for fine-tuning. Further the participants are not allowed to changethe PFL-DocVQA data distribution. Additionally, participants are required to upload a log of thecommunication between the clients and the central party (# bytes) and the final model checkpoint. The organizers evaluate the model utility on a secret test set and thus the model architecture needsto be the same as the initial baseline. While this makes some solutions such model distillationmore challenging, the track is open to a wide range of possible solutions. Participants could, e.g.,utilize parameter-efficient fine-tuning, compression of the FL updates, lower precision or betterhyper-parameters to achieve higher communication efficiency while maintaining a comparable utility.",
  "Baseline Solution Track 1": "The baseline solution for track 1 fine-tunes all parameters of the pre-trained model but the visualmodule. It essentially uses Federated Averaging (FedAvg) [McMahan et al., 2017a]. In each globalround, the central server samples K = 2 clients out of all N = 10, and each of these clients computesthe weight updates locally across multiple local rounds. The central server aggregates the clientupdates and communicates the updated model to the sampled clients in the next round.",
  "MB": "39.67 GB (-99.04%) 0.33 GB (-99.86%) 47 MB (-99.98%) 1. LoRA. Low-Rank Adaptation trains low-rank adapters while freezing the rest of the model [Huet al., 2022a]. We use LoRA to reduce the number of trainable parameters to 3.4M (1.37% from 250M).Using 2 clients per round, we reach the target ANLS in 7 rounds (0.38 GB total communication). 2. Tuning FL hyperparameters. On top of 1. LoRA, we sample 1 client per round (default: 2) andtrain for 16 local epochs (default: 1), which respectively reduces communication and improves utility.With these adjustments, we reach the target ANLS in 2 rounds (55 MB total communication). 3. Quantization is a lossy compression approach which we use to reduce the size of the communicatedLoRA updates. We use NF4 (4-bit) quantization which reduces the message size by 8 whileachieving the target ANLS with the same configuration as 2. (7.7 MB total communication).",
  "Runners-up Track 1: Niwa, Ishii, Yamasaki, Fukami, Tyou, and Yokota": "We briefly present our methods and experimental results. For more detailed information can be foundin Appendix D. We aimed to achieve faster convergence of training for local models with fewercommunication rounds. To achieve this, we utilized Shampoo [Gupta et al., 2018], a second-orderoptimization method, in local update rules by multiplying the local preconditioning matrix to thelocal stochastic gradient. The update rules of our method, named FedShampoo, are outlined in Alg.1 in Appendix D.1. Shampoo enables smooth local updates by geometrically rotating and scalingstochastic gradients. To reduce the memory footprint in computing large-scale preconditioningmatrices, we approximated them by employing layer-wise block-diagonalization. Notably, the localpreconditioning matrices (approximated by sub-matrices) were not transmitted to the central server,thus avoiding excess communication costs. Furthermore, we excluded the embedding layer fromthe optimization target, resulting in a reduction of approximately 26 % in communication per roundcompared to whole parameters3. In , FedShampoo achieved the target ANLS score with 10.01 GB communication cost. Referto Figure A.3 in Appendix D.1 for convergence curves using validation loss, ACC and ANLS. Wesubmitted the model after only R = 3 communication rounds, surpassing the target ANLS score of0.8873 and resulting in an approximately 30 % reduction of the communication cost compared withthe baseline method (using solely AdamW-based optimizer). Furthermore, FedShampoo achievedhigher ACC and ANLS scores compared with the baseline method after exceeding the ANLS targetscore (after 3 communication rounds). This provides as empirical evidence of FedShampoos fasterconvergence, which benefits from applying the preconditioning matrix to the stochastic gradient. Thedetailed experimental configurations, such as hyperparameter tunings of learning rate and clippingthreshold, are summarized in Appendix D.1.",
  "Track 2 Task Formulation": "The objective of track 2 is to achieve the best utility possible while protecting all informationfrom each document provider in the training set, which could be exposed through textual (providercompany name) or visual (logo, presentation) information. Participants are required to train underDP at different levels from medium DP ( = 1) to weak DP ( = 8) to mitigate the risk of providerinformation being leaked. Ultimately, the goal is to achieve the best utility while complying tothe privacy budgets of {1, 4, 8} at = 105. The definition of DP critically depends on theconcept of adjacency of datasets. We seek to protect the privacy of providers and thus the typicaldocument-level adjacency definition would be too weak, as there are many documents from thesame provider and combining them could leak private information. Instead we use provider-leveladd/remove adjacency, where adjacent training datasets can be obtained by adding or removing alldocuments from one provider. Prior work denotes this as group-level DP [Marathe and Kanani, 2022,Galli et al., 2023].",
  "-Organizers (.2)Baseline0.48320.50240.5132": "Participants are required to follow the same rules regarding the pre-trained model and fine-tuningdata as in track 1. Besides uploading the final model checkpoint solutions, they are required tosubmit a theoretical privacy proof and description. The requirement for a theoretical privacy proofin track 2 ensures that the solutions proposed by participants are rigorously validated for theiradherence to differential privacy principles. This proof demonstrates that the final model maintainsthe privacy of all information from each document provider by offering a quantifiable measure ofprivacy loss. Additionally, a thorough description and code submission are necessary to facilitatereproducibility and allow for independent verification of the privacy claims, ensuring transparencyand trustworthiness in the solutions provided.",
  "Baseline Solution Track 2": "The baseline solution for track 2 utilizes DP stochastic optimization. The optimization of the modelis done in multiple global rounds. In each round, the central server first samples a set of clientsfrom all N = 10 clients. Each selected client runs a local instance of federated learning where eachprovider acts as the training data of a virtual client within the real client. The client randomly selectsproviders, clips the per-provider updates and the adds an appropriate amount of noise so that theupdate aggregated by the server is differentially private with respect to all providers over all clients4The privacy loss of the baseline follows the usual analysis of DP stochastic optimisation consisting ofcompositions of sub-sampled Gaussian mechanisms. The loss depends on the number of iterationsTcl, sub-sampling rate q (both over clients and providers) and noise scale [Mironov et al., 2019,Balle et al., 2020]. (See more details in Appendix A.4 and the privacy analysis in Appendix B).",
  "Winner Track 2: Ragul N and Kutum": "Similar to the winning solution for track 1, our method also uses LoRA. We choose LoRA for thefollowing two reasons: First, it significantly reduces the communication cost as shown in .3.Second, empirical results have shown that differentially private adaptation of language models usingparameter-efficient methods such as LoRA outperforms full fine-tuning in centralized settings [Yuet al., 2022b]. These methods reduce the overall noise added by only updating a small proportionof the parameters in the model, thereby increasing the utility of the model. The communicationefficiency of LoRA also allowed us to increase the number of FL rounds from 5 in the baselinemethod to 30 in our method without increasing communication costs. With these changes to thebaseline, our method improved the ANLS by 10-11 percentage points across all privacy settings.",
  "Runners-up Track 2: Fukami, Yamasaki, Niwa, and Tyou": "We briefly present our methods and experimental results. More detailed information can be foundin Appendix D. It is well-known that applying DP to FedAVG with a relatively high privacy leveloften stagnates the model training process due to local parameter drift. This is mainly caused by i)noise addition in DP and ii) data heterogeneity among clients. To address these issues, we proposeDP-CLGECL, which incorporates the DPs Gaussian mechanism into CLGECL Tyou et al. .The update rules in DP-CLGECL are derived by solving a linearly constrained loss-sum minimizationproblem, resulting in robustness against local gradient drift due to data heterogeneity, and this wouldalso be effective in addressing the drift issue due to DPs Gaussian mechanism. Note that the DPanalysis of the private baseline detailed in Appendix B is applicable to our DP-CLGECL. Moredetails about our methodologies are provided in Appendix D.2.",
  "Note when no clients are sampled in a FL round the server still needs to add noise": "gence curves in Figure A.4 are summarized in Appendix D.2. After passing the competition deadline,we observed a negative impact of using AdamW optimizer in the baseline method. The norm ofstochastic gradient, preconditioned by AdamW, often increased, and the gradient clipping used toensure the pre-defined DP levels led to a loss of valuable information in model parameter training.To address this issue, we replaced AdamW with momentum in the local update of DP-CLGECL,resulting in further improved ANLS. Although more details can be found in Figure A.5, the ANLSwas then 0.5918 for = 1 using DP-CLGECL with momentum.",
  "Ensuring that the Track 2 Submissions Are DP": "The track 2 of this competition required participants to provide a model checkpoint trained underDP. Additionally, we asked the participants to provide a privacy proof outlining how their method isformally differential private and requested the source code. Formal privacy proof Asking for a privacy proof from the participants results in two things: (i) Theorganizers can check that a new proposed method is DP; and (ii) The participating team can reflecton ensuring that their method is actually DP. Insufficient formal analysis in prior work has lead toresponse papers [Carlini et al., 2021, 2022] that corrected the wrong analysis. Ensuring that the implementations are DP While the privacy proof ensures that theoreticallythe submissions are DP, even small mistakes in the implementation of DP methods can invalidateor severely weaken the DP guarantees [Tramr et al., 2022b]. Among these are the clipping ofthe updates, the correct noise addition and scaling as well as the subsampling. Thus, members ofthe organizing team have inspected the implementations of the best scoring methods but this is amanual process that does not scale to competitions with a large number of participants. The codereviews could be complemented with automatic tests that increase the chance of finding bugs in theimplementation. Established DP libraries such as Opacas [Yousefpour et al., 2021] use unit tests butthese tests are custom to the implementation that are testing and writing new tests requires much moremanual labour than plain code reviews. Using only established implementations (e.g., like Opacus)for critical parts of the code would reduce the risk of bugs but also limit the possible solutions. Automation of the validation of DP methods and implementations When scaling up the participantnumbers of a competition, processes need to be automated. One example for that is our automaticutility evaluation on the secret test set. Automating the validation of DP methods and implementationsis less straightforward: There are methods for auditing DP implementations [Jagielski et al., 2020,Nasr et al., 2023] but they are computationally expensive. Recent advancements have significantlyreduced the cost of DP auditing [Steinke et al., 2023]. One option would be auditing new submissionsto assist in DP validation but it is unclear how computationally costly that would be. Auditing cannotconclusively prove something DP, so it should only be used to complement privacy proofs and codechecks, not replace them.",
  "Lowering the Threshold for Participation": "Referring to one can see that the competition has received some interest. Also, it led to thedata set being adopted in the privacy community [Wu et al., 2024] and increased the awareness in thedocument intelligence community [Biescas et al., 2024]. Participants were required to be able to traina state-of-the-art Document Visual Question Answering model in a federated learning setting (underDP). The number of potential participants that have the required skill set is not as high as in otherchallenges. Thus it is important that the threshold for participation is as low as possible. We discussmeasures that we took to lower the threshold for participation. Starting Kit All solutions that are described in this analysis report utilized the provided starting kitto some extent. Based on the feedback from the participants, we think that the starting kit was crucialfor them to participate. We can recommend to future organizers to test and document the starting kitextensively and include convenience functions (e.g., to compute communication cost or DP noise). Computational Cost Simulating the FL setting and even just fine-tuning large pre-trained modelsrequires a significant amount of compute. This is especially true under DP [Beltran et al., 2024] asthe privacy/utility trade-off can be improved by training longer [Ponomareva et al., 2023] and usinglarger batch sizes [Ris et al., 2024]. We aimed to lower the threshold for participation by reducingthe size of the client datasets and utilizing not the largest pre-trained model available. Still, executingthe baselines with consumer hardware is hard if not impossible. One possible avenue for the futurewould be to open separate tracks for consumer hardware and provide cloud compute to teams thatcould otherwise not participate. The recent NeurIPS 2023 challenge on LLMs5 introduced some ofthese measures.",
  "and Disclosure of Funding": "This work has been funded by the European Lighthouse on Safe and Secure AI (ELSA) from theEuropean Unions Horizon Europe programme under grant agreement No 101070617. MT, JJand AH have been supported by the Research Council of Finland (Flagship programme: FinnishCenter for Artificial Intelligence, FCAI; as well as grants 356499 and 359111) and the StrategicResearch Council at the Research Council of Finland (Grant 358247). Part of this work has beenperformed using resources provided by the CSC IT Center for Science, Finland, and the FinnishComputing Competence Infrastructure (FCCI). MAS, RT, KN, LK, AB, JL, EV and DK havebeen supported by the Consolidated Research Group 2021 SGR 01559 from the Research andUniversity Department of the Catalan Government, and by project PID2023-146426NB-100 fundedby MCIU/AEI/10.13039/501100011033 and FSE+. RK would like to acknowledge Mphasis F1Foundation for the computing infrastructure and financial support. Views and opinions expressedare however those of the author(s) only and do not necessarily reflect those of the European Unionor the European Commission. Neither the European Union nor the granting authorities can be heldresponsible for them. M. Abadi, A. Chu, I. J. Goodfellow, H. B. McMahan, I. Mironov, K. Talwar, and L. Zhang. Deep learningwith differential privacy. In E. R. Weippl, S. Katzenbeisser, C. Kruegel, A. C. Myers, and S. Halevi,editors, Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security,Vienna, Austria, October 24-28, 2016, pages 308318. ACM, 2016. doi: 10.1145/2976749.2978318. URL S. Appalaraju, P. Tang, Q. Dong, N. Sankaran, Y. Zhou, and R. Manmatha. Docformerv2: Local features fordocument understanding. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages709718, 2024. B. Balle, G. Barthe, M. Gaboardi, J. Hsu, and T. Sato. Hypothesis testing interpretations and renyi differentialprivacy. In S. Chiappa and R. Calandra, editors, The 23rd International Conference on Artificial Intelligenceand Statistics, AISTATS 2020, 26-28 August 2020, Online [Palermo, Sicily, Italy], volume 108 of Proceedingsof Machine Learning Research, pages 24962506. PMLR, 2020. URL",
  "N. Biescas, C. Boned, J. Llads, and S. Biswas. Geocontrastnet: Contrastive key-value edge learning forlanguage-agnostic document understanding. ArXiv preprint, abs/2405.03104, 2024. URL": "A. F. Biten, R. Tito, A. Mafla, L. Gmez, M. Rusiol, M. Mathew, C. V. Jawahar, E. Valveny, and D. Karatzas.ICDAR 2019 competition on scene text visual question answering. In 2019 International Conference onDocument Analysis and Recognition, ICDAR 2019, Sydney, Australia, September 20-25, 2019, pages 15631570. IEEE, 2019a. doi: 10.1109/ICDAR.2019.00251. URL A. F. Biten, R. Tito, A. Mafla, L. G. i Bigorda, M. Rusiol, C. V. Jawahar, E. Valveny, and D. Karatzas.Scene text visual question answering. In 2019 IEEE/CVF International Conference on Computer Vision,ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019, pages 42904300. IEEE, 2019b. doi:10.1109/ICCV.2019.00439. URL",
  "Y. Cattan, C. A. Choquette-Choo, N. Papernot, and A. Thakurta. Fine-tuning with differential privacy necessitatesan additional hyperparameter search. ArXiv preprint, abs/2210.02156, 2022. URL": "C. A. Choquette-Choo, F. Tramr, N. Carlini, and N. Papernot. Label-only membership inference attacks. InM. Meila and T. Zhang, editors, Proceedings of the 38th International Conference on Machine Learning,ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages19641974. PMLR, 2021. URL I. Dayan, H. R. Roth, A. Zhong, A. Harouni, A. Gentili, A. Z. Abidin, A. Liu, A. B. Costa, B. J. Wood, C.-S.Tsai, et al. Federated learning for predicting clinical outcomes in patients with covid-19. Nature medicine, 27(10):17351743, 2021.",
  "S. De, L. Berrada, J. Hayes, S. L. Smith, and B. Balle. Unlocking high-accuracy differentially private imageclassification through scale. ArXiv preprint, abs/2204.13650, 2022. URL": "E. Debenedetti, G. Severi, N. Carlini, C. A. Choquette-Choo, M. Jagielski, M. Nasr, E. Wallace, and F. Tramr.Privacy side channels in machine learning systems. CoRR, abs/2309.05610, 2023. doi: 10.48550/ARXIV.2309.05610. URL T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer. Qlora: Efficient finetuning of quantized llms. InA. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Informa-tion Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL J. C. Duchi, E. Hazan, and Y. Singer.Adaptive subgradient methods for online learning and stochasticoptimization. In A. T. Kalai and M. Mohri, editors, COLT 2010 - The 23rd Conference on Learning Theory,Haifa, Israel, June 27-29, 2010, pages 257269. Omnipress, 2010. URL #page=265.",
  "C. Dwork and A. Roth. The algorithmic foundations of differential privacy. Found. Trends Theor. Comput. Sci.,9(3-4):211407, 2014. doi: 10.1561/0400000042. URL": "C. Dwork, K. Kenthapadi, F. McSherry, I. Mironov, and M. Naor. Our data, ourselves: Privacy via distributednoise generation. In S. Vaudenay, editor, Advances in Cryptology - EUROCRYPT 2006, 25th AnnualInternational Conference on the Theory and Applications of Cryptographic Techniques, St. Petersburg, Russia,May 28 - June 1, 2006, Proceedings, volume 4004 of Lecture Notes in Computer Science, pages 486503.Springer, 2006. doi: 10.1007/11761679_29. URL",
  "A. R. Elkordy and A. S. Avestimehr. Heterosag: Secure aggregation with heterogeneous quantization in federatedlearning. IEEE Trans. Commun., 70(4):23722386, 2022. doi: 10.1109/TCOMM.2022.3151126": "C. Fu, X. Zhang, S. Ji, J. Chen, J. Wu, S. Guo, J. Zhou, A. X. Liu, and T. Wang. Label inference attacks againstvertical federated learning. In K. R. B. Butler and K. Thomas, editors, 31st USENIX Security Symposium,USENIX Security 2022, Boston, MA, USA, August 10-12, 2022, pages 13971414. USENIX Association,2022. URL F. Galli, S. Biswas, K. Jung, T. Cucinotta, and C. Palamidessi. Group privacy for personalized federatedlearning. In P. Mori, G. Lenzini, and S. Furnell, editors, Proceedings of the 9th International Conferenceon Information Systems Security and Privacy, ICISSP 2023, Lisbon, Portugal, February 22-24, 2023,pages 252263. SciTePress, 2023. doi: 10.5220/0011885000003405. URL J. Geiping, H. Bauermeister, H. Drge, and M. Moeller. Inverting gradients - how easy is it to break privacy infederated learning? In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, editors, Advances inNeural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL V. Gupta, T. Koren, and Y. Singer. Shampoo: Preconditioned stochastic tensor optimization. In J. G. Dy andA. Krause, editors, Proceedings of the 35th International Conference on Machine Learning, ICML 2018,Stockholmsmssan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine LearningResearch, pages 18371845. PMLR, 2018.URL N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. de Laroussilhe, A. Gesmundo, M. Attariyan, and S. Gelly.Parameter-efficient transfer learning for NLP. In K. Chaudhuri and R. Salakhutdinov, editors, Proceedings ofthe 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California,USA, volume 97 of Proceedings of Machine Learning Research, pages 27902799. PMLR, 2019. URL E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. Lora: Low-rank adaptationof large language models. In The Tenth International Conference on Learning Representations, ICLR 2022,Virtual Event, April 25-29, 2022. OpenReview.net, 2022a. URL",
  "H. Hu, Z. Salcic, L. Sun, G. Dobbie, P. S. Yu, and X. Zhang. Membership inference attacks on machine learning:A survey. ACM Comput. Surv., 54(11s):235:1235:37, 2022b. doi: 10.1145/3523273": "M. Jagielski, J. R. Ullman, and A. Oprea. Auditing differentially private machine learning: How private isprivate sgd? In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, editors, Advances in NeuralInformation Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020,NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL P. Kairouz, H. B. McMahan, B. Avent, A. Bellet, M. Bennis, A. N. Bhagoji, K. A. Bonawitz, Z. Charles,G. Cormode, R. Cummings, R. G. L. DOliveira, H. Eichner, S. E. Rouayheb, D. Evans, J. Gardner, Z. Garrett,A. Gascn, B. Ghazi, P. B. Gibbons, M. Gruteser, Z. Harchaoui, C. He, L. He, Z. Huo, B. Hutchinson, J. Hsu,M. Jaggi, T. Javidi, G. Joshi, M. Khodak, J. Konecn, A. Korolova, F. Koushanfar, S. Koyejo, T. Lepoint,Y. Liu, P. Mittal, M. Mohri, R. Nock, A. zgr, R. Pagh, H. Qi, D. Ramage, R. Raskar, M. Raykova, D. Song,W. Song, S. U. Stich, Z. Sun, A. T. Suresh, F. Tramr, P. Vepakomma, J. Wang, L. Xiong, Z. Xu, Q. Yang,F. X. Yu, H. Yu, and S. Zhao. Advances and open problems in federated learning. Found. Trends Mach.Learn., 14(1-2):1210, 2021. doi: 10.1561/2200000083. R. Kerkouche, G. cs, C. Castelluccia, and P. Genevs. Compression boosts differentially private federatedlearning. In IEEE European Symposium on Security and Privacy, EuroS&P 2021, Vienna, Austria, September6-10, 2021, pages 304318. IEEE, 2021a. doi: 10.1109/EUROSP51992.2021.00029. URL R. Kerkouche, G. cs, C. Castelluccia, and P. Genevs. Constrained differentially private federated learning forlow-bandwidth devices. In C. P. de Campos, M. H. Maathuis, and E. Quaeghebeur, editors, Proceedings ofthe Thirty-Seventh Conference on Uncertainty in Artificial Intelligence, UAI 2021, Virtual Event, 27-30 July2021, volume 161 of Proceedings of Machine Learning Research, pages 17561765. AUAI Press, 2021b.URL R. Kerkouche, G. cs, and M. Fritz. Client-specific property inference against secure aggregation in federatedlearning. In B. P. Knijnenburg and P. Papadimitratos, editors, Proceedings of the 22nd Workshop on Privacyin the Electronic Society, WPES 2023, Copenhagen, Denmark, 26 November 2023, pages 4560. ACM, 2023.doi: 10.1145/3603216.3624964. URL",
  "A. Kurakin, S. Chien, S. Song, R. Geambasu, A. Terzis, and A. Thakurta. Toward training at imagenet scalewith differential privacy. CoRR, abs/2201.12328, 2022. URL": "J. V. Landeghem, R. Powalski, R. Tito, D. Jurkiewicz, M. B. Blaschko, L. Borchmann, M. Coustaty, S. Moens,M. Pietruszka, B. Anckaert, T. Stanislawek, P. Jziak, and E. Valveny. Document understanding dataset andevaluation (DUDE). In ICCV, pages 1947119483. IEEE, 2023a. doi: 10.1109/ICCV51070.2023.01789. J. V. Landeghem, R. Tito, L. Borchmann, M. Pietruszka, D. Jurkiewicz, R. Powalski, P. Jziak, S. Biswas,M. Coustaty, and T. Stanislawek. ICDAR 2023 competition on document understanding of everything(DUDE). In ICDAR (2), volume 14188 of Lecture Notes in Computer Science, pages 420434. Springer,2023b. doi: 10.1007/978-3-031-41679-8_24. K. Lee, M. Joshi, I. R. Turc, H. Hu, F. Liu, J. M. Eisenschlos, U. Khandelwal, P. Shaw, M.-W. Chang,and K. Toutanova. Pix2struct: Screenshot parsing as pretraining for visual language understanding. InInternational Conference on Machine Learning, pages 1889318912. PMLR, 2023.",
  "V. I. Levenshtein. Binary codes capable of correcting deletions, insertions, and reversals. In Soviet physicsdoklady, volume 10, pages 707710, 1966": "J. Li, Y. Xu, T. Lv, L. Cui, C. Zhang, and F. Wei. DiT: Self-supervised pre-training for document imagetransformer. In J. Magalhes, A. D. Bimbo, S. Satoh, N. Sebe, X. Alameda-Pineda, Q. Jin, V. Oria, andL. Toni, editors, MM 22: The 30th ACM International Conference on Multimedia, Lisboa, Portugal,October 10 - 14, 2022, pages 35303539. ACM, 2022a. doi: 10.1145/3503161.3547911. URL O. Li, J. Sun, X. Yang, W. Gao, H. Zhang, J. Xie, V. Smith, and C. Wang. Label leakage and protection intwo-party split learning. In The Tenth International Conference on Learning Representations, ICLR 2022,Virtual Event, April 25-29, 2022. OpenReview.net, 2022b. URL",
  "T. Li, A. K. Sahu, A. Talwalkar, and V. Smith. Federated learning: Challenges, methods, and future directions.IEEE Signal Process. Mag., 37(3):5060, 2020. doi: 10.1109/MSP.2020.2975749": "X. Li, F. Tramr, P. Liang, and T. Hashimoto. Large language models can be strong differentially private learners.In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29,2022. OpenReview.net, 2022c. URL Z. Li and Y. Zhang. Membership leakage in label-only exposures. In Y. Kim, J. Kim, G. Vigna, and E. Shi,editors, CCS 21: 2021 ACM SIGSAC Conference on Computer and Communications Security, Virtual Event,Republic of Korea, November 15 - 19, 2021, pages 880895. ACM, 2021. doi: 10.1145/3460120.3484575.URL Z. Li, J. Zhang, L. Liu, and J. Liu. Auditing privacy defenses in federated learning via generative gradientleakage. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans,LA, USA, June 18-24, 2022, pages 1012210132. IEEE, 2022d. doi: 10.1109/CVPR52688.2022.00989. URL G. Long, Y. Tan, J. Jiang, and C. Zhang. Federated learning for open banking. In Federated Learning,volume 12500 of Lecture Notes in Computer Science, pages 240254. Springer, 2020. doi: 10.1007/978-3-030-63076-8\\\\_17.",
  "M. Mathew, R. Tito, D. Karatzas, R. Manmatha, and C. V. Jawahar. Document visual question answeringchallenge 2020. CoRR, abs/2008.08899, 2020. URL": "M. Mathew, D. Karatzas, and C. V. Jawahar. Docvqa: A dataset for VQA on document images. In IEEEWinter Conference on Applications of Computer Vision, WACV 2021, Waikoloa, HI, USA, January 3-8, 2021,pages 21992208. IEEE, 2021. doi: 10.1109/WACV48630.2021.00225. URL M. Mathew, V. Bagal, R. Tito, D. Karatzas, E. Valveny, and C. V. Jawahar. Infographicvqa. In IEEE/CVFWinter Conference on Applications of Computer Vision, WACV 2022, Waikoloa, HI, USA, January 3-8, 2022,pages 25822591. IEEE, 2022. doi: 10.1109/WACV51458.2022.00264. URL B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas. Communication-efficient learning of deepnetworks from decentralized data. In A. Singh and X. J. Zhu, editors, Proceedings of the 20th InternationalConference on Artificial Intelligence and Statistics, AISTATS 2017, 20-22 April 2017, Fort Lauderdale, FL,USA, volume 54 of Proceedings of Machine Learning Research, pages 12731282. PMLR, 2017a. URL B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas. Communication-efficient learning of deepnetworks from decentralized data. In A. Singh and X. J. Zhu, editors, Proceedings of the 20th InternationalConference on Artificial Intelligence and Statistics, AISTATS 2017, 20-22 April 2017, Fort Lauderdale, FL,USA, volume 54 of Proceedings of Machine Learning Research, pages 12731282. PMLR, 2017b. URL L. Melis, C. Song, E. D. Cristofaro, and V. Shmatikov. Exploiting unintended feature leakage in collaborativelearning. In IEEE Symposium on Security and Privacy, pages 691706. IEEE, 2019. doi: 10.1109/SP.2019.00029. Y. Miao, R. Xie, X. Li, X. Liu, Z. Ma, and R. H. Deng. Compressed federated learning based on adaptivelocal differential privacy. In Annual Computer Security Applications Conference, ACSAC 2022, Austin, TX,USA, December 5-9, 2022, pages 159170. ACM, 2022. doi: 10.1145/3564625.3567973. URL I. Mironov.Rnyi differential privacy.In 30th IEEE Computer Security Foundations Symposium, CSF2017, Santa Barbara, CA, USA, August 21-25, 2017, pages 263275. IEEE Computer Society, 2017. doi:10.1109/CSF.2017.11. URL",
  "I. Mironov, K. Talwar, and L. Zhang. Rnyi differential privacy of the sampled gaussian mechanism. ArXivpreprint, abs/1908.10530, 2019. URL": "M. Moshawrab, M. Adda, A. Bouzouane, H. Ibrahim, and A. Raad. Reviewing federated learning aggregationalgorithms; strategies, contributions, limitations and future perspectives. Electronics, 12(10):2287, 2023. M. Nasr, R. Shokri, and A. Houmansadr. Comprehensive privacy analysis of deep learning: Passive and activewhite-box inference attacks against centralized and federated learning. In IEEE Symposium on Security andPrivacy, pages 739753. IEEE, 2019. doi: 10.1109/SP.2019.00065. M. Nasr, J. Hayes, T. Steinke, B. Balle, F. Tramr, M. Jagielski, N. Carlini, and A. Terzis. Tight auditing of differ-entially private machine learning. In J. A. Calandrino and C. Troncoso, editors, 32nd USENIX Security Sympo-sium, USENIX Security 2023, Anaheim, CA, USA, August 9-11, 2023, pages 16311648. USENIX Association,2023. URL J. Nguyen, K. Malik, H. Zhan, A. Yousefpour, M. Rabbat, M. Malek, and D. Huba. Federated learning withbuffered asynchronous aggregation. In G. Camps-Valls, F. J. R. Ruiz, and I. Valera, editors, InternationalConference on Artificial Intelligence and Statistics, AISTATS 2022, 28-30 March 2022, Virtual Event, volume151 of Proceedings of Machine Learning Research, pages 35813607. PMLR, 2022. URL",
  "OpenAI. GPT-4 technical report. CoRR, abs/2303.08774, 2023. doi: 10.48550/ARXIV.2303.08774. URL": "A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga,A. Desmaison, A. Kpf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai,and S. Chintala. Pytorch: An imperative style, high-performance deep learning library. In H. M. Wallach,H. Larochelle, A. Beygelzimer, F. dAlch-Buc, E. B. Fox, and R. Garnett, editors, Advances in Neural Infor-mation Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS2019, December 8-14, 2019, Vancouver, BC, Canada, pages 80248035, 2019. URL",
  "B. Pej and G. Biczk. Quality inference in federated learning with secure aggregation. IEEE Trans. Big Data,9(5):14301437, 2023. doi: 10.1109/TBDATA.2023.3280406": "N. Ponomareva, S. Vassilvitskii, Z. Xu, B. McMahan, A. Kurakin, and C. Zhang. How to dp-fy ML: A practicaltutorial to machine learning with differential privacy. In A. K. Singh, Y. Sun, L. Akoglu, D. Gunopulos, X. Yan,R. Kumar, F. Ozcan, and J. Ye, editors, Proceedings of the 29th ACM SIGKDD Conference on KnowledgeDiscovery and Data Mining, KDD 2023, Long Beach, CA, USA, August 6-10, 2023, pages 58235824. ACM,2023. doi: 10.1145/3580305.3599561. URL L. Qi, S. Lv, H. Li, J. Liu, Y. Zhang, Q. She, H. Wu, H. Wang, and T. Liu. Dureader vis: A chinese dataset for open-domain document visual question answering. In S. Muresan, P. Nakov, and A. Villavicencio, editors, Findingsof the Association for Computational Linguistics: ACL 2022, Dublin, Ireland, May 22-27, 2022, pages13381351. Association for Computational Linguistics, 2022. doi: 10.18653/V1/2022.FINDINGS-ACL.105.URL C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu. Exploring thelimits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21:140:1140:67,2020. URL O. Ris, J. Jlk, and A. Honkela. Subsampling is not magic: Why large batch sizes work for differentiallyprivate stochastic optimisation. In Forty-first International Conference on Machine Learning, ICML 2024,Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL A. Rajkumar and S. Agarwal. A differentially private stochastic gradient descent algorithm for multipartyclassification. In N. D. Lawrence and M. A. Girolami, editors, Proceedings of the Fifteenth InternationalConference on Artificial Intelligence and Statistics, AISTATS 2012, La Palma, Canary Islands, Spain, April 21-23, 2012, volume 22 of JMLR Proceedings, pages 933941. JMLR.org, 2012. URL",
  "V. Rawte, A. P. Sheth, and A. Das. A survey of hallucination in large foundation models. CoRR, abs/2309.05922,2023. doi: 10.48550/ARXIV.2309.05922. URL": "R. Shokri and V. Shmatikov. Privacy-preserving deep learning. In 53rd Annual Allerton Conference onCommunication, Control, and Computing, Allerton 2015, Allerton Park & Retreat Center, Monticello, IL,USA, September 29 - October 2, 2015, pages 909910. IEEE, 2015. doi: 10.1109/ALLERTON.2015.7447103.URL R. Shokri, M. Stronati, C. Song, and V. Shmatikov. Membership inference attacks against machine learningmodels. In IEEE Symposium on Security and Privacy, pages 318. IEEE Computer Society, 2017. doi:10.1109/SP.2017.41.",
  "H. S. Sikandar, H. Waheed, S. Tahir, S. U. Malik, and W. Rafique. A detailed survey on federated learningattacks and defenses. Electronics, 12(2):260, 2023": ". imsa, M. ulc, M. Uricr, Y. Patel, A. Hamdi, M. Kocin, M. Skalick`y, J. Matas, A. Doucet, M. Coustaty,et al. Docile benchmark for document information localization and extraction. In International Conferenceon Document Analysis and Recognition, pages 147166. Springer, 2023. J. So, C. J. Nolet, C. Yang, S. Li, Q. Yu, R. E. Ali, B. Guler, and S. Avestimehr. Lightsecagg: a lightweight andversatile design for secure aggregation in federated learning. In MLSys. mlsys.org, 2022. S. Song, K. Chaudhuri, and A. D. Sarwate. Stochastic gradient descent with differentially private updates.In IEEE Global Conference on Signal and Information Processing, GlobalSIP 2013, Austin, TX, USA,December 3-5, 2013, pages 245248. IEEE, 2013. doi: 10.1109/GlobalSIP.2013.6736861. URL T. Steinke, M. Nasr, and M. Jagielski. Privacy auditing with one (1) training run. In A. Oh, T. Naumann,A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information ProcessingSystems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans,LA, USA, December 10 - 16, 2023, 2023. URL",
  "A. Suri, P. Kanani, V. J. Marathe, and D. W. Peterson. Subject membership inference attacks in federatedlearning. CoRR, abs/2206.03317, 2022. doi: 10.48550/ARXIV.2206.03317. URL": "R. Tanaka, K. Nishida, and S. Yoshida. Visualmrc: Machine reading comprehension on document images. InThirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on InnovativeApplications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances inArtificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021, pages 1387813888. AAAI Press, 2021.URL H. Tang, C. Yu, X. Lian, T. Zhang, and J. Liu. Doublesqueeze: Parallel stochastic gradient descent withdouble-pass error-compensated compression. In K. Chaudhuri and R. Salakhutdinov, editors, Proceedings ofthe 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California,USA, volume 97 of Proceedings of Machine Learning Research, pages 61556165. PMLR, 2019. URL R. Tito, D. Karatzas, and E. Valveny.Document collection visual question answering.In ICDAR (2),volume 12822 of Lecture Notes in Computer Science, pages 778792. Springer, 2021a. doi: 10.1007/978-3-030-86331-9\\\\_50. R. Tito, M. Mathew, C. V. Jawahar, E. Valveny, and D. Karatzas. ICDAR 2021 competition on document visualquestion answering. In ICDAR (4), volume 12824 of Lecture Notes in Computer Science, pages 635649.Springer, 2021b. doi: 10.1007/978-3-030-86337-1_42.",
  "R. Tito, D. Karatzas, and E. Valveny. Hierarchical multimodal transformers for multipage docvqa. PatternRecognit., 144:109834, 2023. doi: 10.1016/J.PATCOG.2023.109834": "R. Tito, K. Nguyen, M. Tobaben, R. Kerkouche, M. A. Souibgui, K. Jung, J. Jlk, V. P. DAndecy, A. Joseph,L. Kang, E. Valveny, A. Honkela, M. Fritz, and D. Karatzas. Privacy-aware document visual questionanswering. In E. H. B. Smith, M. Liwicki, and L. Peng, editors, Document Analysis and Recognition - ICDAR2024 - 18th International Conference, Athens, Greece, August 30 - September 4, 2024, Proceedings, PartVI, volume 14809 of Lecture Notes in Computer Science, pages 199218. Springer, 2024. doi: 10.1007/978-3-031-70552-6_12. URL M. Tobaben, A. Shysheya, J. Bronskill, A. Paverd, S. Tople, S. Z. Bguelin, R. E. Turner, and A. Honkela.On the efficacy of differentially private few-shot image classification. Transactions on Machine LearningResearch, 2023. ISSN 2835-8856. URL",
  "F. Tramr, A. Terzis, T. Steinke, S. Song, M. Jagielski, and N. Carlini. Debugging differential privacy: A casestudy for privacy auditing. CoRR, abs/2202.12219, 2022b. URL": "I. Tyou, T. Murata, T. Fukami, Y. Takezawa, and K. Niwa. A localized primal-dual method for central-ized/decentralized federated learning robust to data heterogeneity. IEEE Trans. Signal Inf. Process. overNetworks, 10:94107, 2024. doi: 10.1109/TSIPN.2023.3343616. A. Wainakh, F. Ventola, T. Mig, J. Keim, C. G. Cordero, E. Zimmer, T. Grube, K. Kersting, and M. Mhlhuser.User-level label leakage from gradients in federated learning. Proc. Priv. Enhancing Technol., 2022(2):227244, 2022. doi: 10.2478/POPETS-2022-0043. T. Wu, A. Panda, J. T. Wang, and P. Mittal. Privacy-preserving in-context learning for large language models. InThe Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11,2024. OpenReview.net, 2024. URL J. Xu, W. Du, Y. Jin, W. He, and R. Cheng. Ternary compression for communication-efficient federated learning.IEEE Trans. Neural Networks Learn. Syst., 33(3):11621176, 2022. doi: 10.1109/TNNLS.2020.3041185.",
  "P. Yadav, L. Choshen, C. Raffel, and M. Bansal. Compeft: Compression for communicating parameter efficientupdates via sparsification and quantization. arXiv preprint arXiv:2311.13171, 2023": "J. Yosinski, J. Clune, Y. Bengio, and H. Lipson. How transferable are features in deep neural networks? InZ. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, editors, Advances in NeuralInformation Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014,December 8-13 2014, Montreal, Quebec, Canada, pages 33203328, 2014. URL",
  "Y. Youn, Z. Hu, J. Ziani, and J. D. Abernethy. Randomized quantization is all you need for differentialprivacy in federated learning. CoRR, abs/2306.11913, 2023. doi: 10.48550/ARXIV.2306.11913. URL": "A. Yousefpour, I. Shilov, A. Sablayrolles, D. Testuggine, K. Prasad, M. Malek, J. Nguyen, S. Ghosh, A. Bharad-waj, J. Zhao, G. Cormode, and I. Mironov. Opacus: User-friendly differential privacy library in PyTorch.ArXiv preprint, abs/2109.12298, 2021. URL D. Yu, S. Naik, A. Backurs, S. Gopi, H. A. Inan, G. Kamath, J. Kulkarni, Y. T. Lee, A. Manoel, L. Wutschitz,S. Yekhanin, and H. Zhang. Differentially private fine-tuning of language models. In The Tenth InternationalConference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net,2022a. URL D. Yu, S. Naik, A. Backurs, S. Gopi, H. A. Inan, G. Kamath, J. Kulkarni, Y. T. Lee, A. Manoel, L. Wutschitz,S. Yekhanin, and H. Zhang. Differentially private fine-tuning of language models. In The Tenth InternationalConference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net,2022b. URL J. Zhang, S. P. Karimireddy, A. Veit, S. Kim, S. J. Reddi, S. Kumar, and S. Sra. Why are adaptive methods goodfor attention models? In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, editors, Advances inNeural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL",
  "B. Zhao, K. R. Mopuri, and H. Bilen. idlg: Improved deep leakage from gradients. CoRR, abs/2001.02610,2020. URL": "L. Zhu, Z. Liu, and S. Han. Deep leakage from gradients. In H. M. Wallach, H. Larochelle, A. Beygelzimer,F. dAlch-Buc, E. B. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32:Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019,Vancouver, BC, Canada, pages 1474714756, 2019. URL",
  "A.2Dataset": "This section contains additional information regarding the dataset. The data set is described in moredetail in Tito et al. and is available to download on the ELSA benchmark platform The Dataset is created using images fromthe DocILE dataset [imsa et al., 2023], which was published under the MIT License. For PFL-DocVQA we created new annotations for these images. The created annotations are the OCRtranscriptions (using Amazon Textract) and the pairs of question/answer. The question/answer pairsare generated using key/value pairs extracted by Amazon Textract and then manually verified. Foreach key, a question is formed to ask about it, and the answer is the corresponding value. Thesequestions are generated semi-automatically by creating multiple templates for each key and thenusing a language model OpenAI to rephrase them, achieving linguistic diversity. Our datasetis published under the Licence CC-BY-4.0.",
  "A.4.2Track 2": "The baseline is obtained through 5 FL Rounds. It transmits 1.12GB constantly for each communicationstream, which results in a total of 22.32GB during the entire training process. We sample K = 2clients per round and M = 50 providers on each client. The updates are clipped to a norm of 0.5 andthe Gaussian noise is computed so that the privacy budgets of {1, 4, 8} at = 105.",
  "B.1Definitions": "Definition B.1 (Differential Privacy Dwork and Roth ). A randomized mechanism M withrange R satisfies (, )-differential privacy, if for any two adjacent datasets E and E, i.e., E =E {x} for some x in the data domain (or vice versa), and for any subset of outputs O R, it holdsthatPr[M(E) O] e Pr[M(E) O] + (A1) Intuitively, DP guarantees that an adversary, provided with the output of M, can draw almost thesame conclusions (up to with probability larger than 1 ) about any group no matter if it isincluded in the input of M or not Dwork and Roth . This means, for any group owner, aprivacy breach is unlikely to be due to its participation in the dataset. In Federated Learning, the notion of adjacent (neighboring) datasets used in DP generally refers topairs of datasets differing by one client (client-level DP), or by one group of one user (group-levelDP), or by one data point of one user (record-level DP). Our challenge focuses on the group-levelDP Galli et al. , where each group refers to a provider. We use the Gaussian mechanism to upper bound privacy leakage when transmitting information fromclients to the server.Definition B.2. (Gaussian Mechanism Dwork and Roth ) Let f : Rn Rd be an arbitraryfunction that maps n-dimensional input to d logits with sensitivity being:S = maxE,E f(E) f(E)2(A2)",
  "over all adjacent datasets E and E E. The Gaussian Mechanism M, parameterized by , addsnoise into the output, i.e.,M(x) = f(x) + N(0, 2I).(A3)": "As in Abadi et al. , Mironov et al. , we consider the Sampled Gaussian Mechanism(SGM) a composition of subsampling and the additive Gaussian noise (defined in B.5) for privacyamplification. Moreover, we first compute the SGMs Renyi Differential Privacy as in Mironovet al. and then we use conversion Theorem B.8 from Balle et al. for switching back toDifferential Privacy. Definition B.3 (Rnyi divergence). Let P and Q two distributions on X defined over the sameprobability space, and let p and q be their respective densities. The Rnyi divergence of a finite order = 1 between P and Q is defined as follows:",
  "SGq,= f ({x : x E is sampled with probability q}) + N(0, 2Id),where each element of E is independently and randomly sampled with probability q without replace-ment": "As for the Gaussian Mechanism, the sampled Gaussian mechanism consists of adding i.i.d Gaussiannoise with zero mean and variance 2 to each coordinate value of the true output of f. In fact,the sampled Gaussian mechanism draws vector values from a multivariate spherical (or isotropic)Gaussian distribution which is described by random variable N(0, 2Id), where d is omitted if it isunambiguous in the given context.",
  "B.2Analysis": "The privacy guarantee of FL-GROUP-DP is quantified using the revisited moment accountant Mironovet al. that restates the moments accountant introduced in Abadi et al. using the notionof Rnyi differential privacy (RDP) defined in Mironov . Let 0 denote the pdf of N(0, 2) and let 1 denote the pdf of N(1, 2). Let be the mixture oftwo Gaussians = (1 q)0 + q1, where q is the sampling probability of a single record in asingle round.Theorem B.6. Mironov et al. . Let SGq, be the Sampled Gaussian mechanism for somefunction f and under the assumption 2f 1 for any adjacent E, E E. Then SGq, satisfies(, )-RDP if",
  "To compute A, we use the numerically stable computation approach proposed in Mironov et al. (Sec. 3.3) depending on whether is expressed as an integer or a real value": "Theorem B.7 (Composability Mironov ). Suppose that a mechanism M consists of a sequenceof adaptive mechanisms M1, . . . , Mk where Mi : i1j=1 Rj E Ri. If all the mechanisms inthe sequence are (, )-RDP, then the composition of the sequence is (, k)-RDP. In particular, Theorem B.7 holds when the mechanisms themselves are chosen based on the (public)output of the previous mechanisms. By Theorem B.7, it suffices to compute N (|q) at each stepand sum them up to bound the overall RDP privacy budget of an iterative mechanism composed ofsingle DP mechanisms at each step.Theorem B.8 (Conversion from RDP to DP Balle et al. ). If a mechanism M is (, )-RDPthen it is (( + log(( 1)/) (log + log )/( 1), )-DP for any 0 < < 1.Theorem B.9 (Privacy of FL-GROUP-DP). For any 0 < < 1 and 1, FL-GROUP-DP is(min(Tcl (|q) + log(( 1)/) (log + log )/( 1)), )-DP, where N (|q) is definedin Eq. A5, q =C|M|",
  "mink |Gk|": "The proof follows from Theorems B.6, B.7,B.8 and the fact that a group (provider) is sampled inevery federated round if (1) the corresponding client is sampled, which has a probability of C, and(2) the batch of groups sampled locally at this client contains the group, which has a probability of atmost|M|",
  "CSupplementary Information of .3": "Here, we present details for reproducing the results from .3. In all experiments, clientsperform local fine-tuning with batch size = 16 and learning rate = 2e-4. In our code, we train onemodel at a time using data parallelism. Specifically, we split each batch over 8 GPUs, resulting ina batch size of 2 per GPU (we used 8 GeForce GTX 1080 Ti GPUs). Our code will be shared onGithub:",
  "Table A2: We summarize the three methods used. LoRA reduces the number of trainable parameters,tuning HPs reduces the number of messages, and quantization reduces the parameter bitwidth": "LoRA. While the VT5 architecture contains both a language backbone (T5) and vision backbone(DiT), we only use LoRA on the language backbone and insert 110K new parameters per LoRA rank.For the vision backbone (Base), we directly fine-tune the spatial encoder (2.16M params) and visualembedding projection head (0.59M params). All other parameters in the entire model are frozen.Although LoRA changes the model architecture during training, it can be merged with the pretrainedarchitecture after training is complete, which allowed us to make valid submissions.",
  "(layers) 2 (query and value) 2 (A and B) 768 r (rank) = 110, 592 110K r": "We note that LoRA typically takes more iterations to train than full fine-tuning. While the fullfine-tuning baseline provided by the organizers achieves .8242 validation ANLS in 4 rounds (this is5% below the .8676 ANLS at 10 rounds), we find that LoRA takes 7 rounds ( 2) to achieve thesame ANLS. However, the parameter reduction from LoRA ( 100) greatly offsets this cost. For allexperiments in this section, we use LoRA with rank r = 6.",
  "C.2Tuned FL Hyperparameters": "We find that extended local fine-tuning on a single client is very helpful, as it increases utility with noadditional communication cost. In Table A3, we show that training only on a single client can achieve.8242 ANLS. We also find that sampling a single client is more efficient than averaging multipleclients each round. In Table A4, 1 Client usually outperforms 2 Clients when given double thenumber of rounds.",
  "Table A4: Sampling one client and training fordouble the rounds achieves a higher validationANLS than sampling two clients": "One surprising takeaway from our experiments is that the data from a single client is adequate to traina competitive model. However, there are many limitations with limiting the client subsample, whichwe briefly outline. First, in cross-device FL settings which consider a large network (up to millions) ofclients, extreme subsampling can lead to low-quality global updates. Next, since subsampling slowsdown convergence, the model will take more rounds and thus more wall-clock time to train. Finally,in the context of privacy, sampling fewer clients makes it more difficult to bound the sensitivity of theaggregate update with respect to any individual clients data, which results in greater privacy loss.",
  "C.3Quantization": "By default, each parameter is communicated as a 32-bit floating-point value (FP32). We reduce thisto 4.5 bits ( 7) by using NF4 (normal-float) quantization [Dettmers et al., 2023]. While NF4proposes using LoRA on top of a quantized backbone, we use quantization to reduce the size ofall communicated parameters (in both LoRA and the backbone). Similar recent FL methods havegenerally explored combining LoRA with parameter compression to reduce communication [Yadavet al., 2023, Kuo et al., 2024]. In NF4, each parameter is stored using 4 bits (16 unique values) and each block of k = 64 parametersshares an FP32 normalization factor. This adds up to 4 + (32/k) = 4.5 bits per parameter, asshown in Table A2. Parameters are quantized only before communication, while finetuning andaggregation are all done in full precision. As we show in Table A5, quantization slightly harms modelperformance, but this cost is greatly offset by the reduction in communication.",
  "Total Communication55 MB110 MB7.7 MB15.4 MB": "Table A5: We track the validation ANLS after each stage of communication-efficient FL. Whensampling 2 Clients per round, Finetuning and Upload refer to the average ANLS over the twoclient models. - indicates that the same model(s) are evaluated as the cell above e.g. full-precisionUpload and Download do not change the model(s).",
  "D.1FedShampoo for Track 1": "Update rules of FedShampoo: First, we explain the update rule using Shampoo Gupta et al. .As discussed in Sec. 4.4, Shampoo is a second-order optimization method that involves multiplyingthe preconditioning matrix with the (stochastic) gradient, and the preconditioning technique inShampoo is introduced in the local model update in our FedShampoo, which is summarized in Alg.1. In the optimization of models in the form of neural networks, it is typical for model parameters tobe described by a stack of matrices/tensors to transform each layers input and output. Althoughwe have focused on formulating the update rules in a matrix manner (since we will mainly focuson Transformer-based model), it is not a loss of generality. For all clients i [N] and each layerb [B], let W (t)i,b Rdout,bdin,b be the model parameter in the b-th layer of the neural network, and",
  "where denotes the learning rate, and L(t)i Rdout,bdout,b and R(t)i Rdin,bdin,b are the precondi-tioning matrices for the gradient and the weight matrix, respectively": "In Eq. equation A6, the local preconditioning matrices, Li,b and Ri,b, are multiplied to both sidesof the stochastic gradient in a matrix form Gi,b. This process can be interpreted as mitigatingchanges in the local gradient of loss function through model parameter updates by multiplying localpreconditioning matrices. This supports mitigating the negative effects of complex loss landscape inthe loss function using neural networks, and it can lead to fast convergence to the stationary point. Thanks to the Shampoo application in a layer-wise manner, it is possible to track Li,b and Ri,b foreach layer, which significantly reduces the memory footprint. Specifically, while the full-matrixversion of AdaGrad Duchi et al. requires memory linearly proportional to the number ofmodel parameters O(d2out,bd2in,b), Shampoo only requires memory with O(d2out,b + d2in,b) for eachlayer. Furthermore, the inversion of the preconditioning matrices can be efficient, since it takesO(d3out,b + d3in,b) rather than O(d3out,bd3in,b) in terms of computational complexity. Additionally, element-wise clipping was used in the local model update rule, which is a de-factostandard for stable optimization of the Transformer-based models, as mentioned in e.g., Zhang et al.. Due to the heavy-tailed noise in stochastic gradient, the magnitude of updates in modelparameters has significantly changed, leading to unstable convergence. To address this issue, weeffectively alleviated this phenomenon by incorporating the clipping of the magnitude of each elementof gradients into adaptive updates using Shampoo. Finally, as noted in Sec. 4.4, to reduce the amount of communication per round, the embedding layerwas excluded from the optimization target. This results in a reduction of around 26 % amount ofparameters, rather than transmitting whole parameters.",
  "In the following, experimental setups are explained": "Compared methods: In our experiment, we utilized two methods with differing local updaterules: 1) the baseline method using AdamW optimizer, and 2) FedShampoo using Shampoo-basedpreconditioner to the Stochastic Gradient Descent (SGD). Hyperparameter Tuning: To ensure a fair comparison of the two methods, several hyperparameters(learning rate and element-wise clipping threshold C) were empirically tuned. This was done whilemaintaining fixed values for the total communication rounds R = 10, the number of inner loops forlocal update L = 5000, and the number of client sampling K = 2. In Fig. A.2, a summary of ourhyperparameter tuning for FedShampoo is provided. After performing empirical trials, we selected = 2e4 and C = 0.2.",
  "i=1f i(wi)s.t. wi = wj(i N, j Ei),(A7)": "where f i represents the local loss function and {1, . . . , n} N, {1, . . . , i 1, i + 1, . . . , n} Ei.The derivation details can be found in Tyou et al. . A solver for equation A7 over the centralizednetwork is referred to as CLGECL. Due to the constraint of identical local parameters, CLGECLis expected to be robust to gradient drift. For this competition, we propose DP-CLGECL, whichintroduced AdamW as a local update, client sampling, and Gaussian mechanism in DP for CLGECL,as summarized in Alg. 2. To follow the regulation of this competition task, we specified this operation as follows: First, weassume that each clients data set Dk is partitioned into a set Gk of disjoint and pre-defined groups,and each client has different groups. The server randomly selects a subset K of n clients in eachround to update the global model. Each client receives the global model from the server for eachround. The client selects a random subset M of groups, calculates the gradient wGt by SGD withmomentum for each group, and the gradient wGt is updated with the dual variables , clipping itinto clipped the gradient wGt to have a bounded L2 norm of S, where S denotes the sensitivityof the gradient wGt . The sum of wGt for all groups is calculated and perturbed by the Gaussianmechanism. Finally, the k clients selected by the central server calculate the model update differencew wt1, send it to the server, and update the dual variable .",
  ": Output: Client model wt wt1": "Privacy analysis: In the privacy analysis of DP-CLGECL, we aim to determine and that ensurethat wGt + N(0, 2I) guarantees (, )-RDP. We then apply the composition on the RDP, andconvert the RDP to DP. The privacy analysis of FL-GROUP-DP [Marathe and Kanani, 2022, Galliet al., 2023] demonstrates a a method to guarantee (, )-RDP for wGt + N(0, 2I). This analysiscan be applied to our FL-GROUP-DP.",
  "Compared methods: In our testing, we mainly compared: 1) the baseline method based on FedAVGand 2) DP-CLGECL. We also tested their variant versions, such as replacing AdamW with momentum": "Experiment results: The best ANLS for all was achieved by DP-CLGECL. By tuning thehyperparameter, the baseline method given by the competition organizers was also able to achieve ahigher ANLS than the baseline presented. The ANLS of DPCLGECL was further improved by using momentum instead of AdamW, as shownin Fig. A.5. This could be due to the clipping radius not being well-matched with the stochasticgradient using AdamW. A larger clipping radius can degrade the performance due to noise, thus, itseems better to use momentum than AdamW. In this competition, mitigating the gradient drift withCLGECL was also effective in improving performance. However, calculating the stochastic gradientthat matches the clipping radius was the most effective in improving performance. Figure A.4: convergence curve evaluating using the global model. (a) validation ANLS at = 1,(b) validation ANLS at = 4, (c) validation ANLS at = 8. We used clipping radius S = 0.5, thenumber of client sampling C = 2, the learning rate = 0.0002, and the number of communicationround R = 14 for hyperparameter selection. Figure A.5: convergence curve evaluating using the global model at = 1. (Left) Validation accuracy,(right) Validation ANLS. We used clipping radius S = 0.5, the number of client sampling C = 2,learning rate = 0.0004, and the number of communication round R = 12."
}