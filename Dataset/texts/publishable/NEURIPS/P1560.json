{
  "Abstract": "In this paper we present a novel method for efficient and effective 3D surfacereconstruction in open scenes. Existing Neural Radiance Fields (NeRF) basedworks typically require extensive training and rendering time due to the adoptedimplicit representations. In contrast, 3D Gaussian splatting (3DGS) uses an explicitand discrete representation, hence the reconstructed surface is built by the hugenumber of Gaussian primitives, which leads to excessive memory consumption andrough surface details in sparse Gaussian areas. To address these issues, we proposeGaussian Voxel Kernel Functions (GVKF), which establish a continuous scenerepresentation based on discrete 3DGS through kernel regression. The GVKFintegrates fast 3DGS rasterization and highly effective scene implicit represen-tations, achieving high-fidelity open scene surface reconstruction. Experimentson challenging scene datasets demonstrate the efficiency and effectiveness of ourproposed GVKF, featuring with high reconstruction quality, real-time renderingspeed, significant savings in storage and training memory consumption. Projectpage:",
  "Introduction": "3D surface reconstruction in open scenes holds great significance in various practical applications,such as autonomous driving, virtual reality, urban planning and etc. However, achieving high-fidelityand efficient open scene reconstruction has been a longstanding challenge, due to the trade-offbetween the rendering quality and the required resources for optimization. In pursuit of this goal, two predominant approaches are Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) based methods. On one hand, NeRF-basedimplicit representations typically require extensive training and rendering time, which limits thepractical use in large-scale scene reconstruction . On the other hand, 3DGS adoptsexplicit representations, which enables high-quality novel view synthesis while achieving real-timerendering. This makes 3DGS more feasible for efficient scene reconstruction in the applications suchas autonomous driving and virtual reality. Recently, there are studies using 3DGS technology for novel view synthesis and surface reconstructionin street scenes and urban environments . For instance, SuGaR attempts toreconstruct the 3D surfaces based on Gaussian points. However, it has been noted that overlylarge and sparse Gaussian points can significantly affect the geometric representations of the scene,particularly in background areas. To overcome these challenges, the 2D Gaussian Splatting (2DGS) proposes to use Gaussian surfaces as surfels to represent complex geometries , thereby",
  "Volume RenderingContinuous integrationBetter 3D surface representationLow rendering speed due to con-tinuous sampling": "improving the surface reconstruction quality. Particularly, 2DGS faces challenges when processinglarge-scale scenes, as it requires the explicit representation of a large number of Gaussian primitives,leading to significant GPU memory consumption. Therefore, 2DGS still exhibits limitations in novelview synthesis capabilities and the geometric representation of large-scale scenes. In , we summarize the comparison of 3DGS rendering and volume rendering. To fully leveragethe fast rendering advantages of Gaussian alpha blending while achieving effective implicit scenerepresentation, we propose a novel Gaussian Voxel Kernel Functions (GVKF) method. Firstly, GVKFutilizes voxelization to implicitly represent 3DGS, managing the growth and pruning of Gaussiansplats. This approach retains the expressive power of explicit Gaussian splats while enabling efficientmanagement of these splats. Secondly, we carefully analyze the intrinsic connection betweenGaussian splatting alpha blending rendering and traditional volume rendering from a mathematicalperspective. We establish a 3DGS-based method to represent continuous scene opacity density fieldsthrough kernel regression. This makes it possible for discrete Gaussians to represent continuousscenes. By replacing the discrete opacity values in original 3DGS rendering pipeline (which can beviewed as collapsed kernel functions) with Gaussian kernel functions, we maintain the advantagesof the original 3DGS alpha blending while optimizing the representation of continuous scenes.Moreover, we demonstrate that our proposed rendering method is mathematically consistent withtraditional volume rendering. Thirdly, based on our constructed scene opacity representation, whichis also known as the scene opacity field, we derive the bidirectional mapping relationship betweenopacity and the scene surface. This enables direct mesh extraction for scene surface. In summary, ourcontributions are as follows: We propose GVKF, an implicit continuous scene reconstruction method that integrates theeffectiveness of implicit representation with the fast rasterization advantages of GaussianSplatting, without the need for computationally intensive volume rendering.",
  "Novel View Synthesis": "The introduction of Neural Radiance Fields (NeRF) has significantly advanced the developmentof 3D reconstruction and novel view synthesis. NeRF employs volumetric rendering techniquesto intricately simulate the geometric structure of scenes and viewpoint-dependent characteristics,thereby considerably enhancing the quality of image rendering. Following NeRF, variants such asMip-NeRF and Zip-NeRF have addressed the aliasing issues during rendering. Additionally,UC-NeRF , designed for outdoor scenes, enhances image consistency through color correctionand pose refinement. InstantNGP accelerates training and improves rendering efficiency byoptimizing subvolume processing with grid pyramid techniques. Meanwhile, other feature grid-basedscene representation methods have been extensively explored to enhance the trainingcapability and expressiveness of models. Recently, 3D Gaussian Splatting (3DGS) effectivelyrepresents complex scenes using 3D Gaussian points, significantly boosting the efficiency of real-timehigh-resolution image rendering while maintaining rendering quality. Further research efforts likeScaffold-GS and Octree-GS have attempted more effective methods to organize and manageGaussian points, which helps reduce memory usage and speed up training. Kernel RegressionSparse Voxel",
  "Translation": ": Framework of Gaussian Voxel Kernel Functions (GVKF) for scene representation. In thisframework, discrete Gaussian primitives G represent continuous opacity density (t) on the ray viakernel regression. After slightly modifying the rasterization pipeline, the kernel function can beintegrated into alpha blending rasterization without introducing dense points sampling. Additionally,we directly define the mapping relationship between the neural opacity field and the implicit surface.",
  "Surface Reconstruction": "Traditional isosurface extraction, relying on density thresholds, often struggles with fine details due toresolution and noise constraints. Recent studies propose more complex representation methods .For instance, NeuS uses MLP networks for occupancy grids or SDF, improving reconstructionaccuracy and noise reduction . Techniques like BakedSDF translate theoptimization of NeRF or neural SDFs into 3D meshes, enhancing features through high-resolutiongrids but increasing computational load. NeuS2 introduces a novel formula for second-orderderivatives with multi-resolution hash encoding and CUDA-based MLP technology, significantlyreducing training time. StreetSurf optimizes SDF mappings in open scenes and decouplesstatic and dynamic objects. Despite advancements, NeRF-based methods still need optimization forprocessing speed and real-time rendering. 3DGS has gained attention for its high-quality scene reconstruction and rapid processing capabilities. 3DGS uses multiple 3D Gaussian distributions with anisotropic covariance for precise controlover scene attributes . This technology enhances surface reconstruction methods like SuGaR, which employs Poisson surface reconstruction for fast and accurate mesh extraction. However,irregular Gaussian sphere distribution affects surface quality. To improve this, 2DGS uses2D Gaussian planes for better surface conformity and TSDF for accurate reconstruction, though itmay cause surface fragmentation. GOF directly extracts surfaces using opacity thresholds andtetrahedral mesh extraction but is limited by high VRAM requirements. GSDF combines 3DGSwith a NeuS-like SDF branch for optimized rendering and reconstruction, increasing training time.Despite their potential, 3DGS-based methods face challenges like managing Gaussian points, highVRAM consumption, and degraded rendering quality.",
  "Methods": "As shown in , we first introduce the implicit neural 3DGS primitives representation based on asparse voxel grid, which offers highly efficient storage management and the fitting power of neuralnetworks. Secondly, we present our GVKF-based continuous scene representation, to explain itsrationale, we have analyzed its intrinsic connections with Gaussian alpha blending and traditionalvolume rendering from a statistical analysis perspective. Finally, we describe the relationshipbetween the proposed continuous scene representation (a neural opacity field) and implicit surface,and derive an explicit mapping function for mesh reconstruction.",
  "Voxel Gaussian Representation": "To achieve orderly 3DGS management while minimizing the explicit expression of them to savetraining storage consumption, we use a spatial sparse voxel grid to manage Gaussian primitives.During the initialization phase, the sparse grid is generated from the downsampled SfM point clouds and dynamically grows or being eliminated during training. Each sparse grid is allowed to generateup to m Gaussian primitives, and all these primitives are limited to a small range of space centered atthe voxel grid. Gaussian Generation For a particular 3D Gaussian expression, five attributes are required: p R3(position), R (opacity), R R33 (rotation matrix), s R3 (scaling), and c R3 (color). Then,a Gaussian G(x) can be generated as:",
  "= MLP(F, camera), R = MLPR(F), s = MLPs(F), c = MLPc(F, camera).(2)": "For alpha and color MLPs, the view camera and feature vector F are inputs, facilitating view-dependent fitting. Relative coordinates of Gaussians to the parent voxel center are stored with F,compressing explicit Gaussian components and leveraging the MLPs fitting capacity. Gaussians aredynamically generated each iteration and recycled post-update, reducing memory usage. Voxel Registration. To control Gaussian numbers in large open scene, we eschew the traditionaladaptive density control strategy, adopting a method inspired by scaffold-Gaussian and Octree-Gaussian . The voxel registration is based on gradient accumulation. After each iteration,gradients from 3DGS are recorded and accumulated in their respective voxels, denoted as . Voxelswhere exceeds a set threshold are subdivided into eight subvoxels to increase grid resolution,continuing until the maximum depth is reached. Additionally, less frequently used voxels arediscarded after a specified period.",
  "Neural Opacity Field of 3DGS": "Since 3DGS rasterization rendering and traditional volume rendering share some overlapping con-cepts, in this section, we sort them out and introduce our method from a statistical perspective whileavoiding introducing redundant mathematical symbols. Continuous Scene Description. We define (t) : [0, +] as the opacity density function,which measures the probability of a ray encountering a particle at position t. We define T (t) :[0, +] as the transmission function, which measures the probability that a ray has notencountered any particles from its origin to point t. Considering the probability that a ray does notencounter any particles at time step t + dt, denoted as T (t + dt), it is evident that T (t + dt) =T (t)(1 (t)dt). Solving this differential equation, we obtain the relationship between T (t) and(t):",
  "(t)dt).(3)": "Therefore, we obtain the cumulative distribution function (CDF) of the probability that a ray hits aparticle over the interval [0, t]: (t) = 1 T (t), with the corresponding probability density function(PDF) being (t) = T (t) (t). From the perspective of volume rendering, this PDF is used as theprobability of the appearance of color along the ray, ultimately taking the mathematical expectationof the color as the ray color:",
  "j=1(1 j)(5)": "where opacity i represents the accumulated result in a sampling interval i of volume density i,hence the value of N does not influence the result as long as i is adapted enough. Based on thesimilar idea of volume rendering, the PDF (t) = T (t) (t) can also be reasonably considered as the probability of the appearance of a surface along the ray, where the place with the highestprobability density is most likely to have a surface. Correspondingly, on the CDF (t), this is theplace where the derivative is the largest. In this paper, we use the CDF (t) to describe continuousscenes based on the camera rays, to facilitate integration with the 3DGS rasterization renderingpipeline. In section 3.3 , we will prove that under the 3DGS representation, the place where thederivative of CDF is the largest is not actually the surface, so a method to locate the surface will beintroduced.",
  "Kernel Regression of 3DGS": "In implicit scene representation methods based onvolume rendering, the continuous opacity densityfunction is directly predicted by a MLP. Inour approach, the continuous opacity density func-tion (t) is fitted through kernel regression viadiscrete Gaussian primitives after a differentiabletransformation: Gi(x, y, z) Ki(t ti) from3D to 1D. The transformation consists of threesteps: (1). The Gaussian primitives that the raypasses through are selected as kernel functions.(2). According to the Ray-Gaussian Intersectionmethod , the ray is transformed into the localcoordinate system of each 3DGS to obtain the 1Dprobability density alone the ray. Here, the peak ofthe 1D probability density, denoted as ti, is definedas the Ray-Gaussian Intersection , in-dicating that the 3DGS has the greatest influenceat this point alone the ray. (3). To integrate withregularization methods , we assume thateach 3DGS fits the surface of the object. There-fore, after ti, the probability density continues to remain at its maximum value, indicating that theobject is solid. Without loss of generality, the opacity density (t) on camera ray can be expressed as:",
  "j=1(1 j G2Dj)(7)": "In this scenario, i is constant value representing the opacity of Gaussians. This point-based renderingis coherent with Eq. 5, with extremely sparse sampling points to simulate dense volume rendering.However, it is impossible to recover continuous opacity density alone the ray from such a renderingequation, as illustrated in row-3 of . This is because the third row of the covariance matrix of3DGS is discarded, and it is directly projected onto a 2D plane to evaluate the impact on the opacityof points along the ray. From the perspective of Eq. 6, this means that along the ray, the influencerange of all N kernel functions that intersect with the ray collapses to an infinitesimally value, makingit impossible to recover a continuous opacity density field. To solve this, Eq. 7 can be modified to: 3",
  ":Illustration of functions(u), (u), (u)": "This implicit opacity field (denoted as neural opacity fieldsince it is represented by neural Gaussians) measures theCDF of the probability that a ray hits solid scene surface.In the next section, we introduce the mapping of (t) toimplicit surface. We represent implicit surface with signed distance func-tion (SDF), denoted as function D(t) on the camera ray.To recover D(t) of given (t) that is calculated from welltrained 3DGS, we firstly study the reverse mapping prob-lem: : D(t) (t) Opacity Density Near the Surface. To ensure that the3DGS aligns with the objects surface and thus reflectsthe objects shape, depth distortion regularization is introduced during the Gaussian training process. Thisencourages the distribution of 3DGS along the ray to ag-gregate together, causing the peak of the kernel functions to coincide with the objects surface. Inthe next discussion, the coordinate of object surface on the ray is assumed as t with D(t) = 0.Considering (t) at the interval t [0, t], we have:",
  "](13)": "It is easy to prove that h(u) crosses a unique zero point u0 from top to bottom on the u-axis, andu0 < 0. This means that the peak of (u) will appear before the surface, so it is not reasonable tosimply determine the actual intersection point of the light ray with the surface by directly evaluatingthe peak of (u). To locate the accurate surface, a transcendental equation of u is needed to be",
  "Mapping from Opacity to Surface": "Based on the analysis above, we can always have ex-act number of u0 via numerical computation method.However, it is hard to find out the inverse functionof (u) for directly building the mapping of (t) toD(t). For the balance of surface smoothing whilereducing the indelible error, we represent mappingrelationship of u (u) via Logistic Function asfollows:",
  "+ exp((u u0))(15)": "where represents the smooth factor. We choose Logistic Function because of its formal is conciseand shares similar shape of (u). More importantly, it only has one inflection point at (0, 0.5), whichcan be used to simulate the inflection point of (u) after translation. Finally, we represent implicitSDF function via Inverse function transformation of Logistic Function, as shown in :",
  "PSNR: 29.87PSNR: 27.50": ": Qualitative comparison on the Tanks and Temples dataset shows that our methodexcels in reconstructing complex backgrounds with high geometric granularity. In contrast, 2DGSoften results in fragmented backgrounds, while SuGaR displays uneven spherical shapes, affectingboth visual and geometric quality. : Quantitative evaluation of novel view synthesis and surface reconstruction on the WaymoOpen Scene dataset . Using LiDAR data as ground truth, we calculated Chamfer Distance (C-D)values for reconstruction accuracy. Our method performs excellently in both novel view synthesisand surface reconstruction, outperforming other methods in Gaussian point usage, VRAM occupancy,and real-time rendering.",
  "Experimental Settings": "Datasets. To assess our methods performance against baseline methods in open scenes, we usedthree datasets. We first experimented with the Waymo Open Scene dataset , using three camerasper scene from five available, each scene containing about 600 images. We employed LiDAR pointclouds to evaluate reconstruction quality, although LiDAR data was not used as training input. Wealso tested on the Tank and Temple dataset , which includes trajectories and ground truth for sixselected scenes. Lastly, we evaluated the Mip-NeRF 360 dataset ; due to the absence of groundtruth, our focus was on novel view synthesis to demonstrate our methods efficacy in this aspect. Baselines. In terms of surface reconstruction, we presented the results on the Waymo dataset intables 2 and figures 5, comparing state-of-the-art implicit methods (such as NeuS , F2-NeRF ,StreetSurf ) and explicit methods (such as 3DGS , SuGaR , 2DGS ). We utilizedPSNR to evaluate the results of novel view synthesis and Chamfer distance to measure reconstructionaccuracy, while also recording training time, VRAM usage, and the size of the Gaussian point filespost-training. Additionally, as shown in and , we conducted comparisons on the Tank : Quantitative evaluation on the Tanks and Temples dataset using F1 scores and trainingtime as metrics. Our method outperforms all existing explicit methods in F1 scores and is comparableto implicit methods in reconstruction accuracy, with significantly reduced training time. Theseresults highlight our methods efficiency and accuracy. Comparation of concurrent work GOF ispresented in Appendix A.4.",
  "Mean0.380.350.500.190.090.300.36Time>24 h>24 h>24 h>1 h15 min30 min1.5 h": "and Temple dataset with implicit methods (such as NeuS , Geo-NeuS , Neuralangelo ) andexplicit methods (such as 3DGS , SuGaR , 2DGS ). We used official scripts to evaluateF1 scores. For novel view synthesis, we compared various advanced methods on the Mip-NeRF 360dataset, including NeRF , Deep Blending , Instant NGP , MERF , Mip-NeRF 360, BakedSDF , 3DGS , SuGaR , and 2DGS . We use evaluation metrics such asPSNR, SSIM, and LPIPS. : Quantitative evaluation on the Mip-NeRF360 outdoor scene dataset is presented. Sincethe dataset lacks ground truth for surface recon-struction, we assessed the results of novel viewsynthesis.MethodPSNR SSIM LPIPS NeRF21.460.4580.515Deep Blending21.540.5240.364Instant NGP22.900.5660.371MERF23.190.6160.343Mip-NeRF 36024.470.6910.283BakedSDF22.470.5850.3493DGS24.240.7050.283SuGaR22.760.6310.3492DGS24.330.7090.284GVKF (Ours)25.470.7570.240 Implementation Details Our method modifiesthe representation of 3DGS and slightly adjuststhe opacity weights in the rendering pipelineusing Gaussian kernel functions. This ensurescompatibility with other components of Gaus-sian rasterization rendering. Similarly, we em-ploy the same L1 loss and D-SSIM loss as 3DGSto supervise color loss, and we use the sameGaussian regularization term as 2DGS and GOFto promote alignment between the Gaussiansand the surface. After training, the SDF fieldof the scene can be directly extracted basedon Eq. 16 and exported to a mesh with theMC/MT algorithm. To export com-plete sky and background, the modified MT al-gorithm in GOF is used.",
  "Analysis": "demonstrates the superiority of our method in capturing detailed features of roadside houses,bushes, and other objects. In contrast, the 2DGS method produced more holes and fragmentation,while the StreetSurf method lost some critical geometric features. The results in indicate thatour method surpasses other methods in terms of view synthesis and reconstruction accuracy, and itrequires fewer Gaussian points and VRAM for large-scale scene reconstructions. highlightsour methods excellent performance in scene restoration. The SuGaR method generated excessiveirregular protrusions, and 2DGS exhibited more fragmentation and floating debris. Accordingto the results in , our method outperforms all explicit methods and achieves comparablereconstruction results to implicit methods, while maintaining equivalent GPU time usage. The resultsin confirm that our method leads in novel view synthesis across all compared methodologies.",
  "Ours26.310.36 9 G90 M 1.5 h 15 minw/o voxel 23.60 (-2.71) 0.39 (+0.03) 16 G ( 1.6) 467 M ( 5.2) 1.4 h 15 minw/o sdf26.310.30 (-0.06) 9 G90 M 1.5 h15 min": "presented in . When the voxel size is too large, the sparse neural Gaussians fail to learn thescene representation and return NaN errors. As the number of voxels increases, more Gaussians aregenerated for scene representation, thereby enhancing the quality of novel view synthesis. However,the improvements plateau when the voxel size is reduced to 0.001, which also requires more trainingtime and becomes impractical. Therefore, we set the voxel size to 0.01 to balance training time andrendering quality.",
  "2 k---0.1 80 k 110 k29.341.2 h0.01 90 k 1100 k30.241.5 h0.001 100 k 1100 k30.294 h": "We further conducted ablation study on theTanks and Temples dataset to evaluate theimpact of voxel representation and SDF map-ping. The results are presented in Tab. 6. It canbe observed that utilizing voxel representationsignificantly improves the PSNR for NVS tasksand reduces memory consumption dramaticallycompared to naive 3DGS setup. Although there is a slight decrease in the geometric quality of surfacereconstruction, we consider this trade-off acceptable.",
  "Limitation": "Implicit methods, such as those based on NeRF , typically utilize a global fitting approachfor SDF, which allows them to fully leverage the universal approximation capabilities of MLPs. Thisis advantageous even in areas with sparse viewpoints. However, our current method employs a localline-of-sight-based SDF fitting, a compromise made to adapt to the 3DGS rendering style. Thismeans that regions not covered by the training viewpoints lack fitting capability, resulting in unevensurfaces. In addition, While our method advances 3D surface reconstruction in open scenes, it faces challengeswith dynamic objects and the decoupling of distant and near views, sometimes misrepresenting thesky as a surface enveloping the model. The lack of sufficient prior knowledge for optimizing complexscenes also poses limitations.",
  "Conclusion": "This paper introduces GVKF, combining Gaussian splattings rapid rasterization with the efficiencyof implicit expressions to enhance reconstruction quality and speed significantly. By employing avoxelized implicit representation of 3DGS, GVKF retains the expressive power of explicit Gaussianmaps while managing them effectively. We have explored the relationship between Gaussian splat-tings alpha blending and traditional volume rendering, developing a GS-based method to representcontinuous scene opacity density fields through kernel regression, addressing 3DGSs limitations incontinuous scene representation. Experimental results demonstrate GVKFs effectiveness in open scenes, showing notable improve-ments in reconstruction accuracy, real-time rendering speeds, and reductions in storage and memoryusage. These advancements support applications in fields like autonomous driving and virtual reality,pushing forward surface reconstruction technology.",
  "Qiancheng Fu, Qingshan Xu, Yew-Soon Ong, and Wenbing Tao.Geo-neus: Geometry-consistent neural implicit surfaces learning for multi-view reconstruction, 2022": "Jian Gao, Chun Gu, Youtian Lin, Hao Zhu, Xun Cao, Li Zhang, and Yao Yao. Relightable3d gaussian: Real-time point cloud relighting with brdf decomposition and ray tracing. arXivpreprint arXiv:2311.16043, 2023. Jianfei Guo, Nianchen Deng, Xinyang Li, Yeqi Bai, Botian Shi, Chiyu Wang, Chenjing Ding,Dongliang Wang, and Yikang Li. Streetsurf: Extending multi-view implicit surface reconstruc-tion to street views. arXiv preprint arXiv:2306.04988, 2023.",
  "Michael Oechsle, Songyou Peng, and Andreas Geiger. Unisurf: Unifying neural implicitsurfaces and radiance fields for multi-view reconstruction, 2021": "Hanspeter Pfister, Matthias Zwicker, Jeroen van Baar, and Markus Gross. Surfels: surfaceelements as rendering primitives. In Proceedings of the 27th Annual Conference on ComputerGraphics and Interactive Techniques, SIGGRAPH 00, page 335342, USA, 2000. ACMPress/Addison-Wesley Publishing Co. Christian Reiser, Rick Szeliski, Dor Verbin, Pratul Srinivasan, Ben Mildenhall, Andreas Geiger,Jon Barron, and Peter Hedman. Merf: Memory-efficient radiance fields for real-time viewsynthesis in unbounded scenes. ACM Trans. Graph., 42(4), jul 2023.",
  "Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct voxel grid optimization: Super-fastconvergence for radiance fields reconstruction, 2022": "Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, PaulTsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, Vijay Vasudevan, Wei Han, JiquanNgiam, Hang Zhao, Aleksei Timofeev, Scott Ettinger, Maxim Krivokon, Amy Gao, AdityaJoshi, Yu Zhang, Jonathon Shlens, Zhifeng Chen, and Dragomir Anguelov. Scalability inperception for autonomous driving: Waymo open dataset. In 2020 IEEE/CVF Conference onComputer Vision and Pattern Recognition (CVPR), Jun 2020. Matthew Tancik, Vincent Casser, Xinchen Yan, Sabeek Pradhan, Ben Mildenhall, Pratul Srini-vasan, Jonathan T. Barron, and Henrik Kretzschmar. Block-NeRF: Scalable large scene neuralview synthesis. arXiv, 2022.",
  "A.2More Implementation Details": "As demonstrated in the ablation experiments, to balance quality and speed, we chose to downsamplethe initial Gaussian point cloud using a voxel size of 0.01. Within each voxel, the dimension of F isset to 32, and it stores the relative coordinates of 10 Gaussian points, indicating that the maximumnumber of Gaussians generated per voxel grid is 10. Gaussians with an opacity less than 0 will behidden during each iteration. In each scene, all voxel grids share a total of four MLPs, which decodedifferent Gaussian attributes from the corresponding voxels. Regarding voxel registration, the gradient threshold is empirically set to 2 104, meaning thatvoxel grids with an average gradient exceeding this value after each iteration will be subdivided usingan octree method. The maximum recursion depth is set to 3 to control the number of Gaussians inthe scene, ensuring it does not exceed a certain threshold. Voxel evaluation is performed every 500iterations to determine which voxels should be subdivided or reclaimed. For other settings, we striveto remain consistent with the original 3DGS settings.",
  "A.3More Results": "Our method focuses on the challenging task of open scene reconstruction. Here, we provide acomprehensive quantitative comparison with other related methods on the Mip360 dataset, as shownin . Additionally, we have included more experimental results on the Mip360 and Tank andTemple datasets, as shown in Figures 8 and 9. For more qualitative results, please visit the projectpage. Discussion on indoor scene. We observe that current methods based on 3DGS perform adequately forindoor scenes, where there is typically 360-degree viewpoint coverage. However, they underperformin outdoor scenes due to limited viewpoint coverage. Heuristic splitting and pruning strategies inoriginal 3DGS tend to fit the training viewpoints rather than distributing evenly across the space.This leads to poorer novel view synthesis results in outdoor environments. As illustrated in ,without a voxel grid, heuristic Gaussian growth strategies result in uneven spatial distribution ofGS, sometimes even creating holes. Conversely, using voxel grids to constrain Gaussians allows forefficient management of their spatial distribution, supporting better novel view synthesis.",
  "A.4Comparation to Gaussian Opacity Field": "The similar rendering equation is firstly proposed by GOF , while this work provides in-depthanalysis of the relationship among this rendering strategy, volume rendering and Gaussian alphablending. Different from GOF, our scene representation is implicit, addressing the common issue ofhigh memory consumption faced by 3D Gaussian splatting. Additionally, we developed a mappingfunction from opacity to SDF to alleviate the influence of directly linear transform between thesefields. As shown in Tab. 8, GOF uses explicit Gaussian management, still faces high storage consumptionissues, making training large scenes on a single card challenging. Our method achieves betternovel view synthesis results with less storage usage. However, as shown in Tab. 9, our currentimplementation has some geometric precision gaps compared to GOF, the potential reasons mayinclude:",
  "NeurIPS Paper Checklist": "The checklist is designed to encourage best practices for responsible machine learning research,addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not removethe checklist: The papers not including the checklist will be desk rejected. The checklist shouldfollow the references and follow the (optional) supplemental material. The checklist does NOT counttowards the page limit.",
  "Please provide a short (12 sentence) justification right after your answer (even for NA)": "The checklist answers are an integral part of your paper submission. They are visible to thereviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it(after eventual revisions) with the final version of your paper, and its final version will be publishedwith the paper. The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation.While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided aproper justification is given (e.g., \"error bars are not reported because it would be too computationallyexpensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering\"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, weacknowledge that the true answer is often more nuanced, so please just use your best judgment andwrite a justification to elaborate. All supporting evidence can appear either in the main paper or thesupplemental material, provided in appendix. If you answer [Yes] to a question, in the justificationplease point to the section(s) where related material for the question can be found.",
  ". Claims": "Question: Do the main claims made in the abstract and introduction accurately reflect thepapers contributions and scope?Answer: [Yes]Justification: The abstract and introduction clearly outline the contributions of the paper,including the development of a novel method for open scene surface reconstruction thatcombines the strengths of explicit and implicit approaches, and its application to large-scalescene reconstruction tasks (Sec. 1).Guidelines:",
  "The answer NA means that the paper has no limitation while the answer No means thatthe paper has limitations, but those are not discussed in the paper": "The authors are encouraged to create a separate \"Limitations\" section in their paper. The paper should point out any strong assumptions and how robust the results are toviolations of these assumptions (e.g., independence assumptions, noiseless settings,model well-specification, asymptotic approximations only holding locally). The authorsshould reflect on how these assumptions might be violated in practice and what theimplications would be. The authors should reflect on the scope of the claims made, e.g., if the approach wasonly tested on a few datasets or with a few runs. In general, empirical results oftendepend on implicit assumptions, which should be articulated. The authors should reflect on the factors that influence the performance of the approach.For example, a facial recognition algorithm may perform poorly when image resolutionis low or images are taken in low lighting. Or a speech-to-text system might not beused reliably to provide closed captions for online lectures because it fails to handletechnical jargon.",
  "If applicable, the authors should discuss possible limitations of their approach toaddress problems of privacy and fairness": "While the authors might fear that complete honesty about limitations might be used byreviewers as grounds for rejection, a worse outcome might be that reviewers discoverlimitations that arent acknowledged in the paper. The authors should use their bestjudgment and recognize that individual actions in favor of transparency play an impor-tant role in developing norms that preserve the integrity of the community. Reviewerswill be specifically instructed to not penalize honesty concerning limitations.",
  ". Experimental Result Reproducibility": "Question: Does the paper fully disclose all the information needed to reproduce the main ex-perimental results of the paper to the extent that it affects the main claims and/or conclusionsof the paper (regardless of whether the code and data are provided or not)?Answer: [Yes]Justification: The paper provides detailed information on the datasets used, the experimentalsetup and other relevant details necessary to reproduce the results. (Sections 4)",
  "If the contribution is a dataset and/or model, the authors should describe the steps takento make their results reproducible or verifiable": "Depending on the contribution, reproducibility can be accomplished in various ways.For example, if the contribution is a novel architecture, describing the architecture fullymight suffice, or if the contribution is a specific model and empirical evaluation, it maybe necessary to either make it possible for others to replicate the model with the samedataset, or provide access to the model. In general. releasing code and data is oftenone good way to accomplish this, but reproducibility can also be provided via detailedinstructions for how to replicate the results, access to a hosted model (e.g., in the caseof a large language model), releasing of a model checkpoint, or other means that areappropriate to the research performed. While NeurIPS does not require releasing code, the conference does require all submis-sions to provide some reasonable avenue for reproducibility, which may depend on thenature of the contribution. For example(a) If the contribution is primarily a new algorithm, the paper should make it clear howto reproduce that algorithm.",
  "(b) If the contribution is primarily a new model architecture, the paper should describethe architecture clearly and fully": "(c) If the contribution is a new model (e.g., a large language model), then there shouldeither be a way to access this model for reproducing the results or a way to reproducethe model (e.g., with an open-source dataset or instructions for how to constructthe dataset). (d) We recognize that reproducibility may be tricky in some cases, in which caseauthors are welcome to describe the particular way they provide for reproducibility.In the case of closed-source models, it may be that access to the model is limited insome way (e.g., to registered users), but it should be possible for other researchersto have some path to reproducing or verifying the results.",
  "Guidelines:": "The answer NA means that the paper does not use existing assets. The authors should cite the original paper that produced the code package or dataset. The authors should state which version of the asset is used and, if possible, include aURL. The name of the license (e.g., CC-BY 4.0) should be included for each asset. For scraped data from a particular source (e.g., website), the copyright and terms ofservice of that source should be provided. If assets are released, the license, copyright information, and terms of use in thepackage should be provided. For popular datasets, paperswithcode.com/datasetshas curated licenses for some datasets. Their licensing guide can help determine thelicense of a dataset.",
  ". Code Of Ethics": "Question: Does the research conducted in the paper conform, in every respect, with theNeurIPS Code of Ethics [Yes]Justification: The research adheres to the NeurIPS Code of Ethics, with considerations forreproducibility, transparency, and societal impact addressed throughout the paper.Guidelines: The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. If the authors answer No, they should explain the special circumstances that require adeviation from the Code of Ethics.",
  ". Broader Impacts": "Question: Does the paper discuss both potential positive societal impacts and negativesocietal impacts of the work performed?Answer: [Yes]Justification: The paper includes a section discussing the broader impacts of the proposedmethod, highlighting both potential positive applications and possible negative consequences.(Sec. 5)Guidelines: The answer NA means that there is no societal impact of the work performed. If the authors answer NA or No, they should explain why their work has no societalimpact or why the paper does not address societal impact. Examples of negative societal impacts include potential malicious or unintended uses(e.g., disinformation, generating fake profiles, surveillance), fairness considerations(e.g., deployment of technologies that could make decisions that unfairly impact specificgroups), privacy considerations, and security considerations. The conference expects that many papers will be foundational research and not tiedto particular applications, let alone deployments. However, if there is a direct path toany negative applications, the authors should point it out. For example, it is legitimateto point out that an improvement in the quality of generative models could be used togenerate deepfakes for disinformation. On the other hand, it is not needed to point outthat a generic algorithm for optimizing neural networks could enable people to trainmodels that generate Deepfakes faster. The authors should consider possible harms that could arise when the technology isbeing used as intended and functioning correctly, harms that could arise when thetechnology is being used as intended but gives incorrect results, and harms followingfrom (intentional or unintentional) misuse of the technology. If there are negative societal impacts, the authors could also discuss possible mitigationstrategies (e.g., gated release of models, providing defenses in addition to attacks,mechanisms for monitoring misuse, mechanisms to monitor how a system learns fromfeedback over time, improving the efficiency and accessibility of ML).",
  "According to the NeurIPS Code of Ethics, workers involved in data collection, curation,or other labor should be paid at least the minimum wage in the country of the datacollector": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with HumanSubjectsQuestion: Does the paper describe potential risks incurred by study participants, whethersuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)approvals (or an equivalent approval/review based on the requirements of your country orinstitution) were obtained?Answer: [NA]Justification: The paper does not involve research with human subjects.Guidelines:",
  "The answer NA means that the paper does not involve crowdsourcing nor research withhuman subjects": "Depending on the country in which research is conducted, IRB approval (or equivalent)may be required for any human subjects research. If you obtained IRB approval, youshould clearly state this in the paper. We recognize that the procedures for this may vary significantly between institutionsand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and theguidelines for their institution."
}