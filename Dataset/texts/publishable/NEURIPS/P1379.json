{
  "Abstract": "While novel view synthesis for dynamic scenes has made significant progress,capturing skeleton models of objects and re-posing them remains a challenging task.To tackle this problem, in this paper, we propose a novel approach to automaticallydiscover the associated skeleton model for dynamic objects from videos withoutthe need for object-specific templates. Our approach utilizes 3D Gaussian Splattingand superpoints to reconstruct dynamic objects. Treating superpoints as rigid parts,we can discover the underlying skeleton model through intuitive cues and optimizeit using the kinematic model. Besides, an adaptive control strategy is applied toavoid the emergence of redundant superpoints. Extensive experiments demonstratethe effectiveness and efficiency of our method in obtaining re-posable 3D objects.Not only can our approach achieve excellent visual fidelity, but it also allows forthe real-time rendering of high-resolution images. Please visit our project page formore results:",
  "Introduction": "Novel view synthesis for 3D scenes is important for many domains including virtual/augmented/mixedreality, game or movie productions. In recent years, Neural Radiance Fields (NeRF) havewitnessed significant advances in both static and dynamic scenes. Among them, 3D GaussianSplatting (3D-GS) proposed a novel point-based representation, and is capable of real-timerendering while ensuring the quality of generated images, bringing new insights to more complextask scenarios. Although visually compelling results and fast rendering speed have been achieved in reconstructing adynamic scene, current methods mainly focus on replaying the motion in the video, which means itjust renders novel view images within the given time range, making it hard to explicitly repose orcontrol the movement of individual objects in the scene. For some specific categories such as thehuman body or human head, one main approach is to leverage the category-specific prior knowledgesuch as templates to support the manipulation of reconstructed objects. However, it is hard for thesemethods to generalize to large-scale in-the-wild scenes or human-made articulated objects. Some template-free methods attempt to address these challenges by building reposable modelsfrom videos. Watch-It-Move (WIM) leverages ellipsoids, an explicit representation, to coarselymodel 3D objects, and then estimate the residual by a neural network. The underlying intuition isthat one or more ellipsoids can represent a functional part. By observing the motion of parts frommulti-view videos, WIM can learn both the appearance and structure of articulated objects. However,the reconstruction results of WIM are of low visual quality and the training and rendering speed isslow. Apart from WIM, Articulated Point NeRF (AP-NeRF) samples feature point cloud from apre-trained dynamic NeRF model (TiNeuVox ) and initializes the skeleton tree using the medialaxis transform algorithm. By combining linear blend skinning (LBS) and point-based rendering ,",
  "arXiv:2412.05570v1 [cs.CV] 7 Dec 2024": "AP-NeRF jointly optimizes dynamic NeRF and skeletal model from videos. Compared to WIM,AP-NeRF achieves higher visual fidelity while significantly reducing the training time. However,AP-NeRF cannot achieve real-time rendering, which is still far from practical application. In this paper, we target class-agnostic novel view synthesis of reposable models without the need fora template or pose annotations, while achieving real-time rendering. To enable fast rendering speed,we opt to represent the 3D object as 3D Gaussian Splatting. To be specific, we first reconstruct the3D dynamic model using 3D Gaussians and superpoints, where each superpoint binds Gaussians withsimilar motions together. These superpoints will later be treated as the parts of an object. Afterward, askeleton model is discovered leveraging some intuitive cues under the guidance of superpoint motionsfrom the video. Finally, we jointly optimize the skeleton model and pose parameters to match themotions of the training videos. During the optimization process of object reconstruction, we willinevitably generate a lot of redundant superpoints to fit the complex motion. To simplify the skeletonmodel and avoid overfitting, we employ an adaptive control strategy and regularization losses toreduce the number of superpoints. Our contributions can be summarized as follows: We propose a novel method based on 3D Gaussians and superpoints for modeling appearance,skeleton model, and motion of articulated dynamic objects from videos. Our approach canautomatically discover the skeleton model without any category-specific prior knowledge.",
  "Static and Dynamic Neural Radiance Fields": "In recent years, we have witnessed significant progress in the field of novel view synthesis empoweredby Neural Radiance Fields. While vanilla NeRF manages to synthesize photorealistic imagesfor any viewpoint using MLPs, subsequent works have explored various representations such as 4Dtensors , hash encodings , or other well-designed data structures to improve renderingquality and speed. More recently, a novel framework 3D Gaussian Splatting has receivedwidespread attention for its ability to synthesize high-fidelity images for complex scenes in real-time. Meanwhile, many research works challenge the hypothesis of a static scene in NeRFs and attempt tosynthesize novel-view images of a dynamic scene at an arbitrary time from a 2D video, which is amore challenging task since the correspondence between different frames is non-trivial. One line ofresearch works directly represents the dynamic scene with an additional time dimension or atime-dependent interpolation in a latent space. Another line of work represents the dynamicscene as a static canonical 3D scene along with its deformation fields. While one main bottleneck ofsynthesizing a dynamic scene is speed, some works propose to extend 3D Gaussian Splattinginto 4D to mitigate the problem. Though being able to recover a high-fidelity scene, this methodcannot directly support editing and reposing objects within it. In this work, we leverage 3D GaussianSplatting as the representation for faster rendering speed.",
  "Object Reposing": "Its impractical to directly repose the deformation fields of dynamic NeRFs due to the complexityof high-dimension. Therefore, utilizing parametric templates based on object priors to representdeformation is adopted in many research works. The classes of parametric templates range fromhuman faces , and bodies to non-human objects like animals . With the help ofskeleton-based LBS and 3D or 2D annotations, these parametric templates are capable of representingarticulate human heads and bodies . Though these template-based reposing methodscan synthesize high-fidelity images, they are restricted to certain object classes and mainly deal withrigid motions, not to mention the time-consuming process of annotations. To alleviate the excessive reliance on domain-specific skeletal models, methods based on retrievalfrom database or adaptation from a generic graph are adopted. However, these methodsare still of relatively low flexibility and diversity. Another line of work attempts to learn a more",
  "Discovery": ": The pipeline of proposed approach. Our approach follows a two-stage training strategy.In the first stage (i.e. dynamic stage), we learn the 3D Gaussians and superpoints to reconstruct theappearance. Each superpoint is associated with a rigid part, and the adaptive control strategy is usedto control the count. After finishing the training of dynamic stage, we can discover the skeletonmodel based on superpoints. After we finish the second stage (i.e., kinematic stage), we can obtain anarticulated model based on the kinematic model. general template-free object representation by 3D shape recovery . WIM proposes tojointly learn a surface representation and LBS model for articulation without any supervision or priorknowledge of the structure. However, the reposing images are of low visual quality and the requiredtraining time is considerably long. AP-NeRF achieves a much faster training speed by leveraginga point-based NeRF representation, but cannot support real-time rendering as well.",
  "Methods": "Our goal is to reconstruct a reposable articulated object with real-time rendering speed from videos.The pipeline of proposed method is illustrated in . We represent the appearance of the articulatedobject as 3D Gaussians in the canonical space while aggregating 3D Gaussians with similar motioninto superpoints, which can be treated as rigid parts. It is noteworthy that we apply a time-variant6 DoF transformation matrix to model the motion of the object. Based on these superpoints, weleverage several intuitive observations to guide the discovery of the skeleton model, which includesboth joints and skeletons. Since the observations hold for most objects, our method does not require acategory-specific template or pose annotations. To reduce the redundant superpoint, we propose anadaptive control strategy to densify, prune, and merge superpoints during the training process.",
  "Preliminaries: 3D Gaussian Splatting": "3D Gaussian Splatting (3D-GS) use a set of 3D Gaussians to represent a 3D scene. EachGaussian Gi: {i, qi, si, i, hi} is associated with a position i, a rotation matrix Ri which isparameterized by a quaternion qi, a scaling matrix Si which is parameterized by a 3D vector si,opacity i and spherical harmonics (SH) coefficients hi. Therefore, the anisotropic 3D covariancematrix of Gaussian Gi is defined as i = RiSiSi Ri , which is positive semi-definite matrix. To render images, 3D-GS employs EWA Splatting algorithm to project a 3D Gaussian withcenter i and covariance i to 2D image space, and the projection can be approximated as a 2DGaussian with center i and covariance i. Let Q, K be the viewing transformation and projectionmatrix, i and i are computed as",
  "where RGB color ci is evaluated by SH with coefficients hi and view direction": "Given multi-view images with known camera poses, 3D-GS optimizes a static 3D scene by minimizingthe following loss function:Lrgb = (1 )L1(I, Igt) + LSSIM(I, Igt)(4)where = 0.2, and Igt is the ground truth. Besides, 3D-GS is initialized from from random pointcloud or SfM sparse point cloud, and an adaptive density adjustment strategy is applied to control thenumber of Gaussians.",
  "Discovery of Skeleton Model": "Treating each superpoint as a rigid part of the articulated object, we can discover the skeleton model(i.e., the 3D joints and the connection between joints) based on the motion of superpoints. Similar toWIM , there are some observations to help us discover the underlying skeleton. First, if there is ajoint between two superpoints pa and pb, the position of pa is more likely close to the position ofpb. Second, when the relative pose between two parts changes, the joint between the two parts isrelatively unchanged. Lastly, two connected parts can be merged if they maintain the same relativepose throughout the whole sequence. Let jab R3 be the position of underlying joint between superpoints pa and pb, and Rtab SO(3)is the relative rotation matrix between two superpoints at time t. The relative transform between paand pb can be either represented by the global transform or the rotation of the joint, that is:Rtrttr01",
  "where = 0.1 is the momentum, and is the training iteration": "Similar to WIM , we discover the structure of joints based on the distance dab by the minimumspanning tree algorithm. We first select all pairs (a, b) if superpoint pb is K-nearest neighborhoodfor superpoint pa and sort the list of dab for those pairs in ascending order. We initialize as anempty set. We pick pair (a, b) from the lowest distance to the highest distance, and add this pair to while there is no path between a and b. After finishing the procedure, we obtain the final objectstructure , which is an acyclic graph, i.e., a tree. We choose the node whose length of the longestpath from itself to any other node is the shortest as the root node. If there is more than one candidatenode, we randomly choose one as the root.",
  ": (jk, t) Rk(11)": "Then we forward-warp the superpoint pj from the canonical space to the observation space oftimestamp t via the kinematic model. The local transformation matrix Ttk SE(3) of each joint k isdefined by a rotation Rtk around its parent joint jk. Consequently, the final transformation of eachsuperpoint pj can be computed as a linear combination of bone transformation:",
  "Adaptive Control of Superpoints": "We use the farthest point sampling algorithm to sample M Gaussians to initialize the superpoints.Simply making superpoints learnable is not enough to model complex motion patterns. Moreimportantly, we wish to simplify the skeleton after training to ease pose editing by reducing thenumber of superpoints. Following 3D-GS and SC-GS , we develop an adaptive controlstrategy to prune, densify, and merge superpoints.",
  "Prune: To determine whether a superpoint pj should be pruned, we calculate its overall impactWj =": "i Nj wij, where Nj = {i | j Ni} is the set of Gaussians whose K nearest neighborsinclude superpoints pj. When Wj < prune, we prune this superpoint as it is of little contribution tothe motion of 3D Gaussians. Densify: Two aspects determining whether a superpoint should be split into two superpoints. Onone hand, we clone a superpoint when its impact Wj is greater than a threshold clone, indicatingthere is a great amount of Gaussians associated with this superpoint, and cloning such superpointshelps model fine motion. On the other hand, we calculate the weighted Gaussians gradient norm ofsuperpoint j as:",
  "Datasets and Evaluation Metrics": "To ensure fair comparison with previous work, we choose the same datasets and configurations asAP-NeRF. Specifically, we choose three multi-view video datasets. First, the D-NeRF datasetis a sparse multi-view synthesis dataset, which includes 5 humanoids, 2 other articulated objects, anda multi-component scene. Each scene contains 50-200 frames. We choose 6 of 8 scenes 1 The seconddataset, Robots ,contains 7 topologically varied robots with multi-view synthetic video. We use 18views for training and 2 views for evaluation. The third dataset, ZJU-MoCap , is commonly used",
  "Implementation Details": "We implement our framework using PyTorch. The number of superpoints is initialized as 512. Forboth deformable field and , we adopt the architecture of NeRF, i.e., 8-layers MLP whereeach layer employs 256-dimensional hidden fully connected layer and ReLU activation function. Wealso employ positional encoding for the input coordinates and time. For optimization, we employthe Adam optimizer and use the different learning rate decay schedules for each component: thelearning rate about 3D Gaussians is the same as 3D-GS, while the learning rate of other componentsundergoes exponential decay, ranging from 1e-3 to 1e-5. We conducted all experiments on a singleNVIDIA Tesla V100 (32GB). More implementation details are shown in Appendix A.",
  "Baselines": "We mainly compare our method to state-of-the-art template-free articulated methods for view synthe-sis, i.e. WIM and AP-NeRF . Besides, we also compare our method with NeRF-based and3D-GS-based non-articulated methods. D-NeRF extends NeRF to dynamic scenes by warpinga static NeRF. TiNeuVox improves the visual quality and training speed by using voxel grids.",
  ": Qualitative comparison for the ZJU-MoCap dataset": "HexPlane and K-Planes accelerate NeRF by decomposing the space-time volume intoseveral planes. Similar to D-NeRF, Deformable-3D-GS extends static 3D-GS to the temporaldomain. 4D-GS accelerate Deformable-3D-GS by decomposing neural voxel encoding algorithminspired by HexPlane. Similar to ours, SP-GS and SC-GS employ the superpoints/controlpoints to reconstruct dynamic scenes. However, SP-GS and SC-GS can not extract skeleton fromreconstructed model.",
  "Comparisons on Synthetic Dataset": "In our experiments, we benchmarked our method against several baselines using the D-NeRF datasetand Robots datasets. The quantitative comparison results, presented in Tab. 1, demonstrate the superiorperformance of our approach in terms of both rendering speed and visual quality. Specifically, ourmethod significantly outperforms WIM and AP-NeRF not only in visual quality but also in renderingspeed. Compared to 3D-GS based dynamic scenes reconstruction models, our method not only havesimilar performance, but also discover the skeleton and can repose the object to generate a novel pose.Specifically, the rendering quality of ours is higher than 4D-GS and SP-GS , and lower thanD-3D-GS and SC-GS . Our method also can achieve real-time rendering (>100 FPS), whichis near to the rendering speed of SC-GS . provides the qualitative comparisons of D-NeRFdataset, which demonstrates the advantages of our method over related methods. We also provideresults for Robots datasets, quantitatively in Tab. 2 and qualitatively in . It is also clear that ourapproach is capable of producing high-fidelity novel views with real-time rendering speed. Per-sceneresults are shown in Appendix B.",
  "Comparison on Real-world Dataset": "In Tab. 3 and , we compare our method to WIM and AP-NeRF in the ZJU-MoCap dataset. Weobserve that both methods can recover the 3D shape and skeleton models. However, imperfectionsin the camera calibrations (see Supplement F of ), lead to lower visual quality in our resultscompared to WIM and AP-NeRF. With respect to rendering speed, our approach achieves up to198.23 FPS. In stark contrast, the rendering speed of WIM and AP-NeRF is extremely slow.",
  "Ablation Study": "We use superpoints to model the parts and motion of the object. The adaptive control of superpoints isthe key to reducing the number of superpoints. (a), (b) and (c) intuitively illustrate the impactof this control strategy. Without the control strategy, the distribution of superpoints becomes uneven,with sparse representation in the arm region, which negatively impacts motion modeling. The mergeprocess in the control strategy significantly reduces the number of superpoints (97 vs 608), while thesuperpoints are more distributed in the motion area. Besides, as illustrated in (d), Larap (inAppendix A) also plays an important role in controlling the density of superpoints. See Appendix Cfor more ablation studies.",
  "Limitations": "We have demonstrated that our approach can achieve real-time rendering, state-of-the-art visualquality, and straightforward reposing capability by skeleton and kinematic models. However, thereare some limitations to our approach. Firstly, similar to WIM and AP-NeRF, the learned skeletonmodel of our approach is restricted to the kinematic motion space exhibited in the input video.Therefore, the skeleton model may have significant differences from the actual one, and extrapolationto generate arbitrary unseen poses may cause errors. Secondly, our approach has similar limitationsas other 3D-GS based methods for dynamic scenes. Specifically, the datasets with inaccurate cameraposes will lead to reconstruction failures, and large motion or long-term sequences can also resultin failures. Lastly, the paper focuses on building the kinematic model for one articulated object.Exporting build kinematic models for multi-component objects or complex scenes that containmultiple objects remains an opportunity for future research. Additionally, extending this approach tomotion capture is an interesting research direction.",
  "Conclusion": "We have developed a new method for real-time rendering of articulated models for high-quality novelview synthesis. Without any template or annotations, our approach can reconstruct a kinematic modelfrom multi-view videos. With state-of-the-art visual quality and real-time rendering speed, our workrepresents a significant step towards the development of low-cost animatable 3D objects for use inmovies, games, and education.",
  "Acknowledgements": "This work is supported by the Sichuan Science and Technology Program (2023YFSY0008), ChinaTower-Peking University Joint Laboratory of Intelligent Society and Space Governance, NationalNatural Science Foundation of China (61632003, 61375022, 61403005), Grant SCITLAB-30001of Intelligent Terminal Key Laboratory of SiChuan Province, Beijing Advanced Innovation Centerfor Intelligent Robots and Systems (2018IRS11), and PEKSenseTime Joint Laboratory of MachineVision.",
  "In this stage, we conducted training for a total of 40k iterations. Similar to SC-GS , our trainingscheme is as follows:": "1. In 02k iterations, we fix deformable field .2. In 2k10k iterations, we train deformable field which using the position of Gaussiansrather than superpoints as inputs. At 7500 iteration, we sample M Gaussians by usingfutherest sampling algorithm. At 10k iteration, we initialize superpoints by traind MGaussians, and re-initialize the Gaussians in canonical space.",
  ". In 10 13k iterations, we fix deformable field .4. In 13k 40k iterations, all parameters are optimized": "During the first 20k iterations, We do not discover joints and skeleton, i.e., 1 = 0. During 20k 40k iterations, we update the structure of skeleton every 100 iterations. Note that we stopped thegradient between joints and other parts so that the learning of joints has no effect on the learning ofGaussians, superpoints and deformation field. During training, we adopt the same adaptive control strategy for Gaussians as 3D-GS In details, initerations 17500 and 10k 35k, we densify and prune Gaussians every 100 iterations, while thedensify grad threshold is set to 0.0002 and the opacity threshold for prune is set to 0.005. We resetthe opacity of Gaussians every 3k steps. We densify and prune superpoints every 1000 steps between iterations 20k and 30k, and the hyper-parameter grad = 0.0002 and prune = 0.001. We merge superpoints every 1000 steps betweeniterations 30k and 40k, while threshold merge = 0.0005.",
  "A.2Skeleton Discovery Stage": "In this stage, we aim to initialize deformable field the using the learned deformable field indynamic stage. Firstly, we fix skeleton structure , and initialize the joints jk, k = 1, . . . , M 1by underlying joints jab according to . We then cache the outputs of deformable field for alltimestamps in the training dataset. Next, we use cached motions of superpoints to optimize theparameters of deformable field , the positon of the joints jk by employing Adam optimizer andfollowing loss functions:",
  "i=15(ti ti22 + log(Rti1 Rti)) + 6(Rtip + t) ( Rtip + t) (20)": "where ti and Rti is translations and rotations calculated by deformable field , while ti and Rti istranslations and rotations calculated by deformable field and kinematic model. 5 = 1 controls theweights of the transform matrix difference, and 6 = 0.1 adjusts the weights of relative offset of thesuperpoints position at timestamp t.",
  "(f)": ": Compare rendered images between canonical space and the warp space of timestamp 0. (a)canonical space in Dynamic stage, (b) at time 0 in Dynamic stage, (c) canonical space in Kinematicstage, (d) at time 0 in Kinematic stage, (e)LBS at time 0 in Kinematic stage (f)ground truth at time 0."
}