{
  "Abstract": "The automatic generation of hints by Large Language Models (LLMs) within IntelligentTutoring Systems (ITSs) has shown potential to enhance student learning. However, gener-ating pedagogically sound hints that address student misconceptions and adhere to specificeducational objectives remains challenging. This work explores using LLMs (GPT-4o andLlama-3-8B-instruct) as teachers to generate effective hints for students simulated throughLLMs (GPT-3.5-turbo, Llama-3-8B-Instruct, or Mistral-7B-instruct-v0.3) tackling mathexercises designed for human high-school students, and designed using cognitive scienceprinciples.We present here the study of several dimensions: 1) identifying error pat-terns made by simulated students on secondary-level math exercises; 2) developing variousprompts for GPT-4o as a teacher and evaluating their effectiveness in generating hints thatenable simulated students to self-correct; and 3) testing the best-performing prompts, basedon their ability to produce relevant hints and facilitate error correction, with Llama-3-8B-Instruct as the teacher, allowing for a performance comparison with GPT-4o. The resultsshow that model errors increase with higher temperature settings. Notably, when hintsare generated by GPT-4o, the most effective prompts include prompts tailored to specificerrors as well as prompts providing general hints based on common mathematical errors.Interestingly, Llama-3-8B-Instruct as a teacher showed better overall performance thanGPT-4o. Also the problem-solving and response revision capabilities of the LLMs as stu-dents, particularly GPT-3.5-turbo, improved significantly after receiving hints, especiallyat lower temperature settings. However, models like Mistral-7B-Instruct demonstrated adecline in performance as the temperature increased. This study advances our understand-ing of the potential and limitations of LLMs in educational contexts, towards integratingthese models into pedagogically grounded.Keywords: Large Language Models, educational technologies, mathematics, hint genera-tion, question generation, pedagogical stance.",
  "Tonga Clement Oudeyer": "The exercise and question: {exercise}.Instruction: {instruct}The correct answer to the exercise: {answer}The students reasoning: {gpt_reasoning}The students answer: {gpt_response}Guide according to the cognitive approach: {demarche_cog}JSON output format:{{\"hint\": \"Place the hint here without numbering it...\"}}Ensure the generated output does not contain escape characters such as line breaks(\\\\n) or slashes (\\\\).Please provide a clean and readable output. I insist on this. Do not make anyformatting errors.Follow the output format. I emphasize that the hint should not be numbered andmust be in the form of a question. }",
  ". Advances in ITS in Education through LLMs": "Thanks to their ability to provide students with a personalized and effective learning expe-rience, ITSs have gained popularity in the field of education (Winkler and Sollner, 2018).According to (Feng et al., 2021), these systems can be classified into four main categories.Dialogue-based tutoring ITS, such as AutoTutor (Graesser et al., 2004) and Beetle(Dzikovska et al., 2010) leverage natural language to identify students misconceptions andrespond to their prompts. Constraint-based scaffolding models (Mitrovic et al., 2013),exemplified by KERMIT (Suraweera and Mitrovic, 2002) use constraints predefined byhuman experts to respond to student queries. Model tracing (Liu et al., 2022; Sonkaret al., 2020) monitors students knowledge states to capture their problem-solving skills.Bayesian network modeling (Corbett and Anderson, 1995) extends model tracing byusing Bayesian networks.Furthermore, recent advances in generative artificial intelligence, particularly with theemergence of LLMs such as GPT-4 (Bubeck et al., 2023) from OpenAI and more compactmodels like Llama (Touvron et al., 2023b) from Meta, have demonstrated their poten-",
  ". LLMs for Feedback and Hint Generation": "Before the recent advances in generative artificial intelligence, one of the commonly usedapproaches for generating feedback or hints in the educational field relied on features de-signed to detect errors in students responses. A rule-based system was then employed toprovide relevant comments or hints (Botelho et al., 2023; Kochmar et al., 2020; Lan et al.,2015; Razzaq et al., 2020; Singh et al.; Song et al., 2021). This approach was popular dueto its interpretability and reliability. However, it required significant human effort to adaptto new types of questions. With the advent of LLMs, a more general approach for gen-erating feedback or hints involves using these advanced models either through prompting(Al-Hossami et al., 2023; McNichols et al., 2024; Nguyen et al., 2023; Steiss et al., 2023;Wang et al., 2024) or fine-tuning (Qinjin Jia et al., 2022). Several studies have been con-ducted in this area, particularly in the context of programming education. For example,(Roest et al., 2023) explored how LLMs can contribute to programming education by pro-viding students with automated hints for the next steps.They found that most of thefeedback messages generated by LLMs describe a specific step to follow and are personal-ized based on the students code and approach. However, these hints can sometimes containmisleading information and lack sufficient detail when students are nearing the end of anexercise.Similarly, (Phung et al., 2023) studied the role of generative AI models in providinghuman tutor-like hints to help students resolve errors in their faulty programs. However, when prompting pre-trained LLMs, it is crucial that these models exhibit good behaviorand a clear understanding of educational objectives. For example, despite these advances,a major challenge these models face is their limited accuracy in handling mathematicalcalculations. GPT-4, for instance, showed only 59% accuracy on basic tasks like three-digit multiplication (Dziri et al., 2023). To enhance the mathematical capabilities of LLMs,several methods have been developed, such as the evol-instruct framework of WizardMath(Luo et al., 2023), combining LLMs with symbolic solvers (He-Yueya et al., 2023), or theintroduction of code soliloquies by (Sonkar et al., 2023a), which allow for precise invo-cation of Python calculations whenever a students response requires it. Nevertheless, thehints or feedback generated by the models should be clear, simple, encouraging, positivein tone, and relevant to the learning objectives (Jangra et al., 2024) as well as address theindividual needs of learners (SUAIB, 2019; edu). Many research efforts concentrate on thegeneration of feedback, which is different from hint generation. These studies are predom-inantly oriented towards the field of programming, which is distinct from the focus of ourpaper. Additionally, works in mathematics on feedback or hint generation often benefit frompre-existing datasets, consisting of feedback or hint generated by human experts (Scarlatoset al., 2024). This makes the task of generating feedback somewhat more approachable.In contrast, our study is the first to attempt generating hints in mathematics using LLMswithout the advantage of a pre-existing dataset created by human experts. The absence ofsuch reference data makes our approach innovative and underscores the importance of thisresearch, which aims to address this gap in future studies.",
  ". Simulation of Human Behaviors by LLMs": "LLMs have the ability to simulate human behaviors, a capability that has shown promis-ing applications in the field of education. For instance, (Markel et al., 2023) introducedGPTeach, an interactive teacher training tool based on a chat system. This tool allowsnovice teachers to practice with simulated students, using GPT to take a prompt andgenerate a response to it (Brown et al., 2020). In our study, inspired by the work pre-sented in (Markel et al., 2023), we simulate both teachers and students. Moreover, otherstudies have suggested that LLMs can be prompted to replicate desired model behaviors(Jiang et al., 2021, 2022; Liu et al., 2021). Recently, a study (Argyle et al., 2023) demon-strated that with specific prompting techniques, LLMs can successfully simulate humansub-populations. This work is supported by (Arora et al., 2022), who described variouseffective prompting techniques. In contrast to these approaches, (Park et al., 2023, 2022)used specialized GPT prompting techniques to simulate not just one person, but an entireonline community composed of simulated individuals, each with a unique personality.",
  ". Prompt design for question hint generation": "To obtain high-quality hints, various approaches were considered for designing prompts andcompare them. This led to the development of specific pipelines allowing to test and identifythe best prompts for generating relevant and useful hints. A detailed picture of the pipelinecan be found in Appendix 4(a). This pipeline is divided into two main stages. The firststage is executed first, followed by the second stage using the datasets obtained from thefirst stage.The first stage implements the generation and classification of the student answers. Theobjective of this step is to create, for each exercise, a diverse dataset of incorrect answers,including different incorrect reasoning or solutions.",
  ". Resolution by the student model: For each exercise, the student model solves it usingthe prompt described in Appendix A.3.1, producing reasoning and a response": "2. Verification of the response by GPT-4o: The response from the student model iscompared to the correct solution using the prompt in Appendix A.3.4.These two steps are repeated a predetermined number of times num simulations ,where num simulations represents the number of attempts to solve the same exerciseby the student model. 3. Error classification: After manually evaluating the incorrect responses of simulatedstudents, we identified the following common types of errors: misunderstanding, inter-pretation, calculation, simplification, algebraic errors, partial answers, term grouping,and incorrect substitution. To automate the determination of these error types, weemployed GPT-4o using few-shot prompting (Sahoo et al., 2024), as detailed in theprompt described in Appendix A.3.4, which includes examples for each common errortype. GPT-4o was then used to categorize error types or groups of errors (multipleerrors present in a simulated students incorrect response), allowing for the creationof a diverse error dataset with various reasoning mistakes or incorrect solutions, usingthe prompt described in Appendix A.3.5. At the end, a dataset of exercise solutions is produced that includes all the student modelsresponses, whether correct or incorrect, corresponding to the number of simulations con-ducted. We also have an error classification dataset containing various incorrect responsesand reasoning by error type.The second stage implements the generation of hints and revision of various incorrectresponses. Only one incorrect answer per error type is retained from the error classificationdataset obtained in stage 1 to form the one used in this step. 1. Hint generation by the teacher Model: For an incorrect response in the dataset, theteacher model generates appropriate hints based on the type of prompt used for hintgeneration. These prompts are described in Appendix A.4.",
  ". Verification of the revised response by GPT-4o: The revised response is checked againby GPT-4o to ensure it is now correct. This is done by using the prompt described": "in Appendix A.3.3.These steps are repeated for each incorrect response in the dataset, as well as for eachtype of hint generation prompt. The goal is to find the best prompts, i.e., those thatallow the best correction of errors by the student models. The models simulating the students are powered by GPT-3.5-turbo, Llama-3-8B-Instruct,and Mistral-7B-Instruct-v0.3, while those playing the role of the teachers are either GPT-4oor Llama-3-8B-Instruct. GPT-4o was primarily used as the teacher and for intermediatesteps in the pipeline, except for the student model stages, because it is currently amongthe most powerful model available. As mentioned earlier, we defined several prompts. Theresolution of exercises and the revision of answers by the student models were done throughzero-shot prompting (Sahoo et al., 2024), meaning without providing examples. We choseto use zero-shot prompting because, when using zero-shot Chain of Thought (CoT) (Jinet al., 2024), the student model made fewer errors, which was not relevant to our study.Indeed, we were looking for a model that produced a balanced mix of erroneous and correctresults The phases of answer verification and error classification are also performed by theGPT-4o model in zero-shot prompting. For hint generation, the prompts were also writtenin zero-shot prompting. Once we selected the best prompts, a second pipeline was createdto evaluate these prompts.",
  ". Pipeline for evaluating the best prompts": "To evaluate these prompts, we implemented the pipeline illustrated in (b). Thispipeline describes a process for evaluating the best prompts. Initially, responses are col-lected from a dataset of exercise resolutions generated by a student model. If a responseis incorrect, a teacher model provides a hint using the best-selected prompt. The studentmodel then revises its response based on this hint. The revised response is subsequently sub-mitted to GPT-4o for re-verification, and the outcome is recorded. This process is repeatedfor all responses in the dataset. By the end of the pipeline, we have a comprehensive datasetcontaining all responses before and after hint generation, along with their correspondingevaluations.",
  ". Experiments settings": "For these experiments, we worked with the mathematics exercises from MIA Seconde ed-ucational software. MIA Seconde is an educational tool developed by EvidenceB, whichoffers remediation exercises in French and mathematics for students in general, technolog-ical, and vocational high school classes. These exercises were initially developed throughcollaboration with researchers in cognitive science and neuroscience, drawing on insightsinto how the students brain functions and theories about human mathematical cognition.The theory underlying the MIA Seconde exercises is documented in guides called pedagogi-cal summaries. A pedagogical summary is an official document from EvidenceB that servesas a reference for the design of these exercises. These summaries are organized into differentmodules, objectives, and activities, each focusing on a specific knowledge or skill.Modules correspond to an overarching skill developed in the exercises. It generally refersto a sub-discipline of French or mathematics, for example, quantities and measurements.",
  ". What types of errors student models make when solving math exercises,and how does it depend on the temperature parameter?": "The types of errors considered are: Comprehension Error: the student does not understandthe problem or instructions clearly; Partial Response: the student provides only part of theanswer and fails to complete it correctly; Term Grouping Error: the student incorrectlycombines or groups terms in an expression; Simplification Error: the student simplifies anexpression incorrectly; Calculation Error: the student performs mathematical operationsincorrectly; Incorrect Substitution Error: the student substitutes the wrong value in anexpression or equation; Interpretation Error: the student misinterprets the instructionsor data; Algebraic Error: the student makes mistakes in algebraic manipulations. Theseerrors were identified through manual evaluation and common student mistakes in math,then used in few-shot prompting with GPT-4o for evaluating student model responses, asmentioned in the error classification phase of the pipeline (see 3.1). We manually verifiedthe phases of answer checking, error classification and error type determination to ensureGPT-4os was not making any mistakes. In most cases, the results were correct (around98% of the time).To analyze the types of errors made by the student models when solving the four exercisesunder standard settings (default temperature), the first two steps of the initial stage inthe pipeline for determining the best prompt for hint generation (see (a)) wereexecuted 40 times for each exercise. It is equally interesting to observe how the resultsvary when the temperature value is adjusted, as temperature is a parameter that controlsthe creativity and diversity of the responses generated by the model.To explore this,the process was repeated for each temperature value (0, 0.2, 0.5, 0.8, 1) across all models.Studying the effects of different temperature values would also allow us to determine whetherthe temperature parameter can influence the student models ability to incorporate a hint during revision process.The verification, evaluation, error detection, and classificationsteps were performed by GPT-4o, as mentioned earlier, with a temperature value set to 0to ensure accurate results. It was observed that at higher temperatures (> 0.2), these stepswere not reliable.Studying the effects of different temperature values would also allow us to determinewhether the temperature parameter can influence the student models ability to incorporatea hint during the revision process. : A checkmark () indicates the presence of a specific type of error at a giventemperature, while a dash () indicates its absence. The table shows errors madeby GPT-3.5-turbo (G3.5), Llama-3-8B-Instruct (L8B), and Mistral-7B-Instruct-v0.3 (M7B) in exercise 1 (module 1). As temperature increases, the number andvariety of errors tend to rise, varying across models.",
  "Simplification Error, Comprehension ErrorComprehension Error, Calculation Error, Interpretation Error": "We observed that the types of errors made by the student models vary significantlydepending on the exercise, the model used, and the temperature value applied. Indeed, thehigher the temperature value, the more likely the models are to make errors, as shown in. This Table summarizes , offering a concise view of the types of errorsencountered at each temperature and for each model in exercise 1 (module 1).In thisexercise, Llama made 5 errors, Mistral made 5, and GPT made 9 types of errors. So, Llama-3 and Mistral models tend to make fewer errors types than the GPT-3.5-turbo model. It isalso worth mentioning that the Mistral model exhibits a relatively high number of decodingerrors compared to the other two. For more details, you can refer to the Appendix A.5.3,where we present the evolution of error types based on temperature for other exercises.",
  ". What type of prompt is most effective for generating hints with GPT-4o?": "In order to select the best prompts for generating hints, we defined several prompts, whichcan be grouped into two categories.First, prompts based on the types of errors made by the student modelaim to correct a key aspect of the students response. They are based on reasoning, thestudents response, the instruction, the cognitive approach, the correct answer, and theexercise, and incorporate these elements into the context.They are labelled as follow:prompt hint reason is based on the students reasoning; prompt hint method is based onthe method used by the student; prompt hint concpt is based on the application of concepts;",
  "prompt baseline two0.201.000.730.480.710.000.330.470.250.330.900.33": "The different prompts enable the learning models to correct their errors, as shown in, the lower the mean revision error, the more effective the prompt is.Indeed,regardless of the temperature used, the cues generated by GPT-4o through these promptsallow the LLM models to correct their responses, sometimes entirely. For example, in thecase of the Mistral model at a temperature of 0.5, the mean revision error rate is 0 for thebaseline-type prompts, which means that all errors where correctly revised. Similarly, for GPT-3.5-turbo, the BaselineTwo prompt has a mean revision error of 0, indicating thatall the cues generated by these prompts enabled GPT-3.5-turbo or Mistral to correct theirinitial response. is a condense version of the in Appendix A.5.2 wheremore details about these results are described.We consider the best prompt to be the one that enables the student model to correctitself the most times over the 10 repetitions. This prompt is identified by selecting theone with the lowest mean revision error rate for each temperature across all exercises.The top prompts are those that appear most frequently as the best. The best specializedprompt was found to be the one based on calculation errors, while the best baselineprompt is BaselineTwo. These two prompts were therefore used in the continuation ofour experiments.",
  ".What is the influence of the temperature parameter on the performanceof the student models in solving exercises and revising answers?": "We worked with the best prompts from both categories, specifically the calculation-basedprompt for the specialized prompts and BaselineTwo for the baseline-type prompts. Tostudy how temperature could impact the resolution and revision by the student models, weused the validation pipeline shown in (b). Note that when revising the studentsresponse, the same temperature value used during the resolution is applied. Metric such asaccuracy was used to quantify the performance of these models using the best prompts. Itwas computed as the number of correct responses out of the 40 repetitions divided by thenumber of responses (number of correct+number of incorrect responses).Figures 3 and 2 do not show a clear direct link between the ability of student modelsto solve exercises and revise their answers when GPT-4o or Llama-3-8B-instruct are usedas teachers. However, for the GPT student model, we observe that accuracy during bothsolving and revision decreases when the temperature is set to 1, which is not always the casefor the other student models. Conversely, for lower temperatures (e.g., 0 or 0.2), accuracyincreases.",
  ". How does the accuracy of student model problem-solving evolve beforeand after providing hints when they are guided by Llama-3-8B-Instructcompared to GPT-4o?": "Since our goal is to use LLMs for hint generation, we were curious to see how a smallerlanguage model like Llama-3-8B-Instruct would perform in generating hints. Therefore, weused it as the teacher model in the pipeline shown in (b), utilizing the best prompts.Figures 3 and 2 show that whether the BaselineTwo prompt or the one based on cal-culation errors is used with GPT-4o or Llama-3-8B-instruct, the models manage to correctthemselves. A notable improvement is particularly observed for the GPT-3.5-turbo model.Indeed, the accuracy of this model increases significantly after receiving a hint, even if its ini-tial accuracy was low. In contrast, the other models show a more moderate increase. WhenGPT-4o is used as the teacher, the hints provided by the error-based prompt seem moreeffective in improving the student models performance than those from the BaselineTwoprompt. However, the opposite effect is observed with Llama-3-8B-instruct as the teacher.Comparing the two teacher models, the figures suggest that the overall performance is better",
  "with Llama-3-8B-instruct. However, although accuracy is higher with Llama-3-8B-instruct,it would be crucial to verify the quality and relevance of the generated hints": "For more detailed results by exercise, you can refer to section A.5.1. There, we showhow the accuracy evolves for each exercise. However, exercise 3 (module 7) is particularlychallenging for the models to solve. Only the GPT student model manages to correct itselfafter being given a hint, whether the teacher is GPT-4o or Llama-3-8B-instruct.",
  ". Discussion and Limitations": "Our work addresses a gap in hint generation research within the field of mathematics edu-cation. We demonstrated that language models could identify their own errors when actingas students, with error patterns varying based on the temperature setting. Higher temper-atures led to more diverse outputs but increased errors, while lower temperatures producedmore deterministic. This error detection was crucial for selecting effective prompts for gen-erating a synthetic hint dataset. We found that prompts focused on error correction andthe BaselineTwo prompt were most successful. This aligns with the known challenges lan-guage models face with calculations and reasoning tasks. Interestingly, our results differfrom a previous study (Renze and Guven, 2024) on Multiple-Choice Question Answering(MCQA), which found that temperature did not impact problem-solving abilities but af-fected text variability. In contrast, we observed no clear link between temperature andproblem-solving in our non-MCQA tasks, where we used only zero-shot approaches. GPT-3.5-turbo showed the most effective self-correction after receiving optimized hints,likely due to the prompts being tailored for base GPT models, leading to a significantcorrection gap compared to other student models. Mistral-7B-instruct-v3 and Llama-3-8B-instruct, however, already had high accuracy with the hints, making further improvementharder, though their additional corrections remain noteworthy. Notably, the Llama-3-8B-instruct model outperformed GPT-4o in accuracy when usingthe best prompts, challenging the assumption that larger models like GPT-4o are alwayssuperior. Future work should include a qualitative analysis of the generated hints in relationto pedagogical criteria and their relevance, as well as explore the potential of fine-tuningsmaller models, such as Llama-3-8B-instruct, for hint generation. This study has, however, several limitations. First, we only used four exercises fromdifferent modules, which is not sufficient for a comprehensive analysis, even though eachexercise was solved 40 times. Results may differ with other exercise variants within thesemodules. Additionally, the cost of the API limited the number of exercises we could analyze. The prompts for hint generation were optimized for GPT models, not for other models,so more tailored prompts might produce better results for non-GPT models. We also limitedour analysis to GPT-4o and GPT-3.5-turbo due to cost constraints. Error type classificationwas evaluated only with GPT-4o, without the involvement of human experts, though somehuman verification was done. Including expert evaluation would provide deeper insights.Finally, the lack of qualitative analysis of the generated hints is another limitation, as suchan analysis could offer valuable context and improve the overall assessment of hint quality.",
  "Q-Chat:MeetyournewAItutor|Quizlet.URL": "Erfan Al-Hossami, Razvan Bunescu, Ryan Teehan, Laurel Powell, Khyati Mahajan, andMohsen Dorodchi. Socratic Questioning of Novice Debuggers: A Benchmark Datasetand Preliminary Evaluations.In Ekaterina Kochmar, Jill Burstein, Andrea Horbach,Ronja Laarmann-Quante, Nitin Madnani, Anas Tack, Victoria Yaneva, Zheng Yuan,and Torsten Zesch, editors, Proceedings of the 18th Workshop on Innovative Use of NLPfor Building Educational Applications (BEA 2023), pages 709726, Toronto, Canada,July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.bea-1.57.URL Lisa P. Argyle, Ethan C. Busby, Nancy Fulda, Joshua Gubler, Christopher Rytting, andDavid Wingate. Out of One, Many: Using Language Models to Simulate Human Samples.Political Analysis, 31(3):337351, July 2023. ISSN 1047-1987, 1476-4989. doi: 10.1017/pan.2023.2. URL arXiv:2209.06899 [cs]. Simran Arora, Avanika Narayan, Mayee F. Chen, Laurel Orr, Neel Guha, Kush Bhatia,Ines Chami, Frederic Sala, and Christopher Re. Ask Me Anything: A simple strategy forprompting language models, November 2022. URL [cs]. Anthony Botelho, Sami Baral, John Erickson, Priyanka Benachamardi, and Neil Heffernan.Leveraging natural language processing to support automated assessment and feedbackfor student open responses in mathematics. Journal of Computer Assisted Learning, 39:n/an/a, February 2023. doi: 10.1111/jcal.12793.",
  "Mark Bray.The Shadow education system:private tutoring and its implications forplanners.January 1999.Journal Abbreviation: Publication Title:": "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, PrafullaDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, SandhiniAgarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, AdityaRamesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, MarkChen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, ChristopherBerner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. LanguageModels are Few-Shot Learners, July 2020. URL [cs]. Sebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz,Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, HamidPalangi, Marco Tulio Ribeiro, and Yi Zhang. Sparks of Artificial General Intelligence:Early experiments with GPT-4, April 2023. URL [cs]. Albert T. Corbett and John R. Anderson. Knowledge tracing: Modeling the acquisition ofprocedural knowledge. User Modelling and User-Adapted Interaction, 4(4):253278, 1995.ISSN 0924-1868, 1573-1391. doi: 10.1007/BF01099821. URL Myroslava O. Dzikovska, Johanna D. Moore, Natalie Steinhauser, Gwendolyn Campbell,Elaine Farrow, and Charles B. Callaway. Beetle II: A System for Tutoring and Compu-tational Linguistics Experimentation. In Sandra Kubler, editor, Proceedings of the ACL2010 System Demonstrations, pages 1318, Uppsala, Sweden, July 2010. Association forComputational Linguistics. URL Nouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine Li, Liwei Jiang, Bill Yuchen Lin,Peter West, Chandra Bhagavatula, Ronan Le Bras, Jena D. Hwang, Soumya Sanyal, SeanWelleck, Xiang Ren, Allyson Ettinger, Zaid Harchaoui, and Yejin Choi. Faith and Fate:Limits of Transformers on Compositionality, October 2023. URL arXiv:2305.18654 [cs].",
  "Shi Feng, Alejandra Magana, and Dominic Kao. A Systematic Review of Literature on theEffectiveness of Intelligent Tutoring Systems in STEM. pages 19, October 2021. doi:10.1109/FIE49875.2021.9637240": "Arthur Graesser, Shulan Lu, G. Jackson, Heather Mitchell, Mathew Ventura, Andrew Olney,and Max Louwerse. AutoTutor: a Tutor with Dialogue in Natural Language. BehaviorResearch Methods, 36:180192, June 2004. doi: 10.3758/BF03195563. Arthur Graesser, Mark Conley, and Andrew Olney.Intelligent tutoring systems.APAeducational psychology handbook: Vol. 3. Applications to learning and teaching, 3:451473, January 2012. ISSN 1-4338-0999-0. doi: 10.1037/13275-018. Hippolyte Gros, Jean-Pierre Thibaut, and Emmanuel Sander. Semantic congruence in arith-metic: A new conceptual model for word problem solving. Educational Psychologist, 55(2):6987, April 2020. ISSN 0046-1520, 1532-6985. doi: 10.1080/00461520.2019.1691004.URL",
  "Anubhav Jangra, Jamshid Mozafari, Adam Jatowt, and Smaranda Muresan. Navigatingthe Landscape of Hint Generation Research: From the Past to the Future, April 2024.URL arXiv:2404.04728 [cs]": "Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra SinghChaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lu-cile Saulnier, Lelio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao,Thibaut Lavril, Thomas Wang, Timothee Lacroix, and William El Sayed. Mistral 7b,2023. URL Ellen Jiang, Edwin Toh, Alejandra Molina, Aaron Donsbach, Carrie Cai, and Michael Terry.GenLine and GenForm: Two Tools for Interacting with Generative Language Models ina Code Editor. pages 145147, October 2021. doi: 10.1145/3474349.3480209. Ellen Jiang, Kristen Olson, Edwin Toh, Alejandra Molina, Aaron Donsbach, Michael Terry,and Carrie J Cai. PromptMaker: Prompt-based Prototyping with Large Language Mod-els. In CHI Conference on Human Factors in Computing Systems Extended Abstracts,pages 18, New Orleans LA USA, April 2022. ACM.ISBN 978-1-4503-9156-6.doi:10.1145/3491101.3503564. URL",
  "Andre Knops. Neural and cognitive processes underlying numerosity perception and mentalarithmetic. PhD thesis, Universite Paris Cite, 2022": "Ekaterina Kochmar, Dung Do Vu, Robert Belfer, Varun Gupta, Iulian Vlad Serban,and Joelle Pineau.Automated Personalized Feedback Improves Learning Gains inan Intelligent Tutoring System, May 2020. URL [cs]. Andrew S. Lan, Divyanshu Vats, Andrew E. Waters, and Richard G. Baraniuk. Math-ematical Language Processing: Automatic Grading and Feedback for Open ResponseMathematical Questions, January 2015.URL [cs, stat]. Naiming Liu, Zichao Wang, Richard Baraniuk, and Andrew Lan. Open-ended KnowledgeTracing for Computer Science Education. In Yoav Goldberg, Zornitsa Kozareva, andYue Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural",
  "Language Processing, pages 38493862, Abu Dhabi, United Arab Emirates, December2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.254.URL": "Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neu-big. Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods inNatural Language Processing, July 2021.URL [cs]. Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, XiuboGeng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang.WizardMath: EmpoweringMathematical Reasoning for Large Language Models via Reinforced Evol-Instruct, Au-gust 2023. URL arXiv:2308.09583 [cs]. Julia M. Markel, Steven G. Opferman, James A. Landay, and Chris Piech.GPTeach:Interactive TA Training with GPT-based Students. In Proceedings of the Tenth ACMConference on Learning @ Scale, pages 226236, Copenhagen Denmark, July 2023. ACM.ISBN 9798400700255. doi: 10.1145/3573051.3593393. URL Hunter McNichols, Wanyong Feng, Jaewook Lee, Alexander Scarlatos, Digory Smith, SimonWoodhead, and Andrew Lan. Automated Distractor and Feedback Generation for MathMultiple-choice Questions via In-context Learning, January 2024. URL arXiv:2308.03234 [cs]. Antonija Mitrovic, Stellan Ohlsson, and Devon Barrow. The effect of positive feedbackin a constraint-based intelligent tutoring system. Computers & Education, 60:264272,January 2013. doi: 10.1016/j.compedu.2012.07.002.",
  "Huy A. Nguyen, Hayden Stec, Xinying Hou, Sarah Di, and Bruce M. McLaren. EvaluatingChatGPTs Decimal Skills and Feedback Generation in a Digital Learning Game, June2023. URL arXiv:2306.16639 [cs]": "Allen Nie, Yash Chandak, Miroslav Suzara, Malika Ali, Juliette Woodrow, Matt Peng,Mehran Sahami, Emma Brunskill, and Chris Piech. The GPT Surprise: Offering LargeLanguage Model Chat in a Massive Coding Class Reduced Engagement but IncreasedAdopters Exam Performances, April 2024. URL [cs, stat]. Joon Sung Park, Lindsay Popowski, Carrie J. Cai, Meredith Ringel Morris, Percy Liang,and Michael S. Bernstein.Social Simulacra: Creating Populated Prototypes for So-cial Computing Systems, August 2022.URL [cs]. Joon Sung Park, Joseph C. OBrien, Carrie J. Cai, Meredith Ringel Morris, Percy Liang,and Michael S. Bernstein. Generative Agents: Interactive Simulacra of Human Behavior,August 2023. URL arXiv:2304.03442 [cs].",
  "Lianne Roest, Hieke Keuning, and Johan Jeuring.Next-Step Hint Generation for In-troductory Programming Using Large Language Models, December 2023. URL arXiv:2312.10055 [cs]": "Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, JingXu, Myle Ott, Eric Michael Smith, Y-Lan Boureau, and Jason Weston.Recipes forBuilding an Open-Domain Chatbot. In Paola Merlo, Jorg Tiedemann, and Reut Tsarfaty,editors, Proceedings of the 16th Conference of the European Chapter of the Association forComputational Linguistics: Main Volume, pages 300325, Online, April 2021. Associationfor Computational Linguistics.doi: 10.18653/v1/2021.eacl-main.24.URL Sherry Ruan, Liwei Jiang, Justin Xu, Bryce Joe-Kun Tham, Zhengneng Qiu, Yeshuang Zhu,Elizabeth L. Murnane, Emma Brunskill, and James A. Landay. QuizBot: A Dialogue-based Adaptive Learning System for Factual Knowledge. In Proceedings of the 2019 CHIConference on Human Factors in Computing Systems, pages 113, Glasgow ScotlandUk, May 2019. ACM. ISBN 978-1-4503-5970-2. doi: 10.1145/3290605.3300587. URL Pranab Sahoo, Ayush Kumar Singh, Sriparna Saha, Vinija Jain, Samrat Mondal, andAman Chadha. A Systematic Survey of Prompt Engineering in Large Language Models:Techniques and Applications, February 2024. URL [cs]. Alexander Scarlatos, Digory Smith, Simon Woodhead, and Andrew Lan. Improving theValidity of Automatically Generated Feedback via Reinforcement Learning, March 2024.URL arXiv:2403.01304 [cs] version: 1.",
  "Shashank Sonkar, Andrew E. Waters, Andrew S. Lan, Phillip J. Grimaldi, and Richard G.Baraniuk. qDKT: Question-centric Deep Knowledge Tracing, May 2020. URL arXiv:2005.12442 [cs, stat]": "Shashank Sonkar, MyCo Le, Xinghe Chen, Naiming Liu, Debshila Basu Mallick, andRichard G. Baraniuk.Code Soliloquies for Accurate Calculations in Large LanguageModels, October 2023a. URL arXiv:2309.12161[cs]. Shashank Sonkar, Naiming Liu, Debshila Basu Mallick, and Richard G. Baraniuk. CLASS:A Design Framework for building Intelligent Tutoring Systems based on Learning Scienceprinciples, October 2023b. URL arXiv:2305.13272[cs]. Jacob Steiss, Tate, Steve Graham, Cruz, Hebert, Jiali Wang, Youngsun Moon, Tseng, andWarschauer.Comparing the Quality of Human and ChatGPT Feedback on StudentsWriting. October 2023. doi: 10.35542/osf.io/ty3em.",
  "Pramuditha Suraweera and Antonija Mitrovic.KERMIT: A constraint-based tutor fordatabase modeling. volume 2363, pages 377387, June 2002. ISBN 978-3-540-43750-5.doi: 10.1007/3-540-47987-2 41": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux,Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, AurelienRodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. LLaMA: Open andEfficient Foundation Language Models, February 2023a. URL arXiv:2302.13971 [cs]. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, YasmineBabaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, DanBikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, DavidEsiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, VedanujGoswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan,Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev,Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich,Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-bog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi,Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing EllenTan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan,Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien",
  "Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open Foun-dation and Fine-Tuned Chat Models, July 2023b. URL arXiv:2307.09288 [cs]": "Rose E. Wang, Qingyang Zhang, Carly Robinson, Susanna Loeb, and Dorottya Dem-szky. Bridging the Novice-Expert Gap via Models of Decision-Making: A Case Studyon Remediating Math Mistakes, April 2024. URL [cs]. Rainer Winkler and Matthias Sollner. Unleashing the Potential of Chatbots in Education:A State-Of-The-Art Analysis. Academy of Management Proceedings, 2018:15903, April2018. doi: 10.5465/AMBPP.2018.15903abstract.",
  "A.5 Metrics and Additional Results . . . . . . . . . . . . . . . . . . . . . . . . .29": "A.5.1Additional results across exercises based on temperature for studentmodel accuracy before and after hints guided by Llama-3-8B-Instructvs. GPT-4o . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .30",
  "We used the OpenAI API to interact with models based on GPT, such as GPT-4o andGPT-3.5-turbo. For other open-source models, like Llama-3-8B-Instruct and Mistral-7B-": "Instruct-v0.3, we accessed the resources via the HuggingFace platform. Specifically for thesemodels, the prompts were executed using a 512 GB setup with two A100 GPUs.During the exercise resolution phase and the review of the students responses acrossvarious pipelines, several questions arise:How can we ensure that the hints are given to the same student who made the error orprovided the response? How can we guarantee that no previous hint is reused for the samestudent in the current session?The solution is to only add the students initial solution to the context of the studentfor the current session.Indeed, whether using the API or open-source models, contextmanagement is done manually. There is no contextual dependency between requests unlesswe manually add the previous response to the context using the assistant role role: as-sistant, content: . By doing so, we ensure that the student who provided an incorrectresponse is the one who corrects their initial answer using the hints provided by the teacher.",
  "A.3.1. Prompt for exercise resolution by the student model": "{\"role\": \"system\", \"content\": \"You are a high school student who must solvemathematics exercises.\"},{\"role\": \"user\", \"content\": Your objective is to answer the questions in theexercises by following the given instructions.Exercise and question: {exercise}Instructions: {instruct}Required answer format: use a JSON format with the following structure:{\"reasoning\": \"Explain your reasoning here...\", \"answer\": \"Provide your answerhere...\"}I emphasize that you must follow the required response format, and also thatyou must answer the questions in the exercises by following the instructionsas given, without adding anything.}",
  "A.3.2. Prompt for answer review by the student model": "{\"role\": \"system\", \"content\": \"You are a high school student who must solve mathematicsexercises.\"},{\"role\": \"user\", \"content\": You provided an incorrect answer to a math exercise.A teacher has given you a hint to help you understand your mistake and correctit. Your objective is to review your response to the questions in the exerciseusing the hint provided by the teacher.Exercise and question: {exercise}.Instruction: {instruct}Hint: {hint}",
  "A.3.3. Prompt for classifying hints": "{\"role\":\"system\", \"content\":\"You are an expert in teaching mathematics\"},{\"role\": \"user\", \"content\":Your task is to verify if a students revised answerto a mathematics exercise is correct or not by comparing it with the correctanswer(s) provided. The exercises may have either a single correct answer ormultiple correct answers. The correct answer(s) for the exercise: {answer}The students revised answer: {revised_response}The hint: {hint}1)- If the students revised answer does not match the correct answer or anyof the correct answers (if multiple), then put the hint in the \"wrong_hint\" fieldof the output.2)- If the students revised answer includes at least one correct answer orall the correct answers, put the hint in the \"correct_hint\" field of the output.I insist on this, please follow this criterion.3)- If the hint contains the correct answer(s) or parts of the correct answer(s),then put the hint in the \"wrong_hint\" field of the output.Put the output in a JSON format with the following structure: {{\"correct_hint\":\"\",\"wrong_hinMake sure that the generated output does not contain escape characters such asline breaks (\\\\n) or slashes (\\\\).Please provide a clean and readable output. I insist on this. Do not make anyformatting errors.Follow the output format, and also follow the evaluation criteria and your role.Do not add anything else.}",
  "A.3.4. Prompt for checking if the answer is correct and detecting thetype of error": "{\"role\": \"system\",\"content\": \"You are an expert in teaching mathematics\"},{\"role\": \"user\",\"content\": Your task is to verify whether a students answerto a mathematics exercise is correct or not by comparing it with the correctanswer(s) provided. Exercises may have either a single correct answer or multiplecorrect answers.The correct answer(s) for the exercise: {answer}The students answer: {student_answer}The students reasoning: {reasoning} Categorize the students error. Here are some categories of errors and examples.You can add other categories of errors. If the reasoning contains multiple errors,it is important to list all the present errors.Specify each error distinctly, even if they belong to different categories orcombine together.1) Comprehension error: The student does not clearly understand the problem orthe given instructions.Example: Misreading a problem and confusing the given data.2) Partial answer: The student provides part of the expected answer but failsto complete it correctly.Example: In an equation with two variables, the student finds the value of onevariable but forgets to find the value of the other.3) Term grouping error: The student incorrectly combines or groups terms in amathematical expression.Example: When simplifying the expression 3x + 2x + 5, the student combines theterms 3x and 2x to get 5x^2 instead of 5x.4) Simplification error: The student incorrectly simplifies a mathematical expression.Example: When simplifying 6x/2, the student divides both the numerator and denominatorby x instead of 2, resulting in an incorrect simplification of 6/2x.5) Calculation error: The student incorrectly performs mathematical operations.Example: When multiplying 7 by 8, the student gets 54 instead of 56.6) Incorrect substitution error: The student substitutes an incorrect value intoan expression or equation.Example: In the equation 2x + 3y = 10, the student substitutes x = 4 insteadof y = 2, leading to an incorrect solution.7) Interpretation error: The student incorrectly interprets the problems instructionsor data.Example: In a probability problem, the student confuses the probability of eventA with that of the complementary event of A.8) Algebraic error: The student makes a mistake in algebraic manipulations, suchas distributing, factoring, or solving equations.Example: In solving 2(x + 3) = 10, the student incorrectly divides 10 by x +3 instead of 2, leading to an incorrect answer.1) - If the students answer does not match the correct answer or any of thecorrect answers (if multiple), categorize the type of error and leave the \"correct_answer\"field empty.2) - If the students answer includes at least one correct answer or all thecorrect answers, put the students answer in the \"correct_answer\" field. I insiston this, please follow this criterion.Put the output in a JSON format with the following structure: {{\"error_type\":\"\",\"correct_answer\": \"\"}}Follow the output format, and also follow the evaluation criteria and your role.Do not add anything else.}",
  "A.3.5. Prompt for obtaining the diverse dataset with different reasoningand answers per type of errors": "{\"role\": \"system\", \"content\": \"You are an expert in teaching mathematics\"},{\"role\": \"user\", \"content\": fYour task is to classify a list of reasonings that contain multiple categoriesof errors. For each error category, you must provide the best examples with differentreasoning.In each reasoning, there may be multiple error categories. If thats the case,then you must find examples that are different for that group of error categories. The error categories are already provided in the reasonings.For each error category or group of error categories, you need to identify andprovide the best k examples of different reasoning.### Output FormatMake sure the generated output does not contain escape characters such as linebreaks (\\\\n) or slashes (\\\\).Please provide a clean and readable output. I insist on this. Do not make anyformatting errors. Do not add errors that are not in the list.You must provide the output in JSON format with the following structure: {{\"different_reasoning\": {{\"category_1\": [{{\"gpt_initial_reasoning\": \"\",\"initial_response\": \"\",\"evaluation\":\"\"}},...],\"category_2\": [{{\"gpt_initial_reasoning\": \"\",\"initial_response\": \"\",\"evaluation\":\"\"}},...],...}}}}The list of reasonings is: {list_reasoning}Do not repeat the error groups, for example: calculation error, interpretationerror is the same as interpretation error, calculation error.The final JSON format must accurately reflect the classification you have made.Please insert each reasoning into the appropriate category without modifyingthe content of the reasoning, the initial response, and the evaluation.}",
  "A.4. Prompt for hint generation": "For the generation of hints, there are only a few differences in the users role in each prompt.The rest of the content is identical, which is why we will include a complete example of oneprompt. For the other prompts, we will only provide the users role definition, specifyingthat the rest of the prompt follows the same structure.",
  "BaselineOne prompt": "{\"role\": \"system\", \"content\": \"You are an expert in teaching mathematics, helpingstudents solve a math exercise by providing guiding hints following a specificcognitive approach.\"},{\"role\": \"user\", \"content\": Your goal is to generate progressive hints tohelp students solve an exercise while following the specified cognitive approach.The hints should be given in increasing order of difficulty and should not revealthe final solution. The hints should encourage students to think independentlywhile providing useful guidance. The hints must be in the form of questions,and they must not reveal the correct answer or any part of it|I insist on this. The exercise and question: {exercise}.Instruction: {instruct}The correct answer to the exercise: {answer}Guide according to the cognitive approach: {demarche_cog}Required response format: use a JSON format with the following structure: {{\"hints\":[\"hint1, hint2...\"]}}. Do not number the hints.I insist that you respect the response format and also ensure that the hintsare in the form of questions and follow the specified cognitive approach. Provideonly the hints, do not include any explanations.}",
  "BaselineTwo prompt": "\"role\": \"user\", \"content\": Your goal is to identify the common mistakes thatstudents might make and to generate hints in the form of questions that can helpthem correct their mistakes and progress in solving the exercise. The hints mustbe in the form of questions, and they must not reveal the correct answer or anypart of it, I insist on this...",
  "Prompt based on the students reasoning": "{\"role\":\"system\", \"content\": \"You are an expert in teaching mathematics\"},{\"role\": \"user\", \"content\":Your goal is to provide a clear and relevant hintto the student to help them correct their reasoning mistakes in math exercises.If the student has the correct answer, propose a hint to reinforce their understanding.This hint must be in the form of a question. Additionally, the hint must notinclude the correct answer to the exercise or any part of it.",
  "Prompt based on the method used by the student": "\"role\": \"user\", \"content\":Your goal is to provide a hint that helps the studentreview the method they are using to solve the math exercise. If the student hasa correct method, propose a hint to reinforce their understanding of that method.This hint must be in the form of a question. Additionally, the hint must notinclude the correct answer to the exercise or any part of it...",
  "Prompt based on the application of concepts": "\"role\": \"user\", \"content\":Your goal is to provide a hint that helps the studentreview the application of mathematical concepts to solve the exercise and findthe correct answer. If the student is applying the concepts correctly, proposea hint to reinforce their understanding. This hint must be in the form of a question.Additionally, the hint must not include the correct answer to the exercise orany part of it...",
  "Prompt based on calculations": "\"role\": \"user\", \"content\":Your goal is to provide a hint that helps the studentreview the calculations performed to solve the math exercise. If the studentscalculations are correct, propose a hint to reinforce their understanding ofthe calculation steps. This hint must be in the form of a question. Additionally,the hint must not include the correct answer to the exercise or any part of it...",
  "Prompt based on problem interpretation": "\"role\": \"user\", \"content\":Your goal is to provide a hint that helps the studentreview their interpretation of the math problem. If the student interprets theproblem correctly, propose a hint to reinforce their understanding. This hintmust be in the form of a question. Additionally, the hint must not include thecorrect answer to the exercise or any part of it...",
  "Prompt combining all the above aspects": "\"role\": \"user\", \"content\":Your goal is to provide a clear and relevant hintto the student to help them correct their mistakes and improve their answersin math exercises. This hint must be in the form of a question. Additionally,the hint must not include the correct answer to the exercise or any part of it.Consider the following aspects when generating the hint:- Reasoning- Method- Application of concepts- Calculations- Interpretation of the problem ...",
  "A.6. Description of exercises and pedagogical elements": "This section presents the exercises used in our study, including key pedagogical elementssuch as the cognitive approach associated with each exercise, the type of exercise, the ex-ercise statement, instructions, and the corresponding answer. The exercises were originallywritten in French, and the experiments were conducted in French as well. The promptswere also written in French. For the purpose of this paper, we have translated them intoEnglish.",
  "A.6.1. Exercise 1 - module 1": "Cognitive Approach: Transition from the concept of partitioning to the concept of frac-tion as a quotient, through the imposition of a constraint on the whole.Level 1This series of activities (A7, A8, A9) aims to gradually move beyond the intuitive notionthat an equitable division of a whole composed of multiple units requires taking an equalpart of each unit. Starting from Level 2, a condition imposed in the problem statementforces the student to counter this conception. The goal is to progressively reach the un-derstanding of a fraction as a quotient. At Level 1, the statement aligns with the studentsintuitive conception, with no imposed conditions.Type of Exercise: The student is presented with a problem involving the division ofa whole, composed of n units, into m parts.Example: The whole consists of 4 units, represented by 4 wooden planks. The whole isdivided into 3 equal parts, and the student is asked to interpret the value of one parts size,given the condition of equal portions of each unit. The statement allows the student witha partition-based understanding of fractions to solve the problem by reasoning as follows:43 is like 4 times one-third of 1.Exercise Statement:Elias bought two quiches of the same size. He decides to eat one-quarter of the quiches",
  "(d) Llama-3-8B-instruct": ": Comparison of accuracy before and after providing hints across each exercisefor different student models, using GPT-4o and Llama-3-8B-instruct as teachermodels with the best specialized hint generation prompt focused on cal-culation errors. The results show improved performance when using Llama-3-8B-instruct as the teacher model, but the student models struggled to correctthemselves on exercise 3 - module 7.",
  "(b) Llama-3-8B-instruct": ": Comparison of accuracy before and after providing hints across each exercisefor different student models, using GPT-4o and Llama-3-8B-instruct as teachermodels with the best baseline-type hint generation prompt, named Base-lineTwo.The results show improved performance when using Llama-3-8B-instruct as the teacher model, but the student models struggled to correct them-selves on exercise 3 - module 7.",
  "(c) Mistral-7B-Instruct-v0.3": ": Comparison of error revision rates across different temperatures and prompts forhint generation in three student models (GPT-3.5-turbo, Llama-3-8B-instruct,Mistral-7B-instruct-v3) on exercise 2 - module 2 with GPT-4o as teacher. TheFigure illustrates how each models error revision rate changes with increasingtemperatures and with respect to types of errors, showing that the frequency oferror types tends to increase at higher temperatures. and save the rest for later. He wants to eat an equal part of each quiche. What fractionof each quiche will he eat?Instruction:Complete the following sentence with fractions: Elias will eatof the first quicheandof the second quiche.Answer:Elias will eat 1",
  "A.6.2. Exercise 2 - module 2": "Cognitive Approach: Adopt a dual perspective to model a multi-step algebraic problemusing a literal expression represented both as a sum and as a product.Level 4This series of activities (A1, A2, A3, A4) encourages flexibility in problem-solving strategies,moving beyond the strategy suggested by the problems context and enabling the studentto consider an alternative strategy based on the distributive property. This activity rein-forces mastery of this property. Depending on the problem scenario, the students intuitiveapproach might involve modeling with a literal expression in either expanded form (sum ofexpressions) or factored form (product of expressions).At Level 4, one step in the problem requires expressing one variable in terms of another,with an additional challenge introduced as the relationship between these two variables isexpressed as a ratio. This ratio involves either multiplying by a fraction less than 1 ordividing by a whole number. For example, 1 6 of a tulip corresponds to 1 rose.Type of Exercise: The student is asked to model a two-step problem using a literalexpression by selecting one or more correct answers from four given options.Exercise Statement:To decorate her house, Julie enters a store and buys 5 of each of the following items: greenplants and flower pots. The price of a green plant varies depending on the stores stock. Agreen plant costs 3 times as much as a matching flower pot.Let p be the price of a green plant. How much did Julie pay in total?Instruction:Identify the expressions that represent the total price Julie paid.Select the correct answer(s): 5p + 5p",
  "A.6.4. Exercise 3 - module 7": "Cognitive Approach: Understand how to simplify a fraction to its irreducible form.Level 1This series of activities (A3, A4) focuses on using prime factorization to determine whethera fraction is in its simplest form.It aims to develop conceptual expertise in fractionaloperations and explore the different meanings of fractions in problem-solving contexts. AtLevel 1, the scenarios align with the intuitive understanding of a fraction as a ratio betweena part and a whole.Type of Exercise: The student answers (yes or no) a question regarding the simpli-fiability of the fraction presented in the problem. If the fraction is reducible, the studentwrites its simplified form and completes a response sentence.Exercise Statement:A truck driver covered five thousand five hundred thirty-thirds of a kilometer in two hours.Instruction:Can the fraction that describes the number of kilometers the truck driver cov-ered be simplified?Yes? No?If yes: If the truck driver covered five thousand fivehundred thirty-thirds of a kilometer in two hours, that means he coveredkilometers intwo hours.Answer:The correct answer is yes. If the truck driver covered five thousand five hundred thirty-thirdsof a kilometer in two hours, that means he covered 500"
}