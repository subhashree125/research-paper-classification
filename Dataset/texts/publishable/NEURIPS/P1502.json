{
  "Abstract": "Electromagnetic field simulation is central to designing, optimizing, and vali-dating photonic devices and circuits. However, costly computation associatedwith numerical simulation poses a significant bottleneck, hindering scalability andturnaround time in the photonic circuit design process. Neural operators offer apromising alternative, but existing SOTA approaches,NeurOLight, struggle withpredicting high-fidelity fields for real-world complicated photonic devices, withthe best reported 0.38 normalized mean absolute error in NeurOLight. The inter-plays of highly complex light-matter interaction, e.g., scattering and resonance,sensitivity to local structure details, non-uniform learning complexity for full-domain simulation, and rich frequency information, contribute to the failure ofexisting neural PDE solvers. In this work, we boost the prediction fidelity to anunprecedented level for simulating complex photonic devices with a novel oper-ator design driven by the above challenges. We propose a novel cross-axis fac-torized PACE operator with a strong long-distance modeling capacity to connectthe full-domain complex field pattern with local device structures. Inspired byhuman learning, we further divide and conquer the simulation task for extremelyhard cases into two progressively easy tasks, with a first-stage model learning aninitial solution refined by a second model. On various complicated photonic de-vice benchmarks, we demonstrate one sole PACE model is capable of achieving73% lower error with 50% fewer parameters compared with various recent MLfor PDE solvers. The two-stage setup further advances high-fidelity simulationfor even more intricate cases. In terms of runtime, PACE demonstrates 154-577and 11.8-12 simulation speedup over numerical solver using scipy or highly-optimized pardiso solver, respectively. We open sourced the code and compli-cated optical device dataset at PACE-Light.",
  "Introduction": "With advances in integrated photonics, photonic structures capable of transmitting or processing in-formation are gathering increasing interest, fueled by the optical communication and the recentresurgence of photonic analog computing . Light-empowered communica-tion and computing offer a promising pathway for reshaping future AI systems, prompting the opti-cal community to discover compact, customized devices to overcome the limitationsof bulky optical components. In this optical design process, numerical simulators, e.g., the popularfinite difference frequency domain (FDFD) algorithm , is heavily used to obtain accurate optical",
  ": Challenges of complicated optical device simulation: (a-d) and learning framework (e)": "fields for characterizing and optimizing device behavior. However, the significant time and compu-tational costs associated with Maxwell partial differential equation (PDE) simulations, exacerbatedby the need for finely tailored meshes and numerous simulation runs for iterative optimization, posesubstantial bottlenecks in the design loop. Recently, neural PDE solvers have emerged as promising surrogate mod-els for fast and accurate PDE solving. NeurOLight represents the state-of-the-art (SOTA),extending neural operators to parametric photonic device simulations in a physics-agnostic manner.However, it still exhibits large errors in simulating real-world complicated optical devices, reportinga 0.38 normalized mean absolute error on the etched multi-mode interference (MMI) device .One may wonder what the major challenges are, given the successes of neural operators in manyscientific PDEs. Firstly, for complicated devices, the permittivity distribution is discrete and highlycontrasting, transforming the Maxwell PDE into a multi-scale problem , further leading to com-plex light-matter interactions such as scattering and resonance, as illustrated in (a). Secondly,their optical fields are highly sensitive to local structural changes; even minor alterations can signif-icantly impact the field, as depicted in (b). Moreover, with diversifying field patterns alongthe light propagation path, it shows non-uniform learning complexity especially in regions distantfrom the input light source. Finally, a spectral analysis provides insights into the frequency-domainchallenges, as illustrated in (d). Unlike simpler systems where low frequencies dominate(e.g., Darcy flow shown in ), complicated devices exhibit rich frequency spectra with high-frequency components. This diversity underpins the difficulty faced by previous neural PDE solversin accurately simulating complicated photonic devices, supporting the assertion in that no singlemodel can universally solve all types of PDEs. In this work, we tackle the challenging real-world complicated optical device simulation problem.We vastly boost prediction fidelity and keep 154-577 and 11.8-12 speedup over traditional nu-merical solver on a 20-core CPU with scipy or highly-optimized pardiso solver, respectively.",
  "We introduce a novel cross-axis factorized PACE operator backbone, effectively capturingcomplex physical phenomena across the full domain in a parameter-efficient manner": "We employ a divide-and-conquer approach inspired by human learning for extremely chal-lenging cases, with a first-stage PACE-I to learn a rough approximation of the optical field,refined by a second-stage PACE-II. On various complicated device benchmarks, one sole PACE significantly outperforms base-lines, achieving 73% lower error with 50% fewer parameters. Even compared to the bestbaseline, it lowers prediction error by over 39% with 17% fewer parameters. Our two-stage method further advances high-fidelity simulation for extremely challenging cases.",
  "Recently, neural operators have emerged as a novel approach for developing machine learningmodels aimed at solving partial differential equations (PDEs). These models focus on learning": "the mapping between the function spaces in a purely data-driven fashion. This holds the general-ization capability within a family of PDEs and can potentially be adapted to different discretiza-tions. Various function bases are utilized to build the operator learning model, such as the Fourierbases , wavelet bases , spectral method , and attention layer . Thesemodels have demonstrated remarkable performance and efficiency in solving specific types of prob-lems, often achieving record-breaking results in certain applications. Despite their successes, itsimportant to recognize that the field of PDEs encompasses a wide variety of equations, each withits own unique properties and characteristics. As pointed out in recent research , there is noguarantee that a single type of data-driven model can effectively address all types of PDEs.",
  "Optical Field Simulation with Machine Learning": "Analyzing the propagation of light through optical devices is crucial for the optimization and designof photonic circuits. For a linear isotropic optical device, with a time-harmonic continuous-wavelight beam shining on its input port, we can obtain the steady-state electromagnetic field distributionsE(r) = xEx + yEy +zEz and H(r) = xHx + yHy +zHz by solving the steady-state frequency-domain curl-of-curl Maxwell PDE under absorptive boundary conditions ,(10 ) 20r(r)E(r) = jJe(r), (1r (r)) 200H(r) = jJm(r)(1)where is the curl operator, 0 is the vacuum magnetic permeability, 0 is the vacuum electricpermittivity, r is the relative electric permittivity, and Jm and Je are the magnetic and electric cur-rent sources, respectively. The finite difference frequency domain (FDFD) method, a widely adoptednumerical technique detailed in , is used to discretize these continuous-domain equations into anM N mesh grid. This transforms the Maxwell PDEs into a linear system AX = b. Solving thissystem with a large sparse matrix A CMNMN is computationally expensive and challenging toscale. Although improvements have been made, such as replacing the scipy solver with the moreefficient pardiso solver, the process remains prohibitively costly for large-scale applications. Building neural networks (NNs) to accelerate this time-consuming simulation process has been in-vestigated in predicting some key design parameters or the entire optical field .NeurOLight extends the neural operator to optical field simulation, enabling learning a physics-agnostic parametric Maxwell PDE solver and achieving SOTA accuracy, while its performance onreal-world complicated photonic device is still not satisfactory.",
  "Understand the Problem Setup and Challenge": "In this study, we aim to build a physics-agnostic neural operator for parametric photonic devicesimulation in a data-driven fashion to approximate the ground-truth Maxwell PDE solver : A U described in Eq. (1). Here, U represents the solution space for the optical field in Cdu and A =(, r, , J) represents the observation space of the Maxwell PDE, both defined over the continuous2-D physical solving domain = (lx, lz). We follow NeurOLight to discretize the simulationdomain as = (M, N, lx, lz) with adaptive mesh granularity, i.e., with grid steps lx =lx/M and lz = lz/N. Moreover, (, r, ) in the raw observation A is encoded as informativewave priors, Pz = ej 2r",
  "Challenges in Predicting the Light Field of Complicated Photonic Devices": "NeurOLight delivers a pioneering effort in extending neural operators to the simulation ofphotonic devices, achieving SOTA accuracy. However, it still yields significant errors, particularlyfor real-world complicated devices, with a reported 0.38 normalized mean absolute error for etchedMMI device . This leads us to an interesting reflection: despite the successes of neural",
  "operators in solving scientific PDEs, why do they still fall short in complicated photonic devicesimulation? Below, we provide a detailed analysis that highlights the underlying learning challenges": "Complicated light-matter interaction in the optical field of real-world photonic device.Permittivity r, a critical parameter in photonic devices, greatly impacts how light propagatesthrough media. Designing new devices often involves manipulating the r distribution acrossthe domain. However, due to manufacturing limitations, r changes are discrete rather thansmooth. Moreover, researchers explore patterning materials with highly contrast permittivity todesign compact devices . This discrete and highly contrasting permittivity transformsthe Maxwell PDE into a multiscale PDE problem , with complicated light-matter interactionssuch as scattering resonance happening, shown in (a), which has been shown difficult topredict from both scientific computing and operator learning perspectives . Significant prediction field variations from minor structural changes. Due to the complexlight-matter interactions within the field, even a slight change in the photonic structure can resultin drastically different optical fields under the same input conditions, as shown in (b). Thiscalls for a powerful backbone model that is capable of building the relationship between localrival changes with the global optical field transition. Non-uniform learning difficulty along the spatial domain. As shown in (c), with lightshining in from a specific position and direction, it propagates through the media, resulting innon-uniform learning difficulties along the spatial domain. Due to the vast diversity of potentialinternal structures along the light propagation path, the light patterns are becoming highly diverse.Consequently, the data collected for training also incorporates the same phenomenon where manysimilar patterns are seen during training near the input sources, whereas the model faces morediverse patterns at greater distances. This makes it hard for the model to learn how to predictfurther regions, especially when the domain is elongated. This issue is analogous to the roll-outerror encountered in temporal PDE modeling at the large time steps. Rich frequency information lies in the predicted field. We show the energy spectrum of theoptical field in the frequency domain in (d). The field, characterized by complex interac-tions such as scattering and resonance, exhibits rich frequency information, unveiling the learningcomplexity from a frequency-domain analysis. This confirms the usage of high-frequency modesin NeurOLight, underscoring the need for a parameter-efficient, robust, and powerful backbonemodel to resolve the parameter efficiency and overfitting issue with large modes.",
  "a(r) v0(r) v1(r) vK(r) u(r),r .(3)": "We start with the convolutional stem used in to project the PDE observation a(r) into a higher-dimensional feature space of dimension C. This is followed by a sequence of K cascaded neuraloperator blocks, which gradually reconstruct the complex optical field within the C dimensionalspace. At last, a head with two point-wise convolutional layers projects the vK(r) to the opticalfield space u(r). (a) shows the proposed PACE neural operator block structure, formulated as,",
  "vk+1(r) := FFN(Kvk)(r) + vk+ vk, r ; vk(r) = pre-norm(vk(r)),(4)": "where K is the our proposed PACE operator and FFN() is a feedword network used in . Tostabilize the model performance when scaling to deeper layers, we add pre-normalization andfollow to add a double skip. In this work, we consistently use the NeurOLight operator inthe first two blocks to align our model with the horizontal and vertical wave prior encoding methodadopted from NeurOLight, which we found slightly improves our accuracy.",
  "Parameter-efficient and Effective Cross-axis Factorized PACE Operator": "The neural operator design is key to obtaining satisfactory accuracy on a given PDE task. With thewell-discussed challenges in Sec. 3.1, we derive key insights that have guided the development of ourPACE operator in (b): (1) Long-distance full-domain modeling capacity, especially effectivelymodeling how local features impact the whole domain; (2) Isotropic model architecture with no PACE operator X",
  ": Factorized FNO": "information lying in the optical field requires theuse of large frequency modes, making the FNO with huge parameters and severe overfitting issues.NeurOLight and Factorized FNO proposeto decompose the FNO block with independent 1-DFNO blocks in the full N-dimensional domain (see), therefore, solving the parameter concern whenutilizing high-frequency modes and serving as a regu-larization for overfitting. The only difference betweenNeurOLight and Factorized FNO is whether theychunk the input or copy the input to the independent 1-D FNO block. We argue that their theoreticalsuccess is attributed to the implicit full-domain integration in Corollary 4.1.Corollary 4.1. The factorized Fourier integral operator K factorizes the original Fourierintegral operator along each dimension n in the N-dimension domain ,",
  "n (r1, r2)nvk(r2)ndvk(r2)n. It im-plicitly implements full-domain kernel integration in by stacking K, i.e., K0 K1 ,": "However, the reliance on implementing full-domain integration with multi-layers makes them weakoperator candidates to achieve our first requirement, i.e., a strong model that is capable of buildingfull-domain modeling between local structures with the global fields. Proposed cross-axis 2-D factorized integral kernel. Aware of the above shortcomings of previousfactorized FNO variants, in our 2-D domain, we propose to factorize the full domain integral in across-axis way along the horizontal (h) and vertical (v) axis:",
  "h(r1, r2)vvk(r2)dvk(r2)vdvk(r2)h,r1 .(6)": "This factorization enables an explicit factorized full-domain integration. It provides a strong wayto capture the relationship between points in the domain , building the relationship between localstructure with the complicated field pattern. The implementation of the above cross-axis integralcan be efficiently implemented by Fourier Transform F() when the kernel (r1, r2) = (r1 r2),as follows,",
  "Linear": "X : The proposed cascaded learning flow with two stages. The first stage learns an initial and roughsolution, followed by the second stage to revise it further. A cross-stage distillation path is used to transfer thelearned knowledge from the first stage to the second stage. v. The learnable integral kernel intrinsically performs information exchange along different gridpoints in . Similar to the multi-head design in Transformer, which assumes different heads extractdifferent information, we can also partition the C basis functions into g disjoint sub-groups and feedeach sub-group through our cross-axis factorized kernel. This grouping further reduces the numberof parameters to (h + v) CoCi g, showing significant parameter reduction compared to FNO(h v CoCi) and Factorized FNO ((h + v) CoCi), showing excellent parameter efficiencywhen utilizing large frequency modes is a must. We do an ablation study in Appendix A.4 toinvestigate the choices of different group g, where we find g = 4 strikes the best between parameterefficiency and model performance. Explicit projection unit for extracting high frequency information. The optical field showsrich information in the frequency spectrum, reciting a special care of high-frequency information.Besides utilizing high-frequency modes, we propose to add an explicit projection module beforethe cross-axis integral, which is very simple as one linear layer followed by a non-linear activation,given non-linear activation is known to help generate high-frequency features . Self-weighted path for enhanced instance-based local feature attention. The optical fields re-sponse is intricately linked to the minute variations in different photonic device structures. A self-weighted path is introduced to ensure the model can pay different attention to regions of significantinfluences for varying device structures. An instance-based weight is generated by passing the fea-ture map after the projection unit through a linear layer and a Sigmoid unit, and then multipliedwith the results after the cross-axis integral unit to provide instance-based attention.",
  "Cascaded Learning from Rough to Clear": "With the effective PACE operator design, the prediction fidelity can be largely improved by only us-ing a 12-layer PACE model (see Section. 5.2.1). But for some complicated benchmarks (e.g., etchedMMI 3x3/5x5), it still yields 10% mean squared error, which is not satisfying. A straightforwardsolution might involve scaling up the model size, expecting additional layers would enhance perfor-mance. However, as demonstrated in , scaling to deep layers shows saturated performance afterexceeding a specific number. Existing ML for PDE solving work typically learns a model in a one-shot way by directly learningthe underlying relationship from input-output pairs. Unlike AI systems, humans dont learn newand difficult tasks in a one-shot manner; instead, they learn skills progressively, starting with easiertasks and gradually moving to harder ones. For example, instead of directly learning how to solveequations, students first learn basic operations, such as addition and multiplication, and then moveon to solving complex equations. Hence, inspired by this human learning process, unlike previous work that directly learns a one-stagemodel, we propose to divide the challenging optical field prediction problem into two sequential la-tent tasks. The first task, undergoing the same problem setup as discussed in Sec. 3, could predict aninitial, rough optical field based on the less informative raw PDE observation (we only have the lightsource and device permittivity distribution). Then, the successive second task could refine the roughprediction further by capturing more details and nuances, by accepting the predicted field 1 and device permittivity r as the input. Therefore, we assign higher Fourier modes to enable sufficientcapacity. The divide-and-conquer way results in a cascaded two-stage model architecture, as shownin . The cascaded learning model is trained jointly (PACE-I + PACE-II) with the optimizationtarget as the sum of two losses L(1(a), u) + L(2(1(a), r), u), where the first L(1(a), u)serves as intermediate supervision that enfores the first stage model condensate the learned knowl-edge. To better connect the two-stage model, we propose a cross-stage feature distillation path todistill learned feature from the previous stage to the last by using a simple LinearSigmoid path.",
  "Experimental Setup": "Benchmarks: We evaluate our methods on real-world complicated photonic devices that pose sig-nificant simulation challenges for ML surrogate models. This includes the Etched MMI with ran-domly placed rectangular cavities, used in , and the metaline device featuring two layersof randomly dimensioned meta-atoms. These devices present a highly discrete and contrast per-mittivity distribution and complex light-matter interactions, making them ideal for testing theeffectiveness of our model. We generate our datasets using the open-source 2-D FDFD simulator,Angler , with generation details in Appendix A.1. Baselines: We evaluate the proposed PACE model against a range of baselines, including the SOTAneural operator work, NeurOLight , for optical simulation. We also include representativeoperator learning models for scientific PDEs based on Fourier bases(FNO , Factorized FNO(F-FNO) , U-NO , tensorized FNO (TFNO) ), attention kernels , and the latentspectral method (LSM) . We also incorporate UNet and Dilated ResNet (Dil-ResNet). For a fair comparison, we keep a model size budget of under/near 4 million (M) parametersfor baselines, except LSM where the original implementation is adopted. Details on modelconfigurations are in the Appendix A.3. Training setting and metric: All models undergo training for 100 epochs using the AdamW opti-mizer with a weight decay of 1e5 in a batch size of 4. To balance the optimization among differentfields, we use normalized mean squared error (N-MSE) as the learning objective,",
  "L(a), (a)= ((E(a)) (a)2)/(a)2.(8)": "We dont use the previously-used mean absolute error (MAE) as the metric given for complex-valued optical fields; we argue that L2 distance is a more accurate metric to evaluate the distancein the complex plane with a detailed analysis in Appendix A.6. We adopt the superposition-basedmix-up technique to generate input light combinations randomly to augment training data.",
  "Prediction Quality of Single PACE Model": "In Tab. 1, we compare our 12-layer PACE model with various baselines on multiple real-world de-vice benchmarks, showing significant 73.85% smaller test error with 51.67% fewer parameters onaverage. Notably, even when compared to the best baseline, 16-layer NeurOLight, we show over39% lower test error with over 17% fewer parameters. Given the challenge that trial structurechange can totally change the optical field, model relying on downsampling or patching fails tocapture the local details, confirming the failure of the UNet and Transformer model. Moreover, thechallenge and challenge call for a powerful model with long-distance modeling capability. Al-though Dil-ResNet utilizes a dilated block to enlarge the receptive field, it is insufficient for a largedomain, validated by the result that it shows much better accuracy on the small Metaline than theetched MMI3x3. Capturing long-range dependency with the Fourier operator provides an efficientway to the isotropic model without any downsampling, therefore making the Fourier-operator typemodel show consistently better accuracy than other baseline methods. However, due to the chal-lenge that there is rich frequency information in the predicted field, FNO-2d falls short due to theimpediment of utilizing large modes given the large parameter count. We also compared it with thetensorized FNO 2d. However, we find the general tensor decomposition hurt the accuracy of thischallenging task. NeurOLight shares a similar insight of Factorized FNO by factoring Fourier ker-nel with several independent 1-D Fourier kernels; however, as we argued before, it fails to establish : Comparison of # parameters, training error (last epoch), and test error on three benchmarks among ourPACE and various baselines. We use geo-means to report overall improvements across different benchmarks.",
  "Quality Improvement with Two-stage Model": "We further compare the proposed cascaded two-stage model with the common practice of solelyincreasing # layers. We set the PACE-I as a 12-layer PACE model with Fourier modes(#Mode =70,#Mode =40), and PACE-II as a 8-layer PACE model with larger Fourier modes (#Mode =100, #Mode=40). As shown in Tab. 2, the two-stage setup introduces slight overhead for one extra set of stemand head but shows a clear margin over only increasing the number of layers in terms of both trainerror and test error. The cross-stage feature distillation further provides meaningful guidance bytransferring learned features to the second-stage model, leading to the best accuracy for the two-stage setup. In Appendix A.7, we also show that the cross-stage distillation trick can improve modelaccuracy, similar to a more costly training setup, by training the two-stage models sequentially.",
  ": Speedup of PACE over angler using scipy (S)/ pardiso (P) with simula-tion granularity (0.05nm) and (0.075nm)": "evaluate the speed-up of our PACE model compared tothe FDFD numerical simulator Angler . We vary thesimulation domain size and set the grid step to 0.05 nm,scaling the discretized size pardiso linear solvers, re-spectively and number of frequency modes to ensure themodel has sufficient capacity to capture the entire sim-ulation domain. For comparison, we employ a 20-layerjoint PACE model. As shown in , our PACE modelachieves a speed-up of 150-577 and 12 over Angleron a 20-core Intel i7-12700 CPU using the scipy andWe further set a larger simulation granularity, 0.075 nm,to check speedup if we tolerate simulation quality lossin commercial tools.However, we find that setting alarger granularity results in a significantly different field,as qualitatively shown in reb-, with a correspondingN-MSE error of 1.2. Even though in this case, PACE still shows a 5.1-10.6 speedup over pardiso-based Angler with much better fidelity.",
  "Use TFNO1.0610.809.51 (+4.69)": "within the PACE operator to assesstheir effectiveness.The self-weightedpath, which provides instance-specificweights, significantly improves modelaccuracy across various photonic de-vice patterns.Removing this compo-nent results in a 17% increase in error,highlighting its importance. Similarly,eliminating the high-frequency projec-tion unit leads to a 23% worse error, em-phasizing its crucial role in capturing high-frequency features. To further illustrate this, we visualizethe feature maps in the frequency domain before and after applying the nonlinear activation in thehigh-frequency projection unit. As shown in , the nonlinear activation effectively amplifieshigh-frequency components, supporting our claim and validating the design decision to incorporatean additional high-frequency projection path. Lastly, we replace our cross-axis Factorized integralkernel with a recent tensorized FNO (TFNO) (tucker decomposition with rank 0.02). WhileTFNO effectively models long-range dependencies, matching our parameter count required aggres-sive decomposition, which significantly degraded performance. This comparison underscores theadvantage of our physically grounded cross-axis factorized kernel.",
  ": Generalize to unseen wavelength in inter-ested C-band (1.53-1.565) and outside C-band": "is important to test the generalization for out-of-distribution data with unseen parameters.We re-generate photonic devices with differentdevice configurations (size, etched region, etc.)and unseen frequencies in our interested wave-length range (1.53-1.565 m), i.e., C-band. Asshown in , our PACE model generalizeswell on unseen simulation frequency and newdevices. It is a vital test to prove the useful-ness of PACE in helping device design withinan interested wavelength range. We also testthe accuracy outside the C-band, where PACEshows good accuracy on neighboring wave-lengths while holding a 10-15% error at a further range. This is expected since wave propagation issensitive to frequency. It can be mitigated by incorporating sampled wavelengths into training. NeurOLightPACE Large error in low- freq.Fail to track high- freq.",
  ": The radial energy spectrum of predicted fields from NeurOLight and PACE. NeurOLight fails toalign precisely with the targeted field in both low-frequency and high-frequency parts": "Are PACE a general enhancer module for Fourier-type operator? We further investigate whetherour new PACE operator is a general enhancer for other Fourier operators, rather than a dedicated mod-ule for our own model architecture. We randomly insert four PACE blocks into Facztoried FNO and test the error on Metaline3x3 and Etched MMI 3x3 benchmarks, showing up to 28% error re-",
  "duction as shown in with much fewer parameters": "Comparison with operator for multi-scale PDE. Notic-ing that our problem shares similar complexities in solv-ing multi-scale PDEs with neural operator , wefurther compare our approach with the recent method that alternates Fourier operator with dilated convolutionlayer to better capture local details. On the etched MMI3x3 dataset, we implement a 14-layer model with alter-nating NeurOLight block and dilated convolution layer.It yields a 1.73 M parameter count similar to our PACEbut shows a 17.4 N-MSE error, much worse than ours(10.59). Spectrum of the predicted field: The predicted field spectrums of PACE and NeurOLight arein . Although NeurOLight uses the same frequency modes, it fails to align well with boththe low-frequency and high-frequency regions. PACE excellently aligns with the baseline spectrumcompared to NeurOLight,",
  "Conclusion": "In this work, we pace the simulation fidelity on highly challenging complicated photonic devicesto an unprecedented level. Our novel cross-axis factorized PACE operator enables the neural PDEsolver to capture complex relationships between local device structures and the resulting complexoptical field across the entire simulation domain. Furthermore, we introduce a cascaded two-stagelearning paradigm to further enhance the prediction quality when one sole PACE is not sufficient,demonstrating better quality enhancement than simply adding more layers. Experiments demon-strate that PACE achieves a remarkable 73% reduction in error with 50% fewer parameters comparedto previous methods. Our method also offers significant speedup (11.8x to 577x) over traditionalnumerical solvers. Looking forward, we aim to integrate our model into the design optimizationloop for photonic devices and circuits. Moreover, we want to emphasize that our proposed operatorand learning strategy are not dedicated to photonic cases but generally applied to challenging PDEproblems with similar problem characteristics, e.g., multi-scale PDE problems. Limitations and Broader Impact.This work focuses on steady-state optical field solutions usingthe FDFD method. Exploring the effectiveness of operator learning for the Finite-Difference TimeDomain (FDTD) can be an interesting direction. Moreover, the FFT kernels on GPU are not fully op-timized . Employing specialized, optimized FFT kernels can unlock even greater computationalefficiency on GPUs, further accelerating the neural PDE solver.",
  "Shuhao Cao. Choose a transformer: Fourier or galerkin. Advances in neural informationprocessing systems, 34:2492424940, 2021": "Mingkun Chen, Robert Lupoiu, Chenkai Mao, Der-Han Huang, Jiaqi Jiang, Philippe Lalanne,and Jonathan Fan. Physics-augmented deep learning for high-speed electromagnetic simula-tion and optimization. Nature, 2021. Johannes Feldmann, Nathan Youngblood, Maxim Karpov, Helge Gehring, Xuan Li, MaikStappers, Manuel Le Gallo, Xin Fu, Anton Lukashchuk, Arslan Raja, Junqiu Liu, DavidWright, Abu Sebastian, Tobias Kippenberg, Wolfram Pernice, and Harish Bhaskaran. Parallelconvolutional processing using an integrated photonic tensor core. Nature, 2021. Chenghao Feng, Jiaqi Gu, Hanqing Zhu, Shupeng Ning, Rongxing Tang, May Hlaing, JasonMidkiff, Sourabh Jain, David Z Pan, and Ray T Chen. Integrated multi-operand optical neuronsfor scalable and hardware-efficient deep learning. Nanophotonics, 13(12):21932206, 2024. Chenghao Feng, Jiaqi Gu, Hanqing Zhu, Zhoufeng Ying, Zheng Zhao, David Z Pan, and Ray TChen. A compact butterfly-style silicon photonic-electronic neural chip for hardware-efficientdeep learning. ACS Photonics, 9(12):39063916, 2022.",
  "Daniel Y. Fu, Hermann Kumbong, Eric Nguyen, and Christopher R. FlashFFTConv: Efficientconvolutions for long sequences with tensor cores. 2023": "Jiaqi Gu, Chenghao Feng, Hanqing Zhu, Zheng Zhao, Zhoufeng Ying, Mingjie Liu, Ray TChen, and David Z Pan. Squeezelight: A multi-operand ring-based optical neural network withcross-layer scalability. IEEE Transactions on Computer-Aided Design of Integrated Circuitsand Systems, 42(3):807819, 2022. Jiaqi Gu, Zhengqi Gao, Chenghao Feng, Hanqing Zhu, Ray Chen, Duane Boning, and DavidPan.Neurolight: A physics-agnostic neural operator enabling parametric photonic devicesimulation. Advances in Neural Information Processing Systems, 35:1462314636, 2022. Jiaqi Gu, Hanqing Zhu, Chenghao Feng, Zixuan Jiang, Ray T Chen, and David Z Pan.M3icro: Machine learning-enabled compact photonic tensor core based on programmablemulti-operand multimode interference. APL Machine Learning, 2(1), 2024.",
  "Mario Ohlberger and Barbara Verfurth.A new heterogeneous multiscale method for thehelmholtz equation with high contrast. Multiscale Modeling & Simulation, 16(1):385411,2018": "Bogdan Raonic, Roberto Molinaro, Tim De Ryck, Tobias Rohner, Francesca Bartolucci, RimaAlaifari, Siddhartha Mishra, and Emmanuel de Bzenac. Convolutional neural operators forrobust and accurate learning of pdes. Advances in Neural Information Processing Systems, 36,2024. Bhavin J. Shastri, Alexander N. Tait, T. Ferreira de Lima, Wolfram H. P. Pernice, HarishBhaskaran, C. D. Wright, and Paul R. Prucnal. Photonics for Artificial Intelligence and Neu-romorphic Computing. Nature Photonics, 2021. Yaocheng Shi, Yong Zhang, Yating Wan, Yu Yu, Yuguang Zhang, Xiao Hu, Xi Xiao, HongnanXu, Long Zhang, and Bingcheng Pan. Silicon photonics for high-capacity data communica-tions. Photonics Research, 10(9):A106A134, 2022. Kim Stachenfeld, Drummond Buschman Fielding, Dmitrii Kochkov, Miles Cranmer, TobiasPfaff, Jonathan Godwin, Can Cui, Shirley Ho, Peter Battaglia, and Alvaro Sanchez-Gonzalez.Learned simulators for turbulence. In International conference on learning representations,2021.",
  "Zi Wang, Lorry Chang, Feifan Wang, Tiantian Li, and Tingyi Gu. Integrated photonic meta-system for image classifications at telecommunication wavelength. Nature communications,13(1):2131, 2022": "Haixu Wu, Tengge Hu, Huakun Luo, Jianmin Wang, and Mingsheng Long. Solving high-dimensional pdes with latent spectral models. In Proceedings of the 40th International Con-ference on Machine Learning, ICML23. JMLR.org, 2023. Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang,Yanyan Lan, Liwei Wang, and Tieyan Liu. On layer normalization in the transformer architec-ture. In International Conference on Machine Learning, pages 1052410533. PMLR, 2020.",
  "H. H. Zhu, J. Zou, H. Zhang, Y. Z. Shi, S. B. Luo, et al. Space-efficient optical computing withan integrated chip diffractive neural network. Nature Communications, 2022": "Hanqing Zhu, Jiaqi Gu, Hanrui Wang, Zixuan Jiang, Zhekai Zhang, Rongxing Tang, Cheng-hao Feng, Song Han, Ray T Chen, and David Z Pan. Lightening-transformer: A dynamically-operated optically-interconnected photonic transformer accelerator. In 2024 IEEE Interna-tional Symposium on High-Performance Computer Architecture (HPCA), pages 686703.IEEE, 2024. Hanqing Zhu, Keren Zhu, Jiaqi Gu, Harrison Jin, Ray T Chen, Jean Anne Incorvia, and David ZPan. Fuse and mix: Macam-enabled analog activation for energy-efficient neural acceleration.In Proceedings of the 41st IEEE/ACM International Conference on Computer-Aided Design,pages 19, 2022.",
  "A.1Dataset Generation": "We generate our customized etched MMI and Metaline dataset using the open-source FDFD simu-lator angler . For each type of device, we random sample 5.12 K device configuration followingthe Tab. 4, and generate single-source data by sweeping the input light over the input ports. We ran-domly sample the devices physical dimension, input/output waveguide width and input light sourcefrequencies. For etched MMIs, we randomly sample etched cavity sizes, ratios (which determinethe number of cavities in the MMIs), and permittivities in the controlling region. For Metaline, werandomly sample the metaatom physical dimension with a fixed total number of 20.",
  "A.2Training Settings": "We implement all models and training logic in PyTorch 2.3. We use A100 and A6000 to train ourmodels and report the latency running on a single A100 GPU with torch.compile. For Bench-marking FDFD simulator performance, we use the Intel 12th Gen Intel(R) Core(TM) i7-12700 with20 CPU cores. We split all single-source examples into 72% training data, 8% validation data, and20% test data. For training, we set the number of epochs to 100 with an initial learning rate of 0.002, cosine learningrate decay, and a mini-batch size of 4. We use adamW as the optimizer with the weight decay 1e-5to avoid over-fitting. Moreover, we apply stochastic network depth with a linear scaling strategy.",
  "A.3Model Designs": "To ensure a comprehensive evaluation, we compare our proposed model against recently publishedand available SOTA baselines, encompassing various architectural paradigms such as the Fourier-operator models, attention-based models, and latent space methods. To maintain fairness in com-parison, we constrain the parameter count of all models to be under 4 M in most cases and useopen-sourced implementations.",
  "Dil-ResNet . We use the implementation in open-sourced pdearena 3, with a channel num-ber of 128 and enabled normalization. The total parameter count for Dil-ResNet is 4.17 million": "FNO-2d . We use 6 2-D FNO layers, and the Fourier modes are set to (#Mode =32, #Mode=10) for the etched MMI dataset and (#Mode =16, #Mode =16) for the Metaline dataset, resultingin the total parameter count as 3.99 M or 3.21 M. We use the implementation4. Tensorized FNO-2d . One obvious advantage is that our model features low parameters. Hence,we will compare it with tensorized FNO, which compresses the model weights with the tensor de-composition method. We adopt the implementation in5 and use the model designed for the darcyflow problem. We use 5 2-D FNO layers, and the Fourier modes are set to (#Mode =40, #Mode =20)for the etched MMI dataset and (#Mode =24, #Mode =24) for the Metaline dataset. Tucker decom-position is used with a rank of 0.42. The total parameter count is 2.25 M and 1.58 M, respectively,for the two types of datasets. F-FNO-2d. For factorized Fourier neural operator (F-FNO), we use 13 F-FNO layers with a channelnumber of 52. The Fourier modes are set to (#Mode =70, #Mode =40) for etched MMI dataset and(#Mode =36, #Mode =36) for Metaline dataset, leading to the total parameter count as 4.02 M and2.68M. The FNO-2d implementation is referred to6. We use the same projection head as ours.",
  "U-NO-2d . For U-shaped neural operators, we follow the implementation7. We use their 11-layerUNO with a base channel 24. The total parameter count is 4.38 M": "Attention-based operator . For the attention-based neural operator, we choose the most recentTransformer-type model and use its official implementation8. We use 3 layer-attention with 12heads. The total dimension is 128. The total parameter count is 3.75 M. Latent Spectral Method . For the latent spectral method, we use the original implementationin9. The number of bases is set to 12, and the channel number is 32. The patch size is set to 44.The total parameter count is 4.8 M. NeuroLight . We use the same implementation10 in the original paper with 16 layers. Forthe etched MMI dataset, the Fourier modes are set to (#Mode =70, #Mode =40). For the Metalinedataset, Fourier modes are set to (#Mode =36, #Mode =36). The total number of parameters is2.11M and 1.49 M for the two cases. PACE. For our proposed PACE, we use 12 layers, with the first two being the same factorized layersin , since we found it is important first to generate some meaningful wave patterns and then doglobal information swapping. The Fourier modes are set to (#Mode =70, #Mode =40). We use thesame convolution stem in to extract information before going through the feature propagatorand the same projection head. The total number of parameters is 1.73M. For the second stagePACE-II model, we use 8 layers with all being PACE operators, where Fourier modes are set to(#Mode =100, #Mode =40) For the Metaline dataset, we solely use a 12-layer PACE with Fouiermodes being (#Mode =36, #Mode =36).",
  "A.5Ablation Study of Double Skip and Pre-Normalization": "We further investigate whether the observed improvements in accuracy are attributed to the incorpo-ration of double skip connections and pre-normalization, which were incorporated into our model tostabilize it in deeper layers with better generalization. We add these two techniques to NeurOLightand compare them with ours PACE in Tab.. 6. The double skip and pre-normalization can make themodel generalize well for test data, while the training error is slightly improved as normalization",
  "|z1, z2|1 = |r1 cos 1 r2 cos 2| + |r1 sin 1 r2 sin 2|.(10)": "In the complex plane, optimizing MAE equates to minimizing the summed L1 distances of the realand imaginary components. However, the L1 distance is rotation-variant. A simple rotation of thetwo complex numbers on the plane results in changing L1 distance, as shown in , while thetrue distance does not alter. Therefore, it is not an appropriate metric as it cannot accurately measureproximity in the complex plane. In this way, we use L2 distance in the loss (mean squared loss) thatis rotation invariant, as proved in corollary A.1, which exactly captures the distance in the complexplane. L1 distance",
  "A.9Visualization of feature map before/after non-linear activation in our explicitlydesigned high-frequency projection path": "We visualize the first 6 channels of feature maps before and after the nonlinear activation in thelast PACE layer by showing them in the frequency domain. As shown in , the nonlinearactivation can ignite high-frequency features, which confirms our claim and validates our designchoice of injecting an extra high-frequency projection path.",
  "A.10Visualization of prediction": "We provide visualization figures on etched MMI 3x3 devices in and metaline devices in. We provide the predicted fields (a), the groud-truth field (a) and the residual er-ror (a) (a) of Dil-ResNet, Facztoried FNO, NeurOLight and our PACE. For etched MMItest cases, we show both the single 12-layer PACE model and the joint 20-layer model PACE-I +PACE-II. Our PACE shows much better prediction results with a near-black error map compared toother baseline methods. Before nonlinear After nonlinear : Frequency-domain visualization of feature map before and after non-linear activation in the lastPACE block(The center represents low frequency). The pattern is shifted to the center to understand the fre-quency content better.",
  ". Crowdsourcing and Research with Human Subjects": "Question: For crowdsourcing experiments and research with human subjects, does the pa-per include the full text of instructions given to participants and screenshots, if applicable,as well as details about compensation (if any)?Answer:[NA] 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with HumanSubjectsQuestion: Does the paper describe potential risks incurred by study participants, whethersuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)approvals (or an equivalent approval/review based on the requirements of your country orinstitution) were obtained?Answer:[NA]"
}