{
  "Abstract": "Recent advancements in text-to-image diffusion models have enabled the person-alization of these models to generate custom images from textual prompts. Thispaper presents an efficient LoRA-based personalization approach for on-devicesubject-driven generation, where pre-trained diffusion models are fine-tuned withuser-specific data on resource-constrained devices. Our method, termed HollowedNet, enhances memory efficiency during fine-tuning by modifying the architectureof a diffusion U-Net to temporarily remove a fraction of its deep layers, creating ahollowed structure. This approach directly addresses on-device memory constraintsand substantially reduces GPU memory requirements for training, in contrast toprevious methods that primarily focus on minimizing training steps and reducingthe number of parameters to update. Additionally, the personalized Hollowed Netcan be transferred back into the original U-Net, enabling inference without addi-tional memory overhead. Quantitative and qualitative analyses demonstrate that ourapproach not only reduces training memory to levels as low as those required forinference but also maintains or improves personalization performance compared toexisting methods.",
  "Introduction": "Recent research on text-to-image (T2I) diffusion models , which generate high-resolution imagesfrom text prompts, has increasingly focused on personalizing and customizing these generative modelseffectively . A primary approach, termed subject-driven generation , involves fine-tuningpre-trained diffusion models with a few user-specific images to generate varied representations of asubject using simple text prompts. This allows users to create personalized images of specific subjects,such as family, friends, pets, or personal items, with preferred appearances, backgrounds, and styles.Such capabilities enable creative applications including art renditions, property modifications, andaccessorization. From a practical standpoint, implementing subject-driven generation on-device offers significantbenefits in efficiency and privacy. By operating independently of congested cloud servers or networks,users can generate personalized images anywhere at no additional cost and do not need to compromisetheir privacy as all data and personal information remain on the device.",
  "arXiv:2411.01179v1 [cs.CV] 2 Nov 2024": "Despite extensive research aimed at efficiently personalizing diffusion models, limited attention hasbeen paid to memory I/O, a critical bottleneck in on-device learning. Recent studies have mainlyexplored two strategies: (1) decreasing the number of training steps and (2) reducing the number ofupdating parameters. The first methods utilize additional large pre-trained models to generatea set of personalized Low-Rank Adaptation (LoRA) parameters , text embeddings, or imageprompts from a user-specific image. This strategy provides a better initial setup for personalizing thediffusion models, effectively reducing required training steps. Some models even supportzero-shot personalization, although they underline that further fine-tuning can enhance personalizationquality and address failure cases. Nonetheless, these methods are not viable for environments withseverely limited computational resources, as they necessitate additional inference using large pre-trained models (e.g., 2.7B parameters for BLIP-2 in BLIP-Diffusion and 2.5B for apprenticemodels in SuTI ), which are substantially larger than standard diffusion models (e.g., 1B forStable Diffusion v2 ), making their application challenging in on-device settings. The second approach , often involving LoRA, aims to reduce the number of updating parametersby limiting updates to specific layers or decomposing weight matrices. However, even with fewerparameters to update, these parameters reside within large pre-trained models, and thus the backwardpass through the large models is required to compute gradients. Given limited computational resources,where even simple inference tasks with diffusion models can strain GPU memory, performingbackpropagation while keeping the entire diffusion model in GPU memory remains a significantlimitation. A promising approach to address these challenges is side-tuning , which fine-tunes a smallerauxiliary network rather than directly updating the parameters of a large pre-trained network. Thismethod significantly reduces the heavy memory costs associated with computing backpropagationon the larger network. Particularly for Natural Language Processing (NLP) tasks, Ladder SideTuning (LST) has proven effective, reducing the memory costs required for fine-tuning largelanguage models (LLMs) by 69 percent. However, applying LST directly to diffusion U-Netspresents significant challenges. Unlike transformer layers in LLMs, which maintain consistent inputand output dimensions, diffusion U-Nets have varying spatial dimensions and channels, as well asskip-connections across different blocks. Additionally, the requirements for structural pruning andweight initialization to build side-tuning networks further complicate the rapid adaptability of LST topersonalization tasks across different subjects and domains. To this end, we introduce a novel personalization technique called Hollowed Net, which is illustratedin . Based on our observation that deep layers in the middle of diffusion U-Nets play significantlyless important roles than the rest of the layers, we propose to fine-tune LoRA parameters for thepersonalization using Hollowed Net, a layer-pruned U-Net featuring a central hollow, which isconstructed by temporarily removing the middle deep layers from the pre-trained diffusion U-Net.By utilizing the symmetrical \"U-shape\" architecture of the diffusion U-Net, we avoid complicatedprocesses of applying structural pruning and weight initialization to build a side network, and neitheradditional models nor extensive pre-training with large datasets are required. By fine-tuning LoRA parameters using Hollowed Net, we can significantly reduce the memoryneeded for storing model weights in GPU. Once the LoRA parameters are fine-tuned with HollowedNet, they can be seamlessly transferred back to the original Diffusion U-Net for inference, withoutrequiring any additional memory beyond the small set of transferred parameters. Our experimentsdemonstrate that Hollowed Net enables achieving performance that is comparable to or better thanthe direct fine-tuning with LoRA, while using 26 percent less GPU memory, which is only 11 percentincreased GPU memory relative to an inference. To the best of our knowledge, Hollowed Net is the first technique that addresses subject-drivengeneration in terms of memory efficiency. Our method shows how T2I diffusion models can befine-tuned under extremely limited computational resources with as low GPU memory as required forinference. Furthermore, it is important to note that our method does not preclude the use of previouslydescribed strategies for efficient personalization. Both enhanced parameter-efficient strategies andimproved initializations with additional pre-trained models can be integrated with our approach tofurther increase efficiency according to given resource constraints.",
  ": The LoRA personalization with Hollowed Net for resource-constrained environments. Theinput image is from the DreamBooth dataset": "We introduce Hollowed Net, a novel personalization technique for T2I diffusion modelsunder limited computational resources. Our method significantly reduces the memorydemands on GPU to levels as low as those required for inference, while maintaining ahigh-fidelity personalization capacity. This demonstrates its potential as a feasible on-devicelearning solution for resource-constrained devices. Our method provides a scalable and controllable solution for on-device learning. As thismethod does not require any additional models or pre-training with large datasets, it is easilyscalable to other architectures such as SDXL and Transformers. Moreover, we can simplyadjust the fraction of hollowed layers to control the trade-offs between performance andmemory requirements, depending on the target application and resources. Unlike previous side-tuning methods, Hollowed Net does not need to be retained forinference. The LoRA parameters fine-tuned with Hollowed Net can be seamlessly transferredback to its original network, enabling inference with no additional memory cost.",
  "Efficient Personalization of T2I Diffusion Models": "Recent research on the personalization of T2I diffusion models has introduced various methodsto fine-tune the models for generating diverse images of user-specific subjects from a few givenimages. Two foundational works in this area are Textual Inversion and DreamBooth . TextualInversion aims to learn new text embeddings to represent a given subject, while DreamBooth proposes fine-tuning an entire diffusion model to align the subject with a unique token. Building on these foundational works, recent research has focused on enhancing the efficiency of thispersonalization process, primarily through two approaches. The first approach involves decreasingthe number of training steps, mostly by utilizing an additional large pre-trained model. A popularmethod is to use a pre-trained image/multi-modal encoder to generate personalized text embeddings or image prompts from a user-specific image . Other recent works propose utilizinga set of pre-optimized LoRA parameters or millions of fine-tuned expert models to pre-initialize forefficient fine-tuning or enable zero-shot generation with in-context learning. While these modelsdemonstrate significant reductions in the number of training steps, the requirement for additional largepre-trained models limits their application to on-device settings. Moreover, models with zero-shotpersonalization capacities cannot be a one-size-fits-all solution for addressing differenttypes of user-subject prompts. These models often struggle with flexibility in editing subjects ormaintaining subject fidelity, and in these cases, additional fine-tuning with specific subjects is neededto further enhance their personalization capacity . On the other hand, another stream of work adapts parameter-efficient fine-tuning (PEFT) approaches.These methods demonstrate significant reductions in the number of training parameters by limitingupdates to a small subset of model weights in cross-attention layers or further reducing theupdating parameters by applying singular vector decomposition to weight matrices . However,these methods are still limited in environments with extremely low computational resources, as theyrequire backpropagation over large diffusion models and do not reduce memory usage from the modelweights. Therefore, it is crucial to explore new approaches for personalizing T2I diffusion models inresource-limited settings, as we propose with our novel method, Hollowed Net. Notably, our methodcan be integrated with previously discussed techniques to further improve efficiency based on specificresource constraints.",
  "Fine-Tuning with Side Networks": "The idea of of side-tuning has been introduced by Zhang et al. , proposing the training ofa lightweight \"side\" network instead of directly fine-tuning a pre-trained network for adaptation.In terms of efficiency, Cai et al. has demonstrated an additional lightweight residual modulecan reduce memory overhead associated with the activations of the original network. Similarly,AuxAdapt has shown that a small auxiliary network can be fine-tuned to adjust the mainnetworks decisions, enabling efficient test-time adaptation for video semantic segmentation tasks. In the context of generative models, LST has demonstrated the effectiveness of side networks fordifferent NLP tasks with LLMs by introducing a small side network that takes intermediate activationsof the main network as input via shortcut connections. However, directly applying LST to diffusionU-Nets poses challenges due to varying spatial dimensions, channel sizes, and skip-connections acrossblocks, unlike the consistent dimensions in transformer layers of LLMs. Furthermore, the structuralpruning and specific weight initialization required to construct side-tuning networks complicateLSTs adaptability for personalized tasks across a range of subjects and domains.",
  "Layer Pruning of Large Generative Models": "Several concurrent works demonstrate that layer-pruning methods can be applied to generativemodels, particularly for NLP tasks. Gromov et al. suggest that for fine-tuning LLM models, upto 40% of deep layers can be removed, while still achieving comparable results. The authors proposethat the optimal block of layers to prune can be selected based on similarity across layers. Similarly,Kim et al. also propose a depth-pruning approach by evaluating block-level importance. These approaches differ from ours due to the distinct characteristics of LLMs versus diffusion U-Nets.The aforementioned approaches involve the complete removal of deep layers for both fine-tuningand inference, considering that those layers store less critical knowledge. However, our study findsthat the deep layers of diffusion U-Nets may be less involved with personalization but still containcrucial high-level image features for generating high-fidelity images. Thus, their removal can leadto severe performance degradation, even with additional pre-training , as shown in Appendix A.This highlights the importance of our two-stage fine-tuning strategy, which excludes layers duringfine-tuning to reduce memory overhead while preserving the knowledge from these excluded layersthroughout both training and inference stages.",
  ": Analysis of the LoRA weight change before and after personalization, per block of U-Net": "foundational T2I model, pre-trained on large amount of text-image pairs (P, x), where we haveimage x and associated text prompt P. The SD contains the following components: (a) Autoencoderconsisting of the encoder-decoder pair (E, D), (b) Text Encoder as CLIP ET (), and (c) ConditionalDiffusion Model as U-Net (). The encoder E() processes an image x into a latent space z = E(x),and the decoder is used to reconstruct the input image from latent z such that x D(z). The diffusionprocess of SD is conducted in the latent space. For a randomly sampled noise N(0, I) at timestep t, the standard scheduler produces a noisy latent code zt = tz + t, where t and t arecoefficients controlling the noise schedule. The conditional diffusion model is trained using thefollowing de-noising objective:",
  "minEP,z,,t[|| (zt, t, ET (P))||22].(1)": "After the training is carried out, the conditioned model () is used to predict the noise by usingthe conditional embedding ET (P) and time step t as input. To personalize diffusion models forsubject-driven generation introduced by , the same loss is used except that the data is sampledfrom user-specific subjects such as dog, person, backpack, and etc. For the prompt, a special identifierS is used and described as \"a S person\", \"a S backpack\", etc. For regularization, introducesan additional class-specific prior preservation loss term, written as",
  "where pr is the ground truth noise for the data generated using the frozen pre-trained diffusion modelwith prompts Ppr described more generic as \"a person\", \"a backpack\", and etc": "The diffusion U-Net can be fully fine-tuned, but it is also possible to fine-tune only a subset ofparameters with LoRA for better efficiency. In LoRA, network weight residuals W arefine-tuned instead of the full weights W. For the fine-tuning of W, it is further decomposed intolow-rank matrices A and B such that W = AB. Since A and B are low-rank matrices, the totalnumber of parameters to optimize in W is significantly smaller than in W.",
  "Methodology": "In this section, we describe the details of our novel memory-efficient personalization technique,Hollowed Net, and its fine-tuning strategy. We begin by identifying less significant layers forpersonalization from diffusion U-Nets. Based on these observations, we explain how to constructHollowed Net from a pre-trained U-Net. Next, we present our fine-tuning and inference processes formemory-efficient personalization of T2I diffusion models.",
  "Analysis of the LoRA Weight Changes per Block of U-Net": "To achieve the goal of reducing the required memory for fine-tuning a diffusion model, we firstidentify less significant layers in the diffusion U-Net for personalization. Similar to Li et al. ,Kumari et al. and Shah et al. , we analyze the LoRA weight changes W in the fine-tunedmodel for each block:",
  "i=1|wi wi|,(3)": "where w and w respectively represent the weights before and after personalization, and n is thetotal number of weights in a specific block. This represents the average weight change per elementi. shows the analysis of the weight changes W before and after personalization for eachblock of U-Net: (a) for all subjects from the DreamBooth dataset and (b) for all subjects fromthe CustomConcept101 dataset by fine-tuning Stable Diffusion v2.1 diffusion model for 1000steps with a learning rate of 1e-4. The x-axis shows the changes in LoRA weights before and afterpersonalization, while the y-axis of each plot represents the specific U-Net blocks. For each dataset,we average the weight changes across subjects and provide error bars to indicate the statisticalvariance within each dataset. From the figures, we observe that the average weight changes tend to be close to zero around the cen-tral blocks and become increasing for the layers farther from the mid_block. This demonstrates thatthe blocks around the center are less involved in the personalization compared to those at the begin-ning and end of the U-Net (e.g., down_blocks.0, down_blocks.1, up_blocks.2, and up_blocks.3).We leverage this characteristic for designing Hollowed Net.",
  "Hollowed Net": "Based on the aforementioned observations, we propose fine-tuning a layer-pruned U-Net, whichwe refer to as Hollowed Net, instead of directly fine-tuning the entire diffusion model. The coreconcept of Hollowed Net involves removing deep layers that are not vital for personalization froma pre-trained diffusion U-Net. This strategy decreases the need to store the entire model in GPUmemory, thereby reducing the memory cost associated with the models weights. However, unlike transformer layers in large language models, where input and output maintainthe same data structure, the alterations in spatial and channel dimensions in U-Net architecturescomplicate the removal of its deep layers in the middle. To address this, we utilize the symmetrical\"U-shape\" architecture of the diffusion U-Net, where each down-block layers output is concatenatedwith a corresponding up-block layers input via a skip-connection. This design permits us to select anyup-block layer skip-connected to a down-block layer and hollow out the middle layers between thepair, ensuring that the processed information from the remaining down-blocks can still be transferredto the remaining up-blocks without the need for additional projection layers to adjust for dimensionaldifferences. The missing input for the upper layer, due to the removal of the middle layers, is replacedwith the pre-computed output from the full diffusion U-Net, which is illustrated in the next section.",
  "LoRA Personalization with Hollowed Net": "To optimize GPU memory utilization, we propose a two-stage fine-tuning strategy: (1) pre-computingintermediate activations of the original diffusion U-Net and (2) fine-tuning the Hollowed Net using thepre-computed activations, as shown in the upper and bottom half of , respectively. Initially, we : The quantitative comparisons of fine-tuning methods with three evaluation metrics. Thenumber of parameters are the ones held in GPU memory during fine-tuning stage. The results areobtained by averaging over four runs with different seeds (standard deviation is added in a small-sizedtext).",
  "Hollowed Net0.238T2.004T0.920TLoRA FT-2.148T0.716T": "conduct a forward pass with a pre-trained diffusion model for the specified number of pre-computingsteps. During each step, given input images and sampled noise, we calculate and store intermediateactivations in the data storage, which serve as inputs for the upper-block layer of the Hollowed Net.We also store the sampled noises, time steps, and the IDs when there are multiple user images. Once the data from the pre-trained model is pre-computed, we fine-tune the Hollowed Net by loadingdata from data storage, thereby avoiding the need to keep the original model in GPU memory. Tofurther improve efficiency, we apply LoRA fine-tuning for the Hollowed Net instead of updatingentire parameters. The reduced number of parameters of the Hollowed Net decreases the requiredGPU memory, satisfying the devices low memory I/O threshold and computational load duringbackpropagation. Additionally, our inference process ensures that both the original diffusion model and Hollowed Netare not simultaneously maintained on GPU. Unlike side-tuning networks that differ inarchitecture and parameters from their original models, Hollowed Net maintains the same architecturesand parameters as the original diffusion U-Net, except for the removed middle layers. Thus, the LoRAparameters fine-tuned on Hollowed Net can be seamlessly transferred to the corresponding layers inthe original U-Net. As depicted in , there are two inference paths, respectively corresponds toeach stage of fine-tuning. The first path (green line) represents the process of computing intermediateactivations without using LoRA, aligning with the pre-computing stage. The second path (red line)involves using personalized LoRA parameters, which matches the application of these parametersfor generating personalized images during the fine-tuning stage. By sequentially executing thesepaths, we enable personalized generation using the transferred LoRA parameters without requiringadditional memory beyond the small set of LoRA parameters.",
  "Experimental Settings": "We conduct experiments following the protocol proposed in DreamBooth . We use a total of 131subjects for experiments, utilizing both the DreamBooth and CustomConcept101 datasets. TheDreamBooth dataset includes 30 image sets from 15 different classes, each containing 4-6 images ofa given subject. The subjects are divided into living subjects and objects, and 25 different prompts areassigned based on this division. Meanwhile, the CustomConcept101 dataset includes 101 image sets,each containing 3-15 images of a given subject. The subjects consist of 15 different large categories,with 20 unique prompts assigned to each category. For evaluation, four images with different fixedrandom seeds are generated per subject per prompt for both datasets.",
  "INPUT IMAGE": ": Qualitative generation results of Hollowed Net with different subjects and prompts. Theupper half are the examples from the DreamBooth dataset , and the lower half are the examplesfrom the CustomConcept101 dataset . We adopt the three evaluation metrics from : DINO and CLIP-I for subject fidelity and CLIP-Tfor prompt fidelity. DINO and CLIP-I are the average pairwise cosine similarities between featureembeddings of the real and generated images, using DINO ViT-S/16 and CLIP ViT-B/32, respectively.As DINO is more sensitive to differences between subjects of the same class due to its training oninstance discrimination, the DINO score is considered the preferred metric for measuring subjectfidelity. The CLIP-T score is the average cosine similarity between text prompt embeddings and imageCLIP embeddings. We use the Stable Diffusion v2.1 diffusion model . Following DreamBooth ,we use a prior preservation loss with 1000 pre-generated class samples. LoRA is applied forthe cross and self-attention layers and fine-tuned for 1000 steps. We use AdamW optimizer withthe learning rate of 1e-5 for full-finetuning and 1e-4 for the others. Assuming a resource-constrainedenvironment, we use a batch size of 1 and do not update the pre-trained text encoder, while textembeddings are pre-computed before fine-tuning.",
  "Results": "In this section, we present the results of our proposed Hollowed Net to evaluate its effectivenessin terms of both memory efficiency and personalization performance. We conduct experimentswith Hollowed Net, applying a hollowed fraction of 39.2%. Architectural details are provided inAppendix B. Ablation studies on different fractions of hollowed layers can be found in Sec. 5.3. Inthe main results, the rank of Hollowed Net is fixed to 128. Experimental results on different ranks arepresented in Appendix C. : Analysis of different fractions of hollowed layers. For all figures, the x-axis representsthe fractions of layers removed from the pre-trained diffusion U-Net. The y-axis corresponds to themetric used for each figure. Quantitative EvaluationThe quantitative results are displayed in . For comparison base-lines, we implement full fine-tuning (Full FT) and LoRA fine-tuning (LoRA FT) methods with rank128 and rank 1 . We find that while Full FT results in slightly higher performance than othermethods, particularly in terms of DINO, the differences between Full FT and Hollowed Net are notsignificant as it is within the range of standard deviations of Full FT results across different seeds.Moreover, Full FT requires more than 16GB of GPU memory which is nearly 4.7 times the memorycost of performing an inference with a diffusion U-Net. Clearly, this is not a feasible solution foron-device learning, where computation resources, especially memory I/O, are extremely limited. Our Hollowed Net demonstrates its superior memory efficiency based on a significant reductionin model size, requiring only 3.88GB of GPU memory usage for fine-tuning. This is only an 11%increase compared to inference. Its personalization performance is comparable to or marginally betterthan that of LoRA fine-tuning using the same rank (r = 128), while LoRA requires a 50% increase inGPU memory compared to inference. Using the lowest rank of LoRA (r = 1) does not compete withHollowed Net either, as its memory efficiency is limited by the need to run backpropagation on theentire U-Net, even though the number of fine-tuning parameters is significantly small. Additionally,the use of a low number of fine-tuning parameters significantly degrades personalization capacity. For human evaluation, we conduct user studies with 40 participants, each completing a set of 25comparative tasks. In each task, participants are presented with a reference image, a prompt, and twogenerated images (A and B). They answer two questions: subject fidelity and text fidelity. Each pairof generated images, A and B, is created using Hollowed Net and LoRA FT, and the labels (A or B)are randomly assigned for each task. displays the results of these user studies. These findingsconfirm that users generally perceive the images generated by Hollowed Net and LoRA FT to besimilar in both subject fidelity and text fidelity, consistent with the main results presented in . Additionally, we include the analysis of computational loads for Hollowed Net and LoRA FT in. Each number corresponds to one step of each stage: one forward pass for pre-computingand inference and one forward+backward pass for fine-tuning. For the fine-tuning of Hollowed Net,1000 steps are required, totaling 2.004 1000 = 2004 TFLOPs. For pre-computing, we find 200pre-computed samples are sufficient to achieve high-fidelity results (see Appendix D for a detailedanalysis), requiring 0.238 200 = 47.6 TFLOPs of additional computation. Therefore, the totalcomputation required for training with Hollowed Net is 2004 + 47.6 = 2051.6 TFLOPs, which islower than 2.148 1000 = 2148 TFLOPs needed for LoRA FT. On the other hand, for running aninference pass, Hollowet Net requires approximately 0.204 TFLOPs more than LoRA FT, as it needsto repeat part of the early down-blocks to reproduce the path used in training. Qualitative EvaluationIn , we present qualitative generation results of Hollowed Net forvarious subjects and prompts. The upper half shows examples from the DreamBooth dataset, and thelower half displays examples from the CustomConcept101 dataset. These results demonstrate thatHollowed Net effectively captures the visual details of the target subjects, while maintaining hightext-image alignment for different types of applications including property modification, recontextu-alization, accessorization, and artistic rendition. Its ability enabling high-fidelity personalization withmemory costs as low as those of inference makes it an efficient solution for a range of on-device appli-cations with constrained computational resources. Additional qualitative examples with SDXL are included in Appendix E, illustrating the scalability of our approach in a larger model.",
  "Ablation Study on Fractions of Hollowed Layers": "Based on the symmetrical \"U-shape\" architecture of the diffusion U-Net, we can design differentHollowed Net architectures by selecting a different up-block layer skip-connected to a down-blocklayer and hollowing out the middle layers between the pair. presents experimental resultsacross different fractions of hollowed layers, ranging from around 10% to 85% of layers removed. In, we observe the peak GPU memory usage decreasing linearly with layer removal, as fewermodel weights need to be stored on the GPU during backpropagation. Analyzing the DINO andCLIP-I scores in (b) and (c), we find that the models capacity to preserve subject fidelityremains comparable to or slightly better than LoRA until around 39.2% of layers are removed,where memory cost reduces nearly to the level of inference. Beyond this threshold, however, subjectfidelity significantly diminishes, as fewer layers essential for personalization are included in theHollowed Net. This effect of hollowed layer fractions is also visible in the qualitative results in .Meanwhile, the CLIP-T score does not exhibit a general trend, except in cases of very high hollowedfractions, where the model is not capable of personalization, and thus generates images solely basedon a given prompt. However, note that the increase in CLIP-T remains marginal, as Hollowed Netswith low hollowed fractions also maintain a high capacity for text-image alignment.",
  "Conclusion": "In conclusion, our paper introduces a novel approach for on-device personalization through memory-efficient fine-tuning with Hollowed Net. Hollowed Net effectively leverages the architecture of thediffusion U-Net, enabling fine-tuning with significantly reduced memory costs by minimizing themodels size during fine-tuning without requiring any additional processes such as structural pruningor pre-training on large-scale datasets. However, we observe that, due to the use of non-personalizedprompts with the original network, the models performance can be sensitive to the granularity ofclass token definitions. For example, the DreamBooth dataset contains \"poop emoji\" images, forwhich the class token is very coarsely defined as \"toy\". In this case, non-personalized intermediateactivations generated with prompts using \"toy\" struggle to effectively correlate and generate \"poopemoji\" image. Therefore, a careful choice of fine-grained class tokens is necessary for the effectiveapplication of Hollowed Net. Additionally, it is worth noting that our methodology is orthogonal to existing different PEFTmethods and quantization methods . Thus, our approach offers substantial potentialfor further memory reduction, which is crucial for training under constrained computational resources.Furthermore, while our primary focus in this paper has been on image generation tasks, our methodis not limited to diffusion models and can be seamlessly extended to various NLP tasks with LLMs,which we leave for future work. We anticipate that Hollowed Net will be applied to a wide rangeof tasks requiring constrained computational resources, serving as an efficient solution for variouson-device applications.",
  "Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neuralinformation processing systems, 33:68406851, 2020": "Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjrn Ommer. High-resolutionimage synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computervision and pattern recognition, pages 1068410695, 2022. Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and DanielCohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion.arXiv preprint arXiv:2208.01618, 2022. Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusionmodels. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 38363847,2023. Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dream-booth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of theIEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2250022510, 2023, licensedunder CC BY 4.0. Viraj Shah, Nataniel Ruiz, Forrester Cole, Erika Lu, Svetlana Lazebnik, Yuanzhen Li, and Varun Jampani.Ziplora: Any subject in any style by effectively merging loras. arXiv preprint arXiv:2311.13600, 2023. Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-conceptcustomization of text-to-image diffusion. In Proceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 19311941, 2023. Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Wei Wei, Tingbo Hou, Yael Pritch, Neal Wadhwa, MichaelRubinstein, and Kfir Aberman. Hyperdreambooth: Hypernetworks for fast personalization of text-to-imagemodels. arXiv preprint arXiv:2307.06949, 2023. Rinon Gal, Moab Arar, Yuval Atzmon, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. Encoder-based domain tuning for fast personalization of text-to-image models. ACM Transactions on Graphics(TOG), 42(4):113, 2023. Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang, and Wangmeng Zuo. Elite: Encodingvisual concepts into textual embeddings for customized text-to-image generation. In Proceedings of theIEEE/CVF International Conference on Computer Vision, pages 1594315953, 2023. Wenhu Chen, Hexiang Hu, Yandong Li, Nataniel Ruiz, Xuhui Jia, Ming-Wei Chang, and William W Cohen.Subject-driven text-to-image generation via apprenticeship learning. Advances in Neural InformationProcessing Systems, 36, 2024. Dongxu Li, Junnan Li, and Steven Hoi. Blip-diffusion: Pre-trained subject representation for controllabletext-to-image generation and editing. Advances in Neural Information Processing Systems, 36, 2024.",
  "Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, andWeizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685,2021": "Ligong Han, Yinxiao Li, Han Zhang, Peyman Milanfar, Dimitris Metaxas, and Feng Yang. Svdiff: Compactparameter space for diffusion fine-tuning. In Proceedings of the IEEE/CVF International Conference onComputer Vision, pages 73237334, 2023. Jeffrey O Zhang, Alexander Sax, Amir Zamir, Leonidas Guibas, and Jitendra Malik. Side-tuning: a baselinefor network adaptation via additive side networks. In Computer VisionECCV 2020: 16th EuropeanConference, Glasgow, UK, August 2328, 2020, Proceedings, Part III 16, pages 698714. Springer, 2020.",
  "Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning ofquantized llms. Advances in Neural Information Processing Systems, 36, 2024": "Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang Frank Wang, Kwang-Ting Cheng, and Min-Hung Chen. Dora: Weight-decomposed low-rank adaptation. arXiv preprintarXiv:2402.09353, 2024. Yuzhang Shang, Zhihang Yuan, Bin Xie, Bingzhe Wu, and Yan Yan. Post-training quantization on diffusionmodels. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages19721981, 2023. Xiuyu Li, Yijiang Liu, Long Lian, Huanrui Yang, Zhen Dong, Daniel Kang, Shanghang Zhang, andKurt Keutzer. Q-diffusion: Quantizing diffusion models. In Proceedings of the IEEE/CVF InternationalConference on Computer Vision, pages 1753517545, 2023.",
  "AExperiments with Layer-Pruned Diffusion Models": "As shown in recent work , layer pruning involves the complete removal of selected layers, whichnecessitates extensive pre-training on large datasets to recover lost information and restore modelfunctionality. However, diffusion models often suffer from substantial performance degradationpost-pruning, as the lost information may not be fully recoverable through pre-training. presents experiments with BK-SDM models, layer-pruned SD models, using rank-128LoRA. Compared to the results in , these models achieve memory usage comparable toHollowed Net but show significant performance degradation. Despite extensive pre-training, theirperformance remains compromised. In contrast, Hollowed Net does not completely remove deep layers and requires no additionalpre-training. Instead, we temporarily exclude selected layers during fine-tuning while preservingessential information through a pre-computation stage. Notably, despite this added stage, the overallcomputational load for training Hollowed Net can remain more efficient than LoRA fine-tuning, asdiscussed in Sec. 5.2 and Appendix D.",
  "BArchitectural Details of Hollowed Net": "In this section, we present the architectural details of Hollowed Net. We leverage the skip connectionsinherent in the U-Net architecture to determine which layers to be removed during fine-tuning(hollowed). For our main results, we choose the third block of the down_blocks.2 (block 3-3), theentire down_blocks.3 (blocks 4-1 and 4-2), the entire mid_block (blocks 5-1 and 5-2), and the entireup_blocks.0 (blocks 6-1, 6-2, 6-3, and 6-4) to be hollowed, which corresponds to around 39.2% ofthe U-Nets parameters, as described in .",
  "CExperiments with Different Ranks": "In , we present the results using LoRA and Hollowed Net with different ranks (4 and 16)using the DreamBooth dataset. While the default rank of 4 in the diffusers library is often used, wehave found that it often oversimplifies personalization details or fails to effectively handle a rangeof challenging subjects and prompts. Increasing the rank from 4 to 16 improves subject fidelity.However, to achieve personalization quality comparable to full fine-tuning across all subjects andprompts, we find that the rank of 128 is necessary.",
  "EExperiments with SDXL": "To demonstrate the scalability of Hollowed Net, we present additional analysis and qualitativeexamples using SDXL . shows that similar patterns of weight changes are observablewith SDXL, as displayed in . In , we present qualitative examples of Hollowed Net andLoRA FT with the samples from the DreamBooth dataset using SDXL. Hollowed Net is applied byremoving the entire mid_block layers (410M parameters) of SDXL. The results show that HollowedNet achieves high-fidelity personalization results comparable to LoRA FT."
}