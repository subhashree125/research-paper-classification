{
  "Abstract": "Large, labeled datasets have driven deep learning methodsto achieve expert-level performance on a variety of medicalimaging tasks. We present CheXpert, a large dataset that con-tains 224,316 chest radiographs of 65,240 patients. We de-sign a labeler to automatically detect the presence of 14 ob-servations in radiology reports, capturing uncertainties inher-ent in radiograph interpretation. We investigate different ap-proaches to using the uncertainty labels for training convolu-tional neural networks that output the probability of these ob-servations given the available frontal and lateral radiographs.On a validation set of 200 chest radiographic studies whichwere manually annotated by 3 board-certied radiologists, wend that different uncertainty approaches are useful for differ-ent pathologies. We then evaluate our best model on a test setcomposed of 500 chest radiographic studies annotated by aconsensus of 5 board-certied radiologists, and compare theperformance of our model to that of 3 additional radiologistsin the detection of 5 selected pathologies. On Cardiomegaly,Edema, and Pleural Effusion, the model ROC and PR curveslie above all 3 radiologist operating points. We release thedataset to the public as a standard benchmark to evaluate per-formance of chest radiograph interpretation models.1",
  "Introduction": "Chest radiography is the most common imaging examina-tion globally, critical for screening, diagnosis, and manage-ment of many life threatening diseases. Automated chest ra-diograph interpretation at the level of practicing radiologistscould provide substantial benet in many medical settings,from improved workow prioritization and clinical decisionsupport to large-scale screening and global population healthinitiatives. For progress, there is a need for labeled datasetsthat (1) are large, (2) have strong reference standards, and (3)provide expert human performance metrics for comparison.",
  "arXiv:2411.01053v1 [cs.LG] 1 Nov 2024": "representations it learns. For instance, given three modalities a, b, and c, pairwise CLIP capturesdependencies between a and b, b and c, and a and c, yet cannot capture any conditional dependen-cies, such as between a and b given c. We show in .2 that even in a simple one-dimensionalcontrolled setting where the target b is perfectly predictable from a and c, CLIP performs no bet-ter than random chance. Effective contrastive learning for more than two modalities requires amodel-agnostic approach capable of learning modality-specific representationslike CLIPyet alsocaptures higher-order information between any number of modalitiesunlike CLIP. Methodological contributions.This paper presents Symile, a simple contrastive learning approachthat captures higher-order information between any number of modalities. Symile provides a flexible,architecture-agnostic objective for learning modality-specific representations. To develop Symilesobjective, we derive a total correlation estimator, employing a generalization of inner products tomore than two vectors that allows for the simultaneous contrasting of all modalities and enableszero-shot applications such as classification and retrieval. We then show that the representationsproduced by Symile for any set of modalities form a sufficient statistic for predicting the remainingmodalities not considered in the set. Because it targets total correlation, Symile captures strictly moreinformation than CLIP, guaranteeing performance that matches or surpasses CLIP, except in caseswhere it known that only pairwise statistics are relevant. Given that such prior knowledge is rarelyavailable, Symile should be favored over CLIP. Empirical contributions.We demonstrate that Symile outperforms pairwise CLIP on cross-modalclassification and retrieval across several experiments including on a multilingual dataset of images,text and audio of over 33M examples and a clinical dataset of chest X-rays, electrocardiograms, andlaboratory measurements. We show that Symile retains its advantage over pairwise CLIP even withmodalities missing in the data. We publicly release both the multilingual and the clinical datasets,which are specifically designed to test a models ability to capture higher-order information betweenthree distinct high-dimensional data types.",
  "Background and motivation": "In this section, we first provide background on the original CLIP objective for two modalities, anddescribe how it has been extended to additional modalities. We then present a simple problem set upfor three modalities that illustrates where pairwise contrastive objectives fall short. 2.1Pairwise contrastive learningGiven a batch of (x, y) pairs, separately encoded by f x and f y, respectively, contrastive objectivessuch as CLIP maximize the similarity between representations of correctly paired (positive) samplesand minimize the similarity between representations of incorrectly paired (negative) samples. As is now standard in contrastive learning, in order to construct a batch of data, each modality istreated as the anchor in turn and used to construct a set of positive and negative samples. Letting R+ be a temperature parameter, the CLIP objective when x is the anchor modality is thecategorical cross-entropy of correctly classifying the positive pair out of N possible pairs:",
  "Nj=1 expf x(xi)f y(yj)/.(1)": "The final CLIP objective is an average of the losses in each direction:L(x,y)CLIP (, )=12(xy)(, ) + (yx)(, ). The dot product in Equation (1) serves as a scoring functionthat is trained to assign high values to positive pairs, which are sampled from the joint distributionpx,y, and low values to negative pairs, which are sampled from the product of marginals pxpy. Contrastive methods are typically designed to maximize the mutual information between x and y,which is defined as the KullbackLeibler divergence from the joint distribution to the product of themarginal distributions: I(x; y) = DKLp(x, y) p(x)p(y). It has been shown that Equation (1)maximizes a lower bound on the mutual information between x and y . This informationmaximization ensures that the learned representations preserve all correlations between the modalities,which is essential for downstream tasks.",
  "L(x,y,z)CLIP(, ) = L(x,y)CLIP (, ) + L(y,z)CLIP (, ) + L(x,z)CLIP (, )": "CLIP can either be fine-tuned for downstream tasks or operate as a zero-shot classifier by computingthe similarities between the query embedding from one modality and each candidate embeddingfrom the other modality. In the case of more than two modalities, this generalizes to a sum acrossthe pairwise similarities. The resulting similarity scores are used to rank the candidates, and thecandidate with the highest similarity to the query is chosen . 2.2A simple one-dimensional problem for three binary modalitiesWhile contrastive objectives were originally designed for two modalities, the naive pairwise extensionof CLIP to additional modalities warrants a deeper analysis. To explore this further, we propose asimple problem setup for the following data generating process:",
  "I(a; b) = I(b; c) = I(a; c) = 0,I(a; b | c) > 0": "This explains CLIPs poor performance for the above XOR experiment: the objective maximizes alower bound on the mutual information between pairwise terms, and therefore was not designed tocapture higher-order dependencies such as the dependence between a and b given c.2 Capturingconditional dependencies like this will require the formulation of a new contrastive learning objective.",
  ".(2)": "While, as discussed, contrastive learning was designed to capture the shared information betweenmodalities, Equation (2) indicates that when there are more than two modalities, the scope of what tocapture should extend beyond pairwise information to include conditional interactions (). 2To be specific, we use higher-order information to mean information between two random variables givenany number of additional random variables in the conditioning set.3Symile stands for SYmmetric MultILinear Embeddings. Because it targets total correlation, Symile captures strictly more information than CLIP, guaranteeingperformance that matches or surpasses CLIP, except in cases where only pairwise statistics arerelevant, with no higher-order interactions whatsoever. In such cases, Symile may be less sampleefficient, as it tracks both pairwise and higher-order information. Unless there is prior knowledge thatthe downstream task relies solely on pairwise statistics, Symile should be chosen over CLIP.",
  ": An illustrative comparison of the in-formation captured by CLIP (only pairwise) andSymile (both pairwise and higher-order)": "To illustrate when such higher-order informationmight be relevant, consider again the XOR exper-iment outlined in .2. Because all thepairwise information terms between a, b, andc are zero, the conditional mutual informationterms constitute the only dependence betweenthe variables to track. The XOR experiment represents an extreme casewhere the CLIP target is zero, but most real-world applications will exhibit a combination ofboth pairwise and higher-order information. Forexample, in order to diagnose acute pancreatitis,one might consider a patients clinical historyof abdominal pain, elevated levels of digestiveenzymes, and imaging results consistent withinflammation. While each of these modalities would provide useful information about the likelihoodof pancreatitis (i.e., pairwise information between the modality and the diagnosis is non-zero), noneof them alone would be diagnostic of the condition. Similarly, in the case of Parkinsons disease,clinical evaluation provides valuable information, along with imaging and blood tests to rule outother conditions, but clinicians rely on the integration of all modalities. 3.1Deriving a multi-sample lower bound on total correlationIn order to eventually derive a contrastive objective by maximizing total correlation, we first establisha multi-sample lower bound on total correlation. This lower bound and, in the next section, theSymile objective are illustrated using three modalities for simplicity, but both can be extended to anarbitrary number of modalities, as shown in Appendix B.",
  "ENCODER": ": Symile pre-training and zero-shot prediction on the Symile-M3 multilingual dataset. (a)Given a batch of triples, Symile maximizes the multilinear inner product (MIP) of positive triples(in yellow along the diagonal of the cube) and minimizes the MIP of negative triples. (b) The modelselects the candidate image with the highest similarity to the query audio and text. Notice that the term inside the expectation in Equation (5) is the categorical log likelihood of correctlyidentifying the index of the positive triple in the batch, where the scoring function (or critic) g istrained to assign a high value to positive samples and a low value to negative samples. In Appendix E,we show that the optimal scoring function g is equal to the instantaneous total correlation up toadditive constants:",
  "The Symile objectiveWe now derive the Symile loss by maximizing the total correlation lower bound in Theorem 3.1": "Instead of using the dot product as a scoring function, as CLIP does, Symile uses its generalized form:the coordinate-wise sum of the element-wise product of a set of vectors. We call this the multilinearinner product (MIP): {xi}Mi=1 = Dd=1Mi=1 xi,d. As a scoring function, the MIP strikes a balancebetween computational simplicity and expressive power: it represents one of the simplest possiblegeneralizations of the dot product to more than two modalities, and the vector multiplication ensuresit is expressive enough to model any joint statistic.4 Given a batch of N positive triples (xi, yi, zi), each with N 1 corresponding negative triples(xi, yj, zj), and letting R+ be a temperature parameter, the Symile loss is the negative of anempirical estimate of the expected log likelihood in Equation (5):",
  "j=1expf x(xi), f y(yj), f z (zj)/. (6)": "4Note that the MIP is a measure of similarity defined by the joint distribution of the modalities, rather thana measure of the geometric similarity of the modalities representations. For example, a large MIP for Symilerepresentations rx, ry, rz indicates that the sample (x, y, z) has high probability under the joint likelihood; itprovides no information about whether rx, ry, rz are equal to one another. Minimizing Equation (6) optimizes the lower bound on total correlation by maximizing the MIP ofpositive tuples and minimizing the MIP of negative tuples (a). See Appendix B.4 for theSymile objective generalized to any number of modalities.",
  "# v, u, w:L2-normalized embeddings, each [n, dim]": "def symile_loss(v, u, w):logits_v_uw = np.exp(t) * get_logits(v, u, w)logits_u_vw = np.exp(t) * get_logits(u, v, w)logits_w_vu = np.exp(t) * get_logits(w, v, u)labels = np.arange(n)loss_v_uw = ce_loss(logits_v_uw, labels)loss_u_vw = ce_loss(logits_u_vw, labels)loss_w_vu = ce_loss(logits_w_vu, labels)return (loss_v_uw + loss_u_vw + loss_w_vu)/3 Efficient negative sampling.In the samplingprocedure described in .1, negativessamples for the non-anchor modalities are drawnindependently for each positive triple, whichcan be intensive in terms of both computationand memory. Instead, for efficiency, negativesampling can be approximated within a batchby forming negative tuples from non-matchingcombinations of the non-anchor modalities. Approximating negatives within a batch isstraightforward with two modalities, but in thecase of more than two modalities, both how neg-atives are formed and how many are used be-come design choices. At one extreme, one couldgenerate N 2 1 negative triples for each posi-tive by considering all possible combinations ofthe two remaining non-anchor modalities. Thisapproach, which we call O(N 2), can be compu-tationally and memory intensive. Instead, any subset of these negatives can be used for sampling.For instance, a more efficient approach, which we refer to as O(N), involves randomly permutingthe non-anchor modalities within the batch, providing each data point with N 1 negatives. Thecube in a illustrates the O(N 2) approach and Algorithm 1 presents pseudocode for the O(N)approach, both for three modalities. Missing data.The Symile objective is defined for data in which all modalities are observed.However, in practice, datasets often include samples where not all modalities are available. Thisraises the question: during training how should one incorporate data points for which only a subset ofmodalities is observed? Symile can be easily adapted to such missingness by adding extra dimensionsto the encoder inputs that indicate whether or not a modality is missing, ensuring that missing datapoints are out-of-support. This approach allows Symile to model dependencies between whichevermodalities are observed within a sample. We show in .2 that Symile retains its advantageover pairwise CLIP even with modalities missing in the data.",
  "x, z | f x(x) f z (z)": "The proof can be found in Appendix G. The independence statement in Theorem 3.3 tells us thatthe element-wise product of the Symile representations of any subset of modalities contains all theinformation required to predict the remaining modalities. In other words, once Symile representationshave been computed, access to the full data is no longer needed. Theorem 3.3 confirms Symilesability to learn efficient modality-specific representations for downstream tasks. 3.4Zero-shot prediction using the scoring functionJust as with CLIP, the optimal scoring function g (Lemma 3.2) can be used to predict one of themodalities y Y using instances of the other modalities x, z. If p(y) is uniformly distributed,then the scoring function can be used to rank the candidates for y: arg maxyY p(y = y | x, z) =arg maxyY g(x, y, z). However, this zero-shot approach, whether applied to Symile or to CLIP, does not lead to theBayes optimal prediction and, consequently, does not always yield reliable results when p(y) isnot uniformly distributed (see Appendix H for a detailed discussion). To address this issue, we caninstead compute the desired conditional probability directly using the scoring function: Theorem 3.4 (Conditional Distribution using the Scoring Function). Let x, y, z be three random vari-ables whose optimal representations when trained using Symile are f x(x), f y(y), f z (z), respectively.Let the MIP f x(x), f y(y), f z (z) be the scoring function. Then,",
  "The proof is provided in Appendix H": "If the marginal distribution of y is known, we could then perform zero-shot classification in one oftwo ways. When the distribution p(y | x, z) itself is of interest, as is often the case in healthcare ,we could compute p(y | x, z) directly, following Equation (7). Alternatively, if only predictions areneeded, we could usef x(x), f y(y), f z (z) + log p(y) to rank the possible values for y, as discussed further in Appendix H. If the marginal distribution ofy is not known, then because f x(x) f z (z) is a sufficient statistic for predicting y (Theorem 3.3),we could instead use f x(x) f z (z) to train a simple model to predict any property of y, s(y):p(s(y) | f x(x) f z (z)).",
  "Related work": "Contrastive learning beyond two modalities.As discussed, previous work has extended con-trastive learning to multiple modalities by applying CLIP to pairs of available modalities. Tian et al. distinguish between two such pairwise approaches: core view and full graph. The core viewstrategy fixes one modality and then averages the loss terms between that primary modality and eachof the other modalities . ImageBind exemplifies this approach, using CLIP to alignimage embeddings with embeddings from five other modalities: text, audio, depth, thermal, andmotion sensor data. One advantage of this strategy is that it avoids the need for datasets with allmodalities (though each dataset must still align with a primary modality). As discussed in Sections 3.2and 5.2, Symile representations can be learned even with modalities missing in the data. The full graph strategywhich we have referred to as pairwise CLIP in this paperis to consider allM2contrastive losses . For example, Guzhov et al. extend CLIP to includeaudio with text-to-image, text-to-audio, and image-to-audio losses. While this pairwise strategycaptures strictly more information than the one used by ImageBind, neither pairwise approach is ableto capture the higher-order information that Symile does. Pairwise CLIP has also been applied to architecture-specific fusion models that simultaneouslyprocess modalities to capture cross-modal interactions . For example, Shvetsova et al. train a Transformer to accept any number of modalities, using a weighted sum of contrastive lossesacross all input combinations. Such fusion approaches face a combinatorial explosion not only in thenumber of weighting coefficients to tune, but also in the number of forward passes required per batch.In contrast, Symile is architecture-agnostic and can learn modality-specific representations. Targeting higher-order information with contrastive learning.The use of contrastive meth-ods to target higher-order information has been explored primarily within the context of multipleaugmentations of the same data. For instance, Bai et al. derive a total correlation estimator by 0.00.20.40.60.81.0 p 0.00 0.25 0.50 0.75 1.00",
  "Mutual Information": "I(a; b) = I(b; c)I(a; b|c) = I(b; c|a) : The performance gap between Symile and CLIP on binary synthetic data (left) is aconsequence of the changing information dynamics between the variables as p moves from 0 to 1(right). Mean accuracy is reported across 10 bootstrap samples of the test set. recursively decomposing total correlation into a summation of mutual information terms, to whichvariational estimators are applied (in contrast, Symile optimizes only a single term when targetingtotal correlation). They then use their estimator to maximize the total correlation between four textaugmentations. Shidani et al. develop a pairwise contrastive approach for image representationlearning by generalizing a lower bound on mutual information to one-vs-rest mutual informationacross multiple augmentations. Liang et al. maximize the information in two modalities for aspecific downstream task by targeting higher-order information. The relationship between these studies and our work is analogous to that between SimCLR andCLIP. SimCLR popularized the use of the InfoNCE mutual information estimator for contrastivelearning on two data augmentations. Building on this framework, CLIP applied the approach todistinct modalities, where representations are learned separately for each modality using any encoder.Similarly, while existing work leverages total correlation or mutual information estimators for multi-augmentation contrastive learning, to our knowledge only pairwise applications of CLIP have appliedsuch estimators to more than two distinct modalities. Our work parallels the contributions of InfoNCEand CLIP for cases involving more than two modalities: like InfoNCE, we develop a simple estimatorthat recovers all possible information between any number of modalities, and like CLIP, we showhow this estimator can be used to learn modality-specific representations using any encoder.",
  "Experiments": "In this section, we empirically evaluate Symile on cross-modal retrieval tasks in three settings: a syn-thetic dataset, a multilingual dataset encompassing text, images, and audio, and a clinical dataset withchest X-rays, electrocardiograms, and blood labs. Throughout our experiments, we use pairwise CLIPas a baseline comparison since, as outlined in , it represents the only architecture-agnosticapproach that applies contrastive objectives to more than two modalities. We release all datasets andcode used in these experiments at",
  "We fit three affine linear functions that map a, b, c R5 to representations ra, rb, rc R16,respectively, and evaluate the models ability to correctly predict rb given the pair (ra, rc)": "Results. (left) compares Symile and CLIP across varying values of p. Both models startwith a mean accuracy of 0.0320.001 (SE) at p = 0. As p increases, Symiles accuracy progressivelyclimbs, reaching perfect accuracy at p = 1 0.0 (SE). In contrast, CLIPs accuracy remains nearlyconstant, barely surpassing the baseline random guessing rate of 0.031 (1/32). This performance gap is a consequence of the changing information dynamics between the variablesas p moves from 0 to 1, as shown in (right). When p = 0, b shares no information with a andceither pairwise or conditionallyrendering both models incapable of predicting rb from (ra, rc).As p increases, the higher-order I(a; b | c) and I(c; b | a) rise, driving a corresponding improvementin Symiles performance. However, because the pairwise I(a; b) and I(b; c) are always zero, there isno value of p at which CLIP is able to predict rb from (ra, rc).",
  "(a) Data generation(b) Fully-observed data(c) Data with missingness ( = 2)w": ": (a) Data-generating process for Symile-M3-5. (b) Comparison of Symile and CLIP onthe three versions of Symile-M3 (w {2, 5, 10}). Random chance is 1/1000. Symile successfullyleverages joint information between the modalities, whereas CLIP is limited to pairwise information,resulting in accuracies bounded by 1/w. (c) Symile outperforms the CLIP baseline on Symile-M3-2across varying levels of completeness in the training data. Both plots report mean accuracy across 10bootstrap samples of the test set. 5.2Symile-M3: a multilingual datasetWe now evaluate Symile on a new multilingual dataset comprising 33 million (audio, image, text)samples. The dataset, Symile-M3, is specifically designed to test a models ability to capture higher-order information between three distinct high-dimensional data types: by incorporating multiplelanguages, we construct a task where text and audio are both needed to predict the image, and where,importantly, neither text nor audio alone would suffice. Dataset design and model setup.Let w represent the number of languages in the dataset. An(audio, image, text) sample is generated by first drawing a short one-sentence audio clip fromCommon Voice spoken in one of w languages with equal probability. An image is drawn fromImageNet that corresponds to one of 1,000 classes with equal probability. Finally, text containingexactly w words is generated based on the drawn audio and image: one of the w words in the text isthe drawn image class name in the drawn audio language. The remaining w 1 words are randomlychosen from the ImageNet class names and written in one of the w languages such that there isno overlap in language or class name across the w words in the text. The words are separated byunderscores, and their order is randomized. We release three versions of the dataset: Symile-M3-2,Symile-M3-5, and Symile-M3-10, corresponding to 2, 5, and 10 languages (w). a showsan example of the data-generating process for Symile-M3-5. For each of the three datasets, 10Mtraining, 500K validation, and 500K test samples were generated. We use pre-trained encoders, freezing all parameters except for those in the text encoders embeddinglayer and first encoder layer, which are fine-tuned. We train three linear projections to map eachencoders representation to the same 8192-dimensional space. The Symile loss is trained with O(N)negative sampling. See Appendix I for details. Evaluation and results.We evaluate the learned representations on the zero-shot retrieval task offinding an image of the appropriate class given the audio and text. The most probable image for agiven query audio and text pair, selected from all possible candidate images in the test set, is thatwith the highest similarity score (b). Symile-M3 was designed to ensure that neither text noraudio alone would suffice to predict the image. Therefore, success on this zero-shot retrieval taskhinges on a models ability to capture joint information between the three modalities. As shown in b, Symile successfully leverages this joint information, with mean accuracies of0.939, 0.919, and 0.882 on Symile-M3-2, Symile-M3-5, and Symile-M3-10, respectively, calculatedacross 10 bootstrap samples of the test set, all with standard error less than 4.0 104. In contrast,CLIP, which captures pairwise information between image and text, can only predict an imagerandomly from among the w class labels present in the text, resulting in mean accuracies of 0.473,0.187, and 0.094 on Symile-M3-2, Symile-M3-5, and Symile-M3-10, respectively, all with standarderror 3.01 104. Because CLIP cannot distinguish between the class labels in the text using theaudio language, it can only pick a class label at random, bounding its accuracy by 1/w. Missing data.We also train Symile on a variant of Symile-M3-2 where each modality is indepen-dently missing with probability 0.5 or 0.65, corresponding, respectively, to probabilities 0.125 and0.043 of a complete data sample in the training set (see Appendix I for details). As before, the testset consists of complete triples. As shown in c, even when only 12.5% of the training data is complete, Symile achieves a mean accuracy of 0.906 3.4 104 (SE), far outperforming the CLIPbaseline accuracy of 0.473, despite the adverse effect of missing modalities. Notably, when less than5% of the training data is complete, Symile still exceeds the CLIP baseline. 5.3Chest X-ray prediction using electrocardiograms and laboratory measurementsZero-shot retrieval is widely used in the evaluation of representation learning for healthcare . In this section, we evaluate the Symile objective on Symile-MIMIC, a clinical datasetcomprised of chest X-rays, electrocardiograms, and blood labs from MIMIC-IV andMIMIC-CXR . Since ECGs and labs are both safer than CXRs, this experiment exploreswhether an ECG and labs collected at admission are predictive of a CXR taken shortly thereafter.",
  "+72 hrs": ": (a) Each sample of Symile-MIMIC in-cludes an ECG and blood labs taken within 24hours of the patients admission to the hospital,and a CXR taken in the 24- to 72-hour period post-admission. (b) Retrieval accuracy for identifyingthe CXR corresponding to a given ECG and labspair. Results are averaged over 10 bootstrap sam-ples, with error bars indicating standard error. Dataset design and model setup.Each datasample includes an ECG reading and blood labstaken within 24 hours of the patients admissionto the hospital, and a CXR taken in the 24- to72-hour period post-admission (a). Ouranalysis focuses on the 50 most common bloodlabs, with each sample containing at least one. We split our dataset (11, 622 admissions) intoa train/validation development set (95% of pa-tients) and a test set (5% of patients), ensuringthere is no patient overlap across the splits. Fol-lowing previous work, we use the ResNet-50and ResNet-18 architectures for the CXRand ECG encoders, respectively, and a three-layer neural network to encode the blood labs.All encoders are trained from scratch, and threelinear projections map each encoders repre-sentation to the same 8192-dimensional space.Given the limited size of the dataset, the Symileloss is trained with O(N 2) negative sampling tomitigate overfitting. See Appendix I for details. Evaluation and results.We evaluate the learned representations on the zero-shot retrieval taskof finding the most probable candidate CXR for a given query ECG and labs pair according to thesimilarity score. For each query ECG and labs pair in the test set, we sample nine negative CXRcandidates from the remaining test samples, so that that each query has a total of 10 candidates: onepositive (the true corresponding CXR) and nine negative. In b, we report mean accuracy for Symile and CLIP over 10 bootstrap samples of thetest set. While both models surpass random chance (0.1), Symile achieves an average accuracy of0.435 0.007 (SE), outperforming CLIPs 0.387 0.003 (SE). These results correspond to a 12.5%increase in accuracy for Symile over CLIP.",
  "Conclusion": "This work presents Symile, a simple contrastive learning approach that captures higher-order infor-mation between any number of modalities. Symile provides a flexible, architecture-agnostic objectivefor learning modality-specific representations, maintaining the simplicity of CLIP while deliveringsuperior performance, even in cases of missing modalities. Because it targets total correlation, Symilecaptures strictly more information than CLIP, guaranteeing performance that matches or surpassesCLIP, except in cases where it known that only pairwise statistics are relevant. Given that such priorknowledge is rarely available, Symile should be favored over CLIP. Future work.(1) The sigmoid-based loss function SigLIP was recently introduced as amemory-efficient alternative to traditional softmax-based contrastive objectives. A potential avenuefor future work would be to adapt Symile, and its use of the multilinear inner product, to this sigmoidloss. (2) The proposed implementation of Symile relies on an approximation for negative sampling,and future work could examine how this approximation scales when applied to settings with more thanthree modalities. (3) Future work could integrate pre-trained Symile representations into multimodallarge language models, enabling them to capture higher-order information between modalities.",
  "Acknowledgements": "We are especially grateful to Charley Crissman for his invaluable and meticulous feedback on everyaspect of the paper, from the proofs to the code. We would like to thank Nick Murphy (Pantograph)and Madeleine Murphy for their thoughtful guidance and indispensable support in preparing theillustrative figures. We thank Wanqian Yang for his helpful suggestions and careful editing of the paper.We also thank Leon A. Gatys, Eran Halperin, Andrew C. Miller, Charles Peyser, Pranav Rajpurkar,Ardavan Saeedi, and Jagadish Venkataraman for engaging in valuable discussions throughout thiswork. This work was partly supported by the NIH/NHLBI Award R01HL148248, NSF Award1922658 NRT-HDR: FUTURE Foundations, Translation, and Responsibility for Data Science, NSFCAREER Award 2145542, NSF Award 2404476, ONR N00014-23-1-2634, Apple, and Optum. Hassan Akbari, Liangzhe Yuan, Rui Qian, Wei-Hong Chuang, Shih-Fu Chang, Yin Cui, andBoqing Gong. Vatt: Transformers for multimodal self-supervised learning from raw video,audio and text. Advances in Neural Information Processing Systems, 34:2420624221, 2021. Jean-Baptiste Alayrac, Adria Recasens, Rosalia Schneider, Relja Arandjelovic, Jason Ramapu-ram, Jeffrey De Fauw, Lucas Smaira, Sander Dieleman, and Andrew Zisserman. Self-supervisedmultimodal versatile networks. Advances in Neural Information Processing Systems, 33:2537,2020. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson,Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visuallanguage model for few-shot learning. Advances in neural information processing systems, 35:2371623736, 2022. Rosana Ardila, Megan Branson, Kelly Davis, Michael Henretty, Michael Kohler, Josh Meyer,Reuben Morais, Lindsay Saunders, Francis M Tyers, and Gregor Weber. Common voice: Amassively-multilingual speech corpus. arXiv preprint arXiv:1912.06670, 2019. Ke Bai, Pengyu Cheng, Weituo Hao, Ricardo Henao, and Larry Carin.Estimating totalcorrelation with mutual information estimators. In International Conference on ArtificialIntelligence and Statistics, pp. 21472164. PMLR, 2023. Shruthi Bannur, Stephanie Hyland, Qianchu Liu, Fernando Perez-Garcia, Maximilian Ilse,Daniel C Castro, Benedikt Boecking, Harshita Sharma, Kenza Bouzid, Anja Thieme, et al.Learning to exploit temporal structure for biomedical vision-language processing. In Proceed-ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1501615027, 2023.",
  "David Barber and Felix Agakov. The im algorithm: a variational approach to informationmaximization. Advances in neural information processing systems, 16(320):201, 2003": "Benedikt Boecking, Naoto Usuyama, Shruthi Bannur, Daniel C Castro, Anton Schwaighofer,Stephanie Hyland, Maria Wetscherek, Tristan Naumann, Aditya Nori, Javier Alvarez-Valle,et al. Making the most of text semantics to improve biomedical visionlanguage processing. InEuropean conference on computer vision, pp. 121. Springer, 2022. Brian Chen, Andrew Rouditchenko, Kevin Duarte, Hilde Kuehne, Samuel Thomas, AngieBoggust, Rameswar Panda, Brian Kingsbury, Rogerio Feris, David Harwath, et al. Multimodalclustering networks for self-supervised learning from unlabeled videos. In Proceedings of theIEEE/CVF International Conference on Computer Vision, pp. 80128021, 2021.",
  "Irene Y Chen, Shalmali Joshi, Marzyeh Ghassemi, and Rajesh Ranganath. Probabilistic machinelearning for healthcare. Annual review of biomedical data science, 4(1):393415, 2021": "Sihan Chen, Xingjian He, Longteng Guo, Xinxin Zhu, Weining Wang, Jinhui Tang, and Jing Liu.Valor: Vision-audio-language omni-perception pretraining model and dataset. arXiv preprintarXiv:2304.08345, 2023. Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple frameworkfor contrastive learning of visual representations. In International conference on machinelearning, pp. 15971607. PMLR, 2020. Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wen-zek, Francisco Guzmn, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov.Unsupervised cross-lingual representation learning at scale. arXiv preprint arXiv:1911.02116,2019. Kevin Duarte, Brian Chen, Nina Shvetsova, Andrew Rouditchenko, Samuel Thomas, AlexanderLiu, David Harwath, James Glass, Hilde Kuehne, and Mubarak Shah. Routing with self-attentionfor multimodal capsule networks. arXiv preprint arXiv:2112.00775, 2021. Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala,Armand Joulin, and Ishan Misra. Imagebind: One embedding space to bind them all. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.1518015190, 2023. A. L. Goldberger, L. A. N. Amaral, L. Glass, J. M. Hausdorff, P. Ch. Ivanov, R. G.Mark, J. E. Mietus, G. B. Moody, C.-K. Peng, and H. E. Stanley.PhysioBank,PhysioToolkit, and PhysioNet:Components of a new research resource for complexphysiologic signals.Circulation, 101(23):e215e220, 2000 (June 13).CirculationElectronic Pages: PMID:1085218; doi:10.1161/01.CIR.101.23.e215. Brian Gow, Tom Pollard, Larry A Nathanson, Alistair Johnson, Benjamin Moody, ChrystinneFernandes, Nathaniel Greenbaum, Seth Berkowitz, Dana Moukheiber, Parastou Eslami, TannerCarbonati, Ashish Chaudhari, Elizabeth Herbst, Dana Moukheiber, Seth Berkowitz, RogerMark, and Steven Horng. Mimic-iv-ecg: Diagnostic electrocardiogram matched subset, 2023.URL",
  "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for imagerecognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,pp. 770778, 2016": "Jingjia Huang, Yinan Li, Jiashi Feng, Xinglong Wu, Xiaoshuai Sun, and Rongrong Ji. Clover:Towards a unified video-language alignment and fusion model. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition, pp. 1485614866, 2023. Shih-Cheng Huang, Liyue Shen, Matthew P Lungren, and Serena Yeung. Gloria: A multimodalglobal-local representation learning framework for label-efficient medical image recognition. InProceedings of the IEEE/CVF International Conference on Computer Vision, pp. 39423951,2021. Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana Ciurea-Ilcus, Chris Chute,Henrik Marklund, Behzad Haghgoo, Robyn Ball, Katie Shpanskaya, et al. Chexpert: A largechest radiograph dataset with uncertainty labels and expert comparison. In Proceedings of theAAAI conference on artificial intelligence, volume 33, pp. 590597, 2019.",
  "Alistair Johnson, Matthew Lungren, Yifan Peng, Zhiyong Lu, Roger Mark, Seth Berkowitz,and Steven Horng. Mimic-cxr-jpg - chest radiographs with structured labels, 2024. URL": "Alistair EW Johnson, Tom J Pollard, Nathaniel R Greenbaum, Matthew P Lungren, Chih-ying Deng, Yifan Peng, Zhiyong Lu, Roger G Mark, Seth J Berkowitz, and Steven Horng.Mimic-cxr-jpg, a large publicly available database of labeled chest radiographs. arXiv preprintarXiv:1901.07042, 2019. Alistair EW Johnson, Lucas Bulgarelli, Lu Shen, Alvin Gayles, Ayad Shammout, Steven Horng,Tom J Pollard, Sicheng Hao, Benjamin Moody, Brian Gow, et al. Mimic-iv, a freely accessibleelectronic health record dataset. Scientific data, 10(1):1, 2023. Alexander Khazatsky, Karl Pertsch, Suraj Nair, Ashwin Balakrishna, Sudeep Dasari, SiddharthKaramcheti, Soroush Nasiriany, Mohan Kumar Srirama, Lawrence Yunliang Chen, KirstyEllis, et al. Droid: A large-scale in-the-wild robot manipulation dataset. arXiv preprintarXiv:2403.12945, 2024. Sravan Kumar Lalam, Hari Krishna Kunderu, Shayan Ghosh, Harish Kumar, Samir Awasthi,Ashim Prasad, Francisco Lopez-Jimenez, Zachi I Attia, Samuel Asirvatham, Paul Friedman,et al. Ecg representation learning with multi-modal ehr data. Transactions on Machine LearningResearch, 2023. Eunjung Lee, Saki Ito, William R Miranda, Francisco Lopez-Jimenez, Garvan C Kane, Samuel JAsirvatham, Peter A Noseworthy, Paul A Friedman, Rickey E Carter, Barry A Borlaug, et al.Artificial intelligence-enabled ecg for left ventricular diastolic function and filling pressure. npjDigital Medicine, 7(1):4, 2024. Paul Pu Liang, Zihao Deng, Martin Q Ma, James Y Zou, Louis-Philippe Morency, and RuslanSalakhutdinov. Factorized contrastive learning: Going beyond multi-view redundancy. Advancesin Neural Information Processing Systems, 36, 2023.",
  "Ben Poole, Sherjil Ozair, Aaron Van Den Oord, Alex Alemi, and George Tucker. On variationalbounds of mutual information. In International Conference on Machine Learning, pp. 51715180. PMLR, 2019": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visualmodels from natural language supervision. In International conference on machine learning,pp. 87488763. PMLR, 2021. Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and IlyaSutskever. Robust speech recognition via large-scale weak supervision. In InternationalConference on Machine Learning, pp. 2849228518. PMLR, 2023.",
  "Kevin E Wu, Howard Chang, and James Zou. Proteinclip: enhancing protein language modelswith natural language. bioRxiv, pp. 202405, 2024": "Hongwei Xue, Yuchong Sun, Bei Liu, Jianlong Fu, Ruihua Song, Houqiang Li, and Jiebo Luo.Clip-vip: Adapting pre-trained image-text model to video-language representation alignment.arXiv preprint arXiv:2209.06430, 2022. Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers, AlexanderKolesnikov, and Lucas Beyer. Lit: Zero-shot transfer with locked-image text tuning. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.1812318133, 2022.",
  "Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual languagemodel for video understanding. arXiv preprint arXiv:2306.02858, 2023": "Sheng Zhang, Yanbo Xu, Naoto Usuyama, Jaspreet Bagga, Robert Tinn, Sam Preston, RajeshRao, Mu Wei, Naveen Valluri, Cliff Wong, et al. Large-scale domain-specific pretraining forbiomedical vision-language processing. arXiv preprint arXiv:2303.00915, 2(3):6, 2023. Yuhao Zhang, Hang Jiang, Yasuhide Miura, Christopher D Manning, and Curtis P Langlotz.Contrastive learning of medical visual representations from paired images and text. In MachineLearning for Healthcare Conference, pp. 225. PMLR, 2022.",
  "Our goal in this section is to derive a lower bound on TC(m1, . . . , mM)": "We start by describing in Appendix B.1 the sampling procedure for a batch of (m1, . . . , mM) tuples.In Appendix B.2, we derive the desired lower bound in Theorem 3.1 (our proof was inspired by Pooleet al. s derivation of the InfoNCE lower bound, which does not rely on an approximation usedby Oord et al. ). In Appendix B.3, we show that the bound is closed at optimality. Finally, we usethe lower bound to define the Symile objective in Appendix B.4. B.1Sampling procedureWe start by describing the sampling procedure for the batch of N M-tuples. In contrastive learning,the objective is to differentiate between positive and negative samples constructed from a given batchof matched data. In order to construct these samples, each modality is treated as the anchor in turn,and then for each anchor modality a corresponding set of positive and negative samples is generated.",
  "GSymile learns sufficient statistics": "Theorem G.1 (Symile Sufficient Statistics). Let m1, . . . , mM be M random variables whose optimalrepresentations when trained using Symile are f 1 (m1), . . . , f M(mM), respectively. The element-wise product of any subset of the representations is a sufficient statistic for predicting the remainingrandom variables.",
  "p(y = b)": "While zero-shot classification works well when one modality directly determines another (for example,a text caption precisely specifies its corresponding image), in all other instances, the CLIP or Symilescoring function fails to provide reliable predictions. To address this issue, we demonstrate how the Symile scoring function can be used to compute thedesired conditional distribution, which achieves optimal classification accuracy. (While we illustratethis approach for Symile, it can be applied similarly to CLIP.) Suppose we want to predict modality y from modalities x, z using zero-shot classification. Recallfrom .2 that we use the multilinear inner product (MIP) as the scoring function. Theorem H.1establishes that we can compute p(y | x, z) directly using the MIP. Theorem H.1 (Conditional Distribution using the Scoring Function). Let x, y, z be three random vari-ables whose optimal representations when trained using Symile are f x(x), f y(y), f z (z), respectively.Let the MIP f x(x), f y(y), f z (z) be the scoring function. Then,",
  "y expf x(x), f y(y), f z (z)p(y)dyby Eq. 28": "If the marginal distribution of y is known, we could then perform zero-shot classification in one oftwo ways. When the distribution p(y | x, z) itself is of interest, as is often the case in healthcare ,we could compute p(y | x, z) directly, following Equation (29). Alternatively, if only predictions areneeded, we could usef x(x), f y(y), f z (z) + log p(y)to rank the possible values for y.",
  "Since the above integral is constant with respect to y, Equation (30) will produce the same rankingsfor y as log p(y | x, z)": "If the marginal distribution of y is not known, then because f x(x) f z (z) is a sufficient statistic forpredicting y (Theorem 3.3), we could instead use f x(x) f z (z) to train a simple model to predictany property of y, s(y): p(s(y) | f x(x) f z (z)).",
  "All datasets and code used in this work are publicly available at": "For all experiments, we use the AdamW optimizer . Following , the temperature parameter is directly optimized during training as a multiplicative scalar to avoid the need for separatehyperparameter tuning. Experiments were conducted with 16 CPUs, 200GB of RAM, and a singleNVIDIA A100 80GB PCIe GPU. I.1Simulated data: 1DWe fit a model with three affine linear functions that map the binary data a, b, c to representationsra, rb, rc R16, respectively. The zero-shot classification task is to predict whether rb=0 or rb=1 isthe correct match for a given ra, rc.",
  "aj, bj Bernoulli(0.5),i Bernoulli(p),cj = (aj XOR bj)i a(1i)ja = [a1, . . . , a5],b = [b1, . . . , b5],c = [c1, . . . , c5]": "We construct train, val, and test sets of 10K, 1K, and 5K samples, respectively. We fit three affine linearfunctions that map a, b, c to representations ra, rb, rc R16, respectively. These representationsare then L2-normalized. Both Symile and CLIP are trained for 100 epochs using a batch size of 1000, a learning rate of 0.1,and a weight decay of 0.01. The learned temperature parameter is initialized to 0.3. The Symileloss is trained with O(N) negative sampling. Checkpoints were saved at the end of every epoch, andthe best model was selected based on the lowest validation loss. I.3Symile-M3Dataset.We use images from the ImageNet Large Scale Visual Recognition Challenge (ILSVRC)2012-2017 train set , which we downloaded from Kaggle.5 The ImageNet train set has 1,281,167images from 1,000 categories. We use audio from the Common Voice Corpus . All languages are from versions 16.0 except forEnglish, which is from version 14.0. Each audio clip in the dataset is an MP3 file that consists of asentence being read aloud. We remove any audio clips that have duration 0.0 seconds. We use thefollowing languages for each version of Symile-M3:",
  "For each of the three versions of Symile-M3, 10M training, 500K validation, and 500K test sampleswere generated": "Training.Although Symile does not require the use of pre-trained encoders, we use them toaccelerate training. For audio, image, and text, we use pre-trained encoders from Whisper (Hugging Face model id openai/whisper-large-v3), CLIP (Hugging Face model idopenai/clip-vit-large-patch14), and XLM-RoBERTa (Hugging Face model idxlm-roberta-large), respectively. Audio is downsampled to 16kHz, as expected by Whisper, before being passed to the feature extractor. We freeze the three encoders parameters except forthose in the text encoders embedding layer and first encoder layer, which are fine-tuned. We trainthree linear projections to map each encoders representation to the same 8192-dimensional space,followed by layer normalization. For each combination of objective (Symile or CLIP) and Symile-M3 version (2, 5, or 10), we do agrid search over learning rate (1e-5, 5e-5, 1e-4) and weight decay (0, 1e-4, 1e-3). We also tune thesehyperparameters for the experiments with missing data. All models are trained for 24 epochs usinga batch size of 256. The learned temperature parameter is initialized to 6. The Symile loss istrained with O(N) negative sampling. Checkpoints were saved every two epochs, and the best modelwas selected based on the lowest validation loss. Missingness.We evaluate Symile on a variant of Symile-M3-2 where each modality is indepen-dently missing with probability 0.5 or 0.65, which correspond, respectively, to probabilities 0.125and 0.043 of a complete data sample. For audio and image data, we learn two embeddings, one for observed data points and one for missingdata points. Each embedding matches the dimension of the last hidden layer of the respective audioor image encoder. When a data point is observed, we concatenate its encoder representation and thelearned embedding for observed data points, and pass this combined vector into the linear projectionhead before layer normalization. When a data point is missing, we concatenate the mean encoderrepresentation from the observed training samples and the learned embedding for missing data points,and pass this combined vector into the linear projection head before layer normalization.",
  "For text data, if a data point is missing, we pass into the text encoder the tokenized representation of[MISSING], which is outside of the models vocabulary": "I.4Symile-MIMICSymile-MIMIC is a clinical dataset comprised of chest X-rays, electrocardiograms, and blood labsfrom the MIMIC-IV and MIMIC-CXR datasets. We use admissions andlabs from MIMIC-IV v2.2,7 ECGs from MIMIC-IV-ECG v1.0,8 and CXRs from MIMIC-CXR-JPGv2.0.0.9 Each data sample includes an ECG reading and blood labs taken within 24 hours of the patientsadmission to the hospital, and a CXR taken in the 24-72 hour period post-admission. For eachadmission, we choose the earliest CXR, ECG, and labs. We use CXRs in JPG format, and consider only CXRs with a posteroanterior (PA) or anteroposterior(AP) view. Following Irvin et al. , each CXR is scaled such that the smaller edge is set to 320pixels, followed by a square crop (random for training or center for validation and testing). Imagesare then normalized using the ImageNet mean and standard deviation.",
  "We use 10-second 12-lead ECGs, and remove from consideration any ECGs with NaN values or witha signal of all zeros. The ECG signal is normalized to lie within the range": "We focus on the following 50 most common blood laboratory measurements in our dataset, with eachdata sample containing at least one: Hematocrit, Platelet Count, Creatinine, Potassium, Hemoglobin,White Blood Cells, MCHC, Red Blood Cells, MCV, MCH, RDW, Urea Nitrogen, Sodium, Chloride,Bicarbonate, Anion Gap, Glucose, Magnesium, Calcium Total, Phosphate, INR (PT), PT, PTT,Basophils, Neutrophils, Monocytes, Eosinophils, Lymphocytes, RDW-SD, H, L, I, Alanine Amino-transferase (ALT), Asparate Aminotransferase (AST), Lactate, Alkaline Phosphatase, Bilirubin Total,pH, Albumin, Base Excess, pO2, Calculated Total CO2, pCO2, Absolute Neutrophil Count, AbsoluteEosinophil Count, Absolute Monocyte Count, Absolute Basophil Count, Absolute LymphocyteCount, Creatine Kinase (CK), Immature Granulocytes. For the labs model, we use a 100-dimensional vector as input: the first 50 coordinates are labvalues standardized to percentiles based on the training sets empirical CDF, and the remaining 50coordinates are binary indicators that denote whether each lab value is missing. When a lab value isunobserved, the mean percentile for that lab is substituted. Following previous work , we use the ResNet-50 and ResNet-18 architectures for the CXR and ECG encoders, respectively, and a three-layer neural network to encode theblood labs. All encoders are trained from scratch, and three linear projections map each encodersrepresentation to the same 8192-dimensional space. For Symile and CLIP each, we do a grid search over learning rate (5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2)and weight decay (1e-3, 1e-2, 1e-1, 2e-1, 5e-1). All models are trained for 80 epochs using a batchsize of 280. The learned temperature parameter is initialized to 7. The Symile loss is trained withO(N 2) negative sampling to mitigate overfitting. Checkpoints were saved at the end of every epoch,and the best model was selected based on the lowest validation loss."
}