{
  "Abstract": "Whether the system under study is a shoal of fish, a collection of neurons, or a set ofinteracting atmospheric and oceanic processes, transfer entropy measures the flowof information between time series and can detect possible causal relationships.Much like mutual information, transfer entropy is generally reported as a singlevalue summarizing an amount of shared variation, yet a more fine-grained account-ing might illuminate much about the processes under study. Here we proposeto decompose transfer entropy and localize the bits of variation on both sides ofinformation flow: that of the originating processs past and that of the receivingprocesss future. We employ the information bottleneck (IB) to compress the timeseries and identify the transferred entropy. We apply our method to decomposethe transfer entropy in several synthetic recurrent processes and an experimentalmouse dataset of concurrent behavioral and neural activity. Our approach high-lights the nuanced dynamics within information flow, laying a foundation for futureexplorations into the intricate interplay of temporal processes in complex systems.",
  "Introduction": "Causality forms the backbone of explanations in science. Detecting cause and effect from ob-servational data alone is generally impossible, as there can always be unobserved shared causes.However, certain signatures have been identified that can indicate possible causality, such as Grangercausality , transfer entropy , and directed information . Measuring such signatures can bea powerful step toward understanding a complex system, with applications as broad as groups offish , neural signals , and Earth-scale climate processes .",
  "bca": ": Localizing transfer entropy in the past and future. (a) If the past of a large red fish(the source) helps predict the future of a small blue fish (the target) after accounting for its past, thenthere is positive transfer entropy. Equivalently, if the future of the blue fish helps infer the past ofthe red fish after accounting for the blue fishs past, then there is positive transfer entropy. (b) Amachine learning scheme to extract the transfer entropy from the sources past, using the informationbottleneck (IB). (c) An analogous scheme to extract the transfer entropy from the targets future. mutual information, transfer entropy is nonparametric and generalizes to arbitrary probability distri-butions and relationships between variables, in contrast to Granger causality; transfer entropy andGranger causality are equivalent for Gaussian variables . On the other hand, conditional mutualinformation can be challenging to estimate from limited data samples . Consider modeling the movement of the blue fish in a so as to forecast its future given its past.If the red fish causes the blue fish to alter its behavior, then the accuracy of the forecast will improvewhen information about the past of the red fish is incorporated into the model. The converse is notnecessarily truethat an improvement in accuracy implies a causal relationshipbut quantifyinginformation flow in this way is often the closest to causality one can get with observational data alone(i.e., without having the ability to intervene in the data collection process) . While the above framing in terms of forecasting with and without the sources past is the commonway to present transfer entropy, there is an equivalent alternative that we will leverage in this work.Again employing the fish of a, consider the task of inferring the red fishs movement giventhe blue fishs movement over the same time frame. If the accuracy of the model improves whenincorporating the future of the blue fish, there is information flow from the past of the red fish to thefuture of the blue fish. With these two equivalent expressions for transfer entropy, we can focus onthe effect of incorporating either the past of the red fish or the future of the blue fish into a predictivemodel. The net information change in either case must be equal to the transfer entropy, but theoriginating and terminating variation can look different on either side of the information flow. Ourgoal in this work is to identify the variation on either side by using the information bottleneck. The information bottleneck (IB) is a method to isolate targeted variation in a learned compressionscheme , allowing for control over learned representations and fine-grained inspection ofwhat the information is . Here we propose an IB scheme to encapsulate the transfer entropy fromeither side of an information flow into a learned compression space. The encapsulated informationcan then be decomposed in terms of multiple sources via the distributed IB , inspected as alocal quantity varying in time , and used as a targeted representation for downstream tasks .",
  "Approach": "Transfer entropy measures information flow from a source time series to a target time series andis the information gained in either of two equivalent scenarios (a): (i) the information thatthe sources past adds to the targets past about the targets future, and (ii) the information that thetargets future adds to the targets past about the sources past. We will set up both scenarios as aconstrained communication problem with the information bottleneck in order to identify andthen decompose the bits of transferred entropy in the sources past and in the targets future. Let Xt and Yt represent two random processes, the source and target, and let t be a discrete index fortime. We assume the processes are stationary and will use the same time horizon, , into the past andfuture. For shorthand, Y:0 and Y0: (using python indexing for steps into the past and future,respectively ) will be written as Ypast and Yfuture, respectively. The forecasting information inherent to the target process is the mutual information betweenthe recent past and the near future, I(Ypast; Yfuture). The transfer entropy from source to target is theincrease in forecasting information when incorporating the recent history of the source,",
  "TEXY = I(Yfuture, Ypast; Xpast) I(Ypast; Xpast).(2)": "Eqns. 1 and 2 can be seen as the difference in predictive power between two extremes of a con-tinuous spectrum parameterized by the amount of information incorporated about Xpast and Yfuture,respectively. We will traverse the spectrum by compressing the added piece of information with theinformation bottleneck (b). Importantly, to compress the added information requires Ypast ascontext in both cases, which can simply be passed along as an additional input to the encoder. LettingU = f(Xpast, Ypast) be the compression of the additional information, and be an information cost,minimizing the following Lagrangian with respect to U traverses the spectrum relevant to Eqn. 1,",
  "L = I(Xpast, Ypast; U) I(U, Ypast; Yfuture).(3)": "In the limit , no information is used about the source and we recover the self-forecastinginformation I(Ypast; Yfuture). At the other extreme, 0, all information from the source is used.The difference in the loss between these extremes is the transfer entropy. In practice, we decrease logarithmically over the course of a single training run to obtain the full spectrum of transferredinformation; the transfer entropy is obtained by the difference between the endpoints of the trajectory.To optimize with machine learning, we employ the same information cost as a -VAE , and alower bound on the forecasting information via InfoNCE ,",
  "L = E[DKL(p(u|xpast, ypast)||r(u))] E[LNCE(f(u, ypast), g(yfuture))].(4)": "The first expectation is over p(xpast, ypast) and the second is over p(u|xpast, ypast)p(xpast, ypast, yfuture).r(u) is a prior distribution, taken to be the standard normal N(0, 1), and f() and g() are functionsparameterized by neural networks. While Eqn. 4 targets the transfer entropy in Xpast, the transferentropy in Yfuture is found analogously with the compression variable U = f(Yfuture, Ypast) (c). Without loss of generality, assume the source and target are compositions of processese.g., multiplephysiological signals from each fish in aso that Xt = (X1t , ...XNt ). For added interpretabilityaround the extracted information, we compress each component process and/or each point in timeseparately by distributing an information bottleneck to each as a separate random variable .The Lagrangian corresponding to Eqn. 3 is",
  "Results and Discussion": "We will now work through synthetic examples to build intuition for the transfer entropy decompo-sition, and then analyze an experimental dataset from a group of mice where neural activity andbehavioral data were collected simultaneously. All quantities are in bits unless specified otherwise.Implementation specifics can be found in the Appendix. Synthetic Boolean networks. For the systems of binary-valued processes in , we generatedtrajectories of length 104 and trained the end-to-end setups shown in b&c with a time horizon = 3. A classic example regarding transfer entropy (slightly modified) is shown in a&b.Blue and orange are random processes that determine the next timestep of the green process through",
  "b": ": Transferred entropy in binary-valued recurrent networks. (a) Left: The update rule forfour processes, where nodes without inputs (blue, orange) are randomly sampled at each timestep.The source and target processes are indicated by the shaded boxes and marked X and Y , respectively.Middle: Distributed information planes that visualize the decomposition of transfer entropy in thesources past and the targets future. Right: The share of transfer entropy residing in differenttimesteps of the sources past (top) and targets future (bottom) (taken from the rightmost point of thetrajectories in the middle). (b) Same as panel a, but with different target processes. (c) Same as panela, but with randomly generated connection weights and an integrate-and-fire scheme. an XOR interaction, and the red process stores the most recent state of green. There is positive transferentropy when considering the blue and orange processes together as a single composite source, andeither or both of the green and red processes as targets. With the distributed IB, we bottlenecked eachprocess and each timestep to identify the origin and the terminus of the transferred entropy. The difference in total information I(U, Ypast; Yfuture) between the start and end of optimization is thetransfer entropy, and therefore the same when focusing on the transfer entropys origin in Xpast andits destination in Yfuture. However, the trajectories are qualitatively different for the past and futuredecompositions due to the nature of interactions between the constituent processes . In a,the single bit of transfer entropy starts as two bits in the blue and orange sources past at timestept 1, and then ends as one bit in the future of the red process at timestep t + 1. Note that the greenand red process contain the same information (with a delay of one), meaning the transferred entropyequivalently resides in the green process at timestep t. When repeating the distributed IB optimizationfor a, the transferred entropy was localized in the green process at timestep t in half of theoptimizations and in the red process at timestep t + 1 in the other half. The degeneracy resolves if weignore the green process (b), and the transfer entropy increases to two bits. Four bits originatein the blue and orange processes and then terminate in the red process over the future two timesteps. The benefit of machine learning becomes more apparent when decomposing information flow in morecomplicated processes. In c, a set of connected processes includes two sources (blue and orange,random), two hidden states (grey), and two targets (green and red). Nodes with connections fromthe previous timestep follow an integrate-and-fire update rule, meaning they will fire if the sum oftheir inputs, weighted by +1 or -1 as indicated by the solid and dashed lines, is greater than zero. Theshare of information from different timesteps showcases the nontrivial recurrence established by therandomly selected connection scheme and provides clues about the underlying process interactions. Experimental mouse data. We next analyzed calcium imaging and spontaneous behavioral datafrom six mice publicly uploaded with Benisty et al. , with a time horizon of 1 second (10 samples).A rough schematic of the experimental setup is shown in a.",
  "ab": ": Transfer entropy between brain and behavior. (a) Concurrent neural and behavioralrecordings were taken of six mice; example time series shown on the right with the brain regionsshown with matching colors in the atlas. (b) Pairwise transfer entropy between the 23 brain regions,the reference average signal (Avg), and three behavioral streams. (c) Transfer entropy decompositionfrom behaviors to the purple region from a, the primary somatosensory area for the nose (SSp-n).The instantaneous Kullback-Leibler (KL) cost in natural units (nats) per channel (black) is shownconcurrently with the raw time series (colored). We estimated the pairwise transfer entropy between all 23 regions, their common average signal(Avg), and the three behavioral time series, one pair at a time using the difference in forecastingInfoNCE values with and without the source (b). The largest pairwise values link behavior tobrain region SSp-n, and we decompose the multivariate transfer entropy in c. The pupil timeseries contained the second largest transfer entropy with SSp-n (b), though its contribution wasthe smallest when combined with the face and wheel data, suggesting redundant information. At any point along the information bottleneck spectrum, we can inspect the learned compressionscheme. We encoded a stream of the validation data with the three learned encoders at an intermediatevalue of I(U, Ypast; Yfuture) and display the Kullback-Leibler (KL) costwhose expectation serves asa penalty on the transmitted information in Eqn. 4at each point in time (c, right). Note eachencoding scheme embeds the source past along with the target past as context, meaning the spikes inKL cost could arise from an interaction between the two signals. In this way we obtain a fine-grainedpicture suggesting where in the source time series the transfer entropy spawns, which could be pairedwith a local estimate of the transfer entropy to take a microscope to information transfer betweenprocesses. Discussion. The quantification of information flows between parts of a system via transfer entropy isan important step toward a deeper understanding and serves as a screening for possible causation. Inthis work, we localized transfer entropy on both sides of an information flow: from its origin in thesources past to its terminus in the targets future. We note that although both routes to transfer entropyoutlined are equivalent, the prediction task involved is qualitatively different. When compressing thesources past, the prediction task is to forecast the targets future from its past. By contrast, whencompressing the targets future, the prediction task is to link the targets past with the sources past,which will generally share a lower amount of information than the targets past with its future. Thedifficulty of optimization for the two formulations may thus be different in practice. Finally, we note that conditioning on additional processes when computing transfer entropy allowsone to exclude the influence of the processes . The proposed framework readily handles suchadditional processes by appending them to both instances of Ypast in the schematics of b, c.",
  "Kieran A Murphy and Dani S Bassett. Information decomposition in complex systems viamachine learning. Proceedings of the National Academy of Sciences, 121(13):e2312988121,2024. 2, 3, 4": "Joseph T Lizier, Mikhail Prokopenko, and Albert Y Zomaya. Local information transfer as aspatiotemporal filter for complex systems. Physical Review EStatistical, Nonlinear, and SoftMatter Physics, 77(2):026110, 2008. 2, 5 Damjan Kalajdzievski, Ximeng Mao, Pascal Fortier-Poisson, Guillaume Lajoie, andBlake Aaron Richards. Transfer entropy bottleneck: Learning sequence to sequence informa-tion transfer. Transactions on Machine Learning Research, 2023. ISSN 2835-8856. URL 2, 3",
  "Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastivepredictive coding. arXiv preprint arXiv:1807.03748, 2018. 3": "Hadas Benisty, Daniel Barson, Andrew H Moberly, Sweyta Lohani, Lan Tang, Ronald RCoifman, Michael C Crair, Gal Mishne, Jessica A Cardin, and Michael J Higley. Rapidfluctuations in functional connectivity of cortical networks encode spontaneous behavior. NatureNeuroscience, 27(1):148158, 2024. 4 Ramn Martnez-Cancino, Arnaud Delorme, Johanna Wagner, Kenneth Kreutz-Delgado,Roberto C Sotero, and Scott Makeig. What can local transfer entropy tell us about phase-amplitude coupling in electrophysiological signals? Entropy, 22(11):1262, 2020. 5"
}