{
  "Abstract": "The valid measurement of generative AI (GenAI) systems capabilities, risks, andimpacts forms the bedrock of our ability to evaluate these systems. We introducea shared standard for valid measurement that helps place many of the disparate-seeming evaluation practices in use today on a common footing. Our framework,grounded in measurement theory from the social sciences, extends the work ofAdcock and Collier in which the authors formalized valid measurement of con-cepts in political science via three processes: systematizing background concepts,operationalizing systematized concepts via annotation procedures, and applyingthose procedures to instances. We argue that valid measurement of GenAI systemscapabilities, risks, and impacts, further requires systematizing, operationalizing,and applying not only the entailed concepts, but also the contexts of interest and themetrics used. This involves both descriptive reasoning about particular instancesand inferential reasoning about underlying populations, which is the purview ofstatistics. By placing many disparate-seeming GenAI evaluation practices on acommon footing, our framework enables individual evaluations to be better under-stood, interrogated for reliability and validity, and meaningfully compared. This isan important step in advancing GenAI evaluation practices toward more formalizedand theoretically grounded processesi.e., toward a science of GenAI evaluations.",
  "Introduction": "Whether we are interested in what a generative (GenAI) system is capable of, the risks it poses, or theimpacts of its deployment, we can gain insight into such inquiries by framing them as measurementtasks of the form: measure the [amount] of a [concept] in [instances] from a [population]. Examplesinclude evaluating an LLM-based tutoring systems mathematical reasoning capabilities by measuringits average (amount) performance (concept) on standardized test questions (instances) from theKorean CSAT Math Exam (population); or evaluating the risks posed by a conversational searchsystem by measuring the prevalence (amount) of stereotyping (concept) in the systems outputs(instances) in the current US deployment (population). This template is also sufficiently expressiveto capture many of the impact evaluation tasks described by Solaiman et al. . Recent work hasemphasized the need, when carrying out such measurement tasks, to precisely articulate what is beingmeasuredi.e., to formalize the concept of interest. This includes critical work on dataset diversity, benchmark design for stereotyping and other concepts , and model memorization . These critiques parallel those made two decades ago by Adcock and Collier , who argued thatpolitical scientists were devoting insufficient attention to measurement validityi.e., the question ofwhether researchers reported measurements adequately reflect the concepts they purport to measure.To facilitate this type of reflection, Adcock & Collier introduced a four-level framework that",
  "arXiv:2412.01934v1 [cs.CY] 2 Dec 2024": "formalizes the relationship between a concept of interest and reported measurements for that conceptas a structured progression from a background concept (the set of meanings and understandingsassociated with the concept), to a systematized concept (precise definitions), to annotation procedures(procedures for labeling or scoring instances) that operationalize the systematized concept, to theapplication of those procedures to obtain scores or labels for one or more instances. Validity concernsabout concepts then arise as slippage between these four levels, such as a failure of the annotationprocedures to capture relevant dimensions of the systematized concept. Using this framework, theabove-mentioned critiques of GenAI evaluations can be posed as failures to systematize nebulousbackground concepts (e.g., stereotyping) before jumping to operationalization via annotationprocedures (e.g., instructions that ask crowdworkers to label system outputs as stereotyping or not).",
  "Framework Overview": "Although we wholeheartedly agree with these critiques, we argue that equal attention should be paidto validity concerns that arise from the under-specification of amounts, populations, and instancesentailed in such measurement tasks. In other words, valid measurement of GenAI systems capabili-ties, risks, and impacts requires systematizing, operationalizing, and applying not only concepts, butalso amounts, populations, and instances. This involves both descriptive reasoning about particularinstances and inferential reasoning about underlying populations, which is the purview of statistics. To facilitate reflection on such validity concerns, we introduce an extension of Adcock and Colliersframework . A graphical representation of this framework is depicted in . We developedand iteratively refined the framework using numerous examples of GenAI and non-generative AI sys-tems capabilities, risks, and impacts, including topic modeling, representational harm measurementfor multi-modal models, face verification technologies, automated speech recognition, and others. Like Adcock & Colliers framework, our framework includes processes, depicted as backward arrows,for revising and refining earlier levels based on findings, including validity concerns, that arise inlater levels. These revision and refinement processes operate not only within each column (e.g.,within the Concept column, which is adapted from Adcock & Colliers framework), but also acrossthe columns. We may find, for example, that the variance of our measurements is too high, leadingus to revise the sampling design or even the annotation procedures to improve estimator stability. To better understand the elements of our framework, consider the example of measuring stereotypingin a conversational search system. How we represent an instance (e.g., as a system output vs. asa system output and its corresponding input) may influence whether a system output is labeledas stereotyping. Likewise, if we are interested in the prevalence of stereotyping under typical usepost-deployment, but our sampling design (operationalized population) is based on adversarial redteaming, we are likely to vastly overestimate the prevalence. By structuring the choice of metricas first defining a target parameter (systematized amount) that is then estimated using statisticalestimators (operationalized amount), we can apply established statistical inference methods tooptimally trade off between bias and variance, adjust for sample bias in the data set (i.e., correctfor mismatches between the systematized population and the sampling design), construct uncertaintyintervals, and perform hypothesis tests. In , we provide a high-level example of usingour framework to measure stereotyping in a hypothetical conversational search engine, ChatSearch.",
  "Limitations and Future Work": "Although our framework brings structure to the question of what should be done when measuringGenAI systems capabilities, risks, and impacts, we offer no guidance on how to accomplish suchmeasurement tasks. This limitation therefore remains an important direction to explore in future work. A major limitation of the framework itself is that it does not help with formulating measurementtasks in the first place, nor does it offer guidance on how to interpret resulting measurements orwhat decisions those measurements can or should inform. Just as the initial problem formulationcan have a big effect on the fairness of an ML model or the conclusions drawn aboutdeep-learning optimizer performance , what we choose to measure, in what population, andhow we quantify it matters a great deal . The revision and refinement processes provide onemechanism for revisiting these decisions, but this falls well short of addressing the full complexity offormulating measurement tasksan issue that is critically important to evaluations of GenAI systems.",
  "Refinement": ": Our proposed framework for measurement tasks of the form: measure the [amount] of a[concept] in [instances] from a [population]. The figure shows how the four elements that make upsuch tasksamounts, concepts, instances, and populationsare formalized through the sequentialprocesses of systematization, operationalization and application. Elements in earlier levels (rows)can be revised and refined based on findings, including validity concerns, that arise in later levels.",
  "Conclusion": "Our framework helps place many of the disparate-seeming GenAI evaluation practices in use todayon a common footing. This enables individual evaluations to be better understood, interrogatedfor reliability and validity, and meaningfully compared. In this way, the framework is intended toserve as a shared standard for valid measurement of GenAI systems capabilities, risks, and impacts.For example, using it allows us to (1) more easily identify and remedy validity concerns; and (2)compare different measurement tasks by identifying precisely where within the framework theydiverge. In other words, our framework enables us not only to evaluate evaluations but also toimprove how evaluations of GenAI systems are designed in the first place, thereby helping advancesuch evaluations from their current state of disparate-seeming and ad hoc practices towardmore formalized and theoretically grounded processesi.e., toward a science of GenAI evaluations.",
  "Applying operationalization to obtain observations or realizations": "Evaluation goals: The product team for ChatSearch, an LLM-enabled conversational search engine, wants to assess their system for representational harms. Red teaming results show that the system can output responses that stereotype or demean, and they want to quantify the extent of the problem. : A high-level example of using our framework in a hypothetical evaluation of a conversationalsearch engine, ChatSearch. Each cell provides an overview of what a complete measurementprocedure instantiated using the framework could look like. Note that a full instantiation wouldrequire providing considerable additional information. For example, a fully systematized complexconcept or the full description of a complex sampling design might require several pages of exposition.",
  "A Feder Cooper and James Grimmelmann. The Files are in the Computer: Copyright, Memorization, andGenerative AI. arXiv preprint arXiv:2404.12590, 2024": "A. Feder Cooper, Yucheng Lu, Jessica Forde, and Christopher M De Sa. Hyperparameter Optimization IsDeceiving Us, and How to Stop It. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. WortmanVaughan, editors, Advances in Neural Information Processing Systems, volume 34, pages 30813095.Curran Associates, Inc., 2021. A. Feder Cooper, Katherine Lee, Madiha Zahrah Choksi, Solon Barocas, Christopher De Sa, JamesGrimmelmann, Jon Kleinberg, Siddhartha Sen, and Baobao Zhang. Arbitrariness and Social Prediction:The Confounding Role of Variance in Fair Classification. Proceedings of the AAAI Conference on ArtificialIntelligence, 38(20):2200422012, March 2024. Amanda Coston, Alan Mishler, Edward H Kennedy, and Alexandra Chouldechova. Counterfactual riskassessments, evaluation, and fairness. In Proceedings of the 2020 conference on fairness, accountability,and transparency, pages 582593, 2020. Jesse Dodge, Suchin Gururangan, Dallas Card, Roy Schwartz, and Noah A. Smith. Show Your Work:Improved Reporting of Experimental Results. In Proceedings of the 2019 Conference on EmpiricalMethods in Natural Language Processing and the 9th International Joint Conference on Natural LanguageProcessing (EMNLP-IJCNLP), pages 21852194, Hong Kong, China, November 2019. Association forComputational Linguistics. Daphne Ippolito, Florian Tram`er, Milad Nasr, Chiyuan Zhang, Matthew Jagielski, Katherine Lee, Christo-pher A. Choquette-Choo, and Nicholas Carlini. Preventing Verbatim Memorization in Language ModelsGives a False Sense of Privacy, 2023. URL Benjamin Laufer, Thomas Gilbert, and Helen Nissenbaum. Optimizations neglected normative commit-ments. In Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency, pages5063, 2023. Yu Lu Liu, Su Lin Blodgett, Jackie Cheung, Q. Vera Liao, Alexandra Olteanu, and Ziang Xiao. ECBD:Evidence-centered benchmark design for NLP. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar,editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume1: Long Papers), pages 1634916365, Bangkok, Thailand, August 2024. Association for ComputationalLinguistics. URL Nestor Maslej, Loredana Fattorini, Raymond Perrault, Vanessa Parli, Anka Reuel, Erik Brynjolfsson, JohnEtchemendy, Katrina Ligett, Terah Lyons, James Manyika, Juan Carlos Niebles, Yoav Shoham, RussellWald, and Jack Clark. The AI index 2024 annual report. AI Index Steering Committee, Institute forHuman-Centered AI, Stanford University, April 2024.",
  "Kevin Roose. A.I. has a measurement problem. The New York Times, April 2024. URL Accessed: 2024-09-05": "Irene Solaiman, Zeerak Talat, William Agnew, Lama Ahmad, Dylan Baker, Su Lin Blodgett, Canyu Chen,Hal Daume III, Jesse Dodge, Isabella Duan, et al. Evaluating the social impact of generative ai systems insystems and society. arXiv preprint arXiv:2306.05949, 2023. Jamelle Watson-Daniels, Solon Barocas, Jake M Hofman, and Alexandra Chouldechova. Multi-targetmultiplicity: Flexibility and fairness in target specification under resource constraints. In Proceedings ofthe 2023 ACM Conference on Fairness, Accountability, and Transparency, pages 297311, 2023."
}