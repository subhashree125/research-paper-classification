{
  "Abstract": "Transformers are slow to train on videos due to extremely large numbers of inputtokens, even though many video tokens are repeated over time. Existing methodsto remove such uninformative tokens either have significant overhead, negatingany speedup, or require tuning for different datasets and examples. We presentRun-Length Tokenization (RLT), a simple approach to speed up video transformersinspired by run-length encoding for data compression. RLT efficiently finds andremoves runs of patches that are repeated over time prior to model inference,then replaces them with a single patch and a positional encoding to representthe resulting tokens new length. Our method is content-aware, requiring notuning for different datasets, and fast, incurring negligible overhead. RLT yieldsa large speedup in training, reducing the wall-clock time to fine-tune a videotransformer by 30% while matching baseline model performance. RLT also workswithout any training, increasing model throughput by 35% with only 0.1% dropin accuracy. RLT speeds up training at 30 FPS by more than 100%, and on longervideo datasets, can reduce the token count by up to 80%. Our project page is at",
  "Introduction": "Vision transformers have enjoyed enormous success in modeling images and videos due to theirscaling properties and minimal inductive bias. Unfortunately, training these models on videos, whichgenerally have orders of magnitude more tokens than images, is significantly more expensive. Onecontributing factor is that video transformers tokenize videos by splitting them into uniformly sizedspatiotemporal patches , then embed them into a latent token space. As a result, the number oftokens depends only on the videos length and resolution. Researchers are thus forced to work withvery short videos (<10s), as well as significantly downsample them to low frames-per-second (FPS)and low spatial resolution. One promising solution to this problem is to reduce the number of input tokens. Compared tolanguage input, videos are significantly less dense in information; many works observe that videosconsist mostly of redundant or uninformative tokens . However, existing methods that aimto reduce input tokens to vision transformers have had limited adoption. Learned pruning methods reduce model complexity measured by GFLOPS, but either incur significant overhead duringtraining, or require padding to handle changing numbers of tokens, negating any speed-up duringtraining. Random masking , though fast, decreases accuracy and thus requires more trainingtime to match performance. Moreover, although methods like random masking and Token Merging do lead to wall-clock speedups, they are not content-aware: they only remove a fixed number of",
  "tokens per video, and will reduce the same number of tokens from a high-speed, high-action clip asfrom a still image repeated over time": "We argue that content-awareness can help more effectively reduce the number of input tokens. As anexample, imagine an hour-long video of a lecture. Most of the frames are exactly the same over time,displaying a single slide. Existing methods would produce the same number of tokens from this asfrom an hour of motion-heavy GoPro footage, even though the two videos have significantly differentamounts of content. On the other hand, video compressors, such as H.264 and H.265 , areexplicitly content-aware: rather than encoding frames independently, they encode pixel differencesbetween consecutive frames, drastically reducing video size when there is no change. We propose Run-Length Tokenization (RLT), which combines a simpler version of this idea withclassical run-length encoding to tokenize videos for transformers. Our insight is that we can efficientlyidentify runs of input patches that are repeated over time, enabling us to reduce the number of tokensbased on the video content. When tokenizing the video, we compare consecutive patches in time andgroup together patches with sufficiently small differences. We then remove the repeated patches,and treat the remaining tokens as having variable length. Similar to how the string aaaabb can berun-length encoded as a4b2, we can add length information to each of the tokens, which incurs noadditional overhead while retaining some of the information lost from removing the redundant tokens.Despite its simplicity, RLT works remarkably well - with it, we can fine-tune a video transformer in40% faster wall-clock time than baseline ViTs while matching performance. Our contributions are as follows: we (1) propose RLT, an alternative method to tokenize videosfor vision transformers, (2) thoroughly compare its performance and compare RLTs speed to priormethods, finding significant improvements, (3) evaluate RLTs performance on high-FPS and longervideos, and (4) ablate design choices and qualitatively visualize RLTs output. We believe RLT canbe a key step to significantly accelerate and further scale video understanding.",
  "Related Work": "Video Transformers. Vision Transformers have been successfully adapted to video but are generally trained and evaluated on short (<10s) video clips with relatively fewframes. To efficiently handle videos, many works incorporate video-specific inductive biases in theirarchitectures , such as memory , compression cues , or modified attentionmechanisms . Other methods, especially in video generation, project the video to a smallerlatent space and then split it into patches. We instead use the standard ViT formulation butapply a different tokenization scheme, reducing the number of input tokens to improve speed whilemaintaining performance. : RLT Overview. RLT works by comparing temporally consecutive patches, and retainingthose with L1 difference above a threshold . The remaining tokens are augmented with a lengthencoding to signify their run-length and passed to the transformer. Video Tokenization. Prior to vision transformers, video architectures were designed to take in a fixedsize input . However, transformers can handle arbitrary numbers of input tokens , andtraining on variable-sized inputs is standard in language modeling ; this has been used to trainvision transformers with variable resolutions . However, video transformers still generallyuse the spatiotemporal patch tokenization scheme introduced in which is content-agnostic: thenumber of tokens depends only on the videos length and resolution. Some works attempt to reduceinput size by compressing the video to a latent space, then tokenizing , but the numberof tokens still depends strictly on the input video dimensions. On the other hand, standard videocompressors like HEVC and AVC are content-aware: they actively consider the differencesbetween consecutive frames for more efficient compression. Our work applies this idea to videotransformers by condensing static tokens and tracking their length. Faster ViTs with Fewer Tokens. Several works have attempted to remove uninformative tokensfrom vision transformers. One line of work identifies such tokens either through learned modules orattention scores , and prunes them at each layer. Although transformers can handlevariable sized inputs, these methods require padding as token counts change unpredictably witheach layer. Other works combine tokens instead of pruning them (). Most of theseworks require training a model for pruning or merging, with the exception of Token Merging ,which demonstrates strong results at inference time. Inspired by the success of masked pre-training(), another line of work uses random masking to speed up training. Although maskingleads to worse performance after the same number of batches, the dramatic speedup enables trainingfor more epochs in less time . In contrast, our method matches the performance ofbase models with the same amount of data with large speedups, and can be stacked with randommasking for even more speed benefits. Closely related to our method are EVEREST and STA which both exploit temporal similarity to identify redundant tokens. However, like both theseworks require setting a constant number of tokens to remove from each video, while RLT can removevarying numbers of tokens based on the video content.",
  "Method": "Consider a vision transformer that takes as input a video V RCT HW . The standard tok-enization scheme splits V into a set P of uniformly sized, non-overlapping patches, each with sizeC Dx Dy Dt, with Pt called the tubelet size. These patches are projected to a lower dimensiondembed with an MLP E, resulting in NP tokens, with each corresponding to a distinct spatiotemporallocation. This results in the same number of tokens for any input video that has the same size. In contrast, our goal is to to identify input patches that are extremely similar, then compress theseredundant patches, increasing throughput and training time. Our approach is illustrated in .In particular, we focus on temporally consecutive patches, those which have the same x, y location",
  "Removing Static Patches": "Token Similarity. Unlike prior works, we aim to reduce the number of total input tokens bycomparing patches rather than tokens. By operating on patches, we do not need to run the patchembedding E or any layer of the model. As a result, we do not need to freeze parts of the model orpropagate gradients through the pruning operation, which would require padding and negate potentialspeedups. This contrasts with prior works which progressively prune or combine tokens after eachlayer in the transformer. Furthermore, by identifying redundant patches, we can pre-compute thetoken distributions of various datasets and sizes of examples, allowing us to employ techniques likeexample-packing . Finally, operating on visual patches is more interpretable and is similar to theheuristics used by video encoders . We next define a criterion for determining whether two consecutive patches are static. Considertwo temporally consecutive patches P1, P2 that correspond to spatial location (x, y) and temporallocations t1, t2 with t2 = t1 + Dt. For tubelet sizes with value Pt > 1, each patch consists ofmultiple frame crops, so that P1 = [P t1xy, P t1+1xy, ...P t1+Dt1xy]. Given a threshold , we consider P1and P2 static ifP t2+Dt1xy P t1xy1 < (1) with P t2+Dt1xybeing the temporally last spatial crop of in P2 and P t1xy the first spatial crop of P1.This operation compares the start of the P1 to the end of P2, with the idea being that if the firstcrop of token P1 matches the last crop of token P2, the patches in between likely match as well.Notably, is a hyperparameter that needs to be tuned, but is dataset-agnostic; it simply encodeshow much change between patches is allowed before they are considered different. controls thetrade-off between speed and accuracy; while higher values reduce significantly more tokens, theytreat tokens that are perceptibly different as being the same, reducing accuracy. We use > 0 sinceimperceptible artifacts can occur, and follow standard procedure by running ImageNet normalizationbefore comparing patches. We typically use = 0.1, and provide experiments and visualizations onits effect in .3 and Appendix B. Pruning Procedure.To identify all static tokens, we run the prior comparison on all pairs of tem-porally consecutive patches in P obtaining their differences and only retaining those with differenceless than . We always include the entirety of the first frame since there is no previous patch tocompare it to. This results in a binary mask Mstatic, which we can then apply with",
  "P = P Mstatic(2)": "with P containing NP tokens and P consisting of NP tokens. Note that NP NP is alwaystrue; with RLT, we can never have more tokens than in the standard tokenization procedure, sothe worst-case performance matches the standard vision transformer. RLT also incurs essentiallyno overhead as the entire process can be implemented entirely with parallelizable PyTorch operations on the GPU, so training and inference are strictly faster. The simplicity of RLT is a major advantage: in contrast to other methods, we can take advantage oftransformers ability to handle variable input sizes, and do not need to provide any additional padding.Because we make no changes to the model itself, a video transformer using RLT can make use ofhardware optimizations like Flash Attention and memory efficient kernels . Notably, the pruning procedure is content-aware: some videos with large amounts of static contentwill result in significantly fewer input tokens than videos with significant amounts of camera orsubject motion. This is a desired outcome, and we discuss how to handle training with dynamic inputsizes in .3.",
  "Run-length Positional Encoding": "Although we have reduced the number of input patches, we know that each patch represents a runof static patches, with length 1 corresponding to no static content, and length T corresponding toinput time dimension length. Without information about the length of the run of static patches, thetransformer may not be able to compensate for information removed during the pruning procedure. To address this, Bolya et al. introduced Proportional Attention, which weights each token by thenumber of tokens in each group. On the other hand, we opt to let the model learn this information:we treat each token as having variable length that we can communicate through a new positionalencoding. Specifically, we use a factorized encoding, described in Dehghani et al. , with oneencoding xyt containing positional information and the other L corresponding to the length. Weuse a learnable length bias L consisting of a single parameter of size (T, dembed). For a given runof repeated patches, we always retain the initial patch Pxyt, and thus can compute the new length ias the distance from xyt to the nearest 1 entry in Mstatic along the t-axis. Concretely, for Pxyti = mint (t t),whereMstatic(x, y, t) = 1, t > t(3)",
  "This operation can also be efficiently implemented on the GPU, adding no overhead. Then, the fullpositional encoding becomes(Ti) = xyt(Ti) + L[i](4)": "with the L[i] representing the indexing operator. We add the positional encoding (Ti) to eachtoken after running the patch embedding network E. Unlike the pruning procedure, since we usea learnable length encoding L, we propagate gradients to the positional embedding, enabling themodel to learn how to optimally encode variable length tokens during fine-tuning.",
  "Handling Dynamic Input Sizes": "Since RLT is content-aware, the number of tokens varies significantly per example. Althoughtransformers can natively handle any input size , prior methods like DynamicViT or A-ViT produce different numbers of tokens at each layer; this requires padding or attention maskingto handle batched inference during training. In our case, only the input token count is variable, butthe number of tokens stays constant throughout the network, closer to the setting of NaViT .Furthermore, since we know the input size before running the network, we can employ examplepacking , an idea from language modeling where multiple inputs with variable sizes are packedtogether, and tokens from individual examples attend only to each other. At training time, the input to the transformer consists of a batch of tokenized videos, V1, V2, ...VB,each with size T1, T2, ...TB.Rather than pass an input (B, maxi Ti, dembed) to the network,we concatenate the video tensors to produce V = V1 V2 V3...VB, resulting in input size(1, Bi=1 Ti, dembed). We then construct a block-diagonal attention mask so that tokens only attendto other tokens from the same video, which we add during the attention operation. Since every tokenin V is attending only to tokens from the same example, this does not reduce throughput and isalso compatible with existing hardware-efficient attention implementations. To compute the classprediction in action recognition, we split each example out and compute its prediction as the meanof each example token, as in . We then project it to dimension NC, resulting in output of size(B, NC) to which we can apply standard cross-entropy losses during training. We note that typically example packing results in a constant number of input tokens, with a variablenumber of input examples. A key difference between RLT and Dehghani et al. is that dataaugmentations such as RandAugment can alter the visual content and thus number of tokens ofinput videos, rendering greedy example packing strategies inapplicable during data loading. We optto use a constant number of examples per GPU, with high enough batch size sufficiently reducingvariance in input size.",
  "Experimental Results": "To analyze RLTs impact on performance and speed, we conduct several experiments on standardaction recognition tasks. We measure the speedup on model training at several scales in .1as well as RLTs effect as a drop-in addition at inference time in .2. We perform ablations in.3, then evaluate RLTs effect on higher FPS videos and long video datasets in .4.Finally, we provide qualitative visualizations in .5.",
  ": Training results on action recognition. RLT significantly reduces fine-tuning time withcomparable performance to the baseline on both Kinetics-400 and Something-Something-v2": "comparing the speed and performance with standard tokenization, random masking, and RLT. Weevaluate random masking by removing k tokens, with k being the mean number of tokens pruned byRLT on a given dataset. For the most fair speed comparison, all evaluated models are trained withmixed-precision, memory-efficient attention and Flash Attention where possible using an 8xH100node, as well as the optimized data loader from AVION to avoid data loading bottlenecks. Weuse the standard Vision Transformer rather than more complex architectures such as TimesFormer or MViT ; we found that it was significantly simpler and more efficient, matching observationsfrom Ryali et al. . We limit our analysis to fine-tuning due to computational constraints. Wecompare against the baseline vision transformer, as well as Token Merging and STA . We alsoinclude a random masking baseline where the masking fraction is set to the average number of tokensremoved by RLT, which is a stronger baseline than using a fixed standard fraction such as 0.5. Compared to standard tokenization, RLT achieves a speed-up of up to 40%, even with heavilyoptimized implementations. RLT achieves the best trade-off between performance and speed, withbetter performance than random masking while achieving the same speedup. This demonstrates thatthe choice of which tokens to remove makes a nontrivial difference, and that properly identifyingredundant tokens is important. Compared to standard tokenization, RLT achieves a speed-up of up to 40%, even with heavily opti-mized implementations. RLT achieves the best trade-off between performance and speed, with betterperformance than random masking while achieving the same speedup. In particular, RLT is muchfaster to train than Token Merging since it is compatible with hardware-optimized implementationssuch as Flash-Attention . Unlike random masking, RLT matches the performance of the baselineViT after the same number of training batches, while random masking requires significantly moreepochs to catch up. RLT matches baseline performance across multiple scales, indicating that RLTdoes not degrade performance while considerably accelerating training.",
  "Inference-Time Results": "Although RLT was designed to speed up training, it can be used as a drop-in replacement for standardtokenization, similar to Token Merging. In we compare the top-1 accuracy, GFLOPs andthroughput with RLT to standard tokenization and Token Merging . We also compare againstrandom masking for completeness, although it is intended only for training time . For the mostfair comparison, we randomly mask out P tokens for each example, where P is the mean number oftokens used by RLT; for Kinetics-400 and SSv2 this was P = 0.72. We do not compare to learnedpruning methods like A-ViT since those only present results on images. We measure throughputin clips-per-second, with each model running on a single clip at a time. In practice, video models areevaluated on multiple temporal and spatial crops; following VideoMAE we measure GFLOPs onsingle clip and measure accuracy with 4 temporal and 3 spatial crops. Across model sizes, RLT consistently delivers the best tradeoff between speed and accuracy. Thebenefit becomes more pronounced as model size increases, as at larger parameter counts, the attentionoperation begins to dominate the computation. Compared to baselines, RLT is significantly faster thanToken Merging and outperforms all other baselines on accuracy. Token Merging cannot make use ofFlash Attention and other optimizations due to its reliance on a weighted attention operation, slowing",
  ": Effect of length encoding. When fine-tuning with RLT only, length encoding has mini-mal effect, but helps significantly when combinedwith random masking": "it down in comparison to RLT. Although worse than RLT, random masking performs surprisinglywell, likely due to the fact that most tokens in videos are redundant. Random masking can also becombined with RLT for further speed benefits, with smaller resulting performance gaps than in .However, achieving the optimal performance-throughput tradeoff with random masking requirestuning for each dataset, while RLT is natively content-aware, achieving higher accuracy at similarspeeds without tuning. Similarly, Token Merging requires changing the r parameter based on themodel size and is not content aware, limiting its speed-up in highly static videos.",
  "RLT3076.147.5h2.0": ": Training at higher FPS. RLT en-ables training efficiently for higher FPS, al-lowing us to go beyond the standard low FPSparadigm. As FPS increases, RLT deliverslarger and larger speed-ups over the baselinefor training, with no decrease in accuracy. Difference Threshold.The only tunable hyperparameter in RLT is the threshold , which controlsthe sensitivity to change between temporally consecutive tokens. Lower values of indicate highersensitivty to change. We vary tau and compare the final action recognition accuracy vs. throughputand wall-clock time for several configurations, both for training and inference. These results areshown in . We find that using = 0.1 offered the best tradeoff in speed and performance: itmatches the baseline performance while delivering a 37% speedup in training. Lower values of leadto similar performance, but with less of a speedup, while high values deliver larger speedups at a costto performance. We attribute this to the existence of a difference cut-off: at some point, the tokensare too different to be grouped together, and the resulting tokens do not obey the assumptions madeby RLT. We also note that is dataset-agnostic: it simply describes how much pixel difference isneeded to consider two 16x16 patches different, and the same value of leads to different reductionsacross datasets based on the video content. Length Encoding.We ablate the effect of our length encoding mechanism in . Whenusing RLT by itself, length encoding has minimal effect. However, when combining RLT withrandom masking, we note a clear improvement. Due RLTs structured and predictable pruning, lengthencoding may be unnecessary: the transformer is able to mostly understand the length of varioustokens by their associated spatial positional encoding. However, once random masking is introduced,the structure is removed, and the length encoding adds crucial information. Since including the lengthencoding is strictly more information and has no negative effect, we default to including it.",
  "Longer Videos and Higher FPS": "Standard action recognition datasets consist of short clips with downsampled FPS; an input exampletypically spans 2 seconds. One potential advantage of RLT is that by reducing the total number oftokens, training becomes more tractable for both longer videos and higher FPS. We evaluate theeffect of training with RLT in on action recognition datasets with higher FPS along withtheir training time. As before, we fine-tune these models from pre-trained VideoMAE checkpoints.Although these checkpoints were pre-trained at 7.5 FPS, we can still compare with the baselineperformance to observe differences in training time or quality. Similar to the result from ,we find that ViTs trained with RLT can match performance but train significantly faster, with thespeed-up increasing with the FPS. We next analyze the number of total tokens in RLT compared to the baseline for several video datasetsin , including datasets with longer videos as well as higher FPS. Matching the result from, at higher FPS, RLT consistently reduces the tokens by a higher proportion. This matches : Sample Visualizations. Tokens that are compressed are visualized in gray. RLT retainstokens that change between frames while removing redundant tokens. In the top example, RLTcaptures the static background, and in the bottom example, due to camera motion and the motion ofthe girl, almost no tokens are modified. Video visualizations are available at the project page. our intuition, since tokens between two redundant tokens at lower FPS are likely to be similar andalso be removed. Furthermore, on longer video datasets, RLT can reduce the number of tokens bysignificantly larger margins, with reductions of up to 80% on COIN and Breakfast. These datasets inparticular consist of videos filmed with fixed cameras and largely static backgrounds, demonstratingRLTs potential to drastically speed up transformers on these types of videos. Although in practice,researchers do not typically train on raw videos with large number of frames due to the heavy costof video decoding on academic clusters, RLT presents a promising way to efficiently train on thesevideos at scale.",
  "Visualizations": "We provide some qualitative visualizations of the tokens RLT removes in . As desired, inputpatches that are repeated over time are pruned by RLT. This intuitively matches with how humansoften pay less attention to static tokens over time. In the top example, most of the background is black,with some motion taking place in the foreground. RLT is able to remove the constant black portions,drastically reducing the number of tokens. Similarly in the second example, RLT ensures that thetokens containing motion, with the boys hands and instrument, are not modified, but prunes the staticbackground. In the lower two examples, the person using the drill and the girl in the foreground movearound significantly, reducing the amount of tokens that can be compressed. In such cases wherethere is significant subject or camera motion, RLT removes fewer tokens, resulting in similar token : Effect of . With low values of , the clearest repeated patches are ablated, but imperceptiblevariations can prevent some visibly similar tokens from being pruned. Above = 0.1, some tokenswith slight movement are pruned. counts to standard tokenization. However, the sensitivity of RLT to small perturbations and motiondepends entirely on the hyperparameter. We provide further example visualizations and visualizethe effect of different values of in Appendix B and on our project page. In we demonstratethe effect that the hyperparameter has on the input tokens. We see that as increases, more andmore patches are included, and after = 0.1, some patches that have change in them are prunedincorrectly. On the other hand, = 0 includes many patches with essentially imperceptible change,which is also undesired.",
  "Conclusion": "SummaryWe present Run-Length Tokenization (RLT), a simple alternative to standard videotokenization for video transformers that replaces temporally redundant tokens with a single token ofvariable length. RLT decreases transformer training and inference wall-clock time by up to 40%machieves a better speed-accuracy tradeoff than prior works, and is simple to implement and combinewith other methods. RLT demonstrates strong results during finetuning, especially at higher FPS, andeven works well when applied to models without any training. LimitationsThough RLT works well, it relies on a heuristic to compare temporally consecutivetokens, which could include extra tokens that are unused by the transformer. While RLT speeds upvideo transformers significantly, it cannot be used for dense vision tasks, such as point tracking orvideo generation, that require the same number of output tokens as input tokens; RLT reduces tokensbefore running the model and does not replace them. Furthermore, RLT does not handle cameramotion well: in a video with constant camera motion, few tokens will be removed, leading to nospeedup. Future work will be necessary to overcome these limitations, and we hope that RLT caninspire more research on efficient video transformers.",
  "This work is supported by Fujitsu Research of America, and RC is supported by the NSF GRFP": "Hassan Akbari, Liangzhe Yuan, Rui Qian, Wei-Hong Chuang, Shih-Fu Chang, Yin Cui, andBoqing Gong. Vatt: Transformers for multimodal self-supervised learning from raw video,audio and text. Advances in Neural Information Processing Systems, 34:2420624221, 2021. Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lucic, and CordeliaSchmid. Vivit: A video vision transformer. In Proceedings of the IEEE/CVF internationalconference on computer vision, pages 68366846, 2021.",
  "Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, andJudy Hoffman. Token merging: Your ViT but faster. In International Conference on LearningRepresentations, 2023": "Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr,Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh.Video generation models as world simulators. 2024. URL Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practicalautomated data augmentation with a reduced search space. In Proceedings of the IEEE/CVFconference on computer vision and pattern recognition workshops, pages 702703, 2020.",
  "Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. arXivpreprint arXiv:2307.08691, 2023": "Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R. Flashattention: Fast andmemory-efficient exact attention with io-awareness. Advances in Neural Information ProcessingSystems, 35:1634416359, 2022. Mostafa Dehghani, Basil Mustafa, Josip Djolonga, Jonathan Heek, Matthias Minderer, MathildeCaron, Andreas Steiner, Joan Puigcerver, Robert Geirhos, Ibrahim M Alabdulmohsin, et al.Patch npack: Navit, a vision transformer for any aspect ratio and resolution. Advances inNeural Information Processing Systems, 36, 2024. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprintarXiv:2010.11929, 2020. Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolutionimage synthesis. In Proceedings of the IEEE/CVF conference on computer vision and patternrecognition, pages 1287312883, 2021. Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra Malik, andChristoph Feichtenhofer. Multiscale vision transformers. In Proceedings of the IEEE/CVFinternational conference on computer vision, pages 68246835, 2021. Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. Slowfast networks forvideo recognition. In Proceedings of the IEEE/CVF international conference on computervision, pages 62026211, 2019.",
  "Christoph Feichtenhofer, Haoqi Fan, Yanghao Li, and Kaiming He. Masked autoencoders asspatiotemporal learners. arXiv:2205.09113, 2022": "Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, SusanneWestphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag,et al. The\" something something\" video database for learning and evaluating visual commonsense. In Proceedings of the IEEE international conference on computer vision, pages 58425850, 2017. Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollr, and Ross Girshick. Maskedautoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference oncomputer vision and pattern recognition, pages 1600016009, 2022.",
  "Diederik P Kingma. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013": "Zhenglun Kong, Peiyan Dong, Xiaolong Ma, Xin Meng, Wei Niu, Mengshu Sun, Xuan Shen,Geng Yuan, Bin Ren, Hao Tang, et al. Spvit: Enabling faster vision transformers via latency-aware soft token pruning. In European conference on computer vision, pages 620640. Springer,2022. Bruno Korbar, Du Tran, and Lorenzo Torresani. Scsampler: Sampling salient clips from videofor efficient action recognition. In Proceedings of the IEEE/CVF International Conference onComputer Vision, pages 62326242, 2019. Mario Michael Krell, Matej Kosec, Sergio P Perez, and Andrew Fitzgibbon. Efficient sequencepacking without cross-contamination: Accelerating large language models without impactingperformance. arXiv preprint arXiv:2107.02027, 2021. Hilde Kuehne, Ali Arslan, and Thomas Serre. The language of actions: Recovering the syntaxand semantics of goal-directed human activities. In Proceedings of the IEEE conference oncomputer vision and pattern recognition, pages 780787, 2014. Benjamin Lefaudeux, Francisco Massa, Diana Liskovich, Wenhan Xiong, Vittorio Caggiano,Sean Naren, Min Xu, Jieru Hu, Marta Tintore, Susan Zhang, Patrick Labatut, Daniel Haziza,Luca Wehrstedt, Jeremy Reizenstein, and Grigory Sizov. xformers: A modular and hack-able transformer modelling library. Yanghao Li, Chao-Yuan Wu, Haoqi Fan, Karttikeya Mangalam, Bo Xiong, Jitendra Malik, andChristoph Feichtenhofer. Mvitv2: Improved multiscale vision transformers for classificationand detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 48044814, 2022. Yanghao Li, Haoqi Fan, Ronghang Hu, Christoph Feichtenhofer, and Kaiming He. Scalinglanguage-image pre-training via masking. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, pages 2339023400, 2023. Youwei Liang, Chongjian Ge, Zhan Tong, Yibing Song, Jue Wang, and Pengtao Xie. Not allpatches are what you need: Expediting vision transformers via token reorganizations. arXivpreprint arXiv:2202.07800, 2022.",
  "William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedingsof the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023": "Yongming Rao, Wenliang Zhao, Benlin Liu, Jiwen Lu, Jie Zhou, and Cho-Jui Hsieh. Dynamicvit:Efficient vision transformers with dynamic token sparsification. Advances in neural informationprocessing systems, 34:1393713949, 2021. Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma,Haitham Khedr, Roman Rdle, Chloe Rolland, Laura Gustafson, et al. Sam 2: Segment anythingin images and videos. arXiv preprint arXiv:2408.00714, 2024.",
  "Karen Simonyan and Andrew Zisserman.Two-stream convolutional networks for actionrecognition in videos. Advances in neural information processing systems, 27, 2014": "Gary J Sullivan, Jens-Rainer Ohm, Woo-Jin Han, and Thomas Wiegand. Overview of the highefficiency video coding (hevc) standard. IEEE Transactions on circuits and systems for videotechnology, 22(12):16491668, 2012. Yansong Tang, Dajun Ding, Yongming Rao, Yu Zheng, Danyang Zhang, Lili Zhao, Jiwen Lu,and Jie Zhou. Coin: A large-scale dataset for comprehensive instructional video analysis. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages12071216, 2019. Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. Videomae: Masked autoencoders aredata-efficient learners for self-supervised video pre-training. Advances in neural informationprocessing systems, 35:1007810093, 2022. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural informationprocessing systems, 30, 2017. Limin Wang, Bingkun Huang, Zhiyu Zhao, Zhan Tong, Yinan He, Yi Wang, Yali Wang, andYu Qiao. Videomae v2: Scaling video masked autoencoders with dual masking. In Proceedingsof the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1454914560,2023. Thomas Wiegand, Gary J Sullivan, Gisle Bjontegaard, and Ajay Luthra. Overview of the h.264/avc video coding standard. IEEE Transactions on circuits and systems for video technology,13(7):560576, 2003.",
  "RossWightman.Pytorchimagemodels. 2019": "Chao-Yuan Wu, Manzil Zaheer, Hexiang Hu, R Manmatha, Alexander J Smola, and PhilippKrhenbhl. Compressed video action recognition. In Proceedings of the IEEE conference oncomputer vision and pattern recognition, pages 60266035, 2018. Chao-Yuan Wu, Yanghao Li, Karttikeya Mangalam, Haoqi Fan, Bo Xiong, Jitendra Malik,and Christoph Feichtenhofer. Memvit: Memory-augmented multiscale vision transformer forefficient long-term video recognition. In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 1358713597, 2022.",
  "Zhirong Wu, Zihang Lai, Xiao Sun, and Stephen Lin. Extreme masking for learning instanceand distributed visual representations. arXiv preprint arXiv:2206.04667, 2022": "Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon, Thomas Breuel, Jan Kautz, and XiaolongWang. Groupvit: Semantic segmentation emerges from text supervision. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1813418144,2022. Hongxu Yin, Arash Vahdat, Jose M Alvarez, Arun Mallya, Jan Kautz, and Pavlo Molchanov.A-vit: Adaptive tokens for efficient vision transformer. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition, pages 1080910818, 2022. Pengchuan Zhang, Xiyang Dai, Jianwei Yang, Bin Xiao, Lu Yuan, Lei Zhang, and Jianfeng Gao.Multi-scale vision longformer: A new vision transformer for high-resolution image encoding. InProceedings of the IEEE/CVF international conference on computer vision, pages 29983008,2021.",
  "Our code, demos and associated blog post are all located on our project page. In this section, weprovide further details on implementation details of our experiments": "Architecture.All models used were based on the timm Vision Transformer implementation,and all fine-tuning experiments were done with pre-trained checkpoints from VideoMAE andVideoMAEv2 . As mentioned in 3.3, we compute output predictions for action recognition bytaking the mean across the output tokens, rather than producing a separate class token. Baselines.The baselines we compared to are Token Merging and random masking . For allrandom masking experiments, we set the masking ratio to match the mean RLT token reduction forthe given dataset. For example, on Kinetics-400 at 7.5 FPS, RLT with = 0.1 reduces the number oftokens by 28%, so we randomly drop 28% of the tokens during training. We use the recommendedvalues of r from the Token Merging paper, except on ViT-H, where we use r = 32 due to the largerdepth of the model. Datasets.We train and evaluate RLT on Kinetics-400 (K400) and Something-Something-v2(SSv2) . Both datasets are video classification datasets, with K400 having 400 classes and SSv2having 174. K400 has 240k training examples and 40k test examples, while SSv2 has 170k trainingexamples and 30k test examples. We also included experiments measuring the token reduction onthe Breakfast and COIN datasets, both of which are smaller-scale datasets involving longervideos that range from 2-5 minutes. In particular, these datasets contain lots of fixed-camera videoswith static backgrounds, leading to particularly high token reductions from RLT. Training Recipe.We do not change hyperparameters when finetuning models with differenttokenization strategies, as we found the provided set to be optimal in our experiments. We followthe recommended training recipes from VideoMAE for each model size, namely training for up to100 epochs, with batch size 256, learning rate with warm-up to 1 103 for 5 epochs, then cosineannealing down to 1 106. We also use RandAugment, random erasing, CutMix, and standardcropping/scaling and flipping. We do not use MixUp since it can severely affect the efficacy ofRLT, and we found that removing it and only using CutMix did not affect our experiments. We alsoused random erasing with a single value rather than noise, enabling some of the erased tokens to beremoved by RLT. All experiments were conducted with 8xH100 Nvidia GPUs with 128 CPU cores, with 16 workersper GPU. The inference-time results were computed on a single GPU, along with the throughput andFLOPS analysis. One important detail is that data loading is often a bottleneck. We mainly reliedon the fast video data loader from AVION , but NVIDIA DALI also works very well. However,we only recomend to use DALI on A100 or newer chips, as earlier generations have an insufficientnumber of dedicated decoder hardware. Each training run for the paper is specified in hours, but thisdoes not include a few months of work testing and debugging. We used a single node for all work onthis paper.",
  "BMore Visualizations": "We include some additional visualizations here to qualitatively demonstrate which tokens RLT prunes,as well as to analyze the qualitative effect of varying the difference threshold . In each figure, thewhitened patches represent those RLT identified as static, and that are not passed to the transformer.In , we visualize a diverse range of samples and note that RLT consistently prunes out patchesthat repeat across consecutive frames. One case where RLT fails to remove many tokens is the 4thexample from the top, which is from a ski jumper using a GoPro; the constant camera motion meansthat RLT is unable to identify almost any repeated patches."
}