{
  "Abstract": "We consider the problem of hypothesis testing for discrete distributions. In the standardmodel, where we have sample access to an underlying distribution p, extensive research hasestablished optimal bounds for uniformity testing, identity testing (goodness of fit), and closenesstesting (equivalence or two-sample testing). We explore these problems in a setting where apredicted data distribution, possibly derived from historical data or predictive machine learningmodels, is available.We demonstrate that such a predictor can indeed reduce the numberof samples required for all three property testing tasks. The reduction in sample complexitydepends directly on the predictors quality, measured by its total variation distance from p. A keyadvantage of our algorithms is their adaptability to the precision of the prediction. Specifically,our algorithms can self-adjust their sample complexity based on the accuracy of the availableprediction, operating without any prior knowledge of the estimations accuracy (i.e. they areconsistent). Additionally, we never use more samples than the standard approaches require,even if the predictions provide no meaningful information (i.e.they are also robust).Weprovide lower bounds to indicate that the improvements in sample complexity achieved by ouralgorithms are information-theoretically optimal. Furthermore, experimental results show thatthe performance of our algorithms on real data significantly exceeds our worst-case guaranteesfor sample complexity, demonstrating the practicality of our approach. Part of this work was conducted while the authors were visiting the Simons Institute for the Theory of Computing.Part of this work was done while MA was at the Department of Computer Science at Boston University andthe Khoury College of Computer Sciences at Northeastern University.Supported by NSF awards CNS-2120667,CNS-2120603, CCF-1934846, and BUs Hariri Institute for Computing.Supported by the NSF TRIPODS program (award DMS-2022448) and the Simons Investigator Award.Supported by NSF awards CCF-2006664, DMS-2022448 and CCF-2310818.",
  "Introduction": "Property testing of distributions is a fundamental task that lies at the heart of many scientificendeavors: Given sample access to an underlying unknown distribution p, the goal is to inferwhether p has a certain property or it is -far from any distribution that has the property (in somereasonable notion of distance, such as total variation distance) with as few samples as possible. Overthe past century [NP33], this problem has been extensively explored in statistics, machine learning,and theoretical computer science. Indeed, distribution testing (also called hypothesis testing) is nowa major pillar of modern learning theory and algorithmic statistics, with applications in learningmixtures of distributions such as Gaussians, Poisson Binomial Distributions and robust learning[DKK+19, DKS16, DK14, SOAJ14, DDS12, DKS16]. The framework has also been extensivelystudied under privacy [GKK+20, AJM20, CKM+20, BB20, CKM+19, ACFT19, She18, GR18] andlow-memory constraints [ABS23]. One of the most natural and well-studied questions in this framework is: given sample access toan unknown distribution p, can we determine whether p is equal to another distribution q, or -farfrom it (e.g. in total variation distance)? This problem has been studied under various assumptionsabout how we access q: It is called uniformity testing when q is a uniform distribution, identitytesting (or goodness-of-fit) when a description of q is known, and closeness testing (two-sampletesting or equivalence testing) when we only have sample access to q.The primary goal insolving these tasks is to design algorithms that use as few samples as possible. Optimal samplecomplexity bounds have been established for discrete distributions p and q over a domain of sizen: (n/2) samples for uniformity testing [GR11, BFR+00, Pan08, DGPP19] and identity test-ing [VV17a, ADK15, DGPP18], and n2/3/4/3 + n/2samples for closeness testing [BFR+13,",
  "CDVV14, DGPP19, DGPP18]. Other related versions of uniformity, identity, and closeness testingare presented in [LRR13, AJOS14, BV15, Gol20, DKS18, SDC18, AKR19, AS20, OFFG21]": "Given that the aforementioned results are tight and cannot be improved, any further progressrequires equipping the algorithm with additional functionality. A natural approach is to leveragethe fact that in numerous applications, the underlying distribution is not completely unknown;some prediction of the underlying distribution may be available or can be learned via a predictivemachine learning model. For example, if distributions evolve slowly over time, earlier iterations canserve as approximations for later ones, e.g., network traffic data and search engine queries. Suchestimations can often be learned from older data by using it to train a predictor or regressor. Inlinguistics and text processing tasks that involve distributions over words, the length of a word canapproximate its frequency, since longer words are known to be less frequent. Another example iswhen data is available at different scales. For instance,demographic data on loan defaults at thenational level could be informative for data from specific areas. One challenge to using such information is that it rarely comes with a guarantee of precision. Thisfact leads to the information being deemed unreliable, as it may poorly predict the underlyingdistribution. For example, while the national loan default rate might be close to that of a typicalarea code, it could differ significantly in affluent areas. Thus a natural algorithmic question thatarises is how to design algorithms that can exploit predictive information as much as possible withoutany prior assumptions about its accuracy. The goal is then to design an algorithm that solves theproblem with as few samples as possible, given the quality of the prediction. In this work, we study the fundamental problems of uniformity, identity, and closeness testing inthe setting where a prediction of the underlying distribution is provided. This prediction can beformalized by assuming that the distribution testing algorithm has access to a predicted distribution p of p1. This is in addition to having sample access to p as in the standard model (without access toprediction). Our algorithms achieve the optimal reduction in the number of samples used comparedto the standard case, where the improvement depends on the quality of the predictor p in termsof its total variation distance from p. Our algorithms can also self-adjust their sample complexityto the accuracy of p, minimizing sample complexity wherever feasible, without prior knowledge ofps accuracy. Our approach ensures that the algorithm is resilient to inaccuracies in predictionsand does not exceed the optimal sampling bound in the standard model, even when p significantlydeviates from the actual p. Furthermore, our matching lower bounds demonstrate the optimality ofour algorithms. Experimental results additionally confirm the practicality of our algorithms. Measuring accuracy of predictor.We use the total variation distance between the prediction pand the unknown distribution p as our measure of predictor accuracy. Previous work often assumeda strong element-wise guarantees, where pi is within a constant multiplicative factor of pi for alldomain elements i, a constraint that becomes limiting especially for small pi (see .3). Thispaper is the first to study a notion of average error between p and p, measuring via the TV distance.This metric was chosen for its prevalent use in statistical inference and its intuitive interpretation.",
  "Our approach and problem formulation": "Our approach to solving a distribution testing problem (uniformity, identity, or closeness testing)consists of two components: search and test. At a high level, search aims to guess p pTV, andtest performs the actual distribution testing using the guess of the accuracy provided by search asa suggested accuracy level. More precisely, our augmented test component aims to evaluate whether p = q, while receiving pand a suggested accuracy level (which may or may not reflect the true distance between p and p).In addition to two possible outcomes of accept and reject in the standard setting2, our augmentedtest component may output inaccurate information when it determines that p is not -close to p.Our requirements for the augmented test component are the following: i) If the test is conclusive,i.e., it chooses to output accept or reject, the answer should be correct regardless of ps accuracy.ii) If p is indeed -close to p, the test component should not output inaccurate information. We emphasize that the guess is not guaranteed to be correct or may not even be a valid upperbound on the true TV distance. Thus, the algorithm is afforded a degree of flexibility to foregosolving the problem when the distributions are not within proximity (and can try again with anew guess). Our search component aims to identify an appropriate accuracy level such that the test componentcan test in a conclusive manner by returning accept or reject. Since the true value distance p pTVis not known, we start by guessing a small , corresponding to the most accurate p and the fewestsamples needed, run the augmented test component with this and p, and verify the conclusivenessof the testing. If inconclusive, our guess is increased to a level that we can afford testing bydoubling the sample size, and the search component proceeds with the next . It continues untilthe desired accuracy is reached, and accept or reject is returned. Then, search halts with that result.",
  "Clearly, if the accuracy level guess is at least p pTV, the test component has to output": "1We assume we know all of the probability values of the prediction p at all domain elements.2If p = q, the standard tester must output accept with high probability. And, if p is -far from q, the standardtester must output reject with high probability. If p = q, but is -close to q, either answer is considered correct. accept or reject with high probability.Thus, we show that it is unlikely that search proceedswhen p pTV. Therefore, this method does not significantly increase the sample complexity,potentially increasing it by at most an O(log log(n/)) factor in expectation. The search componentis applicable to all of the problems we study and we defer all details of the search component to. The remainder of the main body focuses on designing the augmented testers, i.e. thetest component. Definition 1.1 (Augmented tester). Suppose we are given four parameters, , (0, 1), (0, 1), n N, and two underlying distributions p and q, along with a prediction distribution pover [n]. Suppose A is an algorithm that receives all these parameters, and the description of p asinput, and it has sample access to p and q. We say algorithm A is an (, , )-augmented tester forcloseness testing if the following holds for every p, q, and p over [n]:",
  "If p is -far from q, then the algorithm outputs accept with a probability at most /2": "In this definition, if the description of q is known to the algorithm instead of having sample access,we say A is an (, , )-augmented tester for identity testing. If q is a uniform distribution over [n],then we say A is an (, , )-augmented tester for uniformity testing. To highlight the distinction between this definition and the standard definition, note that in thestandard regime, no prediction p is available to the algorithm, and the algorithm lacks the optionof outputting inaccurate information.",
  "Our results": "Our theoretical results:In this paper we demonstrate that predictions can indeed reduce thenumber of samples needed to solve the three aforementioned testing problems.Our algorithmsare parameterized by both and . We provide tight sample complexity results (matching upperand lower bounds) for these problems. The sample complexity drops drastically compared to thestandard case, depending on the total variation distance between q and p. Our algorithms are alsorobust: if the prediction error is high, our algorithm succeeds by using (asymptotically) the samenumber of samples as the standard setting without predictions.",
  "Theorem 4 (Informal version of Theorem 8). Augmented closeness testing for distributions over[n], with parameters , , and = 2/3, requires (n2/31/3/4/3 + n/2) samples": "It can be seen that e.g., for closeness testing, our non-trivial predictor improves over the bestpossible sampling bound in the standard model. Specifically, as long as = o(1), our bound inthe augmented model improves over the prior work. At the same time, our algorithms are resilient:even if = (1), our sampling bounds do not exceed those in the standard model. Note that alltheorems are complemented by tight lower bounds. We highlight that all of our algorithms are alsocomputationally efficient, running in polynomial time with respect to n and 1/. The results aresummarized in . : Optimal sample complexity bounds in the standard model versus the augmented bounds. is the suggested accuracy level for the L1-distance between p and p. d denotes the total variationdistance between the known distribution q and p.",
  ": Error vs sample complexity for thetheoretically hard instance (See Sec. 6)": "As a summary, our algorithm can indeed leveragepredictions to obtain significantly improved samplecomplexity over the SOTA approach without predic-tions, as well as SOTA algorithms needing very ac-curate predictions [CRS15]. For distributions similar to our lower bound in-stances, our augmented algorithm achieves >20x re-duction in sample complexity to obtain comparableaccuracy as the standard un-augmented algorithm,as shown in . On real distributions curatedfrom network traffic data, we see sample complexityreductions of up to 40%. Furthermore, our algorithmis empirically robust to noisy predictions, in contrastto prior state of the art approaches which assumevery accurate, point-wise predictions (CRS15 in Fig-ure 1).",
  "It is worth noting that our experiments on networktraffic data reveal that the actual sample complexity": "is much lower than the anticipated worst-case sample complexity of our algorithm. In particular,this holds even when p is far from p in terms of total variation distance. Empirically, if p accuratelyreflects the high-probability elements in p, our algorithm can significantly reduce the sample com-plexity needed for testing by utilizing these heavy hitter hints from p. This is validated by ourresults and depicted in Figures 5 and 7(b).",
  "To the best of our knowledge, there have been only three prior works that studied any distributionproperty testing algorithms with predictions, each assuming a much stronger prediction model:": "Distribution testing with perfect predictors [CR14]: this work studied distribution testing prob-lems, including closeness, identity and uniformity, assuming query access to a perfect predictor,i.e., p = p. They show that, given a perfect predictor, it is possible to obtain highly efficienttesters for a wide variety of problems with a small number of queries to the prediction. Unfortu-nately, the perfect-predictor assumption is often too strong to hold in practice, as demonstratede.g., in [EIN+21] and in our experiments. Distribution testing with (1 + )-approximate predictors [CRS15, OS18]: these works relax theassumption used in the above paper, requiring only that pi = (1 /2)pi for all i and sufficientlysmall > 0.However, this assumption is still quite restrictive, especially for low values of. Indeed in our experimental setting, such algorithms are not robust to prediction errors (see). Support estimation with c-approximate predictors [EIN+21]: this work focused on the singleproblem of support estimation, i.e., estimating the fraction of coordinates i such that pi > 0. Itfurther relaxes the assumption in [OS18] by allowing the predicted probabilities pi to be withina factor of c of the true probabilities pi, for any constant approximation factor c > 1.Thisalgorithm has been shown to work well in practice. However, the techniques presented in thatpaper seem to be applicable exclusively to support estimation. Furthermore, their result provablydoes not hold under the assumption that p and p are close in TV distance, as in this paper. In summary, prior results required either highly restrictive assumptions, or were applicable to only asingle problem. None of the previous algorithms worked under the TV distance assumption used inthis paper which is arguably the most natural. Further exploration of measures such as Lp-distances,KL-divergence, and Hellinger distance are interesting open questions. We remark that we can mathematically show that these alternative oracles yield provably morepower compared to ours (i.e., we make weaker assumptions about the predictor). We provide analternative to our upper bound techniques for these alternate prediction models in Section B. Wedemonstrate how a variant of our algorithm, in conjunction with these stronger predictions, impliesthat uniformity testing, identity testing, and closeness testing in these models can be conductedusing only O(n/2) samples. This low sample complexity effectively circumvents our lower boundfor closeness testing, suggesting that these models provide stronger, and arguably less realistic,predictions.",
  "General property testing of distributions:Testing properties of distributions has been ex-tensively studied over the past few decades. Distribution testing under computational constraints": "has also been explored in [DGKR19, RV23a]. Hypothesis testing (and hypothesis selection) havereceived significant interest within the privacy community in machine learning [GLRV16, CDK17,ADR18, ASZ18, GR18, She18, CKM+19, CKM+20, Nar22]. Examples of other distributional prop-erties that have been examined include testing monotonicity [BFRV10, Can15, AGP+19], testinghistograms [CDKL22, Can23], testing junta-ness [ABR16, CJLW21], and testing under structuralproperties [BKR04, ILR12, DDS+13, DKN15b, DKN15a]. Connections to tolerant testing:Tolerant testing asks us estimate the true TV distancebetween a known distribution and another which we only have sample access to, up to asmall additive error.Readers familiar with strong lower bounds in tolerant testing (see e.g.,[Val11, VV17b, CJKL22] where its shown that (n/ log(n)) samples are needed in the case wherethe known distribution is uniform, compared to only O(n) samples needed for uniformity testing)might find our results surprising. It is incorrect to conclude that our algorithm in , i.e.the search component (which is adaptive to the distance between p and p), can perform toleranttesting between p and p. In reality, the algorithm finds an which allows the testing component toterminate (either with accept or reject). Thus, while we know that the found by our algorithm isnever larger than p pTV, it could in fact be much lower than p pTV, meaning the foundby the search algorithm is not a good estimate for p pTV. Testable learning:Our framework bears some resemblance to testable learning as introducedin [RV23b]. In this framework, the focus is on designing learning algorithms that can check whetherthe required underlying assumptions hold.If the assumptions do not hold, the algorithm mayforego solving the problem. However, if it chooses to solve the problem, it must do so accurately,regardless of whether the assumptions hold. This is similar to our notion of testing with a suggestedaccuracy level, where the algorithm can either forego solving the problem if the assumption doesnot hold or solve it accurately regardless. Some examples of results in this framework are presentedin [GKK23, GKSV23, DKK+23, GKSV24, KSV24b, KSV24a]. Algorithms with predictions:Recently, there has been a burgeoning interesting in augmentingclassical algorithm design with learned information. Most relevant to us are works which studylearning-augmented algorithms under sublinear constraints, such as memory or sample complexity[HIKV19, IVY19, JLL+20, CGP20, DWM21, EIN+21, CEI+22, LLL+23, SAN+23, ACN+23]. Werefer the interested reader to the website foran up-to-date collection of literature on learning-augmented algorithms.",
  "Notation and organization": "Notation:We use [n] to denote the set {1, 2, . . . , n}. All of our distributions will be over thedomain [n], and we assume n is always known. For a distribution p, we denote the probability ofthe i-th element by pi. For any subset of the domain S [n] we denote the probability mass of Saccording to p by p(S). We use ps to denote the probability distribution of s i.i.d. samples drawnfrom p. We say p is a known distribution, if we have access to every pi. We say p is an unknowndistribution if we have only sample access to p. We use Poi() to denote a random variable from aPoisson distribution with mean . Similarly, Ber() indicates a random variable from the Bernoullidistribution that is one with probability . Given two distributions p and q over a domain X, weuse p qTV := supSX |p(S) q(S)| to denote the total variation distance between p and q. We",
  "say p and q are -close (-far), if the total variation distance between p and q is at most (largerthan )": "Organization:We provide an overview of our theoretical results in .The searchalgorithm is detailed in .Augmented uniformity and identity testing are discussed in, where the upper bounds are presented in .1, and the lower bounds are providedin Sections 4.2 and 4.3.Augmented closeness testing is discussed in , with the upper bounddetailed in .1 and the lower bound provided in .2.Our empirical evaluations arepresented in .",
  "Overview of our proofs": "Search algorithm:The search algorithm seeks to find the smallest value of for which theproblem is solvable via the augmented tester. It starts with the lowest value of (most accurateprediction). Then, it iteratively increases the sample budget across rounds. In each round i, itselects an i for the augmented tester, A, ensuring operation within the current sample budget. If Aoutputs accept or reject, the algorithm echoes this outcome. If inaccurate information is returned, thesample budget doubles for the next round. It is worth noting that this search scheme is applicableto a general distribution testing algorithm with polynomial sample complexity. The algorithmspseudocode and its performance proof are in . Upper bound for augmented identity testingLet d represent the distance between p andq (the prediction and the known distribution). For establishing the upper bound, it is essential toassume d > . If not, the prediction proves unhelpful, and we might as well resort to the standardtester. Our upper bound relies on a simple but fundamental observation regarding the total variationdistance: this distance is the maximum discrepancy between the probability masses that two distri-butions assign to any subset of their domain. To prove that total variation between two distributionsis small entails proving that the discrepancy across every domain subset is small. In contrast, toprove a large total variation distance, one only needs to identify a single subset with a large discrep-ancy as evidence of large total variation distance. We use the Scheff set of p and qcharacterizedas the collection of elements x [n] where p(x) < q(x), symbolized by Sas evidence of ps diver-gence from either q or p. More precisely, it is known that S maximizing the discrepancy betweenthe probability masses of p and q, implying: d := q pTV = |q(S) p(S)| . Next, we estimatethe probability of set S according to p with reasonably high accuracy. Given q(S) and p(S), thenp(S) is either significantly different from q(S), or it deviates from p(S). In the first scenario, thisdiscrepancy serves as evidence that p = q, allowing us to output reject. In the second scenario, thedeviation confirms that p is -distant from p, leading us to output inaccurate information. Furtherdetails can be found in .1. Lower bound for augmented uniformity testingWe provide two lower bounds for augmenteduniformity testing. One is purely based on a reduction to standard uniformity testing for the casewhere d. (Recall that d was the total variation distance between q and p). See .2. The other lower bound applies to the setting where < d. One challenge of this problem was that itis hard to find two difficult distributions that the tester has to distinguish between; usually, for a pair of distributions, we could come up with one valid output that could serve them both. For example,for both uniform distribution, and the famous -far uniform distribution that assigns probabilities(1 )/n to the elements, the algorithm may be able to output inaccurate information. Hence, wecannot draw lower bounds just by asking the algorithm to distinguish between two distributions. For this reason, we provide three distributions that look similar when we draw too few samples.We formalize the similarity of these three distributions using a multivariate coupling argument. Weshow that these distributions are such that there is no possible answer that is valid for all three ofthem. Now, (similar to Le Cams method), suppose we feed the algorithm with samples from oneof these three distributions (each with probability 1/3). For any sample set, the algorithm outputsan answer (which may be randomized); however, this answer is considered wrong for at least one ofthe underlying distributions. This is due to the fact that there is no universally valid answer thatis simultaneously correct for all three distributions. Hence, if the algorithm outputs a valid answerwith high probability, it must be able to distinguish the underlying distributions to some degree.On the other hand, the indistinguishability result says it is impossible to tell these distributionsapart. Thus, we reach a contradiction. And, the lower bound is concluded. See .3 forfurther details. Upper bound for augmented closeness testing:Our upper bound is based on a techniquecalled flattening, which was previously proposed by Diakonikolas and Kane [DK16]. We adapt thistechnique for use in the augmented setting, aiming not only to flatten the distribution based onthe samples received but also by exploiting the prediction distribution p. We show that augmentedflattening can significantly further reduce the 22-norm of p leading to efficient testing. This resultis discussed in .1.",
  "Lower bound for augmented closeness testing:We provide two separate lower bounds forcloseness testing based on the relationship between and . Further details are available in Sec-tion 5.2": "Our first lower bound, as detailed in Theorem 11, employs a reduction strategy from standardcloseness testing to augmented closeness testing when . This is achieved by taking instancesused in standard closeness testing for distributions over [n 1] and embedding them into the firstn1 domain elements of a new distribution p defined over [n]. The key to this strategy is to put themajority of the distributions mass ((1) mass) on its final element, and we set the prediction p tobe a singleton distribution over the last element, which is -close to p. Clearly, the prediction doesnot reveal any information about the first n 1 elements of p, implying that testing the closenessof p in the augmented setting is as challenging as in the standard setting, once the parameters areappropriately scaled. Our second lower bound, outlined in Theorem 12, is more involved. In the standard setting, thelower bound for closeness testing is derived from the hard instances for uniformity testing withone crucial adjustment: adding new elements with large probability (approximately n2/3) in thedistributions. These large elements have identical probability masses in both p and q, indicatingthey do not contribute to the distance between the two distributions. However, their presence inthe sample set confuses the algorithm: due to their high probabilities, their behavior in the sampleset may misleadingly suggest non-uniformity, complicating the algorithms task of determining theuniformity of the rest of the distribution. Therefore, the algorithm requires s n2/3 samples tofirst identify these large elements before it can test the uniformity of the remaining distribution.Surprisingly, this requirement of s n2/3 samples is significantly higher than the O(n) samples",
  "typically sufficient for testing uniformity": "The challenge in our case arises because p may disclose the large elements to the algorithm. Toestablish the lower bound, we set p to be the uniform distribution, we generate hard instances of p byadding as many large elements as possible, without altering p, by keeping the overall probability massof the large elements limited to . More precisely, p assigns approximately (1 )/n probabilitymass to O(n) elements chosen at random, and assigns approximately n2/3 probability mass to n2/3 elements in the domain. Now, q has two scenarios. Half the time, q is identical to p. Theother half, q retains the same large elements but deviates slightly from uniformity for the rest ofthe distribution. Specifically, q assigns probabilities (1 ())(1 )/n to the randomly chosenO(n) elements, making it -far from p. It is not hard to show that testing closeness of p and q isa symmetric property (since permuting the domain elements does not affect our construction). Byleveraging the wishful thinking theorem from [Val11], we demonstrate that these two scenarios areindistinguishable unless (n2/31/3) samples are drawn.",
  "Searching for the appropriate accuracy level": "Early on, we defined the augmented tester for testing identity, uniformity, and closeness.Wegeneralize this concept for other properties of distributions. More formally, a property P is a setof distributions, and we say a distribution has property P iff it is in P. The goal is to distinguishwhether an unknown distribution p is in P, or it is far from all distributions that have the property.Similar to Definition 1.1, we define the notion of an augmented tester for P as follows: Definition 3.1. Suppose we are given three parameters, , (0, 1), (0, 1). Assume Ais an algorithm that receives all these parameters, and the description of a known distribution p asinput, and it has sample access to an unknown underlying distribution p. We say algorithm A is an(, , )-augmented tester for property P for every p, q, and p over [n]:",
  "If p is -far from every member of P, then the algorithm outputs accept with a probability atmost /2": "With this definition in mind, suppose we have an augmented tester for a property P. Our goal isto find an appropriate such that the augmented tester solves the problem: it outputs accept orreject, but also, it does not use too many samples. In this section, we introduce a search algorithmthat seeks to find this appropriate . It is important to note that the identified by our algorithmmay not necessarily match the true accuracy level of the prediction, i.e., p pTV. Instead, itcorresponds to the minimum number of samples that the augmented tester can solve the problem. Our search algorithm runs in rounds, each set by a sample budget that increases as the processprogresses.In round i, the algorithm determines an appropriate value for i and invokes theaugmented tester, namely A, with this chosen i. The value of i is selected such that the augmentedtester operates within the sample budget allocated for that round. If the tester outputs acceptor reject, the search algorithm replicates this response. However, if the tester returns inaccurateinformation, we double the sample budget and proceed to the next round. The pseudocode for thisprocedure is provided in Algorithm 1. In the following theorem, we prove its performance.",
  ":Run the standard tester (without any prediction) with confidence parameter and returnthe answer": "Theorem 5. Fix a parameter < 1/2. Suppose A is an (, , )-augmented tester for property Pthat receives a suggested accuracy level and confidence parameter . Let f : N be anon-decreasing function for which A uses f(, )3 samples when it is invoked with parameter andaims for the confidence parameter . Algorithm 1 is a (, )-augmented tester, without knowledgeof , for property P where",
  "In addition, if is the true accuracy level, then Algorithm 1 uses O(f()) samples in expectation": "Proof. First, we define the notation we use in this proof. Let smin and smax represent the smallestand largest sample sizes used by A, respectively. smin may be one or higher, and smax is the samplesize when the prediction did not make any improvements (the sample complexity of a standardtester). Let t := log2 (smax/smin). The algorithms runs in t + 1 rounds i {0, 1, . . . , t}. In roundi < t, our sample budget is 2i smin. We run the algorithm with a parameter i ensuring the samplecomplexity of A remains at most 2i1 smin samples. In cases where multiple values satisfy thiscriterion, we select the largest. We use (abuse in fact) the inverse function notation, defining i asi = f1 2i smin. If A finds an answer (accept or reject), we return the answer; Otherwise, weproceed to the next round. In round i = t, where we have smax 2i smin samples, we run thestandard tester and return its answer. Now, we focus on proof of correctness. Note that we (may) call A t times and the standard testerone time. Each of these tester works with probability at least 1 . Thus, by the union bound, wecan assume that they return the correct answer with probability at least 1 where := /(t+1).As we have noted in Definition 1.1, A is resilient to inaccuracies, implying that even if the suggestedaccuracy level is not valid, if the tester does not output a false accept or reject with probability morethan /2. The same statement is correct for the standard tester. Now, our algorithm here replicatesthe output that is produced by one of the testers. Thus, if they all of them outputs the correctanswer, our algorithm outputs the correct answer as well. And, this event happens with probabilityat least 1 .",
  "Here, we omit the dependence to other non-varying parameters such as and n": "Next, we focus on the analysis of the sample complexity of this algorithm. Let be the trueprediction accuracy, and let i be the first round where i . Recall that we assume that i isthe largest such that f(i) 2i smin. In addition, we assume that f is non-decreasing. Thus,we have4:",
  "(1)": "On the other hand, the probability that algorithms end at round i > i cannot be too high. Ifthe algorithm ends at round i or later, it means that A in rounds i, i + 1, . . . , i 1 must havereturned inaccurate information. However, given our assumption, the true accuracy is not worse thani, which makes outputting inaccurate information wrong. And, this event does not happen in eachround with probability more than /2 for each round independently. Let Iend indicate a randomvariable that indicates the index of the round for which the algorithms ends. Thus, we have forevery i i:",
  "Identity and uniformity testing": "In this section, we focus on the problem of identity testing, which involves testing the equalitybetween a known distribution q and an unknown distribution p. Specifically, our goal is to determinewhether p = q or if they are -far from each other. In an augmented setting, we are provided withanother known distribution p, predicted to represent p, along with a suggested level of accuracy .Surprisingly, our findings indicate that the sample complexity for this problem is influenced by anew parameter: the total variation distance between q and p, denoted by d. In particular, we havethe following theorem:",
  "The proof of this theorem follows from Proposition 4.1, Proposition 4.2, and Proposition 4.3": "Remark 7. A particular instance of this problem is uniformity testing, where q is a uniform distri-bution over [n]. In Theorem 6, our upper bound applies to any arbitrary q. Furthermore, our tightlower bound is based on a hard instance where q is a uniform distribution. These results establishoptimal upper and lower bounds for both identity testing and uniformity testing simultaneously.",
  "Upper bound for identity testing": "Our upper bound relies on a fundamental observation regarding the total variation distance: thisdistance is the maximum discrepancy between the probability masses that two distributions assign toany subset of their domain. Demonstrating a small total variation distance between two distributionsentails proving that the discrepancy across every domain subset is minimal. In contrast, to prove alarge total variation distance, one only needs to identify a single subset with a significant discrepancy.We employ the Scheff set of p and qdefined as the set of elements x [n] where p(x) < q(x),denoted by Sto serve as a witness for the farness of p from either q or p. It is known that Smaximizes the discrepancy between the probability masses of p and q, implying:",
  "d := q pTV = |q(S) p(S)|": "In scenarios where the distance d > , accurately estimating the probability of S according to pwith an accuracy of (d )/4 provides a basis for distinguishing the farness from either q or p. Ifthe probability masses assigned to S by q and p differ by more than (d )/4, it clearly indicatesthat p is not identical to q, leading to outputting reject. Conversely, if the estimated probabilityof p(S) is close to q(S), then the discrepancy between p(S) and p(S) must exceed , leading tooutputting inaccurate information. The pseudocode of our algorithm is presented in Algorithm 2.We prove the performance of our algorithm in the following proposition:",
  "Proof. The proof of the theorem is trivial in the setting where d and1": "(d)2 >n2 .Inthese cases, we use the standard tester for identity testing (e.g., [VV17a, CDVV14, ADK15]) with = /2 = 0.05. Clearly, a wrong answer was produced with probability less than /2. It is knownthat these testers only use O(n/2) samples. Now, suppose d > . A simple application of Chernoffs bound shows that one can estimate theprobability of the Scheff set of q and p, S, up to error (d )/4 with probability at least 1 0.05using O(1/(d )2) samples. We refer to this estimate as , and we have with probability 0.95:",
  "Lower bound for uniformity testing when d d d": "In this section, we focus on the case where d. We observe that for this problem, the numberof samples depends on an additional parameter: the distance between q and p. If this distance isat most then prediction does not help the algorithm. The algorithm would still be required todraw (n/2) samples (which is the sufficient number of samples for the standard case). Our proofis based on a reduction from standard identity testing to the augmented version of this problem,which establishes the desired lower bound. Proposition 4.2. Suppose we are given two known distributions q and p, and an unknown distribu-tion p over [n]. Let d := q pTV. For any in , if d, then any (, , = 2/3)-augmentedtester requires (n/2) samples. Proof. We prove that standard identity testing can be reduced to the augmented version of thisproblem if d. For the standard tester, we are given a known distribution q and an unknowndistribution p. Let p be any arbitrary distribution that is at a distance d from q. Consider A as an(, , = 2/3)-augmented tester. The procedure for the standard tester is straightforward: Run Aon p, p, and q. If it returns accept, we also return accept; if it returns reject or inaccurate information,we return reject.",
  "rejectinaccurate information": ": A diagram indicating the valid answer for the augmented tester A based on the totalvariation distances of p from q and p assuming d . The standard tester requires to output acceptif p = q, the green dot, and reject if p qTV , the red shaded region, with high probability.In addition, the augmented tester may output inaccurate information for when p qTV andp pTV . We show that the augmented tester distinguishes between the cases where p = q and pqTV > .See . More precisely, note that if p = q, then p is within distance of p. Thus, the onlyvalid answer of A in this case is accept. According to Definition 1.1, A returns reject and inaccurateinformation each with a probability of at most /2 = 1/6.Therefore, by the union bound, wereturn accept with a probability of at least 2/3. When p and q are -far from each other, A returnsaccept with a probability of at most /2 = 1/6. Consequently, we output the correct answer with aprobability greater than 2/3. This reduction indicates that A must use at least as many samples as required for identity testing.Considering the existing lower bound for uniformity testing (where q is a uniform distribution over[n]), augmented identity testing necessitates O(n/2) samples [Pan08].",
  "Lower bound for uniformity testing when < d < d < d": "In this section, we consider the lower bound for the uniformity testing problem in the setting where < d. If d is not too small, the required lower bound is only (1/(d )2). Otherwise,(n/2) samples are needed (as is required for the standard tester). At a high level, our proof consists of the following steps. First, we construct three distributions thatlook similar when we draw too few samples. We formalize the similarity of these three distributionsusing a multivariate coupling argument. Next, we describe valid answers for each of the distribu-tions. The main message of this part is that there is no possible answer that is valid for all threedistributions. Now, (similar to Le Cams method), suppose we feed the algorithm with samplesfrom one of these three distributions (each with probability 1/3). For any sample set, the algorithmoutputs an answer (which may be randomized); however, this answer is considered wrong for atleast one of the underlying distributions. This is due to the fact that there is no universally validanswer that is simultaneously correct for all three distributions. Hence, if the algorithm outputs avalid answer with high probability, it must be able to distinguish the underlying distributions to some degree. On the other hand, the indistinguishability result says it is impossible to tell these dis-tributions apart. Thus, we reach a contradiction, and the lower bound is concluded. More formally,we have the following proposition: Proposition 4.3. Suppose we are given two known distributions q and p, and an unknown distri-bution p over [n]. Let d := q pTV. For any [0, 1/2), [0, 1/2), and d [0, 1/2], if d > any (, , = 0.3)-augmented algorithm for testing identity of p and q with prediction p requiress = min1",
  "It is not hard to verify that p UnTV is d, satisfying the assumption made in the statement ofthe proposition. Next, we construct two distributions, p and p": "Let be a number in (, 1/2] where = (). Additionally, let be a number in (0, ) such thatd = (d ). Suppose we have a random vector Z in {1, +1}n/2 where each coordinate Ziis one with probability 1/2 independently. Now, we define the following distributions over [n]:",
  "As long as the problem does not have a pre-determined answer that works for all distributions, one must drawat least one sample": "distances are so small that it is practically impossible for the algorithm to tell them apart. Weformalize this by providing a multivariate coupling between these random variables. We extend LeCams method for three random variables and show that no algorithm with low error probabilityexists unless s is large, thereby establishing the desired lower bound for the problem. Lemma 4.4. Suppose we are given the following parameters , d, , n, and s. Assume d 0.4.Let Un denote the uniform distribution over [n], and let p and p be the distributions defined inEquation (4) and Equation (5). Let S|Un, S|p, and S|p be three random variables that are sample",
  "Next, we use the properties of the coupling derived in Lemma 4.4. In particular, we have shown": "6It is worth noting that a pair of valid sets may have some overlap. That is, the algorithm can output an answerthat is correct for both of the distributions. In other words, technically, the algorithm does not have to distinguishany pair of these distributions as long as there is a valid answer that works for both of them.",
  "Multivariate coupling between the three hard distributions": "In this section, we prove the lemma on coupling that we used to establish our lower bound. Thislemma implies indistinguishability between S|Un, S|p, and S|p. For a definition and some basics oncoupling, see Section A.3. Lemma 4.4. Suppose we are given the following parameters , d, , n, and s. Assume d 0.4.Let Un denote the uniform distribution over [n], and let p and p be the distributions defined inEquation (4) and Equation (5). Let S|Un, S|p, and S|p be three random variables that are sample",
  "Upper bound for closeness testing": "Our upper bound is based on a technique called flattening, which has been previously proposedby Diakonikolas and Kane [DK16]. This technique is instrumental in reducing the variance of thestatistic used for closeness testing by reducing the 22-norm of the input distributions. We adaptthis technique for use in the augmented setting, aiming not only to flatten the distribution basedon the samples received from it but also exploiting the prediction distribution p. We demonstratethat augmented flattening can significantly reduce the 22-norm of p. In our algorithm, we initiallycheck if the 22-norm of p is reduced after flattening to the desired bound. If not, this indicatesthat the prediction was not sufficiently accurate, leading us to output inaccurate. Conversely, if the22-norm of p is small, we proceed with an efficient testing algorithm that requires fewer samples.We describe the standard flattening technique in .1.1. Our flattening technique presentedin .1.2. Finally, we provide our algorithm in .1.3.",
  "Suppose we are given n parameters m1, m2, . . . , mn.One can create a randomized mapping Fthat assigns each i [n] to a pair (i, j), where j is drawn uniformly at random from [mi]. Now,": "consider a given distribution p over [n] and a sample i drawn from p. This mapping induces adistribution over pairs (i, j)s in D := {(i, j)|i [n] and j [mi]}. We denote this new distributionby p(F) satisfying the relation: p(F)(i,j) = pi/mi . The core idea of the above mapping is to divide theprobability of the i-th element into mi buckets. If the values of mis are large for elements in pwith higher probabilities, then the resulting distribution p(F) will avoid having any elements withdisproportionately large probabilities, thereby naturally lowering its 2-norm.Diakonikolas andKane [DK16] showed that if we draw Poi(t) samples from p, and set each mi to be the frequencyof element i among these samples plus one, then we have:",
  "where the expectation is taken over the randomness of the samples": "Connection to distribution testing:Chan et al. in [CDVV14] showed that one can test close-ness of two distributions over [n] using O(n max(p2 , q2)/2) samples. Diakonikolas et al. [DK16]used this tester in combination of the flattening technique to map distributions to distributions overslightly larger domains with the goal of reducing their 2-norms. They have shown that if we use thesame mapping to reduce the 2-norm of both p and q, the 2-distance between the two distributiondoes not change the 1 distance between distributions.",
  "with O(n max(p(F)2 ,q(F)2)/2) new samples. Later, Aliakbarpour et al. in [ADKR19] showedthat O(n min(p2 , q2)/2) samples is sufficient": "The exact sample complexity is determined by balancing the samples needed to determine theflattening and the samples needed to test closeness of p(F) and q(F). Note that flattened distributionshave a larger domain size. Thus, one must also balance the gains obtained from the reduction in 2norm with the increase in the domain.",
  "We adapt the flattening argument in [DK16] to the augmented setting": "Lemma 5.2. Suppose we have an unknown distribution p = (p1, p2, . . . , pn) over [n] and Poi(sf)samples from p. Assume we are given a known distribution p that is -close to p in TV distance.Then for every v 1, there exists a flattening F which increases the domain size by 1/v + sf inexpectation and the expected 22-norm of the p(F) is bounded by:",
  "+ fi + 1 .(9)": "Let p(F) denote the flattened version of p where we split every element i into mi buckets. We showthat the expected 2-norm of p(F) is low. Define i to be pi pi. Suppose A is a set of indicesi [n] for which i pi. For every i A, pi is bounded from above by 2 i. On the other hand,for every i [n] \\ A, pi is bounded by 2 pi.",
  "The algorithm": "In this section, we provide an algorithm for testing closeness of two distribution in the augmentedsetting. Our algorithm receives a suggested accuracy level . Based on , we draw sf (which dependson ) samples from p and use them to flatten p. If the resulting distribution has sufficiently small22-norm we proceed to the testing phase. Otherwise, we declare that the accuracy level providedis not correct.The pseudocode of our algorithm is provided in Algorithm 3, and we prove itsperformance in Theorem 9. Theorem 9. For any given parameters n, , , and any two unknown distributions p and q and aknown predicted distribution p for p, Algorithm 3 is an (, , = 0.3)-augmented tester for testingcloseness of p and q which uses O(n2/31/3/4/3) samples.",
  ".(11)": "To prove the correctness of the algorithm, we show the three desired property of an augmentedtester which are defined in Definition 1.1. First, assume p is -close to p. We show the probabilityof outputting inaccurate information is small. Using Lemma 5.2, after applying F, the expected22-norm of p(F) is bounded by:",
  "n": "Using Markovs inequality, with probability at least 0.95, 22-norm of p(F) is at most 20 times theabove bound. Combined by the upper bound for Lp in Equation (11) and the union bound, in thiscase Lp is bounded by 30 (2/sf + 4/n) with probability at least 0.9. Hence, we the algorithmdoes not output inaccurate information in Line 10 with probability more than 0.1.",
  "Second, we show that if p = q, the algorithm outputs reject with small probability. We may output": "reject in three cases: 1) in Line 5 2) in Line 13 and 3) in Line 15. Using Markovs inequality, aPoisson random variable is more than 10 times its expectation with probability at most 0.1. Weshow the other two cases are unlikely as long as Lp and Lq are accurate estimates of the 22s of p(F) and q(F) (which happens with probability of 0.9). If p = q, p(F) and q(F) are identical. Therefore,in the algorithm Lp and Lq are the estimations of the 22-norm of the same distribution. Note thatwe output reject in Line 13, only when Lp < 3Lq. Using Equation (11), this event does not happenunless at least of the estimates are inaccurate (which has a probability at most 0.1). Furthermoreif the estimated 22-norm are accurate, the minimum 22-norm of p(F) and q(F) are bounded by90 (2/sf + 4/n) implying the tester would work correctly with probability at least 0.9. Using theunion bound, the probability of outputting reject is bounded by 0.3. Third and last, we show that if p is -far from q(F), the probability of outputting accept is small. Weonly output accept in Line 15. Similar to the second case we have discussed, as long as the 22-normare accurate, the tester does not make a mistake with probability more than 0.1. Therefore, theoverall probability of making a mistake is bounded by 0.2.",
  "/3+n2samples. The lower bound holdseven when is provided to the algorithm": "Proof. The (n/2) term comes from the existing lower bound for uniformity testing since we canreduce uniformity testing to augmented closeness testing as follows: Suppose we wish to test whethera distribution q over [n] is uniform or -far from uniform. Suppose A is an (, , 11/24)-augmentedcloseness tester. Set p to be the uniform distribution over [n]. Let p be any distribution that is -farfrom uniform. For instance, p can be a distribution that is (1 + )/n on half of the elements and",
  "(1 )/n on the other half. Since A is an augmented tester, it can be used to distinguish whetherp = q or p qTV > with probability 11/24": "Upon receiving p and samples from p and q, if A returns inaccurate information or reject, we outputreject; otherwise, we output accept. Given Definition 1.1, if q is uniform, we will not output inac-curate information or reject with probability more than 11/24. And, if q is -far from the uniformdistribution (equivalently p), we will not output accept with probability more than 11/48. Hence,we have a tester with confidence parameter 11/48 for uniformity testing. Thus, s must be (n/2),which is the number of samples required for uniformity testing [Pan08]. To prove the lower bound of n2/31/3/4/3samples, we only need to focus on the case where (n2)1. Otherwise, n2/31/3/4/3 = O(n/2), making the lower bound trivial due to the(n/2) lower bound shown earlier. We consider the following cases depending on :",
  "Lower bound for": "Below, we provide a lower bound for augmented closeness testing when . At a high level,Theorem 11 achieves this lower bound by creating a challenging instance. This involves embeddingthe difficult instances of the standard closeness testing problem into another distribution, spanninga domain of [n 1] elements, while concentrating the majority of the distributions mass on thelast element. The prediction, being a singleton distribution centered on the last element, does notfacilitate solving the standard closeness testing problem, which is embedded within the first [n 1]elements. Theorem 11. Suppose we are given and in (0, 1). Assume we have two unknown distributionsp and q and a known distribution p over the domain [n]. If , then any (, , := 11/24)-augmented tester for testing closeness of p and q requires the following number of samples:",
  "qi = qii [n 1] ,qn = 1": "Note that given a sample from p (respectively q), we can generate a sample from p (respectively q)by outputting that sample with probability and n otherwise. Also, if p is equal to q, then p andq are equal. And, if p qTV , then p qTV due to the following:",
  "i[n1]|pi qi| = p qTV": "Let p be the singleton distribution on n (i.e., pn = 1). In this case, p pTV equals . We feedA with p and samples from p and q. Now, if A outputs p = q, we declare p = q. Otherwise, wedeclare p is -far from q. Given our construction, this answer is true with probability 11/24. Note that in this process, in expectation, we draw s samples from p and q. Thus, using Markovsinequality, the probability of drawing more than 100 s samples from each of p and q is at most0.01. Hence, using A, there exists an algorithm for testing closeness of p and q with probability1 (11/24 + 0.01) > 0.53 that uses 100 s samples from p and q. Using the existing lower boundfor the closeness testing problem (see Theorem 4 in [Pan08] and Proposition 4.1 in [CDVV14]), wehave:",
  "Lower bound for inc (n2)1,": "We extend the existing lower bound for closeness testing to the augmented setting. In the standardsetting, the lower bound for closeness testing is derived from the hard instances for uniformitytesting with one key adjustment: introducing elements with large probability ( n2/3) in thedistributions. These large elements share identical probability masses in both p and q, indicatingthat they do not contribute to the distance between the two distributions. However, their presencein the sample set confuses the algorithm: Because of their large probabilities, their behaviour in thesample set may be deceivingly hint not uniform, making it difficult for the algorithm to decidewhether the rest of the distributions are uniform or not. Therefore, the algorithm needs s n2/3 samples to first identify these large elements, and then it can test the uniformity on the rest of thedistributions. The surprising part about this result is that s n2/3 is much larger than the nsamples which suffice for testing uniformity. The challenge in our case is that p may give away the large elements to the algorithm. Hence,to prove the lower bound, we create hard instances by adding as many large elements as possible,without changing p by limiting the overall probability mass of the large elements to . Here is our instance: we assume p is the uniform distribution; p assigns (1 )/n probabilitymass to O(n) elements chosen at random, and assigns n2/3 probability mass to n2/3 manyelements in the domain. Now, q has two options. Half the time, q is equal to p. The other half,q shares the same large elements, but it is not quite uniform on the rest of the distributions. In particular, q assigns probabilities (1 ())(1 )/n.to the chosen O(n) elements randomly,making it -far from p.7 Using this construction, we use a result of Valiant [Val11] to show thatthese two cases are indistinguishable unless we draw (n2/31/3) samples. More precisely, we havethe following theorem: Theorem 12. Suppose we are given the following parameters: a positive integer n, (0, 1/6),and (n2)1, (with this implicit assumption that (n2)1 is less than ). There exists adistribution p and a family of pairs of distribution p and q such that any augmented algorithm forcloseness testing must use",
  "(12)": "for a sufficiently small absolute constant ck which we determine later. Assume we have k samplesfrom p and q that are available to the algorithm. We show that there are two (families of) distribu-tions that any augmented tester has to distinguish them with high probability. However, they areinformation-theoretically indistinguishable using only k samples.",
  "4/3( mod 2 ) .(13)": "It is not hard to see that is an integer. For our proof, we require that and n are either botheven or both odd. Here, the role of the indicator variable is to reduce by one in case they are not.Also, is positive, since due to the range of and , we have:",
  "then no algorithm can distinguish the pairp(1), q(1)fromp(2), q(2)with a success probability morethan 13/24 upon receiving Poi(k) samples from each distribution in the pair": "First, we verify that all the domain elements of p+ and p have probability masses at most1/(1000 k) according to both p+ and p.Recall that < 1.Observe that it suffices to showthat 1/n < 1/(2000 k) and / < 1/(2000 k) . The first inequality holds due to the fact that is inn2)1, :",
  "/3< ck 21/3 < ck < ck 1": "8A symmetric property of a pair of distribution is a property that does not change if we permute the domainelements of two distribution via the same permutation. The closeness testing of p and q in our context is a symmetricproperty since permuting the labels of the elements does not change the distance between p and q; Furthermore, p isidentical for every permutation of domain elements.",
  ": Error as a function of prediction quality for the Hard Instance dataset": "We evaluate our algorithm empirically on real and synthetic data on the closeness testing problem.Recall that given samples to two unknown distributions p and q over the domain [n], we wish totest if p = q or if pqTV . They are referred to as the (p, p) case or the (p, q) case respectively.Our experiments test the performance of our algorithm and baselines.",
  "Datasets": "Hard Instance: This is a simplified version of the hard instance described in .2 forTheorem 12. A conceptually similar hard instance is also used in the classical closeness testingsetting without any learned augmentation, as detailed in .2.2 Let [n] be the domain.In the instance used in the experiments, p and q agree on the firstm := n2/3 elements. Both distributions have probability mass 1/(2m) on these domain elements.p also has probability mass 2/n on domain elements between n/2 and 3n/4 and q has probabilitymass 2/n on domain elements between 3n/4 and n. All other probability masses are 0. We cancheck that pqTV = 1/2. The first m domain elements represent heavy hitters which conspireto fool any testing algorithm and the rest of the domain are items which contribute to all theTV distance but are rarely sampled. Indeed, we built upon this intuition to prove a formal lowerbound in Theorem 12.",
  "We consider hints of the form p = (1)p+Unif(n), where Unif(n) is the uniform distributionon [n] elements and is an interpolation parameter. = 0 corresponds to a perfect hint": "and the quality of the hint degrades as increase. When = 1, we receive p = Unif(n), i.e. ahint which not require any learned information to instantiate. In our experiments, we set n, thedomain size, to n = 106 and our goal is to determine if we are sampling from (p, p) or (p, q). IP: The data is internet traffic data collected at a backbone link of a Tier1 ISP in a data center9.Using the preprocessing in [AAC+23], we construct the empirical distribution over source IPaddresses in consecutive chunks of time. Each distribution represents approximately 170ms ofempirical network traffic data. The support size of each distribution is 45, 000 while the totaldomain size is > 2.5106. Distributions curated from network traffic closer in time are closer in TVdistance. This dataset has been used in other distribution testing literature [AAC+23, EIN+21]. We set p to be distribution for the first chunk of time and we let q be the distribution for thelast chunk of time. Empirically, we have p qTV 0.72. The hint is for p, denoted as p, is thesecond chunk of time. Unlike the previous case, the hint is relatively far from p in terms of TVdistance (p pTV 0.59). Nevertheless, they share similar heavy items (IP addresses withlarge probability mass), since p and p represent distributions of network traffic which are close intime.",
  "Baselines. We compare our main algorithm with the following baselines": "CRS 15. This is the state of the art algorithm with access to almost-perfect predictions from[CRS15]. More precisely, they assume predictions to both distributions p and q, denoted asp and q. Furthermore, they require p(i) = (1 /100)p(i) and q(i) = (1 /100)q(i) for alli [n]. Note that we only assume access to (much weaker) predictions for only p.",
  "Closeness Tester. This is the state of the art algorithm of [DK16] without any predictoraccess. We refer to this algorithm as the standard or the un-augmented tester as well": "Error Measurement. At a high level, both our algorithm (denoted as Augmented Tester andthe prior SOTA un-augmented approach, (abbreviated as Closeness Tester) compute an estimatorZ based on the samples requested. Then, both algorithms threshold the value of Z to determinewhich case we are in (either p = q or p qTV . Note that Z is a random variable in bothcases (since it depends on random samples). Ideally, the distribution of Z is well-separated in thetwo different hypotheses. Thus in our experiments, given a sample budget, we compute the empirical distribution of Z forboth algorithms under both hypothesis cases across 100 trials. For a fixed algorithm, our errormeasurement computes the dissimilarity of the two empirical histograms: if the histograms arewell-separated then we know the algorithm can adequately distinguish the two hypotheses. Onthe other hand, if the histograms of Z in the two cases are highly overlapping, then it is clearthat the algorithm cannot distinguish well. Thus, given the two empirical histograms, we calculatethe best empirical threshold which best separates them. This is done for both algorithms, oursand the classical Closeness Tester. Then we measure the fraction of data points in the histogramwhich are misclassified as the error. Calculating the best empirical threshold also makes both ofthe algorithms more practical as the theoretical thresholds are only stated asymptotically with un-optimized constant factors, making the theoretical values impractical. Finally, note that it is trivialto get 50% error.",
  "FromCAIDAinternettraces2019,see": "CRS 15 is a fundamentally different algorithm. Rather than calculating an estimator, it queriesthe hints p and q on the samples received and check if they are consistent with each other. If allsamples i satisfy p(i) = (1 /10)q(i), the algorithm declares we are in the p = q case. Note that as implemented, our algorithm and the standard Closeness Tester do not require theknowledge of ( is not needed to calculate the estimator Z). On the other hand CRS 15 cruciallyrequires the knowledge of . Thus, we make this baseline even stronger by providing it with theexact knowledge of (which the other two algorithms do not have access to).",
  "Hard Instance. Our results are shown in Figures 1 and 5. We now describe the plots": "displays the performance of all algorithms as a function of the number of samples seen. Note thatthe y axis measures the fraction of times we can distinguish the two hypotheses across 100 trials.In , our algorithm has access to p = p and CRS 15 has access to p = p. Furthermore,it has access to a hint q which satisfies q qTV < 0.002. q hat is created by removing a smallfraction of the mass on a small fraction of the heavy domain elements between 1 and m. We see that the performance of both our Augmented Tester and the standard Closeness Testerimproves as the sample complexity increases. On the other hand, CRS 15 essentially alwaysobtains the trivial guarantee of 50% error even when it is fed a large number of samples. This isbecause with decent probability, we sample an element where q(i) = 0 and p(i) = 0. Thus in thep = q case, it is very likely to output the incorrect answer.",
  "also demonstrates that our augmented tester achieves up to > 20x reduction in errorthan the standard Closeness Tester. It also requires > 2x fewer samples to achieve 0% errorempirically": "Our improvements are due to the fact that the empirical estimators of Z in the two cases of p = qand p = q are very well-separated for the Augmented Tester. In fact, as shown in Figures 6(a) and6(b), if we fix the number of samples to 104, the empirical distributions of Z in the two cases donot overlap at all for the augmented algorithm. On the other hand, thee is a significant overlapfor the un-augmented tester (the standard Closeness Tester). This implies that the ClosenessTester baseline cannot distinguish the two cases as well as the augmented algorithm. IP Data. Our results are shown in (a). Again we see the same qualitative behavior:CRS 15 is quite brittle to prediction errors: in the case (p, p) case, we feed CRS 15 with theperfect prediction p for the first distribution, and p for the second. In this case, it typicallyoutputted the incorrect answer. Furthermore, our augmented algorithm has a better sample vserror trade-off than the un-augmented approach. Indeed, we observed that while the augmentedtester only required 500 samples to obtain 0% error in 100 trials, the standard tester required> 700 samples, implying our augmented approach is > 40% more efficient. Robustness to Prediction Error. Our experiments also demonstrate the robustness of our aug-mented approach to the quality of the predictions. Our theory crisply quantifies how the theoreticalsample complexity is affected by a predictor which satisfies p pTV = , and correspondingly inpractice, we observe that our algorithm is able to take advantage of predictors of varying quality. Indeed, for the hard instance, we observe that the augmented approach still obtains better errorthan the standard tester, even if p pTV is high. This is demonstrated in where wemonotonically increase (as described above). The plot shows that while the error also increases",
  "Indeed, even at high values of , p is still informative of the heavy-hitter structure of p. Therefore,the augmented algorithm can still reliably use p to perform a suitable flattening": "The phenomenon repeats itself in for the IP data. In (b), we compare the performanceof using two different predictors.p1 is the same predictor as (a) whereas p2 is the IPdistribution that is in between p and q (it is formed using the chunk of time that is exactly inbetween the first and the last). We observed that now p p2TV 0.69 which is much higherthan p p1TV. Nevertheless, it is still the case that many heavy-hitters between p and p2 areshared. Thus even though the TV distribution between p and the hint p2 maybe high, our algorithmcan still exploit the shared heavy-hitter structure to obtain improved sample complexity over theun-augmented approach. This demonstrates the practical versatility of our algorithm beyond theprecise theoretical bounds. [AAC+23] Anders Aamand, Alexandr Andoni, Justin Y. Chen, Piotr Indyk, Shyam Narayanan,and Sandeep Silwal. Data structures for density estimation. In Andreas Krause, EmmaBrunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett,editors, Proceedings of the 40th International Conference on Machine Learning, volume202 of Proceedings of Machine Learning Research, pages 118. PMLR, 2329 Jul 2023. [ABR16]Maryam Aliakbarpour, Eric Blais, and Ronitt Rubinfeld. Learning and testing juntadistributions. In Proceedings of the 29th Conference on Learning Theory, COLT 2016,New York, USA, June 23-26, 2016, pages 1946, 2016. [ABS23]Maryam Aliakbarpour, Mark Bun, and Adam Smith. Hypothesis selection with memoryconstraints. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine,editors, Advances in Neural Information Processing Systems, volume 36, pages 5045350481. Curran Associates, Inc., 2023. [ACFT19] J. Acharya, C. Canonne, C. Freitag, and H. Tyagi. Test without trust: Optimal locallyprivate distribution testing. In Proceedings of Machine Learning Research, volume 89 ofProceedings of Machine Learning Research, pages 20672076. PMLR, 2019. [ACN+23] Anders Aamand, Justin Y. Chen, Huy L Nguyen, Sandeep Silwal, and Ali Vakilian.Improved frequency estimation algorithms with and without predictions. In Alice Oh,Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine,editors, Advances in Neural Information Processing Systems 36: Annual Conference onNeural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA,December 10 - 16, 2023, 2023. [ADK15]Jayadev Acharya, Constantinos Daskalakis, and Gautam Kamath. Optimal testing forproperties of distributions. In Advances in Neural Information Processing Systems 28:Annual Conference on Neural Information Processing Systems 2015, December 7-12,2015, Montreal, Quebec, Canada, pages 35913599, 2015.",
  "Processing Systems 32: Annual Conference on Neural Information Processing Systems2019, NeurIPS 2019, 8-14 December 2019, Vancouver, BC, Canada, pages 1087710888,2019": "[ADR18]Maryam Aliakbarpour, Ilias Diakonikolas, and Ronitt Rubinfeld. Differentially privateidentity and equivalence testing of discrete distributions. In Proceedings of the 35th In-ternational Conference on Machine Learning (ICML), volume 80, pages 169178, 2018. [AGP+19] Maryam Aliakbarpour, Themis Gouleakis, John Peebles, Ronitt Rubinfeld, and AnakYodpinyanee. Towards testing monotonicity of distributions over general posets. InProceedings of the Thirty-Second Conference on Learning Theory, COLT, pages 3482,2019. [AJM20]Kareem Amin, Matthew Joseph, and Jieming Mao.Pan-private uniformity testing.In Jacob D. Abernethy and Shivani Agarwal, editors, Conference on Learning Theory,COLT 2020, 9-12 July 2020, Virtual Event [Graz, Austria], volume 125 of Proceedingsof Machine Learning Research, pages 183218. PMLR, 2020.",
  "[AJOS14]Jayadev Acharya, Ashkan Jafarpour, Alon Orlitsky, and Ananda T. Suresh. Sublinearalgorithms for outlier detection and generalized closeness testing. In IEEE ISIT, pages32003204, 2014": "[AKR19]Maryam Aliakbarpour, Ravi Kumar, and Ronitt Rubinfeld. Testing mixtures of discretedistributions. In Alina Beygelzimer and Daniel Hsu, editors, Proceedings of the Thirty-Second Conference on Learning Theory, volume 99 of Proceedings of Machine LearningResearch, pages 83114, Phoenix, USA, 2528 Jun 2019. PMLR. [AS20]Maryam Aliakbarpour and Sandeep Silwal. Testing Properties of Multiple Distributionswith Few Samples. In Thomas Vidick, editor, 11th Innovations in Theoretical ComputerScience Conference (ITCS 2020), volume 151 of Leibniz International Proceedings inInformatics (LIPIcs), pages 69:169:41, Dagstuhl, Germany, 2020. Schloss DagstuhlLeibniz-Zentrum fuer Informatik. [ASZ18]J. Acharya, Z. Sun, and H. Zhang. Differentially private testing of identity and closenessof discrete distributions. In Advances in Neural Information Processing Systems 31:Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018,pages 68796891, 2018. [BB20]Thomas Berrett and Cristina Butucea. Locally private non-asymptotic testing of discretedistributions is faster using interactive mechanisms. In Hugo Larochelle, MarcAurelioRanzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances inNeural Information Processing Systems 33: Annual Conference on Neural InformationProcessing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. [BFR+00]Tugkan Batu, Lance Fortnow, Ronitt Rubinfeld, Warren D. Smith, and Patrick White.Testing that distributions are close. In 41st Annual Symposium on Foundations of Com-puter Science, FOCS 2000, 12-14 November 2000, Redondo Beach, California, USA,pages 259269, 2000.",
  "[BFR+13]Tugkan Batu, Lance Fortnow, Ronitt Rubinfeld, Warren D. Smith, and Patrick White.Testing closeness of discrete distributions. JACM, 60(1):4:14:25, 2013": "[BFRV10]Arnab Bhattacharyya, Eldar Fischer, Ronitt Rubinfeld, and Paul Valiant. Testing mono-tonicity of distributions over general partial orders. Electronic Colloquium on Compu-tational Complexity (ECCC), 17:27, 2010. [BKR04]Tugkan Batu, Ravi Kumar, and Ronitt Rubinfeld.Sublinear algorithms for testingmonotone and unimodal distributions. In Proceedings of the 36th Annual ACM Sym-posium on Theory of Computing, Chicago, IL, USA, June 13-16, 2004, pages 381390,2004. [BV15]Bhaswar B. Bhattacharya and Gregory Valiant. Testing closeness with unequal sizedsamples.In Corinna Cortes, Neil D. Lawrence, Daniel D. Lee, Masashi Sugiyama,and Roman Garnett, editors, Advances in Neural Information Processing Systems 28:Annual Conference on Neural Information Processing Systems 2015, December 7-12,2015, Montreal, Quebec, Canada, pages 26112619, 2015. [Can15]Clment L. Canonne.Big data on the rise: Testing monotonicity of distributions.In Automata, Languages, and Programming - 42nd International Colloquium, ICALP,volume 9134 of Lecture Notes in Computer Science, pages 294305, 2015. [Can23]Clment L. Canonne. Corrigendum: Are few bins enough: Testing histogram distri-butions.In Proceedings of the 42nd ACM SIGMOD-SIGACT-SIGAI Symposium onPrinciples of Database Systems, PODS 2023, Seattle, WA, USA, June 18-23, 2023,page 359. ACM, 2023.",
  "[CGP20]Edith Cohen, Ofir Geri, and Rasmus Pagh. Composable sketches for functions of fre-quencies: Beyond the worst case. In Proceedings of the 37th International Conferenceon Machine Learning, 2020": "[CJKL22]Clment L. Canonne, Ayush Jain, Gautam Kamath, and Jerry Li. The price of tolerancein distribution testing. In Po-Ling Loh and Maxim Raginsky, editors, Conference onLearning Theory, 2-5 July 2022, London, UK, volume 178 of Proceedings of MachineLearning Research, pages 573624. PMLR, 2022. [CJLW21] Xi Chen, Rajesh Jayaram, Amit Levi, and Erik Waingarten.Learning and testingjunta distributions with sub cube conditioning. In Mikhail Belkin and Samory Kpotufe,editors, Proceedings of Thirty Fourth Conference on Learning Theory, volume 134 ofProceedings of Machine Learning Research, pages 10601113. PMLR, 1519 Aug 2021. [CKM+19] Clment L. Canonne, Gautam Kamath, Audra McMillan, Adam D. Smith, andJonathan R. Ullman.The structure of optimal private tests for simple hypotheses.In Moses Charikar and Edith Cohen, editors, Proceedings of the 51st Annual ACMSIGACT Symposium on Theory of Computing, STOC 2019, Phoenix, AZ, USA, June23-26, 2019, pages 310321. ACM, 2019. [CKM+20] Clment L Canonne, Gautam Kamath, Audra McMillan, Jonathan Ullman, and Ly-dia Zakynthinou.Private identity testing for high-dimensional distributions.InH. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advancesin Neural Information Processing Systems, volume 33, pages 1009910111. Curran As-sociates, Inc., 2020. [CR14]Clment L. Canonne and Ronitt Rubinfeld. Testing probability distributions underly-ing aggregated data. In Automata, Languages, and Programming - 41st InternationalColloquium, ICALP, pages 283295, 2014.",
  "[CRS15]Clment L Canonne, Dana Ron, and Rocco A Servedio. Testing probability distributionsusing conditional samples. SIAM Journal on Computing, 44(3):540616, 2015": "[DDS12]Constantinos Daskalakis, Ilias Diakonikolas, and Rocco A. Servedio. Learning poissonbinomial distributions. In Howard J. Karloff and Toniann Pitassi, editors, Proceedingsof the 44th Symposium on Theory of Computing Conference, STOC 2012, New York,NY, USA, May 19 - 22, 2012, pages 709728. ACM, 2012. [DDS+13]Constantinos Daskalakis, Ilias Diakonikolas, Rocco A. Servedio, Gregory Valiant, andPaul Valiant. Testing k-modal distributions: Optimal algorithms via reductions. In Pro-ceedings of the Twenty-fourth Annual ACM-SIAM Symposium on Discrete Algorithms,SODA 13, pages 18331852, Philadelphia, PA, USA, 2013. Society for Industrial andApplied Mathematics. [DGKR19] Ilias Diakonikolas, Themis Gouleakis, Daniel M. Kane, and Sankeerth Rao. Commu-nication and memory efficient testing of discrete distributions. In Alina Beygelzimerand Daniel Hsu, editors, Proceedings of the Thirty-Second Conference on Learning The-ory, volume 99 of Proceedings of Machine Learning Research, pages 10701106. PMLR,2528 Jun 2019. [DGPP18] Ilias Diakonikolas, Themis Gouleakis, John Peebles, and Eric Price. Sample-optimalidentity testing with high probability. In 45th International Colloquium on Automata,Languages, and Programming, ICALP 2018, July 9-13, 2018, Prague, Czech Republic,pages 41:141:14, 2018. [DGPP19] Ilias Diakonikolas, Themis Gouleakis, J. Peebles, and Eric Price. Collision-based testersare optimal for uniformity and closeness.Chicago Journal of Theoretical ComputerScience, 2019(1), MAY 2019.",
  "FOCS 2016, 9-11 October 2016, Hyatt Regency, New Brunswick, New Jersey, USA,pages 685694, 2016": "[DKK+19] Ilias Diakonikolas, Gautam Kamath, Daniel Kane, Jerry Li, Ankur Moitra, and AlistairStewart. Robust estimators in high-dimensions without the computational intractability.SIAM J. Comput., 48(2):742864, 2019. [DKK+23] Ilias Diakonikolas, Daniel Kane, Vasilis Kontonis, Sihan Liu, and Nikos Zarifis. Effi-cient testable learning of halfspaces with adversarial label noise. In Alice Oh, TristanNaumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors,Advances in Neural Information Processing Systems 36: Annual Conference on NeuralInformation Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December10 - 16, 2023, 2023.",
  "[DKN15b] Ilias Diakonikolas, Daniel M. Kane, and Vladimir Nikishkin. Testing identity of struc-tured distributions. In SODA, pages 18411854, 2015": "[DKS16]Ilias Diakonikolas, Daniel M. Kane, and Alistair Stewart. Properly learning poisson bi-nomial distributions in almost polynomial time. In Vitaly Feldman, Alexander Rakhlin,and Ohad Shamir, editors, Proceedings of the 29th Conference on Learning Theory,COLT 2016, New York, USA, June 23-26, 2016, volume 49 of JMLR Workshop andConference Proceedings, pages 850878. JMLR.org, 2016.",
  "[DKS18]Ilias Diakonikolas, Daniel M. Kane, and Alistair Stewart. Sharp bounds for generalizeduniformity testing. In NeurIPS, pages 62046213, 2018": "[DWM21]Elbert Du, Franklyn Wang, and Michael Mitzenmacher. Putting the learning\" intolearning-augmented algorithms for frequency estimation.In Proceedings of the 38thInternational Conference on Machine Learning, pages 28602869, 2021. [EIN+21]Talya Eden, Piotr Indyk, Shyam Narayanan, Ronitt Rubinfeld, Sandeep Silwal, andTal Wagner. Learning-based support estimation in sublinear time. In 9th InternationalConference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7,2021. OpenReview.net, 2021. [GKK+20] Sivakanth Gopi, Gautam Kamath, Janardhan Kulkarni, Aleksandar Nikolov, Zhi-wei Steven Wu, and Huanyu Zhang. Locally private hypothesis selection. In Jacob D.Abernethy and Shivani Agarwal, editors, Conference on Learning Theory, COLT 2020,9-12 July 2020, Virtual Event [Graz, Austria], volume 125 of Proceedings of MachineLearning Research, pages 17851816. PMLR, 2020. [GKK23]Aravind Gollakota, Adam R. Klivans, and Pravesh K. Kothari. A moment-matchingapproach to testable learning and a new characterization of rademacher complexity. InProceedings of the 55th Annual ACM Symposium on Theory of Computing, STOC 2023,pages 16571670, New York, NY, USA, 2023. Association for Computing Machinery.",
  "Processing Systems 36: Annual Conference on Neural Information Processing Systems2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023": "[GKSV24] Aravind Gollakota, Adam R. Klivans, Konstantinos Stavropoulos, and Arsen Vasilyan.An efficient tester-learner for halfspaces. In The Twelfth International Conference onLearning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024, 2024. [GLRV16] M. Gaboardi, H. W. Lim, R. M. Rogers, and S. P. Vadhan. Differentially private chi-squared hypothesis testing: Goodness of fit and independence testing. In InternationalConference on Machine Learning, ICML, pages 21112120, 2016. [Gol20]Oded Goldreich. The uniform distribution is complete with respect to testing identityto a fixed distribution. In Computational Complexity and Property Testing - On theInterplay Between Randomness and Computation, pages 152172. Springer, 2020.",
  "[JLL+20]Tanqiu Jiang, Yi Li, Honghao Lin, Yisong Ruan, and David P. Woodruff. Learning-augmented data stream algorithms. In International Conference on Learning Represen-tations, 2020": "[KSV24a]Adam R. Klivans, Konstantinos Stavropoulos, and Arsen Vasilyan. Learning intersec-tions of halfspaces with distribution shift: Improved algorithms and SQ lower bounds.In The Thirty Seventh Annual Conference on Learning Theory, June 30 - July 3, 2023,Edmonton, Canada, Proceedings of Machine Learning Research. PMLR, 2024. [KSV24b]Adam R. Klivans, Konstantinos Stavropoulos, and Arsen Vasilyan. Testable learningwith distribution shift. In The Thirty Seventh Annual Conference on Learning Theory,June 30 - July 3, 2023, Edmonton, Canada, Proceedings of Machine Learning Research.PMLR, 2024.",
  "[LRR13]Reut Levi, Dana Ron, and Ronitt Rubinfeld. Testing properties of collections of distri-butions. Theory of Computing, 9(8):295347, 2013": "[Nar22]Shyam Narayanan.Private high-dimensional hypothesis testing.In Conference onLearning Theory, 2-5 July 2022, London, UK, volume 178 of Proceedings of MachineLearning Research, pages 39794027. PMLR, 2022. [NP33]Jerzy Neyman and Egon S. Pearson.On the problem of the most efficient tests ofstatistical hypotheses. Philosophical Transactions of the Royal Society of London. SeriesA, Containing Papers of a Mathematical or Physical Character, 231(694-706):289337,1933. [OFFG21] Aadil Oufkir, Omar Fawzi, Nicolas Flammarion, and Aurlien Garivier.Sequentialalgorithms for testing closeness of distributions. In Advances in Neural InformationProcessing Systems 34: Annual Conference on Neural Information Processing Systems2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 1165511664, 2021. [OS18]Krzysztof Onak and Xiaorui Sun. Probabilityrevealing samples. In International Con-ference on Artificial Intelligence and Statistics, volume 84 of Proceedings of MachineLearning Research, pages 20182026. PMLR, 2018.",
  "[Pan08]Liam Paninski. A coincidence-based test for uniformity given very sparsely sampleddiscrete data. IEEE Trans. Inf. Theory, 54(10):47504755, 2008": "[RV23a]Sampriti Roy and Yadu Vasudev. Testing properties of distributions in the streamingmodel. In 34th International Symposium on Algorithms and Computation, ISAAC 2023,December 3-6, 2023, Kyoto, Japan, volume 283 of LIPIcs, pages 56:156:17, 2023. [RV23b]Ronitt Rubinfeld and Arsen Vasilyan. Testing distributional assumptions of learning al-gorithms. In Proceedings of the 55th Annual ACM Symposium on Theory of Computing,STOC 2023, pages 16431656, New York, NY, USA, 2023. Association for ComputingMachinery. [SAN+23]Sandeep Silwal, Sara Ahmadian, Andrew Nystrom, Andrew McCallum, Deepak Ra-machandran, and Seyed Mehran Kazemi. Kwikbucks: Correlation clustering with cheap-weak and expensive-strong signals. In The Eleventh International Conference on Learn-ing Representations, 2023.",
  ":Output the median of Lis": "In this section, we show that one can estimate the 2-norm of a distribution via the number of colli-sions among O(n) samples. This result was implicitly known from previous work including [GR11].Here, we include a formal statement and a proof for the sake of completeness. Fact A.1. [Adapted from [GR11]] Suppose we have a parameter (0, 1) and an arbitrarydistribution p over [n] for which we have sample access to.Algorithm 4 receives n and ands = O (n log(1/)) samples from p and outputs an estimate L of the the 22-norm of p such thatwith probability 1 , we have:p2",
  "(s/2) 1/n 0.1": "The second to the last inequality holds since the 2-norm of any discrete distribution over [n] is atmost 1/n. And, the last inequality holds for s 160n. The above inequality implies that eachLi in the algorithm is within the desired bound with probability at least 0.9. To find an accurate estimate with a probability 1 , we use standard amplification technique: Werepeat this process O(log(1/)) times and take the median of these estimates. Using the Chernoffbound, one can show that median is accurate with probability at least 1 .",
  "In this section, we present the result of [CDVV14] for robust 2-distance estimation between twounknown distributions p and q. Their results implies an 1-tester for closeness testing of p and q as": "it has been used in [DK16]. The important feature of this tester is its adaptive sample complexity tomaxp22 , q22which combined with the flattening technique would achieve the optimal samplecomplexities for several distribution testing problems. Later [ADKR19] have shown that one canmodify the this closeness tester such that the sample complexity only deepens on minp22 , q22.",
  "BAugmented flattening for guarantees other than total variationdistance": "Below, we provide a simple lemma that indicates how one can use p to flatten a distribution basedon various guarantees of the prediction. Suppose our target 22-norm which we desire is v. In thefollowing lemma, we propose a flattening F to decrease the 22 to v up to some multiplicative factorthat depends on the quality of the estimates. Lemma B.1. Suppose we have an unknown distribution p = (p1, p2, . . . , pn) over [n]. Assume forevery i [n] we are give an estimate pi. Then for every v 1, there exists a flattening such that itincreases the domain size by 1/v and the 22-norm of the p(F) is bounded by:",
  "Using ni=1 pi = 1, we conclude the statement of the lemma": "In the following, we discuss a few cases for which we have some guarantee on the accuracy of theestimates and how it affects the 2-norm reduction. Note that our desired case here is to not blowup the domain size by more than a constant factor and bring down the 2-norm to O(1/n)."
}