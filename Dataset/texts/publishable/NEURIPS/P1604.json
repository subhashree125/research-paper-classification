{
  "Abstract": "In time-series analysis, many recent works seek to provide a unified view andrepresentation for time-series across multiple domains, leading to the developmentof foundation models for time-series data. Despite diverse modeling techniques,existing models are black boxes and fail to provide insights and explanationsabout their representations. In this paper, we present VQShape, a pre-trained,generalizable, and interpretable model for time-series representation learning andclassification. By introducing a novel representation for time-series data, weforge a connection between the latent space of VQShape and shape-level features.Using vector quantization, we show that time-series from different domains canbe described using a unified set of low-dimensional codes, where each code canbe represented as an abstracted shape in the time domain. On classification tasks,we show that the representations of VQShape can be utilized to build interpretableclassifiers, achieving comparable performance to specialist models. Additionally,in zero-shot learning, VQShape and its codebook can generalize to previouslyunseen datasets and domains that are not included in the pre-training process. Thecode and pre-trained weights are available at",
  "Introduction": "As one of the fundamental forms of data, time-series (TS) exist in a wide range of domains andapplications, including healthcare, weather, traffic, motions, human activities, sensors, etc. ModelingTS data across multiple domains has been a challenging task since TS data can have diverse samplingrates, lengths, magnitudes, frequencies, and noise levels. Due to this heterogeneity, most of theexisting machine learning methods for TS modeling focus only on a single dataset or a single domain. Recently, motivated by the success of large pre-trained models in natural language processing andcomputer vision, various approaches adopted from these two fields have been proposed to builda unified view and feature space for TS data from different domains [Liang et al., 2024]. Mostof the models use a transformer as the backbone and pre-train it on a diverse range of datasets[Zerveas et al., 2021, Nie et al., 2023, Goswami et al., 2024]. These methods have achieved greatsuccess in TS representation learning, benefiting various downstream tasks and demonstrating their",
  "arXiv:2411.01006v3 [cs.LG] 7 Jan 2025": "generalizability. Despite their success, most of them remain black boxes since they cannot providehuman-understandable representations. While tokenizers have played increasingly important roles inpre-trained models for language and vision, in TS, pre-training is often conducted by predicting thenext or masked timestamp, time window, or patch, lacking the concept of discrete tokens as in LLMs.Very recently, Talukder et al. developed TOTEM, which utilizes VQ-VAE [van den Oord et al.,2017] to obtain the codebook and reconstruct the TS. Nevertheless, like all other VQ-VAE models,the tokens from the codebook are just latent vector representations and lack physical meaning. Alternatively, in interpretable TS modeling, shapelets have been recognized as interpretable andexpressive features for TS data. Initially defined as TS subsequences that discriminate differentcategories in classification [Ye and Keogh, 2011], they were later generalized to representativepatterns [Grabocka et al., 2014]. Specifically, shapelets can transform TS data into low-dimensionalrepresentations either in the form of the distance between a shapelet and a TS, or as a logical predicatethat measures the probability of a shapelet existing in a TS [Lines et al., 2012]. However, despitetheir effectiveness in classification tasks, this shape-level feature lacks flexibility since shapelets withpre-defined lengths are optimized for capturing discriminative features for making dataset-specificpredictions. For example, when measuring human motion with accelerometers, an adult and a childperforming the same gesture may record TS with different offsets, scales, and durations. Althoughthey share the same shape-level concept, multiple shapelets are required to describe them separately.Additionally, shapelet-based interpretable models are specialized to a single dataset, and the learnedshapelets fail to transfer to different domains. In this paper, motivated by the limitations of existing pre-trained models and interpretable models inTS, we propose VQShape, a self-supervised pre-trained model that provides abstracted shapes asinterpretable and generalizable tokens for TS modeling. Firstly, we decompose a TS subsequence intoa set of attributes, including abstracted shape, offset, scale, start time, and duration. By incorporatingvector quantization, VQShape learns a codebook of abstracted shapes that are generalizable anddescriptive, representing TS from various domains. Evaluated on various classification tasks, andwithout fine-tuning, VQShape achieves comparable performance to black-box pre-trained modelswhile additionally providing interpretable latent-space tokens and representations to describe TS data.Our contributions are summarized below: We present an interpretable representation composed of abstracted shapes and attributesto describe TS data based on shape-level features, which enables the learning of dataset-agnostic interpretable features. We introduce VQShape, to the best of our knowledge, the first self-supervised pre-trainedmodel that extracts interpretable representations from any TS data. VQShape also learns acodebook containing abstracted shapes that generalize to multiple datasets. Pre-trained on diverse datasets and without fine-tuning, VQShape achieves comparable per-formance to existing black-box models on benchmark classification datasets. We explicitlydemonstrate that the representations and VQShape are interpretable and generalizable forunseen datasets and domains.",
  "Related Work": "Deep learning methods for TS analysis.Deep learning methods are increasingly applied to TSanalysis. Existing methods can be categorized into two groups depending on whether they use aTransformer structure as the backbone. For non-Transformer-based models, classical deep learningmodels such as MLP, CNN, and ResNet demonstrate decent performance on various tasks [Wanget al., 2017]. Recent methods have developed various feature engineering techniques to model explicitfeatures of TS data. TimesNet [Wu et al., 2023] transforms TS into 2D space to capture multi-periodfeatures in a modularized way, achieving state-of-the-art performance on various tasks. TS2Vec [Yueet al., 2022] employs hierarchical contrastive learning for unsupervised representation learning of TSdata. T-Rep [Fraikin et al., 2024] introduces a self-supervised representation learning approach byaugmenting the TS with time embeddings, providing additional temporal structure to the latent space. Transformers have been increasingly applied to TS analysis, but usually with some modifications tothe original structure. For example, Autoformer [Wu et al., 2021] modifies the attention mechanismby incorporating an Auto-Correlation mechanism to capture temporal dependencies. When applyingTransformers to real-valued data, transforming the inputs into patches has been recognized as an effective approach for images [Dosovitskiy et al., 2021] since the tokens could contain more semanticmeaning, like a word in language. Similarly, PatchTST [Nie et al., 2023] shows that TS analysisalso benefits from combining patched inputs with Transformers, viewing a TS as a sequence of 64words. Pre-trained Models for TS data.The success of large pre-trained models in language and visionmotivates the development of foundation models for TS analysis. Existing approaches aim to finda unified view for TS data from different perspectives. For example, TST [Zerveas et al., 2021]uses the Transformer model [Vaswani et al., 2017] and is pre-trained using masked reconstruction,while TimeGPT-1 [Garza et al., 2023] is pre-trained by generating a forecasting window. MOMENT[Goswami et al., 2024] extends a patch-based Transformer [Nie et al., 2023] to multiple datasetsby unifying the lengths of TS data using padding and sub-sampling. The model is also pre-trainedto reconstruct the masked patches. TOTEM [Talukder et al., 2024] applies a convolutional neuralnetwork (CNN) encoder to raw TS data and uses vector quantization (VQ) on the encoder outputs,providing a discrete and domain-invariant codebook for TS data. TOTEM is pre-trained as a VQ-VAE [van den Oord et al., 2017] to reconstruct the whole TS, viewing the latent-space codes fromconvolutions as a unified representation. UniTS [Gao et al., 2024] introduces a prompt-based methodto unify predictive and generative tasks within a single model and pre-training process. Although thesemethods learn representations that benefit various downstream tasks and demonstrate generalizability,these pre-trained models remain black boxes since they cannot provide human-understandablerepresentations.",
  "Proposed Method": "Towards interpretable TS modeling, we first present the formulations of shape-level representations,describing univariate TS data using a set of abstracted shapes and attributes. Then, we introduce thearchitecture of VQShape and its components with detailed workflow and products from each step. Notations.Let (X, Y) = {(xi, yi)|i = 1, . . . , N} denote a TS classification dataset with Nsamples, where xi RMT is a multivariate TS sample and yi {1, . . . , C} is the class label. Here,M is the number of variables, T is the length in timestamp, and C is the number of categories. Eachmultivariate TS sample xi can be viewed as a set of univariate TS samples where xmi RT denotesthe TS at the mth variable. For simplicity in notations, in this paper, xmi,t1:t2 denotes a subsequence ofxmi between timestamp Tt1 and Tt2, where t1, t2 are relative positions.",
  "lk R1 is the relative length of sk w.r.t. the length of x and lmin lk 1 tk": "Here, lmin is the hyperparameter that defines the minimum length of a shape. We set lmin = 1/64 as itis the length of a patch. In this work, we develop a pre-trained transformer model to produce a set ofattribute tuples T = {k | k = 1, . . . , K} given a univariate TS x. Additionally, the model learns acodebook of abstracted shape z that is reusable and generalizable for datasets from different domains.",
  ": Overview of VQShape": "TS Encoding.VQShape contains a patch-based transformer encoder [Nie et al., 2023, Goswamiet al., 2024] which first transforms a univariate TS x into K non-overlapping fixed-length patcheswith dimension dpatch. Then, the patches are encoded by learnable linear projection and additiveposition embedding, forming patch embeddings that serve as inputs to a transformer model. Thetransformer outputs K latent embeddings h Rdembed. Formally, the TS encoder is denoted by{hk Rdembed | k = 1, . . . , K} = E(x). Note that hk could contain information from all patchesinstead of only the kth patch.",
  "(1)": "Each decoding function in {fz, f, f, ft, fl} is implemented using a multi-layer perceptron (MLP)with one hidden layer and ReLU activation. Following a common notation [Esser et al., 2021], denotes the attribute tuple before quantization. Codebook and Vector-Quantization.The latent-space codebook is denoted by Z = {zq Rdcode |q = 1, . . . , N code}. To learn a generalizable codebook that contains only the abstracted shape-levelfeatures, we use low-dimensional codes with dcode = 8. This configuration also creates a bottleneckfor reconstruction, minimizing additional information that can be inferred besides the abstractedshapes. The quantization follows VQ-VAE [van den Oord et al., 2017] that selects the discrete codebased on Euclidean distance where",
  "zk = arg minzqZzk zq.(2)": "Shape Decoding.The abstracted shape of a TS subsequence is a sequence with its length, offset,and scale information removed through normalizations. Given k = (zk, k, k, tk, lk), we firstextract the target subsequence from x specified by tk and lk denoted by xtk:tk+lk. Then, xt:t+l isinterpolated to a fixed length of ds to remove the length information. The shape decoder S takeszk and outputs another sequence with the same length. Formally, for k, this step produces twosequences",
  "k=1stargetk sk22.(5)": "Vector Quantization.We follow VQ-VAE [van den Oord et al., 2017] to define the vector-quantization objective which trains the encoder E and codebook Z. Additionally, inspired byYu et al. , we add additional entropy terms to encourage codebook usage. We find these termscould improve pre-training stability and avoid collapse of codebook usage. The objective for learningthe codebook is defined by",
  ",(6)": "where sg() is the stop-gradient operator and H() is the entropy function for discrete variables.q(z, Z) = softmaxz zk22 | zk Zmeasures the distance between z and all codes in Z asa categorical distribution. Disentanglement of shapes.In Equation 5, the attributes (zk, l, k) are optimized towardsaccurate subsequence reconstructions. It is important to note that, since (tk, lk) defines stargetk, theyare essential for learning the abstracted shapes and the codebook. However, it is challenging touse gradients from reconstruction in Equation 4 solely to learn (tk, lk) for making informativesubsequence selection. Therefore, we introduce an additional regularization that encourages thelatent-space tokens (attributes) to capture shape-level information with diverse positions and scales.This regularization is defined as",
  "(7)": "In Equation 7, (tk, lk) defines a coordinate transformation which maps (tk, lk) into a space where(1) small lk values become more diverse and (2) large lk values from different tk become moreconcentrated. By making different (tk, lk) diverse in this space, Ldiv encourages the model to capturedisentangled shape-level information while increasing the use of short sequences to capture localdetails. visualizes an example of transformation . is a hyperparameter that defines athreshold distance in the transformed coordinate where two (tk, lk) samples are considered sufficientlydiverse. The overall pre-training objective is to minimizeLpretrain = xLx + sLs + vqLvq + divLdiv,(8)where x, s, vq, div are hyperparameters that define the weighting between the components.During pre-training of VQShape, we set x = s = vq = 1, div = 0.8, and commit = 0.25. Design Analysis.Overall, the encoding process in VQShape (Transformer encoder and attributedecoder) introduces an inductive bias by representing and summarizing univariate TS using a setof abstracted shapes along with their position, length, offset, and scale. The pre-training objectivesguide the components toward learning interpretable representations (via subsequence reconstructionin Equation 5) and disentangled representations (via regularization in Equation 7), while preservingthe information necessary to describe the TS (via reconstruction in Equation 4). These objectivesintroduce interpretability to the conventional deep autoencoder structure. By pre-training on diversedatasets with a universal codebook, VQShape further leverages this inductive bias to produce discreteand dataset-agnostic representations, resulting in a vocabulary of abstracted shapes that can be usedas primitives to describe TS data. Model Configurations.The settings of VQShape related to the model size correspond to those ofthe MOMENT-Small [Goswami et al., 2024] model. Specifically, we interpolate all the input univariateTS x to have length T = 512, which is broken into K = 64 patches with dpatch = 8. The Transformerlayers in the encoder E and decoder D have 8 heads, an embedding dimension dembed = 512, anda feed-forward layer of size 2048. We employ an asymmetric structure with an 8-layer encoder Eand a 2-layer decoder D [He et al., 2022]. The codebook Z contains N code = 512 codes, each withdimension dcode = 8. The subsequences stargetkand decoded sequences sk have length ds = 128.We set the minimum shape length lmin = 1/64. With these settings, VQShape has 37.1 millionparameters. In the pre-training stage, we train VQShape with the AdamW optimizer, using weight decay = 0.01,1 = 0.9, 2 = 0.999, gradient clipping of 1.0, and an effective batch size of 2048. We employ acosine learning rate schedule with an initial learning rate of 1e4, a final learning rate of 1e5, and 1epoch of linear warm-up. The pre-training dataset contains univariate TS extracted from the trainingsplit of 29 datasets from the UEA Multivariate TS Classification Archive [Bagnall et al., 2018],excluding the InsectWingbeat dataset, resulting in 1,387,642 univariate TS. We train VQShape for50 epochs on this dataset using bfloat-16 mixed precision.",
  "VQShape provides two types of representations: Latent-space Tokens and Code Histogram": "Tokens.Similar to the latent-space feature map of typical VQ approaches such as VQ-VAE[van den Oord et al., 2017] and VQ-GAN [Esser et al., 2021], VQShape also provides a set oftokens as representations. For an input univariate TS x, the token representations are composed asT RK(dcode+4) = {k = (zk, k, k, tk, lk) | k = 1, . . . , K}. The token representations can beuseful for general down-stream tasks but are less interpretable than the code histogram representationsin classification tasks. Code Histogram.Inspired by Concept Bottleneck Models (CBMs) [Koh et al., 2020] developed incomputer vision, we can also view each zq Z as a concept for TS data. As CBMs have conceptscores as representations, VQShape provides a similar representation in the form of a histogram ofcodes. Based on Equation 2, we can also have a vector of code indices",
  ".(9)": "Then, the code histogram representation is defined as r RN code = histogram(q) where eachelement in r is the frequency of index q in q. Intuitively, the code histogram representation isanalogous to BOSS [Schfer, 2015] but with non-deterministic window size and dataset-agnosticsymbols. In classification tasks, this type of representation can be more interpretable since classifiersbased on these features are able to produce rule-like predictions that are straightforward to interpretand understand.",
  "Experiments": "Datasets.We evaluate the pre-trained VQShape on multivariate TS classification tasks to demon-strate the effectiveness of learned shape-level representations on down-stream tasks. The evaluationsare performed on the test split of 29 datasets from the UEA multivariate TS classification archive[Bagnall et al., 2018, Ruiz et al., 2021], with the InsectWingbeat dataset excluded. summarizes statistics of those datasets. Details on experiment setups are included in Appendix A. Baselines.We benchmark VQShape against baselines from four categories, including (1) classicalmethods: DTW [Chen et al., 2013] and Shapelet Transform with Random Forest classifier (STRF)[Bostrom and Bagnall, 2017], (2) supervised learning methods: DLinear [Zeng et al., 2023], Auto-former [Wu et al., 2021], FEDformer [Zhou et al., 2022], PatchTST [Nie et al., 2023], and TimesNet[Wu et al., 2023], (3) unsupervised representation learning methods: TS-TCC [Eldele et al., 2021],TST [Zerveas et al., 2021], TS2Vec [Yue et al., 2022], and T-Rep [Fraikin et al., 2024], (4) modelspre-trained on multiple datasets: MOMENT [Goswami et al., 2024] and UniTS [Gao et al., 2024].We compute the classification accuracy as metrics and compare the methods based on the statistics ofaccuracy and ranking. Details on benchmarking setups are included in Appendix A. : Statistic and comparisons of the baselines and VQShape. The best case is marked with bold,the second best is marked with italic, and the third best is marked with underline. Some baselinesfail are incompatible with some datasets which result in N/A. For fair comparison, we report thestatistics with and without N/A. Complete results are presented in .",
  "DTWSTRFDLinearAutoformerFEDformerPatchTSTTimesNetTS-TCCTSTT-RepTS2VecMOMENTUniTSVQShape": "Statistics with N/AMean Accuracy0.6480.6600.6350.5700.6120.6690.7100.6820.6300.7190.7120.6860.6290.723Median Accuracy0.7110.6790.6730.5530.5860.7560.7970.7530.6200.8040.8120.7590.6840.810Mean Rank7.1387.8288.6909.4487.7508.2965.1437.1728.4485.2074.8975.9299.8285.621Median Rank7.08.09.010.08.08.05.08.09.04.03.05.510.05.0 Num. Top-132101013146506Num. Top-385508285412161109Num. Win/Tie14202225192014182015131325-Num. Lose1597497141191416154-Wilcoxon p-value0.2060.0230.0020.0000.0510.0000.8980.1560.0220.5360.5760.7330.000- Statistics without N/AMean Accuracy0.6420.6580.6350.5610.6010.6570.7030.6690.6230.7100.7040.6880.6180.720Median Accuracy0.7140.7120.6900.5520.5850.7390.7970.7520.6380.8020.7480.7590.6790.812Mean Rank7.2317.6928.9239.4627.7318.3465.3087.5388.4625.1925.0385.88510.1155.538Median Rank6.57.59.59.58.08.55.0008.09.54.53.05.510.54.5Num. Top-132101013135505Num. Top-375508274410141008Num. Win/Tie13182022172013171914121223-Num. Lose138649613971214143-Wilcoxon p-value0.1870.0360.0010.0010.1110.0010.8030.0940.0360.6530.6961.0000.000- Results.On multivariate TS classification tasks, for each TS sample xi X, we extract thetoken representations using VQShape and apply a learnable linear layer on this feature vector topredict the class label. summarizes the performance of VQShape and baseline methodson the 29 UEA datasets. Complete results are presented in . From the results, we observethat the best-performing methods, including TimesNet, T-Rep, TS2Vec, MOMENT, and VQShape,have similar performance, and none of the methods has performance that is statistically significantcompared to the others. Therefore, we conclude that VQShape achieves comparable performance tothe state-of-the-art baselines while additionally providing interpretable pre-trained features. Frozen Pre-trained Representations.Next, we compare the frozen pre-trained representationsfrom three existing pre-trained models: MOMENT, UniTS, and VQShape. Since the pre-trainingdataset could have a dominant effect on the models, for fair comparisons, we reproduce MOMENT-small and UniTS by training them on the same datasets as VQShape. summarizes the statisticsof the pre-trained models on the 29 UEA datasets. Complete results are presented in . Fromthe results, we observe that VQShape outperforms MOMENT and UniTS on most of the datasets andin overall statistics.",
  "Mean Accuracy0.6970.5810.7230.6970.5590.723Median Accuracy0.7360.6490.8100.7330.6490.792Mean Rank1.6552.8621.4831.6552.9661.310Num. Top-11301611020": "and its codebook can also generalize to datasets and domains not observed during pre-training. Todemonstrate cross-domain generalizability, we train another model using 9 datasets from the UEAarchive that are commonly selected to train and evaluate deep learning models [Zerveas et al., 2021,Wu et al., 2023], and then evaluate it on all 29 datasets. The right half of summarizes theperformance of this model, compared with MOMENT and UniTS trained with the same setup. Weobserve that VQShape and MOMENT trained on fewer datasets result in similar but slightly worseperformance, indicating that the representations learned by the models can generalize to unseendomains.",
  ": Visualization of the decoded codebookfrom VQShape-64": "Universal Codebook of Abstracted Shapes.One of the most essential components ofVQShape is the dataset-agnostic codebook thatcontains abstracted shapes. In of Ap-pendix C.1, we decode all 512 codes in thecodebook of VQShape to visualize their cor-responding abstracted shapes. We observe that alarge number of codes are decoded into similarshapes, which suggests that the codebook sizecan be further reduced. We then visualize thedistribution of codes learned from pre-training(see ) which contains about 60 clus-ters. Inspired by this observation, we train avariant named VQShape-64 with codebook sizeN code = 64. presents the decodedcodebook of VQShape-64. Interpretable Representations.Overall, theencoding of VQShape can be interpreted as TSx is decomposed into (shape z1 with offset 1and scale 1, at t1 with length l1), . . . , and thedecoding can be interpreted as The composi-tion of (shape z1 with offset 1 and scale 1, at t1 with length l1), . . . becomes x. includesan example of interpretable representations learned by VQShape. From visualizations, we can confirmthat VQShape can learn abstracted shapes that capture shape-level information with various positionsand scales. Discriminative Representations for Classification.We further show that the interpretable rep-resentations produced by VQShape also capture discriminative patterns that distinguish differentcategories in classification tasks. visualizes the average code histogram for samples fromtwo categories. From the feature maps, it is obvious that several codes have significant differencesin frequency between the two categories; these serve as discriminative features in classificationtasks. We decode and visualize their corresponding abstracted shapes. The intuition provided bythe histogram features can be interpreted as: Samples from the CW circle category usually containshape s61 in variate 1, and samples from the CCW circle category contain shape s33 in variate 3, etc. VQShapeEncode VQShapeDecode",
  ": An example of abstracted shapes and their attributes (i.e., token representations) extractedby VQShape. For better presentation, we visualize 6 of the 64 shapes": "Class: CW circle Class: CCW circle Variate 1Variate 2Variate 3 : Example of how the code histogram representations provide discriminative features forclassification. Histogram representations are obtained from VQShape-64. The histograms areaveraged over samples of the two classes from the test split of the UWaveGestureLibrary dataset. Thetop and bottom rows represent samples labeled as CW circle and CCW circle, respectively. Eachcolumn represent a variate (channel).",
  "Mean Accuracy": "TokenHistogram : Mean accuracy of classifierstrained with token and histogram rep-resentations across different codebooksizes. Performance of the token clas-sifiers improve with larger codebook,while performance of the histogram clas-sifiers peak at codebook size 64 and de-cline with larger codebook. Representations and Codebook Scaling.We com-pare the performance of classifiers trained with tokenor histogram representations from the same pre-trainedVQShape models. We also study how the size of the code-book affects the quality of these representations. Based onthe results presented in , we conclude that tokenclassifiers always outperform histogram classifiers sincetoken representations are more expressive than histogramrepresentations from the same pre-trained model. Additionally, the performance of token classifiers increasessteadily as codebook size increases, since tokens fromlarger codebooks could contain more details. However,for histogram representations, choosing the appropriatecodebook size is essential; we observe that a codebooksize of 64 results in the best performance. These resultsmatch our previous observation in .2 where themodel with N code = 512 results in only approximately 60clusters of codes. A small codebook would learn codesthat are too abstract to provide detailed representations,while a large codebook would learn similar codes thatcould lead to misleading features in the histograms. From the experimental results, we conclude that N code = 64 strikes the best balance between abstractionand expressiveness for pre-training on the UEA datasets, producing the best histogram representations. Note that within the scope of this paper, the number of code clusters remains a post-hoc discoverythat can only be determined after pre-training. One could start with a large codebook size such asN code = 512, visualize the distribution of codes, and re-train with the appropriate codebook size.Developing a mechanism to adjust the codebook size dynamically during training or inference wouldbe a valuable direction for future work.",
  "Mean AccuracyToken0.7230.7210.708Median AccuracyToken0.8100.8000.761Mean AccuracyHistogram0.7090.7170.707Median AccuracyHistogram0.7620.8100.747": "Other Ablation Studies.We additionally con-duct two ablation studies: (1) setting dcode =32 to compare high-dimensional and low-dimensional codes, and (2) setting s = 0 toassess the value of introducing shape-level in-formation to the VQ-VAE structure. summarizes the statistics of these two cases andthe default setting. The complete results arepresented in and . From theoverall statistics, pre-training with s = 0 leadsto degraded performance, which indicates thatlearning shape-level abstractions through sub-sequence reconstruction introduces useful information for classification tasks. Using codes withdcode = 32 produces token classifiers with similar performance and better histogram classifiers.However, as stated in .2, we use low-dimensional code to create a bottleneck where thecode should mainly contain the shape-level information. Using high-dimensional code may introduceadditional information beyond the decoded shapes, which reduces interpretability. Therefore, theeffect of using high-dimensional code would require extensive study.",
  "Conclusion and Limitations": "This paper introduces VQShape, a self-supervised pre-trained, interpretable, and generalizable modelfor TS analysis. Taking advantage of large pre-trained Transformers and ideas from shapelets,VQShape extracts a set of interpretable representations from TS data, composed of abstractedshape, offset, scale, position, and duration. Pre-trained and evaluated on multivariate classificationdatasets from the UEA archive, VQShape achieves comparable or even better performance than black-box models pre-trained on large datasets and with significantly more parameters, while providingadditional interpretable representations. Furthermore, using VQShape, we present a codebookcontaining a set of abstracted shapes that generalize to various TS datasets, including unseen datasets. In this paper, we do not term VQShape a foundation model for TS data, since the amount of pre-training data is still limited compared to other large pre-trained models such as MOMENT, and weare focusing only on classification tasks because the extracted shape tokens are mostly interpretablefor classification. As presented in , based on the comparison between VQShape pre-trainedon 29 and 9 datasets, simply adding more pre-training data may degrade performance on somedatasets. This suggests that learning shape-level representations may not benefit from some types ofdata. For example, the BasicMotion datasets mostly contain signals with high-frequency sinusoidalcomponents that do not contain meaningful shape-level information in short subsequences. Suchdatasets may \"pollute\" the pre-training of VQShape since the model will try to capture unnecessaryhigh-frequency features. Therefore, if pre-trained on datasets at scale, additional pre-processing andinput engineering should be included; however, we do not conduct explicit pre-processing in thispaper. It would also be an important future step to develop interpretable frameworks for other TSanalysis tasks, such as forecasting, imputation, and anomaly detection, using the interpretable tokensextracted by VQShape. This work was supported in part by IBM through the IBM-Rensselaer Future of Computing ResearchCollaboration. We thank Dr. Eamonn Keogh for his valuable comments and suggestions for revision.We also appreciate the UEA teams effort in creating and publicly sharing the benchmark datasets. Anthony Bagnall, Hoang Anh Dau, Jason Lines, Michael Flynn, James Large, Aaron Bostrom, PaulSoutham, and Eamonn Keogh. The UEA multivariate time series classification archive, 2018.arXiv preprint arXiv:1811.00075, 2018.",
  "Aaron Bostrom and Anthony Bagnall. A shapelet transform for multivariate time series classification.arXiv preprint arXiv:1712.06428, 2017": "Yanping Chen, Bing Hu, Eamonn Keogh, and Gustavo EAPA Batista. DTW-D: time series semi-supervised learning from a single example. In Proceedings of the 19th ACM SIGKDD internationalconference on Knowledge discovery and data mining, pages 383391, 2013. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, ThomasUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit,and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale.In International Conference on Learning Representations, 2021. Emadeldeen Eldele, Mohamed Ragab, Zhenghua Chen, Min Wu, Chee Keong Kwoh, Xiaoli Li, andCuntai Guan. Time-series representation learning via temporal and contextual contrasting. InProceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21,pages 23522359, 2021. Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution imagesynthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,pages 1287312883, 2021. Archibald Felix Fraikin, Adrien Bennetot, and Stephanie Allassonniere. T-rep: Representationlearning for time series using time-embeddings. In The Twelfth International Conference onLearning Representations, 2024.",
  "Mononito Goswami, Konrad Szafer, Arjun Choudhry, Yifu Cai, Shuo Li, and Artur Dubrawski.Moment: A family of open time-series foundation models. In International Conference onMachine Learning, 2024": "Josif Grabocka, Nicolas Schilling, Martin Wistuba, and Lars Schmidt-Thieme. Learning time-seriesshapelets. In Proceedings of the 20th ACM SIGKDD international conference on Knowledgediscovery and data mining, pages 392401, 2014. Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollr, and Ross Girshick. Maskedautoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computervision and pattern recognition, pages 1600016009, 2022. Pang Wei Koh, Thao Nguyen, Yew Siang Tang, Stephen Mussmann, Emma Pierson, Been Kim, andPercy Liang. Concept bottleneck models. In International conference on machine learning, pages53385348. PMLR, 2020. Yuxuan Liang, Haomin Wen, Yuqi Nie, Yushan Jiang, Ming Jin, Dongjin Song, Shirui Pan, andQingsong Wen. Foundation models for time series analysis: A tutorial and survey. arXiv preprintarXiv:2403.14735, 2024. Jason Lines, Luke M Davis, Jon Hills, and Anthony Bagnall. A shapelet transform for time seriesclassification. In Proceedings of the 18th ACM SIGKDD international conference on Knowledgediscovery and data mining, pages 289297, 2012. Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. A time series is worth64 words: Long-term forecasting with transformers. In International Conference on LearningRepresentations, 2023. Alejandro Pasos Ruiz, Michael Flynn, James Large, Matthew Middlehurst, and Anthony Bagnall.The great multivariate time series classification bake off: a review and experimental evaluation ofrecent algorithmic advances. Data Mining and Knowledge Discovery, 35(2):401449, 2021.",
  "Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. In Journal of MachineLearning Research, volume 9, pages 25792605, 2008": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, LukaszKaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural InformationProcessing Systems, volume 30, 2017. Zhiguang Wang, Weizhong Yan, and Tim Oates. Time series classification from scratch with deepneural networks: A strong baseline. In 2017 International joint conference on neural networks(IJCNN), pages 15781585. IEEE, 2017. Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Autoformer: Decomposition transform-ers with auto-correlation for long-term series forecasting. In Advances in Neural InformationProcessing Systems, 2021. Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng Long. Timesnet:Temporal 2d-variation modeling for general time series analysis. In International Conference onLearning Representations, 2023.",
  "Lexiang Ye and Eamonn Keogh. Time series shapelets: a novel technique that allows accurate,interpretable and fast classification. Data mining and knowledge discovery, 2011": "Lijun Yu, Jose Lezama, Nitesh Bharadwaj Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen,Yong Cheng, Agrim Gupta, Xiuye Gu, Alexander G Hauptmann, Boqing Gong, Ming-Hsuan Yang,Irfan Essa, David A Ross, and Lu Jiang. Language model beats diffusion - tokenizer is key tovisual generation. In The Twelfth International Conference on Learning Representations, 2024. Zhihan Yue, Yujing Wang, Juanyong Duan, Tianmeng Yang, Congrui Huang, Yunhai Tong, andBixiong Xu. TS2Vec: Towards universal representation of time series. In Proceedings of the AAAIConference on Artificial Intelligence, volume 36, pages 89808987, 2022. Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. Are transformers effective for time seriesforecasting? In Proceedings of the AAAI conference on artificial intelligence, volume 37, pages1112111128, 2023. George Zerveas, Srideepika Jayaraman, Dhaval Patel, Anuradha Bhamidipaty, and Carsten Eickhoff.A transformer-based framework for multivariate time series representation learning. In Proceedingsof the 27th ACM SIGKDD conference on knowledge discovery & data mining, pages 21142124,2021. Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. Fedformer: Frequencyenhanced decomposed transformer for long-term series forecasting. In International conference onmachine learning, pages 2726827286. PMLR, 2022.",
  "DatasetTrain SizeTest SizeVariablesLengthCategoriesTypeNtrainNtestMTC": "ArticularyWordRecognition275300914425MOTIONAtrialFibrillation151526403ECGBasicMotions404061004HARCharacterTrajectories14221436311920MOTIONCricket108726119712HARDuckDuckGeese505013452705AUDIOERing302704656HAREigenWorms1281316179485MOTIONEpilepsy13713832064HAREthanolConcentration261263317514OTHERFaceDetection58903524144622EEGFingerMovements31610028502EEGHandMovementDirection16074104004EEGHandwriting15058035226HARHeartbeat204205614052AUDIOInsectWingbeat25000250002002210AUDIOJapaneseVowels27037012299AUDIOLSST2459246663614OTHERLibras18018024515HARMotorImagery2781006430002EEGNATOPS18018024516HARPEMS-SF2671732631447MISCPenDigits749434982810MOTIONPhonemeSpectra331533531121739SOUNDRacketSports1511526304HARSelfRegulationSCP126829368962EEGSelfRegulationSCP2200180711522EEGSpokenArabicDigits65992199139310SPEECHStandWalkJump1215425003ECGUWaveGestureLibrary12032033158HAR Classification tasks.Taking frozen pre-trained representations from VQShape, we learn a linearclassifier to make predictions. When training the linear classifier, we found that token representationswork better with regularization of L2 on classifier weights, while histogram representations aremore compatible with dropout on features. Therefore, to avoid overfitting and obtain the optimalperformance, we tune both the L2 regularization weight (or weight decay) and the dropout rate. When learning the linear classifier, we repeat the experiment with five random seeds and report theaverage accuracy in . The standard deviations of the five runs are included in . Notethat we exclude the InsectWingbeat dataset since the dataset contains inconsistent and very shortTS samples such as T = 1. Considering that the dataset has significantly more samples and channelsthan other datasets, the high volume of such short samples may have a negative effect on our methodsince the short TS do not contain any meaningful shape-level features. Baseline Results.For baseline results presented in and , we reproduce STRFwith the Aeon-Tookit2 and Scikit-learn packages. We reproduce DLinear, Autoformer, FEDformer,PatchTST, and TimesNet using implementation from Wu et al. 3. Results of DTW, TS-TCC,TST, and TS2Vec are obtained from the TS2Vec paper [Yue et al., 2022]. Results of MOMENT",
  "The Aeon-Tookit is available at www.aeon-toolkit.org.3The implementations are available at": "[Goswami et al., 2024] and T-Rep Fraikin et al. are obtained from the original papers. Forresults of UniTS [Gao et al., 2024], since the full results on the UEA datasets are not reported inthe original paper, we follow the official implementation to reproduce the results using a pre-trainedcheckpoint 4 and prompt learning. Benchmarking Frozen Pre-trained Models.To obtain the results in and , wetrain MOMENT and UniTS from scratch using the official implementations 5. Note that the wayMOMENT, UniTS, and VQShape produce predictions can be different. MOMENT and VQShapeprovide frozen representations and learn a separate classifier, whereas UniTS learns additionalprompt tokens and classification heads. Therefore, we follow the default procedure and the officialimplementation to evaluate UniTS. We evaluate MOMENT and VQShape under the exact sameprocedure of learning linear classifiers with regularization tuning. Note that the key difference between results in and is that MOMENT and UniTSin are pre-trained on a significantly larger volume of data (which includes the forecastingdatasets), while the three models are only pre-trained on the UEA datasets in .",
  "Codebook SizeN code:3264128512512512Code Dim.dcode:8888832Shape Loss Weights:111101Representation:TokenTokenTokenTokenTokenToken": "ArticularyWordRecognition0.979 0.0320.988 0.0210.990 0.0240.987 0.0180.981 0.0300.991 0.025AtrialFibrillation0.547 0.0730.520 0.1910.573 0.0680.520 0.0680.547 0.0900.653 0.107BasicMotions0.900 0.0480.925 0.0370.940 0.0290.910 0.0440.950 0.0530.960 0.046CharacterTrajectories0.960 0.0190.965 0.0290.963 0.0270.969 0.0300.958 0.0350.958 0.037Cricket0.978 0.0220.981 0.0190.986 0.0100.978 0.0160.986 0.0230.986 0.032DuckDuckGeese0.384 0.0470.400 0.0390.460 0.0600.360 0.0450.372 0.0430.408 0.046ERing0.966 0.0160.979 0.0060.984 0.0100.960 0.0110.761 0.0220.973 0.016EigenWorms0.589 0.0340.597 0.0360.569 0.0310.603 0.0350.608 0.0350.608 0.034Epilepsy0.730 0.0230.800 0.0310.841 0.0200.893 0.0310.803 0.0290.817 0.030EthanolConcentration0.313 0.0200.329 0.0180.317 0.0190.325 0.0200.335 0.0180.313 0.018FaceDetection0.640 0.0060.657 0.0070.639 0.0130.653 0.0110.672 0.0070.651 0.008FingerMovements0.640 0.0290.646 0.0390.630 0.0290.642 0.0250.644 0.0320.630 0.025HandMovementDirection0.438 0.0460.478 0.0310.459 0.0480.546 0.0490.565 0.0360.457 0.041Handwriting0.234 0.0090.278 0.0160.284 0.0140.270 0.0120.267 0.0100.255 0.013Heartbeat0.635 0.0320.633 0.0210.609 0.0300.663 0.0290.602 0.0280.621 0.024JapaneseVowels0.950 0.0220.950 0.0190.955 0.0140.945 0.0140.937 0.0170.941 0.029LSST0.476 0.0190.483 0.0130.509 0.0120.511 0.0110.502 0.0150.495 0.015Libras0.809 0.0160.804 0.0200.801 0.0130.814 0.0290.769 0.0130.804 0.019MotorImagery0.678 0.0460.670 0.0370.656 0.0290.680 0.0390.668 0.0240.690 0.046NATOPS0.781 0.0200.820 0.0410.800 0.0230.810 0.0210.790 0.0250.800 0.022PEMS-SF0.842 0.0420.876 0.0380.872 0.0590.865 0.0440.840 0.0320.837 0.041PenDigits0.967 0.0250.969 0.0320.970 0.0290.973 0.0380.949 0.0330.968 0.044PhonemeSpectra0.083 0.0030.077 0.0030.085 0.0030.087 0.0030.107 0.0030.092 0.003RacketSports0.864 0.0260.855 0.0280.888 0.0150.851 0.0190.841 0.0130.872 0.029SelfRegulationSCP10.881 0.0230.904 0.0260.887 0.0250.904 0.0390.904 0.0140.880 0.020SelfRegulationSCP20.597 0.0240.593 0.0270.606 0.0340.596 0.0180.587 0.0250.600 0.029SpokenArabicDigits0.971 0.0200.974 0.0200.977 0.0190.976 0.0210.977 0.0280.976 0.023StandWalkJump0.760 0.1080.813 0.1030.733 0.1000.787 0.1310.760 0.1120.787 0.126UWaveGestureLibrary0.893 0.0170.898 0.0090.912 0.0190.887 0.0120.846 0.0100.891 0.013 : Full results of VQShape variants with histogram classifiers. For a frozen pre-trained model,the mean and standard deviation of accuracies over five randomly initialized linear classifiers arereported.",
  "Codebook SizeN code:3264128512512512Code Dim.dcode:8888832Shape Loss Weights:111101Representation:HistogramHistogramHistogramHistogramHistogramHistogram": "ArticularyWordRecognition0.983 0.0560.996 0.0500.993 0.0380.991 0.0340.986 0.0430.988 0.030AtrialFibrillation0.560 0.0840.600 0.0900.533 0.1440.573 0.1000.520 0.1030.507 0.112BasicMotions1.000 0.0411.000 0.0190.985 0.0120.955 0.0290.930 0.0251.000 0.016CharacterTrajectories0.888 0.0330.908 0.0300.931 0.0340.942 0.0400.928 0.0370.938 0.036Cricket0.967 0.0260.986 0.0331.000 0.0220.986 0.0240.975 0.0150.978 0.034DuckDuckGeese0.428 0.0450.468 0.0540.440 0.0390.396 0.0320.360 0.0420.396 0.048ERing0.929 0.0640.956 0.0190.856 0.0180.927 0.0130.935 0.0170.928 0.016EigenWorms0.612 0.0270.571 0.0230.553 0.0170.669 0.0180.747 0.0160.602 0.018Epilepsy0.929 0.0400.959 0.0270.964 0.0330.970 0.0260.858 0.0431.000 0.039EthanolConcentration0.301 0.0120.313 0.0180.308 0.0140.319 0.0170.319 0.0200.310 0.015FaceDetection0.567 0.0090.567 0.0100.578 0.0080.584 0.0060.655 0.0130.601 0.009FingerMovements0.598 0.0280.598 0.0330.600 0.0360.600 0.0240.600 0.0260.600 0.028HandMovementDirection0.441 0.0370.449 0.0280.438 0.0300.432 0.0450.481 0.0310.427 0.046Handwriting0.156 0.0090.212 0.0100.196 0.0070.148 0.0080.235 0.0080.206 0.010Heartbeat0.736 0.0070.743 0.0060.741 0.0110.739 0.0110.730 0.0080.743 0.008JapaneseVowels0.939 0.0480.948 0.0430.945 0.0500.926 0.0480.936 0.0440.942 0.039LSST0.515 0.0340.521 0.0350.542 0.0290.515 0.0250.516 0.0250.556 0.018Libras0.768 0.0200.804 0.0290.811 0.0230.818 0.0200.730 0.0210.850 0.024MotorImagery0.628 0.0240.672 0.0220.650 0.0280.662 0.0300.626 0.0420.678 0.029NATOPS0.847 0.0200.844 0.0220.886 0.0200.808 0.0160.790 0.0220.864 0.020PEMS-SF0.839 0.0500.864 0.0360.853 0.0360.846 0.0660.810 0.0340.824 0.062PenDigits0.898 0.0630.942 0.0750.949 0.0670.969 0.0380.915 0.0480.958 0.030PhonemeSpectra0.116 0.0030.134 0.0060.132 0.0050.143 0.0040.104 0.0050.150 0.004RacketSports0.774 0.0460.879 0.0550.883 0.0380.838 0.0270.861 0.0600.832 0.027SelfRegulationSCP10.731 0.0270.754 0.0270.788 0.0220.762 0.0300.881 0.0150.810 0.028SelfRegulationSCP20.576 0.0270.588 0.0260.577 0.0230.572 0.0220.617 0.0250.583 0.028SpokenArabicDigits0.942 0.0320.959 0.0440.961 0.0480.969 0.0610.975 0.0450.969 0.051StandWalkJump0.680 0.1160.560 0.0600.613 0.0940.613 0.0680.653 0.0730.640 0.073UWaveGestureLibrary0.841 0.0330.864 0.0360.904 0.0160.899 0.0300.821 0.0140.923 0.026",
  "C.3Visualization of transform": "We provide a visualization (t, l) transform discussed in .1 in . The left figureshows (t, l) samples uniformly sampled from the original coordinate. According to .1, (t, l)samples can only appear in the lower-triangular plane. After the (t, l) transform, the correspondingsamples are plotted in the transformed coordinate. Samples with small l in the original coordinatebecomes more separated in the new coordinate, which encourage the model to capture local details inshort subsequences. Samples with large l becomes more concentrated and are less sensitive to their tvalue since they are likely to capture redundant information. Original CoordinateTransformed Coordinate"
}