{
  "Abstract": "In real-world applications, the distribution of the data, and our goals, evolveover time. The prevailing theoretical framework for studying machine learning,namely probably approximately correct (PAC) learning, largely ignores time. As aconsequence, existing strategies to address the dynamic nature of data and goalsexhibit poor real-world performance. This paper develops a theoretical frameworkcalled Prospective Learning that is tailored for situations when the optimalhypothesis changes over time. In PAC learning, empirical risk minimization (ERM)is known to be consistent. We develop a learner called Prospective ERM, whichreturns a sequence of predictors that make predictions on future data. We prove thatthe risk of prospective ERM converges to the Bayes risk under certain assumptionson the stochastic process generating the data. Prospective ERM, roughly speaking,incorporates time as an input in addition to the data. We show that standard ERMas done in PAC learning, without incorporating time, can result in failure to learnwhen distributions are dynamic. Numerical experiments illustrate that prospectiveERM can learn synthetic and visual recognition problems constructed from MNISTand CIFAR-10. Code at",
  "Introduction": "All learning is for the future. Learning involves updating decision rules or policies, based on pastexperiences, to improve future performance. Probably approximately correct (PAC) learning has beenextremely useful to develop algorithms that minimize the risktypically defined as the expectedlosson unseen samples under certain assumptions. The assumption, that samples are independentand identically distributed (IID) within the training dataset and at test time, has served us well. But itis neither testable nor believed to be true in practice. The future is always different from the past:both distributions of data and goals of the learner may change over time. Moreover, those changesmay cause the optimal hypothesis to change over time as well. There are numerous mathematicaland empirical approaches that have been developed to address this issue, e.g., techniques for beinginvariant to , or adapting to, distribution shift , modeling the future as a different task, etc. Butwe lack a first-principles framework to address problems where data distributions and goals maychange over time in such a way that the optimal hypothesis is time-dependent. And as a consequence,machine learning-based AI today is brittle to changes in distribution and goals. This paper develops a theoretical framework called Prospective Learning (PL). Instead of dataarising from an unknown probability distribution like in PAC learning, prospective learning assumesthat data comes from an unknown stochastic process, that the loss considers the future, and thatthe optimal hypothesis may change over time. A prospective learner uses samples received up tosome time t N to output an infinite sequence of predictors, which it uses for making predictions on",
  "data at all future times t > t. We discuss how prospective learning is related to existing problemformulations in the literature in and Appendix A": "Why should one care about prospective learning?Imagine a deployed machine learning system.The designer of this system desires to optimizenot the risk upon the past training data, or the riskon the immediate future databut the risk on all data that the model will make predictions upon inthe future. As data evolves, e.g., due to changing trends and preferences of the users, the optimalhypothesis to make predictions also changes. Time is the critical piece of information if the systemdesigner is to achieve their goals. Both in the sense of how far back in time a particular datum wasrecorded, and in the sense of how far ahead in the future this system will be used to make predictions.The designer must take time into account to avoid retraining the model periodically, ad infinitum. Biology is also rich with examples where systems seem to behave prospectively. The principle ofallostasis, for example, states that regulatory processes of living things anticipate the needs of theorganism and prepare to satisfy these needs before, rather than after, they arise . For example,mitochondria increase their energy production to anticipate the demands of muscles , neuralcircuits anticipate changes in sensory stimuli and the task (i.e., predictive coding ), and individualorganisms optimize their actions with respect to anticipated changes in their environments .These regulatory principles were learned early in evolutionary time so they must be important. Inshort, the worldincluding our internal driveschanges all the time, and learning systems mustanticipate (that is, prospect) these changes to thrive.",
  "and Appendix A put prospective learning in context relative to existing ideas inthe literature to address changes in the data distribution": "takes steps towards a theoretical foundation for prospective learning. We de-fine strongly learnability (i.e., there exists a prospective learner whose risk is arbitrarilyclose to the Bayes optimal learner) and weakly learnability (i.e., there exists a prospectivelearner whose risk is better than chance) . Empirical risk minimization (ERM) withoutincorporating time, can result in failure to strongly, or even weakly, learn prospectively .",
  "A prospective learner minimizes the expected cumulative risk of the future using past data. Such alearner is defined by the following key ingredients (see (left) for schematic illustration)": "Data. Let the input and output at time t be denoted by xt X and yt Y respectively. Letzt = (xt, yt). We will model the data as a stochastic process Z (Zt)tN defined on an appropriateprobability space (, F, P). At time t N, denote past data by zt (z1, . . . , zt) and future data byz>t (zt+1, . . . ). We will find it useful to distinguish between the realization of the data, denoted byzt, and the corresponding random variable, Zt. Hypothesis class.At each time t, a prospective learner selects an infinite sequence h (h1, . . . , ht, ht+1, . . . ) which it uses to make predictions on data at any time in the future. Eachelement of this sequence ht : X Y and therefore ht YX .1 The hypothesis class H is the space 1We will use some non-standard notation in this paper. In particular, a hypothesis h will always refer tosequence of predictors h (h1, . . . , ht, ht+1, . . . ). This helps us avoid excessively verbose mathematicalexpressions. of such hypotheses, h H (YX )N.2 We will again use the shorthand ht (h1, . . . , ht). We willsometimes talk about a time-agnostic hypothesis which will refer to a hypothesis such that ht = htfor all t, t N. Observe that this makes our setup different from the standard setup in PAC learningwhere the learner selects a single hypothesis in YX . One could also think of prospective learning asusing a single time-varying hypothesis h : N X Y, i.e., the hypothesis takes both time and thedatum as input to make a prediction. Learner. A prospective learner is a map from the data received up to time t, to a hypothesis thatmakes predictions on the data over all time (past and future): (X Y)t (YX )N. The learner givesas output a hypothesis h(zt) H. Unlike a PAC learner, a prospective learner can make differentkinds of predictions at different times. This is a crucial property of prospective learning. In otherwords, after receiving data up to time t, the hypothesis selected by the prospective learner can predicton samples at any future time t > t.",
  "Rt(h) = Et(h, Z) | zt=t(h, Z) d PZ|zt ,(2)": "where we assume that h is a random variable and h (Zt) where () denotes the filtration (anincreasing sequence of sigma algebras) of the stochastic process Z. We have used the shorthandE[Y | x] for E[Y | X = x]. Observe that we have conditioned the prospective risk of the hypothesis hupon the realized data zt. We can take an expectation over the realized data, to obtain the expectedprospective risk",
  "E [Rt(h)] =Rt(h) d PZt": "Prospective Bayes risk is the minimum risk achievable by any hypothesis. In PAC learning, itis a constant that depends upon the (fixed) true distribution of the data and the risk function. Inprospective learning, the optimal hypothesis can predict differently at different times. We thereforedefine the prospective Bayes risk at a time t as",
  "Different prospective learning scenarios with illustrative examples": "We next discuss four prospective learning scenarios that are relevant to increasingly more generalclasses of stochastic processes. Our goal is to illustrate, using examples, how the definitions developedin the previous section capture these scenarios. We will assume that for all times t we have Xt = 1,Yt {0, 1}. We will also focus on the time-invariant zero-one loss (t, y, y) = (y = y) for all t, here is the Dirac delta function. shows example realizations of the data for each scenario. Scenario 1 (Data is independent and identically distributed). Formally, this consists of stochasticprocesses where PZt|Zt = PZt for all t, t N. As an example, consider Yt Bernoulli(p) forsome unknown parameter p . Prospective Bayes risk is equal to min(p, 1 p) in this case. A 2When we say that learner selects a hypothesis in the sequel, it will always mean that the learner selects aninfinite sequence from within the hypothesis class H.3The limsup is guaranteed to exist if is bounded. If the series converges, we can use lim instead.4There are many real world scenarios where expected future loss may not be sufficient for good performance,e.g., for portfolio managements or inference by biological learners who optimize for a balance between valueand risk. Moreover, the risk functional could, in general, also change over time. In this paper, we will focus onlyon the expected future loss.",
  "Time": "RealizedPast PotentialFutures Scenario 4 Time (t) 0.2 0.3 0.4 0.5 Prospective risk Independent and identically distributed data Time (t) 0.2 0.3 0.4 0.5 Prospective risk Independent but not identically distributed data Time (t) 0.2 0.3 0.4 0.5 Prospective risk Data from a two-state Markov chain Time (t) 0.2 0.3 0.4 0.5 Prospective risk Data from a two-state Markov decision process MLEProspective learnerVariant 2Variant 3Bayes Risk : A schematic for prospective learning (left) and realizations of the examples for the four scenarios(top right); dots denote 1s and empty spaces denote 0s for Yt {0, 1} with Xt = 1 for all times t. Prospectiverisk of learners at different times is shown in the bottom panels and discussed in .1. Scenario 1: ForBernoulli probability p = 0.2, the maximum-likelihood estimator (MLE) in blue uses a time-agnostic hypothesisht(Xt) = 1(pt > 0.5) where pt = t1 ts=1 ys, ties at pt = 0.5 are broken randomly. The risk of this learnerconverges to the Bayes risk. Scenario 2: For Bernoulli probability p = 0.2, the MLE estimator (blue) performsat chance levels. A prospective learner (orange) that alternates between two predictors at even and odd timesconverges to Bayes risk. Variants of this learner that use less information from the stochastic process (green doesnot know that the data distributions at even and odd times are tied, red does not know that the distribution shiftsat every time-step) also converge to Bayes risk, but more slowly. Scenario 3: For = 0.1 and = 0.9 in thediscounted prospective risk, the MLE estimator (blue) again performs at chance levels. A prospective learnerthat computes an estimate of the transition probability of the two-state Markov chain to estimate P(Yt | yt) forfuture times t > t converges to Bayes risk. Scenario 4: For 0 = 1 = 0.1, the MLE estimator (blue) performsat chance levels. A prospective learner that uses a variant of Q-learning (described in the text and Appendix B.3)converges to the prospective Bayes risk.",
  "time-agnostic hypothesis, for example one that thresholds the maximum likelihood estimator (MLE)of the Bernoulli probability, converges to the limiting prospective Bayes risk.5": "Scenario 2 (Data is independent but not identically distributed). This consists of stochasticprocesses where PZt|Zt = PZt for all t N. Consider Yt Bernoulli(p) if t is odd, and Yt Bernoulli(1 p) if t is even, i.e., data is drawn from two different distributions at alternate times.Prospective Bayes risk is again equal to min(1 p, p) in this case. A time-agnostic hypothesis canonly perform at chance level. But a prospective learner, for example one that selects a hypothesis thatalternates between two predictors at even and odd times, can converge to prospective Bayes optimalrisk. We can also construct variants, e.g., when the relationship between the Bernoulli probabilitiesare not known (Variant 1 in ), or when the learner does not know that the data distributionchanges at every time step (Variant 2 in where we implemented a generalized likelihood ratiotest to determine whether the distribution changes). The risk of these variants also converges toprospective Bayes risk, but they need more samples because they use more generic models of thestochastic process. This scenario is closely related to (online) multitask/meta-learning .Scenario 3 (Data is neither independent nor identically distributed). Formally, this scenarioconsists of general stochastic processes. As an example, consider a Markov process P(Yt+1 = k |Yt = k) = with two states k {0, 1} and Y1 Bernoulli(). The invariant distribution of thisMarkov process is P(0) = P(1) = 1/2. Prospective Bayes risk is also equal to 1/2. For stochasticprocesses that have a invariant distribution, it is impossible to predict the next state infinitely far intothe future and therefore it is impossible to prospect. The prospective Bayes risk is trivially chancelevels. In such situations, the learner could consider losses that are discounted over time. For example,one could use a slightly different loss than the one in Eq. (1) to write",
  "t(h, Z) = (1 ) s=t+1 st1(hs(Xs), Ys)(4)": "for some [0, 1). In this example, we can calculate the prospective Bayes risk analytically;see Appendix B.2. For = 0.9, = 0.1 and the zero-one loss, limiting prospective Bayes risk is0.357. Now consider a learner which computes the MLE of the transition matrix ttt. It calculates 5We show an interesting observation in Appendix B.1: if the prior of a Bayesian learner is different from thetrue Bernoulli probability, then prospective learning can improve upon the maximum a posteriori estimator. P(Yt | yt) = pt where [1pt, pt] = ttt[1yt, yt] and uses the hypothesis ht(Xt) = 1(pt > 0.5)(ties broken randomly). We can see in that this learner converges to the prospective Bayesrisk. This example shows that if we model the changes in the data, then we can perform prospectivelearning. This scenario is closely related to certain continual learning problems .Scenario 4 (Future depends upon the current prediction). Problems where predictions of thelearner affect future data are an interesting special case of Scenario 3. Prospective learning canalso be used to address such scenarios. For 0, 1 , consider a Markov decision process(MDP) P(Yt+1 = j | Yt = j, ht+1(1) = k) = k if j = j and 1 k otherwise. I.e., the predictionht+1(Xt+1) = k (recall that Xt = 1 for all times) is the decision and the MDP remains in the samestate with probability k. Prospective Bayes risk for this example is the same as that of the examplein Scenario 3. We can construct a prospective learner using a variant of Q-learning to first estimatethe hypothesis and then estimate the probability P(Yt | yt) like Scenario 3 above to predict on futuredata at time t. See Appendix B.3. Prospective risk of this learner converges to Bayes risk in .This scenario is closely related to reinforcement learning .",
  "How is prospective learning related to other learning paradigms? 6": "Distribution shift. Prospective learning is equivalent to PAC learning in Scenario 1 whendata is IID. Situations when this assumption may not be valid are often modeled as a distributionshift between train and test data . Techniques such as propensity scoring or domainadaptation reweigh or map the train/test data to get back to the IID setting; techniques likedomain invariance build a statistic that is invariant to the shift. Typically, the loss is unchangedacross train and test data. If the set of marginals {P(Zt)} of the stochastic process only has twoelements, then PL is equivalent to the classical distribution shift setting. But otherwise, in PL, data iscorrelated across time, distributions (marginals) can shift multiple times, and risk changes with time. Multi-task, meta-, continual, and lifelong learning. A changing data distribution could be modeledas a sequence of tasks. Depending upon the stochastic process, different concepts are relevant, e.g.,multi-task learning is useful for Scenario 2 and Appendix D when there are a finite numberof tasks. Much of continual or lifelong learning focuses on task-incremental and class-incremental\" settings , in which the learner knows when the task switches. PL does not makethis assumption, and therefore, the problem is substantially more difficult. Data-incremental (ortask-agnostic) setting , is similar to PL. But the main difference is the goal: continual or lifelonglearning seeks to minimize past error. As a consequence, continual learning methods are poorprospective learners; see . Online meta-learning is close to task-agnostic continuallearning, except that the former models tasks as being sampled IID from some distribution of tasks.Due to this, one cannot predict which task is next, and therefore cannot prospect. Sequential decision making and online learning. PL builds upon works on learning from streamingdata. But our goals are different. For example, Gama et al. minimize the error on samples froma stationary process; Hayes et al. minimize the error on a fixed held-out dataset or on all pastdataneither of these emphasizes prospection. There is a rich body of work on sequential decisionmaking, e.g., predicting a finite-state, stationary ergodic process from past data . Even in thissimple case, there does not exist a consistent estimator using the finite past Z1:t . This isalso true for regression , when the true hypothesis f s.t. Yt = f(Xt) is fixed. In otherwords, Bayes risk Rt in Theorem 1 may be non-zero in PL even for finite-state, stationary ergodicprocesses. Hanneke lifts the restriction on stationarity and ergodicity. They obtain conditions onthe input process X for consistent inductive (predict at time t > t using data up to t), self-adaptive(predict at time t using Zt and Xt+1:t) and online learning (predict at t using Zt). Theyprove the existence of a learning rule that is consistent for every X that admits self-adaptive learning.If X is smooth, i.e., input marginals have a similar support over time, then ERM has a similarsample complexity as that of the IID setting . Haghtalab et al. give algorithmic guaranteesfor several online estimation problems in this setting. The true hypothesis in PL can change over time. This is different from the continual learning settingwhere we can find a common hypothesis for tasks at all time , and this is why our proofs workquite differently from existing ones in the literature. Instead of a hypothesis class H YX , we definethe notion of a hypothesis class that consists of sequences of predictors, i.e., subset of (YX )N; wecan do ERM in this new space. Instead of consistency of prediction as in Hanneke , we giveguarantees for strong learnability, i.e., convergence of the ERM risk to the Bayes risk.",
  "Also see Appendix A for a more elaborate discussion": "Information theory. There are also works that have sought to characterize classes of stochasticprocesses that can be predicted fruitfully. Bialek et al. defined a notion called predictiveinformation (closely related to the information bottleneck principle ) and showed how it is relatedto the degrees of freedom of the stochastic process. Shalizi and Crutchfield showed that acausal-state representation called an -machine is the minimal sufficient statistic for prediction.",
  "Theoretical foundations of prospective learning": "Definition 1 (Strong Prospective Learnability). A family of stochastic processes is stronglyprospectively learnable, if there exists a learner with the following property: there exists a time t(, )such that for any , > 0 and for any stochastic process Z from this family, the learner outputs ahypothesis h such that P [Rt(h) Rt < ] 1 , for any t > t. This definition is similar to the definition of strong learnability in PAC learning with one key difference.Prospective Bayes risk Rt depends upon the realization of the stochastic process zt up to time t.In PAC learning, it would only depend upon the true distribution of the data. Not all families ofstochastic processes are strongly prospectively learnable. We therefore also define weak learnabilitywith respect to a chance learner that predicts E[Y ] and achieves a prospective risk R0t .7 Definition 2 (Weak Prospective Learnability). A family of stochastic processes is weakly prospec-tively learnable, if there exists a learner with the following property: there exists an > 0 such thatfor any > 0, there exists a time t(, ) such that for any stochastic process Z from this family,",
  "PR0t Rt(h) > 1 , for any t > t": "In PAC learning for binary classification, strong and weak learnability are equivalent in thedistribution agnostic setting, i.e., when strong and weak learnability is defined as the ability of alearner to learn any data distribution. But even in PAC learning, if there are restrictions on the datadistribution, strong and weak learnability are not equivalent . This motivates Proposition 1 below.Before that, we define a time-agnostic empirical risk minimization (ERM)-based learner. In PAClearning, ERM selects a hypothesis that minimizes the empirical loss on the training data. It outputs atime-agnostic hypothesis, i.e., using data, say, zt standard ERM returns the same predictor for futuredata from any time t > t. There is a natural application of ERM to prospective learning problems,defined below.",
  "is called a time-agnostic empirical risk minimization (ERM)-based learner": "Time-agnostic ERM in prospective learning may use a time-dependent loss (s, hs(xs), ys) upon thetraining data. This ERM is not very different from standard ERM in PAC learning (when instantiatedwith the hypothesis class that consists of sequences of predictors, that we are interested here). Ifdata is IID (Scenario 1), then there is no information provided by time in the training samples. Butif there are temporal patterns in the data, take Scenarios 2 and 3 or Scenario 4 as examples, thentime-agnostic ERM as defined here will return predictors that are different than those of standardERM that uses a time-invariant loss. Proposition 1. There exist stochastic processes for which time-agnostic ERM is not a weak prospec-tive learner. There also exist stochastic processes for which time-agnostic ERM is a weak prospectivelearner but not a strong one.",
  "Prospective Empirical Risk Minimization (ERM)": "In PAC learning, the hypothesis returned by ERM using the training data can predict arbitrarily well(approximate the Bayes risk arbitrarily well with arbitrarily high probability), with a sufficiently largesample size. This statement holds if (a) there exists a hypothesis in the hypothesis class whose riskmatches the Bayes risk asymptotically, and (b) if risk on training data converges to that on the testdata sufficiently quickly and uniformly over the hypothesis class . Theorem 1 is an analogousresult for prospective learning.Theorem 1 (Prospective ERM is a strong prospective learner). Consider a finite family ofstochastic processes Z. If we have (a) consistency, i.e., there exists an increasing sequence ofhypothesis classes H1 H2 . . . with each Ht (YX )N such that Z Z,",
  "is a strong prospective learner for this family. We define prospective ERM as the learner thatimplements Eq. (8) given train data zt": "E.2 provides a proof, it builds upon the work of Hanneke . The first condition, Eq. (6),is analogous to the consistency condition in PAC learning. In simpler words, it states that the Bayesrisk can be approximated well using the chosen sequence of hypothesis classes {Ht}t=1. The secondcondition, Eq. (7), is analogous to concentration of measure in PAC learning, it requires that thelimsup in Eq. (1) is close to an empirical estimate of the limsup (the second term inside the absolutevalue in Eq. (7)). At each time t, prospective ERM in Eq. (8) selects the best hypothesis h Ht8 forfuture times t > t, that minimizes an empirical estimate of the limsup using the training data zt.Prospective ERM can exploit the difference between the latest datum in the training set with time tand the time for which it makes predictions t by selecting specific sequences inside the hypothesisclass Ht. For example, in Scenario 2 it can select sequences where alternating elements can be usedto predict on data from even and odd times.Remark 1 (How to implement prospective ERM?). An implementation of prospective ERM istherefore not much different than an implementation of standard ERM, except that there are twoinputs: time s and the datum xs. Suppose we use a hypothesis class where each predictor is a neuralnetwork, this could be a multi-layer perceptron or a convolutional neural network. The training setzt consists of inputs xs along with corresponding time instants s and outputs ys. To implementprospective ERM, we modify the network to take (s, xs) as input (using any encoding of time, wediscuss one in ) and train the network to predict the label ys. In Eq. (8) we can set uit t,doing so only changes the sample complexity. At inference time, this network is given the input(t, xt) to obtain the prediction yt. Note that if prospective ERM is implemented in this fashion, thelearner need not explicitly calculate the infinite sequence of predictors.9 Corollary 1. There exist stochastic processes for which time-agnostic ERM is not a strong prospectivelearner, but prospective ERM is a strong learner.Remark 2 (Why we need an increasing sequence of hypothesis classes H1 H2 . . . ). We couldhave chosen Ht = Ht for all t, t N to set up Theorem 1. But since the learner does not have a lot ofdata at early times, it should use a small hypothesis class. Just like PAC learning, the sequence (t)tNin Eq. (7) determines the convergence rate of a prospective learner. Therefore, using a monotonicallyincreasing sequence of hypothesis classes is useful to ensure a good sample complexity.",
  "for any stochastic process Z Z, where h H is a random variable in (Zt), then there exist Ht,ut, and t such that the two conditions of Theorem 1 are satisfied for this family": "E.3 provides a proof. This theorem provides a concrete example for which the assumptionsof Theorem 1 are satisfied. In PAC learning, one first proves uniform convergence for a finitehypothesis class. This can then be used to, say, calculate the sample complexity of ERM, or extendedto infinite hypothesis classes using constructions such as VC-dimension and covering numbers .The above theorem should be understood in the same spirit. It is a step towards characterizing thesample complexity of prospective learning. proves an analogue of Theorem 1 for prospective learning problems with discountedlosses. Appendix D provides illustrative examples of prospective ERM for periodic processes andhidden Markov processes. For periodic processes, we can also calculate the sample complexity.",
  "Experimental Validation": "This section demonstrates that we can implement prospective ERM on prospective learning prob-lems constructed on synthetic data, MNIST and CIFAR-10. In practice, prospective ERM mayapproximately achieve the guarantees of Theorem 1. We will focus on the distribution changing,independently or not (Scenarios 2 and 3). Recall that Scenario 1 is the same as the IID setting used instandard supervised learning problems. Scenario 4 is more involved (see an example in Appendix B.3)and, therefore, we leave more elaborate experiments for future work. We discuss experiments thatcheck whether large language models can do prospective learning in Appendix H. Learners and hypothesis classes.Task-agnostic online continual learning methods are the closestalgorithms in the literature that can address situations when data evolves over time. We use thefollowing three methods. (i) Follow-the-Leader minimizes the empirical risk calculated on all past data and is a no-regretalgorithm . We note that while this is a popular online learning algorithm, we do notimplement the algorithm in an online fashion.",
  "(ii) Online SGD fine-tunes the network using new data in an online fashion. At every time-step,weights of the network are updated once using the last eight samples": "(iii) Bayesian gradient descent is an online continual learning algorithm designed toaddress situations where the identity of the task is not known during both training andtesting, i.e., it implements continual learning without knowledge of task boundaries. These three methods are not explicitly designed for prospective learning but they are designed toaddress the changing data distribution t.10 We calculate the prospective risk of the predictor returnedby these methods; note that they do not output a time-varying predictor and consequently, thesemethods output a time-agnostic hypothesis. As a result, when we evaluate the prospective risk ofthese methods, we use the same hypothesis for all future time. For all three methods, we use amulti-layer perceptron (MLP) for synthetic data and MNIST, and a convolutional neural network(CNN) for CIFAR-10. For prospective ERM the sequence of predictors is built by incorporating time as an additional inputto an MLP or CNN as follows. For frequencies i = /i for i = 1, . . . , d/2, we obtain a d-dimensionalembedding of time t as (t) = (sin(1t), . . . , sin(d/2t), cos(1t), . . . , cos(d/2t)). This is similar tothe position encoding in Vaswani et al. . A predictor ht() uses a neural network that takes asinput, an embedding of time (s), and the input xs to predict the output ys for any time s N. Usingsuch an embedding of time is useful in prospective learning because, then, one need not explicitlymaintain the infinite sequence of predictors h (h1, . . . , ). 10There are many algorithms in the existing literature that the reader may think of as reasonable baselines. Wehave chosen a representative and relevant set here, rather than an exhaustive one. For example, online meta-learning approaches are close to online-SGD; since the learner fine-tunes on the most recent data. Algorithmsin the literature on time-series (i) focus on predicting future data, say, Yt given past data yt without takingcovariates Xt or some exogenous variables Xt into account, (ii) can usually only make predictions for apre-specified future context window , and (iii) work for low-dimensional signals (unlike images). Training setup.We use the zero-one error 1{y = y} to calculate prospective risk for all problems;all learners are trained using a standard surrogate of this objective, the cross-entropy loss. For allexperiments, for each time t, we calculate the prospective risk Rt(h) in Eq. (2) of the hypothesiscreated by these learners for a particular realization of the stochastic process zt. For each prospectivelearning problem, we generate a sequence of 50,000 samples. Learners are trained on data from thefirst t time steps (zt) and prospective risk is computed using samples from the remaining time steps.Except for online SGD and Bayesian gradient descent, learners corresponding to different times aretrained completely independently. See Appendix F for more details. Remark 3 (Why we do not use existing benchmark continual learning scenarios). The tasksconstructed below resemble continual learning benchmark scenarios such as Split-MNIST or Split-CIFAR10 where data from different distributions are shown sequentially to the learner. However,there are three major differences. First, in these existing benchmark scenarios, data distributionsdo not evolve in a predictable fashion, and prospective learning would not be meaningful. Second,existing scenarios consider a fixed time horizon. We are keen on calculating the prospective risk formuch longer horizons whereby the differences between different learners are easier to discern; ourexperiments go for as large as 30,000 time steps. Third, our tasks have the property that the Bayesoptimal predictor changes over time.",
  "We create tasks using synthetic data, MNIST and CIFAR-10 datasets to design prospective learningproblems when data are independent but not identically distributed across time (Scenario 2)": "Dataset and Tasks.For the synthetic data, we consider two binary classification problems (tasks)where the input is one-dimensional. Inputs for both tasks are drawn from a uniform distributionon the set . Ground-truth labels correspond to the sign of the input for Task 1, andthe negative of the sign of the input for Task 2. For MNIST and CIFAR-10 we consider 4 taskscorresponding to data from classes 1-5, 4-7, 6-9 and 8-10 in the original dataset, i.e., the first taskconsiders classes 1-5 labelled 1-5 respectively, the second task considers classes 4-7 labelled 1-4, thethird task considers classes 6-9 labeled 1-4 and the last task considers labels 8-10 labelled 1-3. Inother words, images from class 1 in task 1, class 4 from task 2 and class 6 from task 3 are all assignedthe label 1. For the prospective learning problem based on synthetic data, the task switches every20 time steps. For MNIST and CIFAR-10, the data distribution cycles through the 4 tasks, and thedistribution of data changes every 10 time-steps. For more details, see Appendix F. Task A Task B",
  "Prospective Risk": "Synthetic Scenario 3 Time (t) 0.0 0.2 0.4 0.6 0.8 1.0MNIST Scenario 3 Time (t) 0.0 0.2 0.4 0.6 0.8 1.0CIFAR Scenario 3 Follow-the-leaderProspectiveBayes Risk Figure A.9: For a task defined on a stationary Markov process, the Bayes risk is trivial and can beachieved by a hypothesis that doesnt change over time. We plot the prospective risk across 5 random seeds(which govern the sequence of samples and the weight initialization of the neural networks). In all three cases,both follow-the-leader and prospective ERM approach the Bayes risk. The stationary distribution has an equalprobability of seeing either task and a fixed hypothesis can achieve Bayes risk on this problem. Time (t) 0.0 0.2 0.4 0.6 0.8 1.0 Discounted prospective Risk Synthetic Scenario 3 Time (t) 0.0 0.2 0.4 0.6 0.8 1.0MNIST Scenario 3 Time (t) 0.0 0.2 0.4 0.6 0.8 1.0CIFAR Scenario 3 Follow-the-leaderProspectiveBayes Risk Figure A.10: Both prospective ERM and follow-the-leader achieve similar discounted prospective risks(with discount factor 0.95). We plot the discounted prospective risk across 5 random seeds. Both follow-the-leader and prospective ERM achieve similar discounted risks. Note that the error bars are larger since the risk iscomputed over fewer samples, i.e., the discount factor reduces the effective number of test data points.",
  "Class 1": ": Left: For MNIST and CIFAR-10, we consider 4 tasks corresponding to the classes 1-5, 4-7, 6-9and 8-10. Using these tasks, we construct Scenario 3 problems corresponding to a stochastic process which isa hierarchical hidden Markov model. After every 10 time-steps, a different Markov chain governs transitionsamong tasks (one Markov chain for tasks 1 and 2, and another for tasks 3 and 4). This ensures that thestochastic process does not have a stationary distribution. Right: For synthetic data, the 4 tasks are createdusing two-dimensional input data as shown pictorially above. The four parts of the input domain are {(x1, x2) :1 x1, x2 2}, {(x1, x2) : 1 x1 2, and 2 x2 1}, {(x1, x2) : 2 x1, x2 1} and{(x1, x2) : 2 x1 1 and 1 x2 2}. Colors indicate classes. The hierarchical hidden Markov modelfor transitions among the tasks is identical to the MNSIT and CIFAR-10 setting shown on the left. Time (t) 0.0 0.2 0.4 0.6 0.8 1.0",
  "transitions among tasks (one Markov chain for tasks 1 and 2, and another for tasks 3 and 4, as shownin ). These choices ensure that the stochastic process does not have a stationary distribution.12": "As shows, prospective ERM can prospectively learn problems when data is both in-dependent and not identically distributed (Scenario 3. Stochastic processes in these problemscorresponding to Scenario 3 do not have a stationary distribution. This is why a time-agnostichypothesis (Follow-the-Leader) does not achieve a good prospective risk, unlike prospective ERM.Appendix G discusses additional experiments for Scenario 3 for different kinds of Markov chains.",
  "Discussion": "Prospective learning, as we see it, is a paradigm of learning that characterizes many real-worldscenarios which are currently modeled using much stronger (and less accurate) assumptions. Thesesimplifying assumptions have certainly enabled progress in machine learning. But systems deployedbuilt upon these approaches have proven to be extremely fragile in certain real-world settings. Todaysmachine learning-based systems fail to track changes in the data. They certainly do not model howbiological organisms learn robustly and effectively over time. We believe characterizing which kindsof stochastic processes are prospectively learnable under which kinds of time-sensitive loss functionswill be an important next theoretical step. Developing algorithms, from the perspective of prospectivelearning, which have theoretical guarantees in practice, will be another next step. Finally, buildingalgorithms that scale and can therefore be deployed in real-world systems, will be important todemonstrate the utility of this approach. The precise real-world applications in which prospectivelearning based methods will outperform PAC learning, remains to be seen.",
  "M Kearns and L Valiant. Cryptographic limitations on learning Boolean formulae and finiteautomata. Journal of the ACM, 1994": "Gbor Lugosi and K Zeger. Nonparametric estimation via empirical risk minimization. IEEEtransactions on information theory / Professional Technical Group on Information Theory, 41(3):677687, May 1995. doi: 10.1109/18.382014. Yann LeCun, Bernhard E Boser, John S Denker, Donnie Henderson, Richard E Howard,Wayne E Hubbard, and Lawrence D Jackel.Handwritten digit recognition with a back-propagation network. In Advances in Neural Information Processing Systems, pages 396404,1990.",
  "Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, April2009": "Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adap-tation of deep networks. In Proceedings of the 34th International Conference on MachineLearning-Volume 70, pages 11261135, 2017. Joshua T Vogelstein, Jayanta Dey, Hayden S Helm, Will LeVine, Ronak D Mehta, Tyler MTomita, Haoyin Xu, Ali Geisa, Qingyang Wang, Gido M van de Ven, Chenyu Gao, WeiweiYang, Bryan Tower, Jonathan Larson, Christopher M White, and Carey E Priebe. A simplelifelong learning approach. arXiv [cs.AI], April 2020.",
  "Chen Zeno, Itay Golan, Elad Hoffer, and Daniel Soudry. Task agnostic continual learning usingonline variational bayes. arXiv preprint arXiv:1803.10123, 2018": "Bryan Lim, Sercan Ark, Nicolas Loeff, and Tomas Pfister. Temporal Fusion Transformersfor interpretable multi-horizon time series forecasting. International journal of forecasting, 37(4):17481764, October 2021. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Informa-tion Processing Systems, 2017.",
  "Leslie Valiant. Probably Approximately Correct: Natures Algorithms for Learning and Pros-pering in a Complex World. Basic Books, June 2013. ISBN 9780465032716": "Masashi Sugiyama, Matthias Krauledat, and Klaus-Robert Mller. Covariate Shift Adaptationby Importance Weighted Cross Validation. Journal of machine learning research: JMLR, 8(May):9851005, 2007. ISSN 1532-4435,1533-7928. URL Ashwin De Silva, Rahul Ramesh, Carey Priebe, Pratik Chaudhari, and Joshua T Vogelstein. Thevalue of out-of-distribution data. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, BarbaraEngelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th InternationalConference on Machine Learning, volume 202 of Proceedings of Machine Learning Research,pages 73667389. proceedings.mlr.press, 2023. URL Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-Agnostic Meta-Learning for FastAdaptation of Deep Networks. In Doina Precup and Yee Whye Teh, editors, Proceedings of the34th International Conference on Machine Learning, volume 70 of Proceedings of MachineLearning Research, pages 11261135, International Convention Centre, Sydney, Australia,2017. PMLR.",
  "Mehryar Mohri. Foundations of machine learning, 2018": "Fotios Petropoulos, Daniele Apiletti, Vassilios Assimakopoulos, Mohamed Zied Babai, De-von K Barrow, Souhaib Ben Taieb, Christoph Bergmeir, Ricardo J Bessa, Jakub Bijak, John EBoylan, Jethro Browell, Claudio Carnevale, Jennifer L Castle, Pasquale Cirillo, Michael PClements, Clara Cordeiro, Fernando Luiz Cyrino Oliveira, Shari De Baets, Alexander Doku- mentov, Joanne Ellison, Piotr Fiszeder, Philip Hans Franses, David T Frazier, Michael Gilliland,M Sinan Gnl, Paul Goodwin, Luigi Grossi, Yael Grushka-Cockayne, Mariangela Guidolin,Massimo Guidolin, Ulrich Gunter, Xiaojia Guo, Renato Guseo, Nigel Harvey, David F Hendry,Ross Hollyman, Tim Januschowski, Jooyoung Jeon, Victor Richmond R Jose, Yanfei Kang,Anne B Koehler, Stephan Kolassa, Nikolaos Kourentzes, Sonia Leva, Feng Li, KonstantiaLitsiou, Spyros Makridakis, Gael M Martin, Andrew B Martinez, Sheik Meeran, TheodoreModis, Konstantinos Nikolopoulos, Dilek nkal, Alessia Paccagnini, Anastasios Panagiotelis,Ioannis Panapakidis, Jose M Pava, Manuela Pedio, Diego J Pedregal, Pierre Pinson, Pa-trcia Ramos, David E Rapach, J James Reade, Bahman Rostami-Tabar, Michal Rubaszek,Georgios Sermpinis, Han Lin Shang, Evangelos Spiliotis, Aris A Syntetos, Priyanga DiliniTalagala, Thiyanga S Talagala, Len Tashman, Dimitrios Thomakos, Thordis Thorarinsdottir,Ezio Todini, Juan Ramn Trapero Arenas, Xiaoqian Wang, Robert L Winkler, Alisa Yusupova,and Florian Ziel. Forecasting: theory and practice. International journal of forecasting,38(3):705871, July 2022. ISSN 0169-2070. doi: 10.1016/j.ijforecast.2021.11.001. URL",
  "T Chen, Yulia Rubanova, J Bettencourt, and D Duvenaud. Neural ordinary differential equations.Neural Information Processing Systems, 31:65726583, June 2018": "Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. Are Transformers effective for time seriesforecasting? Proceedings of the ... AAAI Conference on Artificial Intelligence. AAAI Conferenceon Artificial Intelligence, 37(9):1112111128, June 2023. Alexander L Strehl, Lihong Li, and Michael L Littman. Reinforcement learning in finiteMDPs: PAC analysis. Journal of machine learning research: JMLR, 10(84):24132444,2009. ISSN 1532-4435,1533-7928. URL",
  "Steve Hanneke. Learning whenever learning is possible: Universal learning under generalstochastic processes. The Journal of Machine Learning Research, 22(1):57515866, 2021": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Openfoundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, ShreyaPathak, Laurent Sifre, Morgane Rivire, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Openmodels based on gemini research and technology. arXiv preprint arXiv:2403.08295, 2024.",
  "Yue Deng, Wenxuan Zhang, Sinno Jialin Pan, and Lidong Bing. Multilingual jailbreak chal-lenges in large language models. arXiv preprint arXiv:2310.06474, 2023": "Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin,David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Showyour work: Scratchpads for intermediate computation with language models. arXiv preprintarXiv:2112.00114, 2021. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le,Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models.Advances in neural information processing systems, 35:2482424837, 2022.",
  "Acknowledgement": "This work was funded by grants from the National Science Foundation (IIS-2145164, CCF-2212519)and the Office of Naval Research (ONR) N00014-22-1-2255. RR was funded by a fellowshipfrom AWS AI to Penn Engineerings ASSET Center for Trustworthy AI. ADS was supported by afellowship from the Mathematical Institute for Data Science (MINDS) at JHU. We are grateful to allthose who provided helpful feedback on earlier drafts of this work, including Marlos Machado.",
  "AIsnt this just": "When we describe prospective learning to people the first time, they often wonder how it is differentboth conceptually and formallyfrom other previously established learning frameworks. In fact, formany of them, the English language descriptions are seemingly identical to those which describeprospective learning. However, the English language is often imprecise and this has created a lot ofconfusion among both theoreticians and practitioners as to the precise differences, potential benefitsand pitfalls, between different learning frameworks. Here, we provide detailed formal distinctionsbetween prospective learning and other related learning frameworks. Table A.1 summarizes the keydistinctions between several machine learning frameworks, with further details provided below. Thekey difference between prospective learning and all other learning frameworks mentioned below isthat in prospective learning, the hypothesis can make an inference (or take an action) arbitrarily far inthe future. Certain versions of forecasting also have that property (probabilistic forecasting ), butforecasting has several other distinctions. Table A.1: Comparison of different machine learning frameworks in terms of the distributional assumptionson the data. Task IID indicates that data within a task are IID, and that tasks are IID from some meta-distribution.Loss characterizes whether the assumed loss function is instantaneous or time-varying. Optimal hypothesisindicates the total possible number of different optimal hypotheses (assuming each hypothesis has a unique risk).Data availability indicates whether the data are available all at once (in batch), or after each task arrives (tasksequential), or one data sample at a time (sequential). The answers are given for typical settings, further detailsare available in the paragraphs below.",
  "A.1...PAC learning?": "PAC learning is a special case of prospective learning when the stochastic process is time-invariant (meaning the data are IID) and the loss is fixed. Also, it is only concerned with batch data.It is an interesting question as to whether prospective learning as we have defined here is useful forIID data. We do not know yet in general. In Appendix B.1, we provide a simple example whereprospection turns out to be beneficial, even in the IID setting. More broadly, we wonder whether theviewpoint proposed in this paper might lead to novel algorithms for solving learning problems on IIDdata that do not have closed form solutions.",
  "A.2...transfer learning?": "In transfer learning , including domain (covariate) shift (adaptation) , and out-of-distribution (OOD) learning, there are two distributions, a source and a target distribution;thus, the distribution changes only once, rather than potentially once per time step. Depending onwhether the goal is to perform well only on the target, or both the source and the target, there are",
  "A.3...meta-learning?": "Meta-learning is similar to multi-task learning , and includes as special cases zero-shot and few-shot learning . Here, the data are Task IID, meaning that the distributionwithin a task is IID, and the distribution of tasks themselves is also IID, rendering it impossible topredict future distributions very well (the best one could do is guess the next task is whichever taskis most likely). Typically, that the task/distribution changes is known, but not always. In classicalmeta-learning, data are available in one batch, but in online meta-learning, data are sequentiallyavailable . The goal is to perform well on the next (unknown) distribution, as opposed to allfuture (unknown) distributions as in prospective learning.",
  "A.4...lifelong learning?": "Lifelong (continual) learning is nearly identical to online meta-learning . However,the data are typically available in one batch per task. The goal is also a bit different, rather thanperforming well on the next distribution, in lifelong learning, the goal is also to continue performingwell on previous distributions. Often, the learner is aware that the distribution shifted , but notalways .",
  "A.6...forecasting?": "Forecasting , time-series or sequential analysis assume the data follow a stochasticprocess, much like prospective learning often does (e.g., Scenarios 2 and 3), and therefore, thenumber of optimal hypothesis can be equal to the number of time steps. However, in forecasting, theloss is associated with a fixed (pre-specified) horizon, or several horizons . Forecasting also oftenassumes a parametric model, but not always . Probabilistic forecasting can also predictarbitrarily far in the future by iteratively updating its probabilistic forecasts. However, this is prone tonumerical errors, as evidenced in sequential Monte Carlo.",
  "A.7...reinforcement learning?": "Reinforcement learning (RL) is only concerned with situations where there is a control el-ement, that is, the hypothesis chooses an action (which potentially impacts future distributions),rather than merely an inference (which does not). Thus, it excludes Scenarios 2 and 3. Moreover,RL theory focuses on Markov Decision Processes , whereas PL operates on larger classes ofstochastic decision processes, including non-Markov processes (e.g., see examples of non-Markovstochastic processes in Scenario 3). Depending on context, PL also considers loss functions thatare instantaneous. Classical RL assumes data are sequentially available, yet offline RL operates inbatch mode . Perhaps most importantly, in classical RL, the optimal hypothesis (policy) is nottime-varying, though recent generalizations are forthcoming . Also, in RL, there are typicallymany episodes, whereas in prospective learning there is only a single episode (though single-episodeRL is also forthcoming ). To elaborate on the first point above, assume that our decisions do not impact the future, but theoptimal hypothesis is time-dependent, that is ht = ht for some t = t. Why would we care about therisk for any t > t (like RL, but not like online/continual learning), given that our decision at time tdoes not impact Zt at all? We only ever incur the current loss, that is, (t, ht(xt), yt). So, it wouldseem that as long as we minimize this current loss, there is no reason to care about any future loss.First note that minimizing the expected cumulative future loss is sufficient for minimizing the lossaveraged over a finite future horizon, this is formally shown in Corollary 2. But more importantly,these two problem settings are rather different. Minimizing the prospective risk (expected cumulativefuture loss) forces the learner to learn/model all the different modes of variation in the data. Missing even a small (low energy) mode of variation can lead to large prospective risk. If the learner onlyseeks to minimize the current loss, it need not have any representation of how data evolves over time.It will not be a good prospective learner. Recall that online learners (which minimize the current loss)have large worst case regret. Prospective learning effectively evaluates the regret over the infinitefuture horizon. The two settings are closely related if data evolves slowly, as argued in Fakoor et al..",
  "A.8...recurrent neural networks?": "Recurrent neural networks (RNNs), including Long Short Term Memory (LSTM) networks , aswell as echo state machines and liquid state machines (and other reservoir computing techniques ),and Gaussian Processes seem like they are solving prospective learning problems. Indeed, theyare all reasonable architectures for satisfying the conditions of Theorem 1. Insofar as they do satisfythose conditions, then they are indeed prospective learners.",
  "tts=1 ys is the maximum likelihood estimator (MLE). Alternatively, ifwe assume a prior distribution Beta(, ) over p, then the resulting maximum a posteriori (MAP)estimate is pt = +ts=1 zs1": "++t2. We define a second prospective learner based on MAP that returnsthe sequence ht = (ht, ht, ), where ht = threshold(pt > 0.5) for all future times beyond t. Ifthe prior distribution has a small divergence with respect to the true posterior distribution, then thesecond learner converges faster to the Bayes optimal risk; for a poor choice of prior, the convergenceis slower. However, in such situations, we show that we can modify the MAP-based learner to useprospection and incorporate time to result in faster convergence to the Bayes risk.",
  "s=tp(s)": "We refer to this as the prospective MAP estimate. Based on it, we set the hypothesis to be ht =threshold(pt > 0.5) for all future times beyond t. In Figure A.1, we plot the prospective risk the MLE,MAP, and prospective MAP-based learners. Due to an unfavorable prior, the MAP-based learnerconverges slowly. However, prospective MAP-based learner manages to leverage its forecasting toachieve a faster convergence rate despite having the same prior as the MAP. This shows that we canindeed benefit from prospection even in the IID case.",
  "The probability distribution at time t is given by tt(zt, 1 zt)T . The eigenvalues of the transition": "Time (t) 0.0 0.2 0.4 0.6 0.8 1.0 Prospective risk Scenario 1 Independent and identically distributed data MLEMAP (prior = 12, = 16) proMAP (prior = 12, = 16)Bayes risk Figure A.1: Prospective risk of MLE (blue), MAP (purple), and prospective MAP (red) based learners withrespect to time. Both MAP and prospective MAP estimators assume a prior distribution of Beta(12, 16) over p.",
  "B.3Prospective learning in Scenario 4 when the future depends upon the current prediction": "There are two types of prospective learnersone that passively observes the environment and makesinferences and another that acts on the environment and influences it. Scenario 1, Scenario 2,Scenario 3 fall into the first category which is the primary emphasis of our paper. Scenario 4 presentsa prospective learning problem where the learner can influence the future realizations of the stochasticprocess through its decisions. Our prospective learner is inspired from reinforcement learning, where the current state is Yt1, the ac-tion is ht and the next state is Yt. The reward at the tth time-step is r(t, ht+1, yt+1) = 1 {ht+1 = yt+1}as a result of selecting action ht+1 given that the previous output was yt, and next outputyt+1. The learner estimates the transition matrix corresponding to the MDP for each decisionht+1(Xt+1) {0, 1} using a similar procedure as that of Scenario 3,",
  "r(t, ht+1, y) + maxhQt(y, h)": "The value function can be solved using value iteration, iteratively until convergence. For a given ,Banachs fixed point theorem guarantees this procedure will converge to the optimal value functionin the tabular setting . Once we have the Q-values Qt, we can use it to take actions. The optimalaction at time t is arg maxh Q(yt, h). However, unlike reinforcement learning, we do not know ytfor t > t and we must instead make a sequence of decisions using state yt. The sequence of decisionmade by the learner is h>t = (ht+1, . . . ) where",
  "CProspective ERM for discounted losses": "Like we discussed in Scenario 3, in order to prospect meaningfully for some stochastic processes, wemight need to consider a discounted future loss, e.g., the one in Eq. (4). Theorem 1 was proved onlyfor the averaged future loss in Eq. (1). Here, we sketch out the proof of an analogous theorem for thediscounted loss. Let",
  "t(h, Z; ) = lim sup ()t(h, Z; ())": "where denotes the collection {()}=1. We can define Rt(h; ) and Rt () for this discounted lossusing similar expressions as those in Eqs. (2) and (3). For clarity, let us use the notation Rt(h, 1/) Rt(h) and Rt (1/) Rt for the prospective risk and prospective Bayes risks corresponding to theaveraged loss corresponding to ()s= 1/.",
  "Rt(h; ) Rt () cRt(h, 1/) Rt (1/)t N, h Ht(12)": "i.e., if gap in the risk for the discounted loss is dominated uniformly by the gap in the risks for theaveraged loss (over all realizations of the stochastic process), then prospective ERM implementedin Eq. (8) (implemented with the averaged loss) is a strong prospective learner, i.e., its discountedrisk Rt(h, ) converges to the discounted Bayes risk Rt ().",
  "DAn illustrative example of prospective ERM": "Suppose we have a stochastic process such that Zt p(t mod T ) for some known period T, i.e.,data is independent across time but not identically distributed, and the loss function (t, , ) is time-invariant. Scenario 2 is a special case with T = 2. Assume that we can find a countable hypothesisclass G that contains the Bayes plug-in estimator for each pt with t {1, . . . , T}. Then HT ={h : ht+T = ht and ht G t} satisfies Eq. (9), and it is also countable. This implies consistency anduniform concentration of the limsup for sequences in some sub-classes {Ht}t=1 that expands to HT .Note that even if we do not know the period, we can still implement prospective ERM using thehypothesis class T NHT ; this is a countable set. Prospective ERM is therefore a strong prospectivelearner if the period T is bounded.Remark 4 (Implementing prospective ERM for periodic processes). If G has a finite VC-dimension, choosing Ht = HT for any t > T as the increasing sequence of hypothesis classesin Theorem 1 guarantees the existence of limm 1",
  "s=1(s, hs(xs), ys)": "in Eq. (8). For Ht = HT selected above for the periodic process, this is identical to Eq. (5). Inother words, implementing prospective ERM for a periodic process boils down to solving T differenttime-agnostic ERM problems, each using data {zsT +k}s=0, k {1, ..., T}. Observe that this isprecisely the prospective learner we used for the example in Scenario 2 and .Remark 5 (Sample complexity of prospective ERM for a periodic process). We can calculate thesample complexity by exploiting the relatedness of the different distributions in the periodic process.First assume t > T, i.e., at least one sample from each distribution is available. We again pickHt = HT for all t > T. Let us assume that ht G for all times t. Let C C(/16, GT ) denote the covering number of a hypothesis class of T-length sequences of hypotheses GT = {(h, . . . , h) : h G}using balls of radius /16 with respect to loss . Then, using Baxter [84, Theorem 4] we can showthat, if",
  "ERt(h) limt EinfhHTRt(h)+ 2,": "with probability at least 1 . The sample complexity in Eq. (13) is dominated by the first term inthe curly brackets; Baxter [84, Lemma 5] shows that C(, GT ) (C(, G))T . Sample complexity ofprospective ERM grows at most linearly with the period T, as one would expect. Remark 6 (Exact sample complexity for a periodic binary classification with one-dimensionalGaussian inputs). Let the period of the stochastic process be T = 2 with inputs Xt R and outputsYt {1, 1}. Suppose Yt Bernoulli(0.5). The distribution P(Xt | Yt = y) is a Gaussian withmean y + (t mod T) and variance 2. In words, for even times t, the mean of the Gaussians areshifted to the right by . Consider the time-invariant squared error loss (s, y, y) = (y y)2. ChooseG = {1A : A {(, c), (c, ) : c R}} to be the set of predictors for Fishers linear discriminant(FLD); the prospective learner selects each element of its hypothesis from G. The calculations inDe Silva et al. for FLD can be used to show that if Ht = {(h, h, . . . ) : h G} for all times t, thena time-agnostic ERM has risk",
  "t1, t2 N : if ht1+s = ht2+s s {1 . . . , k} , then ht1+k+1 = ht2+k+1": "This is the hypothesis class that contains sequences of predictors that depend only on the past kpredictors. If we assume, as above, that G is countable, then so is Hk. And it also satisfies Eq. (9)because of the kth-order Markov property. We can therefore implement prospective ERM using Hkas the hypothesis class. Observe that in the case when the Markov process underlying the HMMis deterministic, our example models the output from an auto-regressive language model that usesgreedy decoding. The length of the context window is k, the hidden state of the HMM is the logit ateach step (the next hidden state is a deterministic function of the previous k ones), and the outputof the HMM Zt is the next token. Our theory therefore shows that the output of such a model isprospectively learnable if the learner has access to the sequence of tokens.",
  ",": "almost surely; here R1(h) and R2(h) are risks on data from distributions P1 and P2 at odd and eventimes, respectively. The last equation follows from the fact that R1(h) = 1 R2(h) because thelabels are flipped. Prospective Bayes risk is zero if the hypothesis class G contains the Bayes optimalhypotheses for each of the two distributions. The future loss evaluates to 1/2 for all realizationsand so does the prospective risk. The prospective risk of a hypothesis sequence that makes randompredictions (zero or one with equal probability at each instant) is also 1/2. This stochastic process isnot weakly prospective learnable.",
  "if x 0if x = 1": "Inputs are supported on the set {1, 0, 1} this time. Again consider a stochastic process Z such thatZ2t+1 P1 and Z2t P2 for t N. For a time-agnostic learner, since its hypothesis h at each timestep has to predict incorrectly at x = 0, we have R1(h) + R2(h) 1",
  "for any hypothesis. Prospective Bayesrisk is again zero and therefore this stochastic process is not strongly prospectively learnable. It ishowever weakly learnable": "A hypothesis that predicts y = 1 with equal probability has R0t = 0.5. If the data contains samplesfor x {1, 1}, ERM will select a hypothesis that minimizes the empirical risk which necessitatesthat h(1) = 0 and h(1) = 1. Therefore R1(h) + R2(h) 1 3 + , since h predicts correctly at x = 1,and incorrectly at x = 0 exactly one of the two distributions. The constant can be chosen to be t1/2 after receiving data from t timesteps. The probability with which we do not get samplesat x = 1 or at x = 1, is 2 3t. Therefore the probability that R1(h) + R2(h) 1",
  "Rjkt t": "Now choose h(t) = h(jkt) for every t N. Since jkt t, we have Hjkt Ht and (Zjkt ) (Zt).This implies that h(jkt) is an Ht-valued random variable and h(jkt) (Zt). Also, since ikt t and{ut}t=1 is non-decreasing, we have uikt ut for all t N. Hence, with probability one, t t0,",
  "F.1Training and evaluation": "Training setup.Each learner receives a t-length sequence of samples zt drawn from the stochasticprocess, as the training data. Upon training, the learner is expected to make predictions on futuresamples that correspond to times t > t up to a fixed horizon T. At each future time t, we do nottrain (modify the weights) using samples after time t (because we do not have them, but we willmake predictions on these samples). Given samples zt, a time-aware hypothesis class minimizes theempirical prospective risk",
  "For an MLP or CNN, hs corresponds to a network that takes time s as input": "Hyper-parametersAll the networks are trained using stochastic gradient descent (SGD) withNesterovs momentum and cosine-annealed learning rate. The networks are trained at a learning rateof 0.1 for the synthetic tasks, and learning rate of 0.01 for MNIST and CIFAR. The weight-decay is setto 1 105. The images from MNIST and CIFAR-10 are normalized to have mean 0.5 and standarddeviation 0.25. The models were trained for 100 epochs, which is many epochs after achieving atraining accuracy of 1. EvaluationWe estimate the prospective risk of each learner using a Monte Carlo estimate. Fora given training dataset zt, we estimate a sequence predictors h (ht) which we use to makepredictions on future samples. We wish to approximate the prospective risk (Equation (2)) for theestimated sequence of predictors. We do so, for a single future realization z>t of this process, whichyields the estimate",
  "s=t+1s, hs(xjs), yjs)": "In our experiments, T = 50,000 for CIFAR-10 and MNIST while T = 10,000 for the synthetic dataexperiments. For a single learning algorithm, we compute the empirical prospective risk at 15-40different time steps which results in a significant number of GPU hours in order to plot the learningcurves. For every time step, we compute the mean and standard deviation of the empirical prospectiverisk using 5 random seeds.",
  "We considered the following architecture choices for the time-agnostic restropective algorithms likeERM that ignore time and the ordering associated with the samples in zt": "Retrospective-MLP/CNN.A multi-layer perceptron (MLP) with two hidden layers with 256 unitsis used for the synthetic tasks and the MNIST task. For CIFAR-10, we use a a small convolutionalnetwork with 0.12M parameters. It comprises of 3 convolution layers (kernel size 3 and 80 filters)interleaved with max-pooling, ReLU, batch-norm layers, with a fully-connected classifier layer. Prospective ERM with MLP and CNNs.In order to incorporate time into the hypothesis class,we consider an embedding function : R Rd that takes raw time as an input and returns ad-dimensional vector denoted as the time-embedding. In our experiments, we define : R Rd as afunction that maps",
  "where, i = /i, i = 1, . . . , d/2 to the be the collection of angular frequencies. We briefly discuss therationale for this choice in Figure A.7. In our experiments, we use d = 50": "We make our classifiers a function of time by including time t as an input the neural network. Thisallows the network to vary its hypothesis over time. For MLPs, we concatenate the input with itscorresponding time-embedding (t) which is fed as input. For the CNN (see Figure A.5), we add thetime-embedding to the output of the convolutional layers instead of concatenating it to the inputs.We also tried concatenating the time-encoding to the inputs of the CNN but found that it performedpoorly in both scenarios 2 and 3 (see Figure A.6). Time (t) 0.0 0.2 0.4 0.6 0.8 1.0",
  "G.1Markov chain with periodic resets": "Dataset and Tasks.For synthetic data, we consider the 2 binary classification problems describedin .1. For CIFAR-10 and MNIST, we consider 2 tasks corresponding to the classes 1-5, andthe classes 1-5 but with each class y relabeled to (y + 1) (mod 5). Using these tasks, we constructScenario 3 problems corresponding to a stochastic process which is a hidden Markov model on twostates. The tasks are governed by a Markov process with transition matrix P(St+1 = k | St = k) = 0.1,where St is the task at time t. Additionally after every 10 time-steps, the state of the Markov chain isreset to the first task. This ensures that the stochastic process does not have a stationary distribution.Similar to the previous experiments, for each problem, we generate a sequence of 50,000 samples.Learners are trained on data from the first t time steps (zt) and prospective risk is computed usingsamples from the remaining time steps. Learners and hypothesis classes.For this scenario, we conduct experiments using follow-the-leader and prospective ERM. Both methods use MLPs for synthetic and MNIST tasks, and a CNNfor the CIFAR-10 task. Note that prospective ERM uses an embedding of time as input in addition tothe datum. Training and evaluation setup is identical to that of Scenario 2. Time (t) 0.0 0.2 0.4 0.6 0.8 1.0",
  "G.2Stationary Markov chain": "Dataset and Tasks.For synthetic data, we consider the 2 binary classification problems describedin .1. For CIFAR-10 and MNIST, we consider 2 tasks corresponding to the classes 1-5, andthe classes 1-5 but with each class y relabeled to (y + 1) mod 5. Using these tasks, we constructScenario 3 problems corresponding to a stochastic process which is a hidden Markov model on 2states. The tasks are governed by a Markov process with transition matrix P(St+1 = k | St = k) = 0.1,where St is the task at time t. Unlike the previous subsection (Figure A.8), in this experiment, theMarkov chain equilibriates to the stationary distribution. Similar to the previous experiments, foreach problem, we generate a sequence of 50,000 samples. Learners are trained on data from the firstt time steps (zt) and prospective risk is computed using samples from the remaining time steps. Learners and hypothesis classes.For this scenario, we conduct experiments using follow-the-leader and prospective ERM. Both methods use MLPs for the synthetic and MNIST tasks, and a CNNfor the CIFAR-10 task. Note that prospective ERM uses an embedding of time as input in addition",
  "HLarge language models may not be good prospective learners": "It is an interesting question whether LLMs which are trained using auto-regressive likelihoods withTransformer-based architectures can do prospective learning. To study this, we used LLama-7B and Gemma-7B to evaluate the prospective risk for Scenarios 1 to 3. The prompt contains a fewsamples from the stochastic process (sub-sequences of (Yt) consisting of 0s and 1s) and an Englishlanguage description of the family of stochastic processes that generated the samples. The LLM istasked with completing the prompt with the next 20 most likely sequence of samples. Selecting the appropriate prompt LLMs can be brittle and are known to generate different com-pletions depending on if the prompt was in English, Thai or Swahili . This makes it difficult toevaluate prospective learning in LLMs. Therefore, in our experiments, we do not describe prospectiverisk or other details about prospective learnability in the prompt. We simply describe the data generat-ing process and some samples from this process in the prompt and prompt the model to generate themost likely completion. The prompts are described in detail in Appendix H.1; we also experimentedwith a few variants of these prompts. Time (t) 0.0 0.1 0.2 0.3 0.4 Prospective risk Scenario 1 (p=0.9) Llama-7bGemma-7bProspectiveBayes risk Time (t) 0.0 0.2 0.4 0.6 Prospective risk Scenario 2 (p=0.9) Time (t) 0.0 0.2 0.4 0.6 0.8 Discounted prospective risk Scenario 3 (p=0.90) Figure A.11: The prospective risk of LLMs when evaluated on the three scenarios, when averaged over drawsof the training data. The LLM does not improve with more data unlike a prospective MLE-learner. This suggeststhat LLMs are incapable of prospection.",
  "Llama": "Figure A.12: We prompt LLMs to generate the outcomes of 10 Bernoulli trials with p = 0.75. We plot theprobability of generating token 1 over all possible sequences of 10 Bernoulli trials and find that the outcomes aregenerated with probabilities that range from 0 to 1 with an average of 0.5. Ideally, the token 1 should shouldalways be generated with p = 0.75, i.e., the LLMs cannot simulate outcomes of a Bernoulli distribution. We use greedy decoding to generate a sequence of tokens, i.e., the token with the highest probabilityis sampled at every step. We vary the number of time-steps in the prompt from 1 to 100 whichcorresponds to the amount of training data. For a particular value of time t, we generate 20 moretokens and compute (an estimate of) the prospective risk of this completion; this is the test data. Wereport the prospective risk computed on 100 different realizations of the stochastic process, i.e., eachpoint in Fig. A.11 is the prospective risk on the next 20 samples, averaged over 100 realizations ofthe training data. In Fig. A.11, we find that LLMs do not obtain better prospective risk with moresamples, i.e., Llama-7B and Gemma-7B do not seem to be doing prospective learning. It is quitesurprising that they do not achieve Bayes risk even on independent and identically distributed data.We note that these experiments do not definitively answer whether LLMs can learn prospectively. Can LLMs even generate outcomes of a sequence of Bernoulli trials?We prompted an LLM togenerate a sequence of 0s and 1s sampled from a Bernoulli distribution with probability p = 0.75. Wethen plot the probability of generating each 0 or 1, for all sequences of length 10 in Fig. A.12. Ideally,the strip plot would be concentrated around 0.25 for 0 and 0.75 for 1, i.e., 0s should be generatedwith frequency close to 0.25. However, we find this is not the case and LLMs seem incapable of evengenerating a sequence of Bernoulli trials. This provides some context to the results discussed above.LLMs do not seem to be doing prospective learning, but they cannot even sample from a Bernoullidistribution under these experimental conditions.13",
  "H.1Prompts for testing prospective learning in LLMs": "We use the following 3 prompts to generate a sequence of predictions using in LLama-7B andGemma-7B. We found that the LLMs always generated a sequence of 0s and 1s and we did not needto post-process the response or change how the tokens were sampled. We generate 20 samples usinggreedy decoding; the language models are executed with the weights in 16-bit precision. We tried a 13Responses of ChatGPT-turbo and GPT-4o were more verbose compared to those of Llama-7B and Gemma-7B. ChatGPT responded correctly to Scenario 1, perhaps as a result of using a scratchpad for generatingthe results of intermediate steps of the algorithm. But it did not achieve a small prospective risk for Scenarios 2and 3. Gemini and GPT-4o refused to give a complete response to Scenario 3 and only outlined the sequence ofsteps, albeit correctly.",
  ". Theory Assumptions and Proofs": "Question: For each theoretical result, does the paper provide the full set of assumptions anda complete (and correct) proof?Answer: [Yes]Justification: We have developed substantial new theoretical results in this paper. They arediscussed in . All mathematical claims, illustrative examples, and remarks comewith elaborate proofs, both in the main paper and the Appendix.",
  ". Experimental Result Reproducibility": "Question: Does the paper fully disclose all the information needed to reproduce the main ex-perimental results of the paper to the extent that it affects the main claims and/or conclusionsof the paper (regardless of whether the code and data are provided or not)?Answer: [Yes]Justification: We use standard visual recognition datasets along with illustrative exampleson synthetic tasks. All information to reproduce our experiments, e.g., datasets, networkarchitectures, and training procedures, is provided in Appendix F. All the code that was usedto conduct the experiments has been made public.",
  ". Experiment Statistical Significance": "Question: Does the paper report error bars suitably and correctly defined or other appropriateinformation about the statistical significance of the experiments?Answer: [Yes]Justification: For all the learning algorithms considered in every experiment in our paper,we compute empirical prospective and instantaneous risks over multiple random seeds, at15-40 different time steps. This makes our results extremely rigorous and reproducible. Allexperimental results in the paper come with error bars assuming a Normal distribution forthe relevant quantities.",
  ". Experiments Compute Resources": "Question: For each experiment, does the paper provide sufficient information on the com-puter resources (type of compute workers, memory, time of execution) needed to reproducethe experiments?Answer: [Yes]Justification: We have conducted experiments on this problem for close to 1.5 years. Inaddition to the experimental results that are discussed here, there a substantial number ofsecondary experiments, failed attempts, that are not reported. All the experiments in thispaper approximately takes 200 GPU hours to run.",
  ". Code Of Ethics": "Question: Does the research conducted in the paper conform, in every respect, with theNeurIPS Code of Ethics [Yes]Justification: We did not use human subjects or participants in these experiments. Allour datasets are public ones that are used very commonly in the published literature, orsynthetically generated. We do not anticipate societally harmful consequences of thisresearch.",
  ". Broader Impacts": "Question: Does the paper discuss both potential positive societal impacts and negativesocietal impacts of the work performed?Answer: [NA]Justification: This is foundational work and it is not tied to any particular application ordeployment. We do not anticipate negative social impact. Down the line, when this lineof research matures and it is deployed for making predictions on users, we anticipate a lotof positive social impacts, e.g., prospective learning may enable us to deploy ML modelswithout having to retrain them as data changes over time. This improves performancebecause it prevents the predictions from deteriorating, but it also leads to other benefits interms of economic and computational resources necessary to serve ML models.",
  ". Licenses for existing assets": "Question: Are the creators or original owners of assets (e.g., code, data, models), used inthe paper, properly credited and are the license and terms of use explicitly mentioned andproperly respected?Answer: [Yes]Justification: We only use existing public datasets (appropriately cited in the paper) andsynthetic data. No new assets are created from this paper.",
  "Answer: [NA]Justification: No crowdsourcing experiments or research with human subjects": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with HumanSubjectsQuestion: Does the paper describe potential risks incurred by study participants, whethersuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)approvals (or an equivalent approval/review based on the requirements of your country orinstitution) were obtained?Answer: [NA]Justification: No crowdsourcing experiments or research with human subjects."
}