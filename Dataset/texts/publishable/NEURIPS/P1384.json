{
  "Abstract": "In high energy physics, self-supervised learning (SSL) methods have the potentialto aid in the creation of machine learning models without the need for labeleddatasets for a variety of tasks, including those related to jetsnarrow sprays ofparticles produced by quarks and gluons in high energy particle collisions. Thisstudy introduces an approach to learning jet representations without hand-craftedaugmentations using a jet-based joint embedding predictive architecture (J-JEPA),which aims to predict various physical targets from an informative context. Asour method does not require hand-crafted augmentation like other common SSLtechniques, J-JEPA avoids introducing biases that could harm downstream tasks.Since different tasks generally require invariance under different augmentations,this training without hand-crafted augmentation enables versatile applications,offering a pathway toward a cross-task foundation model. We finetune the represen-tations learned by J-JEPA for jet tagging and benchmark them against task-specificrepresentations.",
  "Introduction": "To enable precision measurements of the standard model (SM) of particle physics and searches fornew physics at the CERN LHC, physicists often train machine learning (ML) models using detailed,labeled simulations of proton-proton collisions for a variety of tasks including triggering ,charged particle tracking, calorimetry , particle-flow reconstruction , and jet tagging andmass regression. These trained ML models are subsequently applied to real data. This paradigm iscalled supervised learning because it uses explicit labels derived from simulations, e.g., whether thesimulated event is signal or background. A significant drawback of this, however, is that the performance of ML models trained on simulationsmay not translate to real data, especially due to mismodeling in the former. In this paper, we applya generalized ML approach, in which models are first pretrained on large quantities of unlabeleddata, and subsequently adapted or finetuned using smaller quantities of labeled data for a specificdownstream task. In the pretraining stage, models are trained to learn generic representations of theinput features. Models can be pretrained on unlabeled data through SSL, where the model learns",
  "arXiv:2412.05333v1 [hep-ph] 5 Dec 2024": "meaningful representations by solving auxiliary tasks such as reconstructing missing data, predictingrelationships, or distinguishing augmented versions of the data. This approach leverages the dataitself to create target signals without relying on explicit labels, and hence forces the model to learnthe context of and correlations among elements within the data. In this paper, inspired by Ref. , we propose a novel pretraining approach called the jet-based jointembedding predictive architecture (J-JEPA). Given a jet, we recluster it into subjets, masking someas target subjets and defining others as context subjets. Then, we train a model to predict therepresentations of target subjets based on the representations of context subjets, using the positions ofthe target subjets as joint information. A more detailed explanation is given in . By design, J-JEPA is an augmentation-free method, meaning it does not require data augmentationsunder the assumption of some symmetry. Different downstream tasks often rely on unique symmetries,which can vary significantly. J-JEPA eliminates the needs to handcraft augmentations for eachdownstream task, making it more suitable for a general purpose cross-task foundation model. Related work includes the masked autoencoder (MAE) , using contrastive self-supervision forjet tagging , resimulation based SSL , masked particle modeling , generative pre-training , and dataset scaling . These studies have laid the groundwork for developingfoundation models tailored to the unique challenges of LHC physics, highlighting the potential ofvarious pretraining techniques. Our software is available at Ref. . The primary objective of this paper is to demonstrate that a J-JEPA model can learn useful represen-tations applicable to downstream tasks. Our paper is organized as follows. describes theJ-JEPA architecture along with the pretraining objective and training processes we employ. describes the chosen datasets. discusses our evaluation methods. The results are presentedin . Finally, provides a summary and outlook.",
  "J-JEPA": "The core recipe of J-JEPA is as follows: given some context subjets and the positions of the targetsubjets as joint information, the model learns to predict the representations of various target subjetswithin the same jet. First, a large radius jet, clustered using the anti-kT algorithm with radius parameter R = 0.8 , isreclustered into a variable number of smaller subjets with the CambridgeAachen algorithm with radius parameter R = 0.2, using the FastJet library and its Python bindings . For jetswith fewer than 20 subjets, we pad the remaining dimension with empty subjets for easier processing.Then, we randomly select a fixed number of subjets to use as target subjets, and use the rest as contextsubjets. After obtaining context and target subjets, we pass the full jet through both the context andthe target encoders, producing representations for each subjet within the jet. We then apply masks separately to the outputs of the context and target encoders to obtain represen-tations of context and target subjets, respectively. Since the goal of J-JEPA is to learn physicallymeaningful, high-level, and distinguishing jet representations, it is crucial that we mask the output ofthe encoders rather than the input, so that both encoders have access to the full semantic informationcontained in a jet. After obtaining representations for context and target subjets, a predictor takes inthe context subjet representations conditioned with positional information of the target subjets andoutputs the predicted representations of those target subjets. As shown in , the training objective is to minimize the difference between the output of thepredictor (predicted target representations) and that of the target encoder (actual target representa-tions) by minimizing the L2 loss. The parameters of the predictor and context encoder are learnedthrough gradient-based optimization, while the parameters of the target encoder are updated via anexponentially moving average (EMA) of the context-encoder parameters. As stated in , the useof EMA in target encoder has been shown to be crucial in preventing informational collapse. Weobserve that this principle holds true for J-JEPA as well. The key distinction between our approachand the MAE approach lies in the fact that our predictions are made within the representation space,and the fact that we only mask the outputs of encoders rather than the inputs.",
  "SEL": ": The J-JEPA architecture begins by splitting the large-radius jet (large black cone) intotarget subjets and context subjets. The context encoder and target encoder then separately generaterepresentations for the context subjets and the target subjets. Using the positions of the target subjetsas additional information (hints), the predictor takes the context representations and predicts therepresentations of the target subjets. Finally, the L2 loss function is used to compare the predictedtarget subjet representations with the encoded target subjet representations, minimizing the differencebetween them. computer vision and natural language processing methods, which encode the relative positions oftokens in the input sequence, we provide the momentum direction, in terms of the pseudorapidity and the azimuthal angle of the subjets relative to the jet, as additional information used to createspatial embeddings. Specifically, we use a learnable token added by the spatial embeddings of thetarget subjets. We process to sin(/2) so that the embeddings reflect the Cartesian rather than theangular distance. We also implement a novel embedding method that encodes the coordinates of thesubjets in the four-vector phase space (transverse momentum pT, , , and energy E) and provides ajoint four-vector information to the predictor. In this paper, we will show the training with spatialembeddings only. ArchitecturesWe employ transformer-based architectures, adapted from the standard visiontransformer (ViT) , to serve as the context encoder, target encoder, and predictor, referred toas subjet transformers (SjTs). The primary distinction between a ViT and SjT lies in the lattersuse of nonlinear embedding layers, which are designed to disregard padded particles in subjets andpromote a more robust and enriched subjet representation. We consider two methods for embeddingthe subjets. The first employs a multilayer perceptron (MLP) that takes the flattened array of thesubjets particles four-vectors as input, utilizing GELU activations and incorporating residualconnections between embedding layers. The second option uses an MLP to create embeddings foreach particle within the subjet, processes these particle embeddings through transformer encoderblocks, and finally aggregates them into a single subjet embedding using class attention blocks .We argue the latter is particularly well-suited for this task, as the multihead attention blocks caneffectively disregard the padded particles, and compare the two methods below. In addition to theembedding layers, the size of the hidden layers, the number of attention blocks, and the number ofheads in the context and target encoders follow the same convention as in Ref. . Similar to I-JEPA, our predictor is implemented as a smaller SjT attached with a linear dimension-expanding layer, designed to create an information bottleneck. This encourages the model to distillthe most valuable features from the context representations, facilitating the prediction of targetrepresentations from the context representations. MaskingWe implement a masking strategy inspired by the multi-block masking approach intro-duced in I-JEPA. For each jet, we randomly select 30% of subjets as targets, with the remaining 70%of subjets forming the context. A masking tensor specifies which subjets belong to the target andwhich to the context.",
  "Dataset and Experimental Setup": "DatasetsFor our experiments, we start with a small fraction of the JetClass dataset , consistingof 500 k top jets and 500 k QCD jets, for a total of 1 M jets used in pretraining. In our subsequentfinetuning, we explore two scenarios: one where we used the full3 Top Tagging dataset , torepresent situations where we have an abundance of labeled training samples; and one where we onlyuse 10% of the Top Tagging dataset, to represent situations where labeled training samples are muchmore limited. Experimental SetupOur experiments were performed on a single NVIDIA A100 GPU. Foroptimization, we utilize the AdamW optimizer with a learning rate of 103 and a weight decay of102. We employ a cosine learning rate schedule, with warmup over the first 10% of training steps.Training was performed over 80 epochs with a batch size of 64.",
  "Evaluation Methods": "To evaluate the usefulness of the representations learned through J-JEPA during pretraining, wecompare the finetuning performance of our pretrained model against that of a model with the samearchitecture trained from scratch. For the classification head, we only append a single linear layer tothe target encoder. During finetuning, parameters of both the encoder and the newly added linearlayer are subject to change during training. We use two evaluation metrics: accuracy, defined as thenumber of correct jet classifications divided by the total number of jets, and background rejectioncorresponding to a signal efficiency of 50%, 1/B(S = 0.5).",
  "Results": "We present our finetuning results compared to the baseline of training a model from scratch witheither 10% or 100% of the Top Tagging dataset in . We evaluate two models, SjT-T, whichuses the traditional MLP-based embeddings, and AE-SjT-T, which uses our custom attention-basedembeddings. For each model, we utilize two methods for aggregating subjet representations into jetrepresentations: Flatten, where we simply flatten the subjet representations; and Cls Attn, wherewe use two class attention blocks to aggregate subjet representations. The standard deviations arecalculated from 5 identical trials with random initialization. We observe that finetuning a pretrainedmodel outperforms training from scratch, especially for smaller dataset sizes. : Accuracy and background rejection 1/B(S = 0.5) metrics for the different models andaggregation methods. Baseline indicates training from scratch, while Finetuned indicates finetuning apretrained model. 10% means finetuning/training with 10% of the Top Tagging dataset (120 K jetsfor training, 12 K jets for validation), and Full means finetuning/training with the full Top Taggingdataset 3. The best performing model per metric and dataset size is highlighted in bold.",
  "/B(S = 0.5)": "SjT-TFlatten40.50 1.2670.70 1.4653.67 9.9790.06 3.80SjT-TCls Attn52.56 1.5479.75 5.1261.32 0.6691.51 1.20AE-SjT-TFlatten67.34 1.4097.79 3.9070.47 1.0997.52 1.71AE-SjT-TCls Attn67.19 1.5499.38 2.8068.25 1.6495.47 1.83 shows the rejection power metric 1/B(S = 0.5) as a function of the number of labeledtraining samples used for finetuning under a number of different scenarios. The shaded bandsrepresent standard deviations calculated from 5 identical trials with random initialization. From",
  "We only kept jets with more than 10 valid subjets, leaving us with 785,767 out of 1.2 M jets": "(top), we can see that our J-JEPA method improves the downstream performance of themodel compared with models trained from scratch, where the gain is more significant when labeledtraining samples are limited. (bottom left) shows that for SjT-T, which uses MLP-basedembeddings, using class attention blocks to aggregate subjet representations improves the downstreamperformance compared with simply flattening. Finally, (bottom right), shows that our customattention-based embeddings offer a significant improvement in downstream performance comparedwith the traditional MLP-based embeddings. Number of Labeled Training Samples",
  "Rejection Power": "Attention Based EmbeddingMLP Based Embedding : Comparison of the background rejection metric 1/B(S = 0.5) as a function of thenumber of labeled training samples used for finetuning for a pretrained model versus the one trainedfrom scratch (top), for a pretrained model using Cls Attn versus Flatten for aggregation (bottomleft), and for a pretrained model using an attention-based empbedding versus an MLP (bottomright). The shaded bands represent standard deviations calculated from 5 identical trials with randominitialization.",
  "Summary and Outlook": "In this study, we introduce a jet-based joint embedding predictive architecture (J-JEPA) for self-supervised learning of particle jet representations. In this approach, we train a model to predictrepresentations of masked target subjets based on the representations of unmasked context subjets,using additional positional information of the target subjets as hints. We finetune the target encoderfor the downstream task of jet classification and find that models pretrained with J-JEPA outperformmodels of the same architecture trained from scratch, for the same number of labeled samples.Thus, we demonstrate that J-JEPA effectively facilitates the learning of useful jet representations,highlighting its utility for large-scale pretraining and subsequent fine-tuning. Future work may investigate scaling to larger pretraining datasets, such as the full JetClass dataset ,which contains over 100 million jets, implementing physics-informed architectures for the contextand target encoders, such as the Particle Transformer , and alternative strategies for embeddingand defining targets and context, including clustering jets within entire collision events.",
  "and Disclosure of Funding": "The authors would like to acknowledge Raghav Kansal and Farouk Mohktar for their enlighteningdiscussions and constructive comments, which have significantly contributed to this research. Thiswork was supported by the Research Corporation for Science Advancement (RCSA) under grant #CS-CSA-2023-109, Alfred P. Sloan Foundation under grant #FG-2023-20452, U.S. Department of Energy(DOE), Office of Science, Office of High Energy Physics Early Career Research program underAward No. DE-SC0021187, and the U.S. National Science Foundation (NSF) Harnessing the DataRevolution (HDR) Institute for Accelerating AI Algorithms for Data Driven Discovery (A3D3) underCooperative Agreement OAC-2117997. This work was performed using the Pacific Research PlatformNautilus HyperCluster supported by NSF awards CNS-1730158, ACI-1540112, ACI-1541349, OAC-1826967, the University of California Office of the President, and the University of CaliforniaSan Diegos California Institute for Telecommunications and Information Technology/QualcommInstitute. Thanks to CENIC for the 100 Gpbs networks."
}