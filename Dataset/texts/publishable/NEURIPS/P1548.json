{
  "Abstract": "Transformer-based large language models (LLMs) have displayed remarkable cre-ative prowess and emergence capabilities. Existing empirical studies have revealeda strong connection between these LLMs impressive emergence abilities and theirin-context learning (ICL) capacity, allowing them to solve new tasks using onlytask-specific prompts without further fine-tuning. On the other hand, existingempirical and theoretical studies also show that there is a linear regularity of themulti-concept encoded semantic representation behind transformer-based LLMs.However, existing theoretical work fail to build up an understanding of the con-nection between this regularity and the innovative power of ICL. Additionally,prior work often focuses on simplified, unrealistic scenarios involving linear trans-formers or unrealistic loss functions, and they achieve only linear or sub-linearconvergence rates. In contrast, this work provides a fine-grained mathematicalanalysis to show how transformers leverage the multi-concept semantics of wordsto enable powerful ICL and excellent out-of-distribution ICL abilities, offeringinsights into how transformers innovate solutions for certain unseen tasks encodedwith multiple cross-concept semantics. Inspired by empirical studies on the linearlatent geometry of LLMs, the analysis is based on a concept-based low-noise sparsecoding prompt model. Leveraging advanced techniques, this work showcases theexponential 0-1 loss convergence over the highly non-convex training dynamics,which pioneeringly incorporates the challenges of softmax self-attention, ReLU-activated MLPs, and cross-entropy loss. Empirical simulations corroborate thetheoretical findings.",
  "Introduction": "Recently, a variety of transformer-based large language models (LLMs) have demonstrated remark-able performance across a broad spectrum of machine learning tasks, including natural languageunderstanding , symbolic reasoning , and even heuristics design . One crucial emergingability of these models is their in-context learning (ICL) capacity , which allows them to learnfrom a few demonstrations and conduct predictions on new queries without requiring any further",
  "arXiv:2411.02199v4 [cs.LG] 12 Nov 2024": "fine-tuning. However, the current theoretical understanding of the mechanisms underlying this ICLcapability remains limited, leaving the reasons for the remarkable emergence and generalizationpower of transformer-based LLMs in unseen ICL tasks largely unexplained. In line with traditional topic models , propose that latent concepts / topics underlie naturaltexts, providing a Bayesian inference framework to elucidate the ICL mechanism via Bayesian ModelAveraging (BMA) approach. On the other hand, theoretical and empirical studies have shown thattransformer-based models exhibit linear geometric regularities in their latent representations as aresult of concept or topic learning , where the representations within-concept have positiveinner products while representations cross-concepts exhibit near-orthogonal relationships. Thisstructured semantic geometry has been well-documented in recent research on pre-trained LLMs. However, the connection between this observed multi-concepts latent geometricstructure and the LMs remarkable ICL capabilities remains unclear. Separately, recent theoreticalanalyses have modeled ICL as a martingale process driven by latent concept variables . Yet,these studies have not incorporated the observed multi-concept semantic regularity into their analyses,nor have they discussed the strong out-of-distribution (OOD) ICL abilities exhibited by transformers. Additionally, existing theoretical work on transformer has been conducted on unrealistic, oversimpli-fied settings, such as linear or ReLU transformers , MLP-free attention-only models, QK-combined softmax attention , unrealistic infinite dimensional as-sumption and impractical loss functions like square loss andhinge loss . Furthermore, existing works have only been able to derive linear or sub-linearconvergence rates for the 0-1 loss. Therefore, there is a need for a more advanced analysis that can bridge the understanding betweenthe multi-concept semantic regularity and the mechanisms underlying transformer-based ICL. Thisnaturally leads to the research question:",
  "Whether and how do the geometric regularity of the multi-concept-encoded representationfacilitate transformer in conducting efficient ICL?": "To answer the above question, following the meaningful data modeling ideas in , we conducttheoretical analysis on a concept-specific sparse coding prompt distribution for classification tasks,where the sparse latent variable encodes the information denoting the words belonging concept.Importantly, the features in both the words and labels dictionaries exhibit concept-specific geometricproperties - within-concept positive inner products and cross-concept orthogonal geometric properties- that aligns with the findings in . Our main contributions are highlighted as below. 1. First, we provide a comprehensive analysis of the learning dynamics for a two-layer trans-former model, comprising one attention layer followed by a ReLU-activated feed-forwardnetwork, which is trained using the cross-entropy loss via stochastic gradient descent over aconcept-specific sparse coding prompt distribution. Leveraging advanced analytical tech-niques, we showcase the asymptotic properties governing the coupled learning dynamics ofthe attention and MLP layers. 2. To the best of our knowledge, we are the first to prove an exponential convergence of the 0-1loss over this challenging setting. Despite the highly non-convex optimization landscape, wedemonstrate that the transformer can achieve Bayes optimal test error with just a logarithmicnumber of iterations. 3. We provably show how the multi-concept encoded linear semantic geometry can enabletransformer to efficiently perform certain out-of-distribution ICL tasks. This offers anintuitive explanation for why transformer-based LLMs are able to successfully leveragethe polysemous nature of words to tackle diverse, unseen concept-specific tasks, aligningwell with users practical experiences. Furthermore, our analysis takes a step forwardin providing a potential theoretical underpinning for the innovative capabilities of LLMs,encompassing their ability to achieve cross-concept knowledge intersection. We believe ourfindings provide an initial positive response to Question 5.1.4 in the ICML 2024 positionpaper , which asks whether the observed latent geometry of LLMs can explain theirOOD extrapolation abilities.",
  "Related Work": "Theory of Exponential Convergence Rate of Stochastic Gradient Descent. Our analysis of theexponential convergence rate for the 0-1 loss builds upon prior work linking the excess risk andessential supremum norm to exponentially fast convergence under the hard low-noise condition. This phenomenon has been further explored in more recent studies analyzing the exponentialconvergence of stochastic gradient descent (SGD) , as well as in more generalizedsettings such as multiclass classification and support vector machines . Feature Learning in Learning Theory. Recent works in learning theory have extensively studiedstructured data from a feature learning perspective, examining NNs feature direction reconstructionand noise memorization as a proxy for training or 0-1 loss convergence . While priorstudies often assumed orthogonal features, recent efforts have analyzed non-orthogonal scenarios. Our work extends this line-of-research to challenging nonlinear Attention-MLP transformerswith non-orthogonal structured data representations. Theory of Transformers and In-Context Learning The literature on Transformers and ICL iswide-ranging, and we will selectively address the most relevant ones. Prior studies have analyzedhow transformers learn topic/concept semantics , the origins and biases of LLM representationsusing latent variable models , and ICL from a model averaging perspective . However, albeitincorporating concept variables, these works do not connect the geometric properties of concept-encoded representations to transformers powerful ICL abilities. Another line of research has studiedthe learning dynamics of ICL, including analyses of linear transformers , QK-combinedattention-only models , and multi-head softmax attention over linear regression without MLP. Though relevant, these works rely on simplifications and do not notice the connection betweensemantic regularity and powerful ICL. While also analyzes the learning dynamics of transformerswith softmax attention and ReLU MLPs for in-context classification tasks, making it the most relevantprior work, our analysis differs in several key aspects. Specifically, (i) they consider orthogonaldictionary learning with a single label vector, in contrast to our non-orthogonal concept-encodeddictionaries for both words and labels; (ii) their technique requires a large batch size (at least 2,where is the test error) and long context lengths, which are not required in our result; and (iii) theyutilize an impractical hinge loss and only achieve linear convergence without a relation to , whereaswe analyze the more practical cross-entropy loss and derive an exponential convergence rate in termsof the test error . However, we note that this is only an informal comparison due to the differencesin the models and primary findings. A detailed Related Work Section is deferred to Appendix C.",
  "Problem Setup": "Notations. For l2 and Frobenius norms we utilize and F to denote their computations.Considering two series an and bn, we denote an = O (bn) if there exists positive constant C > 0and N > 0 such that for all n N, |an| C |bn|. Similarly, we denote an = (bn) if bn = O (an)holds, and an = (bn) if an = O (bn) and an = (bn) both hold. Our 1() is to denote theindicator variable of an event. In addition, we denote span(v1, v2, . . . , vk) as the linear subspacespanned by the vectors v1, v2, . . . , vk, and conic(v1, v2, . . . , vk) denotes the conic hull (the set of allnon-negative linear combinations) of the vectors v1, v2, . . . , vk.",
  "Data Distribution": "The data distribution employed in this study draws inspiration from a range of empirical and theoreticalresearch works . This distribution captures context-awareness and can be viewed asa specialized prompt version of PLSA and LDA . In this distribution, each word and label hasmultiple feature embeddings, each embedding corresponding to a different concept. This is achievedthrough the use of a sparse latent concept/topic variable, which happened to be particularly adept atrepresenting language polysemy . Adhering to the LLM representation explored in , thefeatures in both the word and label dictionaries maintain orthogonality across concepts and positiveinner products within concepts. Additionally, the distribution incorporates Gaussian noise accountingfor linguistic ambiguity or the imperfection of the LLMs representation.Definition 1. Polysemous Word Model (Dx, Dy, Dz, Dx, Dy). We assume there exists K1 task-relevant concepts, each characterized by two semantically-opposite words feature vectors +k1 and k1, and their corresponding labels feature vectors q+k1 and qk1, k1 [K1]. There are alsoK2 task-irrelevant concepts denoted by k2, k2 [K2]. The word samples x RdX and theirlabels y RdY are generated from distributions parameterized by a shared latent concept variablez = (z1, , zK) {0, 1}K(K < dX ) capturing the concept-specific information:z Dz,x Dx = N(0, 2IdX ),y Dy = N(0, 2IdY),",
  "x = Mz + x Dx,y = Qz + y Dy,": "where the feature dictionary M = [+1 , 1 , +2 , 2 , , +K1, K1, 1, 2, , K2] RdX Kexhibits positive inner products within concepts and orthogonality across concepts, and the label dic-tionary Q = [q+1 , q1 , q+2 , q2 , , q+K1, qK1, 0, 0] RdYK has similar geometric properties.Specifically, we have k1 [K1], k2 [K2], k1 = k2 = u, qk1 = q, and there existconstants 0 < x, y < 1 such that 0 < +k1, k1 xu2 and 0 < q+k1, qk1 yq2. The detailed formal definition can be found in Appendix E. By this definition, a single word orlabel can possess different features corresponds to different concepts. The illustration of in can be an example, where the Dog vector in the representation space of LLM is decomposedto a direct sum of orthogonal vectors: [Animal] + [Mammal] + , and we can see [Animal]belongs to the concept Organisms Category categorized into labels [Animal] and [Plant], and[Mammal] belongs to the concept of Animals Category characterized by labels [Mammal],[Fish], [Bird], [Reptile]. Besides, in can also be a good support for our modeling,where Ferrari vector consists of [Cars] + [Italian] + . The following definition models the contextual prompts via specifying the statistical property of zamong in-context words, which is a special prompt version of PLSA and LDA . The detailedformal version is available in Appendix E.Definition 2. Concept-specific Contextual Prompt Distribution2. During training, each promptsample S = x1, y1, , xL, yL, xL+1 would share at least one co-concept, which is drawn from amixture distribution DS defined as:",
  "+k P+k,L+1 + k Pk,L+1,(1)": "where Pk,L+1 denotes the k-th concept-specific prompt distribution, and k = (2K1)1 denotes theequal chance of a sample to belong to Pk,L+1. Specifically, a sample Sn Pek,L+1, e [] meansthat the querys label ynL+1 is qek, and we denote ySn := e as the real value label of this prompt.In addition, every demonstration pairs (xnl , ynl ), l [L] in Pek,L+1 contain either (+k , q+k ) or(k , qk ) with equal chance. Also, every znl , l [L + 1] would satisfy P(znl,(2k12k) = 1) = K1,denoting the equal chance to have diverse features other than the current co-concept of the Pek,L+1. This definition suggests that for prompt S sampling from DS, there exists e [], k [K1],such that all the word-label pairs in this prompt share the k-th concept as their co-concept, and thecorresponding real value label of the query in this prompt is e. Besides, the real value label of eachword-label pair in the demonstration would have equal chance to be +1 or 1.",
  "Our theory allows for a broader range of the probability settings stated in the training prompt distribution,but for the sake of simplicity in presentation, we here chose a feasible one": "where R() := Relu(), S() := softmax(), WQ, WK Rmqk(dX +dY), WV Rmv(dX +dY)are the embedding matrices for queries, keys, and values, respectively, and WO Rmmvand r Rm are parameters in the MLP layer. Typically, min (mqk, mv) dX + dY. :={WQ, WK, WV , WO, r} denotes the set of all model weights.",
  "WO = (WyO) ,": "where WxQ, WxK RdX dX , WyV R(mvdX )dY, WyO RmdY. Here, we set the elementsother than WxQ, WxK, WyV and WyO to be zero. Besides, we fix WyV to be I(mvdX )dY. We sampleri from a uniform distribution Unif{1, 1} and fixed during the training process. Based on thissetting, the trainable part we need to consider is actually :=WxQ, WxK, WyO. This problemremains highly non-convex and challenging.",
  "F ,": "where (z) = log(1+exp(z)), ySn is the real value label of the prompt defined in Definition 2, andthe term 2F represents WxQ2F + WxK2F + WyO2F , which is the L2 regularization term with F denoted as the Frobenius norm. The purpose of the regularization in this paper is to accelerateand stabilize the mini-batch with-replacement SGD. The learning step is set to be t =2",
  "Theoretical Results": "In this section, we present our main theoretical results, which is based on the following conditions.We consider the learning iterations 0 t T , where T = (m110 11 m2K1q2((L 1)u2 + 1) log(1)) denotes the maximum admissible iteration.Condition 1. Suppose that there exists a sufficiently large constant C, such that the following hold:",
  "+ e02(1x)2u4/2": "Note that we do not have any requirement upon demonstration length L and batch size B for training,thus the training can be really flexible compared with the strict requirement in . The conditionon dimensionality dX , dY and the network width m ensure the learning problem is in a sufficientlyoverparameterized setting . The condition on ensures the learning step to be smalland thus learning process enjoys an approximation to gradient flow. The condition on the small isto ensure the models sufficient learning before being stuck by regularization . The condition onK is to control the impact of cross-concept contribution in the Attentions learning dynamic, whichcan actually be relaxed at the cost of a denser analysis. The condition on is to ensure that thegradient flows be mildly influenced by the noise. Last but not least, the conditions on 1 guaranteethat the initial beliefs of MLP is small and the gradients of SGD can update the model effectively. Amore detailed discussion over the parameter settings is delayed to Appendix H.Theorem 2. Exponential Convergence of 0-1 loss. Under Condition 1, define",
  "iterations, we have L01D ((T ))": "Note that the bound is valid only when T T, a common threshold in prior convergence rateanalyses . Importantly, the existence of T does not affect the convergence rate as 0,since T is independent of . Our novel analysis generalizes these prior results to our realistic settingshandling the challenges of self-attention, ReLU-MLP, and cross-entropy loss simultaneously. Byconsidering extreme cases, our techniques relax the batch size requirement, enabling more generalresults. Consequently, the sample complexity for Bayes-optimal test error is N = T. Before introducing the next proposition, we highlight a key observation from the semantic geometryin Definition 1. For any k1 [K1], defining ak1 := (+k1 + k1)/2 and bk1 := (+k1 k1)/2, wefind that for k1 = k1, {ak1, bk1} {ak1, bk1} and ak1, bk1 = 0. This structure is exemplified in(b) of , where [Bird] consists of orthogonal steering vectors: plant animal andmammal bird, corresponding to the concept feature ak and semantic label features bk. Here,the term ebk1 in ek1 determines the label assignment. Similarly, defining ck1 := (q+k1 + qk1)/2 anddk1 := (q+k1 qk1)/2 yields analogous properties. Detailed definitions are provided in Appendix I.The following proposition explores the models ability to handle OOD unseen ICL tasks.Proposition 1. Out-of-Distribution-Generalization3. During testing, the learned model admitsprobability distribution shift on Dz and data shift on Dx Dy to generate a new prompt distribution 3Here we do not consider the shift of Dx, Dy for the ease of presentation. However, we assert that this canalso be addressed by leveraging high-dimensional statistical analysis over other well-behaved noise distributions.",
  "The prompt length L can be any positive integer": "Dz can enjoy arbitrary distribution, satisfying that each prompt has at least one co-conceptk [K1], at least one pair shares the query words co-concepts label, and still each wordhas equal chance to have positive or negative semantic labels over its concepts4. DxDy can enjoy a great family of data shift. k = k [K1], k2 [K2], we can have newM and Q such that k = akbk, qk = ckdk, k2 = k2. Here, ak, bk, ck, dk areany vectors belong to the conic hulls of {ak}K1k=1, {bk}K1k=1, {ck}K1k=1, {dk}K1k=1 respectively,satisfying bk ak = (u) and dk ck = (q). k2 = (u) are anyvectors from the complement space of span(M).",
  "Again, the learned model satisfies L01DS ((T ))": "This proposition demonstrates the strong Out-of-Distribution Generalization ability of transformerutilizing multi-concept semantics, suggesting the efficiency transformer to conduct unseen ICL tasksjust by its learned Knowledge on the high-level concept and low-level label semantic informationfrom the two non-orthogonal dictionaries. The admit of shift for Dz denotes that each promptcan enjoy multi-co-concepts and each word-label pair can appear in at least z0 concept-specificprompts/tasks distribution, which aligns the real-world cases. On the other hand, we also believe theadmit of shift for DxDy is inspiring, suggesting that transformer can conduct specific cross-conceptsemantic Knowledge Intersection. As such, this lemma suggest that the transformer can master theregularity of unseen ICL tasks structure in the presence the multi-concept encoded representation.Remark 1. Comparison with Related Work. Theorem 3.4 in and Theorem 2 in addressthe transformers OOD capability in specific structured ICL classification and regression tasks. Ourresults differ by focusing on compositional generalization of learned concepts, grounded in theconcept-specific linear latent geometry observed in LLMs.",
  "Proof Idea": "In a big picture, we simply extend standard expectation-variance reduction techniques to oursetting. .1 defines coefficients to examine NNs expected projection along feature directions..2 provides the convergence of the expected estimator through the lens of coefficientevolution; .3 showcase the exponential convergence by treating the conditional expectationsof the NNs as Doob martingales and exploiting the property of the tails under low-noise conditions.",
  "w=1uwuw = IdX dX .(3)": "Similar techniques are also applied to the labels dictionary: Q := span(Q), where we defineq1 , , qdYK1 as the standard orthogonal basis of the complement space Q. In our subsequentderivation, the expectation E[] is taken over the stochastic gradient descent. Similar to the idea in, we first serve to see how E((t)) evolves. For E((t)), every gradient descent updateby all concepts samples within a soft weight, and thus the analysis is equivalent to gradient descent",
  "E[WyO(i,)(t)qek] = (t)O(i,),k + e (t)O(i,),k,(4)": "for e [], i [m], k [K1] and for e [], s [K1], r [K2], w [dX K], u {es, r, uw}, it holds that E[(WxK(t)u)]E[WxQ(t)es] = 0. Similar conclusions hold when thequery vectors are r and uw, r [K2], w [dX K]. As such, our remaining task is to scrutinizethe coefficients evolution, which would be the key contributors to the expected 0-1 loss convergence.",
  "holds, we have L01D (E((t))) = 0": ": Learning dynamics: (i) training and test loss; (ii) correct attention weight; (iii) maximum values ofQ,s K,s, Q,s K,s, maximum values of the complement products Q,r K,r or Q,2 K,2, and maximumvalues of product-with-noise (WxKx)WxQx; (iv) maximum values of O(i,),k and |O(i,),k|, maximumvalues of the complement coefficients O(i,),w and maximum values of product-with-noise WyO(i,)y.",
  "(6)": "are martingale difference sequences, and for X {Q, K, O} and its corresponding W {WxQ, WxK, WyO}, we have Tt=0 DtX = W(T +1) E[W(T +1)]. Then we utilize the follow-ing lemma in to give a bound over the variance.Lemma 5. Let D1, , DT 1 be a martingale difference sequence. Suppose cT > 0 such thatTt=0 Dt2 c2T , where is the essential supremum of F . Then for > 0, we have",
  "supremum of DtXF . Subsequently, by controlling the martingale sequence norm tail similarly in, we can obtain an exponential convergence rate after T1": "For W {WxQ, WxK, WyO}, to check the decaying cT , we adopt the techniques of inthe following manner. Let Bt be an independent variable from B0, , BT and let Wt(T +1) be anoutput of the algorithm depending on (B0, , Bt1, Bt, Bt+1, , BT ). Then we have",
  "DtX E[W(T +1) Wt(T +1) | B0, , Bt]": "Therefore, one may estimate cTX2 by bounding W(T +1)Wt(T )2 uniformly w.r.t. B0, , BT 1.Such a bound can be derived utilizing stability property of stochastic gradient descent . Forthe OOD scenario, since we require the data shift to be via conic combination, the new words andlabels in each prompt will share the positive/negative real-valued label without any self-conflict. Thenorm requirements and constraints on Dz would ensure the Gaussian noise, concepts other thanthe co-concepts, and probability shifts have limited influence on the prediction compared with theconsiderable scale of coefficients by Lemma 4, laying the groundwork for the proof.",
  "Experiments": "In this section, we demonstrate the validity of our theoretical analysis through simulations ofAlgorithm 1. We use the following parameter settings in : The parameter settings are:the length L = 4, the number of co-concepts K1 = 2, dictionary size K = 104, the number oftest instances ntest = 5000, dimension dX = dY = 1000, MLP width m = 50, feature strengthsu = q = 10, k [K1], the cosine +k , k /u2 = q+k , qk /q2 = 0.5, the initializationparameters 0 = 0.1, 1 = 0.01, and the noise deviation = 0.01. For the optimization, weuse = 0.002, B = 16, = 10000, and the total training epochs is 100. (a-d) usesthe same training settings, but during testing, it applies different configurations: (a) L = 5, (b)L = 2, (c) a 0.8 fraction for the first concept and a 0.2 fraction for the second concepts, and (d)1 = a1 b2, 2 = a2 b1. validates our Theorem 2 and Lemma 4, which showcasesthe fast convergence rate and the evolution of coefficients. validates Proposition 1, wherethe learned model permits certain data shifts.",
  "Conclusion": "This work provides the first exponential convergence analysis of 0-1 loss for transformers withsoftmax attention and ReLU-MLP, trained on a non-orthogonal concept-specific prompt distributionby practical cross-entropy loss. Furthermore, the results demonstrate transformers can perform certainOOD ICL tasks by leveraging the multi-concept semantic linearity, highlighting their innovativepotential. An important future direction is to extend the analysis to more complex scenarios.",
  "Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation ofin-context learning as implicit bayesian inference, 2022": "Xinyi Wang, Wanrong Zhu, Michael Saxon, Mark Steyvers, and William Yang Wang. Largelanguage models are implicitly topic models: Explaining and finding good demonstrations forin-context learning. In Workshop on Efficient Systems for Foundation Models @ ICML2023,2023. Yuchen Li, Yuanzhi Li, and Andrej Risteski. How do transformers learn topic structure: Towardsa mechanistic understanding. In Proceedings of the 40th International Conference on MachineLearning, pages 1968919729, 2023.",
  "Kiho Park, Yo Joong Choe, Yibo Jiang, and Victor Veitch. The geometry of categorical andhierarchical concepts in large language models. arXiv preprint arXiv: 2406.01506, 2024": "Yibo Jiang, Goutham Rajendran, Pradeep Ravikumar, and Bryon Aragam. Do llms dream ofelephants (when told not to)? latent concept association and associative memory in transformers.arXiv preprint arXiv: 2406.18400, 2024. Yufeng Zhang, Fengzhuo Zhang, Zhuoran Yang, and Zhaoran Wang. What and how doesin-context learning learn? bayesian model averaging, parameterization, and generalization.arXiv preprint arXiv: 2305.19420, 2023. Fabian Falck, Ziyu Wang, and Christopher C. Holmes. Are large language models bayesian?a martingale perspective on in-context learning. In ICLR 2024 Workshop on Secure andTrustworthy Large Language Models, 2024. Johannes Von Oswald, Eyvind Niklasson, Ettore Randazzo, Joao Sacramento, AlexanderMordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context bygradient descent. In Proceedings of the 40th International Conference on Machine Learning,volume 202, pages 3515135174. PMLR, 2023.",
  "Yu Huang, Yuan Cheng, and Yingbin Liang. In-context convergence of transformers. arXivpreprint arXiv: 2310.05249, 2023": "Yuandong Tian, Yiping Wang, Beidi Chen, and Simon S Du. Scan and snap: Understandingtraining dynamics and token composition in 1-layer transformer.In Advances in NeuralInformation Processing Systems, volume 36, pages 7191171947, 2023. Yingcong Li, Yixiao Huang, Muhammed E. Ildiz, Ankit Singh Rawat, and Samet Oymak.Mechanics of next token prediction with self-attention. In Proceedings of The 27th InternationalConference on Artificial Intelligence and Statistics, volume 238, pages 685693, 2024. Chenyu Zheng, Wei Huang, Rongzhen Wang, Guoqiang Wu, Jun Zhu, and Chongxuan Li. Onmesa-optimization in autoregressively trained transformers: Emergence and capability. arXivpreprint arXiv:2405.16845, 2024.",
  "Hongkang Li, Meng Wang, Songtao Lu, Xiaodong Cui, and Pin-Yu Chen. How do nonlineartransformers learn and generalize in in-context learning? arXiv preprint arXiv: 2402.15607,2024": "Zixin Wen and Yuanzhi Li. Toward understanding the feature learning process of self-supervisedcontrastive learning. In Proceedings of the 38th International Conference on Machine Learning,volume 139, pages 1111211122. PMLR, 2021. Patrik Reizinger, Szilvia Ujvry, Anna Mszros, Anna Kerekes, Wieland Brendel, and FerencHuszr. Position: Understanding LLMs requires more than statistical generalization. InProceedings of the 41st International Conference on Machine Learning, volume 235, pages4236542390, 2024.",
  "Pascal Massart and lodie Ndlec. Risk Bounds for Statistical Learning. The Annals ofStatistics, 34(5):2326 2366, 2006": "Loucas Pillaud-Vivien, Alessandro Rudi, and Francis Bach. Exponential convergence of testingerror for stochastic gradient methods. In Proceedings of the 31st Conference On LearningTheory, volume 75, pages 250296, 2018. Atsushi Nitanda and Taiji Suzuki. Stochastic gradient descent with exponential convergencerates of expected classification errors. In Proceedings of the Twenty-Second InternationalConference on Artificial Intelligence and Statistics, volume 89, pages 14171426, 2019. Vivien A Cabannes, Francis Bach, and Alessandro Rudi. Fast rates for structured prediction. InProceedings of Thirty Fourth Conference on Learning Theory, volume 134 of Proceedings ofMachine Learning Research, pages 823865. PMLR, 1519 Aug 2021. Shingo Yashima, Atsushi Nitanda, and Taiji Suzuki. Exponential convergence rates of classifica-tion errors on learning with sgd and random features. In Proceedings of The 24th InternationalConference on Artificial Intelligence and Statistics, volume 130, pages 19541962, 2021. Kazusato Oko, Taiji Suzuki, Atsushi Nitanda, and Denny Wu. Particle stochastic dual coordinateascent: Exponential convergent algorithm for mean field neural network optimization. InInternational Conference on Learning Representations, 2022. Stefano Vigogna, Giacomo Meanti, Ernesto De Vito, and Lorenzo Rosasco. Multiclass learningwith Margin: Exponential rates with no bias-variance trade-off. In Proceedings of the 39th In-ternational Conference on Machine Learning, volume 162 of Proceedings of Machine LearningResearch, pages 2226022269. PMLR, 1723 Jul 2022. Vivien Cabannnes and Stefano Vigogna. A case of exponential convergence rates for svm.In Proceedings of The 26th International Conference on Artificial Intelligence and Statistics,volume 206, pages 359374. PMLR, 2527 Apr 2023. Zeyuan Allen-Zhu and Yuanzhi Li. Towards understanding ensemble, knowledge distillationand self-distillation in deep learning. In The Eleventh International Conference on LearningRepresentations, 2023. Yuan Cao, Zixiang Chen, Misha Belkin, and Quanquan Gu. Benign overfitting in two-layer con-volutional neural networks. In Advances in Neural Information Processing Systems, volume 35,pages 2523725250, 2022. Yiwen Kou, Zixiang Chen, Yuanzhou Chen, and Quanquan Gu. Benign overfitting in two-layerreLU convolutional neural networks. In Proceedings of the 40th International Conference onMachine Learning, volume 202, pages 1761517659, 2023.",
  "Hiroaki Yamagiwa, Momose Oyama, and Hidetoshi Shimodaira. Discovering universal geome-try in embeddings with ica. arXiv preprint arXiv: 2305.13175, 2023": "Zixin Wen and Yuanzhi Li. Toward understanding the feature learning process of self-supervisedcontrastive learning. In Proceedings of the 38th International Conference on Machine Learning,pages 1111211122, 2021. Chi Han, Jialiang Xu, Manling Li, Yi Fung, Chenkai Sun, Nan Jiang, Tarek Abdelzaher, andHeng Ji. Word embeddings are steers for language models. In Proceedings of the 62nd AnnualMeeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages1641016430, 2024.",
  "Atsushi Nitanda and Taiji Suzuki. Optimal rates for averaged stochastic gradient descent underneural tangent kernel regime. arXiv preprint arXiv: 2006.12297, 2021": "Yiwen Kou, Zixiang Chen, Yuan Cao, and Quanquan Gu. How does semi-supervised learningwith pseudo-labelers work? a case study. In The Eleventh International Conference on LearningRepresentations, 2023. Difan Zou, Yuan Cao, Yuanzhi Li, and Quanquan Gu. Understanding the generalization ofadam in learning neural networks with proper regularization. In The Eleventh InternationalConference on Learning Representations, 2023.",
  "Iosif Pinelis. Optimum bounds for the distributions of martingales in banach spaces. The Annalsof Probability, 22(4):16791706, 1994": "Moritz Hardt, Ben Recht, and Yoram Singer. Train faster, generalize better: Stability ofstochastic gradient descent. In Proceedings of The 33rd International Conference on MachineLearning, volume 48, pages 12251234, 2016. Difan Zou, Yuan Cao, Yuanzhi Li, and Quanquan Gu. The benefits of mixup for feature learning.In Proceedings of the 40th International Conference on Machine Learning, volume 202, pages4342343479, 2023. Jinghui Chen, Yuan Cao, and Quanquan Gu. Benign overfitting in adversarially robust linearclassification. In Robin J. Evans and Ilya Shpitser, editors, Proceedings of the Thirty-NinthConference on Uncertainty in Artificial Intelligence, volume 216, pages 313323, 2023. Spencer Frei, Niladri S Chatterji, and Peter Bartlett. Benign overfitting without linearity: Neuralnetwork classifiers trained by gradient descent for noisy linear data. In Proceedings of ThirtyFifth Conference on Learning Theory, volume 178, pages 26682703, 2022. Spencer Frei, Gal Vardi, Peter Bartlett, and Nathan Srebro. Benign overfitting in linear classifiersand leaky relu networks from kkt conditions for margin maximization. In Proceedings of ThirtySixth Conference on Learning Theory, volume 195, pages 31733228, 2023. Yiwen Kou, Zixiang Chen, and Quanquan Gu. Implicit Bias of Gradient Descent for Two-layer ReLU and Leaky ReLU Networks on Nearly-orthogonal Data. In Advances in NeuralInformation Processing Systems, volume 36, pages 3016730221. Curran Associates, Inc.,2023. Wei Huang, Ye Shi, Zhongyi Cai, and Taiji Suzuki. Understanding convergence and gener-alization in federated learning through feature learning theory. In The Twelfth InternationalConference on Learning Representations, 2024. Dake Bu, Wei Huang, Taiji Suzuki, Ji Cheng, Qingfu Zhang, Zhiqiang Xu, and Hau-San Wong.Provably neural active learning succeeds via prioritizing perplexing samples. In Proceedings ofthe 41st International Conference on Machine Learning, volume 235, pages 46424695, 2024. Yiwen Kou, Zixiang Chen, Quanquan Gu, and Sham M. Kakade. Matching the statistical querylower bound for k-sparse parity problems with stochastic gradient descent. arXiv preprint arXiv:2404.12376, 2024.",
  "Hongru Yang, Bhavya Kailkhura, Zhangyang Wang, and Yingbin Liang. Training dynamicsof transformers to recognize word co-occurrence via gradient flow analysis. arXiv preprintarXiv:2410.09605, 2024": "Jiarui Jiang, Wei Huang, Miao Zhang, Taiji Suzuki, and Liqiang Nie. Unveil benign overfittingfor transformer in vision: Training dynamics, convergence, and generalization. arXiv preprintarXiv:2409.19345, 2024. Bingrui Li, Wei Huang, Andi Han, Zhanpeng Zhou, Taiji Suzuki, Jun Zhu, and Jianfei Chen.On the optimization and generalization of two-layer transformers with sign gradient descent.arXiv preprint arXiv:2410.04870, 2024. Yoshua Bengio. Learning Deep Architectures for AI. Now Publishers Inc, 2009. Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016. Zeyuan Allen-Zhu and Yuanzhi Li. Backward Feature Correction: How Deep Learning PerformsDeep Learning. In Conference on Learning Theory, COLT 23, 2023. Full version available at Core Francisco Park, Maya Okawa, Andrew Lee, Ekdeep Singh Lubana, and Hidenori Tanaka.Emergence of hidden capabilities: Exploring learning dynamics in concept space. arXiv preprintarXiv:2406.19370, 2024.",
  "Michael Hahn and Navin Goyal. A theory of emergent in-context learning as implicit structureinduction. arXiv preprint arXiv: 2303.07971, 2023": "Emanuele Marconato, Sbastien Lachapelle, Sebastian Weichwald, and Luigi Gresele. Allor none: Identifiable linear properties of next-token predictors in language modeling. arXivpreprint arXiv:2410.23501, 2024. Roger A. Horn and Charles R. Johnson. Matrix Analysis. Cambridge university press, 2012. Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence andgeneralization in neural networks. In Advances in Neural Information Processing Systems,volume 31. Curran Associates, Inc., 2018.",
  "ALimitation and Broader Impact": "The theoretical analysis provided in this work introduces novel perspectives on optimization and generalization,but the data model employed may require additional refinements to better align with practical scenarios, suchas adding more layers of attention. The techniques and findings can inform future empirical and theoreticalexplorations of transformer architectures, though we do not foresee a direct social impact arising from thetheoretical advancements presented.",
  "CAdditional Related Work": "Theory of Convergence Rate of Stochastic Gradient Descent. Our analysis of the exponential convergencerate for the 0-1 loss builds upon a rich body of prior work. In the context of classification, the faster convergencerate mostly based on the excess of risk with some power of the essential supremum norm. Specifically, introduce the Hard low-noise condition over the margin. When there is a hard margin separating the classes, thetest error can exhibit exponentially fast convergence as the number of training samples increases, even whenthe surrogate loss error only decreases polynomially. This phenomenon has been further explored in morerecent studies. have analyzed the exponential convergence of stochastic gradient descentunder various settings. Meanwhile, have investigated hard-margin and exponential rates in the contextof structured prediction, which encompasses traditional classification as a special case. Besides, recent workalso obtain the exponential rates in generalized settings such as Multi-class classification and SVM .Building upon this rich theoretical foundation, our work derives the first exponential convergence analysis forthe 0-1 loss in the specific setting of transformer models with softmax attention and ReLU-activated MLP overthe sparse coding data model, whose surrogate loss function is the cross-entropy loss. Theory of Feature Learning of GD-updated Neural Network. A rich body of recent learning theory researchhas focused on the feature direction recovery view of neural network representations . Rather than directly examining the evolution of the0-1 loss, this line of work explicitly studies the process of reconstruction of the datas feature directions andmemorization of disrupted noise in the networks latent space as surrogate metrics. While most studies in thisarea have assumed (near) orthogonal data, recent efforts by and have made initial attempts to analyzenon-orthogonal data scenarios. Building upon this foundation, our study extends this line of research to nonlinearattention-MLP transformers with within-concept positive inner products and cross-concept orthogonal datarepresentations. The key to our analysis is the assumption of good initialization of attention matrices and asufficiently low-noise condition, which is reasonable for modeling language rather than images. In this setting,SGD allows noise to have only a mild impact on shaping neural network matrices or influencing gradient flow. Theory of Transformers and In-Context Learning. The literature on Transformers and ICL is wide-ranging,and we will selectively address the most relevant ones. Prior studies have analyzed how transformers learntopic/concept semantics , the origins and biases of LLM representations using latent variable models , andICL from a model averaging perspective . However, these works do not connect the geometric properties ofconcept-encoded representations to transformers powerful ICL abilities. Another line of research has studiedthe learning dynamics of transformer, including analyses of linear-attention transformers , QK-combined attention-only models , ReLU-free MLP orwithout MLP , impractical squared or hinge loss . Though relevant, these works rely onsimplifications or do not connect the observed linear semantic representation of large model to the transformersexcelling OOD capability. Concept Learning in Deep Learning. Hierarchical learning has long been regarded as a key factor behind thesuccess of deep learning . Recent research shows that large-scale generative models, such as diffusionmodels and transformers, effectively encode hierarchical concepts in their latent spaces .Moreover, show that transformers can capture hierarchical and compositional structures in data.From a Bayesian perspective, interpret ICL as LLMs predicting outputs based on latent (concept)variable inference. Furthermore, studies reveal a linear structure in LLMs latent space over independentinterpretable concepts: representations of the same concept exhibit positive inner products, while statistically-independent concepts are nearly orthogonal . Interestingly, aligning with the findings in, Independent Component Analysis (ICA) is naturally more suitable than Principal Component Analysis(PCA) for obtaining meaningful feature or label vectors in our prompt modeling. This is because the features orlabels are nearly statistically independent and of equal strength, especially with a large K, while the noise is feeble in our modeling. Building on these insights, we explore in a theoretical context how the compositionalnature of concept representations relates to transformers ability to generalize to OOD tasks through a sparsecoding modeling. We believe our OOD results are not only coincides with the transformers compositionalgeneralization ability on language tasks , but also consistent with other concept learning outcomes ofdiffusion and multi-model model: shows that adjusting the length of semantic representations can directlyaffect image generation behaviors (see ), while reveals that compositing different concepts enablesOOD generalization (e.g. blue square apples in the a in ).",
  "2is the probability of the conditional event {(0)O(i,),k e(0)O(i,),k > 0 | (0)O(i,),k +": "e(0)O(i,),k > 0}, and > 0 due to the larger variance of (0)O(i,),k compared to e(0)O(i,),k. We de-note the probability with since the true value is hard to compute. Subsequently, the event {i [m] | ri =em, (0)O(i,),k (0)O(i,),k > 0} can be seen as a binomial variable with p = 1 +",
  "b(1 e2y(0)2)": "Proof. From the condition we see that z0 < 0 and zt is an increasing sequence (zt z0). Besides, as y0 > 0,during the period where zt 0, we see that yt is monotonically decreasing. Then by (2 + e2yt2 + e2yt2)1 1/4 as well as Comparison Theorem, its obvious that the continuous coupled ODE in Eq.(9) is the lower boundof yt. Then one can readily obtain the result by solving the ODE.",
  "This section provided the detailed formal definitions of the prompt distribution": "Definition 3. (Polysemous Word Model (Dx, Dy, Dz, Dx, Dy) ). We assume there exists K1 concepts ofwords totally. Specifically, each concept k1 [K1] is characterized by two semantically-opposite feature vectorsseparately, denoted as +k1 and k1, and the label vectors that describe their semantics under the co-conceptare q+k1 and qk1. Our word samples x RdX and their corresponding labels y RdY are generated i.i.d.from distribution Dx and Dy, which can be written as the following forms via reparameterization:",
  "K)": "M = [+1 , 1 , +2 , 2 , , +K1, K1, 1, 2, , K2] = [M1, , MK] RdX K is thefeature dictionary matrix, where {k1}K1k1=1 are concept-relevant features, {k2}K2k2=1 are concept-irrelevant features, and k [K], Mk = u. We assume that features of the same concept havepositive inner product: 0 < x < 1, k1 [K1], 0 < +k1, k1 xu2. Meanwhile, we letthe features of different concept be orthogonal: e [], e [], s [K1], r = r [K2], u {es, r}, we have es, u = r, r = 0. Q = [q+1 , q1 , q+2 , q2 , , q+K1, qK1, 0, 0] RdY K is the corresponding label dictionarymatrix, where qk = q, for k [K1]. Similarly, we let the labels of the same concept to havepositive inner product: 0 < y < 1, k1 [K1], 0 < q+k1, qk1 yq2, while the labels ofdifferent concept to be orthogonal: qk , qk = 0, k = k [K1]. Definition 4. (Concept-specific Contextual Prompt Distribution) We consider the case that each prompt isconcept-specific (i.e., the multi-concept words in one prompt would at least share one co-concept). Specifically,the chance for selecting each concept as the co-concept of one particular prompt is (K11), and the chance",
  ",": "where Pl,j := P (zl,j = 1). n [N] where N is the training size, if the training prompt Sn is sampled fromPek,L+1, e [], k [K1], then by Definition 1, the label vector of the query should contain qek, and we callySn = e as the real value label of this k-th concept prompt. Specifically, for k [K1] we define the index setof training prompts sharing the k-th co-concepts as",
  "Vk =n | Sn Pk,L+1": "For sample xl where n Vk, k [K1], l [L + 1], we define the index set for its non-zero elements of znlbesides zn2k1,l and zn2k,l, namely Mnl := {k [K] | znl,k = 1, k / {2k 1, 2k}}. Also, for each promptsharing the k-th co-concept, we define the index set of demonstration in the context:",
  "HDiscussions over Parameter Settings": "Note that we do not have any requirement upon demonstration length L and batch size B for training, thus thetraining can be really flexible compared with the strict requirement in . The condition on dimensionalitydX , dY and the network width m ensure the learning problem is in a sufficiently overparameterized settingwhere the norm and the inner products of the Gaussian noise and initialized NN can be controlled within a certainrange with high probability 1 , which is standard requirements in recent feature learning line-of-research. The weak requirement on network width m allows us to conduct a fine-grainedanalysis based on the network projection length, which is fundamentally differs from the NTK line of research that requires an infinitely wide network to perform linear regression over a prescribed feature map. Thecondition on ensures the learning step to be small and thus learning process enjoys an approximation togradient flow rather than the challenging Oscillation regime , which is analyzable but not necessary inpresenting our theory. The condition on the small is to ensure that the learning dynamic of Attention and MLPwould not stuck at the origin point, and ensure that we can analyze the expected learning dynamic with limitedimpact of the regularization at the initial stage, which is also adopted in . The condition on K is to controlthe impact of cross-concept contribution in the Attentions learning dynamic, which can actually be relaxed atthe cost of a denser analysis. The condition on is to ensure that the impact of the norms and inner-productsinvolving the Gaussian Noise on the gradient cannot surpass those in the order of features norms, which ensuresthe gradient flows to be not too noisy and could converge to the expected gradient flow exponentially. Last butnot least, the conditions on 1 guarantee that the initial beliefs of MLP is small and the gradients of SGD canupdate the model effectively. The condition of 0 is only used when discussing the OOD scenario.",
  "for k1 = k1 [K1]": "Remark 3. We observe that, through this formulation, the shared component ak1 can be interpreted as theconcept part of the two features, while the terms bk1 represent their opposing semantic aspects. Therelevance of this modeling is exemplified by (b) in , where the concept [Bird] is composed oforthogonal steering vectors: plant animal and mammal bird. These vectors correspond to the conceptfeature ak and the semantic label features bk, respectively. Idempotent Operator Trick. Define U := span(M) and its complement space U. By definition, we knowthat dim(U) = K and dim(U) = dX K. Then we can have a set of standard orthogonal basis for Rd,defined as",
  "EBt[W(t+1)Q| E(E((t)))] = E[WxQ(t)] tEBt[WxQ(t)LBt(E((t)))]": "Here, we see E((t)) as fixed matrices and the expectation EBt[] is taken over the stochastic batch at the timestep t. As we are considering expectation over the isotropic prompt distribution, which can be seen as a noiselessdistribution with an averaged categories of words and labels, the expected gradient form could be written assymmetric form:",
  "j )] =exp((t)Q,k (t)K,k/bk2) + exp((t)Q,k (t)K,k/bk2)2.(41)": "This observation under our expectation scenario greatly facilitate our analysis. Since n(t) < 0, its obvious thatthe signal of ri(t)O(i,),k will determine whether the neuron i [m] will serve to increase or decrease the (t)Q,kand (t)K,k during the gradient update. We therefore start to analyze the MLPs update below based on Lemma 16.",
  "Proof. The proof is direct by the symmetric property of prompt distribution in Lemma 22, and the gradientforms in Lemma 19 and Lemma 20": "An interesting fact is that the E(I(t)O(i,),ck,chaos) also contributes to the learning of k-th concept. This actuallysuits our intuition that if similar things appear in various fields (concepts), the learning process can help integrateand facilitate the learning. The following lemma demonstrate the lower bound of the attention assignment, whichemerge from the good property of our expected attention.",
  "I.1First Stage: Growing of Coefficient": "In this stage, the coefficient update dynamic is continually changing without being much influenced by thecomparably feeble regularization. Also, the impact of the decaying learning step t is under controlledduring several periods, which can be safely done due to small initialization by a large , as well as the slowquadratic decaying nature of the derivative of t. We see that at initialization, by Lemma 7 and Lemma 23, theESnDS[f(E(Sn); (0))] satisfies",
  "| EnVek(ef(E(S); E((t))) EnVek(ef(E(S); E((t))))|": "is non-increasing. Intuitively, this observation is due to the inherent nature of cross-entropy loss, whichalways pays more emphasis (has larger derivative) on those low value. Also, another important factor isthe update of those ambiguous neurons coefficient summation would also prefer the low-value one amongEnVek(ef(E(S); E((t))), e [m]. To better present this observation, we define",
  "We claim that if": "i[m] ri(0)O(i,),k > 0 during initialization, the expected attention score will not experiencethis decreasing period due to the expected gradient formula in Lemma 27. Our aim for this period is to examinethe lower bound of the attention score during a limited number of iterations.Lemma 34. Under Condition 1, for k [K1], after at most a certain iterations",
  "l )], we have": "1 E(n(t)) C. Also, m E[|Uek,n(0))|] m/8 by Lemma 7, as well as the fact that E[Wek,n(t)]will at least preserve the neurons of E[Uek,n(0))] along the iterations, without being deactivated as discussed inLemma 29. Also, we note that in this hypothesised decreasing period, the absolute value of the initially negativeE[",
  "+ e2(0)Q,k2/bk2 1),": "Here, by a appropriate chosen small C4, we again ignore the regularization term at this period due to =O((C log(Km/)q)1) for a large C by Condition 1, and the impact of the learning rate is also controlleddue to the slow quadratic decaying nature of t and a small initial 0 O(0.01C1) by Condition 1, so as the",
  "Lemmar 31 we see that the scale of | EnVek[ef(E(S); E((t)))]]| is also (). This suggest that there still exists": "a constant C, during a certain amount of subsequent iterations we would still have that C E[(t)] 1.Also, m E[|Uek,n(0))|] m/8 by Lemma 7, as well as the fact that E[Wek,n(t)] will at least preserve theneurons of E[Uek,n(0))] along the iterations, without being deactivated as discussed in Lemma 29. In addition,recall that in this first stage we also can control the impact of regularization and decaying learning rate by asmall and a big by the sufficiently large C in Condition 1, which indicates we now have",
  "mK1),": "where the first inequality is by the definition of E[n(t)]) (similar to the techniques in Lemma 32) and thedefinition of E[Ak,et]; the second inequality is by g(x) = x < ex 1 as well as S2(1+y)2(1y)2 > 1;the second inequality is by the case (i) hypothesis. Then we would have",
  "(E[e(t)O(i,),k])": "Again, similar to Lemma 36, we can have asymptotic property when considering the decaying impact of thelearning rate, as well as the cross-entropy loss. We directly provide the following two lemmas. Due to thesimilarity of the proof procedures of Lemma 35 and Lemma 36, we omit the proofs of the following two lemmasas well as the constant details for simplicity. Lemma 39. (Asymptotic Property 2). If we consider the impact of the decaying learning rate at the sec-ond stage and do not consider the decaying of cross-entropy loss, for some constants c, d, c, d regardingK1, , u, q, x, y, we will have",
  "= (1)": "Its obvious that the decaying impact of the learning rate and cross-entropy loss are at the similar order. Also,if we consider decaying learning rate, the right side of the inequality would be smaller. z(t) would be in a(log(log(t))) order when z(t) get large, which will make the right side of the y(t)s formula contain anintergral of (log log(t)), which is obviously slower.",
  "m)": "Proof. For i [m], by the gradient update rule in Eq.(22), as well as Lemma 4s insight we see that the lengthsof the WyO on certain projection direction will continue to grow until being stuck by the regularization, which isa -scaled WyO itself. Due to the low-noise condition in Condition 1 with a sufficiently large C as well as theisotropy of noise, the learning progress of features would be the main contributor to the F norm of NN matricesand the noise, validated in (iii-iv). We can consider an extreme case where all the samples in a singlebatch belongs to some concept k [K1], which we can have the upper bound of the first term of the right side ofthe inequality over the k-th concepts corresponding projection direction, and thus we can derive an upper bound",
  "l ((t)S )n": "j 1/4, Eq.(62) in Lemma 41; the second inequalityis by the low noise condition m/(CdX uq1/2) and the large K Cu/(dX ) for a largeC in Condition 1. Thus the update of ak WxQ(t)ak and ak WxK(t)ak are dominated by their regularization,and thus the scale can not be better than the initialization. By Lemma 7, the conclusion holds. On the other hand, we see that by the scaled identity initialization of WxQ(0), WxK(0) and orthogonal rela-tionships of vectors in Lemma 27, the initialization of bk WxQ(t)bk and bk WxK(t)bk are the same, and asthe gradient update is nearly symmetry, which can lead to the fact that (bk WxQ(t)bk) = (bk WxK(t)bk)and bk WxQ(t)WxK(t)bk = (bk WxQ(t)bkbk WxK(t)bk/bk2). By the scaled identity initialization ofWxQ(0), WxK(0), orthogonal relationships of vectors in Lemma 27, Eq.(19) and (20) we can see that",
  "j )))": "We see that bk WxQ(t)bk and bk WxK(t)bk will continue to grow up except always being stuck by the regular-ization. To see the upper bound under this situation, we consider an extreme case where all the samples in asingle batch belongs to some concept k [K1], and there is only one demonstrations in each prompt share thesemantic with the query. Then by the scaled identity initialization of WxQ(0), WxK(0), orthogonal relationshipsof vectors in Lemma 27 and Eq.(18), we can see that the growing of bk WxQ(t)bk and bk WxK(t)bk wouldsatisfy the following and strive to grow up to make the equality holds, which naturally have an upper bound",
  "which denotes e2bk WxQ(t)WxK(t)bk should be the key contributor": "Similar to the claims in Lemma 41, here we see that as the is very small, by the scaled identity initialization ofWxQ(0), WxK(0), orthogonal relationships of vectors in Lemma 27, as well as the low-noise condition by Condi-tion 1, its safe to say that as the learning proceed, the scales of bk WxQ(t)bk, bk WxK(t)bk would completely",
  "2m))": "On the other hand, we see that the maximum gradient F norm on a single batch comes from the maximumchanges of the bk WxQ(t)bk2 (or bk WxK(t)bk2). As we see that the extreme case of the growing is everyconcept k [K1] has been fully learned such that even a batch full of the same concept can not let thecorresponding concepts feature grow. In this case, we see that the maximum gradient F norm should be at theorder of WxQ(t)F (or WxK(t)F ). Thus",
  "Remark 5. Worth noting that this upper bound, as well as the upper bound of WyO(i,)(t)qk /qk in Lemma": "41, are looser in the order of K12 and K11 compared to those of (t)Q,k = (t)K,k and e(t)O(i,),k in Lemma 4.This suits the intuition and statistics since in the practical training setting, for B 1 we can see that sometimesthe samples of a batch all belong to one concept, or sometimes their are not any particular concept in a singlebatch, especially when B is small. Therefore, unless we have the situation where even when every prompt sampleof a batch belong to the same concept the regularization can stuck the growing, there is still chance for thatconcepts features to be learned. In contrast, the expectation considers every concepts sample appear in everybatch scaled by a soft weight in the order of (1/K). As the attention s gradient contain MLP, its orderwould be (1/K2). Besides, we see that this lemmas result contains the scale of L 1, which comes from theextreme case discussion where there is only one demonstration in each prompt sample that share the semanticto those of query. In contrast, when considering the expectation, the number of two opposite semantics is thesame, under which the L/2 would be eliminated in the numerator and denominator. Last but not least, whenestimating the real cases, we have scaled the derivative of to its maximum 1, we do so because in real casesdue to the imbalanced prompt samples in a single batch, it would be inconvenient to consider it is contributed byseverel elements like e(t)O(i,),k, (t)O(i,),k. This actually indirectly demonstrates the superiority of consideringexpectations.",
  "Lemma 43. (Restatement of Proposition 2) t T, when (t) E((t))F holds, we haveL01D ((t)) = L01D (E((t))). Here, 2F := WxQ2F + WxK2F + WyO2F": "Proof. By Lemma 33, we see that our convergence of 0-1 loss is based on the intermediate result thatE[Ak,et] , which will ensure that E[ySn f(E(Sn), E(T ))] /2. Therefore, when conditionedon E[WxQ(t)], E[WxK(t)], a minimum admissible disparity between WyO(t) and E[WyO(t)] corresponds the theminimum admissible disparity between WyO(i,)(t)ck, WyO(i,)(t)dk and (t)O(i,),k, (t)O(i,),k, where would",
  "Proof. We provide the proof by extending the techniques in to Hilbert-Schmidt space, whose innerproduct is defined by trace. First we note that 0 =2": "+1 min{1/(LLogist + ), 1/2}, where LLogist is theL-smooth Lipschitz constant of cross-entropy loss (), which is 1. The first statement can be shown as follows.Since by definition we see that W(t) = Wt(t), we only need to check the maximum disparity of the gradient ina single iteration update, then by Lemma 41 and Lemma 42 we readily obtain the results.",
  "KOut-of-Distribution Generalization": "Lemma 45. OOD 1: Master of Polysemy of Words. During testing, The prompt length L can be anypositive integer. The Dz can have any new probability distribution that differs from the training distribution,satisfying that each prompt has at least one co-concept k [K1], with equal chance to have positive ornegative semantic labels. Additionally, a single (x, y) Dx Dy pair can appear in at least z0 concept-specific prompts/tasks. Importantly, all of the tasks in this new distribution D enjoy Bayes-Optimal test errorL01D ((T )) . This lemma demonstrate the strong OOD Generalization ability of transformer utilizing multi-concept semantics,suggesting the efficiency transformer to conduct unseen ICL tasks just by its learned knowledge on the twonon-orthogonal dictionaries. Also, this lemma showcases an intriguing phenomenon since it allows multipleconcepts with comparable chance along word-demo pairs - even with the same input-output pair and query,the model can produce diverse responses when provided varying contextual (concept / task) information. Forinstance, with the prompt Japan: Sakura; China:, the LLM may output Penoey (national flower) or Panda\"(national symbol), reflecting different conceptual (task) interpretations. Both answers are right since they are allthe co-concept tasks. Interestingly, adding another demonstration like Japan: Sakura, France: Iris germanica,China: stabilizes the response to Penoey, since the only co-concept is left to be national flower. In ourtheory, we make an elementary explanation to this flexible, context-sensitive in-context learning (ICL) behaviorby attributing it to the transformers ability to harness multi-concept semantics.Lemma 46. OOD 2: Innovation. During testing, the distribution of Dx Dy can enjoy data shift. Specifically,suggest we now have a new M and Q to define new Dx, Dy. Specifically, k = k [K1], k2 [K2], we let",
  "enjoys Bayes-Optimal test error L01D ((T ))": "This lemma suggest that transformer-mlp structure empower ICL ability in solving task involving semantics(knowledge) originally from other co-concept prompts training distribution. This cross-concept semanticunderstanding ability ensure the transformer perform an specific OOD ability. For example, when we show a prompt Isaac Newton:Today I designed a machine to capture sunlight; ThomasEdison: to GPT o1, we would obtain an answer Today I invented a lamp that shines without fire. Duringtraining, even when the concept Inventors and Their Inventions may not co-appear with the concept Fabricatea story with high chance, the transformers empower the ICL to perform this interesting Out-of-Distribution task.We believe this can serve as an attempt to explain the innovation power of LLM grounded in the lineargeometric property of LLM representation, since most of the innovative outcomes of human being generatesfrom cross-concept Knowledge Intersection, and as it is not an easy task for human specialist to mastercross-domain knowledge, we claim that LLM can help innovation by leveraging cross-domain knowledge whendeduction over unseen structured task. Similarly, for multi-model scenarios, have shown that compositingdifferent concepts did enable OOD generalization (e.g. blue square apples in the a in ). This lemma seeks to elementarily explain why LLMs ICL can excel in complex tasks when using evolutionarystrategies, especially when the LLMs latent representation based on language only partially captures the relevantfeatures. Such tasks include algorithm design , heuristics , acquisition functions , and solutions tocombinatorial optimization problems . Although the resulting solutions may often seem counterintuitive tohuman experts, a possible explanation is that transformers can perform ICL in OOD scenarios by leveragingweighted combinations of their updated understanding (i.e., changing the identified underlying concepts in theevolution process) of new demo-query pairs, such as randomly sampled TSP instances. These understandings arerooted in the latent structures of the problem instances and can be effectively updated by evolutionary strategiesthat selectively refine and discard certain outcomes. Proof. Proof of Proposition 1. By Proposition 2, we only need to check the expected 0-1 loss L01D (E[]) = 0.Denote E[MySn ] [2K1] as the expected index set denoting the expected shared concept-specific features bythe query and one demonstration. By definition in the Lemma, as the semantic combination is conic combination,we see that E[MySn ] will be either a collection of odd (corresponding to positive label) or even (correspondingto negative label) numbers, and all of the combination of the features and labels in one prompt are correspondingto the same real value label without self-conflict. By Lemma 38, we see that the coefficients are all at asubstantial scale at T . Then by the condition on z and Eq. (31), we can readily check that even when theprobability of the fraction of demonstrations sharing the co-concept label semantic with query is feeble (but atleast one), utilizing the same set of notations, we still have"
}