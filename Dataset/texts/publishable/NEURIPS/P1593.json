{
  "Abstract": "Reconstructing a continuous surface from a raw 3D point cloud is a challengingtask. Recent methods usually train neural networks to overfit on single pointclouds to infer signed distance functions (SDFs). However, neural networks tendto smooth local details due to the lack of ground truth signed distances or normals,which limits the performance of overfitting-based methods in reconstruction tasks.To resolve this issue, we propose a novel method, named MultiPull, to learnmulti-scale implicit fields from raw point clouds by optimizing accurate SDFsfrom coarse to fine. We achieve this by mapping 3D query points into a setof frequency features, which makes it possible to leverage multi-level featuresduring optimization. Meanwhile, we introduce optimization constraints from theperspective of spatial distance and normal consistency, which play a key rolein point cloud reconstruction based on multi-scale optimization strategies. Ourexperiments on widely used object and scene benchmarks demonstrate that ourmethod outperforms the state-of-the-art methods in surface reconstruction. Projectpage:",
  "Introduction": "Reconstructing surfaces from 3D point clouds is an important task in computer vision. It is widelyused in various real-world scenarios such as autonomous driving, 3D scanning and other downstreamapplications. Recently, using neural networks to learn signed distance functions from 3D point cloudshas made huge progress . An SDF represents a surface as the zero-level set of a 3D continuousfield, and the surface can be further extracted using the marching cubes algorithm . In supervisedmethods , a continuous field is learned using signed distance supervision. Some methodsemploy multi-level representations , such as Fourier layers and level of detail (LOD) ,to learn detailed geometry. However, these methods require 3D supervision, including ground truthsigned distances or point normals, calculated on a watertight manifold. To address this issue, severalunsupervised methods were proposed to directly infer an SDF by overfitting neural networkson a single point cloud without requiring ground truth signed distances and point normals. Theyusually need various strategies, such as geometric constraints and consistency constraints, for smoother and more completed signed distance field. However, the raw point cloud is ahighly discrete approximation of the surface, learning SDFs directly from the point cloud is often",
  "(d) Reference(c) Step 3(b) Step 2(a) Step 1": ": Visualization of the 3D shape reconstruction. In (a), (b) and (c), SDFs are learned from apoint cloud by optimizing multi-level query points at multi-step. At each step, we optimize querypoints at one level with frequency features at this specific level as conditions. This enables thenetwork to progressively recover coarse-to-fine geometry details.",
  "inaccurate and highly ambiguous. This makes it hard for the network to learn accurate SDFs on localdetails, resulting in over-smooth reconstruction": "To address this issue, we propose MultiPull, to learn an accurate SDF with multi-scale frequencyfeatures. It enables network to predict SDF from coarse to fine, significantly enhancing the accuracyof the predictions. Furthermore, to optimize the SDF at different scales simultaneously, we introduceconstraints on the pulling process. Specifically, given query points sampled around 3D space as input,we use a Fourier transform network to represent them as a set of Fourier features. Next, we designa network that can leverage multi-scale Fourier features to learn an SDF fields from coarse to fine.To optimize the signed distance fields with multi-scale features, we introduce a loss function basedon gradient consistency and distance awareness. Compared with Level of Detail (LOD) methods, we can optimize the signed distance fields effectively without a need of signed distancesupervision, recovering more accurate geometric details. Evaluations on widely used benchmarksshow that our method outperforms the state-of-the-art methods. Our contribution can be summarizedas follows.",
  "Related Work": "Classic methods for geometric modeling have attempted to analyze the geometric modelingof objects, which do not require large-scale datasets. With the advent of extensive and intricate3D datasets like ShapeNet and ABC , learning-based methods have achieved significantadvancements . These approaches learn implicit representations fromvarious inputs, including multi-view images, point clouds , and voxels . Learning Implicit Functions with Supervision. Supervised methods have made significant progressin recent years. These methods leverage deep learning networks to learn priors from datasets oruse real data for supervision to improve surface reconstruction performance. Somesupervised approaches use signed distances and point normals as supervision, or leverage occupancylabels to guide the networks learning process. In order to improve the generalization ability of neuralnetworks and learn more geometric details, some studies learn geometry prior of shapes throughsupervised learning. Learning Implicit Functions with Conditions. To alleviate the dependence on supervised infor-mation, recent studies focus on unsupervised implicit reconstruction methods. These methods donot require pretrained priors during optimization. For example, NeuralPull (NP) learns SDFby pulling query points in nearby space onto the underlying surface, which relies on the gradientfield of the network. CAP further complements this by forming a dense surface by additionallysampling dense query points. GridPull generalizes this learning method to the grid, by pullingthe query point using interpolated signed distances on the grid. In addition, some studies exploresurface reconstruction more deeply and propose innovative methods, such as utilizing differentiablePoisson solutions , or learning signed or unsigned functions with priors.However, inferring implicit functions without 3D supervision requires a lengthy convergence process,which limits the performance of unsupervised methods on large-scale point cloud datasets. To addressthis, we propose a fitting-based frequency feature learning strategy that efficiently learns implicitfields without the need for additional supervision. Learning Implicit Functions with LOD. Level-Of-Detail (LOD) models are used tosimplify code complexity and refine surface details through the architecture of multi-level outputs.Previous studies have explored multi-scale architectures in various reconstruction tasks. For example,NGLOD uses octree-based feature volumes to represent implicit surfaces, which can adapt toshapes with multiple discrete levels of detail and enable continuous level-of-detail switching throughSDF interpolation. MFLOD applies Fourier layers to LOD, which can offer better feasibility inFourier analysis. However, it is difficult to optimize multi-level features simultaneously to learn 3Dshapes. To address this issue, we propose a novel strategy to optimize multi-level frequency features,allowing the network to progressively learn geometric details from coarse to fine.",
  "Method": "Overview. The overview of MultiPull is shown in . We design a neural network to learn animplicit function f from a single 3D point cloud by progressively pulling a set of query points Q0onto the underlying surface, where Q0 is randomly sampled around the raw point cloud S. Ournetwork mainly consists of two parts as follows. (1) The Frequency Feature Transformation (FFT) Module ( (a)) aims to convert the querypoints Q0 into a set of multi-level frequency features Y = {yi, i [0, NL 1]}. The key insight forintroducing frequency features lies in a flexible control of the degree of details. (2) The Multi-StepPulling (MSP) Module ((b)) is designed to predict f with coarse-to-fine details under theguidance of frequency features Y . At the i-th step, we pull Qi to Qi+1 using the predicted signeddistances si = f(Qi, yi) and the gradients at Qi, according to its feature yi. To this end, we constrainquery points to be as close to their nearest neighbor point on S.",
  "where i and i are the parameters of the network, and NL is the number of layers of the network": "To effectively represent the expression of the raw input in the frequency space, we choose the sinefunction as the activation function and employ the Hadamard product to compute the intermediatefrequency feature output. Since the Hadamard product allows the representation of frequencycomponents as the product of two feature inputs, denoted as a and b, which can be formulated as:",
  "where indicates the Hadamard product, Wi, bi are parameters of the network": "Frequency networks based on the Multiplication Filter Network (MFN) typically employ uniformor fixed-weight initialization for network parameters in practice. This approach overlooks the issueof gradient vanishing in deep network layers during the training process, leading to underfittingand making the network overly sensitive to hyperparameter changes. To address this challenge, wepropose a new initialization scheme that thoroughly considers the impact of network propagation,aiming at ensuring a uniform distribution of initial parameters. Specifically, we dynamically adjustinitial weights, which can be formulated as:",
  "sin(i/NL), i [1, NL 1],(4)": "where NL and are the number of layers and the parameters of the network, respectively. We leveragethe standard deviation as the initialization range to ensure that the parameters in Eq. (3) are within areasonable range. As shown in , we compared the parameter distributions of different linearlayers. The initialization scheme based on MFN results in gradient vanishing and small activations indeeper linear layers. In contrast, our initialization scheme ensures that the parameters of each linearlayer follow a standard normal distribution.",
  "In (b), we demonstrate our idea of learning an accurate implicit function f with multiplefrequency features. Given a set of frequency feature Y , we use frequency features yi in Y as the": "input along with query points Qi for the MSP module. We follow NP to construct initial querypoints and calculate the stride and direction of Qi at i-th step for pulling it to the target surface point.Furthermore, we use the direction of the gradient as f(Qi, yi) and signed distance f(Qi, yi) forthe pulling, where f(Qi, yi) represents the fastest increase in signed distance in 3D space, pointingaway from the surface direction. Therefore, Qi = Qi1 f(Qi1, yi1) f(Qi1, yi1)/ f(Qi1, yi1) 2. For each step of pulling the query points Qi, it corresponds to a nearest pointqi on the surface, and the distance between query points and surface points can be described asDi = ||Qi qi||22. Based on this, we initiate the optimization by pulling query points Qi the targetpoints qi progressively. Therefore, we can obtain the combined loss Lpull under optimal conditions:",
  "i=1Di, i [1, I],(5)": "where I is the step of moving operation. However, optimizing all query points accurately throughthis equation alone is challenging when merely constraining surface points. This is because thequery points Qi may be located across multiple spatial scales with inconsistent gradient directions,indicating that simultaneous optimization becomes challenging. Consequently, some outlier pointsmay not be effectively optimized. Additionally, for sampling points near target points, some surfaceconstraints are required to enable the network to accurately predict their corresponding zero level-set to avoid optimization errors. Therefore, we will further advance Eq. (5) from the perspectivesof distance constraints, gradient consistency, surface constraints in Sec. 3.3 to enhance networkperformance.",
  "Loss Function": "Distance-Aware Constraint.Inspired by FOCAL , we design a novel constraint withdistance-aware attention weights to ensure that the network pays more attention to the optimizationof underfitting query points in space and optimizes the SDFs simultaneously. This allows querypoints at different distances from the surface to be optimized properly, and assigns higher attentionweights for outlier and underlying points:1, 2 = softmax(D1, D2),",
  "Lrecon = 1D1 + 2D2 + D3,(6)": "where 1 and 2 are calculated from D1, D2 by the softmax activation, is a scaling coefficient weset to 2 by default. Here, we only consider 3 steps, which is a trade-off between performance andefficiency. Consistent Gradients. We additionally introduce consistency constraints in the gradient direction.This loss encourages neighboring level sets to keep parallel, which reduces the artifacts off the surfaceand smooths the surface. We add a cosine gradient consistency loss function to encourage the gradientdirection at the query points to keep consistent with the gradient direction at its target point on thesurface, which aims to improve the continuity of the gradient during the multi-step pulling. We useQ1, Q2 and Q3 to represent the query points that have been continuously optimized by the multiplesteps. We take the one with the lowest similarity score to measure the overall similarity.L(Qi) = cos(f(Qi, yi), f(Q0, y0)),Lgrad = 1 min{L(Q1), L(Q2), L(Q3)},(7)",
  "Experiments": "In this section, we evaluate the performance of MultiPull in surface reconstruction by conductingnumerical and visual comparisons with state-of-the-art methods on both synthetic and real-scandatasets. Specifically, in Sec. 4.1, we experiment on synthetic shape datasets with diverse topologicalstructures. Furthermore, in Sec. 4.2, we report our results across various scales on real large-scalescene datasets. Meanwhile, we consider FAMOUS as the verification dataset in the ablation studiesto compare the effectiveness of each module in MultiPull in Sec. 4.3.",
  "Surface Reconstruction for Shapes": "Datasets and Metrics. For the single shape surface reconstruction task, we perform evaluationon multiple datasets including ShapeNet , FAMOUS , Surface Reconstruction Benchmark(SRB) Thingi10K and D-FAUST . We conduct validation experiments on 8 subcate-gories within the ShapeNet dataset, while the remaining datasets are experimented on the completedataset. For metric comparison, we leverage L1 and L2 Chamfer Distance CDL1 and CDL2, NormalConsistency (NC), and F-Score as evaluation metrics. ShapeNet.We evaluate our approach on the ShapeNet according to the experimental settings ofGP . We compared our methods with methods including ATLAS , DSDF , NP , PCP, GP , as shown in Tab. 1. We report CDL2, NC and F-Score metrics for ShapeNet, wherewe randomly sample 10,000 points on the reconstructed object surface for evaluation. MultiPulloutperforms the state-of-the-art methods. Compare to previous gradient-based methods in ,our method performs better by revealing more local details of these complex structures. We providedetailed results in Appendix C.",
  "FAMOUS. We evaluate the performance of our method on the FAMOUS dataset according to theexperimental settings of PCP and NP . Our method demonstrates superiority over recent": "approaches, including GP , PCP , GenSDF , FGC , NP , and IGR . Asshown in Tab. 2, we compared the recent methods using CDL2 and NC metrics, and our methodexhibits outstanding performance. To demonstrate the effectiveness of our method in reconstructionaccuracy, we visualize the error-map for comparison in . Compare to the the state-of-artmethods, our method has better overall reconstruction accuracy (bluer).",
  ": Visual comparison on SRB": "SRB. We validate our method on the real scanned dataset SRB, following the experimental settingsof VisCo and GP . In Tab. 3, we compared our approach with recent methods includingP2M , SAP , NP , BACON , CAP , GP . We use CDL1 and F-Score toevaluate performance , and we surpass all others in terms of these metrics. As depicted in , ourmethod excels in reconstructing more complete and smoother surfaces. D-FAUST. We evaluate our method on the D-FAUST dataset with SAP settings. As indicatedin Tab. 4, we compared our approach with recent methods including IGR , SAP , GP .Our method excels in CDL1, F-Score and NC. As illustrated in , compared to other methods,our approach demonstrates superior accuracy in recovering human body shapes. Thingi10K. We assess the performance of our approach on the Thingi10K dataset, following theexperimental setup of SAP . We compared our approach with recent methods including IGR ,SAP , BACON , GP . As indicated in Tab. 5, our method surpasses existing methodsacross in CDL1, F-Score and NC metrics. As illustrated in , our method can reconstruct surfaceswith more accurate details.",
  "Surface Reconstruction for Real-Scan Scenes": "Datasets and Metrics.For the scene reconstruction task, we validate our method on the 3DScene and KITTI datasets to assess the performance on large-scale datasets. We keep the sameevaluation metrics as those used for shape reconstruction in Sec. 4.1. 3DScene. In accordance with the experimental settings of PCP , we compared our approach withrecent methods including ConvOcc , NP , PCP and GP . We report the evaluationresults of CDL1, CDL2 and NC, and compared our method with the latest approaches listed in Tab. 6.As illustrated in , our method outperforms prior-based methods and overfitting based methods.",
  ": Visual comparison on KITTI": "KITTI. We validate our method on the large-scale scanned point cloud dataset KITTI , whichcontains 13.8 million points. As shown in , our approach is capable of reconstructing morecomplete and accurate surfaces compared to the GP method . GP struggles to reconstruct contin-uous surfaces such as walls and streets, whereas our method achieves a more detailed reconstructionof objects at various scales in real scanned scenes. It demonstrates that our method is robust whenhandling point cloud with various scales.",
  "Ablation Experiments": "Effect of Frequency Layers. We denote the j-th layer of the frequency network as Lj, a specificcombination of frequency feature layers can be formulated as {Li, Lj, Lk}, where {i, j, k} [1, NL 1]. We evaluate the effectiveness of the frequency transformer layers in Tab. 7 with CDL2and NC, replacing the frequency network with linear layers results in a decrease in the performanceof the CDL2 and NC metrics. The performance of using only one layer(L4) surpasses linear layers.With the increase of the frequency layers, {L4,L6,L8} produces best results.",
  "Linear0.0420.920L40.0400.926L4, L60.0370.933L4, L6, L80.0360.948": "Effect of MSP Module. We report comparisons with different features in Tab.8. The Layercolumn denotes the combination of frequency features obtained by the FFT module. For instance,{L4, L6, L8} represent the frequency features from the 4th, 6th, and 8th layers guiding the pulling ofthe query point in the MSP network, respectively. We find that the accuracy of the network increaseswith the number of steps. After considering both performance metrics and time efficiency, we haveset Step=3 by default.",
  "Conclusion": "We propose a novel method to learn detailed SDFs by pulling queries onto the surface at multi-step.We leverage the multi-level features to predict signed distances, which recovers high frequencydetails. Through optimization, our method is able to gradually restore the coarse-to-fine structure ofreconstructed objects, thereby revealing more geometry details. Visual and numerical comparisonsshow that our approach demonstrates competitive performance over the state-of-the-art methods.",
  "William E Lorensen and Harvey E Cline. Marching cubes: A high resolution 3D surface constructionalgorithm. ACM Siggraph Computer Graphics, 21(4):163169, 1987": "Philipp Erler, Paul Guerrero, Stefan Ohrhallinger, Niloy J Mitra, and Michael Wimmer. Points2Surflearning implicit surfaces from point clouds. In European Conference on Computer Vision, pages 108124.Springer, 2020. Chiyu Jiang, Avneesh Sud, Ameesh Makadia, Jingwei Huang, Matthias Niener, Thomas Funkhouser,et al. Local implicit grid representations for 3D scenes. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, pages 60016010, 2020. Julien NP Martel, David B Lindell, Connor Z Lin, Eric R Chan, Marco Monteiro, and Gordon Wetzstein.ACORN: adaptive coordinate networks for neural scene representation. ACM Transactions on Graphics(TOG), 40(4):113, 2021.",
  "Rizal Fathony, Anit Kumar Sahu, Devin Willmott, and J Zico Kolter. Multiplicative filter networks. InInternational Conference on Learning Representations, 2020": "David B Lindell, Dave Van Veen, Jeong Joon Park, and Gordon Wetzstein. BACON: Band-limitedcoordinate networks for multiscale scene representation. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, pages 1625216262, 2022. Towaki Takikawa, Joey Litalien, Kangxue Yin, Karsten Kreis, Charles Loop, Derek Nowrouzezahrai,Alec Jacobson, Morgan McGuire, and Sanja Fidler. Neural geometric level of detail: Real-time renderingwith implicit 3d shapes. In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1135811367, 2021. Yishun Dou, Zhong Zheng, Qiaoqiao Jin, and Bingbing Ni. Multiplicative fourier level of detail. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18081817,2023.",
  "Matan Atzmon and Yaron Lipman. SALD: Sign agnostic learning with derivatives. In InternationalConference on Learning Representations, 2020": "Kyle Genova, Forrester Cole, Avneesh Sud, Aaron Sarna, and Thomas Funkhouser. Local deep implicitfunctions for 3D shape. In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 48574866, 2020. Baorui Ma, Zhizhong Han, Yu-Shen Liu, and Matthias Zwicker. Neural-Pull: Learning signed distancefunction from point clouds by learning to pull space onto surface. In International Conference on MachineLearning, pages 72467257. PMLR, 2021. Baorui Ma, Junsheng Zhou, Yu-Shen Liu, and Zhizhong Han. Towards better gradient consistency forneural signed distance functions via level set alignment. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, pages 1772417734, 2023. Chao Chen, Yu-Shen Liu, and Zhizhong Han. GridPull: Towards scalability in learning implicit represen-tations from 3D point clouds. In Proceedings of the IEEE/CVF International Conference on ComputerVision, pages 1832218334, 2023. Danhang Tang, Mingsong Dou, Peter Lincoln, Philip Davidson, Kaiwen Guo, Jonathan Taylor, SeanFanello, Cem Keskin, Adarsh Kowdle, Sofien Bouaziz, et al. Real-time compression and streaming of 4dperformances. ACM Transactions on Graphics (TOG), 37(6):111, 2018. Fausto Bernardini, Joshua Mittleman, Holly Rushmeier, Cludio Silva, and Gabriel Taubin. The ball-pivoting algorithm for surface reconstruction. IEEE Transactions on Visualization and Computer Graphics,5(4):349359, 1999.",
  "Heewoo Jun and Alex Nichol. Shap-e: Generating conditional 3d implicit functions. arXiv preprintarXiv:2305.02463, 2023": "Tim Brooks, Aleksander Holynski, and Alexei A. Efros. InstructPix2Pix: Learning to follow image editinginstructions. In Proceedings of the IEEE/CVF Conference on Computer Vsion and Pattern Recognition,2023. Zhaoshuo Li, Thomas Mller, Alex Evans, Russell H Taylor, Mathias Unberath, Ming-Yu Liu, and Chen-Hsuan Lin. Neuralangelo: High-fidelity neural surface reconstruction. In IEEE Conference on ComputerVision and Pattern Recognition, 2023. Biao Zhang, Jiapeng Tang, Matthias Niessner, and Peter Wonka. 3dshape2vecset: A 3d shape representationfor neural fields and generative diffusion models. ACM Transactions on Graphics (TOG), 42(4):116,2023. Siyu Ren, Junhui Hou, Xiaodong Chen, Ying He, and Wenping Wang. Geoudf: Surface reconstructionfrom 3d point clouds via geometry-guided distance representation. In Proceedings of the IEEE/CVFInternational Conference on Computer Vision, pages 1421414224, 2023.",
  "Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. Volume rendering of neural implicit surfaces. InAdvances in Neural Information Processing Systems, 2021": "Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan Atzmon, Basri Ronen, and Yaron Lipman.Multiview neural surface reconstruction by disentangling geometry and appearance. Advances in NeuralInformation Processing Systems, 33, 2020. Liang Han, Junsheng Zhou, Yu-Shen Liu, and Zhizhong Han. Binocular-guided 3D gaussian splatting withview consistency for sparse view synthesis. In Advances in Neural Information Processing Systems, 2024. Wenyuan Zhang, Yu-Shen Liu, and Zhizhong Han. Neural signed distance function inference throughsplatting 3d gaussians pulled on zero-level set. In Advances in Neural Information Processing Systems,2024. Francis Williams, Teseo Schneider, Claudio Silva, Denis Zorin, Joan Bruna, and Daniele Panozzo. Deepgeometric prior for surface reconstruction. In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 1013010139, 2019.",
  "Gal Metzer, Rana Hanocka, Denis Zorin, Raja Giryes, Daniele Panozzo, and Daniel Cohen-Or. Orientingpoint clouds with dipole propagation. ACM Transactions on Graphics (TOG), 40(4):114, 2021": "Rui Xu, Zhiyang Dou, Ningna Wang, Shiqing Xin, Shuangmin Chen, Mingyan Jiang, Xiaohu Guo,Wenping Wang, and Changhe Tu. Globally consistent normal orientation for point clouds by regularizingthe winding-number field. ACM Transactions on Graphics (TOG), 42(4):115, 2023. Qing Li, Huifang Feng, Kanle Shi, Yue Gao, Yi Fang, Yu-Shen Liu, and Zhizhong Han. Shs-net: Learningsigned hyper surfaces for oriented normal estimation of point clouds. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition, pages 1359113600, 2023. Junsheng Zhou, Baorui Ma, Liu Yu-Shen, Fang Yi, and Han Zhizhong. Learning consistency-awareunsigned distance functions progressively from raw point clouds. In Advances in Neural InformationProcessing Systems (NeurIPS), 2022. Songyou Peng, Chiyu Max\" Jiang, Yiyi Liao, Michael Niemeyer, Marc Pollefeys, and Andreas Geiger.Shape as points: A differentiable poisson solver. In Advances in Neural Information Processing Systems,2021.",
  "Amos Gropp, Lior Yariv, Niv Haim, Matan Atzmon, and Yaron Lipman. Implicit geometric regularizationfor learning shapes. In International Conference on Machine Learning, pages 37893799. PMLR, 2020": "Wenbin Zhao, Jiabao Lei, Yuxin Wen, Jianguo Zhang, and Kui Jia. Sign-agnostic implicit learning ofsurface self-similarities for shape modeling and reconstruction from raw point clouds. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1025610265, 2021. Yu-Tao Liu, Li Wang, Jie Yang, Weikai Chen, Xiaoxu Meng, Bo Yang, and Lin Gao. Neudf: Leaningneural unsigned distance fields with volume rendering. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, pages 237247, 2023. Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollr. Focal loss for dense objectdetection. In Proceedings of the IEEE international conference on computer vision, pages 29802988,2017.",
  "Gene Chou, Ilya Chugunov, and Felix Heide. GenSDF: Two-stage learning of generalizable signed distancefunctions. In Proc. of Neural Information Processing Systems (NeurIPS), 2022": "Mulin Yu and Florent Lafarge. Finding good configurations of planar primitives in unorganized pointclouds. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),pages 63676376, June 2022. Albert Pumarola, Artsiom Sanakoyeu, Lior Yariv, Ali Thabet, and Yaron Lipman. Visco grids: Surfacereconstruction with viscosity and coarea grids. In Advances in Neural Information Processing Systems,2022.",
  "AImplementation Details": "Our network consists of two main parts: frequency feature transformation and multi-step pullingmodules in Fig 2 (a) and (b), respectively. For frequency feature transformation, we transform theraw point cloud into frequency features M, where M is initialized to 256. Same for the multi-steppulling module, we train a linear sequence neural network (LSNN) with shared parameters and wefix intermediate layer output dimension to 512. In the construction of query points, we establishthe corresponding pairs between query points and their nearest points on surfaces. Specifically, wefollow NP to construct 40 queries for each point of the point cloud, the construction of thesequery points follows a Gaussian distribution. During the reconstruction process, we use the MarchingCubes algorithm to extract the mesh surface. During the training process, we do optimization in 40,000 iterations, with an average time of 24minutes for single-object reconstruction. We utilize a single NVIDIA RTX-3090 GPU for bothtraining and testing.",
  "BAdditional Experiments": "Effect of Frequency Features. To further validate the superiority of frequency features, we excludemulti-step pulling, and only use single-layer frequency feature for performance verification againstlinear layers. We note the frequency feature in the i-th layer as Li. We compared the CDL2 andNC of specific layers(L2, L4, L6, L8) with the linear layers. As shown the Tab. 11 the performanceof frequency features at different layers is superior to the linear layers, and with an increase in thenumber of layers, higher-level frequency conditions enhance the networks performance.",
  "Linear0.0420.920L40.0400.926L60.0380.931L80.0370.935": "Effect of Initialization Strategies. We compared our initialization strategy with random initializationand MFN-based method (BACON ) as example. We compared the metrics of these initializationmethods in Tab. 12, which shows that combining random or BACON initialization with our approachdoes not yield satisfactory results.To further demonstrate the advantages of our initialization method,we visually compared SDF with random initialization and BACON initialization strategies. Asshown in the , our method significantly outperforms other initialization methods in terms ofconvergence speed. In addition, our reconstruction results also indicate that a reasonable initializationmethod can enable the network to learn more accurate signed distance field. We compared theresults with the same iterations and the final results under the default settings for different methods(Final) in , these failed reconstructions based on MFN demonstrate the instability of parameterinitialization.",
  "Random0.0420.938BACON0.0380.946Ours0.0350.950": "Effect of Parameters on Networks. We compared the parameter quantities of the methods listedin Tab. 13 below. It shows that the parameter number of PCP is the largest among all thethree methods, while NP has the least parameters. To further investigate the performance ofnetworks with the similar amount of parameters, we increase the parameters of NP and MultiPull tomatch PCP. The comparison in the Tab. 14 indicates that both NP and MultiPull show the improvedperformance. This demonstrates that more parameters are beneficial to improve the performance, but",
  "Inference Time(min)0.170.1380.141": "Design of Lgrad.We further discuss the design of Lgrad and effectiveness on performance. We useminimum (min) as the baseline and compare it with using the average (avg). As shown in Tab.16, using Lgrad(avg) to calculate the similarity of query points at different time steps results in aslight increase in CD error. In contrast, Lgrad(min) achieves a similar level of similarity but betterperformance in CD metrics. Therefore, we calculate minimum of the gradient similarities as aconstraint to prevent significant deviations in the moving direction during training, making thenetwork more sensitive to changes in gradient direction.",
  "NC0.9450.9500.954": "Feature Comparison. We combine MSP with the linear layers and the traditional feature learningencoder PointMLP to explore the effectiveness of MSP. We present the results of combining differentfeature learning networks with MSP methods in Tab. 17. We denote the single moving operationin MSP as Pull and use multiple feature learning networks as baselines. Due to the lack of an FFTmodule, the same features are used for multiple offsets in linear+MSP and PointMLP + MSP. Asshown in Tab. 17, the combination of different feature extraction networks and MSP achieved betterresults in terms of both CD and NC metrics. We further demonstrate the effectiveness of MSP in and in the PDF. PointMLP / linear+MSP can generate finer local details compared toPointMLP / linear + Pull.",
  "NC0.3250.7650.7840.8470.851": "Computational Complexity. We report our computational complexity in Tab. 23, we presentnumerical comparisons with the latest overfitting-based methods, including NP and PCP, usingdifferent point counts, such as 20K and 40K. The benchmark rounds for both NP and PCP are setat 40K. NP does not require learning priors, resulting in the highest operational efficiency. PCPneeds to learn priors, which requires additional time. To achieve more refined results, we dedicateextra time to learning the frequency features of point clouds and computing the sampling pointstrides. Consequently, our speed is slower compared to NP. However, it is noteworthy that our methodoutperforms PCP and operates faster even without using local priors.",
  "DLimitation": "We propose a method that approximates the accurate signed distance field through multi-step opti-mization, achieving more precise results. However, there is still room for further optimization interms of time and computational efficiency as shown in Tab. 8 and Tab. 14. In future work, we willcontinue to explore how to integrate multi-resolution (such as NGLOD and Instant-NGP )features effectively to balance computational efficiency and accuracy."
}