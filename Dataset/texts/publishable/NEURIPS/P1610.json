{
  "Abstract": "Transformer-based autoregressive sampling has been the major bottleneck forslowing down large language model inferences. One effective way to accelerateinference is Speculative Decoding, which employs a small model to sample asequence of draft tokens and a large model to validate. Given its empirical effec-tiveness, the theoretical understanding of Speculative Decoding is falling behind.This paper tackles this gap by conceptualizing the decoding problem via markovchain abstraction and studying the key properties, output quality and inferenceacceleration, from a theoretical perspective. Our analysis covers the theoreticallimits of speculative decoding, batch algorithms, and output quality-inferenceacceleration tradeoffs. Our results reveal the fundamental connections betweendifferent components of LLMs via total variation distances and show how theyjointly affect the efficiency of decoding algorithms.",
  "Introduction": "The recent surge of scaling Transformer models has led to the flourishing of AI, where success hasbeen witnessed in wide areas such as natural language Touvron et al. , Achiam et al. ,computer vision Dosovitskiy et al. , Han et al. , video generations Arnab et al. ,Ho et al. , and robotics Brohan et al. , Shridhar et al. . In the meantime, thedecoding process also becomes more and more time-consuming as the model size scales up. This ismainly due to the autoregressive nature of Transformers, where each generated token also serves asthe input for future generations. As a result, decoding T tokens would take T forward passes of thefull model. A recent effort to tackle this challenge is speculative decoding (SD) Chen et al. [2023a], Leviathan et al., where the autoregressive sampling is performed on a small draft model and the large languagemodel verifies tokens generated by draft model to decide whether it should be accepted/rejected.Once a token is rejected, the generation process will start from the most recently accepted token,until a full response is completed. Speculative decoding achieves 2-2.5 LLM inference speedupempirically, while preserving the quality of generation. Subsequently, numerous studies Miao et al. , Liu et al. , Kim et al. , Zhou et al. have expanded this methodology, enabling further inference acceleration. Intuitively, forspeculative decoding, when the generation distribution of small model p and large model q are close",
  "In this paper, we answer these questions from the theoretical lens. Our contributions are summarizedas follows": "We formalize the decoding problem through the Markov Chain abstraction that establishes thetheoretical setup. We draw the connection runtime # rejections and use it to measure efficiency.We derive the exact formula, fully characterized by distribution p and q, for the expected rejectionsErNrejs for Speculative Decoding (Theorem 1). This renders a theoretical reference for understandingthe acceleration rate T{ErNrejs. Next, to understand whether Speculative Decoding can be further improved, we generalize it to aclass of rejection-based algorithms 2 where probability bt and distribution Pt can be customized.We prove in Theorem 2 that any unbiased algorithm cannot have fewer rejections than SpeculativeDecoding. This indicates its optimality among the class, and having fewer rejections needs to sufferquality loss or requires extra information. Furthermore, we consider a batch version of Speculative Decoding (Algorithm 4) that utilizes multipledraft sequences. We show our batch algorithm is unbiased. We derive the expected rejections that isfully expressed by p and q and exhibit the improvement over non-batch version in Theorem 3. Weprovide examples and detailed discussion to explain how our theory characterize the improvement. In section 4, we shift from unbiased algorithms and study the tradeoff between inference cost andquality degradation. We formulate this into an optimization model (1). Theorem 5 established alinear Pareto front that characterizes the tradeoff between inference cost and quality degradation(). A simple experiment in .2 is also consistent with our theoretical finding. Last but not least, our technical results involve novel analysis, for instance, the design of V`, V inthe lower bound proof C and the iterative computation for f in D.1. They are the first of its kind andconsist of our technical contributions. We provide a proof sketch section in Appendix A.",
  "Related works": "Speculative Decoding and its applications. Speculative execution, where performing speculativework can expedite computation on parallel machines by allowing certain tasks to commence beforetheir necessity is confirmed, can date back to Burton , Gabbay and Mendelson . Recently,Chen et al. [2023a], Leviathan et al. formalize this idea with rejection sampling based designfor LLM Decoding and achieve multiple-time inference acceleration compared to vanilla auto-regressive decoding. There are fruitful studies Xia et al. , Miao et al. , Liu et al. ,Sun et al. , Kim et al. , Zhou et al. , Spector and Re , Mitchell et al. ,He et al. , Bae et al. , Su et al. , Ahn et al. , Santilli et al. , Xu et al. , Chen et al. [2023b], Xia et al. , Yang et al. , Qian et al. , Sun et al. [2024a],Bergner et al. , Yan et al. , Wang et al. , Huang et al. since then, andthey improve speculative decoding from different angles such as online updating Liu et al. ,multiple candidates Yang et al. , retrieval technique He et al. , Multimodality Gagraniet al. or even decoding without draft models Fu et al. , Bhendawade et al. . Theoretical endeavor for Speculative Decoding. There are also works that study the theoreticalproperties for speculative decoding (SD). In particular, Sun et al. considers speculativedecoding from the optimal transport perspective and show it is optimal in the single token regime. Itfurther extends to the k multiple draft token setting and formulates the optimal transport solution vialinear programming with exponential in k computation time. An approximate sequential selectionalgorithm is also proposed with linear time. Ahn et al. further proposes the improved plan,and Sun et al. [2024b] extends Sun et al. to the block-level optimal transport for SD. Su et al. investigates the synergy between draft length and batch size for Speculative Decoding andformulate the optimal speculation length as the root of a polynomial equation. Miao et al. proposes the SpecInfer, a batch algorithm that uses small speculative models to form the token tree,and proves its output matches the distribution of the large model. Yang et al. considers batchspeculative decoding without replacement to avoid repeatedly sampling rejected tokens and proves itkeeps the decoding quality. Nevertheless, the findings for inference acceleration of these works aremostly empirical, lacking theoretical guarantees.",
  "In this section, we provide the mathematical formulation for decoding problems using Markov Chains,and we explain auto-regressive models and speculative decoding algorithm based upon that": "A Markov Chain Model for Decoding. We denote x0 as the prompt.2 xn is the n-th token outputand x1:T is the trajectory output by the algorithm with T to be the fixed decoding horizon.3. Thenany decoding algorithm can be characterized by a Markov Chain: state at t is described by historyx0:t, the initial state is x0; the state transition Pt maps state x0:t to state x0:t`1. In the context ofdecoding problem, the transition matrix is defined as Ppx0:t`1|x0:tq : ppxt`1|x1:tq. In particular,we use p to denote the (conditional) distribution for the draft model that resembles small speculativemodel, and q for the target model that represents large language model. We use rq ps` to denotethe normalized distribution for maxt0, qpxq ppxqu, @x P V with V being the token vocabulary. Wealso denote Exfrgs :",
  "x fpxqgpxq": "2We use the abstraction x0 : py1, y2, . . . , ynpromptq with yis being the tokens in the prompt.3In general, T is a random variable. However, we can append [EOS] tokens to keep the output length fixed. Auto-Regressive Decoding. Consider sampling a trajectory x1:T from an auto-regressive model,where for the given x1:n1, the next token xn follows the conditional distribution q. This decodingmechanism (Algorithm 3) is the prototype for Transformer-based LLMs (e.g. GPT-4). As mentionedin Qian et al. , the accuracy performance of Transformer-based LLMs has been shown toscale with model size, with larger models demonstrating improved capabilities Kaplan et al. .However, this improvement comes at the cost of higher latency during inference and increasedcomputational requirements. Speculative Decoding. Different from large models, small models are usually much faster at theinference stage. Consequently, we can use a small model to perform auto-regressive samplingand assign large model as a verifier, where the goal of the large model is to check whether thetoken sampled by the small model should be accepted/rejected. Concretely, this procedure can besummarized into the following three steps:",
  "Conditional score computation: given xn:n`K1, computing the logits of the K tokensqp|x1:n1, xn:n`iq, i P rK 1s in parallel;": "Token validation: Accept the candidate token xt (as xn) with some probability 0 bt 1.If accepted, continue to validate the next candidate xt`1, otherwise reject and sample xnfrom some designed distribution Pt. The above process repeats until x1:T are generated, and the whole algorithm is summarized asthe general rejection-based decoding 2. Specifically, Speculative Decoding Chen et al. [2023a],Leviathan et al. designs btpxtq : mint1, qtpxtq",
  "Problem Setup": "Speculative Decoding has two key merits. First, it maintains output quality, meaning the outputdistribution by the algorithm is identical to the output distribution of the large model q, and we term itdistribution unbiasedness. Second, it has fast inference property since the auto-regressive samplingis performed on the small model and the target model only verifies. With (possibly) multiple drafttokens being accepted very round, Speculative Decoding can be much faster than direct decoding onlarge models, and its inference is bottlenecked by the parallel score computation qp|x1:n1, xn:n`iq,i P rT ns. To highlight this, we defined it as the oracle call and make an assumption based on that.",
  "Assumption 1. We assume that: (1) compared to the large model, the computational cost of thedraft/small model is negligible. (2) each oracle call has runtime Op1q": "Remark 1. We assume the above only for the theoretical cleanliness. With negligible draft model, thelookhead K T in Algorithm 2. In other words, given x1:n1, instead of sampling the next K drafttokens xn:n`K1, we are allowed to sample until the end, i.e. xn:T . In practice, Assumption 1(1)also holds true in many cases. One example of negligible-cost model c 0 is n-gram models, andthe empirical evidence in Leviathan et al. , Ou et al. shows n-gram draft model speedsup inference pretty well. In addition, for summarization tasks where long sequences are likely torepeat, any draft model that reuses tokens from the context with a matching prefix, is also costnegligible. Assumption 1(2) is also a standard abstraction, since parallelization consumes (roughly)equal computation as for a single logit. It will consume more memory, but such aspect is beyond thescope of our theoretical study.",
  "Output quality: the distribution bias between the algorithm and the large model distributionq. An algorithm maintains the output quality if it is distribution unbiased": "Rejections, and why consider it? A third metric for decoding is the number of draft tokens beingrejected. With more draft tokens being accepted, the fewer rejections would occur. Therefore,algorithms with large number of rejections are slower than those with fewer rejections. This meansRejections serves as an alternative metric for the inference speedups. Throughout the paper, we usenumber of rejections for measuring inference acceleration. To further motivate why choosing Rejections is appropriate, we draw its connection to inference run-time. For Speculative Decoding, the runtime is dominated by the number of oracle calls (Definition 1).After each rejection (Step 10 of 2 or Step 7 of 1), there is one oracle call, thus we obtain",
  "(2) The output distributions of Algorithm 1 and the target model q are identical, i.e. for any outputsequence x1:T P VT , the joint the distributions over x1:T satisfies: PSDpx1:T q qpx1:T q": "The first part of Theorem 1, to our knowledge, is the first result that characterizes the expected rejectionfor speculative decoding a sequence of length T using p and q. The second part of Theorem 1 showsthe distribution unbiasedness for SD, which has been presented in Chen et al. [2023a], Leviathanet al. . There are three interesting indications:",
  "In general, the accelerate rate for SD is T{ Tn1 ErTVppn, qnqp|x1:n1qs": "Remark 2. Leviathan et al. derive the expected number of token generated per run of Specu-lative Decoding as 1{ErTVpp, qqs for K 8.4 Their result equals T{ Tn ErTVppn, qnqp|x1:n1qswhen ErTVppn, qnqp|x1:n1qs is identical for all n, and this is due to their assumption that theacceptance rates are i.i.d.. In contrast, our guarantee holds for the case that allows the sequentialdependence between different decoding steps. Simulation. We also provide a simulation of Speculative Decoding and compare it with our Theorem 1in the left panel of (a) with horizon T 50, pn, qn, n 1, . . . , 50 are nonstationary MarkovChains. The green line is the empirical average rejections among 100 N runs of Algorithm 1and the orange line the theoretical value computed via Theorem 1. From the simulation, after 5000runs the empirical average rejections converge to our theoretical value 16.41. In this example, theacceleration rate is 50{16.41 3.05. The specifications of the simulation is included in Appendix F.",
  "This is from their equation (1) that has 1{p1 q with Epminpp, qqq and 1 Epminpp, qqq ErTVpp, qqs": ": The numeric instance in this figure chooses p, q to be nonstationary Markov Chainswith horizon T 50. Left paq: A simulation of Speculative Decoding. The green line is theempirical average rejections among 100N runs and the orange line the theoretical value computedvia Theorem 1. Middle pbq: Batch Speculative Decoding simulations with batch M 4, 5. Thegreen/purple lines are the empirical average rejections among 100N runs and the orange/pink linesare the theoretical values computed via Theorem 3. Right pcq: The scaling law of expected rejectionsfor Batch SD as a function of M. It converges to a limit as M 8.",
  "Optimality of Speculative Decoding": "Now we have analyzed the efficiency of speculated decoding and provided mathematical characteri-zation of the number of expected rejections, which is depicted by the TV distance and scales linearlywith the inference time. A natural next-step question to ask is: Is there any other rejection-basedalgorithm 2 that can do better? We answer this question in the next theorem.Theorem 2 (Instance-dependent Rejection Lower Bound). For an instance P : pp, qq, wherep, q stand for the distributions of the draft model and the target model respectively, definingthe family of algorithms as F : tA : A is a specification of Algorithm 2 that satisfies PAtqt @t pi.e., unbiasedqu. For an algorithm A, denote Nrej as the number of rejections. Then we havethe lower bound",
  "n1Ex1:n1q rTVppn, qnqp|x1:n1qs": "Takeaways. Via Theorem 2, the answer to the key question is: No rejection improvement can bemade in Algorithm 2 by changing the acceptance probability bt and the distribution Pt if we wantto keep the distribution unbiasedness. We point out that lower bound of Theorem 2 matches thecomplexity upper bound of Speculative Decoding given by Theorem 1. This result confirms thatspeculative decoding is optimal in the class of all rejection-based methods. The practical implication is that there is no need to tweak acceptance probability, as it will not makeperformance better. In the next .2, we will see that Speculative Decoding can be provablyimproved, as long as one can decode and verify multiple sequence of tokens in parallel. Connection to optimal transport. We mention Sun et al. nicely consider maximizing theacceptance probability from the optimal transport perspective. For a single token, the optimaltransport cost is x minpppxq, qpxqq, which corresponds to the optimal expected rejection TVpp, qq.However, for the sequential T tokens, their ,6 does not provide a explicit formulation foroptimal acceptance/rejections. In this sense, their optimality result can be cast as a special case ofours. However, we do emphasis there are differences in the settings, where Sun et al. considerthe optimal transport and we study the class F.",
  "Analysis for Batch Speculative Decoding": "To further improve the provable efficiency, we consider batch algorithms that extend the speculativedecoding with multiple candidate draft sequences. Most of the existing works Spector and Re ,Su et al. , Miao et al. , Yang et al. , Jeon et al. formulates batch sequencesvia a speculation tree structure. In particular, the representative work Miao et al. propose themerged token tree structure that combines sequences with the same prefix. A depth-first search isdesigned for speculation to ensure the unbiasedness of the algorithm. In addition, Stern et al. devise the parallel decoding structure that speculate the token of each responses given its previous tokens are accepted. Motivated by these works, we consider a batch version of speculative decodingalgorithm using a simpler parallel structure (Left of ). Our Algorithm 4 can be viewed as asimplified approximation to those batch algorithms. There are several differences that distinguishbatch algorithm from the non-batch version. We highlighting them as follows. Difference1: Oracle call. At the beginning of batch speculation, M draft sequences are generated inparallel as well as the corresponding logits q. This corresponds to Step 4-9 of Alg 4 and is defined asone oracle call which we assume to have unit computation Op1q;5 Difference2: Speculation procedure. It follows the DFS principle: If the first token of a responseis accepted, only that response will be speculated until rejection. For instance in the Left panel of, if the token deep is accepted, the algorithm will keep verifying token learn, ing untilrejection and the rest of sequences wont be examined; if deep is not verified, then the algorithmwill keep examing rein. In this case, rejection happens only if deep, rein and atten are allrejected. Once rejection happens, the process will restart. By this design, it still holds true that:inference time # oracle calls # rejections. Again, we measure the output quality and rejections for the batch algorithm. The following mainresult (Theorem 3) provides the explicit characterization for rejections and batch improvement.Detailed proofs and discussions can be found in D.",
  "where fpx1:nq : Ppx1:n X tn-th draft token rejecteduq. f can be iteratively computed via p, q": "Takeaways. The expected rejection of Batch Decoding composes of two parts: (i) Tn EqrTVrq, psswhich is the rejection that matches the non-batch speculative decoding; (ii) the batch improvement(BI) which comes from our design and is always non-negative. To better understand how the batchimprovement scale with M, we instantiate the single token BIpq, pq : TVrq, ps Mm1 TVrqm, pswith two simple examples. Uniform Distribution. For the whole space V, let p be a uniform distribution with support V andq be a uniform distribution over a subset of V with size V 1 V . Let V {V 1 r, in this caseBIpUnifpV 1q, UnifpV qq p1 1",
  "Unattainable Region": ": Left paq: The Pareto Front between Rejection Probability PAprejectq vs. Distribution biasTVrPA, qs. For a given rejection probability, the black line denotes the optimal deviation LossTV.Middle pbq and Right pcq: A numeric example. In the plot, the over acceptance s are set as positiveconstants that define bpxq mint1, qpxq`",
  "ppxq u": "of batch algorithm designs, making it challenging to define a comprehensive class that encompassesa wide range of batch algorithms. While Sun et al. investigate optimal batch algorithmsthrough an optimal transport lens, their work does not extend to calculating optimal rejection ratesor developing an efficient algorithm to achieve this (they only propose an approximate solution).Consequently, the pursuit of batch optimality remains an open field. Identifying the optimal batchalgorithm could yield valuable insights for enhancing practical applications in real-world scenarios. Extending Speculative Decoding to other studies. Speculative Decoding is a generic samplingapproach that extends beyond mere decoding tasks. It holds potential for wider applications suchas search engines and recommendation systems, where it can be employed to quickly generate andrefine search outcomes or content suggestions, enhancing the overall efficiency and user experienceof these systems. We leave these as future works.",
  "Analysis on the Optimal Rejection-Distribution Bias Tradeoff": "In earlier sections, we focus on analyzing SD or Batch SD which are distribution unbiased. To furtherreduce rejections, the algorithm may have to compromise on output quality. Nevertheless, it is usuallyacceptable for many real-world applications. For instance, for the family of Gemini models Teamet al. , Google may deploy the small model Gemini Nano as the draft model p and Gemini Ultraas the target model q and tune the acceptance probability b (in Algorithm 2) higher such that GeminiNano is applied more often for the purpose of less expenditure. Therefore, an intriguing question toask is: For biased algorithms, what is the optimal tradeoff between rejection and distribution bias?",
  "xp1 bpxqqppxq 0": "Theorem 4 is a universal characterization that contains both biased and unbiased situations. 1. Ifb is less than Speculative Decoding threshold, i.e. b mint1, q{pu then A becomes a probabilitydistribution and the optimal distribution P equals A which is also unbiased (LossTVpbq 0); 2. If bexceeds the Speculative Decoding threshold, then A is no longer a distribution and there are multipleoptimal distributions P. In this case, the optimal distribution bias LossTVpbq 0 for Algorithm 2. Main Takeaways. Via Theorem 4, we derive the pareto front (the optimal tradeoff) between rejectionprobability7 Pprejectq vs. distribution distance TVrPA, qs (Left panel of ). The blue regioncan be realized by some algorithm 2, and the red region cannot. p0, 0q is the perfect algorithm(no rejection, no bias) which does not exists, and, in particular, p0, TVrp, qsq stands for SpeculativeDecoding. Surprisingly, the pareto front is a straight line connecting p0, TVrp, qsq and pTVrp, qs, 0q,which represents a linear relationship between the rejection probability and the optimal TV deviation.This is guaranteed by the following theorem.",
  "x bpxqppxq and LossTVpbq : minP TVrPA, qs": "We plot the numerical example using two Markov Chains in the middle and right panel of that coincides with our theoretical finding. In the figure, the acceptance probability is set to bebpxq mint1, qpxq` ppxq u. The orange line in pcq and the green boundary in pbq are computed viaLossTVpbq from Theorem 4. The complete proofs for Theorem 5 and Theorem 4 are deferred toAppendix E. For the clearness of illustration, we focus on the pareto front between rejection vs. theminimal TV deviation for a single token.",
  "Experiment": "We provide an additional simple experiment to show the effectiveness of our Pareto-optimal solutionin Theorem 4. Consider objective (1). For the given acceptance probability b mint1, q{pu, wehave two options for P: 1. Baseline: select P to be suboptimal to (1), i.e. simply set P : q, whichis the target distribution; 2. Set P : P be the optimal distribution of Theorem 4. We call thefirst method Decoding-UNO and the latter one Decoding-OPT. Instead of TV distance, we measurethe quality via WinRate in our experiment. Concretely, for each prompt, we let Decoding-UNO andDecoding-OPT to generate responses independently, and use score models to compare whose genera-tion has higher quality. We specify draft model p as pythia-70m and target model q as pythia-2.8bfrom EleutherAI Biderman et al. . We apply the score model to be RM-Mistral-7B or GPT-4.We test 200 prompts from Alpaca-Farm-Eval Dataset Dubois et al. with 500 respons-es/comparisons per prompt. shows that Decoding-OPT achieves better performance thanDecoding-UNO across different choice of s. Due to space constraint, the missing details are defferedto Appendix G.",
  "The authors would like to thank anonymous reviewers for their valuable feedback. Mengdi Wangacknowledges the support by NSF IIS-2107304, NSF CPS-2312093, and ONR 1006977": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman,Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report.arXiv preprint arXiv:2303.08774, 2023. Kwangjun Ahn, Ahmad Beirami, Ziteng Sun, and Ananda Theertha Suresh. Spectr++: Improvedtransport plans for speculative decoding of large language models. In NeurIPS 2023 WorkshopOptimal Transport and Machine Learning, 2023. Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lucic, and Cordelia Schmid.Vivit: A video vision transformer. In Proceedings of the IEEE/CVF international conference oncomputer vision, pages 68366846, 2021. Sangmin Bae, Jongwoo Ko, Hwanjun Song, and Se-Young Yun. Fast and robust early-exitingframework for autoregressive language models with synchronized parallel decoding. arXiv preprintarXiv:2310.05424, 2023. Benjamin Bergner, Andrii Skliar, Amelie Royer, Tijmen Blankevoort, Yuki Asano, and Babak Ehte-shami Bejnordi. Think big, generate quick: Llm-to-slm for fast autoregressive decoding. arXivpreprint arXiv:2402.16844, 2024. Nikhil Bhendawade, Irina Belousova, Qichen Fu, Henry Mason, Mohammad Rastegari, and MahyarNajibi. Speculative streaming: Fast llm inference without auxiliary models. arXiv preprintarXiv:2402.11131, 2024. Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle OBrien, EricHallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al.Pythia: A suite for analyzing large language models across training and scaling. In InternationalConference on Machine Learning, pages 23972430. PMLR, 2023. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn,Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. Rt-1: Roboticstransformer for real-world control at scale. arXiv preprint arXiv:2212.06817, 2022.",
  "Ziyi Chen, Xiaocong Yang, Jiacheng Lin, Chenkai Sun, Jie Huang, and Kevin Chen-Chuan Chang.Cascade speculative drafting for even faster llm inference. arXiv preprint arXiv:2312.11462,2023b": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, ThomasUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. Animage is worth 16x16 words: Transformers for image recognition at scale.arXiv preprintarXiv:2010.11929, 2020. Yann Dubois, Chen Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, CarlosGuestrin, Percy S Liang, and Tatsunori B Hashimoto. Alpacafarm: A simulation framework formethods that learn from human feedback. Advances in Neural Information Processing Systems, 36,2023.",
  "Kaixuan Huang, Xudong Guo, and Mengdi Wang. Specdec++: Boosting speculative decoding viaadaptive candidate lengths. arXiv preprint arXiv:2405.19715, 2024": "Wonseok Jeon, Mukul Gagrani, Raghavv Goel, Junyoung Park, Mingu Lee, and Christopher Lott.Recursive speculative decoding: Accelerating llm inference via sampling without replacement.arXiv preprint arXiv:2402.14160, 2024. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, ScottGray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models.arXiv preprint arXiv:2001.08361, 2020. Sehoon Kim, Karttikeya Mangalam, Suhong Moon, Jitendra Malik, Michael W Mahoney, AmirGholami, and Kurt Keutzer. Speculative decoding with big little decoder. In Thirty-seventhConference on Neural Information Processing Systems, 2023.",
  "Xiaoxuan Liu, Lanxiang Hu, Peter Bailis, Ion Stoica, Zhijie Deng, Alvin Cheung, and Hao Zhang.Online speculative decoding. arXiv preprint arXiv:2310.07177, 2023": "Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang, Rae Ying Yee Wong,Zhuoming Chen, Daiyaan Arfeen, Reyna Abhyankar, and Zhihao Jia. Specinfer: Acceleratinggenerative llm serving with speculative inference and token tree verification. arXiv preprintarXiv:2305.09781, 2023. Eric Mitchell, Rafael Rafailov, Archit Sharma, Chelsea Finn, and Christopher D Manning. Anemulator for fine-tuning large language models using small language models. arXiv preprintarXiv:2310.12962, 2023.",
  "Jie Ou, Yueming Chen, and Wenhong Tian. Lossless acceleration of large language model viaadaptive n-gram parallel decoding. arXiv preprint arXiv:2404.08698, 2024": "Haifeng Qian, Sujan Kumar Gonugondla, Sungsoo Ha, Mingyue Shang, Sanjay Krishna Gouda,Ramesh Nallapati, Sudipta Sengupta, Xiaofei Ma, and Anoop Deoras. Bass: Batched attention-optimized speculative sampling. arXiv preprint arXiv:2404.15778, 2024. Andrea Santilli, Silvio Severino, Emilian Postolache, Valentino Maiorca, Michele Mancusi, RiccardoMarin, and Emanuele Rodol. Accelerating transformer inference for translation via paralleldecoding. arXiv preprint arXiv:2305.10427, 2023.",
  "Ziteng Sun, Jae Hun Ro, Ahmad Beirami, and Ananda Theertha Suresh. Optimal block-level draftverification for accelerating speculative decoding. arXiv preprint arXiv:2403.10444, 2024b": "Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, RaduSoricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capablemultimodal models. arXiv preprint arXiv:2312.11805, 2023. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimotheLacroix, Baptiste Rozire, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open andefficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Siqi Wang, Hailong Yang, Xuezhu Wang, Tongxuan Liu, Pengbo Wang, Xuning Liang, Kejie Ma,Tianyu Feng, Xin You, Yongjun Bao, et al. Minions: Accelerating large language model inferencewith adaptive and collective speculative decoding. arXiv preprint arXiv:2402.15678, 2024. Heming Xia, Tao Ge, Peiyi Wang, Si-Qing Chen, Furu Wei, and Zhifang Sui. Speculative decoding:Exploiting speculative execution for accelerating seq2seq generation. In Findings of the Associationfor Computational Linguistics: EMNLP 2023, pages 39093925, 2023. Heming Xia, Zhe Yang, Qingxiu Dong, Peiyi Wang, Yongqi Li, Tao Ge, Tianyu Liu, Wenjie Li, andZhifang Sui. Unlocking efficiency in large language model inference: A comprehensive survey ofspeculative decoding. arXiv preprint arXiv:2401.07851, 2024.",
  "A.2High Level Proof Sketch for the second part of Theorem 3": "The derivation for the number of expected rejections using the batch speculative decoding is moreinvolved than the Algorithm 4 due to the parallel response structure. The key step is to computethe intermediate quantity PApxn acc, xn xn|x1:n1q. Let xn1 pp|x1:n2q, then there are twocases: xn1 accepted or rejected. We have",
  "Here the fourth equal sign comes from Speculative Decoding keep the distribution q (Proposition ??),the fifth equal sign comes from the event tRn 1u tReject at nu": "Theorem 7 (Restatement of the second part of Theorem 1). The output distributions of SpeculativeDecoding Algorithm 1 and the large model q are identical, i.e. for any output sequence x1:T P VT ,the joint the distributions over x1:T satisfies: PSpecDecodingpx1:T q qpx1:T q. Remark 3. The proof of Theorem 7 is very similar to Chen et al. [2023a] except we have K 8.In addition, Chen et al. [2023a] proves the distribution match for a single token, we complement theproof to show the distribution match holds for a sequence of tokens x1:T .",
  "Proof. In particular, we use induction to show the stronger result that @ t P rTs, @x1, x2, . . . , xt P V,it holds PAt px1, x2, . . . , xtq qtpx1, x2, . . . , xtq": "Step1: Since x0 is the prompt, its distribution is independent to p, q. Then for t 1, applyingTheorem 1 of Chen et al. [2023a] (with K 8) directly with p and q to be conditional distributionsp1p|x0q and q1p|x0q gives PA1 px1|x0q q1px1|x0q, which further implies PA1 px1q q1px1q (sincethe distribution of x0 is independent of p, q).",
  "n1Ex1:n1q rTVppn, qnqp|x1:n1qs : CpPq": "Remark 4. Theorem 8 shows the rejections of Algorithm 1 is tight at the instance level (over thefamily of algorithms F). Therefore, it attains the instance-optimality over sequential decodingalgorithms family F. Proof. For any algorithm A P F, its bn can be written as a function of the sequence x1:n1, xn with0 bnpxn, x1:n1q 1.9 Based on this, we define the new function n : V Vn1 R accordingto the following equation:",
  "where the first equal sign uses that xn is sampled from pnp|x1:n1q so pnpxn|x1:n1q 0, and thesecond equal sign uses 0 bn 1": "Lemma 2. For any instance P pp, qq, let F : tA : A is a realization of Algorithm 2 s.t. PAt qt @t pi.e., unbiasedqu. Suppose there is an A P F such that EAP rNrejs CpPq, then there exists abn in Line 8 of Template 2 such that Dx, x1:n1",
  "Proof. Step1: Let x1:n1 be the accepted tokens up to n 1. We first show PABn pxn|x1:n1q PLLMnpxn|x1:n1q @xn P V": "We partition the generation of xn into two cases: (i). accepted as the first token of the m-th responses(m 1, . . . , M), or rejected by all M responses and sampled by Line 28 of Algorithm 4; (ii).accepted/rejected as the t-th token of the m-th responses (m 1, . . . , M) for t 2.",
  "x1 maxt0, qnpx1|x1:n1q pnpx1|x1:n1qu qnpxn|x1:n1q": "By the construction of Algorithm 4 (Line 3), when xn is accepted/rejected as the tp 2q-th draft tokenof certain response, we have qnpxn|x1:n1q PLLMnpxn|x1:n1q. This gives PABn pxn|x1:n1q PLLMnpxn|x1:n1q. For the first case. This part of the proof largely follows Theorem 4.2 of Miao et al. . In thiscase, the n-th generated token xn has M ` 1 possibilities: accepted at the m-th response or rejectedby all M responses and sample at Line 28. Since the algorithm will iterate all M responses if notaccepted, we denote qmn as the qn for the m-th response. Then we have the recursion",
  "D.1Expected Rejections for Batch Speculative Decoding (Proof for the second part ofTheorem 3)": "In this section, we derive the number of expected rejections using the batch speculative decoding.However, the analysis is more involved than the Algorithm 4 due to the parallel response structure,since, given the verified token x1:n1, the probability of n-th token being rejected does not possess aunified expression and depends on the location of xn1. We detail this below.",
  "ErRn|x1:n1s PApn-th token rej|x1:n1q 1 PApn-th token acc|x1:n1q": "In this scenario, we cannot obtain PApn-th token rej|x1:n1q TVppnp|x1:n1q, PLLMnp|x1:n1qqsince the conditional rejection probability implicitly encodes the location of pn1q-th token: whetherxn1 pp|x1:n1q is rejected (at the root of the tree) or xn1 pp|x1:n1q is accepted (at thebranch of the tree). To formalize this, given validated token x1:n1, we denote qmn p|x1:n1q to bethe m-th rejection distribution, then by the construction of Algorithm 4 (Line 19),",
  "(19)": "where the first equal sign comes from: since xn1rej implies xn represents the first token of theparallel tree, then it is identical to the proof of the first case of Step1 in Theorem 9. The secondequal sign is from: xn is rejected means all M responses since xn is the first token of the tree. Theconditional rejection probability",
  "since Mm1 TVpqm, pqpxnq 0 as M 0 (note TVpqm, pqpxnq 1 iff there is no overlapbetween qm and p), it implies f 8px1:nq hpxn|xnqrq1px1:n1q f 8px1:n1qs": "Third item: Similar to the second item, it holds true via taking M 8. For proving ErN 8rejs 0,suppose ErN 8rejs 0. Then it holds q1 f 8. By the second item, this further implies q1 f 8 0,which is impossible. Fourth item: We prove by contradiction. Suppose ErNrejs 0, then by the first item and the thirditem this implies q1 f 8. Plug this back to the second item, this further implies f 8 0, soq1 f 8 0 is a contradiction (q1 is a probability distribution)!",
  "FNumerical Simulation Details": "To validate the correctness of our theory, we provide the numeric simulations that are displayed in,2. We model the distribution p1:T and q1:T to be two non-stationary Markov Chains with 7states/tokens. Instead of being ppxn|x1:n1q, for Markov Chain, the one step transition is Markovianwith ppxn|x1:n1q ppxn|xn1q. We set the random seed to be 10. The prompt distribution p0 isset to be Uniform distribution. For , the true value is computed via Theorem 1,3 respectively,and solid line is computed by",
  "Set P : P to be the optimal solution of (28), whose solution is presented in Theorem 4.We call this method Decoding-OPT": "Measuring performance. Instead of TV distance, we measure the quality via WinRate in ourexperiment. Concretely, for each prompt, we let Decoding-UNO and Decoding-OPT to generateresponses independently, and use score models to compare whose generation has higher quality.We specify draft model p as pythia-70m and target model q as pythia-2.8b from EleutherAIBiderman et al. . We apply the score model to be RM-Mistral-7B or GPT-4. We test 200prompts from Alpaca-Farm-Eval Dataset Dubois et al. with 500 responses/comparisonsper prompt. For a given prompt, Decoding-OPT wins if for more than 250 comparisons, scoremodel prefer its response11 over Decoding-UNO. Then the WinRate for each method is computed as#wins{200. Sanity Check for the experiment. To validate having smaller distance w.r.t. the large modelq (pythia-2.8b) indicates higher performance, we preform the WinRate test for decoding viapythia-70m only against decoding via pythia-2.8b only. shows that there is a significantperformance gap between large model and small model, therefore validate the legitimacy of theexperiment in .",
  "x maxtqpxqppxq,u, and we can select P : rAs` (recall rs`in .1), which satisfies P|Apq 0; 0 P|A`pq Apq": "To implement Decoding-UNO, we modify the _speculative_sampling function in HuggingFacetransformers/generation/utils.py file as follows12 (where variable eps_ is in ).This is conducted in a single A100 GPU. 10To be rigorous, we mention we didnt choose P : rq ps` as the baseline since P : rq ps` mightfall in the optimal solution sets of P defined in Theorem 4.11We mention for RM-Mistral-7B it output values, then the response with higer value wins. For GPT-4, itoutputs preference.12Note the HuggingFace code uses p as target model and q as the draft model, which is different from us."
}