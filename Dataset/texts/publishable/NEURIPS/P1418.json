{
  "Abstract": "The well-known empirical risk minimization (ERM) principle is the basis of manywidely used machine learning algorithms, and plays an essential role in the classi-cal PAC theory. A common description of a learning algorithms performance isits so-called learning curve, that is, the decay of the expected error as a functionof the input sample size. As the PAC model fails to explain the behavior of learn-ing curves, recent research has explored an alternative universal learning modeland has ultimately revealed a distinction between optimal universal and uniformlearning rates (Bousquet et al., 2021). However, a basic understanding of suchdifferences with a particular focus on the ERM principle has yet to be developed.In this paper, we consider the problem of universal learning by ERM in the real-izable case and study the possible universal rates. Our main result is a fundamen-tal tetrachotomy: there are only four possible universal learning rates by ERM,namely, the learning curves of any concept class learnable by ERM decay eitherat en, 1/n, log (n)/n, or arbitrarily slow rates. Moreover, we provide a com-plete characterization of which concept classes fall into each of these categories,via new complexity structures. We also develop new combinatorial dimensionswhich supply sharp asymptotically-valid constant factors for these rates, when-ever possible.",
  "Introduction": "The classical statistical learning theory mainly focuses on the celebrated PAC (Probably Approx-imately Correct) model (Vapnik and Chervonenkis, 1974; Valiant, 1984) with emphasis on super-vised learning. A particular setting therein, called the realizable case, has been extensively stud-ied. Complemented by the no-free-lunch\" theorem (Antos and Lugosi, 1996), the PAC frame-work, which adopts a minimax perspective, can only explain the best worst-case learning rate bya learning algorithm over all realizable distributions. Such learning rates are thus also called the uni-form rates. However, the uniform rates can only capture the upper envelope of all learning curves,and are too coarse to explain practical machine learning performance. This is because real-worlddata is rarely worst-case, and the data source is typically xed in a given learning scenario. In-deed, Cohn and Tesauro (1990, 1992) observed from experiments that practical learning rates canbe much faster than is predicted by PAC theory. Moreover, many theoretical works (Schuurmans,1997; Koltchinskii and Beznosova, 2005; Audibert and Tsybakov, 2007, etc.) were able to provefaster-than-uniform rates for certain learning problems, though requiring additional modelling as-sumptions. To distinguish from the uniform rates, these rates are named the universal rates andwas formalized recently by Bousquet et al. (2021) via a distribution-dependent framework. Unlikethe simple dichotomy of the optimal uniform rates: every concept class H has a uniform rate beingeither linear VC(H)/n or bounded away from zero\", the optimal universal rates are captured by atrichotomy: every concept class H has a universal rate being either exponential, linear or arbitrarilyslow (see Thm.1.6 Bousquet et al., 2021).",
  "38th Conference on Neural Information Processing Systems (NeurIPS 2024)": "In supervised learning, a family of successful learners called the empirical risk minimization (ERM)consist of all learning algorithms that output a sample-consistent classier. In other words, an ERMalgorithm is any learning rule, which outputs a concept in H that minimizes the empirical error (seeAppendix A for a formal denition). For notation simplicity, we rst introduce",
  ", denoted by VSn(H) (or Vn(H) for short), is denedas VSn(H) := {h H : h(xi) = yi, i [n]}": "Now given labeled samples Sn := {(xi, yi)}ni=1, an ERM algorithm is any learning algorithm thatoutputs a concept in the sample-induced version space, that is, a sequence of universally measurablefunctions An : Sn hn VSn(H), n N. Throughout this paper, we will simply denote an ERMalgorithm by its output predictors {hn}nN. It is well-known that the ERM principle plays an important role in understanding general uniformlearnability: a concept class is uniformly learnable if and only if it can be learned by ERM. However,while the optimal VC(H)/n rate is achievable by some improper learner (Hanneke, 2016a), ERMalgorithms can at best achieve a uniform rate of (VC(H)/n) log (n/VC(H)). Moreover, such a gaphas been shown to be unavoidable in general (Auer and Ortner, 2007), which leaves a challengingquestion to study: what are the sufcient and necessary conditions on H for the entire family ofERM algorithms to achieve the optimal error? Indeed, many subsequent works have devoted toimproving the logarithmic factor in specic scenarios. The work of Gin and Koltchinskii (2006)rened the bound by replacing log (n/VC(H)) with log ((VC(H)/n)), where () is called thedisagreement coefcient. Based on this, Hanneke and Yang (2015) proposed a new data-dependentbound with log (n1:n/VC(H)), where n1:n is a quantity related to the version space compression setsize (a.k.a. the empirical teaching dimension). As a milestone, the work of Hanneke (2016b) provedan upper bound (VC(H)/n) log (sH/VC(H)) and a lower bound (VC(H)+log (sH))/n, where sHis called the star number of H (see Denition 4 in ). Though not quite matching, these twobounds together yield an optimal linear rate when sH < . Thereafter, the uniform rates by ERMcan be described as a trichotomy, namely, every concept class H has a uniform rate by ERM beingexactly one of the following: 1/n, log (n)/n and bounded away from zero\". From a practical perspective, many ERM-based algorithms are designed and are widely applied indifferent areas of machine learning, such as the logistic regression and SVM, the CAL algorithm inactive learning, the gradient descent (GD) algorithm in deep learning. Since the worst-case natureof the PAC model is too pessimistic to reect the practice of machine learning, understanding thedistribution-dependent performance of ERM algorithms is of great signicance. However, unlikethat a distinction between the optimal uniform and universal rates has been fully understood, howfast universal learning can outperform uniform learning in particular by ERM remains unclear. Fur-thermore, we are lacking a complete theory to the characterization of the universal rates by ERM,though certain specic scenarios that admit faster rates by ERM have been discovered (Schuurmans,1997; van Handel, 2013). In this paper, we aim to answer the following fundamental question:",
  "Question 1. Given a concept class H, what are the possible rates at which H can be universallylearned by ERM?": "We start with some basic preliminaries of this paper. We consider an instance space X and a conceptclass H {0, 1}X. Given a probability distribution P on X {0, 1}, the error rate of a classierh : X {0, 1} is dened as erP (h) := P((x, y) X {0, 1} : h(x) = y). A distribution P iscalled realizable with respect to H, denoted by P RE(H), if infhH erP (h) = 0. Note that inthis denition, h satisfying erP (h) = infhH erP (h) is called the target concept of the learningproblem, and is not necessary in H. We may also say that P is a realizable distribution centered ath. Given an integer n, we denote by Sn := {(xi, yi)}ni=1 P n a i.i.d. P-distributed dataset. Inthe universal learning framework, the performance of a learning algorithm is commonly measuredby its learning curve (Bousquet et al., 2021; Hanneke et al., 2022; Bousquet et al., 2023), that is, thedecay of the expected error rate E[erP (hn)] as a function of sample size n. With these settingssettled, we are now able to formalize the problem of universal learning by ERM.",
  ", if H is universally learnable at rateR by ERM, and is not universally learnable at rate faster than R by ERM. H requires arbitrarily slow rates to be universally learned by ERM": ", if for every rate func-tion R(n) 0, H is not universally learnable at rate faster than R by ERM.Remark 1. The above denition inherits the structure of the denition to the optimal universallearning (Denition 1.4 Bousquet et al., 2021). Here, we are actually considering the worst-case\"ERM algorithm, which is consistent with the PAC theory. A crucial difference between this denitionand the PAC one is that here the constants C, c > 0 are allowed to depend on the distribution P.In other words, the PAC model can be dened similarly, but requires uniform constants C, c > 0.Consequently, H is universally learnbale at rate R by ERM if it is PAC learnable at rate R by ERM.Remark 2. It is not hard to see that the error rate achieved by any ERM algorithm, given Sn P nas input, is at most suphVSn(H) erP (h). Furthermore, for any distribution P RE(H), there existERM algorithms obtaining error rates arbitrarily close to this value. Hence, to obtain upper boundsof the universal rates by ERM, it requires us to bound the random variable suphVSn(H) erP (h),where a common technique is to bound P(suphVSn(H) erP (h) > ) = P(h VSn(H) : erP (h) >). To obtain lower bounds, it requires us to construct specic hard\" distributions.",
  "Basic examples": "In order to develop some initial intuition for what universal learning rates are possible for ERM, werst introduce several basic examples that illustrate the possibilities in the following .2. Toconvince the reader, we provide direct analysis (without using our characterization in .2)for those examples (see details in Appendix B.1).Example 1 (en learning rate). Any nite class H is universally learnable at exponential rate(Schuurmans, 1997).Indeed, according to their analysis, such exponential rates can also beachieved by any ERM algorithm.Example 2 (1/n learning rate). Let Hthresh,N := {ht : t N} be the class of all threshold classi-ers on natural numbers, where ht(x) :=1(x t), for all x N. Hthresh,N is universally learnableat exponential rate since this class does not have an innite Littlestone tree (Bousquet et al., 2021).However, ERM algorithms cannot guarantee such exponential rates but at best linear rates, whenencountering certain realizable distributions centered at the target concept hall-0s, which is the func-tion that labels zero everywhere (see Appendix A).Example 3 (log (n)/n learning rate). Let X = N and Hsingleton,N := {ht : t X} be the classof all singletons on X, where ht(x) :=1(x = t), for all x N. It is clear that VC(Hsingleton,N) =1. Note that Hsingleton,N is universally learnable at exponential rate since it has nite Littlestonedimension LD(Hsingleton,N) = 1. However, the exact universal rate by ERM is instead log (n)/n.This is because Hsingleton,N admits certain realizable distributions centered at hall-0s. Indeed, it is anexample where the universal rate by ERM matches the uniform rate, up to a distribution-dependentconstant.Example 4 (Arbitrarily slow learning rate). Let X =",
  "iN Xi be the disjoint union of nite setswith |Xi| = 2i, for all i N. For each i N, let Hi := {hS :=1S : S Xi, |S| 2i1} andconsider the concept class H =": "iN Hi. H is universally learnable at exponential rate since itdoes not have an innite Littlestone tree. However, a bad ERM algorithm can perform arbitrarilyslowly.Example 5 (Not Glivenko-Cantelli but learnable by ERM). Let X = , H := { 1S : S X, |S| < }, and P be the uniform (Lebesgue) distribution on . H is universally learnableat exponential rate (no innite Littlestone tree). Moreover, H is not a universal Glivenko-Cantelliclass for P (van Handel, 2013), but is still universally learnable by ERM. However, if we considerthe class H {hall-1s}, which is still not a universal Glivenko-Cantelli class for P, but no longeruniversally learnable by any ERM algorithm since erP (hn) = 1 regardless of the sample size. The above examples indicate that the cases of universal learning by ERM do not match the uniformlearning, but contains at least ve possible cases: every nontrivial concept class H is either univer-sally learnable at exponential rate (but not faster), or is universally learnable at linear rate (but notfaster), or is universally learnable at slightly slower than linear rate log (n)/n (but not faster), oris universally learnable but necessarily with arbitrarily slow rates, or is not universally learnable atall. Throughout this paper, we only consider the case where the given concept class is universallylearnable by ERM. We leave it an open question whether there exists a nice characterization thatdetermines the universal learnability by ERM.",
  "Main results": "In this section, we summarize the main results of this paper. The examples in .1 reveal thatthere are at least four possible universal rates by ERM. Interestingly, we nd that these are also theonly possibilities. The following two theorems consist of the main results of this work. In particular,Theorem 1 gives out a complete answer to Question 1. It expresses a fundamental tetrachotomy:there are exactly four possibilities for the universal learning rates by ERM: being either exponential,or linear, or log (n)/n, or arbitrarily slow rates. Moreover, Theorem 2 species the answer bypointing out for what realizable distributions (targets), those universal rates are sharp.Theorem 1 (Universal rates for ERM). For every class H with |H| 3, the following hold: H is universally learnable by ERM with exact rate en if and only if |H| < . H is universally learnable by ERM with exact rate 1/n if and only if |H| = and H doesnot have an innite star-eluder sequence. H is universally learnable by ERM with exact rate log (n)/n if and only if H has an innitestar-eluder sequence and VC(H) < . H requires at least arbitrarily slow rates to be learned by ERM if and only if VC(H) = .Remark 3. The formal denition of the star-eluder sequence can be found in . Unlike theseparation between exact en and 1/n rates is determined by the cardinality of the class, and theseparation between exact log (n)/n and arbitrarily slow rates is determined by the VC dimensionof the class, whether there exists a simple combinatorial quantity that determines the separationbetween exact 1/n and log (n)/n rates is unclear and might be an interesting direction for futurework. We thought that it is likely the star number sH (Denition 4) is the correct characterizationhere, but it turns out not unfortunately (see details in and Appendix B.3). Based on Theorem 1, a distinction between the performance of ERM algorithms and the optimaluniversal learning algorithms can be revealed, which we present in the following table (the requireddenitions in Case\" are deferred to , and examples can be found in Appendix B.2).",
  "Example 16": "Furthermore, the distinction between the universal rates and the uniform rates by ERM can also befully captured, and are depicted schematically in as an analogy to the of Bousquet et al.(2021). Besides the examples in .1, we also need the following additional example con-cerning the Littlestone dimension to appear in the diagram.Example 6 (log (n)/n learning rate and unbounded Littlestone dimension). We consider herethe class of two-dimensional halfspaces, that is, X := R2 and Hhalfspaces,R := { 1(w x + b 0) :w R2, b R}. It is a classical fact that for any integer d, the class of halfspaces on Rd hasa nte VC dimension d, but has an innite Littlestone tree, and thus having unbounded Littlestonedimension (Shalev-Shwartz and Ben-David, 2014). Finally, to show that this class is universallylearnable by ERM at exact log (n)/n rate, we simply consider the subspace S1 X, this is indeedan innite star set of Hhalfspaces,R centered at hall-0s and thus an innite star-eluder sequence. As a complement to Theorem 1, the following Theorem 2 gives out target-specied universal rates.We say a target concept h is universally learnable by ERM with exact rate R if all realizable distri-bution P considered in Denition 2 are centered at h. In other words, H is universally learnable",
  ": A venn diagram depicting the tetrachotomy of the universal rates by ERM and its relationwith the uniform rates characterized by the VC dimension and the star number": "with exact rate R is equivalent to say all realizable target concepts are universally learnable with ex-act rate R. Concretely, for each of the four possible rates stated in Theorem 1, Theorem 2 speciesthe target concepts that can be learned at such exact rate by ERM.Theorem 2 (Target specied universal rates). For every class H with |H| 3, and a targetconcept h, the following hold: h is universally learnable by ERM with exact rate en if and only if H does not have aninnite eluder sequence centered at h. h is universally learnable by ERM with exact rate 1/n if and only if H has an inniteeluder sequence centered at h, but does not have an innite star-eluder sequence centeredat h. h is universally learnable by ERM with exact rate log (n)/n if and only if H has an innitestar-eluder sequence centered at h, but does not have an innite VC-eluder sequencecentered at h. h requires at least arbitrarily slow rates to be universally learned by ERM if and only ifH has an innite VC-eluder sequence centered at h.",
  "All detailed proofs appear in Appendix D. We also provide a brief overview of the main idea of eachproof as well as some related concepts in": "An additional part of this work presents a ne-grained analysis (Bousquet et al., 2023) of the univer-sal rates by ERM, which complements the coarse rates used in Theorem 1. Concretely, we providea characterization of sharp distribution-free constant factors of the ERM universal rates, wheneverpossible. The characterization is based on two newly-developed combinatorial dimensions, calledthe star-eluder dimension (or SE dimension) and the VC-eluder dimension (or VCE dimension)(Denition 9). We say whenever possible\" because distribution-free constants are unavailable forcertain cases (Remark 16). Such a characterization can also be considered as a renement to theclassical PAC theory, in a sense that it is sometimes better but only asymptotically-valid. Due tospace limitation, we defer the denition of ne-grained rates and related results to Appendix C.",
  "Related works": "PAC learning by ERM. The performance of consistent learning rules (including the ERM algo-rithm) in the PAC (distribution-free) framework has been extensively studied. For VC classes,Blumer et al. (1989) gave out a log (n)/n upper bound of the uniform learning rate. Despite thewell-known equivalence between uniform learnability and uniform learnability by the ERM prin-ciple (Vapnik and Chervonenkis, 1971), the best upper bounds for general ERM algorithms differ from the optimal sample complexity by an unavoidable logarithmic factor (Auer and Ortner, 2007).By analyzing the disagreement coefcient of the version space, the work of Gin and Koltchinskii(2006); Hanneke (2009) rened the logarithmic factor in certain scenarios. Furthermore, not onlybeing a relevant measure in the context of active learning (Cohn et al., 1994; El-Yaniv and Wiener,2012; Hanneke, 2011, 2014), the region of disagreement of the version space was found out to havean interpretation of sample compression scheme with its size known as the version space compres-sion set size (Wiener et al., 2015; Hanneke and Yang, 2015). Based on this, the label complexity ofthe CAL algorithm can be converted into a bound on the error rates of all consistent PAC learners(Hanneke, 2016b). Finally, Hanneke and Yang (2015); Hanneke (2016b) introduced a simple combi-natorial quantity named the star number, and guaranteed that a concept class with nite star numbercan be uniformly learned at linear rate. Universal Learning. Observed from empirical experiments, the actual learning rates on real-worlddata can be much faster than the one described by the PAC theory (Cohn and Tesauro, 1990, 1992).The work of Benedek and Itai (1988) considered a setting lies in between the PAC setting and the uni-versal setting called nonuniform learning, in which the learning rate may depend on the target con-cept but still uniform over the marginal distributions. After that, Schuurmans (1997) studied classesof concept chains and revealed a distinction between exponential and linear rates along with a theo-retical guarantee. Later, more improved learning rates have been obtained for various practical learn-ing algorithms such as stochastic gradient decent and kernel methods (Koltchinskii and Beznosova,2005; Audibert and Tsybakov, 2007; Pillaud-Vivien et al., 2018, etc.). Additionally, van Handel(2013) studied the uniform convergence property from a universal perspective, and proposed theuniversal Glivenko-Cantelli property. Until very recently, the universal (distribution-dependent)learning framework was formalized by Bousquet et al. (2021), in which a complete theory of the(optimal) universal learnability was obtained as well. After that, Bousquet et al. (2023) carried out ane-grained analysis on the distribution-free tail\" of the universal learning curves by characterizingthe optimal constant factor. As generalizations, Kalavasis et al. (2022); Hanneke et al. (2023) stud-ied the universal rates for multiclass classication, and Hanneke et al. (2022) studied the universallearning rates under an interactive learning setting.",
  "Technical overview": "In this section, we discuss some technical aspects in the derivation of our main results in .2. Our analysis to the universal learning rates by ERM is based on three new types of complexitystructures named the eluder sequence, the star-eluder sequence and the VC-eluder sequence. Moredetails can be found in Sections 3-4 and all technical proofs are deferred to Appendix D.",
  "of H as DIS(H) := {x X : h, g H s.t. h(x) = g(x)}. Let hbe a classier, the star number of h": ", denoted by sh(H) or sh for short, is dened to be the largestinteger s such that there exist distinct points x1, . . . , xs X and concepts h1, . . . , hs H satisfyingDIS({h, hi}) {x1, . . . , xs} = {xi}, for every 1 i s. (We say {x1, . . . , xs} is a star set of Hcentered at h",
  ", denotedby s(H) or sH, is dened to be the maximum possible cardinality of a star set of H, or sH = ifno such maximum exists": "Remark 4. From this denition, it is clear that the star number sH of H satises sH VC(H).Indeed, any set {x1, . . . , xd} that shattered by H is also a star set of H based on the followingreasoning: Since {x1, . . . , xd} is shattered by H, there exists h H such that h(x1) = =h(xd) = 0. Moreover, for any i [d], there exists hi H satisfying hi(xi) = 1 and hi(xj) = 0 forall j = i. An immediate implication is that a VC-eluder sequence is always a star-eluder sequence(see Denition 6 and Denition 7 below).",
  ", if h(xi) = yi for all i N": "Remark 5. It has been proved that max{sH, log (LD(H))} E(H) 4max{sH,2LD(H)} (Li et al.,2022, Thm.8), where LD(H) is the Littlestone dimension of H. Moreover, the very recent work ofHanneke (2024) proved that E(H) |H| 2sHLD(H), which implies that any concept class withnite star number and nite Littlestone dimension must be a nite class.",
  ", if it is realizable andlabelled by h, and for every integer k 1, {xnk+1, . . . , xnk+k} is a shattered set of Vnk(H)": "Remark 6. In the denitions of eluder sequence and VC-eluder sequence, {(x1, y1), (x2, y2), . . .}centered at h\" simply means the sequence is labelled by h. However, the words centered at\" in thedenition of star-eluder sequence is more meaningful. In this paper, we give them a uniform namein order to make Theorem 2 look consistent. Remark 7. An innite star-eluder (VC-eluder) sequence requires the version space to keep on hav-ing star (shattered) sets with innitely increasing sizes. If the size cannot grow innitely, the largestpossible size of the star (shattered) set is called the star-eluder (VC-eluder) dimension (Denition 9),which plays an important role in our ne-grained analysis (Appendix C). To distinguish the notionof star-eluder (VC-eluder) sequence here from the d-star-eluder (d-VC-eluder) sequence dened inAppendix C, we may call the construction in Denition 6 an innite strong star-eluder sequence",
  "Exact universal rates": "Sections 3 and 4 of this paper are devoted to the proof ideas of Theorems 1 and 2 with furtherdetails. In this section, we give a complete characterization of the four possible exact universal ratesby ERM (en, 1/n, log (n)/n and arbitrarily slow rates) via the existence/nonexistence of the threecombinatorial sequences dened in . For each of the following if and only if\" results(Theorems 3-6), we are required to prove both the sufciency and the necessity. The sufciencyconsists of both an upper bound and a lower bound since we are proving the exact universal rates.The necessity also follows simply by the method of contradiction, given the rates are exact. Alltechnical proofs are deferred to Appendix D.1.",
  "Lemma 2 (en upper bound). If H does not have an innite eluder sequence (centered at h),then H (h) is universally learnable by ERM at rate en": "Proof of Theorem 3. The sufciency follows directly from the lower bound in Lemma 1 togetherwith the upper bound in Lemma 2. Furthermore, Lemma 3 in .2 proves that the existenceof an innite eluder sequence leads to a linear lower bound of the ERM universal rates. Therefore,the necessity follows by using the method of contradiction.",
  "Linear rates": "Theorem 4. H is universally learnable by ERM with exact rate 1/n if and only if H has an inniteeluder sequence but does not have an innite star-eluder sequence.Lemma 3 (1/n lower bound). If H has an innite eluder sequence centered at h, then h is notuniversally learnable by ERM at rate faster than 1/n.",
  "Lemma 4 (1/n upper bound). If H does not have an innite star-eluder sequence (centered at h),then H (h) is universally learnable by ERM at rate 1/n": "Proof of Theorem 4. To prove the sufciency, on one hand, the existence of an innite eluder se-quence implies a linear lower bound based on Lemma 3. On the other hand, if H does not have aninnite star-eluder sequence, Lemma 4 yields a linear upper bound. The necessity can be provedby the method of contradiction. Concretely, if either of the two conditions fail, the universal rateswill be either en or at least log (n)/n, based on Lemma 2 in .1 and Lemma 5 in .3.",
  "Theorem 5. H is universally learnable by ERM with exact rate log (n)/n if and only if H has aninnite star-eluder sequence but does not have an innite VC-eluder sequence": "Lemma 5 (log (n)/n lower bound). If H has an innite star-eluder sequence centered at h, thenh is not universally learnable by ERM at rate faster than log (n)/n.Remark 9. Note that the conclusion in Remark 5 explains why the intersection of innite Little-stone classes\" and classes with nite star number\" is empty in . However, we mentionin Remark 3 that innite star number does not guarantee an innite star-eluder sequence (see Ap-pendix B.3 for details). Hence, Remark 5 cannot explain why the intersection of innite Littlestone",
  "Lemma 6 (log (n)/n upper bound). If H does not have an innite VC-eluder sequence (centeredat h), then H (h) is universally learnable by ERM at log (n)/n rate": "Proof of Theorem 5. To prove the sufciency, on one hand, if H has an innite star-eluder sequence,the universal rates have a log (n)/n lower bound based on Lemma 5. On the other hand, if Hdoes not have an innite VC-eluder sequence, then Lemma 6 yields a log (n)/n upper bound. Thenecessity can be proved using the method of contradiction based on Lemma 4 in .2 andLemma 7 in .4 below.",
  "Equivalent characterizations": "In , it has been shown that the eluder sequence, the star-eluder sequence and the VC-eludersequence are the correct characterizations of the exact universal learning rates by ERM. However,the denitions to them are somewhat non-intuitive. Therefore, in this section, we aim to build con-nections between these combinatorial sequences and some well-understood complexity measures,which will then give rise to our Theorem 1. Concretely, we have the following two equivalences(see Appendix D.2 for their complete proofs).",
  "Proposition 2. sH = if H has an innite star-eluder sequence. Moreover, there exist conceptclasses H with sH = but does not have any innite star-eluder sequence": "Remark 10. Based on the results in , the proposition essentially states that the gap between1/n and log (n)/n exact universal rates by ERM is not characterized by the star number sH. Wewonder whether there is some other simple combinatorial quantity that is determinant to this gap,which would be an valuable direction for future work. Why is the case of star-eluder sequence different from the other two structures? We suspect that sucha distinction may arise from the following: unlike the eluder sequence and the VC-eluder sequence,the centered concept of a star-eluder sequence is much more meaningful (see Remark 6). Concretely,within its denition, the set of the following k points is not only required to be a star set of the versionspace Vnk(H), but is required to be centered at the same labelling target. This intuitively impliesthat there might exists a class such that for arbitrarily large integer k, it can witness a star set of sizek, but with a k-specied center (for different k). Such a class (e.g. Examples 19, 20 in AppendixB.3) does have innite star number but will not have an innite star-eluder sequence. Indeed, therelations between those star-related notions (star number, star-eluder dimension, star set and stareluder sequence) turn out to be more complicated than expected, and we leave it to Appendix B.3."
}