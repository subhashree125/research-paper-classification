{
  "Abstract": "Developing foundational world models is a key research direction for embod-ied intelligence, with the ability to adapt to non-stationary environmentsbeing a crucial criterion. In this work, we introduce a new formalism, HiddenParameter-POMDP, designed for control with adaptive world models. Wedemonstrate that this approach enables learning robust behaviors acrossa variety of non-stationary RL benchmarks. Additionally, this formalismeffectively learns task abstractions in an unsupervised manner, resulting instructured, task-aware latent spaces.",
  "Introduction": "Recent advances in foundational models have achieved remarkable success in NLP and visiontasks . Still, they fall short in addressing the complexities faced by embodied agentsin dynamic, real-world environments. For embodied intelligence, we argue that it is essentialto develop foundational world models that capture the causal nature of the world welive in and can make counterfactual predictions. Furthermore, these models should adaptdynamically to non-stationary environments. Current state-of-the-art approaches for Model-Based Reinforcement Learning (MBRL) often use probabilistic state-space models as a backbone. They learn behaviorsby making counterfactual predictions in the latent space of world models. Often theseapproaches focus on agents mastering a specific, narrow task. Throughout this work, a\"task\" refers to a particular schema of environment dynamics or a specific reward function.In real-world settings, however, tasks are frequently non-stationary and subject to changeover time. Thus, a truly intelligent agent must (1) understand the current task and (2)dynamically adapt its perception, model, and behavior to new tasks with minimal interaction. We identified a gap in the literature regarding MBRL in latent spaces that address multitasklearning and adaptation under non-stationarity. This work makes two key contributions: (1)highlighting the limitations of current state-of-the-art model-based agents in non-stationarysettings, and (2) proposing a new formalism that models non-stationarity as an additionalcausal latent variable, resulting in robust policies.",
  "POMDP formalism": "Existing state-of-the-art MBRL agents that learn in latent spaces typically relyon the partially observable Markov decision process (POMDP) formalism. In this framework,incoming sensory signals are used to update the agents belief about the hidden state of theenvironment, enabling the agent to make decisions under uncertainty. Theoretically, thePOMDP formalism could handle non-stationarity by treating slowly changing, unobservedtasks as part of the latent states . Here the assumption is that the underlying environmentis assumed to be stationary, but the agent has an incomplete view of it . Consequently,single-task frameworks that rely on latent dynamics models for learning should, in theory, beapplicable in streaming settings. However, an unstructured latent state, without inductivebiases, may hinder the learning of sample-efficient adaptive policies.",
  "HiP-POMDP formalism": "A complementary but more popular view in literature for non-stationary RL is that thecomponents of the RL (transition, reward, observation functions, action space, etc.) maydepend upon time . We build our formalism, the HiP-POMDP, upon this non-stationaryfunction view . We start by providing a formal definition of a HiP-POMDP and demonstrate that this simple modification, along with a scalable variationalinference scheme, enables learning adaptive policies across a wide range of non-stationaryscenarios where the changing tasks are unknown to the agent.",
  "{S, A, O, C, L, ps(st+1|at, st, l), po(ot|st, l), r(st, at, l), pc(Cl|l)},": "where S, A, and O are the standard state, action and observation spaces. Additionally, weintroduce a space of latent task variables L, where l L, and a space of context observationsC, where Cl C. Context observations Cl are generated from l according to p(Cl|l). Finally,the transition model ps, observation model po, and the reward function r all depend on thelatent task l. This general definition does not specify how exactly the context can be observed. Throughoutthis work, we assume Cl = {(o, a, r, o)n}Nn=1, i.e., a set of N recent transitions. However,more expressive Cl including temporal embeddings, task metadata, or any other availableinformation about the task could be used in the future. In a HiP-POMDP, the agentsobjective is to infer a latent task distribution p(l | Cl) based on the context observationsCl and learn a latent task conditional policy (at | st, l) that maximizes the expected",
  "Adaptive Latent Space Models for HiP-POMDPs": "In line with standard practices in model-based reinforcement learning (MBRL), we alternatebetween representation learning, behavior learning, and environment interactions to learnpolicies in the latent space of a world model. However, unlike existing approaches, we makeeach of these stages adaptive by conditioning them on an inferred task representation orabstraction. Thus, our work goes in the direction of building foundational multi-task worldmodels and subsequent behavior policies. For efficient learning and inference, we adopt atwo-phase approach, where we first infer the latent task, which we then use to condition themodel, actor, and critic.",
  "we chose these tuples of observations, actions, rewards, and next observations because theyworked well in practice, capturing sufficient task-relevant statistics as shown in": "Now, to form the posterior belief over the latent task variable l, we first extract encodedrepresentations xn with associated variances n from each transition tuple in the context setusing a set encoder network with shared parameters. We assume the latent representation isdistributed according to N (xn | l, diag (l)). This assumption allows us to form Gaussianbeliefs N(l, l) over l using Bayes rule. As shown in , the beliefs over l can becomputed in closed form, given a Gaussian prior p0(l). The update rules and their propertiesare detailed in Appendix B.",
  "Learning Adaptive Representations": "In this stage, we learn representations of generative world models that can make counter-factual predictions of the world states based on imagined actions. We make these learnedrepresentations adaptive to the task at hand based on the generative model shown in . We achieve this by maximizing the conditional data log-likelihood and subsequentlyderiving an evidence lower bound, as in Equation 1. A detailed derivation can be found inAppendix A.",
  "Cl": ": Hidden Parameter RSSM:The latent task variable is inferredfrom context Cl via Bayesian aggre-gation.Solid lines indicate the gen-erative process and dashed lines theinference model. Modifications from are shown in red. The outer expectation can be estimated using areparameterized sample from the latent task pos-terior p (l | Cl). The practical implementation ofthis builds upon the RSSMs used in the popularDreamer series of models , where anadditional deterministic path (using a GRU) is usedin addition to the stochastic SSM for long-term pre-dictions. The subsequent Hidden Parameter-RSSMgenerative model is shown in .",
  "A more detailed description of these scenarios is provided in Appendix C. In all experiments,proprioceptive sensors are used as the source of observations": "Algorithms Compared:We use the Dreamer as our baseline for the POMDP formalism.For the HiP-POMDP, we modify Dreamer by incorporating latent task abstractions, ensuringa fair comparison between the two approaches. Additionally, we include an \"Oracle\" baselinewhere the task is assumed to be directly observed. In this setup, the known task replacesthe inferred latent task variable l, serving as an upper bound on performance. This helpsillustrate the potential gains if perfect task information were available. We evaluate the agents in all experiments by calculating the mean return from 10 trajectoriesevery 25 epochs, each with randomly sampled environmental changes. The performancecurves are computed by averaging the results over 10 different random seeds. Our evaluationanswers the following questions: Can HiP-POMDP agents handle changing dynamics?To evaluate the effectivenessof HiP-POMDP formalism under changing dynamics, here we introduce two tasks: 1) Wemodify the standard HalfCheetah agent by adding joint perturbations of varying magnitudesrandomly, and 2) the Hopper agent by randomly changing the body mass and inertia forrandom number of body parts. Additional evaluation is introduced in Appendix C.3.",
  "As seen in HiP-POMDP agent results in robust performance gains, especially underchallenging intra-episodic changes and even competing with the Oracle": "Can HiP-POMDP agents handle changing objectives?To create non-stationaritywith changing reward functions/objectives, we modify the standard HalfCheetah such that atarget velocity needs to be reached which changes randomly. Additionally, we evaluate theagents on custom-designed multi-task benchmarks using pre-defined tasks from , whereeach task requires the agent to perform different skills (e.g., standing, running, flip) in variousdirections. As such multi-skill objective changes are more challenging, the experiments arerun over 5M steps.",
  ": 2d projections of learned latent state spaces on DMC Cheetah learning 4 skills,": "Does the HiP-POMDP agents learn meaningful latent representations?Finally,we compared the learned state-space representations (the concatenation of st and ht) of theworld model (RSSM) in both POMDP and HiP-POMDP settings. As shown in , thetask abstractions in HiP-POMDP shape a more structured and disentangled latent spacethat aligns with the inferred tasks, unlike the POMDP setting. This disentanglement wasalso observed in the latent task space representation (l) within the HiP-POMDP setup.",
  "Conclusion": "In this work, we introduced the HiP-POMDP formalism to learn adaptive world modelsand behavior policies in latent state spaces. The formalism resulted in algorithms thatlearn meaningful task abstractions and improved performance on a variety of non-stationarybenchmarks. Future work would extend these models to more high-dimensional sensoryinputs like images and point clouds. The authors acknowledge support by the state of Baden-Wrttemberg through bwHPC, aswell as the HoreKa supercomputer funded by the Ministry of Science, Research and the ArtsBaden-Wrttemberg and by the German Federal Ministry of Education and Research.",
  "R. S. Sutton. Reinforcement learning: An introduction. A Bradford Book, 2018": "M. Towers, A. Kwiatkowski, J. Terry, J. U. Balis, G. De Cola, T. Deleu, M. Goulo,A. Kallinteris, M. Krimmel, A. KG, et al.Gymnasium: A standard interface forreinforcement learning environments. arXiv preprint arXiv:2407.17032, 2024. S. Tunyasuvunakool, A. Muldal, Y. Doron, S. Liu, S. Bohez, J. Merel, T. Erez, N. H. Tim-othy Lillicrap, and Y. Tassa. dm_control: Software and tasks for continuous control. Soft-ware Impacts, 6:100022, 2020. doi: URL",
  "where , , and denote element-wise inversion, product,and division, respectively": "Discussion:This closed-form solution represents Bayesian aggregation, which can beinterpreted as a form of probabilistic attention. The uncertainty ln about each transitionin the set encoder functions as attention weights, assigning higher weights to the mostinformative transitions. The derived update equations have only a linear computationalcomplexity of O(N), while similar deep set operations (self-attention) in transformers have a complexity of O(N 2).",
  "Require:": "HyperparametersModel ParametersSeed episodes: SContext set encoder: p (l | Cl) withCollect interval: CCl = {(o, a, r, o)t}Nt=1Batch size: BRepresentation: p (st | st1, at1, ot, l)Context size: NObservation: q (ot | st, l)Sequence length: LTransition: q (st | st1, at1, l)Imagination horizon: HReward: q (rt | st, l)Learning rate: Actor: (at | st, l)Trajectory length: TCritic: v (st, l)Training epochs: EAction Repeat: R",
  "The HiP-Dreamer algorithm operates in four stages:": "1. Dynamics learning phase (Lines 6-14): Batch of B consecutive trajectory data chunks{(o, a, r, o)t}k+Lt=k D is sampled uniformly at random from the replay buffer and usedto train the world model, where k indicates a random start index uniformly sampled atrandom within the episode and is clipped to not exceed the episode length minus the trainingsequence length. Concurrently, B consecutive context chunks of N previous interactiontransitions are drawn Cl = {(o, a, r, o)t}k1t=kN D. If fewer than N previous transitionsare available, the context is zero-padded at the beginning. The context chunk Cl is usedto infer the latent task posterior distribution p (l | Cl) in closed form using Equation 8from . To approximate the outer context-dependent expectation of the objective from.2, a reparameterized sample from the inferred latent task posterior distributionl p (l | Cl) is used, allowing gradients to flow through the sampling process. The latenttask posterior p (l | Cl) is inferred once at the start of each training iteration and used forthe entire sequence length L. All model components (including the context set encoder) aretrained end-to-end using Backpropagation-Through-Time (BPTT). 2. Behavior learning phase (Lines 16-21): The actor chooses actions to predict imaginedsequences of compact model states, while the critic accumulates the future predicted rewardsbeyond the planning horizon. Both actor and critic use learned model states, benefitingfrom the world models representations. Each posterior state inferred during model trainingserves as an initial state for the actors latent trajectory imagination. The latent taskposterior p (l | Cl) remains consistent across the entire horizon length H = 15, a reasonableassumption for such short horizons. The actor aims to output actions that maximize theprediction of long-term future rewards made by the critic. The critic model is trained toregress the returns, computed as in Equation 8 from . The world model (includingthe set encoder) is fixed during behavior learning, so the actor and critic gradients do notaffect its representations. 3. Environment interaction phase (Lines 24-37): Initial context sequence chunk Cl1is collected using random actions and updated in a sliding window manner, incorporatingevery new collected transition (o, a, r, o)t to adapt to environmental changes as fast aspossible. At each point in time, the context chunk Clt is used to infer the latent taskposterior lt p (lt | Clt) and sample lt, followed by inferring an approximate latent stateposterior st p(st | st1, at1, ot, lt). An action trajectory is generated by conditioningthe actor on the inferred latent state st and latent task lt, repeating the chosen action Rtimes. 4. Evaluation phase (Lines 39-43): Every E epoch the agents performance is assessedover X episodes to estimate the mean return. Unlike data collection, the evaluation phaseuses the mean lt of the latent task distribution p (lt | Clt) enabling deterministic behavior,followed by approximating the latent state posterior p (st | st1, at1, ot, lt), repeatingthe chosen action R times.",
  "B.3Hyperparameters": "This section details only those hyperparameter values that differ from the original architec-tures or correspond to new architectural components, such as the set encoder. For additionalhyperparameters related to all algorithm stages not explicitly mentioned here, we refer to. Set encoder.The set encoder includes a shared fully connected layer with 240 units,followed by two separate fully connected layers of 240 units each. One layer computes thelatent task observation xln, while the other computes the latent task variance ln. All setencoder layers use the ELU activation function. The latent task posterior p (l | Cl) ismodelled as a multivariate Gaussian with 20 dimensions. Due to computational and timeconstraints, extensive hyperparameter optimization was not conducted. Learning updates.To train the world model, we use batches of 50 sequences of length50, as in . For the context data, we sample batches of 50 sequences from the replaybuffer, each consisting of 20 prior interaction transitions. If fewer than 20 prior transitionsare available, we use zero-padding to fill the context data before concatenating it with anyavailable transition data. The set encoder updates use the same learning rate as the worldmodel. Objective and Critic.In our objective we use the KL-Balancing technique introducedin , which we found essential for stabilizing learning when using proprioceptive inputs.Additionally, we experienced instability problems w.r.t. critic network in many environments.To stabilize the critic training further, we use target network to calculate the critic targets,using a soft-update, realized as t+1 = t +(1)t1 with = 0.05 for Gymnasium-basedenvironments. For DMC multi-task benchmarks, we use a hard update, copying the criticsweights every 100 gradient steps.",
  "Combined Changes: Concurrent occurrences of multiple dynamical changes or acombination of dynamical and objective changes": "Each category of change can occur either inter-episodically (between episodes) or intra-episodically (within episodes). The temporal dimension specifies when changes occur, whilethe structural dimension identifies which environmental aspects are affected. Addressingthese types of changes enables the design of robust model-based reinforcement learningagents capable of adapting to a broad range of environmental dynamics.",
  "C.1Dynamical Changes": "We implemented various dynamical changes by modifying Gymnasium and DMC ControlSuite environments, particularly focusing on Half Cheetah, Walker, and Hopper robots.The dynamical change values were chosen to ensure that MuJoCos models remained validwhile still challenging pre-trained model-free and model-based agents. In environments with dynamical changes, the Oracle agent is conditioned on a vector thatfully describes the changes. For instance, wind friction on Half Cheetah is represented bya vector of Cartesian forces acting on each body part. Joint perturbations are similarlyrepresented, while actuator masking is indicated by a one-hot vector. In Hopper and Walker,the Oracle receives vectors representing body masses, inertia, and contact friction coefficients,respectively. Details for each environment are provided below:",
  "Intra-Episodic Actuator MaskingIntra-Episodic Wind FrictionIntra-Episodic Contact Friction": ": Dynamical changes on HalfCheetah and Walker. The first row shows inter-episodicdynamical changes, whereas the second row shows intra-episodic dynamical changes with afrequency of 200 environmental steps. illustrates additional experiments involving dynamical changes in the environ-ment. Across both inter- and intra-episodic changes, task-conditioned agents exhibit similarperformance to the vanilla agent, with only minor differences observed. On one hand, this result suggests that the vanilla agent is capable of adapting to variousdynamic changes, indicating that approaches based on the POMDP formalism can indeedlearn representations that support adaptive behavior. On the other hand, the HiP-POMDPformalism generally performs comparably or slightly better, highlighting the benefits andexpressiveness of a learned latent task representation. Interestingly, in some cases, the task-inference agent even outperforms the Oracle agent,despite the former having to learn and utilize a task representation for each task, while theOracle agent is directly provided with task change information. This advantage may be dueto the learned task representation capturing additional task-relevant information beyond thetask changes alone, or it could suggest that the ground-truth task representation providedto the Oracle agent is not optimally structured for adaptation. This observation warrantsfurther investigation in future work.",
  ": Vanilla Dreamer agent under inter-episodic reward change on Half Cheetah withdifferent reward loss scaling factors": "Breaking point of Vanilla Dreamer under objective changes.To investigate thevanilla agents failures under target reward-changing experiments, we adjust the reconstruc-tion loss by giving more weight to reward reconstruction to offset potential higher loss weightsfrom the multi-dimensional observations. illustrates the effect of scaling the rewardreconstruction loss differently. Our observations show that increasing the reward reconstruction weight factor also raisesthe KL divergence between the prior and posterior distributions over the latent state,along with an increased observation reconstruction loss, ultimately degrading performance.These findings suggest the optimization process struggles to balance reconstruction andregularization loss terms, possibly leading to overfitting to specific rewards within eachmini-batch.",
  ": Intra-episodic target velocity change on Half Cheetah. The change frequency isreduced from left to right": "Breaking point of Task-inference Dreamer under objective changes.highlights a breaking point in the latent task inference mechanism when the target velocitychanges every 200 environmental steps.We hypothesize that the standard latent taskaggregation requires more time to infer a new task belief accurately, a necessity for effectiveadaptation, especially when target velocity undergoes drastic changes near boundary values.To address this, we are currently experimenting with a modified latent task update mechanismthat reduces the influence of older transitions, introducing a forgetting effect that couldhelp the agent adapt more rapidly to abrupt task shifts. DMC-Multitask benchmarksWe also assess all agents under more complex objectivechange settings, where each agent must learn multiple skills simultaneously. showsresults on a variety of different custom-designed multi-task benchmarks.",
  ": Inter-episodic objective changes on DMC Multi-task benchmarks on Half Cheetah,Walker, Cup, and Pendulum domains. Each number indicates the number of tasks in eachexperiment": "The vanilla agent struggles to learn multiple skills simultaneously. We hypothesize thislimitation arises from multiple interferences in the learned latent state, resulting in a task-independent latent space structure. In such cases, the reward predictor cannot estimate rewards accurately, leading to suboptimal performance. However, providing task informa-tion enables better multi-skill learning across all benchmarks, potentially creating a morestructured latent space. Additional evidence for these hypotheses is presented in AppendixC.6.",
  ": Multiple changes on HalfCheetah. The first row shows combined dynamical andobjective changes, whereas the second row shows combined dynamical changes": "As seen in , the vanilla agent demonstrates adaptability under multi-modal dynam-ical changes, however, it struggles with combined dynamical and objective changes, largelydue to its limited ability to handle shifts in objectives. Introducing a task representation notably enhances performance, as both the task-inferenceand oracle agents successfully adapt across all combined change scenarios. However, underscenarios involving only combined dynamical changes, the oracle agent exhibits slowerlearning compared to the vanilla agent. This slower adaptation raises questions about thepotential influences of the task representation, warranting further investigation.",
  "C.6Latent Space Visualizations": "This section presents 2D projections of both the world models latent state and the latent taskspaces. The visualizations are generated by recording trajectories of posterior latent states(incorporating both deterministic and stochastic components) and latent task representationsduring evaluations across various tasks, with a final 2D projection achieved using t-SNE . The primary objective of these visualizations is to explore two key questions: (1) Doesconditioning the MBRL agent on either a ground truth or learned task representationintroduce any structural changes in the latent state space? (2) How does the structure ofthe latent state space correlate with the agents performance?",
  ": Comprehensive latent state and task space visualizations across various environ-ments experiencing inter-episodic dynamical changes": "In the visualizations, the vanilla agentgrounded in the POMDP formalismdemonstratestask-dependent latent space structuring under different dynamic changes. Examining theseprojections along with empirical results from reveals a positive correlation betweenthe structured latent space and the agents performance. Notably, conditioning the agent ontask representations produces an even more task-specific latent space structure. Task conditioning appears to provide two main advantages: (1) a more structured latentspace enables enhanced data modeling, as task-specific representations minimize interferences,thus improving data reconstruction; and (2) task conditioning helps disambiguate overlappinglatent states, especially when a state recurs across tasks. This disambiguation contributes tobetter data modeling and aids in achieving adaptive behaviors.",
  ": Comprehensive latent state and task space visualizations for agents learning toadapt to varying objectives, including different target velocities or skills": "reveals two critical observations. When the rewards structure changes, the vanillaagents latent space does not organize itself by task, leading to substantial interference andmaking it challenging for all agents components to infer task information, contributing tosuboptimal performance as evidenced in for changing rewards. In contrast, conditioning the agent on task representations produces a latent space that isnot only more task-aware but also better suited for concurrent multi-task learning. Similarlyto the findings in Section C.6.1 under dynamics changes, a positive correlation emergesbetween the structured latent space and the agents final performance. However, for themost challenging skill-learning experiments, the task inference approach organizes latentspace with some tasks represented jointly, which can hinder unique task identification andcontribute to the observed performance gap in skill-learning tasks as shown in ."
}