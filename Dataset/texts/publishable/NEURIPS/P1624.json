{
  "Abstract": "Determining the 3D orientations of an object in an image, known as single-imagepose estimation, is a crucial task in 3D vision applications. Existing methods typi-cally learn 3D rotations parametrized in the spatial domain using Euler angles orquaternions, but these representations often introduce discontinuities and singulari-ties. SO(3)-equivariant networks enable the structured capture of pose patterns withdata-efficient learning, but the parametrizations in spatial domain are incompatiblewith their architecture, particularly spherical CNNs, which operate in the frequencydomain to enhance computational efficiency. To overcome these issues, we pro-pose a frequency-domain approach that directly predicts Wigner-D coefficientsfor 3D rotation regression, aligning with the operations of spherical CNNs. OurSO(3)-equivariant pose harmonics predictor overcomes the limitations of spatialparameterizations, ensuring consistent pose estimation under arbitrary rotations.Trained with a frequency-domain regression loss, our method achieves state-of-the-art results on benchmarks such as ModelNet10-SO(3) and PASCAL3D+, withsignificant improvements in accuracy, robustness, and data efficiency.",
  "(e.g., Gaussian, vMF, Bingham ..)": ": Types of representations for 3D ro-tation prediction. Existing methods considerpredicting 3D rotations in the spatial domain. Ourmethod predicts Wigner-D coefficients in the fre-quency domain, to obtain accurate pose in contin-uous space using an SO(3)-equivariant network. Predicting the 3D pose of objects, i.e., positionand orientation, in 3D space from an image iscrucial for numerous applications, including aug-mented reality , robotics , au-tonomous vehicles , and cryo-electronmicroscopy . Estimating 3D orientation isparticularly challenging due to rotational symme-tries and the non-linear nature of rotations. Inaddition, unlike translations, rotations introduceunique challenges such as gimbal lock and therequirement for continuous, singularity-free rep-resentations. Existing methods often learn 3Drotations using spatial domain parameterizationslike Euler angles, quaternions, or axis-angle rep-resentations, as illustrated in . However,these parameterizations suffer from issues suchas discontinuities and singularities ,which can hinder the performance and reliability.",
  "arXiv:2411.00543v2 [cs.CV] 4 Nov 2024": "change with the 3D rotation of the input, maintaining rotational consistency between the inputand output across network layers. Despite its importance, many existing methods often design networks without considering SO(3)-equivariance, resulting in suboptimal performancewhen dealing with 3D rotations. In addition, in the context of spherical CNNs for efficient SO(3)-equivariant operations, the 3D rotation parametrization in the spatial domain isinadequate because these SO(3)-equivariant networks operate in the frequency domain. To address these challenges, we propose an SO(3)-equivariant pose harmonics regression networkthat directly predicts Wigner-D coefficients in the frequency domain for 3D rotation regression.Building on prior work , our method leverages the properties of spherical CNNs , whichoperate in the frequency domain, to guarantee SO(3)-equivariant output representation. By directlyregressing Wigner-D matrix coefficients, our approach eliminates the need to convert outputs intospatial representations during training, ensuring alignment with the operations of spherical CNNs.This design allows us to bypass the limitations inherent in traditional spatial parameterizationssuchas discontinuities and singularities resulting in more precise and continuous poseestimation. We further introduce a frequency-domain MSE loss to enable continuous training of3D rotations, with the flexibility to incorporate distributional losses for effectively capturingrotational symmetries in objects. Our method achieves state-of-the-art performance on standard singleobject pose estimation benchmarks, including ModelNet10-SO(3) and PASCAL3D+, demonstratinghigh sampling efficiency and strong generalization to unseen 3D rotations.",
  "Related Work": "SO(3) pose regression. The choice of rotation representation is a fundamental aspect of the currentSO(3) pose estimation methods. In the early stages of deep learning, methods for SO(3) poseregression choose the rotation representation by direct cosine matrices , Euler angles , quaternions , and axis-angles . However, accordingto , for any representation R in a Euclidean space of dimension d 4, such as Euler anglesand quaternions, R is discontinuous and unsuitable for deep learning. In addition, Euler anglescan cause gimbal lock, which restricts certain rotations, whereas quaternions avoid this issue buttheir double representation of rotations in SO(3) can lead to complications such as local minimain optimization problems. As an alternative, a continuous 6D representation with Gram-Schmidtorthonormalization and 9D representation with singular value decomposition (SVD) havebeen proposed, and proposes manifold-aware gradient layer to facilitate the learning of rotationregression. Denoising diffusion models are employed in the context of SO(3) pose regression , orfor solving pose estimation by aggregating rays . In contrast to existing SO(3) pose regressionmethods that formulate rotation representations in the spatial domain, we define the Wigner-Dcoefficients as the output of the network in the frequency domain, using SO(3)-equivariant networks. Pose estimation with a parametric distribution. To model rotation uncertainty, parametric distribu-tions on the rotation manifold are employed in a probabilistic manner. predicts parameters of amixture of von Mises distributions over Euler angles using Biternion networks. utilize theBingham distribution over unit quaternions to generate multiple hypotheses of rotations. leverage the matrix Fisher distribution to construct a probabilistic model for SO(3) pose esti-mation. Additionally, propose the Rotation Laplace distribution for rotation matrices on SO(3)to suppress outliers, and the Quaternion Laplace distribution for quaternions on S3. Nevertheless,parametric models rely on predefined priors. In contrast, our model uses non-parametric modelingduring inference to capture more complex pose distributions. Pose estimation with a non-parametric distribution. Probabilistic pose estimation can alsobe achieved by predicting non-parametric distributions. IPDF introduces the estimation ofarbitrary, non-parametric distributions on SO(3) using implicit functions with MLPs, and Hyper-PosePDF uses hypernetworks to predict implicit neural representations by Fourier embedding.ExtremeRotation predicts discretized distributions over N bins for relative 3D rotations trainedwith cross-entropy loss. RelPose uses an energy-based formulation to represent distributionsover the discretized space of SO(3) relative rotation. Several SO(3)-equivariant modeling methodsconstruct non-parametric distributions by utilizing icosahedral group convolution , projectingimage features orthographically onto a sphere , and satisfying consistency properties of SO(3)by translating them into an SO(2)-equivariance constraint . RotationNormFlow uses dis-crete normalizing flows to directly generate rotation distributions on SO(3). These non-parametric methods, which are trained with loss functions in discretized distributions, such as cross-entropy andnegative log-likelihood, tend to lose precision in rotation prediction. In contrast, our method predictscontinuous SO(3) transformations through regression, eliminating the need to approximate SO(3)poses within a discretized space and enabling our model to achieve accurate 3D rotations.",
  "Representations of Rotations": "Rotation representation in spatial domain. In 3D rotation, Euler angles are a common SO(3)representation but suffer from non-uniqueness and gimbal lock, making them less suitable forneural network predictions. Quaternions offer a solution by preventing gimbal lock, but their non-unique representation (q and -q) can complicate certain optimization processes. The axis-anglerepresentation is intuitive but can encounter singularities. The 6D and 9D representations providenewer approaches that simplify optimization in deep networks by avoiding non-linear constraints andensuring orthogonality. However, they also introduce complexities in maintaining constraints duringthe learning process. Thus, choosing an appropriate rotation representation is crucial for accuratepose estimation in various computational applications. For a detailed explanation, please refer toSec. A.1 and an overview of learning 3D rotations in . Rotation representation in frequency domain. In the frequency domain, 3D rotation is managedby manipulating spherical harmonics coefficients. Spherical harmonics, denoted as Y lm(, ), arefunctions defined on the surface of a sphere using polar () and azimuthal () angles. These harmonicsare characterized by their degree l and order m, truncated to a maximum degree L for computationalfeasibility. The rotation of spherical harmonics is represented by the shift theorem , where arotation operator g acts on spherical harmonics, transforming them via a matrix Dlmn(g):",
  "|n|lDlmn(g)Y ln(x).(1)": "This matrix, part of the irreducible unitary representation of SO(3), expresses how each harmonicchanges under rotation, summing over all orders n from l to l, called Wigner-D matrix. TheWigner-D rotation representation is not limited to a specific case of 3D rotations but can be convertedfrom any 3D rotation representation, such as Euler angles, quaternions, and 3D rotation matrices.Our SO(3) equivariant network predicts the Wigner-D representation in the frequency domain insteadof predicting rotations in the spatial domain. For a detailed explanation, please refer to Sec. A.2.",
  "SO(3)-Equivariance": "Equivariance. Equivariance is a useful property to have because transformations T applied tothe input produce predictable and consistent output of the features through transformations ,enhancing both interpretability and data efficiency. For example, a feature extractor is equivariant toa transformation if applying the transformation to the input and then applying the extractor producesthe same output as applying the extractor first and then the transformation:",
  "(Tg(x)) = T g((x))(2)": "where Tg and T g represent transformations acting on a group g G of the input and output spaces,respectively. This ensures that the networks output remains consistent with transformations appliedto the input. For translation groups, convolution inherently maintains this property. For rotations,additional rotation-equivariant layers are integrated into the network design. Group-equivariant convolutional networks extend this concept to complex groups like rotationsor other symmetries. By designing convolutions that are equivariant to these group actions, thesenetworks can handle a broader range of transformations. This can be mathematically described as:",
  "Fourier transformer": ": Overall architecture. Our network for SO(3)-equivariant pose estimation consists of fourparts: feature extraction, spherical mapper, Fourier transformer, and SO(3)-equivariant layers. First,we extract a feature map using a pre-trained ResNet. Next, the spherical mapper orthographicallyprojects the extracted feature map onto a spherical surface. The Fourier transformer converts thisspatial information into the frequency domain. We utilize spherical convolutions to obtain the finalWigner-D harmonics coefficients which represent SO(3) rotations of spherical harmonics, whereM denotes the total number of Wigner-D matrix coefficients.",
  "ensures that the network remains equivariant to the actions of the group G, allowing it to handleinputs transformed by any element of this symmetry group": "On the sphere in 3D, however, there is no straightforward way to implement a convolution inthe spatial domain due to non-uniform samplings . This challenge arises because traditionalconvolution operations rely on uniform grid structures, which are not applicable to spherical data.To address this, specialized methods such as spherical convolutions or graph-based approaches areemployed to handle the unique structure and sampling patterns of spherical data, thereby ensuringeffective feature extraction and equivariance on spherical surfaces. Spherical convolutions for SO(3)-equivariance. To effectively analyze complex spatial data,such as for volumetric rendering and 3D pose estimation, it is necessary to develop functions withequivariance to the SO(3) group. Early methods for spherical convolution were defined by computingFourier transforms and convolution on the 2-sphere . However, the output of these sphericalconvolutions is a function on the sphere, not on SO(3). Spherical CNNs extended this approachto effectively convolve on the SO(3) group. Using the truncated Fourier transform, signals on S2 aremodeled with spherical harmonics Y ln, and on SO(3) with Wigner-D matrix coefficients Dlmn. To efficiently compute the S2 and SO(3) convolution, generalized fast Fourier transforms (GFFTs)demonstrate optimized computation . The GFFTs show robustness and efficiency in sphericalsignal processing, where the spectral group convolutions become simpler element-wise multiplicationsin the Fourier domain. Specifically, for S2, the process uses vectors of spherical harmonic coefficients,forming a block diagonal matrix analogous to SO(3) convolution. Both convolutions on S2 andSO(3) generate output signals that reside on SO(3).",
  "SO(3)-Equivariant Pose Harmonics Predictor": "The goal of our network is to accurately predict the SO(3) pose of an object in an image. Toachieve, we employ spherical CNNs to obtain SO(3)-equivariant representation, and our model istrained with frequency-domain supervision using Wigner-D coefficients. This approach enhances dataefficiency by capturing patterns with fewer training samples and ensures precise SO(3) pose estimationby aligning the parametrization of 3D rotations with the Wigner-D matrices in the frequency domain. provides an overview of our SO(3)-equivariant pose estimation network. In Sec. 4.1, weexplain the steps for obtaining the Wigner-D representation, following the method described in .In Sec. 4.2, we introduce a frequency-domain regression loss, where we train the network using MSEloss between the predicted representation and the ground truth (GT) Wigner-D coefficients. Finally,in Sec. 4.3, we describe the inference process by constructing an SO(3) grid for evaluation.",
  "In this subsection, we explain our SO(3)-equivariant pose estimation network, highlighting that itskey components are shared with the architecture of": "Image feature extraction. We first apply a feature extractor to obtain an image feature map thatencodes semantic and geometric information: F = (I), where F RCHW and denotesResNet. We utilize a ResNet feature extractor that is pre-trained on ImageNet same to . We then perform dimensionality reduction on the image feature F to match the inputdimension of the subsequent spherical feature using a 1x1 convolution: F = Conv11(F), whereF RCHW .",
  "SO(3)-equivariant convolution layers": ": Illustration of spherical mapper andspherical convolution for SO(3)-equivariance.This structure allows for the prediction of 3D ro-tations while preserving the SO(3)-equivarianceof the input structure. Predicting the Wigner-Dharmonics enables continuous 3D rotation mod-eling, without discretizing the group actions.1 Spherical mapper. To begin, we lift the im-age features to the 2-sphere using orthographicprojection . This involves mapping the 2Dfeature F to a spherical feature RCp,where p denotes the number of points on thesphere. The orthographic projection links pixelsin the image space to points on the sphere by or-thogonally mapping S2 coordinates to the imageplane, thereby preserving the spatial informationof the dense 2D feature map. Initially, we model spherical coordinates usingan S2 HEALPix grid over a hemisphere.Within this hemisphere, the set {xi} S2 rep-resents the vertices of the grid. Each vertex xi ismapped to a position P(xi) on the image plane.Formally, the orthographic projection P maps3D coordinates on the hemisphere to 2D coordi-nates on the image plane as P(x, y, z) = (x, y). Due to the fixed perspective, only one hemi-sphere of the sphere is visible, resulting in alocalized signal (x) = F (P(x)) supportedover this hemisphere. The value of (xi) is ob-tained by interpolating F at the pixels near P(xi) in the image space using an interpolation function, so (xi) = (F , P(xi)). illustrates the processes of the spherical mapper, and thefollowing frequency domain conversion and spherical convolution for SO(3)-equivariance. Convert to the frequency domain. The transition of the spherical feature into the frequencydomain is achieved using the fast Fourier transform (FFT) adapted for spherical topology. Byemploying the FFT, we efficiently convert to spherical signals S, represented as a sum of sphericalharmonics. This transformation allows us to capture and manipulate the spatial frequencies inherentto the spherical surface. Specifically, the transition to the frequency domain enables the derivationof Wigner-D coefficients, which effectively model the SO(3). The Fourier series of S is truncatedat frequency L , expressed as: S(x) Ll=0lm=l clmY lm(x), where S RCN, N is thetotal number of spherical harmonics determined by the maximum frequency L, and Y lm(x) are thespherical harmonics. Operating in the frequency domain facilitates the effective convolution of signalson the sphere (S2) and within the 3D rotation group (SO(3)), preserving the geometric properties ofinput features through spherical equivariance. To address sampling errors from approximating the Fourier series via truncation, we apply twotechniques proposed in . First, to prevent discontinuities on the 2-sphere, we gradually decreasethe magnitude of projected features near the image edge: (xi) = w(xi) (xi). Second, for eachprojection, we randomly select a subset of grid points on the S2 HEALPix grid as a dropout. Spherical convolution for SO(3)-equivariance. We aim to predict 3D rotations while preservingSO(3)-equivariance using the projected features on sphere. First, the spherical signal S is processedwith an S2-equivariant convolutional layer . Unlike conventional convolutions with localfilters, S2 convolution uses globally supported filters, offering a global receptive field. This allowsfor a shallower network, which is important due to the high computational and memory demands ofspherical convolutions at a high bandlimit L.",
  "We use the visualization tools available in the source code of the image2sphere GitHub repository": "In this stage, we obtain SO(3) representations inherent to spherical CNNs . The output of S2convolutions lies in the SO(3) domain because S2 convolutions replace translations with rotations,and the space of 3D rotations forms the SO(3) group. Consequently, we obtain feature results sizedin RCM, where C is the hidden dimension of the SO(3) features, and M is the total number ofWigner-D matrix coefficients, given by M = Ll=0(2l + 1) (2l + 1), created by SO(3) irreps. We apply non-linearities between convolutional layers by transforming the signal to the spatial domain,applying a ReLU, and then transforming back to the frequency domain, following the approach ofspherical CNNs . This method can be extended to FFT-based approximate non-linearity andequivariant non-linearity for tensor field networks . Subsequent to the S2-equivariant convolutional layer, we perform an SO(3)-equivariant groupconvolution using a locally supported filter to refine the SO(3) pose space. Unlike typicalspherical CNNs, we bypass the inverse fast Fourier transform (iFFT) and instead use the outputharmonics of Wigner-D prediction. This approach, unlike that of , improves the efficiency of ourmethod. The final output of the equivariant network is the Wigner-D matrix coefficients RM.",
  "Frequency-Domain Regression Loss": "The output of the SO(3)-equivariant convolutional layers is a linear combination of Wigner-Dmatrices, represented as a flattened vector of the Wigner-D coefficients. The output indicatesspecific object orientations in an image. To generate the ground-truth (GT) Wigner-D coefficients,we convert the GT 3D rotations from Euler angles using the ZY Z sequence of rotation R, expressedas R = Rz()Ry()Rz() to the Wigner-D matrices Dlmn(, , ), where D represents an actionof the rotation group SO(3). We calculate the Mean Squared Error (MSE) loss as follows:",
  "m=lwl(lm GTlm )2,(4)": "where wl are weights assigned to each harmonic frequency level l, normalizing the output Wigner-D matrices for a frequency-domain specific MSE loss. This loss function enables continuousprediction of SO(3) poses using SO(3)-equivariant networks, whereas the previous methods predicted outputs in a discretized distribution, leading to degradation in prediction precision. Withthis re-parametrization in the frequency domain, we use Euclidean distance because it is simple yeteffective for pose prediction. It allows straightforward calculation while considering both the directionand magnitude of the vectors. Many distance metrics defined in the spatial domain maynot be directly appropriate for the frequency domain without adaptation. For example, cosine andangular distances ignore magnitude, where the amplitude of frequency components carries significantinformation. Chordal and geodesic distances require normalization, can be less intuitive, and ofteninvolve more complex computations.",
  "Inference": "For evaluation, the output Wigner-D representation is converted to an SO(3) pose in the spatialdomain. illustrates the inference process inspired by . Specifically, we map thepredicted Wigner-D coefficients from the frequency domain to a 3x3 rotation matrix R by querying on a predefined SO(3) grid. To achieve this mapping, we calculate the similarities between theoutput vector and the SO(3) grid P( | I). These similarities are then normalized using a softmaxfunction to produce a non-parametric categorical distribution P(R | I). The final 3D rotation matrixR is determined either by taking the argmax of this distribution or by applying gradient ascent . To generate the SO(3) equivolumetric grids, we utilize the hierarchical equal area isolatitude pixelationof the sphere (HEALPix) , consistent with methods used in . To lift theS2 HEALPix to SO(3) HEALPix, we create equal-area grids on the 2-sphere and cover SO(3) bythreading great circles through each point using the Hopf fibration from . This inference scheme effectively models objects with ambiguous orientations or symmetries by em-ploying multiple hypotheses, thereby overcoming the limitations of single-modality predictions .In addition to joint training with distributional cross-entropy loss , our network can model thenon-parametric and multi-modal distribution in pose space to address pose ambiguity and aid inmodeling 3D symmetry.",
  "Implementation Details": "We input a 2D RGB image I R3224224. A ResNet backbone, pretrained on ImageNet, extractsfeature maps of shape F R204877. We then perform dimension reduction using a 1x1 convolutionto obtain F R51277. In the spherical mapper, the features are mapped onto an S2 grid generatedby recursion level 2 of HEALPix on half of the sphere, and then sampled at 20 points, resulting in R51220. By converting to the frequency domain, the spherical signals S R51249 areobtained. The Wigner-D representation is implemented in a flattened form across different frequencylevels. For example, the matrix coefficients at a frequency level l are represented as a flattened vectorof size (2l + 1) (2l + 1). We use a maximum frequency level of L = 6, resulting in a total size ofM = 455, computed as 6l=0(2l + 1) (2l + 1). These coefficients are then flattened into a singlevector for the Wigner-D prediction. The spherical convolution on the S2 kernel uses an 8-dimensionalhidden layer with global support to obtain intermediate SO(3) features in R8455. After nonlinearactivation, we finally obtain the 1-dimensional output R1455 using an SO(3) convolution witha locally supported filter to handle rotations up to 22.5. At inference, we employ a recursive level 5of SO(3) HEALPix grid with 2.36 million points, achieving a precision of 1.875, as in .",
  "Benchmarks": "ModelNet10-SO(3) is a common dataset for estimating a 3D rotation from a single image.The images are created by rendering CAD models from the ModelNet10 dataset . The datasetincludes 4,899 objects across 10 categories, each image is labelled with a single 3D rotation. Therotations are uniformly sampled from each CAD model. From a single CAD model, the training setcomprises 100 3D rotations on SO(3), while the test set includes 4 unseen 3D rotations. PASCAL3D+ is a widely-used benchmark for evaluating pose estimation in images captured inreal-world settings. It includes 12 categories of everyday objects, which were created by manuallyaligning 3D models with their corresponding 2D images. This dataset presents challenges due to thesignificant variation in object appearances, the high variability of natural textures, and the presenceof novel object instances in the test set. To be consistent with the baselines, we conduct training dataaugmentation using synthetic renderings . ModelNet10-SO(3) Few-shot Views is used to evaluate the data efficiency of pose estimation models.Unlike the original ModelNet10-SO(3) , we have expanded this to evaluate various amounts oftraining data, by setting the number of training views per CAD model to 3, 5, 10, 20, 30, 40, 50, 70,90, and 100. This benchmark verifies the sampling efficiency of our equivariant networks. We usethe same test dataset as that of ModelNet10-SO(3).",
  ": Results on PASCAL3D+ withResNet-101 backbone. Scores are averagedacross all twelve classes": "310 2050701000.0 0.2 0.4 0.6 0.8Acc@15 310 205070100 # of Training Views 0.0 0.2 0.4 0.6 0.8Acc@30 310 205070100 120Rot Err. (Median) Zhou et al. (2019) Brgier (2021)I-PDFI2SRotLaplace (Res50)RotLaplace (Res101)ours (Res50)ours (Res101) : Experiment on ModelNet10-SO(3) with few-shot training views. Results with solidlines of I-PDF , I2S , and RotLaplace denote to a ResNet-50 backbone, while dottedlines indicate a ResNet-101 backbone. Our method outperforms all metrics and reduces trainingviews. Baseline results were obtained using the source code provided by the authors.",
  "Results": "shows the pose estimation results on the ModelNet10-SO(3) dataset, where our model outper-forms all baselines across multiple evaluation metrics. Notably, our Wigner-D harmonics predictionnetwork surpasses methods in non-probabilistic rotation estimation , parametric probabilitydistribution estimation , and non-parametric distribution prediction ,by leveraging SO(3) equivariance and rotation parametrization in the frequency domain. shows the results on the PASCAL3D+ benchmark. Our SO(3) Wigner-D harmonics predictorachieves state-of-the-art performance on these challenging benchmarks. It demonstrates robustnessto changes in object appearance, real textures, and generalizes well to novel object instances. Weadditionally report fine-scale accuracies, i.e., Acc@3, Acc@5and Acc@10, in Tables A1 and A2,and class-wise evaluation of the results in Tables A3 and A4 of appendix B.",
  "Few-shot Training Views": "illustrates the pose estimation results on ModelNet10-SO(3) with few-shot training views.Notably, as the number of training views from a single CAD model decreases, our model consistentlyachieves the highest accuracy and lowest error. This performance surpasses the baselines of directrotation regression , parametric distribution parameters regression , and non-parametricdistribution estimation . This few-shot training experiment verifies that our SO(3)-equivariantmodel contributes to superior data efficiency and generalization to unseen rotations.",
  "SO(3) Parmetrizations": "shows results validating design choices on ModelNet10-SO(3) with 20-shot training views,using a ResNet-50 backbone. First, we compare the effects of different rotation parametrizations onmodel performance by changing the prediction head and ground-truth rotations, to verify our proposedWigner-D compared to other rotation representations. For all cases, we retained the backbonenetworks and SO(3)-equivariant layers. The only modifications were the output prediction dimensionsize and the ground-truth rotation representation. Our Wigner-D parametrization outperforms Eulerangles (3 dim.), quaternions (4 dim.), axis-angle (4 dim.), and rotation matrices (9 dim.). Thisdemonstrates that frequency domain rotation re-parametrization enables accurate 3D rotations whenused with the SO(3)-equivariant spherical CNNs in the frequency domain.",
  "Loss Functions": "compares various loss functions trained on ModelNet10-SO(3) with 20-shot learning using aResNet-50 backbone. While Huber and L1 losses are alternatives, they do not perform as well asMSE in our context. Cosine loss measures only angle distances between vectors, ignoring magnitude,which is an essential factor in frequency-domain applications. Geodesic loss in the frequency domainis ineffective because it requires separate calculations for each frequency level of the Wigner-Dmatrix, potentially losing the precision of the original 3D rotation, as we truncate the Fourier basis ata frequency level of 6. Therefore, we choose MSE regression loss for our design choice given itssimplicity and effectiveness.",
  "Random SO(3)0.67970.694622.16SuperFibonacci0.67850.693222.15": ": Comparison of results without theSO(3)-equivariant module and with differentSO(3) grids at inference. The first group showsthe results using conventional convolution insteadof equivariant convolution. The second grouppresents the results with different SO(3) grids atinference time. ours denotes the proposed modelarchitecture. In , we experiment with replacing SO(3)-equivariant layers with conventional convolu-tional layers. Specifically, we use two-layer 1x1convolutional layers with ReLU activation and afinal linear layer with 455 output channels. Theresults indicate that CNNs without equivariantlayers perform poorly, especially in terms of me-dian error, suggesting that using the equivariantnetworks generalize better to unseen samples.Additionally, the Wigner-D prediction should bepaired with an SO(3)-equivariant network to en-able reliable 3D rotation prediction in the fre-quency domain. Lastly, we evaluate the impact of different SO(3)grids by switching from a HEALPix grid to arandom SO(3) grid and super-Fibonacci spirals , which use the same number of SO(3) rotations.Our Wigner-D harmonics predictor performs consistently, regardless of the SO(3) grid sampling typeat inference time.",
  "Lwigner+Ldist4.114.433.765.593.932.856.206.665.856.11": ": Results on SYMSOL I and II . We report the average log likelihood on both parts ofthe SYMSOL datasets. Lwigner denotes the results obtained with our Wigner-D regression loss. Ldistdenotes the results using the distribution loss from I-PDF , which are the same as the results ofI2S . The third row presents the results of joint training using both our regression loss and thedistribution loss.",
  "Results on SYMSOL": "shows symmetric object modeling on the SYMSOL datasets . Compared to the first rowand second row , our model with only the Wigner-D regression loss derives on sharp modalities,which can be less effective than for symmetric objects in SYMSOL I. For clearly defined pose cases (e.g., SphereX in SYMSOL II), our Wigner-D loss alone performswell. However, in other SYMSOL II scenarios, the sharp distributions produced by our model canlead to low average log likelihood scores. This metric is particularly harsh on models with sharppeaks, making them vulnerable to very low scores in some failure cases.",
  ": Visualization of pose distribution onSYMSOL. The results are obtained by joint train-ing with both our regression loss and the cross-entropy distribution loss": "In the third row, joint training of our method withthe distribution loss achieves better per-formance than the baseline , demonstratingits ability to model symmetric objects. These re-sults highlight the potential of our method in han-dling complex symmetries and predicting multi-ple hypotheses. shows the visualizationof pose distribution on the SYMSOL I and IIdatasets. Most real-world objects have unique, unam-biguous poses, validating our single pose re-gression method (e.g., ModelNet10-SO(3), PAS-CAL3D+). If the task needs to cover symmetriccases, our model can be modeled with distribu-tion loss .",
  "Conclusion": "In this paper, we proposed a novel method for 3D rotation estimation by predicting Wigner-Dcoefficients directly in the frequency domain using SO(3)-equivariant networks. Our approacheffectively overcomes the limitations of existing spatial domain parameterizations of 3D rotations,such as discontinuities and singularities, by aligning the rotation representation with the operationsof spherical CNNs. By leveraging frequency-domain regression, our method ensures continuousand precise pose predictions and demonstrates state-of-the-art performance across benchmarkslike ModelNet10-SO(3) and PASCAL3D+. Additionally, it offers enhanced data efficiency andgeneralization to unseen rotations, validating the robustness of SO(3)-equivariant architectures. Ourmethod also supports the modeling of 3D symmetric objects by capturing rotational ambiguities, withfurther accuracy improvements achievable through joint training with distribution loss. Future workcan build on this foundation to explore frequency-domain representations in 3D vision tasks, developmore effective rotation representations for 3D space, and further optimize computational efficiency.",
  "Romain Brgier. Deep regression on manifolds: a 3d rotation case study. In 2021 International Conferenceon 3D Vision (3DV), pages 166174. IEEE, 2021. 2, 8, 15, 19, 20, 23": "Michel Breyer, Jen Jen Chung, Lionel Ott, Roland Siegwart, and Juan Nieto. Volumetric grasping network:Real-time 6 dof grasp detection in clutter. In Conference on Robot Learning, pages 16021611. PMLR,2021. 1 Mai Bui, Tolga Birdal, Haowen Deng, Shadi Albarqouni, Leonidas Guibas, Slobodan Ilic, and NassirNavab. 6d camera relocalization in ambiguous scenes via continuous multimodal inference. In ComputerVisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, PartXVIII 16, pages 139157. Springer, 2020. 1, 2 Ruojin Cai, Bharath Hariharan, Noah Snavely, and Hadar Averbuch-Elor. Extreme rotation estimationusing dense correlation volumes. In Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 1456614575, 2021. 2 Jiayi Chen, Yingda Yin, Tolga Birdal, Baoquan Chen, Leonidas J Guibas, and He Wang. Projectivemanifold gradient layer for deep rotation regression. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, pages 66466655, 2022. 2, 15, 23 Kefan Chen, Noah Snavely, and Ameesh Makadia. Wide-baseline relative camera pose estimation withdirectional learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 32583268, 2021. 2, 15 OJ Cobb, CGR Wallis, AN Mavor-Parker, A Marignier, MA Price, M DAvezac, and JD McEwen. Efficientgeneralized spherical cnns. In ICLR 2021-9th International Conference on Learning Representations,volume 9. ICLR 2021-9th International Conference on Learning Representations, 2021. 2",
  "Frellsen, Nuri Jung, Sophia Sanborn, Mingjian Wen, Josh Rackers, Marcel Rd, and Michael Bailey.Euclidean neural networks: e3nn, Apr. 2022. 22": "Igor Gilitschenski, Roshni Sahoo, Wilko Schwarting, Alexander Amini, Sertac Karaman, and Daniela Rus.Deep orientation uncertainty learning based on a bingham loss. In International conference on learningrepresentations, 2019. 2 Krzysztof M Gorski, Eric Hivon, Anthony J Banday, Benjamin D Wandelt, Frode K Hansen, MstvosReinecke, and Matthia Bartelmann. Healpix: A framework for high-resolution discretization and fastanalysis of data distributed on the sphere. The Astrophysical Journal, 622(2):759, 2005. 5, 6, 22",
  "HEALPix Collaboration. Healpix: Hierarchical equal area isolatitude pixelation, n.d. Accessed: 2024-10-22. 6": "Timon Hfer, Benjamin Kiefer, Martin Messmer, and Andreas Zell. Hyperposepdf-hypernetworks pre-dicting the probability distribution on so (3). In Proceedings of the IEEE/CVF Winter Conference onApplications of Computer Vision, pages 23692379, 2023. 2 Owen Howell, David Klee, Ondrej Biza, Linfeng Zhao, and Robin Walters. Equivariant single view poseprediction via induced and restriction representations. Advances in Neural Information Processing Systems,36, 2023. 2, 5, 6, 7, 8, 16, 17, 19, 20, 23 Jiahui Huang, He Wang, Tolga Birdal, Minhyuk Sung, Federica Arrigoni, Shi-Min Hu, and Leonidas JGuibas. Multibodysync: Multi-body segmentation and motion estimation via 3d scan synchronization. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 71087118,2021. 2",
  "Du Q Huynh. Metrics for 3d rotations: Comparison and analysis. Journal of Mathematical Imaging andVision, 35:155164, 2009. 6": "Alex Kendall and Roberto Cipolla. Geometric loss functions for camera pose regression with deep learning.In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 59745983,2017. 2 Alex Kendall, Matthew Grimes, and Roberto Cipolla. Posenet: A convolutional network for real-time6-dof camera relocalization. In Proceedings of the IEEE international conference on computer vision,pages 29382946, 2015. 2",
  "CG Khatri and Kanti V Mardia. The von misesfisher matrix distribution in orientation statistics. Journalof the Royal Statistical Society Series B: Statistical Methodology, 39(1):95106, 1977. 2": "David Klee, Ondrej Biza, Robert Platt, and Robin Walters. Image to icosahedral projection for so (3)object reasoning from single-view images. In NeurIPS Workshop on Symmetry and Geometry in NeuralRepresentations, pages 6480. PMLR, 2023. 2 David Klee, Ondrej Biza, Robert Platt, and Robin Walters. Image to sphere: Learning equivariant featuresfor efficient pose prediction. ICLR, 2023. 2, 4, 5, 6, 7, 8, 10, 16, 17, 18, 19, 20, 21, 22, 23, 24, 27",
  "Jongmin Lee, Yoonwoo Jeong, and Minsu Cho. Self-supervised learning of image scale and orientation. In31st British Machine Vision Conference 2021, BMVC 2021, Virtual Event, UK. BMVA Press, 2021. 15": "Jongmin Lee, Byungjin Kim, and Minsu Cho. Self-supervised equivariant learning for oriented keypointdetection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,pages 48474857, 2022. Jongmin Lee, Byungjin Kim, Seungwook Kim, and Minsu Cho. Learning rotation-equivariant featuresfor visual correspondence. In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 2188721897, 2023. 15 Jake Levinson, Carlos Esteves, Kefan Chen, Noah Snavely, Angjoo Kanazawa, Afshin Rostamizadeh,and Ameesh Makadia. An analysis of svd for deep rotation estimation. Advances in Neural InformationProcessing Systems, 33:2255422565, 2020. 2, 15 Shuai Liao, Efstratios Gavves, and Cees GM Snoek. Spherical regression: Learning viewpoints, surfacenormals and 3d rotations on n-spheres. In Proceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 97599767, 2019. 7, 8, 17, 19, 20, 23",
  "Siddharth Mahendran, Haider Ali, and Rene Vidal. A mixed classification-regression framework for 3dpose estimation from 2d images. arXiv preprint arXiv:1805.03225, 2018. 8, 17, 20, 23": "Ameesh Makadia and Kostas Daniilidis. Direct 3d-rotation estimation from spherical images via ageneralized shift theorem. In 2003 IEEE Computer Society Conference on Computer Vision and PatternRecognition, 2003. Proceedings., volume 2, pages II217. IEEE, 2003. 3, 16 Osama Makansi, Eddy Ilg, Ozgun Cicek, and Thomas Brox. Overcoming limitations of mixture densitynetworks: A sampling and fitting framework for multimodal future prediction. In Proceedings of theIEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 71447153, 2019. 6 Octave Mariotti and Hakan Bilen. Semi-supervised viewpoint estimation with geometry-aware conditionalgeneration. In Computer VisionECCV 2020 Workshops: Glasgow, UK, August 2328, 2020, Proceedings,Part II 16, pages 631647. Springer, 2020. 2 Octave Mariotti, Oisin Mac Aodha, and Hakan Bilen. Viewnet: Unsupervised viewpoint estimation fromconditional generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision,pages 1041810428, 2021. 2 Rowan Thomas McAllister, Yarin Gal, Alex Kendall, Mark Van Der Wilk, Amar Shah, Roberto Cipolla,and Adrian Weller. Concrete problems for autonomous vehicle safety: Advantages of bayesian deeplearning. International Joint Conferences on Artificial Intelligence, Inc., 2017. 1 David Mohlin, Josephine Sullivan, and Grald Bianchi. Probabilistic orientation estimation with matrixfisher distributions. Advances in Neural Information Processing Systems, 33:48844893, 2020. 2, 5, 8, 17,19, 20, 23 Kieran A Murphy, Carlos Esteves, Varun Jampani, Srikumar Ramalingam, and Ameesh Makadia. Implicit-pdf: Non-parametric representation of probability distributions on the rotation manifold. In InternationalConference on Machine Learning, pages 78827893. PMLR, 2021. 2, 5, 6, 8, 10, 16, 17, 19, 20, 23, 24 Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deeplearning library. Advances in neural information processing systems, 32, 2019. 22",
  "A Ren Geist, Jonas Frey, Mikel Zobro, Anna Levina, and Georg Martius. Learning with 3d rotations, ahitchhikers guide to so (3). arXiv e-prints, pages arXiv2404, 2024. 1, 2, 3, 6, 15": "Paul-Edouard Sarlin, Mihai Dusmanu, Johannes L Schnberger, Pablo Speciale, Lukas Gruber, ViktorLarsson, Ondrej Miksik, and Marc Pollefeys. Lamar: Benchmarking localization and mapping foraugmented reality. In European Conference on Computer Vision, pages 686704. Springer, 2022. 1 Hao Su, Charles R Qi, Yangyan Li, and Leonidas J Guibas. Render for cnn: Viewpoint estimation in imagesusing cnns trained with rendered 3d model views. In Proceedings of the IEEE international conference oncomputer vision, pages 26862694, 2015. 2, 7",
  "Shubham Tulsiani and Jitendra Malik. Viewpoints and keypoints. In Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition, pages 15101519, 2015. 2, 8, 17, 20, 23": "Benjamin Ummenhofer, Huizhong Zhou, Jonas Uhrig, Nikolaus Mayer, Eddy Ilg, Alexey Dosovitskiy, andThomas Brox. Demon: Depth and motion network for learning monocular stereo. In Proceedings of theIEEE conference on computer vision and pattern recognition, pages 50385047, 2017. 2 He Wang, Srinath Sridhar, Jingwei Huang, Julien Valentin, Shuran Song, and Leonidas J Guibas. Normal-ized object coordinate space for category-level 6d object pose and size estimation. In Proceedings of theIEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 26422651, 2019. 1 Jianyuan Wang, Christian Rupprecht, and David Novotny. Posediffusion: Solving pose estimation viadiffusion-aided bundle adjustment. In Proceedings of the IEEE/CVF International Conference on ComputerVision, pages 97739783, 2023. 2",
  "Maurice Weiler and Gabriele Cesa.General e (2)-equivariant steerable cnns.Advances in NeuralInformation Processing Systems, 32:1433414345, 2019. 15": "Philippe Weinzaepfel, Vincent Leroy, Thomas Lucas, Romain Brgier, Yohann Cabon, Vaibhav Arora,Leonid Antsfeld, Boris Chidlovskii, Gabriela Csurka, and Jrme Revaud. Croco: Self-supervised pre-training for 3d vision tasks by cross-view completion. Advances in Neural Information Processing Systems,35:35023516, 2022. 21 Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and Jianxiong Xiao.3d shapenets: A deep representation for volumetric shapes. In Proceedings of the IEEE conference oncomputer vision and pattern recognition, pages 19121920, 2015. 7",
  "Li Yi, Haibin Huang, Difan Liu, Evangelos Kalogerakis, Hao Su, and Leonidas Guibas. Deep part inductionfrom articulated object pairs. ACM Transactions on Graphics (TOG), 37(6):115, 2018. 2": "Yingda Yin, Yingcheng Cai, He Wang, and Baoquan Chen. Fishermatch: Semi-supervised rotationregression via entropy-based filtering. In Proceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 1116411173, 2022. 2, 8, 23 Yingda Yin, Yang Wang, He Wang, and Baoquan Chen. A laplace-inspired distribution on so(3) forprobabilistic rotation estimation. In International Conference on Learning Representations (ICLR), 2023.2, 5, 6, 8, 17, 18, 19, 20, 22, 23 Jason Y Zhang, Amy Lin, Moneish Kumar, Tzu-Hsuan Yang, Deva Ramanan, and Shubham Tulsiani. Cam-eras as rays: Pose estimation via ray diffusion. In International Conference on Learning Representations(ICLR), 2024. 2, 21 Jason Y Zhang, Deva Ramanan, and Shubham Tulsiani. Relpose: Predicting probabilistic relative rotationfor single objects in the wild. In European Conference on Computer Vision, pages 592611. Springer,2022. 2 Yongheng Zhao, Tolga Birdal, Jan Eric Lenssen, Emanuele Menegatti, Leonidas Guibas, and FedericoTombari. Quaternion equivariant capsule networks for 3d point clouds. In European conference oncomputer vision, pages 119. Springer, 2020. 2 Ellen D Zhong, Tristan Bepler, Joseph H Davis, and Bonnie Berger. Reconstructing continuous distributionsof 3d protein structure from cryo-em images. In International Conference on Learning Representations,2019. 1 Yi Zhou, Connelly Barnes, Jingwan Lu, Jimei Yang, and Hao Li. On the continuity of rotation represen-tations in neural networks. In Proceedings of the IEEE/CVF conference on computer vision and patternrecognition, pages 57455753, 2019. 1, 2, 8, 15, 19, 20, 23 Andrea Zonca, Leo Singer, Daniel Lenz, Martin Reinecke, Cyrille Rosset, Eric Hivon, and KrzysztofGorski. healpy: equal area pixelization and spherical harmonics transforms for data on the sphere in python.Journal of Open Source Software, 4(35):1298, Mar. 2019. 22",
  "A.1Rotation Representation in Spatial Domain": "We recommend checking out a detailed overview of learning 3D rotations in . Rotations inboth 2D and 3D spaces can be represented using various mathematical frameworks, each with itsown advantages and limitations, crucial for applications in fields such as computer graphics, robotics,and deep learning. In 2D space, rotation angles can be expressed as following SO(2) representations: angle () orTrigonometric functions (cos(), sin()). We present a few studies dealing with rotation-equivariancein 2D: group-equivariant networks , and its application in image matching . In 3D space, Euler angles in R3 are a representative form of SO(3) representation. Euler angleshave 3 DoF consisting of three angles , , [, ) to describe a 3D rotation (roll, pitch, yaw).3D rotation matrix can be composed of a fixed sequence rotation using the angles R(, , ) =Rz()Ry()Rz(). The standard Euler angles can be divided into 2 forms: Tait-Bryan angles (a.k.a.Cardan angles) which consists of permutations of three items (XYZ, XZY, YXZ, YZX, ZXY, ZYX),and proper Euler angles which starts and ends with rotations around the same axis (ZYZ, ZXZ, XYX,XZX, YZY, YXY), total 12 possible unique sequences. Because of non-uniqueness of Euler angles,existing studies do not encourage the Euler angles as an output representation for 3Drotation prediction in deep neural network. Quaternions in S3 is for 4-dimensional complex number to represent a 3D rotation. A quaternion q iscomposed of one real part and three imaginary parts: q = w + xi + yj + zk, where w, x, y, z arereal numbers and i, j, k are the fundamental quaternion units. Quaternions can prevent gimbal lock, aproblem that occurs with Euler angles where one degree of freedom is lost during 3D rotation. Axis-angle in R4 consists of an angle and an axis vector u in 3D space. To rotate a point p R3about the axis u by an angle , you can use Rodrigues rotation formula: p = pcos()+upsin()+u(u p)(1 cos()). The axis-angle representation can be converted into a rotation matrix or aquaternion. Even it has advantages of intuitive form and compact to describe 3D rotation with fourparameters, the axis-angle can suffer from singularities in a scenario of angle multiples of = 2. 6D representation is a relatively newer concept compared to the traditional representation likeEuler angles, rotation matrices and quaternions. A rotation is described by two 3D vectors that areorthogonal to each other, and the rotation matrix can be obtained by Gram-Schmidt orthonormalization(GSO). This representation can be directly predicted and optimized by deep networks because itavoids the non-linear constraints found in quaternions and rotation matrices. Quaternions requiremaintaining a unit norm, which introduces complexity in ensuring the quaternion remains normalizedthroughout the optimization process. Therefore, adopt this 6D representation with GSO. 9D representation is direct parametrization of 3 3 matrix, which can be projected to SO(3)using singular value decomposition (SVD). Predicting 9D representation for rotation matrices involvesorthogonality constraints, meaning the rows and columns must remain orthonormal, and determinantconstraints, where the determinant must equal +1. These constraints complicate the learning andoptimization process, making this representation more suitable for direct prediction and optimizationby deep networks. Therefore, 9D representation is less common as a direct method to predict rotation,but use this to mitigate issues associated with discontinuous parameterizations of pose.",
  "A.2Rotation Representation in Frequency Domain": "3D rotation in the frequency domain is accomplished by manipulating spherical harmonics coefficients.Spherical harmonics Y lm(x) = Y lm(, ) are a function defined on the surface of a sphere, where and are the polar and azimuthal angles, respectively. Here, l represents the degree of the sphericalharmonics, m is the order. To ensure computational feasibility, we truncate the degree of harmonicsto a finite L = lmax. Rotations of spherical functions can be represented by matrices that operate onthe coefficients of their harmonics expansion. The rotation of spherical harmonics is expressed via",
  "|n|lDlmn(g)Y ln(x),(5)": "where gY lm(x) denotes the spherical harmonic Y lm(x) after rotation by g, and g is the rotationoperator. Spherical harmonics Y lm(x), defined by degree l and order m (l 0, |m| l), usex to represent spherical coordinates. The matrix U lmn(g) forms part of the irreducible unitaryrepresentation of SO(3), showing how each harmonic is transformed under rotation. The sum overall orders n from l to l,",
  "|n|l, shows that Y lm is a linear combination of all harmonics of degree l": "This rotation of spherical harmonics can be described by the Wigner-D matrix Dlmn(R), which is aunitary matrix that describes the effect of a rotation R on the spherical harmonics basis functions. Wecan rewrite U lmn(g) = Dlmn(, , ). Each element of the matrix represents the amplitude and phaseshift that a spherical harmonic Y lm undergoes due to the rotation R. The rotation R can be specifiedby Euler angles , , and in the ZYZ-axes configuration. The matrix elements Dlmn(, , ) canbe explicitly expressed as:",
  "n=lDlmn(R)f ln,(8)": "where Dlmn(R) encodes the effect of the rotation on the original coefficients. This transformationpreserves the orthonormality and completeness of the spherical harmonics basis, ensuring that therotated function f (, ) remains a valid representation of the original function f(, ) under therotation R. The expansion coefficients f lm can be calculated using the original spatial coordinates(, ) of the sphere surface according to the spherical harmonic expansion:",
  "B.1.1ModelNet10-SO(3)": "Table A1 presents a comparison of existing methods with different backbones on the ModelNet10-SO(3) dataset3, highlighting performance across multiple accuracy thresholds and median error. Forthe ResNet-50 backbone, our method achieves the highest accuracy at 3 (0.422) and 5 (0.640) witha median error of 15.1, outperforming the existing methods . In particular, our modeldemonstrates significantly better performance than the strong baselines that use equivariantnetworks to estimate non-parametric SO(3) healpix distribution at the finer thresholds. Compared",
  "MethodBackboneAcc@3Acc@5Acc@10Acc@15Acc@30Med. ()": "Deng et al. ResNet-340.1380.3010.5020.5620.69431.6Murphy et al. ResNet-500.2940.5340.6800.7190.73521.5Klee et al. ResNet-500.3100.5610.7050.7280.73615.7Howell et al. ResNet-50-----17.8oursResNet-500.4220.6400.7440.7590.76715.1 Liao et al. ResNet-101---0.4960.65828.7Mohlin et al. ResNet-1010.1640.3890.6150.6930.75717.1Yin et al. ResNet-1010.4470.6110.7150.7420.77212.7Liu et al. ResNet-1010.5110.6370.7190.7440.76812.2oursResNet-1010.5130.6880.7630.7730.78011.9 Table A1: Comparison with finer thresholds on ModelNet10-SO(3). We compare additionalthresholds, including Acc@3, Acc@5, and Acc@10. The tables are organized by the size of thebackbone. The scores are averaged across all ten object categories.",
  "MethodAcc@3Acc@5Acc@10Acc@15Acc@30Med. ()": "Tulsiani & Malik ----0.80813.6Prokudin et al. ----0.83812.2Mahendran et al. ----0.85910.1Liao et al. ----0.81913.0Mohlin et al. 0.0890.2150.4840.6500.82711.5Murphy et al. 0.1020.2420.5240.6720.83810.2Yin et al. 0.1340.2920.5740.7140.8749.3Klee et al. 0.1340.2700.5800.7160.8679.6Liu et al. 0.1170.2640.5520.7060.86310.0Howell et al. -----9.2ours0.1530.3100.5950.7540.8928.6 Table A2: Comparison with finer thresholds on PASCAL3D+. We compare additional metrics,including Acc@3, Acc@5, and Acc@10, adding on the . Most baselines , including ours, use ResNet-101 backbone networks in this experiment. to , our model achieves 11.2%p, 7.9%p, and 3.9%p higher at the 3, 5, and 10 thresholds,respectively. For the ResNet-101 backbone, our method also demonstrates superior performance withthe highest accuracy at 3 (0.513) and 5 (0.688), and the lowest median error of 11.9, comparedto . The table includes accuracies at 10, 15, and 30 thresholds, where our methodconsistently shows top performance across these metrics. The scores are averaged across all tenobject categories.",
  "B.1.2PASCAL3D+": "Table A2 presents a comparison of various methods on the PASCAL3D+ dataset, focusing on fineraccuracy thresholds (Acc@3, Acc@5, and Acc@10) and median error. The table includes methodsthat primarily use the ResNet-101 backbone, comparing our approach against several existing methods.Our method achieves the highest accuracies with 0.153 at 3, 0.310 at 5, and 0.595 at 10, with thelowest median error of 8.6. Compared to the previous state-of-the-art Yin et al. , our methodshows performance improvements of 1.9%p at 3, 1.8%p at 5, and 2.1%p at 10, while also reducingthe median error by 0.7.",
  "B.1.3ModelNet10-SO(3) Few-shot Views": "Figure A1 shows the results with finer thresholds, Acc@3, Acc@5, and Acc@10, on theModelNet10-SO(3) few-shot training views, which are additional results to . The graphsillustrate that our method outperforms all other methods across all metrics (Acc@3, Acc@5, andAcc@10) and requires fewer training views to achieve high accuracy, even at finer thresholds. Thisshows that our model is capable of more precise pose estimation with less number of training data,proving the data efficiency of our SO(3)-equivariant harmonics pose estimator. Baseline results were obtained using the source code provided by the authors. 310 2050701000.0 0.1 0.2 0.3 0.4 0.5 0.6Acc@3 310 205070100 # of Training Views 0.0 0.2 0.4 0.6 0.8Acc@5 310 2050701000.0 0.2 0.4 0.6 0.8Acc@10 I2S (Res50)RotLaplace (Res50)RotLaplace (Res101)ours (Res50)ours (Res101) Figure A1: Results with finer thresholds on ModelNet10-SO(3) few-shot training views. Resultswith solid lines denote a ResNet-50 backbone, while dotted lines indicate a ResNet-101 backbone. Ourmethod outperforms all metrics and reduces training views even at finer thresholds. For comparison,the I2S (ResNet-50) model is shown with a blue line, and the RotLaplace (ResNet-50 andResNet-101) models are depicted with purple solid and dashed lines, respectively.",
  "B.2.1ModelNet-SO(3) Categorical Results": "Table A3 provides a comprehensive comparison across 10 object categories on the ModelNet10-SO(3)dataset, including Bathtub, Bed, Chair, Desk, Dresser, TV Monitor, Night Stand, Sofa, Table, andToilet. Each image is labeled with a single 3D rotation matrix, even though some categories, suchas desks and bathtubs, may have ambiguous poses due to symmetry. This poses a challenge formethods that cannot handle uncertainty over orientation. However, in terms of accuracy at 15, ourmodel consistently achieves the best performance in the desk and bathtub categories, demonstratingrobustness against pose ambiguity and symmetry. The table is divided into sections for ResNet-50 andResNet-101, indicating different network architectures used. For ResNet-50, our method achieves thelowest average median error of 15.1, with particularly strong performance in 9 categories: bed (2.7),chair (3.8), desk (4.2), dresser (2.7), tv monitor (2.7), night stand (3.4), sofa (7.2), and toilet(3.0). For ResNet-101, our model demonstrates the lowest average median error of 11.9. AlthoughLiu et al. (Uni.) generally obtain better results in terms of median error with the ResNet-101backbone, our model outperforms Liu et al. (Uni.) on Acc@15 in most cases. This indicatesthat our model estimates poses correctly at a finer level of detail. In terms of accuracy at 15, our method achieves the highest average accuracy of 0.759 for ResNet-50,with best performance in all categories. For ResNet-101 at 15, our model leads with an averageaccuracy of 0.773, achieving state-of-the-art performance in 8 out of 10 categories. In terms ofaccuracy at 30, our method achieves the highest average accuracy of 0.773 for ResNet-50, withthe best results in 9 out of 10 categories. For ResNet-101 at 30, our model maintains the highestaverage accuracy of 0.905, with strong results in categories such as Chair, Desk, TV Monitor,and Sofa. Overall, our equivariant harmonics pose estimator demonstrates superior performanceacross both ResNet-50 and ResNet-101 architectures in terms of both median error and accuracy atdifferent angles. This highlights its effectiveness and robustness across various object categories inthe ModelNet10-SO(3) dataset, consistently outperforming other recent methods in most categoriesand metrics.",
  "B.2.2PASCAL3D+ Categorical Results": "Table A4 presents comprehensive comparisons on the PASCAL3D+ dataset across 12 categories(Aeroplane, Bicycle, Boat, Bottle, Bus, Car, Chair, Dining Table, Motorbike, Sofa, Train, TV Monitor)using median error and Acc@30 metrics. This benchmark is challenging due to significant variationsin object appearances, high variability of natural textures, and the presence of novel object instancesin the test set. Our method demonstrates superior performance with the lowest average median errorof 8.6 and the highest average accuracy of 0.892, excelling in categories such as \"aero,\" \"bike,\" \"car,\"\"chair,\" \"table,\" and \"mbike\" in median error, and achieving best performance in 8 out of 12 categories",
  "B.3Impact of SO(3) Discretization Sizes and Continuity of Rotations": "Table A5 reports the effect of varying the grid size (Q) on performance for the ModelNet10 benchmark.We observe comparable results in common evaluation metrics, such as Accuracy at 15 degrees(Acc@15) and 30 degrees (Acc@30), even with a lower grid resolution (Q = 4.6K). A higherresolution grid (18.87M) improves performance under stricter evaluation thresholds. With ourchosen grid size of Q = 2.36M, the model achieves strong performance sufficiently, particularly forlow-threshold metrics like Acc@3. Table A1 provides additional comparisons to baseline methods. Therefore, we carefully claim that our learning method focuses on continuous rotations. Our modeldirectly learn the Wigner-D coefficients, which are derived from 3D rotations (Euler angles), withoutany discretization during the training phase. During inference the use of the SO(3) HEALPix gridserves two purposes: 1) To convert SO(3) rotations from the frequency domain to the spatial domain,and 2) To address pose ambiguity by providing multiple solutions. As a result, we obtain a distributionwith very sharp modality. By taking the argmax of this distribution, we achieve sufficient precision in3D orientation estimation, specifically around 1.5. Maintaining continuity in rotations allows our method to deliver more accurate and precise posepredictions, giving us a clear advantage over the methods that experience precision loss fromdiscretization during training. As a result, we achieve consistently high accuracy across differentlevels of discretization.",
  "Median error ()": "Zhou et al. (2019) 19.224.718.954.211.38.49.519.414.922.517.211.417.5Brgier (2021) 20.027.522.649.211.98.59.916.827.921.712.610.220.6Liao et al. (2019) 13.013.016.429.110.34.86.811.612.017.112.38.614.3Mohlin et al. (2020) 11.510.115.624.37.83.35.313.512.512.913.87.411.7Prokudin et al. (2018) 12.29.715.545.65.42.94.513.112.611.89.14.312.0Tulsiani & Malik (2015) 13.613.817.721.312.95.89.114.815.214.713.78.715.4Mahendran et al. (2018) 10.18.514.820.57.03.15.19.311.314.210.25.611.7Murphy et al. (2021) 10.310.812.923.48.83.45.310.07.313.69.56.412.3Yin et al. (2023) 9.48.611.721.86.92.84.87.99.112.28.16.911.6Liu et al. (Uni.) (2023) 10.28.915.224.96.92.94.38.710.712.89.36.311.3Liu et al. (Fisher) (2023) 9.99.612.422.77.53.14.89.28.613.58.66.711.6Klee et al. (2023) 9.89.212.721.77.43.34.99.59.311.510.57.210.6Howell et al. (2023) 9.29.312.617.08.03.04.59.46.711.912.16.99.9ours8.68.312.117.27.92.94.28.15.510.49.37.110.7",
  "B.4Discretised distribution on SO(3)": "Table A6 shows the evaluation results using gradient ascent on the predicted SO(3) pose distributionin ModelNet10-SO(3), to fully exploit the distribution prediction in Sec. 4.3 during inference time.While gradient ascent does provide some performance improvement, the increase in inference timeoutweighs these gains, so argmax is our preferred method for simplicity and fast evaluation.",
  "B.6Searching Frequency Level L": "Table A8 presents the impact of varying the maximum frequency level L by truncation for efficientSO(3) group convolutions on pose prediction accuracy and median error. The results show thatas L increases from 1 to 5, there is a consistent improvement in accuracy metrics. The optimalperformance is observed at L = 5.",
  "Table A8: Results of various num-ber of maximum frequency L inModelNet10-SO(3)20-shottrainingviews": "Beyond this point, additional frequency levels do not con-tribute to improved accuracy and can even degrade per-formance. When L > 5, the accuracy does not improvesignificantly and starts to fluctuate, with rotation errorremaining relatively low up to L = 10. However, atL = 11, accuracy starts to decline more noticeably. ForL 12, there is a sharp decline in performance, withAcc@15 dropping to 0.5815 and continuing to decrease,Acc@30 following a similar trend, and rotation errorincreasing significantly to over 55. We infer that high fre-quencies do not improve performance despite the increasein learnable parameters because they lead to overfittingto high-frequency noise. This overfitting occurs when thehigh-frequency model captures irrelevant noise and pat-terns in the training data, reducing its generalizability tonew, unseen data. In conclusion, including higher frequencies (L > 6) ap-pears to introduce more noise or overfitting, leading todecreased accuracy and increased rotation error. However,we choose a maximum frequency level L = 6 for a faircomparison to , and to balance efficiency and accuracy.",
  "Spherical mapper0.68070.695622.27MLP mapper0.64460.656744.52": "Table A9: Validating the design choice of thespherical mapper. The MLP mapper denotesthe Fourier projection, which directly maps im-age features to harmonics using an MLP, and thespherical mapper denotes our choice of ortho-graphic projection . The spherical mapper in Sec. 4.1 maintains thegeometric structure of the image when project-ing onto the S2 sphere, as detailed in . Thismethod involves lifting the 2D image onto thesphere and converting spherical points usingspherical harmonics. Table A9 shows that thespherical mapper outperforms simple Fouriertransforms on 2D feature maps. Using depth information from methods likeDepthAnythingv2 for 3D lifting is a good ideaand can enhance geometric accuracy. Addition-ally, centroid ray regression has been exploredin research such as . However, incorporatingexternal depth modules increases computationalcosts and broadens our research scope, so weconsider this for future work.",
  "Time (sec. / 1 frame)0.02860.01713.99600.0109GPU memory (GB)1.1560.9121.1305.172": "Table A11: Comparison of computational cost. We compare the inference time of one image andGPU memory consumption on ModelNet10-SO(3) test split. To measure the inference time, weaverage the results of total 18,160 samples of ModelNet10-SO(3) test split. PASCAL3D+. The results are presented in Table A10. As the results indicate, the model does notperform well when evaluated on an out-of-distribution dataset. Nevertheless, we recognize this as animportant area for future research.",
  "B.9Computational Cost Analysis": "Table A11 presents a detailed comparison of computational cost, focusing on both inference timeand GPU memory consumption. The comparison includes the recent baselines; Image2Sphere(2023) , RotationLaplace (2023) , and RotationNormFlow (2023) . The evaluation isbased on the ModelNet10-SO(3) test split, averaging the results from a total of 18,160 samples.We use a machine equipped with an Intel i7-8700 CPU and an NVIDIA GeForce RTX 3090 GPU,utilizing a batch size of 1. The key metrics presented in the table are the inference time per frame (inseconds) and the GPU memory consumption (in gigabytes). Our model demonstrates the best inference time of 0.0109 seconds per frame, significantly outper-forming other models in terms of speed. This efficient inference time translates to approximately92.5 frames per second (FPS) for an image size of 224x224, making our model suitable for real-timeapplications. However, this performance comes at the cost of higher GPU memory consumption,which is recorded at 5.172 GB. Since our model performs all operations on the GPU, we achieve atemporal advantage in inference time, despite having many overlapping modules with Klee et al. ,whose model performs some computations on the CPU. In summary, our model achieves the bestinference time, facilitating real-time application potential, by trading off increased GPU memoryconsumption.",
  "B.10Experiment of Statistical Significance": "Table A12 presents the results of a 5-trial experiment to evaluate the training sensitivity of our models,with ResNet-50 and ResNet-101, on the ModelNet10-SO(3) 20-shot training views. The table listsindividual trial results, along with the average () and standard deviation () for each metric. Thestandard deviation values () for both backbones are relatively small across all metrics, suggestingthat the models yield consistent results over multiple trials. For instance, the standard deviation ofAcc@3 for ResNet-50 is 0.0047, and for ResNet-101, it is 0.0052, which are both quite low. Thislow variance indicates that the training results are stable and reproducible. These findings highlightthe robustness and reliability of the training process and the effectiveness of ResNet-101 for the giventask.",
  "Table A12:Experiment of 5-trials training of our model for statistical significance onModelNet10-SO(3) 20-shot training views. denotes the average, and denotes the standarddeviation": "for model implementation. We use a machine with an Intel i7-8700 CPU and an NVIDIA GeForceRTX 3090 GPU. With a batch size of 64, our network is trained for 50 epochs on ModelNet10-SO(3)taking 25 hours, and for 80 epochs on PASCAL3D+ taking 28 hours. We start with an initial learningrate of 0.1, which decays by a factor of 0.1 every 30 epochs. We use the SGD optimizer with Nesterovmomentum set at 0.9. Unlike baselines that encode object class information via an embedding layerduring training and both training and testing , our model does not use class embeddings,maintaining a class-agnostic framework during both training and testing. Additionally, we train asingle model for all categories in each dataset, unlike , which trains separate models for each class.",
  "DBaselines of Single-View Pose Estimation": "We compare our method against competitive single-view SO(3) pose estimation baselines includingregression methods and distribution learning methods. Zhou et al. predict 6D representationsusing Gram-Schmidt orthonormalization processes for 3D rotations, analyzing the discontinuities inrotation representations. Brgier extends deep 3D rotation regression with a differentiable Pro-crustes orthonormalization, which maps arbitrary inputs from Euclidean space onto a non-Euclideanmanifold. Tulsiani and Malik train a CNN using logistic loss to predict Euler angles. Mahendranet al. predict three Euler angles using a classification-regression loss to estimate fine-posewhile modeling multi-modal pose distributions. Liao et al. also predict Euler angles using aclassification-regression loss by introducing a spherical exponential mapping on n-spheres at theregression output. On the other hand, the other baselines are generating probability distributions for estimating SO(3)pose. Prokudin et al. represents rotation uncertainty with a mixture of von Mises distributionsover each Euler angle, while Mohlin et al. predicts the parameters for a matrix Fisher distribution.Deng et al. predict multi-modal Bingham distributions. Murphy et al. trains an implicitmodel to generate a non-parametric distribution over 3D rotations. Yin et al. predict theparameter of SO(3) parametric distribution using matrix-Fisher distribution and rotation Laplacedistribution, respectively. Klee et al. predicts non-parametric distribution with equivariant featureprediction by orthographic projection, and Howell et al. extends to construct neural architecturesto satisfy SO(3) equivariance using induced and restricted representations. Liu et al. use discretenormalizing flows for rotations to learn various kinds of distributions on SO(3). Results are from theoriginal papers when available.",
  "EQualitative Results": "Figures A2 and A3 show qualitative results randomly selected from ModelNet10-SO(3) and PAS-CAL3D+, respectively. For visualization, we display distributions over SO(3) as proposed inI-PDF . To illustrate the SO(3) distribution, we use the Hopf fibration to visualize the entirespace of 3D rotations . This approach maps each point on a great circle in SO(3) to a point on thediscretized 2-sphere and uses a color wheel to indicate the location on the great circle. Essentially,each point on the 2-sphere represents the direction of a canonical z-axis, and the color represents thetilt angle around that axis. To depict probability score, we adjust the size of the points on the plot.Lastly, we present the 2-spheres surface using the Mollweide projection. Comparison of pose visualization. Figures A4 and A5 show a comparison of pose visualizations onModelNet10-SO(3) and PASCAL3D+, respectively. This visualization method is the same to thoseused in Figures A2 and A3. We compare our model to the I2S baseline. The numbers next to\"Err\" above the input images represent the error in degrees between the models predicted pose andthe ground truth (GT) pose. These results demonstrate that our model provides more accurate andprecise pose estimations, even in cases where the I2S baseline fails. Additionally, on the PASCAL3D+benchmark, which includes objects captured in real-world scenarios, our model consistently showscorrect pose estimations, particularly in challenging scenarios where the I2S baseline struggles.",
  "FLimitation": "Our proposed method significantly advances 3D rotation estimation accuracy; however, a notablechallenge in pose estimation is the issue of pose ambiguity, particularly for objects with symmetricalfeatures or those viewed from certain angles, e.g., bathtub category in ModelNet10-SO(3). Despitehigh accuracy, our method can suffer from significant errors due to the loss of spatial informationwhen projecting 3D data onto spherical harmonics. Future work could integrate additional contextualor spatial information to mitigate these ambiguities, improve reliability, and enhance the modelsrobustness in diverse scenarios. Additionally, while the mathematical rigor of using sphericalharmonics and Wigner-D coefficients supports the models success and improves interpretabilitythrough equivariant networks, further exploration is needed to make the model more interpretable.Finally, the computational cost associated with Wigner-D coefficients and SO(3)-equivariant networksshould be improved to enhance practicality for real-time applications and deployment on deviceswith limited processing power.",
  "GBroader Impacts": "The method proposed in this paper has several potential positive societal impacts. First, it can enhancerobotics and automation. Accurate 3D pose estimation is crucial for these fields, and improvedaccuracy can lead to more efficient and safer robotic systems in manufacturing, healthcare, andservice industries. Second, it can significantly advance augmented reality (AR) applications byproviding more precise alignment of virtual objects with the real world, which can be beneficialin education, gaming, and industrial design. Third, the method can improve autonomous vehicles,which rely on precise 3D pose estimation to understand their environment, contributing to safer andmore reliable autonomous driving systems. Finally, in medical imaging, accurate pose estimationcan improve the analysis and interpretation of complex 3D data, aiding in diagnosis and treatmentplanning. However, the paper also suggests potential negative societal impacts. Improved pose estimationtechniques could be used in surveillance systems, leading to privacy concerns if deployed withoutproper regulations and oversight. Enhanced 3D pose estimation could be exploited to create morerealistic deepfakes, contributing to the spread of disinformation and manipulation. Deployment ofthese technologies could inadvertently reinforce existing biases if the training data is not representativeof diverse populations, leading to unfair treatment of specific groups in applications like securityand hiring. Additionally, the misuse of accurate pose estimation in security-sensitive areas, such asmilitary applications or unauthorized monitoring, could pose significant risks.",
  "To address these potential negative impacts, several mitigation strategies could be implemented.Controlled release of models and methods to ensure ethical and responsible use is one approach": "Err: 1.36 Err: 179.70 Err: 8.17 Err: 5.92 Err: 3.22 Err: 1.94 Err: 2.90 Err: 3.08 Err: 3.70 Err: 1.55 Err: 1.17 Err: 4.67 Figure A2: Randomly selected qualitative results of pose estimation on ModelNet10-SO(3) usingour SO(3) equivariant harmonics pose estimator. The error value, indicating the difference betweenthe estimated and ground truth orientations in degrees, is labeled above each plot. Most images withclearly posed objects in the input image show an error of 10or less, demonstrating high accuracy ofthe pose estimation algorithm. The example in the first row, second column, shows a significant errorof 179.70. This high error is attributed to the ambiguity in pose information, as the projection of the3D object causes a loss of spatial information, resulting in larger discrepancies between the groundtruth and estimated poses. Other examples with low errors, such as the top-left corner (Err: 1.36)and second row, second column (Err: 1.94), indicate successful pose estimations. Regular audits to ensure the training data and algorithms do not propagate biases are also crucial.Implementing robust privacy protection measures can safeguard individual privacy in applicationsinvolving surveillance. Developing complementary technologies to detect and mitigate the effects ofdeepfakes and other forms of disinformation is another necessary step. Err: 25.42 Err: 2.54 Err: 20.46 Err: 1.80 Err: 4.97 Err: 6.42 Err: 11.76 Err: 6.01 Err: 38.50 Err: 28.82 Err: 6.52 Err: 6.48 Figure A3: Randomly selected qualitative results on PASCAL3D+ using our SO(3) equivariant poseharmonics estimator. The error value, indicating the difference between the estimated and groundtruth orientations in degrees, is labeled above each plot. Most images with clearly posed objects inthe input image show an error of 10or less, demonstrating high accuracy of the pose estimationalgorithm. For example, the airplane in the first row, second column, shows a low error of 2.54,indicating precise pose estimation. However, some objects, like the monitor (Err: 38.50), airplane(Err: 28.82) in the fifth row, exhibit larger errors, possibly due to pose ambiguity in the input imageby symmetry. The variability in errors across different objects highlights the our models performancevariability depending on the objects shape and the clarity of pose information in the input image."
}