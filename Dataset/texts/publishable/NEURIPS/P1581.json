{
  "Abstract": "Understanding and predicting human actions has been a long-standing challengeand is a crucial measure of perception in robotics AI. While significant progresshas been made in anticipating the future actions of individual agents, prior workhas largely overlooked a key aspect of real-world human activity interactions.To address this gap in human-like forecasting within multi-agent environments,we present the Hierarchical Memory-Aware Transformer (HiMemFormer), atransformer-based model for online multi-agent action anticipation. HiMemFormerintegrates and distributes global memory that captures joint historical informationacross all agents through a transformer framework, with a hierarchical local memorydecoder that interprets agent-specific features based on these global representationsusing a coarse-to-fine strategy. In contrast to previous approaches, HiMemFormeruniquely hierarchically applies the global context with agent-specific preferencesto avoid noisy or redundant information in multi-agent action anticipation.Extensive experiments on various multi-agent scenarios demonstrate the significantperformance of HiMemFormer, compared with other state-of-the-art methods.",
  "Introduction": "Action detection or anticipation systems aim at forecasting future states of single or multipleagents from history. The recent advances in these areas facilitate embodied or virtual AI systems withthe ability to perceive and interact with other agents and complex environments . Suchability plays a pivotal role in numerous applications, such as autonomous driving , collaborativerobotics , and home automation , where understanding and predicting the actions of variousentities in a shared environment can significantly enhance safety, efficiency, and coordination. Agent memory plays an important role in conducting action anticipation due to the innate depen-dencies among actions . LSTR proposes to capture both long-term andshort-term memory, while MAT additionally incorporates future content in seen scenarios. In themulti-agent scenarios , each agent can be arbitrary or affected by the environment, which suggestsone key to the success of a multi-agent system: how to effectively capture agent behavior at varioustime and social scales. A prominent line of research exploits the ways to obtain a unified single globalfeature representing time, e.g., , and social relations, e.g., . AgentFormer andHiVT further explore combining time and social features with a overall global representation. Despite the significance, these state-of-the-art systems overlook the individual perspective of theproblem: different agents may need time and social features at different scales. From the timeperspective, some agent actions heavily rely on long-term memory, e.g., if they belong to a complexmulti-step action sequence, while some actions are only relevant to short memory, e.g., an instant",
  "()": "anticipation Agent-to-Context Encoder : HiMemFormer Architecture In the Agent-to-Context Encoder, the observed agentslong-term memory is encoded to a abstract representation M(a)Land cross-attention with context pasthistory M(c)L . Then, the Context-to-Agent Decoder utilize both agent and global recent memories tolearn the future information through a two-stage refinement approach. response to a rapid environment change. From the social perspective, similarly, the actions of someagents are much correlated with others during collaboration, while some of mostly stand-alone. Tocapture these agent-specific preferences in feature utilization, we propose to hierarchically capturethe time and social features for each agent-specific decoder to include these global or contextualfeatures with the desired granularity and discard unnecessary information that may introduce noise orlatency to each specific agent. To achieve customized and flexible global feature utilization automatically, we propose the Hierarchi-cal Memory-Aware Transformer (HiMemFormer), a novel approach that simultaneously learns featurerepresentations from both contextual and agent-specified dimensions through a dual-hierarchicalframework. Specifically, its Agent-to-Context Encoder augments the agents long-term historythrough cross-attention with global long-term memory. Then, the encoded long-term memory is fur-ther processed through a hierarchical Agent-to-Context Decoder that offers a coarse prediction givenaugmented long-term memory and contextual short-term memories. Finally, the coarse prediction isgradually refined by each agent-specific network augmented with individual short-term memory toget the anticipated actions. Through the dual-hierarchical network, HiMemFormer manages to modelagents unique short-term memory while learning useful correlations from the contextual memories.This allows us to effectively compress the long-range contextual information without losing importantlower level feature information. In summary, our contributions are three-fold:",
  "Hierarchical Memory-Aware Transformer": "We consider the general setting of multi-agent action learning as, given target agents live stream-ing First-Person-View (FPV) video, along with the Third-Person-View (TPV) video of the wholescene, our goal is to predict individual agents actions in a time period using only past and cur- : Results of online action anticipation on LEMMA using SlowFast features in upto 2 seconds. In particular, we report accuracy in mAP across 4 scenarios, including single agentscenarios where it perform single or multiple task, and multi-agent scenarios where multiple agentscollaborate on single task or carry out separate tasks.",
  "LSTR75.850.947.068.0MAT 73.050.850.467.1HiMemFormer (ours)76.354.248.470.6HiMemFormer+ (ours)76.252.250.569.9": "rent observations. To tackle this problem, we introduce Hierarchical Memory-Aware Transformer(HiMemFormer), a transformer-based model with encoder-decoder architecture, as shown in .In particular, given observed agents long-term history M(a)L , we compress it to a latent representationof fixed size through a transformer unit and then cross-attentioned with contextual long-term historyM(c)L to get the final encoded long-term memory ML. Using a coarse-to-fine strategy, we first decodethe long-term memory by cross-attentioned with short-term global memory M(c)S to learn all possibleactions in the current state, and refine the predicted actions by cross-attention with agents recent pastinformation M(a)Sto get the final anticipated action. See details in Appendix B",
  "Datasets and Metrics": "We evaluate our model on a public-available multi-agent dataset LEMMA , which includes 862compositional atomic-action from 324 activities with 445 egocentric videos (multiple egocentricvideos for multi-agent activities). We follow prior work on the dataset split, evaluating fourscenarios: single-agent single-task (1 1), single-agent multi-tasks (1 2), multi-agent single-task(2 1)and multi-agent multi-tasks(2 2). For online action anticipation, we follow prior works and evaluate on per-frame mean average precision (mAP) to measure the performanceand evaluate over an anticipation period of f = 2s. See details in Appendix C.1.",
  "Results and Discussion": "We compared HiMemFormer with other baseline models on LEMMA . Specifically, weset HiMemFormer with 64 seconds and 5 seconds for long and short-term memories, respectively. demonstrates that HiMemFormer significantly outperforms LSTR by at 0.8%, 4%, 1.9%and 0.8% for all four scenarios respectively in terms of mAP, demonstrating the effectiveness of thehierarchical design of joint agent-specific and contextual memory for inferencing. It is worth notingthat HiMemFormer also outperforms MAT by a larger margin in 2 2 scenario, given that MATadditionally learns features from the future. This critical observation indicates the significance ofhierarchical global information in multi-agent action anticipation. To ensure a fair comparison, wedevelop HiMemFormer+ on top of MAT to align with its temporal feature on utilizing extrafuture features, and integrate the hierarchical transformer block for multi-agent action anticipation.Results shows around 2% improvements over both baselines. More details in Appendix C.3 and C.4.",
  "Conclusion": "We present Hierarchical Memory-Aware Transformer (HiMemFormer), a transformer-based architec-ture with hierachical global and local memory attention mechanisms for online action anticipation, toovercome the weakness of the existing methods that can only complete modeling temporal depen-dency or only modeling agent interaction dependency without considering global historical context.Through experiments on four different scenarios involving multi-agents interactions, we show itscapability of modeling both temporal and spatial dependencies, demonstrating the importance of bothlong-term historical context and short-term agent-specific information.",
  "Y. Abu Farha and J. Gall. Uncertainty-aware anticipation of activities. In Proceedings of theIEEE/CVF International Conference on Computer Vision Workshops, pages 00, 2019": "Y. Abu Farha, A. Richard, and J. Gall. When will you do what?-anticipating temporal occur-rences of activities. In Proceedings of the IEEE conference on computer vision and patternrecognition, pages 53435352, 2018. Y. Abu Farha, Q. Ke, B. Schiele, and J. Gall. Long-term anticipation of activities with cycleconsistency. In Pattern Recognition: 42nd DAGM German Conference, DAGM GCPR 2020,Tbingen, Germany, September 28October 1, 2020, Proceedings 42, pages 159173. Springer,2021. A. Alahi, K. Goel, V. Ramanathan, A. Robicquet, L. Fei-Fei, and S. Savarese. Social lstm:Human trajectory prediction in crowded spaces. In Proceedings of the IEEE conference oncomputer vision and pattern recognition, pages 961971, 2016.",
  "S. Bhagat, S. Stepputtis, J. Campbell, and K. Sycara. Knowledge-guided short-context actionanticipation in human-centric videos, 2023. URL": "J. Chen, Z. Lv, S. Wu, K. Q. Lin, C. Song, D. Gao, J.-W. Liu, Z. Gao, D. Mao, and M. Z. Shou.Videollm-online: Online video large language model for streaming video. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1840718418,2024. D. Damen, H. Doughty, G. M. Farinella, A. Furnari, J. Ma, E. Kazakos, D. Moltisanti, J. Munro,T. Perrett, W. Price, and M. Wray. Rescaling egocentric vision: Collection, pipeline andchallenges for epic-kitchens-100. International Journal of Computer Vision (IJCV), 130:3355,2022. URL R. De Geest, E. Gavves, A. Ghodrati, Z. Li, C. Snoek, and T. Tuytelaars. Online action detection.In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands,October 11-14, 2016, Proceedings, Part V 14, pages 269284. Springer, 2016.",
  "H. Fan, Y. Li, B. Xiong, W.-Y. Lo, and C. Feichtenhofer. Pyslowfast. 2020": "A. Furnari and G. M. Farinella. What would you expect? anticipating egocentric actions withrolling-unrolling lstms and modality attention. In Proceedings of the IEEE/CVF Internationalconference on computer vision, pages 62526261, 2019. A. Furnari, S. Battiato, and G. Maria Farinella. Leveraging uncertainty to rethink loss functionsand evaluation measures for egocentric action anticipation. In Proceedings of the Europeanconference on computer vision (ECCV) workshops, pages 00, 2018. H. Gammulle, S. Denman, S. Sridharan, and C. Fookes. Predicting the future: A jointly learntmodel for action anticipation. In Proceedings of the IEEE/CVF International Conference onComputer Vision, pages 55625571, 2019. H. Girase, N. Agarwal, C. Choi, and K. Mangalam. Latency matters: Real-time action forecast-ing transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1875918769, 2023. D. Gong, J. Lee, M. Kim, S. J. Ha, and M. Cho. Future transformer for long-term actionanticipation. In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 30523061, 2022. K. Grauman, A. Westbury, E. Byrne, Z. Chavis, A. Furnari, R. Girdhar, J. Hamburger, H. Jiang,M. Liu, X. Liu, M. Martin, T. Nagarajan, I. Radosavovic, S. K. Ramakrishnan, F. Ryan,J. Sharma, M. Wray, M. Xu, E. Z. Xu, C. Zhao, S. Bansal, D. Batra, V. Cartillier, S. Crane, T. Do,M. Doulaty, A. Erapalli, C. Feichtenhofer, A. Fragomeni, Q. Fu, C. Fuegen, A. Gebreselasie,C. Gonzalez, J. Hillis, X. Huang, Y. Huang, W. Jia, W. Khoo, J. Kolar, S. Kottur, A. Kumar,F. Landini, C. Li, Y. Li, Z. Li, K. Mangalam, R. Modhugu, J. Munro, T. Murrell, T. Nishiyasu,W. Price, P. R. Puentes, M. Ramazanova, L. Sari, K. Somasundaram, A. Southerland, Y. Sugano,R. Tao, M. Vo, Y. Wang, X. Wu, T. Yagi, Y. Zhu, P. Arbelaez, D. Crandall, D. Damen, G. M.Farinella, B. Ghanem, V. K. Ithapu, C. V. Jawahar, H. Joo, K. Kitani, H. Li, R. Newcombe,A. Oliva, H. S. Park, J. M. Rehg, Y. Sato, J. Shi, M. Z. Shou, A. Torralba, L. Torresani, M. Yan,and J. Malik. Ego4d: Around the World in 3,000 Hours of Egocentric Video. In IEEE/CVFComputer Vision and Pattern Recognition (CVPR), 2022.",
  "A. Hogan, E. Blomqvist, M. Cochez, C. dAmato, G. D. Melo, C. Gutierrez, S. Kirrane, J. E. L.Gayo, R. Navigli, S. Neumaier, et al. Knowledge graphs. ACM Computing Surveys (Csur), 54(4):137, 2021": "Y. Huang, H. Bi, Z. Li, T. Mao, and Z. Wang. Stgat: Modeling spatial-temporal interactionsfor human trajectory prediction. In Proceedings of the IEEE/CVF international conference oncomputer vision, pages 62726281, 2019. Y. Huang, X. Yang, and C. Xu. Multimodal global relation knowledge distillation for egocentricaction anticipation. In Proceedings of the 29th ACM International Conference on Multimedia,MM 21, page 245254, New York, NY, USA, 2021. Association for Computing Machinery.ISBN 9781450386517. doi: 10.1145/3474085.3475327. URL A. Jain, A. Singh, H. S. Koppula, S. Soh, and A. Saxena. Recurrent neural networks for driveractivity anticipation via sensory-fusion architecture. In 2016 IEEE international conference onrobotics and automation (ICRA), pages 31183125. IEEE, 2016.",
  "Y. Li, M. Liu, and J. M. Rehg. In the eye of the beholder: Gaze and actions in first person video.IEEE transactions on pattern analysis and machine intelligence, 45(6):67316747, 2021": "Y. Li, C.-Y. Wu, H. Fan, K. Mangalam, B. Xiong, J. Malik, and C. Feichtenhofer. Mvitv2:Improved multiscale vision transformers for classification and detection. In Proceedings of theIEEE/CVF conference on computer vision and pattern recognition, pages 48044814, 2022. S. B. Loh, D. Roy, and B. Fernando. Long-term action forecasting using multi-headed attention-based variational recurrent neural networks. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, pages 24192427, 2022.",
  "C. Rodriguez, B. Fernando, and H. Li. Action anticipation by predicting future dynamic images.In Proceedings of the European Conference on Computer Vision (ECCV) Workshops, pages 00,2018": "T. Salzmann, B. Ivanovic, P. Chakravarty, and M. Pavone. Trajectron++: Dynamically-feasibletrajectory forecasting with heterogeneous data. In Computer VisionECCV 2020: 16th EuropeanConference, Glasgow, UK, August 2328, 2020, Proceedings, Part XVIII 16, pages 683700.Springer, 2020. P. Schydlo, M. Rakovic, L. Jamone, and J. Santos-Victor. Anticipation in human-robot coopera-tion: A recurrent neural network approach for multiple action sequences prediction. In 2018IEEE International Conference on Robotics and Automation (ICRA), pages 59095914. IEEE,2018. F. Sener, D. Singhania, and A. Yao. Temporal aggregate representations for long-range videounderstanding. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK,August 2328, 2020, Proceedings, Part XVI 16, pages 154171. Springer, 2020. B. Soran, A. Farhadi, and L. Shapiro. Generating notifications for missing actions: Dont forgetto turn the lights off! In Proceedings of the IEEE International Conference on Computer Vision,pages 46694677, 2015.",
  "M. Xu, Y. Xiong, H. Chen, X. Li, W. Xia, Z. Tu, and S. Soatto. Long short-term transformer foronline action detection. In Conference on Neural Information Processing Systems (NeurIPS),2021": "L. Yang, J. Han, and D. Zhang. Colar: Effective and efficient online action detection byconsulting exemplars. In Proceedings of the IEEE/CVF conference on computer vision andpattern recognition, pages 31603169, 2022. C. Yu, X. Ma, J. Ren, H. Zhao, and S. Yi. Spatio-temporal graph transformer networks forpedestrian trajectory prediction. In Computer VisionECCV 2020: 16th European Conference,Glasgow, UK, August 2328, 2020, Proceedings, Part XII 16, pages 507523. Springer, 2020. Y. Yuan, X. Weng, Y. Ou, and K. M. Kitani. Agentformer: Agent-aware transformers for socio-temporal multi-agent forecasting. In Proceedings of the IEEE/CVF International Conferenceon Computer Vision, pages 98139823, 2021.",
  "ARelated Work": "Action AnticipationOnline action anticipation aims to predict the future actions of the agentgiven the past and current action information. Given its increasing popularity and its broad practicalapplications, many large-scale datasets and benchmarks has been proposed to facilitateresearchers in this area. In the field of action anticipation, feature learning and temporalmodeling are the two main streams of approaches. Recurrent neural network (RNN) is widelyadopted by many prior works due to its powerful long-term temporal dependency.For example, RULSTM proposed to anticipate actions via a rolling LSTM to encode historicalinformation and unrolling LSTM make predictions on future actions. To better model time andsocial dimension in a multi-agent setting, a popular line of research uses temporal models tosummarize features over time for each agent separately and then feed temporal features into socialmodels to obtain global-aware agent features. There are also works that uses social modelsto generate social features of for individual agents and apply temporal models to summarize socialfeatures for each agent. For example, Trajection++ design a graph-structured recurrent model thatforecasts the trajectories of a general number of diverse agents while incorporating agent dynamicsand heterogeneous data. However, previous methods failed to consider both temporal dependenciesand social dependencies at once, which can be sub-optimal. More recent work manageto overcome this short coming by considering both time and social dimensions simultaneously,facilitating interaction across temporal domain and spatial domain. Transformer for Video UnderstandingRecently, transformer-based methods stood out in literature because of its strong capability for long-range temporal dependencies.For example, Gong et al. proposed Future Transformer (FUTR), an end-to-end attention neuralnetwork that anticipate actions in parallel decoding, leveraging global interactions between past andfuture actions for long-term action anticipation. LSTR further decomposes the memory encoderinto long and short-term stages for online action detection and anticipation, allowing model to learnmore representative features from the history. MAT proposes a new memory-anticipation-basedparadigm that models the entire temporal structure, including past, present and future. Also to utilizedifferent sources such as optical flow and audio data, multi-modal fusion approaches has been proposed to improve the accuracy of future action prediction. In addition, large languagemodel (LLM) is deployed to tackle action anticipation task due to its strong high-level reasoningcapability.",
  "B.1Agent-to-Context Encoder": "Agent-specific long-term memory provide useful information about the historical actions of the agent,but when placed in a complex environment with multi-agent interactions, it is crucial to pay attentionto contextual information that are shared across all agents. To this end, Agent-to-Context MemoryEncoder follows the specific-to-general approach, managing to augment agents long-term memoryby paying extra attention to global features via cross-attention. Agent Memory Encoding.For each agent in a scene, we input target agents long-term memoryfeatures M(a)Lto the Transformer Block and compress target agents long-term feature into a latentrepresentation of fixed length. Following prior work , we utilize a two-stage memorycompression transformer module, denoted F(a)L , consists of multiple transformer decoder unit to",
  "encoder the agent-specific long-term history M(a)L :M(a)L= F(a)L (M(a)L , M(a)L )(1)": "Context Memory Enhancement.To effectively encode contextual information into agents long-term memory, we propose a specific-to-general approach. In practice, we send contextual long-term history M(c)L (with positional embedding) as queries and M(a)Lto our context encoder, F(c)L ,constructed with a transformer decoder architecture . Using contextual long-term history to guidethe encoded agents long-term history, we acquire the final encoded long-term memory ML:ML = F(c)L (M(c)L , M(a)L )(2)",
  "B.2Context-to-Agent Decoder": "To implement our key idea to predict agent future actions based on both contextual information andagent-specific information, we meticulously design our Context-to-Agent Decoder using a coarse-to-fine apporach. In particular, leveraging informative short-term features, as demonstrated in LSTR, we first make a coarse prediction that contains possible future actions of all agents in thescene using contextual short-term features, and narrow down to target agents future action usingagent-specific short-term features as queries to the transformers units. We also supervise on bothcoarse and precise action predictions. Coarse Action AnticipationTo enable the model to learn future actions, we need to generate alatent embedding that allows the model to learn future actions from the past and the present. Inpractise, we initialize NF learnable query tokens QF RNF D where D is the feature dimensionand concatenate with contextual short-term memories M(c)S to form Mcoarse = {M(c)S , Q(c)F }. Wethen take M(c)coarse as queries and cross-attentioned with augmented long-term memory ML through atransformer decoder architecture Fcoarse to make coarse action predictions given only contextualinformation:Mcoarse = Fcoarse(Mcoarse, ML)(3) Precise Action RefinementUntil now, the future action query token contain general informationabout agents future action. To generate more accurate predicted actions, we leverage the agent-specific short-term history, M(a)S , that contains feature representations of agents unique feature andconcatenate them with learnable query tokens from the coarse prediction QF to form Mfine ={M(a)S , QF }. Following the similar recipe we add another transformer block, denoted Ffine togenerate the final action prediction M:M = Ffine(Mfine, Mcoarse)(4)",
  "The loss function for HiMemFormer comprises two essential components: the coarse action lossLcoarse and the refined action loss Lfine. The overall respresentation is:L = a Lcoarse + b Lfine(5)": "We then use the empirical cross entropy loss between each agents predicted action anticipationprobability distribution Pt RT (K+1) and the ground truth anticipation label yt {0, 1, ..., K}.For the coarse action anticipation, we utilize the ground truth anticipation label of all agents in thescene. For the refined action anticipation, we ony use the ground truth anticipation label of the targetagent.",
  "ML = 3276.353.747.770ML = 6476.354.248.470.6ML = 12877.451.245.468.2ML = 25674.548.946.468.7": "will be trained on 333 out of 445 egocentric videos, as each activity may have multiple egocentricvideos. For action anticipation task, we follow prior work and split training and validation sets withratios 3: 1, 1:3, 1: 3 and 1:3 for the four scenarios 1 1, 1 2, 2 1, 2 2, respectively, resulting in(96,19,16,13) activities for training and (31, 57, 50, 42) activities for evaluation in four scenarios.",
  "C.2Experiment Settings": "We implemented our proposed model in PyTorch and performed all experiments on a system witha single NVIDIA A40 graphics cards. For all transformer blocks inside both encoder and decodermodule, we set the number of heads to 4 and hidden units as 1024 dimensions. The model isoptimized by AdamW optimizer with a weight decay of 1 104 We use warm-up learning ratelinearly increase from zero to 7 105 in the first 10 epoch. In addition, the model is optimizedwith batch size of 16 and training is terminated after 25 epochs. Following prior works experimentsettings, we use a pretrained feature extractor extract action features from the video.",
  "C.3Comparison with Baselines": "We compare HiMemFormer with prior methods on LEMMA for action anticipation in both single-and multi-agent environment. Specifically, both baselines, LSTR and MAT , only takein agents first-person-view live streaming video as the input without considering the contextualinformation from the third-person-view videos. The above set up ensure that the performance gainshown in table 1 come from the proposed multi-view integration of agent and context memory.",
  "C.4Ablation Studies": "Effect of Memory SizeWe first analyze the effect of the length of both short term and longterm memory. Following LSTR , we fix the short-term memory to 5 seconds and test ML {32, 64, 128, 256}, while maintaining same memory size for multi-view videos. Similarly, we fixthe long-term memory to 64 seconds and test MS {2, 5, 10}. Results shown in table 2 and 3reach the same conclusion in , where increasing memory size does not always guarantee betterperformance. Effect of Down-Sampling RateWe also test the effect of compression ratio of long-term memory,and we implement HiMemFormer with 5 seconds of short-term memories (both agent and globalperspective) and 64 seconds of long-term memories. Results are shown in table 4.",
  "C.5Discussion on Future Directions": "HiMemFormer serves as an attempt to tackle action-anticipation in complex multi-agent environment,but theres more to be explored. Future work could focus on expanding HiMemFormers capabilitiesto better interpret complex multi-agent interactions. Potential directions include leveraging largelanguage models (LLMs) to enhance models interpretability and flexibility in the dynamic environ-ment . Knowledge graphs or scene graphs can also serves as a powerful featurerepresentations to further boost the performance. Additionally, while this study demonstratesthe importance of long-term historical context and short-term agent-specific information, exploringadaptive mechanisms that dynamically adjust the emphasis between these dependencies could furtherimprove the models responsiveness to real-time changes."
}