{
  "Abstract": "3D Gaussian Splatting has advanced radiance field reconstruction, enabling high-quality view synthesis and fast rendering in 3D modeling. While adversarial attackson object detection models are well-studied for 2D images, their impact on 3Dmodels remains underexplored. This work introduces the Masked Iterative FastGradient Sign Method (M-IFGSM), designed to generate adversarial noise target-ing the CLIP vision-language model. M-IFGSM specifically alters the object ofinterest by focusing perturbations on masked regions, degrading the performanceof CLIPs zero-shot object detection capability when applied to 3D models. Usingeight objects from the Common Objects 3D (CO3D) dataset, we demonstrate thatour method effectively reduces the accuracy and confidence of the model, with ad-versarial noise being nearly imperceptible to human observers. The top-1 accuracyin original model renders drops from 95.4% to 12.5% for train images and from91.2% to 35.4% for test images, with confidence levels reflecting this shift fromtrue classification to misclassification, underscoring the risks of adversarial attackson 3D models in applications such as autonomous driving, robotics, and surveil-lance. The significance of this research lies in its potential to expose vulnerabilitiesin modern 3D vision models, including radiance fields, prompting the developmentof more robust defenses and security measures in critical real-world applications.",
  "Introduction": "Computer vision has rapidly advanced with the development of large-scale datasets and vision-language models such as CLIP Li u. a. (2022). These advancements have expanded the application ofcomputer vision algorithms into areas critical to modern life, such as humanoid robots, autonomousdriving, and surveillance systems Kim u. a. (2024)Zhou u. a. (2024). Ensuring the accuracy androbustness of these systems is crucial, as errors could lead to severe consequences, from accidents inautonomous vehicles to failures in surveillance systems. While adversarial attacks on 2D vision models have been widely studied Goodfellow u. a. (2015);Szegedy u. a. (2014), their impact on 3D models remains underexplored Li u. a. (2024); Song u. a.(2024). This gap is significant, as 3D models are increasingly used in real-world applications,including robotics, augmented reality, and autonomous systemsZhu u. a. (2023); Zhao u. a. (2019).Recent progress in 3D model rendering techniques, such as 3D Gaussian Splatting (3DGS) Kerbl u. a.(2023), has enabled high-quality view synthesis and efficient rendering in 3D modeling, presentingnew opportunities, and vulnerabilities, for adversarial attacksZeng u. a. (2019). However, there is *Dr. Ergezer holds concurrent appointments as an Associate Professor at Wentworth Institute of Technologyand as an Amazon Visiting Academic. This paper describes work performed at Wentworth Institute of Technologyand is not associated with Amazon.",
  "limited research on the robustness of 3D models against such attacks, especially in vision-languagecontexts Zou u. a. (2023)": "In this work, we introduce the Masked Iterative Fast Gradient Sign Method (M-IFGSM), anovel adversarial attack designed to degrade the performance of the CLIP vision-language modelin 3D object detection. Unlike traditional 2D adversarial attacks Madry u. a. (2018), M-IFGSMfocuses perturbations on masked regions of 3D objects, creating adversarial noise that remains nearlyimperceptible to humans while significantly degrading model accuracy. This masked approachallows for a more targeted attack, where only specific regions of the 3D object are perturbed, makingdetection of the adversarial noise more challenging. We validate our approach through extensive experiments using the CO3D dataset Reizenstein u. a.(2021), demonstrating that M-IFGSM can substantially lower CLIPs top-1 accuracy from 95.4%to 12.5% on training images, and from 91.2% to 35.4% on test images. These results illustrate thepotential risks posed by adversarial attacks on 3D models, which are becoming increasingly prevalentin safety-critical applications such as autonomous driving, robotics, and surveillance. By exposingvulnerabilities in vision-language models when applied to 3D objects, this research underscores thepressing need to develop robust defense mechanisms in these high-stakes domains. Our contributions are threefold: i) First, we propose the M-IFGSM method, which introduces a novelapproach to adversarial attacks in 3D models within a vision-language context. ii) Second, we leverage3D Gaussian Splatting to generate adversarial perturbations that are localized to masked regions,providing a fine-grained method to compromise model performance. iii) Finally, we emphasize thereal-world implications of such attacks, underscoring the urgent need to develop robust defenses forsystems relying on 3D object detection in critical applications. Our approach significantly reducesCLIPs accuracy in 3D object detection with minimal, almost imperceptible noise, resulting in asubstantial drop in top-1 accuracy across both train and test datasets.",
  "Method": "In this section, we introduce our proposed method, the Masked Iterative Fast Gradient Sign Method(M-IFGSM), designed to generate adversarial perturbations specifically targeting 3D models in avision-language context. While most adversarial methods perturb the entire image uniformly, this canlead to suboptimal attacks and degrade downstream tasks, such as 3D reconstruction. Our approachenhances traditional adversarial attack methods by focusing perturbations solely on the object ofinterest within input images. We detail the components of our pipeline, including segmentation usingthe Segment Anything Model (SAM), adversarial perturbation generation with M-IFGSM, and 3Dmodel reconstruction using Gaussian Splatting. Our method consists of three main stages: i) Mask Generation: Utilizing the Segment AnythingModel (SAM) to extract object masks from input images. ii) Adversarial Perturbation Generation:Applying M-IFGSM to generate noise focused on masked regions. iii) 3D Model Reconstruction:Building 3D models from adversarial images using Gaussian Splatting. illustrates the overallpipeline.",
  "Mask Generation with SAM": "We employ the Segment Anything Model (SAM) Kirillov u. a. (2023) to generate accurate segmenta-tion masks for the target objects within the input images. Utilizing the pre-trained sam-vit-h-4b899checkpoint, SAM provides high-quality masks without the need for additional training. The maskgeneration process outputs the segmentation mask Mi, bounding boxes, and quality metrics.",
  "The untargeted attack is formulated as:": ": Two-stage pipeline for generating adversarial 3D models using the proposedMasked Iterative Fast Gradient Sign Method (M-IFGSM) and 3D Gaussian Splatting. In thefirst stage (left), original images Ii are processed to generate a mask Mi, which specifies theregions where adversarial noise will be applied. Using this mask, adversarial perturbationsare crafted to produce adversarial images Ai, which contain noise designed to misleaddetection systems. In the second stage (right), these adversarial images are used to create a3D model through 3D Gaussian Splatting, resulting in a 3DGS point cloud that capturesthe adversarial characteristics in a 3D format. Finally, the 3D model is rendered to produceimages that retain the adversarial properties, allowing for an evaluation of adversarial noiseeffects in 3D object detection.",
  "Algorithm 1 details the iterative process for generating adversarial images. In our experiments, weset the loss threshold to 20.00 for early stopping": "Our pipeline is designed with flexibility in mind, allowing it to generate adversarial noise for a varietyof object classification models, not solely the CLIP model. Our methodology leverages a MaskedIterative Fast Gradient Sign Method (M-IFGSM) that can target any differentiable model f to computegradients and generate adversarial perturbations. This adaptability allows our pipeline to extend",
  "D Model Reconstruction": "After generating adversarial images with M-IFGSM, we reconstruct 3D models using the GaussianSplatting method Kerbl u. a. (2023). We use an 85-15% train-test split, with 35 images for trainingand six for testing. The reconstruction process involves four stages. i) Initialization: Using a sparse point cloud fromStructure from Motion Schonberger und Frahm (2016) to initialize 3D Gaussians. ii) Optimization:Adjusting the Gaussians parameters (position, covariance, opacity, color) via gradient descent tomatch the adversarial images. iii) Adaptive Density Control: Refining the model by adding orsplitting Gaussians in under- or over-reconstructed areas. iiii) Rendering: Employing a tile-basedrasterizer for efficient real-time visualization.",
  "Experimental Setup": "We refer to the attacked model as the victim. We target the CLIP ViT-B/16 model Dosovitskiy u. a.(2021), which combines vision and language understanding. The model divides input images into16 16 patches and processes them using transformer layers. For our dataset, we employed the Common Objects in 3D (CO3D) dataset Reizenstein u. a. (2021),selecting one dataset for each of the eight object classes. Each dataset contains approximately 200images from different angles. We reduce the number to 41 images per class by selecting every fifthimage and resizing them to 224224 pixels to meet the input requirements of CLIP. Our experimentsare conducted on a system with dual NVIDIA RTX 3090 GPUs.",
  "Results": "In this section, we present and analyze the outcomes of our experiments conducted to evaluatethe efficacy of M-IFGSM for generating adversarial noise in the context of 3D Gaussian Splatting(3DGS). Our primary objective was to assess the impact of adversarial attacks on the classificationperformance of the object detection model when subjected to adversarially perturbed 3D models.First, we discuss the results of perturbed images generated by the M-IFGSM method, followed by therendered images generated by the 3DGS technique.",
  "Perturbed Images Results": "presents the classification confidence and accuracy for perturbed images generated usingM-IFGSM. The Original Images column contains the confidence and top-1/top-5 accuracy forunmodified images, serving as baseline performance. The Adversarial Images column shows thesame statistics for perturbed images, highlighting the degradation in the models confidence andaccuracy due to the adversarial perturbations. : Adversarial noise effects on object detection of the hairdryer and hydrant objects. Sampleimages of the original image results (left top and bottom) and their attacked counterparts (right topand bottom).",
  "Figures 3 provide visual confirmation of the adversarial noises effect, as the attacked objects showclear degradation in classification confidence when compared to their original counterparts": ": Average CLIP-ViT-B/16 classification results for input images and adversarial imagesgenerated with M-IFGSM. Confidence1 refers to the average confidence assigned to the correct classfor original images, while Confidence2 is the confidence in incorrect predictions for adversarialimages. Top1 and Top5 represent the occurrence of the true label in the top-1 and top-5predictions, respectively.",
  "Average0.7300.9490.9960.8800.0210.064": "The average results show that for original images, the true label appears in the top-1 predictions94.9% of the time and in the top-5 predictions 99.6% of the time. After the application of M-IFGSM,these figures drop significantly, with top-1 accuracy falling to just 2.1%, and top-5 accuracy to 6.4%.The models misclassification confidence increases dramatically for adversarially perturbed images,showcasing the methods effectiveness. The Adversarial Images column in provides the same statistics for the perturbed imagesgenerated using the M-IFGSM method. These values reflect the impact of the adversarial perturbationson the classification performance, highlighting the effectiveness of the M-IFGSM method in degradingthe models confidence and accuracy. By comparing the results between the original and adversarialimages, we can quantitatively assess the vulnerability of the object detection system to the adversarialattack.",
  "Rendered Images Results": "The additional results on the rendered images further highlight the impact of adversarial perturbationson 3D models. In , we present the results of the classification confidence and accuracy ofrendered images from the 3D Gaussian Splatting (3DGS) models. The original 3DGS model wascreated with a clean image dataset, while the adversarial 3DGS model was created with perturbedimages. At this point, we split the images into 85% train and 15% test sets, meaning 35 out of 41images were used in the 3D modeling process, while 6 out of 41 images camera positions were usedas test renders after the models were created. : Average CLIP-ViT-B/16 classification results for original 3DGS model renders and adversar-ial 3DGS model renders. Confidence1 refers to the average confidence assigned to the correct classfor original model renders, while Confidence2 shows the confidence in incorrect predictions foradversarial model renders. Top1 and Top5 represent the occurrence of the true label in the top-1and top-5 predictions, respectively.",
  "Average (Test)0.7320.9121.0000.4570.3540.800": "The notable case of the couch dataset, where the segmentation process only perturbs one of twocouches in the image, underscores a limitation in current masking techniques when dealing withmultiple instances of the same class in a scene. particularly emphasizes the models struggle when confronted with novel views of the object,where the adversarial perturbation has a diminished impact due to the lack of adaptation for unseencamera positions. Results indicate that renders from training camera locations successfully transfer the adversarial noiseinto 3DGS models, decreasing the confidence level to 12%. Conversely, renders from test cameralocations cause a drastic decrease in the confidence level of the system, though this varies acrossdifferent object classes. The overall degradation in performance demonstrates that the proposedM-IFGSM attack effectively transfers to 3DGS models, though some specific cases, such as the couchdataset, indicate potential avenues for improving adversarial robustness. : Adversarial noise effect on object detection. Sample images of the original image dataset(left top and bottom) and their attacked counterparts (middle top and bottom) and prediction from 3Dmodel renders (right top and bottom). The bottom row is from a test image, indicating that the 3Dmodel has not updated for this veiw",
  "Conclusions": "In this paper, we investigated adversarial attacks targeting vision-language models, specificallyfocusing on CLIPs object detection capabilities, and explored the transferability of these attacks to3D models. Our study primarily centered on the application of the Masked Iterative Fast GradientSign Method (M-IFGSM), and we demonstrated how this adversarial noise could be effectivelyintegrated into 3D Gaussian Splatting models. By employing the Common Objects 3D (CO3D)dataset, we conducted experiments on eight distinct object classes, creating noisy 3D models withadversarial noise from 35 images captured from different angles. The significance of this work lies in its extension of adversarial attacks from 2D vision systems to3D object detection models, which presents a previously unexplored area of research. Our findingsindicate that, with the addition of segmentation, the adversarial noise designed for 2D models notonly successfully transfers to the 3D domain but also leads to a substantial degradation in modelperformance. While the target class was consistently identified as the top-1 prediction in the original3D models, under adversarial conditions, the models top-1 accuracy sharply decreased. For thetraining images, the average top-1 accuracy fell from 95.4% to 12.5%, and for the test images, itdropped from 91.2% to 35.4%. This demonstrates that adversarial perturbations in a 2D context canprofoundly impact the accuracy of 3D models, including those constructed from Gaussian Splattingtechniques. This work represents a novel contribution to the adversarial machine learning field by bridging thegap between 2D adversarial attacks and their effects in 3D environments. Additionally, our approachhighlights the vulnerabilities of cutting-edge models like CLIP when tasked with multi-view object",
  "[Goodfellow u. a. 2015]GOODFELLOW, Ian J. ; SHLENS, Jonathon ; SZEGEDY, Christian: Explain-ing and Harnessing Adversarial Examples. 2015. URL": "[Kerbl u. a. 2023]KERBL, Bernhard ; KOPANAS, Georgios ; LEIMKHLER, Thomas ; DRETTAKIS,George: 3D Gaussian Splatting for Real-Time Radiance Field Rendering. In: ACM Transac-tions on Graphics 42 (2023), July, Nr. 4. URL [Kim u. a. 2024]KIM, Yeseung ; KIM, Dohyun ; CHOI, Jieun ; PARK, Jisang ; OH, Nay-oung ; PARK, Daehyung:A survey on integration of large language models with intelligentrobots. In: Intelligent Service Robotics (2024), August. URL ISSN 1861-2784 [Kirillov u. a. 2023]KIRILLOV, Alexander ; MINTUN, Eric ; RAVI, Nikhila ; MAO, Hanzi ; ROL-LAND, Chloe ; GUSTAFSON, Laura ; XIAO, Tete ; WHITEHEAD, Spencer ; BERG, Alexander C. ;LO, Wan-Yen ; DOLLR, Piotr ; GIRSHICK, Ross: Segment Anything. 2023 [Li u. a. 2022]LI, Feng ; ZHANG, Hao ; ZHANG, Yi-Fan ; LIU, Shilong ; GUO, Jian ; NI, Lionel M. ;ZHANG, PengChuan ; ZHANG, Lei: Vision-Language Intelligence: Tasks, Representation Learning,and Large Models. 2022. URL [Li u. a. 2024]LI, Yanjie ; XIE, Bin ; GUO, Songtao ; YANG, Yuanyuan ; XIAO, Bin: A surveyof robustness and safety of 2d and 3d deep learning models against adversarial attacks. In: ACMComputing Surveys 56 (2024), Nr. 6, S. 137 [Madry u. a. 2018]MADRY, Aleksander ; MAKELOV, Aleksandar ; SCHMIDT, Ludwig ; TSIPRAS,Dimitris ; VLADU, Adrian: Towards deep learning models resistant to adversarial attacks. In:International Conference on Learning Representations (ICLR), 2018 [Reizenstein u. a. 2021]REIZENSTEIN, Jeremy ; SHAPOVALOV, Roman ; HENZLER, Philipp ;SBORDONE, Luca ; LABATUT, Patrick ; NOVOTNY, David: Common Objects in 3D: Large-ScaleLearning and Evaluation of Real-life 3D Category Reconstruction. In: International Conferenceon Computer Vision, 2021 [Schonberger und Frahm 2016]SCHONBERGER, Johannes L. ; FRAHM, Jan-Michael: Structure-From-Motion Revisited. In: Proceedings of the IEEE Conference on Computer Vision and PatternRecognition (CVPR), June 2016 [Song u. a. 2024]SONG, Ziying ; LIU, Lin ; JIA, Feiyang ; LUO, Yadan ; JIA, Caiyan ; ZHANG,Guoxin ; YANG, Lei ; WANG, Li: Robustness-aware 3d object detection in autonomous driving: Areview and outlook. In: IEEE Transactions on Intelligent Transportation Systems (2024)",
  "[Szegedy u. a. 2014]SZEGEDY, Christian ; ZAREMBA, Wojciech ; SUTSKEVER, Ilya ; BRUNA,Joan ; ERHAN, Dumitru ; GOODFELLOW, Ian ; FERGUS, Rob: Intriguing properties of neuralnetworks. 2014. URL": "[Zeng u. a. 2019]ZENG, Xiaohui ; LIU, Chenxi ; WANG, Yu-Siang ; QIU, Weichao ; XIE, Lingxi ;TAI, Yu-Wing ; TANG, Chi-Keung ; YUILLE, Alan L.: Adversarial attacks beyond the image space.In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019,S. 43024311 [Zhao u. a. 2019]ZHAO, Zhong-Qiu ; ZHENG, Peng ; XU, Shou-tao ; WU, Xindong: Objectdetection with deep learning: A review. In: IEEE transactions on neural networks and learningsystems 30 (2019), Nr. 11, S. 32123232 [Zhou u. a. 2024]ZHOU, Xingcheng ; LIU, Mingyu ; YURTSEVER, Ekim ; ZAGAR, Bare L. ;ZIMMER, Walter ; CAO, Hu ; KNOLL, Alois C.: Vision Language Models in Autonomous Driving:A Survey and Outlook. 2024. URL [Zhu u. a. 2023]ZHU, Zijian ; ZHANG, Yichi ; CHEN, Hai ; DONG, Yinpeng ; ZHAO, Shu ; DING,Wenbo ; ZHONG, Jiachen ; ZHENG, Shibao: Understanding the Robustness of 3D Object DetectionWith Birds-Eye-View Representations in Autonomous Driving. In: Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition, 2023, S. 2160021610"
}