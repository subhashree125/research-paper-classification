{
  "arXiv:2412.19806v1 [cs.CV] 8 Oct 2024": "we present VITRON, a universal pixel-level vision LLM designed for comprehen-sive understanding, generating, segmenting, and editing of both static images anddynamic videos. Building on top of an LLM backbone, VITRON incorporatesencoders for images, videos, and pixel-level regional visuals within its frontendmodules, while employing state-of-the-art visual specialists as its backend, viawhich VITRON supports a spectrum of vision end tasks, spanning visual comprehen-sion to visual generation, from low level to high level. To ensure an effective andprecise message passing from LLM to backend modules for function invocation,we propose a novel hybrid method by simultaneously integrating discrete textualinstructions and continuous signal embeddings. Further, we design various pixel-level spatiotemporal vision-language alignment learning for VITRON to reach thebest fine-grained visual capability. Finally, a cross-task synergy module is advisedto learn to maximize the task-invariant fine-grained visual features, enhancing thesynergy between different visual tasks. Demonstrated over 12 visual tasks andevaluated across 22 datasets, VITRON showcases its extensive capabilities in thefour main vision task clusters. Overall, this work illuminates the great potential ofdeveloping a more unified multimodal generalist.",
  "Introduction": "Recently, the field of multimodal large language models (MLLMs) has witnessed rapid and flourishingdevelopment across multiple communities. Extensive research efforts have been directed towardsaugmenting powerful, purely language-based LLMs with modules capable of visual perception,thereby extending their applicability to MLLMs . MLLMs, such as BLIP-2 , LLaVA , MiniGPT-4 and GPT-4V etc., demonstrate a robust and exceptionalcapability in image understanding, paralleling the deep semantic comprehension of language. Inthe realm of vision, the ability to process and comprehend dynamic videos is equally critical.Concurrently, several MLLMs have emerged with a focus on video understanding, e.g., VideoChat and Video-LLaMA , demonstrating significant advancements in video comprehension. Subsequent studies have sought to further expand the capabilities of MLLMs, with efforts bifurcatinginto two primary dimensions. On one hand, theres a deepening of MLLMs understanding ofvision, transitioning from coarse, instance-level comprehension towards a pixel-level, fine-finedunderstanding of images, thereby achieving visual regional grounding capabilities, as seen in GLaMM, PixelLM , and MiniGPT-v2 , etc., alongside the counterparts in pixel-grounding videoLLMs . On the other hand, theres an expansion in the breadth of functionalities MLLMscan support within the vision field. A portion of the research has already ventured into enablingMLLMs not just to comprehend input vision signals but also to support the generation and output ofvision content, with systems like GILL , Emu , etc., flexibly generating image content, andGPT4Video and NExT-GPT achieving video generation. We posit that the future trend of vision LLMs necessarily involves the enhancement of their capabilitiestowards a high degree of unification, i.e., multimodal generalists. However, our observations revealthat despite the diversity of existing vision LLMs developed by the community, there is still a clearlack of unification. First, almost all existing vision LLMs treat images and videos as separate entities,either supporting only images or videos . We argue for a unified vision MLLMframework that concurrently supports both images and videos, acknowledging that vision inherentlycomprises both static images and dynamic videos - both core components of our world and largelyinterchangeable in most scenarios. Second, the current support for vision functionalities in MLLMsis found wanting, with most models only capable of understanding , or at most generatingimages or videos . We contend that future MLLMs should embrace a broader spectrum ofvision tasks and functionalities, enabling unified support for all vision-related tasks and achieving anone for all capability, which is vital for real-world applications, especially in vision creation thatoften involves a series of iterative and interactive operations. For example, users typically start bygenerating images from text, transforming an idea into visual content; and then refining this contentthrough further fine-grained editing to add more details; following, proceeding to create dynamiccontent by generating videos from the images; and finally, engaging in several rounds of iterativeinteraction, such as video editing, to enhance and finalize their creation. Last but not the least, for ageneralist integrated with various multimodal functionalities, one key lies in how to ensure that alltasks achieve their best performance as much as possible. This includes both that, 1) the instructions",
  "from the LLM are precisely conveyed to the downstream decoders, and 2) different tasks do notundermine each other but rather cooperate": "To address all these gaps, this paper introduces VITRON, a pioneering universal pixel-level visionLLM, as shown in . First, VITRON leverages a backbone LLM for comprehending, reasoning,decision-making, and multi-round user interactions. To perceive both image and video modal signalsand support fine-grained user visual inputs, VITRON incorporates encoders for images, videos, andregional box/sketch-specified inputs. On the backend, several state-of-the-art (SoTA) image andvideo modules are integrated for decoding and executing a wide range of vision tasks, spanningfrom lower to higher levels, such as visual understanding (perceiving and reasoning), generating,segmenting (grounding and tracking), editing (inpainting). To ensure that VITRON precisely conveysthe LLMs decisions to various backend decoder modules for function invocation, we propose a novelhybrid method of instruction passing. Specifically, we enable the LLM to output not only discretetextual instructions, but also continuous signal feature embeddings passed to the modules. Finally,to maximize the functionalities of different modules within VITRON, we further devise a synergymodule, where we fully maximize the task-persistent fine-grained visual features to be shared amongdifferent visual tasks. The overall training for VITRON aims to equip it with robust and powerful vision understanding andmanipulation capabilities. We first imbue VITRON basic MLLM skills by carrying out 1) vision-language alignment learning between the frontend encoders and central LLM, also 2) invocation-oriented instruction tuning, and 3) embedding-oriented alignment tuning between LLM and backendmodules. Going beyond this, we further try to strengthen VITRONs capacities. On the one hand, weintroduce fine-grained spatiotemporal vision grounding instruction tuning, training LLM on groundingpredictions and pixel-aware perception for images and videos, such that VITRON sufficiently gainspixel-level visual perception. On the other hand, we utilize adversarial training to decoupletask-specific features from task-invariant fine-grained visual features in signal feature representations,thereby enhancing the synergy between different tasks. Extensive experiments covering 12 tasks across 22 datasets are performed. Leveraging its advancedarchitecture as a multimodal generalist, VITRON demonstrates proficiency in a comprehensive rangeof vision tasks. Notably, the unified systems performance is on par with or even surpasses singletonstate-of-the-art specialists on specific tasks. Further analyses reveal the efficacy of each design of thesystem. Our overall contributions are summarized as follows.",
  ": Comparisons of existing (partially, imperfect coverage) representative vision MLLM": "introduce a more effective LLM-to-decode instruction-passing mechanism over both discrete textsand continuous signal embeddings. 3 We propose carrying out various pixel-level vision-languagespatiotemporal alignment learning for MLLMs to reach the best fine-grained visual capability. 4 Wedevise a synergy module to maximize the task-persistent fine-grained visual features shareable amongall different visual tasks, via which VITRON surpasses existing SoTA specialists performance.",
  "Related Work": "Achieving a profound understanding and comprehensive operational capabilities in vision, rangingfrom low-level visual pixel understanding to high-level compre-hension of overall semantics , represents a significanttopic. Recent years have seen the development of highly potent large-scale vision models, such asViT and CLIP , which have achieved remarkable vision understanding capabilities; modelslike SAM and SEEM have solved vision segmentation tasks; and diffusion-based models have reached unprecedented performance in vision generation. Yet thesemodels might lack an LLM as a central decision processor, unable to flexibly interpret user intentor execute tasks interactively . The emergence of LLMs has exhibited unprecedentedintelligence capability . Extending the success of language understanding in LLMs, re-searchers have promptly investigated and developed various MLLMs, enabling LLMs to comprehendvision. By integrating high-performance vision encoders of images or videos into language-basedLLMs, these models have been made capable of understanding vision signals .Going beyond vision understanding, further research has aimed to enhance MLLMs, for instance, byendowing them with vision generation capabilities or supporting pixel-level understandingand grounding . In we summarize some existing popular visionMLLMs in terms of the vision function support. However, we observe that current research on vision LLMs lacks depth in two critical aspects. Firstly,current vision LLMs tend to separate images and videos, supporting either one or the other. Theconstruction of a unified MLLM is crucial, as vision inherently encompasses both static imagesand dynamic videos, both of which are core components of our visual world. Thus, covering bothaspects simultaneously is essential for optimally adapting to practical applications. Although modelslike NExT-GPT have relatively well-supported unification across various modalities, theyfall short in supporting pixel-level in-depth vision understanding and comprehensive support forvision operation tasks. The second issue is the incomplete support for vision tasks by existingMLLMs. Most current MLLMs primarily support understanding images or videos , with only a few supporting generation or editing/inpainting . Building a generalist thatcan handle (almost) all vision-related tasks and operations in an end-to-end architecture shouldbe the next major trend for vision MLLMs. Yet simply integrating existing visual specialists intoan LLM to form MLLMs is not sufficient enough, as genuine human-level AI should possessuniversal intelligence with robust cross-task generalizability . Thus, it is necessary to furtherconsider how to enable synergy effects among different task specialists within a generalist, forwhich goal, we have devised a synergy strategy in this work. Besides, compared to the multimodalcomprehension capabilities of MLLM, endowing MLLM with strong multimodal generative abilitiesis even more challenging. The key lies in how to effectively and unbiasedly convey MLLMs semanticunderstanding signals to the backbone decoder modules. There are two mainstream approaches toLLM-to-decoder message passing within the MLLM community. One is based on discrete textualinstructions , and the other on continuous signal embeddings . However,we find that these two methods are complementary. Specifically, the former allows the LLM toefficiently convey task execution commands to the backend modules through simple text, but itstruggles to provide modality-specific signals; the latter can conveniently carry the features neededfor tasks, but fails to accurately convey execution intention (especially for managing many modules).In this work, we propose a hybrid method by integrating them together. 3Architecture of VITRONVITRON takes most common encoder-LLM-decoder architecture paradigm, as in existing popularMLLMs . The overall framework is shown in , where three key blocks areincluded: 1) frontend vision&language encoders, 2) central LLM for semantics understanding andtext generation, and 3) backend decoder modules for user responding and vision manipulation. 3.1Frontend Vision-Language EncodingFor both images and videos, we employ the CLIP ViT-L/14@336px as the encoder, respectively.The video encoder independently processes each frame, further employing average pooling acrossthe temporal dimension to yield overall temporal representation features. Then, we employ a regionalpixel-aware visual extractor as the sketch encoder for user interaction, e.g., clicking, drawing boxesor polygons, and making scribbles. We mainly follow , and use the object-based representationsof mask regions that come from users inputs, which not only encode the pixel-level visual featuresbut also gather the spatial position information of each region. The region features are pooled withalso the binary mask of spatial geometry of the object region encoded, and the resulting embeddingsare used. Then, the multimodal feature representations are passed to LLM via linear projection. 3.2Core LLMIn VITRON, an LLM serves as the pivotal agent. Following the most common practice ,we utilize Vicuna (7B, version 1.5). The LLM processes inputs from both language and visualmodalities to perform semantic understanding and reasoning, and then make decisions. For visualcomprehension tasks, LLM directly outputs textual responses for users. On the other side, LLMalso needs to transmit signals and instructions to backend modules, directing them to invocate morecomplex tasks that go beyond text generation, such as visual segmentation, generation, and editing. Asemphasized earlier, the ability of LLMs to effectively and precisely convey messages is crucial to theperformance of complex multimodal tasks. To this end, we propose fully integrating the advantagesof the two common message-passing methods: discrete textual instructions and continuous signalembeddings. The former aids in accurately invoking different backbone modules (thanks to theLLMs proficiency in task dispatching), while the latter supplements with richer modality-preservedvisual features that cannot be directly described through discrete text. As depicted in , theLLM outputs 1) text responses for users, 2) text instructions for module invocation, and 3) featureembeddings of special tokens. The feature embeddings are split into the task-specific features and thetask-invariant fine-grained visual-language features. Both the text instructions and feature embeddingsare passed to backbone modules. 3.3Backend Visual SpecialistsTo enable our MLLM with various visual task abilities, we integrate an array of singleton visionspecialists into LLM. For image generation and editing, we integrate the diffusion-based modelGLIGEN . For image and video segmentation, we opt for SEEM . For video generation,ZeroScope and I2VGen-XL are utilized for text-to-video and image-to-video tasks, respec-tively. Lastly, for video editing functionality, we incorporate StableVideo . The text instructionsfrom LLM first determine which task module to invoke; simultaneously, feature embeddings are fed into the corresponding modules feature encoder to assist with task execution. Specifically, we designa structured invocation template, including 1) Module name, 2) Invocation command, and 3) Region(optional) specifying a fine-grained vision feature needed for certain tasks. The feature embeddingsinclude both task-specific features and task-invariant fine-grained features. The purpose of this designis to achieve feature decoupling, during which we aim to have the task-invariant fine-grained featuresshared as widely as possible among all tasks to facilitate synergy between different tasks.4Pixel-aware Synergistic Vision-Language Understanding TuningWith the VITRON framework, we now train the model with three stages of targets. First, we try toendow it with basic multimodal capabilities, i.e., comprehension and generation. Then, we engage infine-grained vision grounding instruction tuning to further enhance the models pixel-level perceptionabilities. Finally, we carry out cross-task synergy learning, maximizing the shared fine-grainedfeatures among all tasks. 4.1Basic Multimodal Comprehension and Generation Skill TrainingIn the first stage of training, the primary goal is to equip the MLLM with basic multimodal under-standing and generation abilities, including the frontend alignment of encoder-LLM, as well as thebackend alignment of LLM-decoder. Appendix B.1 details all the following three types of training.Overall Vision-Language Alignment Learning.This is to ensure the input vision and language aremapped to a unified feature space. Following prior common practice, we utilize datasets comprisingimage-caption pairs (CC3M ), video-caption pairs (Webvid ), and region-caption pairs(RefCOCO ) drawn from existing established corpora and benchmarks. When provided with animage, video, or specific visual region, we engage the frozen LLM to generate a text description orcaption that aligns with the reference caption.Text Invocation Instruction Tuning.This step of training aims to equip the system with the precisecapability to execute commands, allowing the LLM to generate appropriate and correct invocationtext instructions. To accomplish this, we collect a total of 55,000+ instruction tuning samples.Embedding-oriented Decoder Alignment Tuning.Besides using explicit textual instruction toinvocate downstream modules, the signal feature embedding/representation (from LLM) should alsobe fed to the modules. Following , we align the feature embedding with all the visual modulesinput encoders via the decoding-side projection layers, i.e., by minimizing their distances. 4.2Fine-grained Spatiotemporal Vision Grounding Instruction TuningA visual generalist should require a strong capability of pixel-aware vision understanding of bothimages and videos. Thus, we propose a fine-grained spatiotemporal vision grounding instructiontuning for VITRON. The core idea is to enable the LLM to ground the fine-grained spatiality ofimages and the detailed temporality of videos. Appendix B.2 extends more detailed descriptions ofthe following three learning aspects.Image Spatial Grounding.Considering that the LLM alone can only output text, we design itto respond with the corresponding bounding box areas. We focus on two types of tasks: groundedimage captioning and referring image segmentation .Video Spatial-Temporal Grounding.For videos, the LLM must identify spatial regions andground them within the temporal context of the video, essentially achieving video tracking. Similarly,we explore tasks such as grounded video captioning and referring video tracking .",
  ": Illustration of the synergy module": "Grounding-aware Vision QA.The groundingtasks mentioned above only touch upon the low-level aspects of vision perception. However, inmany scenarios, its essential for the LLM to pos-sess high-level, in-depth vision reasoning capabili-ties, building upon the foundational low-level pixelgrounding. Thus, we further introduce grounding-aware vision QA, including Image-QA and Video-QA , enabling LLM to undertakesemantic-level QA tasks based on the groundedresults. 4.3Cross-task Synergy LearningAs a generalist, directly invoking different specialists leads to a critical issue: how to ensure thatthe different modules (tasks) work together synergistically? Otherwise, without such collaboration, integrating them into a single compound system would be meaningless. To achieve this, here wepropose decomposing the signal feature embeddings into task-specific features and task-invariantfine-grained features. Intuitively, since all the visual tasks we focus on are fine-grained, the moreextensively the task-invariant fine-grained features are shared among different tasks, the more thesetasks can benefit from each other, thus gaining greater synergy. Thereafter, we introduce a cross-tasksynergy learning module, as shown in . We employ adversarial training to decoupletask-specific from task-invariant features. We first let different backbone visual specialists make taskpredictions based on these two features (via concatenation). Meanwhile, we encourage a third-partydiscriminator (acts as a classifier) to determine which is the current task based solely on the sharedfeature representation. Ideally, once the discriminator can no longer accurately identify the task, theshared feature can be considered the most purified and broadly applicable across tasks.",
  "Experiments": "Now we try to quantify the performance of VITRON on the four vision task groups, covering 12 tasksacross 22 datasets. All the training of VITRON is conducted on 10 A100 (80G) GPUs. To ensure afair comparison, all subsequent experiments adopt settings same/similar to those of baseline systems,with evaluations following established practices. See more implementation details in Appendix C.Due to space limits, more experimental results are presented in Appendix D.",
  ": Results (cIoU) of referring image segmentation. w/o syng.: without synergy learning": "Image Segmentation. presents the results of referring image segmentation on three datasets:RefCOCO , RefCOCO+ and RefCOCOg . We compare with several significantmodels, including state-of-the-art non-MLLM approaches and the MLLM baseline, NExT-Chat. It isevident that our VITRON, while slightly underperforming compared to NExT-Chat on the RefCOCOVal&TestA datasets, achieves superior performance on the remaining sets.",
  ": Results of video object segmen-tation on DAVIS 17 Test-Dev set": "Video Segmentation.For video segmentation, we explore two tasks: video spatial grounding (withbounding box) and video object segmentation (aka., video tracking; with mask). showcasesthe comparisons between VITRON and current state-of-the-art (SoTA) video MLLMs in video spatialgrounding. It is clear that VITRON significantly outperforms PG-Video-LLaVA. presents acomparison of VITRON with some SoTA systems in video tracking, where our system continues todemonstrate superior performance.",
  "shown in Tables 5 illustrate that VITRON surpasses the best baseline across various datasets andmetrics, proving its strong and accurate fine-grained semantic understanding of images": "The above two tasks focus solely on the models ability to recognize at the region level. Taking a stepfurther, we delve deeper into assessing the capability for image semantics understanding, particularlythrough image-based Visual Question Answering (VQA) tasks. These tasks effectively reflect themodels proficiency in comprehending the deeper semantic content of images. displaysthe results across a series of six datasets for image-based VQA. We primarily compare two groupsof models: those with and without pixel-wise vision grounding capabilities. The findings indicatethat models equipped with fine-grained grounding abilities indeed show stronger task performance,suggesting that fine-grained grounding contributes to a more profound understanding of semantics.Notably, our VITRON achieves the highest performance among the models evaluated.",
  ":Results (accuracy and confidenceScore) on video QA": "Region-level Video Understanding.Similarly, for videos, we evaluate the Region-level VideoUnderstanding capability. Building on observations from images, we now directly engage in video QAtasks. presents the results on video QA across four representative datasets. Interestingly, whilePG-Video-LLaVA has video grounding capabilities, it does not show better results than Video-LLaVA,which lacks grounding. However, our VITRON achieves superior performance. This indirectly provesthat our system possesses more accurate video grounding capabilities (as previously demonstrated in), aiding in better video semantics understanding.",
  ": Image-to-Video gen-eration on UCF101": "Next, we assess our systems capabilities in vision generation, focusing on three of the most representa-tive types of generation tasks: text-to-image generation, text-to-video generation, and image-to-videogeneration. These tasks broadly cover the spectrum of image generation requirements. Tables 8, 9,and 10 showcase how our VITRON performs in comparison to other SoTA systems, including bothMLLM and non-MLLM synthesizers. The results clearly demonstrate that VITRON outperforms onall three tasks. For instance, in both text-to-image and text-to-video generation tasks, VITRON showsmore advanced performance compared to NExT-GPT. Similarly, in the image-to-video generationtask, VITRON still outshines the SoTA baseline, VideoCrafter1, showcasing superior results. 5.4Results on Vision EditingImage Editing.We use the MagicBrush dataset , which challenges models with an editingquery that demands a series of complex edits to an image. These edits include removing, changing,inpainting, and adding elements. Since there are currently no MLLM systems that support imageediting, our comparison is limited to non-LLM expert systems. In , we present the perfor-mance of different models across various metrics. VITRON demonstrates stronger performance on allmetrics, indicating its stable image editing capabilities.",
  ": Human evaluation on videoediting": "Video Editing.For video editing, the community currently lacks a standardized benchmark andevaluation method akin to those for image editing. Therefore, we opted for a manual evaluationapproach. We asked different video editing systems to edit the same video based on the same query,after which five individuals were asked to score the edited videos. The evaluation focused on 1) thesuccess of target content modifications and 2) the faithfulness/fidelity of non-target content. presents the manual evaluation results for video editing. It is clear that VITRON outperforms the twobaseline systems in both respects, showcasing superior video editing capabilities. Following this, wevisualized the process of video editing by VITRON.",
  "Above we demonstrate the overall efficacy of VITRON via extensive quantitative comparison. Nowwe take one step further, exploring how and why the system advances via in-depth analyses": "Discrete Textual Instruction or Continuous Signal Embedding, Which Better?Firstly, weexplore different message-passing mechanisms to determine whether discrete textual instructionis more beneficial, or whether continuous signal embedding is better for building a multi-modalgeneralist. Also, we validate the pros and cons of the proposed hybrid method of message passing. Weconduct tests on 6 tasks, where we compare the task performance of VITRON using the hybrid method(default setting), without signal embedding and without text instruction, as well as the successfulexecution rate of the backend task module. presents the results. As can be observed, overall,the performance under scenarios utilizing both methods is consistently better, which confirms theeffectiveness of our hybrid mode. Meanwhile, we find that the method of text instruction is moreconducive to the successful execution of backend modules, but soft feature embedding seems to bemore useful in terms of specific task performances.",
  ": The influences of using different strategies for message passing": "How Much Does Each Fine-grained Visual Grounding Learning Contribute?Next, wevalidate the specific contribution of the various fine-grained visual grounding learning strategiesproposed in 4.2. (the top 4 relate to image tasks, and the bottom 4 to video tasks) shows theimpact on performance when a particular learning strategy is removed. Generally, all these 3 types offine-grained visual grounding learning strategies are vital for different downstream tasks. For instance,grounding and referring segmentation tasks directly influence fine-grained visual recognition tasks,whereas tuning for grounding-aware visual QA considerably boosts cognition level QA tasks. Thisverifies the efficacy of our proposed fine-grained visual grounding tuning strategies.",
  ": The synergy correlation between each pair of visual tasks. The deeper the color of the cell,the more synergistic they are in between": "module in to , we can observe that the synergy learning mechanism indeed positivelyinfluences overall performance. In we further study whether there is synergy between differenttasks and their collaborative relations. For ease of study, we considered a one-to-one mappingrelationship, studying the cooperation between pairs of tasks one at a time. It is evident that thecooperative effects vary between different tasks. Tasks or backbone modules that rely more heavilyon fine-grained visual features gained more significant improvements. This also demonstrates thatour synergy learning module can successfully facilitate cross-task synergy.",
  "Conclusion": "In this work, we present VITRON, a grand unified pixel-level vision LLM for seamlessly understanding(perceiving and reasoning), generating, segmenting (grounding and tracking), and editing (inpainting)both images and videos. We further introduce a novel hybrid method of message passing thatcombines discrete textual instructions with continuous signal embeddings to ensure precise functioninvocation. Furthermore, VITRON employs pixel-level spatiotemporal vision-language alignment toenhance its fine-grained visual capabilities. A cross-task synergy module is also developed to optimizethe use of task-invariant fine-grained visual features, boosting synergy across various visual tasks.On 12 visual tasks across 22 datasets, VITRON exhibits extensive capabilities in visual segmentation,fine-grained vision understanding, generation, and editing. Overall, this research showcases the greatpotential to build a vision-language generalist that can advance toward a more unified AI.",
  "This research is supported by Skywork AI, NExT++ Research Center, and CCF-Kuaishou LargeModel Explorer Fund, Project of Future High-tech Video Intelligent Technology Innovation Center": "Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc,Arthur Mensch, Katherine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi,Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob L. Menick, Sebastian Borgeaud,Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals,Andrew Zisserman, and Karn Simonyan. Flamingo: a visual language model for few-shot learning. InProceedings of the NeurIPS, 2022. Jie An, Songyang Zhang, Harry Yang, Sonal Gupta, Jia-Bin Huang, Jiebo Luo, and Xi Yin. Latent-shift:Latent diffusion with temporal shift for efficient text-to-video generation. CoRR, abs/2304.08477, 2023.",
  "Cerspense. Zeroscope: Diffusion-based text-to-video synthesis. 2023. URL": "Wenhao Chai, Xun Guo, Gaoang Wang, and Yan Lu. Stablevideo: Text-driven consistency-aware diffusionvideo editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages2304023050, 2023. Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing,Yaofang Liu, Qifeng Chen, Xintao Wang, et al. Videocrafter1: Open diffusion models for high-qualityvideo generation. arXiv preprint arXiv:2310.19512, 2023. Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman Krish-namoorthi, Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny. Minigpt-v2: large language modelas a unified interface for vision-language multi-task learning. arXiv preprint arXiv:2310.09478, 2023.",
  "Ho Kei Cheng and Alexander G Schwing. Xmem: Long-term video object segmentation with anatkinson-shiffrin memory model. In Proceedings of the ECCV, pages 640658. Springer, 2022": "Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, SiyuanZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-sourcechatbot impressing gpt-4 with 902023. Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, XuezhiWang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, MiracSuzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Y.Zhao, Yanping Huang, Andrew M. Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin,Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-finetuned language models.CoRR, abs/2210.11416, 2022. Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, BoyangLi, Pascale Fung, and Steven C. H. Hoi. Instructblip: Towards general-purpose vision-language modelswith instruction tuning. CoRR, abs/2305.06500, 2023.",
  "Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scalehierarchical image database. In Proceedings of the CVPR, pages 248255. Ieee, 2009": "Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian Sun,Hongyu Zhou, Haoran Wei, et al. Dreamllm: Synergistic multimodal comprehension and creation. arXivpreprint arXiv:2309.11499, 2023. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, ThomasUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit,and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. InProceedings of the ICLR, 2021. Heng Fan, Liting Lin, Fan Yang, Peng Chu, Ge Deng, Sijia Yu, Hexin Bai, Yong Xu, Chunyuan Liao, andHaibin Ling. Lasot: A high-quality benchmark for large-scale single object tracking. In Proceedings ofthe CVPR, pages 53745383, 2019. Hao Fei, Qian Liu, Meishan Zhang, Min Zhang, and Tat-Seng Chua. Scene graph as pivoting: Inference-time image-free unsupervised multimodal machine translation with visual scene hallucination.InProceedings of the ACL, pages 59805994, 2023.",
  "Hao Fei, Shengqiong Wu, Wei Ji, Hanwang Zhang, Meishan Zhang, Mong-Li Lee, and Wynne Hsu.Video-of-thought: Step-by-step video reasoning from perception to cognition. In Proceedings of theICML, 2024": "Hao Fei, Shengqiong Wu, Meishan Zhang, Min Zhang, Tat-Seng Chua, and Shuicheng Yan. Enhancingvideo-language representations with structural spatio-temporal alignment. IEEE Transactions on PatternAnalysis and Machine Intelligence, 2024. Hao Fei, Yuan Yao, Zhuosheng Zhang, Fuxiao Liu, Ao Zhang, and Tat-Seng Chua. From multimodalllm to human-level ai: Modality, instruction, reasoning, efficiency and beyond. In Proceedings of theCOLING: Tutorial Summaries, pages 18, 2024. Weixi Feng, Xuehai He, Tsu-Jui Fu, Varun Jampani, Arjun R. Akula, Pradyumna Narayana, Sugato Basu,Xin Eric Wang, and William Yang Wang. Training-free structured diffusion guidance for compositionaltext-to-image synthesis. CoRR, abs/2212.05032, 2022. Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Franois Laviolette,Mario March, and Victor Lempitsky. Domain-adversarial training of neural networks. Journal of machinelearning research, 17(59):135, 2016. Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, ConghuiHe, Xiangyu Yue, et al. Llama-adapter v2: Parameter-efficient visual instruction model. arXiv preprintarXiv:2304.15010, 2023.",
  "Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning andcompositional question answering. In Proceedings of the CVPR, pages 67006709, 2019": "Jiayi Ji, Yunpeng Luo, Xiaoshuai Sun, Fuhai Chen, Gen Luo, Yongjian Wu, Yue Gao, and RongrongJi. Improving image captioning by leveraging intra-and inter-layer global representation in transformernetwork. In Proceedings of the AAAI, pages 16551663, 2021. Jiayi Ji, Yiwei Ma, Xiaoshuai Sun, Yiyi Zhou, Yongjian Wu, and Rongrong Ji. Knowing what to learn:a metric-oriented focal mechanism for image captioning. IEEE Transactions on Image Processing, 31:43214335, 2022.",
  "Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. Referitgame: Referring to objectsin photographs of natural scenes. In Proceedings of the EMNLP, pages 787798, 2014": "Douwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj Goswami, Amanpreet Singh, Pratik Ringshia,and Davide Testuggine. The hateful memes challenge: Detecting hate speech in multimodal memes. 33:26112624, 2020. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, TeteXiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprintarXiv:2304.02643, 2023.",
  "Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoningsegmentation via large language model. arXiv preprint arXiv:2308.00692, 2023": "Juncheng Li, Kaihang Pan, Zhiqi Ge, Minghe Gao, Hanwang Zhang, Wei Ji, Wenqiao Zhang, Tat-SengChua, Siliang Tang, and Yueting Zhuang. Fine-tuning multimodal llms to follow zero-shot demonstrativeinstructions. In Proceedings of the ICLR, 2023. Juncheng Li, Siliang Tang, Linchao Zhu, Wenqiao Zhang, Yi Yang, Tat-Seng Chua, and Fei Wu. Vari-ational cross-graph reasoning and adaptive structured semantics learning for compositional temporalgrounding. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023. Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi. BLIP-2: bootstrapping language-imagepre-training with frozen image encoders and large language models. In Proceedings of the ICML, pages1973019742, 2023.",
  "Mingxing Li, Li Hu, Zhiwei Xiong, Bang Zhang, Pan Pan, and Dong Liu. Recurrent dynamic embeddingfor video object segmentation. In Proceedings of the CVPR, pages 13321341, 2022": "Xiangtai Li, Wenwei Zhang, Jiangmiao Pang, Kai Chen, Guangliang Cheng, Yunhai Tong, andChen Change Loy. Video k-net: A simple, strong, and unified baseline for video segmentation. InProceedings of the CVPR, 2022. Xiangtai Li, Haobo Yuan, Wenwei Zhang, Guangliang Cheng, Jiangmiao Pang, and Chen Change Loy.Tube-link: A flexible cross tube baseline for universal video segmentation. In Proceedings of the ICCV,2023. Xiangtai Li, Henghui Ding, Wenwei Zhang, Haobo Yuan, Guangliang Cheng, Pang Jiangmiao, Kai Chen,Ziwei Liu, and Chen Change Loy. Transformer-based visual segmentation: A survey. IEEE Transactionson Pattern Analysis and Machine Intelligence, 2024.",
  "Yicong Li, Xiang Wang, Junbin Xiao, Wei Ji, and Tat-Seng Chua. Invariant grounding for video questionanswering. In Proceedings of the CVPR, pages 29182927, 2022": "Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, andYong Jae Lee. Gligen: Open-set grounded text-to-image generation. In Proceedings of the CVPR, pages2251122521, 2023. Yuncheng Li, Yale Song, Liangliang Cao, Joel Tetreault, Larry Goldberg, Alejandro Jaimes, and JieboLuo. Tgif: A new dataset and benchmark on animated gif description. In Proceedings of the CVPR, pages46414650, 2016.",
  "Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.Visual instruction tuning.CoRR,abs/2304.08485, 2023": "Shilong Liu, Hao Cheng, Haotian Liu, Hao Zhang, Feng Li, Tianhe Ren, Xueyan Zou, Jianwei Yang,Hang Su, Jun Zhu, et al. Llava-plus: Learning to use tools for creating multimodal agents. arXiv preprintarXiv:2311.05437, 2023. Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang,Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set objectdetection. arXiv preprint arXiv:2303.05499, 2023. Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao, Wei Zhang, Zhou Yu, Xiaodan Liang, andSong-Chun Zhu. Iconqa: A new benchmark for abstract diagram understanding and visual languagereasoning. arXiv preprint arXiv:2110.13214, 2021.",
  "Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion forediting real images using guided diffusion models. In Proceedings of the CVPR, pages 60386047, 2023": "Meredith Ringel Morris, Jascha Sohl-dickstein, Noah Fiedel, Tris Warkentin, Allan Dafoe, AleksandraFaust, Clement Farabet, and Shane Legg. Levels of agi: Operationalizing progress on the path to agi.arXiv preprint arXiv:2311.02462, 2023. Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. arXivpreprint arXiv:2302.08453, 2023. Shehan Munasinghe, Rusiru Thushara, Muhammad Maaz, Hanoona Abdul Rasheed, Salman Khan,Mubarak Shah, and Fahad Khan. Pg-video-llava: Pixel grounding large video-language models. arXivpreprint arXiv:2311.13435, 2023. Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, BobMcGrew, Ilya Sutskever, and Mark Chen. GLIDE: towards photorealistic image generation and editingwith text-guided diffusion models. In Proceedings of the ICML, pages 1678416804, 2022.",
  "Leigang Qu, Shengqiong Wu, Hao Fei, Liqiang Nie, and Tat-Seng Chua. Layoutllm-t2i: Eliciting layoutguidance from LLM for text-to-image generation. In Proceedings of the ACM MM, pages 643654, 2023": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, GirishSastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learningtransferable visual models from natural language supervision. In Proceedings of the ICML, pages87488763, 2021. Hanoona Rasheed, Muhammad Maaz, Sahal Shaji, Abdelrahman Shaker, Salman Khan, HishamCholakkal, Rao M Anwer, Erix Xing, Ming-Hsuan Yang, and Fahad S Khan. Glamm: Pixel groundinglarge multimodal model. arXiv preprint arXiv:2311.03356, 2023.",
  "Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjrn Ommer. High-resolutionimage synthesis with latent diffusion models. In Proceedings of the CVPR, pages 1067410685, 2022": "Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, AarushKatta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021. Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi.A-okvqa: A benchmark for visual question answering using world knowledge. In Proceedings of theECCV, pages 146162, 2022. Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned,hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the ACL, pages25562565, 2018.",
  "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt:Solving AI tasks with chatgpt and its friends in huggingface. CoRR, abs/2303.17580, 2023": "Shelly Sheynin, Adam Polyak, Uriel Singer, Yuval Kirstain, Amit Zohar, Oron Ashual, Devi Parikh, andYaniv Taigman. Emu edit: Precise image editing via recognition and generation tasks. arXiv preprintarXiv:2311.10089, 2023. Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang,Oron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taigman. Make-a-video: Text-to-videogeneration without text-video data. CoRR, abs/2209.14792, 2022.",
  "Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, and Deng Cai. Pandagpt: One model toinstruction-follow them all. CoRR, abs/2305.16355, 2023": "Peize Sun, Jinkun Cao, Yi Jiang, Zehuan Yuan, Song Bai, Kris Kitani, and Ping Luo. Dancetrack:Multi-object tracking in uniform appearance and diverse motion. In Proceedings of the CVPR, pages2099321002, 2022. Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, JingjingLiu, Tiejun Huang, and Xinlong Wang.Generative pretraining in multimodality.arXiv preprintarXiv:2307.05222, 2023.",
  "Zineng Tang, Ziyi Yang, Chenguang Zhu, Michael Zeng, and Mohit Bansal. Any-to-any generation viacomposable diffusion. CoRR, abs/2305.11846, 2023": "Zongheng Tang, Yue Liao, Si Liu, Guanbin Li, Xiaojie Jin, Hongxu Jiang, Qian Yu, and Dong Xu.Human-centric spatio-temporal video grounding with visual transformers. IEEE Transactions on Circuitsand Systems for Video Technology, 32(12):82388249, 2021. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimotheLacroix, Baptiste Rozire, Naman Goyal, Eric Hambro, Faisal Azhar, Aurlien Rodriguez, Armand Joulin,Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. CoRR,abs/2302.13971, 2023.",
  "Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-play diffusion features fortext-driven image-to-image translation. In Proceedings of the CVPR, pages 19211930, 2023": "Junke Wang, Dongdong Chen, Zuxuan Wu, Chong Luo, Chuanxin Tang, Xiyang Dai, Yucheng Zhao,Yujia Xie, Lu Yuan, and Yu-Gang Jiang. Look before you match: Instance understanding matters in videoobject segmentation. In Proceedings of the CVPR, pages 22682278, 2023. Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, JingrenZhou, and Hongxia Yang. Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework. In Proceedings of the ICML, pages 2331823340. PMLR, 2022.",
  "Dongming Wu, Wencheng Han, Tiancai Wang, Xingping Dong, Xiangyu Zhang, and Jianbing Shen.Referring multi-object tracking. In Proceedings of the CVPR, pages 1463314642, 2023": "Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Weixian Lei, Yuchao Gu, Wynne Hsu, Ying Shan, XiaohuQie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-videogeneration. CoRR, abs/2212.11565, 2022. Jialian Wu, Jianfeng Wang, Zhengyuan Yang, Zhe Gan, Zicheng Liu, Junsong Yuan, and Lijuan Wang.Grit: A generative region-to-text transformer for object understanding. arXiv preprint arXiv:2212.00280,2022. Jianzong Wu, Xiangtai Li, Chenyang Si, Shangchen Zhou, Jingkang Yang, Jiangning Zhang, Yining Li,Kai Chen, Yunhai Tong, Ziwei Liu, et al. Towards language-driven video inpainting via multimodal largelanguage models. arXiv preprint arXiv:2401.10226, 2024. Mingrui Wu, Xinyue Cai, Jiayi Ji, Jiale Li, Oucheng Huang, Gen Luo, Hao Fei, Xiaoshuai Sun, andRongrong Ji. Controlmllm: Training-free visual prompt learning for multimodal large language models.arXiv preprint arXiv:2407.21534, 2024. Shengqiong Wu, Hao Fei, Hanwang Zhang, and Tat-Seng Chua. Imagine that! abstract-to-intricatetext-to-image synthesis with scene graph hallucination diffusion. In Proceedings of the NeurIPS, pages7924079259, 2023.",
  "Junbin Xiao, Angela Yao, Zhiyuan Liu, Yicong Li, Wei Ji, and Tat-Seng Chua. Video as conditionalgraph hierarchy for multi-granular question answering. In Proceedings of the AAAI, pages 28042812,2022": "Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Xintao Wang, Tien-Tsin Wong, and YingShan. Dynamicrafter: Animating open-domain images with video diffusion priors. arXiv preprintarXiv:2310.12190, 2023. Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang. Videoquestion answering via gradually refined attention over appearance and motion. In Proceedings of theACM MM, pages 16451653, 2017.",
  "Ning Xu, Linjie Yang, Yuchen Fan, Dingcheng Yue, Yuchen Liang, Jianchao Yang, and Thomas Huang.Youtube-vos: A large-scale video object segmentation benchmark. arXiv preprint arXiv:1809.03327,2018": "Zhao Yang, Jiaqi Wang, Yansong Tang, Kai Chen, Hengshuang Zhao, and Philip HS Torr.Lavt:Language-aware vision transformer for referring image segmentation. In Proceedings of the CVPR, pages1815518165, 2022. Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang.The dawn of lmms: Preliminary explorations with gpt-4v (ision). arXiv preprint arXiv:2309.17421, 9(1):1, 2023.",
  "Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: A manually annotated datasetfor instruction-guided image editing. 36, 2024": "Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi Shao, Wenwei Zhang, Kai Chen, and Ping Luo.Gpt4roi: Instruction tuning large language model on region-of-interest. arXiv preprint arXiv:2307.03601,2023. Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao, Hangjie Yuan, Zhiwu Qin, Xiang Wang, DeliZhao, and Jingren Zhou. I2vgen-xl: High-quality image-to-video synthesis via cascaded diffusion models.arXiv preprint arXiv:2311.04145, 2023. Tao Zhang, Xiangtai Li, Hao Fei, Haobo Yuan, Shengqiong Wu, Shunping Ji, Chen Change Loy, andShuicheng Yan. Omg-llava: Bridging image-level, object-level, pixel-level reasoning and understanding.arXiv preprint arXiv:2406.19389, 2024. Wenqiao Zhang, Haochen Shi, Siliang Tang, Jun Xiao, Qiang Yu, and Yueting Zhuang. Consensusgraph representation learning for better grounded image captioning. In Proceedings of the AAAI, pages33943402, 2021. Zhu Zhang, Zhou Zhao, Yang Zhao, Qi Wang, Huasheng Liu, and Lianli Gao. Where does it exist: Spatio-temporal video grounding for multi-form sentences. In Proceedings of the CVPR, pages 1066810677,2020.",
  "ADetails of Backbone Visual Modules/Specialists": "To address the inability of text-based LLMs in handling various vision tasks, we consider integratingoff-the-shelf external modules. Once the LLM generates invocation details through understandingthe input and recognizing the users intent, the corresponding modules are activated to producenon-textual outputs. Technically, we employ a variety of current SoTA expert models for visionprocessing. For image generation and editing, we integrate the diffusion-based model GLIGEN .For image and video segmentation, we opt for SEEM . For video generation, ZeroScope and I2VGen-XL are utilized for text-to-video and image-to-video tasks, respectively. Lastly,for video editing functionality, we incorporate StableVideo . In , we summarize thefunctionality of each backend module, along with a specification of the inputs and outputs.",
  "This section extends more details of the 4 in the main article": "B.1Baisc MLLM Skill TrainingOverall Vision-Language Alignment Learning.In line with the methodologies in current MLLMs,our approach involves This step aims at mapping the input vision language features to a unifiedfeature space. This space creates representations that the central LLM can interpret, thereby enablingit to process incoming vision signals effectively. We utilize datasets of image-caption pairs (CC3M), video-caption pairs (Webvid ), and region-caption pairs (RefCOCO ) from existingcorpora. When provided with an image, video, or specific visual region, we engage the frozen LLMto generate a text description or caption that aligns with the reference caption. Invocation-oriented Instruction Tuning.The above phase of training endows both the LLMand the frontend encoders with the ability to understand vision. This step, instruction tuning forinvocation, aims to equip the system with the precise capability to execute commands, allowing theLLM to generate appropriate and correct invocation text. This text is then used to trigger variousbackend task execution modules. Different terminal vision tasks might require distinct invocationcommands. To unify this, we try to standardize the LLMs response output into a structured textformat, which includes: 1) User response output, which directly replies to the users input. 2) Modulename, indicating which function or task is to be executed. 3) Invocation command, a meta-instructionfor triggering the task module. 4) Region (optional), specifying a fine-grained vision feature neededfor certain tasks, such as in video tracking or vision editing, where backend modules require thisinformation. For the region, based on LLMs pixel-level understanding, a bounding box described bythe coordinates (Xl, Yt, Xr, Yb) will be output. Following, we exemplify a video tracking examplefor the structured LLM response for module invocation.",
  "in video segmentation means theintermediate referred video keyframe interpreted within the system": "To teach the LLM to produce the correct invocation responses, we need to create data for instructiontuning. A key is ensuring that the data covers all possible scenarios. We must take into accountdifferent ways users might interact with the system for each functionality mentioned in (except for text generation). For example, when requesting video creation, a user might describe whatthey want purely in text, or provide a reference image as the basis for the desired video. Similarly, forediting images or videos, users could express their editing requests either through text, or by usingsketches, scribbles and other interactions. Thus, the LLM needs to be versatile in accepting varioustypes of user inputs and generating an accurate invocation response that matches the requirementsof the backend modules. Technically, we make use of the existing annotated datasets for variousvision tasks included in this work. For each task under specific different user input scenarios, with thecorresponding data, we design various template dialogue-format examples. Based on these exampleswe then prompt the GPT-4 to generate more samples under various topics and enriched scenarios.Finally, we collect a total of 55,000+ invocation-oriented instruction tuning samples. In weprovide a summary of these datasets. Embedding-oriented Decoder Alignment Tuning.Besides using the explicit textual instruction toinvocate downstream modules, also the signal feature embedding/representation (from LLM) shouldalso be fed to the modules. Denote the task-specific features as vp and task-invariant fine-grainedfeatures as vs. We will concatenate them as one unified feature embedding v = [vp; vs] and thensend v to the downstream module. Following , we align the feature embedding with all the visual modules input encoders via thedecoding-side projection layers. We do this feature alignment learning by minimizing the distancebetween the projected feature embedding and the modules input encoder. For example for diffusion-based image or video generation, we may directly use the textual condition encoder, while keepingall the other modules fixed. Technically, to endow the model to produce other modalities beyond text,we add the signal tokens to the vocabulary of the LLM. In the alignment training phase, we mainlytake the captions from CC3M, WebVid, and AudioCaps as inputs and concatenate them with thespecial signal tokens as outputs. The loss function comprises three key components: 1) the negativelog-likelihood of producing signal tokens, and 2) the caption alignment loss: l2-distance between the",
  "B.2Fine-grained Spatiotemporal Vision Grounding Instruction Tuning": "We propose a fine-grained spatiotemporal vision grounding instruction tuning for VITRON. The coreidea is to enable the LLM to ground the fine-grained spatiality of images and the detailed temporalityof videos. Technically, we leverage LoRA to enable a small subset of parameters within theLLM to be updated during the tuning. Image Spatial Grounding.Considering that the LLM alone can only output text, we design it torespond with the corresponding bounding box areas. We focus on two types of tasks: grounded imagecaptioning and referring image segmentation. For grounded image captioning, we input an image andidentify a specific object within it, prompting the LLM to describe the identified object. Conversely,for referring image segmentation (where we consider outputting a bounding box, akin to phrasegrounding), the task involves inputting a complete image along with a related phrase or sentencedescription, and the LLM is expected to output the objects spatial bounding box, represented bycoordinate numbers (Xl, Yt, Xr, Yb). The X and Y coordinates are normalized real values withinthe range , where <Xl> and <Yt> indicate the top-left corner, and <Xr> and <Yb> representthe bottom-right corner of the bounding box. We consider datasets such as Flickr30K Entities ,RefCOCO , RefCOCO+ and RefCOCOg . Utilizing GPT-4, we preprocess and expandthe original data into a multi-turn QA format, thereby generating the most suitable instruction-tuningdata.",
  "Grounded Image Captioning:Input:": "Please generate a detailed caption for the given image,and clearly link each part of the caption to specific ob-jects or areas in the image which you can denote with abounding box with object: (Xl, Yt, Xr, Yb) format.Output:A girl wearing a pink dress is sitting on a long bench inthe park reading a book.girl: (Xl, Yt, Xr, Yb),pink dress: (Xl, Yt, Xr, Yb), ...",
  "Referring Image Segmentation:Input:": "Please identify the target object fromthe given images based on the follow-ing text query: A man in a black shirtholding a water bottle.Please output the bounding box (Xl,Yt, Xr, Yb) of the target object.Output:(Xl, Yt, Xr, Yb) Video Spatial-Temporal Grounding.For videos, the LLM must not only identify spatial regionsbut also ground these within the temporal context of the video, essentially achieving video tracking.Similarly, we explore tasks such as grounded video captioning and referring video tracking. Whilethe approach is akin to that used for images, it requires not just the output of object bounding boxcoordinates in each frame but also the specification of the duration of the frames (Fs, Fe) in whichthese coordinates are to be outputted. Here <Fs> denotes the starting frame number, and <Fe> meansthe ending frame number. We mainly construct the required instruction tuning data from several keyvideo tracking datasets: LaSOT , GOT10K , DanceTrack , and MOT16 .",
  "Grounded Video Captioning:Input:": "Please generate a caption for the given video, andlink each part of the caption to specific objects inthe video with its temporal presence duration. Youshould denote it with a bounding box with the start-ing and ending frame number in format as object:(Xl, Yt, Xr, Yb | Fs, Fe).Output:A dog is running after a ball on the grass.dog: (Xl, Yt, Xr, Yb | Fs, Fe),ball: (Xl, Yt, Xr, Yb | Fs, Fe), ...",
  "Referring Video Tracking:Input:": "Please track the specified object throughoutthe video based on the following given de-scription, and mark its starting and endingposition in each frame: A man is skiingon a thick-snow-covered slope.Please output in the format of (Xl, Yt, Xr,Yb | Fs, Fe).Output:(Xl, Yt, Xr, Yb | Fs, Fe) Grounding-aware Vision QA.The grounding tasks mentioned above only touch upon the low-level aspects of vision perception. However, in many scenarios, its essential for the LLM to possesshigh-level, in-depth vision reasoning capabilities, building upon the foundational low-level pixelgrounding. Therefore, we further introduce grounding-aware vision QA, enabling the LLM toundertake semantic-level QA tasks based on the grounded results. Specifically, we consider bothimage-grounded QA and video-grounded QA. The data sources for these tasks remain the sameas mentioned above for the grounding tasks. And we again rely on GPT-42 to use its imaginativecapabilities to design reasonable questions and answers based on the given grounding content of theabove grounding/segmentation datasets, formatting these into an instruction-tuning format.",
  "Grounded Image QA:Input:": "Based on the given image, please se-lect the correct answer among all thecandidates: Q: Where is the child sit-ting? A: 1) arms, 2) ground, 3) chair,4) bed.Please first identify and ground the tar-get object (in coordinates) mentionedin the question, and then proceed toanswer the question.Output:The target object mentioned in thequestion is \"child,\" with the positiongiven by (Xl, Yt, Xr, Yb). From this,it can be determined that the child isbeing held by a woman, which meansthe child is sitting in the womans arms.Therefore, the answer is 1) arms.",
  "Grounded Video QA:Input:": "Based on the provided video, answer the following ques-tion by choosing the most appropriate answer from theoptions given. Q: If the rope in the video breaks, whatwould happen to the person in the video? A: 1) running,2) jumping up, 3) flying away, 4) fracture.First, analyze the spatial position and temporality of thetarget object mentioned in the question within the video,and based on this analysis, determine the answer to thequestion.Output:The objects involved in the question are the rope (Xl, Yt,Xr, Yb | Fs, Fe) and the person (Xl, Yt, Xr, Yb | Fs, Fe),with their relationship being that the person is climbingwith the assistance of the rope. Based on common sense,if the rope were to break, the person would fall to theground from a height. The most likely outcome is thatthe person would break a bone.",
  "yk M(v).(1)": "Although we separate the two feature spaces into the shared one vs and private one vp, there are stillchances that the learned shared and the private features are closely entangled, weakening the refiningof the shared task-invariant fine-grained feature. Therefore, we employ a third-party task discriminatorwith adversarial training to refine the features. The discriminator D is a classifier for predicting whatthe current task is, based merely on the task-invariant fine-grained feature representation vs. Ideally,once the discriminator cannot accurately identify the task ID ydk, the task-invariant fine-grainedfeature representation vs can be understood as the most purified one. Specifically, the discriminatoris a 4-layer 768-d Transformer (Trm) network, where we use a feedforward layer (FFN) with Softmaxfor the task prediction:",
  "Step-2.2: When MLLM has the ability for fine-grained spatial understanding, doing theVideo Spatial-Temporal Grounding training, on the grounded video captioning taskand referring video tracking task": "Step-2.3: When the MLLM has learned to have the competent ability of both image andvideo spatiotemporal understanding at the perception level, doing the Grounding-awareVision QA task at the cognition level. Step-3: As the final step, when the overall system has learned to have a competitive ability invarious visual tasks, dining the cross-task synergy learning, cf 4.3. This should be done bycombining both the adversarial training (Lsyn) with the end-task prediction (Lk). So thetotal loss of the step-3 is: Lsyn +",
  "CExtended Experimental Settings": "We quantify the performance of VITRON on a variety of standard benchmarks for downstream visiontasks and compare it against some of the currently strong-performing systems. Given the countlessvision tasks within the community, our experiments focus only on 1-2 of the most representative tasksfrom each task category for validation. To ensure a fair comparison, all subsequent experiments adoptsettings same or similar to those of baseline systems, with evaluations following established practices.Before experiments, we perform targeted pre-training on all of VITRONs backend modules (such asGLIGEN and SEEM) on their respective datasets. This ensures our system is optimized for the bestpossible performance during testing. Our approach centers on training the linear projection layers ofall encoders and efficiently fine-tuning the language model using LoRA. Our backbone LLM is Vicuna3, 7B, version 1.5. The CLIP-ViT encoders for both images and videosare with a patch size of 14, and convert all images and video frames into 336px resolutions. The taskdiscriminator in our synergy module is with a Transformer architecture, with 4 layers and each in768-d representation. To train our model, we employ the AdamW optimizer along with a learningrate scheduler. The pre-training of VITRON unfolds in three phases, all conducted on 1016 A100(80G) GPUs. Initially, we train the model using a global batch size of 128 and a maximum learningrate of 3e-4, a process that takes approximately 40 hours. In the second tuning phase, we adjust themodel with a maximum learning rate of 1e-5, utilizing a global batch size of 90. This stage of traininglasts about 35 hours. The third phase of training employs a global batch size of 128 and maintains themaximum learning rate of 1e-5, completing in roughly 10 hours.",
  "E.1Vision Segmentation": "further demonstrates an example of how our VITRON processes image segmentation tasksin an interactive manner with the user. When users sketch or doodle outlines on specific areas ofan image, VITRON is capable of accurately identifying the corresponding objects within the image.Following this, it precisely generates the bounding box and mask area for the identified objects.",
  "E.3Vision Generation": "illustrates the process of vision generation across different modalities, including text, image,and video. Initially, users start with a basic text command, and VITRON is capable of transforming asimple idea into a detailed video. However, if users are not satisfied with the video generated directlyfrom text, they can first generate an image from text, then fine-tune or edit this image, and finallycreate a satisfying video based on the adjusted image. Our VITRON, thanks to its robust interactivecapability via multi-turn dialogue, enables users to perform a series of consecutive operations,ultimately facilitating smooth content creation. This fully helps meet the demands of real-worldapplication scenarios.",
  "The answer NA means that the paper has no limitation while the answer No means thatthe paper has limitations, but those are not discussed in the paper": "The authors are encouraged to create a separate \"Limitations\" section in their paper. The paper should point out any strong assumptions and how robust the results are toviolations of these assumptions (e.g., independence assumptions, noiseless settings,model well-specification, asymptotic approximations only holding locally). The authorsshould reflect on how these assumptions might be violated in practice and what theimplications would be. The authors should reflect on the scope of the claims made, e.g., if the approach wasonly tested on a few datasets or with a few runs. In general, empirical results oftendepend on implicit assumptions, which should be articulated. The authors should reflect on the factors that influence the performance of the approach.For example, a facial recognition algorithm may perform poorly when image resolutionis low or images are taken in low lighting. Or a speech-to-text system might not beused reliably to provide closed captions for online lectures because it fails to handletechnical jargon.",
  "If applicable, the authors should discuss possible limitations of their approach toaddress problems of privacy and fairness": "While the authors might fear that complete honesty about limitations might be used byreviewers as grounds for rejection, a worse outcome might be that reviewers discoverlimitations that arent acknowledged in the paper. The authors should use their bestjudgment and recognize that individual actions in favor of transparency play an impor-tant role in developing norms that preserve the integrity of the community. Reviewerswill be specifically instructed to not penalize honesty concerning limitations.",
  ". Experimental Result Reproducibility": "Question: Does the paper fully disclose all the information needed to reproduce the main ex-perimental results of the paper to the extent that it affects the main claims and/or conclusionsof the paper (regardless of whether the code and data are provided or not)?Answer: [Yes]Justification: in the Appendix A and Appendix CGuidelines: The answer NA means that the paper does not include experiments. If the paper includes experiments, a No answer to this question will not be perceivedwell by the reviewers: Making the paper reproducible is important, regardless ofwhether the code and data are provided or not.",
  "If the contribution is a dataset and/or model, the authors should describe the steps takento make their results reproducible or verifiable": "Depending on the contribution, reproducibility can be accomplished in various ways.For example, if the contribution is a novel architecture, describing the architecture fullymight suffice, or if the contribution is a specific model and empirical evaluation, it maybe necessary to either make it possible for others to replicate the model with the samedataset, or provide access to the model. In general. releasing code and data is oftenone good way to accomplish this, but reproducibility can also be provided via detailedinstructions for how to replicate the results, access to a hosted model (e.g., in the caseof a large language model), releasing of a model checkpoint, or other means that areappropriate to the research performed. While NeurIPS does not require releasing code, the conference does require all submis-sions to provide some reasonable avenue for reproducibility, which may depend on thenature of the contribution. For example(a) If the contribution is primarily a new algorithm, the paper should make it clear howto reproduce that algorithm.",
  "(b) If the contribution is primarily a new model architecture, the paper should describethe architecture clearly and fully": "(c) If the contribution is a new model (e.g., a large language model), then there shouldeither be a way to access this model for reproducing the results or a way to reproducethe model (e.g., with an open-source dataset or instructions for how to constructthe dataset). (d) We recognize that reproducibility may be tricky in some cases, in which caseauthors are welcome to describe the particular way they provide for reproducibility.In the case of closed-source models, it may be that access to the model is limited insome way (e.g., to registered users), but it should be possible for other researchersto have some path to reproducing or verifying the results.",
  ". Experimental Setting/Details": "Question: Does the paper specify all the training and test details (e.g., data splits, hyper-parameters, how they were chosen, type of optimizer, etc.) necessary to understand theresults?Answer: [Yes]Justification: in the Appendix CGuidelines: The answer NA means that the paper does not include experiments. The experimental setting should be presented in the core of the paper to a level of detailthat is necessary to appreciate the results and make sense of them.",
  ". Experiment Statistical Significance": "Question: Does the paper report error bars suitably and correctly defined or other appropriateinformation about the statistical significance of the experiments?Answer: [Yes]Justification: in the and Appendix D, and All results are reported after the statisticalsignificance tests.Guidelines: The answer NA means that the paper does not include experiments. The authors should answer \"Yes\" if the results are accompanied by error bars, confi-dence intervals, or statistical significance tests, at least for the experiments that supportthe main claims of the paper. The factors of variability that the error bars are capturing should be clearly stated (forexample, train/test split, initialization, random drawing of some parameter, or overallrun with given experimental conditions).",
  "Guidelines:": "The answer NA means that the paper does not use existing assets. The authors should cite the original paper that produced the code package or dataset. The authors should state which version of the asset is used and, if possible, include aURL. The name of the license (e.g., CC-BY 4.0) should be included for each asset. For scraped data from a particular source (e.g., website), the copyright and terms ofservice of that source should be provided. If assets are released, the license, copyright information, and terms of use in thepackage should be provided. For popular datasets, paperswithcode.com/datasetshas curated licenses for some datasets. Their licensing guide can help determine thelicense of a dataset.",
  "According to the NeurIPS Code of Ethics, workers involved in data collection, curation,or other labor should be paid at least the minimum wage in the country of the datacollector": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with HumanSubjectsQuestion: Does the paper describe potential risks incurred by study participants, whethersuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)approvals (or an equivalent approval/review based on the requirements of your country orinstitution) were obtained?Answer: [NA]Justification: the paper does not involve crowdsourcing nor research with human subjects.Guidelines:",
  "The answer NA means that the paper does not involve crowdsourcing nor research withhuman subjects": "Depending on the country in which research is conducted, IRB approval (or equivalent)may be required for any human subjects research. If you obtained IRB approval, youshould clearly state this in the paper. We recognize that the procedures for this may vary significantly between institutionsand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and theguidelines for their institution."
}