{
  "Abstract": "Recent advancements in models linking natural language with human motions haveshown significant promise in motion generation and editing based on instructionaltext. Motivated by applications in sports coaching and motor skill learning, weinvestigate the inverse problem: generating corrective instructional text, leveragingmotion editing and generation models. We introduce a novel approach that, givena users current motion (source) and the desired motion (target), generates textinstructions to guide the user towards achieving the target motion. We leverage largelanguage models to generate corrective texts and utilize existing motion generationand editing frameworks to compile datasets of triplets (source motion, target motion,and corrective text). Using this data, we propose a new motion-language modelfor generating corrective instructions. We present both qualitative and quantitativeresults across a diverse range of applications that largely improve upon baselines.Our approach demonstrates its effectiveness in instructional scenarios, offeringtext-based guidance to correct and enhance user performance.",
  "Introduction": "Corrective instructions are crucial for learning motor skills, such as sports. Without feedback,people are at risk of developing improper, suboptimal, and injury-prone moves that hinder long-termprogress and health. With the growing popularity and immersion of motion-sensing sports games,the increasing accuracy and accessibility of 3D pose estimation techniques, and the advancement offitness equipment and trackers with versatile sensing technologies, the need for intelligent coachingsystems that provide corrective feedback on user motion is becoming increasingly important. In this work, we study the task of Motion Corrective Instruction Generation, which aims to create text-based guidance to help users correct and improve their physical movements. This task has significantapplications in sports coaching, rehabilitation, and general motor skill learning, providing users withprecise and actionable instructions to enhance their performance. By leveraging advancements inhuman motion generation and editing, this task addresses the need for personalized and adaptivefeedback in various instructional scenarios. Recent research in text-conditioned human motion generation has shown impressive progress. Meth-ods like MotionCLIP and TEMOS have utilized neural networks and transformer-basedmodels to align text and motion into a joint embedding space, producing diverse and high-qualitymotion sequences. These models, however, focus primarily on generating motions from text ratherthan generating corrective instructions from motion pairs. Therefore they are not directly suitable foranalyzing and improving user movements based on a comparison of motion sequences.",
  "arXiv:2412.05460v1 [cs.CV] 6 Dec 2024": "Research specifically focused on corrective instruction generation is still in its early stages. Traditionalmethods often rely on building statistical models for specific action categories, which require expertexperience and are difficult to scale and generalize to various actions. For example, Pose Trainer and AIFit employ neural networks and statistical models to provide feedback on specificexercises, but these methods have significant drawbacks: (1) They often require large amounts ofannotated data for each specific action class, making them hard to generalize across different typesof motions. However, unlike text-to-motion or human pose correction (which can be annotatedthrough simple pipelines ), human motion sequences involve temporal changes. Annotating thedifferences between these temporal changes is challenging. (2) Many of these methods are limited toanalyzing static poses or images rather than dynamic sequences of motion, reducing their applicabilityto real-world scenarios where movement dynamics are crucial. LLMs, such as Llama , have shown potential in generating corrective instructions using few-shotor zero-shot learning. However, without proper fine-tuning and additional modalities, LLMs struggleto understand the spatial and temporal context of poses and motions, limiting their effectiveness inspecialized fields like coaching or corrective instruction generation. To address these limitations, we propose a novel approach, CigTime, for generating motion correctiveinstructions. Our method leverages existing motion editing pipelines to create datasets of motiontriplets (source, target, and instruction). The key components of our approach include: Motion-Editing-Based Data Collection: We develop a pipeline that uses motion editing techniques togenerate large datasets of motion pairs and corresponding corrective instructions. This process in-volves using a pre-trained motion editor to modify source motions according to generated instructions,resulting in target motions that reflect the desired corrections. Fine-Tuning Large Language Models:We fine-tune a large language model (LLM) on the generated datasets to enable it to produce preciseand actionable corrective instructions. By training the LLM on a diverse set of motion sequences andcorrections, we enhance its ability to understand and generate contextually relevant feedback.",
  "We introduce a motion-editing-based pipeline to efficiently generate large datasets of correc-tive instructions, reducing the dependency on extensive manual annotations": "We propose a general motion corrective instruction generation method which utilizes a largelanguage model to translate motion discrepancies into precise and actionable instructionaltext, addressing the relationship between language and dynamic motions. Through comprehensive evaluations, we show that our method significantly outperformsexisting models in generating high-quality corrective instructions, providing better guidancefor users in various real-world scenarios.",
  "Text Conditioned Human Motion Generation": "Conditional motion generation aims to synthesize diverse and realistic motion conditioning ondifferent control signals, such as music , action categories , physical signals. Recent years have seen significant progress in text conditioned human motion generation. Some methods align the texts andmotions into a joint embedding space for generation. Benefiting by aligning motion latent to theCLIP embedding space, MotionCLIP could generate out-of-distribution motions. Severalworks utilize other mechanisms to increase the diversity and quality of generated motions. TEMOS and TEACH employ transformer-based VAEs to generate motion sequences based on texts.Guo et al. propose an auto-regressive conditional VAE to generate human motion sequences.Inspired by the achievements in image generation, the diffusion models, such as MotionDiffuse ,MDM and FLAME , have also been applied to motion generation. Some follow-up works attempt to improve the controllability of the diffusion model. Recently, the Vector QuantizedVariational Autoencoder (VQ-VAE) has gained significant traction in being used to convert 3D humanmotion into motion tokens which are subsequently employed alongside language models. TM2T proposes using these quantized tokens to facilitate the mapping between text and motion. T2M-GPT employs an auto-regressive method to predict the next-index token. Further, MotionGPT utilizes large language models (LLMs) to simultaneously handle different motion-related",
  "Motion Editing": "Motion editing enables users to interactively refine generated motions to suit their expectations.PoseFix utilize neural networks to edit 3D poses. Holden et al. employs an autoencoderto optimize the trajectory constraints. MDM , MotionDiffuse and FLAME involveprocessing by masks that designate parts for editing through reverse diffusion. GMD andPriorMDM are designed to edit motion sequences conditioned on joint trajectories. OmniControl incorporates control signals that encourage motions to conform to the spatial constraints whilebeing realistic. Recently, FineMoGen tackles fine-grained motion editing which allows forediting the motion of individual body parts, however its heavy reliance on specific-fine grained formatlimits the smooth coordination among movements of different body parts.",
  "Corrective Instruction Generation": "Traditional methods focus on specific action categories by building statistical models thatrequire expert experience. These methods struggle to scale and generalize to various actions. PoseTutor uses neural networks to learn statistical models but requires large amounts of data for eachaction and can only analyze static images or poses. FixMyPose creates a dataset with human-annotated corrective instructions on synthetic 2D images. PoseFix designs an automatic annotationsystem and a conditioned auto-regressive model for corrective instruction generation, but it is limitedto static poses. Recently, Large Language Models (LLMs) have made significant advances intext generation. With appropriate prompting, LLMs can generate pose corrective instructions withfew-shot or zero-shot example data. However, LLMs access to text makes them less aware of avariety of possible motions that people could perform and links them with languages. Our key insight for corrective instruction generation is to regard this task as a close yet inverseproblem to text-conditioned motion generation and editing, allowing us to bring the progress in thatfast-growing space to this understudied problem: We first propose a novel corrective instruction datacollection pipeline based on motion editing. Subsequently, we design a model that leverage largelanguage models to provide corrective instructions on spatial form and temporal dynamics.",
  "Overview": "We present an overview of our approach in . Given a source motion sequence, xI RT D,where T is the number of frames and D is the dimensionality of the motion representation, and atarget motion sequence, xO RT D, as input, our goal is to learn a function T which maps xI andxO to the corrective text instruction L, i.e., T (xI, xO) = L. To achieve this, we employ a pre-trained motion editor, which takes as input the source motionsequence and ground-truth corrective text, to output target motion sequences. Next, we quantize thesource and target motion sequences into discrete tokens using a VQ-VAE-based network. Finally,we organize these tokens with a predefined template to fine-tune an LLM on the triplets that containsource motion sequence xI, target motion sequence XO, and corrective instruction L for generatinginstructions that can efficiently modify the source to the target motion sequence.",
  "Motion-Editing-Based Data Collection": "The task of generating corrective instructions requires triplet data consisting of the source motion, thetarget motion, and the corrective instruction. Collecting such a dataset through human annotation iscostly and inefficient. We aim to leverage existing pre-trained models to streamline the data collectionprocess. However, there isnt an existing model that generates such triplets. Our fundamental insight is to treat corrective instruction generation as an inverse process of motionediting, which uses a given text to guide an agent in editing its initial motion. We utilize the motionediting process to gather required triplets: we collect a set of source motions and employ a pre-trained",
  "Target Motion": ": Overview of CigTime. Left: We leverage source motion tokens and corrective instructionsas input to a motion editor to produce target motion tokens. Right: We then employ a language modelto generate precise corrective instructions based on a given source and target motion. We demonstratein the example generating corrective instructions for lifting weights with the upper body.",
  "motion editor to edit the source motion based on a corrective instruction, resulting in the targetmotion": "Motion EditingIn this work, we utilize the motion diffusion model (MDM) as the motioneditor. Given the input motion sequence, x, and generation condition, c, MDM uses probabilisticdiffusion models for motion generation. It comprises a forward process, which is a Markov chaininvolving the sequential addition of Gaussian noise to the data, and a reverse process that progressivelydenoises the data to get the edited motion. The forward process of MDM is formulated as,",
  "p(xt1|xt) = N(xt1; (xt, t), 2t I),(4)": "where is the learnable parameters of the diffusion model, which gradually anneals the noise from aGaussian distribution to the data distribution. We train MDM as a conditional generator G(xt, c, t)that outputs x0, where c is the text condition, to maximize p(x0:T ). In inference, MDM takes noise n as xT and applies the reverse process to denoise the input basedon the text condition, c, generating the motion sequences, x0, corresponding to c. For the motionediting task, we utilize the corrective instruction, L, as the generation condition, c, to generate thecorresponding corrective motion sequence, xL. We then calculate the target motion sequence, xO, bycombining the source motion sequence, xI, and the corrective motion sequence, xL,",
  "xO = m xL + (1 m) xI,(5)": "where m is the joint mask for the body part P, and is the element-wise multiplication for maskingoperation. Through the above process, we are able to collect a large amount of xI, xO, L triplets.We use this dataset to fine-tune a large language model (LLM) for the corrective instruction generationtask as in the following.",
  "Fine-tuning LLMs for Corrective Instruction Generation": "With the prepared dataset of triplets from the motion editing process, we learn the inverse processof motion editing, a function, T , that maps source and target motion sequence pairs to correctiveinstructions. We first learn an encoder based on VQ-VAE to tokenize the motion sequences intodiscrete tokens and organize the discrete tokens based on a pre-defined template. Then, we fine-tunedan LLM to generate the corrective instruction, L, based on the tokens of the source and target motionsequence, xI and xO. Tokenizer Pre-trainingCompared to directly feeding the original data to the LLMs, the discreterepresentation has been proven to be more suitable for fine-tuning LLMs with human-motion-relatedtasks . Inspired by these works, we initialize a VQ-VAE-based network, which contains anencoder E, a codebook C, and a decoder D. The encoder E takes motion sequence, x, as input andmaps, x, into discrete features, f T H, where H is the dimensionality of the frame feature. The codebook, C RKH, represents different codes, where K is a predefined number of differentdiscrete codes and ck RH is the k-th code. VQ-VAE quantizes the discontinuous feature f to thediscrete latent codes, z RT H, through codebook, c, by projecting each per-frame feature fi to itsnearest code:",
  "zi = Q (fi) = ck, where k = argminj fi cj22 ,(6)": "where Q represents the quantization operation. The decoder D takes the code, z, as input, andreconstructs the motion sequence, xO. We use the index k as the token of each discrete code, ck, asthe token representation of the frame feature fi. We apply the L2 loss for the training of the tokenizer,",
  "where sg() is the stop gradient operation that helps stabilize the training process": "Fine-tuning LLMInstruction Tuning is a widely used technique to enable LLMs to handle specifictasks. In this work, we employ this technique to fine-tune our LLM. Specifically, given an LLM, T , asource discrete token set, Is = Is0, Is1, ..., Isns, and a target discrete token set, It = It0, It1, ..., Itnt, we organize the input of T to follow the template as shown in . This input is then tokenized intotext tokens U I = uI0, uI1, ..., uInUI . Additionally, we tokenize the ground-truth corrective instruction,L, into text tokens, U O = uO0 , uO1 , ..., uOnUO .",
  "j=0log pL(uOj |uO0:j1, U I).(9)": "By using a structured input template and optimizing the cross-entropy loss, we enable the LLM togenerate accurate and contextually relevant corrective instructions. This approach ensures that themodel effectively learns to convert discrepancies between the source and target motions into preciseand actionable instructional text. Learning Representation for Motion TokensPrevious methods for training text-to-motion modelsinvolve either using an existing vocabulary for motion tokens or assigning new learnableembeddings , followed by fine-tuning with techniques like LoRA. We tried both approaches,but the results of utilizing one of them alone were not satisfying. There are two main reasons: First,using a fixed vocabulary and embeddings prevents capturing the correlation of motion differences andcorrective instructions, as the weights are trained on tasks with a large domain gap. Second, whilenew embeddings can be learned with LoRA, the distribution of the original vocabularys embeddingsimposes constraints, making the learned embeddings suboptimal, especially given the smaller scaleof training data for corrective instructions. To address these challenges, we integrate the goods of both. We use existing vocabulary tokens fortheir rich semantics and fine-tune all embeddings to maximize performance and reduce the domaingap. We also introduce an anchor loss to prevent the embeddings from diverging:",
  "Experiment Setup": "Datasets We obtain the source motion sequences from HumanML3D , a dataset containing 3Dhuman motions and associated language descriptions. We make use of the entire dataset for thecollection of source motions. We then generate triplets based on pre-trained motion editor withinstructions and target motions. We split HumanML3D following the original setting and for eachmotion sequence in HumanML3D, we randomly select one instruction from the correspondingsplit for editing the sequence. We subsequently edit the source motion sequences with MDM conditioned on the corrective instructions to obtain the target sequences. Implementation Details We fine-tune a pre-trained Llama-3-8B using full-parameter fine-tuningfor corrective instruction generation. The model is optimized using the Adam optimizer with aninitial learning rate of 105. We use a batch size of 512 and train on four NVIDIA Tesla A100 GPUsfor eight epochs, which takes approximately 5 hours to complete. Following HumanML3D , thedimensionality, D, of the motion sequences is set to 263 for our experiments.",
  "Evaluation Metrics We evaluate the generated corrective instruction with two types of metrics": "(1) Corrective instruction quality: BLEU , ROUGE , and METEOR are commonlyemployed metrics that assess various n-gram overlaps between the ground-truth text and the generatedtext. Although these metrics focus on structural text similarity, they tend to disregard semanticmeaning. Consequently, we also utilize the cosine similarity of text CLIP embeddings as an evaluationmetric to better compare semantic similarity. (2) Reconstruction accuracy: To evaluate the quality, we use the generated corrective instructionas an editing condition to modify the source motion sequences and obtain the generated targetmotion. We then compare this with the ground-truth target motion. Specifically, we employ MeanPer Joint Position Error (MPJPE) to measure the average Euclidean distance between the generatedand ground-truth 3D joint positions for all joints. Additionally, we calculate the Frchet InceptionDistance (FID) using a feature extractor to evaluate the distance between the feature distributionsof the generated and ground-truth target motions. Ideally, the generated motion sequences shouldclosely resemble the target motion sequences. : Comparison to the Existing Work.We compare our approach against large lan-guage (Llama-3-8B, Llama-3-8B-LoRA, Qwen-7B, Mistral-7B) and motion-language (MotionGPT,MotionGPT-M2T) models. We demonstrate that our approach, CigTime outperforms all the baselinesby a large margin for corrective instruction generation for human motion.",
  "Ours0.240.350.520.820.131.44": "Comparison Baselines To the best of our knowledge, we are the first to generate corrective instructionfor general motion pairs. Thus, we adopt two different kinds of methods designed for general text-based tasks and motion captioning. (1) Llama3 , Qwen and Mistral are all large language models designed for generaltext-based tasks. They can be applied to unseen tasks with just a few-shot data. We utilize thein-context learning technique to generate correction instructions by giving them examples ofthe source-target-instruction triplets. We present the detailed prompts in the supplemental material.In addition to the baselines that use in-context learning with LLMs, we ablate different fine-tuningtechniques. To do so, we compare our approach, which uses full-parameter LLM tuning to a variant,which utilizes the LoRA adapter to fine-tune the Llama 3 8B and Mistral 7B models. (2) MotionGPT . Although MotionGPT isnt trained with corrective instruction data, it has beenproven to have the ability to generalize across different motion-based tasks by utilizing specific inputtemplates for different tasks. Thus, we adopt this method for corrective instruction generation byutilizing the template mentioned in Section. 3.3. In addition, as generating corrective instructions isnot a target for MotionGPT, we create yet another baseline called MotionGPT-M2T that employsMotionGPT to generate captions corresponding for the target motions.",
  "Our quantitative results are presented in Table. 1. We further discuss below the quality of thecorrective instructions and the reconstruction accuracy of target motion after editing": "Corrective Instruction QualityOur method demonstrated superior performance across mostmetrics when compared to baseline methods, as presented in Table. 1. Specifically, our methodachieved the highest BLEU-4, ROUGE-2 and METERO scores of 0.24, 0.35, and 0.52, significantlysurpassing the baseline methods. This indicates that our method generates text with higher precision. Furthermore, our method achieved the highest CLIP Score of 0.82, outperforming other baselines.The CLIP Score indicates the semantic alignment of the generated text with visual content, and ahigher score demonstrates better performance in maintaining this alignment.",
  "We find that the two baselines adopted from MotionGPT both present inferior performances, whichcan be attributed to its training on a text-motion dataset, which lacks the capability to compare": "two motion sequences and identify specific differences. Besides, although MotionGPT excels atgenerating captions for motion sequences, its still difficult to reconstruct the original target motionsequence from the generated descriptions. This is because describing the differences and similaritiesbetween two motion sequences can help us accurately depict the target motion with fewer statements,which MotionGPT does not possess. This evidenced that simply fine-tuning Llama-3 using the generated data would not result in asatisfactory corrective instruction generation, e.g., due to overfitting or catastrophic forgetting.Although the outputs can induce similar target motion sequences compared to the ground truth,the increased variance in the text can lead to a decrease in the overall NLP metrics such as BLEU,ROUGE, and METERO. Overall, these results highlight the effectiveness of our method in generating high-quality correctiveinstructions, with significant improvements in precision, similarity, and visual-semantic consistencyover the baseline methods. : Ablation study with different network structure. We extend the LLMs vocabularies withnew learnable embeddings for the motion tokens and update the corresponding embeddings duringfine-tuning as baselines. We also compare variants that utilizes T5 as the backbone (ours-T5), andcontinous representaion (Ours-Continuous).",
  "Ours-Continuous0.120.240.470.780.202.56Ours-T50.140.250.460.800.335.03Ours0.240.350.520.820.131.44": "Reconstruction AccuracyThe evaluation of reconstruction accuracy highlights the superior perfor-mance of our method in distinguishing between source and target motions. As shown in , ourmethod achieved the lowest MPJPE of 0.1330, indicating the highest accuracy in pose reconstruction.Furthermore, our method also attained the lowest FID - Target score of 1.4442, demonstrating itseffectiveness in generating data that closely matches the target motion. Similarly, MotionGPTsinferior performance in these metrics is a result of its limited ability to analyze differences betweenmotion pairs, as evidenced by its MPJPE of 0.8011 and FID score of 8.8350. Additionally, although LLM models like Llama-3-8B can maintain text consistency via in-contextlearning, they are unable to grasp the intricate connections between motion sequences and language,leading to inferior overall performance compared to our approach. Even when benefiting fromfine-tuning through LoRA, these models still cannot generate high-quality corrective instructions. Overall, these results underline the effectiveness of our method in accurately distinguishing andreconstructing the differences between source and target motions, outperforming the baseline methodsin both MPJPE and FID metrics.",
  ": Visualization of corrective instructions and reconstructed motions for different methods": "conducted a comparison of LLMs trained using token embeddings, as shown in . Althoughfine-tuning with extended vocabulary can enhance the text-based metrics, these instructions causea decline in the motion editing performance, resulting in a reduction in MPJPE and FID. From theperspective of the task definition, we require a model that prioritizes high reconstruction quality overinstruction quality. Therefore, extending vocabulary is more detrimental than beneficial for our task. Besides we fine-tune T5-770M , as in Motion-GPT and AvatarGPT to validate theimpact of different LLM frameworks on the results. The experimental results show that the T5framework does not offer an advantage over larger language models in the Motion CorrectiveInstruction Generation task. We also compared our method with its variant based on continuousrepresentations, as implemented by MotionLlm . As observed, our method still outperforms thecontinuous baseline across all the reconstruction accuracy metrics. Evaluation with Different Motion EditorsDifferent people may perform various actions inresponse to the same instruction. Our goal is for our model to produce instructions that are as accurateand widely accepted as possible. Therefore, we evaluate our methods and baselines using differentmotion editors. In addition to MDM, which we used to generate the ground-truth dataset, we alsoassess the methods with two different versions of PriorMDM as shown in . Our proposed method consistently outperforms other models across different motion editors, demon-strating the lowest MPJPE and FID values, close to the ground truth. This highlights its effectivenessin generating accurate and visually similar corrective motions. In contrast, models like MotionGPTand its variant exhibit significantly higher errors, indicating limitations in their generation capabilities.",
  "Visual Results": "To further analyze the performance of different methods, we present visual comparisons in 3. Asshown in the results, our algorithm largely maintains similar semantics and achieves reconstructionresults that are closely aligned with the ground truth. This demonstrates the accuracy of our algorithmin generating corrective instructions. In contrast, Llama3-8B, despite achieving favorable numericalresults, may incorrectly identify the joint parts involved in motion editing. This highlights ourapproachs superiority in providing accurate and contextually appropriate motion corrections.",
  "Conclusion": "We introduced a new task and a framework for generating corrective instructions that translate a sourcemotion into a target motion. Our key insight is to leverage the fast growing field of text-conditioned motion-editing for this related yet understudied inverse problem. To create a dataset for this task, weproposed a motion editing pipeline that minimizes the need for extensive manual annotations. Wedemonstrated the utility of our approach which largely outperforms existing related models.",
  "We aim to address these limitations in future research, along with further advances of text-conditionedmotion-editing frameworks, which share our challenges, limitations, and potential solutions": "This work is supported by the Early Career Scheme of the Research Grants Council (grant #27207224), the HKU-100 Award, a donation from the Musketeers Foundation, an Academic Giftfrom Meta, and the Microsoft Accelerate Foundation Models Research Program. The authors wouldlike to thank Robert Wang, Shugao Ma, Alexander Winkler, Yijing Li, and Chris Twigg for theirvaluable discussions. Special thanks are also extended to Pei Zhou, Chenming Zhu, and Shumin Sunfor their support in the real-world application.",
  "Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge,Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023": "Satanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation withimproved correlation with human judgments. In Proceedings of the acl workshop on intrinsicand extrinsic evaluation measures for machine translation and/or summarization, pages 6572,2005. Ling-Hao Chen, Shunlin Lu, Ailing Zeng, Hao Zhang, Benyou Wang, Ruimao Zhang, and LeiZhang. Motionllm: Understanding human behaviors from human motions and videos. arXivpreprint arXiv:2405.20340, 2024.",
  "Steven Chen and Richard R Yang. Pose trainer: correcting exercise posture using pose estimation.arXiv preprint arXiv:2006.11718, 2020": "Ginger Delmas, Philippe Weinzaepfel, Francesc Moreno-Noguer, and Grgory Rogez. Pose-fix: Correcting 3d human poses with natural language. In Proceedings of the IEEE/CVFInternational Conference on Computer Vision, pages 1501815028, 2023. Bhat Dittakavi, Divyagna Bavikadi, Sai Vikas Desai, Soumi Chakraborty, Nishant Reddy,Vineeth N Balasubramanian, Bharathi Callepalli, and Ayon Sharma. Pose tutor: an explainablesystem for pose correction in the wild. In Proceedings of the IEEE/CVF conference on computervision and pattern recognition, pages 35403549, 2022.",
  "Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, JingjingXu, and Zhifang Sui. A survey on in-context learning. arXiv preprint arXiv:2301.00234, 2022": "Zhiyang Dou, Xuelin Chen, Qingnan Fan, Taku Komura, and Wenping Wang. C ase: Learningconditional adversarial skill embeddings for physics-based characters. In SIGGRAPH Asia 2023Conference Papers, pages 111, 2023. Mihai Fieraru, Mihai Zanfir, Silviu Cristian Pirlea, Vlad Olaru, and Cristian Sminchisescu. Aifit:Automatic 3d human-interpretable feedback models for fitness training. In Proceedings of theIEEE/CVF conference on computer vision and pattern recognition, pages 99199928, 2021. Anindita Ghosh, Noshaba Cheema, Cennet Oguz, Christian Theobalt, and Philipp Slusallek. Syn-thesis of compositional animations from textual descriptions. In Proceedings of the IEEE/CVFinternational conference on computer vision, pages 13961406, 2021. Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji, Xingyu Li, and Li Cheng. Generatingdiverse and natural 3d human motions from text. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 51525161, 2022. Chuan Guo, Xinxin Zuo, Sen Wang, and Li Cheng. Tm2t: Stochastic and tokenized modelingfor the reciprocal generation of 3d human motions and texts. In European Conference onComputer Vision, pages 580597. Springer, 2022. Chuan Guo, Xinxin Zuo, Sen Wang, Shihao Zou, Qingyao Sun, Annan Deng, Minglun Gong,and Li Cheng. Action2motion: Conditioned generation of 3d human motions. In Proceedingsof the 28th ACM International Conference on Multimedia, pages 20212029, 2020.",
  "Biao Jiang, Xin Chen, Wen Liu, Jingyi Yu, Gang Yu, and Tao Chen. Motiongpt: Human motionas a foreign language. Advances in Neural Information Processing Systems, 36, 2024": "Sai Shashank Kalakonda, Shubh Maheshwari, and Ravi Kiran Sarvadevabhatla. Action-gpt:Leveraging large-scale language models for improved and generalized action generation. In2023 IEEE International Conference on Multimedia and Expo (ICME), pages 3136. IEEE,2023. Korrawe Karunratanakul, Konpat Preechakul, Supasorn Suwajanakorn, and Siyu Tang. Guidedmotion diffusion for controllable human motion synthesis. In Proceedings of the IEEE/CVFInternational Conference on Computer Vision, pages 21512162, 2023. Hyounghun Kim, Abhay Zala, Graham Burri, and Mohit Bansal. Fixmypose: Pose correctionalcaptioning and retrieval. In Proceedings of the AAAI Conference on Artificial Intelligence,volume 35, pages 1316113170, 2021.",
  "Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarizationbranches out, pages 7481, 2004": "Meta. Llama3, 2024. Gyeongsik Moon, Ju Yong Chang, and Kyoung Mu Lee. Posefix: Model-agnostic generalhuman pose refinement network. In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 77737781, 2019. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automaticevaluation of machine translation. In Proceedings of the 40th annual meeting of the Associationfor Computational Linguistics, pages 311318, 2002. Xue Bin Peng, Pieter Abbeel, Sergey Levine, and Michiel Van de Panne. Deepmimic: Example-guided deep reinforcement learning of physics-based character skills. ACM Transactions OnGraphics (TOG), 37(4):114, 2018.",
  "Mathis Petrovich, Michael J Black, and Gl Varol. Temos: Generating diverse human motionsfrom textual descriptions. In European Conference on Computer Vision, pages 480497.Springer, 2022": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visualmodels from natural language supervision. In International conference on machine learning,pages 87488763. PMLR, 2021. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unifiedtext-to-text transformer. Journal of machine learning research, 21(140):167, 2020.",
  "Yonatan Shafir, Guy Tevet, Roy Kapon, and Amit H Bermano. Human motion diffusion as agenerative prior. arXiv preprint arXiv:2303.01418, 2023": "Soyong Shin, Juyong Kim, Eni Halilaj, and Michael J Black. Wham: Reconstructing world-grounded humans with accurate 3d motion. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, pages 20702080, 2024. Guy Tevet, Brian Gordon, Amir Hertz, Amit H Bermano, and Daniel Cohen-Or. Motionclip:Exposing human motion generation to clip space. In European Conference on Computer Vision,pages 358374. Springer, 2022.",
  "Yiming Xie, Varun Jampani, Lei Zhong, Deqing Sun, and Huaizu Jiang. Omnicontrol: Controlany joint at any time for human motion generation. arXiv preprint arXiv:2310.08580, 2023": "Jianrong Zhang, Yangsong Zhang, Xiaodong Cun, Shaoli Huang, Yong Zhang, Hongwei Zhao,Hongtao Lu, and Xi Shen. T2m-gpt: Generating human motion from textual descriptions withdiscrete representations. arXiv preprint arXiv:2301.06052, 2023. Mingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou Hong, Xinying Guo, Lei Yang, andZiwei Liu. Motiondiffuse: Text-driven human motion generation with diffusion model. IEEETransactions on Pattern Analysis and Machine Intelligence, 2024. Mingyuan Zhang, Huirong Li, Zhongang Cai, Jiawei Ren, Lei Yang, and Ziwei Liu. Finemogen:Fine-grained spatio-temporal motion generation and editing. Advances in Neural InformationProcessing Systems, 36, 2024. Yaqi Zhang, Di Huang, Bin Liu, Shixiang Tang, Yan Lu, Lu Chen, Lei Bai, Qi Chu, NenghaiYu, and Wanli Ouyang. Motiongpt: Finetuned llms are general-purpose motion generators. InProceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 73687376,2024. Zixiang Zhou, Yu Wan, and Baoyuan Wang. Avatargpt: All-in-one framework for motionunderstanding planning generation and beyond. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 13571366, 2024. Instruction: I utilize some tokens to represent human motion. You are asked to compare two sequences and output the correctional instruction about what modifications the person should make to transfer from the first action to the second action.",
  "[End of examples]": "I will give you two new sequences. You need to compare the two and provide corresponding corrective instructions.Action 1: [Token list for source motion sequence]Action 2: [Token list for Target motion sequence] : In-context learning for corrective instruction generation. The prompt for the LLMs inin-context learning includes a task description and several examples. This information is given to theLLMs, instructing them to generate correctional instructions for new motion pairs.",
  "B.1Generalization to New Data": "Our algorithm is fully trained and tested on the HumanML3D dataset, which may impact itsgeneralization. To evaluate the generalization ability of our algorithm, we collected 1525 samplesfrom the Fit3D dataset. We present the results in . These results show that the BLEU, ROUGE, and METEOR scoresdecreased from 0.24, 0.35, and 0.52 to 0.03, 0.05, and 0.20, respectively. This indicates that when thedataset changes, the corrective instructions generated by our algorithm deviate from the ground truthin form. However, the changes in CLIP score, MPJPE, and FID are subtle. This suggests that evenafter switching datasets, our algorithm can still effectively capture the differences in motion pairs anddescribe them in appropriate language. Our algorithm therefore generally showcases a notable levelof generalization capability.",
  "B.3Additional Visual Results": "We present visualization examples of our corrective instructions and reconstructed motion sequences in .We observe that although the corrective instructions predicted by our algorithm sometimes differ from theground truth (e.g., \"forehand table tennis\" versus \"throwing a frisbee\"), they can still result in remarkably similarmodified motions. In specific frames, the resulting motions are nearly identical, as seen in the beginning andending frames of the first example. This phenomenon aligns with real-world scenarios where individuals canprovide multiple, semantically distinct suggestions that lead to similar corrective outcomes when correctingothers mistakes. This underscores the robustness of our approach in generating effective motion corrections,even when the specific instructions vary. Considering the diversity of correction instructions, traditional metrics such as BLEU, ROUGE, or METEORalone may not be sufficient to describe their correctness. Thus, we incorporated CLIP score and reconstructionmetrics as supplementary evaluation measures, creating a more exhaustive benchmark for evaluating correctioninstruction generation. We present more visual results in and 8.",
  "C.2Architecture of Our Tokenizer": "We utilize TCN-based structures for both encoder and decoders, which extract spatiotemporal features forhuman motion through convolution with a kernel size of 1. We also extract temporal features through dilationconvolution and larger kernels (9 or 3). We list the details of our network architecture in Tab. 7. The decoders Uand B share the same architecture.",
  "EReal-world Application": "Obtaining precise motion in real life is difficult. However, we find that existing motion estimation algorithmsenable us to obtain usable motion sequences in most cases. To verify whether the current pipeline can be appliedto real life, we conduct the following experiment. We invited two participants, one acting as a coach and the other as a trainee. The trainee first performed a sourcemotion sequence. Then, the coach was tasked with generating a target motion sequence that differed from thesource sequence. We utilized a pose estimation algorithm (WHAM) to extract these motion sequences anduse our method to generate corrective instructions. The trainee is then required to correct his motion based onthe corrective instructions. We present an example in of the global response pdf. In this example, it isevident that existing motion estimation algorithms can accurately estimate the motions of both the trainee and thecoach. Furthermore, our algorithm is capable of understanding these motion sequences to provide appropriatecorrective instructions.",
  "Linear Encoder": "(0): Conv1D(J*3, 256, kernel_size=(3,), stride=(1,), padding=(1,))(1): ReLU()(2): 2 Sequential((0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))(1): ResConv1DBlock((0): (activation1): ReLU()(1): (conv1): Conv1D(256, 256, kernel_size=(3,), stride=(1,), padding=(9,), dilation=(9,))(2): (activation2): ReLU()(3): (conv2): Conv1D(256, 256, kernel_size=(1,), stride=(1,)))(2): ResConv1DBlock((0): (activation1): ReLU()(1): (conv1): Conv1D(256, 256, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))(2): (activation2): ReLU()(3): (conv2): Conv1D(256, 256, kernel_size=(1,), stride=(1,)))(3): ResConv1DBlock((0): (activation1): ReLU()(1): (conv1): Conv1D(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))(2): (activation2): ReLU()(3): (conv2): Conv1D(256, 256, kernel_size=(1,), stride=(1,))))",
  "Decoder": "(0): 2 Sequential((0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))(1): ResConv1DBlock((0): (activation1): ReLU()(1): (conv1): Conv1D(256, 256, kernel_size=(3,), stride=(1,), padding=(9,), dilation=(9,))(2): (activation2): ReLU()(3): (conv2): Conv1D(256, 256, kernel_size=(1,), stride=(1,)))(2): ResConv1DBlock((0): (activation1): ReLU()(1): (conv1): Conv1D(256, 256, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))(2): (activation2): ReLU()(3): (conv2): Conv1D(256, 256, kernel_size=(1,), stride=(1,)))(3): ResConv1DBlock((0): (activation1): ReLU()(1): (conv1): Conv1D(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))(2): (activation2): ReLU()(3): (conv2): Conv1D(256, 256, kernel_size=(1,), stride=(1,))))(2) Conv1D(256, 256, kerne_size=(1,), stride=(1,))(1): ReLU()(2): Conv1D(256, 75, kernel_size=(1,), stride=(1,))"
}