{
  "Abstract": "The rapid advancements in Large Language Models (LLMs) and their enhancedreasoning capabilities are opening new avenues for dynamic, context-aware taskdecomposition, and automated tool selection. These developments lay the ground-work for sophisticated autonomous agentic systems powered by LLMs, whichhold significant potential for process automation across various industries. Thesesystems demonstrate remarkable abilities in performing complex tasks, interact-ing with external systems to augment LLMs knowledge, and executing actionsautonomously. To address the challenges and harness the opportunities presentedby these advances, this paper makes three key contributions. We propose an advanced agentic framework designed to autonomously pro-cess multi-hop user queries by dynamically generating and executing taskgraphs, selecting appropriate tools, and adapting to real-time changes in taskrequirements or tool availability.",
  "We develop a specialized dataset based on the AsyncHow dataset to enablein-depth analysis of agentic behavior across varying task complexities": "Our findings demonstrate that asynchronous and dynamic task graph decomposi-tion significantly improves system responsiveness and scalability, particularly inhandling complex, multi-step tasks. Through detailed analysis, we observe thatstructural and node-level metrics are more critical in sequential tasks, whereastool-related metrics dominate in parallel tasks. In particular, the Structural Sim-ilarity Index (SSI) emerged as the most significant predictor of performance insequential tasks, while Tool F1 Score proved essential in parallel tasks. Thesefindings highlight the need for balanced evaluation methods that capture bothstructural and operational aspects of agentic systems. Our specialized datasetenables comprehensive evaluation of these behaviors, providing valuable insightsinto improving overall system performance, with the importance of both structuraland tool-related metrics validated through empirical analysis and statistical testing.The evaluation of agentic systems presents unique challenges due to the intricaterelationships between task execution, tool usage, and goal achievement. Ourevaluation framework, validated through empirical analysis, offers valuable in-sights for improving the adaptability and reliability of agentic systems in dynamicenvironments.",
  "Introduction": "Recent advances in LLMs have catalyzed the development of sophisticated agentic systems capableof automating multistep tasks, interacting with external systems, and adapting to changing contexts. These systems are promising in industries requiring autonomous workflow processing andtool integration. Despite their potential, LLM-based systems face limitations in industrial settings dueto lack of training on proprietary data and the challenges of fine-tuning. Fine-tuning LLMs for eachbusiness use case requires costly, labor-intensive dataset collection and processing. In such contexts,LLMs are limited in their ability to manage real-time decision making in dynamic environments. A scalable alternative to fine-tuning LLMs for each use case is the development of agentic systemsthat dynamically integrate external tools. Augmenting LLMs with tools allows these systems tohandle complex queries and adapt without constant retraining. Agentic frameworks enable LLMsto decompose tasks into smaller sub-tasks, whose significance is highlighted in , select theappropriate tools for each task, and adjust to real-time changes in tool availability. One of the first frameworks that allowed LLMs to interact with external tools is LangChain Project,which paved the way for more sophisticated agentic platforms such as BabyAGI Project and AutoGen. These systems represent important steps toward the realization of autonomous AI agents, butare often constrained by high latency, limited adaptability, and insufficient support for dynamic toolintegration. Moreover, current systems lack comprehensive evaluation methods that fully capture thecomplexity of task graph generation and tool selection, limiting their scalability and reliability inindustrial applications. To address these challenges, our work presents a framework1 that advances the capabilities oftraditional agentic systems. Our framework integrates real-time tool selection, dynamic task graphgeneration, and an evaluation mechanism to assess agentic behavior across diverse tasks and domains.The proposed architecture consists of five core components: the Orchestrator that generates taskgraphs based on user queries, the Delegator that manages task distribution, ensuring seamlesscommunication between tasks, the agents that autonomously execute tasks using LLMs, the toolsthat provide predefined functions necessary for task completion, and the Executor that handles theexecution sequence, optimizing for both parallel and sequential task execution. A significant aspect of this work is the development of a comprehensive evaluation framework2.Existing agentic systems often lack domain-specific metrics to rigorously assess their performance inhandling task decomposition and tool integration. To fill this gap, we propose these novel metrics: Node and Tool F1 Scores: These metrics assess the systems precision and recall in matchingtask nodes to the expected task graph, ensuring accurate task decomposition, and in selecting theappropriate tools for each task within the graph. Structural Similarity Index (SSI): A metric that assesses the overall fidelity of the task graphgenerated by the system compared to the expected graph, capturing both node and edge similaritiesto ensure the system preserves the logical structure of tasks. Additionally, we introduce a specialized dataset3 designed to evaluate agentic systems, which allowsa detailed analysis of the interdependencies between task decomposition, tool selection, and systemperformance, providing a foundation for evaluating agentic.",
  "A specialized dataset to evaluate agentic behavior across, supporting analysis of task graphgeneration, tool selection, and system performance": "1A detailed guide to replicate our agentic system is in Section S1.1 of the Supplementary Material2A detailed guide to replicate our evaluation framework is in Section S1.2 of the Supplementary Material3The dataset is available at this link The remainder of this paper is structured as follows - discusses related work in agenticsystems, task graph generation, and evaluation of agentic systems. provides a detailedexplanation of the architecture of our proposed framework. presents our evaluation metricsand dataset, followed by empirical results in . Finally, concludes with futuredirections and implications for agentic systems in real-world industrial settings.",
  "Agentic Frameworks for Task Graph Generation and Tool Selection": "The field of AI agent systems powered by LLMs has seen substantial development, with variousframeworks proposed to enhance collaboration, task automation, and system scalability. Recentsurveys by Wang et al., Guo et al., Masterman et al., and Xi et al. have explored these advancementsin detail, highlighting the evolving landscape of LLM-powered AI agents . In ,Crawford et al. thoroughly explored a flexible agent framework with a focus on the planning andexecution components of an AI agent. particularly explores the importance of planning and taskgraph generation. Existing frameworks, such as LangGraph, AutoGen, and BabyAGI, provide various approachesto task generation, tool selection, and handling multi-hop user queries . For instance,LangGraph enables stateful workflows with tasks managed cyclically or sequentially, but it does notdynamically adjust the task graph during execution as our proposed framework does . AutoGenand BabyAGI offer dynamic task generation but are limited by predefined toolsets and scalabilityissues, respectively . Our framework improves upon these methods by introducing real-time adaptability with a Task-Aware Semantic Tool Filtering mechanism, allowing for on-the-flyintegration of new tools and dynamic task graph adjustments to handle increased task complexity. This capability ensures continuous and efficient task execution, enhancing existing approaches.Despite these advancements, several limitations and shortcomings exist in current frameworks: Lack of testability: Existing frameworks often do not support rigorous unit testing of each component,making it challenging to ensure reliability and correctness in complex tasks . Our frameworkarchitecture is highly modular, enabling each agent to be tested individually, addressing this gap. High latency: The absence of efficient task parallelization mechanisms leads to increased latency,hindering real-time performance . Our system decomposes user queries into sub-tasks andexecutes them in parallel wherever possible, reducing latency. Limited customizability: Many frameworks offer limited flexibility for customization, restrictingtheir applicability across diverse domains and specific use cases . Our customizable designensures applicability across various domains.",
  "Evaluation Metrics and Datasets for Agentic Frameworks": "Despite advancements in agentic frameworks, there remains a notable gap in comprehensive eval-uation metrics and datasets that accurately assess the performance of these systems across diversetasks and domains. Existing benchmarks, such as AgentBench and VisualAgentBench, focus on LLMperformance across diverse environments and visual task automation but fall short in evaluating taskgraph structures in depth . Similarly, TASKBENCH introduces \"Tool Graphs\" to measuretask automation processes but provides limited analysis of intermediate steps , while AgentQuestemphasizes multi-step reasoning with little focus on task graph fidelity . Although VillagerAgentexplores multi-agent task dependencies in simulated environments, its focus is more on coordinationthan on detailed task decomposition . Our proposed evaluation framework fills these gaps by introducing detailed metricssuch as Node F1Score, Structural Similarity Index, and Tool F1 Scorepaired with specialized datasets to assess taskdecomposition, tool selection, and execution through task graph metrics. These metrics and datasetsoffer a granular analysis of agent performance, addressing the complexity of multi-step reasoning,task automation, and the interdependencies between evaluation metrics. This approach addressesgaps in current benchmarks by focusing on intermediate steps and structural fidelity, offering a morenuanced assessment applicable to real-world scenarios.",
  ": Overview of the framework architecture, showcasing the flow of a user query through thesystem": "The Orchestrator as shown in analyzes the users input to produce a Directed AcyclicGraph (DAG) with task nodes and dependency edges. Asynchronous task graph decomposition, asexplored by recent work on graph-enhanced LLMs, allows parallel task execution, enabling real-timeadaptation to task changes and dependencies. This reduces execution time by shortening criticalpaths, which is crucial for handling complex, dynamic queries . Hence, borrowing from thiswork and Graph Theory, the Orchestrator can be instructed to produce a task graph that is optimizedfor one or more of the following concepts: Coarse Grained Task Decomposition: A strategy in which the user query is decomposedinto a relatively small number of large tasks, each representing a substantial amount ofwork. This approach minimizes the overhead associated with task management, such asscheduling, synchronization, and inter-task communication, by focusing on larger, moreindependent units of work , . While this can reduce the potential for parallelism, itsimplifies execution and can be more efficient in scenarios where the overhead of managingnumerous small tasks is prohibitive. Fine Grained Task Decomposition: A strategy where a user query is broken down intoa large number of small, granular tasks. Each task represents a minimal unit of work,allowing for a high degree of parallelism as many tasks can be executed simultaneously.However, this approach often requires more sophisticated management of task dependencies,communication, and synchronization, which can introduce significant overhead , . Critical Path Optimization: A strategy that focuses on identifying and shortening thecritical paththe longest sequence of dependent tasks that determines the minimum timerequired to obtain a complete response to a User Query. By optimizing tasks in this path,the overall latency can be reduced . As depicted in , the Delegator receives the task graph from the Orchestrator and is responsiblefor assigning tasks to the appropriate agents or tools. It plays a critical role in managing intra-taskand inter-task communication by utilizing inter-task memory buffers, ensuring that each task has thenecessary context and data from its dependent predecessors. The Delegator also consolidates the results from all completed tasks to form the final response tothe user query. In scenarios where tasks have direct or indirect dependencies, the Delegator ensuresthat the inter-task memory buffers are populated with the necessary task descriptions and executionresults, thereby enabling subsequent tasks to execute with full awareness of the context provided bytheir dependencies. Within the framework, agents are dynamic components, as illustrated in the diagram ().They are responsible for utilizing a LLM to execute specific tasks. For instance, the PythonAgentcan generate Python functions on the fly, acting as tools that can be called with arguments. ThePythonAgent is particularly useful when a task requires a function that is not yet available in theexisting set of tools. Tools are static components that consist of pre-defined Python functions, which can accept argumentsand execute code as needed for various tasks. As shown in , the tool manager is responsiblefor task-aware semantic tool filtering. This ensures that only the most relevant tools are passed tothe LLM based on the specific task requirements, rather than overwhelming it with the entire toolcache. This semantic filtering enhances the efficiency and accuracy of task execution by reducingunnecessary processing overhead . The Executor (see ) is responsible for carrying out the execution of the tasks specified inthe Task Graph produced by the Orchestrator. The Executor interacts with various componentssuch as the Delegator, Agents, and Tool Manager, managing both intra-task and inter-task memorybuffers to ensure efficient task execution. It processes tasks while respecting both direct and indirectdependencies within the task graph, optimizing for parallel execution where possible while ensuringthe correct task order when dependencies exist. In terms of execution, tasks that are independent of each other can be run concurrently, enablingparallelism where possible to maximize efficiency, an example of which is shown in Appendix B.For tasks with direct dependencies, sequential execution is required to ensure that input data frompredecessor tasks is available before execution proceeds. Indirect dependencies, where tasks dependon the results of other tasks via intermediates, are also managed by the system to enforce the correctexecution order.",
  ": Execution flow managed by the Executor showcasing a simpler task flow on the left and amore complex flow on the right": "The execution flow in illustrates two typical scenarios. On the left side, a simpler case ispresented where tasks in a task graph are grouped and executed sequentially (Task A Task B Task C) with direct dependencies, while Task D is executed in parallel. On the right side, a morecomplex scenario shows Task A being executed in parallel, while Tasks B, C, and D have both directand indirect dependencies, requiring a combination of sequential and parallel execution.",
  "Evaluation Framework": "Evaluating agentic systems necessitates a multidimensional analysis of all the individual componentsas well as the final output. Our approach focuses on evaluating intermediate steps as well as finaloutcomes, using a robust set of metrics designed to assess the systems ability to decompose tasks,select appropriate tools, and execute tasks effectively. This evaluation framework is grounded inexisting literature, including the gaps identified by Gioacchini et al., Shen et al., and Liuet al., which highlight the need for a more granular evaluation of agentic behaviors in LLMs.To address these gaps, we developed a comprehensive evaluation dataset inspired by Lin et al.,integrating task graphs, tool usage and final outputs. This dataset supports a more nuanced analysisof agentic systems by capturing both intermediate processes and final results, ensuring a holisticevaluation of system performance.",
  "Dataset Creation": "Our dataset is specifically designed to evaluate the agentic behavior of LLM-driven systems acrossvarious domains and tasks. The dataset creation process was structured to ensure representativeness,and relevance to real-world scenarios based on the AsyncHow dataset. The AsyncHow datasetwas particularly suited for this study due to its unique structure that covers parallel, and sequentialtask graphs. This comprehensive coverage allows for a thorough evaluation of agentic systems, whichneed to handle different types of task relationships and dependencies. The datasets design, whichincludes a mix of simple linear workflows and more intricate interdependent tasks, supports thecomprehensive evaluation of the systems ability to manage these varying complexities effectively.The AsyncHow datasets validation for each scenario within the parallel and sequential categories oftask graphs ensures that it is a robust foundation for evaluating the proposed agentic systems. As thisis one of the most important steps in an agentic system we found it ideal to use it as a foundationaldataset. Below is a detailed breakdown of the steps involved. Task Graph Construction: We randomly sampled 50 task graphs from the AsyncHow dataset,ensuring diversity while maintaining empirical manageability for the evaluation of agentic systems.By selecting 50 scenarios for the parallel and sequential task graph category, the study ensured thatthe dataset remains robust yet feasible for in-depth analysis. The empirical soundness of selecting50 scenarios, resulting in more than 250 tools, was determined to be sufficient for evaluating thecorrectness of tool selection and the overall performance of the agentic systems. Tool Function Generation: Tool descriptions were parsed and translated into synthetic Pythonfunctions. These functions were designed to replicate the behavior of real-world tools, ensuring thatthe agents interactions with these tools are realistic and contextually appropriate. These functionscontribute to creating a realistic and challenging environment for the agentic system as they spana variety of tasks, such as simulating the return of data as JSON from API calls or the data from acandidate for a potential job interview. The functions were then executed to generate final responsesfor each scenario, providing a benchmark for evaluating the systems performance. Final Dataset Composition: The final dataset includes a comprehensive set of components: sce-nario names, task graphs, tool functions, expected tool call sequences, gold standard responses,and complexity categories for each scenario. This structure facilitates a detailed evaluation ofboth intermediate steps and final outcomes, enabling a more nuanced assessment of the agentsperformance.",
  "Evaluation Metrics": "To rigorously evaluate the agentic systems performance, we employ a set of metrics that measure theaccuracy and effectiveness of task decomposition, tool selection, and task execution. These metricsprovide a comprehensive assessment of the systems capabilities, ensuring a thorough evaluation oftask graphs, tool identification, and answer generation accuracy. Precision, Recall, and F1 Score are core metrics used to evaluate the accuracy and completenessof tool identification. The precision score reflects the systems ability to correctly identify relevanttools without including false positives, while recall measures its success in identifying all relevantelements. The F1 Score balances precision and recall, providing a comprehensive view of the systemsperformance in identifying tools.",
  "(u,v)V1V2exp ( |dG1(u, v) dG2(u, v)|)(3)": "Complexity Score: The complexity score as defined by as the total number of vertices andedges within a task graph: |V | + |E|, where V is the number of nodes and E is the number of edges.This metric provides a measure of the graphs structural complexity and helps compare task graphs ofvarying sizes.",
  "Results and Discussion": "Upon investigating our agentic framework we noticed that explicitly mentioning decompositionstrategies (coarse grained vs fine grained) allowed the system to adapt based on the complexity andinterdependence of tasks extracted from a user query and saw a significant improvement in taskaccuracy and reduction in inefficiency - lesser amount of redundant tasks were produced. This isin agreement with , which highlight the importance of multi-granularity approaches in AIframeworks, particularly in complex domains like multi-hop question answering. We conducted an in-depth analysis of the proposed metrics on our agentic system to understand theirimpact on the systems performance for both sequential and parallel tasks. The analysis revealedthat structural and node-level metrics are more critical in sequential tasks, while tool-related metricsare prominent in parallel tasks. Our choice of evaluation metrics is validated by prior research onLLMs and their performance in graph-based tasks. Studies have shown that combining precision andrecall, such as in the Node F1 Score, is essential for accurately evaluating systems that interact withcomplex graph structures, as it minimizes false positives and negatives . The Structural SimilarityIndex (SSI) has also been validated as a reliable metric for assessing the preservation of both thefunctional and structural aspects of task graphs . The Structural Similarity Index (SSI) emerged as the most significant predictor of Answer Score,with a strong positive correlation (r = 0.470, p < 0.001). Plot can be seen in the Appendix in. Node Label Similarity also showed a substantial positive correlation (r = 0.447, p <0.01). Interestingly, Expected Task Complexity exhibited a moderate negative correlation (r = -0.293, p < 0.05), suggesting that as tasks become more complex, performance tends to decrease.These correlations were further reflected in real-world applications of the agentic system, wherescenarios requiring fine-grained task decomposition revealed high Node Precision and Recall, but alsohighlighted challenges with managing complex dependencies, as indicated by lower Edge F1 Scores.When considering the absolute coefficients from our linear regression model, SSI again emerged asthe most important feature, followed by Edge F1 Score and Node Label Similarity. This aligns withour correlation analysis, emphasizing the importance of structural accuracy in sequential tasks. Themodel achieved an R-squared value of 0.3631, indicating that these features explain approximately36.31% of the variance in Answer Score for sequential tasks. For parallel tasks, we observed a shift in the importance of metrics, with tool-related features gainingprominence. Plots can be seen in Appendix A in . Tool F1 Score showed the strongestcorrelation with Answer Score (r = 0.476, p < 0.001), closely followed by Tool Recall (r = 0.474, p <0.01) and Tool Precision (r = 0.414, p < 0.01). SSI and Node Label Similarity both demonstratedmoderate positive correlations (r = 0.380, p < 0.05 for both). Interestingly, our regression analysisrevealed that SSI and Node Label Similarity had the highest importance scores, despite not havingthe strongest correlations. This suggests a complex interplay between these structural features andtool-related metrics in parallel tasks. The model for parallel tasks achieved an R-squared value of0.3933, explaining 39.33% of the variance in Answer Score. Our findings highlight differences in the factors influencing agentic system performance between se-quential and parallel tasks. Structural metrics (SSI and Node Label Similarity) are important acrossboth task types, but their relative importance is higher in sequential tasks. Practical application of ourevaluation framework also supports these findings, where task decomposition in real-world scenariosdisplayed strong structural performance but revealed weaknesses in dependency management (asseen in lower Edge F1 Scores). Tool-related metrics (Tool F1 Score, Recall, and Precision) aremore strongly correlated with performance in parallel tasks, suggesting that effective tool selectionand usage become critical in more complex, non-linear task structures. Edge-related metrics (EdgePrecision and Edge F1 Score) show some importance in sequential tasks but are not significant inparallel tasks, possibly due to the more complex relationships between nodes in parallel structures.",
  "Limitations": "While the proposed agentic framework offers significant advancements in evaluating agentic systems,it faces several limitations. A primary issue is the lack of support for multi-agent communication,which limits the frameworks effectiveness in scenarios requiring task coordination among multipleagents. The system is only suited for single-agent environments or cases where agents operate",
  "independently. Expanding this capability would significantly enhance its versatility in complexsettings": "Precision, Recall, and F1 Scoreare also sensitive to dataset imbalances. When there are very fewor many expected tools, these metrics may not capture performance nuances effectively, leadingto skewed results. Another limitation is that these metrics do not account for the context in whichthe tools are applied. A tool may be identified correctly, but if used in an inappropriate context,this error is not reflected in the metrics, potentially leading to inaccurate performance assessments.Calculating Graph Edit Distance (GED) is computationally intensive, especially for large graphs, andthis sensitivity of GED to cost, along with the ambiguity in its interpretation, reduces its effectivenessin evaluating complex task graphs. The matching of nodes and edges can suffer when multiplenodes or edges possess similar labels, which can require manual intervention or the implementationof additional rules to resolve conflicts. This increases the complexity of the task and reduces thereliability of fully automated methods. Moreover, edge matching is heavily dependent on the accuracyof node matching, which means that any errors in node similarity evaluations can propagate andnegatively affect the assessment of task dependencies. The Structural Similarity Index (SSI) alsopresents certain drawbacks. First, it is highly sensitive to node similarity scores, which can beproblematic when nodes - representing tasks - are described in significantly different ways. Thiscan distort the overall similarity measure. Additionally, SSI assigns equal weight to node andedge similarities, which may not be appropriate in all cases. In some situations, task dependencies,represented by edges, may be more important than the individual tasks themselves, and equalweighting could overlook this context. Finally, SSI computation, particularly for large graphs can becomputationally expensive, further complicating its application in large-scale evaluations.",
  "Future Work": "Building on the limitations identified, future research should focus on the following. First, theintegration of multi-agent communication protocols would allow for dynamic collaboration, enablingthe system to handle more complex, real-time scenarios that require the collaboration of multipleagents. Additionally, introducing causal inference methods to the evaluation framework would offerdeeper insights into system performance by moving beyond correlation-based metrics. Optimizingscalability remains a critical challenge, especially for large-scale real-time environments, wherereducing latency and computational overhead is essential. Finally, developing more sophisticateddatasets that reflect real-world randomness and multi-agent interactions is crucial for testing andimproving the scalability of agentic systems. Refer to Appendix C for detailed concrete futureresearch directions to extend this work.",
  "Conclusion": "Analysis of our agentic framework revealed that task graph based decomposition and explicitlyintegrating coarse-grained and fine-grained decomposition strategies led to improved task accuracyand a reduction in inefficiencies by minimizing redundant tasks. This adaptability, driven by thecomplexity and interdependence of user query tasks, highlights the critical role of multi-granularityapproaches in agentic systems. Moreover, while these strategies enhanced system performance,challenges in managing complex dependencies, especially in fine-grained tasks, became evident. Our work also presents a comprehensive framework for evaluating agentic systems driven by LLMs.We introduce proper metrics, including the Node F1 Score, Structural Similarity Index, and ToolF1 Score, to assess the performance of these systems in task decomposition and tool integrationscenarios. Additionally, our specialized dataset created for this study enables an in-depth analysisof agentic behavior across various task complexities. Our findings emphasize the importance ofboth structural and tool-related metrics in determining overall system performance, with notabledifferences between sequential and parallel tasks. While structural metrics are more critical insequential tasks, tool usage becomes more significant in parallel tasks, highlighting the need forbalanced evaluation methods that capture both the structural and operational aspects of agenticsystems. The proposed framework and evaluation methodology provide a strong foundation forfurther research. Addressing the outlined limitations and pursuing the proposed future work will becritical in advancing agentic systems capable of handling more complex, real-time, and collaborativeenvironments.",
  "G. Chen, S. Dong, Y. Shu, G. Zhang, J. Sesay, B. F. Karlsson, J. Fu, and Y. Shi. Autoagents: Aframework for automatic agent generation. arXiv preprint arXiv:2309.17288, 2023": "W. Chen, Y. Su, J. Zuo, C. Yang, C. Yuan, C.-M. Chan, H. Yu, Y. Lu, Y.-H. Hung, C. Qian, et al.Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors. In TheTwelfth International Conference on Learning Representations, 2023. N. Crawford, E. B. Duffy, I. Evazzade, T. Foehr, G. Robbins, D. K. Saha, J. Varma, andM. Ziolkowski. Bmw agentsa framework for task automation through multi-agent collaboration.arXiv preprint arXiv:2406.20041, 2024.",
  "Y. Shen, K. Song, X. Tan, W. Zhang, K. Ren, S. Yuan, W. Lu, D. Li, and Y. Zhuang. Taskbench:Benchmarking large language models for task automation. arXiv preprint arXiv:2311.18760,2023": "H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozire, N. Goyal,E. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. arXivpreprint arXiv:2302.13971, 2023. L. Wang, C. Ma, X. Feng, Z. Zhang, H. Yang, J. Zhang, Z. Chen, J. Tang, X. Chen, Y. Lin,W. X. Zhao, Z. Wei, and J. Wen. A survey on large language model based autonomous agents.Frontiers of Computer Science, 18(6), 2024.",
  "Q. Wu et al. Autogen: Enabling next-generation large language model applications. MicrosoftResearch, 2024": "Z. Xi, W. Chen, X. Guo, W. He, Y. Ding, B. Hong, M. Zhang, J. Wang, S. Jin, E. Zhou,R. Zheng, X. Fan, X. Wang, L. Xiong, Y. Zhou, W. Wang, C. Jiang, Y. Zou, X. Liu, Z. Yin,S. Dou, R. Weng, W. Cheng, Q. Zhang, W. Qin, Y. Zheng, X. Qiu, X. Huang, and T. Gui. Therise and potential of large language model based agents: A survey, 2023.",
  "S1.1Replicating the Agentic Task Decomposition Framework": "This section provides a detailed guide for replicating the agentic task decomposition framework,including Python function shells with brief descriptions. This guide assumes familiarity with Python,machine learning, and related libraries. Our framework decomposes user queries into a Directed Acyclic Graph (DAG) of tasks and uses anLLM for tool selection and execution. The system components include the Orchestrator, Delegator,ToolManager, and GraphExecutor. These work together to generate tasks, select tools, and ensureparallel or sequential execution.",
  "The TimingProfiler links to both the GraphExecutor and the FeedbackSystem for detailedtime tracking of task execution and feedback generation": "The Pipeline class orchestrates the entire framework, ensuring proper initialization, task decom-position, and execution. It interacts with the Orchestrator, ToolManager, GraphExecutor, andFeedbackSystem to process user queries. class Pipeline:def __init__(self, semantic_tool_filtering=True,include_indirect_dependencies=True,generate_feedback=True,profile_execution_timings=True):\"\"\"Initializes the Pipeline with various options, including:- Semantic tool filtering:Filters tools based on task relevance.- Include indirect dependencies:Includes results from all ancestor tasks.- Generate feedback:Provides real-time feedback during task execution.- Profile execution timings:Tracks the execution time for tasks and tools.\"\"\"self.semantic_tool_filtering = semantic_tool_filteringself.include_indirect_dependencies = include_indirect_dependenciesself.generate_feedback = generate_feedbackself.profile_execution_timings = profile_execution_timingsself.tool_manager = ToolManager()self.graph_executor = Noneself.orchestrator = Orchestrator(gpt_client_manager)self.feedback_system = None def configure_for_query(self, user_query):\"\"\"Configures the pipeline for a new user query.This includes initializing the task graph,selecting tools, and setting upfeedback and profiling options.\"\"\"task_graph = self.orchestrator.produce_task_graph(user_query)self.graph_executor = GraphExecutor(self.orchestrator.client,self.include_indirect_dependencies,self.profile_execution_timings)self.graph_executor.initialize_tool_manager(self.tool_manager)self.graph_executor.initialize_task_graph(task_graph)",
  "S1.1.3Prompt Skeletons": "The framework uses specific prompts to interact with the LLM (GPT-4). These prompts handle taskgraph generation, task execution, tool selection, and user feedback. Below is a skeleton of the mainprompts from the system. PROMPT_TASK_GRAPH = \"\"\"You are responsible for generating a task graphfrom the following user query. Decompose the queryinto individual tasks and create a Directed Acyclic Graph (DAG)with nodes as tasks and edges as dependencies.Ensure there are no cyclic dependencies.",
  "BAdditional plots for timing profiling of parallel execution of task graphs": "shows the specific timing of tasks during a parallel execution scenario. This timing diagramdemonstrates how independent tasks (e.g., Task 1 and Task 4) are executed in parallel, while tasks thathave dependencies (e.g., Task 2 and Task 3) are scheduled sequentially, respecting their dependencies.The timing diagram demonstrates the frameworks ability to efficiently manage parallel executionwhile ensuring dependent tasks are executed in the correct order.",
  "Multi-Agent Communication: Developing and integrating multi-agent communicationcapabilities to enhance the collaborative aspect of our framework. This would allow for": ": Task execution timings for parallel execution. The diagram shows the start and end timesfor four tasks (Task 1, Task 2, Task 3, and Task 4). Tasks 1 and 4, which are independent, are executedin parallel, while Task 2 starts after Task 4, and Task 3 follows Task 2, showing sequential executiondue to dependencies.",
  "more complex task execution scenarios where multiple agents must coordinate and shareinformation": "Causal Influence Score (CIS): Developing the CIS metric to evaluate the causal relation-ships between the tools used and the final outcomes or answers generated by the agent.Unlike traditional precision and recall metrics, CIS would focus on understanding how eachtools use causally impacts overall task success. Dynamic Adaptation Metric (DAM): Introducing the DAM metric to measure an agentsability to dynamically adapt its tool selection and task execution strategies in response tochanging conditions or feedback during task execution. Inter-tool Dependency Metric (ITDM): Creating the ITDM metric to evaluate the degreeof dependency between tools used by the agent within a task. This metric would capturewhether the agent effectively coordinates between multiple tools or relies too heavily oncertain tools. Task Graph Robustness Index (TGRI): Implementing the TGRI metric to measure therobustness of the task graph generated by the agent against errors or changes. This metricwould evaluate how small modifications to the input or environment affect the task graphand its resulting accuracy. Contextual Consistency Score (CCS): Developing the CCS metric to measure how con-sistently an agent applies tools and constructs task graphs in contexts similar to previouslyencountered scenarios. This would assess the agents ability to generalize across similartasks. Explainability Score (ES): Introducing the ES metric to measure how well the agent canexplain its decisions, particularly its choice of tools and task graph construction. This metricwould be crucial for ensuring transparency and building trust in agentic systems. Cumulative Learning Rate (CLR): Developing the CLR metric to measure how quicklyand effectively the agent improves its performance over time as it encounters more tasks.This would capture the agents learning efficiency and its ability to adapt and improve. Interaction Cost Metric (ICM): Creating the ICM metric to evaluate the efficiency ofan agents interactions in terms of resource usage, such as time, computational power, orenergy, in achieving a task. This metric would be vital for optimizing the resource efficiencyof agentic systems. These future directions aim to build on our current findings, enhancing both the theoretical andpractical aspects of agentic system design and evaluation. By developing these additional metrics andcapabilities, we hope to provide a more comprehensive framework for assessing and improving theperformance of agentic systems in increasingly complex environments."
}