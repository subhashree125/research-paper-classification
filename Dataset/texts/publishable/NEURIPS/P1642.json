{
  "Abstract": "We introduce the Autoregressive PDE Emulator Benchmark (APEBench), a com-prehensive benchmark suite to evaluate autoregressive neural emulators for solvingpartial differential equations. APEBench is based on JAX and provides a seamlesslyintegrated differentiable simulation framework employing efficient pseudo-spectralmethods, enabling 46 distinct PDEs across 1D, 2D, and 3D. Facilitating systematicanalysis and comparison of learned emulators, we propose a novel taxonomy forunrolled training and introduce a unique identifier for PDE dynamics that directlyrelates to the stability criteria of classical numerical methods. APEBench enablesthe evaluation of diverse neural architectures, and unlike existing benchmarks, itstight integration of the solver enables support for differentiable physics trainingand neural-hybrid emulators. Moreover, APEBench emphasizes rollout metrics tounderstand temporal generalization, providing insights into the long-term behaviorof emulating PDE dynamics. In several experiments, we highlight the similari-ties between neural emulators and numerical simulators. The code is available at and APEBench can be installed viapip install apebench.",
  "Introduction": "The language of nature is written in partial differential equations (PDEs). From the behavior ofsubatomic particles to the earths climate, PDEs are used to model phenomena across all scales.Typically, approximate PDE solutions are computed with numerical simulations. Almost all relevantsimulation techniques stem from the consistent discretization involving symbolic manipulations ofthe differential equations into a discrete computer program. This laborious task yields algorithms thatconverge to the continuous dynamics for fine resolutions. For realistic models, established techniquesrequire immense computational resources to attain high accuracy. Recent advances of machinelearning-based emulators challenge this. Purely data-driven or with little additional constraints andsymmetries, neural networks can surpass traditional methods in the accuracy-speed tradeoff (Kochkovet al., 2021; List et al., 2022; Lam et al., 2022). The field of neural PDE solvers advanced rapidly over the past years, applying convolutional archi-tectures (Tompson et al., 2017; Thuerey et al., 2020; Um et al., 2020), graph convolutions (Pfaffet al., 2021; Brandstetter et al., 2022), spectral convolutions (Li et al., 2021), or mesh-free approaches(Ummenhofer et al., 2020; Wessels et al., 2020) to replace or enhance classical numerical solvers.However, the relationship between classical solvers, which supply training data, and neural emulators,",
  ": APEBench provides an efficient pseudo-spectral solver to simulate 46 PDE dynamicsacross one to three spatial dimensions. Shown are examples visualized with APEBenchs customvolume renderer": "which attempt to emulate their behavior, is often underexplored. For example, convolutional networksbear a strong resemblance to finite difference methods, and spectral networks can be thought of aspseudo-spectral techniques (McCabe et al., 2023). These parallels suggest that a better understandingof this interplay could help inform how emulator architectures are designed and how effectivelyneural emulators can learn from classical solvers. To help address these questions, we introduce APEBench, a new benchmark suite designed tocomplement existing efforts in evaluating autoregressive neural emulators for time-dependent PDEs.While previous benchmarks such as PDEBench (Takamoto et al., 2022) and PDEArena (Guptaand Brandstetter, 2023) have provided valuable insights into architectural comparisons based onfixed datasets, APEBench aims to extend these efforts by focusing on emulator-simulator interactionvia supporting neural-hybrid approaches and emphasizing training using differentiable physics.Additionally, we place particular focus on unrolled training and rollout metrics, which have been lesssystematically explored in other benchmarks. The key innovation of APEBench lies in its tight integration of a highly efficient pseudo-spectralsolver. This method is used both for procedural data generation and as a differentiable solver thenetworks can dynamically interact with during training. APEBench offers four key contributions:",
  "Taxonomy and Metrics for Unrolling Methodologies: We propose a broad and systematicframework for analyzing the impact of different training paradigms on emulator performance": "Our benchmark includes recipes that rapidly adapt to new architectures and training methodologies.Datasets are re-generated procedurally (and deterministically) in seconds on modern hardware. Thisavoids the need to distribute fixed datasets, improving adoption, and allows for quick modification ofthe underlying phyiscs. For visual analysis of the emergent structures, the benchmark is accompaniedby a fast volume visualization module. This module seamlessly interfaces with the PDE dynamics toprovide immediate feedback for emulator development in 2D and 3D.",
  "tu + cxu = 0withu(t, x = 0) = u(t, x = L),": "which admits an analytical solution where the initial condition moves with c over the domain = (0, L). Let Ph represent a discrete analytical time stepper that operates on a fixed resolutionwith N equidistantly spaced degrees of freedom and time step size t. This simulator advancesa discrete state uh RN to a future time, i.e., u[t+1]h= Ph(u[t]h ). Emulator learning is the task ofapproximating this time stepper using a neural network f Ph.",
  "f(uh) = w uh,": "where denotes cross-correlation. We framefinding w = [center, right]T R2 to approxi-mate Ph with f as a data-driven learning prob-lem, using trajectories produced by the analyt-ical time stepper. If the neural emulator pre-dicts one step into the future, the learning prob-lem over the two-dimensional parameter space R2 becomes convex. Since even-sized con-volutions are typically biased to the right, onecould suspect that the learned minimum of sucha problem is given by the first-order upwind(FOU) method. This numerical (non-analytical)time stepper is found via a consistent approachto discretizing the advection equation using aTaylor series. If we assume c < 0, it leads tocenter = 1 + c t",
  "x and right = c t": "x. However,despite convexity the learned solution is differ-ent. In , we benchmark the long-termperformance of the learned emulator relative to the FOU scheme. It is superior to the numericalmethod, with lower errors for the first 13 steps. Eventually, it diverges because it is not consistent.We can improve the long-term performance of the emulator by training it to predict multiple stepsautoregressively. We call this approach unrolled training in the following; it more closely aligns withthe inference task of long-term accuracy. Indeed, doing so enhances the performance for a smallsacrifice in short-term accuracy. The learned emulator improves in temporal generalization, i.e., itruns stably and accurately for more time steps. For example, when unrolling for 20 steps duringtraining, the learned solution still performs better after 30 steps while having an 11% increased errorat the first step. In the two-dimensional parameter space, more unrolling moves the learned stencilcloser to the FOU scheme. The distance reduces from 0.034 to 0.024 to 0.01 for 1-step, 10-step, and50-step training, respectively. This indicates that unrolling successfully injects knowledge aboutbecoming a good simulator.",
  "The FOU stencil depends only on the Courant-Friedrichs-Lewy (CFL) number c t": "x. It represents away to assess the difficulty of the advection problem. With APEBench, we generalize it and computesimilar stability numbers as intuitive identifiers for all available dynamics. By doing so, we obtain aminimal set of information to describe an experiment, which serves as an exchange protocol in ourbenchmark suite. Our motivational example reveals that even combining simple (linear) PDEs and linear emulatorsleads to non-trivial learning tasks offering interesting insights. We also see that the emulatorsshare similarities with classical numerical methods. In this case, there is a strong relation betweenconvolutional networks and finite difference methods. However, since the emulators free parametersresult from a data-driven optimization problem, not a human-powered symbolic manipulation, theymay deviate from the strict assumptions underlying traditional schemes. Learned emulators can use this to their advantage, and outperform their numerical counterparts for a specific operating regime,i.e., a certain test rollout length. Since this superiority varies with unrolled training steps and iscertainly dependent on the underlying dynamics (in terms of its difficulty), we emphasize that usingrollout metrics is important to understand the temporal behavior of neural emulators. In summary,APEBench provides a suite that holistically assesses all ingredients of the emulator learning pipeline,including a highly accurate solver.",
  "Related Work": "Neural PDE SolversEarly efforts in neural PDE solvers focused on learning the continuoussolution function for an (initial-)boundary value problem via a coordinate network (Dissanayakeand Phan-Thien, 1994; Lagaris et al., 1998). With the rise of automatic differentiation capabilitiesin modern machine learning frameworks, this approach experienced a resurgence under the nameof Physics-Informed Neural Networks (PINNs) (Raissi et al., 2019). However, PINNs do not useautoregressive inference. Early works on stationary neural emulators include solutions to the pressurePoisson equation (Tompson et al., 2017) and fluid simulations (Thuerey et al., 2020). Notableworks employing the autoregressive paradigm are Brandstetter et al. (2022) using supervised datafrom numerical simulators. Successful unsupervised approaches for autoregressive emulators areGeneva and Zabaras (2020) and Wandel et al. (2021). Seminal works highlighting the supremacy ofneural-hybrid emulators are Um et al. (2020) and Kochkov et al. (2021). Oftentimes, the architecturesemployed are inspired by image-to-image tasks in computer vision. A closely-related line of workutilizes neural operators (Kovachki et al., 2023) which impose stronger mathematical requirements onemulators and their architectures, such as representation-equivariance (Bartolucci et al., 2023; Raonicet al., 2023). The most popular operator architecture in autoregressive settings is the Fourier NeuralOperator (FNO) (Li et al., 2021) with numerous modifications and improvements existing (Tran et al.,2023; McCabe et al., 2023). Like implicit methods for numerical simulators, autoregressive modelscan have internal iterations. For example, this includes autoregressive diffusion models (Kohl et al.,2023) or iterative refinements (Lippe et al., 2023). Neural Emulator Benchmarks and DatasetsNotable benchmark papers comparing emulatorarchitectures are PDEBench (Takamoto et al., 2022) and PDEArena (Gupta and Brandstetter, 2023).Benchmarks based on more complicated fluid simulations include Luo et al. (2023), Bonnet et al.(2022), and Janny et al. (2023). BubbleML (Hassan et al., 2023) focuses on two-phase boilingproblems. All the aforementioned benchmark papers release fixed and pre-computed datasets.APEBench is the first to tightly integrate an efficient reference solver, which procedurally generatesall data. Moreover, we thereby uniquely enable benchmarking approaches involving differentiablephysics, e.g., neural-hybrid emulators under unrolled training. Another notable benchmark for ordinary differential equations and deterministic chaos is Gilpin(2021). This benchmark shares our goal of relating the (temporal) performance of emulators withcharacteristic properties of the underlying dynamics, focusing on ODEs instead of PDEs. Beyond works dedicated to benchmarks, several datasets from seminal papers gained popularity inthe community. This includes data on the Burgers equation, Kolmogorov flow and Darcy problemused in the FNO paper (Li et al., 2021). Also, the Kolmogorov trajectories of Kochkov et al. (2021)are widely used. Other examples include the datasets of the DeepONet paper (Lu et al., 2021a), alsoused as part of the DeepXDE library (Lu et al., 2021b). Data Generators and Differentiable Physics Physics-based deep learning often utilizes simplesimulation suites with high-level interfaces like JAX-CFD (Kochkov et al., 2021), PhiFlow (Hollet al., 2020), JAX-MD (Schoenholz and Cubuk, 2020) or Warp (Macklin, 2022). Our referencesolver is based on Fourier pseudo-spectral ETDRK methods for which there is currently no equallycomprehensive package available in the Python deep learning ecosystem. For the problems that fit intothe methods constraints, it is one of the most efficient approaches (Montanelli and Bootland, 2020).Writing numerical solvers in deep learning frameworks provides discrete differentiability, beneficiallyused in recent research (Um et al., 2020; Kochkov et al., 2021). Existing non-differentiable simulationsoftware typically serves as reference generators for purely data-driven approaches. Popular examplesare the Dedalus library in Python (Burns et al., 2020) and the Chebfun package in MATLAB (Driscollet al., 2014). Fluid related work often employs OpenFoam (Weller et al., 1998).",
  "tu = Lu + N(u),": "where the linear differential operator L contains a higher order derivative than the non-linear operatorN(). This includes linear dynamics like advection, diffusion, and dispersion, and it also coverspopular nonlinear dynamics like the viscid Burgers equation, the Korteweg-de Vries equation, theKuramoto-Sivashinsky equation, as well as the incompressible Navier-Stokes equations for lowto medium Reynolds numbers. Additionally, we consider reaction-diffusion equations like theFisher-KPP equation, the Gray-Scott model, and the Swift-Hohenberg equation, demonstrating theapplicability beyond fluid-like problems. The continuous form of all supported dynamics is given in and a visual overview can be found in . For semi-linear PDEs, the class of Exponential Time Differencing Runge-Kutta (ETDRK) methods,first formalized by Cox and Matthews (2002), are one of the most efficient solvers known today(Montanelli and Bootland, 2020). Under periodic boundary conditions, a Fourier pseudo-spectralapproach allows integrating the linear part L exactly via a (diagonalized) matrix exponential. As such,any linear PDE with constant coefficients can be solved analytically, without any temporal or spatialdiscretization error. This makes it possible to reliably and accurately evaluate the correspondinglearning tasks. The non-linear part N() is approximated by a Runge-Kutta method. We elaborate onthe methods motivation, implementation, and limitations in appendix B. For semi-linear problemswith stiffness arising from the linear part, ETDRK methods show excellent stability and accuracyproperties. Ultimately, the cost of one time step in D dimensions is bounded by the Fast FourierTransform (FFT) with O(N DD log(N)). Due to pure explicit tensor operations, the method is wellsuited for GPUs, and discrete differentiability via automatic differentiation is straightforward. PDE Identifiers The ETDRK solver suite operates with a physical parametrization by specifying thenumber of spatial dimensions D, the domain extent L, the number of grid points N, the step size tand constitutive parameters, like the velocity c in case of the advection equation. All these parametersaffect the difficulty of emulation and must, hence, be communicated when evaluating the forecastingof a specific PDE. As an exchange protocol or identifier of an experiment, APEBench also comeswith a reduced interface tailored to identifying the present dynamics, including its discretization, in aminimal set of variables. This allows assigning an ID to each scenario in APEBench, which uniquelyexpresses the discrete dynamics to be emulated. For the s-th order linear derivative with coefficientas, we define two coefficients s and s as",
  "Lsands = sN s2s1D.(1)": "For a specific scenario, the s represent normalized dynamics coefficients, while the s quantifythe difficulty. Gamma values correspond to the stability criteria of the most compact explicit finitedifference stencils of the respective linear derivative (for s = 1, this is the CFL condition). Together,these values make it possible to quickly gain intuition about the dynamics encoded by a chosenPDE system and provide a convenient way to work with different scenarios: A list of gamma (oralpha) values together with N and D is sufficient to uniquely identify any linear dynamics. We havediffusion if only 2 0. For 1 = 0 2 0, we obtain advection-diffusion, while only 3 = 0yields dispersion. The linear derivatives can be combined with a selection of nonlinear components to vary the systemsdynamics, on whose reduction we elaborate in appendix B.7 and B.8. Combining a convectionnonlinearity with 2 0 results in the viscous Burgers equation. If further combined with 3 = 0,the Korteweg-de Vries equation is obtained. Thus, the non-zero coefficients define the type ofdynamics. Their relative and absolute scales define how challenging the emulator learning problem is.With this approach, APEBench intuitively communicates the emulated dynamics.",
  "Neural Emulator ArchitecturesOur benchmark encompasses established neural architecturesadaptable across spatial dimensions and compatible with Dirichlet, Neumann, and periodic boundary": "conditions. This includes local convolutional architectures like ConvNets (Conv) and ResNets (Res)(He et al., 2016) as well as long-range architectures like UNets (Ronneberger et al., 2015) andDilated ResNets (Dil) (Stachenfeld et al., 2021). Orthogonal to the two former classes, we considerpseudo-spectral architectures in the form of the Fourier Neural Operator (FNO), which have a globalreceptive field, but their performance instead depends on the spectrum of the underlying dynamics. Training MethodologiesEmulator training is the task of approximating a discrete numericalsimulator Ph. This solver advances a space-discrete state uh from one time step to the next. Thegoal is to replicate its behavior with the neural emulator f, i.e., to find weights such that f Ph.Since the neural emulator f is trained on data from the numerical simulator Ph, their interplayduring learning is crucial. Many options exist, like one-step training (Tran et al., 2023), supervisedunrolling (Um et al., 2020), or residuum-based unrolling (Geneva and Zabaras, 2020). We introducea novel taxonomy based on unrolling steps during training and the reference branch length, unifyingmost approaches in the literature. Given a dataset of states uh Dh, the objective is",
  ",(2)": "where T is the number of unrolled steps at training time, and the per time step loss l(, ) typically isa mean squared error (MSE). During training, the emulator produces a trajectory {f t+b}, which wecall the main chain. The variable B denotes the length of the branch chain {Pbh()}, which defineshow long the reference simulator is rolled out next to the main chain. The popular one-step supervised learning problem is recovered by setting T = B = 1. Purelysupervised unrolling is achieved with T = B. In such a case, all data can be pre-computed, allowingthe reference simulator Ph to be turned off during training. We also consider the case of divertedchain learning with T freely chosen and B = 1 providing a one-step difference from a referencesimulator while maintaining an autoregressive rollout. This configuration necessitates the referencesimulator Ph to be differentiable which is readily available within APEBench. While our resultsbelow do not modify the gradient flow (Brandstetter et al., 2022; List et al., 2022), the benchmarkframework supports alterations of the backpropagation pass, as outlined in Appendix D.1. Neural-Hybrid EmulationNext to the task of fully replacing the numerical simulator Ph withthe neural network f, which we call prediction, APEBench is also designed for correction. Forthis, we introduce a coarse solver Ph that acts as a predictor together with a corrector network fh(Um et al., 2020; Kochkov et al., 2021). Together, they form a neural-hybrid emulator. For example,we support a sequential layout in which the f of equation 2 is f = f( Ph). The coarse solvercomponent Ph is also provided by the ETDRK solver suite. Any unrolling with T 2 introducesa backpropagation-through-time that requires this coarse solver to be differentiable, which is alsoreadily available. Metrics Since this benchmark suite is concerned with autoregressive neural emulator, we emphasizethe importance of rollout metrics. To compare two states u[t]h and ur,[t]hat time level [t], we provide arange of established metric functions, which we elaborate in appendix F. This includes aggregationsin state and Fourier space using different reductions and normalization. Moreover, we support metriccomputation for certain frequency ranges and the use of derivatives (i.e., Sobolev-based metrics) tohighlight errors in the smaller scales/higher frequencies for which networks often produce blurrypredictions (Rahaman et al., 2019).",
  "Experiments": "We present experiments highlighting the types of studies enabled by APEBench, focusing on temporalstability and generalization of trained emulators. We measure performance in terms of the normalizedRMSE (see equation (31)) to allow comparisons over time if magnitudes decay and across dynamics.Aggregation over time is done with a geometric mean (see equation (32)). Plots show the medianperformance across network initializations, with error bars for the 50% inter-quantile range (IQR).Further specifics are provided in appendix H.",
  "Bridging Numerical Simulators and Neural Emulators": "The motivational example in section 2 demonstrated the emulation of 1D advection for CFL 1 < 1.We saw that purely linear convolutions with two learnable parameters are capable emulators. Thissection expands the experiment by training various nonlinear architectures on this linear PDE forvarying difficulties. Since the CFL condition (|1| < 1) restricts information flow across one cell forfirst-order methods, we hypothesize that emulators with larger receptive fields can handle difficultiesbeyond one. Indeed, when considering standard feedforward convolutional architectures of varyingdepths, there is a clear connection between the effective receptive field and the highest possibledifficulty. With a kernel size of three, each additional stacked convolution adds a receptive field ofone per direction. A depth of zero represents a linear convolution. As shown in (a), suchan approach is only feasible for 1 1, aligning with the CFL stability criterion of the first-orderupwind method and the results from section 2. Beyond this, the network fails to emulate the dynamicsand diverges almost immediately. A similar behavior is observed for a convolution depth of onewith an effective receptive field of two per direction. This is insufficient for advection dynamics ofdifficulty 1 = 2.5. Long-range convolutional architectures, like UNets and Dilated ResNets, perform better acrossthe difficulties, i.e., the error rollout does not steepen as strongly as with the local convolutionalarchitectures. However, they never turn out to be the best architectures. Given their inductive biasesfor long-range dependencies, they spend parameters on interactions with degrees of freedom beyondthe necessary receptive field. This likely explains their reduced ability to produce the best results inthis relatively simple scenario of hyperbolic linear advection with an influence range known a priori. The pseudo-spectral FNO has a performance which is agnostic to changes in 1. This behavior canbe explained by its inherent capabilities to learn band-limited linear dynamics and its similarity withthe data-generating solver. Despite these advantages, local convolutional architectures like a ResNetare on par with the FNO under low difficulties that do not demand a large receptive field. Surprisingly, ResNet and the deepest ConvNet fail at the highest difficulty despite having sufficientreceptive field. However, (b) reveals that under additional unrolling during training, thesame ResNet greatly improves in temporal generalization. In line with the motivational example ofsection 2, this suggests that unrolling during training, rather than exposure to more physics, is key toa better learning signal and achieving desirable numerical properties.",
  "Diverted Chain: A Learning Methodology with A Differentiable Fine Solver": "APEBenchs tight integration with its ETDRK solver suite enables the exploration of promisingtraining setups beyond purely data-driven supervised unrolling. One such setup is the diverted chainapproach as obtained with Eq. 2, combining autoregressive unrolling with the familiar one-stepdifference. Here, the reference solver branches off after each autoregressive network prediction.providing a continuous source of ground truth data during training. As it hinges on fully integratingthe (differentiable) reference solver, this variant has not been studied in previous work. 0.002 0.0030.004 0.006 t = 1 nRMSE BurgersKuramoto-Sivashinsky Korteweg-de Vries t = 100 nRMSE Sup-1Sup-5Div-5 : Comparison of training methodolo-gies for a ResNet emulator on three nonlinear1D dynamics. Emulators benefit from roll-out training, the strongest visible for the KdVcase. Diverted-Chain offers the advantage oflong-term accuracy without sacrificing short-term performance. compares the performance of a ResNetemulator trained using one-step supervised train-ing, 5-step supervised unrolling, and 5-step divertedchain training on three nonlinear 1D dynamics:viscous Burgers, Kuramoto-Sivashinsky (KS), andhyper-viscous Korteweg-de Vries (KdV). The resultsdemonstrate that training with unrolling, regardlessof the specific approach, generally improves long-term accuracy, as indicated by the lower 100-steperror. This improvement is particularly pronouncedfor the KdV equation, likely due to the increasedeffective receptive field seen during training, whichis especially beneficial for strongly hyperbolic prob-lems. Notably, the diverted-chain approach furtherenhances long-term accuracy for the KdV scenario.While it does not surpass supervised unrolling forBurgers and KS in terms of long-term accuracy, itexcels in short-term performance, only slightly un-derperforming compared to the one-step trained em-ulator. These findings confirm that the diverted-chainapproach effectively combines the benefits of trainingtime rollout and one-step differences, demonstratingthe flexibility of APEBench to explore diverse train-ing strategies due to its differentiable solver.",
  "Neural-Hybrid Emulators and Sequential Correction": "Neural-hybrid emulators, which combine neural networks with traditional numerical solvers, are apromising area of research in physics-based deep learning (Kochkov et al., 2021; Um et al., 2020).APEBenchs differentiable ETDRK solver framework facilitates the exploration of such hybridapproaches. In this section, we investigate the sequential correction of a defective solver Ph usingboth a ResNet and an FNO for a 2D advection problem with a difficulty of 1 = 10.5. Threevariations of this task are explored: full prediction, and sequential correction with the coarse solverhandling 10% (1 = 1.05) or 50% (1 = 5.25) of the difficulty. 0.0 0.1 0.2 0.3 Agg. nRMSE Full Pred.10% Corr.50% Corr. Res, 1-StepRes, 5-Step FNO, 1-StepFNO, 5-Step : ResNet and FNO either as full predictionemulators or neural-hybrid emulators for 2D advection(1 = 10.5) with a coarse solver doing 10% or 50% ofthe difficulty. The geometric mean of the rollout errorover 100 time steps is shown. Training with unrollingbenefits the ResNet yet only shows marginal improve-ment for the FNO. The ResNet can work in symbiosiswith a coarse simulator. displays the geometric mean ofthe test rollout error over 100 time steps.The results reveal that supervised unrollingconsistently improves the performance ofthe ResNet and ResNet-hybrid models, out-performing the FNO in every case. Notably,the FNOs performance remains almost un-affected by unrolling and changes in diffi-culty, likely due to its global receptive fieldand ability to capture long-range dependen-cies. In contrast, the ResNet, with its lim-ited receptive field, benefits significantlyfrom unrolling. The ResNet performs bestin the 50% correction task, highlightingthe potential of neural-hybrid approachesthat leverage the strengths of both convolu-tional and pseudo-spectral methods. Thesefindings underscore the importance of tailoring the training strategy and architecture to the specifictask and difficulty level and emphasize the potential of hybrid approaches for superior performancein PDE emulation.",
  "Across": "ResolutionNetwork Type ConvResUNetDilFNO : Comparison of emulator architectures across various PDE dynamics in 1D, 2D, and 3D.The ResNet consistently performs well across all dynamics and dimensions. Local architecturesstruggle with higher-order derivatives, while limited active modes hinder the FNOs performancein some cases. The Dilated ResNet is the better long-range architecture in 1D, whereas the UNet isbetter suited for higher dimensions. the geometric mean of the error over 100 rollout steps, summarized in . The parameter countsacross architectures are almost identical within the same spatial dimensions. We provide ablations onthe choices of optimization configuration (section I.2), training dataset size (section I.3), and networkparameter size (section I.4) in the appendix. Linear PDEs For the 1D hyperbolic dispersion problem, local convolutional architectures (withsufficient receptive fields and low problem difficulty) again excel, closely followed by the pseudo-spectral FNO. The considered higher-dimensional linear PDEs introduce spatial mixing, making thelearning task more challenging. For 2D anisotropic diffusion, both local and global convolutionalarchitectures perform well, with the FNO lagging slightly and the UNet showing a significant spreadin performance. In the 3D unbalanced advection case, the FNOs limited active modes hinder captureof the solutions full complexity. Convolutional architectures struggle to balance short- and long-rangeinteractions across different dimensions, with UNets showing the most consistent performance. Nonlinear PDEsFor the 1D Korteweg-de Vries and Kuramoto-Sivashinsky equations, localarchitectures likely struggle with the hyper-diffusion term due to insufficient receptive fields, givinglong-range architectures an advantage. The FNOs performance is also suboptimal, potentially due tolimited active modes. Notably, the FNO excels in the challenging Navier-Stokes Kolmogorov Flowcase. Reaction-Diffusion Reaction-diffusion problems, characterized by polynomial nonlinearities withno spatial dependence that develop rich (high-frequency) patterns (see ), are best handled bylocal convolutional architectures, particularly ResNets. The FNO was the least suitable architecturefor this class of problems. We attribute this to its low-frequency bias in that it learns predictions in thefrequencies beyond its active modes only indirectly via the energy transfer of the nonlinear activation. Performance across Dimensions Emulating the Burgers equation across dimensions reveals anexponential decrease in performance as dimensionality increases ( (d)). Across all dimensions,ResNets consistently emerge as the top-performing architecture, showcasing their adaptability tovarying spatial complexities. Dilated ResNets, while effective in 1D and 2D, experience a significantperformance drop in 3D. This likely stems from their dilation-based mechanism for long-rangeinteractions resulting in less uniform coverage of the receptive field compared to UNets. Theperformance gap between standard ConvNets and ResNets widens in 3D, highlighting the increasingimportance of skip connections. Performance across Resolution In this example, we emulate a KS equation in 2D using N = 322and N = 1602 as well as in 3D with N = 323. Due to the difficulty mode, the dynamics are adaptedbased on resolution and dimensionality. Counterintuitively, emulation often improves with increasingresolution until the emulators architectures are fully utilized. In contrast, the FNO struggles inthis scenario because, due to the difficulty-based rescaling, the spectrum is fully populated in bothresolutions for the 2D case (see the spectra in ). Across all architectures, the jump to 3Dworsens their performance, which reinforces the observations shown in (d). Notably, theUNet emerges as the best architecture in 3D likely because it has a global receptive field at thisresolution.",
  "Conclusions and Outlook": "We presented APEBench, a benchmark suite for autoregressive neural emulators of time-dependentPDEs, focusing on training methodologies, temporal generalization, and differentiable physics. Thebenchmarks efficient JAX-based pseudo-spectral solver framework enables rapid experimentationacross 1D, 2D, and 3D dynamics. We introduced the concept of difficulties to uniquely identifydynamics and scale experiments. The unified treatment of unrolling methodologies was demonstratedas a foundation to investigate learned emulators across a wide range of dynamics and trainingstrategies. We revealed connections between the performance of an architecture, problem type, anddifficulty that make it possible to understand their behavior with analogies to classical numericalsimulators. Specifically, our benchmark experiments highlight the importance of: Matching the network architecture to the specific problem characteristics. Local problemsbenefit significantly from local convolutions, while global receptive fields are less impactedby unrolled training.",
  "Utilizing training with unrolling to significantly improve performance, particularly forchallenging problems and under limited receptive fields": "Exploring hybrid approaches that combine neural networks with coarse numerical solvers(correction) and differentiable reference solvers (diverted-chain training) to further enhancethe capabilities of learned emulations. In this context, many interesting areas for future work remain. Particularly notable are parameter-conditioned emulators and foundation models that can solve larger classes of PDE dynamics. Perhapsthe most crucial avenue for future research with APEBench lies in conducting an even deeperinvestigation of unrolling and the intricate interplay between emulator and simulator. A deeperunderstanding here could significantly impact the field of neural PDE emulation.",
  "G. Kohl, L. Chen, and N. Thuerey. Turbulent flow simulation using autoregressive conditionaldiffusion models. CoRR, abs/2309.01745, 2023. doi: 10.48550/ARXIV.2309.01745. URL": "N. B. Kovachki, Z. Li, B. Liu, K. Azizzadenesheli, K. Bhattacharya, A. M. Stuart, and A. Anandkumar.Neural operator: Learning maps between function spaces with applications to PDEs. J. Mach.Learn. Res., 24:89:189:97, 2023. URL I. E. Lagaris, A. Likas, and D. I. Fotiadis. Artificial neural networks for solving ordinary and partialdifferential equations. IEEE Trans. Neural Networks, 9(5):9871000, 1998. doi: 10.1109/72.712178. URL R. Lam, A. Sanchez-Gonzalez, M. Willson, P. Wirnsberger, M. Fortunato, A. Pritzel, S. V. Ravuri,T. Ewalds, F. Alet, Z. Eaton-Rosen, W. Hu, A. Merose, S. Hoyer, G. Holland, J. Stott, O. Vinyals,S. Mohamed, and P. W. Battaglia. Graphcast: Learning skillful medium-range global weatherforecasting. CoRR, abs/2212.12794, 2022. doi: 10.48550/ARXIV.2212.12794. URL",
  "T. Pfaff, M. Fortunato, A. Sanchez-Gonzalez, and P. W. Battaglia. Learning mesh-based simulationwith graph networks. 2021. URL": "N. Rahaman, A. Baratin, D. Arpit, F. Draxler, M. Lin, F. A. Hamprecht, Y. Bengio, and A. C. Courville.On the spectral bias of neural networks. In K. Chaudhuri and R. Salakhutdinov, editors, Proceedingsof the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, LongBeach, California, USA, volume 97 of Proceedings of Machine Learning Research, pages 53015310. PMLR, 2019. URL",
  ". For all authors": "(a) Do the main claims made in the abstract and introduction accurately reflect the paperscontributions and scope? [Yes] The capabilities of the benchmark are explained insection 4 and in Appendix 4, including the full list of the 46 dynamics claimed in theabstract. (b) Did you describe the limitations of your work? [Yes] , see section 6 for the mostimportant limitations of the APEBench benchmark suite, also see Appendix B.3 andB.4 for more details on the limitations of the new pseudo-spectral solver suite weprovide.",
  ". If you ran experiments (e.g. for benchmarks)": "(a) Did you include the code, data, and instructions needed to reproduce the main ex-perimental results (either in the supplemental material or as a URL)? [Yes] Thesupplemental material contains all the resources that are part of the benchmark, and weused to create results presented in this paper. (b) Did you specify all the training details (e.g., data splits, hyperparameters, how theywere chosen)? [Yes] , see section H for a description for all experiments conducted inthe main part of the paper as well as section I for ablations. (c) Did you report error bars (e.g., with respect to the random seed after running experi-ments multiple times)? [Yes] , APEBench is fundamentally designed for experimentsthat consider multiple initializations of pseudo-randomness. We describe the aggrega-tion in section H under the paragraph Seed Statistics. All results presented throughoutthis work used at least 20 different initializations. (d) Did you include the total amount of compute and the type of resources used (e.g., typeof GPUs, internal cluster, or cloud provider)? [Yes] , the compute cost is listed insection H under the paragraph Hardware & Runtime.",
  "[Yes] The benchmark suite APEBench is the main contribution of this work. It is aJAX-based Python package, released under a permissive license and provided in thesupplemental material": "(d) Did you discuss whether and how consent was obtained from people whose data youreusing/curating? [Yes] We generate all synthetic trajectories ourselves using the newlyprovided numerical solver (which is part of this publication). The neural architecturesare properly cited but have been re-implemented by us. (e) Did you discuss whether the data you are using/curating contains personally identifiableinformation or offensive content? [NA] The data we use or procedurally generate aresynthetic simulations of simple mathematical models unrelated to individual humans.",
  "Next to the benchmark suite, we also release parts of it as individual Python packages:": "Exponax is a standalone Fourier pseudo-spectral exponential time-differencing Runge-Kutta solver in JAX with a rich feature set, including the ability to be differentiated. Itis hosted open source under an MIT license at can be installed as a Python package via pip install exponax. Publically hosteddocumentation is available at PDEquinox is a collection of neural emulator architectures built on top of Equinox(Kidger and Garcia, 2021). It is hosted open source under an MIT license at and can be installed as a Python package viapip install pdequinox.Publically hosted documentation is available at Trainax is a collection of abstract implementations for various unrolled training strategies,including the presented diverted-chain methodology. It is hosted open source under anMIT license at and can be installed as a Pythonpackage via pip install trainax. Publically hosted documentation is available at Vape4d is a performant spatiotemporal volume render that can be used to quickly assessthe results of neural emulation in higher dimensions. It is hosted open source under aBSD 2-Clause license at and can be installedas a Python package via pip install vape4d. A web-based version running locallyin the browser working with multi-axis numpy arrays is accessible at",
  "B.1Motivation and Background": "Exponential Time Differencing Runge Kutta (ETDRK) methods offer a powerful approach to solvingtime-dependent partial differential equations (PDEs) by leveraging the exact solution of linearordinary differential equations (ODEs) through matrix exponentials. This approach is particularlyadvantageous when dealing with stiff systems, where traditional numerical methods may requireimpractically small time steps to maintain stability. Linear ODEsThe core idea of exponential time differencing methods is that linear ordinarydifferential equations can be solved exactly by the matrix exponential. For this, first consider ascalar-valued linear ODE for the solution function u(t) : R+ R of the form",
  "u[t+1] = eAtu[t]": "The matrix exponential of At can be precomputed. Hence, advancing the state in time is a matrix-vector multiplication. As long as the underlying dynamics are stable (requiring the real part of alleigenvalues of A to be non-positive), the timestepper is unconditionally stable. Such a strategy isviable for small systems with a few degrees of freedom C.",
  "u(0, x) = u0,u(t, 0) = u(t, L),": "where is the diffusion coefficient. This equation only has a single channel (C = 1) that can beinterpreted as a temperature. Following a method-of-lines approach, let us discretize the spatialdomain = (0, L) into N intervals of size x = L/N. We will consider the left end of each intervalas a degree of freedom. As such, the left boundary of the domain is part of the grid and the rightboundary is excluded. This naturally encodes the periodic boundary conditions. We can approximate",
  "Spectral DerivativesIf we have a function u(x) on a periodic domain = (0, L), we can computeits derivative using the Fourier transform F viaux = F1 (ikF(u))": "Here, i is the imaginary unit, and k is the wavenumber. In a discrete setting, if we have u(x) sampledat N points (on a grid similarly as before) we can denote the state vector as uh RN. We can usethe discrete analogon of the Fourier transform, the discrete Fourier transform (DFT) Fh, to computethe derivative givingu",
  "h= F1h(ikh Fh(uh))": "Such an approximation converges quickly (exponentially fast) for smooth functions (Trefethen, 2000;Boyd, 2001). However, more importantly, is that the derivative operator diagonalizes in Fourierspace. In the heat equation example, the discrete second-order derivative Lh was a sparse but non-diagonal matrix. In Fourier space, any derivative operator is simply an element-wise multiplicationwith the general Fourier derivative operator (elementwise) raised to the order of the derivative, (ik)s.Hence, the spectral version of a s-th order derivative issu",
  "If we wanted to solve the heat equation in state space, we could use a forward and inverse Fouriertransformu[t+1]h= F1hexp( Lht) Fh(u[t]h )": "Since the spectral differentiation is exact if the function is bandlimited, this method of solving theheat equation is exact. More generally speaking, if we select a bandlimited initial condition uh,0 (andhave periodic boundary conditions), we can integrate trajectories of any linear PDE (with constantcoefficients) without discretization errors in space and time, and with arbitrarily large time steps.This requires the underlying dynamics to be stable. In the case of the heat equation, we need 0.",
  "u(0, x) = u0,u(t, 0) = u(t, L)": "An exponential time differencing approach breaks down because the nonlinear differential operatordoes not diagonalize in Fourier space. If we transform the space-discrete equation into Fourier space,we getuht = Lh uh + Nh(uh).(3) We can design a pseudo-spectral evaluation strategy for the nonlinear term by evaluating the squarein state space using an inverse and forward Fourier transform. For the Burgers equation, the nonlinearterm isNh(uh) = 1",
  "z,": "which encounter numerical instabilities for small z. To avoid these problems, Kassam and Trefethen(2005) designed a contour integral method in the complex plain to compute the coefficients morestably and accurately, which we chose to implement. Dealiasing for the Nonlinear PartLike any pseudo-spectral method, evaluating the nonlinearterm moves energy between the modes. This can move energy into wavenumbers that cannot berepresented by the grid. Hence, they appear as aliases, potentially corrupting the solution and leadingto instabilities. A common strategy is to set all Fourier coefficients above a certain threshold to zerobefore evaluating the nonlinearity. Orszag (1971) proposed that for quadratic nonlinearities (like inthe Burgers equation), keeping the first 2/3 modes and setting the rest to zero is sufficient to avoidissues caused by aliasing. This does not fully eliminate aliasing (which would require keeping onlyhalf of the modes) but only produces aliases for the modes, which will be zeroed in the next step. LetM be a zero mask at the wavenumber position we want to remove and one otherwise. The correctevaluation of the Burgers nonlinearity then becomes",
  "u(0, x, y) = u0,u(t, 0, y) = u(t, L, y),u(t, x, 0) = u(t, x, L)": "We assume that the domain is the scaled unit-cube = (0, L)2, which is discretized with the samenumber of points in each dimension. Again, the left boundary is part of the discretization, and theright boundary is excluded. Let kh,0 denote the discrete wavenumber grid in x-direction and kh,1 iny-direction. Hence, the linear operator in Fourier space can be written as",
  "B.2ETDRK Methods in other Software Libraries": "The popular ChebFun package in MATLAB (Driscoll et al., 2014) implements pseudo-spectral ETDRKmethods with a range of spectral bases under their spinX.m module. It served as a reliable datagenerator for early works in physics-based deep learning. For instance, it was used to produce thetraining and test data for Raissi and Karniadakis (2018) and parts of the experiments of Li et al. (2021).Due to the two-language nature, with most deep learning research happening in Python, dynamicallycalling MATLAB solvers is hard to impossible. Naturally, this also excludes the option to differentiateover them to allow differentiable physics, for instance, to enable diverted-chain learning as discussedin section 5.2 or correction setups as discussed in section 5.3. We view our ETDRK solver frameworkas a spiritual successor of this spinX.m module. JAX, as the computational backend, elevatesthe power of this solver type with automatic vectorization, backend-agnostic execution, and tightintegration for deep learning via the versatile automatic differentiation engine. Beyond ChebFun, popular implementations of pseudo-spectral implementations can be found in theDedalus package (Burns et al., 2020) in the Python world and the FourierFlows.jl (Constantinou et al.,2023) package in the Julia ecosystem.",
  "B.3Limitations of Fourier Pseudo-Spectral ETDRK Methods": "Fourier pseudo-spectral ETDRK methods are a powerful class of numerical techniques for solvingsemi-linear partial differential equations (PDEs), where the highest-order derivative is linear. Thesemethods excel in scenarios where the stiffness of the linear part poses the primary challenge inintegration. By analytically treating the linear component, they effectively eliminate linear stiffness,enabling efficient and accurate solutions.",
  "However, like any numerical method, Fourier pseudo-spectral ETDRK solvers have inherent limita-tions:": "1. Periodic Domain and Uniform Cartesian Grid: The method relies on the Fast FourierTransform (FFT), which necessitates a periodic domain and a uniform Cartesian grid. Thisrequirement stems from the diagonalization of the linear derivative operator in Fourier space,a crucial step for the methods effectiveness. While the general ETDRK framework can beadapted to other spectral methods like Chebyshev, the efficiency might be reduced. 2. No Channel Mixing in Linear Part: The method assumes that each equation in a systemof PDEs depends solely on its own variables in the linear part. If theres \"channel mixing,\"where the linear terms of one equation depend on variables from other equations, the linearoperator becomes non-diagonal in Fourier space, leading to the methods breakdown. 3. First-Order in Time: ETDRK methods are specifically designed for first-order PDEs intime. Higher-order time derivatives do not conform to the methods structure. Attemptsto reformulate higher-order PDEs into first-order systems often introduce channel mixing,rendering the method inapplicable. 4. Smooth and Bandlimited Solutions: The method assumes smooth and bandlimited solu-tions, meaning that the solutions Fourier spectrum decays rapidly at high frequencies. Thislimitation precludes the simulation of strongly hyperbolic PDEs with discontinuities, suchas the inviscid Burgers, Euler, or shallow water equations. The method can only handle theirviscous counterparts, where viscosity dampens high-frequency modes. 5. Difficulty from Nonlinear Part: When the primary challenge in integration arises fromthe nonlinear part, the advantage of analytically treating the linear part diminishes. TheNavier-Stokes equations at high Reynolds numbers exemplify this scenario, where smalltime steps are necessary due to the dominant nonlinear effects.",
  ". Cartesian Domains: The method assumes a Cartesian domain. On a sphere, the linear partno longer diagonalizes, and the method breaks down": "Despite these limitations, if a problem aligns with the constraints, the Fourier pseudo-spectral ETDRKapproach is one of the most efficient methods available for semi-linear PDEs on periodic boundaries(Montanelli and Bootland, 2020). Its tensor-based operations are well-suited for modern GPUs, andits straightforward integration with automatic differentiation frameworks like JAX simplifies thecomputation of derivatives.",
  ". Same Extent in Each Dimension: We currently support only problems on scaled unitcubes, where each dimension has the same extent, i.e., = (0, L)D": "2. Equal Discretization Points in Each Dimension: The number of discretization pointsN is uniform across all dimensions. While this simplifies the interface, it limits the rangeof problems that can be addressed. However, we believe the remaining problem spaceremains substantial, especially for studying the learning dynamics of autoregressive neuralemulators. 3. Real-Valued PDEs Only: Our implementation focuses on real-valued PDEs. Although theFourier pseudo-spectral ETDRK method is also suitable for complex-valued PDEs, like theSchrdinger or complex Ginzburg-Landau equations, restricting to real-valued problemssimplifies the interface and allows the exclusive use of the real-valued FFT, enhancingcomputational efficiency. 4. Constant Time Step Size: We currently require a constant time step size t, althoughETDRK methods theoretically support adaptive time stepping. This decision was made toalign with the specific requirements of training autoregressive neural emulators, which oftenhave a fixed time step embedded in their architecture. 5. Limited Set of ETDRK Methods: Our implementation only includes the original ETDRK0,ETDRK1, ETDRK2, ETDRK3, and ETDRK4 methods. A recent study has shown thatthese methods remain competitive among solvers for stiff semi-linear PDEs (Montanelli andBootland, 2020).",
  "B.6Generic Time Steppers for General Dynamics": "Our framework, based on defining linear and nonlinear differential operators, enables us to cover awide range of PDEs. Often, PDEs exhibit structural similarities. For instance, transitioning from theBurgers equation to the Korteweg-de Vries (KdV) equation involves simply changing the order ofthe linear term from second to third. Moreover, the viscous KdV equation encompasses the Burgersequation as a special case when dispersivity is absent. This observation motivates the development of generic time steppers that accommodate arbitrarycombinations of linear and nonlinear differential operators. To achieve this, we focus on isotropiclinear operators, which lack spatial mixing (e.g., cross-derivatives). This allows us to represent thelinear operator uniquely by a list of coefficients {aj}Sj=0, where S is the highest derivative orderconsidered. Each coefficient corresponds to the scaling of a particular derivative order, with zerosindicating the absence of specific terms.",
  "jajjuxj + b0u2 + b112( 1)u2 + b212u2": "To illustrate, the Burgers equation can be expressed as a special case of the general convection stepperwith coefficients a0 =, a1 = 0, a2 = , and b1 = 1. This flexibility showcases the power of ourframework in accommodating a wide range of PDEs with diverse linear and nonlinear components.",
  "B.7Normalized Dynamics: A Unifying Framework": "When simulating the dynamics of partial differential equations (PDEs), its crucial to identify theparameters that uniquely determine their behavior. For instance, the one-dimensional advectionequation is governed by the domain extent L, advection speed c, and time step size t. However, thedynamics remain unchanged as long as the ratio ct/L stays constant. This observation leads to theconcept of normalized dynamics, a framework that unifies the characterization of diverse PDEs. Generalizing to Linear OperatorsThis concept extends beyond advection. In the diffusionequation, the dynamics are uniquely determined by the ratio t/L2, where represents thediffusion coefficient. Similarly, for dispersion and hyper-diffusion equations, the governing ratiosare t/L3 and t/L4, respectively. We can generalize this observation to any linear operatorinvolving the j-th derivative with coefficient aj. The normalized dynamics, denoted by j, are givenby",
  "Lj .(22)": "Nonlinear Operators and Composite DynamicsFor nonlinear operators, the situation becomesmore nuanced. We must consider the interplay between the order of derivatives, the nonlinearityitself, and any subsequent derivatives. Taking a coefficient blpre,p,lpost, where lpre and lpost represent theorders of derivatives before and after the nonlinearity, and p denotes the order of the polynomial, thenormalized coefficient is expressed as",
  "Llprep+lpost .(23)": "This expression accounts for the \"amplification\" of derivatives before the nonlinearity due to thepolynomial order. Notably, any polynomial nonlinearity without additional derivatives is normalizedby L0 = 1. Practical Implications for Neural EmulatorsNeural emulators are designed to learn and emulatethe dynamics of complex systems, and their performance is inherently linked to the speed and natureof those dynamics. By characterizing dynamics with a reduced set of normalized coefficients, wesimplify the assessment of neural emulator performance. Instead of manipulating multiple parameters,we can focus on varying the relevant normalized coefficients, enabling a more targeted and efficientevaluation.",
  "B.8Difficulty of Emulating Dynamics: Bridging the Continuous and Discrete": "While normalized dynamics effectively characterize the time-discrete form of a PDE, they do notfully capture the challenges associated with emulating those dynamics in a space-discrete setting.Discretizing a PDE introduces additional complexities due to finite spatial resolution, numericalapproximations, and the potential for instabilities. Thus, understanding the difficulty of emulationrequires considering both the continuous nature of the dynamics, as captured by the normalizedcoefficients, and the discrete aspects of the numerical implementation. The spatial resolution, represented by the number of grid points N in each dimension D, plays acritical role in emulation. As N increases, the receptive field of convolutional architectures, which isgiven in terms of cells per directions, spans a much smaller physical area. Stability is a key factor inthe emulation process. Explicit numerical methods, which compute future states directly from currentvalues, often have stability limitations. For instance, the Courant-Friedrichs-Lewy (CFL) conditiondictates the maximum allowable time step for a first-order upwind scheme applied to the advectionequation by",
  "CFL = 1N": "Similarly, for diffusion and higher-order equations, as well as in higher dimensions, we can defineanalogous difficulty factors j byj = jN j2j1D.(24)These factors provide a quantitative measure of emulation difficulty. When j approaches 1, weare nearing the stability limit of (the most compact) explicit finite difference method. Similarly, wecompute the difficulty of supported nonlinear components as",
  "with m denoting the (expected) maximum absolute of the state throughout the trajectory": "Ultimately, in order to identify a dynamic (and its difficulty of emulation), it is sufficient to know therespective nonzero j and j values (the defaults used for the dynamics in APEBench are listed intable 2) and the resolution N (and the dimension D). In this benchmark suite, difficulties serve as anexchange protocol or an identifier for an experiment where possible.",
  "CEmulator Architectures: Leveraging Equinox for Flexible and PowerfulNeural PDE Emulators": "Our suite of neural PDE emulator architectures is built upon the Equinox library (Kidger and Garcia,2021) for JAX (Bradbury et al., 2018). We provide seamless support for various boundary conditions(Dirichlet, Neumann, and periodic) and enable architectures that are agnostic to spatial dimensions(1D, 2D, and 3D). Our implementations are inspired by PDEArena (Gupta and Brandstetter, 2023) toa large extent. Wrapped Convolutions and Building BlocksAt the core, we provide higher-level abstractionsfor convolutional layers that allow defining the boundary condition instead of the padding kind andpadding amount. Internally, we set up the corresponding padding to ensure a \"SAME\" convolution.We also implemented a spectral convolution which is agnostic to the spatial dimensions to buildFNOs. This routine is inspired by the generalized spectral convolution of the Serket library 2. Wecombine the fundamental convolutional modules into blocks, such as residual or downsamplingblocks. Architectural Constructors and Curated NetworksBased on the blocks, we have architecturalconstructors for sequential and hierarchical networks. With those, we provide a range of curatedarchitectures, including the ones used in main text: feedforward convolutional networks (Conv),convolutional ResNets (Res) (He et al., 2016), UNets (Ronneberger et al., 2015), Dilated ResNets(Dil) (Stachenfeld et al., 2021), and Fourier Neural Operators (FNO) (Li et al., 2021).",
  "Notable Advantages of using Equinox & JAX": "Single-Batch by Design: All emulators have a call signature that does not require arraysto have a leading batch axis. Vectorized operation is achieved with the jax.vmap transfor-mation. We believe that this design more closely resembles how classical simulators areusually set up. Seed-Parallel Training: With only a little additional modification, the automatic vector-ization of jax.vmap can also be used to run multiple initialization seeds in parallel. Thisapproach is especially helpful when a training run of one network does not fully utilize anentire GPU, like in all 1D scenarios. Essentially, this allows for free seed statistics.",
  ",(26)": "with l(, ) being a time-level loss, which typically is the mean squared error (MSE). Optional timestep weights wt and wb can be supplied to differently weigh contributions (e.g., to exponentiallydiscount the error over unrolled steps). We used the notation that a function raised to an exponentdenotes an autoregressive/recursive application. If it is raised to zero, this should be interpreted as thefunction not being applied (i.e., resorting to the identity). In this case, supervised unrolling (T = B)would only leave the application of the fine solver Ph in the second entry and the second sum overthe branch chain (together with t = 0), which then reads",
  ".(27)": "Clearly, we recover the popular one-step supervised training with T = B = 1, which leaves onlyone summand. Beyond that, the main text investigates diverted chain unrolled training with a freelychosen rollout length T and B = 1 for a one-step difference. It requires the numerical simulator Phon the fly and in a differentiable way. It reads",
  ".(28)": "Again, there are as many loss contributions as time steps in the main chain. APEBench also supportsthe most general case with T freely chosen and B 2. In this case, one would get cross terms, andboth sums remain. For brevity, we did not present any results under such a configuration. displays a schematic for a three-step supervised unrolled objective. The curvy lines indicatethe gradient flow. A purple gradient flow represents input-output differentiation over the neuralemulator, which causes the backpropagation through time. Similarly, we present the three-stepdiverted chain unrolled training schematic in . The yellow box around the references denotesthat only the target for the first one-step difference can be precomputed. All targets beyond thatrequire the reference solver to be called dynamically. Consequently, we must also differentiate overit, indicated by the yellow reversely pointing arrow. In , we present the more general scenarioof a diverted-chain setup with T = 3 and B = 2.",
  "D.1Continued Taxonomy": "Gradient cuts interrupt the reverse flow in automatic differentiation and decouple the loss landscapefrom the gradient landscape. Oftentimes, they are used strategically to avoid the problem of vanishingor exploding gradients. In the case of training rollout, this can become relevant for long main chains(List et al., 2022). Alternatively, they can be used with different motivations, for example, to bettercompensate for distribution shift (Brandstetter et al., 2022). It can also be used to avoid undesirableminima and saddle points (Schnell and Thuerey, 2024). One can also imagine a strategic applicationto avoid having the simulator be differentiable (List et al., 2024). Generally speaking, APEBench allows for all the aforementioned approaches by either (sparsely) cut-ting the backpropagation through time and/or the differentiable physics. For example, the pushforward",
  "NNNNNN": ": Three step diverted-chain learning. The autoregressive main chain is the same as ina similar-length supervised setting (see ). However, the reference simulator P is calleddynamically on the outputs produced by the network to generate a one-step error. Only the referencefor the first one-step difference can be precomputed. For the latter two, the reference solver has to becalled during training. As such, we also require it to be differentiable as seen by the gradient flowindicated by the yellow arrow.",
  "D.2Differentiable Physics in Learning Setups": "Below, we provide a decision guide as to which setup requires the autodifferentiable solver. For eachversion, a green color indicates whether we need an autodifferentiable solver. A setup with purplecolor does require a solver to be called on-the-fly, but does not require differentiability. To the best ofour knowledge, configurations with either of the two colors are unique to APEBench and have notyet been part of other benchmark publications. Pure prediction means that the neural emulator f isexclusively made up of the neural network. On the other hand, a correction task is defined in that theneural emulator contains both a neural network and a coarse solver component Ph as discussed insection E.3.",
  "EAPEBench Details": "In APEBench, we call a scenario the fully encapsulated pipeline to train and evaluate an autoregressiveneural emulator. Aside from smaller statistical variations, it is a fully reproducible setup since itprocedurally generates its training data. Each scenario embeds a dynamic defined by the continuousPDE to be emulated, its constitutive parameters, and numerical discretization choices. In the simplestcase, one uses the reduced interface via difficulties.",
  "Additionally, we classify equations/dynamics as follows:": "(L)inear (N)onlinear (D)ecaying (I)infinitely running (essentially the opposite of decaying) Reaching a (S)teady state (reaching a state that is not a domain-wide constant) (M)ulti-Channel (if the number of channels is greater than one) (C)haotic (if the system is sensitive to the initial condition) All (L)inear problems stay bandlimited if they start from a bandlimited initial condition. This mightbe different for (N)nonlinear problems, which can produce higher modes and can become unresolved.This can lead to instability in the simulation, which might require a stronger diffusion.",
  "E.2Preferred Interface Mode and Typical Values": "Most dynamics listed in are best addressed using the difficulties mode (with gammas anddeltas). Alternatively, the normalized interface via alphas and betas can also be chosen. Someequations are only available in the physical parameterization. This includes the special linearscenarios of unbal_adv, diag_diff, aniso_diff, mix_disp, and mix_hyp since their spatialmixing does not align with the requirements of isotropic linear derivatives. The reaction-diffusionscenarios of Gray-Scott and Swift-Hohenberg are also only available in physical parameterization tomore closely follow prior research (Pearson, 1993). For the Navier-Stokes dynamics, we decidedto also only have them in physical parameterization since these dynamics are typically adjusted viathe Reynolds number. The Reynolds number, the resolution N, and the time step t determine theemulations difficulty. lists the default values for each dynamics in APEBench under theirpreferred mode. For nonlinear difficulties, we use a maximum absolute m = 1 for simplicity (see Eq.25), which aligns in that most initial conditions are ensured to have an absolute magnitude of one.",
  "The default discretization is N = 160 in 1D, N = 1602 in 2D, and N = 323 in 3D. Note that somedynamics require specific initial conditions": "Accessing Specific ModesTo utilize a specific scenario in a particular interface mode (DIFF,NORM, or PHY), simply prepend the desired mode to the scenario name. After the scenario hasbeen executed, APEBench will automatically prepend the spatial dimension to the scenario namefor clarity. For instance, to execute the Burgers scenario using the difficulty interface in threedimensions, the user would input diff_burgers with num_spatial_dims=3. The name assignedwill be 3d_diff_burgers. Gray-Scott DynamicsFor the Gray-Scott scenarios, we use an interface to directly define the feedrate f and kill rate k. Alternatively, there is also the gs_type interface, which requires defining thedynamics type. We chose the values according to Pearson (1993), and the defaults are listed in .",
  "E.3Correction Tasks and Neural-Hybrid Emulators": "While the default task for neural emulators in APEBench is prediction, in which the neural networkcompletely replaces the numerical simulator, an alternative correction mode is also available. Inthis mode, a coarse solver ( Ph) and the neural network work together to form a neural-hybridemulator. The default configuration corrects a \"defective\" solver, which only performs a portion ofthe integration time step compared to the reference simulator. : Overview of (preferred) interface mode (DIFFiculty, NORMalized, and PHYsical) for allscenarios listed in . q is the number of substeps. w is the number of warmup steps (relevantonly for chaotic problems). XXX refers to the difficulty of the following nonlinear differentialoperators: CONVection, CONVection_SingleChannel, GradientNorm, and QUADratic polynomial.",
  "NameModeDefault Values under preferred mode": "advdiff,norm,phy = {0, 4, 0, 0, 0}diffdiff,norm,phy = {0, 0, 4, 0, 0}adv_diffdiff,norm,phy = {0, 4, 4, 0, 0}dispdiff,norm,phy = {0, 0, 0, 4, 0}hypdiff,norm,phy = {0, 0, 0, 0, 4}unbal_advphyL = 1, t = 0.1,c = [0.01, 0.04, (0.005)]T diag_diffphyL = 1, t = 0.1, = [0.001, 0.002, (0.0004)]Taniso_diffphyL = 1, t = 0.1, A = [0.001, 0.0005; 0.0005, 0.002]mix_dispphyL = 1, t = 0.001, = 0.00025mix_hypphyL = 1, t = 0.00001, = 0.000075burgersdiff,norm,phy = {0, 0, 1.5, 0, 0}, conv = 1.5burgers_scdiff,norm,phy = {0, 0, 1.5, 0, 0}, conv_sc = 1.5kdvdiff,norm,phy = {0, 0, 0, 14, 9}, conv_sc = 2ks_consdiff,norm,phy = {0, 0, 2, 0, 18}, conv = 1, w = 500ksdiff,norm,phy = {0, 0, 1.2, 0, 15}, gn = 6, w = 500fisherdiff,norm,phy = {0.02, 0, 0.2, 0, 0}, quad = 0.02gsphyL = 1.0, t = 10, q = 10, 0 = 2 105, 1 = 105,f = 0.04, k = 0.06gs_typephyL = 2.5, t = 20, q = 20, 0 = 2 105, 1 = 105t = thetashphyL = 10, t = 0.1, q = 5, r = 0.7, k = 1.0decay_turbphyL = 1, t = 0.1, = 0.0001kolm_flowphyL = 2, t = 0.1, = 0.1, k = 4, = 1/Re,Re = 100, q = 20, w = 500",
  "alpha0.0080.046beta0.0200.046gamma0.0240.056delta0.0280.056epsilon0.020.056theta0.040.06iota0.050.0605kappa0.0520.063": "The two main corrective layouts are sequential (a) and parallel (b). APEBenchallows the selection of either layout, including modified versions that incorporate gradient cuts (Listet al., 2024). For the experiments presented in this paper, sequential correction was chosen, asit facilitates the sharing of the necessary receptive field between the coarse solver and the neuralnetwork.",
  "FMetrics": "APEBench supports a range of metric functions to compare two discrete simulation states uh and urhor reduce a single simulation state to a scalar value. In neural emulator learning, next to the discretestates containing a value to each spatial degree of freedom, they can also hold multiple channels(=species/fields) and more than one sample. Normalization & Aggregation ProcessNormalization is applied independently per channel.Subsequently, each channels contribution is summed, and the average across all samples is calculated.This process ensures consistent metric computation as follows:",
  "An overview of metric functions provided by APEBench and their classifications are shown in": "Consistency with Parsevals Theorem and Function NormsAccording to Parsevals theorem,the mean_<X>MSE metric in APEBench is guaranteed to be equivalent to mean_fourier_<X>MSE,provided that the Fourier-based variant (mean_fourier_<X>MSE) does not introduce any modi-fications to the spectrum, such as selecting specific frequency ranges or applying derivative op-erations. This equivalence similarly applies to the RMSE metrics, meaning mean_<X>RMSE andmean_fourier_<X>RMSE will yield identical values under these conditions. However,Parsevalsidentitydoesnotholdformetricssuchasmean_<X>MAEandmean_fourier_<X>MAE. This discrepancy arises because Parsevals theorem is only applicableto metrics that consistently relate to the L2() function norm. Consequently, absolute-error-basedmetrics like MAE, which do not involve squaring, deviate from Parsevals conditions. This limitation extends to Sobolev-inspired metrics, specifically the mean_H1_<X>MAE metric. Al-though it is derived from a Sobolev space perspective, it relies on Fourier aggregation, meaning it isnot directly consistent with the H1() function norm. The Fourier aggregation used internally inmean_H1_<X>MAE prevents full alignment with the H1 norm, as Parsevals identity does not applyoutside of L2-aligned norms. In cases where derivatives are applied within the Sobolev-based losses, all \"gradient\" directions aresummed. Sobolev-based losses such as those labeled with H1 contain contributions from both thefunction values and their first derivatives, which highlights errors in higher frequencies/smaller scales.",
  "GAn Interactive Transient 3D Volume Renderer for Simulation Trajectoriesin Python": "The seamless exploration and visualization of 3D time-varying data has shown to be difficult withexisting tools. As part of the benchmark, we publish a real-time interactive 4D volume rendering toolfor seamless visualization of volumes within a Python environment. The tool uses a user-defined transfer function and volume rendering for visualization. For each pixel,a ray r is marched through the volume. N samples with distance are taken along the ray, and thetransfer function is used to map each sample to a density and color c. The final pixel color iscomputed using the volume rendering equation:",
  ": Visualization of the time evolution of the first channel of a 3D Burgers dynamic": "Physics and Numerics SetupUnless otherwise stated, we employ N = 160 degrees of freedomper spatial dimension. The dynamics are characterized by their combination of linear difficulties sand nonlinear difficulties s, when applicable. For certain cases that deviate from this framework,such as Navier-Stokes examples (adjusted via Reynolds number) and reaction-diffusion scenarios(often also with non-standard initial conditions), the domain extent L, time step size t, and relevantconstitutive parameters are directly specified. Moreover, linear scenarios in higher dimensions thatinvolve spatial mixing are also handled in this manner. Across all experiments, we utilize the second-order ETDRK method (ETDRK2, section B.1), as weobserved a favorable cost-accuracy trade-off in single-precision floating-point calculations comparedto higher-order methods. For computing the complex-valued coefficients, we set the circle radius to1.0 and choose 16 points on the complex unit circle (Kassam and Trefethen, 2005). The referencestepper typically does not employ substepping unless explicitly mentioned. Train Data TrajectoriesIf not specified otherwise, we draw 50 initial conditions for train-ing and discretize them on the given resolution.The ETDRK solver is then used to au-toregressively roll them out for 50 time steps.Effectively, this results in an array of shape (SAMPLES=50,TIME_STEPS=51,CHANNELS,...) with the ellipsis denoting an arbitrary number ofspatial axes. Within these 50 trajectories, we randomly sample windows of size ROLLOUT+1 forstochastic minibatching.",
  "Test Data TrajectoriesWe draw 30 initial conditions from the same distribution as for the trainingdataset but with a different random seed. The solver suite produces trajectories of 200 time steps": "Neural ArchitecturesWe chose our architectures to have 30k parameters in 1D, 60k in 2D,and 200k in 3D. In section I.4, we ablate parameter sizes of the architectures. All convolutionsuse \"SAME\" periodic/circular padding and a kernel size of three. We do not consider learning theboundary condition. All architectures are implemented agnostic to the spatial dimension.",
  "We employ the following architectural constructors:": "Conv;WIDTH;DEPTH;ACTIVATION: A feedforward convolutional network with DEPTH hid-den layers of WIDTH size. Each layer transition except for the last uses ACTIVATION. Theeffective receptive field is DEPTH + 1. Res;WIDTH;BLOCKS;ACTIVATION: A classical/legacy ResNet with post-activation and nonormalization scheme. Each residual block has two convolutions and operates at WIDTHchannel size. The ACTIVATION follows each of the convolutions in the residual block.There are BLOCKS number of residual blocks. Lifting and projection are point-wise linearconvolutions (=1x1 convs). UNet;WIDTH;LEVELS;ACTIVATION: A classical UNet using double convolution blockswith group activation in-between (number of groups is set to one). WIDTH describes thehidden layers size on the highest resolution level. LEVELS indicates the number of timesthe spatial resolution is halved by a factor of two while the channel count doubles. Skipconnections exist between the encoder and decoder part of the network. Dil;DIL-FACTOR;WIDTH;BLOCKS;ACTIVATION: Similar to the classical post-activationResNet but uses a series of stacked convolutions of different dilation rates. Each convolutionis followed by a group normalization (number of groups is set to one) and the ACTIVATION.DIL-FACTOR of 1 refers to one convolution of dilation rate 1. If it is set to 2, this refers tothree convolutions of rates . If it is 3, then this is , etc. FNO;MODES;WIDTH;BLOCKS;ACTIVATION: A vanilla FNO using spectral convolutionswith MODES equally across all spatial dimensions. Each block operates at WIDTH chan-nel size and has one spectral convolution with a point-wise linear bypass. The activation isapplied to the sum of spectral convolution and bypass result. There are BLOCKS total blocks.Lifting and projection are point-wise linear (=1x1) convolutions.",
  "The concrete architectures, their parameter counts, and effective receptive fields are listed in": "Seed StatisticsWe report statistics over various random seeds. We fix the random seeds for trainand test data generation for each experiment. Then, we use this one set of data to train an ensemble ofnetworks. Each network uses the same initialization routines (following the defaults in Equinox anda reasonable default for the spectral convolution in FNOs) but a different random key. This randomkey also modifies the stochastic mini-batching to which the networks are subject during training. Forone-dimensional (at most realistic resolutions) and two-dimensional problems (with low resolutionN 502), seed statistics can be obtained virtually for free since one network training does not fullyutilize an entire GPU. We run the seed statistics sequentially for three-dimensional experiments andthe two-dimensional problems at N = 1602. We use 50 seeds for experiments in 1D, and 20 seeds forexperiments in 2D and 3D. Statistics are aggregated using the median and display the corresponding50 % inter-quantile range (IQR) (from the 25 percentile to the 75 percentile). We chose the medianaggregator to reduce the influence of seed outliers. Similarly, the 50% IQR is less susceptible tooutliers. Training and OptimizationIf not specified otherwise, we use the Adam optimizer (Kingma andBa, 2015) with a warmup cosine decay learning rate scheduling (Loshchilov and Hutter, 2017). Thisscheduler was also found beneficial in recent physics-based deep learning publications (Tran et al.,2023; Lam et al., 2022). The default optimization duration is 10000 update steps. The number of",
  "Conv;26;12;relu20248913Res;25;6;relu20287612UNet;11;2;relu20032229Dil;2;27;2;relu19272220FNO;5;7;4;gelu196246inf": "effective epochs depends on the number of initial conditions, the train temporal horizon, and theunrolled training length. If not specified otherwise, the batch size is set to 20. This usually reduces to50 100 epochs of 100 200 minibatches. The first 2000 update steps are a linear warmup of thelearning rate from 0.0 to 103. Afterward, it decays according to a cosine schedule, reaching 0.0 atthe last, the 10000-th iteration. We found a decay all the way to zero helpful in forcing the networksinto convergence and reducing the impact of the last minibatch variation on the deduced metric.",
  "Ni=1(uj,i uj,i)2Ni=1(uj,i)2(31)": "over M samples and N degrees of freedom. This metric adjusts for differences in scale with regardto the reference state uh. A value of 1 indicates that the magnitude of the error is the same as themagnitude of the reference, i.e., the predicted state uh is completely different from the reference.This normalization is helpful for error rollouts over time for dynamics that have states changing inmagnitude, e.g., decaying phenomena like the Burgers equation. Additionally, this allows for a clearcomparison across dynamics, which can also have states that differ in magnitude. We use an upper index [t] to denote the loss after t time steps L[t]nRMSE. The loss at [t] = iszero since autoregressive prediction trajectory and reference start at the same initial conditions.We are interested in how the error develops over the time steps since this reveals insights into theemulators temporal generalization capabilities. For aggregating the metric, we choose an upperindex T (by default 100) and use the geometric mean (gmean-mean-nRMSE, in the main text justcalled Aggregated nRMSE or Agg. nRMSE)",
  "Employing the geometric mean reduces the need to handcraft upper limits for temporal aggregationin case error metrics go beyond the value 1": "Hardware & RuntimeWe conducted our experiments on a cluster of eight Nvidia RTX 2080 Tiwith 12GB of video memory each. We used the collection of GPUs to distribute runs with differentinitialization seeds but not to distribute a single network over multiple GPUs. displays thecost of all individual experiments. The total runtime is 900 GPU-hours. Under an ideal loaddistribution, this equals 5 days of full runtime on the cluster we used.",
  "This subsection describes the details of the experiment in section 2": "The difficulty of the problem is set to 1 = 0.75. This is in combination with N = 30 degreesof freedom and D = 1 spatial dimensions. The initial condition distribution follows a truncatedFourier series with cutoff K = 5, zero mean, and max one absolute. For training, we draw five initialconditions and integrate them for 200 time steps with the analytical solver. The EDTRK solver suitegives this analytical stepper since it can integrate any linear PDE with a bandlimited initial conditionexactly. The optimization is performed over the full batch of all samples (across trajectories and all possiblewindows within each trajectory) with a Newton optimizer. We initialize the optimization withthe FOU stencil. For long unrolled training (beyond what we display in this work), we observedconvergence problems, which needed us to initialize the optimization for T + 1 unrolled steps withthe minimizer of T unrolled steps. This also indicates that training with unrolled steps is a tougheroptimization problem, which might need such curriculum strategies. The Newton method is run untilconvergence to double floating machine precision ( 1016); typically achieved within ten iterations. The found stencils are measured against the analytical solution according to the mean-nRMSE error(31) for 50 new initial conditions drawn from the same distribution as for the training dataset butcompared over 200 time steps. The FOU stencil is measured similarly. In , we display thenumeric error values at relevant time steps [t]. displays the optimizers in parameter space. We also present the found stencils for threevariations to highlight that the data-driven optimization problem is non-trivial and sensitive to theconcrete setup:",
  "More Modes: Uses a cutoff of K = 10 instead of K = 5": ": Numeric Values for the linear convolution learning experiment of the 1D advection equation.\"-\" indicates that the value is beyond 1.0. This table also displays the error up to time step 200 whichwas not shown in the main part of the paper. We see that beyond a certain point, the FOU stencilagain becomes superior because it is consistent with the advection equation. Note that after step 30,each row makes a bigger step.",
  "This subsection details the settings of the experiments in section 5.1": "We use the default configuration of the diff_adv scenario in 1D but adapted the difficulty suchthat 1 {0.5, 2.5, 10.5}. The networks are the default configuration for one dimension. Only thefeedforward convolutional network was modified to a different DEPTH. With the depth set to 10, itcorresponds again to the default configuration.",
  "This subsection details the settings of the experiments in section 5.2": "Each of the three nonlinear scenarios uses the default setup listed in . We use the defaultconfiguration for the ResNet in 1D as denoted in . In , we list errors at time steps 1and 100 for all three training configurations in the median over 50 seeds and the limits of the 50%IQR.",
  "This subsection details the settings of the experiments in section 5.3": "This experiment uses the diff_adv scenario with num_spatial_dims=2. To create the threevariations, we fixed the scenarios advection_gamma=10.5 and varied the coarse_proportionin {0.0, 0.1, 0.5}. The ResNet and the FNO are in their default configuration for 2D as displayed in. In , we display the geometric mean (see Eq. 32) of the test error rollout over the first100 time steps.",
  "This subsection details the settings of the experiments in section 5.4": "All emulators are trained for a pure prediction task using a one-step supervised configuration. Wemeasure performance in terms of the geometric mean of the test rollout over 100 time steps. Theresolution for the 3D problems is reduced to N = 323, and we reduce the number of trajectories inthe test dataset from 30 to 10 to ensure that the experiments worked on 12GB GPU memory. For theone-dimensional problem, we produced 50 seeds. For 2D and 3D, we used 20 seeds.",
  "IAblation Studies": "In this section, we ablate choices made for the main experiments in this publication. We found themto be fair settings that also allow for compute-efficient broad comparison across the axes supported bythe benchmark. We stress that APEBench is flexible and allows fine-grain control over these variablesbut also comes with reasonable defaults. : Numeric results of advection learning at the highest difficulty when increasing the unrolledsteps during training for the ResNet architecture. A \"-\" indicates that the value went beyond 1.0. Wealso showcase temporal results up to 200 (the plot in the main text was limited to a horizon of 25)and training with up to 15 steps of unrolling.",
  "Wavenumber": "Swift-Hohenberg 3D : Spectra of the magnitude of Fourier coefficients. For the chaotic problems (Kuramoto-Sivashinsky and Kolmogorov Flow), the spectrum is averaged over all samples and time steps in thetest trajectories. In the case of the reaction-diffusion problems (Gray-Scott and Swift-Hohenberg),the spectrum is averaged, excluding the initial states. For problems in higher dimensions (D 2)we use a binning approach to compute the spectrum. A magnitude contribution is associated withwavenumber k if its wavenumber norm is within a ring, i.e., k2 [k 1",
  ", k + 1": "2). Based on theresolution per dimension, the Nyquist mode is either at 80 or 16. We consider a problem under-resolved if the Nyquist mode (and the hypothetical modes beyond it) are not below the threshold of105. Under this definition, the KS 3D and all reaction-diffusion problems are under-resolved. : Numeric values for the comparison across architectures, dynamics and spatial dimensions.The median over initialization seeds is displayed for the geometric mean of 100 time steps of testrollout. Note that the concrete configuration of the architecture type varies across spatial dimensions;see for details.",
  "Unrolled Training Steps": ": Test rollout performance ofResNet emulators for 1D advection at1 = 10.5 when each unrolled traininguses equal compute time. The gray areaindicates the axis limits in the plots of. Emulators with shorter un-rolling improve, but long unrolling re-mains superior. In this section, we revisit the ResNet emulator for 1Dadvection at difficulty level 1 = 10.5, as previously dis-cussed in .1. However, our focus now shifts tocompensating for the shorter unrolling during training byemploying a greater number of update steps of the net-work (i.e., a larger number of training iterations). Thisadjustment is motivated by the fact that computational cost,along with memory consumption due to reverse-mode au-tomatic differentiation, scales linearly with the number ofunrolled steps in the training phase. By conducting thisablation study, we establish a scenario where each emu-lator receives roughly equivalent wall clock time on theGPU (approximately 45 minutes for 10 seeds in parallelon an Nvidia RTX 2080 Ti). presents the results of these experiments. Nu-meric values are listed in . Notably, we observe asubstantial improvement in the test rollout capabilities ofemulators with shorter unrolling. The one-step supervisedtrained emulator, for instance, demonstrates accuracy overa significantly extended duration. However, the conclusiondrawn in the main paper does not change: configurationscharacterized by more unrolled steps (but fewer total up-date steps) continue to excel in terms of long-term accuracy.",
  "I.2Optimization Configuration Ablation": "In this section, we investigate the impact of modifying the training duration and peak learning ratewhile retaining the cosine learning rate decay scheduler with linear warmup ending at one-fifth ofthe total training duration. displays the results, focusing on the geometric mean over 100time steps of test rollout. All architectures utilize the default configuration, as do the five scenariosexamined: two linear scenarios (advection and diffusion) and three nonlinear scenarios (Burgers,Korteweg-de Vries, and Kuramoto-Sivashinsky) in 1D. The gray line at 104 represents the default",
  "choice employed in all other experiments presented in this work, which is also the default settingacross APEBench. The second column maintains the default peak learning rate of 0.001": "Our findings reveal that the highest investigated peak learning rate of 0.01 is excessive for ConvNet,ResNet, and Dilated ResNet architectures, as they fail to demonstrate clear convergence with addi-tional update steps. In contrast, UNet and FNO architectures generally exhibit stable improvement,with the exception of the advection scenario. Across all cases, the default choice of 0.001 appearsreasonable, although 0.003 could also be viable. A clear improvement in the geometric mean error(serving as the test error, as it measures temporal generalization on a new set of initial conditions) isobserved when networks undergo extended training, as expected. Importantly, the relative orderingamong the architectures remains largely consistent. Only the ResNet-type architectures (includingDilated ResNet) appear to benefit more strongly from extended training. Notably, we do not observesigns of overfitting (except at the highest peak learning rate); instead, model performance tends toplateau.",
  "I.3Ablation for Size of the Training Dataset": "Given that we train on reference trajectories, we ablate the number of initial conditions and the lengthof the training temporal horizon. It is crucial to note that an excessively short horizon might excludecertain regimes of the physics. For instance, a horizon that is too short for the Burgers equation couldomit the shock propagation phase. This is particularly noteworthy because emulators must learn tohandle such situations without explicitly encountering them in the training data, as evidenced by thegeometric mean aggregation over 100 time steps in our test rollout. illustrates our results,with the gray dashed line indicating the configuration used in all other experiments within this work. Across all architectures, there is a consistent performance improvement up to a certain number oftraining samples, beyond which gains become minor but still noticeable. Interestingly, the threeclasses of neural architectures exhibit distinct behaviors. Local convolutional architectures (ConvNetand ResNet) demonstrate remarkable parameter efficiency, converging with as few as five trainingsamples. These are followed by global convolutional architectures and, finally, the FNO (representingpseudo-spectral architectures). In the Korteweg-de Vries scenario, the ResNet underperforms notably. Consistent with the findingsof the optimization configuration ablation in Section I.2, we consider this an intriguing failure modefor the ResNet, though its root cause remains unclear. However, we observe that unrolled trainingsubstantially enhances its performance, as detailed in .2. The impact of extended temporal horizons varies depending on the dynamics under investigation. Aspreviously mentioned, dynamics with multiple stages, such as Burgers and Korteweg-de Vries (whichmust first develop their characteristic spectra), exhibit the most significant improvement in emulatorperformance with longer horizons. Conversely, the Kuramoto-Sivashinsky equation begins within thechaotic attractor, rendering an extended temporal horizon effectively equivalent to additional samples,as evidenced by the faster convergence of the curves. Notably, all curves converge to approximatelythe same level. For the advection problem, different temporal horizons yield almost no difference,whereas diffusion displays a clear trend, though less pronounced than for Burgers and KdV. Thisis attributed to emulators encountering later stages of the dynamics with longer horizons, wherelow-magnitude states are mapped to even lower magnitudes. In conclusion, we find that the default choice of 50 initial condition samples with a training temporalhorizon of 50 strikes a reasonable balance. Crucially, it does not alter the relative performance rankingof different architectures, enabling fair comparisons.",
  "I.4Parameter Scaling Ablation": "In this section, we expand the parameter space of the neural emulator by increasing the number ofhidden channels. Importantly, we refrain from altering settings that could influence the receptivefield, ensuring that each ablated network configuration retains the default receptive field as outlinedin . presents the test error, quantified as the geometric mean over 100 rollout steps. Asanticipated and consistent with observations by List et al. (2024), architectures demonstrate improvedperformance with increased parameter counts. However, the specific convergence rate appears to be Agg. nRMSE Peak LR = 0.0003Peak LR = 0.001Peak LR = 0.003 Scenario = 1d_diff_adv Peak LR = 0.01 Agg. nRMSE Scenario = 1d_diff_diff Agg. nRMSE Scenario = 1d_diff_burgers Agg. nRMSE Scenario = 1d_diff_kdv 10k20k40k80k Number of Training Steps Agg. nRMSE 10k20k40k80k Number of Training Steps 10k20k40k80k Number of Training Steps 10k20k40k80k Number of Training Steps Scenario = 1d_diff_ks",
  "Architecture": "ConvResUNetDilFNO : Parameter size ablation, depicted as the geometric mean over 100 time steps of test rollout.The gray area indicates the parameter range of the networks employed in the main experiments.Overall, all architectures benefit from larger parameter spaces, with convergence rates depending onthe specific dynamics under investigation. problem-dependent. Some architectures either reach a plateau or experience a decline in performancebeyond a certain parameter threshold. We attribute this behavior to improper parameter scaling,which fails to increase the receptive field or adapt the optimization configuration effectively.",
  "JDatasheet": "We have included the datasheet below for completeness but emphasize that APEBench is designed asa benchmark suite with access to a powerful data generator framework in the form of the ETDRKsolver framework. We leverage this by tightly integrating the numerical solver and procedurallyre-creating all training and test trajectories with each new experiment execution. As such, there isno need to distribute separate datasets since the entire emulator learning specification is uniquelydescribed in an APEBench scenario. However, we acknowledge that this approach creates a strong dependency on the JAX and Equinoxecosystem. To mitigate this, we will release a subset of representative trajectories for the defaultdynamics supported by the benchmark. These trajectories can be utilized for purely data-drivenemulator learning tasks and within other ecosystems like PyTorch or Julia. However, its importantto note that most functionalities, such as easy scenario modification, diverted-chain training, andcorrection learning, will remain exclusive to the full benchmark suite.",
  "For what purpose was the dataset cre-ated? Was there a specific task in mind?Was there a specific gap that needed to befilled? Please provide a description": "The discrete emulation of a PDE effectivelyamounts to approximating a numerical simulatorwhile interacting with it during training. Such in-teraction can be purely data-driven, and the simu-lator is turned off during training. However, moreintricate combinations are possible. APEBench isthe first benchmark that recognizes this situationand tightly integrates a differentiable JAX-basedsolver suite.",
  "Composition": "What do the instances that comprise thedataset represent (e.g., documents, pho-tos, people, countries)?Are there multi-ple types of instances (e.g., movies, users,and ratings; people and interactions betweenthem; nodes and edges)? Please provide adescription. There are (more than) 46 distinct PDE scenarios(across three spatial dimensions). Each scenariohas a (reproducible) procedural generation of atrain and test dataset. These come in the formof arrays with defaults of 50 train trajectories of51 time steps and 30 test trajectories of 201 timesteps. The data is in a structured Cartesian for-mat. The subsequent axes depend on channels,the number of spatial dimensions, and resolution.Each scenario also contains metadata that explic-itly describes how the training is run.",
  "Answered above": "Does the dataset contain all possible in-stances or is it a sample (not necessarilyrandom) of instances from a larger set?If the dataset is a sample, then what is thelarger set? Is the sample representative ofthe larger set (e.g., geographic coverage)?If so, please describe how this representa-tiveness was validated/verified. If it is notrepresentative of the larger set, please de-scribe why not (e.g., to cover a more diverserange of instances, because instances werewithheld or unavailable).",
  "No noise exists since all simulated equations aredeterministic, and the data is created synthetically.We also expect no redundancies because each ini-tial condition is drawn separately": "Is the dataset self-contained, or doesit link to or otherwise rely on externalresources (e.g., websites, tweets, otherdatasets)? If it links to or relies on externalresources, a) are there guarantees that theywill exist, and remain constant, over time;b) are there official archival versions of the complete dataset (i.e., including the exter-nal resources as they existed at the timethe dataset was created); c) are there anyrestrictions (e.g., licenses, fees) associatedwith any of the external resources that mightapply to a future user? Please provide de-scriptions of all external resources and anyrestrictions associated with them, as well aslinks or other access points, as appropriate. The dataset/benchmark is based on a Pythonlibrary that is hosted on GitHub ( It dependson three other Python libraries that are part of thispublication, which are also hosted on GitHub( APEBench package and the additional threepackages can all be installed via pip. All librariesdepend on JAX and Equinox (as well as a fewother Python libraries), which are all open-sourceprojects. Does the dataset contain data that mightbe considered confidential (e.g., datathat is protected by legal privilege or bydoctor-patient confidentiality, data thatincludes the content of individuals non-public communications)? If so, please pro-vide a description.",
  "Skipped": "Does the dataset contain data that mightbe considered sensitive in any way (e.g.,data that reveals racial or ethnic origins,sexual orientations, religious beliefs, po-litical opinions or union memberships, orlocations; financial or health data; bio-metric or genetic data; forms of govern-ment identification, such as social secu-rity numbers; criminal history)?If so,please provide a description.",
  "Collection Process": "How was the data associated with eachinstance acquired?Was the data di-rectly observable (e.g., raw text, movie rat-ings), reported by subjects (e.g., survey re-sponses), or indirectly inferred/derived fromother data (e.g., part-of-speech tags, model-based guesses for age or language)?Ifdata was reported by subjects or indirectly in-ferred/derived from other data, was the datavalidated/verified?If so, please describehow.",
  "Options could be control and reinforcement learn-ing. We also think that trying different numericalsimulators using techniques other than pseudo-spectral discretization can be interesting": "Is there anything about the compositionof the dataset or the way it was col-lected and preprocessed/cleaned/labeledthat might impact future uses? For ex-ample, is there anything that a future usermight need to know to avoid uses that couldresult in unfair treatment of individuals orgroups (e.g., stereotyping, quality of serviceissues) or other undesirable harms (e.g., fi-nancial harms, legal risks) If so, please pro-vide a description. Is there anything a futureuser could do to mitigate these undesirableharms?",
  "It is already available and can be installed byfollowing the instructions on the GitHub page:": "Will the dataset be distributed undera copyright or other intellectual prop-erty (IP) license, and/or under applicableterms of use (ToU)? If so, please describethis license and/or ToU, and provide a linkor other access point to, or otherwise repro-duce, any relevant licensing terms or ToU,as well as any fees associated with theserestrictions."
}