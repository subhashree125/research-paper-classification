{
  "Abstract": "Speculative decoding aims to speed up autoregressive generation of a languagemodel by verifying in parallel the tokens generated by a smaller draft model. Inthis work, we explore the effectiveness of learning-free, negligible-cost draft strate-gies, namely N-grams obtained from the model weights and the context. Whilethe predicted next token of the base model is rarely the top prediction of thesesimple strategies, we observe that it is often within their top-k predictions for smallk. Based on this, we show that combinations of simple strategies can achievesignificant inference speedups over different tasks. The overall performance iscomparable to more complex methods, yet does not require expensive preprocess-ing or modification of the base model, and allows for seamless plug-and-playintegration into pipelines.",
  "Introduction": "Large Language Models (LLMs) have had a significant impact across various scientific and industrialdomains. However, their autoregressive decoding process, which generates one new token per modelcall, is computationally expensive. This issue is particularly challenging for larger models, whichtypically exhibit superior performance compared to smaller ones [Brown et al., 2020, Anil et al.,2023, Achiam et al., 2023]. To improve computational efficiency and inference latency, many works have proposed methods forreducing the cost of a single model call. Some examples include quantization methods Yao et al., early-exiting strategies Xin et al. , flash-attention [Dao et al., 2022, Dao, 2023], andmulti-token prediction [Gloeckle et al., 2024]. Another line of work has considered variants of autoregressive decoding aimed at better leveragingthe parallel processing capabilities of GPU/TPU hardware accelerators. In particular, speculativedecoding methods [Stern et al., 2018, Leviathan et al., 2023, Chen et al., 2023a, Xia et al., 2023],sometimes also called guess-and-verify methods, use a smaller draft model to generate proposalsfor multiple future tokens. They then validate all of these tokens in parallel with a single call tothe original model and ensuring that the original model would have predicted the same tokens. Thisapproach is similar to speculative execution [Hennessy and Patterson, 2011], in which a processorexecutes instructions in parallel to verifying if they are needed, trading resources for concurrency, asin just-in-time XLA compilation in JAX, Pytorch, and Tensorflow. The effectiveness of speculative decoding is determined by i) the acceptance rate of the draft specula-tions, ii) the discrepancy in call-time between the original model and the draft model, and iii) theextra cost required for parallel verification by the main model (although this is often assumed to benegligible when using hardware accelerators).",
  "arXiv:2411.03786v1 [cs.LG] 6 Nov 2024": "Choosing a draft model that is compatible with the base model and within the available computebudget can be challenging. To address this issue, several approaches have been developed that involveaugmenting the base model and performing supervised fine-tuning (SFT), with the goal of ensuringthat the draft model and the base model utilize the same feature representations [Cai et al., 2024,Li et al., 2024, Bhendawade et al., 2024]. Although these strategies often achieve high acceptancerates, they come with the drawback of necessitating SFT for each individual model, which can beresource-intensive. To overcome this, researchers have also considered negligible cost draft models, remarking that if thecost of the draft model is close to zero then even a low acceptance rate can yield wall-time speedups.This was first proposed explicitly in Leviathan et al. , where the authors experimented withunigram and bigram draft models, both trained on external data, as well as implicitly by Santilli et al., who used a cost-free draft model, corresponding to the base models greedy predictions at aprevious time step. In this paper, we aim to explore the full potential of negligible-cost draft methods for acceleratingautoregressive decoding. In particular, we argue that even simple strategies based on N-gramsderived from the model and the context can be very effective when combined in a batch toexplore the space of possible future trajectories in parallel. Our proposed methods have the followingdesirable features: (P1) they do not require training a draft model or finetuning the base model, (P2)they make use of no external data or external draft model, and most importantly (P3) they can easilybe integrated with any existing pipeline as an out-of-the-box approach, and moreover they can becombined with other acceleration techniques like the ones mentioned above. We emphasize, the goal of this paper is not to achieve state-of-the-art inference speed-ups but ratherexplore the strengths and weaknesses of simple methods that satisfy the desirable properties (P1), (P2),(P3). Our experiments across different datasets (MTBench Zheng et al. , HumanEval Chenet al. , GSM8K Cobbe et al. ) and models (Mistral7B Jiang et al. , Phi-3 Abdinet al. , Vicuna13B Zheng et al. ) show that such out-of-the-box strategies are surprisinglyeffective.",
  "Further related work": "External draft model.The concept of guess-and-verify using an external draft model was explicitlyproposed in several concurrent works [Leviathan et al., 2023, Chen et al., 2023a, Xia et al., 2023],with Leviathan et al. notably exploring negligible cost models and suggesting that fittingN-grams to the context could potentially be a promising line of future work. Recently, follow-upworks have investigated using a collection of varied size draft models [Chen et al., 2023b], tree-basedguess-and-verify methods [Miao et al., 2023], retrieving speculations from external data sources [Heet al., 2023], and using a collection online-buffers for aligning the draft and base model via training[Liu et al., 2023]. Contrary to these works, we aim to explore strategies that do not require an externaldraft model. Learning by adapting the base model.In order to align the predictions of the draft model withthose of the base model, several works have proposed grafting a draft model on top of the base model,so that both share the same features. Cai et al. propose adding K heads to a model in orderto predict K tokens into the future, together with a tree-based attention mechanism. The authorsexplore i) fine-tuning only the heads with the base-model frozen ii) fine-tuning the base LLM withthe heads. Building on this idea, Li et al. proposes training an auto-regressive decoder fromthe penultimate layer, showing that this approach can obtain very high acceptance rates. Learning-free methods.Santilli et al. proposed initializing a random speculation, andat subsequent decoding steps using the model predictions from the previous step as speculations,in order to improve upon greedy decoding. This process resembles the Jacobi and Gauss-Seideliterative methods (and is hence called Jacobi Decoding), and is implementable just a few lines ofcode. Look-ahead decoding Fu et al. further improves upon the acceptance rate of Jacobidecoding, by using a custom-attention mask to generate an N-gram speculation cache as well asverifying matching speculations in parallel.",
  "SymbolUsage": "Xthe set of tokens that constitute the vocabulary of an LLM.knumber of batched speculations, taken from the top-k of probability over tokens.wnumber of tokens speculated into the future.qnumber of query tokens to match with context when looking for an N-gram.length of context at a decoding step, assumed to be key-valued cached (KV-cached). Limitations.For simplicity and ease of integration, our method incurs extra computation costthrough batching (which could be addressed in follow-up works by incorporating methods suchas bifurcated attention Athiwaratkun et al. ). Further exploration is needed for non-greedysampling methods such as those discussed in [Leviathan et al., 2023], which are commonly deployed.Finally, we have limited our experiments to decoder-only transformer models, and it remains to betested with other architectures such as state-spaced models [Gu and Dao, 2023].",
  "Assumptions on parallelism for verification": "We briefly revisit and clarify the key assumption in the guess-and-verify literature [Leviathan et al.,2023, Chen et al., 2023a, Xia et al., 2023, Santilli et al., 2023] that parallel verification of speculatedtokens by the base model is memory-bound when using hardware accelerators like GPUs/TPUs. Hardware accelerators like GPUs and TPUs divide matrix multiplications (matmuls) into tiles,each assigned to independent computation threads. These operations can be parallelized if theiroperations-to-bytes ratio (OTB) is below the hardwares threshold, allowing all threads to be as-signed to multiprocessors and executed concurrently. If the OTB ratio is above the threshold, theoperation becomes compute-bound (or math-bound), as the number of tiles exceeds the number ofmultiprocessors, requiring the total number of tiles to be quantized to fit the available hardwareresources. For a fixed model and hardware, let denote the length of the given context at any decoding step,with all context tokens (except for the final token), assumed to have KV-cache stored in memory. Let(k, w + 1) denote the dimensions of the input batch, where k 1 denotes the batch size and w 0denotes the number of tokens speculated into the future. With this terminology, the guess-and-verifyassumption above can be rephrased as the following: for a fixed model using KV-caching and a fixedhardware accelerator, and for a given context length 1, the time required to perform a modelcall on an input block of size (k, w + 1) is approximately the same as the time for a model call on aninput block of size (k, 1). When does this assumption hold? For each element in a batch of size k, the attention mechanismrequires multiplying (w + 1) queries by ( + w) keys, resulting in O (kw(w + )) complexity. Sofor a fixed model, accelerator and (, k, w) values, the assumption holds if and only if all matmuls inthe forward pass of the model have an OTB ratio less than the accelerators threshold. In practice,this does not always hold. depicts the phase-transition from memory-bound to computebound, with varied (, k, w), for Mistral 7B on a NVIDIA A100 40GB GPU. One does not see asmooth scaling of O (kw(w + )) in the phase transition, due to the quantization to multiprocessors,resulting in jumps known as wave quantization. In the special case of neglible cost draft models, for which the time to generate speculations isassumed to be near zero, there will be a clear trade-off between the speed-up gained from acceptingextra tokens (by increasing k and w) vs the potential slow-down that could be faced when entering acompute-bound setting, where the guess and verify assumption is broken. 1357911 13 15 w k Slowdown: = 25 1.5 >= 2 1357911 13 15 w k Slowdown: = 100 1.5 >= 2 1357911 13 15 w k Slowdown: = 500 1.5 >= 2 : Memory-bound to compute-bound transition: The heatmaps depict the slowdown ofa model call, for varied batch size k {1, . . . , 32} and speculation length w {0, . . . , 15}. Theslow-downs are relative to that of standard greedy decoding with no speculation i.e. (k, w) = (1, 0).The leftmost plot corresponds to a context-length of = 25, the middle to = 100 and the rightmostplot to = 500. The model used was Mistral 7B at standard bfloat-16 precision, with a singleNVIDIA A100 GPU with 40GB of memory. Each square in the heat-maps corresponds to the averageslow-down over five model calls.",
  "Learning-free drafting": "The use of N-grams to model language dates back at least to Markov , who published a paperin which he used conditional probabilities of constants and vowels (computed by hand) to comparethe poem Eugene Onyegin by Pushkin to other texts, showing how unigram and bigram probabilitiescould mathematically capture an authors style. Many sequences of tokens in natural language /computer code exhibit low entropy, making even simple N-grams effective at predicting them, asdemonstrated in Shannon . In this section, we explore ways to extract N-grams directly from alarge language model and a context, and use these for speculation. These methods are learning-free,as they require no training (P1), nor external data (P2), in contrast to the N-gram models used inLeviathan et al. , which are obtained from external data sources. All the methods discussed inthis section can be implemented with minimal wrapper code (detailed in Appendix B), allowing forthem to be added to existing pipelines with minimal friction (P3).",
  "Model-derived N-grams": "Let M denote a language model with vocabulary X. Let V RXd and U RdX denote themodels input and output embedding layers, with respective row / column word embeddings {vi}i|X|and {ui}i|X|. For a given context c (i.e. a sequence of tokens from X), we denote the next tokendistribution according to M as pM(|c), where",
  "xX pM(x|c) = 1": "Unigram.Consider the function d(x) = ux uV , where u Rd is the mean token outputembedding, and V is the distance induced by the covariance matrix of the input embeddings V , thatis, the inner product u1, u2V = uT1 V T V u2 = xX (uT1 vx)(uT2 vx). This product is more naturalthan the standard one in Rd as two tokens will be close when they lead to similar distributions for thefollowing token for the model, as captured by the vectors (uTi vx : x X) R|X|. We can hencedefine a unigram distribution over tokens using the input and output embeddings as p(x) ed(x). Bigram.We can easily obtain a bigram model from a language model M by calculating pM( | x)for all tokens x X. For typical models, this can be a calculated once for every x X and storedfor quick use later. For example, generating such a bigram model takes 1 minute for Mistral 7B ona single A100 GPU, and is a one-off cost. While this simple bi-gram lacks context for tokens priorto x, it can still be effective, particularly in cases where the preceding context is not essential formaking accurate predictions. Batched drafts.When performing autoregressive decoding, the greedy next token prediction (NTP)of the base model will seldom match with that predicted by the above unigram and bigram models(derived from the base model). However, we remark that the base model NTP appears often amongstthe top-k predictions of the N-grams, even for small k. Consequentially, we propose obtaining k 1.0 1.1 1.2 1.3 1.4 1.5 1.6 Toks / Call MT-Bench Bigram w = 1 Bigram w = 2 Bigram w = 3 Unigram w = 1 k 1.0 1.1 1.2 1.3 1.4 Toks / Call",
  "Human Eval": "Bigram w = 1 Bigram w = 2 Bigram w = 3 Unigram w = 1 : Tokens per call as a function of k, the top-k speculations of the model derived unigram /bigram. In addition, the plot depicts the extended bigram (described below) plotted for w = 2 andw = 3, showing gains comparing w = 1 to w = 2, but diminishing gains going to w = 3. The resultswere obtained on the first 50 examples of MT-bench and Human Eval, using a 7B model (MistralInstruct) [Jiang et al., 2023]. speculations from the top-k of a N-gram model, i.e., s : X q X k1, where q 0 denotes thenumber of last context tokens to use to produce the speculation (i.e. q = 0 for the unigram and q = 1for the bigram), and the speculation s returns the top-k next-word token predictions according to theN-gram. Speculative decoding using our model derived unigram / bigram can be easily implemented in thefollowing manner: i) repeat the context3 to form a batch of k identical rows; ii) append a columncorresponding to the top-k speculations to the end of the batch; iii) call the model on the batch toverify all speculations (rows) in a single forward pass. While adding redundant computation (regarding repeat flops for the context), this implementation isextremely simple to integrate into existing code, and in addition, is fully-compatible with popularinference methods, e.g. , flash-attention [Dao et al., 2022, Dao, 2023] and paged-attention / vLLM[Kwon et al., 2023], which is generally not the case for methods that require custom attention maskingsuch as tree / lookahead attention masking [Fu et al., 2024, Cai et al., 2024]. Extensions.The model-derived bigram and unigram allow for speculating w = 1 token intothe future. In addition, by repeatedly applying either the model bigram (or, alternatively greedily,decoding with the model from the bigram), we can easily extend the model-derived N-grams tospeculate w > 1 tokens into the future s : X q X kw. Just like the model bigram, this extensioncan be generated quickly one time and stored as a O(1) lookup table. Top-k speculation. provides evidence motivating our approach, depicting how speculatingwith the top-k tokens increases the number of tokens per call for a 7B model on the first 50 examplesof MT-Bench and Human Eval, for both the unigram and bigram models, obtained directly from theTransformer. We remark that with w = 2, speculating with the top-25 of the model bigram gives a 50% increase in tokens per call.",
  "Context-derived N-grams": "Another natural idea to obtain N-grams for speculations is to look within the context provided to amodel; indeed this was suggested by Leviathan et al. as a potential avenue for future work. Wepropose looking for all previous occurrences of the last q 1 tokens of the context, and speculatingwith the w 1 tokens that follow a match. To define a discrete probability distribution, we can assigneach match a count i.e. how many times it occurred in the context, with ties being decided by whichmatch occurred later in the context (hence prioritizing more recent matches). For more details on thecontext N-gram please refer to the attached code in Appendix B.2.",
  "Mixed strategies": "For a chosen batch size k > 1, one has the flexibility of using any combination of strategies i.e.model / context derived N-grams, to populate the batch of speculations. This allows for the explorationof diverse combinations of speculation methods. In this work we consider the a straightforward wayto mix strategies: first, we populate the k drafts by using as many speculations from the contextderived N-gram model as possible, depending on how many matches are obtained (possibly zero);then we use a extended model bigram to fill in the remaining speculations. This means that thenumber of speculations allocated to each strategy (context/model bigram) is variable depending onthe context at each step of decoding, which we ablate in .2.",
  "Experiments": "Datasets and models.We assess our proposed mixed strategies described in .3 usingthe same experimental setup and datasets as Cai et al. , Li et al. : MTBench Zhenget al. (a multi-turn question benchmark with many unique tokens), HumanEval Chen et al. (a coding benchmark), GSM8K Cobbe et al. ) (mathematical reasoning problems). Weexperimented with three different instruction-tuned models of various sizes: Phi-3 Abdin et al. ,Mistral7B Jiang et al. and Vicuna13B Zheng et al. . All models are freely availablefrom the Hugging-Face transformers library, with reference url links detailed in the Appendix C. Allexperiments were run on a single Nvidia A100 GPU with 40GB of memory at bfloat-16 precision.We report two metrics of interest: 1. tokens per call: measures how many tokens are produced in a single model call on average,i.e., the acceptance rate. This would be the observed speed-up if one had both true parallelismand zero cost speculation. 2. wall-time speed-up: this is the physical observed speed-up on the hardware. To obtainaccurate timings, we used the CUDA Runtime API and ran all experiments three times,reporting the mean and standard deviation. Mixed strategies.We consider mixed strategies (as detailed in .2), defined by valuesk {1, 5, 10, 20, 25} and w {2, 4, . . . , 14}, with query length q = 14 when deriving speculationsfrom the context. The resulting (k, w) grid totals 35 different strategies to assess, whose performancewill be dictated by trade-offs between i) token per call acceptance and ii) potential compute-boundslowdowns. For comparative purposes, we include the results reported by lookahead decoding [Fu et al., 2024],an effective learning-free guess-and-verify method, using custom-attention masks to grow an N-gramcache in parallel to verifying speculations. Contrary to look-ahead decoding, our method doesnot require custom attention masks, due to the naive batching (P3), and is hence fully compatiblewith methods such as flash-attention [Dao et al., 2022, Dao, 2023], and is comparatively simpler tointegrate / implement. We also include results reported by REST (Retrieval-Based Speculative Decoding) [He et al., 2023],a recent approach which also utilizes negligible draft models. REST requires pre-processed databasesto retrieve tree-based speculations. In contrast, our method requires no external data, using only thecontext and model derived N-grams. We note that it is important to exercise caution when formingexact comparisons between methods and models, since additional factors such as hardware5, tokenizerand instruction formatting will impact the observed speedups for all reported methods.",
  "For each dataset and model, we report the strategy that led to the largest wall-time speedup, which wedenote (k, w). As a reference, we also reported (k, w) = (10, 10), to compare how a square input": "4We experimented with longer query length q = 2 and q = 3, but observed a degradation in both speed-upand tokens per call across all data sets and models.5Lookahead used a GPU with a higher operations-to-bytes (OTB) ratio than our experiments, whilst theexperiments of RAST were conducted on a GPU with a lower OTB ratio.",
  "block, representative of a default non-optimized choice in our sweep fared compared to (k, w). shows the mean and standard deviation across the three runs, for all models and datasets": "For Mistral7B, the average wall-time speedups for the complete grid of strategies is depicted in. The grids all show a clear tradeoff between tokens-per-call (by increasing either k and/orw) and compute-bound slowdowns, with the pattern of shared across the three different taskssuggesting a consistent relationship between (k, w) and speed-up. For reference, the correspondingtokens per call are reported in Appendix A.1. The equivalent plots for both Phi3B and Vicuna13Bcan be found in Appendix A. For Phi3B, we notably observed the model never reached an OTP ratiothat was large enough to incur slowdowns which would outweigh increases in tokens per call. Forthis reason the optimal values were trivially those of maximum value, i.e. (k, w) = (25, 14) ; thetrue optimal speed-up using our batched approach would hence occur at a larger (k, w). Overall, our methods consistently achieve more than 2x speedup across models and tasks (except forthe 7B model on MT bench, which attained a 1.91 times speedup). While the optimal (k, w) variedbetween models and datasets, it can be seen that the representative default (10, 10) achieved goodperformance on all of the settings.",
  "Ablation on strategies": "In order to understand the role that both the model- and context-derived N-grams play in the observedspeed-ups, we ablate the Mistral7B experiment for (k, w) = (10, 10) across the three data sets. Weexplore i) the number of speculated tokens accepted by both the model and context derived N-gramsii) the rank of accepted speculations amongst the top-10 speculations iii) the amount of drafts i.e.rows in the batch, that each of the strategies used. The results are depicted in . Our ablations shine light on the strengths and weaknesses of both the model bigram and contextderived draft strategies. The model-bigram is robust across all tasks, with an additional 1-2 futuretokens frequently found within its top-10 predictions, with the ranking distribution being notablyheavy-tailed (relative to that of the context-derived N-gram). The models bigram weakness lies in its w k 1.361.411.471.491.511.53 1.601.751.791.841.871.83 1.671.841.831.871.911.64 1.671.721.811.591.571.50 1.651.701.521.521.471.39 Speedup MT-Bench 1.75 2.5 w k 1.411.521.551.591.601.65 1.671.891.952.001.981.98 1.751.951.992.052.041.78 1.771.912.001.751.741.66 1.751.881.681.661.631.54 Speedup HumanEval 1.75 2.5 w k 1.431.551.551.571.571.58 1.731.921.921.981.951.91 1.852.011.932.072.031.76 1.902.042.071.811.761.66 1.912.061.771.741.661.54 Speedup GSM8k 1.75 2.5",
  ": Average wall-time speedup across datasets for Mistral7B instruct for varied (k, w)": "Tokens / Call 0% 10% 20% 30% 40% 50% 60% MT-Bench Tokens / Call 0% 10% 20% 30% 40% 50% 60% HumanEval Tokens / Call 0% 10% 20% 30% 40% 50% 60% GSM8K basemodel bigramcontext 123456789100% 20% 40% 60% 123456789100% 20% 40% 60% 123456789100% 20% 40% 60% context Ranking of accepted speculation 0% 5% 10% 15% 20% 25% Ranking of accepted speculation 0% 10% 20% 30% Ranking of accepted speculation 0% 10% 20% 30%model bigram Strategy allocation 0% 10% 20% 30% 40% 50% 60% Strategy allocation 0% 10% 20% 30% 40% Strategy allocation 0% 5% 10% 15% 20% 25% 30% 35% 40% contextmodel bigram : Ablations: Top: distribution of acceptance length for mixed strategies. Middle: distributionof ranking of accepted speculations amongst the top-k. Bottom: allocation distribution of strategiesi.e. number of speculations for each strategy. ineffectiveness for larger values of w, which is expected since it only considers the last token of thecontext rather than encompassing all prior context, making it insufficient for longer speculations. On the other hand, we see that the context derived N-gram compliments the bigrams weakness asit can successfully speculate further into the future, with speculations of length w = 10 acceptedon all tasks. However, its performance is notably less robust across tasks. For example, GSM8Kexhibits a wider distribution of accepted lengths due to the varied sizes of calculations in the mathword-problems, while HumanEval more frequently accepts w = 10 length speculations due tothe coding nature of the task. Furthermore, it exhibits more pronounced diminishing returns frombatching (compared to the bigram), which is a weakness given that it is often allocated the entirebatch for speculations (see the bottom row of ). This suggests that further research intoenhancing strategy allocation could indeed yield further additional gains.",
  "Conclusions": "We introduced a set of learning-free strategies for generating batches of speculative drafts, extractedfrom both the base model and context. Our approach is conceptually simple and is fully compatiblewith other optimization techniques (e.g. quantization, early exiting, flash attention, etc.). Experi-mentally, we observed that our proposed strategies led to significant speedups in auto-regressiveinference, while requiring minimal implementation overhead and being easily integrable. Our analysisdemonstrates that simple strategy combinations can substantially enhance performance across a rangeof different tasks and model sizes, with our ablations shining light on the strengths and weaknessesof the proposed strategies. Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, HanyAwadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al. Phi-3 technical report:A highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman,Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report.arXiv preprint arXiv:2303.08774, 2023. Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXivpreprint arXiv:2305.10403, 2023. Ben Athiwaratkun, Sujan Kumar Gonugondla, Sanjay Krishna Gouda, Haifeng Qian, Hantian Ding,Qing Sun, Jun Wang, Jiacheng Guo, Liangfu Chen, Parminder Bhatia, Ramesh Nallapati, SudiptaSengupta, and Bing Xiang. Bifurcated Attention for Single-Context Large-Batch Sampling, March2024. URL arXiv:2403.08845 [cs]. Nikhil Bhendawade, Irina Belousova, Qichen Fu, Henry Mason, Mohammad Rastegari, and MahyarNajibi. Speculative streaming: Fast llm inference without auxiliary models. arXiv preprintarXiv:2402.11131, 2024. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models arefew-shot learners. Advances in neural information processing systems, 33:18771901, 2020. Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D Lee, Deming Chen, and Tri Dao.Medusa: Simple llm inference acceleration framework with multiple decoding heads. arXivpreprint arXiv:2401.10774, 2024. Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and JohnJumper. Accelerating large language model decoding with speculative sampling. arXiv preprintarXiv:2302.01318, 2023a. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, JaredKaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri,Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan,Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian,Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, FotiosChantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino,Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders,Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa,Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, BobMcGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. EvaluatingLarge Language Models Trained on Code, July 2021. URL arXiv:2107.03374 [cs].",
  "John L Hennessy and David A Patterson. Computer architecture: a quantitative approach. Elsevier,2011": "Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot,Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier,Llio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, ThomasWang, Timothe Lacroix, and William El Sayed. Mistral 7B, October 2023. URL arXiv:2310.06825 [cs]. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E.Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language modelserving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on OperatingSystems Principles, 2023.",
  "Andrey A Markov. An example of statistical investigation of the text eugene onegin concerning theconnection of samples in chains. Science in Context, 19(04):591600, 1913": "Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang, Rae Ying Yee Wong,Zhuoming Chen, Daiyaan Arfeen, Reyna Abhyankar, and Zhihao Jia. Specinfer: Acceleratinggenerative llm serving with speculative inference and token tree verification. arXiv preprintarXiv:2305.09781, 1(2):4, 2023. Andrea Santilli, Silvio Severino, Emilian Postolache, Valentino Maiorca, Michele Mancusi, RiccardoMarin, and Emanuele Rodol. Accelerating transformer inference for translation via paralleldecoding. arXiv preprint arXiv:2305.10427, 2023.",
  "Mitchell Stern, Noam Shazeer, and Jakob Uszkoreit. Blockwise parallel decoding for deep autore-gressive models, 2018. URL": "Heming Xia, Tao Ge, Peiyi Wang, Si-Qing Chen, Furu Wei, and Zhifang Sui. Speculative decoding:Exploiting speculative execution for accelerating seq2seq generation. In Findings of the Associationfor Computational Linguistics: EMNLP 2023, pages 39093925, 2023. Ji Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, and Jimmy J. Lin. Deebert: Dynamic early exiting foraccelerating bert inference. In Annual Meeting of the Association for Computational Linguistics,2020. URL",
  "DKey-Value Cache": "We use a static key-value cache based upon the implementation from Cai et al. , Li et al. .However, we add minimal modifications to i) allow for batching ii) over-write all rows to be thatof the maximum length accepted speculation iii) initialize from a k = 1 cache (since the context isrepeated), via a broadcasting."
}