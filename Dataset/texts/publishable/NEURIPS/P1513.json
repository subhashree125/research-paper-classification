{
  "Abstract": "We study episodic linear mixture MDPs with the unknown transition and adver-sarial rewards under full-information feedback, employing dynamic regret as theperformance measure. We start with in-depth analyses of the strengths and limita-tions of the two most popular methods: occupancy-measure-based and policy-basedmethods. We observe that while the occupancy-measure-based method is effectivein addressing non-stationary environments, it encounters difficulties with the un-known transition. In contrast, the policy-based method can deal with the unknowntransition effectively but faces challenges in handling non-stationary environments.Building on this, we propose a novel algorithm that combines the benefits of bothmethods. Specifically, it employs (i) an occupancy-measure-based global optimiza-tion with a two-layer structure to handle non-stationary environments; and (ii) apolicy-based variance-aware value-targeted regression to tackle the unknown tran-sition. We bridge these two parts by a novel conversion. Our algorithm enjoys anO(d",
  "H3K+": "HK(H + PK)) dynamic regret, where d is the feature dimension,H is the episode length, K is the number of episodes, PK is the non-stationaritymeasure. We show it is minimax optimal up to logarithmic factors by establishinga matching lower bound. To the best of our knowledge, this is the first work thatachieves near-optimal dynamic regret for adversarial linear mixture MDPs withthe unknown transition without prior knowledge of the non-stationarity measure.",
  "Introduction": "Reinforcement Learning (RL) studies the problem where a learner interacts with the environmentsand aims to maximize the cumulative reward [Sutton and Barto, 2018], which has achieved significantsuccess in games [Silver et al., 2016], robotics [Kober et al., 2013], large language model [Ouyanget al., 2022] and so on. The interaction is usually modeled as Markov Decision Processes (MDPs).Research on MDPs can be broadly divided into two lines based on the reward generation mechanism.The first line of work [Jaksch et al., 2010, Azar et al., 2013, 2017, He et al., 2021] considers thestochastic MDPs where the reward is sampled from a fixed distribution. In many real-world scenarios,however, the assumption of fixed reward distributions may not hold, as rewards can vary over time.This motivates the study on adversarial MDPs [Even-Dar et al., 2009, Yu et al., 2009, Zimin andNeu, 2013, Jin et al., 2020a], where rewards might change in an adversarial manner. To address thechallenges of large-scale MDPs, recent studies have extended these two frameworks to incorporatefunction approximation, allowing RL algorithms to handle large state and action spaces. Two popularmodels are linear mixture MDPs [Ayoub et al., 2020] and linear MDPs [Jin et al., 2020b]. In this work, we focus on linear mixture MDPs with adversarial rewards, unknown transition andfull-information feedback. Though significant advances have been achieved for this setting [Cai et al.,2020, He et al., 2022], they choose static regret as the performance measure, which benchmarks the",
  "arXiv:2411.03107v1 [cs.LG] 5 Nov 2024": ": Comparisons of dynamic regret guarantees with previous works studying adversarial linearmixture MDPs with the unknown transition and full-information feedback. Here, d is the featuremapping dimension, H is the episode length, K is the number of episodes, PK and PK are two kindsof non-stationarity measure defined in (3) satisfying PK HPK [Zhao et al., 2022, Lemma 6].",
  "k=1V kk,1(sk,1),(1)": "where V k,1(sk,1) is the expected cumulative reward of policy starting from initial state sk,1 atepisode k and is the policy set. While static regret is a natural choice for online MDPs, the best-fixed policy may perform poorly when the rewards change adversarially. To this end, an enhancedmeasure called dynamic regret is proposed in the literature [Zhao et al., 2022, Li et al., 2023], whichbenchmarks the learners policies against a sequence of changing policies. This measure is defined as",
  "k=1V kk,1(sk,1),(2)": "where c1, . . . , cK is any sequence of policies in the policy set that can be chosen with completeforeknowledge of online reward functions. The dynamic regret in (2) is a stronger notation as itrecovers the static regret in (1) directly by setting c1:K arg maxKk=1 V k,1(sk,1). An idealdynamic regret bound should scale with a certain variation quantity of compared policies denoted byPK(c1, . . . , cK) or PK(c1, . . . , cK) that can reflect the degree of environmental non-stationarity. While the flexibility of dynamic regret makes it well-suited for adversarial settings, it also presentssignificant challenges. The dynamic regret of tabular MDPs with full-information feedback has beenthoroughly studied by Zhao et al. and Li et al. [2024b], who achieved optimal dependenceon K and PK for the known and unknown transition settings, respectively, without requiring priorknowledge of the non-stationarity measure. However, the dynamic regret of adversarial linear mixtureMDPs is still understudied. With the prior knowledge of the non-stationarity measure, Zhong et al. proposed a policy optimization algorithm with the restart strategy [Zhao et al., 2020], achievinga result with suboptimal dependence in H, K and PK. Later, Li et al. significantly improvedtheir results by designing an algorithm with the optimal dynamic regret in K and PK, though thedependence on H remains suboptimal. For the more challenging scenarios where non-stationarity isunknown, Li et al. made an initial solution by introducing a two-layer policy optimizationalgorithm, albeit with an additional term in the dynamic regret involving the switching number of thebest base-learner. To address this limitation, Li et al. [2024b] developed an occupancy-measure-basedalgorithm with two-layer structure that achieves optimal dynamic regret in K and PK. However, theirresult incurs a polynomial dependence on the state space size S, which is statistically undesirable. In this work, we propose an algorithm that achieves the near-optimal dynamic regret in d, H, K andPK simultaneously for adversarial linear mixture MDPs with the unknown transition, without priorknowledge of the non-stationarity measure. We begin with in-depth analyses of the strengths andlimitations of two most popular methods: occupancy-measure-based and policy-based methods. Wefind that while the occupancy-measure-based method is effective in addressing the non-stationaryenvironments, it encounter difficulties with the unknown transition. In contrast, the policy-basedmethod can deal with the unknown transition effectively but faces challenges in handling non-stationary environments. To this end, we propose a novel algorithm that combines the benefits of both methods. Specifically, our algorithm employs (i) an occupancy-measure-based global optimizationwith a two-layer framework to handle the non-stationary environments; and (ii) a policy-basedvariance-aware value-targeted regression to handle the unknown transition. We bridge these two partsthrough a novel conversion. We show our algorithm achieves an O(d",
  "H3K +": "HK(H + PK))dynamic regret and prove it is minimax optimal up to logarithmic factors by establishing a matchinglower bound. presents the comparison between our result and previous works. Our resultsurpasses all previous results, even those that require prior knowledge of the non-stationarity measure. We note a similar combination was firstly employed in Ji et al. , but for distinctly differentpurposes. In their work, the occupancy-measure-based component is used to provide a horizon-free(independent of horizon length H) static regret whereas our objective is to address non-stationaryenvironments. One limitation of this hybrid approach is the computational complexity, which isdominated by the occupancy-measure-based component and thus expensive compared to policy-basedmethod. This issue is the inherent challenge for occupancy-measure-based method and also appearsin several studies [Zhao et al., 2023, Ji et al., 2024]. Nevertheless, our analyses suggest that theoccupancy-measure-based method offers unique advantages in handling non-stationary environments.Investigating whether similar results can be attained by other computationally efficient methods is animportant future work. We believe our work represents a significant step forward, as it is reasonableto prioritize achieving statistical optimality before focusing on computational efficiency. Organization. We review the related work in and formulates the setup in . Weanalyze the challenges and introduce our algorithm in and present the dynamic regret in. concludes the paper. Due to the page limits, we defer all proofs to the appendices.",
  "Related Work": "In this section, we review related works on the dynamic regret of MDPs in non-stationary environ-ments. The studies can be divided into two lines: non-stationary stochastic MDPs and non-stationaryadversarial MDPs. These two categories address distinct challenges and are studied separately. Non-stationary Stochastic MDPs. Non-stationary stochastic MDPs address scenarios where tran-sitions and rewards are stochastically generated from varying distributions. The non-stationaritymeasure is typically defined as the total variation of the transitions or rewards over time. For infinite-horizon MDPs, the seminal work of Jaksch et al. investigates the piecewise stationary settingwhere both the transitions and rewards are subject to changes at specific time and remain fixed inbetween. Ortner et al. further advance the field by allowing for changes at every step. Subse-quently, Cheung et al. introduce the Bandit-over-RL algorithm, which addresses the limitationsof earlier works by eliminating the need for prior knowledge about the non-stationarity. Additionaladvancements have been made in episodic non-stationary MDPs [Mao et al., 2021, Domingues et al.,2021] and episodic non-stationary linear MDPs [Touati and Vincent, 2020, Zhou et al., 2022]. Abreakthrough is the black-box method by Wei and Luo , which can transform any algorithmwith the optimal static regret under certain conditions, into another one that achieves optimal dynamicregret without prior knowledge of the non-stationarity. However, this method is inapplicable in adver-sarial settings. The limitation arises from its dependence on an optimistic estimator, constructed via aUpper Confidence Bound (UCB)-based algorithm for environmental change detection, a techniquethat performs well in stochastic environments but is less effective in adversarial scenarios. Non-stationary Adversarial MDPs. Non-stationary adversarial MDPs consider settings wherethe rewards are generated in an adversarial manner. The non-stationarity measure is defined as thevariation of arbitrary changing compared policies, allowing the policy to adapt to non-stationaryenvironments implicitly. An illustrative difference between non-stationary stochastic and adversarialMDPs is that, in some cases, even if the rewards and transitions change over time, the optimal policymay remain fixed. The dynamic regret of tabular MDPs with full-information feedback has beenthoroughly studied by Zhao et al. and Li et al. [2024b], who achieved optimal dependenceon K and PK for the known and unknown transition settings respectively, without requiring priorknowledge of the non-stationarity measure. However, the dynamic regret of adversarial linear mixture MDPs is still understudied. With the prior knowledge about the non-stationarity measure, Zhong et al. proposed a policy optimization algorithm with restart strategy [Zhao et al., 2020], achieving aresult with suboptimal dependence in H, K and PK. Later, Li et al. significantly improvedtheir results by designing an algorithm with optimal dynamic regret in terms of K and PK, thoughthe dependence on H remains suboptimal. For the more challenging scenarios where non-stationarityis unknown, Li et al. made an initial solution by introducing a two-layer policy optimizationalgorithm, albeit with an additional term in the dynamic regret involving the switching number of thebest base-learner. To address this limitation, Li et al. [2024b] developed an occupancy-measure-basedalgorithm with two-layer structure [Zhang et al., 2018, Yan et al., 2023, Zhao et al., 2024] thatachieves optimal dynamic regret in K and PK. However, their dynamic regret incurs a polynomialdependence on the state space size S, which is undesirable. In this work, we propose an algorithmthat achieves near-optimal dynamic regret in d, H, K and PK simultaneously for adversarial linearmixture MDPs with the unknown transition, without prior knowledge of the non-stationarity measure.",
  "We focus on episodic MDPs with the unknown transition and adversarial reward functions in thefull-information feedback setting. We introduce the problem formulation in the following": "Inhomogeneous, Episodic Adversarial MDPs. We denote an inhomogeneous, episodic adversarialMDP by a tuple M = {S, A, H, {Ph}h[H], {rk,h}k[K],h[H]}, where S is the state space withcardinality |S| = S, A is the action space with cardinality |A| = A, H is the length of each episode,Ph(|, ) : S A S is the transition with Ph(s|s, a) denoting the probability of transitingto state s given the state s and action a at stage h, and rk,h : S A is the reward functionfor episode k at stage h chosen by the adversary. A policy = {h}Hh=1 is a collection of hfunctions, where each h : S (A) maps a state s to a distribution over action space A. For any(s, a) S A, the state-action value function Qk,h(s, a) and value function V k,h(s) are defined as:",
  "where the expectation is taken over the randomness of and P. For any function V : S R, wedefine [PhV ](s, a) = EsPh(|s,a)[V (s)] and [VhV ](s) = [PhV 2](s, a) ([PhV ](s, a))2": "The interaction protocol is given as follows. At the beginning of episode k, the environment choosesthe reward functions {rk,h}h[H] and decides the initial state sk,1, where the reward function maybe chosen in an adversarial manner. Simultaneously, the learner decides a policy k = {k,h}h[H].Starting from the initial state sk,1, the learner chooses an action ak,h k,h(|sk,h), obtains thereward rk,h(sk,h, ak,h), and transits to the next state sk,h+1 Ph(|sk,h, ak,h) for h [H]. Afterthe episode k ends, the learner observes the entire reward function {rk,h}h[H]. The goal of thelearner is to minimize the dynamic regret in (2). Denote by T = KH the total steps. Linear Mixture MDPs. We focus on linear mixture MDPs, which was introduced by Ayoub et al. and has been studied by subsequent works [Cai et al., 2020, Zhou et al., 2021, Li et al., 2024c].Definition 1 (Linear Mixture MDPs). An MDP M = {S, A, H, {Ph}h[H], {rk,h}k[K],h[H]} iscalled an inhomogeneous, episode B-bounded linear mixture MDP, if there exist a known featuremapping (s|s, a) : S A S Rd and an unknown vector h Rd with h2 B, h [H],such that (i) Ph(s|s, a) = (s|s, a)h, (ii) V (s, a)2",
  "a,s qh(s, a, s), (s, a, s, h) S A S [H]": "We denote by the set of all occupancy measures satisfying the above two properties. For a transitionP, denote by (P) the set of occupancy measures whose induced transition Pq is exactly P. Fora collection of transitions P, denote by (P) the set of occupancy measures whose inducedtransition Pq is in the transition set P. We use qk = qP,k, qck = qP,ck to simplify the notation.",
  "Analysis of Two Popular Methods": "Occupancy-measure-based and policy-based methods are the two most popular approaches for solvingadversarial MDPs. Both methods have been extensively studied in the literature and shown to enjoyfavorable static regret guarantees [Zimin and Neu, 2013, Cai et al., 2020]. However, when it comesto dynamic regret, both methods face significant challenges. We introduce the details below.",
  "Framework I: Occupancy-measure-based Method": "The first line of work [Zimin and Neu, 2013, Rosenberg and Mansour, 2019, Jin et al., 2020a] em-ployed the occupancy-measure-based method for adversarial MDPs. This method use the occupancymeasure as a proxy for the policy, optimizing over the occupancy measure rather than the policy di-rectly. While the value function is not convex in the policy space, it becomes convex in the occupancymeasure space, making this approach theoretically more attractive compared to policy-based method.However, this shift introduces new challenges for dynamic regret analysis, as discussed below. Using the concept of the occupancy measure, the dynamic regret in (2) can be rewritten asD-RegK(c1:K) = Kk=1 V ckk,1(sk,1) Kk=1 V kk,1(sk,1) = Kk=1qck qk, rk. By this conver-sion, the online MDP problem is reduced to the standard online linear optimization problem overthe occupancy measure set (P) induced by the true transition P. However, the true transition P isunknown and thus the decision set (P) is inaccessible. To address this issue, a general idea is toconstruct a confidence set Pk at episode k that contains the true transition with high probability thenreplace the decision set by (Pk). Then the dynamic regret can be decomposed into:",
  ",(4)": "where qk is an occupancy measure in the decision set (Pk). The first term is the dynamic regretof online linear optimization over the decision set (Pk), which has been explored in the knowntransition setting [Zhao et al., 2022]. This term can be well controlled by a two-layer structure, asdemonstrated in the online learning literature [Zhang et al., 2018, Yan et al., 2023, Zhao et al., 2024].",
  "The second term reflects the approximation error introduced by using the confidence set Pk as asurrogate for the true transition P, which constitutes the primary challenge of this approach": "Key Difficulties. To control the approximation error in (4), previous works [Rosenberg and Mansour,2019, Jin et al., 2020a] proposed to bound the term Kk=1qk qk1, leveraging Hlders inequality:qk qk, rk qk qk1rk and rk SA. While this approach is effective for tabularMDPs, it fails to exploit the inherent structure of linear mixture MDPs. Although the transition kernelP exhibit a linear structure, the occupancy measure is not linear and retains a complex recursiveform, as highlighted in Zhao et al. . Consequently, the regret bound resulting from this methoddepends on the state space size S, which is undesirable for linear mixture MDPs.Remark 1. Occupancy-measure-based method optimizes a global object that encodes the entirepolicy across all states, which sacrifices some computational efficiency to better handle non-stationaryenvironments. Thanks to their global optimization property, these methods offer favorable dynamicregret guarantees. However, they struggle to address the unknown transition in linear mixture MDPsand suffer from an undesirable dependence on the state space size S in the dynamic regret bound.",
  "Framework II: Policy-based Method": "Policy-based method directly optimizes the policy, making it easier to implement and computationallymore efficient than occupancy-measure-based method. More importantly, it shows advantages inhandling unknown transitions, as it only requires estimating the value function, bypassing the needto estimate the transition kernel explicitly. However, due to their inherent local-search nature, thismethod faces challenges in adapting to non-stationary environments and providing favorable dynamicregret guarantees. We outline the key challenges below.",
  "h=1EckQkk,h(sh, ), ck,h(|sh) k,h(|sh),(5)": "where the expectation is taken over the changing policy sequence c1, . . . , cK. For each state sh, theterm Kk=1Hh=1Qk,h(sh, ), ck,h(|sh) k,h(|sh) is exactly the regret of a A-armed banditproblem, with Qkk,h(sh, ) being the reward vector. Thus, it indicates the dynamic regret of onlineMDPs can be written as the weighted average of MAB dynamic regret over all states, where theweight for each state is its (unknown and time-varying) probability of being visited by c1, . . . , cK. Since the transition is unknown, the policy-based method need to estimate the value function Qkk,h ateach state. By the definition of linear mixture MDPs in Definition 1, for any Vk,h(), it holds that[PhVk,h+1](s, a) = Vk,h+1(s, a), h. Therefore, learning the underlying transition parameters hcan be regarded as solving a linear bandit problem where the context is Vk,h+1(sk,h, ak,h) andthe noise is Vk,h+1(sk,h, ak,h) [PhVk,h+1](sk,h, ak,h). This observation enables the application oflinear bandits techniques to learn the value function effectively for the policy-based method. The key challenge for the policy-based method lies in handling the non-stationarity of environment.Even with accurate estimates of Qkk,h at each state, optimizing the dynamic regret remains difficult.To optimize (5), Li et al. proposed running the Exp3 algorithm [Auer et al., 2002] at each stateand employing the fixed-share technique [Herbster and Warmuth, 2001, Cesa-Bianchi et al., 2012],forcing uniform exploration to deal with non-stationary environments. They update policies byk,h(|s) k1,h(|s) exp Qk1k1,h(s, ),k,h(|s) = (1 )k,h(|s) + u(|s), where is the step size, u(|s) is uniform distribution, and is the fixed-share parameter. They showit ensures D-RegK(c1:K) O(KH3 +H(1+PK)/). To achieve a favorable dynamic regret, thestep size needs to be set as (1 + PK)/(KH2), which is impractical as the non-stationaritymeasure PK is unknown. The standard approach to address this issue in the online learning literatureis to adopt the online ensemble framework [Zhao et al., 2024]. However, this approach encounterschallenges in online MDPs as the dynamic regret in (5) involves the expectation of changing policies,which does not appear in the standard online learning setting. We elaborate on this issue below.",
  "Specifically, the standard procedure of the two-layer framework is as follows. First, we construct astep size pool H = {1, . . . , N} (N = O(log K)) to discretize the range of the optimal step size": "Subsequently, multiple base-learners B1, . . . , BN are maintained, with each associated with a stepsize i H. Finally, a meta-algorithm is employed to track the unknown best base-learner. Then, thedynamic regret can be decomposed into two parts: (i) the dynamic regret of the best base-learner; and(ii) the regret of the meta-algorithm to track the best-learner. Formally,",
  "Our Method: A Novel Combination": "By the analysis in .1, we observe that the occupancy-measure-based method is proficient inaddressing non-stationary environments but shows limited compatibility with unknown transitions. Incontrast, the policy-based method can deal with unknown transitions efficiently but faces challengesin handling non-stationary environments. To this end, we propose a new algorithm named Occupancy-measure-based Optimization with Policy-based Estimation (OOPE), which combines the benefitsof both methods. At a high level, OOPE algorithm consists of two components: (i) an occupancy-measure-based global optimization with a two-layer framework to deal with the non-stationarity ofenvironments; and (ii) a policy-based value-targeted regression to handle the unknown transition.We bridge the two components through a novel analysis that converts the occupancy-measure-basedapproximation error into the policy-based estimation error. We elaborate on the details below.",
  "The occupancy-measure-based optimization follows Li et al. [2024b], using online mirror descent forupdating the occupancy measure and a two-layer structure to manage non-stationary environments": "We first construct a step size pool H = {1, . . . , N} to discretize the range of the optimal step size,then maintain multiple base-learners B1, . . . , BN, each of which is associated with a step size i H.Finally, we use a meta-algorithm to track the best base-learner. At each episode k, we constructa confidence set Ck such that h Ck,h with high probability. The details of the construction ofconfidence set will be introduced later. Then, the base-learner Bi updates the occupancy measure by",
  "T(H log (S2A) + PK log T)": "Remark 3. By the occupancy-measure-based optimization with a two-layer structure, we can handlethe first term in (4) well. It remains to bound the approximation-error term Kk=1qk qk, rk,which arises from employing the confidence set Ck as a surrogate for true transition parameter . Implementation Details of Algorithm 1. The main computation complexity arises from the onlinemirror descent step of (6) in Line 4. This step can be divided into into an unconstrained optimizationproblem and a projection problem. The unconstrained optimization problem can be solved by theclosed-form solution and the main computational cost lies in the projection step. Ji et al. showthat though such a projection can not be formulated as a linear program, they can be efficiently solvedby the Dysktras algorithm as the decision set is an intersection of convex sets of explicit linear orquadratic forms. We refer the readers to Appendix D of Ji et al. for more details.",
  "Occupancy Measure to Policy Conversion": "As discussed in .1.1, previous works [Rosenberg and Mansour, 2019, Jin et al., 2020a, Liet al., 2024b] propose to control the approximation error by bounding the term Kk=1qk qk1.However, though the transition P admits a linear structure, the occupancy measure does not andretains a complex recursive form, which introduces an undesired dependence on the state number Sin the final regret. To take advantage of strength of policy-based method in integrating with linearfunction approximation, we propose to learn value functions as a whole instead of directly controllingthe occupancy measure discrepancies. This strategy diverges from traditional methods that boundqk qk, rk by the transition discrepancies Hh=1 s,aPh(|s, a) Pk,h(|s, a)1, where Pk isthe estimated transition in episode k. Instead, we opt to constrain qk qk, rk through the valuedifference, which effectively integrates reward information. We introduce the details below.",
  "Policy-based Value-targeted Regression": "It remains to build the value function to ensure Vk,h Vk,h. A key observation is that the occupancymeasure qk induces a new MDP whose transition lies in the confidence set Ck with high probability.Thus, it suffices to ensure that Vk,h is an overestimate of the true value function in the confidence set.",
  "2j,h+ 22,": "where 2j,h is the upper confidence bound of the variance [VhVj,h+1](sj,h, aj,h) and is set as 2k,h =max{H2/d, [Vk,hVk,h+1] (sk,h, ak,h)+Ek,h}, where [Vk,hVk,h+1](sk,h, ak,h) is an estimate for thevariance of value function Vk,h+1 under the transition Ph(|sk, ak), and Ek,h is the bonus term to guar-antee the true variance [Vk,hVk,h+1](sk,h, ak,h) is upper bounded by [Vk,hVk,h+1](sk,h, ak,h)+Ek,hwith high probability. By definition, we have [VhVk,h+1](sk,h, ak,h) = k,h,1, h [k,h,0, h]2.Thus, we set [Vk,hVk,h+1] (sk,h, ak,h) = [k,h,1, k,h][0,H2] [k,h,0, k,h]2[0,H], where k,h isused to estimate the second-order moment and is constructed as:",
  "HK(H + PK)for unknown non-stationaritymeasure cases in Li et al. [2024b], our bound removes the dependence on the state number S": "Then, we establish the dynamic regret lower bound for this problem.Theorem 2. Suppose B 2, d 4, H 3, K (d 1)2H/2, for any algorithm and any constant [0, 2KH], there exists an adversarial inhomogeneous linear mixture MDP and a policy sequencec1, . . . , cK such that PK and D-RegK(c1:K) (",
  "K). occupancy-policy-gap. As the value functions are optimistic estimators over the confidence set Ck,the value gap between the occupancy measure and the policy is guaranteed to be non-positive": "estimation-error. Let Qk,h = Qk,hQkk,h and Vk,h = Vk,hV kk,h, then define policy noise Mk,h,1 =Eak(|sk,h)[Qk,h(sk,h, a)] Qk,h(sk,h, ak,h), transition noise Mk,h,2 = [Ph(Vk,h)](sk,h, ak,h) Vk,h(sk,h+1) and bonus k,h = Qk,h (rk,h + PhVk,h+1). The estimation error can be decomposedinto transition noises, policy noises, and bonuses, i.e., Kk=1Hh=1(Mk,h,1 + Mk,h,2 + k,h). Thetransition and policy noises can be bounded by O(",
  "Conclusion and Future Work": "In this work, we study the dynamic regret of adversarial linear mixture MDPs with the unknowntransition. We observe the occupancy-measure-based method is effective in addressing non-stationaryenvironments but struggles with unknown transitions. In contrast, the policy-based method can dealwith unknown transitions effectively but faces challenges in handling non-stationary environments.To this end, we propose a new algorithm that combines the benefits of both methods, achieving anO(",
  "HK(H + PK)) dynamic regret without prior knowledge of the non-stationaritymeasure. We show it is optimal up to logarithmic factors by establishing a matching lower bound": "Currently, we achieve this result by employing a hybrid method. Exploring whether similar results canbe attained using computationally more efficient methods is an important future work. Furthermore,extending our results to other MDP classes, such as generalized linear function approximation [Wanget al., 2021] and multinomial logit function approximation [Li et al., 2024a], is an interesting direction.",
  "Mohammad Gheshlaghi Azar, Rmi Munos, and Hilbert J. Kappen. Minimax PAC bounds on thesample complexity of reinforcement learning with a generative model. Machine Learning, 91(3):325349, 2013": "Mohammad Gheshlaghi Azar, Ian Osband, and Rmi Munos. Minimax regret bounds for reinforce-ment learning. In Proceedings of the 34th International Conference on Machine Learning (ICML),pages 263272, 2017. Qi Cai, Zhuoran Yang, Chi Jin, and Zhaoran Wang. Provably efficient exploration in policy optimiza-tion. In Proceedings of the 37th International Conference on Machine Learning (ICML), pages12831294, 2020.",
  "Nicolo Cesa-Bianchi and Gbor Lugosi. Prediction, Learning, and Games. Cambridge UniversityPress, 2006": "Nicol Cesa-Bianchi, Pierre Gaillard, Gbor Lugosi, and Gilles Stoltz. Mirror descent meets fixedshare (and feels no regret). In Advances in Neural Information Processing Systems 25 (NIPS),pages 989997, 2012. Wang Chi Cheung, David Simchi-Levi, and Ruihao Zhu. Reinforcement learning for non-stationaryMarkov decision processes: The blessing of (more) optimism. Proceedings of the 37th InternationalConference on Machine Learning (ICML), pages 18431854, 2020. Omar Darwiche Domingues, Pierre Mnard, Matteo Pirotta, Emilie Kaufmann, and Michal Valko. Akernel-based approach to non-stationary reinforcement learning in metric spaces. In Proceedingsof the 24th International Conference on Artificial Intelligence and Statistics (AISTATS), pages35383546, 2021.",
  "Chi Jin, Zeyuan Allen-Zhu, Sbastien Bubeck, and Michael I. Jordan. Is Q-learning provablyefficient? In Advances in Neural Information Processing Systems 31 (NeurIPS), pages 48684878,2018": "Chi Jin, Tiancheng Jin, Haipeng Luo, Suvrit Sra, and Tiancheng Yu. Learning adversarial Markovdecision processes with bandit feedback and unknown transition. In Proceedings of the 37thInternational Conference on Machine Learning (ICML), pages 48604869, 2020a. Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I. Jordan. Provably efficient reinforcementlearning with linear function approximation. In Proceedings of the 33rd Conference on LearningTheory (COLT), pages 21372143, 2020b.",
  "Long-Fei Li, Peng Zhao, and Zhi-Hua Zhou. Dynamic regret of adversarial linear mixture MDPs. InAdvances in Neural Information Processing Systems 36 (NeurIPS), pages 6068560711, 2023": "Long-Fei Li, Yu-Jie Zhang, Peng Zhao, and Zhi-Hua Zhou. Provably efficient reinforcement learningwith multinomial logit function approximation. In Advances in Neural Information ProcessingSystems 37 (NeurIPS), page to appear, 2024a. Long-Fei Li, Peng Zhao, and Zhi-Hua Zhou. Dynamic regret of adversarial MDPs with unknowntransition and linear function approximation. In Proceedings of the 38th AAAI Conference onArtificial Intelligence (AAAI), pages 135721358, 2024b. Long-Fei Li, Peng Zhao, and Zhi-Hua Zhou. Improved algorithm for adversarial linear mixtureMDPs with bandit feedback and unknown transition. In Proceedings of the 27th InternationalConference on Artificial Intelligence and Statistics (AISTATS), pages 30613069, 2024c. Weichao Mao, Kaiqing Zhang, Ruihao Zhu, David Simchi-Levi, and Tamer Basar. Near-optimalmodel-free reinforcement learning in non-stationary episodic MDPs. In Proceedings of the 38thInternational Conference on Machine Learning (ICML), pages 74477458, 2021. Ronald Ortner, Pratik Gajane, and Peter Auer. Variational regret bounds for reinforcement learning.In Proceedings of the 35th Conference on Uncertainty in Artificial Intelligence (UAI), pages 8190,2019. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, ChongZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to followinstructions with human feedback.Advances in Neural Information Processing Systems 35(NeurIPS), pages 2773027744, 2022. Aviv Rosenberg and Yishay Mansour. Online convex optimization in adversarial Markov decisionprocesses. In Proceedings of the 36th International Conference on Machine Learning (ICML),pages 54785486, 2019. David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driess-che, Julian Schrittwieser, Ioannis Antonoglou, Vedavyas Panneershelvam, Marc Lanctot, SanderDieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy P. Lillicrap,Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the gameof Go with deep neural networks and tree search. Nature, pages 484489, 2016.",
  "Ahmed Touati and Pascal Vincent. Efficient learning in non-stationary linear Markov decisionprocesses. ArXiv preprint, arXiv:2010.12870, 2020": "Yining Wang, Ruosong Wang, Simon Shaolei Du, and Akshay Krishnamurthy. Optimism in rein-forcement learning with generalized linear function approximation. In Proceedings of the 9thInternational Conference on Learning Representations (ICLR), 2021. Chen-Yu Wei and Haipeng Luo. Non-stationary reinforcement learning without prior knowledge: Anoptimal black-box approach. In Proceedings of the 34th Conference on Learning Theory (COLT),pages 43004354, 2021. Yu-Hu Yan, Peng Zhao, and Zhi-Hua Zhou. Universal online learning with gradient variations: Amulti-layer online ensemble approach. In Advances in Neural Information Processing Systems 36(NeurIPS), pages 3768237715, 2023.",
  "Lijun Zhang, Shiyin Lu, and Zhi-Hua Zhou. Adaptive online learning in dynamic environments. InAdvances in Neural Information Processing Systems 31 (NeurIPS), pages 13301340, 2018": "Canzhe Zhao, Ruofeng Yang, Baoxiang Wang, and Shuai Li. Learning adversarial linear mixtureMarkov decision processes with bandit feedback and unknown transition. In Proceedings of the11th International Conference on Learning Representations (ICLR), 2023. Peng Zhao, Lijun Zhang, Yuan Jiang, and Zhi-Hua Zhou. A simple approach for non-stationarylinear bandits. In Proceedings of the 23rd International Conference on Artificial Intelligence andStatistics (AISTATS), pages 746755, 2020.",
  "AProof of Lemma 1": "Proof. Without loss of generality, we assume that all states are reachable with positive probabilityunder the uniform policy u(a|s) = 1/A, s S, a A. Otherwise, we can simply removethe unreachable states from the state space. Assume K is large enough such that the occupancymeasure of qP,u (P, 1/T). We define uk = (1 1",
  "which the first inequality follows from Holders inequality, the second holds by qck qP,u1 2H,and the last holds for any i [N]. Next, we bound base-regret and meta-regret separately": "Upper bound of base-regret. Since the true transition parameter is contained in the confidenceset Ck by condition, we ensure that uk (P, 1/T 2) (Ck, 1/T 2). By the update rule of qikin (6), we ensure that qik (Ck, 1/T 2), i [K]. The update rule in (6) can be rewritten as:",
  "B.2.1Proof of Lemma 4": "Proof. The proof is similar to that of Ji et al. [2024, Lemma 6.1]. The only difference is that qk isa weighted combination rather than a single occupancy measure in the decision set. For k [K],the occupancy measure qk is given by qk = Ni=1 pikqik. Since qik (Pk, ), we ensure thatqk (Pk, ). Thus, for occupancy measure qk, there exist such that",
  "Vk,h(s) = Eak(|s) Qk,h(s, a),Vk,H+1(s) = 0": "Then, we can prove this lemma by induction. The conclusion trivially holds for n = H + 1.Suppose the statement holds for n = h + 1, we prove it for n = h. For any (s, a) S A, sinceQk,h(s, a) H, so if Qk,h(s, a) = H, then it holds directly. Otherwise, we have",
  "DProof of Theorem 2": "Proof. Our proof is similar to that of Li et al. [2023, Theorem 4]. At a high level, we prove thislower bound by noting that optimizing the dynamic regret of linear mixture MDPs is harder than (i)optimizing the static regret of linear mixture MDPs with the unknown transition, (ii) optimizing thedynamic regret of linear mixture MDPs with the known transition. Thus, we can consider the lowerbound of these two problems separately and combine them to obtain the lower bound of the dynamicregret of linear mixture MDPs with the unknown transition. First, we consider the lower bound of the static regret of adversarial linear mixture MDPs with theunknown transition. From lower bound in He et al. [2022, Theorem 5.3], since the dynamic regretrecovers the static regret by choosing the best-fixed policy, we have the following lower bound fordynamic regret in this case:",
  "K log (SA)).(20)": "Case 2: > 2H. Without loss of generality, we assume L = /2H divides K and split the wholeepisodes into L pieces equally. Next, we construct a special policy sequence such that the policysequence is fixed within each piece and only changes in the split point. Since the sequence changesat most L 1 /2H times and the occupancy measure difference at each change point is at most2H, the total path length in K episodes does not exceed . As a result, we have"
}