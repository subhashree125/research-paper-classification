{
  "Abstract": "Semi-supervised learning (SSL) offers a robust framework for harnessing the po-tential of unannotated data. Traditionally, SSL mandates that all classes possesslabeled instances. However, the emergence of open-world SSL (OwSSL) intro-duces a more practical challenge, wherein unlabeled data may encompass samplesfrom unseen classes. This scenario leads to the misclassification of unseen classesas known ones, consequently undermining classification accuracy. To overcomethis challenge, this study revisits two methodologies from self-supervised and semi-supervised learning, self-labeling and consistency, tailoring them to address theOwSSL problem. Specifically, we propose an effective framework called OwMatch,combining conditional self-labeling and open-world hierarchical thresholding. The-oretically, we analyze the estimation of class distribution on unlabeled data throughrigorous statistical analysis, thus demonstrating that OwMatch can ensure the un-biasedness of the self-label assignment estimator with reliability. Comprehensiveempirical analyses demonstrate that our method yields substantial performanceenhancements across both known and unknown classes in comparison to previousstudies. Code is available at",
  "Introduction": "Deep learning has made remarkable success in various tasks by leveraging substantial labeled trainingdata . However, the costly and time-consuming labeling process limits their application inpractical scenarios. Semi-supervised learning (SSL) significantly reduces the dependency on labeleddata by exploring the inherent structure of unlabeled data . Despite promising results, SSLmethods assume a closed-world scenario where, though limited, all classes possess labeled instances.This assumption may be violated due to difficulties in data collection, such as in medical diagnostics,where it is common to encounter new symptoms or fail to annotate due to technical constraints. As aresult, only a subset of the categories can be precisely labeled during the annotation process. Recently,numerous studies have sought to identify such novel classes effectively. Open-world SSL (OwSSL)is innovative in promoting dual objectives: classifying instances of seen classes and discoveringinstances of novel classes . A notable challenge in OwSSL is the confirmation bias of model: model tends to predict instancesas seen classes owing to the lack of ground-truth supervision of novel-class instances. To eliminatethis bias, existing works utilize unsupervised clustering methods, including contrastive loss andbinary cross-entropy (BCE) loss, to group pairs identified by similarity metrics . Amongthese unsupervised techniques, self-labeling has shown remarkable success, which involvesassigning self-labels to unlabeled data, with the generation of high-quality self-labels being the key",
  "(b)": ": Experimental results on the OwSSL problem. (a) Self-label assignment of seen classes(1-5) and novel classes (6-10) with or without conditional component in self-labeling. (b) Predictiveconfidence and hierarchical threshold for each class. factor. Previous studies utilize optimal transport to align the self-labels for unlabeled data with agiven distribution. However, this self-label generation fully relies on the accurate prior distributionand lack of consideration of the supervision of labeled data. In TRSSL , the unlabeled dataare assigned with a soft self-label based on the inaccurate class distribution, which raises a biasedestimation. Moreover, the confirmation bias still exists even if we use the ground-truth distributionto align the unlabeled data in the same process. In addition to confirmation bias, a new issue calledclustering misalignment arises when self-labeling depends solely on unlabeled data: without properguidance, the self-labeling process may adopt varying criteria for clustering. For example, it mightcluster data based on superficial features like color rather than high-level semantic information.This misalignment can lead to results that deviate from expected outcomes and even contradict theclassification criteria established by labeled data. Consequently, we propose a new self-labeling scheme, conditional self-labeling, designed to addressthe challenges of OwSSL, particularly targeting issues related to confirmation bias and misalignment.This scheme limits self-labels for each class and incorporates labeled data to generate debiasedand informative self-label assignments for all training data, further mitigating the confirmationbias as shown in a. Additionally, as illustrated in b, seen classes typically exhibithigher predictive confidence, while novel clusters demonstrate variability in their internal learningprogresses. The disparities in learning paces between seen and novel classes, coupled with theirdistinct behaviors, necessitate the selection of appropriate thresholds to facilitate cluster learning.To address these challenges and ensure a balanced learning process across classes, we propose ahierarchical thresholding scheme. We demonstrate our contributions as follows: 1) We introduce a novel conditional self-labeling methodto incorporate labeled data into the clustering process, reducing confirmation bias and misalignment.2) We design a hierarchical thresholding strategy that balances learning difficulties across differentclasses, helping unstable clusters gradually form. 3) Our theoretical analysis rigorously discusses theunbiasedness and reliability of conditional self-labeling estimator from population-level statistics. Tothe best of our knowledge, this is the first work proposing the expectation of chi-square statistics (ECS)to evaluate the reliability of self-label assignment estimation. 4) We conduct extensive experimentson various datasets, demonstrating the effectiveness of our approach, OwMatch, through detailedcomparisons. On CIFAR-10, OwMatch significantly outperforms FixMatch by up to 47.3%in all-class accuracy, while on CIFAR-100, it enhances TRSSL , the state-of-the-art model inOwSSL, by up to 14.6% in novel-class and 7.2% in all-class accuracy.",
  "Related work": "Traditional semi-supervised learning (SSL).Traditional SSL assumes that labeled and unlabeleddata share an identical distribution. Extensive researches on SSL have spanned a considerable duration.The commonly employed strategies in SSL consist of entropy minimization , consistencyregularization and holistic methods . The latest progresses in SSL include adaptivethresholding strategies , which enhance model performance by accounting for varying",
  ": Illustration on the hier-archical thresholding scheme": "difficulties and learning conditions across classes, alongside other innovative techniques that employ self-SL approaches to facilitate extracting the semantic information from unlabeleddata. However, traditional SSL algorithms typically struggle to tackle the open-world problem in thepresence of novel-class instances within unlabeled data. Open-set semi-supervised learning (OSSL).OSSL expands the traditional SSL boundaries byallowing novel-class instances or outliers within unlabeled data. A variety of OSSL approacheshave emerged in recent years . A common solution among these methods is theoptimization of the SSL objective exclusively for unlabeled samples deemed inliers. For instance,MTC optimizes the network and estimates the anomaly score of unlabeled data alternately.OpenMatch and T2T train one-vs-all (OVA) classifiers for each known class to detectoutliers. Subsequently, standard SSL objective are applied to the remaining training data,excluding detected outliers. Furthermore, DS3L leverages a bi-level optimization technique totrain a weighting function, which mitigates the passive impact of out-of-distribution (OOD) samples.Nonetheless, these approaches are designed for classifying seen classes, thereby failing to learn fromthe novel class instances. Open-world semi-supervised learning (OwSSL).OwSSL has been proposed to address apractical challenge: enabling the model to effectively cluster novel-class instances while maintainingclassification robustness on seen classes. One predominant research direction in this under-exploreddomain is BCE-based methods, including ORCA and NACH . Additionally, there existmethods to discover novel classes by employing various other clustering techniques: OpenLDN employs bi-level optimization to train a pairwise similarity prediction network, which provides asupervisory signal to the similarity of all pairs; TRSSL converts clustering into the self-labelingproblem and applies Sinkhorn-Knopp algorithm to optimize self-label assignments. One subsequentlyproposed Generalized Category Discovery (GCD) setting is similar to the OwSSL , withdetailed discussion is provided in .3.",
  "i=N l+1, where N = N l + N u and N u N l. Here x(i) Rd is the i-th instance": "with one-hot vector y(i)gt {0, 1}K as the corresponding label , where K is the number of all classes.We denote the set of classes in Dl as Cl and the set of classes in Du as Cu. Previous traditional SSLstudies assume Cl = Cu. Here for OwSSL, we assume Cl = Cu and Cu \\Cl = . Denote Cs = Cl Cuas a set of seen classes, Cn = Cu \\ Cl as a set of novel classes, and C = Cl Cu as a set of allconsidered classes. The desired OwSSL model is required to assign instances to either a previouslyseen class c Cs, or a novel class c Cn. For labeled dataset Dl, standard supervised objective is employed as shown in Equation 9. Addi-tionally, OwMatch primarily incorporates two objectives: a) clustering objective, which leveragesconditional self-labeling to refine the self-label assignment with the assistance of supervision; b)confidence objective, which applies consistency loss with open-world hierarchical thresholdingstrategy to enhance predictive confidence and balance the different learning difficulties across allclasses. We will elaborate on them respectively in .1 and 3.2.",
  "Conditional self-labeling": "To effectively cluster the novel class instances, the self-labeling scheme has been considered inOwSSL. Formally, consider a deep neural network (encoder) f mapping input data x to representationz RD, the representation is followed by a classification head h : RD RK, usually consisting of asingle linear layer, converting the feature vectors into a vector of class scores. Denote g = h fas a probability function, where refers to the SoftMax function. Moreover, denote q(i) RK as thesoft self-label for x(i), and set Q = [q(1), q(2), . . . , q(N)] RKN as the self-label assignment for{x(i)}Ni=1. Asano et al. utilize a constraint of desired partition of Q to construct the transportationpolytope:Q1 := {Q RKN+|Q1N = NP, QT 1K = 1N},(1)where 1v is the v-dimensional vector of all ones, P denotes the desired class distribution. On theother hand, we can obtain a probability output through p(i) = g((x(i))), where () refers toa specific weak augmentation, and denote P = [p(1), p(2), . . . , p(N)] as the matrix of probabilityoutputs. This self-label assignment generation can be understood as solving an optimal transportproblem . It minimizes the cross-entropy loss and aligns the training data with the desired classdistribution:minQQ1 Tr(Q log(PT )),(2) where Tr() is the trace of a given matrix. Obviously, clustering through self-labeling primarilyrelies on the quality of generated self-label assignments. However, optimizing self-label assignmentsthrough unsupervised self-labeling is unreliable owing to the lack of supervision. TRSSL utilizesthe above-unsupervised technique to optimize self-label assignment merely on unlabeled data withthe uniform class distribution. Despite prominent results, this unconditional self-labeling process has a notable flaw: it constructs transportation polytope based on an inaccurate class distribution. Moreover, we consider conducting self-labeling across all training data and constructing a trans-portation polytope with a precise class distribution. To mitigate the confirmation bias, we propose aconditional self-labeling method to refine the self-label assignment under partial supervision. Specifi-cally, we exploit the ground-truth labels from the labeled dataset and introduce another constraintQ2:Q2 := {Q RKN+|q(i) = y(i)gt , i = 1, . . . , N l}.(3)Now, the conditional self-label assignment generation with the above two constraints can be formu-lated as:minQQ1Q2 Tr(Q log(PT )) + E(Q),(4) where E() is the entropy function, is a hyper-parameter controlling the smoothness of Q. We adoptfast version of Sinkhorn-Knopp algorithm to optimize Equation 4 efficiently and denote theoptimal solution as Q = [q(1), q(2), . . . , q(N)]. Empirically, conditional self-labeling significantlyalleviates the confirmation bias, resulting in self-label assignments that are much closer to theexpected distribution, as shown in a. Further theoretical analysis regarding estimators fromunconditional and conditional self-labeling is provided in . Then, the clustering objectivehas the form of: Lcls = 1",
  "Open-world hierarchical thresholding": "Beyond the clustering objective, prompting the predictive confidence has proven effective for clas-sification. A similar goal arises in traditional SSL, wherein entropy minimization is employed toencourage low entropy (i.e., high confidence) in the prediction. FixMatch leverages both con-sistency and pseudo-labeling to achieve exceptional performance with the following regularization:Ni=1 I(max(p(i)) )H(p(i), g(A(x(i)))), where p(i) := arg max(p(i)) is predictive one-hotpseudo-label, with the p(i)-th element set to 1. and A represent weak and strong augmentationrespectively. Here, is a scalar hyperparameter denoting the threshold above which we retain apseudo-label. The effectiveness of the aforementioned regularization depends on accurate and suffi-cient pseudo-labels, which are directly influenced by the thresholding scheme. Under the close-wordassumption, extensive efforts have been devoted to devising thresholding techniquesbased on the idea of balancing learning pace across classes with varying learning difficulties. How-ever, these techniques do not fit with the open-world scenario due to a critical challenge: the learning",
  "pace of novel classes tends to be much slower . The predictive confidence of these two groupsdoes not share the same behavior, as shown in b": "We introduce an open-world hierarchical thresholding scheme to balance this inconsistent learningpace at the group level, leveraging these well-defined thresholds to retain high-quality and adequatepseudo-labels for learning. As shown in , this scheme first estimates the learning conditionsof the two groups and then hierarchically modulates the thresholds in a class-specific fashion withineach group. First, we split the dataset into seen (Cs) and novel (Cn) groups based on the pseudo-label and estimatetheir overall learning condition by predictive confidence. Motivated by FreeMatch , we definethe group-wise learning status for a set of classes Ci = Cs or Cn as",
  "Theoretical analysis of conditional self-labeling": "To illustrate the superiority of conditional self-labeling over unconditional, we evaluate their estima-tors of the class distribution on unlabeled data through rigorous statistical analysis. This transforma-tion is justified as both self-labeling methods produce corresponding self-label assignments, eachrepresenting their estimation of the class distribution on unlabeled data. Formulation.Assuming that the class distribution of real-world data conforms to prior informa-tion P = [p1, p2, , pK]. Suppose real-world data is composed of recognized labeled data andunrecognized unlabeled data, conforming to unknown class distribution Pl = [pl1, pl2, , plK] andPu = [pu1, pu2, , puK] respectively. We independently sample N = N l + N u instances fromrecognized and unrecognized data, respectively. Suppose Ni = N li +N ui is composed of two randomvariables that denote the number of recognized and unrecognized samples belonging to the i-th class.Obviously, we have N l = Ki=1 N li and N u = Ki=1 N ui . Objective.We hope to estimate the unknown class distribution Pu with based on prior in-formation P and observations of N l1, N l2, , N lK, then evaluate from unbiasness and ECS.Evaluation on both metrics requires estimating the number of samples in each class, denoted by A = (A1, A2, . . . , AK). Two self-labeling approaches (unconditional and conditional) can optimizeself-label assignment, therefore obtaining two approximations of A, denoted by Auncon and Acon.Denote the corresponding estimators as uncon and con.",
  "Npi = N lpli + N upui .(10)": "Lemma 4.2. Suppose we want to test the null hypothesis (H0) that categorical data N1, N2, , NCcome from a multinomial distribution with K classes and class probability of P. A chi-squarestatistic can be constructed to test the deviation between the observations n1, , nK and expectedoutcomes for each class.",
  "EP[Ni] 2K1,(11)": "where EP[] denotes the population expectation of random variable. A lower chi-square valuesuggests that the observed data are consistent with H0. Conversely, an exceedingly high chi-squarevalue implies that either H0 is incorrect or an event of low probability has happened. Details of the above lemma are presented in Appendix E. Then, we define the following metric toevaluate the goodness of fit of estimation based on chi-square statistics.Definition 4.3 (Expectation of chi-square statistics (ECS)). The expectation of chi-square statis-tics (ECS) for are defined as the population deviation between the estimator of unlabeled classdistribution and its true distribution Pu:",
  "This section presents a comprehensive evaluation of our approach. It includes experimental resultsand in-depth analysis, demonstrating the effectiveness of our approach": ": Average accuracy on the CIFAR-10/100 and ImageNet-100 with both novel class ratio andlabel ratio of 50%. We compare OwMatch with existing literature on OwSSL. Also compared withother related approaches of traditional SSL, OSSL, and NCD approaches following . Proper modi-fications are made to make these approaches compatible with OwSSL; the details are in Appendix C.The results are averaged over three independent runs. The baseline figures are sourced from therespective papers.",
  "Experimental setup": "Datasets. We evaluate our approach on CIFAR-10/100 , ImageNet-100 and Tiny Im-ageNet . A detailed description of these datasets is provided in Appendix A. Specifically,ImageNet-100 dataset contains 100 classes sub-sampled from ImageNet-1k following . On alldatasets, we first split all classes into seen and novel classes with a novel class ratio. Subsequentexperiments will adopt a novel class ratio of 50% unless otherwise specified. Then, we will randomlyassign labels to a portion of the data from the seen classes according to the specified label ratio, whilethe remaining data, along with all samples from the novel classes, are assigned to the unlabeled set. Implementation details. For a fair comparison, we apply ResNet-50 as the backbone model forImageNet-100 and ResNet-18 for other benchmarks. We train the model with a batch size of 256 forTiny ImageNet and 512 for other benchmarks. Following , experiments across all benchmarks areimplemented based on the pre-trained model from SimCLR . We jointly optimize backbone andprototype parameters using the standard Stochastic Gradient Descent (SGD) with momentum. Weapply the cosine annealing learning rate schedule for all experiments. Techniques including multi-cropand queue structure are employed to enhance the clustering objective. Additionally, RandAugment serves as the strong augmentation for confidence objective. Additional implementation details areavailable in the Appendix B. Evaluation metric. In assessing the efficacy of OwMatch, we adopt a multifaceted approach toevaluate accuracy following . Evaluation metrics include the standard accuracy for seen classes andthe clustering accuracy for novel classes and all classes. Here, we leverage the Hungarian algorithm to align the predicted class assignment for novel-class instances with their ground-truth labels toobtain clustering accuracy. We also report the joint clustering accuracy across all classes using theHungarian algorithm.",
  "Main results": "We consider and evaluate two versions of our method, called OwMatch and OwMatch+. OwMatchrepresents the standard version as illustrated in , while OwMatch+ incorporates the multi-crop technique for additional augmentation. Detailed distinctions between the two versions areprovided in the Appendix B. We evaluate our method on all benchmarks using a label ratio of 10%and 50% with the comprehensive experiment results provided in , 12, and 13. Results in show that OwSSL approaches significantly outperform current state-of-the-art methods intraditional SSL, OSSL, and NCD by a considerable margin. On the other hand, OwMatch achieves state-of-the-art across all benchmarks and evaluation metrics. It can not only classify novel classesaccurately but also maintain robust performance on seen classes. On CIFAR-10, we observedOwMatch outperforms OpenLDN on novel and all classes by 2.0% and 1.4%, respectively. It isnoteworthy that the enhancement brought about by OwMatch is more pronounced on the CIFAR-100dataset, which presents a greater challenge due to the increasing number of classes. RegardingCIFAR-100, our method surpasses TRSSL by approximately 14.6% on novel classes and 7.2% onall classes. Subsequently, we extend to evaluate ImageNet-100 and observe a similar trend, withOwMatch+ showing significant improvement of 1.7% on all-class accuracy compared to previousstate-of-the-art approaches. Principle analysis of conditional self-labeling.OwMatch primarily relies on high-quality self-label assignment to alleviate the models confirmation bias. To clearly illuminate this progress duringtraining, we employ the Manhattan distance iK |ci cgti | as a metric to evaluate the bias betweenthe considered class distribution {ci}Ki=1 and the ground truth {cgti }Ki=1. demonstrates thedebiasing process: the models confirmation bias is pronounced in the early epochs, whereas thebias of optimized self-label assignment is relatively minor. As training advances, the self-labelassignment continues to guide the model, effectively mitigating the confirmation bias, as reflected inthe decreasing Bm and the absolute difference between Bm and Bs. : The Manhattan distance (MD) is used to evaluate the confirmation bias. The first rowpresents the bias between the models predictive class distribution and the ground truth, denotedas Bm. The second row reflects the bias between the self-label assignment and the ground truth,denoted as Bs. The third row computes the absolute difference between Bm and Bs, highlighting thedebiasing effect of high-quality self-label assignments.",
  "Ablations, analysis, and real-scenario applications": "To investigate the impact of each component, we embark on comprehensive ablation studies with bothnovel class ratio and label ratio of 50%. The first row in showcases the foundational modelperformance, whose objective consists of only unconditional clustering objective and supervisedobjective, already achieving impressive performance. We then analyze the effect of integrating aconditional self-labeling framework on CIFAR-100, which boosts novel-class accuracy by 1.0% onaverage. Additionally, the positive impact of consistency regularization is observed: roughly 0.9%enhancement across all evaluation metrics. Our ablation studies highlight the essential contribution ofeach component in OwMatch. Individually, each plays a significant part in the intended functionality,and together, these elements coalesce into a cohesive and robust framework. We also ablate otherfactors, including the number of local views for clustering objectives and iterations for the Sinkhorn-Knopp algorithm, with detailed statements provided in and 16. : Ablation studies on each component with both novel class ratio and label ratio of 50%. Here,ConSL refers to conditional self-labeling, PLCR refers to pseudo-label consistency regularization,and OwHT refers to an open-world hierarchical thresholding scheme.",
  "OwMatch+ResNet-50 91.579.685.5": "On the comparison between OwSSL and Generalized Category Discovery.The OwSSL settingresembles the subsequently proposed Generalized Category Discovery (GCD) setting , with bothassuming the existence of novel classes and that a portion of the data is labeled for seen classes.However, there are notable differences between these two groups of methods: 1) GCD-relatedmethods leverage supervised contrastive learning on labeled data and self-supervised contrastivelearning on all training data, whereas OwSSL typically employs pairwise similarity-based methodsfor clustering samples; 2) GCD-related works typically employ a pre-trained ViT-Base/16 backbone,which has significantly more parameters than the ResNet-18 or ResNet-50 models commonly used inOwSSL methods. It is unfair to compare these two types of methods directly. Here, we still include a comparisonwith those GCD-related works to demonstrate the effectiveness of our method. shows thatour method outperforms existing approaches in novel-class and all-class accuracy on ImageNet-100despite using a simpler model. Ablation study on supervision components in the overall objective.The overall objective ofOwMatch consists of a standard supervised objective, clustering objective, and confidence objective.Both the supervised and clustering objectives involve the use of labeled data, raising concerns aboutoverlap in functionality. To investigate the significance of each component, we conduct an ablationstudy in which we modify the overall objective in two ways: 1) removing the supervised objectiveand 2) excluding labeled data from the online clustering process. The results are reported in . In the first case, we observe a decrease in seen-class accuracy while maintaining novel-class clusteringperformance, while the latter case exhibits the opposite tendency: seen-class accuracy remains high,but novel-class clustering accuracy declines. In comparison to the previous cases, our overall objectiveintegrates both components to strike a balance between clustering and confidence. The supervisedobjective enhances seen-class accuracy through one-hot supervision, while the clustering objectivewith conditional self-labeling improves novel-class clustering accuracy by incorporating labeled data.This harmonious approach yields the best all-class accuracy while roughly maintaining both seen-and novel-class performance.",
  "However, in real-world scenarios, it is crucial to address the dependency on these assumptions. Wewill elaborate on each of these aspects to demonstrate the practical effectiveness of OwMatch": "Estimating the number of novel classes.OwMatch and other baselines typically assume that thenumber of novel classes is pre-determined for clarity in evaluation. However, this prior knowledgeis often unavailable in practice, necessitating a precise estimation of the number of novel classes inadvance. We primarily follow the approaches of GCD and TRSSL to estimate the numberof classes. Specifically, K-means clustering is performed on representations of the entire dataset fromthe pre-trained ViT-B/16 backbone. The optimal value of k is determined by evaluating the clusteringaccuracy on the labeled samples calculated by the Hungarian algorithm. This accuracy serves as ascoring function, optimized using Brents algorithm to find the that maximizes performance on thelabeled data. The estimation results across generic benchmarks are shown in , which illustratesthat the estimated class numbers come close to the ground truth. We also evaluate OwMatchssensitivity to varying extents of class number estimation error, with results reported in Appendix D.",
  "Tiny-ImageNetw/68.842.455.061.725.141.662.421.738.3w/o69.640.654.861.024.940.161.320.336.9": "Data imbalance.Most generic benchmarks feature class-balanced, whereas real-world data tend toexhibit long-tailed class distribution. Our approach accommodate to arbitrary class distribution byconstraining the optimized self-label assignment to comply with the prior class distribution, therebynaturally mitigating performance degradation caused by data imbalance. We evaluate our approach onimbalanced benchmarks, constructed with varying imbalance factors following TRSSL . Resultsin demonstrate that OwMatch effectively addresses the challenge of data imbalance. Training without prior.In scenarios where prior class distribution is unavailable, we propose anadaptive estimation scheme to make OwMatch still function without relying on any prior assumptions.Specifically, we initially adopt class-balanced prior if no prior information is available; then, theclass distribution for conditional self-labeling is estimated and continuously updated based on modelprediction. Next, standard training with estimated class distribution and distribution estimation arealternately conducted, with results reported in . We observe that the reduction in all-classaccuracy achieved through the adaptive estimation scheme remains within 3% across almost allbenchmarks and imbalance factors. These results reveal that the straightforward estimation techniqueperforms robustly in the absence of prior knowledge.",
  "Conclusion": "This work integrates techniques from self-SL and SSL, refining them to present a new perspectiveon solving open-world SSL. We demonstrate that conditional self-labeling can achieve an unbiasedestimation of the class distribution on unlabeled data with prior information, leading to high-qualityself-label assignment with reduced confirmation bias. Our future endeavors will be directed towarddeveloping solutions that are more aligned with realistic scenarios where such prior informationmight not be readily available or hard to be estimated. This will involve exploring methodologies thatcan effectively handle uncertainty and variability inherent in real-world data distributions.",
  "Acknowledgements": "Chao Wang would like to acknowledge the support from the National Natural Science Foundation ofChina under Grant 12201286, the Shenzhen Science and Technology Program 20231115165836001,Guangdong Basic and Applied Research Foundation 2024A1515012347. This research was conductedusing the computing resources provided by the Research Center for the Mathematical Foundations ofGenerative AI in the Department of Applied Mathematics at The Hong Kong Polytechnic University.",
  "Yuki M. Asano, Christian Rupprecht, and Andrea Vedaldi. Self-labelling via simultaneousclustering and representation learning. In International Conference on Learning Representations(ICLR), 2020": "Mahmoud Assran, Mathilde Caron, Ishan Misra, Piotr Bojanowski, Armand Joulin, NicolasBallas, and Michael Rabbat. Semi-supervised learning of visual features by non-parametricallypredicting view assignments with support samples. In Proceedings of the IEEE/CVF Interna-tional Conference on Computer Vision (CVPR), pages 84438452, 2021. David Berthelot, Nicholas Carlini, Ekin D. Cubuk, Alex Kurakin, Kihyuk Sohn, Han Zhang,and Colin Raffel. ReMixMatch: semi-supervised learning with distribution alignment andaugmentation anchoring. In International Conference on Learning Representations (ICLR),2020. David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin ARaffel. Mixmatch: A holistic approach to semi-supervised learning. Advances in NeuralInformation Processing Systems (NeurIPS), 32, 2019.",
  "Kaidi Cao, Maria Brbic, and Jure Leskovec. Open-world semi-supervised learning. arXivpreprint arXiv:2102.03526, 2021": "Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin.Unsupervised learning of visual features by contrasting cluster assignments. Advances in NeuralInformation Processing Systems (NeurIPS), 33:99129924, 2020. Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple frameworkfor contrastive learning of visual representations. In International Conference on MachineLearning (ICML, pages 15971607. PMLR, 2020. Yanbei Chen, Xiatian Zhu, Wei Li, and Shaogang Gong. Semi-supervised learning underclass distribution mismatch. In Proceedings of the AAAI Conference on Artificial Intelligence,volume 34, pages 35693576, 2020. Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practicalautomated data augmentation with a reduced search space. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition (CVPR) Workshops, pages 702703,2020.",
  "Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. Advances inNeural Information Processing Systems (NeurIPS), 26, 2013": "Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scalehierarchical image database. In Proceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition (CVPR), pages 248255. Ieee, 2009. Enrico Fini, Enver Sangineto, Stphane Lathuiliere, Zhun Zhong, Moin Nabi, and Elisa Ricci.A unified objective for novel class discovery. In Proceedings of the IEEE/CVF InternationalConference on Computer Vision (CVPR, pages 92849292, 2021.",
  "Lan-Zhe Guo, Yi-Ge Zhang, Zhi-Fan Wu, Jie-Jing Shao, and Yu-Feng Li. Robust Semi-Supervised Learning when Not All Classes have Labels. In Advances in Neural InformationProcessing Systems, May 2022": "Lan-Zhe Guo, Zhen-Yu Zhang, Yuan Jiang, Yu-Feng Li, and Zhi-Hua Zhou. Safe Deep Semi-Supervised Learning for Unseen-Class Unlabeled Data. In Proceedings of the 37th InternationalConference on Machine Learning, pages 38973906. PMLR, November 2020. Kai Han, Sylvestre-Alvise Rebuffi, Sebastien Ehrhardt, Andrea Vedaldi, and Andrew Zisser-man. Automatically discovering and learning new visual categories with ranking statistics. InInternational Conference on Learning Representations (ICLR), 2020. Kai Han, Andrea Vedaldi, and Andrew Zisserman. Learning to discover novel visual categoriesvia deep transfer clustering. In Proceedings of the IEEE/CVF International Conference onComputer Vision (CVPR), pages 84018409, 2019. Tao Han, Junyu Gao, Yuan Yuan, and Qi Wang. Unsupervised semantic aggregation anddeformable template matching for semi-supervised learning. Advances in Neural InformationProcessing Systems (NeurIPS), 33:99729982, 2020.",
  "Kaiming He, Georgia Gkioxari, Piotr Dollr, and Ross Girshick.Mask r-cnn.In IEEEInternational Conference on Computer Vision (ICCV), pages 29802988, 2017": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-age recognition. In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition (CVPR), pages 770778, 2016. Junkai Huang, Chaowei Fang, Weikai Chen, Zhenhua Chai, Xiaolin Wei, Pengxu Wei, LiangLin, and Guanbin Li. Trash to treasure: Harvesting ood data with cross-modal matching foropen-set semi-supervised learning. In Proceedings of the IEEE/CVF International Conferenceon Computer Vision (CVPR), pages 83108319, 2021. Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, AaronMaschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. Advances in neuralinformation processing systems, 33:1866118673, 2020.",
  "Dong-Hyun Lee. Pseudo-label : The simple and efficient semi-supervised learning methodfor deep neural networks. ICML 2013 Workshop : Challenges in Representation Learning(WREPL), 07 2013": "Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual adversarial training:a regularization method for supervised and semi-supervised learning. IEEE Transactions onPattern Analysis and machine intelligence, 41(8):19791993, 2018. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperativestyle, high-performance deep learning library. Advances in Neural Information ProcessingSystems (NeurIPS), 32, 2019.",
  "Mamshad Nayeem Rizve, Navid Kardan, and Mubarak Shah. Towards realistic semi-supervisedlearning. In European Conference on Computer Vision (ECCV), pages 437455. Springer, 2022": "Kuniaki Saito, Donghyun Kim, and Kate Saenko. Openmatch: Open-set semi-supervisedlearning with open-set consistency regularization. Advances in Neural Information ProcessingSystems, 34:2595625967, 2021. Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin A Raf-fel, Ekin Dogus Cubuk, Alexey Kurakin, and Chun-Liang Li. FixMatch: simplifying semi-supervised learning with consistency and confidence. Advances in Neural Information Process-ing Systems (NeurIPS), 33:596608, 2020. Xin Sun, Zhenning Yang, Chi Zhang, Keck-Voon Ling, and Guohao Peng. Conditional Gaussiandistribution learning for open set recognition. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition (CVPR), pages 1348013489, 2020.",
  "Xudong Wang, Long Lian, and Stella X Yu. Unsupervised selective labeling for more effectivesemi-supervised learning. In European Conference on Computer Vision (ECCV, pages 427445.Springer, 2022": "Yidong Wang, Hao Chen, Qiang Heng, Wenxin Hou, Yue Fan, Zhen Wu, Jindong Wang, MariosSavvides, Takahiro Shinozaki, Bhiksha Raj, et al. Freematch: Self-adaptive thresholding forsemi-supervised learning. arXiv preprint arXiv:2205.07246, 2022. Xin Wen, Bingchen Zhao, and Xiaojuan Qi. Parametric classification for generalized categorydiscovery: A baseline study. In Proceedings of the IEEE/CVF International Conference onComputer Vision, pages 1659016600, 2023.",
  "Qizhe Xie, Zihang Dai, Eduard Hovy, Thang Luong, and Quoc Le. Unsupervised data augmen-tation for consistency training. Advances in Neural Information Processing Systems (NeurIPS),33:62566268, 2020": "Yi Xu, Lei Shang, Jinxing Ye, Qi Qian, Yu-Feng Li, Baigui Sun, Hao Li, and Rong Jin. Dash:Semi-supervised learning with dynamic thresholding. In International Conference on MachineLearning, pages 1152511536. PMLR, 2021. Qing Yu, Daiki Ikami, Go Irie, and Kiyoharu Aizawa. Multi-task curriculum framework foropen-set semi-supervised learning. In European Conference on Computer Vision (ECCV), pages438454. Springer, 2020. Bowen Zhang, Yidong Wang, Wenxin Hou, Hao Wu, Jindong Wang, Manabu Okumura, andTakahiro Shinozaki. Flexmatch: Boosting semi-supervised learning with curriculum pseudolabeling. Advances in Neural Information Processing Systems (NeurIPS), 34:1840818419,2021. Sheng Zhang, Salman Khan, Zhiqiang Shen, Muzammal Naseer, Guangyi Chen, and Fa-had Shahbaz Khan. Promptcal: Contrastive affinity learning via auxiliary prompts for gener-alized novel category discovery. In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 34793488, 2023. Bingchen Zhao, Xin Wen, and Kai Han. Learning semi-supervised gaussian mixture models forgeneralized category discovery. In Proceedings of the IEEE/CVF International Conference onComputer Vision, pages 1662316633, 2023.",
  "Technical Appendices": "Roadmap of technical appendices.These appendices are structured as follows: Appendix Aintroduces dataset details utilized in our experiments. Appendix B outlines the implementationspecifics, including data augmentations and hyperparameters. Modification details for utilizedbaselines are provided in Appendix C. Additional experiment results consisting of supplementarymain results, in-depth analysis, and ablation study of hyperparameters are reported in Appendix D.Complete and rigorous proof of theoretical results is represented in Appendix E. Appendix F discussesthe social impacts of our work, and the limitations are considered in Appendix G",
  "ADatasets": "The details of the datasets utilized in our experiments are provided in , which includes datasetstatements, as well as the corresponding backbones and batch sizes for training. The choice ofbackbone and batch size matches previous works for fair comparison. For CIFAR-10/100datasets, we employ a simple pre-processing encompassing random crop with padding and horizontalflip. To make ResNet-18 compatible with CIFAR input data with a small resolution of 32 32, werefine the CNN by setting the kernel size to 3 3 and applying a stride of 1. We train the model witha batch size of 512 over 300 epochs. For ImageNet-100 and Tiny-ImageNet datasets, raw imagesare pre-processed with random resized crop and horizontal flip . We use the standard version ofResNet-50 and ResNet-18 as the backbone, respectively. We leverage the standard SGD method withmomentum and weight decay to optimize the network parameters; see hyperparameters in .",
  "BImplementation details": "Computational resources.The foundational algorithm of our study is constructed utilizing Python3.8 and PyTorch 2.1 . All experiments are carried out on NVIDIAs Tesla A100 GPU with 40Gmemory. All benchmarks are public and can be easily downloaded. Strong augmentation.Furthermore, we apply the strong augmentation to input data for all experi-ments following FixMatch , including random resized crop, horizontal flip, and RandAugment. It should be noted that the only additional enhancement in strong augmentation is RandAugmentcompared to basic pre-processing. Specifically, RandAugment randomly selects transformationsfrom a collection of options for each sample in a mini-batch. We employed the same sets of imagetransformations as those used in RandAugment. A complete list of these transformations can befound in the original work . OwMatch v.s. OwMatch+.We evaluate two versions of our approach in the main results, asillustrated in , 12 and 13. In short, OwMatch+ further incorporates the multi-crop technique asan additional augmentation to boost clustering capacity and, hence, improve model performance. Themulti-crop strategy was proposed by SwAV to additionally augment images by covering only smallsections. The resulting low-resolution images, referred to as local views, allow more augmentationsat a marginally increased computational cost. Compared to simple preprocessing (global views),local views generated through a multi-crop strategy involve resized cropping at smaller scales butin greater numbers, as illustrated in . And they experience additional color distortion consisting of random color jitters, solarizing, and equalization in pursuit of model robustness. Herewe apply the multi-crop technique to produce many low-resolution images (local views) and set thehyperparameters following PAWS , as detailed in .",
  "CIFAR-10(0.75,1)32(0.3,0.75)1840.5CIFAR-100(0.75,1)32(0.3,0.75)1840.5ImageNet-100(0.2,1)224(0.14,0.3)1841Tiny-ImageNet(0.75,1)64(0.3,0.75)3641": "Except for additional multi-crop augmentations, OwMatch and OwMatch+ differ slightly in the formof clustering objective. Specifically, we augment the input image by taking 2 full-resolution crops(normally and strongly augmented global views) and V low-resolution crops (local views). Note thatwe optimize the self-label q with only global views, since local views can only capture localizedsemantic information and are unable to provide a comprehensive overview of the entire image. Wepromote the model consistency by encouraging the prediction of different local views to be close tothe optimized self-labels. Specifically, the clustering objective of OwMatch+ is formulated by",
  ",(13)": "where q(i) correspond to the optimized self-label of global view, and g(1(x(i))), , g(V (x(i)))stand for predictions of V local views. Increasing the number of random low-resolution cropsencourages the model to learn global-to-local information , which reflects in performance gainacross all benchmarks; see main results in , 12 and 13. The utilization of low-resolutionimages boosts the models efficiency with only a marginal rise in computational costs. The effectiveness of conditional self-labeling is significantly compromised when the ratio of batchsize (B) to class numbers (K) is relatively small. In a scenario where a class is disproportionatelysampled in the labeled data of a batch, the conditional self-labeling mechanism might be unable toreassign the unlabeled data to that particular class. Notably, when this ratio falls below 1, assigningB samples to all K classes becomes unfeasible. Therefore, we leverage the queue structure to storedata from previous batches by following SwAV . In practice, a queue of 1024 logits are storedfor the implementation of the Sinkhorn-Knopp algorithm, which is utilized to derive the self-labelassignment. We then retain the logits from the last batch of the optimized self-labels to construct theclustering objective. Such queue length proves effective in our experiments with a large batch size(e.g., 512) and a relatively small number of categories (e.g., 100 classes for CIFAR-100). However,when dealing with high-resolution images that encompass a greater variety of categories, storingmuch more previous data information is essential. HyperparametersHere, we provide a comprehensive list of hyperparameters in . Forhyperparameters related to the SGD optimizer, we adhere to the settings used in the previous works to ensure a fair comparison. Regarding hyperparameters introduced by our proposed method,we perform ablation studies to determine the most appropriate values, specifically for SK-iterationand the number of local views, as detailed in Appendix D. These hyperparameters are selected basedon a balanced consideration of computational costs and model performance.",
  "CBaseline implementation details": "We compare our proposal with baselines from other settings: traditional SSL, open-set semi-supervised learning (OSSL), and novel class discovery (NCD). We will elaborate on the modificationsto these settings separately. Traditional SSL methods cannot deal with the novel-class instances andwe extend it in the following manner: samples are firstly divided into seen-class and novel-classinstances based on out-of-distribution (OOD) criteria, then we report the standard classificationaccuracy on seen-class instances and apply K-means algorithms to achieve clustering accuracy on thenovel-class instances. Hungarian algorithm is utilized to match the clustering result and theirground-truth labels, this result is reported as novel-class accuracy. For the traditional SSL method,FixMatch , we separate the OOD samples based on confidence scores produced by the Softmaxfunction. Many OSSL approaches like CGDL are naturally capable of detecting outliers, thus wedirectly cluster the considered outliers by the K-means algorithm and report the clustering accuracywithout manually inspecting OOD samples. Since DS3L applies the re-weighting technique todownsize the passive effect of OOD samples, we consider the samples with the lowest weight asoutliers. Both traditional SSL and weighting-based OSSL approaches depend on the OOD likelihoodscore to partition the inliers and outliers. Here we follow ORCA to determine the threshold forOOD samples by using ground-truth class information. NCD methods are trained to discover novel classes in unlabeled data with totally novel-class instances,thus failing to recognize seen-class instances. For NCD approaches without seen-class classificationheads, like DTC and RankStats , we report the performance on novel classes and extendthem to classify seen classes by assuming the seen-class instances in unlabeled data as novel. Thenwe extend the unlabeled classification head to include logits for seen classes by following andleverage the Hungarian algorithm to match the discovered classes with ground-truth labels withinlabeled data, the best assignment is reported as seen-class accuracy. And for recent UNO withexplicitly labeled classification heads, we generate pseudo-labels for both seen- and novel-classinstances based on model predictions from concatenated labeled and unlabeled classification heads.Therefore, both seen and novel class classification accuracy can be directly computed and reported.We apply the same pre-trained model on NCD methods to demonstrate that the enhanced performanceis not attributable to the the application of pre-training. Additionally, we present the clusteringoutcomes based on representations from the pre-trained model.",
  "(c) Epoch 100": ": Confusion matrices on CIFAR-10 with both novel class ratio and label ratio of 50%. Themodel needs to classify the initial five seen classes accurately (as reflected in the diagonal elements).While for the novel classes (6-10), the classes clustering are required to align with the ground-truthlabel (dark blue in one cell). Training process.We plot the confusion matrices on CIFAR-10 with both novel class ratio and labelratio of 50% in . This collection of images compellingly demonstrates that the bias derivedfrom novel classes is progressively mitigated as the experiment advances, leading to continuousimprovement in the models prediction accuracy. As depicted in a, at the beginning of training, the model struggles to effectively distinguishbetween novel and seen class instances, although it can classify seen class instances normally. Astraining progresses, an increasing number of samples are assigned to the novel classes and prediction accuracy for the seen-class instances advances, as illustrated in b. In the later stages oftraining as shown in c, the model becomes capable of accurately classifying the seen-classinstances and clustering novel-class instances simultaneously.",
  ": Accuracy as a function of class number estimation error on CIFAR-100 dataset": "Main results on Tiny-ImageNet.In addition to the results on CIFAR and ImageNet datasets,we also evaluate OwMatch and OwMatch+ on Tiny-ImageNet with 50% novel classes in .We found that both OwMatch and OwMatch+ surpass previous state-of-the-art methods acrossbenchmarks and evaluation metrics. Different novel class ratio.Previously, we assessed models performance with a constant novelclass ratio of 50%, which is also variable in real-world scenarios. Here, we alter this value and fix thelabel ratio within seen classes to 50%; the results are reported in . The models performancegenerally exhibits a declining trend across all benchmarks and evaluation metrics as the novel classratio increases. It is important to note that as the number of novel classes increases, the total amountof labeled data decreases.",
  "%96.597.196.880.163.971.968.842.455.060%96.592.193.980.360.468.168.541.151.770%97.591.093.081.358.665.371.334.745.680%98.492.994.078.958.061.869.532.940.090%97.893.694.082.050.753.569.426.430.3": "Efficient labeling strategy under a fixed budget.In the previously conducted experiments, weassessed the models performance by altering the novel class ratio and label ratio, respectively. Merelyaltering a single factor does not yield highly convincing inferences, as it is impractical to fix thenovel class ratio or the label ratio for some classes in real-world scenarios. Here, we consider a fixedbudget, specifically the total number of labeled data, as illustrated in . This comparison aimsto shed light on how the balance between labeled data in seen classes and the proportion of novelclasses influences model performance under a fixed level of supervision. When the supervisory information is held constant, a configuration with a smaller portion of labeleddata spread across a greater number of different classes results in higher accuracy for both all classesand novel classes. Additionally, it is observed that as the number of novel classes decreases, theaccuracy of the seen classes improves. This improvement is attributed to the reduced complexity ofthe classification task when there are fewer categories to be classified. From these observations, itcan be inferred that within a limited labeling budget, it is more effective to label a broader categoryof samples, thereby capturing as many representative points as possible within the feature space. Thisstrategy appears to optimize the models performance across both known and novel classes. Hierarchical thresholding scheme with scarce supervision information.Previously, we observedthat in the scenario where the label ratio on seen classes is 50%, the performance difference betweenadopting the hierarchical thresholding strategy and setting a high static threshold hyperparameter isnot significant. Here, we maintain the novel class ratio as 50% while decreasing the label ratio to10%. a and b demonstrate that a hierarchical thresholding scheme not only retainsmore pseudo-labels but also preserves the predictive accuracy of selected pseudo-labels.",
  ": Open-world hierarchical thresholding scheme generally selects more instances as pseudo-labels (a), and the quality of pseudo-labels is also enhanced (b)": "the model performance. Here we take the combination of color distortion and random resized crop as augmentation and apply a multi-crop technique to produce many low-resolution images (localviews). We compare the performance with varying number of local views, the results are reported in. The incorporation of local views significantly enhances model performance. However, its observedthat as the number of local views increases, the incremental benefits to the model come to plateau,while the training and computational time considerably escalate. This aligns with the notion thatlocal views assist in capturing local patterns, aiding in the development of robust representations.However, there is a threshold beyond which the addition of more local views contributes less tolearning, overshadowed by the rise in computational demands. Different number of iterations in the Sinkhorn-Knopp algorithm.Conditional self-labelingis proposed to optimize high-quality self-label assignments, which depends on a fast version of theSinkhorn-Knopp algorithm to solve this complex linear programs efficiently. Resolving thisinvolves iterative processes to converge on the optimal solution. We evaluate the model performanceacross different iterations, with the results presented in . Generally, model performance tends to increase with more iterations of the Sinkhorn-Knopp algorithm.Despite the positive correlation tendency, we observe that certain iterations (e.g., 6 for CIFAR-100with both novel class ratio and label ratio of 50%) already achieve satisfactory outcomes, with onlymarginal gains from further iterations.",
  "E.1Proof of Lemma 4.2": "Proof. Under the null hypothesis H0, the sample size of the i-th class follows the Binary distributionwith parameters of N and pi. Therefore, we have the expectation and standard error of Ni asEP(Ni) = Npi and SD(Ni) = Npi(1 pi), respectively, where the standard deviation of Nimeasures the average deviation of random variable Ni from its expected value. And for observationni for each class, the discrepancy can be denoted as ni EP(Ni). To ensure discrepancies for eachclass are evaluated on a consistent basis, dividing them by their standard errors under H0, whichenables us to focus on the standardized variableniEP(Ni)",
  "(Npi(1pi))1/2": "Note that Ki=1 EP(Ni) = Ki=1 Npi = N, therefore the discrepancies across the K classes cannotsimultaneously be all positive or all negative. To measure the discrepancies of all K classes, wesum the squares of the discrepancies of each class. This formulation ensures that discrepancies areindependent of sign, merely reflecting the deviation between observed and expected values under H0:",
  "Npi(1 pK) .(18)": "We then prefer to exclude the factors (1 pi) from the denominators of the sum for two primaryreasons. Firstly, if numerous classes exist and none of them has significant large probabilities,then (1 pi) is almost negligible. Moreover, when the expectation EP(Ni) is substantial, theapproximation of each discrepancy adheres to a standard normal distribution, which leads to theconstruct of chi-square statistics with the degree of freedom of K 1. Then the summary statisticcan be obtained as follows",
  "E.2Proof of Lemma E.1": "Proof. TRSSL posits that unlabeled and real data share the same class distribution, and optimizethe self-label assignment on unlabeled data solely based on prior information P. Owing to thealignment constraint between the generative self-label assignment and prior information, as shown in(1). Then the estimated number of samples in each class from conditional self-labeling is",
  "E.3Proof of Lemma E.2": "Proof. Different from an unconditional setting, conditional self-labeling considers partial supervision.Specifically, except for the constraint brought from prior information, an additional constraint isconstructed to realize the alignment between clustering results and existing labels in labeled data, asshown in (3). Then the estimated number of samples in each class from conditional self-labeling is",
  "Acon = [Np1 N l1, , NpK N lK],(24)": "where N, p1, p2, , pK are static values, and N l1, , N lK are a set of random variables. Note thatin a specific experiment, we can get a set of observations of N l1, , N lK, thus estimation based onconditional self-labeling is feasible. Then consider the estimator con =1",
  "FBroader impacts": "This research delves into the issue of semi-supervised learning (SSL) in situations where not allclasses possess labeled instances, an aspect that has received limited attention within the realm ofSSL. We aim to draw increased focus towards examining the resilience of SSL in diverse real-worldscenarios, thereby fostering a broader application of SSL in various contexts. However, the currentaccuracy is not very high for some challenging datasets. Therefore, the predictive results should bebest used as references rather than treated as ground truth.",
  "GLimitations": "OwMatch, similar to existing methods in OwSSL, faces a significant challenge when applied toimbalanced datasets or unknown prior class distribution. Existing OwSSL methods are typicallyapplied on class-balanced datasets where instances of each class share nearly the same frequency; themodel performance would deteriorate when encountering imbalanced datasets. On the other hand,prior class distributions are not available in real-world applications. Addressing the dependency onprior class distribution and effectively handling datasets of arbitrary composition remain challengingfor existing OwSSL algorithms, including OwMatch. Recognizing this, we propose an adaptive estimation scheme for the OwMatch framework and showits feasibility in the experiments, with results reported in . Although a performance declinewithin 3% may seem acceptable, it is worth further consideration and exploration to determinewhether further optimizations can enhance model performance without any prior. At the same time,we aim to prove the convergence of this adaptive algorithm in our future work."
}