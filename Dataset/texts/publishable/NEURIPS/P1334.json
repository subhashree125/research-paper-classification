{
  "Abstract": "Quantization of Deep Neural Network (DNN) activations is a commonly usedtechnique to reduce compute and memory demands during DNN inference, whichcan be particularly beneficial on resource-constrained devices. To achieve highaccuracy, existing methods for quantizing activations rely on complex mathemat-ical computations or perform extensive searches for the best hyper-parameters.However, these expensive operations are impractical on devices with limited com-putation capabilities, memory capacities, and energy budgets. Furthermore, manyexisting methods do not focus on sub-6-bit (or deep) quantization.To fill these gaps, in this paper we propose DQA (Deep Quantization of DNNActivations), a new method that focuses on sub-6-bit quantization of activationsand leverages simple shifting-based operations and Huffman coding to be efficientand achieve high accuracy. We evaluate DQA with 3, 4, and 5-bit quantizationlevels and three different DNN models for two different tasks, image classificationand image segmentation, on two different datasets. DQA shows significantlybetter accuracy (up to 29.28%) compared to the direct quantization method and thestate-of-the-art NoisyQuant for sub-6-bit quantization.",
  "Introduction": "Quantization is a popular compression technique to reduce the compute and memory demands ofDeep Neural Networks (DNNs). During inference, DNN weights are typically fixed, so they canbe quantized offline. However, activations are dynamically generated, which means that effectivequantization methods must dynamically quantize activations online. There are many methods for quantizing activations. The most straightforward one is to directlyuse the mathematical definition of quantization , but it provides limited accuracy. To get higheraccuracy, more sophisticated approaches are used. For example, NoisyQuant injects noise in theactivations before quantization and removes the noise after de-quantization (the opposite operation toquantization). However, these methods are not specifically designed for resource-constrained devices,where the computational cost of expensive mathematical operations (e.g. matrix multiplication) orlarge online search spaces is impractical. Furthermore, many of these methods do not focus on deepquantization (e.g., quantizing to less than 6 bits), which is preferred for devices with limited memory. To fill these gaps, in this paper we propose DQA (Deep Quantization of DNN Activations), a newmethod to deeply quantize DNN activations suited for resource-constrained devices that provideshigh accuracy. DQA first determines the importance of each activation channel offline using trainingor calibration datasets. For important channels, DQA quantizes their values with m extra bits first andthen right-shifts the m bits to achieve the target number of bits while storing the shifting errors usingHuffman coding. Then, the shifting errors are decoded and added back to the corresponding channelsduring the de-quantization phase. For unimportant channels, DQA uses a direct method to quantizeand de-quantize activations, i.e., it simply applies the mathematical definition of quantization/de-quantization [5; 11]. By learning important channels offline, DQA saves significant computational",
  "arXiv:2412.09687v1 [cs.LG] 12 Dec 2024": "resources during DNN inference. In addition, by using m more bits, DQA can quantize importantchannels with a different number of bits (i.e., mixed-precision quantization [1; 15]), reduce thequantization error exponentially, and efficiently shift back to the target bits without expensivemathematical computations. It is also important to note that: i) by right-shifting, all the quantizedvalues can ultimately be stored using the same bit length, thus avoiding wasted storage ; ii) byapplying Huffman coding to the shifting errors, DQA can reduce the extra memory overhead. We evaluated DQA with three sub-6-bit quantization levels (3, 4, and 5 bits) on two differentdatasets (CIFAR-10 , and CityScapes ) for three different DNN models (ResNet-32 , Mo-bileNetV2 and U-Net ) for two different tasks (image classification and image segmentation).Overall, DQA shows significantly better accuracy (up to 29.28%) than two existing methods: directquantization and the state-of-the-art NoisyQuant .",
  "Background and Related Work": "Quantization is a widely used compression method to lower the precision format of parameters inDNNs, which reduces memory storage and computational requirements [5; 9]. The initial DNNparameters, typically using a floating-point format (e.g., 32 bits), are converted into fixed-point orinteger values that require fewer bits (e.g., 16 or 8 bits). The process normally includes clippingand rounding operations. Clipping restricts the minimum and maximum values of the quantization,whereas rounding approximates values to the nearest integers, which causes rounding errors thatcannot be recovered during de-quantization. De-quantization is the opposite operation of quantizationthat approximately recovers the quantized values back to floating point. Quantization methods are typically divided into uniform and non-uniform. In uniform quantization,the quantized values are evenly spaced, while in non-uniform quantization they are not . Uniformquantization can be further divided into symmetric and asymmetric. In symmetric quantization, theclipping range is symmetric with respect to the origin, while in asymmetric quantization it is not .In this paper, we use uniform symmetric quantization due to its popularity and relatively low cost ofcomputation , and we define the direct uniform symmetric quantization and de-quantization as theDirect method (note that this is the same as the direct quantization method for unimportant channelsmentioned in ). It is important to note that keeping the same bit length for the quantized values across the whole DNNmodel is straightforward but can be sub-optimal for accuracy . To improve this, many methodsapply mixed-precision quantization which assigns different bit lengths to different parts of a DNNmodel, such as layers or channels, to satisfy their different precision sensitivities [1; 15]. However, dueto the differences in bit length among the quantized values, mixed-precision quantization can causeinefficiencies such as wasted storage . Our DQA method adapts mixed-precision quantization inan effective way (see ). Furthermore, previous works on DNN quantization apply it to the model weights and/or activations.For weights, AWQ checks the importance of the weight channels for each layer and scales upimportant channels before quantization to reduce the rounding errors. For activations, quantization isdifficult due to its dynamic nature, i.e., the activation values are typically not known before inference.Therefore, to achieve high accuracy, expensive mathematical operations (e.g., matrix multiplication)or large online search spaces to find the best hyper-parameters are required by many methods. Forexample, NoisyQuant injects noise, obtained by online search within the given search spaces,to the activations before quantization and removes the noise after de-quantization. However, theseprevious methods do not focus on deep quantization like sub-6-bit quantization, thus being lesssuitable for resource-constrained devices. : DQA overview.1 offline, rank the activation channels based on importance usingtraining/calibration data and a greedy search algorithm (green circles represent the most importantchannels for which we skip quantization); 2 during inference, quantize important activation channelswith m extra bits and then right-shift them while saving the shifting errors; 3 the shifting errorsare Huffman-encoded to reduce the memory requirement; 4 de-quantize activation channels. Forimportant channels, decode the Huffman-encoded shifting errors and add them to the quantizedactivation channel values. For non-important channels, use the direct method to de-quantize.",
  "Offline, DQA uses training/calibration data to rank the activation channels (i.e., theirimportance) using a greedy search algorithm that quantizes the target activations (see.1);": "2 During inference, DQA quantizes the important activation channels using m extra bits(i.e., n + m bits in total) while the rest of channels are quantized using n bits. Generally, thehigher the accuracy required, the more important channels are needed (we can determinethem using a tunable pre-selected ratio of the total channels in each layer). Then DQAright-shifts the important activation channels by m bits and saves the shifting errors. Fornon-important channels, DQA uses the direct quantization method;",
  "During inference, DQA encodes the shifting errors for important channels using Huffmancoding (see .2), which reduces the memory requirement": "4 During inference, DQA de-quantizates the activation channels. For important channels,it decodes the Huffman-encoded shifting errors and adds them to the quantized activationchannel values, thus compensating the information loss. For non-important channels, it usesthe direct method to de-quantize. Algorithms 1 and 2 describe steps 2 , 3 and 4 of DQA in more detail for a single layer. Note thatchannels are quantized/de-quantized one by one (see lines 4 and 3 in Algorithms 1 and 2 respectively);se in Algorithm 1 refers to the shifting error. Also note that Important Channels (I) are obtained fromthe rank of the activation channels. Finally, the shifting errors are obtained by reading the lower mbits of each activation value before shifting. Since there are 2m possible combinations of lower mbits that correspond to the shifting errors, they can be converted to decimal floating point values usinga pre-computed table (which maps the lower m bits to shifting errors).",
  "Ranking Important Activation Channels Using Greedy Search": "We hypothesize that there are important activation channels that can be treated differently forquantization, similar to how AWQ processes weights. We define important activation channels asthose for which skipping quantization can yield better accuracy in DNN inference. Since activationsare dynamically generated, it would be too computationally expensive to rank activation channelsduring inference. As a result, we compute the ranks offline using training/calibration data. Assumingthat both training/calibration and inference data are drawn from sufficiently similar distributions,the ranks computed offline can be reused during inference. Therefore, we select important channels(which are fixed and based on a tunable pre-selected ratio of the total channels in each layer) offline.Then, the memory requirement can be accurately calculated offline before inference. To calculate the rank of the activation channels, we avoid brute force and dynamic programmingapproaches due to their impractical time complexity. Instead, we use greedy search that only requiresO(LN) operations, where N is the number of channels and L is the number of layers. In ourgreedy search algorithm (see Algorithm 3 in Appendix), we iterate the activation channels for eachlayer. In each iteration, we skip the current activation channel and only quantize the remainingactivation channels of the layer. We then run inference on evaluation data to measure the impact ofskipping the current activation channel on accuracy. The skipped activation channels that provide thehighest accuracy are considered the most important ones for that layer. From layer two onwards, wefirst quantize the activation channels of the previous layers using their layer ranks to skip the mostimportant channel of each layer.",
  "Quantizing Important Activation Channels": "To preserve more information for the important activation channels, and thus obtain higher accuracy,we quantize them with m extra bits, where m is less than or equal to the target number of bitsn, so that they are quantized using n + m bits. Then, we right-shift the activation values of theimportant channels by m bits to reach the target bit length n. Note that this operation allows storingall quantized values with same bit length n, which helps avoid wasted storage . Also note thatright-shifting is a computationally cheap operation . Since right-shifting will lead to informationloss for the important activation values, we save the shifting errors and add them back during thede-quantization phase to compensate the information loss.",
  "Memory Overhead": "DQA improves the accuracy by adding m extra bits and saving the shifting errors, which involvessome memory overhead. To reduce this overhead, we empirically explore the pattern of the shiftingerrors. As an example, we study the average frequency distribution of the shifting errors (i.e., theaverage value of the shifting errors frequencies calculated during inference with each batch of inputdata) for ResNet-32 and the CIFAR-10 dataset with 3, 4, and 5 bits quantization and m = 3. As we can see in , the distributions of shifting errors are not uniform. Therefore, we can useHuffman coding to compress the errors by encoding the most frequent errors with fewer bits,and the less frequent errors with more bits. Note that in general Huffman coding can generate shorterencoding information compared to directly storing m bits, which justifies its use to compress shiftingerrors.",
  "Experimental Setup": "For image classification, we use ResNet-32 and MobileNetV2 on the CIFAR-10 dataset .For image segmentation, we use U-Net on the CityScapes dataset. Note that ResNet-32 andMobileNetV2 store the activations of the shortcut connections, whereas U-Net stores the activationsof encoder layers. The three DNN models are typical examples where activations need to be i) storedfor a relatively long term and ii) compressed to have a manageable peak memory usage.",
  "Direct591.48NoisyQuant591.41DQA(m = 3)591.46": "Since DQA is designed for deep quantization of DNN activations, we set the target quantizationlevels to 3, 4, and 5 bits; note that m = 3 in all cases to simplify the evaluation (we leave as futurework the exploration of different values of m for different quantization levels). We implement our experiments using PyTorch and run them on an Nvidia RTX 3090 GPU, asthe main goal of this work is to evaluate accuracy (we leave as future work the exploration of moreresource-constrained devices). Every experiment is run 5 times for each quantization level, and wetake the average value in each case. For each DNN model, we created a rank table (which maps the name and activation channel ranks ofeach layer) for each run of the experiments with a random subset of the training data using greedysearch (see .1). The size of the random training data subset for creating each rank map is5000 for image classification and 50 for image segmentation. We select important channels withratios of 40% for image classification and 50% for image segmentation, for all channels in all layers.The batch size is 128 for image classification and 4 for image segmentation in all correspondingexperiments, including the direct quantization method and NoisyQuant.",
  "Results": "We compare DQA with two methods i) direct quantization (defined in ) and the state-of-the-art NoisyQuant . Even though NoisyQuant was designed for Vision Transformers , ourexperiments show that it also works well for other types of DNNs. Note that all experiments onlyquantize activations, i.e., without considering weights. shows the accuracy results for the three DNN models under study. Overall, DQA worksbetter for classification tasks than segmentation task. More specifically, for image classificationmodels DQA achieves up to 29.28% higher accuracy than the direct method and NoisyQuant, whereResNet-32 always obtains higher differences than MobileNetV2. We also see that the lower thequantization bits, the more accuracy advantages DQA can provide. For U-Net (image segmentation),we observe that DQA provides higher accuracy (up to 0.9%) than the direct method and NoisyQuantwhen quantizing with 4 bits, and similar accuracy when quantizing with 5 bits. For 3 bits, DQAimproves over direct quantization but NoisyQuant provides the best accuracy value.",
  "Conclusion": "In this paper we proposed DQA, an efficient method that applies deep quantization (with lessthan 6 bits) to DNN activations and provides high accuracy while being suitable for resource-constrained devices. We evaluate DQA on three DNN models for both image classification and imagesegmentation tasks, showing up to 29.28% accuracy improvement compared to direct quantizationand the state-of-the-art NoisyQuant. As future work, we plan to co-design new versions of DQAand hardware accelerators , which will also allow us to evaluate the system performance (suchinference latency) on resource-constrained devices, thus further exploiting the benefits of DQA.",
  "Chen, Z., Xie, B., Li, J., Shen, C.: Channel-wise mixed-precision quantization for large languagemodels. arXiv preprint arXiv:2410.13056 (2024)": "Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R., Franke, U., Roth,S., Schiele, B.: The cityscapes dataset for semantic urban scene understanding. In: Proc. of theIEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2016) Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani,M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N.: An image is worth 16x16words: Transformers for image recognition at scale. In: International Conference on LearningRepresentations (2021) Elhoushi, M., Chen, Z., Shafiq, F., Tian, Y.H., Li, J.Y.: Deepshift: Towards multiplication-lessneural networks. In: Proceedings of the IEEE/CVF conference on computer vision and patternrecognition. pp. 23592368 (2021) Gholami, A., Kim, S., Dong, Z., Yao, Z., Mahoney, M.W., Keutzer, K.: A survey of quantizationmethods for efficient neural network inference. In: Low-Power Computer Vision, pp. 291326.Chapman and Hall/CRC (2022)",
  "He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. 2016 IEEEConference on Computer Vision and Pattern Recognition (CVPR) pp. 770778 (2016)": "Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., Andreetto,M., Adam, H.: MobileNets: Efficient Convolutional Neural Networks for Mobile VisionApplications. arXiv:1704.04861 (Apr 2017) Jacob, B., Kligys, S., Chen, B., Zhu, M., Tang, M., Howard, A., Adam, H., Kalenichenko, D.:Quantization and training of neural networks for efficient integer-arithmetic-only inference. In:Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR). pp.27042713 (2018)",
  "Krizhevsky, A.: Learning Multiple Layers of Features from Tiny Images. TR (2009)": "Lin, J., Tang, J., Tang, H., Yang, S., Chen, W.M., Wang, W.C., Xiao, G., Dang, X., Gan, C., Han,S.: Awq: Activation-aware weight quantization for on-device llm compression and acceleration.In: Gibbons, P., Pekhimenko, G., Sa, C.D. (eds.) Proceedings of Machine Learning and Systems.vol. 6, pp. 87100 (2024) Liu, R., Wei, C., Yang, Y., Wang, W., Yuan, B., Yang, H., Liu, Y.: A dynamic execution neuralnetwork processor for fine-grained mixed-precision model training based on online quantizationsensitivity analysis. IEEE Journal of Solid-State Circuits 59(9), 30823093 (2024) Liu, Y., Yang, H., Dong, Z., Keutzer, K., Du, L., Zhang, S.: Noisyquant: Noisy bias-enhancedpost-training activation quantization for vision transformers. In: Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition. pp. 2032120330 (2023) Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z.,Gimelshein, N., Antiga, L., Desmaison, A., Kpf, A., Yang, E., DeVito, Z., Raison, M., Tejani,A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., Chintala, S.: Pytorch: an imperative style,high-performance deep learning library. In: NeurIPS (2019)"
}