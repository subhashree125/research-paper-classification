{
  "Abstract": "Causal knowledge about the relationships among decision variables and a reward variable in a banditsetting can accelerate the learning of an optimal decision. Current works often assume the causal graphis known, which may not always be available a priori. Motivated by this challenge, we focus on thecausal bandit problem in scenarios where the underlying causal graph is unknown and may include latentconfounders. While intervention on the parents of the reward node is optimal in the absence of latentconfounders, this is not necessarily the case in general. Instead, one must consider a set of possiblyoptimal arms/interventions, each being a special subset of the ancestors of the reward node, makingcausal discovery beyond the parents of the reward node essential. For regret minimization, we identifythat discovering the full causal structure is unnecessary; however, no existing work provides the necessaryand sufficient components of the causal graph. We formally characterize the set of necessary and sufficientlatent confounders one needs to detect or learn to ensure that all possibly optimal arms are identifiedcorrectly. We also propose a randomized algorithm for learning the causal graph with a limited number ofsamples, providing a sample complexity guarantee for any desired confidence level. In the causal banditsetup, we propose a two-stage approach. In the first stage, we learn the induced subgraph on ancestors ofthe reward, along with a necessary and sufficient subset of latent confounders, to construct the set ofpossibly optimal arms. The regret incurred during this phase scales polynomially with respect to thenumber of nodes in the causal graph. The second phase involves the application of a standard banditalgorithm, such as the UCB algorithm. We also establish a regret bound for our two-phase approach,which is sublinear in the number of rounds.",
  "Introduction": "Causal bandits have been a topic of interest since their inception and have been studied in various contexts[Lattimore et al., 2016]. The authors assumed precise knowledge of the causal graph and the impact ofinterventions or actions on the parents of the reward node. Subsequently, there has been a flurry of researchon causal bandits [Sen et al., 2017, Lu et al., 2020, Nair et al., 2021]. The primary limitation of the majorityof existing works on causal bandits is their assumption of full knowledge of the causal graph, which is oftenimpractical for many real-world applications [Lattimore et al., 2016, Lee and Bareinboim, 2018, Wei et al.,2024]. Recently, efforts have been made to overcome this limitation. In Lu et al. , the authors proposea sample efficient algorithm for cases where the causal graph can be represented as a directed tree or a causalforest and later extend the algorithm to encompass a broader class of general chordal graphs. However, theproposed algorithm is only applicable to scenarios where the Markov equivalence class (MEC) of the causalgraph is known and does not have confounders. In De Kroon et al. , the authors propose a causal banditalgorithm that does not require any prior knowledge of the causal structure and leverages separating sets.However, their theoretical result holds only when a true separating set is known. The paper by Konobeev etal. Konobeev et al. also deals with causal bandits with an unknown graph and proposes a two-phaseapproach. The first phase uses a randomized parent search algorithm to learn the parents of the reward node,",
  "Partial Structure Discovery is Sufficient for No-regret Learning in Causal Bandits": "both X and Z are confounded with reward Y already. Thus, we can delete all such bi-directed edges one byone from G while the set of POMISs learned from each of the intermediate causal graphs stays the same.Consider the second scenario, where G has bi-directed edges between a node Z An(Y ), such that thereis no bi-directed edge between Z and Y in both graphs (G and G) and a node X that has the followingcharacteristics: X MUCT(G",
  "Preliminaries and Problem Setup": "We start with an overview of the causal bandit problem and other relevant background needed on causalmodels. Structural causal model (SCM) is a tuple M = V, U, F, P(U) where V = {Vi}ni=1 {Y } is theset of observed variables, U is the set of independent exogenous variables, F is the set of deterministic",
  "t=1E[Y |do(Wt = wt)],(1)": "where do(Wt = wt) represents the intervention selected by the agent in round t. We use the notation do(w)to define the sub-optimality gap of the corresponding arm do(W = w). We denote the descendants, ancestorsand children of a vertex Vi by De(Vi), An(Vi) and Ch(Vi) respectively. We use the notation Bi(Vi, G) to denotethe set of vertices having bidirected edges to Vi except the reward node Y . We refer to the induced graphbetween observable variables as the observable graph. The transitive closure of a graph, denoted by Gtc,encodes the ancestral relationship in G. That is, the directed edge Vi Vj is included in Gtc only whenVi An(Vj). The transitive reduction, denoted by Tr(G) = (V, Er), is a graph with the minimum numberof edges such that the transitive closure is the same as G. The connected component (c-component) of theDAG G, containing vertex Vi, is denoted by CC(Vi), which is the maximal set of all vertices in G that havea path to Vi, consisting only of bi-directed edges [Tian and Pearl, 2002]. For a subset of vertices W V,we define CC(W) := WiW CC(Wi). In a DAG, a subset of nodes W d-separates two nodes Vi and Vjwhen it effectively blocks all paths between them, denoted as Vi d Vj|W. Blocking is a graphical criterionassociated with d-separation [Pearl, 2009]. A probability distribution is said to be faithful to a graph if andonly if every conditional independence (CI) statement can be inferred from d-separation statements in thegraph. Faithfulness is a commonly used assumption in the existing work on causal discovery [Kocaoglu et al.,2017, Hauser and Bhlmann, 2014]. We assume that the following form of the interventional faithfulnessassumption holds in our setup. Assumption 1. Consider a set of nodes W V and the stochastic intervention do(W, U) on W and anyset U V \\ W. The conditional independence (CI) statement (X Y | Z)MW,U holds in the induced modelif and only if there is a corresponding d-separation statement in post-interventional graph (X d Y | Z)GW,U,where X, Y, and Z are disjoint subsets of V \\ W. The CI statements in the induced model are with respectto the post-interventional joint probability distribution.",
  "Possibly Optimal Arms in Causal Bandits with Unknown CausalGraph": "The optimal intervention in a causal bandit setup is not restricted to the parent set of the reward node whenthe reward node Y is confounded with any node in its ancestors An(Y ) [Lee and Bareinboim, 2018]. Forinstance, consider SCM X1 = U1 and X2 = X1 U2 and reward Y = X2 U2, where U1 Ber(0.5) andU2 Ber(0.5). Note that X2 and reward Y are confounded in this SCM. The optimal intervention in thiscase is do(X1 = 1) since E[Y |do(X1 = 1)] = 1. The intervention on the parent of the reward (Pa(Y ) = X2)is suboptimal because E[Y |do(X2 = 0)] = E[Y |do(X2 = 1)] = 0.5. The example shows that it is possibleto construct SCMs where optimal intervention is on ancestors of the reward node instead of parents whenreward node is confounded with one of its ancestors. The authors in Lee and Bareinboim propose agraphical criterion to enumerate the set of all possibly optimal arms, which they refer to as POMISs. Werevisit some definitions and results from their work.",
  ": True Causal Graph G with four other graphs each with one missing bi-directed edge": "Definition 1. (Unobserved Confounder (UC)-Territory [Lee and Bareinboim, 2018]) Consider acausal graph G(V, E) with a reward node Y and let H be G[An(Y )]. A set of variables T V (H) containingY is called an UC-territory on G with respect to Y if DeH(T) = T and CCH(T) = T. A UC-territory is minimal if none of its subsets are UC-territories. A minimal UC-territory denoted byMUCT(G, Y ), can be constructed by extending a set of variables, starting from the reward {Y }, alternativelyupdating the set with the c-component and descendants of the set until there is no change. Definition 2. (Interventional Border) [Lee and Bareinboim, 2018] Let T be a minimal UC-territoryon G with respect to Y . Then, X = Pa(T) \\ T is called an interventional border for G w.r.t. Y denoted byIB(G, Y ).",
  "Lemma 1. [Lee and Bareinboim, 2018] For causal graph G with reward Y , IB(GW, Y )is a POMIS, for anyW V \\ {Y }": "Although the graphical characterization in Lemma 1 provides a means to enumerate the complete set ofPOMISs, it comes with exponential time complexity. The authors also propose an efficient algorithm forenumerating all POMISs in [Lee and Bareinboim, 2018]. However, this requires knowing the true causal graph,and without it, one has to consider interventions on all possible subsets of nodes, which are exponentiallymany. One naive approach to tackle the problem is to learn the full causal graph with all confounders to listall POMISs. However, a question arises: Do we need to learn/detect all possible confounders since the goal isto find POMISs and not the full graph?Before answering the above question, we start with an example considering the causal graphs in .Using Lemma 1, the set of POMISs for the true graph G is IG = {, {V1}, {V2}, {V3}, {V1, V2}}. However, forG1 which has the bidirected edge V2 Y missing, the set of POMISs is IG1 = {, {V2}, {V1, V2}}. Also forG2 which has the bidirected edge V1 V2 missing, the set of POMISs is IG2 = {, {V1}, {V2}, {V1, V2}}. Inboth cases, we miss at least one POMIS, and since it is possible to construct an SCM compatible with thetrue causal graph G where any arm in POMIS is optimal, if this arm is not learned, we can suffer linear regretLee and Bareinboim . Although the graph G3 has the bidirected edge V1 V3 missing, it still has thesame set of POMISs as the true graph, i.e., IG3 = {, {V1}, {V2}, {V3}, {V1, V2}}. This example shows thatonly a subset of latent confounders affect the POMISs learned from the graph. We formally prove that it isnecessary and sufficient to learn/detect all latent variables between the reward and its ancestors becausemissing any one of them will cause us to miss at least one of POMISs leading to linear regret for some banditinstances.",
  "Lemma 2. It is necessary to learn/detect the latent confounders between reward node Y and any nodeX An(Y ) in causal graph G to learn all the POMISs correctly and hence avoid linear regret": "Theorem 1. Consider a causal graph G(V, E) and another causal graph G such that they have the samevertex set and directed edges but differ in bidirected edges, with the bidirected edges in G being a subset of thebidirected edges in G. The graphs will yield different collections of POMISs if and only if there exists someZ An(Y ) such that either (1) or (2) is true:",
  "Finite Sample Causal Discovery Algorithm": "In this section, we propose a sample-efficient algorithm to learn causal graphs with latent confounders. Wepropose a two-phase approach. In the first phase, the algorithm learns the observable graph structure, i.e.,the induced graph between observed variables. In the second phase, it detects the latent confounders. Inthe next section, we use the proposed discovery algorithm to construct the algorithm for causal banditswith an unknown graph. We begin by proposing two Lemmas to learn the ancestrality relations and latentconfounders using interventions. Lemma 3.Consider a causal graph G(V, E) and W V. Furthermore, let X, T V \\ W be any twovariables. Under the faithfulness Assumption 1 (X An(T))GW if and only if for any w [K]|W|, we haveP(t|do(w)) = P(t|do(w), do(x)) for some x, t [K]. Lemma 4. Consider two variables Xi and Xj such that Xj / An(Xi) and a set of variables (Pa(Xi) Pa(Xj)\\{Xi}) W and Xi, Xj / W. Under the faithfulness Assumption 1 there is latent confounder betweenXi and Xj if and only if for any w [K]|W|, we have P(xj | do(xi), do(W = w)) = P(xj | xi, do(W = w))for some realization xi, xj [K]. These Lemmas are modified versions of Lemma 1 in Kocaoglu et al. and Interventional Do-seetest in Kocaoglu et al. , respectively. The difference between Lemma 3 and Lemma 1 in Kocaogluet al. is that we have an inequality test that can be used in the sample-efficient discovery instead of astatistical independence test. The Interventional Do-see test in Kocaoglu et al. is valid for adjacentnodes only; however, our Lemma 4 can be used to test presence of latent confounder between any pair ofnodes. This is because the condition in Lemma 4, Xj / An(Xi), can always be satisfied for any pair byflipping the order when one node is an ancestor of the other. In order to provide theoretical guarantees onsampling complexity, the inequality conditions are not enough; we need to assume certain gaps similar to [Luet al., 2021, Konobeev et al., 2023, Greenewald et al., 2019]. Assumption 2. Consider a causal graph G(V, E) and W V. Furthermore, let X, T V \\ W be anytwo variables. Then, we have (X An(T))GW if and only if for any w [K]|W|, we have |P(t|do(w)) P(t|do(w), do(x))| > for some x, t [K], where > 0 is some constant. Assumption 3. Consider two variables Xi and Xj such that Xj / An(Xi) and a set of variables (Pa(Xi) Pa(Xj) \\ {Xi}) W and Xi, Xj / W. There is a latent confounder or a bidirected edge between Xi and Xjif and only if for any w [K]|W|, we haveP(xj | do(xi), do(W = w)) P(xj | xi, do(W = w)) > forsome realization xi, xj [K] and some constant > 0.",
  "We propose Algorithm 1 to learn the transitive closure under any arbitrary intervention do(W), denoted byGtc": "W. We use the Assumption 2 to bound the number of samples for ancestrality tests. We start with an emptygraph and add edges by running ancestrality tests for all pairs of nodes in V \\ W, resulting in the transitiveclosure Gtc W. We recall that the transitive reduction Tr(G) = (V, Er) of a DAG G = (V, E) is unique, withEr E, and it can be computed in polynomial time [Aho et al., 1972]. Also, note that Tr(G) = Tr(Gtc). Wepropose a randomized Algorithm 2 similar to the one proposed in Kocaoglu et al. that repeatedly usesAlgorithm 1 to learn the observable graph structure. The motivation behind the randomized Algorithm 2 isLemma 5 from Kocaoglu et al. , which states that for any edge (Xi, Xj), consider a set of variablesW such that {Wi : (Wi) > (Xi) & Wi Pa(Xj)} W where is any total order that is consistent withthe partial order implied by the DAG, i.e., (X) < (Y ) iff X An(Y ). In this case, the edge (Xi, Xj) willbe present in the graph Tr(GW). Algorithm 2 randomly selects W, computes the transitive reduction ofthe post-interventional graphs, and finally accumulates all edges found in the transitive reduction acrossiterations. Algorithm 2 takes a parameter dmax, which must be greater than or equal to the highest degreefor our theoretical guarantees to hold.",
  "Learning the Latent Confounders": "Assumption 3 can be used to test for latents between any pair of observed variables. Note that while usingAlgorithm 2, we save and return all the interventional data samples, these samples can be reused to detectlatent confounders in the next phase. For any variables Xi and Xj such that Xj / An(Xi), we need accessto interventional samples do(W = w) such that (Pa(Xi) Pa(Xj) \\ {Xi}) W and Xi & Xj / W. In thesupplementary material, we demonstrate that randomly selecting the target set W in Algorithm 2 ensuresthat we have access to all such datasets for all pairs of observed variables with high probability. In additionto simple causal effects we need to estimate the conditional causal effect of the form P(xj|xi, do(W = w)).To bound the number of samples required to ensure accurate estimation of the conditional causal effects, werely on Assumption 4. Note that Assumption 4 does not restrict the applicability of our algorithm; it simplyassumes that under an intervention do(W = w), either the probability of observing a realization Xi = xi iszero or is lower-bounded by some constant > 0. The role of this assumption is to bound the number ofinterventional samples required for accurate estimation of the conditional causal effects.",
  "2for every pair Xi, Xj V do": "If Xj An(Xi), swap them.Find interventional data sets do(W = w) and do(Xi = xi, W = w) from IData s.t.(Pa(Xi) Pa(Xj) \\ {Xi}) W and Xi & Xj / WGet max(0, C B) new samples for do(W = w)if xi, xj [K] s.t. | P(xj|do(xi), do(w)) P(xj|xi, do(w))| >",
  "log( 2K2": "4 ) samples from do(W = w) for a fixed w [K]|W| and W V such that (Pa(Xi) Pa(Xj) \\{Xi}) W and Xi & Xj / W. Then, with probability at least 1 1 3 4, we have a latent confounderbetween Xi and Xj iff xi, xj [K] s.t. P(xj|do(xi), do(w)) P(xj|xi, do(w)) >",
  "A 1": "B. Thevalue of the constant 0 < < 1 is usually small in practical scenarios, so the quantity C is much greaterthan both B or A. This implies that the number of samples required to test the presence of latent variablesis greater than that required to learn ancestral relations. This is because we need to accurately estimateconditional causal effects to detect latent variables, which requires a large number of samples compared tosimple causal effects. Theorem 1 is useful here because it shows that we do not need to test for confoundersbetween all pairs of nodes among ancestors of the reward node to learn the POMIS set.",
  "Algorithm for Causal Bandits with Unknown Graph Structure": "Algorithm 4 is the sketch of our algorithm for causal bandits with unknown graph structure. The detailedalgorithm with all steps explained is given in the supplementary material (Algorithm 6). Algorithm 4 firstlearns the transitive closure of the graph Gtc to find ancestors of the reward node Y . This is because POMISsare only subsets of An(Y ). The next step is to learn the observed graph structure among the reward Y andnodes in An(Y ). Instead of detecting the presence of confounders between all pairs of nodes in An(Y ) as inAlgorithm 3, we focus on identifying the necessary and sufficient ones, as characterized by Theorem 1. Thisapproach is more sample-efficient since it tests for fewer latent confounders. The exact saving in terms ofsamples depends on the underlying causal graph and is hard to characterize in general. The last step ofAlgorithm 4 is to run a simple bandit algorithm, e.g., UCB algorithm Lattimore and Szepesvri , toidentify the optimal arm from the POMISs. Given that Assumptions 2, 3, and 4 hold, and the reward isbinary (Y {0, 1}), using the results from Lemma 6 and Theorem 3, we provide a worst-case regret boundfor Algorithm 4 in Theorem 4.",
  "Experiments": "Theorem 4 establishes the worst-case upper bound for cumulative regret when we need to test latentconfounders between all pairs of nodes within An(Y ). However, Algorithm 4 selectively examines only asubset of latent confounders sufficient to infer the true POMIS set, as outlined in Theorem 1. Although theadvantage is hard to quantify in general, we demonstrate it using simulations on randomly generated graphs.We sample a random ordering among the vertices. Then, for each nth node, we determine its in-degree asXn = max(1, Bin(n 1, )), followed by selecting its parents through uniform sampling from the precedingnodes in the ordering. Finally, we chordalize the graph using the elimination algorithm [Koller and Friedman,2009], employing an elimination ordering that is the reverse of . Additionally, we introduce a confounderbetween every pair of nodes with a probability of L. For all the simulations, we randomly sample 50 causalgraphs with different values of densities and L and assume that all variables are binary for simplicity,i.e., K = 2. We set the value of to 0.99, and the gaps = = 0.01 and = 0.05. We plot interventionalsamples used to learn the induced observable graph on An(Y ) with and without latent confounders, as wellas the samples required to learn the POMIS set by Algorithm 4. The width of confidence interval is set to 2standard deviations.The simulation results in demonstrate that Algorithm 4 requires fewer samples than learningthe induced graph on An(Y ), which includes all confounders. However, as L increases for a fixed , thisadvantage diminishes, as illustrated in . The trend remains consistent as the default parameters and L are varied from 0.2, 0.4, and 0.6. The plots in compare the exponentially growing arms",
  ": Cumulative regret for Algorithm 4 versus learning all possible latents ( = L = 0.3)": "We also run the UCB algorithm on the learned POMIS set and plot the cumulative regret in .Since the number of time steps T is on the order of 108, it is not feasible to store and plot cumulative regretfor every time step over multiple randomly sampled graphs; therefore, we downsample the cumulative regretto show the overall trend. The downsampling, along with the large scale of the y-axis, makes the regret in thediscovery phase appear linear with a fixed slope, although it is piece-wise linear if we zoom in. Also, the UCBphase converges very fast compared to the discovery phase because the number of POMISs for randomlysampled graphs is small. We plot the results for graphs with 10, 15, and 20 nodes, and in all cases, we cansee the advantage of partial discovery compared to full discovery, since Algorithm 4 finds the POMIS setwith fewer samples.",
  "Conclusion": "We show that partial discovery is sufficient to achieve sublinear regret for causal bandits with an unknown causalgraph containing latent confounders. Without relying on causal discovery, one must consider interventions onall possible subsets of nodes, which is infeasible. Therefore, we propose a two-phase approach where the firstphase learns the induced subgraph on the ancestors of the reward node, along with a subset of confounders,to construct a set of possibly optimal arms. The next phase involves applying the Upper Confidence Bound(UCB) algorithm to the reduced action space to find the optimal arm.",
  "Jin Tian and Judea Pearl. A general identification condition for causal effects. In Aaai/iaai, pages 567573,2002": "Christian Toth, Lars Lorch, Christian Knoll, Andreas Krause, Franz Pernkopf, Robert Peharz, and JuliusVon Kgelgen. Active bayesian causal inference. Advances in Neural Information Processing Systems, 35:1626116275, 2022. Lai Wei, Muhammad Qasim Elahi, Mahsa Ghasemi, and Murat Kocaoglu. Approximate allocation matchingfor structural causal bandits with unobserved confounders. Advances in Neural Information ProcessingSystems, 36, 2024.",
  "A.1Review of d-separation:": "Consider three disjoint sets of nodes X, Y, and Z in the causal graph G = (V, E). The sets of nodes Xand Y are d-separated given Z, denoted by (X d Y|Z)G, if and only if there exists no path, directed orundirected, between any node in set X and any node in set Y such that for every collider on the path, eitherthe collider itself or one of its descendants is included in the set Z, and no other non-collider nodes on thepath are included in the set Z. (A collider on a path is a node with both arrows converging, e.g., B is acollider on the path ABC in A B C).",
  "), B =82 log 2nK2": "2if Xj An(Xi) swap them.Find interventional data sets do(W = w) and do(Xi = xi, W = w) from IData s.t.(Pa(Xi) Pa(Xj) \\ {Xi}) W and Xi & Xj / WGet max(0, B C) new samples for do(W = w)if xi, xj [K] s.t. | P(xj|do(xi), do(w)) P(xj|xi, do(w))| >",
  "Lemma. 3.2: It is necessary to learn/detect the latent confounders between reward node Y and any nodeX An(Y ) in causal graph G to learn all the POMISs correctly and hence avoid linear regret": "Before proceeding to the proof, we recall an important result from Lee and Bareinboim : For acausal graph G with reward variable Y , IB(GW, Y ) is a POMIS for any W V \\ Y .Proof: Consider a causal graph G(V, E) with a node X An(Y ) such that there exists a latent confounderbetween X and the reward Y . Suppose we do not detect the presence of the confounder and have access toanother causal graph G with everything the same as G except that there is no confounder between X and Y .We show that there exists one such POMIS that we cannot learn from G, which actually exists in the true",
  "W, Y ).This implies that Z Pa(X) s.t. Z IB(G": "W, Y ). However, for the true graph G, we have a differentIB(GW, Y ) for the same definition of W because it contains the bi-directed edge between X and Y , whichimplies that X MUCT(GW, Y ), and as a result, Pa(X) IB(GW, Y ). Also, in the case Pa(X) = , we havea different POMIS. On this side, note that X / MUCT(G",
  "W, Y ), which implies either X or one of itsdescendants on the path from X to Y is in IB(G": "W, Y ), which is not the case for G since X MUCT(GW, Y ).Thus, we have different interventional boundary or POMIS for the two causal graphs G and G given theabove choice of W, even if X has no parents.The next step is to show that the particular POMIS IB(GW, Y ) cannot be learned from the DAGG, i.e., IB(GW, Y ) = IB(G W, Y ) for any W V.We need to show this because of the graphicalcharacterization of POMISs in Lemma 1. Using the definition of W, note that Pa(X) IB(GW, Y ) and forall Z Ch(Pa(X)) \\ {X}, there exists either Z IB(GW, Y ) or De(Z) \\ {Y } IB(GW, Y ). Also, if thereare such nodes in CC(X) \\ {X, Y } which do not have a path to X comprised of directed edges only, callsuch set of nodes T. If T = , then for all t T, we have either t IB(GW, Y ) or De(t) \\ {Y } IB(GW, Y ).Also, note that Z De(X) {X} such that Z IB(GW, Y ). Now consider DAG G with the bi-directededge between X and Y missing. Assume by contradiction W V such that IB(GW, Y ) = IB(G",
  "W, Y ), which under the given choice of W isonly possible when X MUCT(G": "W, Y ), which would require is a bi-directed edge between X and Y in theDAG G, which is a contradiction. Also, for the case when Pa(X) = , we have a contradiction because werequire the following to be true: Z De(X) {X} such that Z IB(G W, Y ). For the given choice of W,it implies that there is a bi-directed edge between X and Y in the DAG G, which is again a contradiction.Thus, by contradiction, we show that W V such that G, i.e., IB(GW, Y ) = IB(G W, Y ). This impliesthat we will miss at least one POMIS if we do not learn or detect latent confounders between the rewardnode Y and any node X An(Y ), and may incur linear regret. This completes the proof of Lemma 2.",
  "Before proving Theorem 1, we state and prove another Lemma. We then extend this Lemma to proveTheorem 1": "Lemma 8. Consider a causal graph G(V, E) and another graph G such that they have the same vertex setand directed edges but differ in bidirected edges, with the bidirected edges in G being a subset of the bidirectededges in G. The graphs will yield different collections of POMISs if there exists some Z An(Y ) such thateither (1) or (2) is true:",
  "Pa(Z),Bi(Z,G) , Y ) and Z but not in G": "Proof: One direction for Theorem 1 is proved already in Lemma 8. We only to need to prove the otherdirection which is that two causal graphs G and G such that they have the same vertex set and directededges, but differ in bi-directed edges will yield same collections POMISs when neither of statements (1)and (2) is true. Note when neither of (1) or (2) is true the graphs G and G might still have a different setof bi-directed edges. We will have two possible scenarios here. Suppose G has a bi-directed edge betweensome Z An(Y ) and some X An(Y ), such that there is a bi-directed edge between pair of vertices (Z, Y )and (X, Y ) in both the graphs and the bi-directed edge between X and Z is absent in G. Further, assumeneither of statements (1) and (2) hold. In this case, despite the absence of a bi-directed edge between Xand Z in G, the graphs will yield the same set of POMISs. This is because Z / MUCT(GW, Y ) for someset of nodes W only when Z W, and the same is the case for G because they share a bi-directed edgebetween Z and Y . By symmetry, we have the argument hold for X as well. So, the presence or absenceof bi-directed edges between X and Z does not change the set of POMISs learned from the graph when",
  "W, Y ).This implies that N Pa(Z) \\ An(X) s.t. N IB(G": "W, Y ). However, for the true graph G, we have adifferent IB(GW, Y ) for the same definition of W because it contains the bi-directed edge between X andZ, which implies that Z MUCT(GW, Y ), and as a result, Pa(Z) \\ An(X) IB(GW, Y ). Also, in the casePa(Z) \\ An(X) = , we have different a POMIS. On this side, note that Z / MUCT(G",
  "W, Y ), whichimplies either Z or one of its descendants on the path from Z to Y is in IB(G": "W, Y ), which is not the case forG since Z MUCT(GW, Y ). Thus, we have different interventional boundary or POMIS for the two causalgraphs G and G given the above choice of W.The next step is to show that the particular POMIS IB(GW, Y ) cannot be learned from the DAG G, i.e.,IB(GW, Y ) = IB(G W, Y ) for any W V. We need to show this because of the graphical characterizationof POMISs in Lemma 1. Using the definition of W, note that Pa(Z) \\ An(X) IB(GW, Y ) and for allN Ch(Pa(Z)\\An(X))\\{Z}, there exists either N IB(GW, Y ) or De(N)\\{Y } IB(GW, Y ). Also, if thereare such nodes in Bi(Z, G) \\ {X, Z, Y } which do not have a path to Z comprising of directed edges only, callsuch set of nodes T. If T = , then for all t T, we have either t IB(GW, Y ) or De(t) \\ {Y } IB(GW, Y ).Also, note that N De(Z) {Z} such that N IB(GW, Y ). Now consider the DAG G with the bi-directededge between Z and Y missing. Assume by contradiction W V such that IB(GW, Y ) = IB(G",
  "W, Y ), which under the given choiceof W is only possible when Z MUCT(G": "W, Y ), which would require a bi-directed edge between Z andX in the DAG G, which is a contradiction. Also, for the case when Pa(Z) = , we have a contradictionbecause we require the following to be true: N De(Z) {Z} such that N IB(G W, Y ). For the givenchoice of W, it implies that there is a bi-directed edge between Z and X in the DAG G, which is again acontradiction. Thus, by contradiction, we show that W V such that G, i.e., IB(GW, Y ) = IB(G",
  "We now proceed to the formal proof for Theorem 1:": "Theorem. 3.1: Consider a causal graph G(V, E) and another DAG G such that they have the same vertexset and directed edges but differ in bidirected edges, with the bidirected edges in G being a subset of thebidirected edges in G. The graphs will yield different collections of POMISs if and only if there exists someZ An(Y ) such that either (1) or (2) is true:",
  "W, Y ) or N Bi(Z, G) such that N MUCT(G": "W, Y ). Since bi-directed edgesin G are a subset of bi-directed edges in G, we have: Either N Pa(Z) such that N MUCT(GW, Y ) orN Bi(Z, G) such that N MUCT(GW, Y ). Note that any MUCT is closed under the De(.) and CC(.)operations, i.e., for any MUCT, say T, we have De(T) = T and CC(T) = T. if N Pa(Z) such thatN MUCT(GW, Y ) or N Bi(Z, G) such that N MUCT(GW, Y ), we already have Z MUCT(GW, Y )using the definition of MUCT. The bi-directed edge between X and Z will play a role only when N Pa(Z)such that N MUCT(GW, Y ) and N Bi(Z, G) such that N MUCT(GW, Y ) for any choice of W. Recallthat the given condition X MUCT(G",
  "Pa(Z),Bi(Z,G), Y ) already implies that either": "N Pa(Z) such that N MUCT(GW, Y ) or N Bi(Z, G) such that N MUCT(GW, Y ). Thus absenceor presence of bi-directed edge between X and Z will have no effect on POMISs learned from graph G inthis scenario as well. Combining both of the scenarios when neither of the conditions of (1) and (2) hold, allother bi-directed edges from G, which are absent in G, can be removed one by one from G while keeping thePOMISs learned from both the intermediate graphs the same. Since G and G only differ in bi-directed edges,with bi-directed edges in G being a subset of those in G, eventually both graphs will become identical, whichproves the statement: Two graphs G and G will have the same POMISs if neither of the statements (1) or (2)hold true. This completes the proof of the Theorem 1.",
  "A.6Proof of Lemma 3:": "Consider a causal graph G(V, E) and W V. Furthermore, let X, T V \\ W be any two variables.Fix some realization w [K]|W|. Under post interventional faithfulness Assumption 1 we want to prove:(X An(T))GW P(t|do(w)) = P(t|do(w), do(x)) for some x, t [K].Forward Direction ( = ): (X An(T))GW = P(t|do(w)) = P(t|do(w), do(x)) for some x, t [K].By contradiction, assume P(t|do(w)) = P(t|do(w), do(x)), x, t [K]. This implies that P(t|do(w), do(x)) =P(t|do(w)) = some function of only t and w. This implies that for the sub-model MW,X the following CIstatements holds: (T X)MW,X. However, note that if (X An(T))GW, then we still have (X An(T))GW,X.This implies there is a directed path from X to T in the post-interventional graph GW,X. Therefore, wehave: (T d X)GW,X. Note that under the post interventional faithfulness Assumption 1, the CI statement(T X)MW,X can hold only if the d-separation statement holds (Td X)GW,X, which is clearly acontradiction. This completes the proof for the forward direction.Reverse Direction ( = ): (X An(T))GW = P(t|do(w)) = P(t|do(w), do(x)) for some x, t [K].We prove the contrapositive statement instead, i.e., (X / An(T))GW = P(t|do(w)) = P(t|do(w), do(x)),x, t [K]. Note that (X / An(T))GW clearly implies that (X / An(T))GW,X which implies that (T dX)GW,X. Thus, using Rule 3 of Pearls do calculus, we have: P(t|do(w), do(x)) = P(t|do(w)), x, t [K].This completes the proof of the reverse direction.",
  "A.7Proof of Lemma 4:": "Consider two variables Xi and Xj such that Xj / An(Xi) and a set of variables (Pa(Xi)Pa(Xj)\\{Xi}) Wand Xi & Xj / W. Fix some realization w [K]|W|. Under the post-interventional faithfulness Assumption1 we want to show that: There is latent confounder between Xi and Xj P(xj | do(xi), do(W = w)) =P(xj | xi, do(W = w)) for some realization xi, xj [K].Forward Direction ( = ): There is latent confounder between Xi and Xj such that Xj / An(Xi)= P(xj | do(xi), do(W = w)) = P(xj | xi, do(W = w)) for some realization xi, xj [K]. By contradictionassume P(xj | do(xi), do(W = w)) = P(xj | xi, do(W = w)) xi, xj [K]. Recall that: Xj = fj(Pa(Xj), Uj).",
  "P(xj | do(xi), do(W)) = P(xj | do(xi), do(pa(Xi)), do(pa(Xj) \\ {xi})))(5)": "where the interventions do(Pa(Xi)) and do(Pa(Xj))) are consistent with do(xi) and do(W = w). Theequation 5 holds by the application of Pearls do-calculus Rule 3 because, by definition of the set W, we have(Pa(Xi) Pa(Xj) \\ {Xi}) W and Xi, Xj / W. All the extra intervention targets can simply be deleted,and we are left with intervention on Xi, Pa(Xi), and Pa(Xj).",
  "P(xj | xi, do(W)) = P(xj | xi, do(pa(Xi)), do(pa(Xj) \\ {xi})))(7)": "The equation 7 holds by the application of Pearls do-calculus Rule 3 because, by definition of the set W,we have (Pa(Xi) Pa(Xj) \\ {Xi}) W and Xi, Xj / W. All the extra intervention targets can simply bedeleted, and we are left with conditioning on Xi = xi and interventions on Pa(Xi) and Pa(Xj).",
  "P(Uj = uj, Lij = lij|xi, do(pa(Xi)), do(pa(Xj) \\ {xi}))(8)": "Using Pearls do-calculus Rule 2, we can replace the conditioning Xi = xi with the intervention do(xi) inP(xj | xi, do(pa(Xi)), do(pa(Xj) \\ {xi}), Uj = uj, Lij = lij) because Xj / An(Xi) and Pa(Xi) are alreadyintervened on. Also, the latent confounder Lij is conditioned on, so there is no open backdoor path from Xito Xj. Thus, we have:",
  "P(Uj = uj, Lij = lij|xi, do(pa(Xi)), do(pa(Xj) \\ {xi})) = P(Uj = uj, Lij = lij).(11)": "However, since we know that Lij is a confounder between Xi and Xj, we have an edge Lij Xi in thecausal graph, which implies that under any intervention do(Z) such that Xi / Z, we must have (Lij Xi)MZby interventional faithfulness Assumption 1. This implies that there exists a realization xi and lij such that:",
  "P(Uj = uj, Lij = lij|xi , do(pa(Xi)), do(pa(Xj) \\ {xi})) = P(Uj = uj, Lij = lij)(12)": "Now, using the combination do(W = w) and a special choice of realizations xi and lij, we must have at leastone special realization xj such that: P(xj | do(xi ), do(Pa(Xi)), do(Pa(Xj) \\ {xi}), Uj = uj, Lij = lij) > 0.Combining this with Equations 12 and 10, we conclude for some xi , xj [K], we have P(xj | do(xi ), do(W =w)) = P(xj | xi , do(W = w)). Thus this leads to contradiction. Thus if there is a latent confounder betweenXi and Xj = P(xj | do(xi), do(W = w)) = P(xj | xi, do(W = w)) for some realization xi, xj [K]. Thiscompletes the proof of the forward direction.Reverse Direction ( = ): For a pair of variables Xi and Xj such that Xj / An(Xi), if P(xj |do(xi), do(W = w)) = P(xj | xi, do(W = w)) for some realizations xi, xj [K], then there is a latentconfounder between Xi and Xj. We prove the contrapositive statement instead, i.e., if there is no latentconfounder between Xi and Xj, then P(xj | do(xi), do(W = w)) = P(xj | xi, do(W = w)), xi, xj [K].Note that by construction, we have: (Pa(Xi) Pa(Xj) \\ {Xi}) W. For such choice of set W and the factthat Xj / An(Xi) and there is no latent confounder between Xi and Xj, we have (Xj Xi)GXiW. Thus,",
  "W) and updates all the edges to construct the observable graph. To prove the results in Theorem 2, werely on Lemma 5 from Kocaoglu et al. , which is stated below:": "Lemma 9. Kocaoglu et al. Consider a graph G with observed variables V and an intervention setW V. Consider post-interventional observable graph GW and a variable Xj V \\ W. Let Xi Pa(Xj)be such that all the parents of Xj above Xi in partial order are included in the intervention set W. Thisimplies that {Wi : (Wi) > (Xi) & Wi Pa(Xj)} W . Then, the directed edge (Xi, Xj) E(Tr(GW)).The properties of transitive reduction yields Tr(GW) = Tr(Gtc",
  "W , i.e., Tr(Gtc": "W) = Tr(GW) may be used to learn the directed edge (Xi, Xj).(Note: E(G) denotes the edges of the graph G and is any total order that is consistent with the partial orderimplied by the DAG, i.e., (X) < (Y ) iff X is an ancestor of Y). Assume that the number of the direct parents of Xj above Xi is dij where dij dmax. Let Ei(Xj) be thefollowing event: Xi, Xj / W & {Wi : (Wi) > (Xi) & Wi Pa(Xj)} W. The probability of this eventfor one run of the outer loop in Algorithm 2 with the assumption that 2dmax >= 2 is given by:",
  "K2 .(34)": "Where Cxi is the number of samples where Xi = xi among the C samples for the intervention do(w). Notethe we cant directly control Cxi and its value depends on the true interventions distribution P(xi, do(w))along-with the number of samples C. Suppose if we can set Cxi 82 log 2K2",
  "n": "2dmax 2 , we will be able to find an appropriate interventional datasetto test the presence of latent confounders between any pair of variables using Assumption 3 after runningAlgorithm 2. We still need to make sure we have enough interventional samples to be able to test the latents.This is because we need to accurately estimate conditional effects to carry out the test, as in Assumption 3.We first consider estimation of the causal effect P(xj|do(xi), do(w)) for any randomly sampled set W. Now,",
  ".(52)": "The last inequality holds for dmax 2. Note that we reuse all the interventional data samples fromAlgorithm 2 in Algorithm 3. Under Assumption 3, if the event Eij happens with a large enough number ofsamples, we can detect the presence or absence of latent confounders between Xi and Xj. The outer loopruns for 8dmax log(n) iterations, and the elements of the set W are independently sampled. The probabilityof failure, i.e., the event under consideration does not happen for all runs of the outer loop in Algorithm 2, isbounded as follows:",
  "This implies that under the good event, for every randomly sampled intervention set W V, the estimate ofthe conditional causal effect is accurate within the desired": "4 threshold. This would imply that the test fordetection of latent variables is perfect under this good event. We have already shown that to ensure we haveaccess to sufficient datasets to detect latent variables between any pair of nodes, the 8dmax log n randomlysampled target sets in Algorithm 2 are sufficient. Combining these results with the results from Theorem 2,we have the following:The Algorithm 3 learns the true causal graph along with all latents with a probability of at least 1 1",
  "while There is a new pair that is tested do": "Find a new pair (Z, X) s.t. Z An(Y ) such that Z and Y dont have a bi-directed edge betweenthem in G and X MUCT(GPa(Z),Bi(Z,G), Y )# Test for the latent between the pair (Z, X) and update G.Set Xi := Z, Xj := Xif Xj An(Xi) swap them.Find interventional data sets do(W = w) and do(Xi = xi, W = w) from IData s.t.(Pa(Xi) Pa(Xj) \\ {Xi}) W and Xi & Xj / WGet max(0, B C) new samples for do(W = w)if xi, xj [K] s.t. | P(xj|do(xi), do(w)) P(xj|xi, do(w))| >",
  "log 4nK2": ".The next step is to learn the complete observable graph induced on the reward node and its ancestors andthen learn/detect only a subset of latent confounders which are characterized to be necessary and sufficient tolearn the true set of POMISs (Theorem 1). Although this step saves us interventional samples compared tothe full discovery Algorithm 3, which learns/detects latents between all pairs of variables, the exact saving willdepend on the structure of the underlying causal graph. For the regret upper bound, we can use the resultsfrom Theorem 3 to bound the number of interventional samples for learning the true POMIS set from theancestors of the reward node. This implies that given the true set of ancestors of the reward An(Y ), we can"
}