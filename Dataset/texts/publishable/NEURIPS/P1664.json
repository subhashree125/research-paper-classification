{
  "Abstract": "Neural Audio Codecs, initially designed as a compression technique, have gainedmore attention recently for speech generation. Codec models represent each au-dio frame as a sequence of tokens, i.e., discrete embeddings. The discrete andlow-frequency nature of neural codecs introduced a new way to generate speechwith token-based models. As these tokens encode information at various levelsof granularity, from coarse to fine, most existing works focus on how to bettergenerate the coarse tokens. In this paper, we focus on an equally important butoften overlooked question: How can we better resynthesize the waveform fromcoarse tokens? We point out that both the choice of learning target and resynthesisapproach have a dramatic impact on the generated audio quality. Specifically,we study two different strategies based on token prediction and regression, andintroduce a new method based on Schrdinger Bridge. We examine how dif-ferent design choices affect machine and human perception. Audio demo page:",
  "Introduction": "Neural Codec models initially emerged as compression techniques for audio compression.Despite being originally proposed for compression, neural audio codec models significantly impactedspeech and audio modeling due to their discrete and low-frequency nature. Having a tokenizedrepresentation of audio introduces many benefits. For example, token-based modeling approachessimilar to language models can be adopted for audio generation: VALL-E , Speech-X , Audi-oLM , MusicLM and MusicGEN , just to name a few. Besides audio generation tasks, audiocodecs can also be applied to cross-modality applications, e.g., making audio and large languagemodel (LLM) integration seamless . As shown in , codec models are trained to compress audio into discrete tokens at a low frequencyrate to reduce the cost of transmission and storage. Formally, an encoder would first encode a slice(typically around 10 to 20ms) of signal s into a latent d-dimensional embedding z Rd. z willthen be iteratively quantized through N Residual Vector Quantization (RVQ) layers intox1, x2, ..., xN where",
  ": SI-SNR and ESTOI of decodedaudio. See A.1 for data and metric description": "and Qi RV d is the codebook containing V codes (d-dimensional vectors) of the i-th RVQ layer.Notice how each quantized embedding xi is a code within the corresponding codebook Qi. Theinput audio can therefore be compressed into a sequence of discrete variables which are the indicesof the codes in their corresponding codebook. The bandwidth of neural audio codec models can becontrolled by varying the number of codebooks N and the size of each codebook V . The goal ofaudio codec models is to restore the input signal with the quantized embedding and a decoder suchthat s Decoder(Ni=1 xi). Due to the hierarchical structure of RVQ layers, the information carried by the first layer RVQ code x1is at a coarse level2, and that of the remaining layers x2:N gradually becomes more fine-grained .Recent speech generation models have chosen to prioritize generating the coarseembedding x1. With the generated coarse embedding x1, the final step to synthesize audio is treatedas a separate follow-up question and has received less attention. Solutions are often ad-hoc, forexample, training a coarse-to-fine codec predictor with task-specific information like text and enrolledaudio recordings , or building a text-and-audio-conditioned codec vocoder . In this work, we aim to study a question that has been overlooked in codec-based speech generationthus far How to resynthesize speech using only the coarse representation? We focus on unconditionalresynthesis that assumes only the coarse embedding x1 is available, and no other task-specificinformation (such as transcription, speaker, or audio prompt) of the target speech is given. Thisassumption allows us to develop general methods not restricted to tasks or data annotation. We refer tothe problem as Codec Resynthesis since the ultimate goal is to resynthesize audio from limited codes.Starting from coarse-to-fine resynthesis, we take a deep dive into unconditional codec resynthesis.With the insight into the learning target, we show how regressing continuous embedding instead oftokens is better. We further improve the modeling approach, introducing a discrete-to-continuousCodec Schrdinger Bridge. Finally, we present the strengths as well as the limitations of differentmethods, along with more challenges in codec resynthesis.",
  "Neural Codec Resynthesis": "We consider codec resynthesis at the sequence level with non-autoregressive models, i.e., generatingcomplete speech from the sequence of discrete embeddings (x(1)1 , ..., x(l)1 , ..., x(L)1) where L is thesequence length. We shorthand xi x(1:L)ihereafter for simplicity. Coarse-to-fine Resynthesis. Due to the hierarchical structure of RVQ layers, each RVQ code xidepends on all the codes from prior layers x1:i1. A simple method for codec resynthesis is thereforeto iteratively predict the RVQ codes x2:N given the first x1. This can be achieved by training a model,parameterized by , to maximizeN",
  "i=2p(xi|x1:i1, i)(2)": "where p(xi|) is a categorical distribution over codebook Qi. The process can be viewed as acoarse-to-fine prediction since the later RVQ codes encode fine-grained audio information. We notethat similar coarse-to-fine models have been studied in prior works where they are autoregressive or text-and-audio-conditioned . 2In practice, the set of coarse tokens can be defined as the first c RVQ codes x1:c. This work considers themost common case c = 1 without loss of generality since all methods can be extended to c > 1. Is predicting the remaining RVQ codes x2:N necessary? While it is reasonable to resynthesizespeech in a coarse-to-fine manner, modeling codes from all RVQ layers introduces a multi-tasklearning overhead during training and the risk of error propagation during inference. Prior work attempted to address the issue through parallel prediction but found coarse-to-fine prediction workedbest empirically. To find an alternative, we take a closer look at the quality of the audio decoded from representationsat different layers of the codec model. Results are shown in . As expected, audio qualitygradually improved when involving more fine-grain embeddings. However, a key observation is thatthe pre-quantized embedding z, although never used as decoder input during training, yields the bestaudio quality. Since the ultimate goal is to generate high-fidelity audio, this observation suggests thatpredicting the remaining RVQ codes x2:N may not be necessary.",
  "f(x1) z.(3)": "We refer to this regression-based method as one-step resynthesis since projecting x1 to z requires onlya single forward pass of the model, and the result can be directly applied for decoding. Conversely,the coarse-to-fine method requires N 1 iterations to acquire x2:N. Prior work has found one-stepresynthesis to be beneficial with audio and text conditions available , but it is unclear whetherunconditional resynthesis is possible. Codec Schrdinger Bridge Resynthesis. Although one-step generation sounds appealing, recenttrends in generative models suggested iterative models, such as diffusion model , tend to syn-thesize data of better quality. From the output audio of one-step resynthesis (see demo page), wealso find it results in robotic-sounding artifacts in the speech. This motivated us to explore iterativemethods operating in the continuous embedding space to learn the mapping between x1 and z. The Schrdinger Bridge (SB) problem aimed to find the entropy-regularized optimal transportbetween two arbitrary distributions, px0 and px1. The solution to SB can be expressed by the followingforward (4a) and backward (4b) stochastic differential equations (SDEs) :",
  "T , 2": "T , ..., 1}, ft(xt) is thelinear drift, t Rd is the diffusion, Wt, W t Rd are the standard and reversed Wiener process.The terms logt(xt) and log t(xt) are the forward and backward non-linear drifts for SB with, being the solution to the following coupled PDEst = f 1",
  "SB provides a general mathematical framework for distribution mapping, but solving it can bechallenging in practice since and are often intractable in real world applications": "Fortunately, prior work has shown that SB can be tractable for certain applications where paired dataof the two distributions is available . By setting f := 0 (merging linear drift into non-linear drift)and 0 := x0 (Dirac delta distribution centered around x0), a neural network for estimating thescore function log t of the backward SDE (4b) can be trained through minimizing",
  "with 2t := t0 d and 2t := 1t d": "For codec resynthesis, we are interested in transporting between the distribution px0 pz of pre-quantized embedding x0 z and the first RVQ code distribution px1. Since the pair relation (x0, x1)is available through the audio encoding process, Codec Schrdinger Bridge can be trained directly : Codec resynthesis results. All metrics are the higher the better except WER, see A.1 for detailed explanation. Bestresults are bolded. Number of function evaluations (NFE) reflects the number of forward passes required for synthesis.",
  "Topline8 RVQ code x1:84.340.884.272.70.861-Pre-quantize emb. z4.870.954.552.70.922-Ground Truth-1.005.002.41.0003.74 0.11": "with Eq. 6. During inference, Codec Schrdinger Bridge can be used to construct the backward SDE(Eq. 4b) and derive pre-quantized embedding x0 from x1 using DDPM . The backward processcan be simulated with different step sizes by breaking down the schedule from t = 1 to t = 0 intomore/less segments, traversing iteratively from x1 to x0. In practice, smaller step sizes result in bettergeneration quality at a cost of more forward passes through the model.",
  "Experiments": "Due to space constraints, details on model architecture, training hyperparameters, datasets, andevaluation metrics are provided in Appendix Section A.1. In , we report resynthesis resultson LibriSpeech test-clean. The performance baseline is obtained from audio decoded fromthe first RVQ code x1 without resynthesis. For resynthesis methods, the decoder of Encodec takesthe resynthesis model output as input. Different toplines should be considered for different methods:(1) decoding with full RVQ codes x1:8, which is the topline for coarse-to-fine method; (2) decodingwith pre-quantized embedding z, which is the topline for one-step regression and Schrdinger Bridge.Ground truth is the raw audio used as the reference for intrusive metrics. The pre-quantized embedding is consistently a better target. As in the findings shown in Fig 2,the results in indicate that pre-quantized embedding z not only provides a higher performanceupper bound (see toplines), but also results in better models when used as a learning target. The one-step regression model performs better on all intrusive metrics at a significantly lower inference cost(NFE=1). In addition, Schrdinger Bridge consistently performs better than the coarse-to-fine modelin both objective and subjective metrics at the same (NFE=7) or lower (NFE=4) inference cost. Inshort, pre-quantized representations of codec models are better than tokens for resynthesis. A good objective score does not imply better audio quality. It is worth noting that the one-stepregression model is considered the best model in terms of WER and all intrusive metrics in ,including SI-SNR and ViSQOL that are commonly adopted for codec model development .However, this result contradicts human perception as reflected by MOS. In fact, resynthesized speechfrom one-step regression exhibited a lot of artificial sounding and robotic voices (see audio samples onthe demo page), resulting a similar MOS to the coarse-to-fine model. In contrast, Codec SchrdingerBridge significantly reduced the artifacts when taking a smaller step size (more NFE as a trade-off),resulting in more natural output as reflected by MOS. This finding suggests that the well-knownconcept that denoising methods like diffusion are strong in generating high-fidelity data alsoholds for codec resynthesis. Iterative methods improve sound quality, not content. Next, we are interested in finding out thereason why iterative methods provided a better MOS yet worse WER. Interestingly, we found thatspeech intelligibility (as measured by Whisper) does not increase as a function of NFE as shown in. This is surprising since single NFE degenerates these iterative methods into one-step methods.For the coarse-to-fine model, single NFE predicts only the second RVQ code x2, which suggests thatthe (predicted) fine-grain representation x3:8 actually introduces more noise than signal with regard tocontent. Single NFE Schrdinger Bridge is conceptually the same as the one-step regression model,",
  "Coarse-to-fine22.20.46924.60.435One-step11.50.49528.80.233SB (NFE=1)14.50.49116.00.482SB (NFE=7)19.70.50622.70.485": "which results in 14.5% WER that is closer to that of the latter 11.5% (). This indicates thatthe content of speech is harder to preserve through a more sophisticated backward process. We notethat the problem could potentially be solved with ad-hoc model design for specific applications, e.g.,conditioning the coarse-to-fine model with phone sequence . On the other hand, speaker similaritygenerally improved as NFE increased, and we hypothesize that naturalness plays an important role.The difference in reference-free MOS between different NFEs with Schrdinger Bridge in also supported this hypothesis. Iterative methods are less prone to overfitting. In , we report the results of training modelson LibriSpeech (960 hour) instead of LibriLight (60k hour) to observe model behavior when massivetraining data is not available. Surprisingly, the one-step method suffered significantly from the lackof training data, resulting in the worst WER and SIM. In contrast, the coarse-to-fine and SchrdingerBridge models have significantly less loss in performance. In practice, we also found that the one-stepregression model overfit the smaller training set after 300k updates (best result reported), whereasboth the coarse-to-fine and Schrdinger Bridge model consistently improved throughout the 500ktraining steps. These results suggest that iterative methods that partition the task into smaller subtasksare less prone to overfitting and more robust to low-resource scenarios.",
  "Conclusion": "Summarizing different methods. We first conclude that among the methods explored in thiswork, coarse-to-fine generation is the least ideal method with poor audio quality and high inferencecost. The one-step regression model is efficient and effective in subjective metrics but results inworse objective scores and robustness. Codec Schrdinger Bridge falls slightly behind but can beparticularly useful for generating high-fidelity audio. Finally, we note that while a clear improvementis made by all three methods over the baseline, results suggest that these methods still have muchroom for improvement. Rethinking the evaluation metrics. We note that the ultimate goal of codec resynthesis is findingthe mapping between a coarse-level codec and realistic audio, which can be a one-to-many mapping.In other words, any resynthesized codec embedding that decodes into realistic audio should beconsidered a success, the ground truth should only be used for training but not evaluation. This isespecially true for applications like text-to-speech. Therefore, intrusive metrics are not ideal for thetask. While reference-free machine perceptual metrics like WER can be a good alternative, it doesnot reflect human preference. Finding a better automatic subjective metric that better reflects humanpreference is still an important open question. Revisiting token-based and bridge-style generation. Recent token-based models and bridge-style models each have their strengths in speech generation. While they appear as two distinctconcepts, our findings suggest it is possible to combine the advantages of the two different methods,as demonstrated by the proposed Codec Schrdinger Bridge. Limitations. We left exploring the generalizability of codec resynthesis as an important future work.For example, the multilingual setup is expected to be even more challenging. Besides speech, soundand music are also common uses of audio codec models. Generalizing codec resynthesis beyondspeech can potentially reduce the burden of more audio generative models, e.g., music generationmodel that have to autoregressively generate codec both left-to-right and course-to-fine.",
  "Valentin De Bortoli et al. Diffusion schrdinger bridge with applications to score-basedgenerative modeling. In: Advances in Neural Information Processing Systems 34 (2021),pp. 1769517709": "Vassil Panayotov et al. Librispeech: an asr corpus based on public domain audio books. In:2015 IEEE international conference on acoustics, speech and signal processing (ICASSP).IEEE. 2015, pp. 52065210. J. Kahn et al. Libri-Light: A Benchmark for ASR with Limited or No Supervision. In:ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Process-ing (ICASSP). 2020, pp. 76697673.",
  "A.1Experiment Setup": "Model. The 6kbps Encodec is used for the codec model with d = 128 dimensional embeddingat 75Hz, N = 8 RVQ layers each with a codebook of size V = 1024. For all three methods,the input is the discrete embedding x1 of the first Encodec RVQ layer. We trained a 12-layerTransformer Encoder with a 16-head self-attention, an embedding dimension of 1024/4096for self-attention/feedforward layers, and a 0.05 layer dropout. For the coarse-to-fine model, weused a learnable stage embedding to encode the state i. For the Schrdinger Bridge model, we usedsinusoidal positional embedding to encode the timestep t. Adaptive LayerNorm is used tonormalize the output of each layer conditioning on the stage or time embedding. We used the Adamoptimizer with a weight decay of 0.01 to train each model for 1M steps, with learning ramping up in32k steps and then linearly decaying to 0 for the remaining steps. The peak learning rate for eachmethod is swept, 1e-4/5e-4/5e-4 is used for the coarse-to-fine/one-step/SB model based on validationloss. Models have around 210M parameters, and training takes 9 days on 4 A6000 GPUs. For theSchrdinger Bridge model, we set T = 1000 and t followed a symmetric noise scheduling peaking at 0.3. Additionally, we found that conditioning the model on initial point x1 (by projectingand adding it to the Transformer input) regardless of t helpful. Data. All models are trained on LibriLight , an audiobook corpus with 60k hours of Englishspeech. The dev-clean / test-clean subset of LibriSpeech is used for validation/evaluationrespectively. Each audio is randomly cropped to equal length to form an 800-second batch. Evaluation Metrics. To assess the quality of the resynthesized speech, the following metrics wereused: Scale-Invariant Signal-to-Noise Ratio (SI-SNR); Extended Short-Time Objective Intelligibility(ESTOI; ); ViSQOL , an intrusive perceptual metric that estimates mean opinion score basedon spectral similarity; Word Error Rate (WER) comparing the recognition result of Whisper v2 on resynthesized speech versus ground truth transcriptions; Speaker Similarity (SIM), the cosinesimilarity between the speaker embedding of ground truth and resynthesized speech extracted byECAPA-TDNN; Subjective Mean Opinion Score (MOS) assessing audio naturalness andquality on a scale of 1 to 5, with increments of 1. We randomly selected 40 sentences from the testset and collected 10 ratings for each model for each sample. We followed the standard approach tocollect ratings through AmazonTurk with the reward of $0.05 per rating. Each worker rated8 sentences and for each sentence audio from 6 different systems were presented. We reported theaverage rating for each system on a 95% confidence interval."
}