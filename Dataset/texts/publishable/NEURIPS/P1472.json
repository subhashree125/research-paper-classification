{
  "Abstract": "We show that variational learning can significantly improve the accuracy andcalibration of Low-Rank Adaptation (LoRA) without a substantial increase in thecost. We replace AdamW by the Improved Variational Online Newton (IVON)algorithm to finetune large language models. For Llama-2 with 7 billion parameters,IVON improves the accuracy over AdamW by 2.8% and expected calibration errorby 4.6%. The accuracy is also better than the other Bayesian alternatives, yetthe cost is lower and the implementation is easier. Our work provides additionalevidence for the effectiveness of IVON for large language models. The code isavailable at",
  "Introduction": "Bayesian methods are expected to improve the accuracy and calibration performance of LargeLanguage Models (LLMs) on downstream tasks, but they rarely succeed at such massive scale and,even when they do, often there is a substantial cost to pay. This is certainly true for finetuning withLow-Rank Adaptation , where many Bayesian variants have recently been proposed but theyall require additional computations compared to standard finetuning practices. For example, theSWAG-LoRA method needs additional computation to obtain an estimation of the posterior.LoRA ensemble requires multiple LoRA checkpoints to be trained. Methods such as Laplace-LoRA require an additional pass through the data to compute a Hessian or Fisher approximation.It is then natural to ask whether it is ever possible to use Bayes to improve LoRA without suchoverheads and increase in the costs. Here, we show that the variational (Bayesian) learning can significantly improve both the accuracy andcalibration of LoRA finetuning without a substantial increase in the cost. Our proposal is to simplyreplace the standard optimizers like AdamW by a variational learning algorithm called the ImprovedVariational Online Newton (IVON) algorithm . IVON uses a nearly identical implementationas AdamW and the swap requires just a few lines of code change. The main advantage of IVONis that its scale vector, used for the learning rate adaptation, also yields an estimate of posteriorvariance for free. The only minor overhead is due to sampling from the posterior but we show thatthis cost is negligible in practice (approximately 1% of the total training time). We achieve significantimprovements in performance when finetuning the Llama-2 model with 7 billion parameters on arange of commonsense reasoning tasks: accuracy increases by 2.8% while expected calibration error(ECE) decreases by 4.6%. The accuracy is also better than the other Bayesian alternatives, yet thecost is much lower and the implementation is easier. Our work provides additional evidence for thework of Shen et al. , showing effectiveness of IVON for large deep networks.",
  "Variational low-rank adaptation using IVON": "We will introduce our approach that we call IVON-LoRA. The idea is simple: we replace the standardAdamW optimizer by IVON which optimizes a variational-Bayesian objective. In other words, weswitch the standard objective used by AdamW to a variational one. More formally, let us denotethe AdamW objective by () where is the vector containing all the entries of LoRAs low-rankparameters (often denoted by A and B). The variational learning minimizes a different objectivewhere an expectation of () over a distribution q() is used (shown on the right),",
  "DKL[q() p()].(1)": "IVON uses a Gaussian q() = N(m, diag(v)). The mean m plays a similar role to obtained byAdamW, while the posterior variance vector v is an additional quantity. The prior p() = N(0, v0I)is a zero mean isotropic Gaussian with a scalar variance v0. A scalar weighting parameter is usedto take care of the data size N. This is because () is often an average over the whole dataset.Therefore, when using = N, we target the posterior distribution while with larger values we gotowards a colder posterior . Despite such differences in the objectives, the implementation of IVON is nearly identical to AdamWwhich makes the replacement easy and can be done by just a few lines of code change. The keypoint is that estimation of v is automatically done through the scale vector that adapts the learningrate. Therefore, posterior variances are obtained for free. The only additional step is to sample N(m, diag(v)) to evaluate the expectation in Eq. 1, but its overhead can be reduced by usingone Monte-Carlo sample per iteration. For the details, we refer to the original IVON paper by Shenet al. . Overall, IVON is a promising alternative to the existing Bayesian approaches that requireadditional overheads due to either post-processing or extra training runs.",
  "Experiments": "To evaluate the effectiveness of the proposed method, we use IVON to finetune a pretrained Llama-2model with 7 billion parameters on six datasets with commonsense reasoning multiple-choiceor true/false questions. These six datasets are WinoGrande-Small (WG-S), WinoGrande-Medium(WG-M) , ARC-Challenge (ARC-C), ARC-Easy (ARC-E) , OpenBookQA (OBQA) ,and BoolQ . We evaluate the performance of the trained LoRA adapters by calculating theaccuracy and Expected Calibration Error (ECE) on the test set. We also use test Negative Log-Likelihood (NLL) and Brier score because ECE can be sometimes unreliable . As for the baselinemethods, we compare the performance of IVON-LoRA adapters with LoRA adapters trained usingAdamW. We also consider other Bayesian alternatives, including Monte Carlo Dropout (MC Dropout), Laplace Approximation (LA) , Stochastic Weight Averaging (SWA) , and SWA-Gaussian (SWAG) . IVON is evaluated in two ways at test time: first, by using the prediction just at the mean m, andsecond, by using an averaged prediction over 10 samples from the posterior distribution. The twomethods are referred to as IVON@mean and IVON, respectively. For a fair comparison, we usethe same number of samples for MC Dropout and SWAG. The results are summarized in . First, we observe that IVON, as an alternative to AdamW,significantly improves the generalization of LoRA finetuning. When evaluated at the mean, IVONoutperforms AdamW finetuning and other Bayesian adaptations of LoRA on all datasets in termsof accuracy, often by a large margin. We also observe that IVON exhibits improved calibrationcompared to AdamW and MC Dropout, as indicated by the lower ECE, NLL and Brier values. Next, we observe that ensembling with samples from IVONs posterior distribution further improvescalibration. When evaluate at an ensemble of 10 samples drawn from the posterior distribution,IVON outperforms all other methods and is comparable to the best-performing LA (with a Kronecker-factored Hessian) and SWAG on ECE, NLL and Brier. Notably, IVON achieves this despite usinga diagonal Hessian and without an additional pass through the data for computing Hessians at theconverged point as in Laplace methods. With this improvement in calibration, IVON still maintainscomparable or better accuracy over other methods. : Comparison of techniques applied to finetuning/finetuned Llama-2 7B model across com-monsense reasoning datasets. Results at the end of training are reported, with subscripts indicatingstandard error of the mean across 3 runs. We show the relative metric changes achieved by usingIVON over AdamW in parentheses, with improvements in blue and degradation in red. The methodsmarked with * do not require customized pipeline or additional computation during inference.",
  "IVON(0.18) 0.480.01(0.16) 0.470.01(0.08) 0.210.01(0.07) 0.440.01(0.11) 0.280.01(0.02) 0.210.00(0.10) 0.35": "It is also possible to interpolate between IVON@mean and IVON to achieve the best of both.Specifically, at test time, we can scale v by a scalar > 0, that is, we predict using parameterssampled from N( | m, diag(v)). For = 0, we get IVON@mean and, for = 1, we get IVON.Increasing then allows us to gradually explore the neighboring solutions around the mean and takeadvantage of the diversity to improve calibration with a graceful loss of the accuracy. This is shownin where we see that, as increases, the error increases (accuracy decreases) but the NLLdecreases (calibration improves). In practice, this simple scaling technique is useful to get a desirabletrade-off between accuracy and calibration for specific applications. Finally, we observe that the per-step time overhead of IVON is negligible compared to AdamW.We profile our training code on an NVIDIA RTX 6000 Ada GPU. In our test run, the forward pass,loss computation, and backward pass of a training step take in total 316.3ms on average. As for theoverhead of IVON, the sampling procedure and the optimization step of each training step take 1.8msand 1.0ms on average, respectively, which is less than 1% of the running time of a training step. Theoverall training speed of IVON and AdamW are similar as shown in .",
  "Discussion": "Our direct variational learning approach using IVON effectively improves calibration and accuracy inLoRA finetuning. Given the strong results, we hope that this work invigorates research in variationalmethods for LLMs. Reasons for IVONs success are not fully understood, but one hypothesis is theprevention of overfitting as the finetuning datasets are often comparably small. This may be attributedto the preference for simpler solutions (flatter minima) which is inherent in variational learning . In a broader context, several recent works consider related approaches to improve language modelfinetuning. Following a PAC-Bayesian framework, Liu et al. proposes to finetune the full modelusing perturbed gradient descent. Chen and Garner uses variational learning to estimate parameter 0.00.51.0 30.0 32.0",
  "Error": "0.00.51.0 23.3 23.4 23.5 23.6 0.00.51.0 13.8 14.0 14.2 1.2 1.6 2.0 ARC-C 2.4 2.8 3.2 WG-S 0.56 0.60 0.64 NLL OBQA 0.4 0.5 0.6 0.7 ARC-E 2.2 2.3 2.4 WG-M 0.34 0.35 0.36 NLL BoolQ Validation Error ()NLL () : Interpolation between IVON@mean and IVON enables us to trade-off accuracy forbetter calibration at test time. Essentially, we use N( | m, diag(v)) with a scalar . For = 0, we recover IVON@mean (leftmost marker) and, for = 1, we recover IVON (rightmostmarker). Generally, as is increased, the error increases while the NLL decreases. The trend isconsistent across datasets (with a few minor exceptions). Metrics are averaged over 3 runs. importance in adaptive LoRA (AdaLoRA) . However, neither of them has been shown to workfor recent billion-scale LLMs. Similar to Liu et al. , Zhelnin et al. shows that Gaussiannoise injection can improve instruction tuning of LLMs. Different from our work, they finetune ona significantly larger instruction dataset, which is more resilient to bad calibration and overfitting.Nevertheless, these methods still demonstrate the potential of Bayesian methods and noise injectionin improving LLM finetuning. On most of the datasets, ensemble of IVON samples outperforms IVON evaluated at the posteriormean on ECE, NLL and Brier but at the cost of a slight decrease in accuracy. This is perhaps due to thelimited number of samples used in the ensemble. In our experiments, we draw 10 samples for all theensemble-based methods, both to follow the setting in Yang et al. and to keep the computationalcost manageable. It is possible that using more IVON samples could further improve the performanceof the ensemble, which is reported in Shen et al. on image classification tasks. Nevertheless, theparameter uncertainty obtained by IVON is expected to be useful for several downstream tasks suchas sensitivity analysis and model merging , which will be explored in future work. A limitation shared with other Bayesian LoRA methods is that the learned posterior overthe increment low-rank parameters is non-Gaussian because it is a product of two Gaussian randomvariables. If this is indeed a problem, a workaround could be to use a variational low-rank correctionto correct the mean and variance of a Laplace approximation of the original model. van Niekerk andRue propose such a low-rank approach in the context of latent Gaussian models, and adaptingthese ideas to large language models may be an interesting direction for future work. IVON also has some practical limitations. The method introduces two new hyperparameters overAdamW, namely the weighting parameter and the initialization of the posterior variance v. Thismakes tuning IVON a bit more involved than tuning AdamW and the results depend on setting theseparameters well. A good heuristic is to set as small as possible while still retaining stable trainingand setting the posterior initialization in the order of magnitude of the final posterior variance. Theheuristic works well in practice but the method could still benefit from more principled and automaticways to set the hyperparameters reliably. 0.0h0.5h1.0h Time (hours)",
  "and Disclosure of Funding": "This work is supported by JST CREST Grant Number JPMJCR2112. This research work has beenfunded by the German Federal Ministry of Education and Research and the Hessian Ministry ofHigher Education, Research, Science and the Arts within their joint support of the National ResearchCenter for Applied Cybersecurity ATHENE. Y. Shen and D. Cremers are supported by the MunichCenter for Machine Learning (MCML) and the ERC Advanced Grant SIMULACRON.",
  "Haolin Chen and Philip N Garner.A Bayesian interpretation of adaptive low-rank adaptation.arXiv:2409.10673, 2024": "Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and KristinaToutanova. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In Annual Conferenceof the North American Chapter of the Association for Computational Linguistics (NAACL), 2019. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, andOyvind Tafjord. Think you have solved question answering? try ARC, the AI2 reasoning challenge.arXiv:1803.05457, 2018. Nico Daheim, Thomas Mllenhoff, Edoardo Maria Ponti, Iryna Gurevych, and Mohammad EmtiyazKhan. Model merging by uncertainty-based gradient matching. In International Conference on LearningRepresentations (ICLR), 2024.",
  "P Izmailov, AG Wilson, D Podoprikhin, D Vetrov, and T Garipov. Averaging weights leads to wider optimaand better generalization. In Conference on Uncertainty in Artificial Intelligence (UAI), 2018": "Guangliang Liu, Zhiyu Xue, Xitong Zhang, Kristen Johnson, and Rongrong Wang. PAC-tuning: Fine-tuning pre-trained language models with PAC-driven perturbed gradient descent. In Conference onEmpirical Methods in Natural Language Processing (EMNLP), 2023. Wesley J Maddox, Pavel Izmailov, Timur Garipov, Dmitry P Vetrov, and Andrew Gordon Wilson. A simplebaseline for Bayesian uncertainty in deep learning. Advances in Neural Information Processing Systems(NeurIPS), 2019.",
  "Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, Sayak Paul, and Benjamin Bossan.Peft: State-of-the-art parameter-efficient fine-tuning methods. 2022": "Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity?A new dataset for open book question answering. In Conference on Empirical Methods in NaturalLanguage Processing (EMNLP), 2018. Peter Nickl, Lu Xu, Dharmesh Tailor, Thomas Mllenhoff, and Mohammad Emtiyaz Khan. The memoryperturbation equation: Understanding models sensitivity to data. In Advances in Neural InformationProcessing Systems (NeurIPS), 2023. Emre Onal, Klemens Flge, Emma Caldwell, Arsen Sheverdin, and Vincent Fortuin. Gaussian stochasticweight averaging for Bayesian low-rank adaptation of large language models. In Symposium on Advancesin Approximate Bayesian Inference (AABI), 2024.",
  "Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. WinoGrande: An adversarialWinograd schema challenge at scale. Communications of the ACM, 2021": "Yuesong Shen, Nico Daheim, Bai Cong, Peter Nickl, Gian Maria Marconi, Bazan Clement Emile MarcelRaoul, Rio Yokota, Iryna Gurevych, Daniel Cremers, Mohammad Emtiyaz Khan, and Thomas Mllenhoff.Variational learning is effective for large deep networks. In International Conference on Machine Learning(ICML), 2024. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation andfine-tuned chat models. arXiv:2307.09288, 2023.",
  "Details on experimental setup": "Our experimental design is based on Yang et al. . We utilize the PEFT library for LoRAadaptation, and apply LoRA to the query and value weights of the attention layers. Unlike in Yanget al. , we do not apply LoRA to the output layer due to numerical instability encountered insome preliminary experiments. The base model is quantized to 8-bit precision, with LoRA weightsmaintained in 16-bit precision. Finetuning is performed on a single NVIDIA RTX 6000 Ada GPUwith a batch size of 4 for 10,000 steps, without gradient accumulation. To finetune a pretrained language model which predicts the next token in a sequence for solvingmultiple-choice or true/false questions, we need to wrap the text and the choice of each questionwith predefined prompt templates to an instruction. We then use the pretrained model to predictthe next token of the wrapped instruction, and extract the output logits for the tokens standing for\"True\"/\"False\" or \"A\"/\"B\"/\"C\"/\"D\" choices. For the prompt templates, we use the same ones as inYang et al. . An example of such a prompt (used for WG-S and WG-M datasets) is as follows:",
  "Hyperparameters": "As for the hyperparameters of LoRA and AdamW finetuning, we use the same settings as in Yanget al. , which are also the default settings in Huggingfaces Transformers and PEFT library. For LoRA, we set the rank r to 8, to 16, and the dropout rate to 0.1. For AdamW optimizer,we set the initial learning rate to 5 105, weight decay to 0, and use a linear learning rate schedulerwhich decays the learning rate to 0 at the end of the training. Working IVON hyperparameters and guidelines for choosing them are discussed in Shen et al. .Still, it is not well understood how to choose them in the context of LoRA finetuning. We empiricallyfind that setting as small as possible while still retaining stable training is a good heuristic. Tochoose the initialization value v0 of the posterior variance, we track the mean value of the runningaverage of the posterior variance for the first few training steps. We notice that if the mean valuechanges significantly during the first few steps, then the initialization value is likely too far from areasonable one. We follow the guideline in Shen et al. and set the learning rate of IVON to 0.03,Hessian momentum to 1105, and clip radius to 103. Finally, We summarize the hyperparametersof IVON used in our experiments in ."
}