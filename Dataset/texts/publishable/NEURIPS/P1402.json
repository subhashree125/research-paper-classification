{
  "Abstract": "We introduce Condition-Aware Self-Supervised Learning Representation (CA-SSLR), a generalist conditioning model broadly applicable to various speech-processing tasks. Compared to standard fine-tuning methods that optimize fordownstream models, CA-SSLR integrates language and speaker embeddings fromearlier layers, making the SSL model aware of the current language and speakercontext. This approach reduces the reliance on the input audio features whilepreserving the integrity of the base SSLR. CA-SSLR improves the models capa-bilities and demonstrates its generality on unseen tasks with minimal task-specifictuning. Our method employs linear modulation to dynamically adjust internalrepresentations, enabling fine-grained adaptability without significantly alteringthe original model behavior. Experiments show that CA-SSLR reduces the numberof trainable parameters, mitigates overfitting, and excels in under-resourced andunseen tasks. Specifically, CA-SSLR achieves a 10% relative reduction in LIDerrors, a 37% improvement in ASR CER on the ML-SUPERB benchmark, and a27% decrease in SV EER on VoxCeleb-1, demonstrating its effectiveness.",
  "Introduction": "The emergence of Self-Supervised Learning Representations (SSLRs) models has revolutionizedspeech processing, setting new standards in the field. Pioneering models like Wav2vec 2.0 Baevskiet al. , HuBERT [Hsu et al., 2021], and WavLM [Chen et al., 2022a] leverage unlabeled audiodata to learn rich representations of spoken language. These models are pivotal in a wide rangeof applications, including Speech Recognition (ASR) [Chang et al., 2021], Speaker Verification(SV) [Chen et al., 2022b, Tak et al., 2022], Language Identification (LID) [Bartley et al., 2023], andSpeech Translation (ST) [Tang et al., 2022]. Benchmarks such as SUPERB [Yang et al., 2021] andML-SUPERB [Shi et al., 2023a] have been crucial in evaluating SSL model performance, providingstandardized tasks. Although SSLR training approaches combine speech from various sources, these models learnrepresentations solely from unpaired audio-only data. When extending SSLR features to multilingualscenarios and low-resource languages, unsupervised training limits the models ability to distinguishbetween different languages, resulting in unified features for all languages. Additionally, labeling allSSL training data with language and speaker information requires significant human effort and isimpractical. Thus, a post-training conditioning approach is more favorable. In other fields, methodslike [Zhang et al., 2023] and IP-Adaptor [Ye et al., 2023] in image processing, and CTRL [Keskaret al., 2019] in NLP, have successfully integrated conditioning into pretrained models, demonstratingpotential applications for speech processing.",
  "arXiv:2412.04425v1 [eess.AS] 5 Dec 2024": "as language identification, multilingual speech recognition, and speaker verification. Unlike standardadaptation methods that optimize the SSLR parameters for downstream models, CA-SSLR integrateslanguage and speaker embeddings from earlier layers, making the SSLR aware of the current languageand speaker context. This technique enables the creation of models that perform multiple tasks with asingle adapted SSL encoder by strategically injecting conditional adapters into each encoder blockwhile freezing the pretrained encoder weights. CA-SSLR follows a hierarchical self-adaptationstructure, where adapters at each layer are conditioned on intermediate task-specific embeddingsestimated from lower layers. Attention mechanisms and linear modulation dynamically adjust scalingand biasing, tailoring the models response at each time step. The initialization techniques allowthe conditioning module to perform identity transformations, ensuring the existing model behavioris maintained when incorporating new conditions. This approach reduces the number of trainableparameters, mitigates overfitting, and avoids catastrophic forgetting. We conduct experiments onthree popular types of multilingual speech processing tasksASR, LID, and SV to demonstrate theversatility and efficiency of CA-SSLR. This works main contribution is introducing a novel method for conditioning the SSLRs with limitedsupervised labels. This leads to generalized speech representation with improved performance usingminimal trainable parameters and maintains the models behavior. This includes: Hierarchical Dynamic Conditioning: We design attention-based conditional adapters andintegrate them into the SSL model. Our approach dynamically tailors the models behaviorto the input language and speaker characteristics at each time step, which are periodicallyestimated from previous layers. Preservation of Pre-trained Weights with Efficient Parameter Utilization: The modelcapitalizes on the knowledge of the foundational model pre-trained weights and introduceslightweight adapters that modulate the encoder hidden representation by a scalar and bias, significantly reducing the trainable parameters. This strategy ensures more stable andparameter-efficient training. Harmonized Task Compatibility with Notable Performance Improvements: Our experi-ments show that CA-SSLR reduces the number of trainable parameters, mitigates overfitting,and excels in under-resourced and unseen tasks. Specifically, CA-SSLR achieves an 27%relative reduction in LID errors, a 37% improvement in ASR CER on the ML-SUPERBbenchmark, and a 27% decrease in SV EER on VoxCeleb-1. These results highlight CA-SSLRs effectiveness in enhancing multilingual SSLRs while also lowering computationalcosts for multitask fine-tuning.",
  "Related Work": "Self-supervised learning representation.Self-Supervised Learning (SSL) models, epitomized byWav2Vec 2.0 [Baevski et al., 2020], HuBERT [Hsu et al., 2021], and WavLM [Chen et al., 2022a],have significantly advanced speech processing by leveraging vast amounts of unlabeled audio data.These models excel in extracting rich speech representations, capturing its intricate acoustic, phonetic,and semantic nuances. These models are fine-tuned on smaller, labeled datasets to adapt the genericrepresentations for specific tasks, achieving impressive results. In the realm of cross-lingual speech representation, Wav2Vec 2.0-XLSR (Cross-Lingual SpeechRepresentation) [Babu et al., 2021] takes a significant leap forward. It builds on the robust architectureof Wav2Vec 2.0 but is pre-trained on a diverse, multilingual dataset, learning universal representationstransferable across languages. Similarly, mHuBERT (Multilingual HuBERT) [Lee et al., 2021]extends the foundational HuBERT model to process multiple languages effectively. This makes themimmensely powerful for multilingual speech recognition and understanding tasks. The benchmarkfor the SSL models also extends from monolingual SUPERB [Yang et al., 2021] to multilingualML-SUPERB [Shi et al., 2023a], building new standards for the SSLR models. Adaptation Methods.In many studies Yang et al. , Shi et al. [2023a], Chen et al. [2023a],the SSLR remains frozen while decoders are trained for a specific task. Since the encoder is sharedacross all tasks, this approach offers the advantage that it allows us to evaluate multiple tasks on agiven speech signal with just one encoder run. However, systems of this kind often exhibit poorerperformance when compared to those incorporating some degree of adaptation of the SSLR to thetarget task. The latter can involve fine-tuning the entire SSL encoder Chen et al. [2022a], a subset of",
  ": CA-SSLR scheme and its time-channel attention conditioner. Only the conditioner andlinear projections for the decoders are trainable, and all other parameters are frozen during adaptation": "its layers, or introducing lightweight adapters [Chen et al., 2023b] within its layers. Unfortunately,this results in employing a distinct encoder per task, leading to a large increase in computational loadthat scales linearly with the number of tasks to be assessed. Conditioning Pre-trained Models.Image processing has successfully integrated conditioninginto pretrained models using methods like ControlNet [Zhang et al., 2023] and IP-Adaptor [Yeet al., 2023]. ControlNet allows for precise control over generated images by incorporating additionalconditions such as edge maps or sketches, while IP-Adaptor uses small-scale adapter modules toadjust the models behavior based on specific conditions without altering the pre-trained modelsparameters. These techniques have achieved significant success and offer insights for potentialapplications in speech processing. Similarly, in Natural Language Processing (NLP), models like theConditional Transformer Language Model (CTRL) [Keskar et al., 2019] have introduced conditioningto improve model performance. CTRL uses control codes to guide text generation based on specifiedattributes like style or domain, allowing for efficient adaptation without extensive retraining. Thesuccesses in image processing and NLP highlight the potential for conditioning pre-trained SSLRs inspeech processing. Hierarchical Conditioning.Hierarchical models have been used in previous speech models.[Sanabria and Metze, 2018] proposes a multi-task ASR model that improves intermediate repre-sentations by performing Connectionist Temporal Classification at different levels of the networkwith targets of different granularity. Essentially, representations in lower layers are used to predictcharacter tokens, while higher layers predict subword units with growing vocabulary sizesfrom300 to 10k subword units in the last layer. [Chen et al., 2023a] further explored this by integratinghierarchical conditional layers within the ASR decoder, using ASR tokens predicted from precedinglayers to inform subsequent layers.",
  "Methodology": "We propose Condition-Aware SSLR (CA-SSLR), designed to serve as a universal encoder for multipledownstream speech tasks. CA-SSLR enhances pre-trained SSL models by integrating intermediateLID and SV predictions to condition and adapt subsequent layers dynamically. This approach allowsthe model to capture essential language and speaker characteristics, refining its outputs progressivelyand making it particularly effective in multilingual and multispeaker scenarios.",
  ": Architecture of the CA-SSLR model employing hierarchical self-conditioning with Time-Channel Attention Conditioners (TCACs)": "modulates the hidden representations of the SSL encoder layers based on conditioning featuresderived from intermediate LID and SV embeddings. This hierarchical conditioning mechanismenables the model to adapt dynamically to different input conditions while keeping the majority ofthe pre-trained parameters fixed. In the following sections, we detail the components of CA-SSLR,starting with the conditioner module and then explaining how it integrates into the overall architecture.We also describe the incremental training strategy employed to incorporate conditioning informationwithout catastrophic forgetting.",
  "Channel-wise and Time-wise Attention Conditioner": "A central component of CA-SSLR is the channel-wise conditioner (CC) or the time-channel attentionconditioners (TCAC), which modulates the SSL encoders hidden representations based on condi-tioning features. As depicted in b, the TCAC tasks the latent representations S(l) RCTfrom layer l of the SSL encoder and a conditioning feature vector z RR, derived from intermediateLID or SV embeddings. The TCAC outputs modulated latent representations S(l) by applyingtime-channel-dependent scaling and bias:",
  "+ b(l) )(4)": "where f(.) is a ReLU non-linearity, W(l) RC(C+R), b(l) RC, and v RC. Theconditioning feature z is obtained by processing the intermediate embeddings e RE from theLID or SV decoders, as z = LayerNorm(We + b), where W RRE and b RR are sharedlinear transformation parameters, and LayerNorm() denotes layer normalization. In scenarios wheretime-based modulation is unnecessary, the model can switch to the simpler Channel-wise Conditioner(CC) by using only the channel-dependent components and . This flexibility in conditioning design enables the model to be tailored to various speech tasks with differing complexity requirements.By integrating these conditioning methods, CA-SSLR dynamically adapts its internal representationsbased on language and speaker characteristics. This mechanism enables the integration of conditioninginto the models latent representations without altering the pre-trained encoders original parameters.",
  "Hierarchical Self-Conditioning in CA-SSLR": "Building upon the TCAC module, CA-SSLR employs a hierarchical self-conditioning mechanismwithin the SSL encoder layers. As shown in , the SSL encoder is partitioned into layer groups,with TCACs inserted after the attention module in each layer to modulate hidden representationsbased on updated conditioning features. The model aggregates SSL features through a weighted sum,combining outputs from all preceding layer groups. These aggregated features are then provided tothe LID and SV decoders, where LID and SV embeddings are extracted and processed through alinear layer followed by layer normalization to create the conditioning feature z for the TCACs. The conditioning feature z is re-estimated at intervalsevery three layers for LID and every sixlayers for SVusing the aggregated SSL features from previous groups. This hierarchical designprogressively refines the models representations, adapting to the inputs language and speakercharacteristics at different depths of the network. For example, the initial SSL layer group capturesbasic language and speaker characteristics, generating embeddings that condition the next groupof layers via TCACs. This ongoing refinement allows the model to dynamically adapt based onintermediate predictions, resulting in a context-aware and dynamic representation. Each layer group uses distinct TCAC parameters, enabling tailored scaling and bias adjustments atdifferent stages of the model. Notably, only the TCACs and the linear projections for the decodersare trainable, while all other SSL encoder parameters remain fixed during the conditioning insertion.This design minimizes overfitting and accelerates training due to the smaller number of trainableparameters. This hierarchical self-conditioning mechanism enables the model to dynamically capturediverse aspects of input audio, making it a robust tool for comprehensive speech analysis.",
  "Incremental Training Strategy": "Incorporating new components into a pre-trained SSL encoder poses the risk of catastrophic forgetting,where the model loses previously acquired knowledge. To mitigate this, we adopt an incrementaltraining strategy that gradually integrates the conditioning information. We initialize the TCACparameters to ensure that the initial modulated features are identical to the original SSL features.Specifically, we set the initial values such that t = 1 for all t, c = 1, and c = 0 for all c.According to Eq. (1), this initialization means that S(l)t,c = S(l)t,c at the start of training, allowing asmooth transition from the pre-trained model to the conditioned model.",
  "Datasets": "For the LID and ASR tasks, we utilized the ML-SUPERB benchmark [Shi et al., 2023a]. Thiscorpus comprises two distinct data configurations: 10-minute/language and 1-hour/language foreach of the 123 well-represented languages (Normal). Additionally, both configurations include fivetraining utterances for each of 20 selected low-resource languages (Few-shot)1. For the Few-shotlanguages, we also considered an Extended Few-shot condition in which we augmented the amountof data to match that of the Normal languages. However, the Extended Few-shots only included",
  "We discovered that a portion of the few-shot Lithuanian (lit) training and testing data was erroneouslysubstituted with Italian (it), leading us to omit the Lithuanian outcomes from the evaluation": "language labels but not ASR transcripts. This aims to analyze the behavior of few-shot languages withimproved language ID accuracy, as achieving satisfactory language accuracy with just five utterancesis challenging. This is also a reasonable assumption since obtaining data with language labels is easierand cheaper than obtaining transcribed data. The moderate size of this dataset was ideal for testingour approach, as it allowed us to conduct multiple ablation experiments with limited computingresources. As ML-SUPERB lacks speaker labels, we combined it with VoxCeleb2 [Nagrani et al.,2017] for training models incorporating the SV task. VoxCeleb2 contains 1,092 speech hours from5,994 speakers, although it lacks LID labels and ASR transcripts. The SV task was tested on theVoxCeleb1 original set. The speech was augmented with Musan noise Snyder et al. andreverberation Ko et al. during SV training.",
  "Model Architecture": "SSLR Models.In our system, we employed the best multilingual SSL back-bones in the ML-SUPERB benchmark: Wav2Vec2-XLSR with 300M parameters, trained on 128 languages2, andthe 100M parameter multilingual Hubert (mHuBERT) model [Lee et al., 2021], trained on English,Spanish, French data from VoxPopuli [Wang et al., 2021] unlabeled speech as our foundationalacoustic encoders. These models have demonstrated their efficacy in processing a wide range oflinguistic inputs and form the backbone of our system. We experimented with the S3PRL [Yang et al.,2021] and ESPnet [Watanabe et al., 2018] toolkits. Our training dataset combined data labeled forASR+LID labels, LID only, or SV only. Hence, we computed the losses only for the available tasksfor each sample. Detailed information on the remaining training hyperparameters is provided in theappendix, and the code will be made available for reproducibility. A models training takes about oneday using 2 A100 GPUs. Speaker and Language Decoders.The speaker and language decoders are based on the ECAPA-TDNN architecture [Desplanques et al., 2020]. Initially, a convolutional layer projects SSL represen-tation to the decoder dimension (512 for LID and 1024 for SV). This is followed by a sequence of1-dimensional SE-Res2Net [Gao et al., 2021] layers (one for LID and three for SV). Next, channel-wise attentive statistic pooling aggregates the frame-level features into a single utterance-level vector,which is projected into lower-dimensional speaker embedding. The training loss was Additive An-gular margin-softmax [Deng et al., 2019] with margin=0.3 for SV and margin=0.0 for LID. Largemargin helps to create highly compact speaker representations [Villalba et al., 2022], while beingdetrimental in LID [Villalba et al., 2023]. The SV and LID decoders producing the final resultconsume a weighted average of all SSL layers. Meanwhile, the ones estimating the conditioningembeddings use a weighted average of the SSL layers evaluated up to that point in the chain. Notethat all SV and LID decoders share parameters, so the number of trainable parameters remainsindependent of the frequency with which we re-compute the conditioning embeddings. ASR decoder.The ASR decoder conforms to the framework set by the ML-SUPERB benchmark[Shi et al., 2023a], facilitating comparable evaluations. A convolutional downsampling layer halvesthe SSL feature sequence duration. These features are channeled into a two-layer Transformer with256-dim self-attention, eight attention heads, and 1024-dim feed-forward layers. A linear output layerwith connectionist temporal classification (CTC) loss predicts multi-lingual character-level tokens.",
  "Generalization Ability on Unseen Tasks": "Experiment Setting.We conducted experiments to evaluate the generalization capabilities of theadapted SSLR models on LID, ASR, and SV tasks. The SSLR models were adapted for one task(either LID or ASR) and then evaluated on both the adapted task and an unseen task. For LIDadaptation, the SSLR was trained exclusively with LID labels. We compared three setups: fullfine-tuning (LID-FT), Houlsby adaptors Houlsby et al. (LID-Houlsby), and our proposedcondition-aware approach (LID-CA-XLSRLdual). In this setup, we employed an additional LID decoderusing the pre-trained SSLR to pre-generate language embeddings, which were then used to conditionthe SSLR model for a second inference pass. For ASR adaptation, the models were trained withASR loss using three setups: full fine-tuning (ASR-FT), Houlsby adaptors (ASR-Houlsby), and our : Evaluation of adapted XLSR models on the 10-min ML-SUPERB and VoxCeleb dataset forLID, ASR, and SV tasks. These evaluations test the encoders generalizability across different tasks,demonstrating effectiveness without further task-specific tuning.",
  "+ ASR-FT-17.132.21.290.095+ ASR-Houlsby25620.334.61.370.097+ ASR-CA-XLSRL (ours)25618.631.61.150.088": "proposed hierarchical conditioning method with TCAC layers integrated into the SSLR model withsingle inference (ASR-CA-XLSRL). During ASR adaptation, the LID decoder is integrated into theSSLR model to provide conditioning features, but SV information was not included during training. Results.In LID adaptation (a), both LID-FT and LID-Houlsby improved LID performancecompared to the pre-trained SSL baseline. However, on the unseen ASR task, the fully fine-tunedSSLR encoder improved ASR CER by only 2%, while LID-Houlsby showed limited generalization,with CER improvements of 5.4% and 3.9% for normal and few-shot languages, respectively. OurLID-CA-XLSRLdual method achieved significantly better generalization, improving ASR CER by 7.3%and 6.6% for normal and few-shot languages. In ASR adaptation (b), all models enhancedASR performance, but ASR-Houlsby and full fine-tuning degraded SV performance relative tothe baseline, highlighting their limited generalization. Our ASR-CA-XLSRL approach not onlypreserved but improved SV performance, reducing EER by relative 10.9% and DCF by 5.4%,showcasing strong generalization to the unseen SV task. These results demonstrate that CA-SSLRsignificantly outperforms full fine-tuning and standard adaptation methods in terms of generalization.By effectively leveraging conditioning information, CA-SSLR adapts across tasks while maintainingperformance on unseen ones. Our proposed conditioner offers both robust adaptations on trainingtasks and superior generalization, making CA-SSLR a versatile and effective solution for multilingualand multispeaker speech processing.",
  "Condition-Aware SSLR Model": "Experiment Setting. investigates the CA-SSLR approach with hierarchical languageconditioning. The first block of the table refers to the baseline where the foundational modelsare frozen, while the second block (CA-XLSRLdual) utilizes a separate task-specific LID model topre-generate the language embedding. The third block presents our proposed approach, where were-estimate the language embedding every fourth or third layer (CA-XLSRL (4L, 3L)) within theXLSR model, not required a separate LID system. The experiments utilized two types of conditioners:TCAC, which incorporates attention, and a variant without attentionreferred to as Channel-wiseConditioners (CC)where the same scale and bias are applied uniformly across all time frames. Thereal-time factors (RTF) as proc-time/signal-length are provided for assessing efficiency3.",
  "CA-XLSRL,S (CC)0.0321.3489.118.81.040.07588.115.00.940.073CA-XLSRL,S (TCAC)0.0321.3489.018.31.110.08693.514.41.010.077": "Results.First, we observed that both CA-XLSRLdual and CA-XLSRL systems with TCAC (withattention) generally performed better than the CC (w/o attention) counterparts, reaffirming thebenefits of the time-wise attention design. In the second block, CA-XLSRLdual slightly outperformedCA-XLSRL in terms of CER for both the 10-minute and 1-hour datasets. However, its real-timefactor (RTF) is akin to the combined RTFs of separate LID and ASR models since it runs Wav2Vec2twiceonce for language embedding extraction and again for ASR conditioningposing challengesfor streaming applications. On the other hand, CA-XLSRL(CC, 3L) excelled among the threeapproaches, achieving a 35.9% and 19.0% relative improvement in Normal and few-shot languages,respectively, compared to the baseline in the 10-minute setup, and 33.5% and 19.8% in the 1-hoursetup. LID accuracy remained comparable among the various CA-XLSR models, with a notableperformance improvement from 90.9% to 93.4% in 1-hour setup.",
  "Generalist Condition-Aware SSLR Model": "Experiment Setting. presents results for general CA-SSLR models that combine Multi-lingual ASR, LID, and SV tasks. The table compares the baselines, with frozen and fine-tuned SSLmodels, to two different CA-SSLR Hierarchical models (CA-SSLRL and CA-SSLRL,S). We furtherinclude another well-known multilingual SSLR model, mHuBERT, for a comprehensive comparison.The LID conditioning systems (CA-SSLRL) are the same as from the previous section, conditioningthe SSL model only on LID embeddings, with the SV decoder added on top of SSL features withoutfurther adaptation. The LID + SV conditioning system (CA-SSLRL,S) combines both LID and SVembeddings and is jointly trained on ASR, SV, and LID losses. The intermediate LID embeddingswere recomputed every three layers as the best configuration in , and SV embeddings wererecomputed every six SSL layers. Apart from ASR CER and LID Acc on ML-SUPERB, we presentSV equal error rates (EER) and detection cost function (DCF), measured at target prior probability",
  "p = 0.05 [Sadjadi et al., 2022], on VoxCeleb1. SV performance varied depending on whether wetrained the model combining 10min ML-SUPERB + VoxCeleb2 or 1h ML-SUPERB + VoxCeleb2": "Fine-tuning Baseline.In the fully fine-tuning experiment, we initialized the model with pretrainedASR, LID, and SV decoders and fine-tuned for a few epochs. However, this approach resulted insuboptimal performance compared to the frozen SSLR baseline. The \"FT\" experiments showeddegraded performance, with LID accuracy decreasing by 5.7%, ASR CER increasing by 3.1%, andSV EER worsening by 4.2 in absolute values on average across the four settings. This declineis unexpected, as fine-tuning typically improves performance. This suggests that simultaneousadaptation of the SSL layers to multiple tasks causes conflicting adjustments, reducing the modelsrobustness. Consequently, catastrophic forgetting led to worse performance compared to the baseline.Conversely, the condition-aware SSLR models exhibited superior performance comparing with thefrozen baseline, indicating that training the inserted condition layers does not alter the modelsbehavior for downstream tasks but improves its ability to represent the input speech data. Language Conditioned SSLR.CA-SSLRL(CC) notably enhanced SV performance w.r.t. thebaseline, despite its encoder being solely tuned for ASR and LID tasks. For XLSR, the EER improvedby 14% and 20% relative for the 10-min. and 1-h. sets, respectively, while DCF improved by 16-18%.Similarly, for mHuBERT, we observed comparable enhancements, with the EER improving by 17%in both sets and the DCF improving by 17-18%. This demonstrates that the CA-SSLR approach offerssuperior generalization capabilities compared to the original pre-trained SSL encoder, deliveringimproved performance. CA-SSLRL(TCAC) performance in SV is comparable to its non-attentioncounterpart with better performance in LID and ASR as discussed in Sec.5.2.",
  "Language and Speaker Conditioned SSLR.Adding a speaker conditioner to CA-SSLRL,S fur-ther improved its performance. In ASR tasks, incorporating the speaker conditioner to CA-XLSRL,S": "reduced CER by 3.1% for the 10-min. set and 6.2% for the 1-hr set, relative to CA-XLSRL. For LIDtask, CA-SSLRL,S shows similar performance to other models with relative differences below 3%.For SV, CA-XLSRL,S using channel-wise conditioner (CC) reduced EER by 19.4-27.1%, outper-forming CA-XLSRL. Switching from CC to TCAC yielded additional gains in ASR, adding a relativeimprovement of 2.7-4.0%. In contrast, its impact on SV was more modest, with improvements inEER by 14.0-21.7%. Overall, TCAC demonstrated better adaptation ability, while CC excelled ingeneralization. ASR and RTF Discussion.Generally, we observed the largest improvement for ASR when in-cluding the language conditioner, as it enables the system to adapt to produce output tokens inthe correct language. Conversely, adapting the model to the input speaker provided fewer ASRgains. The XLSR model benefitted from our approach better than mHuBERT, possibly becausemHuBERT is 3 smaller than XLSR, but more importantly, because mHuBERT was trained on justfour languages compared to 128 in XLSR. Therefore, the pre-trained mHuBERT has not encounteredenough diversity in terms of languages and speakers, thereby limiting its performance in multi-lingualASR and SV. In terms of RTF, while the conditioned models are 13-34% slower compared to sharingthe pre-trained SSL encoder for the three tasks, both CA-SSLRL and CA-SSLRL,S offer superiorperformance while being much faster than running task-specific models separately, indicating a moreefficient use of computational resources while running the generalist model.",
  "Analysis of the TCA Conditioner": "Ablation study of Conditioning Approach. conducts an ablation study for differentconditioning methods with CA-XLSRLdual settings within the ML-SUPERB 10min dataset regardingASR CER. First, we used conditioners without attention (CC) on the ground truth LID predictions(G.T.), serving as the upper bound for the performance of our proposed approach. This improvedthe Normal languages from 29.0% to 17.2%, and Few-shot languages from 39.0% to 27.9%, w.r.t.the pre-trained XLSR model. This showcases the potential of the condition-aware SSLR. Following,we compared conditioning on hard-predicted language labels (Hard), soft-predicted language labels(Soft), and language embeddings from the LID decoder bottleneck layer (Embed) for comparison.Conditioning on Hard LID labels improved the most in Few-shot languages, improving by 26%relative to the baseline. On the other hand, the Embed case outperformed the Soft case and providedbalanced performance for both Normals and Few-shots languages. Additionally, we compared CC to : Ablation study of condition-awaresettings for ASR-adapted XLSR models on10-min ML-SUPERB dataset, using CC orTCAC. Conditioning is based on predictedlanguage labels or LID embeddings, ex-cept in the ground truth (G.T.) experiment.",
  "TCAC. The TCAC provided the best overall results, improving Normals and Few-shots by 38.6% and18.5%, respectively, w.r.t. baseline": "Parameter Efficiency in CER Reduction. compares CER versus the number of trainableparameters for different adaptation methods, including our proposed Channel-wise Conditioner andTime-Channel Attention Conditioner (CC-TCAC), the Houlsby adapter, LoRA [Hu et al., 2021], fullfine-tuning (FT), and the baseline XLSR model. The Houlsby adapters, with hidden dimensions of256 and 512, have 18.4M and 30.9M trainable parameters. In comparison, the CC-TCAC approach,conditioned on precomputed LID embeddings with 256 dimensions (18.7M for CC and 22.6M forTCAC), achieves lower CERs with similar parameter counts. LoRA provided only marginal gainsover the baseline, aligning with findings from Chen et al. [2023b]. In contrast, FT required fine-tuning1624 layers (200300M parameters) to achieve comparable CER reductions, making CC-TCACabout ten times more efficient. As discussed in Sec 5.1, CC-TCACs key contribution is its superiorgeneralization ability. While the Houlsby adapter enhances task-specific adaptation, it falls short ingeneralizing to unseen tasks. In contrast, CC-TCAC achieves both effective adaptation and robustgeneralization, making it a versatile solution for diverse applications.",
  "Conclusion": "This paper introduces the CA-SSLR framework, an innovative approach that integrates conditioninginto pre-trained Self-Supervised Learning (SSL) models by adapting only the trainable conditioner.Through a hierarchical self-conditioning mechanism, where intermediate language and speakerfeatures condition the upper layers of the SSL model, CA-SSLR achieves a 33% reduction inCharacter Error Rate compared to the pre-trained baseline, matching the performance of single-taskfully fine-tuned models. Additionally, it improves Speaker Verification EER by 27% and reduceLanguage Identification errors by relative 10% in average. The results indicate that condition-awareSSLR models enhance the models interpretation of input speech data, providing superior performancecompared to traditional fine-tuning methods. This improvement is achieved by dynamically tailoringthe models response to the input language and speaker characteristics, ensuring robust generalizationacross various tasks. In summary, CA-SSLR offers a versatile and efficient approach to integratingconditioning information into pre-trained models. This method not only enhances performance acrossmultiple tasks but also ensures efficient parameter utilization, supported by an improved RTF thatfacilitates its application in real-world scenarios. Broader Impact and LimitationsThe CA-SSLR methodology improves the conditioning of pre-trained Self-Supervised Learning (SSL) models for speech processing, improving performance withminimal fine-tuning and reducing computational resource requirements. This advancement facilitatesthe deployment of robust models in resource-constrained environments, promoting broader access toadvanced speech technology. However, there are potential risks. The conditioning mechanisms mightamplify biases in the training data, leading to unfair outcomes, particularly for underrepresentedlanguages and speaker groups. Ensuring diverse and balanced datasets, along with continuousmonitoring, is crucial to mitigate these risks and prevent perpetuating existing inequities. Arun Babu, Changhan Wang, Andros Tjandra, Kushal Lakhotia, Qiantong Xu, Naman Goyal, KritikaSingh, Patrick von Platen, Yatharth Saraf, Juan Pino, et al. Xls-r: Self-supervised cross-lingualspeech representation learning at scale. arXiv preprint arXiv:2111.09296, 2021. Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: A frameworkfor self-supervised learning of speech representations. Advances in neural information processingsystems, 33:1244912460, 2020. Travis M Bartley, Fei Jia, Krishna C Puvvada, Samuel Kriman, and Boris Ginsburg. Accidentallearners: Spoken language identification in multilingual self-supervised models. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages15. IEEE, 2023. Xuankai Chang, Takashi Maekaku, Pengcheng Guo, Jing Shi, Yen-Ju Lu, Aswin ShanmugamSubramanian, Tianzi Wang, Shu-wen Yang, Yu Tsao, Hung-yi Lee, et al. An exploration of self-supervised pretrained representations for end-to-end speech recognition. In 2021 IEEE AutomaticSpeech Recognition and Understanding Workshop (ASRU), pages 228235. IEEE, 2021. Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, NaoyukiKanda, Takuya Yoshioka, Xiong Xiao, et al. Wavlm: Large-scale self-supervised pre-trainingfor full stack speech processing. IEEE Journal of Selected Topics in Signal Processing, 16(6):15051518, 2022a. Sanyuan Chen, Yu Wu, Chengyi Wang, Shujie Liu, Zhuo Chen, Peidong Wang, Gang Liu, Jinyu Li,Jian Wu, Xiangzhan Yu, et al. Why does self-supervised learning for speech recognition benefitspeaker recognition? arXiv preprint arXiv:2204.12765, 2022b. William Chen, Brian Yan, Jiatong Shi, Yifan Peng, Soumi Maiti, and Shinji Watanabe. Improvingmassively multilingual asr with auxiliary ctc objectives. In ICASSP 2023-2023 IEEE InternationalConference on Acoustics, Speech and Signal Processing (ICASSP), pages 15. IEEE, 2023a. Zih-Ching Chen, Chin-Lun Fu, Chih-Ying Liu, Shang-Wen Daniel Li, and Hung-yi Lee. Exploringefficient-tuning methods in self-supervised speech models. In 2022 IEEE Spoken LanguageTechnology Workshop (SLT), pages 11201127. IEEE, 2023b. Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular marginloss for deep face recognition. In 2019 IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR), pages 46854694, 2019. doi: 10.1109/CVPR.2019.00482. Brecht Desplanques, Jenthe Thienpondt, and Kris Demuynck. Ecapa-tdnn: Emphasized chan-nel attention, propagation and aggregation in tdnn based speaker verification. arXiv preprintarXiv:2005.07143, 2020. Shang-Hua Gao, Ming-Ming Cheng, Kai Zhao, Xin-Yu Zhang, Ming-Hsuan Yang, and Philip Torr.Res2Net: A New Multi-Scale Backbone Architecture. IEEE Transactions on Pattern Analysis andMachine Intelligence, 43(2):652662, Feb 2021. ISSN 1939-3539. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe,Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning fornlp. In International Conference on Machine Learning, pages 27902799. PMLR, 2019. Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov,and Abdelrahman Mohamed. Hubert: Self-supervised speech representation learning by maskedprediction of hidden units. IEEE/ACM Transactions on Audio, Speech, and Language Processing,29:34513460, 2021.",
  "Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,and Weizhu Chen.Lora: Low-rank adaptation of large language models.arXiv preprintarXiv:2106.09685, 2021": "Nitish Shirish Keskar, Bryan McCann, Lav R Varshney, Caiming Xiong, and Richard Socher.Ctrl: A conditional transformer language model for controllable generation. arXiv preprintarXiv:1909.05858, 2019. Tom Ko, Vijayaditya Peddinti, Daniel Povey, Michael L Seltzer, and Sanjeev Khudanpur. A study ondata augmentation of reverberant speech for robust speech recognition. In 2017 IEEE InternationalConference on Acoustics, Speech and Signal Processing (ICASSP), pages 52205224. IEEE, 2017. Ann Lee, Hongyu Gong, Paul-Ambroise Duquenne, Holger Schwenk, Peng-Jen Chen, ChanghanWang, Sravya Popuri, Yossi Adi, Juan Pino, Jiatao Gu, et al. Textless speech-to-speech translationon real data. arXiv preprint arXiv:2112.08352, 2021.",
  "Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. Voxceleb: a large-scale speaker identifica-tion dataset. arXiv preprint arXiv:1706.08612, 2017": "Ethan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin, and Aaron Courville. Film: Visualreasoning with a general conditioning layer. In Proceedings of the AAAI conference on artificialintelligence, volume 32, 2018. Seyed Omid Sadjadi, Craig Greenberg, Elliot Singer, Lisa Mason, and Douglas Reynolds. The 2021NIST Speaker Recognition Evaluation. In Proc. The Speaker and Language Recognition Workshop(Odyssey 2022), pages 322329, 2022. doi: 10.21437/Odyssey.2022-45.",
  "Ramon Sanabria and Florian Metze. Hierarchical multitask learning with ctc. In 2018 IEEE SpokenLanguage Technology Workshop (SLT), pages 485490. IEEE, 2018": "Jiatong Shi, Dan Berrebbi, William Chen, Ho-Lam Chung, En-Pei Hu, Wei Ping Huang, XuankaiChang, Shang-Wen Li, Abdelrahman Mohamed, Hung-yi Lee, et al. Ml-superb: Multilingualspeech universal performance benchmark. arXiv preprint arXiv:2305.10615, 2023a. Jiatong Shi, William Chen, Dan Berrebbi, Hsiu-Hsuan Wang, Wei-Ping Huang, En-Pei Hu, Ho-LamChuang, Xuankai Chang, Yuxun Tang, Shang-Wen Li, et al. Findings of the 2023 ml-superbchallenge: Pre-training and evaluation over more languages and beyond. In 2023 IEEE AutomaticSpeech Recognition and Understanding Workshop (ASRU), pages 18. IEEE, 2023b.",
  "David Snyder, Guoguo Chen, and Daniel Povey. Musan: A music, speech, and noise corpus. arXivpreprint arXiv:1510.08484, 2015": "Hemlata Tak, Massimiliano Todisco, Xin Wang, Jee-weon Jung, Junichi Yamagishi, and NicholasEvans. Automatic speaker verification spoofing and deepfake detection using wav2vec 2.0 anddata augmentation. arXiv preprint arXiv:2202.12233, 2022. Yun Tang, Hongyu Gong, Ning Dong, Changhan Wang, Wei-Ning Hsu, Jiatao Gu, Alexei Baevski,Xian Li, Abdelrahman Mohamed, Michael Auli, et al. Unified speech-text pre-training for speechtranslation and recognition. arXiv preprint arXiv:2204.05409, 2022. Jess Villalba, Bengt J Borgstrom, Saurabh Kataria, Magdalena Rybicka, Carlos D Castillo, JaejinCho, L. Paola Garca-Perera, Pedro A. Torres-Carrasquillo, and Najim Dehak. Advances in cross-lingual and cross-source audio-visual speaker recognition: The jhu-mit system for nist sre21. pages213220. ISCA, 6 2022. doi: 10.21437/Odyssey.2022-30. URL Jess Villalba, Jonas Borgstrom, Maliha Jahan, Saurabh Kataria, Leibny Paola Garcia, Pedro Torres-Carrasquillo, and Najim Dehak. Advances in Language Recognition in Low Resource AfricanLanguages: The JHU-MIT Submission for NIST LRE22. In Proc. INTERSPEECH 2023, pages521525, 2023. doi: 10.21437/Interspeech.2023-1094. Changhan Wang, Morgane Riviere, Ann Lee, Anne Wu, Chaitanya Talnikar, Daniel Haziza, MaryWilliamson, Juan Pino, and Emmanuel Dupoux. Voxpopuli: A large-scale multilingual speechcorpus for representation learning, semi-supervised learning and interpretation. arXiv preprintarXiv:2101.00390, 2021. Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki Hayashi, Jiro Nishitoba, Yuya Unno, NelsonEnrique Yalta Soplin, Jahn Heymann, Matthew Wiesner, Nanxin Chen, et al. Espnet: End-to-endspeech processing toolkit. arXiv preprint arXiv:1804.00015, 2018. Shu-wen Yang, Po-Han Chi, Yung-Sung Chuang, Cheng-I Jeff Lai, Kushal Lakhotia, Yist Y Lin,Andy T Liu, Jiatong Shi, Xuankai Chang, Guan-Ting Lin, et al. Superb: Speech processinguniversal performance benchmark. arXiv preprint arXiv:2105.01051, 2021.",
  "A.1Decoder Models for ASR, LID, and SV": "The ASR, LID, and SV decoder models were optimized for their respective tasks through carefulselection of hyper-parameters and architectural configurations. The ASR model directly follows thesetting in ML-SUPERB benchmark [Shi et al., 2023a] for comparison. summarizes theseconfigurations. The full means one epoch is trained by passing all the training data.",
  "A.2CA-SSLR Hierarchical Models": "provides detailed configurations for the CA-SSLR model. In the CA-SSLRL,S setup, two 256-dimensional embeddings are used to encapsulate language (L) and speaker (S) information, whichthen determine the parameters (L, L, L) and (S, S, S) following the procedure outlined in Eq. 5.The training adopts a stepwise approach, using initial parameters from an earlier phase to set up thenext. The pretrained ASR, LID, and SV decoders serve as the foundation for initializing CA-SSLRL;the SV decoder is fine-tuned further on top of CA-SSLRL; and both CA-SSLRL and fine-tuned SVdecoder initialize CA-SSLRL,S. In the tables Trainable modules row, the notations LIDFeat and SVFeat indicate that the feature projection layers of the LID and SV decoders are adjustable duringthe training process. We conduct the in and multiple times, and the variation are allwithin 0.2% CERs range.",
  "A.3.1ML-SUPERB Dataset": "The ML-SUPERB dataset is assembled from a wide collection of multilingual speech corpora, witheach contributing corpus being governed by one of a variety of open-source licenses, such as CreativeCommons, MIT, GNU, or Free-BSD. These licensing agreements guarantee that the dataset is openlyavailable and can be used freely for both commercial and scholarly research purposes. The 10-minutetraining set encompasses 37.4 hours of data, and the 1-hour dataset increases the total to 222.4 hoursof data. Additionally, the dataset includes development and testing sets, containing 41.8 hours and45.0 hours of data, respectively. This dataset is designed for multilingual speech recognition andlanguage identification, as used in our work.",
  "A.3.2VoxCeleb Dataset": "The VoxCeleb dataset is available under the Creative Commons Attribution 4.0 International licenseand encompasses comprehensive training, development, and testing data collection. Specifically, itcontains 1092 hours of audio from 5,994 speakers for training, 110 hours from 4,933 speakers fordevelopment, and 20 hours from 40 speakers designated for testing. Designed to facilitate speakerverification and identification tasks, aligns with our usage in the speaker verification task. To ensureprivacy, speaker names within the dataset are anonymized and represented through unique speakerIDs.",
  "BCER vs. Trainable Parameters": "compares the mHuBERT models ASR performance against the number of trainable parame-ters, where the XLSR counterpart is shown in . Both CA-mHubertLdual and CA-mHubertL,Sdual arewith 256 condition feature dimensions. Notably, the CA-mHubertLdual model excels in few-shots sce-narios, while the CA-mHubertL,Sdual yields CERs for normal languages comparable to a fully fine-tuned12-layer mHuBERT model using only 15.9M parameters. This efficiency demonstrates the TCAconditioners capability in the CA-SSLR framework to deliver fine-tuned levels of ASR accuracy witha significantly reduced parameter count, providing an optimal balance for practical ASR applications.",
  "CTraining Efficiency and Resource Usage": "We compare the training speed and resource consumption of different adaptation methods, includingHoulsby Adapters, CA-SSLR, and full fine-tuning (FT). summarizes the bottleneck dimen-sions, training times, and peak memory usage for each method. We evaluate the training speechfor 10k iterations with batch size 8. We found that the CA-SSLR approach ranks second comparedto the Houlsby Adapter and a fully fine-tuning approach in speed and memory usage. However, itis important to note that CA-SSLR surpasses the Houlsby Adapter in adaptation effectiveness andgeneralization ability, as demonstrated in . These results indicate that although CA-SSLRincurs a moderate increase in training resources, it provides benefits in performance and generaliza-tion. We acknowledge that the current implementation of CA-SSLR is not yet optimized for speedand memory efficiency. Future work will focus on optimizing the model to reduce training time andmemory consumption without compromising performance.",
  "EFew-shots Results": "Within the ML-SUPERB datasets 20 few-shots languages, we examined the performance of CA-SSLR against the established SSL baselines, XLSR and mHuBERT, on models trained in the10-minute ML-SUPERB set. The LID results indicate a close match between CA-SSLR and thebaseline, with approximately half of the few-shot languages exhibiting improvements or matchingtheir baseline performance. .4 reveals that SSL-based LID models are inherently effective,and extending full fine-tuning does not necessarily enhance results. This observation aligns with theoutcomes of other classification tasks adeptly handled by SSL models, as documented in [Chen et al.,2023b]. Furthermore, the CA-SSLR framework demonstrates subtle enhancements for the Normal",
  "languages in the 10-minute set in , indicating that the LID performance remains robust despitethe encoders additional modifications": "Regarding the ASR results, most languages achieved significant CER reductions, ranging froma modest few percent to over 30%, when compared with SSL baselines. Notably, the Bosnian(bos) language experienced an impressive 45.1% relative improvement in CER, while Cebuano(ceb) improved by 39.5% with the XLSR model. With the mHuBERT model, the most substantialgains were observed in Sundanese (sun) and Toki Pona (took), with 19.9% and 17.2% CER relativeimprovements, respectively. These results underscore the CA-SSLR frameworks profound effectin bolstering ASR performance for few-shot languages, especially demonstrating more pronouncedimprovements with the XLSR model. When examining the correlation between LID accuracy and ASR performance, it is apparent thata lower CER does not necessarily align with high LID accuracy. For instance, Serbian (srp) onthe XLSR model, despite having a modest LID accuracy of 50.9%, shows a CER improvementfrom 57.4% to 48.1%. Conversely, Fulah (ful), the sole language to exhibit a CER increase inthe XLSR model, presents a higher LID accuracy of 67.5%. This indicates that the CA-SSLRframeworks efficacy is not solely contingent on high LID prediction accuracy. CA-SSLRs relianceon embeddings instead of one-hot hard labels for predictions enables the model to maintain orimprove ASR performance despite suboptimal LID scores. This approach allows the model to utilizeembeddings to distinguish between easily confused languages, enabling the ASR model to predictthe correct language accurately.",
  "FDecode Examples": "visualizes ASR outcomes for the XLSR and CA-SSLRL,S models on the ML-SUPERB10-minute dataset, covering both few-shot and standard language scenarios. It highlights CA-SSLRssuperior language recognition capabilities and success in rectifying the misclassifications encountered : Evaluation of LID and ASR performance in terms of Accuracy (Acc) and Character ErrorRates (CERs) for few-shot learning in low-resource languages using the ML-SUPERB 10-minute set,comparing XLSR and mHuBERT models.",
  "Acc CER Acc CER Acc CER Acc CER": "bos82.021.370.011.730.029.028.026.0ceb97.620.597.612.492.927.297.625.4dan89.544.776.537.080.149.180.447.5epo81.715.376.914.546.224.852.925.4frr87.533.689.329.967.940.463.437.7ful55.027.167.528.237.534.762.532.0kaz98.032.699.321.591.437.888.736.0kea84.128.990.927.665.935.275.033.1lit87.352.087.745.279.352.482.549.3luo100.029.495.124.492.729.492.730.1srp64.857.450.948.153.556.745.756.2sun93.526.494.419.194.432.793.526.2tok98.515.498.513.198.523.298.519.2tos100.049.199.444.799.453.199.448.9tso84.025.481.321.783.329.481.326.0tsn87.122.785.717.383.627.384.323.9tur82.460.079.137.062.665.157.761.7umb64.024.640.023.344.029.836.130.1vie94.788.492.983.185.880.174.280.3zul60.620.462.314.453.724.352.020.4 with XLSR, often resulting in completely incorrect transcriptions. This is evident in languagessuch as Lithuanian and Turkish, categorized as few-shot, and Bulgarian, which is better resourced(normal). These findings demonstrate the TCA conditioners effectiveness in accurately managingLID embedding features and distinguishing between languages for downstream tasks. Moreover, the results from other samples suggest that CA-SSLR can achieve better outcomes duringtraining due to its incorporation of language information, even when the XLSR model correctlypredicts the language. This underscores the efficacy of the TCA conditioner in exploiting language-specific data, thereby enabling CA-SSLR to achieve heightened accuracy across a diverse range oflanguages.",
  "GEthical Statement": "We affirm our commitment to ethical research practices, including respect for privacy and the re-sponsible use of data. The proposed CA-SSLR model improves multi-lingual ASR in 143 languages,including 20 low-resource ones with just five training utterances. In this manner, CA-SSLR con-tributes to the democratization of speech technology, fostering inclusivity for previously underservedcommunities. Furthermore, CA-SSLR prioritizes the reduction of computational costs at evaluationtime, thereby aiming to mitigate the environmental impact associated with speech applications. Weutilized publicly available datasets, namely ML-SUPERB and VoxCeleb, chosen for their moderatesize to minimize computing requirements. Our utilization of pre-trained models, specifically XLSRand mHuBERT, aligns with their intended research purposes, as they are widely used within thespeech research community. However, the capacity for conducting speech and speaker recognition in human conversations poses anotable ethical concern linked to covert eavesdropping by nefarious entities. This capability could beexploited by authoritarian government bodies seeking to suppress free speech and identify dissidents.Therefore, it is imperative to promote public awareness and comprehension regarding the automatedanalysis of spontaneous speech and its ramifications."
}