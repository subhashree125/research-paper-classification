{
  "Abstract": "Since the inception of the classicalist vs. connectionist debate, it has been arguedthat the ability to systematically combine symbol-like entities into compositionalrepresentations is crucial for human intelligence. In connectionist systems, thefield of disentanglement has emerged to address this need by producing repre-sentations with explicitly separated factors of variation (FoV). By treating theoverall representation as a string-like concatenation of the inferred FoVs, however,disentanglement provides a fundamentally symbolic treatment of compositionalstructure, one inherently at odds with the underlying continuity of deep learningvector spaces. We hypothesise that this symbolic-continuous mismatch producesbroadly suboptimal performance in deep learning models that learn or use suchrepresentations. To fully align compositional representations with continuousvector spaces, we extend Smolenskys Tensor Product Representation (TPR) andpropose a new type of inherently continuous compositional representation, SoftTPR, along with a theoretically-principled architecture, Soft TPR Autoencoder,designed specifically for learning Soft TPRs. In the visual representation learningdomain, our Soft TPR confers broad benefits over symbolic compositional repre-sentations: state-of-the-art disentanglement and improved representation learnerconvergence, along with enhanced sample efficiency and superior low-sampleregime performance for downstream models, empirically affirming the value of ourinherently continuous compositional representation learning framework.",
  "Introduction": "Compositional structure, capturing the property of being decomposable into a set of constituentparts, is ubiquitous in our surroundings from the recursive application of syntax in language,to the parsing of richly complex visual scenes into their constituent parts. Given the central rolesuch structure plays in our understanding of the world, it is highly intuitive that deep learningrepresentations also embody compositional structure. Indeed, empirical evidence highlights theusefulness of explicitly compositional representations, showcasing a multitude of benefits, includingincreased interpretability , reduced sample complexity , increased fairness [ 20 , 25 ,",
  "], and improved performance in out-of-distribution generalisation": "We consider the following, intuitive notion of compositional representations.A represen-tation of compositionally-structured data is a compositional representation if it has a struc-ture that faithfully reflects the compositional structure of the represented data .Inthe visual representation learning domain, data is clearly compositionally-structured, asimages can be decomposed into a set of constituent factors of variation (FoVs), e.g.,{magenta floor,orange wall, aqua object colour, oblong object shape} for the image in .",
  "arXiv:2412.04671v1 [cs.LG] 5 Dec 2024": "A widely explored representation learning framework producing explicitly compositional represen-tations is that of disentanglement. We adopt the conventional , intuitive definitionof a disentangled representation, which states that a representation, (x), is disentangled if eachof the underlying FoVs can be cleanly separated into a distinct dimension (or contiguous subset ofdimensions) of (x), or, in other words, if each FoV has a 1-1 correspondence with a distinct partof the representation . Framed in this way, it is apparent that disentangled representations areexplicitly compositional by nature. The majority of state-of-the-art disentanglement approaches use avariational autoencoder backbone, and rely on weak supervision , or a penalisationof the aggregate posteriorq(z|x)p(x)dx to promote disentanglement. Morerecent approaches depart from the restrictive assumptions of a variational framework, and instead usestandard autoencoding , or energy-function based optimisation , with additional inductivebiases to encourage disentanglement. Despite the diversity of methods characterising existing work,we make a crucial observation which unifies them together: by enforcing the 1-1 correspondencebetween FoVs and distinct parts of the representation, existing approaches [ 10 , 13 , 14 , 17 , 22 , 24 , 26 , 31 , 32 , 33 , 35 , 36 , 47 ] essentially produce compositional representations corresponding to a concate-nation of scalar-valued or vector-valued FoV tokens, as illustrated in a. This concatenativeapproach enforces a rigid, slot-based representational structure that constraints how information canbe represented and combined, and mirrors symbolic systems, where distinct symbols occupy discreteslots within a representation. We argue that this fundamentally symbolic approach creates a deepincompatibility with the inherent continuity of the vector spaces underlying deep learning for thefollowing reasons (see A.3 for further details): 1. Misalignment with Gradient-Based Learning: Gradient-based optimisation relies onsmooth, continuous transformations to propagate gradients effectively. However, symboliccompositional representations, which allocate FoVs to discrete, non-overlapping repre-sentational slots impose rigid, non-differentiable boundaries at the edges of these slots,fundamentally disrupting gradient flow. More concretely, this slot-based structure promotesa categorical, non-differentiable confinement of gradients to the dimensions associated withspecific slots (e.g., when modifying an individual FoV), fragmenting the smooth flow ofgradients across all dimensions of the representational space. Furthermore, such a slot-basedstructure induces abrupt, discontinuous shifts in the representational space when transition-ing between FoV updates, destabilising learning dynamics and potentially complicatingconvergence. 2. Restrictive / Incompatible Structure: By allocating distinct representational slots for eachFoV, the symbolic approach imposes a rigid representational structure that prevents therepresentation from exploiting the expressivity inherent in continuous vector spaces. Moreconcretely, this slot-based framework prevents each FoV given only a subset of dimensionsof the entire representational space from being encoded as flexible combinations ofbasis vectors spanning the entire representational space an approach that is not onlymore intuitive, but also critical for capturing rich interactions and complex dependenciesamong FoVs. By failing to allow for this flexibility, the symbolic approach prevents therepresentation from leveraging the full expressivity of its underlying vector space. Critically, we hypothesise that the fundamental incompatibility between the symbolic treatment ofcompositional structure provided by disentanglement, and the continuous vector spaces of deeplearning produces suboptimal behaviour in the models that learn or use these representations. Thishypothesis prompts the following question: can we instead represent compositional structure inan inherently continuous manner? A continuous compositional representation would yield therepresentation, (x), by continuously combining the FoVs in the same underlying vector space,rather than maintaining a discrete, slot-based separation, as in the symbolic approach. The continuousapproach to representing compositional structure is thus, arguably a more mathematically intuitiveframework in the context of deep learning. Pioneered by Smolensky, the Tensor Product Representation is a specific representational formthat encodes compositional structure in an inherently continuous manner. At the crux of it, TPRsare formed by continuously blending the FoVs together into the overall representation, in a manneranalogous to superimposing multiple waves together to produce a complex waveform, as illustrated in b. For a representation to qualify as a TPR, it must adhere to a highly specific mathematicalform, which confers upon the TPR valuable theoretical properties (elaborated on in .2 ), butalso imposes two major limitations (see B.1 for further details). First, as depicted by the stars in Figure",
  "c)": ": (a) Disentangled representations can be conceptualised as a concatenation of FoV tokens (colouredblocks), effectively enforcing a string-like, symbolic compositional structure, where each FoV is allocated to adiscrete slot in the representation. We instead, consider a continuous representation of compositional structure,(b), where the FoVs (first 6 waves) are continuously superimposed together to produce the overall representation,(x) (in red). (c) Only a subset of points (stars) in the underlying representational space (rainbow manifold)satisfy the TPR specification. The Soft TPR relaxes this, capturing larger, continuous regions of the underlyingrepresentational space (the translucent circles), while preserving the TPRs key properties. 1 c, only a discrete subset of points in the underlying representational space, V , satisfies the stringentmathematical criteria to qualify as TPRs. Consequently, to learn TPRs, representation learners mustmap from the data manifold onto this discrete subset, which constitutes a highly constrained andinherently challenging learning task. Second, the TPR specification enforces a strict, algebraicdefinition of compositional structure, limiting the TPRs ability to faithfully represent real-worlddata which is often quasi-compositional, only approximately adhering to a rigid, formal definitionof compositionality. Historically, these limitations have confined TPR learning to formal domainscharacterised by explicit, algebraic structure as evidenced by the near exclusive deployment ofTPRs in language and, to contexts where strong supervision from highlystructured downstream tasks is available to steer the representation learning process [ 23 , 28 , 38 , 51 ]. To negate these drawbacks and extend continuous compositional representations to weaklysupervised, non-formal domains, we propose Soft TPR, a new, inherently continuous compositionalrepresentation that can be thought of as a continuous relaxation of the traditional TPR, as illlustratedby the translucent circular regions in c. At its core, the Soft TPR is designed to promoterepresentational flexibility and ease of learning while simultaneously preserving the structural andmathematical integrity of the traditional TPR. We additionally introduce Soft TPR Autoencoder, atheoretically-principled weakly-supervised architecture for learning Soft TPRs, which we use tooperationalise the Soft TPR framework in the visual representation learning domain. Our main contributions are threefold: i) We propose a novel compositional representation learningframework, introducing the inherently continuous Soft TPR compositional form, alongside a dedicated,weakly-supervised architecture, Soft TPR Autoencoder, for learning this form. ii) Our framework isthe first to learn continuous compositional representations in the non-formal, less explicitly algebraicdomain of vision. iii) We empirically affirm the far-reaching benefits of enhanced vector spacealignment produced by the Soft TPR framework, demonstrating that Soft TPRs achieve state-of-the-art disentanglement, accelerate representation learner convergence, and provide downstream modelswith enhanced sample efficiency and superior low-sample regime performance.",
  "Related Work": "Disentanglement: In aiming to produce explicitly compositional representations without strongsupervision, our work shares the same objective as disentangled representation learning. Priorto the highly influential work of , which proved the impossibility of learning disentangledrepresentations without supervision or other inductive biases, disentangled representations werelearnt in a completely unsupervised fashion . Our use of weak supervisionis inspired by the work relating to this highly influential impossibility result.In particular, we leverage the type of weak supervision termed match pairing , where pairs,(x, x), differing in values for a subset of known FoVs are presented to the model, to incentivisedisentanglement. Our work, however, fundamentally diverges from all disentanglement work weare aware of, by adopting an inherently continuous representation of compositional structure, whichcontrasts with the symbolic representations of compositional structure characterising existing work. TPR-based Work: Existing TPR-based approaches generate continuous representations of compo-sitional structure by producing an element with the explicit mathematical form of a TPR. To learnthis highly specific form, these approaches rely on the algebraic characterisation of compositionalitypresent in formal domains, such as mathematics , or language in addition to strong supervision signals from highly structured downstream tasks, such as part-of-speech tagging, and answering structured language or mathematics questions . In contrast, our SoftTPR eases these stringent constraints by offering a relaxed specification of inherently continuouscompositional structure. This allows our approach to extend continuous representations of composi-tional structure to an orthogonal and less structured domain, that of visual representation learning,while also reducing reliance on annotated data by instead using weak supervision to learn this relaxedrepresentational form.",
  "A Formal Framework for Compositional Representations": "We adopt a generalised, non-generative version of the definition of compositional representationsfrom . Data x X is compositionally-structured if there exists a decomposition function :X A1 . . .An decomposing x into constituent parts, i.e. (x) = {a1, . . . , an}, where ai Ai.A map : X VF produces a compositional representation if (x) = C(1(a1), . . . , n(an))where i : Ai Vi denote component functions that independently embed the parts of x intovector spaces, and C : V1 . . . Vn VF denotes a composition function that combines theembedded parts of x together to form the overall representation. Intuitively, this definition enforces afaithful structural correspondence between the constituency structure of the data, x (i.e., the parts,{a1, . . . , an}) and the constituency structure of the representation, (x) (i.e., the embedded parts,{1(a1), . . . , n(an)}) (assuming C is invertible). We formalise a symbolic compositional representation, s(x), as a compositional representationwhere C is a concatenation operation. Thus, s(x) =1(a1)T , . . . , n(an)T T for any symboliccompositional representation, s(x). Clearly, the aforementioned disentanglement methods [ 8 , 10 , 13 , 14 , 17 , 22 , 24 , 26 , 31 , 32 , 33 , 35 , 36 , 37 , 47 ] all fit this framework. While we observe that thefundamentally symbolic definition of C as concatenation produces an inherent misalignment withthe continuous vector spaces of deep learning for the reasons elaborated on in , it has onenotable benefit: the embedded FoVs, {i(ai)}, can be easily recovered from the representation,s(x), by simply partitioning s(x). It is highly intuitive that for any compositional representation, (x), to be broadly useful, the FoVs,{ai}, should be easily recoverable from (x) (here we assume the is are invertible, and so, that thisproperty corresponds to being able to recover the representational components, {i(ai)} from (x)).We thus explore whether an alternative C exists that simultaneously 1) combines the embeddedFoVs into the overall representation in an inherently continuous manner and 2) preserves the directrecoverability of the embedded FoVs.",
  "The TPR Framework": "TPR is a specific type of representation that is compositional, continuous, and under certainconditions, ensures the direct recoverability of the embedded parts {i(ai)} from the overall rep-resentation. We briefly introduce essential aspects of the framework, deferring further details andformal proofs to Appendix A . The TPR framework views compositionally-structured objects aspossessing a number of (potentially infinite) roles 2 , where each role is bound to a corresponding filler.It thus defines the constituent parts {ai} of any compositionally-structured object as a set of role-fillerbindings. This role-filler binding formalism has predominantly been applied in the natural languagedomain , with fillers often corresponding to words and roles to grammatical categories(e.g., the word cat as a filler, and the the category noun as a role). We translate this formalism intothe domain of visual representation learning by informally equating roles as FoV types, and fillers asFoV values, e.g., {floor colour, wall colour, object colour, object size, object shape, orientation}and {blue, magenta, orange, green, small, medium, large, oblong, cube, . . .} respectively for theShapes3D domain of . The binding of a filler, f, from a set F of NF fillers, to a role,r, from a set R of NR roles, such as the filler magenta to the role object colour conveys a sortof filler-specific, role-modulated semantic content, and is denoted by f/r. The compositional",
  "We assume a finite set of NR roles, an intuitive assumption to make in the context of visual representationlearning": "structure of the image, x, in would thus correspond to the following set of role-filler bind-ings: (x)= {magenta/floor colour, orange/wall colour, aqua/object colour, large/object size,oblong/object shape}. To produce the TPR, the roles and fillers for each binding in x are independently embedded using roleand filler embedding functions, R : R VR, F : F VF respectively. To produce an embeddingof the binding f/r, a tensor product, which we denote by , is then taken over the embedded role,R(r), and embedded filler, F (f), comprising the binding. Finally, a summation is performed overall embedded bindings in x to produce the overall representation. More concretely, the TPR, tpr(x),is defined as:",
  "ui = F (fm(i)),(2)": "where ui is a vector corresponding to the i-th column of U T , the (left) inverse of the matrix formed bytaking all (linearly independent) role embeddings as columns. By repeating the unbinding procedureusing each of the NR unbinding vectors, the embedded fillers bound to each role, and hence, theembeddings (F (fm(i)), R(ri)) comprising each binding embedding, F (fm(i)) R(ri), can berecovered from the overall representation, tpr(x). Thus, provided that the linear independencecondition is satisfied, the TPR represents a continuous compositional representation that retains thekey benefit of symbolic compositional representations: the direct recoverability of the representationalparts, {i(fm(i), ri)}.",
  "The TPRs highly specific representational form, tpr(x) :=": "i F (fm(i)) R(ri), can only besatisfied by a discrete subset of points in the underlying representational space, VF VR. Thisimposes an arduous learning task on representation learners: to parameterise the highly constrainedmap from the data manifold to a discrete subset of points. This representational form additionallyassumes a strict algebraic definition of compositionality that corresponds to a set of bindings, whereeach binding comprises a single role and a single filler, precluding the TPR from representing quasi-compositional objects that only approximately satisfy this strict, algebraic definition of compositionalstructure (e.g., French liaison consonants, where a weighted sum of multiple fillers, rather than asingle filler, bind to a role ). Our primary insight is that both these drawbacks can be mitigatedby continuously relaxing the TPR specification (see B.1 ). This relaxation allows for a wider varietyof mappings within a cloud around each TPR (represented by the translucent circular regions in c), which theoretically eases the difficulty of representation learning. Furthermore, it relaxes",
  "We omit the dependence of m on x for notational clarity": "the rigid role-filler based specification of compositional structure into a softer, less rigid notion,enabling the representation of nuanced, quasi-compositional data. We thus introduce the Soft TPR, acontinuously relaxed, less stringently defined variant of the explicit TPR that simultaneously retainsthe TPRs key properties of 1) continuous compositional structure, and 2) direct recoverability ofrepresentational parts {F (fm(i)), R(ri)} from the overall representation. Consider an element, z, in VF VR, the vector space underlying TPRs produced by an arbitrary roleembedding function R : R VR and an arbitrary filler embedding function F : F VF . If z issufficiently close to some TPR, tpr",
  "= F (fm(i)) + i =: fi, where i = zui F (fm(i)).(4)": "Thus, performing unbinding on an element z in VF VR where the sufficient closeness conditionholds recovers a soft filler embedding, fi, that approximates the true filler embedding, F (fm(i)),of the filler bound to role ri for tpr with approximation error i. We define such elements asSoft TPRs, noting that these elements both 1) softly approximate the continuous compositionalstructure captured by some explicit TPR, tpr, as z is sufficiently close to tpr based on the chosendistance metric, and 2) approximately preserve the recoverability of the representational components{(F (fm(i)), R(ri))} of the explicit TPR, tpr, they approximate, with the only difference being thatsoft filler embeddings, fi are returned in place of the actual filler embeddings, F (fm(i)). DefiningSoft TPRs in this way 1) provides a less restrictive representational specification that can be satisfiedby any arbitrary element from VF VR provided that, for some explicit TPR, tpr, the sufficientcloseness requirement, ||z tpr||F < , holds, and 2) allows learned representations to embody amore flexible, relaxed notion of compositional structure.",
  "Soft TPR Autoencoder: A Concrete Implementation of Learning Soft TPRs": "We define our vector spaces of interest over the reals as VF := RDF and VR := RDR where DF , DRdenote the dimensionality of the filler and role embedding spaces. The main insight underlying ourmethod is that, as the Soft TPR is effectively any arbitrary element from a vector space 5 RDF DRthat is sufficiently close to some explicit TPR, any (DF DR)-dimensional vector produced by anencoder in a standard autoencoding framework can be treated as a Soft TPR candidate. This suggeststhat a simple autoencoding framework only needs to be slightly modified to produce Soft TPRs.Briefly speaking, our Soft TPR Autoencoder contains a standard encoder, E, the TPR decoder, and astandard decoder, D, where the encoder output, z, corresponds to the Soft TPR. At a high level, ourframework aims to ensure two properties: 1) representational form, and 2) representational content.Representational form requires that the encoder output, z, has the desired Soft TPR form (i.e. that||z tpr||F < for some TPR, tpr). However, having a Soft TPR form alone is insufficient; therepresentation produced by the autoencoder must also reflect the true role-filler content of the datato be a good representation, as required by the aim of representational content. These 2 propertiesare (mostly) respectively achieved using the unsupervised and weakly supervised components of ourmethod. Representational Form: to encourage the autoencoder to produce elements that are Soft TPRs, wepenalise the Euclidean distance ||z tpr||2 between the encoder output, z, and the explicit TPR,tpr, that z best approximates. To obtain tpr, needed to penalise the above distance, we derive anexplicit analytical form for tpr and construct elements satisfying this analytical form using a roleembedding matrix, MR containing NR DR-dimensional role embedding vectors, {R(ri)}, and afiller embedding matrix, MF containing NF DF -dimensional filler embedding vectors, {F (fi)}.To define an explicit analytical expression for tpr, we are guided by the intuition that the TPR that z 4We occasionally omit the dependence on x for notational clarity.5Due to isomorphism of vector spaces RDF RDR = RDF DR, we henceforth use vectors from RDF DRin place of rank-2 tensors from RDF RDR, and the Euclidean norm instead of the Frobenius norm to align theSoft TPR framework more seamlessly with the autoencoding framework.",
  "Filler embedding matrixRole embedding matrix": ": Diagram illustrating the Soft TPR Autoencoder. We encourage the encoder Es output, z, to have theform of a Soft TPR by penalising its distance with the greedily defined, explicit TPR, tpr of Equation 5 that zbest approximates. tpr is recovered using a 3 step process performed by our TPR decoder (center rectangle): 1)unbinding, 2) quantisation, and 3) TPR construction. The decoder, D, reconstructs the input image using tpr.",
  "iF (fm(i)) R(ri), where m(i) := arg minj|| fk F (fj)||2, and fk := zui.(5)": "That is, we define tpr as the TPR constructed from explicit filler embeddings F (fj) with thesmallest Euclidean distance to the soft filler embeddings fk of z. To construct elements satisfying (5),we use a 3-step process carried out by the novel TPR decoder we introduce, visible in : 1)Unbinding: The unbinding module consists of a fixed semi-orthogonal role embedding matrix MRand recovers the soft filler embeddings { fi} associated with each encoder output, z, by performingthe TPR unbinding operation. We elaborate on our theoretically-informed reason for this choiceof role embedding matrix, how the unbinding vectors are obtained, and for not backpropagatinggradient to MR in B.3.1 . 2) Quantisation: The quantisation module, containing a learnable fillerembedding matrix MF , employs the VQ-VAE vector quantisation algorithm to both 1) learnthe explicit filler embeddings, and 2) quantise the soft filler embeddings { f1, . . . , fNR} produced bythe unbinding module into the explicit filler embeddings {F (f1), . . . , F (fNR)} with the smallestEuclidean distances. 3) TPR Construction: The TPR construction module recovers tpr by simplyperforming the TPR operation i F (fm(i))R(ri) over the fixed, semi-orthogonal role embeddingvectors with the corresponding explicit filler embeddings produced by the quantisation module. Toensure the quantised filler embeddings {F (fm(i))} depend explicitly on the reconstructed image,we pass the output of the TPR decoder, tpr to the image decoder, D, for reconstruction. The overallunsupervised loss, Lu, is thus given by the following, where s denotes the stop-gradient operator, ahyperparameter, and Lr any suitable image-based reconstruction loss (we use L2):",
  "(6)": "Representational Content: While the unsupervised loss Lu encourages the autoencoder to produceencodings z with the desired Soft TPR form, it may not ensure that the content of z accuratelyreflects the true role-filler semantics of the represented data. To address this, we introduce a weaklysupervised loss to ensure that the explicit TPR, ztpr, which z best approximates, reflects the ground-truth semantics of the image. We employ a match-pairing context similar to , whereimage pairs (x, x) share the same role-filler bindings for all but one of the roles, ri, and the identityof ri is known, but not any of the fillers, or role-filler bindings. Our intuition is that, for the role-fillerbinding embeddings of z to reflect the semantics of the represented image, the Euclidean distancebetween the quantised fillers of x and x bound to role ri should be maximal, relative to the distancesbetween the pairs of filler embeddings for all other roles rj, j = i. To encourage this, we apply thecross entropy loss corresponding to the 3rd term in Eq 7 , where q denotes the NR-dimensionalvector with each dimension (q)k populated by the Euclidean distance between the quantised fillers of x and x for role rk, and l denotes the one-hot vector of dimension NR with the index for ri setto 1. Additionally, we apply a reconstruction loss using the TPRs, stpr(x) and stpr(x), which areconstructed by swapping the quantised filler embeddings of x and x bound to role ri, to reconstructx and x respectively.",
  "Results": "To assess the compositional representations produced by our Soft TPR framework, we performevaluation along three dimensions: 1) Compositional Structure / Disentanglement: What is thedegree to which Soft TPR representations achieve explicitly compositional structure? 2) Represen-tation Learner Convergence Rate: Can representation learners learn the inherently continuouscompositional structure embodied in the Soft TPR faster than symbolic alternatives? 3) DownstreamModels: Does the enhanced vector space alignment produced by the Soft TPR facilitate benefits fordownstream models using compositional representations? We benchmark against a suite of weakly supervised disentanglement baselines: Ada-GVAE ,GVAE , ML-VAE , SlowVAE , and the GAN-based model of , which we henceforthrefer to as Shu. These models all produce symbolic compositional representations corresponding toa concatenation of scalar-valued FoV tokens. Like our model, Ada-GVAE, GVAE, ML-VAE, andShu are trained with paired samples (x, x) sharing values for all but a subset of FoVs types (roles), I.ML-VAE, GVAE, and Shu assume access to I, the FoV types (roles) that differ between x and x,matching our models level of supervision, while Ada-GVAE does not. We thus modify Ada-GVAE(method detailed in Appendix C.2.2 ) for more direct comparability, denoting our modification byAda-GVAE-k. In contrast, SlowVAE is trained with pairs of samples where all underlying FoV valueschange, and also assumes that this change can be characterised as a sample from a Laplacian. Weadditionally benchmark against 2 baselines producing vector-tokened compositional representations:COMET , and Visual Concept Tokeniser (VCT) . These vector-tokened models cannotbe directly compared to our model as they are fully unsupervised, however, we include them forcompleteness. We train 5 instances of each representation learning model using 5 random seeds for200,000 iterations across all datasets, and report results averaged over the 5 random runs.",
  "Compositional Structure / Disentanglement": "To evaluate the degree to which Soft TPRs achieve explicitly compositional structure, we quantifyrepresentational disentanglement using standard disentanglement datasets Cars3D , MPI3D ,and Shapes3D (see C.4.1 for further details). As can be seen in , our model achievesstate-of-the-art disentanglement for all datasets, with notable DCI metric increases of 29% and 74%on the 2 more challenging datasets of Cars3D and MPI3D respectively. To rule out the possibilitythat the improvement in disentanglement produced by our model is due to a slight increase of 13,568(Cars3D), 1,824 (Shapes3D), 1,600 (MPI3D) learnable parameters produced by the addition of thefiller embedding matrix, MF , to a standard (variational) encoder-decoder framework, we modifymodels with fewer parameters to have an identical number of parameters as ours. We note thatthe modifications are applicable only for the scalar-tokened baselines, as our model has tens ofmillions less parameters than COMET and VCT. In line with , we observe that the performanceof scalar-tokened disentanglement models remains fairly consistent, or even deteriorates, when thenumber of learnable parameters increases, so we defer control experiment results, and our associatedmethod to Appendix C.2.4 . Key Implication: The Soft TPRs superior level of explicit compositionality (as quantified by the dis-entanglement metrics) in a controlled, parameter-count environment, suggests that its fundamentallycontinuous representational form is potentially easier for deep learning models to learn compared tosymbolic representational forms characterising existing disentanglement work.",
  "Representation Learner Convergence Rate": "To evaluate whether the inherently continuous compositional form of the Soft TPR can be learnedmore quickly than symbolic alternatives, we consider representations produced at 100, 1,000, 10,000,100,000 and 200,000 iterations of training, and evaluate 1) their disentanglement, and 2) their utility,as quantified by the performance of downstream models using these representations. For downstreammodel performance, we consider two commonly used tasks: the classification-basedabstract visual reasoning task of and a regression task involving the prediction of continuousFoV values for the disentanglement datasets. Downstream models are evaluated on a fixed, held-outtest set for both tasks. While our framework does not promote faster disentanglement convergence(results in Appendix C.4.1 ), it interestingly, promotes accelerated learning of useful representationsfor both downstream tasks compared to baselines. The downstream performance improvements areparticularly pronounced in the low iteration regime of 100 iterations of representation learner training,as demonstrated by the improvements of 10%, 10%, and 31% in and the 27% improvementin . To ensure fair comparison, we embed baseline representations of both higher, and lowerdimensionality into the same space as our model. For each baseline model, we take the best resultfrom either the original, or the modified model (denoted by ), and present the full suite of results,and details of our embedding method and downstream model setup in Appendix C . Key Implication: The representation learning convergence results suggests that while our modelmay not learn the explicit compositional structure captured by disentanglement metrics more quicklythan baselines (though in the limit, the greatest possible disentanglement is higher), it learns usefulinformation for downstream tasks more quickly than baselines, where that useful information isencoded in the relaxed compositional structure of the Soft TPR.",
  "Downstream Models": "To evaluate whether the enhanced alignment between compositional representations and continuousvector spaces produced by our Soft TPR benefits downstream models, we examine downstream1) sample efficiency, and 2) raw performance in the low sample regime. We use the previouslymentioned tasks of abstract visual reasoning and FoV regression. To quantify sample efficiency, inline with , we use a ratio-based metric obtained by dividing the performance of the downstreammodel when trained using a restricted number of samples (100, 250, 500, 1,000 and 10,000 samples),by its performance when trained using all samples (between 19,104-1,036,800 samples depending onthe task). As illustrated in , our model has superior sample efficiencies compared to baselines, especially in the most restrictive case where downstream models have access to only 100 samplesproduced by representation learners, achieving a 93% improvement. The Soft TPR representationsproduced by our model additionally produce substantial raw performance increases in the low sampleregime, as evidenced in , where its performance in the low-sample regimes of 100 and 200samples constitutes a respective 138% and 168% improvement, and in , a 30% improvement.",
  "- Weak supervision0.225 0.034- Explicit filler dependency0.718 0.051- Semi orthogonality0.756 0.039Full0.828 0.015": "Key Implication: The consistent performance improvement we observe in generic downstreammodels with no a priori knowledge of the Soft TPRs representational form, suggests the relaxed, in-herently continuous representation of compositional structure embodied by our Soft TPR can be moreefficiently leveraged by downstream models compared to symbolic compositional representations,benefiting both sample efficiency, and raw performance in the low sample regime.",
  "We additionally repeat our full suite of experiments using the explicit TPR, tpr, produced by theTPR decoder, in place of our Soft TPR, z. These experiments, a subset of which is presented in Table": "6 , empirically demonstrate that the Soft TPRs continuously relaxed specification of compositionalstructure confers exclusive benefits for both the representation learner and the downstream modelsnot captured by the traditional TPR (see Appendix C.6.1 ). Note for MPI3D, the explicit TPR has alower raw R2 score when fully trained (0.785 0.019 vs 0.882 0.016), contributing to its highersample efficiency in . We additionally examine the importance of the following properties ofour model in producing explicitly compositional Soft TPR representations: 1) the presence of weaksupervision, by setting 1 = 2 = 0 in Equation 7 , 2) the explicit dependency between the quantisedfiller embeddings and the decoder output, by instead using the Soft TPR to reconstruct the inputimage, and 3) the semi-orthogonality of the role embedding matrix, MR by removing this constraintin the random initialisation of MR, with the results of these ablations illustrated in .",
  "Conclusion": "In this work, we address a longstanding issue in the connectionist approach to compositionality: thefundamental mismatch between disentangled representations and the inherently continuous nature ofdeep learning vector spaces. To overcome this, we introduce Soft TPR, a novel, inherently continu-ous compositional representational form that extends Smolenskys Tensor Product Representation,together with the Soft TPR Autocoder, a theoretically-principled architecture designed for learningSoft TPRs. Our flexible, continuous framework yields substantial improvements in the visual domain,enhancing compositional structure, accelerating convergence in representation learners, and boostingefficiency in downstream models. These wide-ranging empirical benefits underscore the importanceof rethinking compositional representations to honour deep learnings continuous foundations. Futurework will extend our continuous framework to hierarchical forms of compositionality, enabling boundfillers themselves to decompose into role-filler bindings for enhanced representational expressivity.",
  "Paul Smolensky and Matthew A. Goldrick. Gradient Symbolic Representations in Grammar: The caseof French Liaison. In: 2016. URL:": "Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, ShakirMohamed, and Alexander Lerchner. beta-VAE: Learning Basic Visual Concepts with a ConstrainedVariational Framework. In: International Conference on Learning Representations. 2017. Aaron van den Oord, Oriol Vinyals, and koray kavukcuoglu koray. Neural Discrete RepresentationLearning. In: Advances in Neural Information Processing Systems. Ed. by I. Guyon, U. Von Luxburg,S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett. Vol. 30. Curran Associates, Inc.,2017. Tameem Adel, Zoubin Ghahramani, and Adrian Weller. Discovering Interpretable Representations forBoth Deep Generative and Discriminative Models. In: Proceedings of the 35th International Conferenceon Machine Learning. Vol. 80. Proceedings of Machine Learning Research. PMLR, 2018, pp. 5059. Diane Bouchacourt, Ryota Tomioka, and Sebastian Nowozin. Multi-Level Variational Autoencoder:Learning Disentangled Representations From Grouped Observations. In: Proceedings of the AAAIConference on Artificial Intelligence 32.1 (2018).",
  "Adam Santoro, Felix Hill, David Barrett, Ari Morcos, and Timothy Lillicrap. Measuring abstractreasoning in neural networks. In: International conference on machine learning. 2018, pp. 44774486": "Kezhen Chen, Qiuyuan Huang, Hamid Palangi, Paul Smolensky, Kenneth D. Forbus, and Jianfeng Gao.Natural- to formal-language generation using Tensor Product Representations. In: CoRR abs/1910.02339(2019). arXiv: 1910.02339 . URL: . Elliot Creager, David Madras, Joern-Henrik Jacobsen, Marissa Weis, Kevin Swersky, Toniann Pitassi,and Richard Zemel. Flexibly Fair Representation Learning by Disentanglement. In: Proceedings of the36th International Conference on Machine Learning. 2019. Muhammad Waleed Gondal, Manuel Wthrich, undefinedor de Miladinovic, Francesco Locatello, MartinBreidt, Valentin Volchkov, Joel Akpo, Olivier Bachem, Bernhard Schlkopf, and Stefan Bauer. Onthe transfer of inductive bias from simulation to the real world: a new disentanglement dataset. In:Proceedings of the 33rd International Conference on Neural Information Processing Systems. 2019. Haruo Hosoya. Group-based learning of disentangled representations with generalizability for novelcontents. In: Proceedings of the 28th International Joint Conference on Artificial Intelligence. 2019,pp. 25062513.",
  "Emile Mathieu, Tom Rainforth, N Siddharth, and Yee Whye Teh. Disentangling Disentanglement inVariational Autoencoders. In: Proceedings of the 36th International Conference on Machine Learning.2019": "Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, TrevorKilleen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang,Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, JunjieBai, and Soumith Chintala. PyTorch: An Imperative Style, High-Performance Deep Learning Library.In: Advances in Neural Information Processing Systems 32. Curran Associates, Inc., 2019, pp. 80248035. URL: .",
  "Imanol Schlag and Jrgen Schmidhuber. Learning to Reason with Third-Order Tensor Products. In:Advances in Neural Processing Information Systems. 2019": "Imanol Schlag, Paul Smolensky, Roland Fernandez, Nebojsa Jojic, Jrgen Schmidhuber, and JianfengGao. Enhancing the Transformer with Explicit Relational Encoding for Math Problem Solving. In:ArXiv abs/1910.06611 (2019). URL: . Sjoerd van Steenkiste, Francesco Locatello, Jrgen Schmidhuber, and Olivier Bachem. Are disentan-gled representations helpful for abstract visual reasoning? In: Proceedings of the 33rd InternationalConference on Neural Information Processing Systems. 2019.",
  "Junxiang Chen and Kayhan Batmanghelich. Weakly Supervised Disentanglement by Pairwise Sim-ilarities. In: Proceedings of the AAAI Conference on Artificial Intelligence 34.04 (2020), pp. 34953502": "Zheng Ding, Yifan Xu, Weijian Xu, Gaurav Parmar, Yang Yang, Max Welling, and Zhuowen Tu. GuidedVariational Autoencoder for Disentanglement Learning. In: Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition (CVPR). 2020. F. Locatello, B. Poole, G. Rtsch, B. Schlkopf, O. Bachem, and M. Tschannen. Weakly-SupervisedDisentanglement Without Compromises. In: Proceedings of the 37th International Conference onMachine Learning (ICML). Vol. 119. Proceedings of Machine Learning Research. PMLR, 2020, pp. 63486359. R. Thomas McCoy, Tal Linzen, Ewan Dunbar, and Paul Smolensky. Tensor Product DecompositionNetworks: Uncovering Representations of Structure Learned by Neural Networks. In: Proceedingsof the Society for Computation in Linguistics 2020. Association for Computational Linguistics, 2020,pp. 277278. URL: .",
  "Yilun Du, Shuang Li, Yash Sharma, B. Joshua Tenenbaum, and Igor Mordatch. Unsupervised Learningof Compositional Energy Concepts. In: Advances in Neural Information Processing Systems. 2021": "Insu Jeon, Wonkwang Lee, Myeongjang Pyeon, and Gunhee Kim. IB-GAN: Disentangled Representa-tion Learning with Information Bottleneck Generative Adversarial Networks. In: Proceedings of theAAAI Conference on Artificial Intelligence 35.9 (May 2021), pp. 79267934. DOI: 10.1609/aaai.v35i9.16967 . URL: . Yichen Jiang, Asli Celikyilmaz, Paul Smolensky, Paul Soulos, Sudha Rao, Hamid Palangi, RolandFernandez, Caitlin Smith, Mohit Bansal, and Jianfeng Gao. Enriching Transformers with StructuredTensor-Product Representations for Abstractive Summarization. In: Proceedings of the 2021 Conferenceof the North American Chapter of the Association for Computational Linguistics: Human LanguageTechnologies. Online: Association for Computational Linguistics, 2021, pp. 47804793. DOI: 10.18653/v1/2021.naacl-main.381 . URL: . David A. Klindt, Lukas Schott, Yash Sharma, Ivan Ustyuzhaninov, Wieland Brendel, Matthias Bethge,and Dylan Paiton. Towards Nonlinear Disentanglement in Natural Data with Temporal Sparse Coding.In: International Conference on Learning Representations. 2021. URL: . Milton Llera Montero, Casimir JH Ludwig, Rui Ponte Costa, Gaurav Malhotra, and Jeffrey Bowers. Therole of Disentanglement in Generalisation. In: International Conference on Learning Representations.2021. URL: . Sungho Park, Sunhee Hwang, Dohyung Kim, and Hyeran Byun. Learning Disentangled Representationfor Fair Facial Attribute Classification via Fairness-aware Information Alignment. In: Proceedings ofthe AAAI Conference on Artificial Intelligence 35 (2021), pp. 24032411. DOI: 10.1609/aaai.v35i3.16341 . URL: . L. Schott, J. von Kgelgen, F. Truble, P. Gehler, C. Russell, M. Bethge, B. Schlkopf, F. Locatello, andW. Brendel. Visual Representation Learning Does Not Generalize Strongly Within the Same Domain.In: ICLR 2021 - Workshop on Generalization beyond the training distribution in brains and machines.2021. Frederik Truble, Elliot Creager, Niki Kilbertus, Francesco Locatello, Andrea Dittadi, Anirudh Goyal,Bernhard Schlkopf, and Stefan Bauer. On Disentangled Representations Learned from CorrelatedData. In: Proceedings of the 38th International Conference on Machine Learning. Vol. 139. Proceedingsof Machine Learning Research. 2021, pp. 1040110412. Milton Montero, Jeffrey Bowers, Rui Ponte Costa, Casimir Ludwig, and Gaurav Malhotra. Lost inLatent Space: Examining failures of disentangled models at combinatorial generalisation. In: Advancesin Neural Information Processing Systems. Ed. by S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave,K. Cho, and A. Oh. Vol. 35. Curran Associates, Inc., 2022, pp. 1013610149.",
  "Tao Yang, Yuwang Wang, Yan Lu, and Nanning Zheng. Visual Concepts Tokenization. In: Advances inNeural Information Processing Systems. 2022": "H. Zhang, Y.-F. Zhang, W. Liu, A. Weller, B. Schlkopf, and E. Xing. Towards Principled Disentangle-ment for Domain Generalization. In: Proceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition (CVPR). 2022, pp. 80248034. URL: . Thaddus Wiedemer, Prasanna Mayilvahanan, Matthias Bethge, and Wieland Brendel. CompositionalGeneralization from First Principles. In: Thirty-seventh Conference on Neural Information ProcessingSystems. 2023. URL: . Haoyang Li, Xin Wang, Zeyang Zhang, Haibo Chen, Ziwei Zhang, and Wenwu Zhu. DisentangledGraph Self-supervised Learning for Out-of-Distribution Generalization. In: Forty-first InternationalConference on Machine Learning. 2024. URL: .",
  ": X 2F R; x {(f, r)|f/r},(8)": "where F denotes a set of fillers, and R denotes a set of roles. Note that in contrast to the formaldefinition of we use in .1 , which assumes each x X is decomposable into a set ofn parts, the above decomposition allows objects to be decomposed into a variably-sized set ofrole-filler bindings, with this set corresponding to an element in the powerset of F R. For thevisual representation learning domain we consider, all considered disentanglement datasets clearly have the property of being decomposable into a fixed size set of role-filler bindings, as all images inthese datasets contain the same number of FoV types and each FoV type is bound to a FoV token.Due to this property, we can take as a special subcase of the generalised definition in Equation 8 :",
  ": X A : x {(fm(i), ri)|fm(i)/ri}.(9)": "where m : {1, . . . , NR} {1, . . . , NF } denotes a matching function that associates each role riwith the filler it binds to in (x) (we again drop the dependence of m on x for ease of notation), andA denotes the set of all possible bindings produced by binding a filler to each of the NR roles, withsizeNF +NR1NR(we assume the same filler can bind to multiple roles).",
  "ii(fm(i), ri),": "where ai := (fm(i), ri) (x), i : F R VF VR; (fm(i), ri) F (fm(i)) R(ri), and Cis ordinary vector space addition. Hence, almost trivially, tpr(x) clearly has the required form to bea compositional representation. Now, we prove the recoverability of the embedded components {i(fm(i), ri)} from the TPR,tpr(x), provided that the set of all role embedding vectors, {R(ri)}, are linearly independent.Similar variants of this proof can be found in , . Proof. Assume the set of all role embedding vectors {R(ri)} are linearly independent. Then, therole embedding matrix, MR := (R(r1) . . . R(rNR)) formed by taking the role embedding vectorsas columns, has a left inverse, U, such that:",
  "= F (fm(i))": "Thus, the embedding of the filler, fm(i), bound to each role, ri, can be recovered through use of atensor inner product with the unbinding vector, ui, corresponding to the i-th column of U T . Notethat the representational components of tpr(x), i.e., the embedded bindings, i(fm(i), ri) are fullydetermined by the embedding of the role, R(ri), and filler, F (fm(i)), comprising the binding, asF (fm(i), ri) simply corresponds to their tensor product. Thus, recovering ((fm(i)), R(ri)) foreach binding in (x) corresponds to recovering the representational component, i(fm(i), ri). So,provided the set of role embeddings are linearly independent, and they can be obtained (e.g. througha look-up table of role embeddings), all representational components, i(fm(i), ri) can be directlyrecovered from the overall TPR representation, tpr(x).",
  "A.3Shortcomings of Symbolic Compositional Representations and How TPR Helps": "Here, we provide a more detailed elaboration on the shortcomings of symbolic compositionalrepresentations as outlined in 1 . We additionally illustrate how TPR-based continuous compositionalrepresentations circumvent such limitations through use of a concrete example. We employ the formal definition of a symbolic compositional representation from .1 ,which states a representation, s(x), is a symbolic compositional representation if s(x) =1(a1)T , ..., n(an)T T , i.e., if s(x) corresponds to a concatenation of representational compo-nents, {i(ai)}. For more direct comparison between the symbolic compositional representation, s(x) and theTPR-based compositional representation, tpr(x), we rewrite the TPR-based representation astpr(x) = (a1) + . . . + n(an). Thus, the TPR can be viewed as an ordinary vector sum ofcomponents, { (ai)}, where each component, i corresponds to an embedded role-filler binding(i.e., each component function, i, takes a part, ai, corresponding to a role-filler binding ai :=(fm(i), ri) (x), and embeds it, so we have i : F R VF VR; (f, r) F (f) R(r)). Now, consider the following example: suppose there are 2 FoV types (i.e., roles), colour, shape and2 FoV values (i.e., fillers), purple, square. For the symbolic approach, suppose the representational",
  ". Misalignment with Gradient-Based Learning 6": ": The symbolic compositional representa-tion allocates the FoVs to 2 distinct, independent slots of d(x). This introduces discreteboundaries to be managed for each FoV at the edge of the slots (i.e., in this case, the edgebetween the first two and last two dimensions), potentially complicating gradient-basedoptimisation. A pertinent, potential complication of this symbolic structure is the slot-basedconfinement of gradient, which restricts gradient flow in a categorical, non-differentiablemanner to only those dimensions corresponding to specific slots. For instance, updating theFoV square/shape requires the gradient to be confined to the last two dimensions whileenforcing that no gradient propagates through the first two dimensions (to ensure colourremains unmodified). Such categorical, non-differentiable restrictions greatly constraingradient flow across the underlying representational space. In this case, the resulting gradientupdate can be formulated as follows:",
  ", where a, b R": "In contrast, the continuous, TPR-based compositional representation integrates the FoVscontinuously into the same underlying representational space R4 (i.e., (purple/colour) =(1 1 0 0)T and (square/shape) = (2 3 2 3)T are summed together in R4), eliminating thenecessary existence of non-differentiable, discrete boundaries for each FoV. This enablessmooth gradient flow across all dimensions of the representational space, R4, when updatingthe FoV square/shape:",
  ", where a,b, c, d R": "By promoting smooth gradient flow throughout the entire representational space, thecontinuous TPR-based approach circumvents potential optimisation difficulties associatedwith the discrete, slot-based structure of the symbolic approach. Another potential complication of the symbolic approach is that its slot-based structureinduces a series of abrupt, discontinuous shifts in the representational space when FoVs areconsecutively updated: i.e.,: 6Please note there is a small error in the main Author Rebuttal in L16 under section 1) Incompatibility between disentangled repre-sentations and deep learnings continuous vector spaces, where the tensor product is mistakenly takenover col(purple) sh(square) to produce the representation, instead of computing F (purple) R(col) + F (square) R(sh), but the conclusion remains exactly the same. (Note that col(purple) =F (purple) R(col) and sh(square) = F (square) F (sh) should both be 4-dimensional, not 2-dimensional.",
  ",": "The continuous, TPR-based approach is thus able to avoid abrupt, discontinuous changes inthe representational space. This promotes smoother learning dynamics, facilitating morestable learning convergence. 2. Restrictive / Incompatible Structure: In the example above, both s(x) and tpr(x)belong to R4, however, the FoVs for s(x) are restricted to 2-dimensional elements (i.e.,colour(purple), shape(square) R2). While non-variational, symbolic methods cantheoretically model rich interactions and dependencies between these 2-dimensional FoVs, since they are not bound by loss functions assuming joint independence they, like allsymbolic methods, remain inherently limited in their expressivity. Symbolic methods,by definition, cannot fully exploit the representational capacity of the underlying space(R4 in this case), as they encode FoVs using only subsets of the available dimensions.Consequently, they are unable to leverage combinations of the 4-dimensional basis vectorsthat span the representational space to represent complex dependencies and rich interactions.In contrast, the continuous approach allows the FoVs, (purple/colour), (square/shape),to exist within the same underlying space as the representation, tpr(x). This allows therepresentation learner to encode FoVs using combinations of the 4-dimensional basis vectorsspanning the representational space, R4. Such flexibility naturally captures dependenciesand complex interactions between FoVs, significantly enhancing the expressivity of theresulting representations.",
  "B.1Shortcomings of the TPR and How Soft TPR Helps": "In this subsection, we provide a more detailed explanation regarding the fundamental limitationsproduced by the TPRs stringent specification (Eq 1 ). For concreteness, we use an example. Weconsider the following set of roles and fillers, R = {shape, colour}, F = {red, blue, square}.Additionally, we define the role R : R R2 and filler F : F R3 embedding functions asfollows:",
  "=": "These 2 possible TPRs form a discrete, 2 element subset of the underlying representationalspace, R6, T = {(2 2 4 2 2 3)T , (1 2 4 1 2 3)T }. Relaxing the TPR specification to the SoftTPR allows any point, z, in the underlying representational space, R6 to qualify as a SoftTPR provided that for some explicit TPR, tpr T, the sufficient closeness requirementholds: ||z tpr||2 < . So, if we consider the set of possible Soft TPRs, TS, we have =TS := {(2 2 4 2 2 3)T + , (1 2 4 1 2 3)T + : || < }. Hence, TS clearly has strictly morepoints than T, so there should be more functions parameterising the map from the observeddata to TS compared to T. Furthermore, in contrast to T, which contains discrete (singular)points scattered around in R6, TS contains continuous cloud-like regions centered at eachtpr T. Both these factors should make the Soft TPR representation potentially easier tolearn and extract information from, compared to the TPR. This is reflected in our empiricalresults in Section C.6.1 , where, compared to the traditional TPR, the Soft TPR demonstrates1) greater representation learner convergence, 2) superior sample efficiency for downstreamtasks, and 3) superior raw downstream performance in the low sample regime.",
  ". Quasi-Compositional Structure: The traditional TPR enforces a strict algebraic definitionof compositionality (i.e": "i F (fm(i)) R(ri)). Even explicitly algebraically-structureddomains such as natural language, do not always adhere to this rigid specification ofcompositional structure (e.g. French liaison consonants, which consist of a weighted sum offillers bound to a single role ). The Soft TPRs continuous relaxation of this constraintallows it to represent structures that only approximately satisfy the TPRs strict definitionof compositionality. A more relevant example could be the construction of compositionalrepresentations of coloured squares where the colour is a weighted combination of fillers blueand red (i.e., varying shades of purple). In this case, even if purple has not been seen bythe TPR Autoencoder previously (i.e., there is no quantised filler representing F (purple)),it may be possible for the TPR Autoencoder to produce a Soft TPR corresponding topurple square if there is a suitable -level relaxation of any of the explicit TPRs (i.e.,red square, blue square) that represents a purple square. 3. Serial Construction: Building explicit TPRs requires that the embedded fillers and rolescomprising a binding (i.e. (F (fm(i)), R(ri))) are first tokened before the entire compo-sitional representation, tpr(x) can be produced . This sort ofsequential approach, where constituents must be tokened before the compositional represen-tation can be formed, is a key characteristic of the symbolic representation of compositionalstructure . In contrast, the Soft TPR allows the Encoder to produce any arbitrary elementof VF VR (in this case = R6), provided that the sufficient closeness requirement holds.Thus, once the Encoder is trained, it is theoretically possible for the Soft TPR Autoencoder toexploit vector space continuity to generate approximately compositional representations bymapping directly from the data to a Soft TPR, without needing to token any representationalconstituents.",
  "B.2Alternative Formulations": "In contrast to the greedily optimal analytic form of tpr that we derive in Equation 5 , it is possibleto instead derive a globally optimal TPR that the (DR DF )-dimensional vector, z, produced bythe encoder, E, best approximates with reference to the Frobenius norm metric of ||z tpr||F .We fix the set of fillers, F, and roles, R, as well as the embedding functions, F : F RDF andR : R RDR to be arbitrary sets and embedding functions respectively. Note that with F, R,F , and R fixed, the only degree of freedom in defining the space of possible TPRs is given bydefining M, the set of role-filler matching functions that define the permissible role-filler bindingdecompositions of x X. The globally optimal TPR that z best approximates, opttpr, is given by:",
  "i F (fm(i)) R(ri)}mM denotes the set of all possible TPRs given the choicesof F, R, F , R and M. Again, we drop the dependency of both tpr and m on x for ease ofnotation": "If M is a subset of the entire set of matching functions, M, i.e., if there are some constraints to whatfillers can be bound to what roles, then, solving for 10 is NP-complete. If M however correspondsto the entire set of possible matching functions, i.e., M = M, then, 10 can be solved by simplyusing any method that solves the (one-to-many) assignment problem. Noting the poor worst casetime complexity of O(n3) for such solutions, and guided by our intuition that there should be amore explicit dependency between tpr and the structure of z, we thus propose the greedily optimaldefinition of tpr in 5 , which creates an explicit dependency between the unbound soft fillers of z,{ fi}, and tpr.",
  "B.3Soft TPR Autoencoder": "Note that defining m as m(i) := arg minj || fk F (fj)||2 in Equation 5 effectively per-mits any filler, fj, to bind to a role, ri.This is somewhat inevitable given the weaklysupervised framework we are situated in, as without knowledge of the ground-truth FoVtype-token bindings, it is impossible to define M as a subset of all possible matching func-tions, M, that produces the ground-truth set of role-filler binding decompositions, i.e.wecannot knowm M|{(F (fm(i)), R(ri))} = (x) xX.We observe, however, that thesize of M is combinatorially large, i.e., |M| =NF +NR1NR, and contains role-filler bind-ing decompositions that are clearly misaligned with the ground-truth semantics of images, e.g.,{cube/object colour, purple/object shape, green/orientation, blue/wall colour, 2/floor colour}for the image in . We thus, decide to design our Soft TPR Autoencoder with 2 orthogonaland separately achieved aims: 1) representational form, to ensure the produced representation has theform of a Soft TPR, and 2) representational content, to ensure that m produces sensible role-fillerbindings, and hence, that the Soft TPR, z, which best approximates T P R reflects the ground-truthFoV type-token bindings of the image.",
  "B.3.1Representational Form": "We now provide additional details on how our method achieves the representational form property.Recall from .2, that we penalise the Euclidean distance between the encoder, Es output,z, and the explicit TPR tpr that z best approximates with the analytic form of Equation 5 . Such apenalisation should ideally encourage the Soft TPR Autoencoder to produce encodings of inputtedimages that have the form of a Soft TPR, as all of Es outputs are directly penalised to satisfy thesufficient closeness property of ||z tpr||2",
  "i F (fm(i)) R(ri) with the analytic form of 5 . We now provide additional details on these 3modules": "1) Unbinding: While FoV types (roles) can be bound to the same FoV value (fillers), e.g.object colour/magenta, floor colour/magenta, each FoV type (role) clearly represents an inde-pendent visual concept type, and so, it is reasonable to use linearly independent embedding vectorsfor the FoV types. Thus, within our considered domain of visual representation learning, we canreasonably satisfy the linear independence requirement that ensures recoverability of the embeddedrepresentational components (F (fm(i)), R(ri)) from the TPR representation. Note that any ran-domly initialised role embedding matrix MR RDRNR is likely (though not guaranteed) to consistof NR linearly independent role embedding vectors provided DR >> NR, and furthermore, thatwe can encourage that the linear independence property to be preserved during all stages of trainingby adding orthogonality regularisation, ||M TRMR INRNR||F . There is one pertinent problem,however, if we simply let the role embedding matrix, MR, consist of any set of arbitrary, linearlyindependent role embedding vectors, {R(ri)}. That is, for any choices of NR, DR, where thesevalues are large, it is computationally expensive to obtain U, the (left) inverse of MR required to 7We again assume we are working with vector spaces over the reals, and use the vector space isomorphismproperty of RDF RDR = RDF DR to align the TPR framework more seamlessly with the autoencodingframework. produce the unbinding vectors, {ui}, needed for unbinding. We thus, decide to leverage the propertiesof (left-invertible) semi-orthogonal matrices, A Rdn, for which AT A = Inn. In this case,U T = MR, and so, the i-th unbinding vector, ui simply corresponds to the i-the role embeddingvector, R(ri). Thus, we can simply unbind the soft filler embedding from z bound to role ri bytaking the (tensor) inner product between z and R(ri), the i-th column of MR. To accomplish this in practice, our unbinding module randomly initialises the role embedding matrix,MR as a semi-orthogonal matrix, through use of torch.nn.utils.parameterizations.orthogonal,and fixes MR during all stages of training (i.e., gradient is not backpropagated to MR), to ensurethat the semi-orthogonal property is preserved during all stages of training. Fixing MR may promptconcerns that that role embedding vectors do not capture semantically informative content. However,for the visual representation learning domain we consider, it is intuitive that role embeddingsthemselves do not need to convey much semantic content, as the majority of an images semanticcontent can be conveyed through filler embeddings and the bindings of fillers to roles. More concretely,filler embeddings, being learnable, can convey semantic information by being close (in some norm) toone another, e.g., the filler embeddings for {green, magenta, blue} may be close in Euclidean distanceto one another, and thus convey the semantic information that they correspond to values for the same(colour) role. Additionally, as the decoder, D, receives information from all embedded bindings inthe form of the TPR, tpr, to perform image reconstruction, D should intuitively learn through thereconstruction loss term of Equation 6 , the semantics associated with a role. For example, giventhe embedding for binding (oblong/object shape), D should learn from the reconstruction loss, themapping between the randomly initialised role embedding vector for object colour and the semanticsof the corresponding FoV type (i.e., that this role changes the colour of the object in the image).",
  ") Quantisation": "The quantisation module relies on the VQ-VAE vector quantisation algorithm to learn the fillerembedding vectors of the filler embedding matrix, MF , and to quantise the soft filler embeddings{ fi} into the explicit filler embeddings {F (fi)} with the closest Euclidean distances. Brieflyspeaking, to perform vector quantisation, the VQ-VAE algorithm simply quantises each soft fillerfi into the embedding vector from MF with the smallest Euclidean distance . This clearlycorresponds with the definition of m in Equation 5 (i.e. for each soft filler embedding, fi, m matches an explicit filler embedding with the smallest Euclidean distance to fi to produce tpr). Asthis quantisation operation corresponds to an argmax and is thus non-differentiable, the VQ-VAEalgorithm uses a simple L2 loss, the codebook loss, to move the embedding vectors R(ri) towards thesoft filler embeddings, as captured by the first term of the VQ-VAE quantisation loss term in Equation 6 . To prevent the embedding space from growing arbitrarily, a commitment loss, corresponding tothe final term of the VQ-VAE quantisation loss in 6 is added, to ensure the encoder commits to anembedding.",
  "j=iF (fm(j)) R(rj) + F (fm(i)) R(ri),(12)": "where m and m denote the matching functions for x and x respectively. That is, we constructswapped TPRs by simply swapping the quantised filler embedding associated with role ri inconstructing new TPR representations for x and x. Note that in contrast to a similar operation thatmight be applied to scalar-tokened or vector-tokened compositional representations, swapping thequantised filler between the representations for x and x in this case produces a global (not local)effect on the resulting representation.",
  "B.3.3Model Architecture": "We now provide concrete details of our model architecture. Our model consists of a standard Conv-based encoder, E, ( ) our TPR decoder, ( ) and a standard Conv-transpose based decoder,D ( ). The only learnable component of the TPR decoder corresponds to the filler embeddingmatrix, MF , and so, our Soft TPR Autoencoder only adds an additional DF NF parameters to astandard (variational) autoencoding framework consisting of the encoder and decoder, where DF andNF correspond to the dimensionality of the filler embedding space, and number of filler embeddingvectors respectively.",
  "Encoder": "Input 64 64 cConv 32, 4 4, stride = 2, padding = 1BatchNorm 32ReLU 32Conv 64, 4 4, stride = 2, padding = 1BatchNorm 64ReLU 64Conv 256, 4 4, stride = 2, padding = 1BatchNorm 256ReLU 256Conv 512, 4 4, stride = 2, padding = 1BatchNorm 512ReLU 512Flatten 512 4 4Linear 1024ReLU 1024Linear 512ReLU 512Linear 512ReLU 512Linear DF DR",
  "Decoder": "Input DF DRConvTranspose 512, 4 4, stride = 2, padding = 1BatchNorm 512ReLU 512ConvTranspose 256, 4 4, stride = 2, padding = 1BatchNorm 256ReLU 256ConvTranspose 64, 4 4, stride = 2, padding = 1BatchNorm 64ReLU 64ConvTranspose 32, 4 4, stride = 2, padding = 1BatchNorm 32ReLU 32ConvTranspose 3, 4 4, stride = 2, padding = 1",
  "B.4Model Hyperparameters and Hyperparameter Tuning": "The tunable hyperparameters of the Soft TPR Autoencoder are the following following: 1) archi-tectural hyperparameters NR, NF , DR, and DF corresponding to the number of role (respectivelyfiller) embedding vectors and the dimensionalities of their respective embedding spaces, and 2) lossfunction hyperparameters (Equation 6 ), 1 (Equation 7 ) and 2 (Equation 7 ). In line with VQ-VAE , we set , the coefficient for the VQ-VAE commitment loss to 0.5. As NRcorresponds to the number of FoV types, to ensure fair comparisons with scalar-tokened generativebaselines, and COMET, which all assume that the number of FoVs equals 10, (VCT assumes 20),we fix NR to be 10. We tune remaining hyperparameters by running hyperparameter optimisationusing the open-source hyperparameter sweep framework of Weights and Biases (WandB). We setthe search method as Bayesian search, and the optimisation criterion to be the fully unsupervisedMSE reconstruction loss corresponding to the second term in Equation 6 . During hyperparameteroptimisation, we train all models for between 50,000-100,000 iterations. The obtained hyperparametervalues for the Soft TPR Autoencoder are listed in . See Section C.6.2 for ablation experimentsdemonstrating our models robustness to hyperparameter configurations.",
  "(fixed)0.50.50.5": "In practice, we find that the Soft TPR penalty of Equation 6 is negligible, most likely becausethe codebook loss of VQ-VAE, which pushes the quantised fillers {F (fm(i))} and the soft fillerembeddings { fi} of z together, is already sufficient to push z to tpr. To prevent redundantlycalculating this term, we remove it from the overall loss in the implementation of our model. Our model is implemented in Pytorch and trained using the Adam optimiser on the losscorresponding to 7 . We use a learning rate of 1e4, and the default setting of (1, 2) = (0.9, 0.999)across all instances of model training.",
  "C.1Datasets": "For disentanglement, we consider the standard disentanglement datasets: Cars3D , Shapes3D, and MPI3D , where for MPI3D, we consider the real variant of the dataset, (i.e., not therealistic, toy, or complex variants of the dataset). These datasets contain 3, 6, and 7 ground-truthFoVs respectively, and are procedurally generated by taking the Cartesian product of all possible FoVvalues for each FoV type. We provide metadata in Tables 12 , 13 and 14 and denote all continuous-valued FoV types by the symbol . As the colours in the Shapes3D dataset correspond to 10 linearlyspaced values with a natural ordering (e.g. red, with value 0, is perceptually more similar to orange,with value 1, compared to aqua, with value 5), we treat all FoV colour types in the Shapes3D datasetas continuous-valued variables, in line with .",
  "Object colour6 coloursObject shape6 shapesObject size2 sizesCamera height3 heightsBackground colour3 coloursHorizontal axis40 valuesVertical axis40 values": "For the downstream task of regression, we simply use the above disentanglement datasets, regressingon each datasets continuous-valued FoVs, as indicated by the symbol in Tables 12 , 13 and 14 . Forthe downstream task of abstract visual reasoning , we use the dataset from . Each samplefrom this dataset consists of a Ravens Progressive Matrix (RPM) style question (samples of thedataset can be found in ). Each RPM-style question has a set of 8 context panels, and 6 answerpanels, where each panel corresponds to an image from the Shapes3D dataset. For the model to selectthe correct answer out of a selection of 6 possible answer panels, it must correctly infer the abstractvisual relation that each row of the context panels share.",
  "C.2.1Experiment Compute Resources": "For the generative weakly supervised, scalar-valued baselines (i.e., AdaGVAE-k, GVAE, MLVAE,SlowVAE, the Shu model), and our model, we perform model training on a single Nvidia RTX4090GPU. We also perform all associated experiments for these models (i.e., downstream model training,downstream model evaluation, disentanglement evaluation, etc.) on this single Nvidia RTX4090GPU. Our model takes approximately 1.5 hours to fully train (i.e., run 200,000 iterations) on theCars3D and Shapes dataset, and approximately 4.0 hours to fully train on the MPI3D dataset.",
  "C.2.2Disentanglement Models": "For Ada-GVAE, GVAE, MLVAE and SlowVAE, use the Pytorch-based, open-source implementationof , which was verified by authors to reproduce official reported results for each model. ForCOMET, and VCT, we use official open-source implementations published by the authors of thecorresponding models . For the GAN-based model of Shu, we convert the official Tensorflow-based implementation of to Pytorch and verify that our implementation reproduces official results.For all baseline models, we use suggested hyperparameters where dataset-specific hyperparametersare given, otherwise, we perform hyperparameter tuning using an identical WandB hyperparametersweep setup as our model. As mentioned in the first paragraph of , for the weakly supervised baselines of Ada-GVAE,GVAE, MLVAE, SlowVAE and Shu, all models with exception to Ada-GVAE assume access to I,the FoV that differ between each pair in an observed sample (x, x), and thus have identical levels ofweak supervision as our model. Ada-GVAE, however, does not assume access to I, so we make thefollowing modification to make Ada-GVAE more comparable with our model. Ada-GVAE adaptivelyestimates I using a method that relies on the estimation of k := |I|, i.e., the number of FoVs thathave changed in each observed sample. Thus, we simply amend Ada-GVAE by providing it accessto the ground-truth value for k (note that if we instead provide Ada-GVAE with knowledge of theFoVs comprising I itself, it becomes identical to GVAE). We empirically verify that our modification,which we denote by Ada-GVAE-k, produces superior or in worst case, comparable results to theoriginal Ada-GVAE model. We train all models, with exception to SlowVAE, which assumes allFoV values in observed pairs (x, x) change, with k = 1 (i.e., only 1 FoV changes between x and x).This ensures all models are trained in an identical setting as our framework. For the selected hyperparameter configuration associated with each model, including our own, weobtain 5 trained models using 5 random seeds, and collate results over these 5 seeds. All models aretrained for 200,000 iterations on all datasets.",
  "C.2.3Downstream Models": "For the task of FoV regression, in line with , we use a simple, generic MLP model, with thearchitecture listed in . For each disentanglement dataset, the MLP regression model receivesrepresentations of images produced by a representation learner (i.e., a disentanglement model), and istrained to predict the corresponding ground-truth FoV values in a supervised fashion. We evaluateregression performance by computing the R2 score on a held out, randomly selected test set of 1,000samples. For each of the 5 instances of a representation learning model produced by the 5 random seeds, weobtain 2 MLPs, resulting in a total of 10 MLP models for each representation learner. We obtain eachMLP by uniformly sampling the number of output nodes in the first, second, and fifth layers of theMLP, as indicated in 15 . All reported results are averaged over these 10 MLP models. Note that allMLP regression models have no a priori knowledge of the representational form (i.e. scalar-tokenedsymbolic compositional representation, vector-tokened symbolic compositional representation, fullycontinuous compositional representation) that they will receive during training as we do not provideany inductive biases to the MLP models or optimise them in any representation-learner specific way(apart from through using the supervised, MSE-based training loss).",
  "Linear d1, d1 ReLULinear d2, d2 ReLULinear dim(z)ReLULinear dim(z)ReLULinear d3, d3 ReLULinear k": "For the abstract visual reasoning task, in line with , we use the Wild Relation Network (WReN)model on representations obtained by all representation learners to predict the ground-truthanswer for each RPM matrix. For each of the 5 instances of a model produced by the 5 random seeds,we randomly sample 2 possible configurations of the WReN model, producing 10 WReN models foreach representation learning model. In line with , for the edge MLP, g, in the WReN model, weuniformly sample either 256 or 512 hidden units. Similarly, we uniformly sample either 128 or 256hidden units for the graph MLP, f. We however, fix the number of hidden layers in g to 2, and f in 1,representing the smallest possible number of hidden layers, to constrain the capacity of the WReNmodel. We refer readers to for further details on the WReN architecture. All WReN modelsare trained using Adam optimisation on the BCE loss between the predicted logits and ground-truthlabels, with a learning rate of 1e5 and the default setting of (1, 2) = (0.9, 0.999).",
  "C.2.4Experimental Controls": "Recall that our main hypothesis is that neural networks can both learn explicit compositional structure,and leverage it more easily when that structure is instantiated in a fully continuous way, e.g., whenembodied by our Soft TPRs. To ensure that the experimental results indeed provide empiricial supportfor this hypothesis, we apply a series of controls to rule out the contribution of any confoundingvariables to the empirical results. We detail these controls below.",
  "To ensure that any performance boosts on the disentanglement metrics (shown in Tables 16 and": "17 ) are not attributable to the slight increase in learnable parameters of our model (note that ourmodel only adds one learnable component, i.e., the filler embedding matrix, MF on top of astandard (variational) autoencoding framework, with this corresponding to 13,568, 1,824, and 1,600additional parameters for the Cars3D, Shapes3D, and MPI3D datasets respectively), we perform acontrol to fix the number of learnable parameters in baseline models with fewer learnable parameterscompared to our model. The baseline models with fewer learnable parameters than our model areSlowVAE, Ada-GVAE-k, GVAE, ML-VAE, and the Shu model. VCT and COMET are substantiallymore parameter hungry (10+ million) than our model, so we do not perform the controls on thesemodels. We increase the number of parameters by increasing the number of filters in the convolution(and transpose-convolution) layers of the generative baseline models, and/or increasing the numberof convolution (and transpose-convolution) layers until the modified model has at least as many parameters as our model, repeating this process for each of the disentanglement datasets. We denotethese parameter-controlled models in Tables 16 and 17 with the symbol . In line with and asshown in Tables 16 and 17 , we do not observe any increase in disentanglement performance conferredby increasing the number of learnable parameters in these generative, scalar-tokened baselines, so weapply representation learning convergence experiments, and downstream model experiments usingthe original models, and not their parameter-controlled variants.",
  "Downstream Task Controls": "To ensure that any performance boosts on the downstream tasks of FoV regression and abstractvisual reasoning are not attributable to our representations increased dimensionality, we post-process representations produced by all models (including models producing higher-dimensionalrepresentations) to match the dimensionality of our representation. Note that all generative, weaklysupervised models producing scalar-tokened representations produce representations with a dimensionof 10 for all datasets, which contrasts with the representational dimensions of 1536 (Cars3D), 512(Shapes3D), and 320 (MPI3D) of our Soft TPRs, highlighting the necessity of such a control. To perform this control, we apply separate methods for the scalar-tokened and vector-tokened models.For scalar-tokened models, we multiply each dimension k of the latent vector, lk with an randomembedding vector ek Rd from an fixed, randomly initialised, semi-orthogonal embedding matrixE Rd10, and subsequently concatenate all multiplied random embedding vectors together toform the dimensionality-controlled representation, (l1ek, . . . , l10e10) R10d. d is a dataset specificinteger that ensures that the size of the dimensionality-controlled representation is at least as smallas the dimensionality of the Soft TPR representation for the given dataset, i.e., d := dim(z)/10,where z is the Soft TPR representation produced by a given dataset. Note that we choose a semi-orthogonal embedding matrix with the intuition that this ensures the maximal distinguishability ofeach continguous subset of dimensions corresponding to liei. For COMET and VCT, which both produce vector-tokened representations, we consider alternativemethods of performing dimensionality-control. COMETs representations have a dimensionalityof 640 for all datasets, and so, the model produces lower-dimensional representations than oursfor Shapes3D and Cars3D, but not MPI3D. VCT, on the other hand, produces representations withdimension 5120, and so, produces higher-dimensional representations than ours for all datasets. Forany vector-tokened representation with higher dimensionality than our representation, we apply PCA-based postprocessing to the model representation, reducing the dimensionality of each vector-tokenedvalue in the representation to the required dimensionality, d, where d := dim(z)/dim(zbaseline),before concatenating all PCA-reduced vectors together to produce the modified representation. Forany vector-tokened representation produced by COMET with lower dimensionality than ours, weapply a simple matrix multiplication between the representation, and a randomly initialised matrix ofrequired dimensionality to embed the representation into the same-dimensional space as ours.",
  "C.3Disentanglement": "In reporting the disentanglement metric results for baseline models, we use published results whereapplicable, i.e., we use the results published in for VCT and COMET and the published resultsfor SlowVAE . For GVAE, MLVAE, and Ada-GVAE-K, we evaluate disentanglement using thePytorch-based implementation of disentanglement metrics, , which corresponds to a Pytorch-based implementation of the official disentanglement lib of . We also use this implementation toevaluate the disentanglement of representations produced by all models.",
  "C.3.1Disentanglement Metrics": "In line with , we consider the following 4 disentanglement metrics: the FactorVAE score ,the DCI Disentanglement score (we refer to DCI Disentanglement as DCI), the BetaVAE score, and the MIG score . We provide a brief overview of all 4 disentanglement metrics, andrefer interested readers to the papers these metrics are introduced in for further details, as well as theAppendix of for more details on how the metrics are implemented in and . FactorVAE Metric: A randomly selected FoV of the dataset is fixed, and a mini-batch of observationsis subsequently randomly sampled. The representation learner produces representations for thesamples. Disentanglement is quantified using the accuracy of a majority vote classifier that predictsthe index of the ground-truth fixed FoV based on the index of the representation vector with thesmallest variance. DCI Metric: A Gradient Boosted Tree (GBT) is trained to predict the ground-truth FoV values fromrepresentations produced by a representation learner. The predictive importance of the dimensionsof a representation is obtained using the models feature importances. For each sample, a score iscomputed that corresponds to one minus the entropy of the probability that a dimension of the learnedrepresentation is useful for FoV prediction, weighted by the relative entropy of the correspondingdimension. An average of these scores over the mini-batch of samples is taken to produce the finalscore. BetaVAE Metric: This metric quantifies disentanglement by predicting the index of a fixed FoV fromrepresentations produced by a representation learner, similar to the FactorVAE metric. In contrast tothe FactorVAE metric, however, the BetaVAE metric uses a linear classifier on difference vectorsto predict the index of the fixed FoV. Each difference vector is produced by taking the differencebetween representations produced for a pair of samples (x, x) with one underlying fixed FoV. MIG Metric: For each FoV, the MIG metric computes the mutual information between eachdimension in the representation, and the corresponding FoV. The score is obtained by computing theaverage, normalised difference between the highest and second highest mutual information of eachFoV with the dimensions of the representation.",
  "C.3.2Evaluating the Disentanglement of Soft TPRs": "Since disentanglement metrics are typically computed under the assumption that the representationcorresponds to a concatenation of scalar-valued FoV tokens, we now detail how we computethe above disentanglement metrics on the Soft TPR, a continuous compositional representation.Similar amendments have been made in COMET and VCT, so that the disentanglement of theirrepresentations, corresponding to a concatenation of vector-valued FoV tokens, can be computed. FactorVAE metric: To compute the FactorVAE score of our Soft TPR, we produce a NR-dimensionalvector, v, for each Soft TPR, where NR corresponds to the number of roles, i.e. the FoV types. Weproduce v by simply populating each dimension, vi, with the index of the quantised filler that role riis bound to, i.e. we set vi to m(i). We use the resulting vs to compute the variances required by theFactorVAE metric, noting that if the FoV type corresponding to role i is fixed, and this is reflected inour representation, dimension i of the v vectors produced in a mini-batch should clearly have thesmallest variance, as all fillers, fm(i), that role ri binds to, will have the same identity across themini-batch. DCI metric: As the DCI relies on computing the ground-truth FoV values from NR-dimensionalrepresentations produced by the representation learner, we again, follow a similar procedure as theabove, in converting our (DF DR)-dimensional Soft TPR representation to a NR-dimensionalrepresentation. That is, we simply consider a NR-dimensional vector, v, where each dimension, vi, ispopulated by the index of the quantised filler, m(i) that role ri is bound to, and use this vector tocompute the corresponding DCI result. BetaVAE metric: For the BetaVAE metric, as each sample used to train the linear classifier consistsof a NR-dimensional difference vector obtained by computing the difference between the NR-dimensional scalar-tokened compositional representations, we obtain the following difference vector,d, for our Soft TPR representations. For each role i {1, . . . , NR}, we obtain the correspondingquantised filler embeddings for each sample x, x in the pair, i.e., we obtain F (fm(i)), F (fm(i)).For each pair of quantised filler embeddings, we obtain a scalar distance measure corresponding tothe cosine similarity between the the pair. The i-th dimension of d is populated using this value. Weuse difference vectors obtained in this way to compute the BetaVAE metric. MIG metric: For the MIG metric, which relies on a discretisation of the values in each dimension iof the NR-dimensional scalar-tokened compositional representations to compute the discrete mutualinformation, we apply the same postprocessing technique as in the FactorVAE, and DCI metric, toevaluate MIG on our Soft TPRs. That is, we produce the same NR-dimensional vector, v, noting thatthis choice of postprocessing also performs the discretisation required by the MIG computation.",
  "C.3.3Full Results": "We now present unabridged results for all considered disentanglement metrics, denoting the learnableparameter control modification of each relevant baseline with the symbol . As can be seen inTables 16 and 17 , our Soft TPR representations are explicitly more compositional (as quantified bythe disentanglement metric scores) compared to all considered baselines, with especially notableperformance increases on the more challenging datasets of Cars3D and MPI3D.",
  "Symbolic scalar-tokened compositional representations": "Slow-VAE0.178 0.0080.196 0.0240.196 0.0280.228 0.0300.361 0.0280.396 0.022Slow-VAE0.146 0.0840.149 0.0340.163 0.0390.237 0.0230.345 0.0210.959 0.009Ada-GVAE-k0.188 0.0230.198 0.0180.203 0.0070.194 0.0080.295 0.0280.472 0.037Ada-GVAE-k0.182 0.0070.234 0.0240.285 0.0250.319 0.0040.662 0.0230.954 0.009GVAE0.171 0.0090.189 0.0080.182 0.0130.188 0.0200.319 0.0380.478 0.032GVAE0.180 0.0220.237 0.0180.287 0.0140.306 0.0200.684 0.0200.936 0.008MLVAE0.171 0.0160.188 0.0090.193 0.0120.194 0.0150.293 0.0120.432 0.023MLVAE0.177 0.0120.221 0.0230.277 0.0230.325 0.0170.644 0.0260.925 0.024Shu0.175 0.0090.184 0.0180.200 0.0100.202 0.0140.288 0.0380.310 0.038Shu0.177 0.0140.230 0.0450.285 0.0280.302 0.0340.444 0.0130.444 0.013",
  "C.4Representation Learning Convergence": "We additionally examine representation learning convergence by evaluating the representationsproduced at 100, 1,000, 10,000, 100,000, and 200,000 iterations of model training, where the latterstage of 200,000 iterations corresponds to fully trained models. To quantify representation learningconvergence, we evaluate both 1) the explicit compositionality of representations produced at thesestages of training (as quantified by disentanglement metric performance), and 2) the usefulness ofthese representations for the downstream tasks of FoV regression and abstract visual reasoning. As mentioned in .1, the representation learning convergence of our model as measuredby disentanglement performance is comparable with baselines, however, our model consistentlyconverges faster than baselines in producing representations that can be effectively leveraged forboth downstream tasks, as indicated by higher downstream model performance across the majority",
  "of representation learner training stages. We now present the full suite of unabridged results, firstpresenting disentanglement results, and subsequently downstream results": "In all line plots, we plot the mean, and indicate the standard deviation by the shaded regions. We usethe same legend for all plots, where 0 (grey) denotes SlowVAE, 1 (orange) denotes AdaGVAE-k, 2(green) denotes GVAE, 3 (red) denotes MLVAE, 4 (purple) denotes Shu, 5 (pink) denotes VCT, 6(brown) denotes COMET, and 7 (blue) denotes our model, Soft TPR Autoencoder.",
  "C.4.1Disentanglement": "We first present line plots of representation learner convergence for each of the four considereddisentanglement metrics (i.e., the FactorVAE, DCI, BetaVAE and MIG scores) for all three disentan-glement datasets (Cars3D, Shapes3D, MPI3D). A series of tables containing the values associatedwith these line plots is presented following the plots. As the disentanglement results producedby our learnable parameter controls for the models of Ada-GVAE-k, GVAE, ML-VAE and Shu,do not achieve superior disentanglement results compared to the original models, we only presentdisentanglement convergence results for the original variants of all baseline models. Iteration count 0.0 0.2 0.4 0.6 0.8 1.0 Factor score Cars3D01234567",
  "We additionally evaluate the representation learning convergence by examining the usefulness ofrepresentations produced at different stages of training (i.e., 100, 250, 500, 1,000, 10,000, 100,000": "and 200,000 iterations of training). To quantify usefulness, we consider performance of downstreammodels on the tasks of FoV regression, and abstract visual reasoning when trained using represen-tations produced by each stage of training. For each task, we present line plots, and additionallytables with values corresponding to each of the plots. We use the same legend as in the previoussection, where 0 (grey) denotes SlowVAE, 1 (orange) denotes AdaGVAE-k, 2 (green) denotes GVAE,3 (red) denotes MLVAE, 4 (purple) denotes Shu, 5 (pink) denotes VCT, 6 (brown) denotes COMET,and 7 (blue) denotes our model, Soft TPR Autoencoder. We additionally provide all results forthe dimensionality-control setting, where the dimensionality of representations produced by allrepresentation learners is held constant following the approach detailed in C.2.4 , denoting this clearlyin plot captions, and by the symbol in the tables.",
  "FoV Regression": "As clearly visible in the tables and plots, generic regression models are able to more effectively userepresentations produced by our Soft TPR Autoencoder produced across almost all stages of trainingfor all disentanglement datasets. Improvements are most notable in the low-iteration regime of 102iterations, and across most stages of training for the more challenging task of FoV regression on theMPI3D dataset (Figures 19 and 20 ). Iteration count 0.0 0.2 0.4 0.6 0.8 1.0 RSq Cars3D01234567",
  "Abstract Visual Reasoning": "We present now present our full suite of results for the downstream task of abstract visual reasoning.As demonstrated in and the corresponding Figures 21 and 22 , representations producedby our model at only 102 iterations of training are able to be leveraged by downstream models toachieve a 80.04% accuracy for the challenging abstract visual reasoning task, in contrast to the valueof 63.1% obtained by the best performing baseline, representing a 26.78% performance increase.This again provides strong empirical support for our hypothesis that the approximate, continuously-instantiated compositional structure embodied by our Soft TPR can be learnt more quickly byrepresentation learners than alternative, symbolic representations of compositional structure, therebyallowing downstream models to effectively leverage these representations despite a small number ofrepresentation learner training iterations.",
  "C.5.1Sample Efficiency Results": "For sample efficiency, as mentioned in .2, and in line with , we compute a ratio-basedmetric obtained by dividing the performance of the downstream model when trained using a restrictednumber of 100, 250, 500, 1,000 and 10,000 samples, by its performance when trained using allsamples. The total number of samples corresponds to 19,104, 480,000, 1,036,800 and 100,000 for thetasks of regression on the Cars3D, Shapes3D, MPI3D datasets, and the abstract visual reasoning taskrespectively. As this metric is dependent on the performance of downstream models when trainedusing all samples, we do not compute this metric for representation learners where the correspondingdownstream models achieve an R2 score of less than 0.5 for regression, as this may produce sampleefficiency scores with very little semantic meaning (e.g. a model that achieves a sample efficiencyscore of 0.9 when its final R2 score is 0.1). This corresponds to removing the Shu model fromShapes3D sample efficiency calculations, and COMET and Shu from the Cars3D sample efficiencycalculations. As many models for the abstract visual reasoning task have low classification accuracies on theheld-out test set following training with the maximal number of 100,000 samples, we do not computesample efficiencies for this task, and instead refer readers to results in Section C.5.2 for the rawclassification accuracies associated with each model. Note that for all box plots, we follow standard convention, and display the median in each box with asolid line, where the box shows the quartiles of the corresponding values, and the whiskers extendto 1.5 times the interquartile range. We again, use the same legend, where grey denotes SlowVAE,orange denotes AdaGVAE-k, green denotes GVAE, red denotes MLVAE, purple denotes Shu, pinkdenotes VCT, brown denotes COMET, and blue denotes our model, Soft TPR Autoencoder. 0.2 0.4 0.6 0.8 1.0 Rsq ratio Sample efficiency (100/all) 0.0 0.2 0.4 0.6 0.8 1.0 Rsq ratio Sample efficiency (250/all)",
  "C.5.2Low Sample Regime Results": "To evaluate the utility of our Soft TPR representation from the perspective of downstream models,we additionally evaluate the raw performance of downstream models as a function of the number ofsamples they have been trained on (again considering 100, 250, 500, 1,000, 10,000 and all samples).We find that our Soft TPR representation contributes to a substantial performance boost in thedownstream models performance in a low-sample regime where the downstream model has beentrained with 100, 250, 500, and 1,000 samples. We present our full suite of experimental results,and highlight the particular performance differentials conferred by our representational form in thelow-sample regime.",
  "C.6.1Soft TPR vs TPR": "To verify that our Soft TPR confers exclusive benefits compared to TPRs, we repeat all our experi-ments using the explicit TPR that our TPR decoder produces, which represents the Soft TPRs greedilyoptimal explicit TPR counterpart, tpr. In all plots, we use the same legend, denoting the explicitTPR as yellow (0) and the Soft TPR as blue (1). Across all considered cases: i.e., 1) convergence rateof representation learning (as measured by the downstream models ability to effectively leveragerepresentations produced at different stages of training), 2) sample efficiency of downstream models,and 3) raw performance of downstream models in the low sample regime, the Soft TPR confersdifferential performance boosts compared to the explicit TPR. This offers strong empirical evidencefor our hypothesis that embodying a more approximate form of explicitly compositional structurehelps representation learners by alleviating the strigent requirement of having to produce explicitTPRs, and additionally provides representation learners with more knowledge of the entire manifoldunderlying compositional structure, which TPRs cannot fully capture either due to representationlearner deficits, or the inherent quasi-compositional structure of represented objects.",
  "C.6.2Robustness to Hyperparameter Choices": "We perform an additional experiment to empiricially verify that our model is robust to differenthyperparameter choices. For the MPI3D dataset, which disentanglement models experience thegreatest difficulty in producing disentangled representations for (see ), we randomly pickanother set of hyperparameters, shown in , from the top 5 models based on the MSE losscriterion. For this randomly chosen hyperparameter configuration, we evaluate the disentanglementof the representations on the MPI3D dataset produced by the resulting model.",
  "D.1Extension to Linguistic Domains": "Applying the Soft TPR to the TPRs typical domain of language is an intriguing future direction,especially as language can deviate from strict algebraic compositionality for instance, idiomaticexpressions such as spill the beans cannot be understood as a function of their constituents alone.Soft TPRs more flexible specification allows it to capture approximate forms of compositionalityprecluded from the TPRs strict algebraic definition (Eq 1 ), thereby potentially providing the SoftTPR the ability to better handle the nuance and complexity of language. To adapt our framework to language, we replace our Conv/Deconv encoder/decoders with simpleRNNs, retrain our TPR decoder, and remove the semi-supervised loss, using Eq 6 as the full loss.Preliminary results in on the BaBI dataset are compared with TPR baselines from AID. Our Soft TPR Autoencoder does not presently surpass AID, but notable points include:",
  "D.2Need for Weak Supervision": "To produce a compositional representation, (x) = C(1(a1), . . . , n(an)) (as in .1 ), eachrepresentational constituent, i(ai), must map 1-1 to a data constituent, ai. Without supervision(i.e., access only to observational data, {xi}), this is challenging, because the data constituents {ai}underlying each object, x, are unknown and cannot be identified. We can frame the above intuition in a more mathematically rigorous way, using the generativeframework. Essentially, as formally proved in , it is impossible to identify the true distributionfor the data constituents (generative factors), p(a), using observational data, {xi}, alone as thereare infinitely many bijective functions f : supp(a) a such that: 1) a and f(a) are completelyentangled (i.e. non-diagonal Jacobian) and 2) the marginal distributions of a and f(a) are identical(meaning the marginal distributions of the observations are also identical, i.e.,p(x|a)p(a)da =p(x|f(a))p(f(a))da). Thus, without inductive biases, it is impossible to infer the data constituents{ai} of any observation, x, from observational data {xi} alone. To combat this non-identifiability result, in line with , we use weak supervision,presenting the model with data pairs (x, x) where x and x differ in a subset of FoVs, e.g. (x) ={shape/cube, colour/purple, size/large}, (x)={shape/cube, colour/cyan, size/large},where the FoV types corresponding to the different FoV values are known to the model. Notethat this weak supervision is minimal, only providing the model to access to the differing FoV types inan index set, I, (i.e., I := {colour}) and not any of the FoV values (i.e., {cube, purple, cyan, large}are all not known by the model).",
  "Some possible future extensions to reduce this level of weak supervision, or alternative forms ofweak supervision include:": "1. Embodied Learning: In the visual domain, some roles, e.g. object position correspond toaffordances. Embodied agents may be able to reduce the need for explicit supervision bycollecting (x, x) and I through interaction with their environment. 2. Pretrained Filler Embeddings: Initialising the filler embedding matrix, MF with em-beddings learnt by a pre-trained vision model could impart knowledge of domain-agnosticfillers (e.g., colours, shapes), reducing the need to explicitly provide (x, x) and I to themodel.",
  "D.3Downstream Utility": "Our investigation of downstream utility centers on two selected tasks FoV regression/classificationand abstract visual reasoning, which aligns with the standard framework for assessing the quality anddownstream utility of compositional representations . While existing work [ 30 , 33 ] demonstrates that explicitly compositional representations enhance downstream sample efficiencycompared to non-compositional representations, a result we improve upon ( C.5.1 ), the broader utilityof compositional representations remains a topic of ongoing exploration . Theoretical perspectives argue that explicitly compositional representations are fundamentalin the production of productive, systematic, and inferentially coherent thought 3 key propertiescharacterising human cognition. Investigating how explicitly compositional representations canyield empirical benefits across these dimensions represents an essential avenue for future research.Although preliminary studies do not find strong evidence that explicitly compositionalrepresentations improve compositional generalisation (a key aspect of systematicity), suggeststhat this finding is because compositional representations are necessary, but not sufficient to inducesystematicity; an explicitly compositional processing approach is also required. Future work could extend our theoretical framework with the hope of producing empirical resultsconsistent with the theoretical arguments of . In particular, as our unbinding module is designedto provably and efficiently recover structured role-filler constituents from the Soft TPR, it maybe possible to exploit this module to systematically reconfigure roles and fillers from existingrepresentations to create representations of novel combinations of role-filler bindings (i.e., novel",
  "The Soft TPR belongs to VF VR, which is a DF DR dimensional space, which grows mul-tiplicatively in DF , DR. Several factors, however, mitigate scalability concerns in light of thisfact:": "1. Independence of Embedding Space Dimensionality: We note that the dimensionalityof the role and filler embedding spaces (DR, DF respectively) are properties of the corre-sponding embedding functions (R : R VR and F : F VF ) and thus, can be fixedindependently of the number of roles, NR, the number of fillers, NF , or the number oftotal role-filler bindings (which we denote by n) within a domain. Thus, it is possible tofix the Soft TPRs dimensionality (DF DR) to be smaller than NF NR (the numberof roles/FoV types multipled by the number of fillers/FoV tokens) or n (the number ofbindings), which all may be large in complex visual domains. As illustrated in ,the dimensionality of the TPR is smaller than NF NR in both the Shapes3D and MPI3Ddomains. 2. Relaxing Orthogonality: While DF can be set a priori with no regard to NR, NF or n,we require DR NR for semi-orthogonality of the role-embedding matrix MR, whichguarantees faithful (see proof 2 of A.2 ) and computationally efficient (see Unbindingheading of B.3.1 ) recoverability of constituents. It is, however, possible to relax thisconstraint (i.e., to have DR < NR) to further reduce dimensionality. In this case, semi-orthogonality of MR is impossible and hence the recoverability of constituents cannot beguaranteed, however, there are some less stringent guarantees on the outcome of unbindingthat can still be derived (see p291 of for more details). We also more explicitly compare the dimensionality of the Soft TPR with baselines in .Scalar-tokened symbolic representations have a low dimensionality of 10 (NR) at the expense ofrepresentational expressivity (each representational constituent i(ai) is scalar-valued). In contrast,Soft TPR has vector-valued representational constituents (i.e. F (fm(i)) R(ri)), similar to VCTand COMET. When compared to these models, the Soft TPR has significantly lower dimensionalitycompared to VCT and is comparable with COMET.",
  "In the Soft TPR Autoencoder, the expensive tensor product operation is employed to generate tpr.Given the computational cost of the tensor product, we more concretely examine the computational": "cost of training the Soft TPR Autoencoder by computing the FLOPs for a single forward passon a batch size of 16 using the open-source implementation of fvcore , visible in . This data demonstrates that,despite the tensor products computational cost, the mathematically-informed derivation of our modelallows it to obtain compositional representations with vector-valued representational constituents at asignificantly lower cost compared to relevant, vector-tokened baselines (2 orders of magnitude lessthan VCT, and 4 orders of magnitude less than COMET). Future work could explore the use of tensor contraction techniques to reduce computational expense.For instance uses a Hadamard product based tensor product compression technique. This reducescomputational cost from n2 (tensor product of 2 vectors) to n (Hadamard product), but comprisesthe theoretical guarantees on constituent recoverability. We believe developing tensor contractiontechniques within the TPR framework is an important direction for future research, to ensure efficientTPR-based representations with provable recoverability of constituents.",
  "The answer NA means that the paper has no limitation while the answer No means thatthe paper has limitations, but those are not discussed in the paper": "The authors are encouraged to create a separate Limitations section in their paper. The paper should point out any strong assumptions and how robust the results are toviolations of these assumptions (e.g., independence assumptions, noiseless settings,model well-specification, asymptotic approximations only holding locally). The authorsshould reflect on how these assumptions might be violated in practice and what theimplications would be. The authors should reflect on the scope of the claims made, e.g., if the approach wasonly tested on a few datasets or with a few runs. In general, empirical results oftendepend on implicit assumptions, which should be articulated. The authors should reflect on the factors that influence the performance of the approach.For example, a facial recognition algorithm may perform poorly when image resolutionis low or images are taken in low lighting. Or a speech-to-text system might not beused reliably to provide closed captions for online lectures because it fails to handletechnical jargon.",
  "If applicable, the authors should discuss possible limitations of their approach toaddress problems of privacy and fairness": "While the authors might fear that complete honesty about limitations might be used byreviewers as grounds for rejection, a worse outcome might be that reviewers discoverlimitations that arent acknowledged in the paper. The authors should use their bestjudgment and recognize that individual actions in favor of transparency play an impor-tant role in developing norms that preserve the integrity of the community. Reviewerswill be specifically instructed to not penalize honesty concerning limitations.",
  ". Experimental Result Reproducibility": "Question: Does the paper fully disclose all the information needed to reproduce the main ex-perimental results of the paper to the extent that it affects the main claims and/or conclusionsof the paper (regardless of whether the code and data are provided or not)?Answer: [Yes]Justification: We provide all information (specification of model architecture, computingresources, hyperparameters) to replicate experimental results in the Appendix.Guidelines: The answer NA means that the paper does not include experiments. If the paper includes experiments, a No answer to this question will not be perceivedwell by the reviewers: Making the paper reproducible is important, regardless ofwhether the code and data are provided or not.",
  "If the contribution is a dataset and/or model, the authors should describe the steps takento make their results reproducible or verifiable": "Depending on the contribution, reproducibility can be accomplished in various ways.For example, if the contribution is a novel architecture, describing the architecture fullymight suffice, or if the contribution is a specific model and empirical evaluation, it maybe necessary to either make it possible for others to replicate the model with the samedataset, or provide access to the model. In general. releasing code and data is oftenone good way to accomplish this, but reproducibility can also be provided via detailedinstructions for how to replicate the results, access to a hosted model (e.g., in the caseof a large language model), releasing of a model checkpoint, or other means that areappropriate to the research performed. While NeurIPS does not require releasing code, the conference does require all submis-sions to provide some reasonable avenue for reproducibility, which may depend on thenature of the contribution. For example(a) If the contribution is primarily a new algorithm, the paper should make it clear howto reproduce that algorithm.",
  "(b) If the contribution is primarily a new model architecture, the paper should describethe architecture clearly and fully": "(c) If the contribution is a new model (e.g., a large language model), then there shouldeither be a way to access this model for reproducing the results or a way to reproducethe model (e.g., with an open-source dataset or instructions for how to constructthe dataset). (d) We recognize that reproducibility may be tricky in some cases, in which caseauthors are welcome to describe the particular way they provide for reproducibility.In the case of closed-source models, it may be that access to the model is limited insome way (e.g., to registered users), but it should be possible for other researchersto have some path to reproducing or verifying the results.",
  ". Open access to data and code": "Question: Does the paper provide open access to the data and code, with sufficient instruc-tions to faithfully reproduce the main experimental results, as described in supplementalmaterial?Answer: [Yes]Justification: All datasets are open-access. We provide sufficient instructions in our Ap-pendix to reproduce experimental results. Furthermore, we provide the main code for ourSoft TPR model architecture with this submission. We plan to release a streamlined versionof our entire code base used to conduct all experiments if the paper is accepted.Guidelines:",
  ". Experimental Setting/Details": "Question: Does the paper specify all the training and test details (e.g., data splits, hyper-parameters, how they were chosen, type of optimizer, etc.) necessary to understand theresults?Answer: [Yes]Justification: We provide these details in the Appendix, and additionally, in the code.Guidelines: The answer NA means that the paper does not include experiments. The experimental setting should be presented in the core of the paper to a level of detailthat is necessary to appreciate the results and make sense of them.",
  ". Experiment Statistical Significance": "Question: Does the paper report error bars suitably and correctly defined or other appropriateinformation about the statistical significance of the experiments?Answer: [Yes]Justification: We present results with standard deviations, as well as the IQR and whiskersextending to 1.5 times the IQR for all box plots.Guidelines: The answer NA means that the paper does not include experiments. The authors should answer Yes if the results are accompanied by error bars, confidenceintervals, or statistical significance tests, at least for the experiments that support themain claims of the paper. The factors of variability that the error bars are capturing should be clearly stated (forexample, train/test split, initialization, random drawing of some parameter, or overallrun with given experimental conditions).",
  ". Experiments Compute Resources": "Question: For each experiment, does the paper provide sufficient information on the com-puter resources (type of compute workers, memory, time of execution) needed to reproducethe experiments?Answer: [Yes]Justification: We specify the specific GPUs we use for all experiments in the Appendix,as well as the average amount of time it takes to train the Soft TPR Autoencoder for eachdataset.Guidelines: The answer NA means that the paper does not include experiments. The paper should indicate the type of compute workers CPU or GPU, internal cluster,or cloud provider, including relevant memory and storage.",
  "Guidelines:": "The answer NA means that there is no societal impact of the work performed. If the authors answer NA or No, they should explain why their work has no societalimpact or why the paper does not address societal impact. Examples of negative societal impacts include potential malicious or unintended uses(e.g., disinformation, generating fake profiles, surveillance), fairness considerations(e.g., deployment of technologies that could make decisions that unfairly impact specificgroups), privacy considerations, and security considerations. The conference expects that many papers will be foundational research and not tiedto particular applications, let alone deployments. However, if there is a direct path toany negative applications, the authors should point it out. For example, it is legitimateto point out that an improvement in the quality of generative models could be used togenerate deepfakes for disinformation. On the other hand, it is not needed to point outthat a generic algorithm for optimizing neural networks could enable people to trainmodels that generate Deepfakes faster. The authors should consider possible harms that could arise when the technology isbeing used as intended and functioning correctly, harms that could arise when thetechnology is being used as intended but gives incorrect results, and harms followingfrom (intentional or unintentional) misuse of the technology. If there are negative societal impacts, the authors could also discuss possible mitigationstrategies (e.g., gated release of models, providing defenses in addition to attacks,mechanisms for monitoring misuse, mechanisms to monitor how a system learns fromfeedback over time, improving the efficiency and accessibility of ML).",
  ". Safeguards": "Question: Does the paper describe safeguards that have been put in place for responsiblerelease of data or models that have a high risk for misuse (e.g., pretrained language models,image generators, or scraped datasets)?Answer: [NA]Justification: Our work poses no such risks.Guidelines: The answer NA means that the paper poses no such risks. Released models that have a high risk for misuse or dual-use should be released withnecessary safeguards to allow for controlled use of the model, for example by requiringthat users adhere to usage guidelines or restrictions to access the model or implementingsafety filters.",
  ". Licenses for existing assets": "Question: Are the creators or original owners of assets (e.g., code, data, models), used inthe paper, properly credited and are the license and terms of use explicitly mentioned andproperly respected?Answer: [Yes]Justification: In the main paper, as well as the Appendix, we cite the papers associatedwith each dataset we use. We additionally make explicit references in the Appendix toall externally created code that we use in our experiments. Furthermore, if the paper is tobe accepted, we will clearly provide licenses and attribution to the original authors whereapplicable in the code files.Guidelines:",
  ". New Assets": "Question: Are new assets introduced in the paper well documented and is the documentationprovided alongside the assets?Answer: [Yes]Justification: The code we submit in our submission corresponds with our Soft TPR Au-toencoder architecture, and is thus reasonably straightforward to understand, especiallywhen supplemented by this paper. If this paper is accepted, however, we will include moreextensive written documentation for the official, more extensive codebase we release.Guidelines: The answer NA means that the paper does not release new assets. Researchers should communicate the details of the dataset/code/model as part of theirsubmissions via structured templates. This includes details about training, license,limitations, etc.",
  "According to the NeurIPS Code of Ethics, workers involved in data collection, curation,or other labor should be paid at least the minimum wage in the country of the datacollector": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with HumanSubjectsQuestion: Does the paper describe potential risks incurred by study participants, whethersuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)approvals (or an equivalent approval/review based on the requirements of your country orinstitution) were obtained?Answer: [NA]",
  "The answer NA means that the paper does not involve crowdsourcing nor research withhuman subjects": "Depending on the country in which research is conducted, IRB approval (or equivalent)may be required for any human subjects research. If you obtained IRB approval, youshould clearly state this in the paper. We recognize that the procedures for this may vary significantly between institutionsand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and theguidelines for their institution."
}