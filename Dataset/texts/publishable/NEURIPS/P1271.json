{
  "Abstract": "We study the problem of learning general (i.e., not necessarily homogeneous) halfspacesunder the Gaussian distribution on Rd in the presence of some form of query access. In theclassical pool-based active learning model, where the algorithm is allowed to make adaptive labelqueries to previously sampled points, we establish a strong information-theoretic lower boundruling out non-trivial improvements over the passive setting. Specically, we show that anyactive learner requires label complexity of (d/(log(m))), where m is the number of unlabeledexamples. Specically, to beat the passive label complexity of O(d/), an active learner requiresa pool of 2poly(d) unlabeled samples. On the positive side, we show that this lower bound can becircumvented with membership query access, even in the agnostic model. Specically, we givea computationally ecient learner with query complexity of O(min{1/p, 1/}+ d polylog(1/))achieving error guarantee of O(opt) + . Here p [0, 1/2] is the bias and opt is the 0-1 lossof the optimal halfspace. As a corollary, we obtain a strong separation between the active andmembership query models. Taken together, our results characterize the complexity of learninggeneral halfspaces under Gaussian marginals in these models. Supported by NSF Medium Award CCF-2107079 and an H.I. Romnes Faculty Fellowship.Supported by NSF Medium Award CCF-2107547 and NSF Award CCF-1553288 (CAREER).Supported by NSF Award CCF-2144298 (CAREER).",
  "Introduction": "In Valiants PAC learning model [Val84a, Val84b], the learner is given access to random labeledexamples and aims to nd an accurate approximation to the function that generated the labels.The standard PAC model is passive in the sense that the learner has no control over the selectionof the training set. Here we focus on interactive learning between a learner and a domain expert thatcan potentially lead to signicantly more ecient learning procedures. A standard such paradigmis (pool-based) active learning [MN+98], where the learner has access to a large pool of unlabeledexamples S and has the ability to (adaptively) select a subset of S and obtain their labels. We willhenceforth refer to this type of data access as label query access. An even stronger interactive modelis that of PAC learning with membership queries [Ang88, Fel09]. A membership query (MQ) allowsthe learner to obtain the value of the target function on any desired point in the support of themarginal distribution. This model captures the ability to perform experiments or the availability ofexpert advice. While in active learning the learner is only allowed to query the labels of previouslysampled points from S, in MQ learning the learner has black-box access to the target function (seeDenition 1.4 and Denition 1.3). Roughly speaking, when the size of S becomes exponentiallylarge (so that it is a good cover of the space), the model of active learning converges to the modelof learning with MQs. This intuitive connection will be useful in the proceeding discussion.Active learning is motivated by the availability of large amounts of unlabeled data at low cost.As such, the typical goal in this model is to develop algorithms with qualitatively improved labelcomplexity (compared to passive learning) at the expense of a largerbut, ideally, still reasonablyboundedset of unlabeled data. Over the past two decades, a large body of work in theoreticalmachine learning has studied the possibilities and limitations of active learning in a variety of naturaland important settings; see, e.g., [FSST97, Das04, Das05, DKM05, BBZ07, BHV10, H+14, HY15,KLMZ17, HKL20, HKLM20, BCBL+22, DMRT24, KMT24b, KMT24a].A prototypical setting where active learning leads to substantial savings is for the task of learninghomogeneous Linear Threshold Functions (LTFs) or halfspaces. An LTF is any function h : Rd {1} of the form h(x) = sign(w x + t), where w Sd1 is called the weight vector and t is calledthe threshold. If t = 0, the halfspace is called homogeneous. The problem of learning halfspaces isone of the classical problems in machine learning, going back to the Perceptron algorithm [Ros58]and has had a great impact on many other inuential techniques, including SVMs [Vap97] andAdaBoost [FS97].For the class of homogeneous halfspaces under well-behaved distributions (including the Gaus-sian and isotropic log-concave distributions), prior work has established that O(d log(1/)) labelqueries suce, where d is the dimension and is the desired accuracy [BBZ07, DKM05, BL13]. More-over, there are computationally ecient algorithms with near-optimal label complexity for this task[ABL17, YZ17, She21], even in the agnostic model that achieve error O(opt+). Unfortunately, thislogarithmic dependence on 1/ breaks down for general (potentially biased) halfspaces. Intuitively,this holds because if the bias of a halfspace (the probability mass of the small class) is p, then we needto obtain at least 1/p labeled examples before we see the rst point in the small class. This impliesan information-theoretic label complexity lower bound of (min{1/p, 1/} + d log(1/)) [Das05],even for realizable PAC learning under the uniform distribution on the sphere.Balcan, Han-neke, and Vaughan [BHV10] showed an information-theoretic label complexity upper bound ofO((1/p)d3/2 log(1/)) for general halfspaces under the uniform distribution on the sphere (via anexponential-time algorithm).In summary, prior to this work, the possibility that there is an active learner with label com-plexity O(d polylog(1/) + min{1/p, 1/}) and unlabeled sample complexity poly(d/) remainedopen. Our rst main result is an information-theoretic lower bound ruling out this possibility.",
  "Theorem 1.1 (Main Lower Bound). For any active learning algorithm A, there is a halfspace h": "that labels S with bias p such that if A makes less than O(d/(p log(m))) label queries over S, a setof m i.i.d. points drawn from N(0, I), then with probability at least 2/3 the halfspace h output by Ahas error more than p/2 with respect to h. In particular, if p is chosen as ( log(1/)), learning a p-bias halfspace with error C (for anyxed constant C) would require a learning algorithm to either make (d1c/) label queries orhave a pool of (2dc) unlabeled examples for any small constant c > 0. Our information-theoreticlower bound essentially shows that the active setting does not provide non-trivial advantages forthe class of general halfspaces, unless the learner is allowed to obtain exponentially many unlabeledexamples. (As already mentioned, in this extreme setting, the active learning model approximatesPAC learning with MQs.) This motivates the study of learning halfspaces in the stronger modelwith MQs, where better upper bounds may be attainable.To circumvent the aforementioned lower bound, we consider the stronger model of PAC learningwith MQs. We are interested in understanding the query complexity of learning general halfspacesunder the Gaussian distribution. We study this question in the agnostic learning model and establishthe following positive result. Theorem 1.2 (Main Algorithmic Result). Consider the problem of agnostic PAC learning halfspaceswith membership queries under the Gaussian distribution. There is an algorithm such that for everylabeling function y(x) and for every , (0, 1), it makes M = O(min{1/p, 1/} + d polylog(1/))1 memberships queries, runs in poly(d, M) time, where p is the bias of the optimal halfspace h, andoutputs an h H such that with probability at least 1 , err(h) O(opt) + . In other words, we provide a computationally ecient constant-factor agnostic query learner withquery complexity O(min{1/p, 1/}+ d polylog(1/)). The query complexity upper bound achievedby our algorithm is new, and essentially optimal (see subsequent discussion), even without thecomputational considerations. Moreover, our algorithm runs in polynomial time and achieves aconstant-factor approximation to the optimal accuracywhich is best possible for proper learners. Computational Complexity vs Error GuaranteeIn the passive PAC model, there existdpoly(1/) complexity lower bounds for achieving an error of opt + [DKZ20, DKPZ21, DKR23] forour problem. Consequently, the majority of work [ABL17, DKS18, DKTZ22] in the passive settinghad focused on designing ecient learners achieving a constant factor approximation of O(opt) + .These passive learning algorithms have sample complexity poly(d, 1/). Note that, by Theorem 1.1,it is impossible to modify these algorithms (for general halfspaces) to achieve an active learner withlow label complexity. Finally, we remark that even in the presence of query access, [DKK+23b]showed that it is computationally hard to achieve error opt + for proper learning. Optimality of Query ComplexityIn the realizable setting under the Gaussian distribution, alearner may query many points that are extremely far from the origin to nd examples from thesmall class with few queries. However, such an algorithm is quite fragile to even a tiny amount ofnoise. In particular, the query complexity achieved by our algorithm establishing Theorem 1.2 isnearly optimal in the agnostic setting.On the one hand, (d log(1/)) queries are required because describing a halfspace up to error requires d log(1/) bits of information [KMT93]. On the other hand, we argue that the overheadterm of (min{1/p, 1/}) cannot be avoided in the agnostic setting.Such a statement can bededuced from a lower bound of [HKL20]: they showed that in the realizable setting, any algorithm",
  "In this paper, we use O to hide the dependence on polylog(1/) and use O to hide the dependence on polylog(1/)": "requires at least ((1/p)1o(1)) MQs to see the rst example from the small class (where p is thebias of the target halfspace with respect to the uniform distribution on the unit ball); they alsoshowed a similar lower bound of (1/p) if the underlying distribution is the uniform distributionover the unit sphere. As the dimension d increases, the standard Gaussian distribution is very wellapproximated by the uniform distribution over a d-dimensional sphere with radius d. Thus, anexponentially small level of noise would make every query far from this sphere contain no usefulinformation. This allows us to show that, under the Gaussian distribution with a tiny amount oflabel noise, ((1/p)1o(1)) queries are needed to see a single example from the small class. Theproof of this statement is essentially identical to the argument in [HKL20] for unit ball.Though (min{1/p, 1/}) queries for exploring small-class examples are in general unavoidable,in some practical applications, the learner could obtain a small number of random small-classexamples from existing training datasets without making exploration. In fact, assuming we have anoracle that can give us a random small-class example (when t 0, the oracle will return a randomexample with a negative label), the learning algorithm in Theorem 1.2 can be implemented by callingthe oracle O(1) times and making O(d polylog(1/)) membership queries. This suggests that theonly reason for the (min{1/p, 1/}) term is that the learner needs to explore small-class examples.We defer the discussion of implementing the learning algorithm in Theorem 1.2 to Appendix F.",
  "Preliminaries": "Basic NotationWe will use Sd1 to denote the 2-unit sphere on Rd. For a halfspace h(x) =sign(w x + t), w Sd1, t > 0, we use p(t) = PrxN(0,I)(h(x) = 1) to denote its bias. Fora halfspace h(x), we dene its Chow parameters vector (or simple Chow parameters) under thestandard Gaussian distribution to be ExN(0,I)[h(x)x]. Let y(x) : Rd {1} be a (randomized)labeling function for examples in Rd. We denote by err(h) = PrxN(0,I)(h(x) = y(x)) to be theerror of the hypothesis h and opt = minhH err(h), where H is the class of halfspaces over Rd. Wewill use h to denote the halfspace with an error equal to opt. When there is no confusion, we willuse p to denote the bias of the optimal halfspace h.Let Dx be a distribution over Rd, y(x) be a labeling function over Rd, and S = {(xi, y(xi))}mi=1be a set of i.i.d. examples drawn from the distribution D over Rd {1} such that the marginaldistribution of D is Dx. A membership query takes an x in the support of Dx as input and outputsy(x). A label query takes an xi, where (xi, y(xi)) S as input and outputs y(xi). A learningalgorithm A is allowed to use membership queries/label queries and aims to output a halfspacehypothesis h such that err(h) O(opt) + by making as few queries as possible. Problem DenitionsFor concreteness, we record the formal denitions of our two learningmodels.We focus on the agnostic model under Gaussian marginals for the class of halfspaces,which is the setting considered in this paper. Denition 1.3 (Learning Halfspaces with Membership Queries). Let H = {h(x) = sign(w x + t) :Rd {1} | w Sd1, t 0} be the class of halfspaces over X = Rd. The labeling functiony(x) : X {1} is a random function that maps each x X to an unknown binary randomvariable. For each h H, denote by err(h) = PrxN(0,I) (h(x) = y(x)), opt := minhH err(h) andh(x) = sign(w x+t) any halfspace with error opt. A membership query takes x X as an inputand returns a label y y(x). We say that a learning algorithm A is a constant-factor approximatelearner if for every labeling function y(x), and for every , (0, 1), it outputs some h H byadaptively making memberships queries, such that with probability at least 1, err(h) O(opt)+.",
  "The query complexity of A is the total number of membership queries it uses during the learningprocess": "Denition 1.4 (Active Learning of Halfspaces with Label Queries). Let H = {h(x) = sign(wx+t) :Rd {1} | w Sd1, t 0} be the class of halfspaces over X = Rd. Let D be a distribution overRd {1} such that Dx, the marginal distribution over x, is the standard Gaussian distributionN(0, I). For each h H, denote by err(h) = Pr(x,y)N(0,I) (h(x) = y), opt := minhH err(h) andh(x) = sign(w x + t) any halfspace with error opt. Let S be a set of m i.i.d. labeled examplesdrawn from D. An active learning algorithm (with label query access) is given S but with hiddenlabels and is allowed to make a label query for each x S and observe its label y(x). We say thata learning algorithm A is a constant-factor approximate learner if for every distribution D andfor every , (0, 1), it outputs some h H by adaptively making label queries over a set of mexamples drawn i.i.d. from D, such that with probability at least 1 , err(h) O(opt) + . Thelabel complexity of A is the total number label queries made over S during the learning process.",
  "Lower Bound on Label Complexity: Proof of Theorem 1.1": "In this section, we prove our information-theoretic lower bound on the label complexity of activelearning general halfspaces under the Gaussian distribution.Before presenting our proof, we provide high-level intuition behind Theorem 1.1 and the strategyof our proof. Previous work, see, e.g., [Das04, DKM05, HKL20], showed that if S is a set of examplesdrawn uniformly from the unit sphere, and if h is a halfspace with bias p that is chosen uniformly,the following holds: no matter which query strategy a learning algorithm A uses, for the rst rqueries, in expectation only pr of them fall into the small cap on the sphere cut by h. Thus, if Amakes less than 1/(2p) queries, it will with constant probability not see any negative examples; andit is therefore impossible to learn the target halfspace.In the Gaussian case, we will use a similar but stronger idea. If we are able to learn h up toerror p/2 with few queries, then we can randomly partition S into two sets, use the rst set to learnthe halfspace and use the second part to nd d negative examples by paying another O(d) queriesin expectation. Formally, we have the following statement. Lemma 2.1. Suppose there is an active learning algorithm that can make r label queries over a poolS of m poly(d/p) examples drawn from N(0, I) and learn any halfspace h(x) = sign(w x + t)with bias p up to error p/2 with probability at least 2/3. Then there is an algorithm such that given apool of 2m random examples S drawn from the standard Gaussian distribution with hidden labels bysome halfspace h(x) = sign(w x + t) with bias p, it makes r + O(d) queries and nds d negativeexamples from S with probability 1/2. Proof. Let A be such a learning algorithm. We select a random set of m examples S1 and give itto A. With probability at least 2/3, A makes r queries and learns a halfspace h with error p/2with respect to h. This implies that given a Gaussian example, with probability at least p/2 it willpredict negative, and given it predicts negative, with probability at least 1/2 it is actually negative.Since m is at least poly(d, 1/p), we know that with enough high probability, at least (d) exampleswill be predicted by negative by h and at least a constant fraction of these examples are actuallynegative. Thus, given such a h with probability at least 3/4, we can nd d negative examples in Sby randomly querying O(d) examples that are predicted as negative by h.",
  "We will show that nding d negative examples from S requires many queries. The idea is thatsince S is sampled from a standard Gaussian in high dimensions, every pair of examples is almost": "orthogonal unless m is as large as 2d. If we have made 1/p queries over S and found our rstnegative example, then this negative example will only provide us with very little knowledge to ndthe next negative example as no example in the pool has a large correlation with it. Therefore,it will still take us another approximately 1/p queries to nd the next negative example. Such anissue only disappears after we have already found roughly d negative examples; at which time, theaverage of the d examples has a good correlation with w. Therefore, it would take us roughly d/pqueries in total. We remark that such an argument is hard to formalize, because, besides negativeexamples, the algorithm has also seen many positive examples in the process. It is thus challengingto argue that the algorithm cannot make good use of the information obtained from these positiveexamples.To overcome this diculty, our proof strategy works as follows.Each algorithm A can bedescribed as a decision tree.Each tree node represents the example queried in a given round.Every time the algorithm sees a negative example, it moves to the left; otherwise, it moves to theright. Suppose that A wants to nd k negative examples with r queries. Then there are at mostrk (er/k)k paths of the tree, where A successfully nds k negative examples, and for each ofthe paths there are exactly k examples that are negative upon queried. For a k-tuple of examples,we will derive a deterministic condition such that if the k examples satisfy the condition, a randomhalfspace with bias p will have only roughly pk probability to label all of the k examples negative.Formally, we establish the following technical lemma Lemma 2.2. Let A Rkd be a matrix with row vectors x1, . . . , xk. Let t > C > 0 for somesuciently large constant C. Let h(x) = sign(w x + t) be a random halfspace with bias p withw Sd1 chosen uniformly from Sd1. IfAA dI2 O(d/(t)2), then with probability atmost O(p log(1/p))k, where p is the bias of h under the Gaussian distribution, h(xi) = 1 fori = 1, . . . , k. Proof. Let v = Aw = (w x1, . . . , w xk). Consider the projection of w over the subspacespanned by the row vectors of A, A(AA)1Aw. Assuming that x1, . . . , xk are all negative, thenv2 k(t)2. This implies that the square of the norm of the projection of w onto the subspaceis",
  "The last inequality follows by Fact B.3": "Thus, if therktuples all satisfy such a condition, then A will succeed with a fairly tinyprobability unless r is larger than (k/p). So, in the last step of the proof, we will show in Lemma 2.3that by taking k d/(log(m)polylog(1/p)), with high probability every k-tuple of examples in Swill satisfy the deterministic condition. Thus, no algorithm can succeed with a constant probability,unless it makes (d/(p log(m))) queries. Lemma 2.3. Let S Rd be a set of m examples drawn i.i.d. from N(0, I). Let t > C > 0 fora suciently large constant C and k = O(d/ log(m)(t)4). Then, with probability at least 2/3, forevery k-tuple of examples {x1, . . . , xk} S,AA dI2 d/(t)2, where A Rkd be a matrixwith row vectors x1, . . . , xk. Proof. We will rst show that for a given A Rkd,AA dI2 is small with high probabilityif the rows of A are drawn i.i.d. from d-dimensional standard Gaussian.Let N be an 1/4-netof Sk1. By standard results (see, e.g., [Ver18]), we know that |N| e3k andAA dI2 2 supuNuT (AA dI)u. Thus, to show thatAA dI2 is small with high probability, it isequivalent to show with high probability for every u N,u(AA dI)u is small.Fix u N to be a unit vector. We have",
  "We are now ready to prove our main lower bound result": "Proof of Theorem 1.1. We will start by showing that, given a set S of m points drawn i.i.d. froma Gaussian distribution, the following holds. With probability at least 2/3, for every algorithm Athere exists a halfspace h = sign(wx+t) with bias p such that if A makes only r = O(d/p log(m))label queries over S, then with probability at least 2/3 it will not be able to nd k negative examplesin S for some k d. By Yaos minimax principle, it is sucient to show that there is a distributionover halfspaces h such that for any deterministic active learning algorithm, the following holds:given m random Gaussian examples, if the learning algorithm makes r queries, with probability 2/3 it cannot nd k negative examples. We will x the threshold t of h and draw w uniformly fromthe unit sphere.By Lemma 2.3, we know that by choosing k = O(d/ log(m)(t)4), with probability at least 2/3,for every k-tuple of examples x1, . . . , xk S,AA dI2 d/(t)2, where A Rkd is a matrixwith row vectors x1, . . . , xk. By Lemma 2.2, we know that every k-tuple of examples x1, . . . , xk Shas a probability k, which is at most O(p log p)k to be labeled all negative by the random halfspaceh. Notice that every query algorithm can be expressed as a binary tree T. Each node of the treerepresents an example where the algorithm makes queries at a time. If the example at node v isnegative, then the algorithm will query the left child of v, and otherwise it will query the right childof v. The algorithm stops making queries when either it has queried r examples or it has queriedk negative examples. In particular, for a given search algorithm, there are at mostrkdierentpossible outcomes where it successfully nds k negative examples. Furthermore, for each of thepossible outcomes, there is a set of k examples in S that correspond to the k negative examples thealgorithm nds. Thus, the probability that the algorithm successfully nds k negative examples isbounded above by the probability that there exists one of therkk-tuples of examples in S thatare all labeled negative by h. Such a probability can be bounded above byrk",
  "k O(p log(1/p))k 2/3 ,": "if r O(k/p log(1/p)) = O(d/(p log(m)polylog(1/p)). By Lemma 2.1, we know that if we can makeO(d/(p log(m)polylog(1/p)) label queries to learn a p-biased halfspace up to error p/2 over a set Sof m/2 Gaussian examples, then we can use O(d/(p log(m)polylog(1/p)) queries to nd d negativeexamples among m Gaussian points. This leads to a contradiction. Thus, the label complexity ofthe learning problem is (d/(p log(m))), as desired.",
  "In this section, we present our main algorithmic result, Theorem 1.2.We refer the reader to": "for the full proof of Theorem 1.2.Throughout the paper, we will assume for con-venience that the noise level opt . Such an assumption can be made without loss of generality,as discussed in Appendix A.1. We rst present our main algorithm, Algorithm 1. Algorithm 1 willmaintain a list of polylog(1/) candidate hypotheses at least one of which has error O(opt) + . Wewill then use a standard tournament approach to nd an accurate hypothesis among them.At the beginning of Algorithm 1, we will use random queries to approximately estimate the biasp of the optimal halfspace up to a constant factor. As we will discuss in Appendix A.2, such anestimation can be done with only O(min{1/p, 1/}) queries by applying a doubling trick to the coinestimation problem. In particular, if we nd p < C, we can directly output a constant hypothesisas it has error only O(). Since t is unknown to us, such an approach can prevent us from usingsome t which is much larger than t in the rest of the learning procedure, which will potentiallylead to a larger query complexity. With such a p, t will fall into a reasonable range [ta, tb]. We nextpartition [ta, tb] into a grid of size O(1/ log(1/)) and use each of the grid points as an initial guessof t. In particular, at least one of these grid points tj is O(1/ log(1/)) close to t. Although sucha tj is not accurate enough to be used in the nal output hypothesis, as t log(1/), we willshow later that such a tj is enough for us to use it to learn w, t accurately. Suppose now we havesuch a good tj. We will design two subroutines that make use of tj to produce a good hypothesis",
  ": return h": "sign(wT x+ t). The rst algorithm will take tj and the noise level as its input and produce a unitvector w0 as an initialization. We will show in .2 that as long as |tj t| 1/ log(1/),we can with probability at least 1/ log(1/) produce some w0 such that (w0, w) O(1/tj). Byrepeating such an initialization algorithm polylog(1/) times, with high probability one of these runswill succeed. In particular, such an algorithm has a query complexity of O(1/p + d polylog(1/)).Now assume we have such a w0 as a warm-start. Our second subroutine is to rene the directionw0 and the threshold tj. More specically, we will maintain a unit vector wi such that i = (wi, w)and an upper bound i for sin(i/2). In each round of the rening algorithm, we will use O(d) queriesto update wi. In particular, in each round i will decrease by a constant factor and thus after atmost T = O(log(1/)) rounds, we will have sin(T /2) T = C exp(t2j/2). As we will show in .1, provided the correct t, sign(wT x + t) is at most O() far from h. However, tooutput a good hypothesis, we still need to learn t up to a high accuracy. When t is small, weeven have to estimate t up to error O(), which typically needs many queries. However, as we willshow in .1, given wT close enough to w, we are able to combine the localization techniqueused in [DKS18] with this fact to learn t using only O(log(1/)) queries. This gives an overview ofAlgorithm 1 and its query complexity.",
  "Theorem 3.1. Let h(x) = sign(w x + t) be a halfspace such that err(h) = opt . Lett O(": "log(1/)), w0 Sd1 be inputs of Algorithm 3. If t1/ log(1/) t t, t exp((t)2/2) 1/(C) for some large enough constant C and sin((w0, w)/2) 0 := min{1/t, 1/2}, thenAlgorithm 3 makes M = O(d polylog(1/)) membership queries, runs in poly(d, M) time, andoutputs (wT , t) such that with probability at least 1 O(), err(sign(wT x + t)) O().",
  "As we discussed in , we will assume we have some t such that t 1/ log(1/) t t": "and some w0 such that sin(0/2) 0 = min{1/t, 1/2}, i.e., some initial knowledge of t, w. Ouralgorithm runs in iterations and will maintain some wi in round i. We will maintain some unitvector wi and use wi w = 2 sin(i/2) to measure the progress made by Algorithm 3. Themethod we use to update wi is a simple projected gradient descent algorithm. Specically, we willconstruct a random vector Gi over Rd such that Gi wi and in expectation gi = E Gi has boundedlength and a good correlation with respect to w. We will show in the following lemma that byestimating E Gi up to constant error with gi and using the update rule wi+1 = projSd1(wi + i gi),we are able to signicantly decrease i. The proof of Lemma 3.1 can be found in Appendix B.1. Lemma 3.1. Let w, wi Sd1 such that w = aiwi + biu, where u Sd1, u wi, ai, bi >0, a2i + b2i = 1. Let i = (wi, w). Let Gi be a random vector drawn from some distribution D suchthat with probability 1, Gi wi. Let gi be the mean of Gi. Let gi be the empirical mean of Gi andi > 0. The update rule wi+1 = projSd1(wi + i gi) satises the following property,",
  "wi+1 w2 wi w2 2ibigi u + 2ibi gi gi + 2igi2": "Furthermore, if sin(i/2) i (0, 1) and there exist constant c1, c2 such that gi u c1/10, gi c1 and gi gi c2 c1/40, then there exist constant C1, C2 > 8 such that by taking i = i/C1and i+1 = (1 1/C2)i, it holds that sin (i+1/2) i+1. In particular, if sin(i/2) 3i/4 andgi c1 then sin (i+1/2) i+1 always holds. In the rest of the section, we will show that as long as wi is not good enough, we can alwayseciently construct a random vector Gi whose expectation points to the correct direction and we canuse very few queries to estimate its expectation up to a desired accuracy. We adapt the localizationtechnique used in [DKS18] to achieve this goal.",
  "Gi := projwi zy(A1/2iz twi),": "where z N(0, I), Ai = I (1 2i )wiwTi and t (0, t) is a scalar. To see why Gi is a good choice,we will start by analyzing Gi assuming the noise rate opt = 0. To simplify the notation, denote byi(z) = sign((aiwi + biui/i)z + (t at)/i) and gi = EzN(0,I) projwi zi(z). A simple calculationgives us the following result. Fact 3.2. Let h(x) = sign(w x + t) be a halfspace. Let v Sd1 such that w = av + bu, wherea, b > 0, a2 + b2 = 1, u Sd1, u v. Let s, > 0 be real numbers and dene A = I (1 2)vvT .For each z Rd, dene z := A1/2z sv. Then h(z) = (z), where is the following halfspace",
  "a2i +b2i /2iand gi = gi is exactly the ui component of the Chow vector": "In particular, if Ti is constant, then by estimating gi using gi up to a small constant error usingO(d) queries, we are able to use Lemma 3.1 to improve wi. Assuming we set t = t, as it 1and bi O(i), it is easy to check Ti can be bounded by some universal constant. However, as wementioned before, we only know |t t| 1 log(1/), when wi getting close to w, i could becomevery small and an error of 1/ log(1/) could potentially blow up Ti, making the signal we want quitesmall. Such an issue is problematic for the algorithm, especially when fi(z) is a noisy version ofi(z).To overcome such an issue, we prove the following structural lemma in Appendix B.2 showingthat we can always check whether the choice of t is good or not, by looking at the bias of (z),using O(1) queries.Using this method, we can perform a binary search for t to nd a correctchoice in at most log(1/) rounds. Furthermore, as long as we select the correct t, it must hold thatt t O(i). In particular, as T = C exp((t)2/2), such a t is a good enough estimate for t",
  "Robustness Analysis": "So far, we have only considered the case when opt = 0. Due to the presence of noise, it is impossiblefor us to estimate gi = EzN(0,I) projwi zi(z) because we only have a noisy version fi(z) of i(z). Inthis section, we will show that as long as wi is close to w and |t t| 1/ log(1/), the probabilitythat for a Gaussian point z, i(z) = fi(z) is at most a tiny constant. This is incomparable with thebias of z(z) if t is chosen correctly, and does not aect the algorithm too much.We start with the following lemma which bounds the probability of i(z) = fi(z). Lemma 3.3. Let h(x) = sign(w x + t) be a halfspace such that err(h) = opt .Lett, i, t be real numbers such that t t and it 1, i 1/2.Let w = aiwi + biui, whereui Sd1, u wi, ai, bi > 0, a2i + b2i = 1. Dene i(z) = sign((aiwi + biui/i)z + (t at)/i)and fi(z) = y(A1/2iz twi). Then PrzN(0,I)(i(z) = fi(z)) exp(t2/2 + 4)/i. In particular, ifi C exp((t)2/2), for some sucient large constant C, then there is a suciently small constantc such that PrzN(0,I)(i(z) = fi(z)) c e40.",
  "Lemma 3.4 says as the noise level is small, it will not aect the structure lemma we established in": "Lemma 3.2 too much, and thus we are able to nd the correct threshold t by checking the probabilityof fi(z) = 1. Furthermore, as long as we choose the correct threshold t, gi, the noisy version of gistill satises the conditions in the statement of Lemma 3.1 and thus can be used to improve wi.",
  "In .1, we have shown that given some w0 non-trivially close to w and some t such thatt 1": "log(1/) t t, we can use Algorithm 3 to learn a good hypothesis with high probability. Inthis section, we show how to nd such a good initialization w0 using a few membership queries.The most common way to get such a warm-start is by robustly estimating the Chow vector (seefor example [She21, YZ17]) using Fact 3.3. Such an approach does not work for general halfspacesbecause the length of the length of the Chow parameter vector can be as small as O(p), and thusneeds roughly d/p random queries to estimate. In this section, we show how to overcome such anissue using a label smoothing technique, which has been useful in related problems [DKK+23b].The main result in this step can be summarized as follows. The proof of Theorem 3.4 is deferredto Appendix C.2 Theorem 3.4. Let h(x) = sign(w x + t) and y(x) be any labeling function such that err(h) =opt 1/C for some large enough constant C. If |t t| 1/ log(1/), then with probability atleast 1/3, Algorithm 2 makes M = O(1/p + d log(1/)), runs in poly(d, M) time, and outputs somew0 such that sin((w0, w)/2) max{min{1/t, 1/2}, O(",
  "2w x)/": "Let h = sign(w x + t) be an optimal halfspace and let y(x) be any labeling function suchthat err(h) = opt . For x Rd, we denote by (x) := Pr(h(x) = y(x)), the noise level of thesmoothed label. Assuming that we are given a random negative example x0, then with constantprobability, it is close to the decision boundary, i.e., w x0 (t 1 t , t). This implies thatthe threshold of h, the halfspace corresponding to the smoothed label at x0, is between (1, 1).Moreover, the Chow parameter vector of h under the standard Gaussian distribution is parallel tow with a constant length, by Fact 3.3. If opt = 0, then for every t log(1/), we only needanother O(d log(1/)) queries to estimate the Chow parameter of h up to error O(1/t); thus, weget a warm start w0 such that sin(0/2) 1/t, given |t t| is small. Therefore, the total numberof queries we use to run Algorithm 2 is O(1/p + d log(1/p)). However, in general, it is impossibleto estimate w up to arbitrary accuracy even using an innite number of queries becauseof the presence of noise. In fact, using a random x0 is important for Algorithm 2 to succeed. Ifwe are given some adversarially selected x0, even if it is close to the decision boundary, the abovemethod can easily fail. This is because almost all the queries we made are in a small neighborhoodof x0 and could be corrupted by noise arbitrarily. However, we show in Appendix C.1 that, with aprobability at least 2/3, the noise level (x0) of the smoothed label around x0 is at most O(/p), ifx0 is a random example given y(x0) = 1; and thus we can still estimate w to a desired accuracyprovided /p is not too large. Lemma 3.5. Let h(x) = sign(w x + t) be a halfspace and y(x) be any labeling function suchthat err(h) = opt . Let x N(0, I) conditioned on y(x) = 1 be a Gaussian example with anegative label. If p > C for some large enough constant C, then with probability at least 1/2 wehave (x) 5/p and w x (t 1/t, t). Finally, we briey discuss how to obtain a warm start when the threshold t is very large. Thedetails of this method can be found in Appendix D. By Theorem 3.4, when p is small, we are onlyable to get some w0 such that sin((w0, w)) O( log(1/)) for = /p. One possible approachis to use the localization technique we use in .1 to rene such w0.However, such anapproach fails because after localization the noise rate would be possibly larger than the length ofthe Chow parameter that we want to estimate. This makes it impossible for us to learn the usefulsignal. On the other hand, [DKS18] gave a randomized localization method that can make theexpected noise level suciently smaller than the length of the Chow parameter we want to estimate;and thus will succeed with constant probability in each round of renement. Unfortinately, such anapproach cannot be used in a query-ecient manner, because to implement such a method we needto know (wi, w) up to an error 1/ log(1/), in each round of renement. This implies that if wemake a random guess of (wi, w), the probability of success in each round drops to only 1/ log(1/),which requires to rerun the whole algorithm too many times in order to succeed once.Such an issue could be addressed in a similar but more complicated way to the method we use in",
  "Lemma 3.2, by looking at the bias of the halfspace after localization. The second issue is that eventhe noise level is smaller than the length of the Chow parameter we want to estimate, the length of": "the Chow parameter is only 1/pc, for some small constant c, as we can only make 0 smaller thansome small constant. This still requires us to use d/pc queries to estimate it. Such an issue canagain be addressed using the smoothed label method, where we use only 1/pc queries to search asmall class example and use another O(d) queries to estimate the Chow parameter. Importantly,even such a method only succeeds with constant probability overall. As the renement stage onlyruns for O(log log(1/)) rounds, we only need to rerun the entire algorithm O(log(1/)) times tosucceed once.",
  "[DG03]S. Dasgupta and A. Gupta. An elementary proof of a theorem of johnson and linden-strauss. Random Structures & Algorithms, 22(1):6065, 2003": "[DKK+23a] I. Diakonikolas, D. M. Kane, V. Kontonis, S. Liu, and N. Zaris. Ecient testable learn-ing of halfspaces with adversarial label noise. In Proceedings of the 37th InternationalConference on Neural Information Processing Systems, pages 3947039490, 2023. [DKK+23b] I. Diakonikolas, D. M. Kane, V. Kontonis, C. Tzamos, and N. Zaris. Agnosticallylearning multi-index models with queries. arXiv preprint arXiv:2312.16616, 2023. Con-ference version in FOCS24. [DKM05]S. Dasgupta, A. T. Kalai, and C. Monteleoni.Analysis of perceptron-based activelearning. In Learning Theory: 18th Annual Conference on Learning Theory, COLT2005, Bertinoro, Italy, June 27-30, 2005. Proceedings 18, pages 249263. Springer,2005. [DKPZ21]I. Diakonikolas, D. M. Kane, T. Pittas, and N. Zaris. The optimality of polynomial re-gression for agnostic learning under gaussian marginals in the SQ model. In Proceedingsof The 34th Conference on Learning Theory, COLT, 2021. [DKR23]I. Diakonikolas, D. Kane, and L. Ren. Near-optimal cryptographic hardness of agnosti-cally learning halfspaces and relu regression under gaussian marginals. In InternationalConference on Machine Learning, pages 79227938. PMLR, 2023.",
  "[DKS18]I. Diakonikolas, D. M. Kane, and A. Stewart. Learning geometric concepts with nastynoise.In Proceedings of the 50th Annual ACM SIGACT Symposium on Theory ofComputing, pages 10611073, 2018": "[DKTZ22]I. Diakonikolas, V. Kontonis, C. Tzamos, and N. Zaris. Learning general halfspaceswith adversarial label noise via online gradient descent. In International Conferenceon Machine Learning, pages 51185141. PMLR, 2022. [DKZ20]I. Diakonikolas, D. Kane, and N. Zaris. Near-optimal sq lower bounds for agnosticallylearning halfspaces and relus under gaussian marginals. Advances in Neural InformationProcessing Systems, 33:1358613596, 2020.",
  "[YZ17]S. Yan and C. Zhang. Revisiting perceptron: Ecient and label-optimal learning ofhalfspaces. Advances in Neural Information Processing Systems, 30, 2017": "The Appendix is organized as follows: In Appendix A, we discuss why we can without loss ofgenerality assume that the noise level opt and how to learn p up to a constant factor withO(1/p) queries. In Appendix B, we present the omitted proofs in .1 about how to learna good hypothesis given a good initialization. In Appendix C.2, we present the omitted proofs in.2 about how to nd a good initialization when the threshold is not extremely large. InAppendix D, we design an algorithm that nds a good initialization when the threshold is very large.In Appendix E, we prove Theorem 1.2.",
  "A.1Discussion on the Noise Level opt": "We notice that learning a hypothesis with an error of O(opt)+ is equivalent to learning a hypothesiswith an error of O(opt + ), because if we have an algorithm that achieves the latter guarantee, wecan run the same algorithm with a constant-factor smaller to get a hypothesis with error O(opt)+.So, we only need to show that to get a hypothesis with error O(opt + ), we can without loss ofgenerality to assume opt .Assuming we know some such that /2 opt , then learning h upto error O(opt + )is equivalent to learning it up to error O(). By guessing = 2i for i = 0, . . . , O(log(1/)), wecan always obtain a desired and use it to run the learning algorithm and get a good hypothesis.Such an approach will generate a list of O(log(1/)) dierent hypotheses, nding a good enoughhypothesis among them only costs polylog(1/) queries using a standard tournament approach, suchas the following lemma. Lemma A.1 (Lemma 3.6 in [DKK+23a]). Let , (0, 1) and D a distribution over Rd {0, 1}.Given a list of hypothesises {h(i)}ki=1, there is an algorithm that draws O(k2 log(k/)/) unlabeledexamples from Dx and performs O(k2 log(d/)) label queries runs in poly(d, , ) times and outputa hypothesis h such that",
  "A.2Approximate Bias Estimation Using Queries": "In this part, we describe a simple approach to estimate the bias p up to a constant factor usingO(1/p) queries. To do this we will estimate p = PrxN(0,I)(y(x) = 1), the noise version of p as|p p| . If we can estimate p such that p/2 p p or verify that p (C 1), then p satisesour purpose.By Chebyshevs inequality, if p 3p/4, then taking O(1/p) random queries at x and computingthe empirical probability of y(x) = 1, with probability 2/3, we are able to verify this fact bychecking whether the empirical probability is less than 5p/6. On the other hand, if 4p/5 p p,with probability 2/3 we are able to verify this fact by checking whether the empirical probability isgreater than 5p/6. Furthermore, by repeating this approach O(log(1/)) times and using a majorityvoting trick, we can boost the probability of success up to 1 . We will run the above approachfor p = (4/5)i/2 for i = 0, 1, . . . until we nd p (4/5)p or p = C for some constant C. In therst case (4/5)p p (25/24)p and we nd a good approximation for p and thus for p. In thesecond case, we can conclude that p is smaller than O()",
  ": Let C1, C2 be large enough constants5: for i = 0, . . . , T do": "6:Ai I (1 2i )wiwTi , i i/C17:Find t {0, , 2, . . . , t} using the following binary search method, if no such t is found,then stop the algorithm and return wT = 0. Find the correct threshold to constructthe gradient. 8:Draw O(log(1/)) Gaussian samples z N(0, I), query A1/2iz twi and compute p(t), theempirical probability that a query returns 1. If p(t) < e17, properly decrease t, if p(t) > e17,properly increase t. Otherwise, declare that t is found.",
  "= wi w2 2ibigi u + 2ibi gi gi + 2i gi2": "Here, in the second equality, we use the fact that gi wi and in the fourth equality, we use the factthat (gi gi) w = (gi gi) aiwi + (gi gi) biu = (gi gi) biu.Next, we assume that sin(i/2) i and show that we can carefully choose parameter i, i+1 tomake sin(i+1/2) i+1. We consider two cases. In the rst case, we assume 3i/4 sin(i/2) i.",
  "The (v, s, )- rejection procedure satises the following property": "Lemma B.1 (Lemma C.7, Lemma C.8 in [DKS18]). If x N(0, I) is fed into the (v, s, )- rejectionprocedure, then it is accepted with probability exp(s2/(2(1 2))). In particular, when s 2and 1/2, the accepted probability is at least exp(s2/2 4). Moreover, the distribution on xconditioned on acceptance is that of N(sv, Av,), where Av, = I (1 2)vvT .",
  "t2 + 2": "Fact B.4 (Lemma B.4 in [DKTZ22]). Let D be a distribution on Rd {1} with standard normalx-margin and let w, u be two orthogonal unit vectors. Let B be any interval over R and let S(x, y)be any event over Rd {1}, such that S(x, y) {w x B} then it holds",
  "Pr(S(x, y)) )": "Proof of Lemma 3.4. We start by proving the rst part of Lemma 3.4. By Lemma 3.3, we know thati := PrzN(0,I)(i(z) = fi(z)) e40. This implies thatPrzN(0,I)(i(z) = 1) pi e40. Werst show that when pi is in a reasonable range, |Ti| < 6. Assuming by contradiction that |Ti| 6,then by Fact B.3, the bias of i(z) must be at most exp(T 2i /2)/(2Ti) e20, which implies thatit cannot be the case where pi (e18, 1 e18). Similarly, if |Ti| < 5, then by Fact B.3, thebias of i(z) must be at least exp(T 2i /2)/20 e15.5.As the noise level i e40, we havepi (e18, 1 e18).Next, we prove the second part of Lemma 3.4. We start by bounding the correlation between giand ui. We have",
  "(2)": "Since opt , this implies that by providing a good enough estimation of t, we found a hypothesiswith error at most O(). Now we show that this is actually true. For i = 0, sin(0/2) 0 holds byour assumption.Now, we assume this is correct for the i-round and we show this holds with high probability forthe i + 1-th round. We will show that with high probability the gradient gi we use in the updatewi+1 = projwi (wi + i gi) satises the condition of Lemma 3.1.",
  "sign((aiwi + biui/i)z + (t at)/i), gi = EzN(0,I) projwi zi(z) and gi = EzN(0,I) projwi zfi(z),": "where fi(z) = y(A1/2iz twi). And i := PrzN(0,I)(i(z) = fi(z)) < e40 by Lemma 3.3.We rst show that with high probability, Algorithm 3 must be able to select a correct thresholdt t such that |Ti| 6. Denote by pi the probability that fi(z) = 1. We notice that for each xedt by randomly querying O(log(1/)) fi(z), we can with high probability check if pi (e17, 1e17)or not. This can be done using the same method we used in Appendix A.2.Since bi/2 = sin i/2 sin(i/2) i, we know from Lemma 3.4 that as long as we nd somet such that pi (e17, 1 e17), we have have |Ti| 6. By Lemma 3.2 and Lemma 3.4, we knowthat there exists an interval Ii [0, t] of length at least i > such that for every t Ii, |Ti| < 5and thus pi (e16, 1e16). Thus, by performing a binary search at most O(log(1/)) times, withhigh probability, we are able to nd such a t such that |Ti| < 6. Given that we nd such a correctt, we will consider two cases.First, we assume that 3i/4 sin(i/2) i. We will show that with high probability gi andits empirical estimation gi satisfy the condition in the statement of Corollary B.1 and thus provesin(i+1/2) i+1. Since projwi zfi(z) is 1-subgaussian random vector, by Hoedings inequality,",
  "Thus, by Corollary B.1, we can conclude that sin(i+1/2) (11/C2)i = i+1, for a large constantC2": "Next, we consider the case where sin(i/2) < 3i/4. In this case, as we have shown that giis bounded by some universal constant, the condition of Lemma 3.1 is fullled automatically andthus sin(i+1/2) (1 1/C2)i = i+1, for a large constant C2.By induction, with a high probability for each i, we have sin(i/2) i and thus wT is a goodapproximation of w. It remains to show that t is also a good approximation of t. Recall thatt = t < t such that |TT | < 6. Lemma 3.2 implies thatt t 40T = 40C exp (t)2/2. Thus,",
  "sign(wT x + t) = sign(wT x + t) O()": "Thus, with high probability err(sign(wT x + t)) O().Finally, we count the number of queries used by Algorithm 3. In each round of the algorithm,we perform O(log(1/)) binary searches to nd the correct parameter t, each of which takes us onlyO(1) queries. We also make O(d) queries to construct gi in each round of the algorithm. Thus, eachround of Algorithm 3 takes O(d + log(1/)) queries. Since there are at most O(log(1/)) rounds,the query complexity of Algorithm 3 is O(d polylog(1/)).",
  "log(1/)}": "We next address the case when t > 1. By Lemma 3.5, we know that with probability at least1/2, we have (x) 5/p and w x (t 1/t, t). We will assume these two events happenin the rest of the proof. Let z N(0, I) and by Fact 3.6, dene",
  "falls in this range. Formally, we prove the following theorem": "Theorem D.1. Let h(x) = sign(w x + t) be a halfspace with bias p and y(x) be any labelingfunction such that err(h) = opt 1/C for some large enough constant C. Let t be a scalar suchthat t 1/ log(1/) t t and 1/(400t) log(1/) 1/C for some large enough constant C,where = /p, Algorithm 5 makes M = O(1/p+d log(1/)) membership queries, runs in poly(d, M)time and with probability at least 1/polylog(1/), outputs some w0 such that sin((w0, w)/2) 1/t. The high-level idea of our algorithm is as follows. Although Algorithm 2 will not provide us aw0 such that 0 O(1/t), 0 is still smaller than a suciently small constant. We want to use thelocalization technique to rene w0 so that after T rounds of renement, sin(T /2) T = 1/t. Recallin Appendix B, we introduce Denition B.2, (v, s, )-rejection procedure, which can be simulatedusing membership query. Passing a Gaussian random point to the (v, s, )-rejection procedure, byLemma B.1, we will get a another distribution over Rd {1} that behaves the same as anotherhalfspace h.In this section, we want to design a (v, s, )-rejection procedure such that the direction of thehalfspace h has a constant correlation with respect to w and the noise level after the rejectionprocedure is much smaller than the length of the Chow parameter vector of h. Write w = aiwi+biui.We want to set up v = wi, = 1/t and s (ait, ait + bi) uniformly. Such a method is called therandomized threshold method in [DKS18]. This method has the following property. Lemma D.1 (Proposition C.11 in [DKS18]). Let a, b, t > 0 such that a2 + b2 = 1 and t larger thansome constant C. Let w Sd1. Let s [at, at + b] uniformly. For each x Rd, the expectedprobability that x is accepted by the (w, s, )-rejection procedure is at most /b, where = 1/t. Lemma D.1 implies that in expectation over the randomness of s, only /bi-fraction of the noisypoints will pass the (wi, s, )-rejection procedure.If we use query to simulate such a rejectionprocedure, by Lemma B.1, with a constant probability, the noise rate among our queries would beO( exp(s2/2)/bi). However, as we do not know bi, using some b that is slightly far from bi wouldmake the noise level too high for us to learn the signal we want. To overcome this, we design thefollowing test approach to show that given a b, we can with high probability check if it can be usedto construct the rejection procedure or not and in particular, when b 1/ log(1/) < bi < b, such ab is guaranteed to pass our test. Lemma D.2. Let h(x) = sign(w x + t) be a halfspace and y(x) be any labeling function suchthat err(h) = opt . Let w Sd1 be unit vector such that w = aw + bu, a, b > 0 and(a)2 + (b)2 = 1, b < 1/4.Let t > 0 such that t exp(t2/2) 1/(C) for a suciently largeconstant C. Let a, b (0, 1) such that a2 + b2 = 1. Let b, t, w, be input of Algorithm 4. Lets (at, at + b) uniformly. Denote by p(b, s) be bias of a halfspace with threshold Tbs := (t as)/b.Let (z) = sign((aw + bu) z + t as) be a halfspace with bias ps, where If the probabilitythat ps > p(b, s)/4 is at most 1/2, then with probability at least 1 , Algorithm 4 output No. Ifthe probability that ps > p(b, s)/2 is at least 29/30, then with probability at least 1 , Algorithm 4output Yes. Furthermore, the query complexity of Algorithm 4 is O(1/p2(b, at)) = O(1/",
  ": elsereturn No": "Proof of Lemma D.2. By Lemma B.1 and Lemma D.1, we know that over the randomness of s, withprobability at least 5/6, := PrzN(0,I)(h(Az sw) = y(Az sw)) 6 exp(s2/2)/b. We assume,for now, such an event happens. We rst show that such a noise rate is much smaller than p(b, s).Write s = at + , where [0, b], then we have",
  "b2 )) C1e := (C)1,(3)": "where, in the rst inequality, we use the fact that Tbs bt, in the second inequality, we use thefact that t exp(t2/2) 1/(C) for a suciently large constant C, and in the last inequality, we usethe fact that a2 + b2 = 1, 2 < b2. By Fact B.3, we know that exp(T 2bs/2)/Tbs is at most 3 timesp(b, s), and thus p(b, s)/C for a large enough constant C.By Fact 3.2, we know that the ground truth label (z) = h(Az sw) = sign((aw + bu) z +t as). By Hoedings inequality, with high probability, we are able to estimate the probabilityof y(Az sw) = 1 up to error p(b, s)/20 using O(1/p2(b, s)) queries. In particular, since Tbs tb < 1/4, by Fact B.3, we know that p(b, s) > p1/4 and will cost us only O(1/ p) queries.If the probability that ps > p(b, s)/4 is at most 1/2, then in each round i of Algorithm 4, withprobability at least 1/3 it holds simultaneously that ps < p(b, s)/4 and p(b, s)/C. In this case,with high probability ps < p(b, s)/3 and Count does not increase. Thus, with probability at least1 , after T = O(log(1/)) rounds, Count < 3T/4 by Hoedings inequality.Similarly, if the probability that ps > p(b, s)/2 is at least 29/30, then in each round i of Algorithm 4, with probability at least 4/5 it holds simultaneously that ps > p(b, s)/2 and p(b, s)/C.In this case, with high probability ps > p(b, s)/3 and Count increases.Thus, withprobability at least 1 , after T = O(log(1/)) rounds, Count > 3T/4 by Hoedings inequality.",
  "log(p/) holds throughout the proof, since the constant before/p": "log(p/) can always be assumed to be normalized as 1/C is large enough.In round i ofthe algorithm, we write w = aiwi + biui where ai, bi > 0, a2i + b2i = 1. Similar to the analysisof Algorithm 3, we will show that if sin(i/2) i then with probability 1/3 it also holds thatsin(i+1/2) i+1. If this is true then since 1/t > 1/",
  "log(1/) after O(log log(1/)) rounds, wehave sin(T /2) 1/t with probability at least 1/polylog(1/)": "Recall the notation in the proof of Lemma D.2. Given b, we dene p(b, s) to be the bias ofa halfspace with a threshold Tb,s = (t as)/b. By Fact 3.2, we dene (z) = h(Az swi) =sign((aiwi + biu) z + t ais) the ground truth label of y(Az swi), ts to be its threshold andps to be the bias of (z).By Lemma D.2, we know that as long as bi > 1.5/t, with high probability Algorithm 4 willoutput Yes for some b such that with probability at least 1/2, ps > p(b, s)/2 > p1/4. On theother hand, by Equation (3), we know that with probability at least 5/6, := PrzN(0,I)((z) =y(Az swi)) p(b, s)/C for a suciently large constant C. Thus, with a probability at least 1/3,ps > p(b, s)/2 and p(b, s)/C hold simultaneously. For now, we assume this happens and wewill analyze the smoothed label around some z0 such that y(Az0 swi) = 1. By Fact 3.6, thesmoothed label around z0 with respect to halfspace (z) can be seen as a halfspace",
  "(ai)2+b2i, for": "some (e2, 1). Since the noise level of the smoothed label around z0 is as small as 1/C for somelarge enough constant C, by Hoedings inequality, we know thatgi EzN(0,I) projwi zz0(z)can be smaller than some tiny constant with high probability.As it always holds that i 1/t for each i, we will consider two cases.In the rst case,sin(i/2) 3i/4 and gi is bounded by some universal constant.In the second case, we have 3i/4 sin(i/2) i. In this case we know that EzN(0,I) projwi zz0(z) =",
  "(ai)2=b2i": "e5. Using Lemma 3.1, we know that sin(i+1/2) (1 1/C1)i = i+1.Finally, we prove the query complexity of Algorithm 5. By Theorem 3.4, it takes us O(1/p +d log(1/)) queries to get some w0 by running Algorithm 2. After obtaining w0, in each round ofAlgorithm 4, we will run Algorithm 4 O(log(1/)) times to nd a desired b and each round takesus O(1/p2(b)) 1/p2c 1/ p queries, because p(b) is the bias of a halfspace with thresholdTb = bt, which is smaller than t by a tiny constant factor. Furthermore, after obtaining b it takesus O(1/p(b) + d log(1/)) queries to perform the gradient descent update. So, in total Algorithm 5has query complexity at most O(1/p + d log(1/)).",
  "EProof of Theorem 1.2": "We rst show the correctness of Algorithm 1. When we run Algorithm 1, we will start with someinterval [ta, tb] such that any halfspace with a threshold t [ta, tb] must have bias (p). Next,Algorithm 1 partition [ta, tb] into grid such that |tj+1 tj| 1/ log(1/). This implies that theremust be some tj [ta, tb] such that tj 1/ log(log(1/)) t tj. By Theorem 3.4 and Algorithm 5,as long as p > C, with probability at least 1/polylog(1/), we can nd some w0 such thatsin(0/2) min{1/tj, 1/2}.In particular, by running Algorithm 2 or Algorithm 5 polylog(1/)times, at least one of these w0 satises the condition. Furthermore, with such a w0, we know fromTheorem 3.1 that we can with high probability get some h such that err(h) O(opt + ). Thuswithin the list C of the candidate hypotheses maintained by Algorithm 1 at least one of them haserror O(opt + ). By Lemma A.1, we can with high probability nd a hypothesis among C, whoseerror is at most 10 times the error of the best hypothesis in C and thus has error O(opt + ).Next, we prove the query complexity of Algorithm 1. By Appendix A.2, we know that nding aninterval [ta, tb] costs us O(min{1/p, 1/}) queries. If we nd p < C then we are done. Otherwise, wewill run the initialization algorithm and the renement algorithm. By Theorem 3.4 and Algorithm 5,each time we run an initialization algorithm, it takes us O(1/p + d polylog(1/)) queries.By Algorithm 3, each time we run Algorithm 3, it takes us O(d log(1/) queries. Since we will runthese algorithms at most polylog(1/) times.We will in total make O(1/p + d polylog(1/))queries. Finally, by Lemma A.1, nding a good hypothesis from the list of candidate hypotheseswill only take us polylog(1/) queries. Thus, we conclude the query complexity of Algorithm 1 isO(min{1/p, 1/} + d polylog(1/)).",
  "FImplementing the Learning Algorithm via A Small-Class Oracle": "In this section, we discuss how to implement Algorithm 1 to get an even smaller query complexityO(d polylog(1/)), assuming there is an oracle that can return a random small-class example.Before presenting the denition of the small-class oracle, we remind the reader that the notation Ohides the dependence on polylog(1/), and the notation O hides the dependence on polylog(1/).A small class oracle is dened as follows. Denition F.1 (Small-Class Oracle). Let D be a distribution over Rd {1} and h = sign(w x + t), w Sd1, t > 0 be an optimal halfspace such that err(h) = opt = minhH err(h). Asmall-class oracle EX()(D) draws (x, y) D |y=1 and returns x. In other words, a small-class oracle simulates the following rejection sampling procedure, wherea learner keeps drawing x N(0, I), querying its label and stops when it sees some x0 withy(x0) = 1. Such a procedure requires (1/p) queries to implement, which is costly when p issmall.By Theorem 3.1, even without the small-class oracle, the query complexity of Algorithm 3 isalways O(d polylog(1/)). Thus, a small-class oracle would only help reduce the query complexityof Algorithm 2 and Algorithm 5. In the rest of the section, we show that by calling the small-classoracle O(1) = O(polylog(1/)) times, we can reduce the query complexity of Algorithm 2 andAlgorithm 5 to O(d polylog(1/)).We rst consider Algorithm 2. By Theorem 3.4, the query complexity of Algorithm 2 is O(1/p+d log(1/)), where Line 3 in Algorithm 2 takes 1/p queries to nd a random small-class example andLine 4-Line 5 in Algorithm 2 takes O(d log(1/)) queries. As a small-class oracle simulates the samerejection sampling procedure as Line 3 in Algorithm 2, we can implement Line 3 in Algorithm 2 with a single small-class oracle.Thus, by a single call of the small-class oracle, we are able toimplement Algorithm 2 with O(d polylog(1/)) query complexity.Next, we consider Algorithm 5. Each implementation of Algorithm 5 runs in O(log log(1/))iterations. In each iteration, we call Algorithm 4 polylog(1/) times in Line 5, use queries to estimateps in Line 9, nd a single-small class example in Line 11 and improve the current hypothesis withO(d) queries in Line 12. Furthermore, only operations in Line 5, Line 9, and Line 11 have querycomplexity much larger than polylog(1/). Thus, we only need to show with a small-class oracle,we can signicantly reduce the query complexity of these steps.We start with Algorithm 4. In Line 9 in Algorithm 4, we use query to estimate the probability ofy(Azsw) = 1 with an error up to error p(b, s). By Lemma B.1, we know that if we pass a randomsample x N(0, I) to the (w, s, )-rejection procedure, then the resulting distribution is N(sw, A).Thus, the probability of y(Az sw) = 1 is exactly equal to the fraction of negative examplesamong examples that pass the (w, s, )-rejection procedure. Specically, for the (w, s, )-rejectionprocedure, we denote by q the probability that a random example passes the rejection procedureand denote by q the probability that a random negative example passes the rejection procedureand p the fraction of the negative example. Then we have PrzN(0,I) (y(Az sw) = 1) = pq/q.This implies that if we know p and q, then estimating PrzN(0,I) (y(Az sw) = 1) is equivalentto estimating q, which can be done by calling the small-class oracle several times and estimatethe probability that these examples pass the (w, s, )-rejection procedure. By Lemma B.1, we knowthat exp(s2/(2(1 2))) can be computed precisely using the parameter s, .However, wedo not know p precisely, as this requires us to know t up to a high accuracy. To overcome thisdiculty, we use p, the bias of a halfspace with threshold t, because we only need to ensure thecorrectness of the algorithm when our guess t is close to t. In fact, when |t t| 1/ log(1/),p [(1 1/C)p, (1 + 1/C)p] for some large enough constant C, which is enough for ensuring thecorrectness of Algorithm 4. So, to estimate PrzN(0,I) (y(Az sw) = 1) up to error p(b, s), weonly need to estimate q up to error qp(b, s)/p. We have",
  "= (1/Tbs) (1/ log(1/)),(5)": "where we use Fact B.3 and s = at+, [0, b]. This implies that we only need to call a small class or-acle O(1) = O(polylog(1/)) times to estimate q and thus can compute PrzN(0,I) (y(Az sw) = 1)up to error p(b, s). In particular, in this implementation, we only need to call the small-class oracleand do not need to make membership queries.Similarly, to implement Line 9 in Algorithm 5, we also only need to call the small-class oracleO(1) times and do not need to make membership queries.Finally, we show that by calling the small-class oracle O(1) times, we are able to implement Line11 in Algorithm 5. By Lemma B.1, Line 11 in Algorithm 5 draws a random negative example thatpasses the (wi, s, )-rejection procedure. By Lemma D.2, we know that ps = pq/q (p(b, s)).This implies that q (p(b, s)q/p) (1/ log(1/)), by Equation (5). Thus, with high probability,we only need to pass O(log(1/)) examples from the small class oracle to the (wi, s, )-rejectionprocedure to get one negative example that passes this rejection procedure. Thus, in each iterationof Algorithm 5, we will call O(1) times the small-class oracle and make O(d) membership queries.In summary, we count the number of queries in Algorithm 1 using the new implementation with a small class oracle. Notice that with a small-class oracle, we do not need to worry aboutusing some guess t much larger than t because the query complexity in the initialization step nowhas no dependence on the bias of the target halfspace. So we do not need to implement line 4 inAlgorithm 1 but only need to guess t = i/ log(1/) for i = 0, . . . , polylog(1/). This means inAlgorithm 1, we will call Algorithm 2 and Algorithm 5 in total at most polylog(1/) times, so wewill make O(1) small-class oracles and make O(d polylog(1/)) membership queries."
}