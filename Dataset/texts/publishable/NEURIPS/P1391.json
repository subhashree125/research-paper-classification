{
  "Abstract": "While recent works on blind face image restoration have successfully producedimpressive high-quality (HQ) images with abundant details from low-quality (LQ)input images, the generated content may not accurately reflect the real appearanceof a person. To address this problem, incorporating well-shot personal images asadditional reference inputs could be a promising strategy. Inspired by the recentsuccess of the Latent Diffusion Model (LDM), we propose ReF-LDMan adapta-tion of LDM designed to generate HQ face images conditioned on one LQ imageand multiple HQ reference images. Our model integrates an effective and efficientmechanism, CacheKV, to leverage the reference images during the generationprocess. Additionally, we design a timestep-scaled identity loss, enabling ourLDM-based model to focus on learning the discriminating features of human faces.Lastly, we construct FFHQ-Ref, a dataset consisting of 20,405 high-quality (HQ)face images with corresponding reference images, which can serve as both trainingand evaluation data for reference-based face restoration models.",
  "Introduction": "Recent works have achieved impressive results in generating a realistic high-quality (HQ)face image from an input low-quality (LQ) image. However, the important features of a persons facemay be corrupted in the LQ image, and thus the reconstructed image may look like a different person.To tackle this problem, besides the LQ image, additional HQ images of this person can be used asreference input. Moreover, allowing multiple reference images may lead to better quality becausethey offer more comprehensive appearance of this person in different conditions, e.g., different poses,expressions, or lighting. A previous work has explored using multiple reference images for face restoration. Theirmethod, however, depends on a face landmark model to detect facial components (i.e., eyes, nose, andmouse), which may become unreliable when the input LQ image is severely degraded. Besides, latentdiffusion model (LDM) has also been used in different image generating tasks with differentinput conditions, such as low-resolution images, semantic maps, or sketch images . Inspired by the recent success of LDM, we propose ReF-LDM for reference-based face imagerestoration. Unlike previous conditional LDM methods where their input conditions are usuallyspatially aligned with the target image, the reference images are not aligned with the target HQ imagein our case. Therefore, we design a CacheKV mechanism, which effectively and efficiently integratesthe reference images, albeit with different poses and expressions. Furthermore, we introduce atimestep-scaled identity loss to drive the reconstructed image to look like the same person of theLQ and reference images. Lastly, we also construct a new large-scale dataset of face images withcorresponding reference images, which can serve as both training and evaluation datasets for futurereference-based face restoration research.",
  "(d) Input reference images": ": Reference-based face image restoration. Given an input low-quality face image (a), aLatent Diffusion Model (LDM) can reconstruct a high-quality image (b); however, it may not befaithful to the individuals facial identity. To address this problem, we propose ReF-LDM, whichrestores a high-quality image with faithful details (c) by utilizing additional reference images (d). With the above components, our ReF-LDM outperforms recent state-of-the-art methods with asignificant improvement in face identity similarity. Extensive ablation studies for the proposedCacheKV mechanism and timestep-scaled identity loss are also conducted and reported. The maincontributions of this work can be summarized as:",
  "Related work": "Face restoration without personal reference imagesNumerous studies have been proposedfor blind face image restoration . Recent works such as VQFR andCodeFormer have achieved promising results by exploiting VQGAN, while DAEFR furtheremploys a dual-branch encoder to mitigate the domain gap between LQ and HQ images. Inspired bythe success of diffusion models, several works have adopted diffusion models forface image restoration. However, as these methods do not leverage reference images, the restoredimages may differ from the authentic facial appearance of a person, especially when an input imageis severely degraded. Face restoration with personal reference imagesSeveral methods have attemptedto utilize additional reference images to enhance personal fidelity in face restoration. GFRNet warps a single reference image to match the face pose of the LQ image, while ASFNet selectsthe reference image with the closest matching facial landmarks to serve as the network input. Closerto the setting of this work, DMDNet also utilizes multiple reference images. It detects facial landmarks on the LQ image and the reference images to extract features of facial components, andthen integrates these features into the model by querying the corresponding components. However,their method relies on landmark detection, which may not be robust on severely degraded LQ images.In contrast, our ReF-LDM implicitly learns the correspondences between the features of the LQ imageand the reference images, without the need for landmark detection. From a different perspective,MyStyle adopts a per-person optimization setting, leveraging hundreds of images of an individualto define a personalized subspace within the latent space of a StyleGAN . In comparison, ourapproach offers greater flexibility, capable of utilizing one to several reference images without theneed for personalized model optimization for each individual. Latent diffusion models with image conditionsPrevious work demonstrates that LDM cangenerate an image from a low-resolution image by simple channel-axis concatenation . However,reference images in our task are not spatially aligned with the target HQ image, thus requiring amore sophisticated integration mechanism. MasaCtrl achieves text-to-image synthesis with asingle reference image by replacing the original keys and values tokens with those from the referenceimage. However, their solution requires passing the reference image through the denoising networkfor multiple timesteps, which increases computation and limits its feasibility for extending to multiplereference images. In contrast, we propose an efficient CacheKV mechanism that leverages multiplereference images by eliminating the redundant network passes.",
  "The proposed ReF-LDM model": "In this section, we present the proposed ReF-LDM model. We introduce the network architecture inSec. 3.1, where a CacheKV mechanism is designed to leverage reference images. We illustrate howto train our model with the timestep-scaled identity loss in Sec. 3.2. : The proposed ReF-LDM pipeline. Our model accepts a low-quality image and multiplehigh-quality reference images as input and generates a high-quality image. The blue top panel alonerepresents a typical LDM denoising process. For an LQ image xLQ, we concatenate its latent zLQwith zt along the channel axis to serve as the input for the denoising U-net. For the reference images{xref}, we design a CacheKV mechanism, depicted in the red panel, to extract and cache their keyand value tokens using the same denoising U-net for just one time. These cached KV tokens can thenbe utlized repeatedly in each of the T timesteps of the main denoising process. During training, weadopt the classic LDM loss (LLDM) and introduce a timestep-scaled identity loss (Ltime ID).",
  "Preliminaries on Latent Diffusion Model": "To generate an image, an image diffusion model starts from a noisy image xT RHW 3,initialized with a Gaussian distribution, and gradually denoises it to a clean image x0 with a denoisingnetwork over T timesteps. A latent diffusion model operates similarly, but the diffusion processtakes place in a more compact latent space of a pre-trained and frozen autoencoder (encoder E anddecoder D). That is, it begins with a random latent zT RHzWzCz and progressively denoises itto a clean latent z0. A clean image is then generated by passing the clean latent through the decoderduring the inference phase, i.e., x0 = D(z0); conversely, a ground truth clean latent is obtained byencoding a clean image with the encoder during the training phase, i.e., z0 = E(x0). A typical choicefor the denoising network is a U-net with self-attention layers at multiple scales.",
  "CacheKV: a mechanism for incorporating reference images": "As illustrated in , our ReF-LDM leverages an input LQ image xLQ and multiple reference images{xref} to generate a target HQ image xHQ. For an LQ image, we simply concatenate its latent encodedby the frozen encoder, zLQ = E(xLQ), with the diffusion denoising latent zt along the channel axis toserve as the input of the denoising U-net. For reference images, we design a CacheKV mechanism.Essentially, we extract and cache the features of reference images using the same denoising U-netjust once; these cached features can then be used repeatedly at each of the T timesteps in the maindenoising process. Specifically, we pass the encoded latent of each reference image, zref = E(xref),through the U-net to extract their keys and values (KVs) at each self-attention layer and store themin a CacheKV. Subsequently, during the main diffusion process, within each self-attention layer ofthe U-net, we concatenate the reference KVs (from the corresponding self-attention layer) with themain KVs along the token axis. This mechanism enables the U-net to incorporate the additional KVsfrom the reference images into the main denoising process. When extracting KVs from the referenceimages, we use a timestep embedding of t = 0 and pad zref with a zero tensor to accommodate theadditional channels introduced for the LQ image. To summarize, for inference, we first run the U-net once to extract CacheKV from the referenceimages; subsequently, we proceed through the main denoising process for T timesteps, during whichthe U-net integrates zLQ and reference CacheKV. For training, in each iteration, we first run the U-netto extract CacheKV, and then we run the U-net again to estimate the target latent from a samplednoisy latent zt, incorporating the conditions zLQ and reference CacheKV.",
  "Comparing CacheKV with other designs": "There are other intuitive designs for integrating the reference latents {zref} into the diffusion denoisingprocess. However, they are either ineffective or computationally inefficient compared to the proposedCacheKV. The quantitative evaluation and computational analysis is reported in Sec. 5.2.1. We depictthese designs in and provide an intuitive explanation as follows: Channel-concatenation: Concatenating the condition with zt along the channel axis workswell for LQ images (and for other 2D conditions such as semantic maps ); however, it is noteffective for reference images. A critical difference between these conditions is thatwhile theLQ image is spatially aligned with the target HQ image, the reference images are not. Therefore,it is challenging for the model to leverage reference images using simple channel-concatenation. Cross-attention: Cross-attention layers have been proven useful for text conditions in text-to-image models . In our ablation experiment, we insert a cross-attention layer after eachself-attention layer and use the reference latents {zref} to produce keys and values. Whilecross-attention appears to have the potential to address the spatial misalignment problem, it stillfails to effectively utilize the reference images. The difference between our CacheKV and thecross-attention setting is that CacheKV provides the reference images in a more aligned featurespace for the main denoising process to leverage. Specifically, the CacheKV is extracted usingthe same U-net and the corresponding self-attention layer as in the main denoising process. In",
  "contrast, the cross-attention setting processes the reference images only with the frozen encoder,resulting in features that are less aligned with those in the U-net of the denoising process": "Spatial-concatenation: Concatenating {zref} with zt along the spatial dimension to serveas the input for U-net also effectively leverages the reference images. Conceptually, spatial-concatenation treats reference images in a very similar way to our CacheKV. In both mechanisms,{zref} are processed through the denoising U-net, allowing the reference KVs to be accessedby the queries (Qs) of the main diffusion latent zt. However, spatial-concatenation requiressignificantly more computational resources compared to our CacheKV. It passes {zref} with ztto the U-net at each of the T denoising timesteps, whereas CacheKV only passes {zref} throughthe U-net once. Moreover, spatial-concatenation also requires significantly more GPU memory,as the spatial size of the input for the U-net increases with the number of reference images.As for a self-attention layer in the U-net, both mechanisms increase memory usage; CacheKVintroduces additional reference KVs, while spatial-concatenation introduces reference QKVs.",
  "Timestep-scaled identity loss": "To validate the benefits of the proposed timestep-scaled identity loss, we train ReF-LDM with threedifferent loss settings: without identity loss (LLDM), with naive identity loss (LLDM + LID), andwith the proposed timestep-scaled identity loss (LLDM + Ltime ID). As show in and ,while the naive identity loss can improve identity similarity (IDS), our timestep-scaled identity losscan do so without sacrificing the image quality (NIQE). As explained in Sec. 3.2, we employ t to scale down the identity loss for a larger and noisiertimestep t. In , we compare this design choice with other alternative scaling factors, 1t<100and 1t<500, which apply the identity loss only when the sampled timestep t is smaller than 100 or500, respectively. The results suggest that t is more effective than the alternatives.",
  "Training ReF-LDM with timestep-scaled identity loss": "We train our ReF-LDM with the classic LDM loss and the proposed timestep-scaled identity loss:Ltotal = LLDM + time ID Ltime ID(3)Recall that the denoising U-net estimates the target latent in the latent space of the frozen autoencoder,and a typical LLDM is computed as the L1 distance between the estimated latent and the target latent.To compute the identity loss with the face recognition model, which accepts an image as input, wedecode the estimated latent into the image space using the frozen decoder, i.e, x0 = D(z0). Theexperiments in Sec. 5.2.2 show that timestep-scaled identity loss can improve face similarity withoutdegrading image quality, unlike the naive usage of identity loss.",
  "FFHQ-Ref dataset": "Recent works for non-reference-based face restoration commonly train their models with FFHQdataset , which comprises 70,000 high-quality face images of wide appearance variety withappropriate licenses crawled from Flickr. These images are not provided with reference labelsoriginally; however, we find that a good portion of the images are of the same identities. Thus,we construct a reference-based datasetFFHQ-Refbased on the FFHQ dataset, with carefulconsideration described as follows.",
  "Finding reference images of the same identity": "To determine whether two images belong to the same identity, we utilize the face recognitionmodel ArcFace . Specifically, we first extract the 1D ArcFace embeddings for all images. Then,for each image, we compute the cosine distances between its embedding and the embeddings ofall other images. A distance less than a threshold r = 0.4 indicates that the images are validreferences belonging to the same person. Following this procedure, we identify 20,405 images withcorresponding reference images.",
  "Splitting data according to identity": "To enable the FFHQ-Ref dataset to serve as both training and evaluation datasets for reference-basedface restoration models, we divide the images into train, validation, and test splits. However, randomdata splitting may result in the train and test splits containing images of the same individual, which isnot ideal for a fair evaluation. To ensure that all images of a single identity are assigned to only onedata split, we group the images based on their identities. Specifically, we consider identity groupingas a graph problem, where each image acts as a vertex and any pair of images with a distance lessthan r are connected by edges. We then apply the connected component algorithm from graph theory,where each connected component represents a group of images belonging to the same person. Finally,we identified 6,523 identities and divided them into three splits: a train split with 18,816 images of6,073 identities, a validation split with 732 images of 300 identities , and a test split with 857 imagesof 150 identities. We report more statistics in Appendix C.",
  "Constructing evaluation dataset with practical considerations": "Practical ConsiderationsFor a fair and meaningful evaluation, the input reference images shouldnot be excessively similar to the target image; hence, we set a minimum cosine distance thresholdof 0.1 for the test set. Additionally, we manually check the images in the test split to verify that allreference images indeed correspond to the same identity. Furthermore, in the context of reference-based face restoration applications, it is preferable to select input reference images that capture a morecomprehensive representation of a persons appearance, such as varying face poses or expressions.Although a target image in the test split of our FFHQ-Ref may have two to nine reference images,different reference-based methods may have their own constraints on the maximum number of inputreference images. To emulate a more representative set of reference images, we sort all availablereference images of a target image using farthest point sampling on the ArcFace distance.",
  "Comparison between FFHQ-Ref and existing datasets": "summarizes the differences between our proposed FFHQ-Ref and existing datasets. While theCelebRef-HQ dataset has been constructed to train and evaluate reference-based face restorationmodels, our FFHQ-Ref dataset contains twice as many images and six times the number of identitiescompared to CelebRef-HQ. Moreover, built upon FFHQ , FFHQ-Ref provides superior imagequality over CelebRef-HQ, as indicated by the lower NIQE score (3.68 vs. 3.97). Some ground-truth images in CelebRef-HQ are affected by watermarks and mirror padding artifacts, as shown inAppendix B.",
  "Implementation details": "To exploit more ground truth images without available reference images, we use 68,411 imagesin the FFHQ dataset to train a VQGAN as the frozen autoencoder and an LDM with only LQcondition. We then finetune our ReF-LDM from the LQ-conditioned LDM with the 18,816 images inour FFHQ-Ref dataset. All models are trained excluding the test split images to ensure fair evaluationon our FFHQ-Ref benchmark. In our experiments, we adopt a 512x512 image resolution, fix thenumber of reference images to five, and set loss scale time ID to 0.1. During training, we synthesizeinput LQ images with , r, , and q sampled from , , , and , respectively.For inference, we use 100 DDIM steps and a classifier-free-guidance with a scale of 1.5towards reference images. We provides more implementation details in the Appendix G.",
  "Evaluation datasets and metrics": "For evaluation datasets, we use the test split of our FFHQ-Ref with two different degradationlevels: severe and moderate. In addition, previous non-reference-based methods commonly useCelebA-Test for evaluation, which comprises 3,000 LQ and HQ image pairs sampled from theCelebA-HQ dataset . Therefore, we follow the same procedures described in Sec. 4 to constructa subset of 2,533 images with available reference images, termed CelebA-Test-Ref.",
  "For evaluation metrics, we adopt the identity similarity (IDS) , which is the cosine similaritycalculated using the face recognition model ArcFace . We also use the widely used perceptual": "metrics LPIPS . As face pixels are more of concern in the task of face restoration, we alsomeasure the face-region LPIPS (fLPIPS), which is the LPIPS calculated using only the pixels in faceregions. For assessing no-reference image quality, we adopt NIQE . Furthermore, we measurethe FID , using 70,000 images from the FFHQ dataset as the target distribution.",
  "CacheKV and other mechanisms": "The CacheKV is proposed for integrating the input reference images into the diffusion denoisingprocess. We compare it with other mechanisms illustrated in Sec. 3.1.3. According to ,channel-concatenation and cross-attention fail to leverage reference images to improve the identitysimilarity (IDS). In contrast, both spatial-concatenation and our CacheKV significantly enhance IDS.Moreover, our CacheKV is more computationally efficient than spatial-concatenation, requiring only20% of the inference time and 39% of the GPU memory.",
  "Multiple input reference images": "There are two to nine reference images for a target image in the test split of our FFHQ-Ref. Whilewe fix the number of reference images to five when training ReF-LDM, the proposed CacheKVmechanism has the flexibility to take varying number of reference images during inference. Tovalidate the effectiveness of utilizing multiple reference images, we evaluate ReF-LDM with amaximum of 1, 3, 5, and 8 reference images, respectively. As shown in , using more referenceimages significantly improves the identity similarity (from 0.52 to 0.66). However, increasing thenumber of reference images also increases the computation time, as shown in . Since usingeight reference images encounters an out-of-memory issue on a single GTX 1080, we use at mostfive reference images in our experiments for simplicity.",
  "Quantitative comparison": "We compare our ReF-LDM with state-of-the-art methods on FFHQ-Ref-Severe, FFHQ-Ref-Moderate,and CelebA-Test-Ref. reports the performance of competing methods in terms of IDS, fLPIPS,LPIPS, and FID (targeting the FFHQ image distribution). Without the information in reference images,the existing non-reference-based restoration methods (CodeFormer , VQFR , and DAEFR )fail to preserve the facial identity, leading to significantly lower IDS. The reference-based method,DMDNet , fails to restore the severely degraded images because it depends on unreliable faciallandmark detection, reflected by higher fLPIPS. In contrast, our ReF-LDM consistently outperformsDMDNet on identity similarity and other metrics, owing to the proposed CacheKV mechanism andtimestep-scaled identity loss, which effectively leverage the input reference images without the needfor landmark detection. We also note that our method exhibits slightly inferior results in LPIPS metric.This is due to the difference in the background pixels, we provide further details in the Appendix D.It is also worth mentioning that the competing methods benefit from data leakage on the FFHQ-Refbenchmarks, as their models are trained with the entire FFHQ dataset or with a different train splitthan the identity-based one in the proposed FFHQ-Ref.",
  "Qualitative comparison": "In , we present a qualitative comparison between our ReF-LDM, the pre-trained LDM with-out reference images, CodeFomer (a SOTA non-reference-based method), and DMDNet (a SOTAreference-based method). Given the severely degraded image, DMDNet generates distorted faceimages based on incorrectly detected landmarks. While CodeFormer yields realistic face images, itdoes not preserve the facial identity well. In contrast, our ReF-LDM produces results that are bothrealistic and faithful to the individuals facial identity.",
  "Limitations": "When the face region is occluded by other objects, our model may generate artifacts. For certainface poses (e.g, side face), the reconstructed eyes may appear unnatural. These problems are alsocommonly observed in other methods and might be caused due to the lack of such training images.However, there are some examples showing that these problems can be alleviated if our model isprovided reference images with similar face poses to the target image. Visual examples of theselimitations are provided in Appendix F.",
  "Conclusion": "In summary, we propose ReF-LDM, which incorporates the CacheKV mechanism and the timestep-scaled identity loss, to effectively utilize multiple reference images for face restoration. Additionally,we construct the FFHQ-Ref dataset, which surpasses the existing dataset in both quantity and quality,to facilitate the research in reference-based face restoration. Evaluation results demonstrate thatReF-LDM achieves superior performance in face identity similarity over state-of-the-art methods. AcknowledgmentsThe authors wish to express their gratitude to Professor Wei-Chen Chiu for hisvaluable suggestion to exclude the reference images that are too similar to the targetimages when constructing the proposed FFHQ-Ref dataset. M. Cao, X. Wang, Z. Qi, Y. Shan, X. Qie, and Y. Zheng. Masactrl: Tuning-freemutual self-attention control for consistent image synthesis and editing. InProceedings of the IEEE/CVF International Conference on Computer Vision(ICCV), pages 2256022570, October 2023. C. Chen, X. Li, L. Yang, X. Lin, L. Zhang, and K.-Y. K. Wong. Progressivesemantic-aware style transformation for blind face restoration. In Proceedingsof the IEEE/CVF conference on computer vision and pattern recognition, pages1189611905, 2021.",
  "J. Deng, J. Guo, X. Niannan, and S. Zafeiriou. Arcface: Additive angularmargin loss for deep face recognition. In CVPR, 2019": "P. Esser, R. Rombach, and B. Ommer. Taming transformers for high-resolutionimage synthesis. In Proceedings of the IEEE/CVF conference on computervision and pattern recognition, pages 1287312883, 2021. Y. Gu, X. Wang, L. Xie, C. Dong, G. Li, Y. Shan, and M.-M. Cheng. Vqfr:Blind face restoration with vector-quantized dictionary and parallel decoder. InEuropean Conference on Computer Vision, pages 126143. Springer, 2022. M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter. Ganstrained by a two time-scale update rule converge to a local nash equilibrium.Advances in neural information processing systems, 30, 2017.",
  "J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. Ad-vances in neural information processing systems, 33:68406851, 2020": "R. Huang, S. Zhang, T. Li, and R. He. Beyond face rotation: Global and localperception gan for photorealistic and identity preserving frontal view synthesis.In Proceedings of the IEEE international conference on computer vision, pages24392448, 2017. K. Karkkainen and J. Joo. Fairface: Face attribute dataset for balanced race,gender, and age for bias measurement and mitigation. In Proceedings ofthe IEEE/CVF winter conference on applications of computer vision, pages15481558, 2021.",
  "T. Karras, T. Aila, S. Laine, and J. Lehtinen. Progressive growing of gans forimproved quality, stability, and variation. arXiv preprint arXiv:1710.10196,2017": "T. Karras, S. Laine, and T. Aila. A style-based generator architecture forgenerative adversarial networks. In Proceedings of the IEEE/CVF conferenceon computer vision and pattern recognition, pages 44014410, 2019. Y.-F. Lau, T. Zhang, Z. Rao, and Q. Chen. Ented: Enhanced neural textureextraction and distribution for reference-based blind face restoration. In Pro-ceedings of the IEEE/CVF Winter Conference on Applications of ComputerVision, pages 51625171, 2024.",
  "Y. Nitzan, K. Aberman, Q. He, O. Liba, M. Yarom, Y. Gandelsman, I. Mosseri,Y. Pritch, and D. Cohen-Or. Mystyle: A personalized generative prior. ACMTransactions on Graphics (TOG), 41(6):110, 2022": "S. Pouyanfar, S. Sengupta, M. Mohammadi, E. Abraham, B. Bloomquist,L. Dauterman, A. Parikh, S. Lim, and E. Sommerlade. Frr-net: A real-timeblind face restoration and relighting network. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition, pages 12401250,2023. E. Richardson, Y. Alaluf, O. Patashnik, Y. Nitzan, Y. Azar, S. Shapiro, andD. Cohen-Or.Encoding in style: a stylegan encoder for image-to-imagetranslation. In Proceedings of the IEEE/CVF conference on computer visionand pattern recognition, pages 22872296, 2021. R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer.High-resolution image synthesis with latent diffusion models. In Proceedings ofthe IEEE/CVF conference on computer vision and pattern recognition, pages1068410695, 2022. C. Saharia, J. Ho, W. Chan, T. Salimans, D. J. Fleet, and M. Norouzi. Imagesuper-resolution via iterative refinement. IEEE transactions on pattern analysisand machine intelligence, 45(4):47134726, 2022.",
  "In , we plot the distribution of the number of available reference images": "Furthermore, we assess the race, age, and gender distributions of the dataset usinglabels predicted by FairFace . As depicted in , the race distribution withinFFHQ-Ref is imbalanced, with a predominance of the white category. To mitigatethis, we intentionally sampled a greater number of images from other races toconstruct a more balanced test set. Additionally, as illustrated in , FFHQ-Refencompasses a broad age range, from infants (0-2 years) to the elderly (70+ years).However, the distribution is not uniform across ages and genders. For example, thereis a notably higher proportion of young females (29.2% of 20-29 female).",
  "EExamples of illumination change": "shows an example where ReF-LDM exhibits warmer illumination comparedto LDM. We conjecture that this may be due to the impact of the strong warmlighting in the input reference images. To address this issue, one could employpost-processing tricks, such as adjusting the means of the R, G, B channels to matchthose of the input LQ image. Another potential solution might be training ReF-LDMwith data augmentation on the illuminations of input reference images, to encouragethe model to disregard the illuminations of input references and maintain consistencywith that of the input LQ image.",
  "FExamples of failure cases": "Here we provide visual examples for the limitation described in Sec. 6. As shown, when the face region is occluded, our ReF-LDM and the prior models tendto generate from unnatural artifacts. For side face images, the ReF-LDM may notwork well when the input reference images do not contain faces of similar pose,as shown in . However, suggests that our ReF-LDM can effectivelyexploit the reference images of similar face poses to improve the results.",
  "G.2Data augmentation for input reference images": "During the training phase, we use a fixed number of five input reference images.When a target images with less than five reference images are sampled, we repeatthe reference images to obtain five reference images. In addition, we apply imageaugmentation to the input reference images with the following operations: colorjitter (brightness 0.2, contrast 0.2, saturation 0.2, hue 0.02), affine transform(rotation 2, translation 0.05, scale 0.05), perspective transform (scale 0.2,probability 0.5), and horizontal flip (probability 0.5). Lastly, we randomly shuffle theorder of available reference images for a target image, so that a different combinationof reference images can be sampled at each training iteration. In , we providean example where a set of two reference images is augmented to a set of fivereference images.",
  "G.3Training details": "We trained the VQGAN for 200,000 iterations with batch size 32 on four A6000GPUs for 7 days. We trained the LDM with only LQ condition for 500,000 iterationswith batch size 40 on four A6000 GPUs for 7 days. We finetuned the ReF-LDM for150,000 iterations with batch size 8 on four 3090 GPUs for 6 days. For training losses,the LDM is trained using only the typical LDM loss LLDM, while the ReF-LDM istrained with both LLDM and the proposed Ltime ID."
}