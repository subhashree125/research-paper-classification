{
  "Abstract": "The widespread adoption of machine learning in scientific research has created afundamental tension between model opacity and scientific understanding. Whilstsome advocate for intrinsically interpretable models, we introduce ComputationalInterpretabilism (CI) as a philosophical framework for post-hoc interpretability inscientific AI. Drawing parallels with human expertise, where post-hoc rationalisa-tion coexists with reliable performance, CI establishes that scientific knowledgeemerges through structured model interpretation when properly bounded by em-pirical validation. Through mediated understanding and bounded factivity, wedemonstrate how post-hoc methods achieve epistemically justified insights withoutrequiring complete mechanical transparency, resolving tensions between modelcomplexity and scientific comprehension.",
  "Introduction": "The increasing adoption of machine learning (ML) in scientific research has created a fundamentaltension between the opacity of complex ML models and the need for scientific understanding. Al-though ML models achieve unprecedented predictive performance across various scientific domains,from protein structure prediction to climate modelling, their complexity often renders them epistemi-cally opaque, resistant to direct human understanding. This opacity presents a significant challengeto scientific practice, which traditionally relies on clear theoretical understanding and explanatorypower. The challenge has led to two dominant approaches in the ML community. One advocates for intrin-sically interpretable models [Rudin, 2019], arguing that high-stakes (social) scientific applicationsdemand transparent reasoning processes. The other develops post-hoc interpretability methods thatattempt to explain already-trained complex models. This latter approach, while pragmatically valu-able, has faced mounting scepticism: recent empirical studies have revealed limitations in currentpost-hoc methods [Hooker et al., 2019, Adebayo et al., 2022, Bilodeau et al., 2024], raising deeperphilosophical questions about their epistemic justification. In response to this tension, particularly regarding the epistemic status of post-hoc interpretabilitymethods in scientific AI applications, we introduce Computational Interpretabilism (CI), a philo-sophical stance that acknowledges that while complete, factive explanations of complex AI systemsmight be currently unattainable or pragmatically limiting, we can achieve epistemically justifiedunderstanding within bounded approximations. This argument rests on two key principles: (1)scientific knowledge can emerge through structured interpretation of model behaviour, even withoutdirect access to the models internal mechanisms; and (2) approximative explanations can provideepistemically justified scientific insights when we carefully define their limitations and verify theirreliability through empirical testing.",
  "On Interpretability": "Human expertise has long been characterised by a distinctive gap between performance and \"factive\"explanation, particularly in domains where intuitive decision-making plays a central role [Dreyfus,1972, Dreyfus and Dreyfus, 1986, Kahneman, 2003, Gobet and Chassy, 2008, 2009, Gobet, 2012].Experts across domains routinely make effective decisions while their subsequent explanations ofteninvolve post-hoc rationalisation rather than complete, factive accounts of their decision-makingprocesses [Bilalic et al., 2008a,b]. This disjunction between performance and explanation is not a flaw but a fundamental feature ofexpertise, reflecting how complex knowledge is encoded and deployed in human cognition. Ouracceptance of this epistemological limitation in human expertise, when contrasted with our scepticismtoward similarly limited post-hoc explanations, raises fundamental questions about interpretabilityin scientific AI systems. Can explanation and performance be epistemically decoupled withoutcompromising the justification of either? What can the limitations of human expertise teach us aboutepistemically justifying post-hoc interpretability methods? These questions challenge us to reconsidernot just the technical aspects of post-hoc models, but the fundamental nature of explanation andunderstanding in scientific practice.",
  "Before exploring these implications further, it is essential to establish key definitions and assumptionsto frame our discussion. We begin by precisely defining what we mean by \"interpretability\"": "Interpretability in AI systems is fundamentally pluralistic [Zednik, 2021], encompassing multipledistinct concepts and serving diverse stakeholder needs. As Lipton and Beisbart and Rz emphasise, interpretability is not a monolithic concept but rather reflects different expectations,questions, and explanatory virtues depending on the stakeholder and context. Different stakeholders approach interpretability with distinct questions and needs. Computer scientiststypically focus on understanding how inputs are mechanistically processed to produce outputs whatwe might call mechanistic interpretability [Nanda et al., 2023]. In contrast, domain experts in scientificfields often seek to understand how model outputs inform or align with real-world phenomena whatwe term phenomenological interpretability. While these perspectives are distinct, they are arguablyreciprocal. As Ali et al. suggest, mechanistic interpretability often serves as a prerequisite formeaningful explanation, while the process of developing explanations can provide insights into themodels functioning. For scientific applications, which are the focus of this paper, interpretability takes on additionaldimensions beyond just understanding model mechanics. It encompasses our epistemic capacity tounderstand and articulate how AI systems generate insights about natural phenomena in ways thatadvance scientific understanding. This broader conception is particularly important when consideringpost-hoc interpretability methods, whose criticisms often stem from evaluating them solely ontheir ability to faithfully explain a models learned function (h(X)) through some interpretableapproximation (p(X)). However, in scientific contexts, the key relationship is not just betweenh(X) and p(X), but how both relate to the underlying natural phenomenon f(X) being studied.(A detailed visualisation and explanation of these functions and their relationships is providedin Appendix A.) This three-way relationship suggests that post-hoc methods can serve a vitalepistemological role: bridging between model computations and scientific understanding of naturalphenomena, even when they may not perfectly capture all details of model behaviour. To furthercontextualise our discussion, we establish several key assumptions: 1. Accessibility of AI Systems: When discussing post-hoc explanations, we concentrate on openand accessible black-box algorithms, rather than proprietary systems. The primary challenge inunderstanding these algorithms stems from their inherent complexity, rather than a complete lack ofknowledge. 2. Scientific AI Models: Our analysis centres on supervised learning models designed to aid in scienceor knowledge discovery, such as predictive models in scientific research. We deliberately excludediscussion of interpretability in generative models, as they present distinct challenges beyond ourcurrent scope. 3. Imperfect but Meaningful Approximations: We assume post-hoc methods provide approximationswith imperfect but non-trivial fidelity to the original model. By this, we explicitly exclude methodsthat perform no better than random attribution (e.g., Hooker et al. , Bilodeau et al. ), focusing instead on approaches that, while not perfect, demonstrably capture meaningful patterns inmodel behaviour. While these approximations may not capture all nuances of model behaviour, theymaintain sufficient accuracy to provide meaningful insights about both the model and the underlyingphenomenon.",
  "On Reliability and Justifiability": "The parallel between human expertise and AI interpretability becomes particularly instructive whenexamining reliability and epistemic justifiability. Traditional epistemology distinguishes betweentwo forms of justification [Pappas, 2005]: internalist (requiring accessible reasons) and externalist(focusing on reliable processes). This framework illuminates how both human expertise and post-hocAI interpretability can achieve epistemic justification despite limited explicit articulation. Human experts, too, primarily provide externalist justification. Their expertise stems from extensiveexperience and training, which develops incomprehensible (yet sophisticated) cognitive processesconnecting features of their current experience with vast stores of domain-specific knowledge [Dreyfusand Dreyfus, 1984, 1986, Kahneman and Klein, 2009, Gobet, 2012]. Just as we trust our visual systemwithout understanding its neural mechanisms, we accept expert judgment based on demonstratedreliability rather than complete explanation. Studies by Bilalic et al. [2008a,b] reveal that expertsattention patterns often diverge from their reported reasoning, suggesting their explicit explanationsare post-hoc reconstructions rather than precise accounts of their decision processes. This recognition of post-hoc rationalisation in human expertise reshapes our perspective on AIinterpretability: if we accept human expert judgments despite their post-hoc nature, we mightsimilarly justify post-hoc AI interpretability methods when they demonstrate reliable knowledgegeneration. The key is not perfect mechanistic transparency but rather reliable processes for generatingand validating insights. Moreover, this parallel suggests that requiring complete internalist justification (full explicit expla-nation) from AI systems may be not just practically challenging but philosophically unnecessary(e.g., also see Sullivan ). While intrinsic interpretability aligns with internalist approachesby emphasising direct access to reasoning processes, the success of human expertise suggests thismay be unnecessarily restrictive. Just as human expertise explains implicit pattern recognition withexplicit domain knowledge, AI systems might achieve scientific explanation through a combinationof complex pattern recognition and post-hoc methods that reliably connect these patterns to domainknowledge. This framework sets the stage for Computational Interpretabilism, which will show how post-hocmethods can achieve epistemic justification through bounded factivity and mediated understanding,even when their explanations are approximative. The key insight is that justification emerges notfrom complete mechanistic transparency but from reliable processes of knowledge generation andvalidation a principle that applies equally to human expertise and AI systems.",
  "Computational Interpretabilism": "Computational Interpretabilism (CI) emerges as a philosophical framework that addresses one ofthe most pressing challenges in modern artificial intelligence: how to epistemically justify theuse of post-hoc interpretability methods for scientific discovery. While machine learning modelshave demonstrated remarkable capabilities in analysing complex phenomena, their opacity raisesfundamental questions about their role in generating scientific knowledge. CI provides a systematicapproach to resolving this tension by establishing that post-hoc explanations can contribute legitimatescientific insights when they operate within specific epistemological boundaries [Beisbart and Rz,2022]. The framework synthesises multiple philosophical perspectives, including Sullivans linkuncertainty, Andrews theory-laden understanding, and Freiesleben et al.s holistic representationality,to demonstrate how approximative explanations can bridge the gap between model complexity andscientific understanding. A broader treatment of philosophical perspectives on AI interpretability,including discussions beyond these frameworks, is provided in Appendix C.",
  "Philosophical Foundations": "The epistemological relationship between machine learning models and scientific understandingemerges as a complex interplay of multiple philosophical challenges and frameworks, particularlyin justifying post-hoc interpretability methods for scientific inquiry. Freiesleben et al.s distinctionbetween elementwise representationality (ER) and holistic representationality (HR) provides anessential foundation1 for understanding how modern ML models differ from traditional scientificmodels. This distinction gains deeper significance when viewed through multiple philosophicallenses: Andrews theory-ladenness, Beisbart and Rzs factivity dilemma, Sullivans link uncertainty,and fundamental principles from philosophy of science such as epistemic accessibility [Longino,1990, Kitcher, 2001] and intersubjective verifiability2 [Kuhn, 1997, Popper, 2005]. These perspectivestogether suggest that understanding emerges through mediated interpretation rather than direct accessto model mechanics. The shift from ER to HR that Freiesleben et al. propose can be understood as a practical responseto the factivity dilemma rather than attempting to maintain perfect fidelity at the componentlevel (which would make models incomprehensible), HR acknowledges the necessity of holisticinterpretation while providing rigorous methods for doing so through property descriptors. Thisapproach aligns with Bilodeau et al.s empirical findings that task-specific interpretability methodsoutperform general-purpose approaches, suggesting that effective scientific understanding requirestargeted methods tailored to specific research contexts. However, as Andrews reminds us, thesemethods are inevitably theory-laden the very choice of property descriptors and their implementationreflects our theoretical understanding and assumptions about both the phenomenon and the model.This theory-ladenness aligns with Douglass (2009) recognition that scientific inquiry is inherentlyvalue-laden, requiring careful consideration of how interpretability methods mediate between modelbehaviour and human scientific understanding. Sullivans concept of link uncertainty provides a crucial bridge in understanding how post-hoc inter-pretability methods can be epistemically justified. These methods dont eliminate link uncertainty,but rather help manage it by mediating between model behaviour and phenomenal understandingthrough rigorous, testable connections. This connects to Poppers principle of falsifiability inter-pretability methods must generate scrutinisable and potentially falsifiable claims about both modelbehaviour and phenomena. Beyond mere validation or falsification of existing knowledge, thesemethods can advance scientific understanding by revealing novel patterns and relationships that mightnot be apparent through traditional scientific approaches. The four-step framework proposed byFreiesleben et al. (formalisation, identification, estimation, and uncertainty quantification) can beseen as a systematic approach to reducing link uncertainty while acknowledging the theory-ladennature of scientific practice. Importantly, this framework demonstrates how post-hoc methods can beepistemically justified through their role in mediating between model behaviour, empirical validation,and scientific understanding. The challenge of justifying post-hoc interpretability methods isnt just a technical problem but afundamental epistemological challenge that requires careful attention to how scientific understandingemerges through mediated interpretation. Drawing on pragmatist philosophy [Putnam, 1995], wemust recognise that interpretabilitys epistemic value lies in how it mediates between ML systemsand human scientific understanding, facilitating oversight and integration with existing knowledgesystems. Post-hoc interpretability methods emerge not just as technical tools for validation butas epistemological interfaces that actively participate in knowledge falsification and expansion,capable of generating new scientific insights while maintaining scientific rigour. They must makeML-generated knowledge accessible and comprehensible to the scientific community while enablingcollaborative verification and critique, fundamentally shaping how we understand both ML systemsand the phenomena they study. Building on these philosophical foundations, CI establishes two key principles that together justifypost-hoc interpretability methods in scientific ML. Mediated understanding reveals how scientificknowledge emerges through structured interactions between models, methods, and domain knowledge,while bounded factivity demonstrates why such mediated processes can be epistemically validdespite their approximative nature. Together, these principles provide a comprehensive philosophical 1The key difference is that CI sees interpretability methods not just as tools for extraction but as activeparticipants in knowledge creation. This leads to the concept of \"mediated understanding\".2That scientific claims should be verifiable by multiple observers.",
  "Mediated Understanding": "Scientific understanding through machine learning emerges not through direct model interpretation,but through a complex process of mediated interaction. The concept of \"mediated understanding\" inCI describes how scientific knowledge emerges through the structured interaction between four keyelements: model behaviour, interpretability methods, domain knowledge, and empirical validation.This principle recognises that scientific understanding through ML is inherently mediated directaccess to model mechanics is not necessary for scientific insight [Sullivan, 2022, Beisbart and Rz,2022]. Instead, understanding emerges through a dialectical process of interpretation, where therelationship between model understanding and phenomenal understanding is reciprocally constitutive. The epistemic validity of post-hoc methods in CI stems from their role as structured mediators in abidirectional knowledge-creation process. In one direction (Model Phenomenon), interpretabilitymethods reveal patterns in model behaviour, which then tentatively suggest hypotheses about phe-nomena. These hypotheses, when tested empirically, provide new phenomenal understanding. Inthe other direction (Phenomenon Model), domain knowledge guides the selection and refinementof interpretability methods, while empirical validation helps refine our interpretive approaches andidentify relevant model behaviours for investigation. This bidirectional mediation provides epistemicjustification because it ensures that interpretability methods are not merely describing model be-haviour, but are actively participating in a cycle of hypothesis generation, empirical validation, andknowledge refinement the very essence of scientific inquiry. Consider a medical diagnosis ML system as an illustrative example [Sullivan, 2022]. Featureattribution methods serve as epistemic mediators by translating opaque model computations intotestable hypotheses about biological mechanisms. When these model-derived insights are empiricallyvalidated against existing scientific evidence, they contribute to medical knowledge not despite theirpost-hoc nature, but precisely because their mediated interpretation enables systematic comparisonbetween model behaviour and real-world phenomena. This demonstrates how CIs concept ofmediated understanding resolves the apparent conflict between post-hoc interpretation and scientificvalidity the very process of mediation, when properly structured and validated, becomes a legitimatesource of scientific knowledge. A detailed examination of how mediated understanding operatesdifferently across theory-rich versus theory-poor contexts is provided in Appendix B.",
  "Bounded Factivity": "Building on our discussion of mediated understanding, which shows how scientific knowledgeemerges through structured interactions between models and methods, we now turn to another keyconcept that supports CIs defense of post-hoc interpretability: bounded factivity. This concepthelps resolve fundamental tensions between the approximative nature of post-hoc methods and theirepistemic value for scientific understanding. In philosophy of science, the relationship between factivity and scientific understanding has beenextensively debated. Traditional accounts often assume that genuine understanding requires strictlyfactual, true beliefs. However, recent work in philosophy of science has highlighted how idealisationand approximation play essential roles in scientific practice [Beisbart and Rz, 2022, Sullivan, 2022,Freiesleben et al., 2024]. Scientists routinely use simplified models that deliberately deviate fromreality to gain understanding of complex phenomena. These non-factive elements, rather than beingmere compromises, often prove essential for scientific progress. This recognition of strategic simplifications role in science helps us reconceptualise the epistemicstatus of post-hoc interpretability methods. Rather than demanding complete factivity perfect corre-spondence between interpretation and model mechanics CI advocates for what we term \"boundedfactivity\": truth within explicitly acknowledged limits and simplifications. Recent empirical workunderscores the importance of this bounded approach. While Bilodeau et al. demonstratedthat many popular post-hoc methods perform no better than random attribution, the authors alsoshowed that carefully designed, task-specific approaches can provide reliable insights. By aligninginterpretability methods with specific scientific goals [Freiesleben et al., 2024] and validating them",
  "through systematic empirical testing, we can achieve meaningful understanding within acknowledgedbounds, just as traditional scientific models advance understanding despite their simplifications": "The concept of bounded factivity finds a natural parallel in Herbert Simons bounded rationality[Simon, 1957, Wheeler, 2018] both frameworks acknowledge inherent limitations while affirmingthe validity of strategic simplification. Just as bounded rationality accepts \"satisficing\" solutions undercognitive constraints, bounded factivity embraces carefully bounded approximations in interpretation.This pragmatic orientation helps justify post-hoc methods by showing how they can advance scientificunderstanding even without achieving perfect fidelity. These parallels have important philosophical implications for how we justify post-hoc interpretabilityin scientific ML. Just as bounded rationality challenged idealised notions of human decision-makingwhile preserving the validity of human judgment, bounded factivity challenges the assumption thatperfect explanations are necessary for genuine scientific understanding while defending the epistemicvalue of post-hoc interpretations. It suggests that valid scientific understanding emerges throughthe careful management of trade-offs between accuracy and comprehensibility, realised throughthe process of mediated understanding where model behaviour, interpretive methods, and empiricalvalidation interact cyclically. This mediated process, operating within explicitly acknowledgedbounds, enables post-hoc methods to generate reliable scientific insights by systematically refiningboth our interpretation of models and our understanding of phenomena precisely what well-designedpost-hoc methods aim to achieve.",
  "Approximation and Fidelity": "Rudin and Ghassemi et al. argue that post-hoc explanations are problematic due to theirapproximative nature, this critique necessitates careful examination of distinct but related concerns:the factivity of explanations and the nature of understanding in scientific practice. The dilemmapresents itself thus: completely accurate explanations of complex ML models would merely duplicatetheir opacity, while simplified explanations necessarily introduce some degree of falsehood. Thisapparent tension can be productively addressed through the lens of non-factive understanding inscience [Beisbart and Rz, 2022]. Interpretability exists on a spectrum, with increased epistemic value and practical utility correlatingwith higher degrees of interpretability. This approach aligns with the concept of verisimilitude,where approximations to truth, though imperfect, retain epistemic worth [Oddie, 2001]. Althoughpost-hoc explanations lack performance guarantees and do not fully capture model behaviour, thislimitation need not compromise their epistemological value if we maintain awareness of departure[Kvanvig, 2009] conscious recognition of where and how our explanations diverge from groundtruth. Many scientific and analytical tools rely on strategic idealisations that, despite their non-factivenature, provide valuable insights and practical utility. The key is maintaining empirical accountabilitythrough testable predictions, situating approximations within relevant theoretical frameworks, andproviding clear scope conditions for the validity of interpretations. This perspective aligns with CIs broader commitment to epistemic accessibility while acknowledgingthat accessibility often requires trade-offs with complete accuracy. Just as scientific models generallyinvolve idealisations that technically violate factivity without compromising their utility for under-standing, post-hoc explanations can provide genuine scientific insight even while containing strategicsimplifications. This suggests that the key to maintaining scientific rigour lies not in perfect factivity,but in transparent acknowledgment of simplifications coupled with continuous refinement throughempirical validation. The relationship between model, interpretation, and reality can be illuminated through Korzybskisdictum, \"The map is not the territory\". Just as a map is a simplified representation of reality,both intrinsically interpretable models and post-hoc explanations are simplifications of the complexsystems they represent. Accepting an intrinsically interpretable model as \"understandable\" and havingsome fidelity to the real world is philosophically analogous to accepting a post-hoc explanation that is\"understandable\" and has some fidelity to the original model. The fidelity between complex systems(real world or AI) and any model (intrinsic or post-hoc) is inherently imperfect, yet this imperfectiondoes not negate their scientific value when properly bounded and validated.",
  "Faithful Explanation and Confirmation Bias": "Rudin identifies two potential pitfalls of post-hoc models incomplete (local) explanations andunjustifiable explanations. The critique of local explanations underestimates their unique epistemolog-ical value in scientific practice. Rather than viewing local explanations as merely incomplete versionsof global understanding, CI recognises them as distinct epistemic tools that offer granular insightsinto model behaviour. Through mediated understanding, these local insights can generate testablehypotheses about both model behaviour and phenomenal relationships, identify edge cases that revealimportant patterns, and expose nuances that global explanations might miss. When properly boundedand validated, local explanations complement rather than compete with global understanding. Regarding unjustifiable explanations, CI posits that even apparently problematic model behaviours such as scientifically unsound judgments or confounding variables can advance scientific under-standing when properly interpreted. Through bounded factivity, we recognise that identifying flawsin model reasoning contributes valuable knowledge about both model limitations and phenomenalcomplexity. This aligns with how sciences historically progress through understanding both positiveand negative results. Ghassemi et al. raise a complementary concern about confirmation bias in interpreting post-hoc explanations, suggesting that humans might draw overconfident conclusions from potentiallyunreliable interpretations. This \"interpretability gap\" could potentially foster false confidence inthe models reliability or fairness. The limitations Ghassemi et al. describe are not unique to AIexplanations but are inherent in complex judgements, whether human or artificial. Human experts, like AI systems, can fall prey to confirmation bias, potentially leading to overcon-fidence in their interpretations or explanations. Following this reasoning to its logical conclusion,one might argue that we should be equally sceptical of human expert explanations as we are ofAI-generated ones. Taken to an extreme, this line of thinking could lead to an argument for minimis-ing reliance on expert explanations altogether, whether human or AI-generated. Instead, one mightadvocate for sole reliance on predefined, explicit \"if-then\" rules, aiming to eliminate the subjectivityand potential biases inherent in both human and AI interpretations. Yet, this conclusion overlooks thevalue of both human and AI-generated post-hoc explanations. Rather than suggesting we should abandon post-hoc explanations in favor of purely rule-basedapproaches, CI advocates for their refinement and systematic validation. Just as sciences havedeveloped methods for managing human cognitive biases while preserving the value of expert insight,we can develop approaches to post-hoc interpretation that acknowledge limitations while maximisingepistemic value. This calls the fields attention to validation procedures and explicit acknowledgmentof bounds.",
  "Conclusion": "Attempts to interpret AI models, particularly through post-hoc explanations, are analogous to humanexperts translating their intuitive impressions into deliberate explanations. Just as human expertsoften provide post-hoc rationalisations for their judgments, post-hoc explainability methods seek tomake sense of the complex, compiled knowledge within AI. This process is inherently imperfect, ashuman experts also rarely access the full breadth of their internalised chunks and productions [Newelland Simon, 1972, Simon and Chase, 1973, Gobet and Clarkson, 2004]. However, this parallel revealsinstructive differences in how we can approach AI interpretation. Unlike the black box of humancognition, AI systems permit systematic investigation of their internal states, allowing us to preciselyspecify the bounds of factivity and empirically verify our approximations. This accessibility enablesa more rigorous approach to post-hoc interpretation than is possible with human expertise. The epistemological framework of Computational Interpretabilism suggests that post-hoc inter-pretability methods serve a crucial mediating function in scientific ML, analogous to how expertexplanations bridge specialised knowledge and broader understanding. When medical experts trans-late complex diagnoses for patients, they necessarily simplify their understanding into comprehensibleexplanations. Similarly, post-hoc interpretability methods transform opaque model computations intoscientifically meaningful insights. The epistemic value lies not in perfect mechanical reproduction ofthe underlying process, but in reliable knowledge generation that can be validated against empiricalevidence and integrated with domain expertise. Julius Adebayo, Michael Muelly, Harold Abelson, and Been Kim. Post hoc explanations may be ineffective fordetecting unknown spurious correlation. In International conference on learning representations, 2022. Sajid Ali, Tamer Abuhmed, Shaker El-Sappagh, Khan Muhammad, Jose M Alonso-Moral, Roberto Confalonieri,Riccardo Guidotti, Javier Del Ser, Natalia Daz-Rodrguez, and Francisco Herrera. Explainable artificialintelligence (xai): What we know and what is left to attain trustworthy artificial intelligence. Informationfusion, 99:101805, 2023.",
  ": A comparative analysis of intrinsically interpretable and post-hoc explainable AI models": "This diagram illustrates the key functional relationships in both intrinsically interpretable and post-hoc explain-able AI models within scientific contexts. The framework is grounded in three fundamental functions: thetrue phenomenon f(X), the learned model h(X), and for post-hoc methods, the interpretable approximationp(X). In both cases, we begin with a true function f(X) representing the natural phenomenon of interest. The modellearns an approximation h(X) within a hypothesis space H, where H represents the set of possible modelbehaviours {h1, ..., hn}. For this analysis, we assume ideal conditions where the training data accuratelyrepresents the population distribution, allowing us to focus on the core relationships between these functions.",
  "In post-hoc models, an additional function p(X) is learned from a space of possible explanations Pto approximate h(X)s behaviour in an interpretable way": "The learning paths (shown in red for intrinsic and blue for post-hoc) illustrate how each approach navigates thetension between model capability and human comprehensibility. While intrinsically interpretable models main-tain a direct connection to human understanding, post-hoc methods introduce a secondary layer of interpretationthat can potentially capture more complex relationships between the model and the underlying phenomenon.",
  "BTheoretical Contexts and Epistemic Value in Machine LearningInterpretation": "While Andrews demonstrates how theoretical considerations play essential roles throughout the MLpipeline in cases like AlphaFold from data engineering to architecture design and model evaluation notall AI models for science are built with this same level of theoretical rigour. CI expands this understandingby demonstrating that post-hoc interpretability methods can be epistemically justified across the full spectrumof theoretical contexts through structured mediated understanding. This broadens the scope of scientific MLbeyond just theory-rich applications. In theory-rich cases like AlphaFold, CI shows how post-hoc interpretability methods [Tan and Zhang, 2023]derive their epistemic validity from their role in a theoretically-grounded mediation process. They help validatewhether models implement intended theoretical principles, enable scientists to verify if learned representationsalign with established theories, support discovery of potential new theoretical insights, and provide meansto examine whether theoretical assumptions embedded in model design function as intended. The mediationprocess here acts as a bridge between explicit theoretical commitments and model behaviour, with interpretabilitymethods serving as epistemically-justified tools for both validation and discovery precisely because they operatewithin a well-defined theoretical framework. Even more significantly, CI demonstrates how post-hoc methods maintain epistemic validity in theory-poorcontexts where ML models are applied with minimal theoretical consideration. Here, the mediation processtakes on a different but equally valid epistemic role: helping uncover implicit theoretical assumptions embeddedin data selection and preprocessing, providing ways to retrospectively examine model behaviour against domainknowledge, revealing potential biases or problematic patterns, and identifying opportunities for theoreticalincorporation. Through mediated understanding, these methods create an epistemically rigorous path toreconstruct and validate theoretical frameworks that might have been overlooked in model development effectively building theoretical bridges where none previously existed. CI thus resolves a key epistemological challenge in machine learning interpretation: how to justify post-hocinterpretability methods across varying levels of theoretical grounding. The framework demonstrates that throughproperly structured mediation processes, these methods maintain epistemic validity whether they are validatingexisting theoretical frameworks or helping construct new ones. In both cases, post-hoc interpretability methodshold epistemic value because they enable systematic scientific scrutiny of how these models operate within theirdomains, regardless of how deliberately theory was incorporated into their development. This unified treatmentof theory-rich and theory-poor contexts represents a significant advance in our understanding of how ML cancontribute to scientific knowledge across diverse applications.",
  "C.1Sullivans (2022) Link Uncertainty": "According to Sullivan, the relationship between explanation and understanding in complex models hingescritically on the concept of \"link uncertainty\" the gap between a models theoretical predictions and empiricalreality. Sullivan argues that while models can be epistemically opaque (meaning their internal workings arenot fully transparent), this opacity does not necessarily prevent them from providing genuine understanding,provided there is sufficient empirical evidence connecting the model to real-world phenomena. In other words,we do not necessarily need to fully understand how a model works internally; what matters more is understandinghow the model connects to the real-world system it is studying. Sullivan identifies three distinct types of explanatory questions we can ask about models: how the modelitself works, how-possibly questions about potential mechanisms, and why/how-actually questions about real-world phenomena. How-possibly explanations demonstrate potential mechanisms or causes, showing howsomething could theoretically occur. However, these fall short of explaining how things actually work in reality.Using Schellings segregation model as an example, Sullivan shows that while the model can demonstratehow segregation could emerge from individual preferences, it only provides genuine understanding if there isempirical evidence showing these mechanisms actually operate in real-world segregation patterns. The decisive factor in moving from how-possibly to how-actually explanations is reducing link uncertaintythrough scientific evidence. This evidence must connect the models theoretical insights to actual causalmechanisms in the target phenomenon. Importantly, Sullivan argues that understanding does not necessarilyrequire complete knowledge of how a model works internally. Instead, what matters is the strength whetherit be the amount, kind or quality of scientific and empirical evidence connecting the models predictions orinsights to real-world phenomena.",
  "C.2Andrews (2023) Theory-ladenness of Machine Learning": "The debate over Machine Learnings impact on science has generated what scholars call the \"distinctness claim\".The claims core argument articulated by several philosophers like Boge, Sreckovic et al., and Boon is thatML, particularly deep learning, represents a fundamental departure from traditional scientific methods. Theyprimarily base this on two key distinctions: (1) ML methods are supposedly \"theory-free\" or \"theory-agnostic\",operating without prior theoretical assumptions or conceptualisations of target phenomena, and (2) ML modelsprioritise prediction over explanation and understanding, making them epistemically opaque in novel ways. Thisperspective has gained significant traction not only in philosophical discourse but also among scientists andengineers who view ML as fundamentally different from traditional scientific approaches.",
  "Extending Leonelli, Andrews fundamentally challenges this perspective with the theory-laden nature of scientificdata and practice:": "Even the most simplistic of experimental designs reveals the nature and extent to whichdata, and scientific practice at large, are theory-laden. The very act of investigation involvescommitment to the existence and in-principle measurability of some phenomenon and, if weare making measurements and performing quantitative analyses thereon, commitment to itsquantitative nature...Measurement cannot be total, and therefore there is always a commitment as to what to lookat experimentally and what to exclude. There is always a commitment to the appropriatelevel of abstraction at which to study the phenomenon in play in terms of such things asinstrument settings like degree of magnification or periodicity of sampling. The very designof our instruments of measure and their calibration includes various commitments to thenature of the worldly phenomena under investigation. [Andrews, 2023, pp. 6] This understanding of datas theory-laden nature is now widely accepted in philosophy of science. However, asLeonelli notes, unfortunate relics of this view viewing data as mere empirical input for modelling remainwidespread. This persistent misconception underlies many arguments about MLs theory-free nature. Thereality is that all scientific data, whether used in traditional methods or ML, necessarily involves theoreticalassumptions and conceptual frameworks in its collection, preparation, and interpretation. This view challengesthe technological determinism implicit in many discussions of ML in science the belief that certain effectsor limitations of ML are fixed, inevitable consequences of the technology itself. Rather than accepting currentlimitations as inherent features, Andrews argues we should recognise them as methodological challenges thatcan be addressed through improved practices and understanding. Building on this theoretical foundation, Andrews demonstrates how the impossibility of \"theory-free\" learningis established by both philosophy of science and theoretical computer sciences understanding of inductivegeneralisation. At its core, machine learning performs inductive inference - extrapolating from limited instances to general cases. Drawing on Nortons material theory of induction, Andrews notes that successful inductiveinference never proceeds through universal, domain-generic formal rules, but rather requires the applicationof local rules warranted by empirical facts specific to each research domain. This philosophical insight findsindependent confirmation in computer science through the No Free Lunch theorems, which mathematicallydemonstrate the impossibility of universal domain-generic inference rules. While these theorems were derived inspecific formal settings, their implications for ML practice are profound: inductive inference fundamentally re-quires domain-specific inductive biases. This convergence of philosophical and mathematical results underminesclaims about MLs theory-independence. These insights reveal the \"distinctness claim\" as fundamentally misguided. Rather than representing a rev-olutionary break from traditional scientific methods, ML should be understood as a new set of tools whoseproper application still requires theoretical understanding and methodological rigour. This perspective suggestsa more nuanced approach to ML in science: one that acknowledges how theoretical considerations may enterdifferently in ML workflows, while recognising their essential role in ensuring sound scientific practice. Such anunderstanding is crucial for developing appropriate methodological standards for ML in science, rather thanaccepting current limitations as inevitable features of the technology.",
  "C.3Beisbart and Rzs (2022) Factivity Dilemma": "The factivity dilemma in understanding Deep Neural Networks (DNNs) centers on a fundamental tensionbetween accuracy and comprehensibility. The principle of factivity demands that explanations and understandingbe grounded in facts, yet modern DNNs have become so complex that we can only comprehend them throughsimplifications and idealisations. As Rudin (2019) pointedly argues, a perfectly accurate explanation wouldsimply duplicate the original models complexity, defeating the purpose of explanation. This creates whatappears to be an insurmountable challenge: explanations must either sacrifice accuracy for comprehensibility ormaintain accuracy at the cost of being unusable. This tension has deep roots in the philosophy of science, particularly in debates about the relationship betweenexplanation and understanding. Traditional accounts of scientific explanation, such as the Deductive-Nomologicalmodel, typically require factivity - the premises in an explanation must be true. However, the requirements forscientific understanding are more nuanced. Non-factivists like Elgin argue that simplified models can providelegitimate understanding despite imperfect accuracy, while factivists such as Lawler maintain that simplificationsare merely instruments toward understanding rather than constituting understanding itself. These opposing viewsreflect a broader debate about whether understanding necessarily requires truth or can be achieved through usefulapproximations. A potential resolution emerges when we distinguish between mechanistic interpretability and scientific under-standing in the context of DNNs. While mechanistic interpretability aims for factual explanations of modelbehaviour, scientific understanding of phenomena through post-hoc interpretative models may not require thesame level of factivity. This distinction suggests that while complete, accurate explanations remain an importantgoal, we can develop meaningful understanding through carefully constructed simplified models. The key liesin maintaining awareness of these models limitations while leveraging their insights - acknowledging themas useful approximations rather than complete representations of reality. This approach offers a practical wayforward, recognising both the current constraints in explaining DNNs and the necessity of working with thesesystems, even with imperfect understanding.",
  "C.4Freiesleben et al.s (2024) Holistic Representationality": "Freiesleben et al. address a fundamental challenge in modern scientific research: how to derive meaningful scien-tific insights from machine learning models that, unlike traditional scientific models, lack direct interpretability oftheir components. Traditional scientific models followed what the authors call \"elementwise representationality\"(ER), where each model component whether parameters, variables, or relationships directly representedsomething meaningful about the phenomenon being studied. For instance, in a simple physics model, mass andvelocity parameters directly correspond to physical properties. However, modern ML models, particularly neuralnetworks, do not offer this kind of straightforward interpretation - their individual components (like networkweights) do not map clearly to real-world phenomena (e.g., see Freiesleben ). Rather than viewing this as a limitation, the authors propose a framework based on \"holistic representationality\"(HR). Instead of trying to interpret individual components, they suggest analysing the models behaviour as awhole through what they call \"property descriptors\" (e.g., cPDP, cFI, SAGE, and PRIM for global property, andICE, cSV, ICI and Counterfactuals for local property, see pp.21-25 for further details). This approach aligns withrecent findings from Bilodeau et al. , who demonstrate that generic feature attribution methods can beunreliable for inferring model behaviour, but task-specific approaches can dramatically improve interpretability.While Freiesleben et al. provide a theoretical framework, Bilodeau et al. offer practical evidence of itsimportance, showing how domain-specific interpretability methods (e.g., perturbation) can be more reliable thangeneral-purpose approaches like SHAP or Integrated Gradients. The authors provide a systematic four-step framework for this approach (pp. 14-20): first, formalising thescientific question as a statistical query, which involves translating research questions into precise mathematicalformulations; second, identifying how to answer it using the whole model through property descriptors thatare continuous functions mapping from model space to answer space; third, estimating the answer usingthe trained model, which requires careful consideration of data distribution and model behavior; and fourth,quantifying the uncertainty in the results through both model error (difference between optimal and trained modelpredictions) and estimation error (uncertainty in the property descriptor estimates themselves). This frameworkis particularly notable for its rigorous treatment of uncertainty quantification, which is often overlooked intraditional interpretable ML approaches. The paper demonstrates the practical applicability of this framework by showing how existing interpretableML methods can serve as property descriptors. Using a concrete example of analysing student academicperformance, they illustrate how these methods can provide scientifically meaningful insights while maintainingrigorous standards of inference. The authors emphasise that while this approach differs from traditional scientificmodelling, it does not sacrifice scientific rigour it simply provides a different path to extracting knowledgefrom our models, one thats better suited to the capabilities and limitations of modern machine learning systems.This conclusion resonates with Bilodeau et al.s findings that success in model interpretation often dependson carefully defining concrete end-tasks and developing targeted evaluation methods rather than relying ongeneral-purpose interpretation tools.",
  "C.5Lazars (2024) Democratic Duties of Explanation": "Lazars central contribution to AI explainability discourse stems from his recognition that computational systems,especially AI, are increasingly being used to \"govern\" us that is, to settle, implement, and enforce the normsthat determine how institutions function. When computational systems are deployed by government agenciesin administrative functions or by private companies to police online behaviour and determine our informationaccess, they are effectively governing us. For such governing power to be legitimate, Lazar argues, it must beaccountable to democratic oversight through public explanation to the community as a whole. Unlike approachesfocused on individual rights or technical transparency, Lazar emphasises that explainability is fundamentally ademocratic duty it is not about individual decision subjects understanding their particular outcomes, but aboutenabling the collective community to determine whether these computational governance systems are beingused legitimately and with proper authority. Lazar argues that this collective explainability requirement hasspecific implications for computational governance systems: they must reveal not just their decision rules, butalso demonstrate the appropriateness of their training data as evidence, the robustness of their decision-makingprocesses, and their ability to make the right decisions for the right reasons.",
  "C.6Vredenburghs (2022) Informed Self-advocacy": "Vredenburghs central contribution addresses the fundamental tension between algorithmic opacity and individualrights. Rather than demanding complete technical transparency of complex AI models, she argues for a claimright to explanations that can be provided post-hoc, grounded in what she calls \"informed self-advocacy\" acluster of abilities that allows individuals to represent their interests and values to decision-makers and furtherthose interests within institutions. This right becomes particularly crucial in institutions where algorithmicdecisions significantly impact individuals lives. Vredenburgh argues that post-hoc explanations must take two specific forms: rule-based normative explanations(explaining why a decision was appropriate) and rule-based causal explanations (explaining how inputs relate tooutputs). She advocates for \"functional transparency\" high-level explanations of how inputs relate to outputs rather than structural or run transparency of the underlying model (pp. 13). While acknowledging that simplifiedexplanations of complex algorithms may be somewhat inaccurate, she argues they can still be sufficient forinformed self-advocacy if properly calibrated to stakes: when decisions distribute harms or entitlements (versusbenefits), there are stronger requirements for clear explanations and human expert support. This pragmaticframework shows how post-hoc explanations, even if they do not fully capture the complexity of AI systems,can satisfy legitimate needs for accountability while remaining feasibly implementable, as evidenced by existinglegal requirements for explanation across various domains.",
  "C.7Durns (2023) Computational Realibilism": "The central motivation of justification in DNNs is primarily driven by their inherent methodological and epistemicopacity [Humphreys, 2009]. This opacity manifests in two distinct yet interrelated ways [Durn, 2023]. First,the algorithmic complexity of DNN systems encompassing myriad functions, variables, decisions, and data renders it impossible for any individual or group to fully comprehend which elements are pivotal in generating aspecific output. Second, this complexity imposes cognitive limitations on human agents, hindering our ability toderive meaningful interpretations of the algorithm and its results. Both aspects of opacity potentially undermine",
  "the justificatory basis for ascribing scientific value to DNN outputs, either due to the \"black-box\" nature of thesystem or the cognitive constraints of human interpreters": "Durans computational reliabilism (CR) addresses this epistemic challenge by proposing a framework forjustifying belief in DNN outputs if and when they are produced by reliable belief-forming methods [Durn andFormanek, 2018, Durn, 2023, Javed et al., 2023]. CR delineates three categories of reliability indicators: (i)Technical Robustness of Algorithms, encompassing the design, implementation, and maintenance factors thatcontribute to a DNN systems robustness; (ii) Computer-based Scientific Practice, which involves the algorithmicimplementation of scientific theories and principles, or expert assessment within established scientific knowledge;and (iii) Social Construction of Reliability, referring to the socially mediated processes that confer acceptanceof DNN and its outputs across diverse communities. At its core, CR adopts a frequentist approach, positingthat beliefs formed by demonstrably reliable algorithms warrant greater justification than those produced byunreliable ones."
}