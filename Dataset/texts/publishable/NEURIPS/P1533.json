{
  "Abstract": "Assistive agents should make humans lives easier. Classically, such assistanceis studied through the lens of inverse reinforcement learning, where an assistiveagent (e.g., a chatbot, a robot) infers a humans intention and then selects actionsto help the human reach that goal. This approach requires inferring intentions,which can be difficult in high-dimensional settings. We build upon prior workthat studies assistance through the lens of empowerment: an assistive agent aimsto maximize the influence of the humans actions such that they exert a greatercontrol over the environmental outcomes and can solve tasks in fewer steps. Welift the major limitation of prior work in this area scalability to high-dimensionalsettings with contrastive successor representations. We formally prove that theserepresentations estimate a similar notion of empowerment to that studied by priorwork and provide a ready-made mechanism for optimizing it. Empirically, ourproposed method outperforms prior methods on synthetic benchmarks, and scalesto Overcooked, a cooperative game setting. Theoretically, our work connects ideasfrom information theory, neuroscience, and reinforcement learning, and charts apath for representations to play a critical role in solving assistive problems.1",
  "Introduction": "AI agents deployed in the real world should be helpful to humans. When we know the utilityfunction of the humans an agent could interact with, we can directly train assistive agents throughreinforcement learning with the known human objective as the agents reward. In practice, agentsrarely have direct access to a scalar reward corresponding to human preferences (if such a consistentmodel even exists) , and must infer them from human behavior . This inference can bechallenging, as humans may act suboptimally with respect to their stated goals, not know theirgoals, or have changing preferences . Optimizing a misspecified reward function can have poorconsequences . An alternative paradigm for assistance is to train agents that are intrinsically motivated to assisthumans, rather than directly optimizing a model of their preferences. An analogy can be drawn toa parent raising a child. A good parent will empower the child to make impactful decisions andflourish, rather than proscribing an optimal outcome for the child. Likewise, AI agents mightseek to empower the human agents they interact with, maximizing their capacity to change theenvironment . In practice, concrete notions of empowerment can be difficult to optimize as anobjective, requiring extensive modeling assumptions that dont scale well to the high-dimensionalsettings deep reinforcement learning agents are deployed in.",
  "Empowerment": "Caption: We propose an algorithm training assistive agents to empower human users the assistant should take actions that enable human users to visit a wide range of future states, and the human's actions should exert a high degree of influence over the future outcomes. Our algorithm scales to high-dimensional settings, opening the door to building assistive agents that need not directly reason about human intentions.",
  "Without EmpowermentEmpowerment": ": We propose an algorithm training assistive agents to empower human usersthe assistantshould take actions that enable human users to visit a wide range of future states, and the humansactions should exert a high degree of influence over the future outcomes. Our algorithm scales tohigh-dimensional settings, opening the door to building assistive agents that need not directly reasonabout human intentions. environment. This approach only requires one structural assumption: the AI agent is interacting withan environment where there is a notion of actions taken by the human agenta more general settingthan the case where we model the human actions as the outcome of some optimization procedure, asin inverse RL or preference-based RL . Prior work has studied many effective objectives for empowerment. For instance, Du et al. approximates human empowerment as the variance in the final states of random rollouts. Despiteexcellent results in certain settings, this approach can be challenging to scale to higher dimensionalsettings, and does not necessarily enable human users to achieve the goals the want to achieve. Bycontrast, our approach exclusively empowers the human with respect to the distribution of (useful)behaviors induced by their current policy, and can be implemented through a simple objective derivedfrom contrastive successor features, which can then be optimized with scalable deep reinforcementlearning (). We provide a theoretical framework connecting our objective to prior work onempowerment and goal inference, and empirically show that agents trained with this objective canassist humans in the Overcooked environment as well as the obstacle gridworld assistancebenchmark proposed by Du et al. . Our core contribution is a novel objective for training agents that are intrinsically motivated to assisthumans without requiring a model of the humans reward function. Our objective, Empowerment viaSuccessor Representations (ESR), maximizes the influence of the humans actions on the environment,and, unlike past approaches for assistance without reward inference, is based on a scalable model-freeobjective that can be derived from learned successor features that encode which states the human islikely to want to reach given their current action. Our objective empowers the human to reach thedesired states, not all states, without assuming a human model. We analyze this objective in termsof empowerment and goal inference, drawing novel mathematical connections between time-seriesrepresentations, decision-making, and assistance. We empirically show that agents trained with ourobjective can assist humans in two benchmarks proposed by past work: the Overcooked environment and an obstacle-avoidance gridworld .",
  "Assistive Agents.There are two lines of past work on assistive agents that are most relevant": "The first line of work focuses on the setting of an assistance game , where a robot (AI) agenttries to optimize a human reward of which it is initially unaware. Practically, inverse reinforcementlearning (IRL) can be used in such a setting to infer the humans reward function and assist thehuman in achieving their goals . The key challenge with this approach is that it requires modelingthe humans reward function. This can be difficult in practice, especially if the humans behavioris not well-modeled by the reward architecture. Slightly mispecified reward functions can lead tocatastrophic outcomes (i.e., directly harmful behavior in the assistance context) . By contrast,our approach does not require modeling the humans reward function. The second line of work focuses on empowerment-like objectives for assistance and shared autonomy.Empowerment generally refers to a measure of an agents ability to influence the environment .In the context of assistance, Du et al. show one such approximation of empowerment (AvE) canbe approximated in simple environments through random rollouts to assist humans. Meanwhile,empowerment-like objectives have been used in shared autonomy settings to assist humans withteleoperation and general assistive interfaces . A key limitation of these approaches forgeneral assistance is they only model empowerment over one time step. Our approach enables a morescalable notion of empowerment that can be computed over multiple time steps. Intrinsic Motivation.Intrinsic motivation broadly refers to agents that accomplish behaviors in theabsence of an externally-specified reward or task . Common applications of intrinsic motivationin single-agent reinforcement learning include exploration and skill discovery , empowerment, and surprise minimization . When applied to settings with humans, theseobjectives may lead to antisocial behavior . Our approach applies intrinsic motivation to the settingof assisting humans, where the agents goal is an empowerment objectiveto maximize the humansability to change the environment. Information-theoretic Decision Making.Information-theoretic approaches have seen broad appli-cability across unsupervised reinforcement learning . These methods have been appliedto goal-reaching , skill discovery , and exploration . In the contextof assisting humans, information-theoretic methods have primarily been used to reason about thehumans goals or rewards . Our approach is made possible by advances in contrastive representation learning for efficientestimation of the mutual information of sequence data . While these methods have been widelyused for representation learning and reinforcement learning , to the best of ourknowledge prior work has not used these contrastive techniques for learning assistive agents.",
  "Preliminaries": "Formally, we adapt the notation of Hadfield-Menell et al. , and assume a robot (R) and human(H) policy are training together in an MDP M = (S, AH, AR, R, P, ). The states s consist of thejoint states of the robot and the human; we do not have separate observations for the human and robot.At any state s S, the robot policy selects actions distributed according to R(aR | s) for aR ARand the human selects actions from H(aH | s) for aH AH. The transition dynamics are definedby a distribution P(s | s, aH, aR) over the next state s S given the current state s S and actionsaH AH and aR AR, as well as an initial state distribution P(s0). For notational convenience,we will additionally define random variables st to represent the state at time t, and aRt R( | st)and aHt H( | st) to represent the human and robot actions at time t, respectively. Empowerment.Our work builds on a long line of prior methods that use information theoreticobjectives for RL. Specifically, we adopt empowerment as an objective for training an assistiveagent . This section provides the mathematical foundations for empowerment, as developed in prior work. Our work will build on the prior work by (1) providing an information geometricinterpretation of what empowerment does (.3) and (2) providing a scalable algorithm forestimating and optimizing empowerment, going well beyond the gridworlds studied in prior work. The idea behind empowerment is to think about the changes that an agent can effect on a world; anagent is more empowered if it can effect a larger degree of change over future outcomes. Followingprior work , we measure empowerment by looking at how much the actions taken nowaffect outcomes in the future. An agent with a high degree of empowerment exerts a high degree ofcontrol of the future states by simply changing the actions taken now. Like prior work, we measurethis degree of control through the mutual information I(s+; aH) between the current action aH andthe future states s+. Note that these future states might occur many time steps into the future. Empowerment depends on several factors: the environment dynamics, the choice of future actions,the current state, and other agents in the environment. Different problem settings involve maximizingempowerment using these different factors. In this work, we study the setting where a human agentand a robot agent collaborate in an environment; the robot will aim to maximize the empowerment ofthe human. This problem setting was introduced in prior work . Compared with other mathematicalframeworks for learning assistive agents , framing the problem in terms of empowerment meansthat the assistive agent need not infer the humans underlying intention, an inference problem that istypically challenging . We now define our objective. To do this, we introduce random variable s+, which corresponds to astate sampled K Geom(1 ) steps into the future under the behavior policies H and R. Wewill use (s+ | st) to denote the density of this random variable; this density is sometimes referred toas the discounted state occupancy measure. We will use mutual information to measure how muchthe action at at time t changes this distribution:",
  "(b) Mutual information(c) Maximizing empowerment": ": The Information Geometry of Empowerment, illustrating the analysis in .3.(Left) For a given state st and assistant policy R, we plot the distribution over future states for 6choices of the human policy H. In a 3-state MDP, we can represent each policy as a vector lying onthe 2-dimensional probability simplex. We refer to the set of all possible state distributions as thestate marginal polytope. (Center) Mutual information corresponds to the distance between the centerof the polytope and the vertices that are maximally far away. (Right) Empowerment corresponds tomaximizing the size of this polytope. For example, when an assistive agent moves an obstacle out ofa human users way, the human user can spend more time at desired state.",
  "Intuition and Geometry of Empowerment": "Intuitively, the assistive agent should aim to maximize the number of future outcomes. We willmathematically quantify this in terms of the discounted state occupancy measure, (s+ | s).Intuitively, an agent has a large empowerment if the future states for one action are very differentfrom the future actions after taking a different action; i.e., when (at = a1; s+ | st) is quite differentfrom (at | s2; s+ | st) for actions a1 = a2. The mutual information (Eq. (1)) quantifies this degreeof control: I(at; s+ | st). One way of understanding this mutual information is through information geometry .For a fixed current state st, assistant policy R and human policy H, each potential action at thatthe human takes induces a different distribution over future states: R,H(s+ | st, at). We canthink about the set of these possible distributions: {R,H(s+ | st, at) | at A}. (Left)visualizes this distribution on a probability simplex for 6 choices of action at. If we look at anypossible distribution over actions, then this set of possible future distributions becomes a polytope(see orange polygon in (Center)). Intuitively, the mutual information I(at; s+ | st) used to define our empowerment objective corre-sponds to the size or volume of this state marginal polytope. This intuition can be formalized byusing results from information geometry . The human policy H(at | st) places probabilitymass on the different points in (Center). Maximizing the mutual information correspondsto picking out the state distributions that are maximally spread apart (see probabilities in (Center)). To make this formal, define",
  "I(at; s+ | st) = maxat DKL(at; s+ | st) (s+ | st) dmax.(4)": "This distance is visualized as the black lines in . When we talk about the size of the statemarginal polytope, we are specifically referring to the length of these black lines (as measured with aKL divergence). This sort of mutual information is a way for measuring the degree of control that an agent exertson an environment. This measure is well defined for any agent/policy; that agent need not bemaximizing mutual information, and could instead be maximizing some arbitrary reward function.This point is important in our setting: this means that the assistive agent can estimate and maximizethe empowerment of the human user without having to infer what reward function the human is tryingto maximize. Finally, we come back to our empowerment objective, which is a discounted sum of the mutualinformation terms that we have been analyzing above. This empowerment objective says that thehuman is more empowered when this set has a larger size i.e., the human can visit a wider range offuture state (distributions). The empowerment objective says that the assistive agent should act to tryto maximize the size of this polytope. Importantly, this maximization problem is done sequentially:the assistive agent wants the size of this polytope to be large both at the current state and at futurestates; the humans actions should exert a high degree of influence over the future outcomes both nowand in the future. Thus, our overall objective looks at a sum of these mutual informations.",
  "Relating Empowerment to Reward": "In this section we take aim at the question: when humans are well-modeled as optimizing a rewardfunction, when does maximizing empowerment help humans maximize their rewards? Answeringthis question is important because for empowerment to be a safe and effective assistive objective,it should enable the human to better achieve their goals. We show that under certain assumptions,",
  "empowerment yields a provable lower bound on the average-case reward achieved by the human forsuffiently long-horizon empowerment (i.e., 1)": "For constructing the formal bound, we suppose the human is Boltzmann-rational with respectto some reward function R R and rationality coefficient . The distribution R could be interpretedas a prior over the humans objective, a set of skills the human may try and carry out, or a populationof humans with different objectives that the agent could be interacting with. Our quantity of interest,the average-case reward achieved by the human with our empowerment objective, is given by",
  "This notation is formalized in Appendix B": "The two key assumptions used in our analysis are Assumption 3.1, which states that the human willoptimize for behaviors that uniformly cover the state space, and Assumption 3.2, which simply statesthat with infinite time, the human will be able to reach any state in the state space.Assumption 3.1 (Skill Coverage). The rewards R R are uniformly distributed over the scaled|S|-simplex |S| such that:R +1|S|1",
  "PH,R(s+ = s | s0) > 0for all s S, (0, 1).(8)": "Our main theoretical result is Theorem 3.1, which shows that under these assumptions, maximizingempowerment yields a lower bound on the (squared) average-case reward achieved by the human forsufficiently large . In other words, for a sufficiently long empowerment horizon, the empowermentobjective Eq. (2) is a meaningful proxy for reward maximization.Theorem 3.1. Under Assumption 3.1 and Assumption 3.2, for sufficiently large and any > 0,",
  "Estimating and Maximizing Empowerment with ContrastiveRepresentations": "Directly computing Eq. (2) would require access to the human policy, which we dont have. There-fore, we want a tractable estimation that still performs well in large environments which are moredifficult to model due to the exponentially increasing set of possible future states. To better-estimateempowerment, we learn contrastive representations that encode information about which future statesare likely to be reached from the current state. These contrastive representations learn to modelmutual information between the current state, action, and future state, which we then use to computethe empowerment objective.",
  "To estimate this empowerment objective, we need a way of learning the probability ratio inside theexpectation. Prior methods such as Du et al. and Salge et al. rollout possible future states": "and compute a measure of their variance as a proxy for empowerment, however this doesnt scalewhen the environment becomes complex. Other methods learn a dynamics model, which also doesntscale when dynamics become challenging to model . Modeling these probabilities directly ischallenging in settings with high-dimensional states, so we opt for an indirect approach. Specifically,we will learn representations that encode two probability ratios. Then, we will be able to compute thedesired probability ratio by combining these other probability ratios.",
  "Our method learns three representations:": "1. (s, aR, aH) This representation can be understood as a sort of latent-space model,predicting the future representation given the current state s and the humans current actionaH as well as the robots current action aR. 2. (s, aR) This representation can be understood as an uncontrolled model, predictingthe representation of a future state without reference to the current human action aH. Thisrepresentation is analogous to a value function.",
  "C1": "Note that the expected value of the first term is the conditional mutual information I(st+K; aH | s).Our empowerment objective corresponds to averaging this mutual information across all the visitedstates. In other words, our objective corresponds to an RL problem, where empowerment correspondsto the expected discounted sum of these log ratios:",
  "Input: Human policy H(a | s)Randomly initialize assistive agent policy R(a | s), and representations (s, aR, aH), (s, aT ), and (g).Initialize replay buffer B.while not converged do": "Collect a trajectory of experience with human policy and assistive agent policy, store in replay buffer B.Update representations (s, aR, aH), (s, aT ), and (g) with the contrastive losses in Eq. (10).Update R(a | s) with RL using reward function r(s, aR, aH) = ((s, aR, aH) (s, aR))T (g).",
  "Algorithm Summary": "We propose an actor-critic method for learning the assistive agent. Our method will alternate betweenupdating these contrastive representations and using them to estimate a reward function (Eq. (13))that is optimized via RL. We summarize the algorithm in Algorithm 1. In practice, we use SAC as our RL algorithm. In our experiments, we will also study the setting where the human user updatestheir policy alongside the assistive agent.",
  "ESR-NormGoal InferenceReward Inference": ": We apply our method to the benchmark proposed in prior work , visualized in a.The four subplots show variant tasks of increasing complexity (more blocks), (1 SE). We compareagainst AvE , the Goal Inference baseline from which assumes access to a world model, andReward Inference where we recover the reward from a learned q-value. These prior approachesfail on all except the easiest task, highlighting the importance of scalability.",
  "Empty Pot": ": In Coordination Ring, our ESR agent learns to wait for the human to add an onion to the pot,and then adds one itself. There is another pot at the top which is nearly full, but the empowermentagent takes actions to maximize the impact of the humans actions, and so follows the lead of thehuman by filling the empty pot.",
  "Experiments": "We seek to answer two questions with our experiments. First, does our approach enable assistance instandard cooperation benchmarks? Second, does our approach scale to harder benchmarks whereprior methods fail? Our experiments will use two benchmarks designed by prior work to study assistance: the obstaclegridworld and Overcooked . Our main baseline is AvE , a prior empowerment-basedmethod. Our conjecture is that both methods will perform well on the lower-dimensional grid-world task, and that our method will scale more gracefully to the higher dimensional Overcookedenvironment. We will also compare against a nave baseline where the assistive agent acts randomly.",
  "Do contrastive successor representations effectively estimate empowerment?": "We test our approach in the assistance benchmark suggested in Du et al. . The human (orange)is tasked with reaching a goal state (green) while avoiding the obstacles (purple). The AI assistantcan move blocks one step at a time in any direction . While the original benchmark used N = 2obstacles, we will additionally evaluate on harder versions of this task with N = 5, 7, 10 obstacles.We show results in . On the easiest task, both our method and AvE achieve similar asymptoticreward, though our method learns more slowly than AvE. However, on the tasks with moderate andhigh degrees of complexity, our approach (ESR) achieves significantly higher rewards than AvE,which performs worse than a random controller. These experiments support our claim that contrastivesuccessor representations provide an effective means for estimating empowerment, and hint that ESRmight be well suited for solving higher dimensional tasks.",
  "Does our approach scale to tasks with image-based observations?": "Our second set of experiments look at scaling ESR to the image-based Overcooked environment.Since contrastive learning is often applied to image domains, we conjectured that ESR would scalegracefully to this setting. We will evaluate our approach in assisting a human policy trained withbehavioral cloning taken from Laidlaw and Dragan . The human prepares dishes by pickingup ingredients and cooking them on a stove, while the AI assistant moves ingredients and dishesaround the kitchen. We focus on two environments within this setting: a cramped room where thehuman must pass ingredients and dishes through a narrow corridor, and a coordination ring where thehuman must pass ingredients and dishes around a ring-shaped kitchen (Figs. 4b and 4c). As before,we compare with AvE as well as a nave random controller. We report results in . On bothtasks, we observe that our approach achieves higher rewards than AvE baseline, which performs nobetter than a random controller. In , we show an example of one of the collaborative behaviorslearned by ESR. Taken together with the results in the previous setting, these results highlight thescalability of ESR to higher dimensional problems.",
  "Discussion": "One of the most important problems in AI today is equipping AI agents with the capacity to assisthumans achieve their goals. While much of the prior work in this area requires inferring the humansintention, our work builds on prior work in studying how an assistive agent can empower a humanuser without inferring their intention. Relative to prior methods, we demonstrate how empowermentcan be readily estimated using contrastive learning, paving the way for deploying these techniques onhigh-dimensional problems. Limitations.One of the main limitations of our approach is the assumption that the assistive agenthas access to the humans actions, which could be challenging to observe in practice. Automaticallyinferring the humans actions remains an important problem for future work. A second limitation isthat the method is currently an on-policy method, in the sense that the assistive agent has to learn bytrial and error. A third limitation is that the ESR formulation assumes that both agents share the samestate space. In many cases the empowerment objective will still lead to desirable behavior, however,care must be taken in cases where the agent can restrict the information in its own observations, whichcould lead to reward hacking. Finally, our experiments do not test our method against real humans,whose policies may differ from the simulated policies. In the future, we plan to investigate techniquesfrom off-policy evaluation and cooperative game theory to enable faster learning of assistive agentswith fewer trials. We also plan to test the ESR objective in environments with partial observabilityover the humans state. Safety risks.Perhaps the main risk involved with maximizing empowerment is that it may be atodds with a humans agents goal, especially in contexts where the pursuit of that goal limits thehumans capacity to pursue other goals. For example, a family choosing to have a kid has many feweroptions over where they can travel for vacation, yet we do not want assistive agents to stymie familiesfrom having children. One key consideration is whom should be empowered. The present paper assumes there is a singlehuman agent. Equivalently, this can be seen as maximizing the empowerment of all exogenousagents. However, it is easy to adapt the proposed method to maximize the empowerment of a singletarget individual. Given historical inequities in the distribution of power, practitioners must take carewhen considering whose empowerment to maximize. Similarly, while we focused on maximizingempowerment, it is trivial to change the sign so that an assistive agent minimizes empowerment.One could imagine using such a tool in policies to handicap ones political opponents. Acknowledgments.We would like to thank Micah Carroll and Cameron Allen for their helpfulfeedback, as well as Niklas Lauffer for suggesting JaxMARL. We especially thank the fantasticNeurIPS reviewers for their constructive comments and suggestions. This research was partlysupported by ARL DCIST CRA W911NF-17-2-0181 and ONR N00014-22-1-2773, as well as NSFHCC 2310757, the Jump Cocosys Center, Princeton Research Computing, and the DoD through theNDSEG Fellowship Program. Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, Jrmy Scheurer, JavierRando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro Freire, et al. Open Problemsand Fundamental Limitations of Reinforcement Learning From Human Feedback.2023.arXiv:2307.15217.",
  "Christian Wirth, Riad Akrour, Gerhard Neumann, and Johannes Frnkranz. A Survey ofPreference-based Reinforcement Learning Methods. Journal of Machine Learning Research,18(136):146, 2017": "Micah Carroll, Rohin Shah, Mark K. Ho, Thomas L. Griffiths, Sanjit A. Seshia, Pieter Abbeel,and Anca Dragan. on the Utility of Learning About Humans for Human-ai Coordination. InConference on Neural Information Processing Systems. 2019. Alexander Pan, Kush Bhatia, and Jacob Steinhardt. the Effects of Reward Misspecification:Mapping and Mitigating Misaligned Models. In International Conference on Learning Repre-sentations, arXiv:2201.03544. 2022. Jeremy Tien, Jerry Zhi-Yang He, Zackory Erickson, Anca D. Dragan, and Daniel S. Brown.Causal Confusion and Reward Misidentification in Preference-based Reward Learning. InInternational Conference on Learning Representations. 2023.",
  "Ildefons Magrans de Abril and Ryota Kanai. A Unified Strategy for Implementing Curiosityand Empowerment Driven Reinforcement Learning. 2018. arXiv:1806.06505": "Sean Chen, Jensen Gao, Siddharth Reddy, Glen Berseth, Anca D. Dragan, and Sergey Levine.ASHA: Assistive Teleoperation via Human-in-the-loop Reinforcement Learning. In Interna-tional Conference on Robotics and Automation. 2022. Siddharth Reddy, Sergey Levine, and Anca Dragan. First Contact: Unsupervised Human-machine Co-adaptation via Mutual Information Maximization. Advances in Neural InformationProcessing Systems, 35:3154231556, 2022.",
  "Ben Poole, Sherjil Ozair, Aaron Van Den Oord, Alex Alemi, and George Tucker. on VariationalBounds of Mutual Information. In International Conference on Machine Learning. 2019": "Jongwook Choi, Archit Sharma, Honglak Lee, Sergey Levine, and Shixiang Shane Gu. Varia-tional Empowerment as Representation Learning for Goal-conditioned Reinforcement Learning.In International Conference on Machine Learning, pp. 19531963. 2021. Shakir Mohamed and Danilo Jimenez Rezende. Variational Information Maximisation forIntrinsically Motivated Reinforcement Learning. In Advances in Neural Information ProcessingSystems, volume 28. 2015.",
  "Susanne Still and Doina Precup. an Information-theoretic Approach to Curiosity-driven Rein-forcement Learning. Theory in Biosciences, 131:139148, 2012": "Nikolay Nikolov, Johannes Kirschner, Felix Berkenkamp, and Andreas Krause. Information-Directed Exploration for Deep Reinforcement Learning.In International Conference onLearning Representations. 2019. Erdem Biyik, Dylan P. Losey, Malayandi Palan, Nicholas C. Landolfi, Gleb Shevchuk, and DorsaSadigh. Learning Reward Functions From Diverse Sources of Human Feedback: OptimallyIntegrating Demonstrations and Preferences. In Int. J. Robotics Res. 2022.",
  "Benjamin Eysenbach, Ruslan Salakhutdinov, and Sergey Levine. the Information Geometry ofUnsupervised Reinforcement Learning. In International Conference on Learning Representa-tions. 2022": "Robert G. Gallager. Source Coding With Side Information and Universal Coding. 1979. Boris Yakovlevich Ryabko. Coding of a Source With Unknown but Ordered Probabilities.Problems of Information Transmission, 15(2):134138, 1979. R. Duncan Luce. Individual Choice Behavior. Individual Choice Behavior. John Wiley, 1959. Brian D Ziebart, Andrew Maas, J Andrew Bagnell, and Anind K Dey. Maximum EntropyInverse Reinforcement Learning. In AAAI Conference on Artificial Intelligence. 2008. Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan,Vikash Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, et al. Soft Actor-critic Algorithmsand Applications. 2018. arXiv:1812.05905. Divyansh Garg, Shuvam Chakraborty, Chris Cundy, Jiaming Song, and Stefano Ermon. Iq-learn:Inverse Soft-Q Learning for Imitation. In Advances in Neural Information Processing Systems,volume 34, pp. 40284039. 2021.",
  "AExperimental Details": "We ran all our experiments on NVIDIA RTX A6000 GPUs with 48GB of memory within aninternal cluster. Each evaluation seed took around 5-10 hours to complete. Our losses (Eqs. 10and 13) were computed and optimized in JAX with Adam . We used a hardware-acceleratedversion of the Overcooked environment from the JaxMARL package . The experimental resultsdescribed in were obtained by averaging over 5 seeds for the Overcooked coordination ringlayout, 15 for the cramped room layout, and 20 for the obstacle gridworld environment. Specifichyperparameter values can be found in our code, which is available at",
  "A.1Network Architecture": "In the obstacle grid environment, we used a network with 2 convolutional and 2 fully connected layersand SiLU activations. In Overcooked, we adapted the policy architecture from past work , using 3convolutional layers followed by 4 MLP layers with Leaky ReLU activations . We concatenate inaR and aH to the state as one-hot encoded channels, i.e. if the action is 5, 6 additional channels willbe concatenated to the state with all set to 0s except the 5th channel which is set to 1s.",
  "BTheoretical Analysis of Empowerment": "To connect our empowerment objective to reward, we will extend the notation in .1 to includea distribution over possible tasks the human might be trying to solve, R, such that each R Rdefines a distinct reward function R : S R. We assume R tries to maximize the -discountedempowerment of the human, defined as",
  "st = st aHt1.(16)": "Note that the definition of empowerment in Eq. (6) differs slightly from the original constructionEq. (2) we condition on the full history of human actions, not just the most recent one. Thisdistinction becomes irrelevant in practice if our MDP maintains history in the state, in which case wecan equivalently use st in place of st.",
  "V H,RR,(s0).(Eq. 5)": "Note that this definition of H depends on R and R and is bounded 0 JR(H) 1. As in theCIRL setting , we assume robot is unable to access the true human reward R : S R. One wayto think of the robots task is as finding a Nash equilibrium between the objectives Eq. (6) and thehuman best response in Eq. (17).",
  "Our approach will be to first relate the empowerment (influence of aHt on s+) to the mutual informationbetween aHt and the reward R": "Then, we will connect this quantity to a notion of advantage for the human (Eq. 27), which in turncan be related to the expected reward under the humans policy. In its simplest form, this argumentwill require an assumption over the reward distribution:Assumption 3.1 (Skill Coverage). The rewards R R are uniformly distributed over the scaled|S|-simplex |S| such that:R +1|S|1",
  ").(7)": "In other words, Assumption 3.1 says our prior over the humans reward function is uniform with zeromean. This is not the only prior for which this argument works, but for general R we will need acorrection term to incentivize states that are more likely across the distribution of R. Another way toview Assumption 3.1 is that the human is trying to execute diverse skills z Unif(|S|). We also assume ergodicity (Assumption 3.2). In the special case of an MDP that resets to somedistribution with full support over S, this assumption is automatically satisfied.Assumption 3.2 (Ergodicity). For some H, R, we have",
  "In our estimation of empowerment (Eq. 12) we supply the robot action aR when learning both and, however, the theoretical empowerment formulation in .3 does not require it": "To evaluate the impact of including aR, we run an additional ablation without it on the gridworldenvironment, shown in . This ablation shows that the inclusion of aR is most impactful inmore challenging (higher number of boxes) environments. We hypothesize that conditioning therepresentations on the robot action reduces the noise in the mutual information estimation, and alsoreduces the difficulty of classifying true future states.",
  "D.1ESR Training Example": "In , we show the mutual information during training of the ESR agent in the gridworldenvironment with 5 obstacles. he mutual information quickly becomes positive and remains sothroughout training. As long as the mutual information is positive, the classifier is able to reward theagent for taking actions that empower the human.",
  "ESimplifying the Objective": "The reward function in Eq. (13) is itself a random variable because it depends on future states g. Thissubsection describes how this randomness can be removed. To do this, we follow prior work in arguing that the learned representations (g) follow a Gaussian distribution:Assumption E.1 (Based on Wang and Isola ). The representations of future states (g) learnedby contrastive learning have a marginal distribution that is Gaussian:",
  "= C1EP((s+))e(s,aR,aH)T ((s, aR, aH) (s, aR))T": ": Visualizing training empowerment in a 5x5 Gridworld with 10 obstacles. Our empowermentobjective maximizes the influence of the humans actions on the future state, preferring the statewhere the human can reach the goal to the trapped state. This corresponds to maximizing the volumeof the state marginal polytope, which is proportional to the number of states that the human can reachfrom their current position. To visualize the representations, we set the latent dimension to 3 insteadof 100."
}