{
  "Abstract": "Deep learning sometimes appears to work in unexpected ways. In pursuit of adeeper understanding of its surprising behaviors, we investigate the utility of asimple yet accurate model of a trained neural network consisting of a sequence offirst-order approximations telescoping out into a single empirically operational toolfor practical analysis. Across three case studies, we illustrate how it can be appliedto derive new empirical insights on a diverse range of prominent phenomena in theliterature including double descent, grokking, linear mode connectivity, and thechallenges of applying deep learning on tabular data highlighting that this modelallows us to construct and extract metrics that help predict and understand the apriori unexpected performance of neural networks. We also demonstrate that thismodel presents a pedagogical formalism allowing us to isolate components of thetraining process even in complex contemporary settings, providing a lens to reasonabout the effects of design choices such as architecture & optimization strategy, andreveals surprising parallels between neural network learning and gradient boosting.",
  "Introduction": "Deep learning works, but it sometimes works in mysterious ways. Despite the remarkable recentsuccess of deep learning in applications ranging from image recognition [KSH12] to text generation[BMR+20], there remain many contexts in which it performs in apparently unpredictable ways: neuralnetworks sometimes exhibit surprisingly non-monotonic generalization performance [BHMM19,PBE+22], continue to be outperformed by gradient boosted trees on tabular tasks despite successeselsewhere [GOV22], and sometimes behave surprisingly similarly to linear models [FDRC20]. Thepursuit of a deeper understanding of deep learning and its phenomena has since motivated manysubfields, and progress on fundamental questions has been distributed across many distinct yetcomplementary perspectives that range from purely theoretical to predominantly empirical research. Outlook. In this work, we take a hybrid approach and investigate how we can apply ideas primarilyused in theoretical research to investigate the behavior of a simple yet accurate model of a neuralnetwork empirically. Building upon previous work that studies linear approximations to learning inneural networks through tangent kernels (e.g. [JGH18, COB19], see Sec. 2), we consider a modelthat uses first-order approximations for the functional updates made during training. However, unlikemost previous work, we define this model incrementally by simply telescoping out approximationsto individual updates made during training (Sec. 3) such that it more closely approximates the truebehavior of a fully trained neural network in practical settings. This provides us with a pedagogicallens through which we can view modern optimization strategies and other design choices (Sec. 5), anda mechanism with which we can conduct empirical investigations into several prominent deep learningphenomena that showcase how neural networks sometimes generalize seemingly unpredictably.",
  "arXiv:2411.00247v1 [cs.LG] 31 Oct 2024": "Across three case studies in Sec. 4, we then show that this model allows us to construct and extractmetrics that help predict and understand the a priori unexpected performance of neural networks.First, in Sec. 4.1, we demonstrate that it allows us to extend [CJvdS23]s recent model complexitymetric to neural networks, and use this to investigate surprising generalization curves discoveringthat the non-monotonic behaviors observed in both deep double descent [BHMM19] and grokking[PBE+22] are associated with quantifiable divergence of train- and test-time model complexity.Second, in Sec. 4.2, we show that it reveals perhaps surprising parallels between gradient boosting[Fri01] and neural network learning, which we then use to investigate the known performancedifferences between neural networks and gradient boosted trees on tabular data in the presence ofdataset irregularities [MKV+23]. Third, in Sec. 4.3, we use it to investigate the connections betweengradient stabilization and the success of weight averaging (i.e. linear mode connectivity [FDRC20]).",
  "Background": "Notation and preliminaries. Let f : X Rd Y Rk denote a neural network parameterizedby (stacked) model weights Rp. Assume we observe a training sample of n input-output pairs{xi, yi}ni=1, i.i.d. realizations of the tuple (X, Y ) sampled from some distribution P, and wish tolearn good model parameters for predicting outputs from this data by minimizing an empiricalprediction loss 1 nni=1 (f(xi), yi), where : Rk Rk R denotes some differentiable lossfunction. Throughout, we let k = 1 for ease of exposition, but unless otherwise indicated ourdiscussion generally extends to k > 1. We focus on the case where is optimized by initializing themodel with some 0 and then iteratively updating the parameters through stochastic gradient descent(SGD) with learning rates t for T steps, where at each t [T] = {1, . . . , T} we subsample batchesBt [n] = {1, . . . , n} of the training indices, leading to parameter updates t := t t1 as:",
  "|Bt|ft1(x1), . . . , 1{nBt}": "|Bt|ft1(xn)] has as columns the gradients of the modelprediction with respect to its parameters for examples in the training batch (and 0 otherwise). Beyondvanilla SGD, modern deep learning practice usually relies on a number of modifications to the updatedescribed above, such as momentum and weight decay; we discuss these in Sec. 5. Related work: Linearized neural networks and tangent kernels. A growing body of recent workhas explored the use of linearized neural networks (linear in their parameters) as a tool for theoretical[JGH18, COB19, LXS+19] and empirical [FDP+20, LZB20, OJMDF21] study. In this paper, wesimilarly make extensive use of the following observation (as in e.g. [FDP+20]): we can linearizethe difference ft(x) := ft(x) ft1(x) between two parameter updates as",
  "ft(x) = ft1(x)t + O(||t||2) ft1(x)t := ft(x)(2)": "where the quality of the approximation ft(x) is good whenever the parameter updates t from asingle batch are sufficiently small (or when the Hessian product ||t 2ft1(x)t|| vanishes).If Eq. (2) holds exactly (e.g. for infinitesimal t), then running SGD in the networks parameter spaceto obtain t corresponds to executing steepest descent on the function output f(x) itself using theneural tangent kernel Kt (x, xi) at time-step t [JGH18], i.e. results in functional updates",
  "|Bt|ft1(x)ft1(xi). (3)": "Lazy learning [COB19] occurs as the model gradients remain approximately constant during training,i.e. ft(x) f0(x), t [T]. For learned parameters T , this implies that the approxi-mation f linT (x) = f0(x) + f0(x)(T 0) holds which is a linear function of the modelparameters, and thus corresponds to a linear regression in which features are given by the modelgradients f0(x) instead of the inputs x directly whose training dynamics can be more eas-ily understood theoretically. For sufficiently wide neural networks the ft(x), and thus thetangent kernel, have been theoretically shown to be constant throughout training in some settings[JGH18, LXS+19], but in practice they generally vary during training, as shown theoretically in[LZB20] and empirically in [FDP+20]. A growing theoretical literature [GPK22] investigates con-stant tangent kernel assumptions to study convergence and generalization of neural networks (e.g.",
  "ft(x) := ft1(x)T t": ": Illustration of the telescoping model of a trained neural network. Unlike the morestandard framing of a neural network in terms of an iteratively learned set of parameters, the telescoping modeltakes a functional perspective on training a neural network in which an arbitrary test examples initially randomprediction, f0(x), is additively updated by a linearized adjustment ft(x) at each step t as in Eq. (5). [JGH18, LXS+19, DLL+19, BM19, GMMM19, GSJW20]). This present work relates more closelyto empirical studies making use of tangent kernels and linear approximations, such as [LSP+20,OJMDF21] who highlight differences between lazy learning and real networks, and [FDP+20] whoempirically investigate the relationship between loss landscapes and the evolution of Kt (x, xi).",
  "A Telescoping Model of Deep Learning": "In this work, we explore whether we can exploit the approximation in Eq. (2) beyond the lazinessassumption to gain new insight into neural network learning. Instead of applying the approximationacross the entire training trajectory at once as in f linT (x), we consider using it incrementally at eachbatch update during training to approximate what has been learned at this step. This still providesus with a greatly simplified and transparent model of a neural network, and results in a much morereasonable approximation of the true network. Specifically, we explore whether instead of studyingthe final model fT (x) as a whole we can gain insight by telescoping out the functional updatesmade throughout training, i.e. exploiting that we can always equivalently express fT (x) as:",
  "fT (x) = f0(x) + Tt=1[ft(x) ft1(x)] = f0(x) + Tt=1 ft(x)(4)": "This representation of a trained neural network in terms of its learning trajectory rather than itsfinal parameters is interesting because we are able to better reason about the impact of the trainingprocedure on the intermediate updates ft(x) than the final function fT (x) itself. In particular, weinvestigate whether empirically monitoring behaviors of the sum in Eq. (4) while making use of theapproximation in Eq. (2) will enable us to gain practical insights into learning in neural networks,while incorporating a variety of modern design choices into the training process. That is, we explorethe use of the following telescoping model fT (x) as an approximation of a trained neural network:",
  "Telescoping model of a trained neural network": "where KTt (x, xi) is determined by the neural tangent kernel as tKt (x, xi) in the case of standardSGD (in which case (ii) can also be interpreted as a discrete-time approximation of [Dom20]s pathkernel), but can take other forms for different choices of learning algorithm as we explore in Sec. 5. Practical considerations. Before proceeding, it is important to emphasize that the telescopingapproximation described in Eq. (5) is intended as a tool for (empirical) analysis of learning in neuralnetworks and is not being proposed as an alternative approach to training neural networks. ObtainingfT (x) requires computing ft1(x) for each training and testing example at each training stept [T], leading to increased computation over standard training. Additionally, these computationalcosts are likely prohibitive for extremely large networks and datasets without further adjustments; forthis purpose, further approximations such as [MBS23] could be explored. Nonetheless, computingfT (x) or relevant parts of it is still feasible in many pertinent settings as later illustrated in Sec. 4.",
  ": Approximation error ofthe telescoping ( ft(x), red) and thelinear model (f lint (x), gray)": "How good is this approximation? In , we examinethe quality of ft(x) for a 3-layer fully-connected ReLU net-work of width 200, trained to discriminate 3-vs-5 from 1000MNIST examples using the squared loss with SGD or AdamW[LH17]. In red, we plot its mean average approximation error(1 xXtest |ft(x) ft(x)|) and observe that for smalllearning rates the difference remains negligible. In gray weplot the same quantity for f lint (x) (i.e. the first-order expansionaround 0) for reference and find that iteratively telescoping outthe updates instead improves the approximation by orders ofmagnitude which is also reflected in their prediction perfor-mance (see Appendix D.1). Unsurprisingly, controls approxi-mation quality as it determines ||t||. Further, interacts withthe optimizer choice e.g. Adam(W) [KB14, LH17] naturallymakes larger updates due to rescaling (see Sec. 5) and thereforerequires smaller to ensure approximation quality than SGD.",
  "A Closer Look at Deep Learning Phenomena Through a Telescoping Lens": "Next, we turn to applying the telescoping model. Below, we present three case studies revisitingexisting experiments that provided evidence for a range of unexpected behaviors of neural networks.These case studies have in common that they highlight cases in which neural networks appear togeneralize somewhat unpredictably, which is also why each phenomenon has received considerableattention in recent years. For each, we then show that the telescoping model allows us to construct andextract metrics that can help predict and understand the unexpected performance of the networks. Inparticular, we investigate (i) surprising generalization curves (Sec. 4.1), (ii) performance differencesbetween gradient boosting and neural networks on some tabular tasks (Sec. 4.2), and (iii) the successof weight averaging (Sec. 4.3). We include an extended literature review in Appendix A, a detaileddiscussion of all experimental setups in Appendix C, and additional results in Appendix D.",
  "Case study 1: Exploring surprising generalization curves and benign overfitting": "Classical statistical wisdom provides clear intuitions about overfitting: models that can fit the trainingdata too well because they have too many parameters and/or because they were trained for too long are expected to generalize poorly (e.g. [HTF09, Ch. 7]). Modern phenomena like double descent[BHMM19], however, highlighted that pure capacity measures (capturing what could be learned in-stead of what is actually learned) would not be sufficient to understand the complexity-generalizationrelationship in deep learning [Bel21]. Raw parameter counts, for example, cannot be enough to under-stand the complexity of what has been learned by a neural network during training because, even whenusing the same architecture, what is learned could be wildly different across various implementationchoices within the optimization process and even at different points during the training process ofthe same model, as prominently exemplified by the grokking phenomenon [PBE+22]. Here, with thegoal of finding clues that may help predict phenomena like double descent and grokking, we explorewhether the telescoping model allows us to gain insight into the relative complexity of what is learned. A complexity measure that avoids the shortcomings listed above because it allows to consider aspecific trained model was recently used by [CJvdS23] in their study of non-deep double descent.As their measure p0s builds on the literature on smoothers [HT90], it requires to express learnedpredictions as a linear combination of the training labels, i.e. as f(x) = s(x)y = i[n] si(x)yi.Then, [CJvdS23] define the effective parameters p0s used by the model when issuing predictions forsome set of inputs {x0j}jI0 with indices collected in I0 (here, I0 is either Itrain = {1, . . . , n} orItest = {n + 1, . . . , n + m}) as p0s p(I0,s()) =n",
  "jI0 ||s(x0j)||2. Intuitively, the larger p0s,the less smoothing across the training labels is performed, which implies higher model complexity": "Due to the black-box nature of trained neural networks, however, it is not obvious how to link learnedpredictions to the labels observed during training. Here, we demonstrate how the telescoping modelallows us to do precisely that enabling us to make use of p0s as a proxy for complexity. We considerthe special case of a single output (k = 1) and training with squared loss (f(x), y) = 1",
  "(8)": "which means that the first telescoping predictions f1(x) are indeed simply linear combinations of thetraining labels (and the predictions at initialization)! As detailed in Appendix B.1, this also impliesthat recursively substituting Eq. (7) into Eq. (5) further allows us to write any prediction ft(x) asa linear combination of the training labels and f0(), i.e. ft(x) = st(x)y + c0t(x) where the1n vector st(x) is a function of the kernels {Ktt(, )}tt, and the scalar c0t(x) is a functionof the {Ktt(, )}tt and f0(). We derive precise expressions for st(x) and c0t(x) for differentoptimizers in Appendix B.1 enabling us to use st(x) to compute p0s as a proxy for complexity below.",
  ":Double descent inMSE (top) and effective param-eters p0s (bottom) on CIFAR-10": "Double descent: Model complexity vs model size. While trainingerror always monotonically decreases as model size (measured by pa-rameter count) increases, [BHMM19] made a surprising observationregarding test error in their seminal paper on double descent: theyfound that test error initially improves with additional parametersand then worsens when the model is increasingly able to overfit tothe training data (as is expected) but can improve again as modelsize is increased further past the so-called interpolation thresholdwhere perfect training performance is achieved. This would appearto contradict the classical U-shaped relationship between modelcomplexity and test error [HTF09, Ch. 7]. Here, we investigatewhether tracking p0s on train and test data separately will allow usto gain new insight into the phenomenon in neural networks. In , we replicate the binary classification example of doubledescent in neural networks of [BHMM19], training single-hidden-layer ReLU networks of increasingwidth to distinguish cats and dogs on CIFAR-10 (we present additional results using MNIST inAppendix D.2). First, we indeed observe the characteristic behavior of error curves as describedin [BHMM19] (top panel). Measuring learned complexity using p0s, we then find that while ptrainsmonotonically increases as model size is increasing, the effective parameters used on the test dataptestsimplied by the trained neural network decrease as model size is increased past the interpolationthreshold (bottom panel). Thus, paralleling the findings made in [CJvdS23] for linear regression andtree-based methods, we find that distinguishing between train- and test-time complexity of a neuralnetwork using p0s provides new quantitative evidence that bigger networks are not necessarily learningmore complex prediction functions for unseen test examples, which resolves the ostensible tensionbetween deep double descent and the classical U-curve. Importantly, note that ptestscan be computedwithout access to test-time labels, which means that the observed difference between ptrainsand ptestsallows to quantify whether there is benign overfitting [BLLT20, YHT+21] in a neural network. Grokking: Model complexity throughout training. The grokking phenomenon [PBE+22] thenshowcased that improvements in test performance during a single training run can occur long afterperfect training performance has been achieved (contradicting early stopping practice!). While[LMT22] attribute this to weight decay causing ||t|| to shrink late in training which they demon-strate on an MNIST example using unusually large 0 [KBGP24] highlight that grokking can alsooccur as the weight norm ||t|| grows later in training which they demonstrate on a polynomialregression task. In we replicate2 both experiments while tracking p0s to investigate whether 2As detailed in Appendix C, we replicate [KBGP24]s experiment exactly but adapt [LMT22]s experimentinto a binary classification task with lower learning rate to enable the use of fT (x). The reduction of isneeded here as the t are otherwise too large to obtain an accurate approximation and has a side effect that thegrokking phenomenon appears visually less extreme as perfect training performance is achieved later in training. : Grokking in mean squared error on a polynomial regression task (1, replicated from [KBGP24]) andin misclassification error on MNIST using a network with large initialization (2, replicated from [LMT22]) (top),against effective parameters (bottom). Column (3) shows test results on MNIST with standard initialization(with and without sigmoid activation) where time to generalization is quick and grokking does not occur. this provides new insight into this apparent disagreement. Then, we observe that the continuedimprovement in test error, past the point of perfect training performance, is associated with diver-gence of ptrainsand ptestsin both experiments (analogous to the double descent experiment in ),suggesting that grokking may reflect transition into a measurably benign overfitting regime duringtraining. In Appendix D.2, we additionally investigate mechanisms known to induce grokking, andshow that later onset of generalization indeed coincides with later divergence of ptrainsand ptests. Inductive biases & learned complexity. We observed that the large 0 in [LMT22]s MNIST exam-ple of grokking result in very large initial predictions |f0(x)|1. Because no sigmoid is applied, themodel needs to learn that all yi by reducing the magnitude of predictions substantially large0 thus constitute a very poor inductive bias for this task. One may expect that the better an inductivebias is, the less complex the component of the final prediction that is learned from data. To test whetherthis intuition is quantifiable, we repeat the MNIST experiment with standard initialization scale, withand without sigmoid activation (), in column (3) of (training results shown in Appendix D.2for readability). We indeed find that both not only speed up learning significantly (a generalizingsolution is found in 102 instead of 105 steps), but also substantially reduce effective parametersused, where the stronger inductive bias using () indeed leads to the least learned complexity.",
  "Case study 2: Understanding differences between gradient boosting and neural networks": "Despite their overwhelming successes on image and language data, neural networks are perhapssurprisingly still widely considered to be outperformed by gradient boosted trees (GBTs) on tabulardata, an important modality in many data science applications. Exploring this apparent Achilles heelof neural networks has therefore been the goal of multiple extensive benchmarking studies [GOV22,MKV+23]. Here, we concentrate on a specific empirical finding of [MKV+23]: their results suggestthat GBTs may particularly outperform deep learning on heterogeneous data with greater irregularityin input features, a characteristic often present in tabular data. Below, we first show that the telescopingmodel offers a useful lens to compare and contrast the two methods, and then use this insight to provideand test a new explanation of why GBTs can perform better in the presence of dataset irregularities. Identifying (dis)similarities between learning in GBTs and neural networks. We begin byintroducing gradient boosting [Fri01] closely following [HTF09, Ch. 10.10]. Gradient boosting (GB)also aims to learn a predictor f GB : X Rk minimizing expected prediction loss . While deeplearning solves this problem by iteratively updating a randomly initialized set of parameters thattransform inputs to predictions, the GB formulation iteratively updates predictions directly withoutrequiring any iterative learning of parameters thus operating in function space rather than parameterspace. Specifically, GB, with learning rate and initialized at predictor h0(x), consists of a sequencef GBT(x) = h0(x)+ Tt=1 ht(x) where each ht(x) improves upon the existing predictions f GBt1(x).The solution to the loss minimization problem can be achieved by executing steepest descent infunction space directly, where each update ht simply outputs the negative training gradients of the lossfunction with respect to the previous model, i.e. ht(xi) = git where git = ( f GBt1(xi), yi)/ f GBt1(xi). However, this process is only defined at the training points {xi, yi}i[n]. To obtain an estimate of theloss gradient for an arbitrary test point x, each iterative update instead fits a weak learner ht() tothe current input-gradient pairs {xi, git}i[n] which can then also be evaluated new, unseen inputs.While this process could in principle be implemented using any base learner, the term gradient boost-ing today appears to exclusively refer to the approach outlined above implemented using shallow treesas ht() [Fri01]. Focusing on trees which issue predictions by averaging the training outputs in eachleaf, we can make use of the fact that these are sometimes interpreted as adaptive nearest neighbor es-timators or kernel smoothers [LJ06, BD10, CJvdS24], allowing us to express the learned predictor as:",
  "where lht(x) denotes the leaf example x falls into, nl(x) =": "i[n] 1{lht(x) = lht(xi)} is the numberof training examples in said leaf and Kht(x, xi) = 1/nleaf(x)1{lht(x) = lht(xi)} is thus the kernellearned by the tth tree ht(). Comparing Eq. (9) to the kernel representation of the telescoping modelof neural network learning in Eq. (5), we make a perhaps surprising observation: the telescoping modelof a neural network and GBTs have identical structure and differ only in their used kernel! Below,we explore whether this new insight allows to understand some of their performance differences. Why can GBTs outperform deep learning in the presence of dataset irregularities? ComparingEq. (5) and Eq. (9) thus suggests that at least some of the performance differences between neuralnetworks and GBTs are likely to be rooted in the differences between the behavior of the neuralnetwork tangent kernels Kt (x, xi) and GBTs tree kernels Kht(x, xi). One difference is obviousand purely architectural: it is possible that either kernel encodes a better inductive bias to fit theunderlying outcome-generating process of a dataset at hand. Another difference is more subtleand relates to the behavior of the learned model on new inputs x: the tree kernels are likely tobehave much more predictable at test-time than the neural network tangent kernels. To see this,note that for the tree kernels we have that x X and i [n], 0 Kht(x, xi) 1 and i[n] Kht(x, xi) = 1; importantly, this is true regardless of whether x = xi for some i or not.For the tangent kernels on the other hand, Kt (x, xi) is in general unbounded and could behavevery differently for x not observed during training. This leads us to hypothesize that this differencemay be able to explain [MKV+23]s observation that GBTs perform better whenever features areheavy-tailed: if a test point x is very different from training points, the kernels implied by the neuralnetwork kt (x) := [Kt (x, x1), . . . , Kt (x, xn)] may behave very differently than at train-timewhile the tree kernels kht(x) := [Kht(x, x1), . . . , Kht(x, xn)] will be less affected. For instance,1n ||kht(x)||2 1 for all x while ||kt (x)||2 is generally unbounded.",
  ": Neural Networks vs GBTs: Rel-ative performance (top) and behavior of ker-nels (bottom) with increasing test data irreg-ularity using the houses dataset": "We empirically test this hypothesis on standard tabularbenchmark datasets proposed in [GOV22]. We wish toexamine the performance of the models and the behav-ior of the kernels as inputs become increasingly irregular,evaluating if GBTs kernels indeed display more consis-tent behavior compared to the networks tangent kernels.As a simple notion for input irregularity, we apply prin-cipal component analysis to the inputs to obtain a lowerdimensional representation of the data and sort the ob-servations according to their distance from the centroid.For a fixed trained model, we then evaluate on test setsconsisting of increasing proportions p of the most irreg-ular inputs (those in the top 10% furthest from the cen-troid). We compare the GBTs to neural networks by ex-amining (i) the most extreme values their kernel weightstake at test-time relative to the training data (measured",
  "MSE0NNMSE0GBT )changes as the proportion p of irregular examples in-creases. In using houses and in Appendix D.3": "using additional datasets, we first observe that GBTs outperform the neural network already in theabsence of irregular examples; this highlights that there may indeed be differences in the suitability ofthe kernels in fitting the outcome-generating processes. Consistent with our expectations, we then findthat, as the test data becomes more irregular, the performance of the neural network decays faster thanthat of the GBTs. Importantly, this is well tracked by their kernels, where the unbounded nature ofthe networks tangent kernel indeed results in it changing its behavior on new, challenging examples. Takeaway Case Study 2. Eq. (5) provides a new lens for comparing neural networks to GBTs, and high-lights that unboundedness in kt (x) can predict performance differences due to dataset irregularities.",
  "Case study 3: Towards understanding the success of weight averaging": "The final interesting phenomenon we investigate is that it is sometimes possible to simply average theweights 1 and 2 obtained from two stochastic training runs of the same model, resulting in a weight-averaged model that performs no worse than the individual models [FDRC20, AHS22] which hasimportant applications in areas such as federated learning. This phenomenon is known as linearmode connectivity (LMC) and is surprising as, a priori, it is not obvious that simply averaging theweights of independent neural networks (instead of their predictions, as in a deep ensemble [LPB17]),which are highly nonlinear functions of their parameters, would not greatly worsen performance.While recent work has demonstrated empirically that it is sometimes possible to weight-average aneven broader class of models after permuting weights [SJ20, ESSN21, AHS22], we focus here onunderstanding when LMC can be achieved for two models trained from the same initialization 0. In particular, we are interested in [FDRC20]s observation that LMC can emerge during training: theweights of two models tjT , j {1, 2}, which are initialized identically and follow identical optimiza-tion routine up until checkpoint t but receive different batch orderings and data augmentations aftert, can be averaged to give an equally performant model as long as t exceeds a so-called stabilitypoint t, which was empirically discovered to occur early in training in [FDRC20]. Interestingly,[FDP+20, Sec. 5] implicitly hint at an explanation for this phenomenon in their empirical study oftangent kernels and loss landscapes, where they found an association between the disappearance ofloss barriers between solutions during training and the rate of change in Kt (, ). We further explorepotential implications of this observation through the lens of the telescoping model below. Why a transition into a constant-gradient regime would imply LMC. Using the weight-averagingrepresentation of the telescoping model, it becomes easy to see that not only would stabilization of thetangent kernel be associated with lower linear loss barriers, but the transition into a lazy regime duringtraining i.e. reaching a point t after which the model gradients no longer change can be sufficientto imply LMC during training as observed in [FDRC20] under a mild assumption on the performanceof the two networks ensemble. To see this, let L(f) := EX,Y P [(f(X), Y )] denote the expectedloss of f and recall that if supL(ft1T +(1)t2T ) [L(ft1T ) + (1 )L(ft2T )] 0 then",
  "LMC is said to hold. If we assume that ensembles f (x) := ft1T (x) + (1 )ft2T (x) perform no": "worse than the individual models (i.e. L( f ) L(ft1T )+(1)L(ft2T ) , as is usuallythe case in practice [ABPC23]), then one case in which LMC is guaranteed is if the predictions ofweight-averaged model and ensemble are identical. In Appendix B.2, we show that if there existssome t [0, T) after which the model gradients ftjt () no longer change (i.e. for all t t the",
  "f (x) ft1T +(1)t2T (x) ft (x) + ft (x) Tt=t+1(t1t + (1 )t2t). (10)": "That is, transitioning into a regime with constant model gradients during training can imply LMCbecause the ensemble and weight-averaged model become near-identical. This also has as animmediate corollary that models with the same 0 which train fully within this regime (e.g. thosediscussed in [JGH18, LXS+19]) will have t = 0. Note that, when using nonlinear (final) outputactivation () the post-activation model gradients will generally not become constant during training(as we discuss in Sec. 5 for the sigmoid and as was shown theoretically in [LZB20] for generalnonlinearities). If, however, the pre-activation model gradients become constant during trainingand the pre-activation ensemble which averages the two models pre-activation outputs beforeapplying () performs no worse than the individual models (as is also usually the case in practice[JLCvdS24]), then the above also immediately implies LMC for such models. : Linear mode connectivity and gradient changes by t. (1) Decrease in accuracy when usingaveraged weights t1T + (1 )t2T for randomly initialized (orange) and pre-trained ResNet-20 (green).(2) & (3) Changes in model gradients by layer for a randomly initialized (2) and pretrained (3) model. This suggests a candidate explanation for why LMC emerged at specific points in [FDRC20]. To testthis, we replicate their CIFAR-10 experiment using a ResNet-20 in . In addition to plottingthe maximal decrease in accuracy when comparing ft1T +(1)t2T (x) to the weighted average ofthe accuracies of the original models as [FDRC20] to measure LMC in (1), we also plot the squaredchange in (pre-softmax) gradients (ft+390(x) ft (x))2 over the next epoch (390 batches)after checkpoint t, averaged over the test set and the parameters in each layer in (2). We find that thedisappearance of the loss barrier indeed coincides with the time in training when the model gradientsbecome more stable across all layers. Most saliently, the appearance of LMC appears to correlatewith the stabilization of the gradients of the linear output layer. However, we also continue to observesome changes in other model gradients, which indicates that these models do not train fully linearly. Pre-training and weight averaging. Because weight averaging methods have become increasinglypopular when using pre-trained instead of randomly initialized models [NSZ20, WIG+22, CVSK22],we are interested in testing whether pre-training may improve mode connectability through stabilizingthe model gradients. To test this, we replicate the above experiment with the same architecturepre-trained on the SVHN dataset (in green in (1)). Mimicking findings of [NSZ20], we firstfind the loss barrier to be substantially lower after pre-training. In (3), we then observe thatthe gradients in the hidden and final layers indeed change less and stabilize earlier in training thanin the randomly initialized model yet the gradients of the BatchNorm parameters change more.Overall, the findings in this section thus highlight that while there may be a connection betweengradient stabilization and LMC, it cannot fully explain it suggesting that further investigation intothe phenomenon using this lens, particularly into the role of BatchNorm layers, may be fruitful. Takeaway Case Study 3. Reasoning through the learning process by telescoping out functional updatessuggests that averaging model parameters trained from the same checkpoint can be effective if theirmodels gradients remain stable, however, this cannot fully explain LMC in the setting we consider.",
  "The Effect of Design Choices on Linearized Functional Updates": "The literature on the neural tangent kernel primarily considers plain SGD, while modern deep learningpractice typically relies on a range of important modifications to the training process (see e.g. [Pri23,Ch. 6]) this includes many of the experiments demonstrating surprising deep learning phenomena weexamined in Sec. 4. To enable us to use modern optimizers above, we derived their implied linearizedfunctional updates through the weight-averaging representation ft(x) = ft1(x)t, whichin turn allows us to define KTt (, ) in Eq. (5) for these modifications using straightforward algebra.As a by-product, we found that this provides us with an interesting and pedagogical formalism toreason about the relative effect of different design choices in neural network training, and elaborateon selected learnings below. Momentum with scalar hyperparameter 1 smoothes weight updates by employing an exponentiallyweighted average over the previous parameter gradients as t = t111t1tk=1 tk1Tkgkinstead of using the current gradients alone. This implies linearized functional updates",
  "|Bk|ft1(x)fk1(xi) denotes the cross-temporal tangent kernel.Thus, the functional updates also utilize previous loss gradients, where their weight is determined": "using an inner product of the model gradient features from different time steps. If ft(x) isconstant throughout training and we use full-batch GD, then the contribution of each training examplei to ft(x) reduces to tK0 (x, xi) 11 1t1 [tk=1 tk1gik], an exponentially weighted movingaverage over its past loss gradients making the effect of momentum on functional updates analogousto its effect on updates in parameter space. However, if ft(x) changes over time, it is e.g.possible that Kk,t(x, xi) has opposite sign from Kt (x, xi) in which case momentum reduces insteadof amplifies the effect of a previous git. This is more obvious when re-writing Eq. (11) to collect allterms containing a specific git, leading to KTt (x, xi) = Tk=t k111k1 kt1Kk,t(x, xi) for Eq. (5).",
  "(1 )t1ft1(x)0(12)": "For full-batch GD and constant tangent kernels, K0 (x, xi)[git t1k=1(1 )t1kgik]is the contribution of each training example to the functional updates, which effectively decays theprevious contributions of this example. Further, comparing the signs in Eq. (12) to Eq. (11) highlightsthat momentum can offset the effect of weight decay on the learned updates in function space (inwhich case weight decay mainly acts through the term decaying the initial weights 0). Adaptive & parameter-dependent learning rates are another important modification in practicewhich enable the use of different step-sizes across parameters by dividing t elementwise by a p1scaling vector t. Most prominently, this is used to adaptively normalize the magnitude of updates(e.g. Adam [KB14] uses t =",
  "|Bt|ft1(x)diag( 1": "t )ft1(xi). This expressionhighlights that t admits an elegant interpretation as re-scaling the relative influence of features onthe tangent kernel, similar to structured kernels in non-parametric regression [HTF09, Ch. 6.4.1]. Architecture design choices also impact the form of the kernel. One important practical exampleis whether f(x) applies a non-linear activation function to the output g(x) R of its final layer.Consider the choice of using the sigmoid (z) =1",
  "K,t(x, xi) = (gt(x))(1 (gt(x)))(gt(xi))(1 (gt(xi)))K,gt(x, xi)(13)": "indicating that K,t(x, xi) will give relatively higher weight in functional updates to trainingexamples i for which the model is uncertain ((g(xi)) 1/2)) and lower weight to examples wherethe model is certain ((gt(xi)) 0 1) regardless of whether (gt(xi)) is the correct label.Conversely, Eq. (13) also implies that when comparing the functional updates of (g(x)) to thoseof g(x) across inputs x X, updates with () will be relatively larger for x where the model isuncertain ((gt(x)) 1/2)). Finally, Eq. (13) also highlights that the (post-activation) tangent kernelof a model with sigmoid activation will generally not be constant in t unless the model predictions(gt(x)) do not change.",
  "Conclusion": "This work investigated the utility of a telescoping model for neural network learning, consistingof a sequence of linear approximations, as a tool for understanding several recent deep learningphenomena. By revisiting existing empirical observations, we demonstrated how this perspectiveprovides a lens through which certain surprising behaviors of deep learning can become moreintelligible. In each case study, we intentionally restricted ourselves to specific, noteworthy empiricalexamples which we proceeded to re-examine in greater depth. We believe that there are therefore manyinteresting opportunities for future research to expand on these initial findings by building upon theideas we present to investigate such phenomena in more generality, both empirically and theoretically.",
  "Acknowledgements": "We would like to thank James Bayliss, who first suggested to us to look into explicitly unravellingSGD updates to write trained neural networks as approximate smoothers to study deep double descentafter a seminar on our paper [CJvdS23] on non-deep double descent. This suggestion ultimatelyinspired many investigations far beyond the original double descent context. We are also grateful toanonymous reviewers for helpful comments and suggestions. AC and AJ gratefully acknowledgefunding from AstraZeneca and the Cystic Fybrosis Trust, respectively. This work was supported by aG-Research grant, and Azure sponsorship credits granted by Microsofts AI for Good Research Lab.",
  "[Bre01]Leo Breiman. Random forests. Machine learning, 45:532, 2001": "[BSM+22]Frederik Benzing, Simon Schug, Robert Meier, Johannes Von Oswald, Yassir Akram,Nicolas Zucchet, Laurence Aitchison, and Angelika Steger. Random initialisationsperforming above chance and how to find them. arXiv preprint arXiv:2209.07509,2022. [CJvdS23]Alicia Curth, Alan Jeffares, and Mihaela van der Schaar. A u-turn on double descent:Rethinking parameter counting in statistical learning. Advances in Neural InformationProcessing Systems, 36, 2023. [CJvdS24]Alicia Curth, Alan Jeffares, and Mihaela van der Schaar. Why do random forests work?understanding tree ensembles as self-regularizing adaptive smoothers. arXiv preprintarXiv:2402.01502, 2024.",
  "[Die02]Thomas G Dietterich. Ensemble learning. The handbook of brain theory and neuralnetworks, 2(1):110125, 2002": "[DLL+19]Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descentfinds global minima of deep neural networks. In International conference on machinelearning, pages 16751685. PMLR, 2019. [DLM20]Michal Derezinski, Feynman T Liang, and Michael W Mahoney. Exact expressions fordouble descent and implicit regularization via surrogate random design. Advances inneural information processing systems, 33:51525164, 2020.",
  "[Dom20]Pedro Domingos. Every model learned by gradient descent is approximately a kernelmachine. arXiv preprint arXiv:2012.00152, 2020": "[dRBK20]Stphane dAscoli, Maria Refinetti, Giulio Biroli, and Florent Krzakala. Double troublein double descent: Bias and variance (s) in the lazy regime. In International Conferenceon Machine Learning, pages 22802290. PMLR, 2020. [DVSH18]Felix Draxler, Kambis Veschgini, Manfred Salmhofer, and Fred Hamprecht. Essentiallyno barriers in neural network energy landscape. In International conference on machinelearning, pages 13091318. PMLR, 2018.",
  "[FB16]C Daniel Freeman and Joan Bruna. Topology and geometry of half-rectified networkoptimization. arXiv preprint arXiv:1611.01540, 2016": "[FDP+20]Stanislav Fort, Gintare Karolina Dziugaite, Mansheej Paul, Sepideh Kharaghani,Daniel M Roy, and Surya Ganguli. Deep learning versus kernel learning: an em-pirical study of loss landscape geometry and the time evolution of the neural tangentkernel. Advances in Neural Information Processing Systems, 33:58505861, 2020. [FDRC20]Jonathan Frankle, Gintare Karolina Dziugaite, Daniel Roy, and Michael Carbin. Linearmode connectivity and the lottery ticket hypothesis. In International Conference onMachine Learning, pages 32593269. PMLR, 2020.",
  "[GK24]Samuel James Greydanus and Dmitry Kobak. Scaling down deep learning with mnist-1d.In Forty-first International Conference on Machine Learning, 2024": "[GMMM19] Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Limita-tions of lazy training of two-layers neural network. Advances in Neural InformationProcessing Systems, 32, 2019. [GOV22]Lo Grinsztajn, Edouard Oyallon, and Gal Varoquaux. Why do tree-based modelsstill outperform deep learning on typical tabular data? Advances in neural informationprocessing systems, 35:507520, 2022.",
  "[GPK22]Eugene Golikov, Eduard Pokonechnyy, and Vladimir Korviakov. Neural tangent kernel:A survey. arXiv preprint arXiv:2208.13614, 2022": "[GSJW20]Mario Geiger, Stefano Spigler, Arthur Jacot, and Matthieu Wyart. Disentangling featureand lazy training in deep neural networks. Journal of Statistical Mechanics: Theoryand Experiment, 2020(11):113301, 2020. [HHLS24]Moritz Haas, David Holzmller, Ulrike Luxburg, and Ingo Steinwart. Mind the spikes:Benign overfitting of kernels and neural networks in fixed dimension. Advances inNeural Information Processing Systems, 36, 2024.",
  "[HTF09]Trevor Hastie, Robert Tibshirani, and Jerome H Friedman. The elements of statisticallearning: data mining, inference, and prediction, volume 2. Springer, 2009": "[HXZQ22]Zheng He, Zeke Xie, Quanzhi Zhu, and Zengchang Qin. Sparse double descent:Where network pruning aggravates overfitting. In International Conference on MachineLearning, pages 86358659. PMLR, 2022. [HZRS16]Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning forimage recognition. In Proceedings of the IEEE conference on computer vision andpattern recognition, pages 770778, 2016.",
  "[Ide]Yerlan Idelbayev. Proper ResNet implementation for CIFAR10/CIFAR100 in PyTorch. Accessed: 2024-05-15": "[IPG+18]Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew Gor-don Wilson. Averaging weights leads to wider optima and better generalization. arXivpreprint arXiv:1803.05407, 2018. [IWG+22]Gabriel Ilharco, Mitchell Wortsman, Samir Yitzhak Gadre, Shuran Song, Hannaneh Ha-jishirzi, Simon Kornblith, Ali Farhadi, and Ludwig Schmidt. Patching open-vocabularymodels by interpolating weights. Advances in Neural Information Processing Systems,35:2926229277, 2022.",
  "[LJ06]Yi Lin and Yongho Jeon. Random forests and adaptive nearest neighbors. Journal ofthe American Statistical Association, 101(474):578590, 2006": "[LJL+24]Kaifeng Lyu, Jikai Jin, Zhiyuan Li, Simon Shaolei Du, Jason D Lee, and Wei Hu.Dichotomy of early and late phase implicit biases can provably induce grokking. InThe Twelfth International Conference on Learning Representations, 2024. [LKN+22]Ziming Liu, Ouail Kitouni, Niklas S Nolte, Eric Michaud, Max Tegmark, and MikeWilliams. Towards understanding grokking: An effective theory of representationlearning. Advances in Neural Information Processing Systems, 35:3465134663, 2022.",
  "[LMT22]Ziming Liu, Eric J Michaud, and Max Tegmark. Omnigrok: Grokking beyond algo-rithmic data. In The Eleventh International Conference on Learning Representations,2022": "[LPB17]Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalablepredictive uncertainty estimation using deep ensembles. Advances in neural informationprocessing systems, 30, 2017. [LSP+20]Jaehoon Lee, Samuel Schoenholz, Jeffrey Pennington, Ben Adlam, Lechao Xiao,Roman Novak, and Jascha Sohl-Dickstein. Finite versus infinite neural networks: anempirical study. Advances in Neural Information Processing Systems, 33:1515615172,2020.",
  "[LVM+20]Marco Loog, Tom Viering, Alexander Mey, Jesse H Krijthe, and David MJ Tax. Abrief prehistory of double descent. Proceedings of the National Academy of Sciences,117(20):1062510626, 2020": "[LXS+19]Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, JaschaSohl-Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolveas linear models under gradient descent. Advances in neural information processingsystems, 32, 2019. [LZB20]Chaoyue Liu, Libin Zhu, and Misha Belkin. On the linearity of large non-linearmodels: when and why the tangent kernel is constant. Advances in Neural InformationProcessing Systems, 33:1595415964, 2020.",
  "[Mac91]David MacKay. Bayesian model comparison and backprop nets. Advances in neuralinformation processing systems, 4, 1991": "[MBB18]Siyuan Ma, Raef Bassily, and Mikhail Belkin. The power of interpolation: Understand-ing the effectiveness of sgd in modern over-parametrized learning. In InternationalConference on Machine Learning, pages 33253334. PMLR, 2018. [MBS23]Mohamad Amin Mohamadi, Wonho Bae, and Danica J Sutherland. A fast, well-foundedapproximation to the empirical neural tangent kernel. In International Conference onMachine Learning, pages 2506125081. PMLR, 2023.",
  "[Nea19]Brady Neal. On the bias-variance tradeoff: Textbooks need an update. arXiv preprintarXiv:1912.08286, 2019": "[NKB+21]Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and IlyaSutskever. Deep double descent: Where bigger models and more data hurt. Journal ofStatistical Mechanics: Theory and Experiment, 2021(12):124003, 2021. [NMB+18]Brady Neal, Sarthak Mittal, Aristide Baratin, Vinayak Tantia, Matthew Scicluna, SimonLacoste-Julien, and Ioannis Mitliagkas. A modern take on the bias-variance tradeoff inneural networks. arXiv preprint arXiv:1810.08591, 2018.",
  "[NVKM20] Preetum Nakkiran, Prayaag Venkat, Sham Kakade, and Tengyu Ma. Optimal regular-ization can mitigate double descent. arXiv preprint arXiv:2003.01897, 2020": "[NWC+11] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Baolin Wu, Andrew YNg, et al. Reading digits in natural images with unsupervised feature learning. In NIPSworkshop on deep learning and unsupervised feature learning, volume 2011, page 7.Granada, Spain, 2011. [OJFF24]Guillermo Ortiz-Jimenez, Alessandro Favero, and Pascal Frossard. Task arithmeticin the tangent space: Improved editing of pre-trained models. Advances in NeuralInformation Processing Systems, 36, 2024. [OJMDF21] Guillermo Ortiz-Jimnez, Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard. Whatcan linearized neural networks actually say about generalization? Advances in NeuralInformation Processing Systems, 34:89989010, 2021.",
  "[Pri23]Simon JD Prince. Understanding Deep Learning. MIT press, 2023": "[PVG+11]F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel,P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau,M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python.Journal of Machine Learning Research, 12:28252830, 2011. [RKR+22]Alexandre Rame, Matthieu Kirchmeyer, Thibaud Rahier, Alain Rakotomamonjy, PatrickGallinari, and Matthieu Cord. Diverse weight averaging for out-of-distribution gen-eralization. Advances in Neural Information Processing Systems, 35:1082110836,2022. [SGd+18]Stefano Spigler, Mario Geiger, Stphane dAscoli, Levent Sagun, Giulio Biroli, andMatthieu Wyart. A jamming transition from under-to over-parametrization affects losslandscape and generalization. arXiv preprint arXiv:1810.09665, 2018. [SIvdS23]Nabeel Seedat, Fergus Imrie, and Mihaela van der Schaar. Dissecting sample hard-ness: Fine-grained analysis of hardness characterization methods. In The TwelfthInternational Conference on Learning Representations, 2023.",
  "[SJ20]Sidak Pal Singh and Martin Jaggi. Model fusion via optimal transport. Advances inNeural Information Processing Systems, 33:2204522055, 2020": "[SKR+23]Rylan Schaeffer, Mikail Khona, Zachary Robertson, Akhilan Boopathy, KaterynaPistunova, Jason W Rocks, Ila Rani Fiete, and Oluwasanmi Koyejo. Double descentdemystified: Identifying, interpreting & ablating the sources of a deep learning puzzle.arXiv preprint arXiv:2303.14151, 2023. [TLZ+22]Vimal Thilak, Etai Littwin, Shuangfei Zhai, Omid Saremi, Roni Paiss, and JoshuaSusskind. The slingshot mechanism: An empirical study of adaptive optimizers and thegrokking phenomenon. arXiv preprint arXiv:2206.04817, 2022.",
  "[VvRBT13] Joaquin Vanschoren, Jan N. van Rijn, Bernd Bischl, and Luis Torgo. Openml: networkedscience in machine learning. SIGKDD Explorations, 15(2):4960, 2013": "[WIG+22]Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, RaphaelGontijo-Lopes, Ari S Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, SimonKornblith, et al.Model soups: averaging weights of multiple fine-tuned modelsimproves accuracy without increasing inference time. In International Conference onMachine Learning, pages 2396523998. PMLR, 2022. [WOBM17] Abraham J Wyner, Matthew Olson, Justin Bleich, and David Mease. Explaining thesuccess of adaboost and random forests as interpolating classifiers. Journal of MachineLearning Research, 18(48):133, 2017. [YHT+21]Yaoqing Yang, Liam Hodgkinson, Ryan Theisen, Joe Zou, Joseph E Gonzalez, KannanRamchandran, and Michael W Mahoney. Taxonomizing local versus global structure inneural network loss landscapes. Advances in Neural Information Processing Systems,34:1872218733, 2021. This appendix is structured as follows: Appendix A presents an extended literature review, Ap-pendix B presents additional theoretical derivations, Appendix C presents an extended discussionof experimental setups and Appendix D presents additional results. The NeurIPS paper checklist isincluded after the appendices.",
  "A.1The model complexity-performance relationship (Sec. 4.1)": "Classical statistical textbooks convey a well-understood relationship between model complexity historically captured by a models parameter count and prediction error: increasing modelcomplexity is expected to modulate a transition between under- and overfitting regimes, usuallyrepresented by a U-shaped error-curve with model complexity on the x-axis in which test error firstimproves before it worsens as the training data can be fit too well [HT90, Vap95, HTF09]. While thisrelationship was originally believed to hold for neural networks as well [GBD92], later work providedevidence that when using parameter counts to measure complexity this U-shaped relationship nolonger holds [NMB+18, Nea19]. Double descent.Instead, the double descent [BHMM19] shape has claimed its place, whichpostulates that the well-known U-shape holds only in the underparameterized regime where thenumber of model parameters p is smaller than the number of training examples n; once we reach theinterpolation threshold p = n at which models have sufficient capacity to fit the training data perfectly,increasing p further into the overparametrized (or: interpolation) regime leads to test error improvingagain. While the double descent shape itself had been previously observed in linear regression andneural networks in [VCR89, BO96, ASS20, NMB+18, SGd+18] (see also the historical note in[LVM+20]), the seminal paper by [BHMM19] both popularized it as a phenomenon and highlightedthat the double descent shape can also occur tree-based methods. In addition to double descent as afunction of the number of model parameters, the phenomenon has since been shown to emerge alsoin e.g. the number of training epochs[NKB+21] and sparsity [HXZQ22]. Optimal regularization hasbeen shown to mitigate double descent [NVKM20]. Due to its surprising and counterintuitive nature, the emergence of the double descent phenomenonsparked a rich theoretical literature attempting to understand it. One strand of this literature hasfocused on modeling double descent in the number of features in linear regression and has producedprecise theoretical analyses for particular data-generating models [BHX20, ASS20, BLLT20, DLM20,HMRT22, SKR+23, CMBK21]. Another strand of work has focused on deriving exact expressionsof bias and variance terms as the total number of model parameters is increased in a neural network bytaking into account all sources of randomness in model training [NMB+18, AP20, dRBK20, LD21].A different perspective was presented in [CJvdS23], who highlighted that in the non-deep doubledescent experiments of [BHMM19], a subtle change in the parameter-increasing mechanism isintroduced exactly at the interpolation threshold, which is what causes the second descent. [CJvdS23]also demonstrated that when using a measure of the test-time effective parameters used by the modelto measure complexity on the x-axes, the double descent shapes observed for linear regression,trees, and boosting fold back into more traditional U-shaped curves. In Sec. 4.1, we show that thetelescoping model enables us to discover the same effect also in deep learning. Benign overfitting. Closely related to the double descent phenomenon is benign overfitting (e.g.[BMM18, MBB18, BLLT20, CL21, MSA+22, WOBM17, HHLS24]), i.e. the observation that,incompatible with conventional statistical wisdom about overfitting [HTF09], models with perfecttraining performance can nonetheless generalize well to unseen test examples. In this literature, it isoften argued in theoretical studies that overparameterized neural networks generalize well becausethey are much more well-behaved around unseen test examples than examples seen during training[MSA+22, HHLS24]. In Sec. 4.1 we provide new empirical evidence for this by highlighting thatthere is a difference between ptrainsand ptests. Understanding modern model complexity. Many measures for model complexity capture someform of capacity of a hypothesis class, which gives insight into the most complex function thatcould be learned e.g. raw parameter counts and VC dimensions [BBL03]. The double descent andbenign overfitting phenomena prominently highlighted that complexity measures that consider onlywhat could be learned and not what is actually learned for test examples, would be unlikely to helpunderstand generalization in deep learning [Bel21]. Further, [CJvdS23] highlighted that many othermeasures for model complexity so-called measures of effective parameters (or: degrees of freedom)including measures from the literature of smoothers [HT90, Ch. 3.5] as well as measures relyingon the models Hessian [Moo91, Mac91] (which have been considered for use in deep learning in[MBW20]) were derived in the context of in-sample prediction (where train- and test inputs wouldbe the same) and do thus not allow to distinguish differences in the behavior of learned functions ontraining examples from new examples. [Cur24] highlight that this difference in setting the movefrom in-sample prediction to measuring performance in terms of out-of-sample generalization iscrucial for the emergence of apparently counterintuitive modern machine learning phenomena suchas double descent and benign overfitting. For this reason, [CJvdS23] proposed an adapted effectiveparameter measure for smoothers that can distinguish the two, and highlighted that differentiatingbetween the amount of smoothing performed on train- vs test examples is crucial to understandingdouble descent in linear regression, trees and gradient boosting. In Sec. 4.1, we show that thetelescoping model makes it possible to use [CJvdS23]s effective parameter measure for neuralnetworks, allowing interesting insight into implied differences in train- and test-time complexity ofneural networks. Grokking. Similar to double descent in the number of training epochs as observed in [NKB+21](where the test error first improves then gets worse and then improves again during training), thegrokking phenomenon [PBE+22] demonstrated the emergence of another type of unexpected be-havior during the training run of a single model. Originally demonstrated on arithmetic tasks, thephenomenon highlights that improvements in test performance can sometimes occur long after perfecttraining performance has already been achieved. [LMT22] later demonstrated that this can also occuron more standard tasks such as image classification. This phenomenon has attracted much recentattention both because it appears to challenge the common practice of early stopping during trainingand because it showcases further gaps in our current understanding of learning dynamics. A numberof explanations for this phenomenon have been put forward recently: [LKN+22] attribute grokkingto delayed learning of representations, [NCL+23] use mechanistic explanations to examine casestudies of grokking, [VSK+23] attribute grokking to more efficient circuits being learned later intraining, [LMT22] attribute grokking to the effects of weight decay setting in later in training and[TLZ+22] attribute grokking to the use of adaptive optimizers. [KBGP24] highlight that the latter twoexplanations cannot be the sole reason for grokking by constructing an experiment where grokkingoccurs as the weight norm grows without the use of adaptive optimizers. Instead, [KBGP24, LJL+24]conjecture that grokking occurs as a model transitions from the lazy regime to a feature learningregime later in training. Finally, [LBBS24] show analytically and experimentally that grokking canalso occur in simple linear estimators, and [MOB24] similarly study grokking outside neural net-works, including Bayesian models. Our perspective presented in Sec. 4.1 is complementary to theselines of work: we highlight that grokking coincides with the widening of a gap in effective parametersused for training and testing examples and that there is thus a quantifiable benign overfitting effect atplay.",
  "A.2Weight averaging in deep learning (Sec. 4.3)": "Ensembling [Die02], i.e. averaging the predictions of multiple independent models, has long estab-lished itself as a popular strategy to improve prediction performance over using single individualmodels. While ensembles have historically been predominantly implemented using weak base learn-ers like trees to form random forests [Bre01], deep ensembles [LPB17] i.e. ensembles of neuralnetworks have more recently emerged as a popular strategy for improving upon the performance ofa single network [LPB17, FHL19]. Interestingly, deep ensembles have been shown to perform wellboth when averaging the predictions of the underlying models and when averaging the pre-activationsof the final network layers [JLCvdS24]. A much more surprising empirical observation made in recent years is that, instead of averagingmodel predictions as in an ensemble, it is sometimes also possible to average the learned weights 1and 2 of two trained neural networks and obtain a model that performs well [IPG+18, FDRC20]. This is unexpected because neural networks are highly nonlinear functions of their weights, so itis unclear a priori when and why averaging two sets of weights would lead to a sensible model atall. When weight averaging works, it is a much more attractive solution relative to ensembling:an ensemble consisting of k models requires k p model parameters, while a weight-averagedmodel requires only p parameters making weight-averaged models both more efficient in terms ofstorage and at inference time. Additionally, weight averaging has interesting applications in federatedlearning because it could enable the merging of models trained on disjoint datasets. [IPG+18] werethe first to demonstrate that weight averaging can work in the context of neural networks by showingthat model weights obtained by simple averaging of multiple points along the trajectory of SGDduring training a weight-space version of the method of fast geometric ensembling [GIP+18] could improve upon using the final solution directly. Mode connectivity. The literature on mode connectivity first empirically demonstrated that thereare simple (but nonlinear) paths of nonincreasing loss connecting different final network weightsobtained from different random initializations [FB16, DVSH18, GIP+18]. As discussed in the maintext, [FDRC20] then demonstrated empirically that two learned sets of weights can sometimes belinearly connected by simply interpolating between the learned weights, as long as two models weretrained together until some stability point t. [ABNH23] perform an empirical study investigatingwhich networks and optimization protocols lead to mode connectivity from initialization (i.e. t = 0)and which modifications ensure t > 0. As highlighted in Sec. 4.3, our theoretical reasoning indicatesthat one sufficient condition for linear mode connectivity from initialization is that models stay in aregime in which the model gradients do not change during training. In the context of task arithmetic,where parameters from models finetuned on separate tasks are added or subtracted (not averaged) toadd or remove a skill, [OJFF24] find that pretrained CLIP models that are finetuned on separate tasksand allow to perform task arithmetic do not operate in a regime in which gradients are constant. Methods that average weights. Beyond [IPG+18]s stochastic weight averaging method, whichaverages weights from checkpoints within a single training run, weight averaging has also recentlygained increased popularity in the context of averaging multiple models finetuned from the same pre-trained model [NSZ20, WIG+22, CVSK22]: while [NSZ20] showed that multiple models finetunedfrom the same pretrained model lie in the same loss basin and are linearly mode connectible, themodel soups method of [WIG+22] highlighted that simply averaging the weights of multiple modelsfine-tuned from the same pre-trained parameters with different hyperparameters leads to performanceimprovements over choosing the best individual fine-tuned model. A number of methods have sincebeen proposed that use weight-averaging of models fine-tuned from the same pretrained modelfor diverse purposes (e.g. [RKR+22, IWG+22]). Our results in Sec. 4.3 complement the findingsof [NSZ20] by investigating whether fine-tuning from a pre-trained model leads to better modeconnectivity because the gradients of a pre-trained model remain more stable than those trained froma random initialization. Weight averaging after permutation matching. Most recently, a growing number of papers haveinvestigated whether attempts to merge models through weight-averaging can be improved by firstperforming some kind of permutation matching that corrects for potential permutation symmetriesin neural networks. [ESSN21] conjecture that all solutions learned by SGD are linearly modeconnectible once permutation symmetries are corrected for. [SJ20, AHS22, BSM+22] use differentmethods for permutation matching and find that this improves the quality of weight-averaged models.",
  "B.1Derivation of smoother expressions using the telescoping model": "Below, we explore how we can use the telescoping model to express a function learned by aneural network as ft(x) = st(x)y + c0t(x), where the 1n vector st(x) is a function of thekernels {Ktt(, )}tt, and the scalar c0t(x) is a function of the {Ktt(, )}tt and the networksinitialization f0(). Note that, as discussed further in the remark at the end of this section, the kernelsKtt(, ) for t > 1 are data-adaptive as they can change throughout training. Vanilla SGD. Recall that letting y = [y1, . . . , yn] and ft= [ft(x1), . . . , ft(xn)], theSGD weight update with squared loss (f(x), y) =12(y f(x))2, in the special case of sin-gle outputs k = 1, simplifies to t = tTt(y ft1), where Tt is the p n matrix",
  "(15)": "where the p n matrix Tt = [ft1(x1), . . . , ft1(xn)] differs from Tt only in that itincludes all training examples and is not normalized by batch size. Then note that Eq. (15) is indeed afunction of the training labels y, the predictions at initialization f0 and the model gradients {Tt}Tt=1traversed during training (captured in the nn matrix ST and the n1 vector cT ) alone. Similarly,we can also write the weight updates (and, by extension, the weights T ) using the same quantities, i.e.t = tTt(In St1)y tTtct1. By Eq. (5), this also implies that we can write predictionsat arbitrary test input points as a function of the same quantities:",
  "ft(x) instead of Tt alone; then defining the matrices similarly proceeds by recursively unravelingupdates using ft(x) = tft1(x)diag( 1": "t )Tt(y ft1). Both momentum and weightdecay lead to somewhat more tedious updates and necessitate the introduction of additional notation.Let st(x) = st(x) st1(x), with s0(x) = 01n and ct(x) = ct(x) ct1(x), withc0(x) = f0(x), so that sT (x) = Tt=1 st(x) and cT (x) = f0(x) + Tt=1 ct(x). Further,we can write",
  "where all terms are as in Eq. (19) and Eq. (20)": "Remark: Writing fT = ST y + cT is reminiscent of a smoother as used in the statistics literature[HT90]. Prototypical smoothers issue predictions y = Sy which include k-Nearest Neighborregressors, kernel smoother, and (local) linear regression as prominent members , and are usuallylinear smoothers because S does not depend on y. The smoother implied by the telescoping model isnot necessarily a linear smoother because ST can depend on y through changes in gradients duringtraining, making fT an adaptive smoother. This adaptivity in the implied smoother is similar to treesas recently studied in [CJvdS23, CJvdS24]. In this context, effective parameters as measured by p0scan be interpreted as measuring how non-uniform and extreme the learned smoother weights arewhen issuing predictions for specific inputs [CJvdS23].",
  "Here, we compare the predictions of the weight-averaged model ft1T +(1)t2T (x) to the ensemblef (x) = ft1T (x) + (1 )ft2T (x) if the models transition into a lazy regime at time t t": "We begin by noting that the assumption that the gradients no longer change after t (i.e. ftjt() ft () for all t t) implies that the rate of change of ft (x) in the direction of theweight updates must be approximately 0. That is, 2ft (x)( t) 0 for all stablej,or equivalently all weight changes in each stablejare in directions that are in the null-space of theHessian (or in directions corresponding to diminishingly small eigenvalues). To avoid clutter innotation, we use splitting point t = t below, but note that the same arguments hold for t > t.",
  "C.1Case study 1 (Sec. 4.1) and approximation quality experiment (Sec. 3, )": "Double descent experiments.In , we replicate [BHMM19, Sec. S.3.3]s only binary classifi-cation experiment which used fully connected ReLU networks with a single hidden layer trained usingthe squared loss, without sigmoid activation, on cat and dog images from CIFAR-10 [KH+09]. Like[BHMM19], we grayscale and downsize images to d = 88 format and use n = 1000 training exam-ples and use SGD with momentum 1 = 0.95. We use batch size 100 (resulting in B = 10 batches),learning rate = 0.0025, and test on ntest = 1000 held out examples. We train for up to e = 30000epochs, but stop when training accuracy reaches 100% or when the training squared loss does notimprove by more than 104 for 500 consecutive epochs (the former strategy was also employed in[BHMM19], we additionally employ the latter to detect converged networks). We report results us-ing {1, 2, 5, 7, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 70, 85, 100, 200, 500, 1000, 2000, 5000} hiddenunits. We repeat the experiment for 4 random seeds and report mean and standard errors in allfigures. In Appendix D.2, we additionally repeat this experiment with the same hyperparameters using MNISTimages [LBBH98]. To create a binary classification task, we similarly train the model to distinguish3-vs-5 from n = 1000 images downsampled to d = 8 8 format and test on 1000 examples. Likelybecause the task is very simple, we observe no deterioration in test error in this setting for any hiddensize (see ). Because [NKB+21] found that double descent can be more apparent in the presenceof label noise, we repeat this experiment while adding 20% label noise to the training data, in whichcase the double descent shape in test error indeed emerges. As above, we repeat both experiments for4 random seeds and report mean and standard errors in all figures. Further, in Appendix D.2 we additionally utilize the MNIST-1D dataset [GK24] which was pro-posed recently as a sandbox for investigating empirical deep learning phenomena.We repli-cate a binary classification version of their MLP double descent experiment with added 15%label noise from [GK24] (which was itself adapted from the textbook [Pri23]). We select onlyexamples with label 0 and 1, and train fully connected neural networks with a single hiddenlayer with batch size 100, learning rate = 0.01 for 500 epochs, considering models with hidden units.",
  "d and targets y are generatedas y(x) = 1": "2(x)2. In this setup, used in the activation function of the network controls howeasy it is to fit the outcome function (the larger , the better aligned it is for the task at hand), whichin turn controls whether grokking appears. In the main text, we present results using = .2; inAppendix D.2 we additionally present results using = .05 and = 0.5. Like [KBGP24], we used = 100, ntrain = 550, ntest = 500, initialize all weights using standard normals, and train usingfull-batch gradient descent with = B = 500 on the squared loss. We repeat the experiment for 5random seeds and report mean and standard errors in all figures. In panel (2) of , we report an adapted version of [LMT22]s experiment reporting grokkingon MNIST data. To enable the use of our model, we once more consider the binary classificationtask 3-vs-5 from n = 1000 images downsampled to d = 8 8 features and test on 1000 held-outexamples. Like [LMT22], we use a 3-layer fully connected ReLU network trained with squaredloss (without sigmoid activation) and larger than usual initialization by using 0 instead of thedefault initialization 0. We report = 6 in the main text and include results with = 5 and = 7 in Appendix D.2. Like [LMT22] we use the AdamW optimizer [LH17] with batches of size200, 1 = .9 and 2 = .99, and use weight decay = .1. While [LMT22] use learning rate 103,we need to reduce this by factor 10 to = 104 and additionally use linear learning rate warmupover the first 100 batches to ensure that weight updates are small enough to ensure the quality ofthe telescoping approximation; this is particularly critical because of the large initialization whichotherwise results in instability in the approximation early in training. Panel (C) of uses anidentical setup but lets = 1 (i.e. standard initialization) and additionally applies a sigmoid to theoutput of the network. We repeat these experiments for 4 random seeds and report mean and standarderrors in all figures. Compute: Replicating [KBGP24]s experiments required training num_settings num_seeds(3 5 = 15) models for T = 100, 000 gradient steps. Each training run including all gradientcomputations took less than 1 hour to complete. Replicating [LMT22]s experiments requiredtraining num_settings num_seeds (3 4 = 12) for T = 100, 000 gradient steps. Each trainingrun including all gradient computations took around 5 hours to complete. The MNIST experimentswith standard initialization required training num_settingsnum_seeds (24 = 8) for T = 1000gradient steps, these took no more than 2 hours to complete in total. Approximation quality experiment ()The approximation quality experiment uses theidentical MNIST setup, training process and architecture as in the grokking experiments (differingonly in that we use standard initialization and no learning rate warmup). In addition to the vanillaSGD and AdamW experiments presented in the main text, we present additional settings usingmomentum alone, weight decay alone and using sigmoid activation in Appendix D.1. In particular,we use the following hyperparameter settings for the different panels:",
  "C.2Case study 2 (Sec. 4.2)": "In Figs. 5 and 14 we provide results on tabular benchmark datasets from [GOV22]. We select fourdatasets with > 20,000 examples (houses, superconduct, california, house_sales) to ensurethere is sufficient hold-out data for evaluation across irregularity proportions. We apply standardpreprocessing including log transformations of skewed features and target rescaling. As discussedin the main text, irregular examples are defined by first projecting each (normalized) datasets inputfeatures onto its first principal component and then calculating each examples absolute distance tothe empirical median in this space. We note that several recent works have discussed metrics of anexamples irregularity or hardness (e.g. [KAF+24, SIvdS23]) finding the choice of metric to behighly context-dependent. Therefore we select a principal component prototypicality approach basedon its simplicity and transparency. The top K irregular examples are removed from the data (theseform the irregular examples at test-time) and the remainder (the regular examples) is split intotraining and testing. We then construct test datasets containing 4000 examples, constructed from amixture of standard test examples and irregular examples according to each proportion p. We train both a standard neural network (while computing its telescoping approximation as de-scribed in Eq. (5)) and a gradient boosted tree model (using [PVG+11]) on the training data.We select hyperparameters by further splitting the training data to obtain a validation set of size2000 and applying a random search consisting of 25 runs. We use the search spaces suggested in[GOV22]. Specifically, for GBTs we consider learning_rate LogNormal[log(0.01), log(10)],num_estimators LogUniformInt[10.5, 1000.5], and max_depth [None, 2, 3, 4, 5] with respec-tive probabilities [0.1, 0.1, 0.6, 0.1, 0.1]. For the neural network, we consider learning_rate LogUniform[1e 5, 1e 2] and set batch_size = 128, num_layers = 3, and hidden_dim= 64 with ReLU activations throughout. Each model is then trained on the full training set with itsoptimal parameters and is evaluated on each of test sets corresponding to the various proportions ofirregular examples. All models are trained and evaluated for 4 random seeds and we report the meanand a standard error in our results.",
  "TTt=1 maxjIptest ||kt(xj)||": "1TTt=1 maxiItrain ||kt(xi)||, which measures how the kernelsbehave at their extreme during testing relative to the maximum of the equivalent values measured forthe training examples such that the test values can be interpreted relative to the kernel at train time(i.e. values > 1 can be interpreted as being larger than the largest value observed across the entiretraining set). Compute: The hyperparameter search results in num_searches num_datasets num_models(25 4 2 = 200) training runs and evaluations. Then the main experiment requires num_seeds num_datasets num_models (4 4 2 = 32) training runs and num_seeds num_datasets num_models num_proportions (4 4 2 5 = 160) evaluations. This results in a total of 232training runs and 360 evaluations. Individual training and evaluation times depend on the model anddataset but generally require < 1 hour.",
  "C.3Case study 3 (Sec. 4.3)": "In we follow the experimental setup described in [FDRC20]. Specifically, for each model wetrain for a total of 63,000 iterations over batches of size 128 with stochastic gradient descent. At apredetermined set of checkpoints (t )we create two copies of the current state of the network and train until completion with different batchorderings, where linear mode connectivity measurements are calculated. This process sometimes alsoreferred to as spawning [FDP+20] and is repeated for 3 seeds at each t. The entire process is repeatedfor 3 seeds resulting in a total of 3 3 = 9 total values over which we report the mean and a standarderror. Momentum is set to 0.9 and a stepwise learning rate is applied beginning at 0.1 and decreasingby a factor of 10 at iterations 32,000 and 48,000. For the ResNet-20 architecture [HZRS16], we use",
  "an implementation from [Ide]. Experiments are conducted on CIFAR-10 [KH+09] where the inputsare normalized with random crops and random horizontal flips used as data augmentations": "Pretraining of the finetuned model model is performed on the SVHN dataset [NWC+11] which isalso an image classification task with identically shaped input and output dimensions as CIFAR-10.We use a training setup similar to that of the CIFAR-10 model but set the number of training iterationsto 30,000 and perform the stepwise decrease in learning rate at iterations 15,000 and 25,000 decayingby a factor of 5. Three models are trained following this protocol which achieve validation accuracyof 95.5%, 95.5%, and 95.4% on SVHN. We then repeat the CIFAR-10 training protocol for finetuningbut parameterize the three initialization with the respective pretrained weights rather than randominitialization. We also find that a shorter finetuning period is sufficient and therefore finetune for12,800 steps with the learning rate decaying by a factor of 5 at steps 6,400 and 9,600. Also following the protocol of [FDRC20], for each pair of trained spawned networks (f1&f2) weconsider interpolating their losses (i.e. avg:= (f1(x), y)+(1)(f2(x), y)) and parameters(i.e. lmc:= (flmc(x), y) where lmc = 1 + (1 )2) for 30 equally spaced values of .In the upper panel of we plot the accuracy gap at each checkpoint t (i.e. the point fromwhich two identical copies of the model are made and independently trained to completion) whichis simply defined as the average final validation accuracy of the two individual child models minusthe final validation accuracy of the weight averaged version of these two child models. Beyond theoriginal experiment, we also wish to evaluate how the gradients ft() evolve throughout training.Therefore, in panels (2) and (3) , at each checkpoint t we also measure the mean squaredchange in (pre-softmax) gradients (ft+390(x) ft (x))2 between the current iteration t",
  "and those at the next epoch t + 390, averaged over a set of n = 256 test examples and the parametersin each layer": "Compute:We train num_outer_seeds num_inner_seeds num_child_models num_checkpoints (3 3 2 12 = 216) networks for the randomly initialized model. Forthe finetuned model this results in 3 3 2 10 = 180 training runs. Additionally, we require thepertaining of the 3 base models on SVHN. Combined this results in a total of 216 + 180 + 3 = 399training runs. Training each ResNet-20 on CIFAR-10 required <1 hour including additional gradientcomputations.",
  "C.4Data licenses": "All image experiments are performed on CIFAR-10 [KH+09], MNIST [LBBH98], MNIST1D[GK24], or SVHN [NWC+11].Tabular experiments are run on houses, superconduct,california, and house_sales from OpenML [VvRBT13] as described in [GOV22]. CIFAR-10 is released with an MIT license. MNIST is released with a Creative Commons Attribution-ShareAlike 3.0 license. MNIST1D is released with an Apache-2.0 license. SVHN is released with aCC0:Public Domain license. OpenML datasets are released with a 3-Clause BSD License. All thedatasets used in this work are publicly available.",
  "D.1Additional results on approximation quality (supplementing )": ": Approximation error of the telescoping ( ft(x), red) and the model linearized around theinitialization (f lint (x), gray) by optimization step for different optimization strategies and other designchoices. Iteratively telescoping out the updates using ft(x) improves upon the lazy approximationaround the initialization by orders of magnitude. : Test accuracy of the telescoping ( ft(x), red, top row) and the model linearized aroundthe initialization (f lint (x), blue, bottom row) against accuracy of the actual neural network (gray) byoptimization step for different optimization strategies and other design choices. While the telescopingmodel visibly matches the accuracy of the actual neural network, the linear approximation around theinitialization leads to substantial differences in accuracy later in training. In , we present results investigating the evolution of approximation errors of the telescopingand linear approximation around the initialization during training using additional configurationscompared to the results presented in in the main text (replicated in the first two columns of). We observe the same trends as in the main text, where the telescoping approximation matchesthe predictions by the neural network by orders of magnitudes better than the linear approximationaround the initialization. Importantly, we highlight in that this is also reflected in how welleach approximation matches the accuracy of the predictions of the real neural network: while thesmall errors of the telescoping model lead to no visible differences in accuracy compared to the realneural network, using the Taylor expansion around the initialization leads to significantly differentaccuracy later in training.",
  "D.2Additional results for case study 1: Exploring surprising generalization curves andbenign overfitting": ": Double descent experiments using MNIST, distinguishing 3-vs-5, with 20% added labelnoise during training (left) and no added label noise (right). Without label noise, there is no doubledescent in error on this task; when label noise is added we observe the prototypical double descentshape in test error. Double descent on MNIST.In , we replicate the CIFAR-10 experiment from the maintext while training models to distinguish 3-vs-5 on MNIST. We find that in the absence of labelnoise, no problematic overfitting occurs for any hidden size; both train and test error monotonicallyimprove with increased width. Only when we add label noise to the training data, do we observe the : Double descent experiment using MNIST-1D, distinguishing class 0 and 1, with 15%added label noise during training. Mean squared error (top) and effective parameters (bottom) fortrain and test examples by number of hidden neurons. characteristic double descent behavior in error this is in line with [NKB+21]s observation thatdouble descent can be more pronounced when there is noise in the data. Importantly, we observe thatas in the main text, the improvement of test error past the interpolation threshold is associated withthe divergence of effective parameters used on train and test data. In we additionally repeatthe experiment using the MNIST-1D dataset with 15% labelnoise as in [GK24], and find that thedecrease in test error after the interpolation threshold is again accompanied by a decrease in effectiveparameters as the number of raw model parameters is further increased in the interpolation regime. Additional grokking results.In , we replicate the polynomial grokking results of [KBGP24]with additional values of . Like [KBGP24], we observe that larger values of = 0.5 lead to lessdelayed generalization. This is reflected in a gap between effective parameters on test and trainemerging earlier. With very small = .05, conversely, we even observe a double descent-likephenomenon where test error first worsens before it improves later in training. This is reflectedalso in the effective parameters, where ptestsfirst exceeds ptrainsbefore dropping below it as benignoverfitting sets in later in training. In , we replicate the MNIST results with additional valuesof ; like [LMT22] we observe that grokking behavior is more extreme for larger . This is indeedalso reflected in the gap between ptestsand ptrainsemerging later in training. Additional training results on MNIST with standard initialization.In , we present trainand test results on MNIST with standard initialization to supplement the test results presented in themain text. Both with and without sigmoid, train and test behavior is almost identical, and learning isorders of magnitude faster than with the larger initialization. The stronger inductive biases of smallinitialization, and additionally using sigmoid activation, lead to much lower learned complexity onboth train and test data as measured by effective parameters.",
  "D.3Additional results for Case study 2: Understanding differences between gradientboosting and neural networks": "In , we replicate the experiment from Sec. 4.2 on three further datasets from [GOV22]s tabularbenchmark. We find that the results match the trends present in in the main text: the neuralnetwork is outperformed by the GBTs already at baseline, and the performance gap grows as the testdataset becomes increasingly more irregular. The growth in the gap is tracked by the behavior of thenormalized maximum kernel weight norm of the neural networks kernel. Only on the californiadataset do we observe a slightly different behavior of the neural networks kernel: unlike the other"
}