{
  "Abstract": "Large Language Models (LLMs) have demonstrated remarkable generalization andinstruction-following capabilities with instruction tuning. The advancements inLLMs and instruction tuning have led to the development of Large Vision-LanguageModels (LVLMs). However, the competency of the LLMs and instruction tuninghave been less explored in the molecular domain. Thus, we propose LLaMo:Large Language Model-based Molecular graph assistant, which is an end-to-end trained large molecular graph-language model. To bridge the discrepancybetween the language and graph modalities, we present the multi-level graphprojector that transforms graph representations into graph tokens by abstractingthe output representations of each GNN layer and motif representations with thecross-attention mechanism. We also introduce machine-generated molecular graphinstruction data to instruction-tune the large molecular graph-language model forgeneral-purpose molecule and language understanding. Our extensive experimentsdemonstrate that LLaMo shows the best performance on diverse tasks, such asmolecular description generation, property prediction, and IUPAC name prediction.The code of LLaMo is available at",
  "Introduction": "In recent years, molecular machine learning has received significant attention, addressingdiverse tasks in the chemical domain. The predominant approach for molecular tasks is graph machinelearning that leverages the molecular graph structure, which is a natural and expressiverepresentation of molecules. Although graph-based methods have successfully represented molecules,they have limited interpretability and incompatibility to solve multi-modal molecular tasks dealingwith pairs of texts and molecules. To address these issues, recent works train both a languagemodel and a graph encoder with cross-modal contrastive learning. However, the models trained withcross-modal contrastive learning are insufficient to perform open-ended molecule-to-text generationtasks , which are more applicable to practical use. Large Language Models (LLMs) have shown impressive progress and accomplishedhuman-like open-ended text generation with the power of billions of parameters. To leverage theinstruction-following capability of LLMs, many works employ instruction-tuning approaches for general-purpose language models. Motivated by the development of LLMs and instructiontuning, Large Vision-Language Models (LVLMs) have recently been explored and achieved successon image comprehension and image-to-text generation tasks . Despite thesuccess of LLM-based approaches on natural language processing and machine vision domains,the research on the integration of language models and molecular graphs has been less studieddue to the lack of consideration of the architecture design of Large Molecular Graph-LanguageModels (LMGLMs) and the molecular graph instruction data.",
  "arXiv:2411.00871v1 [cs.LG] 31 Oct 2024": "In this paper, we propose LLaMo: Large Language Model-based Molecular graph assistant, whichseamlessly integrates a molecular graph encoder and a large language model to enable the instruction-following response generation in molecular domain. Specifically, LLaMo consists of the moleculargraph encoder, large language model, and multi-level graph projector that bridges the graph encoderand large language model. The multi-level graph projector abstracts the representation of each GNNlayer and motif representation using a cross-attention mechanism, ensuring a thorough understandingof molecular structures. Furthermore, we introduce machine-generated molecular graph instructiondata through the pipeline to convert molecular descriptions and IUPAC names into a multi-turnconversation format. The generated instruction-following data enhances the models ability toperform general-purpose molecule and language understanding, bridging the gap between moleculargraph analysis and language-based tasks. Our proposed LLaMo outperforms the LLM-based workssuch as GPT-4 across diverse tasks, including molecular description generation, property prediction,and IUPAC name prediction.",
  "Our contributions are summarized as follows:": "We propose LLaMo: Large Language Model-based Molecular graph assistant consisting ofgraph encoder, language model, and multi-level graph projector equipped with a multi-levelgraph projector that captures rich information of the graph structure at multiple levels. We introduce GPT-4 generated molecular graph-text multi-turn conversation data to addressthe data scarcity problem of molecule-text datasets and improve the instruction-followingcapabilities of a large molecular graph-language model.",
  "Related works": "Molecular graph modeling. Molecular graphs serve as a natural and expressive representation ofmolecules, effectively capturing the structural information. Graph neural networks arecommonly utilized architectures for molecular graph representations. To learn graph neural networkswith the limited molecular graph data , self-supervised learning has been explored. For example,various approaches have been developed to capture multi-level features of molecular graphs,such as node-level masked atom modeling , motif-based self-supervised learning , andgraph-level contrastive learning . With the advance of multi-modal large language models,molecule-language tasks such as molecule-text retrieval or molecule captioning have recentlydrawn significant attention. Recent works have attempted to enable language models tounderstand molecular graphs. treated nodes of molecular graphs as tokens of language models.Some works have adopted GNN-based encoders, either by propagating their outputs to languagemodels through MLP or employing cross-modal projectors . However, these methods failto consider molecular graphs at multiple levels and are hindered by inherent limitations of graphencoders, such as the over-smoothing problem . To address these challenges, we propose a novelarchitecture, LLaMo, which effectively propagates multi-level information of molecular graphs tolanguage models. Instruction tuning. Recent advancements of LLMs lead to extensive research on instruction tuning,aimed at improving the models capability to follow human instructions . Toconstruct high-quality instruction tuning data, a line of previous approaches has adoptedexisting human-annotated datasets and integrated them with a new structure and template. On theother hand, recent studies on instruction tuning have collected data samples from strongLLMs like GPT-4 . These works first manually construct annotated seed instruction samples andexpand them by prompting LLMs. As a result, several instruction-tuned LLMs havebeen proposed from the open-source LLMs, e.g., LLaMA and shown generalizability across awide range of instructions. More recently, those studies on instruction tuning have been expandedto visual instruction tuning in image and video domain to enable the model tounderstand the visual contents. Inspired by the instruction tuning for multi-modal LLMs in otherdomains, in this work, we study instruction tuning specifically for molecule graphs, which has beenunderexplored in the literature.",
  "Graph Projector": ": Overall framework of LLaMo. LLaMo consists of a graph neural network, a multi-levelgraph projector, and a large language model. It first encodes an input 2D molecular graph with thegraph neural network and then converts the encoded graph into molecular graph tokens with themulti-level graph projector. Finally, the large language model generates the instruction-followingresponse given the input SMILES, graph tokens, and the instruction.",
  "LLaMo: Large Language Model-based Molecular Graph Assistant": "The primary goal is to seamlessly integrate a molecular graph encoder and a Large LanguageModel (LLM) to generate instruction-following responses to the input texts and molecules. Toachieve it, we propose LLaMo: Large Language Model-based Molecular graph assistant, a general-purpose Large Molecular Graph-Language Model (LMGLM) equipped with a multi-level graphprojector. Specifically, the proposed framework utilizes three input modalities: 1D SMILES , 2Dmolecular graph, and text (instruction). SMILES is a 1D representation of a molecule, and a 2Dmolecular graph is processed by a GNN. The three input modalities are fed as a sequence of tokensand our LLaMo autoregressively generates text responses. Formally, given SMILES S, moleculargraph tokens G, and text (instruction) T, the proposed method renders the response Y = {yi}Ki=1 as:",
  "Model Architecture": "The overall architecture of LLaMo is illustrated in . LLaMo consists of a graph encoder, amulti-level graph projector, and a backbone large language model. The graph encoder g() takes a2D molecule graph as an input and outputs their node representations as a sequence of tokens. Themulti-level graph projector ProjMG() transforms the sequence of node representations into moleculartokens to align them with the LLM. Then, the LLM f() processes molecular and text tokens andprovides a response in an autoregressive manner. Graph encoder. We adopt Graph Neural Networks (GNNs) as a molecular graph encoder. Giventhe graph G, graph neural networks g () iteratively update node representation z(l)v Rd(l) via themessage-passing framework. With the message-passing, L-layer GNN provides node representationsz(L)vthat express an L-hop ego-graph given the node v as a center node. More details about graphneural networks are in the Appendix C.",
  ": Node representations of graph encoder with 1,2,4,5 layers. As the number of layersincreases, node representations collapse": "Multi-level graph projector. The goal of a multi-level graph projector is to align the graph encoderwith the LLM by transforming a set of node representations Z(L)graph into a sequence of molecular graphtokens Hgraph. It enables the language model to utilize graph information. In the literature, projectorshave been proposed mainly for Large Vision-Language Models (LVLMs) . Theyare usually implemented using a linear projection or an abstraction of visual features ,which are outputs of the final layer of a visual encoder given input image. Analogously, we can designthe projector for large molecular graph-language models with a linear projection or an abstraction ofhigh-level node representations from the pre-trained graph encoder, which is formulated as:",
  "Rd(L) from L-th layer GNN and Proj () is the projector": "However, we observe that the high-level representation is not effective in capturing the local infor-mation due to the over-smoothing problem , which means that the node representations becomeindistinguishable, as the number of layers in the GNN increases. depicts node represen-tations (yellow dots) of graph encoder with 1,2,4,5 layers on one molecular graph sample. (Moresamples are in Appendix I.) As mentioned above, node representations become over-smoothed asthe number of layers increases, leading to nearly identical node representations in the final layer.Consequently, conventional projectors relying on high-level node representations have a limitedcapability to preserve the detailed or local information of molecular graphs. Moreover, many tasksrequire multi-scale information, including atom, atomic group, and molecule levels. Hence, theprojector that solely utilizes features from the top layer is suboptimal for the tasks. Motivated by the observations, we propose a novel multi-level graph projector to generate graphtokens that contain richer information reflecting the graph structure at multiple levels. The multi-levelgraph projector ProjMG () is formulated as",
  "l=0 = g (G) .(3)": "The method captures multi-hop graph information by leveraging node representations from all layersof a GNN. To handle an arbitrary number of nodes, yielding a variable length |V| L features, weadopt the cross-attention with learnable tokens P(l) =p(l)1 , . . . , p(l)b Rbd for l = 0, . . . , L,where b is the number of learnable prompts. Here, indicates the concatenation operation. Thelearnable tokens aggregate l-th layer GNN representations into a fixed number of tokens as:",
  "where Attn (Q, K, V ) is the attention operation with query Q, key K, and value V": "For more detailed representations of the input molecule, LLaMo also has learnable tokens P(motif)for motif-level representations. We use the functional groups as motifs, which are the statisticallyimportant subgraphs in the molecular graphs. To construct functional group representations ZFG, weinitially identify functional groups, following . Then, we vectorize the main characteristics of eachfunctional group, which is represented as zFG,i. Finally, the functional group representations ZFG",
  "(b) Stage 2: instruction-tuning": ": Two-stage training pipeline. Stage 1 involves training the graph encoder, and stage 2 entailsfine-tuning the LLM using LoRA. In both stages, the multi-level graph projector is continuouslytrained. All training processes are performed by generating the instruction-following response. are constructed by concatenating all individual functional group representations zFG,i, which isformulated as ZFG = [zFG,0, . . . , zFG,M], where M indicates the number of the functional groups inthe given molecular graph. Given the functional group representations ZFG of the input molecule, weobtain P(motif) = Attn(motif) P(motif), ZFG, ZFGwith the cross-attention.",
  "Similar to most LVLMs , we train LLaMo in the two-stage pipeline: (1) pre-training formolecular graph-language alignment and (2) instruction-tuning end-to-end as in": "Stage 1. Pre-training for molecular graph-language alignment. The first stage focuses on thealignment between the graph encoder and a large language model by learning our multi-level graphprojector. In this stage, with the LLM frozen, we train the multi-level graph projector and thegraph encoder by generating molecule descriptions. For training, we use a molecule-description pairdataset (e.g., PubChem ) consisting of a 1D SMILES representation of molecule and moleculargraph and its corresponding description. Stage 2. Instruction-tuning end-to-end. In the second stage, we train the LLM to enhance theinstruction-following capabilities and enable a deeper understanding of molecular graphs. In thisstage, we freeze the graph encoder and train both the multi-level graph projector and the LLM. Since itis too expensive to train the full LLM, we employ LoRA to adapt LLM to the data. For instruction-following, we use the GPT-generated instruction-following multi-turn conversation dataset, whichwill be introduced in . In addition to our generated instruction-following dataset, we use adiverse set of datasets with various instructions: molecule description generation, molecular propertyprediction, IUPAC name generation, forward reaction prediction, and retrosynthesis datasets.",
  "GPT-assisted Molecular Graph Instruction Data Generation": "Instruction data are essential for improving the instruction-following capabilities of LLM-basedmodels. Despite active research on instruction-tuning, the instruction-following data for moleculargraphs have been less explored in the literature since annotations require expertise. To alleviate the : One example to illustrate the instruction-following data. The top block shows the contextssuch as SMILES, description, and IUPAC name used to prompt GPT, and the bottom block shows theresponse of GPT-4. Context type 1: SMILES representationCCCCC(C)/C=C(\\C)/C=C/C(=O)NC1=C[C@]([C@@H](CC1=O)O)(/C=C/C=C/C=C/C(=O)NC2=C(CCC2=O)O)OContext type 2: DescriptionThe molecule is a polyene antibiotic that is TMC-1A in which the 2,4-dimethyloct-2-enoyl group hasbeen replaced by an (E,E)-4,6-dimethyldeca-2,4-dienoyl group. TMC-1C is an antitumour antibioticisolated from Streptomyces sp. A-230...<omitted>Context type 3: IUPAC name(2E,4E)-N-[(3S,4R)-3,4-dihydroxy-3-[(1E,3E,5E)-7-[(2-hydroxy-5-oxocyclopenten-1-yl)amino]-7-oxohepta-1,3,5-trienyl]-6-oxocyclohexen-1-yl]-4,6-dimethyldeca-2,4-dienamide Response: Multi-turn conversationQuestion:What is the IUPAC name of the molecule you are analyzing?===Answer:The IUPAC name of the molecule is (2E,4E)-N-[(3S,4R)-3,4-dihydroxy-3-[(1E,3E,5E)-7-[(2-hydroxy-5-oxocyclopenten-1-yl)amino]-7-oxohepta-1,3,5-trienyl]-6-oxocyclohexen-1-yl]-4,6-dimethyldeca-2,4-dienamide.===Question:Can you identify the type of acid or base this molecule can act as in a reaction?===Answer:This molecule can act as a weak acid due to the presence of the carboxylic acid group.",
  "need for expertise and minimize the manual efforts, we utilize GPT-4 to generate moleculargraph-text instruction-following data using graph-text pair datasets": "Inspired by previous results , we construct multi-turn conversation datasets, which are morediverse and effective for instruction tuning compared to simple pairs of questions and answers. Weleverage GPT-4 to generate multi-turn conversations with tailored contexts/prompts that consist of tworepresentations for molecular graphs and description: (i) SMILES representation that describes thechemical structures with special strings, (ii) captions that explain the molecule, and (iii) IUPAC namethat describes the molecule based on its chemical composition and structure. These representationsenable the GPT-4, which inherently lacks in-depth molecular knowledge, to understand and generatea diverse and high-quality set of examples. One example of the input representations is shown in thetop block of . Specifically, we generate the multi-turn conversation data in three steps: 1) select exemplar con-versations among machine-generated instruction-tuning data, 2) generate multi-turn conversationsvia in-context learning with the exemplar conversations as prompts, and 3) filter out incompleteconversations and those with many turns. In the first step, we generate exemplars with a briefhuman-written instruction as shown in Appendix H. However, we found that GPT-4 frequently fails togenerate complete multi-turn conversations without the exemplars. To address this issue, we generatethe instruction data with in-context learning. We sample exemplars from a small set of completeconversations generated by GPT-4 in the first step. Then, GPT-4 generates the complete multi-turnconversation data for the instruction tuning guided by the prompts wrapped with the generatedexemplars. To validate the quality of the generated conversation, we sample 500 subsets generatedvia in-context learning. We find that some conversations consisting of a large number of turns areprone to generating incomplete and inaccurate outputs. So, we filter out incomplete conversationsand those with many turns. The example of the generated multi-turn conversation is in the bottomblock of . In total, we generate 12K unique molecular graph-language instruction-followingsamples using PubChem324k dataset . : Performance (%) of generalist models on three tasks: molecule description generation,IUPAC prediction, and property prediction. Mol. Inst. tuned denotes the molecular instruction-tunedmodel. The result is not available since LLaMA2 fails generating numerical outputs. denotes theexperimental results drawn from Mol-Instruction .",
  "Experimental Settings": "Benchmarks. To evaluate the efficacy of the proposed method, we evaluate the model for three taskssuch as 1) molecule description generation, 2) IUPAC name prediction, 3) property prediction (re-gression). We conducted experiments under two major settings: generalist and specialist models. Inthe generalist setting, one model handles all three tasks, whereas in the specialist setting, we train amodel for each downstream task. More details about benchmarks are in Appendix G. Implementation details.For the generalist models,we train our LLaMo based onLlama-2-7b-chat for a fair comparison with Mol-Instructions . For the specialist models,we train our LLaMo with Galactica 1.3B for a fair comparison with MolCA . To train thegeneralist variant of LLaMo, we use a training split of molecular description generation dataset ofMol-Instruction in stage 1. In stage 2, the model is instruction-tuned with a training split ofdescription generation, property prediction, forward reaction, and retrosynthesis instruction dataset ofMol-Instruction , IUPAC name prediction from , and our GPT-generated instruction-followingdata. To train the specialist variant of LLaMo, we follow MolCA to train the model with apretraining split of PubChem324kV2 in the stage 1 phase and fine-tune the model for each specificdownstream task in the stage 2. We adopt a long training schedule (epoch 1 pre-training, epoch3 instruction tuning) for the final models. For analysis, we use a short training schedule (epoch 1pre-training, epoch 1 instruction tuning). For further implementation details, refer to Appendix E.1. Baselines. For the generalist models, we compare our LLaMo with (1) LLM-based generalistmodels including Galactica , LLaMA2-7B , GPT-3.5, and GPT-4, (2) Molecule-specializedLLM such as Text+Chem T5 , and (3) Molecule instruction-tuned generalist model such as Mol-Instructions . Since GPT-3.5 and GPT-4 have difficulty in solving the tasks without in-contextlearning, we additionally measure the performance of GPT-3.5 and GPT-4 with 4-shot in-contextlearning, which are GPT-3.5 (ICL) and GPT-4 (ICL). For the specialist models, we use single-taskspecialist molecule-language models as baselines, including MolT5 , MoMu , and MolCA .",
  "Experimental Results": "Generalist models. We provide the experimental results of generalist models in molecular descriptiongeneration, IUPAC name generation, and property prediction tasks. Our LLaMo is built on LLaMA-7B and it is fine-tuned by our instruction-tuning method. shows that our LLaMo achieves thebest performance in all three tasks. In comparison to GPT-4 (ICL), which is GPT-4 with in-context-learning, LLaMo shows a performance improvement of 11.9 in BLEU-4 and 14.9 in METEOR formolecular description generation. Furthermore, LLaMo outperforms Mol-Instructions, an instruction-tuned model with molecular data, by a substantial performance gain of 41.7 in METEOR for moleculardescription generation and a 0.007 performance gain in MAE on the property prediction task. Moreexperimental results on forward reaction prediction and retrosynthesis are in Appendix D.",
  "MGProj (Ours)37.866.149.670.90.007": "Specialist models. We also evaluate the performance of specialist models to validate the effectivenessof our LLaMo, which is individually fine-tuned for each dataset. demonstrates that ourLLaMo consistently achieves the best performance across all tasks and datasets. Specifically, LLaMooutperforms the second-best model MolCA with Galactica 1.3B, by 4.1 in BLEU-score and 2.4 inMETEOR on the PubChem324kV2 dataset. For IUPAC name prediction, LLaMo also shows superiorperformance, achieving a METEOR score of 73.4, which surpasses MolCA with Galactica 1.3B by amargin of 1.3 points. This experimental result indicates that our LLaMo is consistently effective incomprehending molecular graphs based on diverse large language models.",
  "Analysis": "Impact of multi-level graph projector. To validate the effectiveness of our multi-level graphprojector, we compare the performance of the multi-level graph projectors (denoted by MGProj)with other projectors in , including two widely-used projectors such as MLPs and resamplers.Additionally, we measure the performance of the base model without a graph (and a projector) denotedas w/o Graph for the ablation study. MLP (w/ low-level) and MLP (w/ high-level) denote the MLPprojectors where the input is low-level representation Z(1)graph and high-level representation Z(L)graph,respectively. MLP (w/ concat) indicates the MLP projector with the concatenated representationsof all GNN layers as an input. Resampler denotes the cross-attention based resampler projectordesigned in Qwen-VL . MGProj (w/o motif) and MGProj are our multi-level graph projectorwithout and with motif tokens P(motif). shows that our multi-level graph projector (MGProj) achieves the best performance acrossall three tasks. Specifically, the multi-level graph projector achieves 49.6 BLEU and 70.9 METEORscores with a significant improvement compared to MLP projectors in the IUPAC prediction task.These experimental results demonstrate that our multi-level graph projector is more effective thanconventional projectors by capturing multi-scale information, including atom, atomic group, andmolecule-level information.",
  "Layer": ": Visualization of attention maps for samples with coarse-grained caption (left) and fine-grained caption (right). The attention scores of high-level features are relatively high when generatingcoarse-grained captions, whereas those of low-level features are high for fine-grained captions. Impact of GPT-generated instruction-tuning data. In , we provide the ablation studies ofeach training stage and our GPT-generated instruction dataset. The experimental results reveal thatthe instruction tuning with our generated multi-turn conversation data enhances the performance ofLLaMo compared to the models trained via one or two-stage training without our GPT-generatedinstruction data. This indicates that instruction tuning with our GPT-generated multi-turn conversationdata provides the model with more detailed and instruction-following guidance. Instruction tuning v.s. multi-task learning. shows the advantages of instruction-tuningbased on task instructions compared to multi-task learning using the simple task identifier. We usethe task name as a simple task identifier for multi-task learning. From the table, the model withoutinstruction tuning (Stage 1) achieves BLUE score of 35.5 and 7.3 on molecule description andIUPAC prediction tasks, respectively. The multi-task learning approach improves the scores to 36.9for molecule description and 49.4 for IUPAC prediction. However, the instruction-tuning methoddemonstrated the most significant enhancement, achieving the highest scores of 37.8 for moleculedescription and 49.6 for IUPAC prediction. These results indicate that instruction tuning outperformsboth the baseline and multi-task learning methods, suggesting its effectiveness in improving modelperformance on general-purpose training. Visualization of attention maps. We visualize the attention map to explore the effect of the multi-level graph projector in . The figure illustrates the attention maps of graph tokens forgenerating coarse-grained (left) and fine-grained (right) descriptions. Interestingly, the attentionscores of the low-level are relatively higher than the high-level when generating fine-grained captions,whereas the attention value of the high levels is high when generating coarse-grained captions.This indicates that both low and high-level graph structural information is crucial in expressing themolecules, and the attention matrix is adaptive to the caption types.",
  "Qualitative analysis. shows a GT description and the molecular descriptions generated bythe model with and without the molecular graph (SMILES representation only). As shown in the": "GT: The molecule is an omega-hydroxy fatty acid anoin that is the conjugate base of 18-hydroxylinoleic acid, obtained by deprotonation of the carboxy group; major species at pH 7.3. It is a polyunsaturated fatty acid anion and an omega-hydroxy-long-chain fatty acid anion. It is a conjugate base of a 18-hydroxylinoleic acid.",
  "LLaMo w/ MGProj: The molecule is a member of pyrazines, a secondary carboxamide and a tertiary carboxamide": ": An example of molecular description generation results of LLaMo w/o MGProj and LLaMow/ MGProj given the molecule (C[C@@H1]1CN(C(=O)C2=C(C(=CC=C2)NC(=O)C3=NC=CN=C3)O[C@@H1]1CNC)[C@H1](C)CO). In the top box, the molecular graphs of IUPAC and func-tional groups in the descriptions are depicted. figure, LLaMo with a graph denoted as LLaMo w/ graph generates a better molecular descriptioncompared to LLaMo without a graph (LLaMo w/o graph). The GT description explains the moleculewith omega-hydroxy-long-chain fatty acid anion. Since LLaMo w/o graph does not have any graphstructural information, it fails to generate a description with an invalid IUPAC name (1-hydroxy-2-oxo-4-oxocyclohexane-1,2-diol), while LLaMo w/ graph generates a more related description withhydroxy-long-chain fatty acid anion. In addition, we know that LLaMo w/ graph accurately predictsthe long-chain structure of the molecule. We also perform another qualitative analysis by comparing molecular descriptions generated fromthe model with and without our Multi-level Graph Projector (MGProj) denoted by LLaMo w/ MGProj and LLaMo w/o MGProj in . The figure shows that the multi-level graph projectorplays a crucial role in capturing the details of the molecule. Compared to LLaMo w/o MGProjgenerating pyridine, the model with MGProj generates accurate molecular description includingpyrazine same as GT description. This demonstrates that the multi-level graph projector is effectivein molecule understanding and generation by preserving the molecular graph structural information.",
  "Conclusion": "We propose LLaMo: Large Language Model-based Molecular graph assistant, an end-to-end trainedlarge molecular graph-language model, to perform various molecule-related tasks with a singlemodel. For the projector, we newly introduce a multi-level graph projector, which addresses theover-smoothing problem of the graph encoder and captures multi-hop graph information. We alsopresent machine-generated instruction-following data in the form of multi-turn conversations toimprove the instruction-following capabilities of the large language model.",
  "Acknowledgement": "This work was partly supported by ICT Creative Consilience Program through the Institute ofInformation & Communications Technology Planning & Evaluation (IITP) (IITP-2024-RS-2020-II201819, 10%) and the National Research Foundation of Korea (NRF) (NRF-2023R1A2C2005373,45%) grant funded by the Korea government (MSIT), and the Virtual Engineering Platform Project(Grant No. P0022336, 45%), funded by the Ministry of Trade, Industry & Energy (MoTIE, SouthKorea). We appreciate Dr. Jaesung Kwak for valuable comments and discussions. Zheni Zeng, Yuan Yao, Zhiyuan Liu, and Maosong Sun. A deep-learning system bridging moleculestructure and biomedical text with comprehension comparable to human professionals. Nat. Commun.,13(1):862, 2022. Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia,Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language model for science.arXiv:2211.09085, 2022. Zhiyuan Liu, Sihang Li, Yanchen Luo, Hao Fei, Yixin Cao, Kenji Kawaguchi, Xiang Wang, and Tat-SengChua. Molca: Molecular graph-language modeling with cross-modal projector and uni-modal adapter. InEMNLP, 2023. Bing Su, Dazhao Du, Zhao Yang, Yujie Zhou, Jiangmeng Li, Anyi Rao, Hao Sun, Zhiwu Lu, and Ji-RongWen. A molecular multimodal foundation model associating molecule graphs with natural language.arXiv:2209.05481, 2022. David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel, AlnAspuru-Guzik, and Ryan P Adams. Convolutional networks on graphs for learning molecular fingerprints.In NeurIPS, 2015.",
  "Carl Edwards, ChengXiang Zhai, and Heng Ji. Text2mol: Cross-modal molecule retrieval with naturallanguage queries. In EMNLP, 2021": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation andfine-tuned chat models. arXiv:2307.09288, 2023. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman,Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report.arXiv:2303.08774, 2023. Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, RaduSoricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capablemultimodal models. arXiv:2312.11805, 2023.",
  "Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang,and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model, 2023": "Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, ConghuiHe, Xiangyu Yue, et al. Llama-adapter v2: Parameter-efficient visual instruction model. arXiv:2304.15010,2023. Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu,Hongsheng Li, and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-initattention. In ICLR, 2024. Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, BoyangLi, Pascale N Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models withinstruction tuning. In NeurIPS, 2023.",
  "Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew MDai, and Quoc V Le. Finetuned language models are zero-shot learners. In ICLR, 2022": "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions withhuman feedback. In NeurIPS, 2022. Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V Le,Barret Zoph, Jason Wei, et al. The flan collection: Designing data and methods for effective instructiontuning. In ICML, 2023. Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, AntoineChaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. Multitask prompted training enables zero-shottask generalization. In ICLR, 2021.",
  "Huayang Li, Siheng Li, Deng Cai, Longyue Wang, Lemao Liu, Taro Watanabe, Yujiu Yang, and ShumingShi. Textbind: Multi-turn interleaved multimodal instruction-following. In ACL findings, 2024": "Yin Fang, Xiaozhuan Liang, Ningyu Zhang, Kangwei Liu, Rui Huang, Zhuo Chen, Xiaohui Fan, andHuajun Chen. Mol-instructions: A large-scale biomolecular instruction dataset for large language models.In ICLR, 2024. Dimitrios Christofidellis, Giorgio Giannone, Jannis Born, Ole Winther, Teodoro Laino, and Matteo Manica.Unifying molecular and textual representations via multi-task language modelling. In ICML, 2023. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, andJingren Zhou. Qwen-vl: A frontier large vision-language model with versatile abilities. arXiv:2308.12966,2023.",
  "Canwen Xu, Daya Guo, Nan Duan, and Julian McAuley. Baize: An open-source chat model withparameter-efficient tuning on self-chat data. In EMNLP, 2023": "Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Diego Rojas, Guanyu Feng,Hanlin Zhao, Hanyu Lai, et al. Chatglm: A family of large language models from glm-130b to glm-4 alltools. arXiv:2406.12793, 2024. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothe Lacroix,Baptiste Rozire, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundationlanguage models. arXiv:2302.13971, 2023.",
  "Shengding Hu, Ning Ding, Weilin Zhao, Xingtai Lv, Zhen Zhang, Zhiyuan Liu, and Maosong Sun.Opendelta: A plug-and-play library for parameter-efficient adaptation of pre-trained models. In ACL, 2023": "Shengchao Liu, Weili Nie, Chengpeng Wang, Jiarui Lu, Zhuoran Qiao, Ling Liu, Jian Tang, Chaowei Xiao,and Animashree Anandkumar. Multi-modal molecule structuretext model for text-based retrieval andediting. Nat. Mach. Intell., 5(12):14471457, 2023. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, GirishSastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models fromnatural language supervision. In ICML, 2021."
}