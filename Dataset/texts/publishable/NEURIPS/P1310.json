{
  "Abstract": "The exceptional capabilities of large language models (LLMs) have substantiallyaccelerated the rapid rise and widespread adoption of agents. Recent studies havedemonstrated that generating Python code to consolidate LLM-based agents ac-tions into a unified action space (CodeAct) is a promising approach for developingreal-world LLM agents. However, this step-by-step code generation approach oftenlacks consistency and robustness, leading to instability in agent applications, par-ticularly for complex reasoning and out-of-domain tasks. In this paper, we proposea novel approach called Tree-of-Code (ToC) to tackle the challenges of complexproblem planning and execution with an end-to-end mechanism. By integrating keyideas from both Tree-of-Thought and CodeAct, ToC combines their strengths toenhance solution exploration. In our framework, each final code execution result istreated as a node in the decision tree, with a breadth-first search strategy employedto explore potential solutions. The final outcome is determined through a votingmechanism based on the outputs of the nodes. Experimental results on complicatedtask datasets demonstrate that our method provides more stable results comparedto Tree-of-Thought and achieves higher accuracy than CodeAct.",
  "Introduction": "In recent years, the application of code generation techniques to complex task planning and executionhas garnered significant attention , particularly with the emergence of CodeAct approaches. CodeAct has demonstrated remarkable efficiency in generating executable code forcomplex tasks, improving overall performance in terms of speed and accuracy. However, the lackof consistent reasoning in CodeAct leads to frequent interruptions during multi-step generation,causing fragmented and stalled thinking. Additionally, this process accumulates significant model",
  "hallucinations , with increasing randomness, which ultimately undermines the robustness requiredfor reliably solving complex problems": "To address this challenge, we developed an end-to-end thought-code-execution pipeline that enablesmodels to autonomously generate plans and decompositions for complex tasks, with clear reasoning.Post planning, the model produces implementation code that references earlier reasoning and leveragesits innate code processing capabilities, making implicit reasoning explicit through code. Executionof this code yields results. Inspired by the Tree-of-Thought paradigm, our approach emphasizesstructured solution exploration using decision trees and incorporates continuous code reflection. Themain approach can be summarized into three parts: 1. End-to-End Code Generation: By producing complete code solutions end-to-end, we mini-mize the need for intermediate reflection on execution results, thus enhancing stability, anddesign a long thought-code reasoning process. We introduce llm-function to enable largemodels to generate prompts and summarize corresponding outcomes. 2. Exploration of Incomplete Nodes: Leveraging the Tree-of-Thought methodology, we explorenodes with incomplete execution results by varying prompts, large language models, andmodel temperatures to improve result stability.",
  "Related Work": "LLM Reasoning: Enhancing reasoning capabilities is a crucial step toward improving generalLLMs. The current mainstream approaches focus on training techniquessuch as pretraining ,SFT , prompt-tuning , model editing , and reinforcement learning as wellas the widely used method of prompt engineering to reduce output hallucinations and improvereasoning . The latter primarily involves designing various frameworks for structured chains ofthought. Longer thought chains seem to always yield better results . The recent success ofGPT-o1 has also highlighted the importance of extended chains of thought (CoT) . Notably,existing CoT method, even with simple prompts like \"lets think step by step,\" can achieve over a30% improvement. The Tree of Thoughts (ToT) framework further enhances LLM reasoningby generating and evaluating intermediate steps, employing search algorithms to systematicallyexplore and backtrack thought paths, enabling more effective problem-solving. Other emergingframeworks, like Graph-of-Thought and Algorithm-of-Thought , are also contributing tothis area of research. Code Generation: It is widely accepted that training LLMs with codes cansignificantly enhance their reasoning abilities . Furthermore, research has shown thatcode, as a formal programming language, closely mirrors logical structures, inherently containingflow control mechanisms that can stimulate reasoning capabilities. For instance, argue thatcode serves as an effective medium for representing reasoning processes. Recent works combiningcode with agents have mainly focused on task completion in purely programming-related domains,such as software development and assisting humans in competitive programming tasks. However, these models do not use code as a scalable language, such as JSON, for directcommunication in everyday tasks. CodeAct attempted to use code directly to solve entire tasks,but the granularity of each step was too small, leading to error accumulation during function calls.",
  "Tree-of-Code: Code Generation Through Tree-Structured Reflection": "The Tree-of-Code (ToC) method combines the structured exploration of Tree-of-Thought withthe efficient task planning of CodeAct. ToC introduces a tree-based generation and explorationmechanism that utilizes various code generation strategies and models, enhancing the robustnessof execution results. By treating code as a form of reasoning, ToC capitalizes on the uniquecharacteristics of code, such as consistency and determinism, to develop a more effective agent : An Overview of our method ToC and CodAct comparisons. (a) CodeAct receives inputand performs a cycle of execution and correction, but the process is carried out in an iterative,round-by-round manner. (b) ToC applies execution-level reflection in the decision-tree structure.At each layer, different nodes are executed in parallel; if executed correctly, they are stored in thecandidate pool for voting, if a node fails, it requires further reflection. Yellow blocks mean continuedreflection. Both red and green blocks are done: red blocks are discarded by LLM voting, while greenblocks are collected and accepted. system with improved interpretability and reduced AI hallucinations. This approach acknowledgesthat well-structured planning does not inherently ensure correct code execution. Unlike traditionalreasoning methods, which rely on pre-organized plans, code necessitates a distinct methodology.Therefore, the code-reasoning process is decomposed into a structured framework that emphasizesreflection from post-execution feedback to inform iterative improvements. However, this directtreatment of code as logical reasoning introduces two significant challenges. 1) Limited diversity:Code generation tends to follow a single path, lacking exploration of alternative branches due tothe inertia of models. 2) Limited strategy: Code generation lacks a systematic approach to evaluatevarious options or apply feedback to refine them. To address these shortcomings, we introduce the Tree of Code (ToC), a framework that allowsmodels to explore multiple solution paths using a variety of strategies and models, with reflection onpost-generation execution built into the process.",
  "Overview the Tree-of-Code System": "The Tree-of-Code (ToC) framework presents a comprehensive code reasoning process comprisingfour key stages: thought, code generation, execution, and reflection. We represent this reasoningprocess as a tree T = (N, L) In contrast to the thought chain series, where thought steps areorganized into nodes (N), our approach emphasizes end-to-end generation. Here, we define nodes asthe combination of code generation and its corresponding execution. This tight coupling betweencode and execution results ensures logical consistency and correctness while facilitating high-qualityoutcomes for subsequent model training. To enhance post-generation reflection through iterativerefinement, we model the reflection process as lines (L), which can encompass various strategiesfor improvement: 1) System-level reflection: Sampling from diverse models to explore multiplesolutions. 2) Operation-level reflection: Modifying evaluation and reflection strategies to iterativeenhance output. This structured approach guarantees that the final output is both accurate and diverse,striking a balance between correctness and a variety of reasoning pathways to yield robust solutions.",
  "Thought And Code Generator": "Code generation is a critical component of code-as-reasoning. In our framework, the thought and codegeneration stage integrates interactions between the user, environment, and agent: The user providesthe task queries. The environment executes the generated code, providing execution feedback. Theagent synthesizes information from the user, environment, and history, translating this into codes,which it then executes. The process of transforming thought into executable codes can be formalizedas:Execution(i) = Code(FC(i), FR(i), Thought(i), Functions)i {1, . . . , n}(1)where Thought(i) represents the agents internal reasoning at step i, and Code (FC(i), FR(i) andThought(i), Functions) converts that reasoning into Python code for execution. FC(i) representsthe code of father node of the current node. FR(i) represents the code execution result of father nodeof the current node. Functions represent all the tools that current task can use. To generate an end-to-end solution for current task, we add a llm-function tool into Functions set.With llm-function, our code generation LLMs will generate prompt from the current context and callllm-function to generate final results.",
  "Tree Expansion and State Evaluation": "We initialize from a root node and do tree expansion recursively based on termination criteria. Everytime a nodes code is executed, state evaluation will be executed based on the code execution results.The process is as follows: 1) The process starts at the root node, expanding the tree incrementally,with each node representing generated thoughts, code, and execution results. 2) Generated codes ofeach leaf node will be executed in a Python environment. When errors are detected, child nodes areexpanded from the erroneous node to analyze specific issues and explore alternative solutions. 3)This expansion continues until all leaf nodes execute successfully or a maximum depth is reached.This strategy effectively manages complexity and ensures correct and efficient code execution results.",
  "Experiment and Analysis": "Our experiments evaluated the effectiveness of the ToC framework, comparing its performanceprimarily against the CodeAct framework. We used the M3ToolEval, a newly curated benchmarkfor evaluating performance across complex multi-scene tasks, the same used in CodeAct , tobenchmark ToC. We evaluated the input samples from M3ToolEval. The context window was fixed at3k tokens, and the generation depth was set at 3 for consistency in computational costs. To highlightthe superiority of our method, we directly compared it to the best-performing model in CodeAct,which is based on GPT-4. In the tests, ToC achieved a 7.2% higher accuracy than CodeAct in tasks,and it reduced interaction steps significantly, demonstrating overall effectiveness and robustness intask completion, as shows. To ensure diverse outputs, we applied a multi-level sampling strategy: 1) Model Diversity: A range ofgenerative models (e.g., GPT-4 , ERNIE-4.0-Turbo , DeepSeek Coder , and Claude 3.5 )was utilized to introduce variability at the model level. 2) Temperature Variability: Adjustmentsto generation temperatures were made within specified bounds to encourage content variation. 3)Prompt Variation: Different role instructions and adaptive strategies were explored to further enhanceadaptive diversity. Further analysis compared ToC against both the JSON-based and thought-based modes of CodeAct.Across all criteria, ToC consistently outperformed other traditional reasoning frameworks, particularlyin managing complex reasoning tasks.5Conclusion",
  "Mix-modal sampling for Toc.2 GPT-4, best performance model for CodeAct": "generation in our approach might intrigue internal processes of reasoning, while the decision-treestructured exploration enables reflection based on external execution. With efficient model integrationand prompt optimization, we achieved results significantly surpassing the baselines on complex taskdatasets. Furthermore, our ongoing, yet unpublished, work suggests that this approach will yieldsimilarly promising results in real-world applications and better performance after few-shot SFT.These results will be shared in future workstay tuned. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman,Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXivpreprint arXiv:2303.08774, 2023. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman,Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXivpreprint arXiv:2303.08774, 2023.",
  "Banghao Chen, Zhaofeng Zhang, Nicolas Langren, and Shengxin Zhu. Unleashing the potential of promptengineering in large language models: a comprehensive review. arXiv preprint arXiv:2310.14735, 2023": "Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, XuezhiWang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models.Journal of Machine Learning Research, 25(70):153, 2024. Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Diego Rojas, Guanyu Feng,Hanlin Zhao, Hanyu Lai, et al. Chatglm: A family of large language models from glm-130b to glm-4 alltools. arXiv preprint arXiv:2406.12793, 2024. Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi,Yu Wu, YK Li, et al. Deepseek-coder: When the large language model meets programmingthe rise ofcode intelligence. arXiv preprint arXiv:2401.14196, 2024. Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi,Yu Wu, YK Li, et al. Deepseek-coder: When the large language model meets programmingthe rise ofcode intelligence. arXiv preprint arXiv:2401.14196, 2024.",
  "Md Ashraful Islam, Mohammed Eunus Ali, and Md Rizwan Parvez. Mapcoder: Multi-agent codegeneration for competitive problem solving. arXiv preprint arXiv:2405.11403, 2024": "Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, AndreaMadotto, and Pascale Fung. Survey of hallucination in natural language generation. ACM ComputingSurveys, 55(12):138, 2023. Chen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang, Jiahao Li, Cheng Yang, Weize Chen,Yusheng Su, Xin Cong, et al. Chatdev: Communicative agents for software development. In Proceedingsof the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),pages 1517415186, 2024.",
  "Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao Peng, and Heng Ji. Executablecode actions elicit better llm agents. arXiv preprint arXiv:2402.01030, 2024": "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, DennyZhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neuralinformation processing systems, 35:2482424837, 2022. Jiaxin Wen, Ruiqi Zhong, Pei Ke, Zhihong Shao, Hongning Wang, and Minlie Huang. Learning taskdecomposition to assist humans in competitive programming. In Proceedings of the 62nd Annual Meetingof the Association for Computational Linguistics, 2024.",
  "Jiaxin Wen, Ruiqi Zhong, Pei Ke, Zhihong Shao, Hongning Wang, and Minlie Huang. Learning taskdecomposition to assist humans in competitive programming. arXiv preprint arXiv:2406.04604, 2024": "Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Qingwei Lin,and Daxin Jiang. Wizardlm: Empowering large pre-trained language models to follow complex instructions.In The Twelfth International Conference on Learning Representations, 2024. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan.Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural InformationProcessing Systems, 36, 2024. Eric Zelikman, Georges Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, and Noah D Goodman. Quiet-star:Language models can teach themselves to think before speaking. arXiv preprint arXiv:2403.09629, 2024. Qihao Zhu, Daya Guo, Zhihong Shao, Dejian Yang, Peiyi Wang, Runxin Xu, Y Wu, Yukun Li, Huazuo Gao,Shirong Ma, et al. Deepseek-coder-v2: Breaking the barrier of closed-source models in code intelligence.arXiv preprint arXiv:2406.11931, 2024.",
  "NeurIPS Paper Checklist": "The checklist is designed to encourage best practices for responsible machine learning research,addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not removethe checklist: The papers not including the checklist will be desk rejected. The checklist shouldfollow the references and follow the (optional) supplemental material. The checklist does NOT counttowards the page limit.",
  "Please provide a short (12 sentence) justification right after your answer (even for NA)": "The checklist answers are an integral part of your paper submission. They are visible to thereviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it(after eventual revisions) with the final version of your paper, and its final version will be publishedwith the paper. The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation.While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided aproper justification is given (e.g., \"error bars are not reported because it would be too computationallyexpensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering\"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, weacknowledge that the true answer is often more nuanced, so please just use your best judgment andwrite a justification to elaborate. All supporting evidence can appear either in the main paper or thesupplemental material, provided in appendix. If you answer [Yes] to a question, in the justificationplease point to the section(s) where related material for the question can be found.",
  "The answer NA means that the paper has no limitation while the answer No means thatthe paper has limitations, but those are not discussed in the paper": "The authors are encouraged to create a separate \"Limitations\" section in their paper. The paper should point out any strong assumptions and how robust the results are toviolations of these assumptions (e.g., independence assumptions, noiseless settings,model well-specification, asymptotic approximations only holding locally). The authorsshould reflect on how these assumptions might be violated in practice and what theimplications would be. The authors should reflect on the scope of the claims made, e.g., if the approach wasonly tested on a few datasets or with a few runs. In general, empirical results oftendepend on implicit assumptions, which should be articulated. The authors should reflect on the factors that influence the performance of the approach.For example, a facial recognition algorithm may perform poorly when image resolutionis low or images are taken in low lighting. Or a speech-to-text system might not beused reliably to provide closed captions for online lectures because it fails to handletechnical jargon.",
  "If applicable, the authors should discuss possible limitations of their approach toaddress problems of privacy and fairness": "While the authors might fear that complete honesty about limitations might be used byreviewers as grounds for rejection, a worse outcome might be that reviewers discoverlimitations that arent acknowledged in the paper. The authors should use their bestjudgment and recognize that individual actions in favor of transparency play an impor-tant role in developing norms that preserve the integrity of the community. Reviewerswill be specifically instructed to not penalize honesty concerning limitations.",
  ". Experimental Result Reproducibility": "Question: Does the paper fully disclose all the information needed to reproduce the main ex-perimental results of the paper to the extent that it affects the main claims and/or conclusionsof the paper (regardless of whether the code and data are provided or not)?Answer: [Yes]Justification: the paper will open source code on github for other researchers to reproduceexperiments.Guidelines:",
  "If the contribution is a dataset and/or model, the authors should describe the steps takento make their results reproducible or verifiable": "Depending on the contribution, reproducibility can be accomplished in various ways.For example, if the contribution is a novel architecture, describing the architecture fullymight suffice, or if the contribution is a specific model and empirical evaluation, it maybe necessary to either make it possible for others to replicate the model with the samedataset, or provide access to the model. In general. releasing code and data is oftenone good way to accomplish this, but reproducibility can also be provided via detailedinstructions for how to replicate the results, access to a hosted model (e.g., in the caseof a large language model), releasing of a model checkpoint, or other means that areappropriate to the research performed. While NeurIPS does not require releasing code, the conference does require all submis-sions to provide some reasonable avenue for reproducibility, which may depend on thenature of the contribution. For example(a) If the contribution is primarily a new algorithm, the paper should make it clear howto reproduce that algorithm.",
  "(b) If the contribution is primarily a new model architecture, the paper should describethe architecture clearly and fully": "(c) If the contribution is a new model (e.g., a large language model), then there shouldeither be a way to access this model for reproducing the results or a way to reproducethe model (e.g., with an open-source dataset or instructions for how to constructthe dataset). (d) We recognize that reproducibility may be tricky in some cases, in which caseauthors are welcome to describe the particular way they provide for reproducibility.In the case of closed-source models, it may be that access to the model is limited insome way (e.g., to registered users), but it should be possible for other researchersto have some path to reproducing or verifying the results.",
  ". Experimental Setting/Details": "Question: Does the paper specify all the training and test details (e.g., data splits, hyper-parameters, how they were chosen, type of optimizer, etc.) necessary to understand theresults?Answer: [Yes]Justification: Yes, the paper used open sourced dataset and include all the details of thedatasets.Guidelines: The answer NA means that the paper does not include experiments. The experimental setting should be presented in the core of the paper to a level of detailthat is necessary to appreciate the results and make sense of them.",
  ". Experiment Statistical Significance": "Question: Does the paper report error bars suitably and correctly defined or other appropriateinformation about the statistical significance of the experiments?Answer: [Yes]Justification: Yes, the paper reported error bars properly.Guidelines: The answer NA means that the paper does not include experiments. The authors should answer \"Yes\" if the results are accompanied by error bars, confi-dence intervals, or statistical significance tests, at least for the experiments that supportthe main claims of the paper. The factors of variability that the error bars are capturing should be clearly stated (forexample, train/test split, initialization, random drawing of some parameter, or overallrun with given experimental conditions).",
  ". Experiments Compute Resources": "Question: For each experiment, does the paper provide sufficient information on the com-puter resources (type of compute workers, memory, time of execution) needed to reproducethe experiments?Answer: [Yes]Justification: Yes, the paper includes all the computational resources information.Guidelines: The answer NA means that the paper does not include experiments. The paper should indicate the type of compute workers CPU or GPU, internal cluster,or cloud provider, including relevant memory and storage.",
  "Guidelines:": "The answer NA means that the paper poses no such risks. Released models that have a high risk for misuse or dual-use should be released withnecessary safeguards to allow for controlled use of the model, for example by requiringthat users adhere to usage guidelines or restrictions to access the model or implementingsafety filters.",
  ". Licenses for existing assets": "Question: Are the creators or original owners of assets (e.g., code, data, models), used inthe paper, properly credited and are the license and terms of use explicitly mentioned andproperly respected?Answer: [Yes]Justification: Yes, our open sourced code will be licensed properly.Guidelines: The answer NA means that the paper does not use existing assets. The authors should cite the original paper that produced the code package or dataset. The authors should state which version of the asset is used and, if possible, include aURL. The name of the license (e.g., CC-BY 4.0) should be included for each asset. For scraped data from a particular source (e.g., website), the copyright and terms ofservice of that source should be provided. If assets are released, the license, copyright information, and terms of use in thepackage should be provided. For popular datasets, paperswithcode.com/datasetshas curated licenses for some datasets. Their licensing guide can help determine thelicense of a dataset.",
  ". New Assets": "Question: Are new assets introduced in the paper well documented and is the documentationprovided alongside the assets?Answer: [Yes]Justification: We introduced a new method for the problem and will release our code ongithub if accepted.Guidelines: The answer NA means that the paper does not release new assets. Researchers should communicate the details of the dataset/code/model as part of theirsubmissions via structured templates. This includes details about training, license,limitations, etc.",
  "According to the NeurIPS Code of Ethics, workers involved in data collection, curation,or other labor should be paid at least the minimum wage in the country of the datacollector": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with HumanSubjectsQuestion: Does the paper describe potential risks incurred by study participants, whethersuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)approvals (or an equivalent approval/review based on the requirements of your country orinstitution) were obtained?Answer: [NA]Justification: We did not crowsourcing our research experiments.Guidelines:",
  "The answer NA means that the paper does not involve crowdsourcing nor research withhuman subjects": "Depending on the country in which research is conducted, IRB approval (or equivalent)may be required for any human subjects research. If you obtained IRB approval, youshould clearly state this in the paper. We recognize that the procedures for this may vary significantly between institutionsand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and theguidelines for their institution."
}