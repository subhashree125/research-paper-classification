{
  "Abstract": "We present RUBIX, a fully tested, well-documented, and modular Open Sourcetool developed in JAX, designed to forward model IFU cubes of galaxies fromcosmological hydrodynamical simulations. The code automatically parallelizescomputations across multiple GPUs, demonstrating performance improvementsover state-of-the-art codes by a factor of 600. This optimization reduces computetimes from hours to only seconds. RUBIX leverages JAXs auto-differentiationcapabilities to enable not only forward modeling but also gradient computationsthrough the entire pipeline paving the way for new methodological approachessuch as e.g. gradient-based optimization of astrophysics model parameters. RUBIXis open-source and available on GitHub2.",
  "Motivation": "In the field of astrophysics, researchers are divided into two main groups: observers and theorists.Observers build and operate advanced instruments and telescopes, such as the James Webb SpaceTelescope (JWST) and the Very Large Telescope (VLT), to collect empirical data from distant galaxiesand stars by counting photons. Integral Field Unit (IFU) spectroscopy is one key observational tech-nique that produces datacubes with spatially resolved spectra. Theorists, on the other hand, developand refine physical equations to model the Universes behavior. They use high-end supercomputersto run cosmological simulations, to replicate the conditions of the early universe. These simulationshelp test the implications of various physical theories and require advanced computational techniquesand statistical analysis. One significant challenge in astrophysics is bridging the gap between observa-tional data and theoretical models. Forward modeling techniques, which translate simulation outputsinto observable data, are crucial for effective collaboration. The advances of Machine Learning",
  "arXiv:2412.08265v1 [astro-ph.IM] 11 Dec 2024": "models are hugely influenced by the improvements of hardware that has an architecture that workswell with the calculations performed in ML applications: the GPU. They are extremely well suited toperform calculations in parallel, hence implementing a IFU forward model code that works on GPUsis a major advantage to current state-of-the-art codes and will enable us to produce a sufficient numberof samples required for statistical analysis, which was so far the bottleneck for Machine Learning(ML) applications. Additionally, with our JAX implementation we are able to compute gradientsneeded to perform optimization in the context of ML and Simulation Based Inference. This paperaims to bridge the gap between observers and theorists by introducing a forward modelling of mockIFUs: RUBIX is written in JAX, runs natively parallel on multiple GPUs and leverages performanceimprovements from just-in-time compilation using XLA.",
  "Related Work": "There are several codes that can forward model IFU data that are commonly used in astrophysicsresearch. One of the most popular codes is SimSpin , which is written in R and is a CPU onlypackage that takes in a simulation of a galaxy and produces mock IFU observations. The user canfreely choose any instrument configuration and spectral library to create mock observations. Thereis extensive documentation and examples available, which makes the usage of the package veryuser-friendly. Another code called GalCraft generates mock IFU data cubes of the Milky Way(MW). GalCraft uses the mock stellar catalog that is based on the analytical chemodynamical modelof . The analytical model predicts the joint distribution of position, velocity, age, extinction,photometric magnitude and the chemical abundances of stars in the MW, which is then used toproduce mock IFU data cubes. In , the authors emulated observation data from the MaNGAsurvey using IllustrisTNG data to generate mock observations. Previously, emulated 893MaNGA observations using the RealSimIFS code from data of the TNG50 simulation . Asimilar project of produced a catalog of around 1000 unique mock IFU observation to mimic theMaNGA primary sample again using data from the TNG50 simulation. However, all current codesare CPU only use and have no option of calculating the gradient of the forward modelling processwith respect to the input parameters - hence limiting their applicability within the context of ML.",
  "RUBIX Codebase": "The RUBIX pipeline is a modular and efficient framework for forward modeling IFU data fromcosmological simulations, leveraging the power of JAX for high-performance computing. RUBIX isimplemented as a linear pipeline, where each function sequentially transforms the input data, ensuringthat the framework remains extremely modular and easily extensible for future developments. RUBIXutilizes multiple GPUs for parallel computation and significantly reduces processing time. TheInput Handler extracts and transforms relevant star and gas particle information from cosmologicalsimulation data into a unique data file, which is the input for the RUBIX pipeline. The first step inthe pipeline is to orientate the galaxy in the field of view, following the specifications provided inthe configuration file. Next, the particles are assigned to the IFU spaxels, accounting for telescope-specific configurations. The key part is the spectra calculation. Each star spectrum is calculated as alookup from a simple stellar population (SSP) library. Then the spectra are Doppler shifted basedon the galaxy distance and line-of-sight velocity of each stellar particle. Additional resampling isperformed to match the wavelength grid of the observed telescope. Afterwards, the stellar spectrain each spaxel are summed up. To simulate observational effects, we apply a point-spread function(PSF) and line-spread function (LSF) convolution and add realistic noise. The entire pipeline isconfigurable using JSON. Each pipeline run starts with a JSON or Python dictionary, where the userchooses all the hyperparameters, i.e SSP library, galaxy distance and orientation, telescope, etc.",
  "Results": "Qualitative AnalysisTo verify the output, RUBIX is executed for different Subhalos from theIllustrisTNG simulation using the IllustrisAPI. For a set of galaxies from the TNG50-1 simulation,snapshot 99, mock observations are created with a MUSE instrument configured with fov=20 and aGaussian PSF and LSF. The Mastar_CB19_SLOG_1_5 SSP template is employed to compute thestellar spectra, and RUBIX is executed on eight NVIDIA A100 GPUs. The mock MUSE observations SubhaloID 99-37 M/M=0.64e10, z=0.02 [] Flux (erg/s/cm2) 1e18 SubhaloID 99-63874 M/M=12.31e10, z=0.02 [] 0.5 1.0 1.5 2.0 Flux (erg/s/cm2) 1e17",
  ": MUSE mock observations for different Subhalos, the total flux in each pixel is shown asan image representation on the left. On the right, the spectra of three different spaxels are plotted": "are illustrated in . The galaxies were chosen to have an increasing mass, with a dwarf galaxyon the left and a massive spherical galaxies on the right. For each galaxy, an image representation isprovided on the left, where the total flux in each spaxel is summed to produce a two-dimensionalarray. The right column presents the spectra (in units of erg/s/cm) for three different spaxels withincreasing distance from the galactic center. Spectra from the center of the galaxy exhibit higherflux compared to those from the outskirts, which is expected due to the higher density of stars in thegalaxys center. The shapes of the spectra differ significantly; spectra from the outskirts tend to beflatter. In general we can observe that RUBIX can reproduce the trends that we expect. Galcraft[CPU]Rubix[CPU]Rubix[GPU] Compute Time (seconds) 5400.00 s 123.57 s 43.70x faster 8.58 s 629.37x faster Comparison of Compute Speed: Rubix vs Galcraft",
  ": Speed comparison the execution timeof different codes are compared. Note that the y-axis is logarithmic": "Speed comparisonThe primary objective ofthis paper is to highlight the methodologicalimprovements RUBIX provides for the forwardmodeling process. In , the computetimes of different codes are compared. Accord-ing to , the authors state that \"for a typicalMUSE FoV containing 6 106 particles, the exe-cution time spent with a 24-core CPU (2.50GHz)is 1.4 hours.\" This result serves as a benchmarkto contextualize the execution time of RUBIX.A galaxy with a comparable number of parti-cles (6 106) is forward modeled both on theCPU and GPU using RUBIX. Running RUBIX ona 24-core CPU (AMD Epyc 7452, 2.35 GHz)takes 123.57 seconds, representing a 43.7-foldimprovement over the Galcraft code. When themock observation is computed on a single NVIDIA A-100 GPU, the execution time is reduced to8.58 seconds, which is 600 times faster than Galcraft and 14.4 times faster than the same RUBIX codeexecuted on the CPU. Despite the clear performance improvements, we should take the benchmarkcomparisons with caution, because we use different hardware configurations. GPUs and CPUshave different architecture and GalCraft does not share the exact same methodology. Despite thesedifferences, the comparison still offers a valuable general trend. One significant reason for RUBIXssuperior speed is its efficient implementation. In RUBIX, instead of naively looping over the particles,every function is vectorized using vmap. This approach leverages XLA to fuse operations together,resulting in substantial speed improvements. Strong ScalingIn (a), the average runtime is plotted against the number of particles.At each number of particles, the runtime is measured five times to get some statistics. The redshaded area represents the 1 range, indicating the variability in the runtime measurements. Onecan observe that as the number of particles increases, the average runtime also increases, but notlinearly. This indicates that RUBIX does not have perfect strong scaling, which may be caused by thecommunication overhead between the GPUs. Weak ScalingTo evaluate how compute time scales with the number of GPUs, RUBIX is initiallyrun with 10,000 particles on a single GPU, and the runtime is measured. Next, the number of particlesis doubled, and the code is executed on two GPUs, continuing this process until the maximum number Number of Particles 5.0 5.5 6.0 6.5 7.0 Average Runtime (seconds) Average Runtime Scaling with Number of Particles Average Runtime1 Range",
  "(c) Scaling Efficiency": ": Scaling plots (a) Strong Scaling: Increasing particle size, while keeping number ofGPUs fixed (8 NVIDIA A100 GPUs). (b) Average runtime of different RUBIX runs, where weproportionally increase particle size and number of available GPUs, such that the workload per GPUremains constant. (c) Scaling efficiency calculated as the ratio of the runtime with one GPU to theruntime with multiple GPUs. of GPUs is reached, which in this case is eight NVIDIA A100 GPUs. In (b), the averageruntime of different RUBIX runs is measured. Each run is repeated 5 times to get some statistics, andthe 1 area is shaded in the background (blue: starts with 10,000 particles; green: split data intofour batches on each GPU; orange: starts with 40,000 particles and batching). Ideally, in a best-casescenario, the compute time should remain constant as both the workload (number of particles) andthe compute resources (number of GPUs) increase proportionally. This would demonstrate perfectscaling. From (b) we can clearly see that the scaling is not perfect. The runtime increasesslowly with the number of GPUs, indicating that the computational work is not distributed equallybetween the GPUs or that communication overheads are still large in RUBIX. Scaling EfficiencyTo make this more quantitatively, we can measure the scaling efficiency as:Scaling Efficiency =T1TN where T1 is the runtime with one GPU and TN is the runtime with NGPUs. Ideally, this scaling efficiency should be close to one. In (c) we clearly see thatthe scaling efficiency decreases with increasing number of GPUs, which means that the scaling isnot optimal. One major factor is the communication overhead between GPUs, which can becomesignificant as more GPUs are added. Additionally, the efficiency of load balancing can decreasewith more GPUs. Furthermore, the complexity of managing more GPUs can introduce inefficienciesin the parallelization process, such as increased latency in coordinating tasks and distributing dataevenly among the GPUs. This indicates that RUBIX is not yet fully optimized and requires furtherimprovements. One significant bottleneck might be the current implementation of pmap and jit. Inthe current version of RUBIX, only the datacube calculation inside the pipeline is parallelized acrossthe GPUs using pmap. However, during the pipeline assembly, all the functions are concatenated, andthe final function is just-in-time compiled using jit. There is a known issue in JAX warning usersthat using jit on a pmap-function can lead to inefficient data movement, as it essentially collects alldata onto a single device. This issue is discussed in detail on the official JAX GitHub page3.",
  "Conclusions and Limitations": "RUBIX represents a significant leap forward in computational efficiency and flexibility for modelingIFU observations from cosmological hydrodynamical simulations. Its ability to rapidly process large-scale simulations and its potential for future enhancements makes it a powerful tool for astrophysicalresearch. The combination of high performance and open-source accessibility underscores thecontribution of RUBIX to the field, facilitating innovation and collaboration within the scientificcommunity. Despite its impressive performance, there remains potential for further optimization, i.e.further profiling is required. Apart from speed improvements, there are additional features that willbe implemented into RUBIX. Some of those include:",
  "Dust Modeling Adding support for dust attenuation models will provide more realisticmock observations, that should closer relate to real observations": "Radiative Transfer Implementing advanced radiative transfer models will enhance theprecision of the RUBIX simulations. This will allow for a more realistic representation ofhow light propagates through various media. However, this needs to be implemented in pureJAX, which can be a quite challenging task. With these additional features RUBIX will be ideally suited to tackle key scientific machine learningtasks in astrophysics, such as performing SBI inference of fundamental galaxy parameters with high-dimensional complex observational data, perform Bayesian model comparison, do gradient basedoptimization tasks on the forward modelling pipeline and incorporate the differentiable forward modelRUBIX into machine learning architectures to train them end-to-end, e.g. build hybrid NN encoder-physics-based-decoder architectures. As such, we think that RUBIX provides the astrophysicalcommunity with a unique, versatile and new methodological approach to perform downstreamscientific tasks.",
  "Connor Bottrell and Maan H Hani. Realistic synthetic integral field spectroscopy with realsim-ifs. Monthly Notices of the Royal Astronomical Society, 514(2):28212838, June 2022": "Kevin Bundy, Matthew A. Bershady, David R. Law, Renbin Yan, Niv Drory, Nicholas MacDon-ald, David A. Wake, Brian Cherinka, Jos R. Snchez-Gallego, Anne-Marie Weijmans, DanielThomas, Christy Tremonti, Karen Masters, Lodovico Coccato, Aleksandar M. Diamond-Stanic,Alfonso Aragn-Salamanca, Vladimir Avila-Reese, Carles Badenes, Jsus Falcn-Barroso,Francesco Belfiore, Dmitry Bizyaev, Guillermo A. Blanc, Joss Bland-Hawthorn, Michael R.Blanton, Joel R. Brownstein, Nell Byler, Michele Cappellari, Charlie Conroy, Aaron A. Dutton,Eric Emsellem, James Etherington, Peter M. Frinchaboy, Hai Fu, James E. Gunn, Paul Hard-ing, Evelyn J. Johnston, Guinevere Kauffmann, Karen Kinemuchi, Mark A. Klaene, Johan H.Knapen, Alexie Leauthaud, Cheng Li, Lihwai Lin, Roberto Maiolino, Viktor Malanushenko,Elena Malanushenko, Shude Mao, Claudia Maraston, Richard M. McDermid, Michael R. Mer-rifield, Robert C. Nichol, Daniel Oravetz, Kaike Pan, John K. Parejko, Sebastian F. Sanchez,David Schlegel, Audrey Simmons, Oliver Steele, Matthias Steinmetz, Karun Thanjavur, Ben-jamin A. Thompson, Jeremy L. Tinker, Remco C. E. van den Bosch, Kyle B. Westfall, DavidWilkinson, Shelley Wright, Ting Xiao, and Kai Zhang. Overview of the SDSS-IV MaNGA Sur-vey: Mapping nearby Galaxies at Apache Point Observatory. Astrophysical Journal, 798(1):7,January 2015.",
  "Katherine Harborne. Simspin: Kinematic analysis of simulated galaxies, 2023. Publications ofthe Astronomical Society of Australia, Volume 40, article id. e048, Oct 2023": "Lorenza Nanni, Daniel Thomas, James Trayford, Claudia Maraston, Justus Neumann, David RLaw, Lewis Hill, Annalisa Pillepich, Renbin Yan, Yanping Chen, and Dan Lazarz. iMaNGA:mock MaNGA galaxies based on IllustrisTNG and MaStar SSPs I. Construction and analysisof the mock data cubes. Monthly Notices of the Royal Astronomical Society, 515(1):320338,06 2022. Dylan Nelson, Annalisa Pillepich, Volker Springel, Rdiger Pakmor, Rainer Weinberger, ShyGenel, Paul Torrey, Mark Vogelsberger, Federico Marinacci, and Lars Hernquist. First resultsfrom the tng50 simulation: galactic outflows driven by supernovae and black hole feedback.Monthly Notices of the Royal Astronomical Society, 490(3):32343261, August 2019. Annalisa Pillepich, Dylan Nelson, Volker Springel, Rdiger Pakmor, Paul Torrey, RainerWeinberger, Mark Vogelsberger, Federico Marinacci, Shy Genel, Arjen van der Wel, and LarsHernquist. First results from the tng50 simulation: the evolution of stellar and gaseous discsacross cosmic time. Monthly Notices of the Royal Astronomical Society, 490(3):31963233,September 2019. Regina Sarmiento, Marc Huertas-Company, Johan H. Knapen, Hctor Ibarra-Medel, AnnalisaPillepich, Sebastin F. Snchez, and Alina Boecker. Mangia: 10 000 mock galaxies for stellarpopulation analysis. Astronomy &; Astrophysics, 673:A23, April 2023. Sanjib Sharma, Michael R Hayden, and Joss Bland-Hawthorn. Chemical enrichment and radialmigration in the Galactic disc the origin of the [Fe] double sequence. Monthly Notices of theRoyal Astronomical Society, 507(4):58825901, 07 2021. S. F. Snchez, J. K. Barrera-Ballesteros, E. Lacerda, A. Meja-Narvaez, A. Camps-Faria,Gustavo Bruzual, C. Espinosa-Ponce, A. Rodrguez-Puebla, A. R. Calette, H. Ibarra-Medel,V. Avila-Reese, H. Hernandez-Toledo, M. A. Bershady, M. Cano-Diaz, and A. M. Munguia-Cordova. Sdss-iv manga: pypipe3d analysis release for 10,000 galaxies. The AstrophysicalJournal Supplement Series, 262(2):36, sep 2022. Zixian Wang, Sanjib Sharma, Michael R. Hayden, Jesse van de Sande, Joss Bland-Hawthorn,Sam Vaughan, Marie Martig, and Francesca Pinna. Validating full-spectrum fitting with asynthetic integral-field spectroscopic observation of the Milky Way. MNRAS, 534(2):11751204, October 2024."
}