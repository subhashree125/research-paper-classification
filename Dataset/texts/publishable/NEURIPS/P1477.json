{
  "Abstract": "While synthetic data hold great promise for privacy protection, their statisticalanalysis poses significant challenges that necessitate innovative solutions. Theuse of deep generative models (DGMs) for synthetic data generation is known toinduce considerable bias and imprecision into synthetic data analyses, compro-mising their inferential utility as opposed to original data analyses. This bias anduncertainty can be substantial enough to impede statistical convergence rates, evenin seemingly straightforward analyses like mean calculation. The standard errorsof such estimators then exhibit slower shrinkage with sample size than the typical 1over root-n rate. This complicates fundamental calculations like p-values and con-fidence intervals, with no straightforward remedy currently available. In responseto these challenges, we propose a new strategy that targets synthetic data createdby DGMs for specific data analyses. Drawing insights from debiased and targetedmachine learning, our approach accounts for biases, enhances convergence rates,and facilitates the calculation of estimators with easily approximated large samplevariances. We exemplify our proposal through a simulation study on toy data andtwo case studies on real-world data, highlighting the importance of tailoring DGMsfor targeted data analysis. This debiasing strategy contributes to advancing thereliability and applicability of synthetic data in statistical inference.",
  "Introduction": "The concept of generating synthetic data as a means of privacy protection was initially introduced byRubin (1993) within the framework of multiple imputation, a widely used technique for managingthe statistical analysis of incomplete data sets. Since its inception, a substantial body of literatureon synthetic data has emerged (Raghunathan et al., 2003; Raghunathan, 2021; Drechsler, 2011;Raab et al., 2016; Reiter, 2005), with a recent surge in interest propelled by advancements in deepgenerative modelling technology (Raghunathan, 2021; van Breugel et al., 2023; Wan et al., 2017;Yan et al., 2022; Nowok et al., 2016; Endres et al., 2022; Hernandez et al., 2022). In this work, wefocus on tabular synthetic data and its inferential utility, which captures whether synthetic data can beused to obtain valid estimates and inference for a population parameter (Decruyenaere et al., 2024).",
  "arXiv:2411.04216v1 [stat.ML] 6 Nov 2024": "The substantial privacy protection potential offered by synthetic data is marred by significant chal-lenges that undermine their inferential utility (Raab et al., 2016). Previous work by Decruyenaereet al. (2024) has shown that these challenges are much more severe when using Deep GenerativeModels (DGMs), rather than parametric statistical models, which is why we focus on the former. First,standard confidence intervals and p-values obtained on synthetic data may drastically underestimatethe uncertainty in synthetic data as these ignore the size of the original data. Indeed, synthetic dataobtained via generators trained on small datasets will unsurprisingly deliver much worse quality thansynthetic data obtained via generators trained on large datasets. This uncertainty in the generatormust therefore be translated into analysis results, such as confidence intervals and p-values. Standardconfidence intervals and p-values ignore this, as they do not distinguish whether the data is syntheticor real. Second, it is well known from the literature on plug-in estimation that data-adaptive methods(such as DGMs) cannot succeed to estimate all features of the data-generating distribution well(Bickel et al., 1993; van der Laan and Rose, 2011; Chernozhukov et al., 2018; Hines et al., 2022).These methods are designed to optimally balance bias and variance w.r.t. a chosen criterion, such asprediction error. However, they cannot guarantee that such an optimal trade-off is simultaneouslymade w.r.t. all possible discrepancy measures that exist, such as mean squared error in specificfunctionals (mean, variance, least squares projections...) of the observed data distribution. Suchdata-adaptive methods therefore leave non-negligible bias in estimators of such functionals, leadingto excess variability, slow convergence, and confidence intervals that do not cover the truth at nominallevel (and may even never contain the truth, even in large samples) (Decruyenaere et al., 2024). Related work.While several approaches have been proposed to account for the uncertainty arisingfrom synthetic data generation, we are not aware of strategies for generating and analysing DGM-based synthetic data that guarantee valid inference. Raghunathan et al. (2003) developed a frameworkinspired by the work on multiple imputation for missing data, by combining the results of multiplesynthetic datasets, but this is not readily applicable to DGM-based synthetic data. Ris et al. (2023)extended this work for differentially private (DP) synthetic data, acknowledging the additional DPnoise, but continue to consider parametric (Bayesian) data generation strategies. Our work instead focuses on obtaining valid inference from a single synthetic dataset, which isarguably more attractive for use by practitioners. Raab et al. (2016) derived alternative combiningrules that reduce to the correction factor 1 + m/n for the standard error (SE) of an estimator inthe case of inference from a single (non-DP) synthetic dataset of size m generated from an originaldataset of size n. The method suggested by Awan and Cai (2020) to preserve efficient estimators ina single (DP or non-DP) synthetic dataset relies on generating data conditional on the estimate inthe original data. Both procedures are only applicable to parametric generative models and thereforesuffer from the same limitation as the aforementioned approaches. To enable Bayesian inferencefrom a single DP synthetic dataset, Wilde et al. (2021) proposed a corrected analysis that relies onthe availability of additional public data, while Ghalebikesabi et al. (2022) investigated importanceweighting methods to remove the noise-related bias, but they do not study the impact on inference. Contributions.Our work is the first to propose a generator-agnostic solution that mitigates theimpact of the typical slower-than-n-convergence of estimators in synthetic data created by DGMs.As far as we are aware, our approach is thus the only one that provides some formal guarantees for(more) honest inference in this setting. In this paper, we show how the statistical bias in estimatorscan be removed, by adapting results on debiased or targeted machine learning (van der Laan and Rose,2011; Chernozhukov et al., 2018) to the current setting where machine learning is not necessarilyused in the analysis, but rather in the generation of synthetic data. Although we build upon ideasfrom existing work, our extension is non-trivial, since (1) those works did not consider synthetic data;and (2) we demonstrate, with significant generality, how to mitigate the estimation errors in the DGMthat would otherwise propagate into the estimator calculated on synthetic data. In and 3, we propose a generator-agnostic debiasing strategy, directed towards the down-stream statistical analysis of the synthetic data. As such, we obtain estimators that are less sensitive tothe typical slow (statistical) convergence affecting the generators. We illustrate this with a simulationstudy in , showing that the coverage of both the mean and linear regression coefficientestimators indeed improves. In , we further cement the utility of our debiasing strategy ina practical setting through two case studies. While the proposed strategy is generator-agnostic, wefocus our analyses on two DGMs for tabular data: CTGAN and TVAE (Xu et al., 2019). Finally, concludes with a discussion on our method and its limitations.",
  "Notation and Set-Up": "The aim of this paper is to use synthetic data in order to learn a specific functional (.) of the observeddata distribution P. We formalise the problem of learning (P) based on synthetic data as follows.Suppose that, based on n independent (possibly high-dimensional) samples O1, ..., On from P, weestimate the observed data distribution as Pn. Here, Pn may be obtained by fitting parametric modelsto the distribution of the observed data, or alternatively by training DGMs. Based on m independent(synthetic) random samples S1, ..., Sm from Pn, we then estimate (P). For this, the data analyst(to whom Pn is unknown) uses the m synthetic samples S1, ..., Sm to approximate Pn as Pm, andobtains an estimator of (P) given by ( Pm). We use Pn to denote the empirical distribution of theobserved data and Pm to denote the empirical distribution of the synthetic data. We index expectationsby the distribution under which they are taken; e.g., EP (Y ) denotes the population expectation of Y . Throughout, we assume that the parameter of interest is sufficiently smooth in the data-generatingdistribution so that root-n consistent estimators (i.e., with SEs shrinking at 1 over root-n rate) canbe obtained. Formally, we assume the (P) is pathwise differentiable with efficient influence curve(EIC) (., P) under the nonparametric model, as is satisfied for (most) standard statistical analyses.The EIC is a functional derivative of (P) w.r.t. the data-generating distribution P (in the sense of aGateaux derivative), which has mean zero under P (Fisher and Kennedy, 2021; Hines et al., 2022). In what follows, we illustrate our debiasing strategy via two examples. Here, we briefly outline someof their theoretical foundations we build upon. Appendix A.1 clarifies how these debiased estimatorsoriginate from their EIC. We further refer to them as debiased or EIC-based estimators. Population mean.Adapting the traditional formulation of the population mean to the context ofsynthetic data, we choose Pm = Pm. As a result, we will study the large sample behaviour of thesynthetic data sample average:",
  "The EIC of this sample average is (S, Pn) = S ( Pn)": "Linear regression coefficient.Second, suppose we are interested in a specific coefficient (P)of some exposure A for the outcome Y in the linear model EP (Y |A, X) = A + (X), where Xis a possibly high-dimensional vector of covariates and (.) is an unknown function. Our proposalallows (X) to correspond to a standard linear model in all covariates, but is more generally valid.Building upon the nonparametric definition of as given in Appendix A.1, we adjust it to obtain anestimator in the setting of synthetic data. Let Y , X and A be jointly or sequentially modelled by somegenerative model, from which a single synthetic dataset Si = (Yi, Ai, Xi) is sampled (i = 1, . . . , m).We then consider the estimated regression coefficient of exposure A given by",
  "+ (O, Pn) (O, P)d(Pn P).(2)": "We now examine the eight terms of this equation and discuss why some may be negligible whileother may introduce bias. First, R() are remainder terms, which can generally be shown to be small,but must be studied on a case-by-case basis; see Hines et al. (2022) for worked out examples. In orderfor these remainder terms to be op(m1/2) and op(n1/2), we generally need that faster than n1/4 convergence rates are obtained for the unknown functionals of Pm and Pn that appear in the EICs. Itis unknown whether these convergence rates are attainable for DGMs; whether they are, will partlydepend on the number of parameters in the DGM itself, the dimension of the data and the complexityof the observed data distribution. The simulation study in will give further insight into this. Second, the two empirical process terms (1) and (2) in the von Mises expansion can be shown to beop(m1/2) and op(n1/2) by Markovs inequality, under weak conditions. For (1) to converge tozero, we will need the difference between Pm and Pn to converge to zero (in L2( Pn) at any rate),and the estimator to be calculated on a different part of the data than the one on which Pm wasestimated (Chernozhukov et al., 2018). For (2) to converge to zero, we will need Pn (e.g., the DGM)to consistently estimate P (in L2(P) at any rate). In addition, it can be argued that the DGM needsto be trained on a different part of the data than that on which the debiasing step (see later) will beapplied. In Appendix A.3, we elaborate on the necessity of sample splitting.",
  "i=1(Oi, Pn)(6)": "need some further discussion. Term (5) generally fails to have mean zero because the synthetic dataSi do not originate from the distribution Pm. This term therefore induces a bias in ( Pm) that resultsfrom using data-adaptive estimates Pm on the synthetic data; a similar term would appear if weinstead analysed the real data. Term (6) likewise fails to have mean zero because the observed dataOi do not originate from the distribution Pn. Also this term thus induces a bias, now resulting fromthe use of a generative model to obtain Pn. It may be large relative to (3) when DGMs are used,because of slow convergence of Pn. It is precisely this term that causes estimators based on syntheticdata to converge slowly with increasing sample size, as observed in Decruyenaere et al. (2024). After identifying the two problematic terms, we now propose in a second step a targeting or debiasingstrategy to remove these bias terms (5) and (6). As in van der Laan and Rose (2011) and Chernozhukovet al. (2018), we will remove bias term (5) by analysing the data with debiased estimators basedon the EIC that ensure that this bias term then becomes zero. Novel to our proposal is that wewill additionally shift the generated data to ensure that also bias term (6) becomes zero. This biasterm depends on the EIC, which itself depends on the target parameter of interest. In the next twoparagraphs, we discuss how this can be done for the two considered estimators. Note that the proposedstrategy does not require any actual finetuning or retraining of the DGM. For a given parameter ofinterest, a mere post-processing of the synthetic samples, based on access to the DGM as well as",
  "the original data it was trained on, allows eliminating the bias for a given parameter of interest. Agraphical summary of the problem setting and our debiasing strategy can be found in Appendix A.4": "3.1Population meanFor the population mean, the debiasing step with respect to term (5) is implicit since the traditionalestimator as given in is a debiased estimator. Therefore, the proposal to debias a givenDGM with respect to the population mean (P) =odP(o) amounts to first training the DGM andthen augmenting the output for the variable of interest to ensure that bias term (6) is zero, or hence",
  "i=1Oi ( Pn) = O ( Pn) = 0": "In other words, the population mean of the synthetic data under the DGM should match the sampleaverage of the real data. Here, ( Pn) can be approximated by generating a very large sample k(e.g., one million observations) based on the DGM and calculating their sample mean Y . Thegenerative model must then be corrected, to ensure that this sample mean equals O. To obtain adebiased synthetic sample, we shift all samples generated by the given DGM by adding O Y to theconsidered variable. Note that the sample average of such a set of m corrected synthetic samples willgenerally differ from O. 3.2Linear regression coefficientFor linear regression coefficients, this section shows how the samples generated by a given DGMneed to be adapted to eliminate bias term (6), written as follows (see Appendix A.5):",
  "ni=1Ai E Pn(A|Xi)2 ( Pn).(7)": "To compute this bias, we must generate, for each observed value Xi, a very large sample of measure-ments of A and Y with the given level Xi, based on the DGM. Then E Pn(Y |Xi) and E Pn(A|Xi)can be estimated as the sample average of those values for respectively Y and A. Further based on avery large DGM-generated sample k (e.g. one million observations), we calculate ( Pn) as follows:",
  "Debiasing of the DGM can now be done by adding the product b Ai E Pn(A| Xi)to the synthetic": "outcome observations Yi generated by the DGM. An in-depth elaboration is provided in AppendixA.5. With the proposed shifting, we ensure that the debiasing with respect to term (6) is completed.We then proceed our analysis with these shifted synthetic observations and employ the EIC-basedestimator of , which ensures that bias term (5) equals zero as well.",
  "i=1(Oi, P) + op(n1/2) + op(m1/2)": "In particular, the resulting estimator ( Pm) may even converge at root-n rates (under standardconditions of pathwise differentiability (Bickel et al., 1993; Hines et al., 2022)) and has an easy-to-calculate variance that acknowledges the uncertainty in the generation of synthetic data (providedthat the statistical convergence of the generator is not too slow). In Appendix A.6, we show that thevariance of ( Pm) may be approximated by 1",
  "thereby generalising results known for parametric synthetic data generators (Raab et al., 2016;Decruyenaere et al., 2024), where the correction factor": "1 + m/n was proposed. Thus, whenm , the debiased estimator ( Pm) based on debiased synthetic data has the same distribution aswhen the real data were analysed. Therefore, the proposed debiasing strategy delivers analysis resultsthat are asymptotically equivalent to those obtained from the same analysis on the real data, providedthat n/m = o(1). This means that results of the same quality and confidence intervals of the sameexpected length are then obtained, as will be illustrated in the case study in .2.",
  "i=1(Si, Pm)2": "This is a consistent estimator when Pm converges to Pn as m goes to infinity, and moreover, Pnconverges to P as n goes to infinity. We note that this sample variance will be subject to bias thatresults from poor tuning of the DGM. Removing this bias is not required, because this variancewill be scaled by 1/m + 1/n so that any bias becomes negligible in large samples. While the use ofdebiased estimators based on the EIC of E(O, P)2may potentially improve performance, thisgoes beyond the scope of this work. Practical implications.Sections 3.1 and 3.2 show how bias term (6) is eliminated by shifting thesynthetic variable of interest. We describe how bias term (5) is also removed by using debiasedestimators, which we referred to as EIC-based estimators (see ). As mentioned earlier, theEIC-based estimator for the population mean always coincides with the sample average, while for thelinear regression coefficient it only reduces to the ordinary least squares estimator when data-adaptiveestimation of the nuisance parameters is not used. This implies that it may suffice for the appliedresearcher to 1) shift the synthetic data and apply the traditional estimators, and 2) to multiply the SEof the estimator with 1 + m/n to obtain valid inference from a single synthetic dataset. However,the EIC-based estimators are recommended since they are robust against model misspecification byallowing for more flexibility in the estimation of the nuisance parameters.",
  "Simulation study": "Our proposed debiasing strategy is empirically validated by a simulation study that covers bothestimators 3.1 (sample mean) and 3.2 (linear regression coefficient). Having full control over thedata generating process allows us to calculate the bias, SE and convergence rate of both estimatorsin synthetic data, with and without our debiasing strategy. The data generating process consists ofthe following four variables: age (normally distributed), atherosclerosis stage (ordinal with fourcategories), therapy (binary), and blood pressure (normally distributed). The Directed Acyclic Graph(DAG) in represents the dependency structure and we refer to Appendix A.7.1 for moredetails. This setting allows us to simultaneously target the population mean of age and the populationeffect of therapy (A) on blood pressure (Y ) adjusted for stage (X).",
  ": DAG for the variables in the simulation study": "4.1Set-upWe conduct a Monte Carlo simulation study, where n independent records are sampled fromthe data generating process, forming the observed original dataset O1, ..., On.This processis repeated 250 times, with the sample size n varying log-uniformly between 50 and 5000(i.e., n {50, 160, 500, 1600, 5000}). Per original dataset, following DGMs are trained: CTGAN andTVAE (Xu et al., 2019), of which a detailed explanation can be found in Appendix A.7.2. From theseDGMs m synthetic data records are sampled that constitute the default synthetic dataset S1, ..., Sm.We set m = n to retain the dimensionality of the original data. Subsequently, each default syntheticdataset is debiased with respect to both estimands using the steps provided in , leading toa debiased synthetic dataset. Finally, two estimators are calculated in each of three datasets: thesample mean of age and the linear regression coefficient of therapy on blood pressure adjusted for 0% 25% 50% 75% 100% Mean age (MLE-based) Effect therapy(MLE-based) n (log scale)",
  "stage. We always report the maximum likelihood estimation (MLE)-based estimators, as used intraditional statistical analysis, of which the standard errors (SEs) are inflated with the correctionfactor": "1 + m/n to acknowledge the sampling variability of synthetic data. These estimators willdeliver similar estimates as the EIC-based estimators since no data-adaptive estimation is used (see.3 and Appendix A.7.5). 4.2ResultsWe now present the results of our simulation study. The DGMs were trained using the defaulthyperparameters as suggested by the package Synthcity (Qian et al., 2023). We also show resultsobtained for other hyperparameters (the default in the package SDV (Patki et al., 2016)) in AppendixA.7.4. In .2.1 we evaluate the empirical coverage of the 95% confidence interval (CI)for the population parameters. Our debiasing strategy should enhance the coverage, preferablyto the nominal level, allowing for (more) honest inference, which is the main contribution of ourstrategy. Then, we analyse step by step the various components that may influence the coverageby investigating the bias and SE of the estimators in .2.2, and their convergence ratesin .2.3. Additional results are presented in Appendix A.7.4, including the convergencerates of the nuisance parameters estimated by the DGMs. Our code is available on Github: 4.2.1CoverageBy definition, 95% (empirical) of the 95% CIs (nominal) should cover the population parameter. depicts the empirical coverage of the 95% CIs obtained from both original and syntheticsamples for the population mean of age and the population effect of therapy on blood pressureadjusted for stage. The results indicate that our debiasing strategy delivers empirical coverage levelsfor the population mean that approximate the nominal level for all sample sizes and DGMs considered.By contrast, the coverage based on the default synthetic datasets decreases with increasing n dueto slower shrinkage of the SE than the typical 1 over root-n rate, as calculated in .2.3and previously elaborated in Decruyenaere et al. (2024). For the population regression coefficient,debiasing delivers coverage at the nominal level for TVAE across all sample sizes, but not for CTGAN,although it clearly provides more honest inferences than based on the default synthetic datasets. Theresidual loss of coverage likely results from not using (efficient) sample splitting (see Appendix A.3). 4.2.2Bias and Standard ErrorFigures 3a and 3b depict the estimates and their SE, respectively, for the sample mean of age obtainedin the default and debiased synthetic datasets. Figures A5 and A6 in the appendix show these for thelinear regression coefficient of therapy on blood pressure adjusted for stage. In a, each dot isan estimate per Monte Carlo run and the true population parameters are represented by the horizontaldashed line. This figure allows a qualitative assessment of two key properties of estimators: empiricalbias (i.e., the average difference between the estimates and the population parameter, as representedby the solid line) and empirical SE (i.e., the standard deviation of the estimates, as indicated by thevertical spread of the estimates). Ideally, both converge to zero as the sample size grows larger. Theconvergence rate conveys the rate at which this happens. The funnel represents the default behaviourof an unbiased estimator based on original data of which the SE diminishes at a rate of 1 over root-n.",
  "(b)": "Figure A13: Figure (a) shows the empirical coverage of the 95% CI for the true proportion of deathin the aspirin treatment arm. In Figure (b), one can find the empirical type 1 error rate for therisk difference in death between aspirin and no aspirin group based on original data, default andtargeted synthetic data. The null hypothesis states that the risk difference is equal to 0.009, the riskdifference as observed in the population (i.e. the original IST data). Tests were conducted at the 5%significance level, where the black horizontal line on the figure depicts this nominal level.",
  "Case studies": "To illustrate our findings and highlight their implications for the applied researcher, we conduct twocase studies. First, .1 transfers the framework from our simulation study to the InternationalStroke Trial (IST) dataset (Sandercock et al., 2011). Second, .2 describes whether analysisresults from the Adult Census Income dataset (Becker and Kohavi, 1996) are similar to those obtainedfrom the real data, when the sample size m of the generated synthetic data is very large. In bothcase studies, estimated SEs in the default synthetic and debiased synthetic datasets are corrected bymultiplying with factor",
  "+ m/n to acknowledge the sampling variability of synthetic data": "5.1International Stroke TrialWe adapt the framework discussed in .1 to the IST dataset, one of the biggest randomisedtrials in acute stroke research (Sandercock et al., 2011). The dataset with 19285 complete cases now constitutes our population. We mimic different hypothetical settings where an institution only hasaccess to a limited sample of observations, with the sample size n varying between 50 and 5000. Inorder to easily share the data with other researchers, the institution generates a synthetic dataset withsample size m, where m = n. We repeated this process 100 times per sample size n to be able tocalculate the empirical coverage levels. For illustration purposes, we focus on the effect of aspirin onthe outcome at 6 months and report the proportion of deaths for the two treatment arms (aspirin andno aspirin), and its corresponding risk difference. For each value of n, two default synthetic datasetswere generated using both CTGAN and TVAE. Next, for each of these, we first split the default syntheticdataset by treatment, debias the data with relation to the population mean within each treatment arm,and then combine them back into one debiased synthetic dataset. We noticed that using the samehyperparameters as in the simulation study resulted in biased estimates, as can be seen in FigureA12 in the appendix. For this reason, we highlight the results obtained by training with the defaulthyperparameters suggested by the package SDV (Patki et al., 2016) instead. One of the original research questions in Sandercock et al. (2011) was whether or not there is adifference in risk of death between the treatment arms. Suppose a researcher can repeatedly collectinformation on 500 subjects and uses original, default synthetic and debiased synthetic data to makean inferential statement about this risk difference. depicts the confidence intervals for thefirst 15 repetitions, with the vertical dashed lines representing the true risk difference of 0.009 ascalculated based on our population (the full dataset). Should the researcher use the default syntheticdata, they would falsely conclude (in 7 out of these 15 repetitions) that the risk is significantlydifferent from 0.009, while using the debiased synthetic dataset basically eliminates this highnumber of false-positives (with all these 15 intervals containing the population parameter), as is thecase in the original data as well. More results can be found in Appendix A.8.1. 0.200.2 Original data 0.200.2 Default CTGAN 0.200.2 Debiased CTGAN Risk difference death",
  "Adult Census Income Dataset": "We also perform a case study on the Adult Census Income dataset, which comprises 45222 completecases and 14 unique variables (Becker and Kohavi, 1996). We assume the researchers interest liesin inferring the population mean of hours worked per week (estimated via the sample mean) andthe average sex-adjusted difference in age between persons with an income of > $50K a year vs.not (estimated via a linear regression model age (Y ) income (A) + sex (X)). Our goal in thiscase study is to confirm whether inferential results obtained using the debiased synthetic datasetare asymptotically equivalent (i.e. with m >> n in our debiasing strategy) to those obtained usingthe original data. To test this across different sample sizes, the original data constitute five differentsamples of the Adult Census Income dataset with sizes n varying log-uniformly between 50 and45222. For each original dataset, a default synthetic dataset of size m = 106 was generated by TVAE.Subsequently, this dataset was debiased following the steps described in .1 (sample mean)and .2 (linear regression coefficient). depicts the 95% CIs for both estimators, the five original sample sizes and the three versionsof datasets. This indeed confirms that analysis on the debiased synthetic dataset leads to results ofsimilar quality and CIs of similar length compared to the original dataset. By contrast, the analysison the default synthetic dataset may yield results of inferior quality and even incorrect conclusions.",
  "Discussion": "In this paper, we propose a new debiasing strategy that targets synthetic data created by DGMstowards the downstream task of statistical inference from the resulting synthetic data. We establishour theory for two estimators by applying insights from debiased or targeted machine learningliterature (van der Laan and Rose, 2011; Chernozhukov et al., 2018) to the current setting wheremachine learning is not necessarily used in the analysis, but rather in the generation of syntheticdata. We obtain estimators that are less sensitive to the typical slow (statistical) convergence affectingDGMs and thereby improve the inferential utility of the synthetic data. We illustrated the impact of our proposal through a simulation study on toy data and two casestudies on real-world data. Our debiasing strategy results in root-n consistent estimators based onthe synthetic data and thereby better coverage of the confidence intervals, allowing for more honestinference. While coverage was clearly improved, it was not guaranteed to be at the nominal level.Indeed, it may remain anti-conservative for some estimators and DGMs, due to slow convergenceinherent to these models and/or due to residual overfitting bias that could not be removed sincesample splitting was not performed. Future work should focus on efficient sample splitting, wherethe resulting bias reduction outweighs the increase in finite-sample bias that arises from trainingon smaller sample sizes. Alternatively, findings from Ghalebikesabi et al. (2022) on importanceweighting could be incorporated, with the weights being targeted to eliminate the impact of thedata-adaptive estimation of the weights. This may potentially relax the fast baseline convergenceassumption, and enable the same debiased synthetic data to be used for multiple downstream analyses. A key advantage of our debiasing strategy is that it may deliver synthetic data created by DGMs ofwhich the analysis is equivalent to the original data analysis, provided that the synthetic sample size ischosen to be much larger than the original sample size. Although the risk of disclosure may increasewith the size of non-DP synthetic data (Reiter and Drechsler, 2010), this trade-off is beyond the scopeof our paper. More interestingly, in the case of DP synthetic data, our debiasing strategy exploits theirpost-processing immunity that allows for data transformations without compromising any privacyguarantees (Dwork and Roth, 2014). However, our strategy needs to be extended to incorporate theDP-constraints when studying the difference between ( Pm) and (P) in DP synthetic data. Limitations of our proposal include the low-dimensional setting of our simulation and case studies,for which DGMs might be less well suited. The positive results found for two widely used estimatorsin this simple setting highlight the utility of a debiasing approach and are encouraging in terms offuture larger-scale applications. However, before addressing these, it is important to first understandlow-dimensional settings, where valid inference is already challenging to attain. While our debiasingstrategy boils downs to a post-processing step, one could argue that the lack of change to the DGMstraining strategy itself is actually a strength, since it renders our strategy generator-agnostic. Another limitation concerns the fact that our debiasing strategy for a regression coefficient requiressampling of synthetic data conditional on a covariate, which is not available in all DGMs. However,this issue is partially mitigated in the case of conditioning on categorical variables, since one canalways generate a synthetic dataset unconditionally and then only select samples that fit the condition though this approach also has its limits, especially when conditioning on multiple covariates at once.Zhou et al. (2023) propose a deep generative approach to sample from a conditional distribution, evenwhen working with high-dimensional data. Future work could explore this strategy. Finally, our proposal requires that the person generating synthetic data is aware of the analyses thatwill be run on those data, and has access to the corresponding EICs needed for debiasing (whichin particular rules out the possibility for debiasing w.r.t. non-pathwise differentiable parameters,such as conditional means or predictions). For each parameter of interest, the data generated by theDGM will then need to be debiased (simultaneously) w.r.t. that parameters EIC, which is left forfuture research. In the case of original data, several debiased estimation strategies that do not requireexact knowledge about the EIC already exist. These methods include a) approximating the EICthrough finite-differencing (Carone et al., 2019; Jordan et al., 2022) or stochastic approximations viaMonte Carlo (Agrawal et al., 2024), and b) automatically estimating the EIC from the data throughauto-DML (Chernozhukov et al., 2022). Alternatively, kernel debiased plug-in estimation methods(Cho et al., 2023) enable simultaneous debiasing of all pathwise differentiable target parametersthat meet certain regularity conditions, without requiring any knowledge about the EIC. Integratingtheir insights could further strengthen the foundations of our current work on the interplay betweensynthetic data, deep generative modelling, and debiased machine learning.",
  "Fisher, A. and Kennedy, E. H. (2021). Visually communicating and teaching intuition for influencefunctions. The American Statistician, 75(2):162172": "Ghalebikesabi, S., Wilde, H., Jewson, J., Doucet, A., Vollmer, S., and Holmes, C. (2022). Mitigatingstatistical bias within differentially private synthetic data. In Cussens, J. and Zhang, K., editors,Proceedings of the Thirty-Eighth Conference on Uncertainty in Artificial Intelligence, volume 180of Proceedings of Machine Learning Research, pages 696705. PMLR. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A.,and Bengio, Y. (2014). Generative adversarial nets. Advances in neural information processingsystems, 27.",
  "Raghunathan, T. E., Reiter, J. P., and Rubin, D. B. (2003). Multiple imputation for statistical disclosurelimitation. Journal of official statistics, 19(1):1": "Ris, O., Jlk, J., Kaski, S., and Honkela, A. (2023). Noise-aware statistical inference withdifferentially private synthetic data. In International Conference on Artificial Intelligence andStatistics, pages 36203643. PMLR. Reiter, J. P. (2005). Releasing multiply imputed, synthetic public use microdata: an illustrationand empirical study. Journal of the Royal Statistical Society Series A: Statistics in Society,168(1):185205.",
  "Vansteelandt, S. and Dukes, O. (2022). Assumption-lean inference for generalised linear modelparameters. Journal of the Royal Statistical Society Series B: Statistical Methodology, 84(3):657685": "Wan, Z., Zhang, Y., and He, H. (2017). Variational autoencoder based synthetic data generation forimbalanced learning. In 2017 IEEE symposium series on computational intelligence (SSCI), pages17. IEEE. Wilde, H., Jewson, J., Vollmer, S., and Holmes, C. (2021). Foundations of bayesian learning fromsynthetic data. In International Conference on Artificial Intelligence and Statistics, pages 541549.PMLR.",
  "i=1Si": "Linear regression.Suppose we are interested in a specific coefficient (P) of some exposureA in the linear modelEP (Y |A, X) = A + (X),where X is a possibly high-dimensional covariate and (.) is an unknown function. Our proposalallows (X) to correspond to a standard linear model, but is less restrictive. The parameter in thislinear model can be nonparametrically defined as",
  "EP{A EP (A|X)}2": "Denote the obtained synthetic data samples as S = (Y , A, X). An estimator ( Pm) is then obtainedby substituting in the above expression for (P), the first expectation in the numerator and denomi-nator by a sample average, EP (A|X) and EP (Y |X) by data-adaptive predictions E Pm(A| Xi) and",
  "A.2Derivation of the impact of uncertainty affecting deep generative models": "We are interested in knowing how much ( Pm) differs from (P). For this, we study the difference( Pm) (P), for which we consider 2 von Mises expansions (i.e., functional Taylor expansions).Throughout the calculation below, we use that influence curves (O, P) have the property of beingmean zero when averaging w.r.t. the distribution P. We moreover let R(.) be a remainder term,which can generally be shown to be small, but must be studied on a case-by-case basis.",
  "i=1(Si, Pn)": "equals 1 over root-m times a term that converges to E(S, Pn)2| Pn1/2times a standard normaldistribution (as m goes to infinity). This follows from the mean zero property of influence curvesand the fact that the synthetic data are drawn from the distribution Pn. This mean zero propertyis essential as it implies that the variation of Pn across repeated (observed data) samples does not",
  "E(S, Pn)2| Pn=(o, Pn)2d Pn(o)": "is generally smooth (continuous) in the distribution of the data (as in the considered two examples).Moreover, the flexibility offered by deep generative models (DGMs) makes it reasonable to assumethat Pn converges to P (e.g., in L2(P)); while this convergence may be slow, no requirements on therate of convergence are needed for the above assumption to be satisfied.",
  "In order to let (O, Pn) (O, P)d(Pn P)": "converge to zero, we will need the DGM to be trained on a different part of the data than the oneon which the debiasing step will be applied. Such sample splitting is needed to prevent overfittingbias that may otherwise result from the highly data-adaptive nature of DGMs. In addition, we needPn (e.g., the DGM) to consistently estimate P in the sense that the squared mean (under P) of(O, Pn) (O, P) (at fixed Pn) converges to zero in probability. To prevent efficiency loss with sample splitting, one may consider the use of k-fold cross-fitting. Forthis, we randomly split the data in k folds, each time train the DGM on k 1 folds to obtain anestimator of the observed data distribution P(k1)n/k and calculate the bias",
  "based on the n/k data points Oi in the remaining fold. The average of the k obtained bias estimatescan next be used for debiasing (see the next section for specific examples)": "However, it remains to be seen from future work if the resulting bias reduction outweighs the increasein finite-sample bias that may result from training the DGM on smaller sample sizes. Preliminarysimulations with a simple implementation suggested that this was not the case, which is why ourresults in the main text are reported without the use of sample splitting. Furthermore, the remainderof the theory discards this nuance for now as well.",
  "Additional synthetic data uncertainty": "Figure A1: Suppose that interest lies in inferring the mean (.) (orange solid vertical line) of the ground truthdistribution P (orange solid curve) from synthetic data. (1) First, a random sample of original data of size nwith empirical distribution Pn (orange histogram) is collected from this ground truth. Theory on asymptoticlinearity prescribes that the sample mean (orange dashed vertical line) will deviate from the population meanby order of 1 over root-n (grey arrow). (2) Subsequently, a deep generative model is trained on these originaldata, yielding an estimated distribution Pn (blue solid curve). Its mean (blue solid vertical line) may in turndiffer from the original sample mean by an order larger than 1 over root-n (red arrow), which is referred toas regularisation bias in Decruyenaere et al. (2024). (3) Finally, synthetic data of size m (here m = n) aresampled from the estimated distribution Pn, forming the empirical distribution Pm = Pm (blue histogram withsample mean indicated by the blue dashed vertical line). The mean of both distributions Pn and Pm will againdiffer by order of 1 over root-m (green arrow). Ideally, the data analyst, who uses the synthetic data to estimate(P) by ( Pm), needs to take into account these three sources of random variability. (4) The large samplebehaviour of the synthetic data estimator ( Pm) is depicted by repeating the above procedure multiple timesacross increasing sample sizes of n = m and storing each estimate of the synthetic sample mean. Althoughthe estimator remains unbiased for (P) (dashed line), its empirical standard error becomes larger than in theoriginal data due to the additional sources of variability. However, the correction factor 1 + m/n to themodel-based standard error previously proposed by Raab et al. (2016) only captures the original data samplingvariability (grey funnel) and a lower bound of the synthetic data sampling variability (green funnel), while theuncertainty associated with the regularisation bias (red funnel) remains unaccounted for. Since it cannot readilybe expressed analytically, our debiasing strategy will eliminate the latter by shifting the mean of the distribution( Pn) estimated by the generative model towards the original sample mean (thereby removing the red arrowand funnel). Additionally, choosing synthetic sample sizes of m will shrink the synthetic data samplingvariability (ultimately removing the green arrow and funnel), such that the synthetic data estimator exhibitssimilar large sample behaviour as in original data.",
  "ni=1Ai E Pn(A|Xi)2 ( Pn)": "To compute this bias, we must generate, for each observed value Xi, a very large sample of measure-ments of A and Y with the given level Xi, based on the DGM. Then E Pn(Y |Xi) can be calculatedas the sample average of those values for Y , and likewise E Pn(A|Xi) can be calculated as thesample average of those values for A. Further based on a large sample generated based on the DGM,we calculate ( Pn) as the sample average ofA E Pn(A|X) Y E Pn(Y |X)divided by the",
  "sample average ofA E Pn(A|X)2": "Debiasing of the DGM can now be done by adding b{ Ai E Pn(A| Xi)} to the synthetic outcomeobservations generated by the DGM. This change does not affect the predictions E Pn(Y |Xi) fromthe DGM (because Ai E Pn(A|Xi) averages to zero for each choice of Xi). Further, with this",
  "=EE(Si, Pn)| Pn(Oj, P)= 0": "where in the second equality we use that Pn is determined by O1, ..., On, in the third equality weuse that Si only depends on O1, ..., On via Pn, and in the final equality we use that (Si, Pn) hasmean zero when the synthetic data are sampled from Pn. This renders these terms asymptoticallyindependent and their sum, hence, asymptotically normal. Moreover, since the variance of (Si, Pn)converges to E(O, P)2(see the previous section; Appendix A.2), the variance of ( Pm) maythus be approximated by 1",
  "Di {age, stage, therapy, bp}end": "Inspired by an applied medical setting, we create a hypothetical disease, defined by a low-dimensionaltabular data generation process. The dependency structure depicted by the directed acyclic graph(DAG) in in the main text displays the presence of four variables, each of them chosen toobtain a mix of data types. In our hypothetical disease, it is assumed that a patient is observed ata given point in time. At this time, patient data about age, atherosclerosis stage, and the randomassignment of therapy is gathered. The continuous outcome variable blood pressure is evaluated at alater time point, making this design a simplification, since we do not consider the data as longitudinal. The exact routine to reconstruct this data generating process is presented in the pseudo-code inAlgorithm 1. Age (continuous) follows a normal distribution with mean 50 and standard deviation10. Atherosclerosis stage (ordinal) was generated according to a proportional odds cumulative logitmodel where an increase in age causes an increase in the odds of having a stage higher than agiven stage k (age = 0.05 and intercepts stage = {2, 3, 4} for stage I-III). Therapy (binary)is considered to be 1:1 randomly assigned and is therefore sampled from a Bernouilli distributionwith a probability of 0.50. The last variable, blood pressure (continuous), is sampled from a normaldistribution with standard deviation 10 and where the baseline average of 120 increases with higheratherosclerosis stage (stage = {0, 10, 20, 30} for stage I-IV, respectively) and absence of therapy(therapy = 20).",
  "A.7.2Deep Generative Models": "We elaborate on the DGMs used to create synthetic data in our simulation study and cases studies. Allexperiments were run on our institutional high performance computing cluster using a single GPU(NVIDIA Ampere A100; 80GB GPU memory) and single CPU (AMD EPYC 7413), taking lessthan 24 hours to complete (simulation study: less than 15 minutes per individual run across 5 samplesizes; International Stroke Trial case study: less than 75 minutes per individual run across 5 samplesizes; Adult Census Income Dataset case study: less than 4 hours). We focus on two commonly usedDGMs, namely Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) and VariationalAutoencoders (VAEs) (Kingma and Welling, 2013). A GAN consists of two competing neural networks, a generator and discriminator, and aims toachieve an equilibrium between both (Hernandez et al., 2022). This translates to a mini-max game,since the generator aims to minimise the difference between the real and generated data, while thediscriminator aims to maximise the possibility to distinguish the real and generated data (Goodfellow",
  "et al., 2014). We use the CTGAN implementation that was designed specifically for tabular data,proposed by Xu et al. (2019)": "A VAE is a deep latent variable model, consisting of an encoder and a decoder (Kingma and Welling,2013). The encoder models the approximate posterior distribution of the latent variables given aninput instance, whereby typically a standard normal prior is assumed for the latent variables. Thedecoder allows reconstructing an input instance, based on a sample from the predicted latent spacedistribution. Encoder and decoder can be jointly trained by maximising the evidence lower bound(ELBO), i.e. the marginal likelihood of the training instances. Maximising the ELBO corresponds tominimising the Kullback-Leibler (KL) divergence between the predicted latent variable distributionfor a given input instance and the standard normal priors, and minimising the reconstruction error ofthe input instance at the decoder output. Once again, we use the tabular implementation of a VAE(TVAE) proposed by Xu et al. (2019). The results presented in the rest of the Appendix are obtained by training both types of DGM withdefault hyperparameters as suggested by the packages Synthcity and SDV (for both CTGAN andTVAE). A comparison of the default hyperparameters in both packages is provided in Tables A1 andA2. Note that both packages implement the CTGAN and TVAE modules as originally proposed by Xuet al. (2019), where a cluster-based normaliser is used to preprocess numerical features.",
  "GeneratorDebiasedn = 50n = 160n = 500n = 1600n = 5000": "CTGAN (SDV)no0.8490.9240.9390.9630.980CTGAN (SDV)yes0.8600.9350.9570.9660.981CTGAN (Synthcity)no0.8380.8940.8670.8900.929CTGAN (Synthcity)yes0.8490.9060.8850.8970.933TVAE (SDV)no0.7040.8190.9080.9850.980TVAE (SDV)yes0.7340.8320.9130.9870.983TVAE (Synthcity)no0.7910.8300.8870.9290.977TVAE (SDV)yes0.8130.8600.9130.9430.980 Failed generators.CTGAN (Synthcity) could not be trained in 14 runs (runs 38, 69, 102, 225for n = 500; runs 19, 23, 98, 107, 117, 128, 129, 140, 148, 169 for n = 5000) due to an internal errorin the package. As such, it was not possible to generate default and debiased synthetic data withCTGAN (Synthcity) in these runs. This comprises 1.12% (14/1250) of all CTGAN (Synthcity)trained and 0.28% (14/5000) of all generators trained. The other generative models did not produceerrors during training, so that default synthetic data could be generated in every run. Failed debiasing.TVAE (SDV) could not be debiased in 82 runs (runs 1, 3, 4, 5, 6, 8,15, 23, 24, 28, 30, 42, 46, 50, 51, 58, 64, 72, 78, 92, 96, 104, 106, 111, 112, 117, 118, 119, 122, 126,127, 130, 133, 135, 140, 143, 146, 149, 154, 157, 159, 160, 166, 167, 168, 170, 178, 180, 187, 191,193, 196, 197, 200, 205, 210, 214, 216, 218, 220, 225, 230, 231, 235, 236, 245, 247, 248 for n = 50;runs 13, 22, 97, 117, 139, 145, 146, 148, 161, 186, 193, 197, 235, 246 for n = 160) due to sparsedata, especially for small sample sizes. In particular, some Xi in the original dataset may notbe present in the default synthetic dataset generated by TVAE (SDV), leading to an error whenestimating the nuisance parameters needed for the debiasing step. This comprises 6.56% (82/1250)of all TVAE (SDV) trained and 1.64% (82/5000) of all generators trained. The other generativemodels did not produce errors during debiasing, so that debiased synthetic data could be generated inevery run. Exact memorisation.A sanity check was conducted to ensure that no records of the original datawere memorised by the generative model. CTGAN (Synthcity) made the following number ofexact copies of the original data in the synthetic data: one record (2.00%) for n = 50 in three runs(runs 63, 158, 192). The other generative models did not make exact copies. Non-estimable estimators.Due to sparse data, especially for small sample sizes, the linear regres-sion coefficient of therapy on blood pressure adjusted for stage could not be estimated in a smallsubset of the 11140 (original and synthetic) datasets, producing extremely small (< 1e10) or large(> 1e2) standard errors. Overall, 0.09% (10/11140) regression coefficient estimates could not beobtained: in 4 runs for default synthetic dataset of size m = 50 generated by CTGAN (SDV) and in 1run for default synthetic dataset of size m = 160 generated by CTGAN (Synthcity), and for theircorresponding debiased synthetic datasets. The sample mean of age was always estimable.",
  "A.7.4Additional results": "While the main text only reports results using the Synthcity default hyperparameters, we addition-ally wanted to report a wider variety of trained models, without manually tuning anything, which iswhy results for SDV are presented here as well. Coverage for all estimators and models.The results shown in Figure A2 indicate that ourdebiasing strategy delivers empirical coverage levels for the population mean that approximate thenominal level for all sample sizes and DGMs considered. By contrast, the coverage in the defaultsynthetic datasets decreases with increasing n due to slower shrinkage of the standard error (SE) thanthe typical 1 over root-n rate, as calculated in .2.3 and previously addressed in Decruyenaere et al. (2024). While our debiasing strategy clearly improves the empirical coverage for the populationregression coefficient, it does, however, not guarantee this to be at the nominal level for all samplesizes and DGMs considered. In particular, debiasing only seems to achieve coverage at the nominallevel for TVAE (Synthcity) across all sample sizes and for TVAE (SDV) at sufficiently large samplesizes. Our approach still falls short for CTGAN (Synthcity) and CTGAN (SDV), although providingmore honest inference than the default synthetic datasets. 0% 25% 50% 75% 100% Mean age (MLE-based) Effect therapy(MLE-based) n (log scale)",
  "Figure A2: Empirical coverage of the 95% confidence interval for the MLE-based estimators": "Bias and SE for mean age.As shown in Figure A3, the sample mean of age is unbiased in thedefault synthetic datasets, but exhibits large variability that shrinks slowly with sample size due to thedata-adaptive nature of DGMs. Debiasing reduces this variability and accelerates shrinkage. FigureA4 indicates that the empirical SE for the sample mean of age is indeed, on average, underestimatedby the MLE-based SE in default synthetic datasets. After debiasing, the average MLE-based SEapproximates the empirical SE, albeit with minor deviations at smaller sample sizes. CTGAN(SDV)",
  "CTGAN": "(Synthcity) TVAE(SDV) TVAE (Synthcity) Debiased: no 505005000 505005000 505005000 Debiased: yes n (log scale) Effect therapy (MLE-based) Figure A5: The horizontal dashed line represents the population effect of therapy on blood pressureadjusted for stage and each dot is a MLE-based estimate per Monte Carlo run (250 dots in total pervalue of n). The dashed funnel indicates the behaviour of an unbiased and n-consistent estimatorbased on observed data. CTGAN(SDV)",
  "Figure A6: The empirical and MLE-based standard error for the (MLE-based) effect of therapy onblood pressure adjusted for stage": "Convergence of SE for all estimators and models.The convergence rate of the empirical SE forthe MLE-based estimators is shown in Table A4 and represented by the slope in Figure A7. Thedashed line indicates the behaviour of the SE of an unbiased and n-consistent estimator basedon observed data, whereas the dotted line indicates the assumed behaviour of the SE of the sameestimator based on synthetic data, following the correction proposed by Raab et al. (2016). Lines forthe empirical SE that are parallel to these lines indicate that the estimator converges at 1 over root-nrate, whereas more horizontal or vertical lines imply slower or faster shrinkage, respectively. Afterdebiasing, all lines are parallel for both estimators, suggesting n-consistency. Note that the verticaloffset of some lines, which represents the log asymptotic variance, is still too large for some DGMsdespite debiasing. This is the case for the linear regression coefficient for therapy on blood pressureadjusted for stage obtained in the debiased synthetic dataset generated by CTGAN (SDV) and CTGAN(SDV).",
  "Debiased synthetic datasetsMean age0.42 [0.36; 0.48]0.50 [0.46; 0.54]0.45 [0.44; 0.47]0.47 [0.44; 0.50]Effect therapy0.58 [0.43; 0.73]0.43 [0.31; 0.55]0.61 [0.43; 0.79]0.46 [0.38; 0.55]": "DefaultDebiased Mean age: MLE-based Effect therapy: MLE-based n (log-scale) log(SE) Empirical SE Original dataCTGAN (SDV)CTGAN (synthcity)TVAE (SDV)TVAE (synthcity) Theoretical SE Original dataSynthetic data Figure A7: Convergence rate of the empirical standard error (SE) for the MLE-based estimators. Ifthe SE is of the form SE = cna, where c is a constant, then log (SE) = log(c) + (a) log (n).Therefore slope a represents the convergence rate and the vertical offset log(c) indicates the logasymptotic variance. The dashed line indicates the behaviour of the SE of an unbiased and n-consistent estimator based on observed data, whereas the dotted line indicates the assumed behaviourof the SE of the same estimator based on synthetic data, following the correction proposed by Raabet al. (2016). Nuisance parameters.Our debiasing strategy assumes that the two remainder terms in the vonMises expansion presented in are op(m1/2) and op(n1/2). This requires the differencebetween Pm (i.e., estimate of Pn based on the sampled synthetic dataset of m records) and Pn(i.e., estimate of P by the DGM trained on n original records) and the difference between Pn and P(i.e., the data generating process), to converge to zero in L2( Pn) and L2(P), respectively, at faster thann-to-the-quarter convergence rates. While this holds true for the convergence of Pm to Pn (see TableA5), this seems not always strictly attained for the functionals of Pn with respect to P that appear inthe efficient influence curves for the regression coefficient (see Table A6), in particular for CTGAN",
  "FunctionalCTGAN (SDV)CTGAN (Synthcity)TVAE (SDV)TVAE (Synthcity)": "E[A|X = I]0.24 [0.12; 0.36]0.16 [0.02; 0.30]0.46 [0.08; 0.84]0.30 [0.25; 0.35]E[A|X = II]0.19 [-0.01; 0.38]0.22 [0.08; 0.36]0.64 [0.25; 1.04]0.32 [0.24; 0.40]E[A|X = III]0.18 [-0.03; 0.40]0.22 [0.08; 0.36]0.51 [0.09; 0.94]0.27 [0.15; 0.39]E[A|X = IV ]0.19 [0.01; 0.38]0.23 [0.08; 0.38]0.44 [0.07; 0.82]0.21 [0.02; 0.40]E[Y |X = I]0.37 [0.08; 0.66]0.21 [0.06; 0.35]0.31 [0.16; 0.46]0.27 [0.15; 0.39]E[Y |X = II]0.30 [0.08; 0.53]0.14 [0.01; 0.27]0.49 [0.03; 0.95]0.24 [0.10; 0.38]E[Y |X = III]0.27 [0.04; 0.51]0.17 [0.03; 0.32]0.31 [-0.01; 0.63]0.18 [0.07; 0.30]E[Y |X = IV ]0.26 [0.02; 0.49]0.21 [0.10; 0.33]0.17 [0.02; 0.33]0.19 [0.04; 0.34]( Pn)0.33 [0.04; 0.61]0.35 [0.28; 0.43]0.37 [0.12; 0.62]0.29 [0.07; 0.51] Summary.Table A7 summarises the effect of our debiasing strategy on bias, SE and coveragein the simulation study. Our strategy results in uniformly valid coverage for the population mean,allowing for honest inference. For the regression coefficient, coverage was clearly improved but mayremain anti-conservative for some DGMs. This may originate from residual overfitting bias inherentto these DGMs that could not be removed since (efficient) sample splitting was not performed (seeAppendix A.3).",
  "A.7.5Influence curve based estimation": "Here, two estimators are estimated in the original and synthetic datasets: a maximum likelihoodestimation (MLE)-based one, as used in traditional statistical analysis, and an efficient influence curve(EIC)-based one, as proposed in this paper and obtained after 5-fold cross-fitting during estimationof the nuisance parameters. Note that sample splitting was not used during the debiasing step. Forthe former, SEs are calculated via the regular expressions which discard the uncertainty associatedwith data-adaptive prediction during estimation, while for the latter, SEs are based on the EIC whichacknowledges this uncertainty. Throughout, all estimated (model- or EIC-based) SEs are correctedwith 1 + m/n to acknowledge the sampling variability of synthetic data. This correction factorwas initially proposed by Raab et al. (2016) for parametric synthetic data generators, but was foundto be insufficient for synthetic data created by DGMs (Decruyenaere et al., 2024). In .3, wegive the formula for the variance of our EIC-based estimator on the debiased synthetic data, whichgeneralises this correction factor to the setting where synthetic data were generated by DGMs. The results of our simulation study using the EIC-based estimators, as shown below in FiguresA8-A11 and Table A8, remain unchanged as compared to using the MLE-based estimators, sinceno data-adaptive predictions (e.g., machine learning) were used during estimation of the nuisanceparameters. If data-adaptive estimation were to be used, we expect the MLE-based estimators to beoverly optimistic, while the EIC-based estimators could handle the additional uncertainty introducedby data-adaptive estimation. 0% 25% 50% 75% 100% MLE-basedEIC-based Mean age 0% 25% 50% 75% 100% Effect therapy n (log scale)",
  "Estimate": "Figure A12: Estimates for the proportion of death in both treatment arms and their correspondingrisk difference estimates. We show results for original data, and both default synthetic data (left)and debiased synthetic data (right) for all four generators. The horizontal dashed line represents thepopulation proportion of death in each group and the corresponding risk difference, and each dot isan estimate per Monte Carlo run (100 dots in total per value of n). The dashed funnel indicates thebehaviour of an unbiased and n-consistent estimator based on observed data. 0% 25% 50% 75% 100% n (log scale)",
  "EstimatorCTGAN (SDV)CTGAN (Synthcity)TVAE (SDV)TVAE (Synthcity)": "Default synthetic datasetsMean age (MLE)0.33 [0.04; 0.62]0.01 [-0.16; 0.18]0.10 [-0.13; 0.32]0.21 [0.04; 0.39]Mean age (EIC)0.33 [0.04; 0.62]0.01 [-0.16; 0.18]0.10 [-0.13; 0.32]0.21 [0.04; 0.39]Effect therapy (MLE)0.23 [-0.09; 0.56]0.31 [0.22; 0.40]0.28 [0.10; 0.46]0.09 [-0.13; 0.31]Effect therapy (EIC)0.33 [-0.11; 0.78]0.31 [0.24; 0.38]0.30 [0.13; 0.48]0.10 [-0.13; 0.32] Debiased synthetic datasetsMean age (MLE)0.42 [0.36; 0.48]0.50 [0.46; 0.54]0.45 [0.44; 0.47]0.47 [0.44; 0.50]Mean age (EIC)0.42 [0.36; 0.48]0.50 [0.46; 0.54]0.45 [0.44; 0.47]0.47 [0.44; 0.50]Effect therapy (MLE)0.58 [0.43; 0.73]0.43 [0.31; 0.55]0.61 [0.43; 0.79]0.46 [0.38; 0.55]Effect therapy (EIC)0.63 [0.50; 0.75]0.43 [0.32; 0.54]0.64 [0.48; 0.79]0.46 [0.38; 0.55] DefaultDebiased Mean age: MLE-based Mean age: EIC-based Effect therapy: MLE-based Effect therapy: EIC-based n (log-scale) log(SE) Empirical SE Original dataCTGAN (SDV)CTGAN (synthcity)TVAE (SDV)TVAE (synthcity) Theoretical SE Original dataSynthetic data Figure A11: Convergence rate of the empirical standard error (SE) for the maximum likelihoodestimation (MLE)-based and efficient influence curve (EIC)-based estimators. If the SE is of theform SE = cna, where c is a constant, then log (SE) = log(c) + (a) log (n). Therefore slope arepresents the convergence rate and the vertical offset log(c) indicates the log asymptotic variance.The dashed line indicates the behaviour of the SE of an unbiased and n-consistent estimator basedon observed data, whereas the dotted line indicates the assumed behaviour of the SE of the sameestimator based on synthetic data, following the correction proposed by Raab et al. (2016).",
  "A.8.1International Stroke Trial": "We adapt the framework discussed in .1 to the International Stroke Trial (IST), one of thebiggest randomised trials in acute stroke research (Sandercock et al., 2011). The dataset with 19285complete cases now constitutes our population. We mimic different hypothetical settings wherean institution only has access to a limited sample of observations, with the sample size n varyingbetween 50 and 5000. In order to easily share the data with other researchers, the institution generates a synthetic datasetwith sample size m, where m = n. Similarly to the simulation study, we repeated this process100 times per sample size n, to be able to calculate the empirical coverage levels. For illustrationpurposes, we focus on the effect of aspirin on the outcome at 6 months and report the proportion ofdeaths for the two treatment arms (aspirin and no aspirin), and its corresponding risk difference. For each value of n, two default synthetic datasets were generated using both CTGAN and TVAE. Giventhe interest in the proportion of death in the group with and without aspirin, we use the debiasingstrategy with respect to the population mean. This implies that the default synthetic dataset was firstsplit by treatment and then debiased with relation to the population mean within each treatment arm.The two debiased subdatasets were then afterwards combined into one debiased synthetic dataset foreach generative model. For both the default and debiased synthetic dataset, the sampling variabilityof synthetic data is acknowledged by inflating the standard errors (SEs) by the correction factor",
  "+ m/n": "The funnel plots for the proportion of deaths in both treatment arms and the risk difference areshown in Figure A12. We noticed that using the same hyperparameters as in the simulation studyresulted in biased estimates, as can be seen in Figure A12. For this reason, we highlight the resultsobtained by training with the default hyperparameters suggested by the package SDV (Patki et al.,2016) instead. Analogously to the simulation study, our debiasing strategy decreases the variance ofthe mean estimator in both treatment arms, remedying the slower-than-n-convergence observedin the default synthetic datasets. The impact for the applied researcher can be better understood bylooking at the empirical coverage levels of the 95% CI for the true proportion of deaths in the aspirinarm, for all sample sizes and DGMs considered. Figure A13a illustrates that in contrast to the defaultsynthetic datasets, the coverage levels based on the debiased synthetic datasets are all positionedaround the nominal level. One of the original research questions in Sandercock et al. (2011) was whether or not there is adifference in risk of death between the treatment arms. Figure A13b depicts the empirical type 1error rate for the risk difference in death between aspirin and no aspirin group based on originaldata, default and debiased synthetic data. For the aforementioned reason, we focus on the resultsobtained by training with the default hyperparameters suggested by the package SDV (Patki et al.,2016). Should the researcher use the default synthetic data, they would very often falsely concludethat the risk is significantly different from the true difference of 0.009, as calculated based on ourpopulation (the full dataset), while using the debiased synthetic dataset basically eliminates this highnumber of false-positives, as is the case in the original data as well. Table A9: Estimated exponent a [95% CI] for the power law convergence rate na for empiricalSE. Note that a convergence rate could not be estimated for the default synthetic data when hyperpa-rameters suggested by the package Synthcity. This occurred because there was no variance in theestimates for sample sizes of 1600 and 5000.",
  "EstimatorOriginalCTGAN (SDV)CTGAN (Synthcity)TVAE (SDV)TVAE (Synthcity)": "Default synthetic datasetsProportion death aspirin group0.54 [0.50; 0.59]0.02 [-0.06; 0.11]NaN [NaN; NaN]0.23 [-0.17; 0.62]NaN [NaN; NaN]Proportion death no aspirin group0.54 [0.52; 0.57]0.02 [-0.15; 0.18]NaN [NaN; NaN]0.23 [-0.14; 0.61]NaN [NaN; NaN]Risk difference death0.54 [0.51; 0.56]0.01[-0.24; 0.26]NaN [NaN; NaN]0.46 [0.14; 0.78]NaN [NaN; NaN] Debiased synthetic datasetsProportion death aspirin group-0.54 [0.48; 0.61]0.59 [0.52; 0.67]0.53 [050; 0.56]0.60 [0.51; 0.70]Proportion death no aspirin group-0.52 [0.46; 0.59]0.58 [0.55; 0.62]0.53 [0.49; 0.58]0.58 [0.49; 0.67]Risk difference death-0.55 [0.48; 0.62 ]0.57 [0.53; 0.60]0.53 [0.50; 0.57]0.57 [0.53; 0.61] 0.2 0.4 0.6 0.8 Original data CTGAN (SDV): default CTGAN (SDV): debiased CTGAN (synthcity): default CTGAN (synthcity): debiased TVAE (SDV): default TVAE (SDV): debiased TVAE (synthcity): default TVAE (synthcity): debiased"
}