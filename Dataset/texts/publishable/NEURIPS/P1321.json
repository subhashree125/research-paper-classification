{
  "Abstract": "For embodied reinforcement learning (RL) agents interacting with the environment,it is desirable to have rapid policy adaptation to unseen visual observations, butachieving zero-shot adaptation capability is considered as a challenging problemin the RL context. To address the problem, we present a novel contrastive promptensemble (CONPE) framework which utilizes a pretrained vision-language modeland a set of visual prompts, thus enabling efficient policy learning and adaptationupon a wide range of environmental and physical changes encountered by embod-ied agents. Specifically, we devise a guided-attention-based ensemble approachwith multiple visual prompts on the vision-language model to construct robust staterepresentations. Each prompt is contrastively learned in terms of an individualdomain factor that significantly affects the agents egocentric perception and obser-vation. For a given task, the attention-based ensemble and policy are jointly learnedso that the resulting state representations not only generalize to various domainsbut are also optimized for learning the task. Through experiments, we show thatCONPE outperforms other state-of-the-art algorithms for several embodied agenttasks including navigation in AI2THOR, manipulation in egocentric-Metaworld,and autonomous driving in CARLA, while also improving the sample efficiency ofpolicy learning and adaptation.",
  "Introduction": "In the literature of vision-based reinforcement learning (RL), with the advance of unsupervisedtechniques and large-scale pretrained models for computer vision, the decoupled structure, in whichvisual encoders are separately trained and used later for policy learning, has gained popularity . This decoupling demonstrates high efficiency in low data regimes with sparse reward signals,compared to end-to-end RL. In this regard, several works on adopting the decoupled structure toembodied agents interacting with the environment were introduced , and specifically, pretrainedvision models (e.g., ResNet in ) or vision-language models (e.g., CLIP in ) were exploitedfor visual state representation encoders. Yet, it is non-trivial to achieve zero-shot adaptation to visualdomain changes in the environment with high diversity and non-stationarity, which are inherent forembodied agents. It was rarely investigated how to optimize those popular large-scale pretrainedmodels to ensure the zero-shot capability of embodied agents. Embodied agents have several environmental and physical properties, such as egocentric cameraposition, stride length, and illumination, which are domain factors making significant changes inagents perception and observation. In the target (deployment) environment with uncalibrated settingson those domain factors, RL policies relying on pretrained visual encoders remain vulnerable todomain changes.",
  ": Visual Domain Changes of Embodied Agents": "provides an example of egocen-tric visual domain changes experienced byembodied agents due to different camera po-sitions. When policies learned in the sourceenvironment are applied to the target en-vironment, zero-shot performance can besignificantly degraded, unless the visual en-coder could adapt not only to environmentaldifferences but also to the physical diversityof agents. In this paper, we investigate RL policy adaptation techniques for embodied agents toenable zero-shot adaptation to domain changes, by leveraging prompt-based learning for pretrainedmodels in the decoupled RL structure. To this end, we present CONPE, a novel contrastive promptensemble framework that uses the CLIP vision-language model as the visual encoder, and facilitatesdynamic adjustments of visual state representations against domain changes through an ensembleof contrastively learned visual prompts. In CONPE, the ensemble employs attention-based statecomposition on multiple visual embeddings from the same input observation, where each embeddingcorresponds to a state representation individually prompted for a specific domain factor. Specifically,the cosine similarity between an input observation and its respective prompted embeddings is used tocalculate attention weights effectively. Through experiments, we demonstrate the benefits of our approach. First, RL policies learned viaCONPE achieve competitive zero-shot performance upon a wide variety of egocentric visual domainvariations for several embodied agent tasks, such as navigation tasks in AI2THOR , vision-basedrobot manipulation tasks in egocentric-Metaworld, and autonomous driving tasks in CARLA .For instance, the policy via CONPE outperforms EmbCLIP in zero-shot performance by 20.7%for unseen target domains in the AI2THOR object navigation. Second, our approach achieves highsample-efficiency in the decoupled RL structure. For instance, CONPE requires less than 50.0%and 16.7% of the samples compared to ATC and 60% and 50% of the samples compared toEmbCLIP to achieve comparable performance in seen and unseen target domains in the AI2THORobject navigation. In the context of RL, our work is the first to explore policy adaptation using visual prompts forembodied agents, achieving superior zero-shot performance and high sample-efficiency. The maincontributions of our work are as follows.",
  "Problem Formulation": "In RL formulation, a learning environment is defined as a Markov decision process (MDP) of(S, A, P, R) with state space s S, action space a A, transition probability P : S A Sand reward function R : S A R. The objective of RL is to find an optimal policy : S A maximizing the sum of discounted rewards. For embodied agents, states might not be fullyobservable, and the environment is represented by a partially observable MDP (POMDP) of a tuple(S, A, P, R, , O) with an observation space o and a conditional observation probability O : S A . Given visual domains in the dynamic environment, we consider policy adaptation to find the optimalpolicy that remains invariant across the domains or is transferable to some target domain, where eachdomain is represented by a POMDP and domain changes are formulated by different O. We denotedomains as D = (, O). Aiming to enable zero-shot adaptation to various domains, we formulate",
  "where p(D) is a given domain distribution and is a discount factor of the environment": "For embodied agents, the same state can be differently observed depending on the configuration ofproperties such as egocentric camera position, stride length, illumination, and object style. We referto such a property causing domain changes in the environment as a domain factor. Practical scenariosoften involve the interplay of multiple domain factors in the environment.",
  "Framework Structure": "To enable zero-shot policy adaptation to unseen domains, we develop the CONPE framework consist-ing of (i) prompt-based contrastive learning with the CLIP visual encoder, (ii) guided-attention-basedprompt ensemble, and (iii) zero-shot policy deployment, as illustrated in . The capabilityof the CLIP visual encoder is enhanced using multiple visual prompts that are contrastively learnedon expert demonstrations for several domain factors. This establishes the visual prompt pool in(i). Then, the prompts are used to train the guided-attention-based ensemble with the environmentin (ii). To enhance learning efficiency and interpretability of attention weights, we use the cosinesimilarity of embeddings. The attention module and policy are jointly learned for a specific task sothat resulting state representations tend to generalize across various domains and be optimized for tasklearning. In deployment, a non-stationary environment where its visual domain varies according to theenvironment conditions and agent physical properties is considered, and the zero-shot performance isevaluated in (iii).",
  "pv = [ev1, ev2, ..., evu], evi Rd(2)": "where evi is a continuous learnable vector with the image patch embedding dimension d (e.g.,768 for CLIP visual encoder) and u is the length of a visual prompt. Let a pretrained model Tparameterized by maps observations o to the embedding space Z. With a contrast functionP : {0, 1} to discriminate whether an observation pair is positive or not,",
  "softmax": ": Guided-Attention-based Prompt Ensemble. The cosine similarity-guided attention moduleG yields task-specific state representations from multiple prompted embeddings and is learned with apolicy network . consider an m-sized batch of observation pairs BP = {(oi, oi)}im containing one positive pair{(ok, ok)|P(ok, ok) = 1} for some k m. Then, we enhance the capability of T by learning avisual prompt pv through contrastive learning, where the contrastive loss function is defined as",
  "Guided-Attention-based Prompt Ensemble": "To effectively integrate individual prompted embeddings from multiple visual prompts into a task-specific state representation, we devise a guided-attention-based prompt ensemble structure, as shownin where the attention weights on the embeddings are dynamically computed via the attentionmodule G for each observation. Given observation o and the learned visual prompt pool pv, an image embedding z0 = T(o) andprompted embeddings z = [z1 = T(o, pv1), ..., zn] are calculated. Then, zo and z are fed to theattention module G, where attention weights i for each prompted embedding zi are optimized. Sincedirectly computing the attention weights using z0 and z is prone to have an uninterpretable localoptima, we introduce a guidance score gi based on the cosine similarity between the input image andvisual prompted image embeddings in Z, i.e., gi =z0,zi z0zi. Given that larger gi signifies a strongerconformity of an observation to the domain factor relevant to the prompted embedding zi, we usegi to steer the attention weights, aiming to not only improve learning efficiency but also provideinterpretability. With guidance gi, we compute the attention weights i by",
  "i=1izi.(6)": "Algorithm 1 shows the procedures in CONPE, where the first half corresponds to prompt-basedcontrastive learning (in .2) and the other half corresponds to joint learning of a policy (Z)and the attention module G. As G is optimized by a given RL task objective in the source domains (inline 12), the resulting Z tends to be task-specific, while Z is also domain-invariant by the ensemble ofcontrastively learned visual prompts based on G with respect to the combinations of multiple domainfactors. The entire algorithm can be found in Appendix.",
  "Evaluation": "Experiments. We use AI2THOR , Metaworld , and CARLA environments, specificallyconfigured for embodied agent tasks with dynamic domain changes. These environments allow usto explore various domain factors such as camera settings, stride length, rotation degree, gravity,illuminations, wind speeds, and others. For prompt-based contrastive learning (in .2), we usea small dataset of expert demonstrations for each domain factor (i.e., 10 episodes per domain factor).For prompt ensemble-based policy learning (in .3), we use a few source domains randomlygenerated through combinatorial variations of the seen domain factors (i.e., 4 source domains). In ourzero-shot evaluations, we use target domains that can be categorized as either seen or unseen. Theseen target domains are those encountered during the phase of prompt-based contrastive learning,while these domains are not present during the phase of prompt ensemble-based policy learning. Onthe other hand, the unseen target domains refer to those that are entirely new, implying that they arenot encountered during either learning phases. Baselines. We implement several baselines for comparison. LUSR is a reconstruction-baseddomain adaptation method in RL, which uses the variational autoencoder structure for robust rep-resentations. CURL and ACT employ contrastive learning in RL frameworks for highsample-efficiency and generalization to visual domains. ACO utilizes augmentation-driven andbehavior-driven contrastive tasks in the context of RL. EmbCLIP is a state-of-the-art embodiedAI model, which exploits the pretrained CLIP visual encoder for visual state representations. Implementation. We implement CONPE using the CLIP model with ViT-B/32, similar to VPT and CoOp . In prompt-based contrastive learning, we adopt various contrastive learning schemesincluding augmentation-driven and behavior-driven contrastive learning,where the prompt length sets to be 8. In policy learning, we exploit online learning (i.e., PPO )for AI2THOR and imitation learning (i.e., DAGGER ) for egocentric-Metaworld and CARLA. : Zero-shot Performance. The policies of each method (CONPE and the baselines) are learnedon 4 source domains. The Source column presents the performance for those source domains. In allevaluations, we use 30 seen target domains and 10 unseen target domains. The Seen Target columnpresents the performance for the seen target domains, and the Unseen Target column presents theperformance for the unseen target domains. The unseen target domains are not used for representationlearning.",
  "Zero-shot Performance": "shows zero-shot performance of CONPE and the baselines across source, seen and unseentarget domains. We evaluate with 3 different seeds and report the average performance (i.e., tasksuccess rate in AI2THOR and egocentric-Metaworld, the sum of rewards in CARLA). As shownin (a), CONPE outperforms the baselines in the AI2THOR tasks. It particularly surpassesthe most competitive baseline, EmbCLIP, by achieving 5.2 5.7% higher success rate for seentarget domains, and 6.920.7% for unseen target domains. For egocentric-Metaworld, as shownin (b), CONPE demonstrates superior performance with a significant success rate for bothseen and unseen target domains, which is 17.3 24.0% and 18.0 20.0% higher than EmbCLIP,respectively. For autonomous driving in CARLA, we take into account external environment factors,such as weather conditions and times of day, as domain factors that can influence the driving task. In(c), CONPE consistently maintains competitive zero-shot performance across all conditions,outperforming the baselines. In these experiments, LUSR shows relatively low success rates, as the reconstruction-based represen-tation model can abate some task-specific information from observations, which is critical to conductvision-based complex RL tasks. EmbCLIP shows the most comparative performance among thebaselines, but its zero-shot performance for target domains is not comparable to CONPE. In contrast,",
  "(b) Success Rate in Unseen Target Domain": ": Sample-efficiency of Prompt Ensemble-based Policy Learning for Object Navigation inAI2THOR. The x-axis represents the number of samples (timesteps) used for policy learning, whilethe y-axis represents the task success rate for zero-shot evaluation. Sample Efficiency. presents performance with respect to samples (timesteps) that are usedby CONPE and baselines for policy learning. Compared to the most competitive baseline EmbCLIP,CONPE requires less than 60.0% timesteps (online samples) for seen target domains and 50.0% forunseen target domains to have comparable success rates.",
  "(b) Attention Weights": ": Prompt Ensemble Interpretability. In (a), the embeddings in the big circle are intra promptedembeddings obtained by varying domains within a domain factor, and the embeddings in the rectangleare inter prompted embeddings obtained by changing the visual prompts with aligned observation.The closely located intra prompted embeddings indicate the domain-invariant knowledge, while theinter prompted embeddings clustered by different visual prompts indicate the alignment between thevisual prompts and the domain factors. In (b), each cell represents attention weight i applied forprompted embedding zi. Prompt Ensemble Interpretability. (a) visualizes the prompted embeddings using theprompt pool obtained through CONPE. For intra prompted embeddings, we use observation pairs,where each pair is generated by varying domains within a domain factor. We observe that theembeddings are paired to form the domain-invariant knowledge because the visual prompt is learnedthrough prompt-based contrastive learning. The inter prompted embeddings specify that each promptdistinctly clusters prompted embeddings that correspond to the domains associated to its domainfactor. (b) shows examples of the attention weight matrix of CONPE for four differentdomains. The x-axis denotes the visual prompts and the y-axis denotes timesteps. This shows theconsistency of the attention weights on the prompts across the timesteps in the same domain.",
  "the pretrained policy so that prompted embedding z0 with the task-relevant features is incorporatedinto the guided-attention-based ensemble, i.e., (G(z0, z)), where z0 = T(o, pvpol)": ": Prompt Ensemble with a Pretrained Policy. The pretrained policies of each task are learnedon 4 source domains. The Source column presents the performance for those source domains. In allevaluations, we use 40 unseen target domains. The Target column presents the performance for theunseen target domains not used for policy training.",
  "Pretrained100.00.065.76.4100.00.058.05.8100.00.016.82.3100.00.035.66.2CONPE100.00.074.75.0100.00.075.79.0100.00.073.78.3100.00.093.21.1": "reports zero-shot performance for the scenarios when a pretrained policy is given. We evaluatetwo different cases: aligned (Aln.) when prompt-based contrastive learning is conducted on datafrom the same task of a pretrained policy; otherwise, not aligned (Not Aln.). In AI2THOR, weuse data from the object goal navigation task for prompt-based contrastive learning, while eachpretrained policy is learned individually through one of tasks including object goal navigation, pointgoal navigation, image goal navigation, and room rearrangement. Similarly, in egocentric-Metaworld,we use data from the reach task for prompt-based contrastive learning, while each pretrained policy islearned individually through one of tasks including reach, reach-wall, button-press, and door-open.In (a), CONPE enhances zero-shot performance of the pretrained policies by 3.57.0% forunseen target domains in AI2THOR. This prompt ensemble adaptation requires only 400K samples,equivalent to 10% of the total samples used for policy learning. In (b), CONPE significantlyboosts zero-shot performance of the pretrained policies by 9.057.6% in egocentric-Metaworld.",
  "COM-UNI-AVG52.912.251.47.643.112.7COM-WEI-AVG63.611.242.35.250.36.1ENS-UNI-AVG88.61.479.83.465.55.0ENS-WEI-AVG94.16.375.53.561.212.7CONPE96.31.083.30.379.76.4": "Prompt Ensemble Scalability. evaluates CONPE with respect to the number of prompts(n). CONPE effectively enhances zero-shot performance for both seen and unseen target domainsthrough prompt ensemble that captures various domain factors. Compared to the case of n = 2, forn = 10, there was a significant improvement in zero-shot performance for both seen and unseentarget domains, with increases of 42.8% and 36.7%, respectively. For n 10, we observe stableperformance that specifies that CONPE can scale for combining multiple prompts to some extent. Prompt Ensemble Methods. Tabel 4 compares the performance of various prompt integrationmethods including our guided attention-based prompt ensemble. We denote prompt-levelintegration as COM, and prompted embeddings-level integration as ENS. UNI-AVG and WEI-AVGrefer to uniform average and weighted average mechanisms, respectively. CONPE achieves superiorsuccess rates over the most competitive ensemble method ENS-UNI-AVG, showing 3.5% and 14.2%performance gain for seen and unseen target domains.",
  "Related Work": "Adaptation in Embodied AI. In the literature of robotics, numerous studies focused on developinggeneralized visual encoders for robotic agents across various domains , exploiting pretrainedvisual encoders , and establishing robust control policies with domain randomized tech-niques . Furthermore, in the field of learning embodied agents, a few works addressedadaptation issues of agents to unseen scenarios in complex environments, using data augmentationtechniques or adopting self-supervised learning schemes . Recently,several works showed the feasibility and benefits of adopting large-scale pretrained vision-languagemodels for embodied agents . Our work is in the same vein of these prior works ofembodied agents, but unlike them, we explore visual prompt learning and ensembles, aiming toenhance both zero-shot performance and sample-efficiency. Decoupled RL Structure. The decoupled structure, where a state representation model is separatedfrom RL, has been investigated in vision-based RL . Recently, contrastive representationlearning on expert trajectories gains much interest, as it allows expert behavior patterns to beincorporated into the state encoder even when a policy is not jointly learned . They establishedgeneralized state representations, yet in that direction, sample-efficiency issues in both representationlearning and policy learning remain unexplored. Prompt-based Learning. Prompt-based learning or prompt tuning is a parameter-efficient opti-mization method for large pretrained models. Prompt tuning was used for computer vision tasks,optimizing a few learnable vectors in the text encoder , and it was also adopted for visiontransformer models to handle a wide range of downstream tasks . Recently, visual prompting was introduced, and both visual and text prompt tuning were explored together in the multi-modalembedding space . We also use visual prompt tuning, but we concentrate on the ensembleof multiple prompts to tackle complex embodied RL tasks. We take advantage of the fact that thegeneralized representation capability of different prompts can vary depending on a given task anddomain, and thus we strategically utilize them to enable zero-shot adaptation of RL policies.",
  "Conclusion": "Limitation. Our CONPE framework exploits visual inputs and their relevant domain factors forpolicy adaptation. For environments where domain changes extend beyond those domain factors, theadaptability of the framework might be constrained. In our future work, we will adapt the frameworkwith semantic knowledge based on pretrained language models to improve the policy generalizationcapability for embodied agents in dynamic complex environments and to cover various scenariosassociated with multi-modal agent interfaces. Conclusion. In this work, we presented the CONPE framework, a novel approach that allowsembodied RL agents to adapt in a zero-shot manner across diverse visual domains, exploring theensemble structure that incorporates multiple contrastive visual prompts. The ensemble facilitatesdomain-invariant and task-specific state representations, thus enabling the agents to generalize to vi-sual variations influenced by specific domain factors. Through various experiments, we demonstratedthat the framework can enhance policy adaptation across various domains for vision-based objectnavigation, rearrangement, manipulation tasks as well as autonomous driving tasks.",
  "Acknowledgement": "We would like to thank anonymous reviewers for their valuable comments and suggestions. This workwas supported by Institute of Information & communications Technology Planning & Evaluation(IITP) grant funded by the Korea government (MSIT) (No. 2022-0-01045, 2022-0-00043, 2020-0-01821, 2019-0-00421) and by the National Research Foundation of Korea (NRF) grant funded by theMSIT (No. NRF-2020M3C1C2A01080819, RS-2023-00213118).",
  "Ashish Vaswani et al. Attention is all you need. In: Advances in neural information process-ing systems (2017)": "Denis Yarats, Ilya Kostrikov, and Rob Fergus. Image Augmentation Is All You Need: Regu-larizing deep reinforcement learning from pixels. In: Proceedings of the 9th InternationalConference on Learning Representations. 2021. Samarth Sinha, Ajay Mandlekar, and Animesh Garg. S4RL: Surprisingly Simple Self-Supervision for Offline Reinforcement Learning in Robotics. In: Proceedings of the 5thConference on Robotics Learning. 2021, pp. 907917.",
  "Chong Liu et al. Vision-language navigation with random environmental mixup. In: Pro-ceedings of the International Conference on Computer Vision. 2021, pp. 16241634": "Jialu Li, Hao Tan, and Mohit Bansal. EnvEdit: Environment Editing for Vision-and-LanguageNavigation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition. 2022, pp. 1538615396. Xin Wang et al. Reinforced cross-modal matching and self-supervised imitation learningfor vision-language navigation. In: Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition. 2019, pp. 66296638.",
  "When conducting prompt-based contrastive learning, as shown in and explained below, wespecifically adopt several methods to generate positive visual observation pairs for different domainfactors": "Consider a pretrained model T parameterized by that maps observations o to the embeddingspace Z. The contrast function P : {0, 1} discriminates whether an observation pair ispositive or not. Then, we fine-tune T by learning a visual prompt pv through contrastive learning,where the contrastive loss function is defined as Equation (2) in the main manuscript.",
  ": Prompt-based Contrastive Learning withDifferent Contrastive Tasks": "Behavior-driven Contrast. Similar to ,we exploit expert actions to obtain positivesample pairs from expert trajectories of dif-ferent domains. With observation and ac-tion pairs (o, a), (o, a), a behavior-drivencontrast function is defined as Pbeh(o, o) =1a=a. If the environment has a discrete ac-tion space, the behavior-driven contrast canbe applied immediately to obtain positivesample pairs; otherwise, it can be appliedafter discretizing continuous actions withunsupervised clustering algorithms such ask-means clustering . Augmentation-driven Contrast.Simi-lar to visual domain randomization tech-niques , we use data augmentation(e.g., color perturbation ) for unstruc-tured pixel-level visual domain factors suchas illumination. An augmentation-drivencontrast function is defined as Paug(o, o) =1o=AUG(o), where AUG augments o. Timestep-driven Contrast. Similar to ,we exploit timesteps of expert trajectory toobtain positive sample pairs across differentdomains. With observation and timestep pairs (o, t), (o, t), a timestep-driven contrast function isdefined as Ptim(o, o) = 1t=t, where t k t t + k and k is hyperparameter.",
  "BPrompt Ensemble with a Pretrained Policy": "As mentioned in the main manuscript, we devise an optimization method to update G specificallyfor a pretrained policy . Specifically, we use a policy prompt pvpol that focuses on task-relevantfeatures from observations for . By incorporating the prompted embedding z0, which contains thesetask-relevant features, into the guided-attention-based ensemble, we can effectively integrate thepolicy with the attention module, resulting in (G(z0, z)). Here, z0 is obtained by applying thetransformation to the observation o using the policy prompt pvpol. As such, CONPE enables efficient adaptation of the attention module to a pretrained policy by fine-tuning only a small number of parameters. This achieves robust zero-shot performance for unseendomains in different tasks. Algorithm 2 shows the procedures of CONPE to adapt the attention module for a pretrained policy.The first half corresponds to prompt-based contrastive learning and the other half corresponds tolearning of the attention module G with a pretrained policy (Z). This is slightly extended fromthe algorithm in the main manuscript, where joint learning of the attention module and a policy isexplained.",
  "pol": ": Guided-Attention-based Prompt Ensemble. The cosine similarity-guided attention moduleG generates task-specific state representations by combining multiple prompted embeddings, and it islearned with a policy network . The left part of the figure illustrates prompt ensemble adaptation toa pretrained policy, while the right part shows the semantic regularized data augmentation scheme. Algorithm 2 Procedure in CONPE with a Pretrained PolicyDataset D = {(o1, o1), ...}, replay buffer ZD , pretrained vision-language model TVisual prompt pool pv = [pv1, ..., pvn], attention module GPretrained policy",
  "CSemantic Regularized Data Augmentation": "For a source environment that is sufficiently accessible, the attention module and policy networkcan be jointly trained by RL algorithms. In this case, to address overfitting problems to the sourceenvironment, data augmentation methods can be adopted. For example, when training a policy, it isfeasible to add Gaussian noise to each prompted embedding as part of data augmentation to avoidoverfitting . To enhance both policy optimization and zero-shot performance, we investigate semantic regular-ization schemes in the CLIP embedding space, which are specific to the prompt ensemble-basedpolicy learning. Specifically, using a few object-level descriptions in datasets, we control the noiseeffectively. In our semantic regularization, the language prompt pli = [el1, el2, ..., elu], eli Rd is pretrained withdescription data and fixed pvi . Then, pli is used as a semantic regularizer, where eli is a continuouslearnable vector of the word embedding dimension d (e.g., 512 for CLIP language encoder) and u isthe length of a language prompt. Similar to , we adopt language prompt learning schemes. We",
  "This tends to achieve more generalized representations for a specific domain that is relevant to theobject-level descriptions, while maintaining semantic information in the representations": "Algorithm 3 shows the procedure of CONPE with semantic regularized data augmentation. Thisalgorithm includes three steps: the first step corresponds to prompt-based contrastive learning (sameas the algorithm in the main manuscript), the second step corresponds to language prompt learning(addition for this algorithm), and the third step corresponds to the modified process of joint learningfor policy (Z) and the attention module G with semantic regularized data augmentation. Algorithm 3 Procedure of CONPE with Semantic Regularized Data AugmentationDataset D = {(o1, o1, m), ...}, replay buffer ZD , pretrained vision-language model TVisual prompt pool pv = [pv1, ..., pvn], Language prompts pl1, ..., plnAttention module G, policy",
  "D.1AI2THOR": "Environment settings. We use AI2THOR , a large-scale interactive simulation platform forEmbodied AI. In AI2THOR, we use iTHOR datasets that have 120 room-sized scenes with bedrooms,bathrooms, kitchens, and living rooms. iTHOR includes over 2000 unique objects based on Unity 3D.Among embodied AI tasks in AI2THOR, we evaluate our framework with object goal navigation andpoint goal navigation tasks. We also test the image goal navigation task, a modified version of theobject goal navigation, as well as the room rearrangement task for adaptation to a pretrained policy.",
  "PickUp [Object Type]Open [Object Type]PlaceObject": "Object Goal Navigation. The object navigation task requires an agent to navigate through itsenvironment and find an object of a given category (e.g., apple). The agent is initially placed at arandom location in a near-photorealistic home, and it receives an egocentric viewpoint image andone target object instruction for each timestep. The agent uses several navigation actions such asMoveAhead and RotateRight to complete the task. Point Goal Navigation. In the point goal navigation task, an agent is given a specific coordinate inthe environment as its target goal. Similar to the object goal navigation task, the agent receives anegocentric viewpoint image and a target coordinate for each timestep. Image Goal Navigation. In the image goal navigation task, an agent is provided with a target imagethat represents a desired scene configuration. The agents goal is to navigate through the environmentand reach a location where the observed scene matches the target image. This task involves usingvisual perception to compare the current scene to the target image and selects appropriate navigationactions to achieve the desired scene configuration. The agent receives egocentric viewpoint imagesand target image for each timestep. The task is considered successful when the agent reaches aspecific position where the observed scene closely resembles the target image. Room Rearrangement. In the room Rearrangement task, the goal of an agent is to reach the goalconfiguration by interacting with the environment. At each timestep, the images of both the currentstate and goal state are given, and the agent uses navigation actions and higher-level semantic actions(e.g., PickUp CUP) to rearrange the objects and recover the goal room configuration.",
  "Our experiment settings. We adopt the similar configuration in for our experiments, while somesettings are modified to evaluate zero-shot adaptation for different domains": "We use FloorPlan21 as our default environment. For zero-shot adaptation scenarios, 75 differentdomains are randomly generated with several predefined domain factors (i.e., camera field of views,stride length, rotation degrees, look degrees and illuminations). These factors, previously examined inembodied RL studies , can be characterized by either discrete or continuous values basedon their intrinsic properties. For instance, we treat rotation degrees as a discrete factor, determinedbased on the feasibility of task success; conversely, brightness is treated as a continuous factor, with arange extending from 0.0 to 1.0. Each of these domain factors is randomly selected from a uniformdistribution, leading to a combination of varied domains. Using the domain factors, we define severalseen domains that can be used for representation and policy learning as well as unseen domains forevaluating the zero-shot adaptation performance. Datasets. Using rule-based policies, we create expert datasets for contrastive representation learning.The datasets comprise 28,464 samples for AI2THOR. illustrates the examples of the expertdatasets in which the experts of each domain reflect external differences amongst domains andphysical differences of agents. SPL is the evaluation metric, success weighted by (normalizedinverse) path length . LENGTH is the average episode length of entire trajectories.",
  "D.2Egocentric-Metaworld": "Environment settings. The Metaworld benchmark includes diverse table-top manipulationtasks that require a Sawyer robot to interact with various objects. With different objects, such as doorand button, the robot needs to manipulate them based on the objects affordance, leading to differentreward functions. At each timestep, the Sawyer robot conducts a 4-dimensional fine-grained actionthat determines the 3D position movements of the end-effector and the variation of gripper openness. For embodied AI settings, we slightly modify Metaworld to have egocentric images as observations.We use several tasks such as reach-v2, reach-wall-v2, button-press-topdown-v2 and door-open-v2 forour experiments.",
  "Reach. In the Reach task, the objective is to control a Sawyer robots end-effector to reach a targetposition. The agent directly controls the XYZ location of the end-effector": "Reach-Wall. In the Reach-Wall task, the agent controls the Sawyer robots end-effector to reach atarget position in the presence of obstacles such as walls. The agent needs to plan and navigate a paththat avoids collisions to the walls while reaching the target. Button-Press. In the Button-Press task, the agent is required to accurately guide the Sawyer robotsend-effector to a designated button and press it. This task involves precise control and coordinationto successfully interact with the button.",
  "Door-Open. In the Door-Open task, the agents objective is to manipulate a Sawyer robots end-effector to open a door. The agent needs to grasp and manipulate the door handle to open thedoor": "Our experiment settings. For zero-shot adaptation scenarios, 70 different domains are randomlygenerated with predefined domain factors such as camera positions, gravity, wind speeds, and illumi-nations. Each domain factor can be represented as either discrete or continuous values, depending onits inherent nature. Regarding sampling methods, these domain factors are individually drawn from auniform distribution to produce combinatorial domain variations. Datasets. For predefined seen domains, we implement a rule-based expert policy to collect experttrajectory data. The datasets comprise 3,840 samples for Metaworld. illustrates a fewexamples of our expert dataset where experts of each domain reflect external differences amongdomains and physical differences in agents.",
  "D.3CARLA": "Environment settings. CARLA is a self-driving simulation environment where an agentnavigates to the target location while avoiding collisions and lane crossings. For experiments, weuse the CARLA simulator v0.9.13 and choose Town10HD as our map. For RL formulation, weincorporate the RGB image data and sensor values (acceleration, velocity, angular velocity) intostates, and use control steer, throttle, and brake as actions. Each action ranges from -1 to 1. Theactions are automatically calibrated when the speed of the car reaches the maximum. The agent isevaluated based on the reward function below that involves the desired velocity and goal distance,",
  "Maximum speed20km/h": "Our experiment settings. To implement 50 different domains, we use camera positions, camera fieldof views, weather conditions, times of day, and different ranges of action magnitude as domain factors.Each domain factor can be represented as either discrete or continuous values, depending on itsinherent nature. For example, we treat weather conditions as a discrete factor, which can be classifiedas either clear, rainy, cloudy, or other. Through these factors, we define several seen domains that canbe used for representation and policy learning as well as unseen domains for evaluating the zero-shotadaptation performance. For prompt-based contrastive learning, the training dataset consists of asingle trajectory for each of 50 domains. In policy learning, we utilize 4 source domains across 2different maps. Datasets. For predefined seen domains, we implement a rule-based expert policy to collect experttrajectory data. The datasets comprise 7,394 samples for CARLA. The detailed information of theexpert dataset is explained in .",
  "E.1LUSR": "LUSR is a domain adaptation method that utilizes the latent embedding of encoder-decoder models toextract generalized representations. LUSR uses -VAE to learn disentangled representations of differ-ent visual domains. For implementation, we use the open source ( implementing LUSR, we use a CNN encoder for both DAE and -VAE. We conduct onlinepolicy learning with PPO algorithms . The hyperparameter settings are summarized in .",
  "CURL and ATC are a contrastive learning based framework for visual RL. CURL uses con-trastive representation learning to extract discriminative features from raw pixels which greatly": "enhance sample efficiency in RL training. ATC enables the training of an encoder to associatepairs of observations separated by short time difference, leading to RL performance enhance-ment. For implementation, we use the open source ( ( The hyperparameter settings of CURLand ATC are summarized in and 11, respectively, where the other settings are the same asin (b).",
  "E.4EmbCLIP": "EmbCLIP is a state-of-the-art model for embodied AI tasks.By using CLIP as the visualencoder, EmbCLIP extracts generalized representation which is useful for an embodied agent,enabling the agent to effectively generalize to different environments and tasks.We use theopen source ( for the implementation of AI2-THORenvironments.To evaluate this with the CARLA simulator, we also use the open source( The configurations for policy learning are the same as in Ta-ble 9(b).",
  "The procedure of our CONPE consists of prompt learning and policy learning steps": "Prompt-based Contrastive Learning. In prompt learning step, for sample-efficiency, CONPEconducts prompt-based contrastive learning, exploiting the pretrained CLIP model. We set the lengthof visual prompts to be 8 for each contrastive learning with Equation (2) in the main manuscript. Incases when metadata is available (the cases of using the semantic regularized data augmentation), 8language prompts are used for (7). The hyperparameter settings are summarized in .",
  "F.1Zero-shot Performance for Seen Domains Factors": "presents the zero-shot performance of CONPE across specific domain factors (DF). Forexample, DF0 refers to various domains where the camera position is a domain factor of interest,and LUSRs 48.5 in DF0 indicates the success rate of LUSR for the domains generated by differentcamera positions. As shown, CONPE maintains robust zero-shot performance for all the cases (DF0 DF9), comparedto the baselines. Additionally, as shown in where a specific domain factor is changed, theattention weights assigned to the prompted embedding (denoted as Pn, where n = {0..9}) that istrained for the corresponding domain factor are high (in the bright color).",
  "F.2Prompt Ensemble with a Pretrained Policy": "reports detailed zero-shot performance for the scenarios when a pretrained policy is given.We additionally test widely used baselines that leverage large pretrained models in the fields ofvision and natural language, specifically in the context of prompt-meta learning. ATTEMPT is aparameter-efficient multi-task language model tuning method that transfers knowledge across differenttasks via a mixture of soft prompts. SESoM is a soft prompts ensemble method that leveragesmultiple source tasks and effectively improves few-shot performance of prompt tuning by transferringknowledge from the source tasks to a target task. CONPE demonstrates the ability to effectivelyimprove zero-shot performance with a small number of samples, especially when a pretrained policyis given. As shown in (a) and (b), prompt ensemble adaptation demonstrates an increasein success rate with a small number of samples in both the train and unseen environments of thepretrained policy, compared to other baselines. This results present the sample-efficiency of ourprompt ensemble adaptation method.",
  "F.3Semantic Regularized Data Augmentation": "shows the zero-shot performance for semantic regularized data augmentation in CONPE,where language data is additionally used. As shown, CONPE with language data (w Semantic Reg.)consistently yields better performance over CONPE without language data (w/o Semantic Reg.)across various noise scales (). As the deviation () of the Gaussian noise varies, it is observedthat larger deviation does not necessarily lead to performance improvement. This indicates that 0.00.20.40.60.81.0 Timesteps (M) Success Rate (%) EmbCLIPATCConPE",
  "(b) Success Rate of Unseen Environment": ": Sample-efficiency of Prompt Ensemble with a Pretrained Policy. The pretrained policy islearned on the Object Goal Navigation task and then adapted to the Point Goal Navigation task withpolicy prompt. enhancing data augmentation diversity through higher noise deviation may not always be beneficial.However, when maintaining the semantics (w Semantic Reg.), the performance can improve withlarger deviation."
}