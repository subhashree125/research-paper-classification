{
  "ABSTRACT": "An unintended consequence of the vast pretraining of Large Language Models(LLMs) is the verbatim memorization of fragments of their training data, whichmay contain sensitive or copyrighted information. In recent years, unlearning hasemerged as a solution to effectively remove sensitive knowledge from models af-ter training. Yet, recent work has shown that supposedly deleted information canstill be extracted by malicious actors through various attacks. Still, current at-tacks retrieve sets of possible candidate generations and are unable to pinpointthe output that contains the actual target information.We propose activationsteering as a method for exact information retrieval from unlearned LLMs. Weintroduce a novel approach to generating steering vectors, named AnonymizedActivation Steering. Additionally, we develop a simple word frequency methodto pinpoint the correct answer among a set of candidates when retrieving un-learned information. Our evaluation across multiple unlearning techniques anddatasets demonstrates that activation steering successfully recovers general knowl-edge (e.g., widely known fictional characters) while revealing limitations in re-trieving specific information (e.g., details about non-public individuals). Overall,our results demonstrate that exact information retrieval from unlearned models ispossible, highlighting a severe vulnerability of current unlearning techniques.",
  "INTRODUCTION": "Large language Models (LLMs) are trained on vast amounts of text data curated from diversesources. The extensive training process enables LLMs to generate high-quality responses to a widerange of topics. However, an unintended consequence of this approach is the verbatim memorizationof fragments of their training data, which may contain sensitive information. Here, the scale of thetraining data prevents the reliable identification and removal of all instances of private or copyrightedinformation, such as personal addresses, passages from copyrighted books, or proprietary code snip-pets. Regulations, such as the EUs General Data Protection Regulation (GDPR) (Voigt & Von demBussche, 2017), aim to give individuals control over their personal information through measures,such as the Right to erasure, requiring companies to delete user data upon request (Zhang et al.,2023a). However, as retraining large models from scratch is impractical to delete user information,unlearning has emerged as an alternative solution to effectively remove knowledge from modelswhile preserving their overall performance (Maini et al., 2024). Evaluating the success of an unlearning method is a difficult task. Even if the model does not directlyrespond correctly to questions about the unlearned topic, it doesnt necessarily mean the concept isfully forgotten. Several existing approaches aim to recover unlearned or deleted information (Lynchet al., 2024). Existing works consider an attack successful if the correct answer is present amongthe candidates but are unable to pinpoint the answer containing the correct information (Patil et al.,2024; Schwinn et al., 2024). In contrast, we argue that for real-world applications, informationretrieval systems should incorporate scoring mechanisms that account for the accuracy of selectingthe correct answer, which results in a more accurate assessment of leakage risk during deployment.",
  "arXiv:2411.02631v1 [cs.CL] 4 Nov 2024": "We introduce a novel attack based on activation steering (see 2) against LLM unlearning.To this end, we deploy a novel way of generating pairs of prompts to calculate activationdifferences Anonymized Activation Steering. We evaluate our approach on three unlearning methods and three different datasets. Con-trary to existing attacks, we demonstrate that our proposed approach can retrieve unlearnedinformation in an exact manner pinpointing the correct answer among a set of candidateswith high accuracy. We investigate failure scenarios for our approach and find that while exact informationretrieval is successful on general knowledge (i.e., models unlearned on Harry Potter) itfails for specific, less known information.",
  "We provide a new dataset based on existing work (Schwinn et al., 2024) for Harry Potterinformation retrieval, enabling more accurate assessments of unlearning methods": "In this work, we demonstrate the power of activation steering as the evaluation tool for targetedunlearning of LLMs. Our results highlight its effectiveness for broad topics, such as removingcopyright-related information, while revealing its limitations when applied to more specific knowl-edge. By exploring these strengths and constraints, we provide deeper insights into how activationsteering affects the behavior of unlearned LLMs, contributing to the development of safe LLMs.",
  "RELATED WORK": "LLM unlearning is a popular research topic due to its efficiency compared to retraining from scratch.Eldan & Russinovich (2023) make a model forget the entire Harry Potter franchise, while Li et al.(2024) unlearn harmful information in biology, chemistry, and cyber-security domains. Apart fromunlearning entire domains of information, methods exist that solve the privacy issue of LLMs, delet-ing personal information at request (Jang et al., 2023; Wu et al., 2023). Meng et al. (2022) replacemore specific information from an LLM, unlearning a single sentence at a time. To effectively test unlearning methods, a clear benchmark is essential. In this context, Maini et al.(2024) introduce the TOFU dataset, which consists of synthetic data about fictitious authors. Be-cause the data is artificial, it allows for direct comparison between a model trained on this datasetthat has undergone unlearning, against a baseline model, that is guaranteed to never have seen thisdataset during training. Similarly, Li et al. (2024) provide the WMDP dataset, designed specifi-cally for benchmarking unlearning techniques. This dataset contains harmful information, enablingevaluation of a models ability to effectively forget potentially dangerous content. Similar to retrieving information from an unlearned model, researchers perform attacks on standardmodels that refuse to answer harmful prompts. A lot of research in this area is in a jailbreaksetting where the model already knows the information but refuses to answer (Chu et al., 2024).Shi et al. (2024) identify if a given data was seen during training to recover private information.Attacking an unlearned model is similar, as the goal is to recover information, but in this case, themodel does not refuse to answer but outputs seemingly random answers or replaced informationsuch as in unlearning done with ROME method by Meng et al. (2022). Patil et al. (2024) show thatROME and other unlearning methods such as MEMIT (Meng et al., 2023) are prone to attacks, andthe information is not truly deleted. Activation steering is a technique for manipulating LLMs latent space and guiding the generatedresponses in the desired direction. This method is applied to overcome refused prompts (Arditi et al.,2024; Rimsky et al., 2024), reduce toxicity (Turner et al., 2023), enhance truthfulness (Li et al.,2023a), and adjust the tone of responses (von Rutte et al., 2024) in LLMs. It works by calculatinga steering vector that reflects the target direction, derived from pairs of inputs that differ based onthe intended direction of behavior (e.g., toxic and non-toxic pairs). The steering vector is obtainedthrough simple subtraction or more advanced techniques like Principal Component Analysis (PCA)or Logistic Regression (Tigges et al., 2023). This vector is then used during generation to steer themodels output towards or away from the desired direction.",
  "PROBLEM DESCRIPTION": "We aim to extract information from an unlearned model about the unlearning topic by asking ques-tions. We ask straightforward questions that have specific keywords as their correct answers. Ourobjective is to develop a method that functions without prior knowledge of the unlearning topic(such as finetuning the model on a particular area and querying about others) and reliably deter-mines whether the models response is accurate. For the base, non-unlearned model, this task of determining the correct answer is trivial, as the mostfrequent response is typically correct if the model possesses the necessary knowledge in the firstplace. In contrast, for the unlearned model, information leakage occasionally makes the correctanswer appear among the sampled responses, though at a much lower frequency. If the correctanswer frequency (CAF) is increased to become the most common response, the user can simplychoose the most likely answer, effectively undoing the unlearning process. For this purpose ofincreasing the CAF, we propose Anonymized Activation (AnonAct) Steering.",
  "ANONACT": "We employ a standard activation steering scheme from literature (Arditi et al., 2024; Rimsky et al.,2024) with two distinct types of prompts. Our novel contribution to generating these pairs of promptsis the anonymization of sentences. For a given question Q, we generate multiple anonymized versions. We do it manually for simpleentities like character first names and using a large language model, GPT-4, to anonymize morecomplex terms, like places or institutions. shows an example of this anonymization strategy. The goal of anonymization is to create prompts close to the original question but without any relationto the unlearned domain. This way, the differences between their activations give us the direction ofthe unlearned domain. Our method is the first to suggest this type of generalized anonymization tocreate the difference vectors. Rimsky et al. (2024) use contrastive pairs (such as Yes/No), and Arditiet al. (2024) use completely different prompts to generate the steering vectors. Who is the ghost of Gryffindor house in the Harry Potter series? Who is the ghost of Braveheart house in the Caleb Mitchell series? with domain knowledge with domain knowledge without domain knowledge without domain knowledge",
  ": An example anonymization of a question": "After creating the anonymized questions, we follow established methods in the literature for activa-tion steering mainly from Arditi et al. (2024). Specifically, we extract the internal representationsbetween layers of the LLM for each anonymized question, as illustrated in . We then com-pute the difference between these representations and those of the corresponding original questions.By averaging these differences across all anonymized questions, we obtain the steering vector forthat layer. During generation, this steering vector is added back with a scaling factor, but only for thegeneration of the first token. We limit the application of the steering vector to the first token becausethe internal model representations are captured at that stage. This approach is motivated by findingsfrom previous research, which show that influencing the generation of the first token significantlyimpacts the entire sequence (Zhang et al., 2023b).",
  "MODELS": "For the initial experiments, we use the WhoIsHarryPotter (Eldan & Russinovich, 2023) model,which unlearns all Harry Potter-specific knowledge using sources related to the Harry Potter fran-chise (books, articles, news posts). It is based on the Llama-2-Chat model, which is a fine-tunedLlama2 (Touvron et al., 2023) for dialog use cases. Furthermore, to evaluate our approach on other unlearning methods, we use the base model (thatwas finetuned on the dataset) and codebase authors of TOFU provide to unlearn information from amodel. Their model was based on Phi-1.5 (Li et al., 2023b). For ROME (Meng et al., 2022), we useGPT2-XL (Radford et al., 2019) and the code the authors provide to unlearn single facts one by one.",
  "DATASETS": "For the Harry Potter unlearning experiments, we curate a dataset of 62 questions using GPT4 bybuilding upon an existing dataset by Schwinn et al. (2024). The dataset is in the format of a Q&Awith simple What or Who questions. We specifically choose questions that are easily answeredby a base model (Llama2 7B) to ensure that unlearning is the reason for incorrect answers and notthe lack of knowledge in the base model. For the TOFU dataset (Maini et al., 2024), we utilize 40 questions provided by the authors, focusingon two fictitious authors. These questions are originally designed with open-ended responses. Toensure consistency with our other experiments, we transform this dataset into a keyword-based Q&Aformat, similar to the Harry Potter dataset, by extracting key terms from the original answers. Whilethe original study uses ROUGE-L score (Lin & Och, 2004) to evaluate answer correctness, we usethe extracted keywords to denote an answer as correct.",
  "EXPERIMENT SETTING": "For the experiments, we follow the work by Arditi et al. (2024) but apply it in an unlearning settinginstead of refusal of models. We create the input text using the prompt templates, system prompts,questions, and answer starts, such that the first token to be selected by the model should be thefirst token of the correct keyword. We compute the internal representations (activations) betweenlayers during the generation of this first token. Then, we calculate the mean activations for theanonymized prompts we generated. Finally, we subtract these mean activations from the activationsfor the original text to generate the steering vectors (see ). We add the steering vectors back during sampling at the generation of the first token only. We usea Temperature value of 2 and Top K value of 40 to generate diverse answers. We sample 2000answers for each question and for each answer we stop generation at 10 tokens for the Harry Potterand ROME datasets and 50 for the TOFU dataset. To find the best setting, we ablate different parameters. We use different coefficients for addingthe steering vector during sampling, apply our method to different layers (and multiple layers), andfinally, use two strategies for generating the steering vector. The first one is local calculated as",
  "n=1Al(Qn)(1)": "where Sl is the steering vector for the layer l, Al the activation, Q the given question, and Qn oneanonymized version of Q among N anonymized samples. The second setting is called global, wherewe take the mean over all the questions of the local steering vectors and use this mean steering vectorfor all questions during generation. The global setting requires a dataset of questions and thus is noteasily applicable to real-world settings. After sampling answers using the calculated steering vector, we observe the CAF among the answersgenerated by the unlearned model without and with our method. We define a correct answer as ananswer with data leakage. That is, it must include a keyword that shows that unlearned informationis leaked. For example, as the answer for Who is Harry Potters best male friend? we acceptRon. and Ron Weasley. both as correct, since they both represent data leakage. To better quantify our results, we calculate the frequencies of words (excluding stop words, which iscommon practice) and set the probability of an answer being correct to be the maximum frequencyvalue among the words in that answer. We then plot the RoC curve and calculate the AUC scorebased on these probabilities. An ideal model that only generates correct answers would have anAUC score of 1. If our method pushes the correct keywords to be the most frequent ones, this wouldalso result in a score of 1. This scoring gives us a better indication of performance than just usingaccuracy or checking if the correct keyword is in a sample of N candidates.",
  "RESULTS & DISCUSSION": "We conduct an initial sampling experiment using the Harry Potter dataset, applying a coefficient of 2and implementing our method just before the models final layer. We apply the local AnonAct Steer-ing as it more closely aligns with real-world applications. We present the results in a. Weobserve an increase in the CAFs for many questions, with some showing substantial improvements.However, we also acknowledge a slight decrease in performance for a small subset of questions. Wenote that our objective is to increase the CAFs to the point where they become the most common.For example, a keyword with a low frequency is still the most frequent if no other candidate keywordappears more often. Thus, just looking at the increase in CAFs is insufficient. To better quantify the success of our method, we employ the simple most frequent keywordsapproach with RoC plots detailed above. We run this experiment in three settings: Base model,unlearned model, and unlearned model using our method. b shows the RoC curves for thesethree settings. We observe that LLAMA2 gets an almost perfect score (0.98). At the same time,the unlearned model has a score of 0.75, which is better than random, showing that informationleakage already occurs even without any intervention. Our method sits in the middle of these twobut is closer to the base LLAMA2 model with a score of 0.92. The fact that our simple methodfor determining the correct answer yields a high AUC score indicates that we effectively extractadditional information from the model that is supposed to be unlearned. Next, we conduct the same experiment on the TOFU model and dataset to evaluate how our methodgeneralizes to other unlearning techniques. We assess whether the CAF increases for different un-learning methods. illustrates the outcomes of this experiment, where the CAF is eitherlower or the same for almost all questions, in contrast to our initial experiments. This shows that ourmethod does not generalize to the TOFU unlearning setting.",
  "(b)": ": Experiment results for the Harry Potter dataset. (a) shows the comparison of CAFs be-tween the sampling with an unlearned model without and with using AnonAct. Questions on thex-axis are sorted in ascending order by the difference in the CAF. (b) displays the RoC plots for thebase model, unlearned model, and with AnonAct, using keyword frequencies as scores.",
  ": The CAFs from sampling without and with AnonAct for the TOFU experiment. Questionsare sorted in ascending order by the increase in performance": "To test the generalization of our method further, we use another unlearning method, ROME. We no-tice that many of the answers in the CounterFact dataset consist of a single token. Therefore, insteadof sampling, we use this subset of questions with single-token answers to plot the probabilities forthe top tokens. shows the probabilities of the top 40 tokens for the unlearned model withoutand with our method for two example questions. Note that ROME is different from the previous twomethods, as it replaces the information with another one for unlearning. Our method successfullychanges the prediction, leading the sampling away from the forced false token, bringing the distri-bution to a more uniform state. However, it fails to recover the original true answer: Although thecorrect answers are in the top tokens for the questions in , this is because the given namesin the questions are indicative of what the answer might be; not because the model recognizes thesubjects. Our experiments across the three unlearning schemes show that our method is effective in certaincases. We hypothesize that this variation stems from the scope of the subject that was unlearned. Thekey difference between the successful Harry Potter case and the unsuccessful ones lies in the breadthof the subject matter. Harry Potter represents a large media universe, and unlearning it requiressevering connections between its many elements, making it difficult to retrieve related informationfrom any single entity, such as a name. This unlinking effect is effectively mitigated by activationsteering with anonymization, allowing the model to restore conceptual associations. In contrast,the TOFU setup involves a single author, and the task is to delete the information about a specificindividual. ROME targets even more granular data by removing and replacing a single fact.",
  ": Next token probabilities for the completion of the sentences, between the unlearned modeland with AnonAct. Blue is the original answer, and red is the replaced one": "While a model follows numerous paths to connect Harry Potter to his best friendthrough in-universeconcepts like their school, shared adventures, or dialogues, as well as various training data such asbooks, scripts, articles, and blog posts; the connection in the TOFU case such as between an authorand their birthplace, is much more limited. In TOFU, the model learns this connection from a simplesentence about the authors birthplace. Recovering this deleted information by merely steering themodel toward the authors direction proves to be ineffective.",
  "In this work, we contribute to information retrieval from unlearned models. To this end, we proposeactivation steering as a powerful method for this task": "Our novel method employs activation steering in the unlearning context and shows great perfor-mance in recovering lost information from broad subjects, such as ones that were unlearned dueto copyright. We also show and acknowledge the shortcomings of our method when applied tonarrower settings, such as more granular information deletion. We attribute this difference in per-formance to the way information is unlearned from models. We note that the broader setting, HarryPotter, has many links between the in-universe concepts, and activation steering provides a way torecover these, while narrower topics need a different approach to retrieve unlearned information.",
  "Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.805": "Kenneth Li, Oam Patel, Fernanda Viegas, Hanspeter Pfister, and Martin Wattenberg. Inference-timeintervention: Eliciting truthful answers from a language model. In Thirty-seventh Conference onNeural Information Processing Systems, 2023a. Nathaniel Li, Alexander Pan, Anjali Gopal, Summer Yue, Daniel Berrios, Alice Gatti, Justin D.Li, Ann-Kathrin Dombrowski, Shashwat Goel, Gabriel Mukobi, Nathan Helm-Burger, RassinLababidi, Lennart Justen, Andrew Bo Liu, Michael Chen, Isabelle Barrass, Oliver Zhang, Xi-aoyuan Zhu, Rishub Tamirisa, Bhrugu Bharathi, Ariel Herbert-Voss, Cort B Breuer, AndyZou, Mantas Mazeika, Zifan Wang, Palash Oswal, Weiran Lin, Adam Alfred Hunt, JustinTienken-Harder, Kevin Y. Shih, Kemper Talley, John Guan, Ian Steneker, David Campbell, BradJokubaitis, Steven Basart, Stephen Fitz, Ponnurangam Kumaraguru, Kallol Krishna Karmakar,Uday Tupakula, Vijay Varadharajan, Yan Shoshitaishvili, Jimmy Ba, Kevin M. Esvelt, AlexandrWang, and Dan Hendrycks. The WMDP benchmark: Measuring and reducing malicious usewith unlearning. In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, NuriaOliver, Jonathan Scarlett, and Felix Berkenkamp (eds.), Proceedings of the 41st InternationalConference on Machine Learning, volume 235 of Proceedings of Machine Learning Research,pp. 2852528550. PMLR, 2127 Jul 2024.",
  "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Languagemodels are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019": "Nina Rimsky, Nick Gabrieli, Julian Schulz, Meg Tong, Evan Hubinger, and Alexander Turner.Steering llama 2 via contrastive activation addition. In Lun-Wei Ku, Andre Martins, and VivekSrikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers), pp. 1550415522, Bangkok, Thailand, August 2024. Asso-ciation for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.828. Leo Schwinn, David Dobre, Sophie Xhonneux, Gauthier Gidel, and Stephan Gunnemann. Softprompt threats: Attacking safety alignment and unlearning in open-source llms through the em-bedding space. arXiv preprint arXiv:2402.09063, 2024. Weijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao Liu, Terra Blevins, DanqiChen, and Luke Zettlemoyer. Detecting pretraining data from large language models. In TheTwelfth International Conference on Learning Representations, 2024.",
  "Curt Tigges, Oskar John Hollinsworth, Atticus Geiger, and Neel Nanda. Linear representations ofsentiment in large language models. arXiv preprint arXiv:2310.15154, 2023": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-lay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open founda-tion and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. Alexander Matt Turner, Lisa Thiergart, Gavin Leech, David Udell, Juan J Vazquez, Ulisse Mini, andMonte MacDiarmid. Activation addition: Steering language models without optimization. arXivpreprint arXiv:2308.10248, 2023.",
  "Dimitri von Rutte, Sotiris Anagnostidis, Gregor Bachmann, and Thomas Hofmann. A languagemodels guide through latent space. arXiv preprint arXiv:2402.14433, 2024": "Xinwei Wu, Junzhuo Li, Minghui Xu, Weilong Dong, Shuangzhi Wu, Chao Bian, and Deyi Xiong.DEPN: Detecting and editing privacy neurons in pretrained language models. In Houda Bouamor,Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methodsin Natural Language Processing, pp. 28752886, Singapore, December 2023. Association forComputational Linguistics. doi: 10.18653/v1/2023.emnlp-main.174. Dawen Zhang, Pamela Finckenberg-Broman, Thong Hoang, Shidong Pan, Zhenchang Xing, MarkStaples, and Xiwei Xu. Right to be forgotten in the era of large language models: Implications,challenges, and solutions. arXiv preprint arXiv:2307.03941, 2023a. Hangfan Zhang, Zhimeng Guo, Huaisheng Zhu, Bochuan Cao, Lu Lin, Jinyuan Jia, Jinghui Chen,and Dinghao Wu. On the safety of open-sourced large language models: Does alignment reallyprevent them from being misused? arXiv preprint arXiv:2310.01581, 2023b."
}