{
  "Abstract": "Different camera sensors have different noise patterns, and thus an image denoisingmodel trained on one sensor often does not generalize well to a different sensor.One plausible solution is to collect a large dataset for each sensor for training orfine-tuning, which is inevitably time-consuming. To address this cross-domainchallenge, we present a novel adaptive domain learning (ADL) scheme for cross-domain RAW image denoising by utilizing existing data from different sensors(source domain) plus a small amount of data from the new sensor (target domain).The ADL training scheme automatically removes the data in the source domain thatare harmful to fine-tuning a model for the target domain (some data are harmfulas adding them during training lowers the performance due to domain gaps).Also, we introduce a modulation module to adopt sensor-specific information(sensor type and ISO) to understand input data for image denoising. We conductextensive experiments on public datasets with various smartphone and DSLRcameras, which show our proposed model outperforms prior work on cross-domainimage denoising, given a small amount of image data from the target domainsensor.",
  "Introduction": "Noise generated by electronic sensors in a RAW image is inevitable. Over the past few years, learning-based methods have made significant progress in RAW image denoising . However,building a large-scale real-world dataset with noise-clean pairs for training a denoising model istime-consuming and labor-intensive. It is hard to collect ground truth that is noise-free and has nomisalignment with the input noisy data. Moreover, due to the different noise distributions of differentsensors (such as read noise and shot noise), the collected data from a particular sensor usually cannotbe used to train the denoising model of other sensors, which causes a waste of resources. Therefore,it is important to develop a method to solve this problem. Existing solutions to data scarcity in RAW image denoising can be divided into two categories,noise calibration and self-supervise denoising . Noise synthesis andcalibration methods first build a noise model, optimize for noise parameters according to a particularcamera, and then synthesize training pairs from the noise model to train a network. Self-superviseddenoising is designed based on the blindspots schemes. When the input noisy image masks out somepixels and forms a similar but different image from the input, the network learns to denoise instead ofidentity mapping. Therefore, the network can learn to denoise without pairwise noise-clean data.",
  "arXiv:2411.01472v1 [cs.CV] 3 Nov 2024": "While the noise synthesis and calibration methods are top-performing ones for RAW data denoisingand self-supervised denoising does not need to collect pairwise data, both of them have their practicallimitations. First, noise synthesis and calibration methods are not able to obtain the exact noise modelof the real noise. For example, fixed pattern noise such as dark signal non-uniformity (DSNU) andPhoto-response non-uniformity (PRNU) are not included in the model. As a result, some of thesampled noise training pairs might be harmful to the training of the denoising model (i.e., decreasein performance). Second, building a calibration model still needs to collect data under particularcircumstances. Third, these models can only be used to synthesize training data for specific sensors,which leads to a waste of resources. On the other hand, self-supervised denoising is designedunder some unverified assumptions of noise distribution. First, the noise distribution has zeromeans. Second, the noise in different pixels is independent of each other. These assumptions do notmatch the noise in the real world, especially when the noise distribution is complicated. Therefore,self-supervised denoising does not achieve state-of-the-art denoising performance. Different from prior work, we solve this problem by proposing a cross-domain RAW image denoisingmethod, adaptive domain learning (ADL). Our method can utilize existing RAW image denoisingdatasets from various sensors (source domains) combined with very little data from a new sensor(target domain) together to train a denoising model for that new sensor. Some data in a source domainmay be harmful to fine-tuning a model due to the large domain gap: for instance, synthetic data maybe harmful to training a model for real-world applications if the synthetic data imposes unrealistic andunreasonable assumptions. In such cases, our method dynamically evaluates whether a data samplefrom a source domain is beneficial or harmful by evaluating the performance on a small validation setof the target domain, before and after fine-tuning the model on this data sample. If the performanceimproves after fine-tuning, we can use this data sample for training; otherwise, we should ignoreit. As for the network architecture, we design a modulation network that takes sensor-dependentinformation as input (sensor types and ISO), which aligns the features from different sensors into thesame space and ensembles useful common knowledge for denoising. To evaluate our proposed model with ADL, we compare our model against prior methods on diversereal-world public datasets captured by both smartphone and DLSR cameras. The resultsdemonstrate that our method outperforms the prior work and shows consistent state-of-the-artperformance with ADL on RAW data denoising, given a small amount of data in the target domain.We also demonstrate that our ADL can be applied to fine-tuning existing noise calibration modelswith cross-domain data to further improve its performance.",
  "Raw Data Denoising": "In recent years, methods based on RAW data denoising draw a lot of attention . SID shows that RAW image denoising can perform well with a naive U-netarchitecture. Besides, they find difficulties in collecting large-scale datasets. Aware that collectingdatasets is the research bottleneck, many approaches attempt to synthesize more realistic data. UIP and CycleISP attempt to inverse the image signal processing pipeline and synthesize noise inRAW space to train a RAW denoising framework. However, the generated pseudo-RAW data stillhas great differences compared to real RAW data. Jin et al. utilize different noise distributionparameters to form a simulation camera to train a network. However, they still need to build a noisemodel to synthesize data. Another kind of approach is the noise calibration method .",
  "t1Otherwise": ": The overall pipeline of our adaptive domain learning (ADL) algorithm. The networkparameter 0 is first initialized, then the small target domain training set will be used to train a modelwith parameter . In the source domain adaptive learning stage, in iteration t, data from the sourcedomain will be used to update the network parameter from t1 to . Then a dynamic validation setwill judge whether the data is useful. If so, set t = and repeat the process. If not, retrieve thenetwork parameter from to t1. Finally, the target domain data will be used to fine-tune to T",
  "Meta-transfer learning in low-level vision": "The gap between different domains (synthetic and real, daylight and night, etc.) is a great challengein the field of low-level vision. To solve the problem, a set of approaches based on meta-transferlearning is proposed . Park et al. and Soh et al. utilizeMAML algorithm in super-resolution to obtain a model with better initialization implicitly fromthe source domain, then fine-tune it to fit the target domain better. Kim et al. transferred theuseful features from the synthesis noise model to the real-world noise model to overcome the domaingap problem between real-world noise and synthetic noise with an adaptive instance normalizationlayer to help the synthetic noise better adapt to real-world noise. For the meta-transfer learning method, it is very hard to tell whether the implicitly learned information is usefulor not. Some of the transferred features might be harmful. In contrast, our ADL can learn commonknowledge and remove harmful information explicitly.",
  "Given the small training set of the target domain T adp, we first pre-train a model for the target domainby minimizing pixel-wise L1 Loss": "Target domain pre-training has benefits in two aspects. First, although the domain gap exists betweenthe source domain and the target domain, denoising is a task that shares similar implicit featurerepresentations. Therefore, pre-training can provide better initialization for the adaptive domainlearning stage. Second, there is only very little data from the target domain, and the data from thesource domain can be 100 times more than the data in the target domain. Pre-training on the targetdomain can improve the robustness and ensure the dominant position of the target domain data in thewhole training process to prevent our model from overfitting to the source domain.",
  "Source Domain Adaptive Learning": "However, due to the domain gap between the source domain and the target domain, not all the datafrom the source domain contribute to the training of the target domain, some data might be harmfuland will lead to performance reduction. Therefore, we proposed adaptive domain learning (ADL), toeliminate harmful data and make use of the one that has contributions to our model.",
  "where is the learning rate and L(S) is the L1 loss defined on S": "Dynamic validation set To tell whether the data batch S has contributions to our model, we evaluatethe updated parameter on a target domain validation set T val that is sampled from the target domaindataset T adp. The selection of the validation set in each iteration t is crucial to the performance ofour method. Fixed validation set selection for each iteration may make the training stuck in the localminima and easily overfit to the validation set. To avoid these problems from happening, in eachiteration t, we randomly sampled a dynamic validation set V of size k from the target domain datasetT adp to let our model explore the feature space in a stochastic way. On the other hand, the rest of thedataset from T adp, denoted as T T rain, will combined with S to provide the correct direction for thetraining process. At the beginning of the training, k is set to 20% of the size of T adp and increasesduring the training process. At the end of the training, 50% of T adp will be used. Moreover, inspired by , when the size of T adp is extremely small, i.e., smaller than 10, weintentionally select the data that has very diverse system gain from T adp to form V in each iterationto avoid the over-fitting problem. Dynamic average PSNR We evaluate whether the data batch S is useful or not by comparingthe PSNR of the result of the updated network parameter to the PSNR of the result of previousiteration t on the sampled validation set V . However, hard criteria based on PSNR usually makethe training procedure unstable under the setting of the dynamic validation set. When the size ofthe dynamic validation set V is small, the variance of PSNR is large and thus is not that reliable.Some useful data might be removed accidentally. In such cases, we want our model to take a databatch S as useful data if S has a trend to improve the performance of our model. We design softcriteria based on PSNR by maintaining a priority queue Qeval of max size M that stores the value ofhighest PSNR value in the history during the training process. Qeval is ranked by the value of PSNRin ascending order. We denote the PSNR on our dynamic validation set V of model at iteration tin our training process as Eval(V , t). At the beginning of the adaptive domain learning stage, wepush Eval(V , t=0) into Qeval. During the training, if the PSNR of the updated parameter onthe dynamic validation set V , Eval(V , ), is higher than the average PSNR in Qeval, we keep theupdated parameter and push Eval(V , ) into Qeval and pop out the first element in Qeval if it isfull. Else, we retrieve the network parameter from to t1. This process can be characterized as",
  "N = I + KNdep + Nindep,(3)": "where K is the system gain, Ndep is the signal dependent noise and Nindep is the signal independentnoise. Based on this modeling, we propose a channel-wise modulation network to adjust the featurespace by embedding two easy-to-access parameters, the sensor type and the ISO in our network. ISOis proportional to system gain K, while the sensor type can help the network know how to utilize theISO to learn the signal-dependent noise Ndep and recognize the signal-independent noise Nindep. Given the one-hot encoding of the sensor type p R1n and the corresponding ISO s R1n(duplicate n times in the vector), our channel-wise modulation layer transfers the concatenatedmetadata (p, s) into a channel-wise scale and shifts by",
  "Ground truthBlind2Unblind Transfer learning Our error map": ": The error map of our method compares against state-of-the-art approaches. The firstrow is the result from the SIDD dataset, and the second row is the result from the SID dataset. Wecan see that our method is able to generate the image with smaller errors and less noise compared toprevious work.",
  "Experimental Setup": "Datasets. We evaluate the performance of our ADL and modulation on the dataset captured bysmartphones in normal light conditions (SIDD dataset ), and the dataset captured by DLSR camerasin extremely low light conditions (ELD and SID dataset). Compared to RAW data capturedby smartphones, RAW data captured by DSLR cameras in extremely low light environments is moredifficult to denoise because the noise distribution is more complicated and the noise level is larger.Moreover, the domain gap between the RAW data captured by different DSLR cameras is larger thanthe domain gap between the RAW data captured by smartphones. SIDD is a popular RAW denoising dataset that contains 160 pairs of noisy and ground-truth RAWdata from 5 different smartphone cameras (G4, GP, IP, N6, S6) of different scenes. ELD Dataset contains RAW data captured by 3 different brands of DSLR cameras (Nikon, Canon, Sony) withdifferent ISO and light factors, while the SID dataset captured RAW data using 2 different brandsof DSLR cameras (Sony and Fuji). Note that the ELD dataset and SID dataset are using thesame DSLR camera (Sony A7S2). We only use the data captured by this camera from SID dataset tokeep the domain gap between each set of domains. Besides, the input RAW data of the ELD andSID datasets are captured in extremely low light environments, while the ground truth is captured innormal light conditions. We follow the training strategy in by multiplying a light factor by theinput RAW data to keep the input RAW data and ground-truth RAW data in the same space. Baselines and training settings. To evaluate the performance of our framework, we compareour method against several baselines: the fundamental baselines pre-train and then fine-tune. Self-supervise denoising methods: Blind2unblind , ZS-N2N , and DIP . Meta transferlearning method MZSR , Prabhakar et al. and Kim et al. (denoted as transfer learning).Traditional approach BM3D . Calibration Free Method Led . Note that we try our best to findall possible work that has the same goal as our method for the comprehensive baseline comparisons.Although the approach might be different, we make the experiment as fair as possible with propersettings. We compare the performance of our method against the above baselines by cross-validation on eachsensor of all three datasets. In each experiment, we take one sensor as the target domain to representthe sensor with a very small number of data (around 20 pairs of data). The data from all other sensorswill form the source domain, which represents the existing dataset. For the baselines, we only usethe data from the target domain for the training of all self-supervised denoising methods, no datafrom the source domain is used since cross-domain data will reduce the overall performance of thesemethods. For fine-tuning, we first pre-train the model with the data from the source domain usingU-net, then use the data from the target domain for fine-tuning. For DIP , the model is trained on",
  "MethodG4GPIPN6S6Avg": "Fine-tuning50.17/0.968 43.53/0.914 52.77/0.977 43.86/0.917 37.88/0.863 45.58/0.928BM3D 50.08/0.968 42.14/0.909 52.39/0.972 43.40/0.916 35.52/0.855 44.71/0.924DIP 46.91/0.931 39.88/0.896 48.81/0.955 41.73/0.906 35.23/0.855 42.51/0.909ZS-N2N 48.86/0.941 41.54/0.909 50.06/0.968 41.88/0.910 35.07/0.856 43.48/0.917MZSR 51.84/0.972 44.58/0.921 53.74/0.982 45.07/0.924 37.21/0.868 46.49/0.933Transfer learning 52.28/0.974 44.96/0.923 53.04/0.982 44.77/0.923 40.10/0.898 47.03/0.940Blind2Unblind 51.78/0.970 44.91/0.919 54.12/0.985 46.02/0.928 38.85/0.892 47.14/0.939Prabhakar et al. 51.76/0.972 44.68/0.919 53.82/0.983 44.92/0.922 38.67/0.878 46.34/0.933Ours52.55/0.975 45.18/0.923 54.37/0.987 46.13/0.932 40.16/0.901 47.68/0.944",
  "MethodSonyFujiNikonCanonAvg": "Fine-tuning35.94/0.857 36.37/0.862 35.22/0.853 35.63/0.855 35.79/0.857BM3D 35.61/0.856 35.88/0.857 35.37/0.853 35.07/0.852 35.48/0.855DIP 31.02/0.696 29.44/0.611 30.71/0.652 30.53/0.641 30.42/0.650ZS-N2N 32.15/0.724 30.39/0.632 30.46/0.643 31.34/0.707 31.09/0.677MZSR 36.21/0.861 36.98/0.866 36.14/0.860 35.89/0.857 36.31/0.861Transfer learning 36.92/0.864 37.33/0.869 36.49/0.862 35.77/0.858 36.63/0.863Blind2Unblind 36.71/0.866 36.57/0.866 35.88/0.857 35.49/0.855 36.16/0.861Prabhakar et al. 36.12/0.859 36.33/0.864 35.47/0.854 35.72/0.857 36.01/0.861Ours37.28/0.871 37.58/0.872 36.74/0.866 36.45/0.868 37.01/0.868 : The quantitative PSNR and SSIM results compared to the baselines on the SIDD (G4,GP, IP, N6, and S6), ELD (Nikon, and Canon), and SID (Fuji and Sony) datasets. Fine-tuningmeans training on source domain data and then fine-tuning on target domain data. The camera nameon the top row means that we keep this camera as the target domain and use data from other camerasas the source domain. each test data. We first split the whole dataset into two parts, the training set, and the test set. Thetraining set T adp is used for the training of the baselines. All training RAW data will first be packedinto 4-channel according to the Bayer pattern and then cropped into patches with shape 256 256.We train our ADL in 300k iterations, using AdamW as the optimizer with a learning rate of 3 103.",
  "Results on Real Data": "We quantitatively evaluate our method by comparing the PSNR against other baselines on thesmartphone dataset SIDD and DSLR camera dataset ELD and SID. Note that we only report PSNR because there are no other systematic evaluation metrics designed for RAW data. Other popularevaluation metrics like LPIPS is not suitable for RAW data. We also present the SSIM metrics in the supplementary material. The result is illustrated in . Here, fine-tuning denotesthe experiment training on the source domain and fine-tuning on the target domain. The camera nameon the top row means that we take this sensor as the target domain while keeping the other sensors asthe source domain. For example, G4 in means that this experiment takes G4 as the targetdomain and all data from the other four sensors, GP, IP, N6, and S6 will form the source domaindataset. From the table, we can see that our proposed method has the best performance on boththe smartphone dataset and the DSLR dataset. To be specific, our method has 0.71dB and 0.39dBperformance gains on average compared to MZSR and transfer learning baseline. The PSNRvalues in the table of the ELD and SID dataset are much lower than those in the table of the SIDDdataset because the ELD and SID datasets are captured in extremely low light environments: the noiselevel is higher, and the scenes are much more complicated than the SIDD dataset. Note that althoughthe AIN module in Kim et al. has a similar function and architecture to our modulation strategy,they can only estimate the noise level, which is very limited when the source domain contains datafrom many sensors. Our modulation strategy is more extensible and can somehow provide morehyper information(if provided in the dataset) and can let the network know the difference betweenthe sensors. Qualitative Result Since the PSNR of the RAW denoising is relatively high and difficult to tell thedifference from the human visual perspective, We evaluate our method qualitatively by comparingthe error map against all three baselines. As illustrated in , since the RAW data is hard to",
  ": The analysis of calibration model and non-calibration model. The SSIM result is included inthe supplementary material": "visualize, we transfer the ground truth into the sRGB domain using LibRAW to better demonstratethe color and details. For the restored RAW images, it is hard to use a well-designed ISP (ImageSignal Processing) pipeline to obtain visually pleasant sRGB images for different sensors, we onlydemonstrate the result on RAW space. The error map is calculated in RAW space. We can see thatthe error between the ground truth and our output noise-free image is much smaller compared to allbaseline methods. For more qualitative results, please refer to supplementary material.",
  "Analysis of Calibration-related Methods": "Noise calibration methods Although the noise calibration method is powerful, the calibrated modelstill does not include out-of-model noise such as fixed-pattern noise. Thus, fine-tuning using real datacan further improve the performance of the model trained by those synthetic data. However, when thereal data used for fine-tuning is scarce, the improvement is usually marginal. In such cases, we maywant to utilize more data from different domains to further improve the result. In this section, weanalyze the performance of applying our ADL to fine-tune the existing noise calibration model withdata from multi-domain. We utilize the noise calibration method proposed by Zhang et al. . Theresult is illustrated in (a). Here Single means that only the data from that correspondingsensor is used for fine-tuning, while Multiple means that the data from all sensors in that datasetis used for fine-tuning. The result shows that when utilizing limited data from a single domain, theimprovement of both naive fine-tuning and ADL is marginal. When we use more data from multipledomains, naive fine-tuning cannot learn useful information from various domains and thus leads to adrop in the final performance. However, our ADL can remove the harmful data, and ensemble theuseful information from the different domains to help the training. Calibration-free methods Different from the noise calibration method, the calibration-free methodLed embed noise distribution from the simulated camera into the pre-train model. AlthoughLED is a calibration-free method, the network also has no prior knowledge of the noise distributionof the target domain data during the pre-train stage. The large intensity distribution gap betweenthe simulation cameras and the test set will lead to low performance. However, our ADL has priorknowledge of the target domain noise intensity distribution throughout the training process, whichcan gain similar robustness to the calibration method. As illustrated in (b), we replace thesynthetic data from the well-calibrated simulated camera in Led with the data from the sourcedomain(data from existing sensors), which is the same as our method in the experiments. Our methodcan outperform Led in this case. This is because the performance of Led highly depends onthe prior knowledge of noise distribution learning from the simulated camera. Our method will notuse data with a huge gap in intensity distribution between the source domain and the target domain. PMN PMN aims to overcome the bottleneck of the learnability in real RAW denoising byreforming the data. It can be generalized and applied to all real RAW image denoising methods andimprove their performance including our ADL. As illustrated in , the performance of ourADL can be improved by applying the training strategy proposed in PMN.",
  "Ours37.28/0.871 37.58/0.872 36.74/0.866 36.45/0.868 37.01/0.868Ours+PMN 37.43/0.872 37.89/0.874 36.91/0.869 36.63/0.868 37.19/0.871": ": The quantitative PSNR and SSIM results with and without the training strategyproposed in PMN on the SIDD (G4, GP, IP, N6, and S6), ELD (Nikon, and Canon), and SID(Fuji and Sony) datasets. PMN can improve the performance of our ADL. The camera name on thetop row means that we keep this camera as the target domain and use data from other cameras as thesource domain. Size of Target Domain Data 34.0 34.5 35.0 35.5 36.0 36.5 37.0 PSNR(dB)",
  "Analysis on Useful and Harmful Data": "The domain gap in the deep learning model of other tasks is usually caused by the scene between thetraining data and test data. As for RAW image denoising, the difference in noise intensity amongdifferent sensors is more crucial. For example, if the noise intensity of two sensors is the same, thenthe noise model trained on one sensor can be generalized to the other. On the contrary, the model willfail if the noise intensity between these two sensors is very different. Based on this observation, wecan say that the source domain data with similar noise intensity to the target domain data is useful,while the data with a large noise intensity difference is harmful. However, in cross-domain training,the harmful data has a more negative impact, because in cross-domain training, the model will tend tocompromise different domains to reach the global minimum. As illustrated in , we utilize thetarget domain data from the ELD dataset as the base set, and we build two Harmful datasets. HereHarmful1 is synthesized by using the ground truth of the SIDD dataset that is captured in brightlight conditions and naive Gaussian noise with noise level harmful = 30, and Harmful2 set isthe data pairs that have mis-alignment. For example, the input and ground truth are from differentscenes, or the ground truth is black. In these cases, even though we add more data, the performancestill drops. However, our ADL ignores the harmful data and always optimizes the model towards thenoise intensity of the target domain.",
  "Ablation Study": "ADL and Modulation Strategy We conduct an ablation study on the effectiveness of our ADL andmodulation strategy. The ablation study is conducted on SID and ELD datasets with the same trainingsettings and configuration as in the previous section. We ablate over the strategies of our method bytraining models without applying the target domain pretraining, source domain adaptive learning,and modulation(including sensor type and ISO modulation). As illustrated in , the resultdemonstrated that the target domain pretraining, source domain adaptive learning, ISO and sensortype modulation, and dynamic validation set all contribute to the final result. The source domainadaptive learning, which automatically evaluates whether data from the source domain is harmful ornot, is the most crucial strategy in our framework.",
  "ADL ISO Type Pre DynSonyFujiNikonCanon": "36.15/0.858 36.44/0.859 36.52/0.861 36.00/ 0.85737.13/0.868 37.41/0.871 36.66/0.860 36.27/0.85736.81/0.862 36.93/0.864 36.46/0.858 36.11/0.85535.88/0.855 36.14/0.856 35.97/0.856 34.69/0.78836.89/0.866 37.41/0.871 36.42/0.862 36.23/0.86137.28/0.871 37.58/0.872 36.74/0.866 36.45/0.864 : The PSNR and SSIM result of the ablation study on the ELD and SID datasets. Thecamera name on the top row means that we keep this camera as the target domain and use data fromother cameras as the source domain. ADL means Adaptive domain learning, ISO\" means use ISOin modulation, Type\" means use sensor type in modulation Pre means target domain pretraining,and Dyn means dynamic validation set.",
  "FTFTADLFTADL": "Sony35.01/0.80534.59/0.77235.13/0.81219.06/0.21634.99/0.808Fuji34.97/0.80634.69/0.77135.21/0.82320.14/0.24435.06/0.807Nikon34.68/0.78234.42/0.76535.85/0.85321.26/0.29734.62/0.782Canon34.76/0.79434.37/0.75234.88/0.79721.17/0.26834.71/0.792 : The PSNR and SSIM result of our ADL comparing to naive fine-tuning on base set andsynthetic harmful dataset. Here FT means naive fine-tuning, Base means only using data fromthe corresponding sensor to fine-tune, while Harmful1 means using naive Gaussian synthetic datawith the different light conditions to fine-tune, and Harmful2 means using the misaligned input andground truth data to fine-tune. Size of the Target Domain Data To evaluate how our method performs on different sizes of thetarget domain data, we conduct the ablation study on two sensors, G4 in the SIDD dataset and Sonyin the SID dataset. demonstrates the PSNR against the size of the target domain datasetT adp compared to the fundamental baseline fine-tuning and our method without using the dynamicvalidation set strategy and diverse system gain selection strategy. It can be observed that our methodcan outperform fine-tuning when the size of the target domain data is extremely small. Besides, ourdynamic validation set strategy also prevents the training from over-fitting when the target domaindata is scarce.",
  "Conclusion": "We have proposed a novel adaptive domain learning (ADL) scheme for cross-domain image denoising.We leverage the data from other sensors to help the training of the data from new sensors in a smartfashion: ADL removes harmful data and utilizes useful data from the source domain to improve theperformance in the target domain. Our proposed modulation strategy provides extra camera-specificinformation, which helps differentiate the noise patterns of input data. We evaluate our method onsmartphone and DSLR camera datasets, and the results demonstrate that our method outperformsstate-of-the-art approaches in cross-domain image denoising. Moreover, we show that ADL can alsobe easily extended to image deblurring. We believe ADL is general and can be generalized to othercross-domain tasks, which can be further explored in the future.",
  "We thank SenseTime Group Limited for supporting this research project": "Abdelrahman Abdelhamed, Marcus A Brubaker, and Michael S Brown. Noise flow: Noise modeling withconditional normalizing flows. In Proceedings of the IEEE/CVF International Conference on ComputerVision, pages 31653173, 2019. Abdelrahman Abdelhamed, Stephen Lin, and Michael S Brown. A high-quality denoising dataset forsmartphone cameras. In Proceedings of the IEEE conference on computer vision and pattern recognition,pages 16921700, 2018. Tim Brooks, Ben Mildenhall, Tianfan Xue, Jiawen Chen, Dillon Sharlet, and Jonathan T Barron. Unpro-cessing images for learned raw denoising. In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 1103611045, 2019.",
  "Chen Chen, Qifeng Chen, Jia Xu, and Vladlen Koltun. Learning to see in the dark. In Proceedings of theIEEE conference on computer vision and pattern recognition, pages 32913300, 2018": "Zhixiang Chi, Yang Wang, Yuanhao Yu, and Jin Tang. Test-time fast adaptation for dynamic scenedeblurring via meta-auxiliary learning. In Proceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 91379146, 2021. Kostadin Dabov, Alessandro Foi, Vladimir Katkovnik, and Karen Egiazarian. Image denoising by sparse3-d transform-domain collaborative filtering. IEEE Transactions on image processing, 16(8):20802095,2007. Hansen Feng, Lizhi Wang, Yuzhi Wang, Haoqiang Fan, and Hua Huang. Learnability enhancement forlow-light raw image denoising: A data perspective. IEEE Trans. Pattern Anal. Mach. Intell., 46(1):370387,2024.",
  "Bo Jiang, Yao Lu, Bob Zhang, and Guangming Lu. Few-shot learning for image denoising. IEEE Trans.Circuits Syst. Video Technol., 33(9):47414753, 2023": "Xin Jin, Jia-Wen Xiao, Ling-Hao Han, Chunle Guo, Ruixun Zhang, Xialei Liu, and Chongyi Li. Lightingevery darkness in two pairs: A calibration-free pipeline for raw denoising. arXiv preprint arXiv:2308.03448,2023. Yoonsik Kim, Jae Woong Soh, Gu Yong Park, and Nam Ik Cho. Transfer learning from synthetic toreal-noise denoising with adaptive instance normalization. In Proceedings of the IEEE/CVF conference oncomputer vision and pattern recognition, pages 34823492, 2020. Alexander Krull, Tim-Oliver Buchholz, and Florian Jug. Noise2void-learning denoising from single noisyimages. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages21292137, 2019.",
  "Samuli Laine, Tero Karras, Jaakko Lehtinen, and Timo Aila. High-quality self-supervised deep imagedenoising. Advances in Neural Information Processing Systems, 32, 2019": "Wooseok Lee, Sanghyun Son, and Kyoung Mu Lee. Ap-bsn: Self-supervised denoising for real-worldimages via asymmetric pd and blind-spot network. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, pages 1772517734, 2022. Jaakko Lehtinen, Jacob Munkberg, Jon Hasselgren, Samuli Laine, Tero Karras, Miika Aittala, and TimoAila. Noise2noise: Learning image restoration without clean data. arXiv preprint arXiv:1803.04189, 2018.",
  "Yijun Li, Chen Fang, Jimei Yang, Zhaowen Wang, Xin Lu, and Ming-Hsuan Yang. Universal style transfervia feature transforms. Advances in neural information processing systems, 30, 2017": "Ali Maleky, Shayan Kousha, Michael S Brown, and Marcus A Brubaker. Noise2noiseflow: Realisticcamera noise modeling without clean images. In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 1763217641, 2022. Youssef Mansour and Reinhard Heckel. Zero-shot noise2noise: Efficient image denoising without anydata. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages1401814027, 2023. Kristina Monakhova, Stephan R Richter, Laura Waller, and Vladlen Koltun. Dancing under the stars:video denoising in starlight. In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1624116251, 2022. Reyhaneh Neshatavar, Mohsen Yavartanoo, Sanghyun Son, and Kyoung Mu Lee. Cvf-sid: Cyclic multi-variate function for self-supervised image denoising by disentangling noise from image. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1758317591, 2022. Yizhong Pan, Xiao Liu, Xiangyu Liao, Yuanzhouhan Cao, and Chao Ren. Random sub-samples generationfor self-supervised real image denoising. In Proceedings of the IEEE/CVF International Conference onComputer Vision, pages 1215012159, 2023. Seobin Park, Jinsu Yoo, Donghyeon Cho, Jiwon Kim, and Tae Hyun Kim. Fast adaptation to super-resolution networks via meta-learning. In Computer VisionECCV 2020: 16th European Conference,Glasgow, UK, August 2328, 2020, Proceedings, Part XXVII 16, pages 754769. Springer, 2020. Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. Semantic image synthesis with spatially-adaptive normalization. In Proceedings of the IEEE/CVF conference on computer vision and patternrecognition, pages 23372346, 2019. K. Ram Prabhakar, Vishal Vinod, Nihar R. Sahoo, and Venkatesh Babu Radhakrishnan. Few-shot domainadaptation for low light RAW image enhancement. In 32nd British Machine Vision Conference 2021,BMVC 2021, Online, November 22-25, 2021, page 327. BMVA Press, 2021. Jae Woong Soh, Sunwoo Cho, and Nam Ik Cho. Meta-transfer learning for zero-shot super-resolution. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 35163525,2020.",
  "Ziyu Wan, Bo Zhang, Dongdong Chen, and Jing Liao. Bringing old films back to life. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1769417703, 2022": "Ziyu Wan, Bo Zhang, Dongdong Chen, Pan Zhang, Dong Chen, Jing Liao, and Fang Wen. Bringingold photos back to life. In proceedings of the IEEE/CVF conference on computer vision and patternrecognition, pages 27472757, 2020. Zejin Wang, Jiazheng Liu, Guoqing Li, and Hua Han. Blind2unblind: Self-supervised image denoisingwith visible blind spots. In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR), pages 20272036, June 2022. Kaixuan Wei, Ying Fu, Jiaolong Yang, and Hua Huang. A physics-based noise formation model forextreme low-light raw denoising. In Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 27582767, 2020.",
  "Wenhao Wu, Tao Wang, Zhuowei Wang, Lianglun Cheng, and Heng Wu. Meta transfer learning-basedsuper-resolution infrared imaging. Digital Signal Processing, 131:103730, 2022": "Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, Ming-HsuanYang, and Ling Shao. Cycleisp: Real image restoration via improved data synthesis. In Proceedings of theIEEE/CVF conference on computer vision and pattern recognition, pages 26962705, 2020. Feng Zhang, Bin Xu, Zhiqiang Li, Xinran Liu, Qingbo Lu, Changxin Gao, and Nong Sang. Towardsgeneral low-light raw noise synthesis and modeling. In Proceedings of the IEEE/CVF InternationalConference on Computer Vision, pages 1082010830, 2023. Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonableeffectiveness of deep features as a perceptual metric. In Proceedings of the IEEE conference on computervision and pattern recognition, pages 586595, 2018. Yi Zhang, Hongwei Qin, Xiaogang Wang, and Hongsheng Li. Rethinking noise synthesis and modelingin raw denoising. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages45934601, 2021."
}