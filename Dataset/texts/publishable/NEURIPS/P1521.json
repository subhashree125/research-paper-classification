{
  "Abstract": "Large vision-language models (VLLMs) exhibit promising capabilities for pro-cessing multi-modal tasks across various application scenarios. However, theiremergence also raises significant data security concerns, given the potential in-clusion of sensitive information, such as private photos and medical records, intheir training datasets. Detecting inappropriately used data in VLLMs remains acritical and unresolved issue, mainly due to the lack of standardized datasets andsuitable methodologies. In this study, we introduce the first membership infer-ence attack (MIA) benchmark tailored for various VLLMs to facilitate trainingdata detection. Then, we propose a novel MIA pipeline specifically designed fortoken-level image detection. Lastly, we present a new metric called MaxRnyi-K%,which is based on the confidence of the model output and applies to both text andimage data. We believe that our work can deepen the understanding and method-ology of MIAs in the context of VLLMs. Our code and datasets are available at",
  "Introduction": "The rise of large language models (LLMs) has inspired the exploration of largemodels across multi-modal domains, exemplified by advancements like GPT-4 and Gemini .These large vision-language models (VLLMs) have shown promising ability in various multi-modaltasks, such as image captioning , image question answering , and image knowledgeextraction . However, the rapid advancement of VLLMs also causes user concerns about privacyand knowledge leakage. For instance, the image data used during commercial model training maycontain private photographs or medical diagnostic records. This is concerning since early work hasdemonstrated that machine learning models can memorize and leak training data . Tomitigate such concerns, it is essential to consider the membership inference attack (MIA) ,where attackers seek to detect whether a particular data record is part of the training dataset .The study of MIAs plays an important role in preventing test data contamination and protecting datasecurity, which is of great interest to both industry and academia . When exploring MIAs in VLLMs, one main issue is the absence of a standardized dataset designedto develop and evaluate different MIA methods, which comes from the large size and multi-modality of the training data, and the diverse VLLMs training pipelines . Therefore, oneof the main goals of this work is to build an MIA benchmark tailored for VLLMs. Beyond the need for a valid benchmark, we lack efficient techniques to detect a single modalityin VLLMs. The closest work to ours is , which performs MIAs on multi-modal CLIP bydetecting whether an image-text pair is in the training set. However, in practice, it is more common",
  "instructionimage": ": MIAs against VLLMs. Top: Our image detection pipeline: In the generation stage, wefeed the image and instruction to the target model to obtain a description; then during the inferencestage, we input the image, instruction, and generated description to the model, and extract the logitsslices to calculate metrics. Bottom: MaxRnyi-K% metric: we first get the Rnyi entropy of eachtoken position, then select the largest k% tokens and calculate the average Rnyi entropy. to detect a single modality, as we care whether an individual image or text is in the training set.Therefore, we aim to develop a pipeline to detect the single modality from a multi-modal model.Moreover, existing literature on language model MIAs, such as Min-K% and Perplexity ,mostly are target-based MIAs, which use the next token as the target to compute the predictionprobability. However, we can only access the image embedding instead of the image token in VLLMs,and thus only target-free MIAs can be directly applied. Therefore, we first propose a cross-modal pipeline for individual image or description MIAs onVLLMs, which is distinguished from traditional MIAs that only use one modality . We feedthe VLLMs with a customized image-instruction pair from the target image or description. We showthat we can perform the MIA not only by the image slice but also by the instruction and descriptionslices of the VLLMs output logits, see . Such a cross-modal pipeline enables the usage oftext MIA methods on image MIAs. We also introduce a target-free metric that adapts to both imageand text MIAs and can be further modified to a target-based way.",
  "Overall, the contributions and insights can be summarized as follows": "We release the first benchmark tailored for the detection of training data in VLLMs, calledVision Language MIA (VL-MIA) (). By leveraging Flickr and GPT-4, weconstruct VL-MIA that contains two images MIA tasks and one text MIA task for variousVLLMs, including MiniGPT-4 , LLaVA 1.5 and LLaMA-Adapter V2 . We perform the first individual image or description MIAs on VLLMs in a cross-modalmanner. Specifically, we demonstrate that we can perform image MIAs by computingstatistics from the image or text slices of the VLLMs output logits ( and .1). We propose a target-free MIA metric, MaxRnyi-K%, and its modified target-basedModRnyi (.2). We demonstrate their effectiveness on open-source VLLMs andclosed-source GPT-4 (). We achieve an AUC of 0.815 on GPT-4 in image MIAs.",
  "Membership Inference Attack (MIA) aims to classify whether a data sample has been used intraining a machine learning model . Keeping training data confidential is a desired property for": "machine learning models, since training data may contain private information about an individual . Popular MIA methods can be divided into metric-based and shadow model-based MIAs .Metric-based MIAs determine whether a data sample has been used for trainingby comparing metrics computed from the output of the target model with a threshold. Shadowmodel-based MIAs need shadow training to mimic the behavior of the target model , which iscomputationally infeasible for LLMs. Therefore, we focus on the metric-based methods in this work. MIAs have been extensively researched in various machine learning models, including classifica-tion models , generative models , and embedding models . Withthe emergence of LLMs, there are also a lot of work exploring MIAs in LLMs .Nevertheless, MIAs for multi-modal models have not been fully explored. perform MIAsusing the similarity between the image and the ground truth text, which detects the image-text pairinstead of a single image or text sequence. However, detecting an individual image or text is morepractical in real-world scenarios and poses additional challenges. To the best of our knowledge, weare the first to perform the individual image or text MIA on VLLMs. In this paper, we also consider a more difficult task, to detect the pre-training data from the fine-tunedmodels, that is, to detect the base LLM pertaining data from VLLMs. First, compared with detectingfine-tuning data , pretraining data come from a much larger dataset and are used only once,reducing the potential probability for a successful MIA . In addition, compared to thedetection of pretraining data from the pre-trained models , catastrophic forgetting in the fine-tuning stage also makes it harder to detect the pre-training data from downstream models.To the best of our knowledge, we are the first to perform pre-training data MIAs on fine-tuned models. Large Vision-Language Models (VLLMs) incorporate visual preprocessors into LLMs to manage tasks that require handling inputs from text and image modalities. A foundational approachin this area is represented by CLIP , which established techniques for aligning modalities betweentext and images. Further developments have integrated image encoders with LLMs to create enhancedVLLMs. These models are typically pre-trained on vast datasets of image-text pairs for featurealignment , and are subsequently instruction-tuned for specific downstream tasks torefine the end ability. MiniGPT , LLaVA , and LLaMA Adapter series havedemonstrated significant capabilities in understanding and inference in this area.",
  "In this section, we introduce the main notation and problem settings for MIAs": "Notation.The token set is denoted by V.A sequence with L tokens is denoted by X :=(x1, x2, . . . , xL), where xi V for i [L]. Let X1 X2 be the concatenation of sequenceX1 and X2. An image token sequence is denoted by Z. In this work, we focus on the VLLM,parameterized by , where the input is the image Z followed by the instruction text Xins, and theoutput is the description text Xdes. We use Ddes and Dimage to represent the description training setand image training set, respectively. Detailed notations are summarized in of the appendix. Attackers goal. In this work, the purpose of the attacker is to detect whether a given data point(image Z or description Xdes) belongs to the training set. We formulate this attack as a binary classi-fication problem. Let Aimage(Z; ) : {0, 1} and Ades(X; ) : {0, 1} be two binary classificationalgorithms for image and description respectively, which are implemented by comparing the metricScore(Z Xins Xdes; ) with some threshold . When detecting image Z, we feed the model with the target image with a fixed instruction promptsuch as Describe this image in detail, denoted as Xins. The model then generates the descriptiontext Xdes. The algorithm Aimage(Z; ) is defined by",
  "Ades(Xdes; ) =1(Xdes Ddes), if Score(Zept Xept Xdes; ) < ,0(Xdes / Ddes), if Score(Zept Xept Xdes; ) .(2)": "Attackers knowledge. We assume a grey-box setting on the target model, where the attacker canquery the model by a custom prompt (including an image and text) and have access to the tokenizer,the output logits, and the generated text. The attacker is unaware of the training algorithm andparameters of the target model.",
  "Dataset construction": "We construct a general dataset: Vision Language MIA (VL-MIA), based on the training data used forpopular VLLMs, which, to our knowledge, is the first MIA dataset designed specifically for VLLMs.We present a takeaway overview of VL-MIA in . We also provide some examples in VL-MIA,see in the appendix. The prompts we use for generation can be found in .",
  "MiniGPT-4 instruction-tuning textGPT-generated answersMiniGPT-4": "Target models. We perform MIAs open-source VLLMs, including MiniGPT-4 , LLaVA-1.5 ,and LLaMA-Adapter V2.1 . The checkpoints and training datasets of these models are providedand public. The training pipeline of a VLLM encompasses several stages. Initially, an LLM undergoespre-training using extensive text data such as LLaMA . Meanwhile, a vision preprocessor, e.g.,CLIP , is pre-trained on a large number of image-text pairs. Subsequently, a VLLM is constructedbased on the LLM and the vision preprocessor and is pre-trained using image-text pairs. The finalstage involves instruction-tuning the VLLM, which can be performed using either image-text pairsor image-based question-answer data. In the instruction tuning stage of a VLLM, every data entrycontains an image, a question, and the corresponding answer to the image. We use the answer text asmember data and GPT-4 generated answers under the same question and same image as non-memberdata. Specifically, for LLaVA 1.5 and LLaMA-Adapter v2, we use the answers in LLaVA 1.5sinstruction tuning as member data. VL-MIA/DALL-E. MiniGPT-4, LLaVA 1.5, and LLaMA-Adapter V2 use images from LAION ,Conceptual Captions 3M , Conceptual 12M and SBU captions datasets (collectivelyreferred to as LAION-CCS) as pre-training data. BLIP provides a synthetic dataset with image-caption pairs for LAION-CCS used in MiniGPT-4 and LLaVA 1.5. We first detect the intersectionof the training images used in these three VLLMs. From this intersection, we randomly selecta subset to serve as the member data for our benchmark. For the non-member data, we use thecorresponding BLIP captions of the member images as prompts to generate images with DALL-E23. This process yields one-to-one corresponding pairs of the generated images (non-member) andthe original member images. Consequently, our dataset comprises an equal number of memberimages from the LAION-CCS dataset and non-member images generated by DALLE, allowing us toevaluate MIA performance comprehensively. We have 592 images in VL-MIA/DALL-E in total. VL-MIA/Flickr. MS COCO co-occurs as a widely used dataset in the training data of the targetmodels, so we use the images in this dataset as member data. Given the fact that such member dataare collected from Flickr4, we filter Flickr photos by the year of upload and obtain new photos fromJanuary 1, 2024, as non-member data, which are later than the release of the target models. Weadditionally prepare a set of corrupted versions, where the member images are deliberately corrupted",
  "to simulate real-world settings. More results of the corrupted versions are discussed in .5.This dataset contains 600 images": "VL-MIA/Text. We prepare text MIA datasets for the VLLMs instruction-tuning stage. LLaVA 1.5 andLLaMA Adapter v2.1 both use the LLaVA-Instruct-150K in instruction-tuning, which consists ofmulti-round QA conversations. We first select entries with descriptive answers of 64 words. Next, wefeed the corresponding questions and images into GPT-43, and ask GPT-4 to generate responses of thesame length, treating these generated responses as non-member text data. In addition, since MiniGPT-4 employs long descriptions of images for instruction-tuning, typically beginning with the image,we prompt GPT to generate descriptions based on the MS COCO dataset, starting with this imageto ensure similar data distributions. We also prepare different versions of the datasets by truncatingthe text into different word lengths such as 16, 32, and 64. Each text dataset contains 600 samples. VL-MIA/Synthetic We synthesize two new MIA datasets:VL-MIA/Geometry and VL-MIA/Password. The image in the VL-MIA/Geometry consists of a random 4x4 arrangementof geometrical shapes, and the image in the VL-MIA/Password consists of a random 6x6 arrangementof characters and digits from MNIST and EMINST . The associated text for each imagerepresents its content (e.g., specific characters, colors, or shapes), ordered from left to right andtop to bottom. Half of the dataset can be selected as the member set for VLLM fine-tuning, withthe remainder as the non-member set. This partition ensures that members and non-members areindependently and identically distributed, avoiding the latent distribution shift between the membersand non-members in current MIA datasets . Our synthetic datasets are ready to use, and can beapplied to evaluate any VLLM MIA methods through straightforward fine-tuning. We provide someexamples of this dataset in in the Appendix C.",
  "A cross-modal pipeline to detect image": "VLLMs such as LLaVA and MiniGPT project the vision encoders embedding of the image intothe feature space of LLM. However, a major challenge for image MIA is that we do not have theground-truth image tokens. Only the embeddings of images are available, which prevents directlytransferring many target-based MIA from languages to images. To this end, we propose a token-levelimage MIA which calculates metrics based on the output logit of each token position. This pipeline consists of two stages, as demonstrated in . In generation stage, we providethe model with an image followed by an instruction to generate a textual sequence. Subsequently,in inference stage, we feed the model with the concatenation of the same image, instruction, andgenerated description text. During the attack, we correspondingly slice the output logits into image,instruction, and description segments, which we use to compute various metrics for MIAs. Ourpipeline considers the information from the image, the instructions and the descriptions followingthe image. In practice, even if there is no access to the logits of the image feature and instructionslice, we can still detect the member image solely from the model generation. We visually describe aprompt example with different slice notations presented in Appendix A.2. Our pipeline operates on the principle that VLLMs responses always follow the instructionprompt , where the images usually precede the instructions and then always precede the descrip-tions. For causal language models used in VLLMs that predict the probability of the next token basedon the past history , the logits at text tokens in the sequence inherently incorporate informationfrom the preceding image.",
  "H1(p) = j pj log pj, H(p) = log max pj": "To be more specific, given a token sequence X := (x1, x2, . . . , xL), let p(i)() = P(|x1, , xi) bethe probability of next-token distribution at the i-th token. Let Max-K%(X) be the top K% from thesequence X with the largest Rnyi entropies, the MaxRnyi-K% score of X equals",
  "In our experiments, we vary = 1": "2, 1, 2, and +; K = 0, 10, 100. As increases, the top percentileof distribution p will have more influence on H(p). When = 1, H1(p) equals the Shannonentropy , and our method at K = 100 is equivalent to the Entropy . When = , weconsider the most likely next token probability . In contrast, Min-K% only deals with thetarget next token probability. When the sequence is generated by the target model deterministically,i.e., when the model always generates the most likely next token, our MaxRnyi-K% at = isequivalent to the Min-K%.",
  "j(pj) 1, 0 < < , = 1. H(p) is": "also further defined at = 1, as H1(p) = lim1 H(p) = H1(p). Assuming the next token ID isy, recall that a small entropy value or a large py value indicates membership, we want our modifiedentropy to be monotonically decreasing on py and monotonically increasing on pj, j = y. Therefore,we propose the modified Rnyi entropy on a given next token ID y, denoted by H(p, y):",
  "Let 1, we have H1(p, y) = lim1 H(p, y) =": "j=y pj log(1 pj) (1 py) log py,which is equivalent to the Modified Entropy . In addition, our more general method does notencounter numerical instability in Modified Entropy as pj 0, 1 at = 1. For simplicity, we letthe ModRnyi score be the averaged modified Rnyi entropy of the sequence.",
  "Experiments": "In this section, we conduct MIAs across three target models using various baselines, MaxRnyi-K%,and ModRnyi. Experiment setup is provided in .1. The results on text MIAs and imageMIAs are present in .2 and .3, respectively. In .4, we show that theproposed MIA pipeline can also be used in GPT-4. Ablation studies are present in .5. Theversions and base models of VLLMs we use are listed in of the appendices.",
  "Experimental setup": "Evaluation metric. We evaluate different MIA methods by their AUC scores. AUC score is the areaunder the receiver operating characteristic (ROC) curve, which measures the overall performance ofa classification model in all classification thresholds . The higher the AUC score, the more effectivethe attack is. In addition to the average-case metric AUC, we also include the worst-case metric, theTrue Positive Rate at 5% False Positive Rate (TPR@5%FPR) in Appendix D suggested by . Baselines. We take existing metric-based MIA methods as baselines and conduct experiments on ourbenchmark. We use the MIA method from , which compares the feature vectors produced by theoriginal image with the augmented image. We use KL-divergence to compare the logit distributionsand term it Aug-KL in this paper. We also use Loss attack , which is perplexity in the caseof language models. Furthermore, we consider ppl/zlib and ppl/lowercase , which comparethe target perplexity to zlib compression entropy and the perplexity of lowercase texts respectively. proposes Min-K% method, which calculates the smallest K% probabilities corresponding to the ground truth token. Min-K% is currently a state-of-the-art method to detect pre-training data of LLMs.For both Min-K% and MaxRnyi-K%, we vary K = 0, 10, 100. In addition, we consider K = 20 forMin-K% as suggested in . We further include the max_prob_gap metric that can represent theextreme confidence in certain tokens by the model. That is, we subtract the second largest probabilityfrom the maximum probability in each token position and calculate the mean as metric.",
  "Image MIA": "We first conduct MIAs on images using VL-MIA/Flickr and VL-MIA/DALL-E in three VLLMs.For the image slice, it is not possible to perform target-based MIAs, because of the absence ofground-truth token IDs for the image. However, our MIA pipeline presented in can stillhandle target-based metrics by accessing the instruction slice and description slice. As demonstrated in , MaxRnyi-K% surpasses other baselines in most scenarios. An valueof 0.5 yields the best performance in both VL-MIA/Flickr and VL-MIA/DALL-E. As increases,performance becomes erratic and generally deteriorates, though it remains superior to all target-basedmetrics. Overall, target-free metrics outperform target-based metrics for image MIAs. Anotherinteresting observation is that instruction slices result in unstable AUC values, sometimes fallingbelow 0.5 in target-based MIAs. This can be partially explained by the fact the model is more familiarwith the member data. As a result, after encountering the first word Describe, the model is moreinclined to generate the description directly than generating the following instruction of Xins, i.e.,this image in detail. This is an interesting phenomenon that we leave to future research. The performance of the image MIA model is influenced by its training pipelines. Recall that MiniGPT-4 only updates the parameters of the image projection layer in image training, and LLaMA Adapterv2 applies parameter-efficient fine-tuning approaches. In contrast, LLaVA 1.5 training updates boththe parameters of the projection layer and the LLM. The inferior performance of MIAs on MiniGPT-4and LLaMA Adapter compared to LLaVA 1.5 is therefore consistent with that more parametersupdates make it easier to memorize training data. We find that VL-MIA/DALL-E is a more challenging dataset than VL-MIA/Flickr, reflected in theAUC being closer to 0.5. In VL-MIA/DALL-E, each non-member image is generated based on thedescription of a member image. Therefore, member data have a one-to-one correspondence withnon-member data and depict a similar topic, which makes it harder to discern.",
  "Rnyi ( = )Max_0%0.6000.6380.6070.6150.6880.669Max_10%0.6180.7630.5860.5480.6270.667Max_100%0.5570.6940.5460.5270.5840.634": "Text member data might be usedin different stages of VLLM train-ing, including the base LLM modelpre-training and the later VLLMinstruction-tuning. We hypothesizethat after the last usage of the mem-ber data in its training, the more themodel changes, the better the target-free MIA methods compared to target-based ones, and vice-versa.Theheuristic is that if the models param-eters have changed a lot, target-freeMIA methods, which use the wholedistribution to compute statistics, aremore robust than target-based meth-ods, which rely on the probability atthe next token ID. On the other hand,if the member data are seen in recentfine-tuning, the next token will conveymore causal relations in the sequenceremembered by the model, and thustarget-based ones are better. : Image MIA. AUC results on VL-MIA under our pipeline. img indicates the logitsslice corresponding to image embedding, inst indicates the instruction slice, desp the generateddescription slice, and inst+desp is the concatenation of the instruction slice and description slice.We use an asterisk in superscript to indicate the target-based metric. Bold indicates the best AUCwithin each column and underline indicates the runner-up.",
  "Rnyi ( = )Max_0%0.6230.5590.5590.5670.5340.3860.4940.4390.4610.5540.460Max_10%0.6990.5590.5630.5800.5550.3860.4950.4160.5100.5560.515Max_100%0.5450.5870.5640.5770.5500.3940.5170.4730.5060.5770.530": "MIA on VLLM instruction-tuning texts We detect whether individual description texts appearin the VLLMs instruction-tuning. We use the description text dataset VL-MIA/Text of lengths (32,64), constructed in . We present our results in the first column of . We observe thattarget-based MIA methods are significantly better than target-free ones, confirming our hypothesis. MIA on LLM pre-training texts. We use the WikiMIA benchmark , which leverages theWikipedia timestamp to separate the early Wiki data as the member data, and recent Wiki data as thenon-member data. The early Wiki data are used in various LLMs pre-training . We use WikiMIAof different lengths (32, 64, 128, 256), and expect the membership of longer sequences will be moreeasily identified. We present our results in the second column of . We observe that on LLaVA,our target-free MIA methods on large consistently outperform target-based MIA methods, which",
  "Rnyi ( = )Max_0%0.6850.561Max_10%0.7080.549Max_100%0.7810.583": "In this section, we demonstrate the feasibility of imageMIAs on the closed-source model GPT-4. Our experi-ments use two image datasets: VL-MIA/Flickr and VL-MIA/DALL-E, detailed in . We choose GPT-4-vision-preview API, which was trained in 2023 and likelydoes not see the member data in either dataset. We ran-domly select 200 images per dataset and prompt GPT-4 todescribe them in 64 words. We then apply MIAs based onthe generated descriptions. Since GPT-4 can only providethe top-five probabilities at each token position, we cannot directly use the proposed MaxRnyi-K% that requiresthe whole probability distribution. To address this issue,we assume the size of the entire token set is 32000 and theprobability of the remaining 31995 tokens are uniformlydistributed. The AUC results are present in . Weomit the result of perplexity and Min-K% since they areequivalent to MaxRnyi-K% with = in the greedy-generated setting, as discussed in .2. Surprisingly,we observe that in VL-MIA/DALL-E, the best-performedmethod MaxRnyi-K% ( = 0.5) can achieve an AUC of 0.815. This indicates a high level ofeffectiveness for MIAs on GPT-4, demonstrating the potential risks of privacy leakage even withclosed-source models.",
  "Ablation study": "Does the length of description affect the image MIA performance? We conduct ablation exper-iments on LLaVA 1.5 targeting the length of generated description texts with MaxRnyi-10%. Inthe generation stage, we restrict the max_new_tokens parameter of the generate function to (32,64, 128, 256) to obtain description slices of different lengths. As presented in a, when thelength of the description increases, the AUC of the MIA becomes higher and enters a plateau whenmax_new_tokens reaches 128. This may be because a shorter text contains insufficient informationabout the image, and in an excessively long text, words generated later tend to be more generic andnot closely related to the image, thereby contributing less to the discriminative information that helpsdiscern the membership. Can we still detect corrupted member images? The motivation is to detect whether sensitive imagesare inappropriately used in VLLMs training even when the images at hand may get corrupted. Weleverage ImageNet-C to generate corrupted versions of member data in VL-MIA/Flickr: Snow,Brightness, JPEG, and Motion_Blur, with the parameters in . The corrupted examples andcorresponding model output generations are demonstrated in Appendix C and . Wetake MaxRnyi-K% ( = 0.5) as the attacker and the results of LLaVA are presented in b.Corrupted member images make MIAs more difficult, but can still be detected successfully. We alsoobserve that reducing model quality (JPEG) or adding blur (Motion_Blur) degrade MIA performancemore than changing the base parameter (Brightness) or overlaying texture (Snow). Can we use different instructions? We conduct image MIAs on VL-MIA/Flickr with LLaVAthrough three different instruction texts: Describe this image concisely., Please introduce thispainting., and Tell me about this image.. We present our results in of the appendix. Ourpipeline successfully detects member images on every instruction, which indicates robustness acrossdifferent instruction texts. max_new_tokens 0.60 0.62 0.64 0.66 0.68 0.70 0.72 AUC",
  "(b) MaxRnyi-K% on corrupted images": ": Ablation study (a) on max_new_tokens with MaxRnyi-10%. Allowing VLLMs togenerate longer descriptions can increase the AUC of desp slices, but we encounter a plateau whenmax_new_tokens equals 128. (b) on image MIAs against corrupted versions of VL-MIA/Flickr withMaxRnyi-K% ( = 0.5). Three levels of corruption are applied to the images: Marginal, Moderate,and Severe. The dotted line indicates the AUC on raw images without corruption.",
  "Conclusion": "In this work, we take an initial step towards detecting training data in VLLMs. Specifically, weconstruct a comprehensive dataset to perform MIAs on both image and text modalities. Additionally,we uncover a new pipeline for conducting MIA on VLLMs cross-modally and propose a novel methodbased on Rnyi entropy. We believe that our work paves the way for advancing MIA techniques and,consequently, enhancing privacy protection in large foundation models.",
  "Acknowledgements": "This work was carried out when Zhan Li and Yihang Chen were interns in the EPFL LIONS group.This work was supported by Hasler Foundation Program: Hasler Responsible AI (project number21043). This research was sponsored by the Army Research Office and was accomplished under GrantNumber W911NF-24-1-0048. This work was supported by the Swiss National Science Foundation(SNSF) under grant number 200021_205011. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia LeoniAleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4technical report. arXiv preprint arXiv:2303.08774, 2023. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, ChangZhou, and Jingren Zhou. Qwen-vl: A frontier large vision-language model with versatileabilities. arXiv preprint arXiv:2308.12966, 2023. Nicholas Carlini, Chang Liu, lfar Erlingsson, Jernej Kos, and Dawn Song. The secret sharer:Evaluating and testing unintended memorization in neural networks. In 28th USENIX securitysymposium (USENIX security 19), pages 267284, 2019. Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Kather-ine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, et al. Extracting trainingdata from large language models. In 30th USENIX Security Symposium (USENIX Security 21),pages 26332650, 2021. Nicholas Carlini, Steve Chien, Milad Nasr, Shuang Song, Andreas Terzis, and Florian Tramer.Membership inference attacks from first principles. In 2022 IEEE Symposium on Security andPrivacy (SP), pages 18971914. IEEE, 2022. Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushingweb-scale image-text pre-training to recognize long-tail visual concepts. In Proceedings of theIEEE/CVF conference on computer vision and pattern recognition, pages 35583568, 2021. Dingfan Chen, Ning Yu, Yang Zhang, and Mario Fritz. Gan-leaks: A taxonomy of member-ship inference attacks against generative models. In Proceedings of the 2020 ACM SIGSACconference on computer and communications security, pages 343362, 2020. Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechu Liu, Pengchuan Zhang, RaghuramanKrishnamoorthi, Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny. Minigpt-v2: largelanguage model as a unified interface for vision-language multi-task learning. arXiv preprintarXiv:2310.09478, 2023. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source chatbotimpressing gpt-4 with 90%* chatgpt quality. See lmsys. org (accessed 14 April2023), 2(3):6, 2023. Christopher A Choquette-Choo, Florian Tramer, Nicholas Carlini, and Nicolas Papernot. Label-only membership inference attacks. In International conference on machine learning, pages19641974. PMLR, 2021. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, AdamRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1113, 2023. Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre Van Schaik. Emnist: Extendingmnist to handwritten letters. In 2017 international joint conference on neural networks (IJCNN),pages 29212926. IEEE, 2017. Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, WeishengWang, Boyang Li, Pascale N Fung, and Steven Hoi. Instructblip: Towards general-purposevision-language models with instruction tuning. Advances in Neural Information ProcessingSystems, 36, 2024.",
  "Li Deng. The mnist database of handwritten digit images for machine learning research. IEEESignal Processing Magazine, 29(6):141142, 2012": "Michael Duan, Anshuman Suri, Niloofar Mireshghallah, Sewon Min, Weijia Shi, Luke Zettle-moyer, Yulia Tsvetkov, Yejin Choi, David Evans, and Hannaneh Hajishirzi. Do membershipinference attacks work on large language models? arXiv preprint arXiv:2402.07841, 2024. Wenjie Fu, Huandong Wang, Chen Gao, Guanghua Liu, Yong Li, and Tao Jiang. Practical mem-bership inference attacks against fine-tuned large language models via self-prompt calibration.arXiv preprint arXiv:2311.06062, 2023. Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu,Conghui He, Xiangyu Yue, Hongsheng Li, and Yu Qiao. Llama-adapter v2: Parameter-efficientvisual instruction model. arXiv preprint arXiv:2304.15010, 2023.",
  "Nikhil Kandpal, Eric Wallace, and Colin Raffel. Deduplicating training data mitigates privacyrisks in language models. In International Conference on Machine Learning, pages 1069710707. PMLR, 2022": "Ronald Kemker, Marc McClure, Angelina Abitino, Tyler Hayes, and Christopher Kanan.Measuring catastrophic forgetting in neural networks. In Proceedings of the AAAI conferenceon artificial intelligence, volume 32, 2018. James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins,Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al.Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy ofsciences, 114(13):35213526, 2017. Myeongseob Ko, Ming Jin, Chenguang Wang, and Ruoxi Jia. Practical membership inferenceattacks against large-scale multi-modal models: A pilot study. In Proceedings of the IEEE/CVFInternational Conference on Computer Vision, pages 48714881, 2023.",
  "Robert Konig, Renato Renner, and Christian Schaffner. The operational meaning of min-andmax-entropy. IEEE Transactions on Information theory, 55(9):43374347, 2009": "Klas Leino and Matt Fredrikson. Stolen memories: Leveraging model memorization forcalibrated {White-Box} membership inference. In 29th USENIX security symposium (USENIXSecurity 20), pages 16051622, 2020. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-imagepre-training with frozen image encoders and large language models. In International conferenceon machine learning, pages 1973019742. PMLR, 2023. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, PiotrDollr, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ComputerVisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014,Proceedings, Part V 13, pages 740755. Springer, 2014.",
  "Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong JaeLee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024. URL": "Hongbin Liu, Jinyuan Jia, Wenjie Qu, and Neil Zhenqiang Gong. Encodermi: Membershipinference against pre-trained encoders in contrastive learning. In Proceedings of the 2021 ACMSIGSAC Conference on Computer and Communications Security, pages 20812095, 2021. Yunhui Long, Vincent Bindschaedler, Lei Wang, Diyue Bu, Xiaofeng Wang, Haixu Tang, Carl AGunter, and Kai Chen. Understanding membership inferences on well-generalized learningmodels. arXiv preprint arXiv:1802.04889, 2018. Gen Luo, Yiyi Zhou, Tianhe Ren, Shengxin Chen, Xiaoshuai Sun, and Rongrong Ji. Cheapand quick: Efficient vision-language instruction tuning for large language models. Advances inNeural Information Processing Systems, 36, 2024.",
  "Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving languageunderstanding by generative pre-training. OpenAI, 2018": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visualmodels from natural language supervision. In International conference on machine learning,pages 87488763. PMLR, 2021. Alfrd Rnyi. On measures of entropy and information. In Proceedings of the fourth Berkeleysymposium on mathematical statistics and probability, volume 1: contributions to the theory ofstatistics, volume 4, pages 547562. University of California Press, 1961. Ahmed Salem, Yang Zhang, Mathias Humbert, Pascal Berrang, Mario Fritz, and MichaelBackes. Ml-leaks: Model and data independent membership inference attacks and defenses onmachine learning models. arXiv preprint arXiv:1806.01246, 2018. Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, ClaytonMullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Opendataset of clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021.",
  "Virat Shejwalkar, Huseyin A Inan, Amir Houmansadr, and Robert Sim. Membership inferenceattacks against nlp classification models. In NeurIPS 2021 Workshop Privacy in MachineLearning, 2021": "Weijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao Liu, Terra Blevins,Danqi Chen, and Luke Zettlemoyer. Detecting pretraining data from large language models.In The Twelfth International Conference on Learning Representations, 2024. URL Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inferenceattacks against machine learning models. In 2017 IEEE symposium on security and privacy(SP), pages 318. IEEE, 2017.",
  "Congzheng Song and Ananth Raghunathan. Information leakage in embedding models. InProceedings of the 2020 ACM SIGSAC conference on computer and communications security,pages 377390, 2020": "Congzheng Song and Vitaly Shmatikov. Auditing data provenance in text-generation models.In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery& Data Mining, pages 196206, 2019. Congzheng Song, Thomas Ristenpart, and Vitaly Shmatikov. Machine learning models thatremember too much. In Proceedings of the 2017 ACM SIGSAC Conference on computer andcommunications security, pages 587601, 2017.",
  "Liwei Song, Reza Shokri, and Prateek Mittal. Membership inference attacks against adversari-ally robust deep learning models. In 2019 IEEE Security and Privacy Workshops (SPW), pages5056. IEEE, 2019": "Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu,Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highlycapable multimodal models. arXiv preprint arXiv:2312.11805, 2023. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Openfoundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. Jiayuan Ye, Aadyaa Maddi, Sasi Kumar Murakonda, Vincent Bindschaedler, and Reza Shokri.Enhanced membership inference attacks against machine learning models. In Proceedings of the2022 ACM SIGSAC Conference on Computer and Communications Security, pages 30933106,2022. Samuel Yeom, Irene Giacomelli, Matt Fredrikson, and Somesh Jha. Privacy risk in machinelearning: Analyzing the connection to overfitting. In 2018 IEEE 31st computer securityfoundations symposium (CSF), pages 268282. IEEE, 2018.",
  "Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understandingdeep learning (still) requires rethinking generalization. Communications of the ACM, 64(3):107115, 2021": "Pan Zhang, Xiaoyi Dong Bin Wang, Yuhang Cao, Chao Xu, Linke Ouyang, Zhiyuan Zhao,Shuangrui Ding, Songyang Zhang, Haodong Duan, Hang Yan, et al. Internlm-xcomposer: Avision-language large model for advanced text-image comprehension and composition. arXivpreprint arXiv:2309.15112, 2023. Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun Zhou, Xiangfei Hu, Shilin Yan, PanLu, Hongsheng Li, and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models withzero-init attention. arXiv preprint arXiv:2303.16199, 2023. Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: En-hancing vision-language understanding with advanced large language models. In The TwelfthInternational Conference on Learning Representations, 2023.",
  ": A schematic of VLLM": "As shown in , the VLLMs consist of a vision encoder, a text tokenizer, and a language model.The output of the vision encoder (image embedding) have the same embedding dimension d as thetext token embedding. When feeding the VLLMs with an image and the instruction, a vision encoder transforms thisimage to L1 hidden embeddings of dimension d, denoted by Eimage RdL1. The text tokenizerfirst tokenizes the instruction into L2 tokens, and then looks up the embedding matrix to get itsd-dimensional embedding Eins RdL2. The image embedding and the instruction embeddingare concatenated as Eimgins = (Eimage, Eins) Rd(L1+L2), which are then fed into a languagemodel to perform next token prediction. The cross-entropy loss (CE loss) is calculated based on thepredicted token id and the ground truth token id on the text tokens. We can see that in this process, image embeddings are directly obtained from the vision encoder andthere are no image tokens. There are no causal relations between consecutive image embeddings aswell. Threfore, as we stated in , target-based MIA that requires token ids cannot be directlyapplied.",
  "The versions, base models and training datasets of target VLLMs are listed in . We runexperiments on a single NVIDIA A100 80GB GPU, where the image MIA costs less than 2 hours forone model": "In our experiments on description text MIA, we do not detect the member text during the VLLMimage-text pre-training stage (the 3rd row) because the captions used in pre-training are usually fewerthan 10 words and do not contain sufficient information. Therefore, we only detect member textduring the VLLM instruction tuning stage.",
  "B.1Text MIA: Complete results": "In addition to .3, we present our complete Text MIA results on LLaVA, MiniGPT-4, andLLaMA-Adapter in and . We find that no current method can effectively detect LLMpre-training text corpus from MiniGPT-4, even the AUC on 256-length Wiki text is only 0.6. Apartfrom that, we note that on the LLaMA-Adapter, target-based methods perform similarly or better thantarget-free ones. In the following, we explain this phenomenon by examining the training pipelinesof two different models. The wiki data is used in the pre-training stage of LLaMA 7b, which is further fine-tuned to LLaMA 7bChat. The base MML of LLAVA is fine-tuned from Vicuna 7b v1.5, which is further fine-tuned fromLLaMA 7b Chat. Therefore, the LLM model of LLAVA has changed a lot since wiki datas last usage,and target-free methods are preferable here. In contrast, LLaMA-Adapter is parameter-efficientlyfine-tuned from LLaMA 7b, and the models parameters have not changed much since the wikisdata last usage, and the target-based MIA methods can still retain their performance. In addition, to",
  "B.2Abalation on MaxRnyi and MinRnyi": "Given a sequence of entropies calculated on each position, it would be natural to ask whether thehigher percentile or lower percentile should be used to determine the score of the whole sequence forMIA. In the main paper, we use max, which coincides with Min-K% under some special cases.We provide more evidence of the advantages of max or min. We define MinRnyi-K% as similar toMaxRnyi-K%, but consider the smallest K% entropies in the sequence X. We conduct the pre-training text MIA on LLAVA. We also use the WiKiMIA benchmark of differ-ent lengths (32, 64, 128, 256). We set K = 0, 10, 20, since when K = 100, MaxRnyi-K% coincideswith MinRnyi-K%. We observe that usually MinRnyi-K% is slightly inferior to MaxRnyi-K%.Therefore, we adopt MaxRnyi-K% in our main paper.",
  "Max_100%0.7020.7260.7130.7280.7020.6090.7000.7040.7020.7090.7080.726": "Rnyi ( = 1)Max_0%0.5030.7080.6850.7250.5030.6190.6490.6430.5030.6140.6490.635Max_10%0.6230.7080.6980.7430.6230.6190.6700.7070.6230.6140.6710.699Max_100%0.7020.7200.7020.7210.7020.6130.6930.7020.7020.6630.6960.714 Rnyi ( = 2)Max_0%0.5830.6820.6730.7050.5830.5840.6370.6690.5830.5850.6300.619Max_10%0.6210.6820.6850.7250.6210.5840.6600.6700.6210.5850.6550.672Max_100%0.6820.6940.6830.7030.6820.5710.6780.6810.6820.6240.6760.690 Rnyi ( = )Max_0%0.5880.6460.6510.6740.5880.6360.6290.6660.5880.5780.6220.603Max_10%0.5930.6460.6690.6990.5930.6360.6560.6730.5930.5780.6460.657Max_100%0.6690.6730.6670.6870.6690.5390.6710.6670.6690.6080.6620.675 The VLLMs consist of a vision encoder, a text tokenizer, and a language model. The vision encoderand the text tokenizer have the same embedding dimension d. When feeding the VLLMs withan image and the instruction, a vision encoder transforms this image to L1 hidden embeddings ofdimension d, denoted by eimage RdL1. The text tokenizer first tokenizes the instruction into L2tokens, and then looks up the embedding matrix to get its d-dimensional embedding eins RdL2.The image embedding and the instruction embedding are concatenated as eimgins = (eimage, eins) Rd(L1+L2), which are then fed into a language model to generate a description that has L3 tokens.The cross-entropy loss (CE loss) is calculated based on the predicted token id and the ground truthtoken id on the instruction and description tokens. We can see that in this process, image embeddings are directly obtained from the vision encoder andthere are no image tokens. Threfore, as we stated in , target-based MIAs that require tokenids cannot be directly applied. Furthermore, given the ouput logits of the shape (L1 + L2 + L3) |V|, where V is the vocabularyset, we can access the logits of the image by the slice [0 : L1], the logits of instruction by the slice[L1 : L1 + L2], and the logits of description by the slice [L1 + L2 : L1 + L2 + L3].",
  "FBroader impacts": "In this paper, we present the first multi-modalities MIA benchmark for VLLMs, and propose a novelmetric MaxRnyi-K% for MIA. We recognize that our research has significant implications for thesafety and ethics of VLLMs and may lead to targeted MIA defense by developers. Nevertheless, ourfindings provide valuable insights into data contamination, which could contribute to the training datasplit. Additionally, our method empowers individuals to detect their private data within the trainingdataset, which is essential for ensuring data security. We believe our work can raise awareness aboutthe importance of privacy protection in multi-modal language models.",
  "GLimitation": "The first limitation of this work is that the best methods on the pre-training dataset can only achievean AUC of 0.688. This is because the detection of pre-training data will become more challenging asthe VLLMs are further fine-tuned. We believe increasing the performance for detecting pre-trainingdata would be promising in the future. Secondly, the proposed MaxRnyi-K% method requires accessto the full probability distribution over the predicted tokens. Although we have demonstrated the : Image MIA. TPR at 5% FPR results on VL-MIA under our pipeline. img indicatesthe logits slice corresponding to image embedding, inst indicates the instruction slice, despthe generated description slice, and inst+desp is the concatenation of the instruction slice anddescription slice. We use an asterisk in superscript to indicate the target-based metric. Boldindicates the best AUC within each column and underline indicates the runner-up.",
  "CorruptionGenerated output": "NoneThe image features a large, bright orange truck with a black front end, parked ona dirt road. The truck has a distinctive appearance, as it is equipped with massivetires, giving it a monster truck-like look. The truck is positioned in the middle of BrightnessThe image features a large, red four-wheel-drive truck with a black front end,parked on a dirt road. The truck is equipped with massive tires, giving it adistinctive appearance. The trucks front end is adorned with a Ford logo, further Motion_BlurThe image features a large, orange four-wheel-drive truck driving down a dirtroad. The truck is the main focus of the scene, occupying a significant portion ofthe image. The truck appears to be in motion, possibly on a trail or a dirt road. SnowThe image features a large, red pickup truck parked on a dirt road. The truckis equipped with massive tires, giving it a distinctive appearance. The truck ispositioned in the middle of the scene, with the front end facing the viewer. JPEGThe image features a large, bright orange truck with a black front end, parkedon a dirt road. The truck is equipped with massive tires, giving it a monster-likeappearance. The truck is positioned in the center of the scene, with the front endfacing the vie"
}