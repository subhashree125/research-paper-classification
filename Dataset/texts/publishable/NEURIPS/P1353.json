{
  "Abstract": "Three-dimensional (3D) understanding of objects and scenes play a key role inhumans ability to interact with the world and has been an active area of research incomputer vision, graphics, and robotics. Large scale synthetic and object-centric 3Ddatasets have shown to be effective in training models that have 3D understandingof objects. However, applying a similar approach to real-world objects and scenesis difficult due to a lack of large-scale data. Videos are a potential source for real-world 3D data, but finding diverse yet corresponding views of the same contenthas shown to be difficult at scale. Furthermore, standard videos come with fixedviewpoints, determined at the time of capture. This restricts the ability to accessscenes from a variety of more diverse and potentially useful perspectives. We arguethat large scale 360 videos can address these limitations to provide: scalablecorresponding frames from diverse views. In this paper, we introduce 360-1M,a 360 video dataset, and a process for efficiently finding corresponding framesfrom diverse viewpoints at scale. We train our diffusion-based model, ODIN1, on360-1M. Empowered by the largest real-world, multi-view dataset to date, ODIN isable to freely generate novel views of real-world scenes. Unlike previous methods,ODIN can move the camera through the environment, enabling the model to inferthe geometry and layout of the scene. Additionally, we show improved performanceon standard novel view synthesis and 3D reconstruction benchmarks. See webpageand code.",
  "Introduction": "Humans have the ability to understand and reason about the 3D geometry of the world, which is keyfor everyday tasks such as navigation and object manipulation . In machine learning,3D perception and reasoning has been a long-standing goal for researchers with broad applicationsin robotics , vision , and graphics . Fueled by large-scale datasets ofsynthetic objects , recent generative models have shown impressive understanding of 3Dobjects . While these models ability to generate synthetic objects is impressive, enabling3D generative models for real world scenes and objects remains an open challenge. One intuitive source for scalable data has been video as it implicitly contains rich information aboutthe 3D world. However, learning 3D modeling from video has been elusive despite impressiveeffort . The key problem has been how to consistently transform video into a",
  "Starting Input View": ": By learning from the largest real-world, multi-view dataset to date, our model ODIN,can synthesize novel views of rich scenes from a single input image with free camera movementthroughout the scene. We can then reconstruct the 3D scene geometry from these geometricallyconsistent generations. form amenable to learning about the 3D world. Existing 3D models learn from multi-view data, acollection of images of scenes or objects and their respective camera pose. Creating such multi-viewdatasets from video requires finding sets of corresponding frames that capture similar parts of thescene but from different locations (). This search for corresponding frames in video has proven difficult at scale for a few reasons. First,correspondences are sparsely distributed throughout the video because the trajectory of the camerais fixed at the time of capture. Ideally, the camera operator would focus on a specific object orportion of the scene while moving around it. However, in-the-wild videos are far from this ideal. Forexample, if a person records themselves walking in the park, it is rare that they consistently focus thecamera on the same object such as a bench as they walk towards, past, and away from it. Second,the computational cost of checking whether frames form a correspondence is expensive ,therefore searching extensively is infeasible. Given these limitations, the largest real-world multi-viewdatasets to date utilize Amazon Mechanical Turkers to manually record video clips of objects,and are limited to 50 and 238 object categories respectively. To address these limitations, we collect one million 360 videos from YouTube, introduce a processto efficiently transform 360 video into multi-view data, and train a diffusion-based novel-viewsynthesis (NVS) model on the dataset. Our model named ODIN, is the first to reasonably synthesizereal-world 3D scenes and reconstruct their geometry conditioned on a single image. Quantitativelywe evaluate our method on standard novel view synthesis benchmarks (DTU and MipNeRF360) andfind improved performance compared to existing models without fine-tuning our own. Additionally,we compare ODIN to existing methods for 3D reconstruction on Google Scanned Objects as wellas a held-out set of 360-1M and show significantly improved performance, especially on complexreal-world scenes. We will open-source our model and dataset.",
  "Related Work": "Novel View Synthesis.NeRF optimizes a volumetric scene function using sparse 2D images,representing the scene as a continuous 5D function. MipNeRF extends NeRF with a multi-scalerepresentation to enhance detail and reduce aliasing. Plenoctree combines NeRF principles with an octree structure for efficient rendering. DIVeR proposes a deterministic volumetric renderingfor NeRF. Gaussian Splatting uses Gaussian functions and splatting techniques for detailedscene representation and rendering. Unlike these methods which rely on densely sampled multi-viewimages and known camera poses, our approach captures extensive real-world scenes from widelyvarying camera views. PixelNeRF and DietNeRF extend NeRF to handle sparse input viewsbut only for controlled settings. Recent works leverage powerful generative (diffusion) models for novel view synthesis ofobjects , and more recently for scenes . ZeroNVS uses a 3D-aware diffusionmodel with novel camera conditioning to generate 360-degree views from a single image, focusingon depth-scale ambiguity and background diversity with synthetic and real-world datasets. Diffusionwith Forward Models integrates a forward model into the diffusion process for unsupervisedtraining on partial observations, solving inverse problems like view synthesis without direct signalsupervision. ReconFusion combines NeRFs with diffusion priors to enhance 3D reconstructionfrom limited views, improving geometry and texture plausibility with real and synthetic multi-view datasets. LucidDreamer and RealmDreamer use a multi-step pipeline involving pointcloud guidance and Gaussian splats to generate detailed 3D scenes from text or image prompts butlacks physical realism and has limited control over viewpoint changes. In contrast, our methodleverages a large-scale collection of 360-degree YouTube videos to train a diffusion-based model,enabling the synthesis of diverse real-world 3D scenes and reconstruction from a single image, thusaccommodating significant camera view changes and a broader range of scenarios. Camera Pose Estimation and Structure from Motion.Estimating camera pose and structure-from-motion (SfM) have a rich history in computer vision . Camera pose estimationconsists of estimating the 6 degrees of freedom of cameras from which images were taken and thecamera intrinsics. The process typically involves finding corresponding key-points between multipleimages of a scene, and using their apparent motion within the images to infer the 3D geometry ofthe scene and relative location of each camera. For multi-view datasets and novel view synthesis,works typically use COLMAP or a SLAM variant . We choose to use the recentmethod Dust3R as it is computationally faster and allows for as few as 2 images whereas mostSfM methods require dozens. This enables us to scan much more quickly through videos for framecorrespondences and create the large-scale dataset from 360 video. Multi-View Datasets.Existing multi-view datasets such as MVImageNet , CO3D ,RealEstate10K , ACID , Epic-Kitchens , MipNeRF-360 , and Epic-Fields providevaluable multi-view sequences of real-world scenes and objects but are often constrained by thespecific environments or objects they capture. MVImageNet is the largest multi-view dataset to datewith over 200,000 video clips captured of 238 object categories. Though this effort is impressive,using Mechanical Turkers to manually capture videos of objects is difficult to scale further and limitscontent diversity. In we show examples of correspondences within MVImageNet whichcan be compared to correspondences of 360-1M in . Large-scale 3D object datasets likeObjaverse , Objaverse-XL , and infinigen focus on detailed 3D object assets to generatesynthetic objects and scenes. Autonomous driving and 3D reconstruction datasets such as Kitti ,DTU , ShapeNet , and Google Scanned Objects offer multi-view data for specific taskslike driving scenarios, 3D modeling, and object classification. In contrast, our dataset leverages a large-scale collection of 360-degree YouTube videos, providing avastly more diverse and extensive source of real-world data. Our dataset accommodates significantcamera view changes and broader real-world applications, going beyond the constraints of controlledmulti-view datasets and specific domain focuses.",
  "Multi-View Data from 360 Video": "There are two key elements missing from current multi-view datasets: scale and real-world data.Various datasets and works have managed to make progress along these dimensionsindividually, however, no current datasets afford both aspects. The key challenge in collecting large-scale, multi-view datasets derives from the difficulty of findinghigh-quality frame correspondences, and estimating their relative poses. Existing structure-from-motion algorithms, such as COLMAP and HLOC , are slow and require many images of",
  "Scalable Correspondence Search": "There are two properties of corresponding video frames that are necessary for training novel viewsynthesis models: sufficiently differing viewpoints and overlapping content. In manually collectednovel view synthesis (NVS) datasets this is accomplished by taking a video while circling the object.Finding frames that fit these criteria from in-the-wild video is much more difficult. A major reason is that high-quality correspondences are sparsely distributed in standard videos. Forexample, someone taking a video while walking down the street often keeps their camera view facingtheir direction of travel. So while they may capture a parked car on the side of the road while walkingtowards it, they likely will not pan their camera to capture it from many angles while walking past oraway from it. Therefore, it is difficult to obtain paired images of the scenes or objects from distantlocations and diverse views at scale. One solution to this problem is to leverage 360 videos. The360 nature allows the views of frames to be rotated such that they contain overlapping content.Therefore given two frames that are close enough in spatial location, in theory we can align the viewsto look at the various regions of the scene to form multiple view correspondences. Now we describe how we operationalize this approach. We begin by sub-sampling frames of the360 video at r = 1 frame-per-second. We find empirically this to be a sufficiently fast frame rategiven the movement speed of the camera. The computation of the correspondence search scales withr2, therefore we judicially select the frame rate. Next we perform pairwise comparison betweenframes within a frame window of length, L = 20. We map the 360 panoramic frames using anequirectangular projection E(I, , ) where is the pitch, is the yaw, and I is the image. We mapthe panoramic image to four different views E(I, j /2, 0) for j {1, . . . , 4}. Thus a panoramicframe, Ft at time t produces four frames {Ft,0, Ft,/2, . . . , Ft,3/2}. We then pass all pairs withinthe time window to the Dust3r model which outputs relative pose estimate, P and confidencemap, C. We take the mean confidence over the spatial components of the confidence map with heighth and width w, c =1hw",
  "cC c, and filter out frames below threshold, = 4. A higher meanconfidence means that the frames must be overlapping as the model can accurately estimate the pose": "Once the correspondences have been found we refine the relative pose between them by performinggradient descent on the pitch and yaw of both equirectangular projections with respect to c. Intu-itively, we can think of this as rotating the cameras to maximize the overlap (). After allcorrespondences have been found, we discard pairs with relative translation less than .25 m becausethey provide minimal information for training the model.",
  "Correspondence Propagation": "Computing relative pose between frames, especially for video, has been computationally prohibitiveand a major bottle-neck for large-scale multi-view datasets . An exhaustive search betweenall frames of a video would incur a cost of s2r2 where s is the number of seconds, and r is the framerate. A common approach is to limit the search with a window of size L to reduce the cost to L2,however this limits the pairs to short-range correspondences. We propose a hybrid approach that enables finding long-range correspondences with limited additionalcompute. After the initial frames have been found as detailed in section 3.1, we create a graph inwhich the nodes are frames and an edge exists if two frames have correspondence. We then performthe same procedure outlined in section 3.1 for all connected frames in each sub-graph. Intuitively, iftwo frames share a corresponding third frame (connected in the graph)then the two are also likely toshare a correspondence.",
  "Resolving Scale Ambiguity": "Dust3R and other structure-from-motion methods output relative camera poses indimensionless quantities, therefore we need to calibrate them to a universal scale. We do so byfusing the depth map estimates, D, from an off-the-shelf depth estimator , with the point map,X Rhw3, predicted by Dust3R. A pointmap is a correspondence between each pixel (i, j) andthe point in 3D where the ray from pixel (i, j) intersects the scene. We anchor the dimensionlesspointmap to the depth math D by optimizing for a scale factor, , in the following equation:",
  "j=1|Cij(zij Dij)|,(1)": "where zij is the depth component of Xij and Cij is the confidence map output by Dust3R. Cij isclose to 0 for points which the model has high uncertainty and acts as a filter for points with poorestimates. We choose L1 distance to limit the effect of outliers. Once we recover this scale factor, wemultiply the translation of the estimated camera pose (R, t) by to obtain the metric pose estimate.",
  "Collecting 360 Video": "We collect all meta-data from YouTube in order to filter 360 videos. The meta-data providesinformation on duration, view count, format, and subject category among other fields. We filter forthe equirectangular format which indicates 360 video and results in 1,076,592 total videos. We thendownload the videos in the equirectangular format at the best quality available. We will release themeta-data for the 360 videos alongside the dataset. We filter the downloaded videos for empty, and duplicate videos. We remove duplicated videos witha deduplication model run on the thumbnails of the videos. This does not guarantee the contentsof the video are unique, however running over all frames is computationally infeasible.",
  "Dataset Statistics": "360-1M consists of 80,567,325 unique frames extracted from 1,076,592 videos with an averageof 74.83 unique frames per video. The average video length is 6.3 minutes and is distributed in along-tail fashion (). When searching for correspondences, we sample the videos at 1 FPS.The videos are distributed evenly across 15 subject categories, with the most popular category beingTravel and Events (149,534 videos) and the least popular being Pets and Animals (8802 videos)(). We find 363,417,730 total frame correspondences along with their relative camera poses.",
  "Zero 1-to-3": ": Qualitative comparison of novel view synthesis on real-world scenes. The left and rightimages are conditioned on camera views from the left and right respectively. In the middle scene ofthe kitchen, ODIN accurately models the geometry of the table counter and chairs as well as unseenparts of the scene such as the living room.",
  "Method": "Our final goal is to generate images along a viewpoint trajectory conditioned on a single image of ascene a task known as novel view synthesis (NVS). Note that our task differs from tradition novelview synthesis work such as which aim to generate novel views after training on many images ofa single scene. Similar to prior works , we leverage a diffusion-based model. This class ofmodels have shown impressive capabilities in learning priors from large-scale data. An alternative isa NeRF based approach which is mainly effective in small-scale settings.",
  "Viewpoint Conditioned Diffusion": "Given a single image, x Rhw3, of a scene, our objective is to generate a sequence of images,xi from different viewpoints, (R, t) where R is the relative rotation and t is the translation betweenviews. Following , we use a latent diffusion architecture which consists of an encoder E, adenoiser U-net f, and decoder D. The standard diffusion training objective is:",
  "minEzE(x),t,N (0,1) ( f(zt, t, f(x, R, t)))22": "Our modeling objective differs from previous work in that we condition on both rotation, R, andtranslation t. The long-range correspondences in our training data afford much freer camera movementthroughout the scene compared to previous works. Due to the limitations of previous training data,other methods can only rotate about a center point of the object or scene.",
  "Motion Masking": "Learning how to perform novel view synthesis from videos poses a challenge as it assumes the sceneitself does not vary with time when generating images from novel viewpoints. Previous approacheshave addressed this challenge by training solely on videos of static scenes such as only indoorhouses or manually filtering videos . However, such approaches limit the diversityand scale of the data. Therefore, to learn from in-the-wild videos, we propose motion masking, anapproach for handling dynamic objects. Motion masking consists of predicting a dense mask of values between 0 and 1, which we apply tothe output by the U-net f through elementwise multiplication. This soft mask allows the model tofilter out portions of the scene which may be difficult to predict due to object movement. To producethe motion mask we add an additional channel to the U-Net denoiser, which outputs a dense mask",
  "Generated 3D ScenesSingle Input Images": ": Examples of generated 3D scenes using ODIN. The blue dot indicates the location ofthe input image and the red lines indicate the trajectory of the camera which generated the images.ODIN is capable of long-range generation of geometrically consistent images. In the bottom scene,we see the model accurately infers the geometry of the unseen cathedral ceiling and the long hallway.",
  "D Reconstruction": "We present 3D scenes reconstructed from ODIN generated images along a trajectory ().Quantitative comparison can be found in . For Google Scanned Objects our methodis comparable to Zero-1-to-3 and outperforms other methods. Comparable performance to Zero-1-to-3 is expected as it was designed for synthetic objects. We compare with ZeroNVS for scenereconstruction on a held-out set of 360-1M ( in Appendix). Other methods are not capable ofgenerating scenes therefore we only benchmark against this method.",
  "Experiments": "In this section we benchmark our model, ODIN, against existing methods for novel view synthesisand 3D reconstruction. We improve performance on standard benchmarks which consist of relativelysimple scenes with minimal camera translation, all without fine-tuning on the target task. Qualitativelywe find that ODIN has new capabilities in generating real-world scenes from long-range novel views.",
  "Experimental Setup": "We evaluate our model on the standard novel view synthesis (NVS) benchmarks, DTU , andMip-NeRF 360 . DTU consists of table-top items and Mip-NeRF 360 consists of scenes withviews rotated 360 around a point. We report the standard NVS metrics, PSNR, LPIPS, and SSIM.As noted by previous literature, PSNR and SSIM are not well correlated with human evaluationso we primarily focus on LPIPS and qualitative comparison. Furthermore, to showcase the novelcapabilities of our model, we evaluate our method on a held-out set of 360-1M constructed fromone-thousand 360 videos. For 3D reconstruction we compare with Zero1-to-3 , MCC, SJC-I, and Point-E on GoogleScanned Objects (GSO) and ZeroNVS on our held-out set of 360-1M. For 360-1M we derive thepseudo-ground truth from a Dust3R model which is trained on all ground truth views of the scenegiven by the video. We report Chamfer-Distance for 360-1M in addition to volumetric IoU for GSO.The 3D reconstructions for our model are created by generating images along trajectories then usingDust3r to reconstruct the scene.",
  "PixelNeRF 0.71816.500.556Zero-1-to-3 0.66711.700.196ZeroNVS 0.62513.200.240ODIN (Ours)0.58716.840.537": "The models we benchmark against are trained on a variety of 2D and multi-view data sources. Thediffusion-based methods, Zero1-to-3 and ZeroNVS start from a StableDiffusion pretrainedmodel. Zero1-to-3 fine-tunes on Objaverse , while ZeroNVS fine-tunes on Co3D ,ACID , and Real-Estate10k . When possible we evaluate the models provided by the originalworks. Most closely related to our work in architecture is Zero1-to-3 with the key differencebeing our addition of motion masking for training on video.",
  "Novel View Synthesis": "We observe improved performance on DTU and Mip-NeRF 360 on the standard NVS metrics (Tables 1and 2). Our improvement on DTU is relatively small which is to be expected as the dataset consists ofsimple objects, with black backgrounds. On Mip-NeRF 360, which consists of real-world scenes, wesee significant improvement. In particular, the other methods struggle to generate reasonable imagesfrom views that differ significantly from the input view. In we compare qualitatively toother recent works. We observe that Zero1-to-3 cannot generate full scenes and struggles to generatereal objects as expected due its training data. ZeroNVS generates more plausible views, but is stillconsiderably worse for more complex scenes.",
  "Limitations and Broader Impact": "The framework and method we presented in this work are a promising step towards large-scale 3Dmodels, however there are some limitations to our approach. From a modeling perspective, themotion mask allows us to filter portions of scenes which have dynamic elements, however ideally wewould like to learn to model the dynamic elements as well. Some progress has been made on 4DNeRF models which can move the camera view in both time and space, however generalized 4Dmodels are largely unexplored. Our work may have positive societal impact in the creation of 3D assets for AR and VR or downstreamapplications such as robotic navigation. From a negative perspective our work could be used to createfake images or inappropriate scenes.",
  "Conclusion": "In this work, we propose a scalable approach to constructing real-world multi-view data and show themerits of our model, ODIN, trained on the largest multi-view dataset, 360-1M, to date. Enabled by thescale, diversity, and long-range correspondences in 360-1M, ODIN demonstrates capabilities beyondthose of previous methods in generating 3D-consistent novel views of real-world scenes with freecamera movement. On novel view synthesis and 3D reconstruction benchmarks ODIN outperformsexisting methods without fine-tuning to the target data. While ODIN shows impressive results, webelieve that there is further potential in the use of 360-1M and 360 video for novel view synthesis aswell as other domains such as video generation. For novel view synthesis an exciting next step wouldbe to model dynamics to generate 4D scenes. We will open-source our code, models, and dataset.",
  "and Disclosure of Funding": "We would like to thank Kuo-Hao Zeng for his feedback on the manuscript. We acknowledge fundingfrom NSF IIS 1652052, IIS 1703166, DARPA N66001-19-2-4031, DARPA W911NF-15-1-0543 andgifts from Allen Institute for Artificial Intelligence, Google and Apple. Sham Kakade acknowledgesfunding from the Office of Naval Research under award N00014-22-1-2377. This work has beenmade possible in part by a gift from the Chan Zuckerberg Initiative Foundation to establish theKempner Institute for the Study of Natural and Artificial Intelligence.",
  "J. T. Barron, B. Mildenhall, D. Verbin, P. P. Srinivasan, and P. Hedman. Mip-NeRF 360:Unbounded anti-aliased neural radiance fields. In CVPR, 2022. 3, 8": "E. Brachmann, J. Wynn, S. Chen, T. Cavallari, . Monszpart, D. Turmukhambetov, and V. A.Prisacariu. Scene coordinate reconstruction: Posing of image collections via incrementallearning of a relocalizer. arXiv preprint arXiv:2404.14351, 2024. 2 E. R. Chan, K. Nagano, M. A. Chan, A. W. Bergman, J. J. Park, A. Levy, M. Aittala, S. D.Mello, T. Karras, and G. Wetzstein. GeNVS: Generative novel view synthesis with 3D-awarediffusion models. In ICCV, 2023. 3 A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li, S. Savarese, M. Savva,S. Song, H. Su, et al. ShapeNet: An information-rich 3D model repository. arXiv preprintarXiv:1512.03012, 2015. 3",
  "J. Chung, S. Lee, H. Nam, J. Lee, and K. M. Lee. Luciddreamer: Domain-free generation of 3dgaussian splatting scenes. arXiv preprint arXiv:2311.13384, 2023. 3": "D. Damen, H. Doughty, G. M. Farinella, S. Fidler, A. Furnari, E. Kazakos, D. Moltisanti,J. Munro, T. Perrett, W. Price, et al. The epic-kitchens dataset: Collection, challenges andbaselines. IEEE Transactions on Pattern Analysis and Machine Intelligence, 43(11):41254141,2020. 3, 5 M. Deitke, D. Schwenk, J. Salvador, L. Weihs, O. Michel, E. VanderBilt, L. Schmidt, K. Ehsani,A. Kembhavi, and A. Farhadi. Objaverse: A universe of annotated 3D objects. arXiv preprintarXiv:2212.08051, 2022. 1, 3, 8 M. Deitke, R. Liu, M. Wallingford, H. Ngo, O. Michel, A. Kusupati, A. Fan, C. Laforte, V. Voleti,S. Y. Gadre, E. VanderBilt, A. Kembhavi, C. Vondrick, G. Gkioxari, K. Ehsani, L. Schmidt, andA. Farhadi. Objaverse-XL: A universe of 10M+ 3D objects. arXiv preprint arXiv:2307.05663,2023. 1, 3",
  "R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesiswith latent diffusion models. arXiv, 2021. 3": "M. S. Sajjadi, D. Duckworth, A. Mahendran, S. Van Steenkiste, F. Pavetic, M. Lucic, L. J.Guibas, K. Greff, and T. Kipf. Object scene representation transformer. Advances in NeuralInformation Processing Systems, 35:95129524, 2022. 1 M. S. Sajjadi, H. Meyer, E. Pot, U. Bergmann, K. Greff, N. Radwan, S. Vora, M. Lucic,D. Duckworth, A. Dosovitskiy, et al. Scene representation transformer: Geometry-free novelview synthesis through set-latent scene representations. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition, pages 62296238, 2022. 1 M. S. Sajjadi, A. Mahendran, T. Kipf, E. Pot, D. Duckworth, M. Lucic, and K. Greff. Rust:Latent neural scene representations from unposed imagery. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition, pages 1729717306, 2023. 1 K. Sargent, Z. Li, T. Shah, C. Herrmann, H.-X. Yu, Y. Zhang, E. R. Chan, D. Lagun, L. Fei-Fei,D. Sun, et al. Zeronvs: Zero-shot 360-degree view synthesis from a single real image. arXivpreprint arXiv:2310.17994, 2023. 3, 6, 7, 8 P.-E. Sarlin, C. Cadena, R. Siegwart, and M. Dymczyk. From coarse to fine: Robust hierarchicallocalization at large scale. In Proceedings of the IEEE/CVF conference on computer vision andpattern recognition, pages 1271612725, 2019. 2, 3",
  "S. Wang, V. Leroy, Y. Cabon, B. Chidlovskii, and J. Revaud. Dust3r: Geometric 3d vision madeeasy. arXiv preprint arXiv:2312.14132, 2023. 3, 4, 7": "Z. Wang, C. Lu, Y. Wang, F. Bao, C. Li, H. Su, and J. Zhu. ProlificDreamer: High-fidelity and di-verse text-to-3D generation with variational score distillation. arXiv preprint arXiv:2305.16213,2023. 3 C.-Y. Wu, J. Johnson, J. Malik, C. Feichtenhofer, and G. Gkioxari. Multiview compressivecoding for 3d reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 90659075, 2023. 9 L. Wu, J. Y. Lee, A. Bhattad, Y.-X. Wang, and D. Forsyth. Diver: Real-time and accurateneural radiance fields with deterministic integration for volume rendering. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1620016209,2022. 3",
  "A. Yu, V. Ye, M. Tancik, and A. Kanazawa. pixelNeRF: Neural radiance fields from one or fewimages. In CVPR, 2021. 3, 8": "X. Yu, M. Xu, Y. Zhang, H. Liu, C. Ye, Y. Wu, Z. Yan, C. Zhu, Z. Xiong, T. Liang, et al.Mvimgnet: A large-scale dataset of multi-view images. In Proceedings of the IEEE/CVFconference on computer vision and pattern recognition, pages 91509161, 2023. 2, 3, 5, 6 X. Zhao, A. Colburn, F. Ma, M. A. Bautista, J. M. Susskind, and A. G. Schwing. Is gener-alized dynamic novel view synthesis from monocular videos possible today? arXiv preprintarXiv:2310.08587, 2023. 1",
  "FTraining Details": "We train ODIN, for 2 weeks on 16 A40s for 100 epochs. We used a batch size of 1024 where onesample consists of a frame correspondence. We use a learning rate of 1e 4 with a constant learningrate. In general, if not otherwise specified we use the default hyper-parameters from",
  "GAblations": "In table 6 we show performance for various values of /lambda, the coefficient for motion maskingdetailed in section 5.2. In table 5 we show ablation over sampling various frames per second for 10kvideos. We find that performance increases with higher FPS, and chose a reasonable balance betweenperformance and compute cost at 1 FPS when scaling to the larger 1 million datasets."
}