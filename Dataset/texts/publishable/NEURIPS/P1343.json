{
  "Abstract": "Districting is a complex combinatorial problem that consists in partitioning a ge-ographical area into small districts. In logistics, it is a major strategic decisiondetermining operating costs for several years. Solving districting problems usingtraditional methods is intractable even for small geographical areas and existingheuristics often provide sub-optimal results. We present a structured learningapproach to find high-quality solutions to real-world districting problems in a fewminutes. It is based on integrating a combinatorial optimization layer, the capaci-tated minimum spanning tree problem, into a graph neural network architecture.To train this pipeline in a decision-aware fashion, we show how to construct targetsolutions embedded in a suitable space and learn from target solutions. Experi-ments show that our approach outperforms existing methods as it can significantlyreduce costs on real-world cities.",
  "Introduction": "Districting aims to partition a geographical area made of several basic units (BUs) into small,balanced, and connected areas known as districts. Districting has a wide array of applications in manyareas such as electoral politics (Williams, 1995; Webster, 2013; Ricca et al., 2013), sales territorydesign (Lpez-Prez and Ros-Mercado, 2013; Zoltners and Sinha, 2005), school zoning (Ferlandand Gunette, 1990), and distribution (Zhou et al., 2002; Zhong et al., 2007). These problems arechallenging because of the combinatorial complexity of assigning BUs to districts. In this paper, we focus on districting and routing, one of the more complex forms of districting. In thatcase, the goal is to minimize the routing costs over all districts, where each district is serviced by avehicle starting from a common depot. Since districting is a long-term strategic decision, the deliveryrequests in each district are unknown and modeled as a point process. As a two-stage stochasticand combinatorial problem, districting and routing can be solved to optimality only on very smallinstances. For example, in our experiments, finding the optimal solution of a districting problem with60 BUs and 10 districts required around 400 CPU-core days. Further, while several heuristic methodshave been proposed based on estimating district costs (Daganzo, 1984; Figliozzi, 2007), they tend tolead the search towards low-quality solutions on large instances (Ferraz et al., 2024). In this study, we present a structured-learning approach called DISTRICTNET that integrates anoptimization layer in a deep learning architecture, as shown in . DISTRICTNET learns toapproximate districting problems with a simpler one: the capacitated minimum spanning tree (CMST).DISTRICTNET predicts the cost of each arc of the CMST graph using a graph neural network (GNN)",
  "arXiv:2412.08287v1 [cs.LG] 11 Dec 2024": "trained in a decision-aware fashion. This surrogate optimization model captures the structure ofdistricting problems while being much more tractable. Using the CMST as a surrogate model isespecially relevant because there is a surjection from the space of districting solutions to the space ofCMST solutions. In other words, there always exists a CMST solution that is optimal for the originalproblem. The main challenge is to train a model that can find it.",
  "Decoder": ": DISTRICTNET solves a complex districting problem by parameterizing and solving aCMST. The GNN predicts a vector of edge weights based on the covariates of the instance x.These edge weights parameterize a CMST, which is solved using a black-box combinatorial solver.The CMST solution y is finally converted into a districting solution . Training this pipeline in adecision-aware manner requires propagating a loss gradient back to the GNN. DISTRICTNET is trained to imitate a few optimal solutions obtained on small instances. We showthat it leads to high-quality solutions for large districting problems and great out-of-distributiongeneralization. Further, our approach is robust to changes in the problem parameters such as a varyingnumber of districts. This is valuable for practitioners, who usually evaluate a family of problemsbefore picking a solution. Our contributions. We present a structured learning approach to obtain high-quality solutions tolarge districting problems. The main component of our pipeline is a combinatorial optimizationlayer, a parameterized CMST, that acts as a surrogate to the original districting problem. We showhow to learn from districting solutions by embedding CMST solutions into a suitable space andconstructing target CMST solutions from districting ones. This allows training DISTRICTNET byimitation, minimizing a Fenchel-Young loss on a set of target solutions obtained on small instances.The value of our approach is demonstrated on real-world problems as DISTRICTNET generalizes tolarge problems and outperforms existing benchmarks by a large margin thanks to the combination ofGNN and structured learning. While we focus our experiments on districting and routing, the methodpresented is not tailored to this specific application. DISTRICTNET is generic and could be applied toany geographical districting problem.",
  "Problem statement": "We model a geographical area as an undirected graph G = (V, E), where V is a set of verticesrepresenting the BUs and E is a set of edges representing the connections between them. A districtd V is a subset of BUs. We say that a district d is connected if its graph is connected. Let 1() bethe indicator function that returns one if its argument is true and zero otherwise. We denote by D theset of connected districts and N = |V | the number of BUs. Routing costs. Let d D be a connected district. In districting and routing, the cost of a districtis the expected distance of the smallest route that satisfies the demand requests. Hence, it is theexpected cost of a traveling salesman problem CTSP(d) = E[TSP(d, )], where is the collectionof demand requests in a district d. Since the demand is not known a priori, it is modeled as a random variable. We assume that anaverage demand-generating distribution exists for each basic unit over the planning horizon so thati Dv, v V . Let v0 be an additional vertex that represents the depot and is connected to allother nodes. Let also () be the set of all routes that satisfy the demand requests of a district dwhile starting and finishing at the depot, and denote by dist() the total travel distance of a route . Given a set of demand requests , the minimum transportation costs are achieved by the routethat visits all demand locations with minimum distance starting and ending at the depot so that",
  "dD d = k,(1c)": "where d is a binary variable that tracks whether a district is selected. Constraint (1b) states that eachBU is selected in exactly one district of the solution. Constraint (1c) specifies that exactly k districtsare selected. Additional constraints on the districts can be readily added to the problem. For instance, districtingand routing typically aim to obtain districts close to a target size t and include constraints on theminimum and maximum size of a district. Problem (1) can be formulated over a restricted set ofdistricts Dr D that satisfy the minimum and maximum size constraints. Formally, it ensures thatd Dr, d |d| d, where (d, d) are user-specified upper and lower bounds on the district size. Although seemingly simple, Formulation (1) has an exponential number of variables. The number offeasible connected districts of size t within a geographical area comprising N BUs is on the order ofO((e( 1))t1. N t ) where is the maximum number of neighbors for a given BU (Komusiewiczand Sommer, 2021). For instance, a city of N = 120 BUs and a target size of t = 20 with = 13has on the order of 1029 possible connected districts. The districting-and-routing problem has alsobeen shown to be NP-hard by Ferraz et al. (2024). Hence, solving Problem (1) to optimality is onlypossible for small problem sizes. Existing approaches. For real-world cities, evaluating a districts cost is too computationallydemanding to be performed at each step of a search algorithm. Existing methods replace thecost CTSP(d) with a surrogate cost estimator. The defining aspect of existing methods is thespecific model used to estimate the routing costs. Most approaches are based on the formulaof BeardwoodHaltonHammersley (BHH), which estimates the distance to connect randomlydistributed points (Beardwood et al., 1959). This formula has been embedded into a hybrid searchmethod combining a gradient method and a genetic algorithm (Novaes et al., 2000) and an adaptivelarge neighborhood search metaheuristic (Lei et al., 2015). Extensions of the BHH formula includeDaganzo (1984), who adapted it for routing problems, and Figliozzi (2007) who extended it furtherfor non-uniformly distributed demand requests. Recent works have shown that the BHH formulaestimates stochastic TPS costs remarkably well empirically against more sophisticated regressionfunctions for uniform distributions (see, e.g., Kou et al., 2022). Most closely related to our work,Ferraz et al. (2024) trained a GNN to predict district costs and embedded it in an iterative localsearch algorithm. They focused on solving single districting instances but did not investigate thegeneralization capabilities of their approach. In contrast to the literature, we learn to approximate districting problems by using a surrogateoptimization problem trained in a decision-aware fashion. An advantage of this framework is that itapplies to a wide range of constrained partitioning problems: it can work with any general districtingcost function C(d). Hence, it could readily consider other metrics such as fairness, balancing andcompactness of the district. Multi-instance learning and generalization. Often, practitioners do not solve a single districtingproblem but study a family of problems with varying settings and parameters. Because there is asubstantial effort needed to train a model, our goal is to obtain a pipeline able to solve multipleinstances of districting. In our setting, generalizing over multiple instances means being able togeneralize across: Cities: each city has unique geographical and social characteristics, which affect the optimaldistricting solution. Our model should be able to identify the impact of these differences on districtingand provide high-quality solutions for different cities. Instance sizes: the model should be able to handle instances with a different (typically larger)number of BUs than it was trained on. Since the training effort increases with the number of BUs,this allows solving large instances with a small training budget. Problem parameters: the model should be able to generalize to slight variations in the problemparameters, such as the minimum and maximum size of districts.",
  "DISTRICTNET: From CMST to Districting": "DISTRICTNET takes as input a labeled graph containing all the data of an instance. A graph neuralnetwork w turns this labeled graph into edge weights. These weights parameterize a CMST instance,which is then solved using a dedicated algorithm. The CMST solution is then converted into adistricting solution. The first component of DISTRICTNET is a GNN w : Rdf R that assigns a weight to an edgedepending on its feature vector. Applying this model to all edges of an instance returns a vector ofedge weights R|E|. Our model is made of two components: graph convolution layers that learnlatent representations of graph edge features, and a deep neural network that converts these latentrepresentations into edge weights.",
  "jN (e) W (l)1 h(l)j,(2)": "where h(l)eis the feature vector of edge e at the l-th layer, is a non-linear activation function,W0 and W1 are learnable weight matrices, and N(e) is the set of neighboring edges of an edge e.Convolutional layers allow to capture the structure of the graph while being able to apply theprediction model to any graph size and connectivity structure. After the convolutional layers, a fullyconnected deep neural network transforms the latent representation produced by the GNN into anedge weight.",
  "Combinatorial Optimization Layer": "The second component of DISTRICTNET is the CMST layer parameterized by the vector of edgeweights . This layer acts as a surrogate for the original districting problem. The CMST is a well-studied graph optimization problem based on the minimum spanning tree problem. A minimumspanning tree is the subset of edges of a weighted graph that spans all vertices of the graph whileminimizing the sum of the weight of its edges. Classically, the CMST extends this problem with theconstraint that the number of vertices in each subtree does not exceed a predetermined capacity. Let T be the set of connected subtrees with minimum and maximum size (d, d). We consider that anedge e is in a subtree s if both its extreme points are inside the subtree. The CMST problem with atarget number of subtrees is then given by",
  "sT s = k.(3c)": "Formulation (3) highlights the strong link between the CMST and the districting problem in (1).Both problems partition a graph into connected components with cardinality constraints. Any CMSTsolution can be converted into a districting solution by collecting all the nodes of a subtree into adistrict. Since several subtrees lead to the same district, this is a surjective mapping from the spaceof subtrees T to the space of districts D. This also implies that, for any districting problem, therealways exists a CMST problem such that their optimal solutions coincide.",
  "a combinatorial problem. Several methods have been proposed to solve it, ranging from expensiveexact methods to quick heuristics": "DISTRICTNET is agnostic to the choice of CMST solver. In our experiments, we use the exactformulation in (3) to solve small instances to optimality, and we apply an iterated local search (ILS)heuristic for large instances. ILS alternates between two steps: (i) a local improvement step thatguides to a local minimum, and (ii) a perturbation step to diversify the search. A key component ofthe algorithm is the initial solution. Randomly allocating BUs to districts is unlikely to return feasiblesolutions and, even when it does, it often leads to very poor solutions. Here, we use a modifiedKruskal algorithm to exploit the structure of the CMST and quickly find a good initial solution. Thedetails of our implementation of the ILS are given in Appendix A.",
  "DISTRICTNET is trained to imitate the solutions of a training setxi, ini=1. Each instance xi =(Gi, fi) is a labeled graph with Gi = (Vi, Ei) the instance graph and fi =f ei Rdf , e Ei": "the set of edge feature vectors. A target districting solution i is associated with each instance xi.Since producing optimal or near-optimal districting problems is hard, building this training set isexpensive. Thus, we train our model on a dataset of small instances with the hope that it generalizeswell on large instances. Training DISTRICTNET by imitation amounts to solving",
  "i=1Lw(xi), yi,": "where L : (, y) L(, y) is a loss function that quantifies the distance between a CMST solutioncorresponding to the edge weights with a target CMST solution y. However, our training set doesnot contain any CMST targets but only districting targets. Training DISTRICTNET is thus achievedby three main steps: (i) introducing a new embedding of CMST solutions, (ii) converting districtingtargets i into CMST targets yi, and (iii) defining a suitable loss function with desirable propertiesand deriving its gradient.",
  "maxyY y,(4)": "where, with a slight abuse of notation, we changed the problem from min to max, which is withoutloss of generality. Since Y is finite, its convex hull C is a polytope. Therefore, the linear programgiven by maxC is equivalent to Problem (4). The change of notation from a vertex y to amoment emphasizes that takes values inside the convex hull C. We now use the shorthandnotation () to denote an optimal solution to argmaxC . From Districting to CMST. As discussed previously, there is a surjection from the space of CMSTsolutions to the space of districting solutions. Given a target districting solution , we denote by Y()the set of feasible CMST solutions that lead to . To recover a target CMST solution, we introduce the constructor algorithm A : y that maps adistricting solution to a CMST solution y Y(). This algorithm can be randomized. For instance,DISTRICTNET constructs districting solutions by solving a minimum spanning tree problem withrandom edge weights for each district d . This can be efficiently done by applying Kruskalsalgorithm in parallel for all the selected districts.",
  "Fenchel-Young loss and stochastic gradient": "Let be a target CMST solution. We want DISTRICTNET to minimize the non-optimality of ()compared to the target , that is, minimizing the loss maxC . Minimizing this lossdirectly does not work because = 0 is a trivial optimal solution. However, given a smooth strictlyconvex regularization function (y), we can define the regularized problem (Blondel et al., 2020)",
  "LFY(, ) = maxC () ( ()).(7)": "Denote by () the Fenchel conjugate of , the regularized loss in Equation (7) is equal toLFY(, ) = () + () (Dalle et al., 2022), which we recognize as the Fenchel-Younginequality (Blondel et al., 2020). Fenchels duality theory then ensures that this loss has desirableproperties. Notably, it is convex in , non-negative, equal to 0 only if y is the optimal solution of (6),and its gradient can be expressed as L(, ) = argmaxC () . Practically, it remains to choose a suitable regularization function . When using a black-boxoracle, a convenient choice that exploits the link between perturbation and regularization (Berthetet al., 2020; Dalle et al., 2022) is to define () as the Fenchel dual of the perturbed objectiveF() = EmaxC ( + Z), where Z is a random variable with positive and differentiabledensity on R|E|. In that case, the gradient of the Fenchel Young loss with respect to can becomputed as (Berthet et al., 2020)",
  "LFY(, ) = EZ[( + Z)] .(8)": "A stochastic gradient can therefore be conveniently computed using the Monte Carlo approximation1MMm=1 argmaxC( + Zm) for the expectation, where {Zm}Mm=1 are sampled perturbations.If A is randomized, we also use a Monte Carlo approximation to estimate . Summary. The novelty of our approach lies in our reconstruction of a CMST moment from adistricting solution , which can be seen as a partially specified target y. Alternatives in the literaturegenerally consider completing the partially specified solution into the fully specified solution thatminimizes the Fenchel Young loss (Cabannnes et al., 2020; Stewart et al., 2023). Our approach hasthe advantage of leading to the classic Fenchel Young loss, which is convex, whereas the infinum lossof Stewart et al. (2023) is only a difference of convex functions.",
  "Numerical Study": "We now evaluate the performance of DISTRICTNET on real-world districting and routing problems.We run repeated experiments and compare our approach to other learning-based benchmarks. Weinvestigate the following aspects of DISTRICTNET: (i) its ability to generalize to large out-of-distribution instances from training on a few small instances, (ii) to variations in the instanceparameters such as the district sizes, (iii) the role of the CMST surrogate model to allow thisgeneralization. Our experiments are implemented in Julia (Bezanson et al., 2017) except for the district evaluationmethods, which are taken from Ferraz et al. (2024) and implemented in C++. All experiments are runon a computing grid. Each experiment is run on two cores of an AMD Rome 7532 with 2.4 GHz andis allocated 16 GB RAM. The code to reproduce all experiments presented in this paper is publiclyavailable at under an MIT license.",
  "Experimental Setting": "Our instances include all the real-world cities in the United Kingdom used by Ferraz et al. (2024) andwe extend it with additional cities in France. Hence, our test set contains seven real-world cities inthe United Kingdom and France (Bristol, Leeds, London, Lyon, Manchester, Marseilles, and Paris),which contain between 120 and 983 BUs. Our goal is to provide high-quality solutions for several",
  "values of the target district size t. This target size sets the bounds (d, d) on the district size t 20%and the target number of districts as k = N/t": "Features. Each BU is summarized by a set of characteristics including its population, density, area,perimeter, compactness, and distance to the depot. For test instances, these summarizing statistics aretaken from real-world data. The edge feature vector is constructed by averaging the feature vectorsof the two BUs it connects. Additionally, we include the distance between the center of the twoconnecting BU into the edge feature vector. Thus, an instance is fully described by its geographicaland population data. Training set. To assemble a large and diverse training set, we generate new cities by perturbingreal-world ones. First, we read the geographical data of 27 real-world cities in England (excluding theones from the test set). From these initial cities, we generate n = 100 random connected subgraphsof size N = 30 BUs and sample the population of each BU according to a normal distributionN(8 000, 2 000) truncated between 5 000 and 20 000. For each instance xi, we compute its optimalsolution for the target size t = 3 by fully enumerating the possible districting solutions and evaluatingtheir costs. This procedure generates our training set of n = 100 instances and associated solutions. Benchmarks. We evaluate our method against four benchmark approaches that are based on learningestimators of district costs CTSP(d). They can be combined with any search method. In theseexperiments, they are also integrated within an ILS with a time limit of 20 min. We include (i) the method of Daganzo (1984) (BD), (ii) the method of Figliozzi (2007) (FIG), (iii) themethod of Ferraz et al. (2024) (PREDGNN), and (iv) a deterministic approximation of the stochasticdistricting problem called AVGTSP. The first two approaches are extensions of BHHs formula andtherefore are simple linear regression models. The third benchmark is based on training graph neuralnetworks to estimate the district costs. Ferraz et al. (2024) train a GNN model to predict district costsfor a fixed city and set of problem parameters and show that this approach outperforms BD and FIG.Since our focus is on generalization to multiple cities and problem parameters, our implementationextends the one of Ferraz et al. (2024): we train a single GNN using data from multiple small citiesand evaluate its ability to generalize out-of-distribution. In contrast, AVGTSP estimates districtcosts by solving a TSP over the barycentres of all BUs within a district. This is a single-scenarioapproximation of the original stochastic districting problem that considers the expected demandrealization in each BU. The above benchmarks do not use a surrogate optimization model. Instead, they are trained in atraditional supervised learning fashion: to minimize the cost-estimation error on a training set of10 000 districts and true costs taken from the same training instances as DISTRICTNET.",
  "Main results": "We evaluate the ability of the different methods to generate good solutions on a diverse set of out-of-distribution instances. All methods are evaluated using the same performance metric: the totaldistricting cost CTSP(d) of a districting solution as presented in Problem (1), where the expectedcosts are evaluated using a Monte Carlo approximation. We restrict the cities to N = 120 BUs andvary the target district size t {3, 6, 12, 20, 30} for each of our seven test cities. This provides a setof 35 test instances, completely independent of the training data. The results are presented in in the form of an ablation study. The table shows the relative difference in average costs achievedby each method on the test instances and the statistical significance is assessed using a one-sidedWilcoxon test. Example districting solutions are also given in . shows that DISTRICTNET consistently outperforms the benchmarks as it produces districtingsolutions with significant cost reductions of around 10% compared to all other methods. Thebenchmarks all provide similar performance, even PREDGNN, which uses a graph neural networkbut does not use structured learning. This highlights the ability of DISTRICTNET to generalize acrossvarious city structures and for larger instances thanks to the combination of a graph neural networkand a differentiable optimization layer.",
  "Average relative costp-value": "Benchmark 1: BD, linear regression9.92 %4.9e-09Benchmark 2: FIG, linear regression10.01 %8.9e-09Benchmark 3: PREDGNN, unstructured learning with GNN11.91 %1.5e-10Benchmark 4: AVGTSP, no learning4.44 %2.7e-04DISTRICTNET: structured learning with CMST and GNN0.0 %- Large cities. We perform an additional experiment to investigate the generalization to cities of largesizes. In , we show the cost of the districting solution obtained with the benchmark methodsrelative to DISTRICTNET for varying city sizes. A value greater than 100% means that the benchmarkperforms worse than DISTRICTNET. We increase the number of BUs for each city and keep constantthe target number of BUs in a district as t = 20. The time limit of ILS is kept to 20 min for allmethods when N < 400 and increased to 60 min when N 400.",
  ": Cost relative to DISTRICTNET for target district size t = 20 and varying city size": "The results show that DISTRICTNET provides very good solutions up to the largest city sizes. Itconsistently outperforms the benchmarks even for large cities. These results are achieved despiteDISTRICTNET being trained on small instances of size N = 30 BUs. We investigate further the scalability of our approach by considering a large instance with 2,000 BUsin the Ile-de-France region. Each method is allowed 60 minutes to compute the districting solution,with the target district size set to 20 BUs. The results are presented in . DistrictNet providesthe best performance, showing that it generalizes even to instances that are more than 60 times largerthan the training ones.",
  "Result 2. DISTRICTNET provides high-quality solutions to even the largest real-world problems": "Why does unstructured learning fail? One potential explanation for the poor performance of thebenchmarks, in particular for PREDGNN, is the change in distribution between the training and testinstances. As shown in , the distribution of district costs varies greatly with the city andparameters. Since there is a shift in the data-generating distribution, the benchmarks, which ignorethe structure of the districting problems, are not able to accurately predict the district costs resultingin poor overall performance.",
  ":Relative cost ofDISTRICTNET with increasingdata": "The value of data for decision-aware learning. Finally, we investigate the value of training data forDISTRICTNET. In , we study the out-of-distribution performance of DISTRICTNET as thesize of the training set increases. We show the average districting cost over all cities and target sizesrelative to the cost for n = 20 and add a 95% confidence interval as a shaded area. A value smallerthan 100% means that DISTRICTNET improves compared to its training with n = 20. The figureshows that DISTRICTNET can achieve low costs even with a surprisingly small number of trainingexamples (n = 50). Increasing the number of examples tends to improve the results although with adiminishing return.Result 4. DISTRICTNET can be trained with a small computational budget and benefits fromincreasing the number of training examples.",
  "Related literature": "In this work, we find near-optimal solutions to complex combinatorial problems by introducing acombinatorial layer in a deep neural network trained in a decision-aware fashion. Our work lies at theintersection of the learning-to-optimize literature and decision-aware learning. Learning to optimize. Recent years have seen a significant increase in the use of machine learningfor solving hard combinatorial optimization problems. Several graph-based learning approacheshave been proposed for general combinatorial problems (Cappart et al., 2023). Deep reinforcementlearning has been applied to solve typical combinatorial problems such as the traveling salesman andknapsack problems (Bello et al., 2016) and the minimum vertex cover and maximum cut problems(Dai et al., 2017). Gasse et al. (2019) presented a technique to improve the branch-and-bound processin mixed-integer linear programming by using graph convolutional neural networks. The main advantage of learning-based methods is that, after a potentially expensive training procedure,they can generate good solutions to combinatorial problems in a short time. This has been shown to beeffective in solving routing problems, which are complex combinatorial problems. Joshi et al. (2019)applied a beam search to solve the Euclidean TSP using GNNs and show notable improvements insolution quality, speed, and efficiency. Kool et al. (2019) combined a greedy rollout baseline and adeep attention model to solve several challenging routing problems such as the orienteering problemand the prize-collecting TSP. Decision-aware learning. Decision-aware learning, on the other hand, looks into including anoptimization layer in deep learning architectures. A key challenge in this area is to propagate ameaningful gradient through this non-smooth layer. Amos and Kolter (2017) developed a method tocompute the gradients of quadratic programs by differentiating the KarushKuhnTucker optimalityconditions. In the case of (integer) linear programs, propagating this gradient can be performed for instance by introducing a log-barrier term to the LP relaxation (Mandi and Guns, 2020), using apiecewise-linear interpolation technique (Vlastelica et al., 2019), or using perturbation (Berthet et al.,2020). We refer the interested reader to Mandi et al. (2023) and Sadana et al. (2023) for surveys ondecision-aware learning and its generalization as contextual stochastic optimization. Decision-awarelearning has diverse applications such as approximating hard optimization problems by learninglinear surrogate models (Ferber et al., 2023; Dalle et al., 2022). Decision-aware learning allows the integration of complex algorithms with combinatorial behaviorinto deep learning architectures. Wilder et al. (2019) learn to solve hard combinatorial problems bylearning from incomplete graphs using a differentiable k-means clustering algorithm. Stewart et al.(2023) present a differentiable clustering approach with a partial cluster-connectivity matrix. Ourpaper differs in two significant ways. First, we use a surrogate model with substantially more structurethan clustering since it includes constraints on the size of the districts. This raises computationalchallenges, since the surrogate model remains NP-hard, but allows significant benefits in the qualityof solutions. Second, while we have full districting solutions available, i.e., we know exactly whatnodes need to be in the same clusters for a given training example, we have no information on thecorresponding CMST solution. This leads us to introduce a randomized target construction algorithm,which leads to a well-defined loss function with advantageous properties. Our approach belongs to the research stream that learns to approximate hard problems by easierones. Key advantages of these approaches include being efficient at inference time since most ofthe computation effort is shifted offline, and great performance on test instances if they remainclose to the training distribution. There are also downsides. First, until now, there are no knownworst-case theoretical guarantees on the quality of the solution. Second, learning may be intensivecomputationally, both when generating the training instances and in the training algorithm. Werefer the reader to Aubin-Frankowski et al. (2024) for a general discussion of these aspects and ananalytical characterization of generalization bounds.",
  "Conclusion": "This paper presented a general pipeline to learn to solve graph-partitioning problems. It integrates aCMST as a surrogate optimization layer, which allows it to capture efficiently the structure of graphpartitioning while providing solutions in a short time. We demonstrate the value of our pipeline ona districting and routing application. We show that our method outperforms recent and traditionalbenchmarks and is able to generalize to out-of-distribution instances. Thus, it can be trained on asmall set of examples and applied to a wide array of cities, instance sizes, and hyperparameters. Future work could investigate alternative approaches to solve the CMST problem during training andtesting, such as exact methods based on column generation, which would integrate seamlessly withthe pipeline presented in this paper. A limitation of our approach is applying DISTRICTNET only todistricting and routing. Other geographical partitioning problems such as designing voting or schooldistricts could be considered in future research.",
  "J. Mandi and T. Guns. Interior point solving for LP-based prediction+optimisation. In Advances inNeural Information Processing Systems, volume 33, pages 72727282, 2020": "J. Mandi, J. Kotary, S. Berden, M. Mulamba, V. Bucarey, T. Guns, and F. Fioretto. Decision-focused learning: Foundations, state of the art, benchmark and future opportunities. arXiv preprintarXiv:2307.13565, 2023. C. Morris, M. Ritzert, M. Fey, W. L. Hamilton, J. E. Lenssen, G. Rattan, and M. Grohe. Weisfeilerand leman go neural: Higher-order graph neural networks. Proceedings of the AAAI Conferenceon Artificial Intelligence, 33(01), 2019.",
  "A.1.1Illustrative Example": "We illustrate the link between districting problems and CMST problems in . First, we showa simple districting instance with N = 7 BUs with the source node shown as a yellow star. In (b), we show the corresponding CMST instance that would be obtained when applyingDISTRICTNET to predict the edge costs. The width of each edge is shown proportional to the edgesweight. In (c) we show the solution of this CMST instance. The solution has two subtreesstemming from the source node. Each subtree corresponds to a district.",
  "A.1.2Training Algorithm": "DISTRICTNET is trained following Algorithm 1. The hyperparameters of DISTRICTNET includethe typical ones used in deep learning (number of epochs, batch size, learning rate) as well as thehyperparameters of the perturbation (number of samples M and temperature ). We note again that,in our implementation, DISTRICTNET uses the exact approach to solve each perturbed CMST, whichis based on enumerating all the possible districting solutions and solving the exact formulation givenin (3).",
  "A.2Additional Details on Iterated Local Search (ILS)": "All districting and CMST problems on large instances are solved using ILS. The detailed algorithm isgiven in Algorithm 2. ILS iteratively applies two methods: a local search that guides a solution toa local optimum, and a perturbation that explores the solution space. The local search algorithm isgiven in pseudocode in Algorithm 3. The perturbation algorithm is the same as the local search oneexcept that each possible move is implemented with a given probability even if it does not improvethe solution. This probability is typically very low. In our experiments, after hyperparameter tuning,we find that a probability of 1.5% works well in most instances, which is consistent with the resultsof Ferraz et al. (2024).",
  "flow formulation proposed by Ferraz et al. (2024) does not scale well to large instances. Hence, wedevelop the following heuristic": "Given a set of edge weights, we first use the modified Kruskal algorithm given in Algorithm 4. Thisalgorithm starts with all nodes in their own cluster and sorts all edge weights in increasing order.Then, it greedily merges clusters on the extreme points of the edges with lowest cost if the size of themerged cluster is below the maximum size. This provides a first solution that may still have too manyclusters. In that case, we run the greedy merging algorithm given in Algorithm 5. This algorithmfurther reduces the number of clusters by continuously merging the two neighboring clusters thathave the smallest combined size until the number of clusters meets the desired target k.",
  "end while": "However, the initial solutions obtained may not always meet the limit size requirements. To addressthis, we have incorporated a penalty function in the local search algorithm, which penalizes clustersnot conforming to size limits, thereby guiding the local search towards feasible solutions. In ourexperiments, a feasible solution is thus almost always found at the first iteration of the ILS algorithm. Empirically, this method efficiently finds feasible initial solutions in our main experiments. However,they are not sufficient for the largest instances (when N 600 in our experiments). Thus, weintroduce an additional repair algorithm given in Algorithm 6, which is used only in the experimentpresented in . This repair algorithm adjusts each district to meet the specified minimum andmaximum size constraints by adding or removing nodes from neighboring districts while maintainingoverall connectivity.",
  "where Ad =": "id ai represents the total area of the district with ai being the area of the ith BU,and Rd denotes the expected number of demand requests within the district. The term d is theaverage distance between the depot and a demand point. Because we do not make assumptions on theshape of the BUs, we compute this term using a Monte Carlo approximation. The parameter is a",
  "fi,d = (pi, pi, ai, ai, qi, i, i, ei,d)(11)": "where pi is the population of the BU i, qi its perimeter, ai its area, i its density, i its distance tothe depot, and eid is an inclusion variable that takes the value 1 if BU i belongs to the district d and0 otherwise. The GNN processes Gd to learn a latent representation that captures the structure ofthe district. This is done in two phases. First, a message-passing algorithm is applied at the nodelevel, then an aggregation layer summarizes the whole graph into a single latent vector. This latentgraph representation is then fed to a feedforward NN to predict the district cost. This architecture issummarized in .",
  "h(xi)Costestimate": ": PREDGNN estimates the cost of a district using a GNN and a feedforward NN. First, theGNN applies a message-passing algorithm to capture the structure of the graph. Then, an aggregationlayer provides the graph embedding. This is post-processed by the feedforward NN, which outputs acost estimate. Key differences with DISTRICTNET. PREDGNN and DISTRICTNET both use GNN to learn thestructure of labeled graphs. PREDGNN uses node-based features available at the BU level. Incontrast, DISTRICTNET uses edge-based features, averaging the attributes of the two BUs connectedto each edge. Hence, while PREDGNN focuses on the properties of individual BUs, DISTRICTNETalso captures the spatial relationships between connected BUs. Further, DISTRICTNET does not usethe final aggregation layer for global graph embedding of PREDGNN since it is applied independentlyto each edge. This allows a finer-grained representation of the graph.",
  "dDTSP(d, E[])d,(12)": "that is, it exchanges the min and expectation operations in the objective function of the originalproblem. This yields a deterministic problem, where the expected districting cost is approximatedby the cost of the expected demand in each BUs. In many stochastic problems, this approach oftenleads to poor results (see e.g., Chapter 4.2 of Birge and Louveaux (2011)). This is because thisapproximation ignores the variance of the random variable . Still, we implement this method as a non-learning-based baseline. In our setting, the demanddistribution is known and the expected demand in each BU can be easily computed. For each BU, theaverage demand request E[] is a single request that appears at the barycentre of the area. Evaluatinga district cost thus reduces to solving a TSP over the barycentres of all its BUs. As for all othermethods, we integrate this approximation in an iterated local search.",
  "A.4Architectures and Hyperparameters": "Several hyperparameters control the different methods used in this paper. We give here the ar-chitectures and hyperparameters used in all our experiments. We also provide a summary of thecomputational effort for training the model in . The table illustrates well the two main sourcesof computational efforts for the GNN-based methods. The training time of PREDGNN is relativelylonger because it uses a large number of district costs examples. On the other hand, DISTRICTNETneeds to evaluate more districts to find the optimal solutions of its 100 training instances but is quickto train. This can be seen in the second column of that shows the number of districts whosecost is evaluated to compute the optimal solution of n = 100 instances. Notably, the district costevaluation can all be performed in parallel.",
  "BD1041048 secondsFIG1041043 secondsPREDGNN10410416 hoursDISTRICTNET10262 10438 minutes": "Architecture. The GNN of DISTRICTNET uses three graph convolution layers, each with a hiddensize of 64 and Leaky ReLU activation functions. The DNN section uses three dense layers: twolayers mapping with 64 inputs to 64 outputs and then one layer with an output of size 32. All threelayers use Leaky ReLU activations. Finally, the final layer converts the latent 32-dimension vectorinto a one-dimensional output. For PREDGNN, we maintain the structure proposed by Ferraz et al. (2024), with the exception thatwe replace the Structure2vec layers with GraphConv layers. The update rule for the GraphConvlayers is xt+1i= ReLU(W1xti + jN(i) W2xtj), where W1 and W2 are the weight matrices andN(i) is the set of neighbors for node i. In contrast, the Structure2vec update rule applied in theoriginal model is xt+1i= ReLU(W1xti + W2",
  "jN(i) xtj). This is a minor modification and shouldnot alter its performance, as also stated by Ferraz et al. (2024)": "The architecture of PREDGNN thus consists of four GraphConv layers. Each layer has a hidden sizeof 64 units and uses ReLU activation functions. The final layer produces an output of size 1028. Wethen aggregate these node embeddings to form a global graph embedding. This global embedding isinitially processed by a feedforward layer, resulting in a 100-dimensional vector. A second and finallayer reduces this to a single output, representing the cost of the district.",
  "Hyperparameters. The only hyperparameter of BD and FIG is the number of samples used in theMonte Carlo approximation of the average distance to requests. We use 100 scenarios in all ourexperiments": "PREDGNN uses a batch size of |B| = 64, a learning rate of 1e4, and train the model for 104 epochs.We further set a time limit of 24 hours for training and stop the process early if no significant changeof the loss (i.e., greater than 1e4) is observed in the last 1 000 epochs. DISTRICTNET uses a batch size of |B| = 1 and a learning rate with initial value of 1e3 with adecay rate of 0.9 applied every 10 epochs and a minimum rate of 1e4. The model is trained for100 epochs. The target CMST solution in Equation (5) is constructed using a single observation ofour random constructor (Kruskal with random weights). The perturbation Z is set to a multivariatestandard Gaussian and we M = 20 samples to approximate the expected gradient in Equation (8).The randomized target constructor uses 1 000 samples.",
  "BSupplementary Material: Data Collection and Generation": "In this part, we describe how to collect and generate the real-world data used in all experiments. First,we describe the test instances, the four large cities on which we evaluate all the methods. Then, wepresent how we use real-world data to generate a set of training instances that can be arbitrarily large.We also discuss the distributional assumptions used to simulate the random demand over a city.",
  "B.1Test Instances: Real-World Cities and Population": "We use the four cities of Bristol, Leeds, London, and Manchester for our test instances. A summaryof these instances is given in . The table shows the statistics on the population, area, anddensity of the BUs composing the four test cities. It shows that, while the area and density may varyacross cities, the population statistics are relatively constant. This is not surprising since BUs tendto be designed to have similar populations. The geographical data including the boundaries of eachcity and BUs can be accessed at Uber Movement: Additionally,census data is available at the Office for National Statistics (ONS) website: ONS. For the Frenchcities, geographical data including the boundaries were obtained from Opendatasoft. Population datafor these regions was sourced from the National Institute of Statistics and Economic Studies (INSEE)available at INSEE.",
  "B.2Training Instances": "For our experiments, we utilize real-world data from a diverse set of 27 cities shown in .Apart from two outliers, these cities are smaller than our test instances, with around 30 to 50 BUs.While we read the true boundaries of the cities, we generate their population randomly. We use anormal distribution N(8 000, 2 000) truncated within the range to be close to the truepopulation distribution of BUs. To generate the city graphs used in the training of DISTRICTNET, we sample randomly connectedsubgraphs from the train cities. The training instance generation process is given in pseudocode inAlgorithm 7, and the subgraph sampling algorithm is given in Algorithm 8. Finally, an artificialcentral depot is placed at the centroid of the resultant polygon. This procedure allows us to create atraining set of arbitrary size that contains realistic (but small-sized) training instances. Note also thatthere is no contamination between the training and test instances.",
  "B.3District Demand and District Cost": "We suppose that the demand within each BU follows a fixed distribution. In each period (e.g., aday), a number of demand requests in each BU. The number of demand requests follows a Poissondistribution with the rate being proportional to the population density of the BU, expressed as n .Here, n represents the population count of the BU, and is set to 96/(8 000 t). This value of isselected to realistically approximate a scenario where a district typically handles around a hundredstops. Each demand request is located randomly with a uniform distribution over the geographiclocation covered by the BU. Evaluating the cost of a district requires solving a stochastic TSP since CTSP(d) = E[TSP(d, )].This is done through a Monte Carlo estimation. To accurately estimate the operational costs of ourdistricting solutions, we sample 100 demand scenarios for each basic unit. For each district, wecollect the demand requests from the BUs it contains and solve 100 independent TSP problems. Wethen evaluate a districts expected operational cost by averaging the calculated TSP costs. Each TSP is solved using the Lin-Kernighan-Helsgaun (LKH) algorithm (Lin and Kernighan, 1973;Helsgaun, 2017). We use the publically available implementation from: ~keld/research/LKH-3/. This heuristic is widely accepted as a state-of-the-art heuristic for TSPs.It is based on a local search strategy that consistently returns near-optimal solutions.",
  "In this part, we provide additional experimental results. We give:": "an overview of results on small training and validation instances in Appendix C.1, a detailed table presenting the individual results of experiments in Appendix C.2, a table presenting the compactness of the districts obtained in Appendix C.3, and examples districting solutions for all methods on various instances in Appendix C.4.",
  "C.1Out-of-Sample but In-Distribution": "We perform a small experiment to investigate the in-distribution generalization ability of the differentmethods. That is, we study the setting where the training and test instances are sampled from thesame distribution. We generate 200 instances following the procedure described in Appendix B.2 andsplit this into two sets of 100 instances. The first set is used to train all the methods and the second isused to evaluate them out-of-sample. All instances are of size N = 30 and with a target district size t = 3. Hence, we can solve themto optimality in a reasonable time using a full enumeration of the possible districts and the exactformulation given in Problem (1). We solve to optimality all the districting problems of BD, FIG andPREDGNN and all the CMST problems of DISTRICTNET during both training and testing. Further,since we consider only small instances here, we can measure the optimality gap of all methods: therelative difference between the cost of the true optimal solution and the one of the methods. The optimality gap of the four methods is shown in for both the training and testing instances.The results show that DISTRICTNET achieves the smallest average test gap from the optimal solutionscompared to other benchmarks. BD and FIG have similar performance, which can be expected sincethey estimate the district costs similarly. PREDGNN has the worst average performance, althoughless variations since it has a smaller maximum gap than BD and FIG.",
  "C.2Detailed results for each city": "In , we present the individual experiment results that have been summarized in the ablationstudy presented in . The table shows the districting cost achieved by the methods on eachtest city and for each target district size. The lowest cost is shown in blue and the second-best inorange. It highlights that DISTRICTNET provides the lowest count on 27 out of 35 test instances.Further, it shows that DISTRICTNET lead to significant savings, as it can reduce costs by up to 13%in the best case. Overall, the cost savings are higher for large target district sizes, and for cities in theUnited Kingdom that are closer to the training instances.",
  "C.3District compactness": "Compactness is an important criterion in districting problems as it often leads to efficient and fairdivisions. Compactness here does not refer to the topological property. In the districting terminology,it refers to a shape property. We measure compactness using Reocks score, a commonly usedmetric measure defined as the ratio of the area of the district to the area of the minimum enclosingcircle that contains the district (Reock, 1961). A higher score indicates greater compactness, withcompactness equal to 1 when the district is circular. As shown in Table , DISTRICTNETconsistently provides more compact districts compared to other methods, suggesting that it can designgeographically tighter districts. : Districting costs across different cities and target district sizes for our method and benchmarks.The best result is shown in blue and the second best in orange. The last column shows the relativedifference between DISTRICTNET and the best or second-best method.",
  "C.4Example Districting Solutions": "We illustrate the districting strategies of the four methods considered in this paper by showingexamples of districting solutions on the test instances. shows the districting solutions of thefour methods for the city of London with t = 6 and t = 20 BUs. shows the same results forthe city of Manchester with t = 6. These solutions correspond to the costs shown in . Thesefigures show that DISTRICTNET tends to return compact, homogeneous districts. On the contrary,the three benchmarks tend to find districts in a \"U\" shape. This is especially visible for large districtsizes such as t = 20. Interestingly, BD and FIG provide visually similar results, especially for thecity of London."
}