{
  "Abstract": "Modern optimizers such as AdamW, equipped with momentum and adaptive learn-ing rate, are designed to escape local minima and explore the vast parameter space.This exploration is beneficial for finding good loss basins when training fromscratch. It is not necessarily ideal when resuming from a powerful foundationmodel because it can lead to large deviations from the pre-trained initializationand, consequently, worse robustness and generalization. At the same time, strongregularization on all parameters can lead to under-fitting. We hypothesize that se-lectively regularizing the parameter space is the key to fitting and retraining the pre-trained knowledge. This paper proposes a new weight decay technique, SelectiveProjection Decay (SPD), that selectively imposes a strong penalty on certain layerswhile allowing others to change freely. Intuitively, SPD expands and contractsthe parameter search space for layers with consistent and inconsistent loss reduc-tion, respectively. Experimentally, when equipped with SPD, Adam consistentlyprovides better in-distribution generalization and out-of-distribution robustnessperformance on multiple popular vision and language benchmarks. Code availableat",
  "Introduction": "Modern optimizers, such as Adam , LARS , and LAMB usually include momentum andadaptive learning rates. They help optimizers avoid local minima and accelerate learning toexplore wider parameter spaces. However, we hypothesize that this behavior is not always beneficialfor fine-tuning from a well pre-trained foundation model, especially when fine-tuning a few layers isalready sufficient for fitting the target data . Several prior works have found that unnecessaryexploration will lead to large deviation from the initialization and worse robustness , andconstraining the deviation can improve a models generalization on in-distribution (ID) data androbustness to out-of-distribution (OOD) data 1. For example, L2-SP imposes aregularization term on the distance between the current and pre-trained models. More recently,TPGM and FTP propose to learn different hard constraints for each layer. These newworks have demonstrated impressive results on benchmarks. However, they are either difficult totune, specialized to specific settings, or require significant computation and storage overhead. Thismotivates us to ask whether a simple few-liner solution exists for this fundamental problem.",
  "022(1)": "where denotes the model parameters, 0 the initialization, L() the original objective function, and the hyper-parameter for regularization strength. When 0 = 0, L2-SP reduces to an ordinary weightdecay. This simple method should be effective enough to constrain the model, as our experimentsshow it can reduce the deviation between the fine-tuned and pre-trained models (Sec. 4.1). However,it is held back by an important design choice: the penalty is always applied to all model parameters.Our empirical results identify that a large prevents every layer from deviating too much and leadsto poor fitting, while a small cannot provide enough regularization. This significantly limits theotherwise effective design (Sec. 4.1). So, what is missing in this algorithm? Recent works in robust fine-tuning and parameter-efficient fine-tuning (PEFT) have shown thatcustomizing constraints for each layer and selectively choosing layers for fine-tunning can improverobustness . Inspired by these findings, we hypothesize that selectively imposing theregularization to different layers is the key. Therefore, we propose a simple selective version ofL2-SP weight decay: selective projection decay (SPD). This new algorithm innovates in two aspects:a selection condition and a regularization strength ratio. The former determines when to applyregularization to a layer, and the latter determines the strength of regularization for intuitive hyper-parameter tuning. Specifically, we derive the selection condition from hyper-optimization by treating the condition as an optimizable parameter (Sec. 3.3), and the regularization strengthratio by re-writing L2-SP as a projection operation (Sec. 3.4). Intuitively, when the condition ismet, the algorithm imposes large regularizations on selected layers. This allows the algorithmto avoid unnecessary deviation and simultaneously fit into the fine-tuning data. We test SPD onlarge-scale computer vision, and NLP benchmarks with popular foundation models and test ID andOOD performance on various distribution and domain shifts. SPD achieves SOTA performance whilebeing much simpler than other competing methods. Our contributions are: We propose a selective projection decay, a selective variant of the popular L2-SP/weightdecay regularization methods, for robust fine-tuning of large foundation models. We showthat selectivity is important to make regularization effective. We conduct a detailed study of ID and OOD performance on image classification andsemantic segmentation with natural distribution and domain shifts. SPD improves ID andOOD performance on these benchmarks.",
  "Related Works": "Robust Fine-Tuning with Distance Regularization. Constraining the distance or deviation betweenthe fine-tuned and pre-trained models has been studied in several prior works. L2-SP explicitlyadds an L2 norm penalty on the deviation and shows improved ID generalization for fine-tuning.MARS-SP studies different forms of norms as the penalty. It shows that the Matrix Row Sum(MARS) norm can be a superior alternative to the L2 norm. These two methods impose softpenalties and can be less effective . Instead, LCC proposes constraining the deviationthrough direct projection on the parameters, which also enforces a hard constraint on the Lipschitzcontinuity of the fine-tuned model. However, LCC is hard to tune because the projection radius isnot an intuitive hyper-parameter. Furthermore, using a single projection constraint for all layers isnot an ideal strategy . More recently, TPGM proposes to automatically learn the constraintsin LCC during fine-tuning, customizing a different projection radius for each layer through a bi-level optimization scheme. FTP further improves the computation efficiency of TPGM byadopting hyper-optimization in its computation. Nevertheless, FTP is still difficult tocontrol because hyper-optimization requires a secondary optimizer with additional optimizationhyper-parameters, and the learned regularization can be too strong with no intuitive way to adjust. Incontrast, SPD is a much simpler and more intuitive method, which can be implemented with just afew lines of code. The superior controllability makes SPD potentially applicable to more applications. Parameter Efficient Fine-Tuning (PEFT). PEFT methods such as adapters and LoRA have been proposed to reduce training memory usage and computation complexity. Recent workshave found that PEFT methods also provide good robustness because they modify fewer parametersand retain more knowledge of the pre-trained models . Surgical fine-tuning concludes thatfine-tuning a selective few layers can improve ID generalization. These new works motivate usto re-evaluate L2-SP and weight decay, often uniformly applied to all layers. We identify that theinferior performance of the simple methods is because of this uniformity, which exhibits a strongtrade-off between fitting and regularization. Other robust fine-tuning methods, such as LP-FT and FLYP , focus on feature distortion. We will review them in the Appendix 8.1. Other Robust Fine-Tuning Methods. WiSE-FT discovers that linearly interpolating betweenthe fine-tuned and pre-trained models after fine-tuning can improve out-of-distribution robustness.This demonstrates that a closer distance to the pre-trained model can improve robustness. However, itonly applies to models with zero-shot capabilities. Another orthogonal line of research for robust fine-tuning focuses on feature distortion. LP-FT shows that fine-tuning with a randomly initializedhead layer distorts learned features. It proposes a simple two-stage method to train the head layerfirst and then fine-tune the entire model. FLYP shows that fine-tuning a foundation modelusing the same objective as pre-training can better preserve the learned features. Our contributionis an optimization method to penalize the derivation between the fine-tuned and pre-trained modelsexplicitly during fine-tuning, which is orthogonal to them.",
  "Selective Projection Decay (SPD)": "Formulation. SPD is a regularization technique that penalizes significant deviation from the pre-trained model. We motivate the formulation from an existing method: L2-SP (Eq. 1). L2-SPadds a distance penalty on the deviation between the fine-tuned and pre-trained models. The penaltyis applied to all model parameters at all times. A large prevents every layer from deviating toomuch and empirically leads to poor fitting, while a small cannot provide enough regularization.This significantly limits the otherwise effective design. We propose a selective version of this simpletechnique: selective projection decay (SPD). We will examine L2-SP and SPD in Alg. 1 and Alg. 2.",
  "Notations. We follow the notations in prior works . Let mt, vt denote the moving average ofthe gradient and squared gradient, 1, 2 their hyper-parameters, and the learning rate": "Alg. 1 shows the Adam optimizer with the L2-SP regularization in Eq. 1. The effects of theregularization are highlighted in blue, also shown in Eq. 2. Intuitively, the regularization leads toan interpolation-like equation2. If the product = 1, then t 0 and if = 0, then t t,where 0 and t denote the initialization and the updated model without regularization.",
  "Intuition Behind SPD": "SPD prioritizes layers with consistent improvement. SPD adds regularization on layers that meetthe condition ct < 0 to slow their growth. The condition is determined by the sign of the innerproduct between two vectors. One vector is the negative gradient direction (gt), i.e., the descentdirection, and the other is the current progress direction (t1 0). The inner product betweenthem measures the alignment between the vanilla3 update direction and the progress so far. Whenthe inner product is positive, the current progress direction generally points to a low loss region, andfollowing it will lead to consistent loss reduction. Conversely, if the inner product is negative, thecurrent progress direction will likely lead to a higher loss region, indicating inconsistent improvement.In this case, SPD will impose a penalty to slow down updates for those layers. Recall that modernoptimizers use momentum to escape local minima and explore wider regions. Without this penalty,the model will likely head towards the higher loss region to overcome it. SPD chooses to slow downthese layers and prioritizes layers with more consistent loss reduction. We will motivate this strategyin a principled manner and validate it in our experiments.",
  "Deriving Condition from Hyper-Optimization": "Previously, we explained the intuition behind SPD. Specifically, we interpreted the condition ct as ameasure of alignment and a test of update consistency. Nevertheless, there is a more profound reasonwhy the quantity ct is a natural choice for selective regularization. In this section, we motivate SPDfrom a more mathematical perspective. Hyper-Optimization Setup. Hyper-optimization is a technique to optimize hyper-parameters insidean optimizer . They treat the hyper-parameters as trainable parameters and optimize themusing another gradient-based optimizer. Lets start from the vanilla Adam with L2-SP algorithm(Alg. 1) and treat the regularization strength hyper-parameter as a trainable parameter. To update ,we need to obtain its gradient by taking a derivative w.r.t. after applying it.",
  "= gt+1(t 0).(5)": "Selection Condition ct. Intuitively, if the quantity is negative, applying the update in gradientdescent will increase the value of , thus increasing the regularization strength of L2-SP. Conversely,a positive quantity will decrease the regularization strength. Therefore, the sign(gt+1(t 0))determines the change of regularization strength in the hyper-optimization of . Formally, we definethe condition ct as,",
  "3the direction w/o momentum": "For memory efficiency, we use (t 0) instead of (t 0) because both vectors point in the samedirection and wont affect the sign of ct. This allows us to discard t. Otherwise, we need to keep anadditional copy in memory. In summary, when ct < 0, we apply a regularization for that layer asshown in Alg. 2. This calculation is done for each layer, and the regularization is selectively applied. Alternative Interpretation: We just interpreted the selection condition ct in SPD as a measure ofconsistency between the current heading direction and the gradient direction. This perspective ismore valid when the algorithm has accumulated some updates, i.e., t 02 0, and less justifiedwhen a heading has not been established at the beginning of training. To analyze this, we discussthe behavior SPD from the perspective of stochastic optimization when t 02 is small at thebeginning of training in the Appendix 8.1.",
  "Deriving rt from Projection": "The selection condition ct determines when to apply regularization to which layers. However, oneremaining question is the strength of regularization, which is not intuitive to tune. To overcome this,we introduced an analytical quantity, the deviation ratio rt, in Eq. 2 and Alg. 1. In this section, wewill motivate it from the perspective of projection. L2-SP is projection. Projection onto a norm ball is common in constrained optimization. WhileL2-SP is not a constrained optimization problem, its operation bears similarity to projection. Supposewe project a model t to an L2-norm ball with radius centered around its initialization 0. Theequation of projection is the following,",
  "t(9)": "where t := t 02 and t := t1 02 denote the current deviation (before regularization)and the previous deviation from the initialization 0, respectively. We use rt in SPD (Alg. 2) toreplace the learning rate in L2-SP (Alg. 1) to make hyper-parameter () tuning more intuitive.Specifically, suppose the hyper-parameter = 1, then the regularization in SPD is:",
  "Compatibility with PEFT methods": "As shown in Alg. 2, SPD retains a copy of the pre-trained model in memory. This adds additionalmemory requirements to the overhead of vanilla optimizers. While this is practical for moderate-sizedmodels, as fine-tuning focuses more and more on large models, additional memory requirementsbecome undesirable. Fortunately, in extremely large models, the prevalent fine-tuning strategy isparameter-efficient fine-tuning (PEFT), such as LoRA , series adapters , and parallel adapters .SPD is naturally compatible with these methods without the additional memory. Intuitively, SPDselectively projects the current model towards the pre-trained initialization. PEFT methods generallyinitialize new parameters to add to the original model weights. To recover the behavior of SPD, wecan instead project the new parameters towards the origin, equivalent to a selective version of regularweight decay, i.e., replacing 0 with 0 in Alg. 2. Consequently, this does not require a memory copyof the pre-trained model. It consistently improves PEFT fine-tuning for large language models oncommon sense reasoning benchmarks in Sec. 4.4.",
  "h = Wtx = (W0 + Wt)x W0x + WupWdownx(11)": "where W0 Rmn, Wup Rmr and Wdown Rrn are the pre-trained model, up-projection anddown-projection matrices. If r min{m, n}, (WupWdown) is a low-rank approximation of Wt.To regularize the overall deviation Wt W02, it suffices to regularize WupWdown2 to be closeto zero. In this case, SPD acts as selective weight decay on Wup and Wdown individually. In summary, we propose selective projection decay (SPD) to impose strong regularization on layersduring fine-tuning selectively. As shown in , SPD regularizes the deviation of the fine-tunedmodel from the pre-trained model Wt W02 for full fine-tuning and the deviation from the originWt2 for PEFT fine-tuning.",
  "Experiments": "We test Selective Projection Decay on a diverse set of benchmarks, architectures, and tasks todemonstrate its effectiveness. We will test both ID generalization and OOD robustness across variousdomain and distribution shifts. Image Classification. We first analyze the behavior of SPD on conventional image classificationdatasets DomainNet and ImageNet . We use a CLIP ViT-Base model for both experiments asthe pre-trained initialization . Specifically, DomainNet consists of images from several domainswith 345 classes. We fine-tune on one domain and test on all domains. ImageNet is a large-scaledataset with 1000 classes. We fine-tune on ImageNet and test on ImageNet and four variants, namelyImageNet-V2 , ImageNet-A , ImageNet-R , and ImageNet-S . Semantic Segmentation. We further test SPD on the PASCAL-Context semantic segmentationdataset . Following prior works , we use a Swin ViT-Tiny , pre-trained on ImageNet-22K, and Segformer segmentation architecture. To construct the OOD datasets, we followthe popular natural robustness literature and apply four representative image corruptions (fog,defocus blur, Gaussian noise, and brightness) with 5 severity each. We fine-tune on the cleansegmentation data and test on clean and corrupted data. Common Sense Reasoning. Moreover, we show that SPD can benefit PEFT fine-tuning on largelanguage models (LLMs). We use the Commonsense-170K dataset , which consists of trainingdata from eight common sense reasoning benchmarks. Following the prior work , we fine-tuneLLaMa-7B (-13B) using LoRA , series adapters , and parallel adapters . Visual Question Answering. Finally, we demonstrate SPDs superiority on multi-modal task. Weuse Googles recently released PaliGemma pretrained on a broad mixture of large-scale vision-language tasks. We fine-tune on VQAv2 and test on nine OOD datasets using LoRA .For the near OODs, we evaluate on VQAv2s six variants, namely IV-VQA , CV-VQA ,VQA-Rephrasings , VQA-CP v2 , VQA-CE and AdVQA , which cover uni-modal,multi-modal and adversarial distribution shifts from VQAv2. We also include TextVQA ,VizWiz and OK-VQAv2 , which are constructed from different sources than VQAv2, as thefar OOD datasets.",
  "DomainNet Experiments": ": Comparisons between AdamW and Adam-SPD on DomainNet. A pre-trained CLIP ViT-Base model is fine-tuned on each of the five domains in DomainNet and tested on all domains. Eachrow represents the evaluation of a model fine-tuned on a domain. ID performance is highlighted inblue. The last column shows the deviation of the final model from its initialization. Adam-SPD showsmuch better OOD performance with significantly less Deviation (t 02) than vanilla AdamW.",
  "WISE-FT80.9472.4733.1863.3354.2055.5860.82WISE-SPD81.7073.2934.3763.6954.5556.4861.52": "SPD outperforms more complicated works on image classification. Following the training recipefrom the prior work , we fine-tune a CLIP ViT-Base model on ImageNet using Adam-SPD.We use the same hyper-parameters as the prior work and only adjust the regularization hyper-parameter in SPD. In Tab. 3, we observe that Adam-SPD provides the best ID performance (strongID generalization) and best average OOD performance (strong OOD robustness) on four ImageNetvariants. SPD achieves a level of competitive performance with just a few lines of code. SPDssimplicity and strong performance show that selective regularization is a fundamental improvementfor robust fine-tuning. Training Details. For Adam-SPD, we fine-tune the model with a learning rate of 3e 5 and = 1.4.The regularization hyper-parameter is found through cross-validation, and the model with the best IDvalidation accuracy is taken. More details are in Appendix 8.4.",
  "Adam-SPD74.2771.7453.4144.1770.9260.0612.4736.50": "SPD outperforms more complicated works on semantic segmentation. The same trend is observedon semantic segmentation in Tab. 4. Again, SPD achieves the best ID generalization and OODrobustness across four different corruptions. This shows that proper regularization is not onlyimportant for achieving strong ID generalization (performance on the test set) but also for strong OODrobustness (performance on distribution shifted test sets) to domains shift (Tab. 3) and distributionshift such as natural corruptions (Tab. 4). The model fine-tuned with SPD is consistently more robustacross different levels of corruption and severity. Training Details. For Adam-SPD, we fine-tune the model with a learning rate of 1e 4 and = 2.2.The regularization hyper-parameter is found through cross-validation, and the model with the best IDvalidation accuracy is taken. More details are in Appendix 8.4.",
  "Adam-SPD (1.2)72.985.680.792.083.785.671.685.682.2": "SPD is compatible and consistently improves PEFT methods. Previous experiments have shownthat SPD imposes effective regularization for full fine-tuning. Furthermore, SPD can also improve theperformance of PEFT methods. We fine-tune LLaMa-7B (-13B) models on the Commonsense-170kdataset . As shown in Tab. 5, SPD consistently improves regular fine-tuning with AdamW,which uses a uniform weight decay for all tested PEFT methods. This demonstrates that selectiveregularization benefits full fine-tuning and PEFT fine-tuning. Combined with its simplicity, SPD canpotentially improve generalization and robustness for more tasks in deep learning.",
  "VQAv2IV-VQACV-VQAVQA-RephrasingsVQA-CP v2VQA-CEAdVQATextVQAVizWizOK-VQA": "Zero-Shot54.4263.9544.7250.1054.2930.6830.4614.8616.8428.60Vanilla FT(LoRA)86.2994.4369.3678.9086.2171.7349.8242.0822.9248.30Linear Prob.78.2487.8363.8769.6178.4861.6642.9029.6118.8042.27LP-FT(LoRA)85.9793.3065.9376.4986.1672.7345.6831.4119.0143.27WiSE-FT(LoRA)71.3685.0664.5566.4270.8948.7443.9536.9822.4142.35Adam-SPD(LoRA)87.3995.2568.8579.4887.2773.5250.9043.5623.0550.11 SPD shows competitiveness across ID, near OOD, and far OOD datasets on multimodal tasks.Apart from uni-modal tasks, SPD outperforms other baselines on multi-modal tasks. We fine-tunePaliGemma-3B model on VQAv2 dataset with LoRA. In Tab. 6, SPD improves vanilla fine-tuning and other robust fine-tuning methods, achieving best ID and average OOD performance w.r.t.distribution shifts across single modalities such as vision, question, answer and combinations ofmultiple modalities. We also show the performance evaluation for both near and far OOD datasets.SPD is consistently more robust under different types and degrees of distribution shifts. Training Details. For Adam-SPD, we fine-tune the model with a learning rate of 1e 3 and = 0.5.The regularization hyper-parameter is found through cross-validation, and the model with the best IDvalidation accuracy is taken. More details are in Appendix 8.4.",
  "Limitations": "SPD is a selective regularization technique explicitly designed for fine-tuning. While it can betheoretically used for pre-training, it will likely lead to poor performance because it will hinder thetraining of some layers. For fine-tuning, it works well because the pre-trained foundation model isassumed to be a good initialization, and only small changes in a selected few layers can lead to a goodlocal minimum. Furthermore, the level of performance gain depends on how well the foundationmodels are exposed to the fine-tuning and OOD data distributions during pre-training. For example,in the DomainNet experiment (Tab. 1), fine-tuning a CLIP ViT model on any other domain does nothave reasonably good OOD robustness on the Quickdraw domain. One can deduce that Quickdraw isnot well represented in the pre-training data of CLIP ViT.",
  "Conclusion": "Fine-tuning differs from training from scratch because it starts from a good initialization. Therefore,effective regularization is critical to retaining the knowledge of the pre-trained foundation modelwhile fitting a model to the target distribution. We identified that 1) regularization is necessary tokeep the fine-tuned model close to its initialization and maintain robustness; 2) uniform regularizationcan hurt model fitting if regularization is too strong. In this paper, we proposed selective projectiondecay (SPD), a selective version of the popular weight decay/L2-SP regularization method. With anadditional few lines of code, SPD can be integrated into existing optimizers and performs selectiveregularization. It demonstrates superior regularization performance on different tasks and modalitiesin our experiments.",
  "Yang You, Igor Gitman, and Boris Ginsburg. Large batch training of convolutional networks.arXiv preprint arXiv:1708.03888, 2017": "Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli,Xiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. Large batch optimization fordeep learning: Training bert in 76 minutes. arXiv preprint arXiv:1904.00962, 2019. Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance ofinitialization and momentum in deep learning. In International conference on machine learning,pages 11391147. PMLR, 2013.",
  "Sachin Goyal, Ananya Kumar, Sankalp Garg, Zico Kolter, and Aditi Raghunathan. Finetune likeyou pretrain: Improved finetuning of zero-shot vision models. arXiv preprint arXiv:2212.00638,2022": "Juntang Zhuang, Tommy Tang, Yifan Ding, Sekhar C Tatikonda, Nicha Dvornek, XenophonPapademetris, and James Duncan. Adabelief optimizer: Adapting stepsizes by the belief inobserved gradients. Advances in neural information processing systems, 33:1879518806,2020. Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Momentmatching for multi-source domain adaptation. In Proceedings of the IEEE/CVF internationalconference on computer vision, pages 14061415, 2019. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and patternrecognition, pages 248255. Ieee, 2009. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visualmodels from natural language supervision. In International Conference on Machine Learning,pages 87488763. PMLR, 2021.",
  "Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenetclassifiers generalize to imagenet? In International Conference on Machine Learning, pages53895400. PMLR, 2019": "Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Naturaladversarial examples. In Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 1526215271, 2021. Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo,Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness:A critical analysis of out-of-distribution generalization. In Proceedings of the IEEE/CVFInternational Conference on Computer Vision, pages 83408349, 2021.",
  "Yen-Cheng Liu, Chih-Yao Ma, Junjiao Tian, Zijian He, and Zsolt Kira. Polyhistor: Parameter-efficient multi-task adaptation for dense vision tasks. arXiv preprint arXiv:2210.03265, 2022": "Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and BainingGuo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedingsof the IEEE/CVF international conference on computer vision, pages 1001210022, 2021. Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, and Ping Luo.Segformer: Simple and efficient design for semantic segmentation with transformers. Advancesin Neural Information Processing Systems, 34:1207712090, 2021.",
  "Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to commoncorruptions and perturbations. arXiv preprint arXiv:1903.12261, 2019": "Zhiqiang Hu, Lei Wang, Yihuai Lan, Wanyu Xu, Ee-Peng Lim, Lidong Bing, Xing Xu, SoujanyaPoria, and Roy Ka-Wei Lee. Llm-adapters: An adapter family for parameter-efficient fine-tuningof large language models. arXiv preprint arXiv:2304.01933, 2023. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-the Lacroix, Baptiste Rozire, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Openand efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Lucas Beyer, Andreas Steiner, Andr Susano Pinto, Alexander Kolesnikov, Xiao Wang, DanielSalz, Maxim Neumann, Ibrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello,Thomas Unterthiner, Daniel Keysers, Skanda Koppula, Fangyu Liu, Adam Grycner, AlexeyGritsenko, Neil Houlsby, Manoj Kumar, Keran Rong, Julian Eisenschlos, Rishabh Kabra,Matthias Bauer, Matko Bonjak, Xi Chen, Matthias Minderer, Paul Voigtlaender, Ioana Bica,Ivana Balazevic, Joan Puigcerver, Pinelopi Papalampidi, Olivier Henaff, Xi Xiong, Radu Soricut,",
  "Jeremiah Harmsen, and Xiaohua Zhai. PaliGemma: A versatile 3B VLM for transfer, July 2024.arXiv:2407.07726 [cs] version: 1": "Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making theV in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering,May 2017. arXiv:1612.00837 [cs]. Vedika Agarwal, Rakshith Shetty, and Mario Fritz. Towards Causal VQA: Revealing and Re-ducing Spurious Correlations by Invariant and Covariant Semantic Editing. In 2020 IEEE/CVFConference on Computer Vision and Pattern Recognition (CVPR), pages 96879695, Seattle,WA, USA, June 2020. IEEE.",
  "Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh,and Marcus Rohrbach. Towards VQA Models That Can Read, May 2019. arXiv:1904.08920[cs]": "Jeffrey P Bigham, Chandrika Jayant, Hanjie Ji, Greg Little, Andrew Miller, Robert C Miller,Robin Miller, Aubrey Tatarowicz, Brandyn White, Samual White, and Tom Yeh. VizWiz: nearlyreal-time answers to visual questions. Benjamin Z. Reichman, Anirudh Sundar, Christopher Richardson, Tamara Zubatiy, PrithwijitChowdhury, Aaryan Shah, Jack Truxal, Micah Grimes, Dristi Shah, Woo Ju Chee, Saif Punjwani,Atishay Jain, and Larry Heck. Outside Knowledge Visual Question Answering Version 2.0. InICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing(ICASSP), pages 15, June 2023. ISSN: 2379-190X. Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, andHerve Jegou. Training data-efficient image transformers and distillation through attention. InInternational Conference on Machine Learning, volume 139, pages 1034710357, July 2021.",
  "Extended Related Works": "Other Robust Fine-Tuning Methods. WiSE-FT discovers that linearly interpolating betweenthe fine-tuned and pre-trained models after fine-tuning can improve out-of-distribution robustness.This demonstrates that a closer distance to the pre-trained model can improve robustness. However, itonly applies to models with zero-shot capabilities. Another orthogonal line of research for robust fine-tuning focuses on feature distortion. LP-FT shows that fine-tuning with a randomly initializedhead layer distorts learned features. It proposes a simple two-stage method to train the head layerfirst and then fine-tune the entire model. FLYP shows that fine-tuning a foundation modelusing the same objective as pre-training can better preserve the learned features. Our contributionis an optimization method to penalize the derivation between the fine-tuned and pre-trained modelsexplicitly during fine-tuning, which is orthogonal to them.",
  "Interpreting Condition as an Early Layer Selection Criterion": "In previous sections, we interpreted the selection condition ct in SPD as a measure of consistencybetween the current heading direction and the gradient direction. This perspective is more valid whenthe algorithm has accumulated some updates, i.e., t 02 0, and less justified when a headinghas not been established at the beginning of training. This section discusses SPD from the perspectiveof stochastic optimization when t 02 is small at the beginning of training. Inner product of gradients captures gradient variance. Modern deep learning models are trainedby stochastic optimization techniques, e.g., mini-batch SGD, leading to stochasticity due to sampling.We first show that the inner product of gradients captures the variance of a sampling process. Weinvoke a common assumption in the convergence analysis of stochastic gradient descent .Assuming that the stochastic gradient gt is a stationary process G over a short period, with a smallstep size, successive gradients, e.g., gt, gt+1, can be seen as samples drawn from the same distributionG. Given two successive draws g1 and g2, we can approximate the first and second moment of G.",
  "g2 Eg2 E [g] 2= g2 V ar(g)": "Remarks. Eq. 13 shows that the inner product of two consecutive stochastic gradients, under certainassumptions, can be seen as the estimator for the difference between the gradient norm and thevariance of gradients. When the inner product is negative, this indicates that the variance outweighsthe magnitude of the gradient. SPD prioritizes layers with higher expected gain. At the beginning of training, the headingdirection (1 0) is dominated by early gradients. For example, at t = 2 the direction of (1 0)is the same as g1 in Adam. The sign of g2(1 0) is the same as the sign of g2g1. This showsthat the condition ct captures the difference between gradient norm and gradient variance. With thisinterpretation, we show that ct reflects expected performance gain in stochastic optimization. Tosee it, we can invoke the descent lemma for SGD. For an L-smooth function f(W) , the descentlemma for SGD states that,",
  "Remarks. The term on the left hand side E[f(k+1)] f(k) is the expected performance improve-ment for each step. Ideally, this should be a negative quantity. On the right-hand side, we observe": "that improvement depends on two quantities gk2 and V ar(gk). To lower the upper bound, we wanta large gk2 and a small V ar(gk). According to the decoupling Eq. 13, the inner product betweensuccessive gradients approximates this proportionality. Consequently, a negative ct likely indicatesa higher upper bound on the expected gain, meaning a smaller improvement. Therefore, SPD willprioritize layers with potentially larger expected gains.",
  "t t (t1 0)": "The Adam with L2-SP Regularization algorithm in the main paper is not the precise mathematicalimplementation of the original formulation written in Eq. 1. To see the difference, we compare oursand the accurate implementation here in Alg. 3 and Alg. 4. In our implementation, we replacedt1 0 (Alg. 4) with t 0 (Alg. 3). This is done intentionally to improve memory efficiencywhen transitioning to the selective version (see Adam-SPD in Sec. 3.3). We can make the followingsubstitution to see how this modification changes computation. Starting from our implementation,",
  "ct = gt (t2 0).(16)": "At the current time step t, we only have access to the parameters t1 from the previous step t1. Tocalculate the ct in Eq. 16, we would have to store the weights from two steps back in memory. Thisincreases memory consumption of the algorithm. As we have shown in Eq. 15, our implementationonly differs from the original implementation slightly but reduces memory consumption. Therefore,we decided to make this approximation.",
  "Training Details": "DomainNet. We use the vision transformer public repository for DEIT to fine-tune all methods.Standard augmentations are used for all: weight-decay (0.1), drop-path (0.2) , label-smoothing(0.1) , Mixup (0.8) and Cutmix (1.0) . The learning rate is 2e 5 and trained for 60epochs for Tab. 1 and 30 epochs for Tab. 2. We use = 1 for all Adam-SPD results in Tab. 1. Weuse 1 A40 GPU for each experiment. ImageNet. The same procedure as the DomainNet experiment is used for training the ImageNetmodels. Standard augmentations are used for all: weight-decay (0.1), drop-path (0.2) , label-smoothing (0.1) , Mixup (0.8) and Cutmix (1.0) . We fine-tune all methods for 30epochs and use the best hyper-parameters reported by the prior work . For Adam-SPD, wefine-tune the model with a learning rate of 3e 5 and = 1.4. The regularization hyper-parameteris found through cross-validation, and the model with the best ID validation accuracy is taken. Weuse 2 A40 GPUs for each experiment. Pascal Segmentation. We follow the training code released by a prior work . We fine-tune allmethods for 60 epochs and use the best hyper-parameters reported by the prior work. For Adam-SPD,we fine-tune the model with a learning rate of 1e4 and = 2.2. The regularization hyper-parameteris found through cross-validation, and the model with the best ID validation accuracy is taken. Weuse 4 2080Ti GPUs for each experiment. Commonsense-170K. We follow the training code released by a prior work . We report thebest performance from the original paper and compare them with Adam-SPD. For Adam-SPD, wefine-tune the model with an identical hyper-parameter setup as the released code and only adjust theregularization strength . The regularization hyper-parameter is found through cross-validation, andthe model with the best ID validation loss is taken. We use 1 A40 GPU for each experiment. Visual Question Answering. We follow the LAVIS public repository to fine-tune all methods.We use the PaliGemma model pretrained with 224224 input images and 128 token input/outputtext sequences and fine-tune with the precision of bfloat16. Standard hyper-parameters are used for all:learning rate (1e 3), weight-decay (1e 4), optimizer (AdamW), scheduler (Linear Warmup WithCosine Annealing), warm-up learning rate (1e 4), minimum learning rate (1e 4), accumulationsteps (2), beam size (5). The model is trained for 10 epochs with a batch size of 16 for Tab. 6. ForLoRA , we limit our study to only adapting the attention weights and freeze the MLP modulesfor parameter-efficiency, specifically apply LoRA to Wq, Wk, Wv, Wo with r = 8 in Tab. 6. We use = 0.5 for SPD. The regularization hyper-parameter is found through cross-validation, and themodel with the best ID validation accuracy is taken. We use 8 A40 GPU for each experiment."
}