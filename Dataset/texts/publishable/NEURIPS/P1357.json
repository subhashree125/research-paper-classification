{
  "Abstract": "We present Statistical Calibrated Activation Pruning (SCAP), a post-training acti-vation pruning framework that (1) generalizes sparsification by input activations ofFully-Connected layers for generic and flexible application across Transformers,and (2) features a simple Mode-Centering technique to pre-calibrate activationdistributions for maximizing post-training sparsity. Our results demonstrate robustPareto efficiency compared to prior methods, translating to a 1.5 additional LLMdecoding speedup against CATS at iso model quality. SCAP effectiveness isempirically verified across a wide range of models, including recent TransformerDecoders, MoE, Mamba2, Encoding Transformer, and pre-quantized models, high-lighting its practicality and scalability. The code is available here.",
  "Introduction": "Activation sparsity is an emerging model optimization method for efficient deployment of LargeLanguage Models (LLMs). Extensive studies in Lazy Neuron Phenomenon reveal a high preva-lence of activation sparsity at the output of ReLU after pretraining, in both encoder or decoderTransformers, across different model sizes and tasks, including language and vision. Intriguingly,larger models tend to exhibit greater sparsity. Deja Vu coined the term of contextual sparsity anduncovered that, along with the feed-forward networks (FFN), sparsity also exists within the activationof attention layers on a per-input basis. To utilize this sparsity for inference efficiency, sparse neuronpredictors have been introduced to dynamically forecast and skip the redundant operations inattention heads and FFNs. further leverage the locality of sparse neurons to develop efficientweight partitioning strategies, addressing the memory challenges when deploying LLMs on consumerCPU-GPU systems. While sparse activation can be exploited for accelerating inference, prior methods hinge on theinherent sparsity of ReLU, which presents a challenge as ReLU has fallen out of favor in recentLLM families (see ). Due to their greater training convergence, SiLU and GELU haveseen increased adoption, prompting new methods to induce sparsity in their dense activations (see). Relufication advocates the reinstatement of ReLU as the primary activation function andas a means for pruning activations within the LLMs. Variants of Relufication have been explored tomaximize sparsity on non-ReLU LLMs. ReLU2 chains two ReLUs as the activation functionwhile dReLU discards the SiLU but places ReLU at the output of Up and Gate projections,respectively in the GLU-based FFN. Despite attaining high sparsity, the representation of RelufiedLLMs is significantly disrupted, necessitating extensive full-model uptraining and advanced recipes to restore the models capabilities. This process demands cluster-level compute resources, whichare often inaccessible to many. Along with prolonged turnaround times, Relufication inflates costsand limits the scalability of LLM deployment.",
  ": ReLU output is sparse while SiLU andGELU outputs are dense. Extracted from": "In contrast, post-training optimization offers a simpler and more cost-effective approach, as ex-emplified by weight quantization methods such as GPTQ and AWQ. These techniquesdetermine quantization parameters through calibration with a small set of text prompts on pretrainedor instruction-tuned LLMs, making them highly efficient and potentially allowing the optimization tobe executed directly on the target deployment device rather than computing clusters. To our knowledge, CATS represents the first instance of post-training activation sparsification inthe LLM era. Similar to the magnitude-based weight pruning , CATS masks out post-SiLUactivations of Mistral-7B and Llama-7B based on a calibrated threshold. Unlike the rigidly sparseRelufied LLMs, CATS outperforms Relufication by demonstrating that activation sparsity of theGLU-FFNs can be controlled to trade off downstream task performance within 1-2% of the originalmodels, all without any additional training. In this early work, we extend the generality of the post-training activation pruning framework with astatistical pre-processing technique, which we refer to as Statistical Calibrated Activation Pruning(SCAP). Our contributions include: 1. Generalized Activation Pruning for Pareto Efficiency: SCAP proposes to sparsify inputactivations of Fully-Connected (FC) layers, entailing a universal pruning and kernel im-plementation across all FCs within Transformer architectures. This approach goes beyondconventional post-activation sparsification, allowing for flexible sparsity across differentlayers without requiring additional predictor training or custom inference patterns. Wedemonstrate that SCAP surpasses the prior post-training CATS method in both accuracyand sparsity, achieving a more optimal trade-off between computational efficiency and taskperformance. Notably, with only a -1.5% deviation from the baseline on Mistral-7B acrossa set of zero-shot tasks, SCAP attains 48.5% FFN sparsity compared to CATS 33.3%,translating to a 1.5 additional speedup in decoding latency over CATS, a 27.1% overallimprovement against the dense model. 2. Mode-Centering for Enhanced Sparsity: We empirically observed that skewed and shiftedactivation distributions, artifacted by preceding layer, limit prunability. To address this, weintroduce a novel Mode-Centering pre-calibration technique that estimates the mode of anactivation distribution and shifts it to zero while preserving computation outcomes. Thisapproach effectively maximizes sparsity opportunities for L1-based pruning, particularlyin the Down projection of non-GLU FFNs. Empirical results show up to a 44.7% increasein input sparsity with negligible loss in performance of MPT-7B, along with a substantialincrease in Falcon-7B and Vision Transformer. 3. Extensive Model Coverage: We showcased the applicability of SCAP to a wide range ofmodels, including Transformer Decoders, MoE, Mamba2 and Vision Encoding Transformers,as well as pre-quantized models (see ). This highlights the advantages of SCAPin terms of turnaround time and scalability, emphasizing the practicality of post-trainingactivation sparsification.",
  "Generalized Post-Training Activation Pruning": "The premise of acceleration through sparsity lies in the elimination of ineffectual operations involvingzeros. During the decoding phase of LLMs, each newly generated token serves as input for subsequentforward pass, necessitating matrix-vector multiplications (GEMV) at every fully-connected (FC)layer. A sparse input vector (activation) forms a dot product with a correspondingly structured sparsematrix, leading to computational efficiency. The resultant dot product reduces memory bandwidthrequirements by eliminating the need to fetch sparse weight channels, a critical improvement sincememory bandwidth is the primary bottleneck during the decoding phase. It also reduces multiply-add-accumulate compute cycles along the sparse inner dimensions. Challenges of reliance on post-activation sparsity often involves intricate execution schemes tomaximise sparse execution. For instance, CATS adopts post-SiLU activation pruning. To attainhigher sparsity in the GLU-based FFNs, it is necessary to compute the Gate projection, followed bySiLU and pruning operator in advance to identify redundant output channels in the Up projection.However, this approach may be suboptimal when the Up and Gate projections are consolidated intoa single execution, or in cases such as parallel Attention-FFN architectures, where Query,Key, Value, Up, and Gate projections are typically fused for efficiency. An alternate approachis to employ predictors to estimate a priori the locations of sparse neurons in thepost-activation, thereby avoiding unnecessary output channels in the Up projection. Contrary to recent activation pruning techniques, which predominantly target the output of activationfunctions in FFNs, we propose a generalization by sparsifying only the input activations to FClayers. This approach enables a unified calibration process and a generic sparse kernel implementationacross any FC layers within Transformers, including those in attention blocks. It decouples targetedFCs, allowing for a flexible combination of input sparsities, resulting in more and sparser FCs forgreater acceleration. In addition, direct pruning on the input activation of the Up/Gate projection alsoeliminates the additional cost of training predictors, streamlining the optimization process, as well asreducing the inference overhead associated with runtime predictions. ReLU",
  ": Activation Sparsification across methods on SwiGLU": "Concretely, consider a linear layer in Transformer with input activation X RNIC, weight matrixW RICOC and bias b ROC, the output activation Y RNOC is given by:Y = XW + b(1)where N is batch size, IC and OC are input and output channel dimensions of the weight. Thegoal is to induce sparsity in input activation X with a pruning operator. The pruner measures theimportance of neurons (activations) on-the-fly and only propagates downstream the neurons withimportance above a calibrated threshold.",
  "Pruner(X) =Xij,if |Xij| > 0,otherwise, where = Quantile(|Xcalib|, s)(2)": "Considering the cost of online computation, the importance of each activation (neuron) is calculatedusing the L1 norm |Xij|, which is simply the absolute value of the activation. The pruning threshold is a hyperparameter that correlates with the intensity of pruning and can be determined throughcalibration. By feeding few-shot of data, the activation tensors of interest can be saved, forminga representative sample for estimating population importance. Subsequently, a quantile functionapplied to this sample identifies the corresponding importance value for a desired sparsity s. Forexample, setting = Quantile(|Xcalib|, 0.3) implies that thresholding on |X| with value of isexpected to yield 30% of sparsity in activation X. The formulation presented thus far is conceptually similar to CATS, except that our approachapplies pruning to the input activations of the projection layers in Transformers instead of activationfunction in FFN. The resulting dynamic sparse FC is",
  "Sparse FC, Y = XW + b(3)": "In the results, we demonstrate that this approach leads to a more favorable Pareto front and executionscheme. Appendix A provides empirical evidence showing that the observed sparsity, on average,aligns closely with the target sparsity across a set of 10 downstream tasks. In the subsequent section,we will detail further strategies to enhance the sparsity level of activations with skewed and shifteddistributions.",
  "Activation Mode Centering": "The prunability of input activations of FC layers depends on the preceding layer. Empirically, wefound that not all activations can be sparsified to high levels without compromising task performance.Upon analyzing the distribution of these activations, we identified two primary patterns: one with themode centered around zero (a, 3b), and another with the peak away from zero (c). L1-based pruning inherently targets elements within a narrow range around zero, making it partic-ularly effective for zero-centered distributions due to the dense concentration of near-zero values.However, for distributions with a mode away from zero, near-zero elements are less frequent, andachieving higher sparsity requires raising the threshold. This, in turn, introduces non-trivial distortionsto the activation representation. To overcome this limitation, we propose a Mode-Centering Calibration that statistically conditionsthe targeted activations to center their mode to a near-zero value, which in turn improving prunabilitywith L1 thresholding. This calibration, applied prior to activation pruning forms the main ingredientof our proposal, which we name our method as Statistical Calibrated Activation Pruning (SCAP).",
  "Y = (X )W + W + b(5)": "Dynamic mode incurs higher cost due to just-in-time mode estimation and realization of thecompensating W. If is static where its value is determined offline during pre-deployment,the compensating term can be fused to the bias b since W, , b are frozen during deployment.The inference overhead is minimal since is a scalar, requiring only broadcast and element-wisesubtraction from X.Y = (X )W + bfused(6) Determination of mode value, : A fast estimation of mode can be obtained through an empiricalmean or median of the activations collected over a calibration dataset. As illustrated in c,both the mean and median can approximate the actual mode of a distribution. For a more preciseestimation of the mode, a probability density estimation algorithm can be employed, many of whichare readily available in statistical software. One such example is Kernel Density Estimation (KDE),which is implemented in Scipy. The KDE can be looked up for its the mode value. Since thisestimation is typically performed during offline, more involved algorithms are also feasible. Pruner(eq. 2)",
  "Results and Discussions": "Implementation: Our experiments compared SCAP to contemporary activation sparsification meth-ods, including post-training CATS and TurboSparse, a state-of-the-art Relufication technique.We closely aligned our setup with these works by focusing on sparsity within the FFN layers, asmost of these methods do. Specifically, we targeted two pruning locations: one at the input of the Upprojection or the common activation that fans into the Up and Gate projections of the GLU-FFN, andthe other at the input of the Down projection. We grouped activations by these locations across Transformer blocks to prune at a uniform targetsparsity, hence requiring two sparsity levels corresponding to the two groups. We swept the two axesin grid, with increments of 10% (or down to 5% in some cases) within the 20-80% range. SCAP useda calibration set of 64 text slices, each consisting of 256 tokens sampled from the C4 dataset, avalidation set from WikiText, and downstream tasks aligning with the target comparison methodas the test set. We note that mode-centering calibration was only applied to non-GLU FFN, asactivations in GLU were observed to be centered. Further details on the experiments discussed belowcan be found in Appendix D.",
  ": Pareto front of CATS and SCAP (Ours) across LMs, with numbers provided in Section D.1": "Compared to CATS across the Mistral-7B-v0.1 and Llama-2-7B models, SCAP consistently maintainsaccuracy close to baseline zero-shot tasks while achieving higher FFN sparsity. This demonstratesSCAPs Pareto-efficient trade-off between sparsity and task performance. Although the task trade-offis use-case dependent, SCAP offers multiple viable candidates within the commonly accepted -1%tolerance in task accuracy, as highlighted in the shaded region of the . The sharper decline observed in CATS is attributed to its sole reliance on post-SiLU sparsification,limiting optimization to a single axis and enforcing shared sparse channels between the Up and Downprojectors, thereby forgoing alternative sparsity combinations. CATS also overlooks the sparsityopportunities in the Gate projection. In contrast, SCAP applies sparsification more broadly acrossinput activations of FC layers, leading to higher FFN sparsity that effectively utilizes all three FClayers in the SwiGLU FFN. A detailed breakdown of sparsity in the accompanying and 6 further illustrates that theDown projectors activations are more prunable than those of the Up projector. This highlightsthe importance of flexibility in layer-specific sparsification to achieve robust compression. Ourpreliminary results, obtained through a grid search on two group-level sparsity, validate the trade-offefficiency of this approach. The exploration of a more fine-grained, layer-wise sparsity search isdeferred to future studies.",
  "Decoding Speedup": "Our kernel implementation is discussed at length in Appendix B and the latency is confirmed to becomparable to CATS which scales proportionally to sparsity. The primary interest of this sectionis the actual acceleration of decoding stage by activation sparsity. From , we selected a pairof CATS and SCAP-pruned Mistral-7B models that are near-equivalent in task performance, andbenchmarked them for 128-token generation with varying input prompt lengths. The results arepresented in .",
  "Geomean17.7%27.1%": "The table reveals that SCAP consistently out-performs CATS in decoding speedup.Un-derscored by the geometric mean across ex-perimented prompt lengths, SCAP achieves aspeedup of 27.1%, compared to CATSs 17.7%.Crucially, SCAP extends CATSs speedup by1.5 (27.1%/17.7%). This gain stems from thehigher FFN sparsity achievable by SCAP whilemaintaining quality at the same level. The diminishing speedup with increasingprompt length in both methods is expected, asthe growing runtime contribution of attention re-duces the impact of sparse GEMV layers. Thiscould be mitigated by combining activation spar-sity with efficient attention or KV compressionmethods.",
  "Ablations of Activation Mode Centering": "Non-GLU based LLMs like the Falcon and MPT families exhibited limited prunability withpost-training sparsification methods, particularly in the input activation to the Down projection whichoriginates from the GELU function. As shown in , at a -1% relative drop in a set of zero-shottasks, Falcon-7B achieved only 30.5% sparsity, while MPT-7B struggled even more, with only 12.7%sparsity. Applying SCAPs Mode-Centering technique, where activations are shifted by the estimated mode,allows for significant sparsity through subsequent L1 magnitude thresholding without compromisingquality. Notably, for Falcon-7B, the exploitable sparsity in the Down projector increased by 1.6times, rising from 30.5% to 50.3%. MPT-7B showed an even more remarkable improvement, withsparsity jumping by 44.7 points, from 12.7% to 57.4%. We further confirm that mode-centering isalso applicable to Transformer encoder and vision modalities (see bottom 2 rows of ). FCdownSparsity(%) ZeroShotTasks(%) +19.8pts 30.5%50.3% 1%tolerance Falcon7B BaselineSCAPSCAP(ModeCentering)",
  "Comparison to SOTA Relufication": "More elaboration on TurboSparse and the consideration of SCAP input model can be found inSection D.4. From , TurboSparse achieved significantly higher FFN sparsity levelscompared to SCAP, reaching 82.2% versus SCAPs 42.3%. This was primarily driven by two factors:(1) the retrofitting of two ReLUs, enabling a staggering 91.1% sparsity in the Down projector, and(2) the use of a predictor that identified and skipped sparse output channels in the Up and Gate FCs.TurboSparse also demonstrated a higher average score across the tasks governed by the OpenLLMleaderboard. This was largely due to its outsized performance on GSM8K (65.7% vs. 37.9%), whichsignificantly elevated its overall average. It is important to note that TurboSparse benefited from a set",
  "curated pretraining and SFT data, including math-related , which contributed to its efficacyon the Math dataset GSM8K and MMLU": "For the remaining tasks, SCAP-pruned Mistral Instruct models outperformed TurboSparse whileachieving 42.3% FFN sparsity. This was accomplished without the need for hundreds of billions oftokens of uptraining, downstream instruction tuning, predictor training, or the significant demands ofdatacenter-class GPUs. While TurboSparses task performance may potentially benefit from furthertraining for greater language modeling, post-training methods like SCAP democratize activationsparsification with its drastic lower computing resource needs, and controllable sparsity-task trade-offas shown in .1. On sparsity front, we hypothesize that combining SCAP with parameter-efficient fine-tuning could push further.",
  "Conclusions": "In this work, we developed Statistical-Conditioned Activation Pruning (SCAP), a post-training methodthat effectively induces activation sparsity in LLMs without the need for sparsification training. Byfocusing on input activations to FC layers, SCAP generalizes both pruning and implementation withinTransformer, achieving a greater trade-off between computational efficiency and task performancecompared to prior methods. The introduction of Mode-Centering pre-calibration addresses the limitedpost-training sparsity in activations with non-zero mode, leading to substantial increases in theirsparsity. Our experimental results validate these findings. We further present detailingSCAPs applicability across a range of Transformer models, including pre-quantized ones and theemerging MoE and Mamba2, all within a -1% relative task accuracy from their baselines whileattaining sufficient sparsity at targeted activations. This work highlights the potential of post-trainingmethod for activation sparsity, while laying aspects for future explorations in Appendix C.",
  "and Disclosure of Funding": "The authors would like to thank Intel Labs and the OpenVINO team for their valuable discussionsand support throughout this work. We are especially grateful to Tingqian Li, Cheng Luo, Xian FuWong, Gopi Krishna Jha, Nikita Savelyev, and Alexander Kozlov for their collaborative efforts andcontributions, which enriched the development of this work. Yash Akhauri, Ahmed F. AbouElhamayed, Jordan Dotzel, Zhiru Zhang, Alexander M. Rush,Safeen Huda, and Mohamed S. Abdelfattah. ShadowLLM: Predictor-based Contextual Sparsityfor Large Language Models, June 2024. arXiv:2406.16635 [cs]. Keivan Alizadeh, Iman Mirzadeh, Dmitry Belenko, Karen Khatamifard, Minsik Cho, Carlo C.Del Mundo, Mohammad Rastegari, and Mehrdad Farajtabar. LLM in a flash: Efficient LargeLanguage Model Inference with Limited Memory, July 2024. arXiv:2312.11514 [cs]. Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, RuxandraCojocaru, Mrouane Debbah, tienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malar-tic, Daniele Mazzotta, Badreddine Noune, Baptiste Pannier, and Guilherme Penedo. The FalconSeries of Open Language Models, November 2023. arXiv:2311.16867 [cs]. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto,Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, RaulPuri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, BrookeChan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, MohammadBavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, MatthiasPlappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, AlexNichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra,Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer,Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and WojciechZaremba. Evaluating large language models trained on code, 2021. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, AdamRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, ParkerSchuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes,Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson,Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, XavierGarcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, DavidLuan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, ShivaniAgrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat,Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, ZongweiZhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei,Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. PaLM: ScalingLanguage Modeling with Pathways, October 2022. arXiv:2204.02311 [cs]. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and PatternRecognition, pages 248255, 2009.",
  "Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers, March 2023. arXiv:2210.17323[cs]": "Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, CharlesFoster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, NiklasMuennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron,Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A frameworkfor few-shot language model evaluation, 12 2023. Song Han, Jeff Pool, John Tran, and William Dally. Learning both Weights and Connections forEfficient Neural Network. In Advances in Neural Information Processing Systems, volume 28.Curran Associates, Inc., 2015. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu,Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large lan-guage model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposiumon Operating Systems Principles, 2023.",
  "Je-Yong Lee, Donghyun Lee, Genghan Zhang, Mo Tiwari, and Azalia Mirhoseini. CATS:Contextually-Aware Thresholding for Sparsity in Large Language Models, April 2024.arXiv:2404.08763 [cs]": "Zonglin Li, Chong You, Srinadh Bhojanapalli, Daliang Li, Ankit Singh Rawat, Sashank J Reddi,Ke Ye, Felix Chern, Felix Yu, Ruiqi Guo, and Sanjiv Kumar. The Lazy Neuron Phenomenon:On Emergence Of Activation Sparsity In Transformers. 2023. Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, GuangxuanXiao, Xingyu Dang, Chuang Gan, and Song Han. AWQ: Activation-aware Weight Quantizationfor LLM Compression and Acceleration, July 2024. arXiv:2306.00978 [cs]. Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao Song, Anshumali Shrivas-tava, Ce Zhang, Yuandong Tian, Christopher R, and Beidi Chen. Deja Vu: contextual sparsityfor efficient LLMs at inference time. In Proceedings of the 40th International Conference onMachine Learning, volume 202 of ICML23, pages 2213722176, Honolulu, Hawaii, USA, July2023. JMLR.org.",
  "Keiran Paster, Marco Dos Santos, Zhangir Azerbayev, and Jimmy Ba. Openwebmath: An opendataset of high-quality mathematical web text, 2023": "Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, JonathanHeek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently Scaling Transformer Inference.Proceedings of Machine Learning and Systems, 5:606624, March 2023. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unifiedtext-to-text transformer. arXiv e-prints, 2019.",
  "Ben Wang and Aran Komatsuzaki. Gpt-j-6b: A 6 billion parameter autoregressive languagemodel, 2021": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, AnthonyMoi, Pierric Cistac, Tim Rault, Rmi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer,Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, SylvainGugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language processing. In Conference on Empirical Methods in Natural LanguageProcessing, 2019. Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and Byung-Gon Chun.Orca: A distributed serving system for Transformer-Based generative models. In 16th USENIXSymposium on Operating Systems Design and Implementation (OSDI 22), pages 521538,Carlsbad, CA, July 2022. USENIX Association. Zhengyan Zhang, Yixin Song, Guanghui Yu, Xu Han, Yankai Lin, Chaojun Xiao, ChenyangSong, Zhiyuan Liu, Zeyu Mi, and Maosong Sun. ReLU2 Wins: Discovering Efficient ActivationFunctions for Sparse LLMs, February 2024. arXiv:2402.03804 [cs].",
  "ATarget vs Actual Activation Sparsity": "0.30.40.50.60.70.8 0.3 0.4 0.5 0.6 0.7 0.8 ActualSparsity Layer0 0.30.40.50.60.70.8 0.3 0.4 0.5 0.6 0.7 0.8 Layer6 0.30.40.50.60.70.8 0.3 0.4 0.5 0.6 0.7 0.8 Layer12 0.30.40.50.60.70.8 TargetSparsity 0.3 0.4 0.5 0.6 0.7 0.8 ActualSparsity Layer18 0.30.40.50.60.70.8 TargetSparsity 0.3 0.4 0.5 0.6 0.7 0.8 Layer24 0.30.40.50.60.70.8 TargetSparsity 0.3 0.4 0.5 0.6 0.7 0.8 Layer30 arc_challengearc_easyboolqhellaswaglambada_openaipiqasciqtriviaqawikitextwinogrande : Target vs. Actual input Sparsity of FCdown across various Transformer Layers in Llama.Each point represents for a different task, with its empirical sparsity closely aligning with diagonalline, indicating a strong correlation with the target. This illustrates the effectiveness of the L1thresholding based pruning mechanism in maintaining consistent dynamic sparsity across layers forthe majority of input prompts.",
  ": output y": "SCAP proposes a generic sparse-by-input activation across targeted FClayers, entailing a single kernel im-plementation. This implementation,referred to as SCAP_FC in Alg. 2,provides a bias-free version of Eq.2 and 3. If mode-centering is ap-plied, the corresponding mode shift-ing and realization of Eq. 6 is de-tailed in Alg. 3. These proceduresare separated for brevity but can bemerged. Alg. 1 and 2 compare the SwiGLUkernel between CATS and SCAP.CATS requires a rigid computationorder of gate projection path firstand a sparse mask coupled for theUp and Down weights. In contrast,SCAP demonstrates the reusabilityof SCAP_FC, which can also be ap-plicable to FCs in attention block iftheir inputs are sparsified.",
  "We implemented SCAP kernels byadapting the official CATS codesand performed latency benchmarkswith a sweep of FFN sparsity on a": "single NVIDIA L40S GPU (consistent with CATS experiments). Since the FFN is composed ofthree equal-sized layers, FFN sparsity = 2/3 ssilu = 2/3 sx + 1/3 sgated. shows thatSCAPs SwiGLU performs similarly to, or marginally faster than, CATSs, which is expected giventhat the memory and compute savings are proportionate in both cases. The plot is clipped at about55% of FFN sparsity due to CATS reaching over 80% sparsity in the Up and Down projectors. Weanticipate proportional improvements if SCAP prunes to a lower level of FFN sparsity. The main advantage of SCAP is its flexibility in decoupling the sparsity between the Gate, Up,and Down inputs, resulting in a favorable Pareto trade-off in task performance, as experimented in.1. This means achieving higher sparsity at a given task tolerance, leading to overall higheracceleration, as confirmed in .",
  "CAcceleration Challenges of Batched Sparse Activation": "Inference acceleration through activation sparsity has primarily focused on the token-to-token decod-ing phase of language models, operating under the assumption that a single dynamic sparse activationvector can trigger a structured sparse weight pattern, alleviating memory bottlenecks. While manystudies demonstrate significant acceleration using this approach, it is mainly effective for a singlevector (i.e., batch size of 1 in FC layers). However, in practice, generation such as beam search orbatched sampling (common in code generation) which give higher quality outputs require handlingmultiple activation vectors simultaneously. This requires overlapping sparse locations across vectorsto maintain structured weight sparsity. In our analysis of TurboSparse Mistral 7B, one of the sparsest models achieved through training-based sparsification, sweeping the beam width clearly revealed a decline in overlapping sparsity (see). This issue is further exacerbated in high-throughput serving systems, such as vLLM,which employs iteration-level batching. High numbers of parallel batch requests can significantlylimit the overall decoding speedup. On the other hand, the prefill stage of language models and transformer encoders faces similarchallenges, if not more pronounced, as their activations consist of multiple vectors. For example,ViT/DeiT3-large with 384x384 image tokenized by a patch size of 16 entails 576 activationvectors at the FC layers. shows that this model can attain up to 59% of model-wise activationsparsity. While not displayed, we observed that prefill activations exhibit sparsity level similar tothose in the decoding phase. Therefore, relying on overlapping sparsity across vectors is leaving asignificant amount of sparsity untapped for acceleration. We emphasize the need to address theseinference setups to broaden the applicability of acceleration with sparse activation. TokenIndex OverlappingSparsity(%) InputSparsityofoneFCDownovergeneratedtokens BeamWidth",
  "DSupplementary Experiment Details": "The generic implementation of SCAP is outlined at the beginning of . Our implementationis available at here. Essentially, our implementation is primarily based within the Hugging Faceecosystem . All pretrained or instruction-tuned models used by SCAP are directly sourced fromthe model hub. For task evaluations, we leverage the Language Model Evaluation Harness forzero-shot tasks2. When evaluating for the Open LLM leaderboard, we utilize LightEval. In terms of compute resources, calibration and sparsification are performed mostly on a singleA100-80GB GPU for smaller models and up to 4xA100 GPUs for larger models. For zero-shot taskevaluations, we parallelize across more GPUs as needed. Further specific details are provided below.",
  "D.1For .1": "mistralai/Mistral-7B-v0.1 and meta-llama/Llama-2-7b-hf were the input models used forthis study, aligning to CATS. We directly referenced the results reported in . Pareto fronts ofSCAP were constructed based on grid search explained in , with particular task performance,group sparsities presented in & 6. Each evaluation was conducted using an identical set ofzero-shot tasks2.",
  "D.2For .2,": "Our kernel implementation is detailed in Appendix B. This section provides additional implementationdetails for , which benchmarked the actual acceleration of the decoding stage achieved throughactivation sparsity. We profiled the greedy decoding of dense, CATS and SCAP-pruned Mistral-7B inFP32 precision on a single Nvidia L40S GPU with a batch size of 1. This setup was consistent withthe original CATS implementation. We reproduced the CATS models through our own implementation, ensuring tasks were comparableand observed sparsity was at least equivalent to the target. We then extracted the CATS and SCAPpruning thresholds and proceeded with the benchmarks. Each benchmark consisted of a story segmenttruncated to the target number of input tokens, requiring the generation of 128 tokens. The reporteddecoding latency was an average over the generation of the last 127 tokens. For further details, pleaserefer to our codes.",
  "D.4For .4": "TurboSparse , a SOTA Relufication method, retrofits two ReLUs into pretrained LLMs, asillustrated in b. For our evaluation, we utilized the official TurboSparse-Mistral-Instruct,a Relufied version of Mistral-7B that had been uptrained on hundreds of billions of tokens fromcurated datasets and further fine-tuned for instruction following. While it was not possible to align pretraining and instruction datasets perfectly, the closest com-parable model was a SCAP on Mistral-7B-Instruct-v0.2, an instruct fine-tuned version ofMistral-7B-v0.2 by MistralAI. We performed a grid search to identify the SCAP-pruned modelthat maintained overall task performance within a 1% margin of the baseline average. Both methodswere evaluated on the same set of tasks governed by the Open LLM leaderboard using LightEval,with the results and sparsity breakdown provided in . We observed slight variations inTurboSparse scores compared to those originally reported in the paper.",
  "D.5For": "lists the models pruned by SCAP to maintain within a -1% tolerance of their baselineperformance, using the grid search outlined in . All input models, except one, were sourcedfrom the Hugging Face model hub, with hyperlinks provided in the table. The Llama3.1 8B modelwas locally quantized to 8-bit using data-free symmetrical weight quantization via Optimum-Intel.For task evaluation, language models were assessed on zero-shot tasks2, while Vision Transformerswere evaluated using Top-1 accuracy on ImageNet-1k. The Sparsity column records the actual activation sparsity observed during task evaluation, denomi-nated by all targeted FC layers. The last column details the specific FC layers targeted, along withtheir input sparsity for SCAP calibration. For example, the last row represents a better-trained VisionTransformer with pruning applied to the shared input of QKV, input to Output, Up, and Downprojection layers, using SCAP with Mode-Centering. This configuration achieved 59% activationsparsity during ImageNet-1k evaluation.",
  "CATS 50%5005033.374.2-1.5%72.580.194.861.081.978.550.4CATS 70%7007046.772.5-3.8%71.980.092.960.680.374.946.9CATS 90%9009060.046.9-60.6%56.360.042.233.670.937.527.7": "SCAP (sup/gate: 10%, sdown: 50%)13.213.252.726.475.30.0%74.580.496.061.683.480.650.6SCAP (sup/gate: 10%, sdown: 60%)13.213.262.229.675.2-0.1%74.380.595.761.982.880.151.0SCAP (sup/gate: 20%, sdown: 50%)22.922.952.132.675.2-0.1%74.080.595.761.583.980.550.3SCAP (sup/gate: 20%, sdown: 60%)22.922.961.735.875.0-0.3%74.680.595.961.982.680.049.8SCAP (sup/gate: 35%, sdown: 45%)37.037.047.140.474.9-0.6%74.080.495.760.983.880.249.1SCAP (sup/gate: 30%, sdown: 70%)32.732.771.345.674.5-1.0%72.780.395.862.082.479.549.2 SCAP (sup/gate: 40%, sdown: 60%)42.042.061.748.574.2-1.5%71.980.095.160.882.979.848.7SCAP (sup/gate: 40%, sdown: 70%)42.742.771.952.572.8-3.3%69.179.891.860.181.779.247.7SCAP (sup/gate: 40%, sdown: 80%)42.842.881.455.771.6-4.8%67.679.486.560.679.879.148.5SCAP (sup/gate: 50%, sdown: 70%)52.452.472.159.069.4-7.9%70.479.475.556.577.578.547.8SCAP (sup/gate: 50%, sdown: 80%)52.552.581.462.267.8-9.9%69.379.471.555.375.477.746.5SCAP (sup/gate: 60%, sdown: 70%)61.961.972.065.366.7-11.5%68.378.270.852.775.177.144.3",
  "CATS 50%5005033.368.9-2.8%67.576.992.757.172.674.441.2CATS 70%7007046.766.0-7.3%66.975.890.255.065.970.138.1CATS 90%9009060.051.4-37.7%57.466.361.138.562.845.728.1": "SCAP (sup/gate: 30%, sdown: 40%)32.232.242.535.770.7-0.1%69.777.694.057.077.576.243.1SCAP (sup/gate: 20%, sdown: 40%)22.622.642.529.270.7-0.1%68.777.993.557.477.676.343.6SCAP (sup/gate: 10%, sdown: 50%)12.912.952.326.170.7-0.1%69.678.193.457.377.676.242.8SCAP (sup/gate: 20%, sdown: 50%)22.622.652.332.570.7-0.2%69.278.193.757.377.376.542.6SCAP (sup/gate: 30%, sdown: 50%)31.731.751.538.370.6-0.3%70.677.793.457.077.375.742.6SCAP (sup/gate: 35%, sdown: 50%)36.536.551.541.570.3-0.7%69.077.693.356.877.176.142.4SCAP (sup/gate: 35%, sdown: 60%)36.536.561.244.770.0-1.1%68.477.393.557.076.475.142.5SCAP (sup/gate: 40%, sdown: 60%)41.441.461.248.070.0-1.1%68.677.693.856.476.674.642.6SCAP (sup/gate: 40%, sdown: 70%)41.941.971.551.869.6-1.7%68.777.993.056.575.873.941.4 SCAP (sup/gate: 50%, sdown: 60%)51.551.561.955.068.8-2.8%67.076.693.355.475.973.340.4SCAP (sup/gate: 50%, sdown: 70%)51.551.571.558.268.7-3.0%67.377.093.655.174.873.339.8SCAP (sup/gate: 50%, sdown: 80%)51.651.681.061.467.7-4.6%64.775.892.755.173.671.240.8SCAP (sup/gate: 60%, sdown: 70%)61.261.271.464.667.2-5.4%65.575.293.553.074.471.037.8"
}