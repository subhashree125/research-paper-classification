{
  "Abstract": "Widely adopted in modern Vision Transformer designs, Softmax attention caneffectively capture long-range visual information; however, it incurs excessivecomputational cost when dealing with high-resolution inputs. In contrast, linearattention naturally enjoys linear complexity and has great potential to scale up tohigher-resolution images. Nonetheless, the unsatisfactory performance of linearattention greatly limits its practical application in various scenarios. In this paper,we take a step forward to close the gap between the linear and Softmax attentionwith novel theoretical analyses, which demystify the core factors behind the per-formance deviations. Specifically, we present two key perspectives to understandand alleviate the limitations of linear attention: the injective property and the localmodeling ability. Firstly, we prove that linear attention is not injective, which isprone to assign identical attention weights to different query vectors, thus adding tosevere semantic confusion since different queries correspond to the same outputs.Secondly, we confirm that effective local modeling is essential for the successof Softmax attention, in which linear attention falls short. The aforementionedtwo fundamental differences significantly contribute to the disparities betweenthese two attention paradigms, which is demonstrated by our substantial empiricalvalidation in the paper. In addition, more experiment results indicate that linearattention, as long as endowed with these two properties, can outperform Softmaxattention across various tasks while maintaining lower computation complexity.Code is available at",
  "Introduction": "Recent years have witnessed the unprecedented success of Transformer and attention in the fieldof computer vision . Softmax attention, also known as dot-product attention, has demonstratedremarkable expressive power, leading to state-of-the-art performance across various vision tasks . However, applying Softmax attention in vision also faces challenges. The quadraticcomplexity of Softmax attention results in prohibitively high computational cost when applied witha global receptive field. Previous works have strived to reduce the computationcomplexity by restricting receptive fields or introducing sparsity. Although effective, these approachesinevitably compromise Softmax attentions ability for long-range modeling and scalability.",
  "which can be decomposed into kernels, i.e., the linear attention replaces the original score func-tion Sim(Q, K) = Softmax(QK/": "d) with Sim(Q, K) = (Q)(K), where () is the kernelfunction. This substitution enables a change in the computation order from(Q)(K)V to(Q)(K)Vbased on the associative law of matrix multiplication, reducing the complexityfrom O(N 2) to O(N) w.r.t. the sequence length N. Nevertheless, every coin has two sides. Linearattention proves to be less effective than Softmax attention , whose poor expressive powerlimits its practical application. Although many pieces of research have attemptedto alleviate this issue in different ways, we still do not have a complete understanding of the keyfactors that contribute to the gap between linear and Softmax attention. In this paper, we delve into the fundamental differences between linear and Softmax attention,offering two insightful perspectives to demystify the topic: the injective property and local modelingcapability. Firstly, we consider attention as a function that maps a query to an attention score. We findthat the injectivity of this attention function greatly affects the performance of the model. Specifically,if the attention function is not injective, different queries will induce identical attention distributions,leading to severe semantic confusion within the feature space. Our rigorous analysis has demonstratedthat the Softmax attention function is an injective function, whereas the linear attention functionis not. Therefore, linear attention is vulnerable to the semantic confusion problem, which largelyleads to its insufficient expressiveness. Secondly, our analysis of the attention weight distributionhas confirmed that the success of Softmax attention is not solely dependent on its strong long-rangemodeling capabilities. Effective local modeling is also crucial to achieving optimal outcomes. To validate our analyses, we present two simple yet effective methods to endow linear attentionwith the injective property and the local modeling ability, respectively. The widely employed SwinTransformer architecture is used to validate our findings. The results highlight the importanceof both properties in the gap between linear and Softmax attention. Moreover, comprehensiveexperiments demonstrate that linear attention, endowed with these two properties, outperforms thewidely used Softmax attention across diverse tasks. Our main contributions and takeaways are summarized as follows: (1) Injectivity is a key disparitybetween linear and Softmax attention. While Softmax attention is injective, the non-injective natureof linear attention causes semantic confusion and severely impairs model performance. To the best ofour knowledge, our work is the first to conceptualize attention as a mapping function and prove thevital importance of its injective property. (2) Local modeling is still essential to the effectivenessof the attention mechanism, even though it is renowned for its large receptive field and outstandinglong-range modeling ability. (3) We challenge the viewpoint that linear attention is inferior to Softmaxattention and demonstrate that with the above two properties, linear attention can outperform Softmaxattention while maintaining lower computation complexity.",
  "Related Works": "Vision Transformer and Softmax Attention. Vision Transformer is the pioneer work thatintroduces self-attention to vision. Since then, attention has found success in various vision tasks . The widely used attention mechanism is Softmax attention , also known as dot-productattention, which computes the similarity between all query-key pairs. Although effective, its quadraticcomputation complexity leads to unmanageable cost when processing global feature maps. Therefore,various approaches have been proposed to reduce the computational overheadof Softmax attention. PVT employs downsampling of keys and values to reduce computationalcomplexity. Swin Transformer restricts the receptive field by introducing window attentionpattern. NAT mimics convolution and calculates attention within the neighborhood of eachfeature, and DAT presents an input-dependent sparse attention pattern. Linear Attention. As opposed to Softmax attention, Linear attention is another attention paradigmwith a natural linear complexity of O(N). Linear attention replaces Softmax with kernel functions,thereby reducing computational complexity to linear through a change in computation order. Nonethe-less, prior studies have demonstrated that linear attention performs markedlyworse than Softmax attention. CosFormer attributed this discrepancy to the efficient re-weightingmechanism of Softmax attention and proposed cosine re-weighting to enhance linear attention. Nys-trmformer and SOFT use matrix decomposition to further approximate Softmax operation.Efficient Attention applies Softmax function to queries and keys. TransNormer identifies",
  "Legend": ": An illustration of injective property and confusion problem. Non-injectivity leadsto various semantic confusions in linear attention when different kernel functions are employed.(a) With () = ReLU(), linear attention assigns the same attention values to collinear queries ofvarying lengths. (b) Using () = ReLU(A +b), linear attention faces severe confusion problem,producing identical attention distribution for certain queries with different directions and lengths. that unbounded gradients and attention dilution harm linear attention. FLatten Transformer introduces a focused function to address the over-smoothing issue. MLLA draws inspirationfrom Mamba to improve linear attention. Despite their elegant outcomes, the fundamental reason for the disparity between linear attention andSoftmax attention remains unclear. In this work, we perform an in-depth analysis of the disparitiesbetween linear and Softmax attention, identifying two crucial properties of high-performance Softmaxattention: injectivity and local modeling ability. We present both theoretical proofs and experimentalverification to validate our findings.",
  "Preliminaries": "Attention Formulation. Let x RNC be an input of N tokens. In each self-attention head, x istransformed into Q = xWQ, K = xWK, V = xWV through projection matrices WQ/K/V RCd,where C and d are the channel dimension of module and each head. Therefore, we have Q, K, V RNd, and Qi, Ki, Vi Rd. Based on this, Softmax attention computes the attention weightsand calculates the output as the weighted sum of value:",
  "d in exp(Qi Kj/": "d) since we can equivalently renormalize Q and K. Thisattention paradigm has been highly successful in modern vision Transformers. However, it shouldcompute the similarity between all query-key pairs, resulting in O(N 2) complexity. Consequently,employing Softmax attention with a global receptive field results in overwhelming computation cost. Linear attention was proposed to efficiently handle the computation challenge with linearcomplexity of O(N). Specifically, exp(Qi Kj) is replaced by (Qi)(Kj), where is kernelfunction. In this way, linear attentions reformulate eq. (1) as:",
  "Analysing the Gap between Linear and Softmax Attention": "Due to its linear computation complexity, linear attention is considered a promising solution toaddress the computational challenges of Softmax attention in high-resolution scenarios. However,previous works have shown that linear attentions expressive power is significantly lowerthan that of Softmax attention, rendering it impractical for real-world applications. In this section, weconducted an in-depth analysis of the gap between linear and softmax attention from two perspectives:injective mapping and local modeling capability, and offer both theoretical proofs and experimentalverification to enhance understanding of the key disparities between these two attention types.",
  "SK, LK : Rd RN,SK(Qi) = Si,LK(Qi) = Li,(3)": "where Qi denotes the query, and Si, Li are the attention scores in eq. (1) and eq. (2). Given keysK RNd, SK, LK can be viewed as the function of query q, mapping each q to its correspondingSoftmax and linear attention scores, SK(q) and LK(q). Then the final outputs of Softmax and linearattention corresponding to q can be formulated as OS = SK(q)V and OL = LK(q)V . Injective property. In this work, we identify that the injective property of the attention functionsignificantly impacts model performance, which may largely contribute to the gap between linearand Softmax attention. Specifically, we prove that under mild assumptions, the Softmax attentionfunction SK is injective, whereas linear attention function LK is not (Proposition 1 and 2. Please referto Appendix for complete proof). As a consequent, for two different queries p and q (p = q), Softmaxattention should produce different attention distributions SK(p) = SK(q), while linear attentionmay yield the same linear attention values LK(p) = LK(q). Since different queries p = q typicallyrepresent distinct semantics, the non-injective property of linear attention actually leads to semanticconfusion, i.e. LK(p) = LK(q) and OLp = LK(p)V = LK(q)V = OLq , making the model unableto distinguish certain semantics.",
  "Proposition 2 (Linear attention is not injective) Let : Rd Rd be a continuous function. p, q Rd, p = q, s.t. LK(p) = LK(q)": "We provide an example to better understand the injective property and confusion problem. As shownin (a), there are four collinear vectors with different lengths. Benefiting from injectivity, Softmaxattention ensures that each of these four queries obtains distinct attention scores, producing morefocused attention distributions for longer queries. Nevertheless, with kernel function () = ReLU(),linear attention fails to distinguish the same semantics with different intensities, i.e. collinear querieswith varying lengths, resulting in identical attention scores for all these four queries. Consequently, linear attention is unable to yield more focused attention scores for stronger semantics, whichmay explain the lack of focus ability discussed in . When using kernel functions with strongernonlinearity, linear attention encounters more pronounced confusion issues. For instance, in (b),employing kernel function () = ReLU(A +b), linear attention assigns exactly the same attentionscores to four queries with different directions and lengths. This serious semantic confusion candirectly impair models performance. 0121 22 23 24 25 26 27 28 29 210 211 212",
  ": The distribution of thenumber of times each image encoun-ters confusion during inference": "Confusion problem in real models. While illustratesthe concept of confusion, it is also crucial to verify if this issueoccurs in real models. Therefore, we conduct statistical analy-sis based on Deit-T. We count the occurrences of confusion foreach image (i.e., p = q but SK(p)=SK(q) or LK(p)=LK(q))during inference on the ImageNet validation set. As itis rare for two vectors to be strictly equal in floating-pointrepresentation, we consider them approximately equal if theL2 norm of their difference is less than 1e-3. The resultsare provided in . Almost all samples did not encounterconfusion on model employing Softmax attention, whereas alarge number of samples encountered confusion more than 25times on linear attention model. This proves the existence ofconfusion problem with linear attention in real models.",
  "Acc.72.270.669.9": "The importance of injectivity. Wefurther verify the importance of injec-tivity by inducing confusion in Soft-max attention. To achieve this, weapply additional non-injective map-ping functions to each query before the Softmax attention calculation, i.e., introducing Qi =f(Qi)prior to eq. (1), where f is a non-injective function. Specifically, we use f1(q) =ReLU(q)",
  "ReLU(q) tosimulate the confusion observed in linear attention using the kernel function () = ReLU(), asdepicted in (a), and employ f2(q)=ReLU(Aq+b)": "ReLU(Aq+b) to replicate the confusion in (b). Asshown in Tab. 1, introducing confusion leads to an obvious decrease in performance, underscoringthe crucial role of the attention functions injective property. Therefore, the non-injectivity of linearattention is likely a key factor leading to its limited expressive capacity. Make linear attention injective. We propose a simple yet effective solution to make linear attentionan injective function. The proof of Proposition 2 (see Appendix) demonstrates that = 0, (p)obtains identical scores in linear attention due to the omission of in division, resulting in non-injectivity. Hence, we simply transform the normalization of linear attention from division tosubtraction, presenting our injective linear attention (InLine) as follows:",
  "N ,(4)": "and the attention output corresponding to Qi can be written as OIi = InLK(Qi)V . This modificationensures that the attention weights still sum up to 1, while transforming the attention function into aninjective one (see Proposition 3). Thus, injective linear attention can distinguish different queries,akin to Softmax attention, and it no longer suffers from confusion problem.",
  ": Visualizations of attention distributions. Softmax attention exhibits strong local bias. Theother two attention types yield meaningful attention distributions, but focus more on global modeling": ": Model performances on ImageNet-1K when masking out tokens from different positions.Loc. kk means masking out tokens in local kk windows for each query. Rand n representsrandomly masking out n tokens out of local 33 windows for each query. The attention scores ofeach query still sum up to 1. These models are tested directly without retraining.",
  ": The sum of attention scores in thelocal 33 windows of each query from DeiT-T": "In , we compute the sum of attention valuesassigned to local 33 neighborhoods for each queryusing DeiT-T. With a total of 1414+1=197 tokensin each attention layer of DeiT-T, if attention scoresare randomly assigned, the expected sum of attentionfor a 33 neighborhood would be9 197. The resultshows that all three attention paradigms tend to paymore attention to the neighborhoods of each query,revealing local bias, especially in shallow layers.Notably, Softmax attention allocates a substantialamount of attention to local windows, suggestinga stronger local modeling ability compared to theother two attention paradigms. Visualizations are provided in to further confirm this finding. We speculate that Softmax attentions superior performance stems from robust local priors and stronglocal modeling capabilities. To validate this hypothesis, we employ attention masks to mask outtokens from various positions and assess their effect on model performance.The results are presentedin Tab. 2. Two key observations emerge: 1. Masking out local tokens significantly decreases modelperformance, while randomly masking out the same number of tokens has a minor impact on results.2. Softmax attentions performance suffers more severely than InLine attention when local tokens aremasked out. These findings demonstrate the significance of local modeling for both attention typesand prove that Softmax attentions advantage over InLine attention primarily attributes to its strongerlocal modeling ability. Based on our analysis, increasing local bias may enhance the expressive power of InLine attention.In light of this, we employ a MLP to predict additional local attention residual for InLine attention.",
  "j=1rjV N(i)j, r = MLP(x),(6)": "where InLK() denotes InLine attention function, x is the average of input tokens, r is the predictedlocal attention residual, and V N(i)jrepresents the value in the 33 neighborhood of Qi. In this way,we explicitly enhance InLine attentions local bias by introducing local attention residual term. Werefer to InLine attention with local attention residual, i.e. eq. (6), as InLine attention module. Asthe local residual term introduces little computational cost Nd + d2 + 9Nd, the InLine attentionmodule still maintains a linear complexity of O(N).",
  "Implementation": "We utilize the popular Swin Transformer architecture to investigate the effects of injectivityand local modeling capability. Specifically, we substitute the original Softmax attention in Swin-T with linear attention to establish the baseline model. Subsequently, we introduce the injectiveproperty and local bias in turn to assess their respective impacts. To fully verify the effectiveness ofInLine attention module, we further apply it to four advanced and representative Transformer modelsincluding DeiT , PVT , Swin , CSwin and offer broad comparisons with variousstate-of-the-art methods using Softmax attention.",
  "Datasets and Experiment Details": "ImageNet classification. The ImageNet-1K recognition dataset contains 1.28M training imagesand 50K validation images with a total of 1,000 classes. For a fair comparison, we train our modelusing identical settings as the corresponding baseline model. We use AdamW optimizer to trainall our models from scratch for 300 epochs, employing cosine learning rate decay with 20 epochs oflinear warm-up. The initial learning rate is 1 103, and the weight decay is 0.05. Augmentationand regularization strategies consist of RandAugment , Mixup , CutMix , and randomerasing . Following CSwin , EMA is used in the training of InLine-CSwin models. COCO object detection. COCO object detection and instance segmentation dataset has 118Ktraining and 5K validation images. We follow the training and testing strategies of the correspondingbaseline model and employ pretrained InLine backbones to conduct experiments. ADE20K semantic segmentation. ADE20K is a well-established benchmark for semanticsegmentation which encompasses 20K training images, 2K validation images and 150 semanticcategories. The same setting as baseline model is adopted.",
  "Empirical Analysis of Injectivity and Local Modeling": "Injective property. As shown in Tab. 3, we adopt four different kernel function () to validate theeffect of injectivity. As discussed in Sec. 4.1, with kernel function () = ReLU(), linear attentionfails to distinguish the same semantics with different intensities. Addressing this issue with InLineattention leads to a 2.5 increase in accuracy. When using () = ReLU(A +b), linear attentionfaces more severe semantic confusion, and introducing injective property results in a significantaccuracy boost of 9.8, from 70.2 to 80.0. These obvious improvements fully prove the significance ofinjectivity and validate the effectiveness of our injective linear attention. We also employ two kernelfunctions that do not ensure non-negativity. Consistent with the findings in , linear attentionfails to converge without non-negativity assurance. We attribute this to extreme semantic confusion.For example, with () = Identity(), linear attention is unable to distinguish completely oppositesemantics, assigning identical attention scores to q and q. Local modeling capability. Tab. 4 highlights the importance of local modeling capability. In the lefttable, we apply pure InLine attention to Swin-T and gradually increase the window size from 72 to562. Due to the linear complexity of InLine attention, we can adopt different window sizes whilepreserving identical computational cost. Larger window sizes lead to larger receptive fields, typicallyassociated with improved performance. However, the results show that the model performance doesnot improve with increasing window sizes. We believe this can be attributed to the insufficient localmodeling capability: a small window size restricts the receptive field but introduces strong localbias, enhancing local modeling, while a large window size enlarges the receptive field but furtherdiminishes local modeling ability. To validate this, we apply InLine attention with local residual andpresent the results in the right table. Significant improvements can be observed upon the introductionof the local residual term. Additionally, the increase in window size leads to steady performanceimprovement after introducing local residual, which strongly supports our analysis.",
  "Main Results and Broad Comparisons": "As shown in Tab. 4, InLine-Swin-T with local residual achieves better results than the Swin-T baseline,increasing from 81.3 to 82.4. Therefore, we wonder whether InLine attention module can performbetter than the widely adopted Softmax attention in various scenarios. To validate this, we furtherapply it to several representative Transformers and conduct comprehensive comparisons on imageclassification, object detection, and semantic segmentation. ImageNet classification. Firstly, We apply our InLine attention module to DeiT , PVT ,and Swin Transformer , presenting the results in Tab. 5. It can be seen that substituting Softmaxattention with our method results in notable improvements. For example, InLine-PVT-S outperformsPVT-L with 30% of the parameters and 40% of the FLOPs. Subsequently, We apply our module tothe advanced Transformer design, CSwin Transformer , and offer a broad comparison with variousstate-of-the-art models on ImageNet-1K. As depicted in Tab. 6, our InLine-CSwin model not onlyyields better results than CSwin, but also surpasses various SOTA CNN and Transformer designs.These results demonstrate that our InLine attention module tends to be a superior alternative to thewidely used Softmax attention. Inference throughput analysis. We offer real speed measurements in . As shown in (a),InLine models achieve an obviously better trade-off between accuracy and latency. In (b), weincrease the window size from 72 to 562. Due to the quadratic complexity of Softmax attention,Swin-Ts speed drops sharply as window goes larger. On the contrary, InLine-Swin-T with linearcomplexity even exhibits higher speed with larger windows. This may be due to the reduction in thelatency caused by the window partition. With a global receptive field, InLine benefits from both highperformance (see Tab. 4) and fast speed. Furthermore, (c) shows the significant computationaladvantage of InLine in high-resolution scenarios.",
  "InLine (Ours)6.5M1.1G74.5": "COCO object detection. Tab. 7 shows that In-Line attention consistently improves the resultsin object detection tasks. For instance, InLine-PVT-S outperforms PVT-T with 6.7 box AP un-der similar FLOPs, and InLine-PVT-L surpassesPVT-M by 3.4 box AP with fewer FLOPs, show-ing the advantage of InLine attentions linearcomplexity in high-resolution scenarios. ADE20K semantic segmentation. We employour model on two representative segmentationmodels, SemanticFPN and UperNet .As depicted in Tab. 8, benefited from injectivityand effective local modeling ability, Our InLineachieves better results under all settings withobviously lower computational cost. Comparison with SOTA linear attention de-signs. As shown in Tab. 9, our simple InLineattention design outperforms various linear at-tention methods without bells and whistles. Ad-ditionally, our Inline attention can possibly in-tegrate with previous designs to achieve betterresults, which we leave for future work. Forinstance, the advanced focused function in FLat-ten can also be employed in InLine attention.",
  "Ablation Study": "The effectiveness of our two key designs has been verified and detailed analyzed in Sec. 5.3. InTab. 10, we offer additional results to validate the impact of different kernel functions. It is shownthat our InLine attention can effectively work with different kernel functions, further validating theeffectiveness of our method. The ReLU and Exponential functions achieve slightly better results. Inthis paper, we use Identity() as default for simplicity.",
  "Conclusion": "In this paper, we shed some light on the core factors leading to the performance gap between linearand Softmax attention. We identify and validate two fundamental disparities between these twoattention paradigms: injective property and local modeling capability. Injectivity implies that theattention function assigns distinct attention scores to queries with varying semantics, reflectingthe ability to distinguish different semantics. Using different kernel functions, linear attentionsnon-injectivity results in various semantic confusions. Furthermore, despite being recognized fortheir robust long-range modeling capability, attention mechanisms heavily depend on effective localmodeling for impressive results. Thorough empirical validation unequivocally supports our analyses.Our findings also demonstrate that with the above two properties, linear attention can outperformSoftmax attention with lower computation complexity.",
  "Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scalehierarchical image database. In CVPR, 2009. 5, 7, 15": "Xiaoyi Dong, Jianmin Bao, Dongdong Chen, Weiming Zhang, Nenghai Yu, Lu Yuan, DongChen, and Baining Guo. Cswin transformer: A general vision transformer backbone withcross-shaped windows. In CVPR, 2022. 1, 7, 8, 9 Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021.1, 2",
  "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. 1, 2, 3": "Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, PingLuo, and Ling Shao. Pyramid vision transformer: A versatile backbone for dense predictionwithout convolutions. In ICCV, 2021. 1, 2, 7, 9, 16 Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu,Ping Luo, and Ling Shao. Pvt v2: Improved baselines with pyramid vision transformer.Computational Visual Media, 2022. 2, 8",
  ". c = 0. Then eq. (8) pKj =qKj K(p q)=0. As rank(K) = d, we have p = q,which contradicts the assumption that p = q": "2. c = 0. Then eq. (8) pKj + c = qKj Kj p + c = Kj q Kj (p q)/c = 1.Therefore, we have Kj (p q)/c = 1 K(p q)/c = 1N1, which contradicts the factthat rank([K, 1N1]) = d + 1 and equation Kx = 1N1 does not have a solution.",
  "Both cases arrive at a contradiction, which proves the original proposition": "Notably, the two assumptions rank(K) = d and rank([K, 1N1]) = d + 1 are easy to satisfy inreal models. In practice, the number of tokens N is usually much larger than head dimension d. Forinstance, in the first stage of Swin Transformer , N = 562 = 3136 and d = 32. Therefore, wehave K RNd, N > d. If rank(K) < d, it indicates that the N key tokens lie in a low-dimensionsubspace of Rd. If rank([K, 1N1]) < d + 1, it means that the N key tokens are in a hyperplaneof Rd. Given that N is much greater than d, it is highly likely that the N key tokens have variousdirections in Rd, instead of locating in a low-dimension subspace or hyperplane. In this case, it holdsthat rank(K) = d and rank([K, 1N1]) = d + 1.",
  "Proof. Let f : U S, f(x) =(x)": "(x), U =x | (x) = 0, x Rd, S =x | x = 1, x Rd. is a continuous function, so f is continuous on U. is an injective function, so it has no more thanone zero point. Therefore, U = Rd orx | x = x0, x Rdis an open subset of Rd, where x0 isthe zero point of . f(U) S is not an open set, as every point of f(U) is not an interior point. Assume f is injective. Then f : U S is a continuous injective map. According to BrouwerInvariance of Domain Theorem, U is an open set f(U) is also open, which contradicts the factthat f(U) is not an open set.",
  "DLimitations": "In this paper, we shed some light on the core factors leading to the performance gap between linearand Softmax attention. We identify and validate two fundamental and essential disparities betweenthese two attention paradigms: injective property and local modeling capability. Firstly, we provethat linear attention is not injective, which is prone to assign identical attention weights to differentquery vectors, thus adding to severe semantic confusion problem. Secondly, we confirm that effectivelocal modeling is important for the success of Softmax attention, in which linear attention falls short.The aforementioned two essential differences significantly contribute to the disparities between thesetwo attention paradigms, which is unequivocally proved by our thorough empirical validation in thepaper. However, there may be other differences between Softmax and linear attention, and this paperis not exhaustive."
}