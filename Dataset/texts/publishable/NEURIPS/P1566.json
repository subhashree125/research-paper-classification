{
  "Abstract": "Object-centric learning (OCL) aims to learn representations of individual objectswithin visual scenes without manual supervision, facilitating efficient and effec-tive visual reasoning. Traditional OCL methods primarily employ bottom-upapproaches that aggregate homogeneous visual features to represent objects. How-ever, in complex visual environments, these methods often fall short due to theheterogeneous nature of visual features within an object. To address this, wepropose a novel OCL framework incorporating a top-down pathway. This pathwayfirst bootstraps the semantics of individual objects and then modulates the modelto prioritize features relevant to these semantics. By dynamically modulating themodel based on its own output, our top-down pathway enhances the representa-tional quality of objects. Our framework achieves state-of-the-art performanceacross multiple synthetic and real-world object-discovery benchmarks.",
  "Introduction": "Object-centric learning (OCL) is the task of learning representations of individual objects fromvisual scenes without manual labels. The task draws inspiration from the human perception whichnaturally decomposes a scene into individual entities for comprehending and interacting with thereal world visual environment. Object-centric representations provides improved generalizationand robustness , and have been proven to be useful for diverse downstream tasks such as visualreasoning , simulation , and multi-modal learning . In this context, OCL which learnssuch representations without labeled data has gained increasing attention. A successful line of OCL builds upon slot attention . This method decomposes an image intoa set of representations, called slots, that iteratively compete with each other to aggregate imagefeatures. Reconstructing the original image from the slots, they are encouraged to capture entitiesconstituting the scene. This simple yet effective method has been further advanced by novel encoderor decoder architectures , optimization technique , and new query initializationstrategies . It is worth noting that all these methods are fundamentally considered bottom-up models, as theyrely on aggregating visual features without incorporating high-level semantic information from thebeginning. This bottom-up approach assumes that visual features within an object are homogeneousand can be clustered in the feature space, which only holds for simplistic objects that can be identifiedusing low-level cues such as color . In complex real-world scenarios where visual entities ofthe same semantics exhibit diverse appearances, this homogeneity often breaks down, leading tosuboptimal object representations . Thus, we take an approach different from the previousline of research: introducing top-down information into slot attention, such as object categories andsemantic attributes.",
  "Codebook": ": The overall pipeline of our framework. A top-down pathway is introduced into slotattention to utilize top-down information. The pathway consists of two parts: bootstrapping top-downknowledge and exploiting them. Firstly, semantic information is bootstrapped from slot attentionoutputs by mapping slots to discrete codes from a learned codebook through vector quantization.Secondly, slot attention is modulated using these codes and its attention maps, transforming it intoa self-modulating module. Inner activations are modulated across channels with codes and acrossspace with centered attention maps. Slot attention is then repeated with these modulated activations,yielding more representative slots. Incorporating top-down information enables slot attention to specialize in discerning objects withinspecific semantic categories. For instance, identifying vehicles in a complex urban environmentcan be challenging due to the diverse and cluttered nature of the scene. Top-down information canguide the model to prioritize vehicle-specific features, such as wheels and windows. This inhibits thecontributions of irrelevant features when computing slots, and enhances the aggregation of visualfeatures of individual vehicles into slots. Nevertheless, devising such a top-down approach is notstraightforward since OCL assumes an unsupervised setting without any labeled data, making it hardto identify and exploit the high-level semantics typically obtained from annotated datasets. We propose a novel framework that incorporates a top-down pathway into slot attention to provide andexploit top-down semantic information; illustrates of our framework. The pathway consists oftwo parts: bootstrapping semantics and exploiting them for better representations. Firstly, top-downsemantic information is bootstrapped from the output of slot attention itself, by mapping continuousslots to discrete codes selected from a finite learned codebook. Such an approach allows the codebookto learn prevalent semantics in the dataset, with each code representing a specific semantic concept.Thus, semantic information can be bootstrapped without any object-level annotations and used toprovide top-down semantic information. Secondly, slot attention is modulated using bootstrappedtop-down cues obtained from the first phase, which we call self-modulation. In this phase, thetop-down pathway dynamically guides the slot attention by re-scaling its inner activations basedon the top-down information. This self-modulation process enables the model to focus on featuresub-spaces where object homogeneity is more consistent, thereby improving its performance indiverse and realistic settings.",
  "Related Work": "Object-centric learning OCL aims to learn representations of individual objects within an image.The object-centric dimension is orthogonal to the conventional representation learning whichlearns representations independent of the composition of the image. The structured nature of object-centric representations offers improved generalization , making it valuable for variousapplications, including visual reasoning , dynamics simulation , and multi-modal learning . A foundational method in this field is slot attention , which introduced a simple yeteffective framework that employs a competitive attention mechanism between slots. Followingslot attention, many recent works have proposed improvements by introducing novel encoder ordecoder formulations , optimization techniques , additional slot refinementmodules , and expansions to video modality . These methods are primarilybottom-up models, while our approach proposes to bootstrap and incorporate top-down information. Incorporating top-down information The human visual system perceives scenes by leveragingboth top-down and bottom-up visual information . Top-down information represents task-driven contextual cues, such as high-level semantics and prior knowledge about the scene. Incontrast, bottom-up information is derived directly from the sensory input. Inspired by this dual-processing mechanism of the human visual system, several studies have attempted tomodel this approach within deep learning, achieving significant improvements across various tasks.Our work follows in a similar direction, specifically focusing on introducing top-down informationinto the representative OCL method, slot attention. By incorporating top-down semantic and spatialinformation, we aim to enhance the performance of slot attention in diverse visual environments,addressing the limitations of previous bottom-up methods Discrete representation learning Discrete representations within neural networks are consideredeffective for modeling discrete modalities and tackling generation tasks . Particularly,the pioneering work, VQ-VAE , introduced a method for learning discrete latent representationsthrough vector quantization. This model uses a discrete codebook, where the encoder maps input datato discrete codes using nearest-neighbor lookup and the decoder reconstructs the input from the codes.Another notable approach is the Gumbel-softmax trick , which provides a differentiableapproximation of sampling from a categorical distribution. Recent advancements have aimed toincorporate a more sophisticated formulation of codebooks or improve codebook utilization to better handle discrete representations. Recent work by Wallingford et al. is related to ourresearch in terms of using vector quantization for segmentation task. However, our method differs inthat the quantized codes are used to modulate bottom-up slot attention, while in Wallingford et al., the codes are used solely for segmentation labeling.",
  "Method": "We propose an OCL framework that incorporates top-down semantic information, such as objectcategories and semantic attributes, into slot attention through a top-down pathway. illustratesthe overall pipeline of our framework. Firstly, slot attention is applied to visual features extractedfrom an image encoder to output slots (Sec. 3.1). Then, a top-down pathway leverages the slots toidentify semantics in the input image and modulate slot attention. The pathway consists of two parts:bootstrapping top-down semantic information from a learned codebook and attention maps (Sec. 3.2)and modulating the inner activations of slot attention with this semantic information (Sec. 3.3).During the self-modulation stage, slot attention is repeated with the modulated activations, resultingin more representative slots.",
  "Slot Attention": "Slot attention is a recurrent bottom-up module that aggregates image features into slots through aniterative attention process, where each of the resulting slots represent an entity in the input image.Within our framework, these slots are used to bootstrap top-down information in the later stage(Sec. 3.2). The module takes in the initial slots, S0 RKD, and visual features extracted from an imageencoder, x RNDfeat. The initial slots are obtained by sampling K vectors from a learnableGaussian distribution using a reparameterization trick. The slots S = [s1, s2, . . . , sK] RKD,where each represents an individual object in the image, are computed by iteratively updating theinitial slots T times asS := ST , where St+1 = slot_attnx, St.(1)In each iteration, the slots attend to the visual features, refining their representations through a seriesof attention-based updates. Let q(), k(), and v() represent linear projections from dimension d to",
  "dh.(2)": "Unlike the original attention introduced in transformer which normalizes across the keys, theattention in slot attention is normalized across the slots. Such a distinct normalization scheme makesthe slots compete with each other to aggregate the visual features, encouraging each slot to representa distinct object in the scene. Then, the computed attention map A is normalized across the rows intoA and used to update the slots as",
  "Bootstrapping Top-down Information": "Our idea to bootstrap top-down information without annotations is based on our observation that theslots S, which are outputs of the bottom-up attention module, contain rough semantic informationabout objects. We leverage this coarse information to bootstrap both the semantic and spatial top-down information about the objects coarsely represented by the slots. Top-down semantic informationpertains to the specific semantic categories or attributes of the objects (what), while top-down spatialinformation indicates the locations or regions within the image where these objects are located(where). Incorporating such knowledge can guide slot attention to focus on the features most relevantto the objects expected to appear, enabling it to accurately capture objects that are obscured or havehigh intra-object variance, such as people with different hairstyles or clothing. Firstly, we extract the what information from the slots S using Vector Quantization (VQ), whichmaps each slot to one of the semantic concepts learned throughout training. Specifically, each slotSk is mapped to the nearest code in a finite codebook, C = [c1, c2, . . . , cE] RED with size E.The mapped code ck RD is considered a top-down semantic cue for the slot sk. Formally, thisquantization process can be written as",
  "ck = arg mincCsk c22.(4)": "Since the arg min operation is non-differentiable, we use the straight-through estimator forbackpropagation. During training, the codebook learns to store distinct semantic patterns recurringwithin the dataset by quantizing continuous slot embeddings into a limited number of discreteembeddings. Thereby, each code can act as automatically discovered top-down semantic information. Secondly, we obtain the where information from the attention maps of the last layer of slot attention.For each slot sk, the k-th row vector of the attention map A, denoted as ak RN, is used toaggregate visual features and update sk. This attention map provides useful spatial prior informationabout where each extracted top-down semantic information is located in the image.",
  "Self-modulating Slot Attention": "In the original slot attention (Sec. 3.1), the slot updates are driven purely by visual features extractedfrom the input without incorporating higher-level semantic information that can provide additionalcontext. To address this limitation, we introduce self-modulating slot attention, which modulates thecomputation of the slot updates based on the top-down information obtained in the bootstrappingstage (Sec. 3.2). This bootstrapped top-down information is used to dynamically amplify or inhibitspecific channel dimensions or regions of the value-projected visual features, while keeping the modelparameters unchanged. Formally, self-modulating slot attention can be represented by conditioningslot attention with the vector quantized slots ck and their corresponding slot-wise attention map ak:",
  ": return ST": "where S represents the slots from the self-modulating slot attention, different from the slots of theoriginal slot attention denoted by S. Note that the original slot attention (Sec. 3.1) and self-modulatingslot attention share parameter weights and initial slots, such that S0 = S0. Specifically, we modulate slot attention with a modulation map Mk RND computed from ck andak. Each element of the modulation map represents the relevance score between the correspondingvisual feature element and the top-down information of the expected object. This modulation mapcan be used to guide the update of each slot Sk (Eq. 3), by prioritizing specific value elements withthe high relevance scores. In the self-modulating slot attention, computation of the slot update U isreplaced with:",
  "Mk = msk mck RND.(7)": "For predicting channel-wise modulation vector mck, quantized slot ck is used, which tells us whatthe object appearing in the image is. The channel-wise scaling is designed to enforce the model tofocus on certain feature subspaces closely correlated to the semantic concept identified. Specifically,channel-wise modulation vector mck can be obtained by feeding quantized slot ck to the MLP, whichis represented as:",
  "msk = 1 + (ak ak) RN,(9)": "where ak is for the average of the attention score of ak. Using an attention map as is for modulationwill make all values down-scaled, while some regions likely to contain the object should be highlightedfor effective incorporation of the spatial top-down information. Thus, we use the attention mapshifted to have a mean value of 1 for the spatial-wise modulation map.",
  "Training": "Slot attention is trained within an autoencoding framework, using a decoder that reconstructs visualfeatures output by the image encoder or the original image from the slots. In this paper,we choose the visual feature reconstruction as our training objective since it is known to provide morerobust training signals for real-world datasets . We also employ a vector quantization objective for the codebook C only, which thereby learns to minimize the mean-squared error between the slotand the sampled codes. The reconstruction objective Lrecon and vector quantization objective LVQ aregiven by",
  "Lrecon = Dec( S; x) x22,LVQ = sg(S) C22,(10)": "where sg() represents stop gradient operation and C = [c1, c2, . . . , cK] RKD. For the decoder,we utilized the autoregressive slot decoder . The reconstruction objective ensures that thelearned slot representations capture essential information about the objects in the scene, while thevector quantization objective encourages the codebook to capture recurring semantic concepts in thedataset.",
  "Experimental Settings": "Datasets To verify the proposed method in diverse settings, including synthetic and authentic datasets,we considered four object-centric learning benchmarks: MOVI-C , MOVI-E , PASCAL VOC2012 , and MS COCO 2017 . MOVI-C and MOVI-E are synthetic datasets, adopted forvalidating our method in relatively simple visual environments. MOVI-C contains 87,633 images fortraining and 6,000 images for evaluation, while MOVI-E contains 87,741 and 6,000, respectively.To evaluate the proposed model in real-world settings, we leverage the VOC and COCO datasets.Following DINOSAUR , we use the trainaug variants, containsing 10,582 training images, forVOC dataset. For the evaluation, we use the validation split containing 1,449 images. The COCOdataset consists of 118,287 training images and 5,000 images for evaluation. While the VOC datasetincludes some images with a single object, images of the COCO dataset always contain 2 or moreobjects, making it the most challenging. MOVI datasets are licensed under apache license 2.0 andCOCO is licensed under CC-BY-4.0. Metrics We evaluate our method with three metrics: foreground adjusted random index (FG-ARI),mean best overlap (mBO), and mean intersection over union (mIoU). The FG-ARI is the ARI metriccomputed for foreground regions only (objects), which measures the similarity between differentclustering results. The mBO and mIoU are both IoU-based metrics, computed for all regions includingthe background. The mBO computes the average IoU between ground truth and prediction pairs,obtained by assigning each prediction to the ground truth mask with the largest overlap. The mIoU iscomputed as the average IoU between ground truth and prediction pairs obtained from Hungarianmatching. For COCO and VOC, mBOi and mBOc indicate the mBO metric computed using semanticsegmentation and instance segmentation ground truth. We use the instance segmentation ground truthfor other metrics. Following previous work , the internal attention maps of the autoregressivedecoder are used as the mask prediction results of the slots. Implementation details To assess the effectiveness of the proposed top-down pathway, our modelis implemented based on DINOSAUR , a representative slot-based OCL method. For theencoder and decoder, we use a DINO pretrained ViT-B/16 and an autoregressive transformerdecoder , respectively. The model is trained using an Adam optimizer with an initiallearning rate of 0.0004, while the encoder parameters are not trained. The number of slots K is setto 11, 24, 7, and 6 for MOVI-C, MOVI-E, COCO, and VOC, respectively. The codebook size E isset to 128 for synthetic datasets (MOVI-C and MOVI-E) and 512 for authentic datasets (COCO andVOC). The model is trained for 250K iterations on VOC and for 500K iterations on the others. Forthe ablation study and analysis, models are trained for 200K iterations on COCO, as this was enoughto reveal overall trends given limited computational resources. Full training of the model takes 26hours using a single NVIDIA RTX 3090 GPU. Codebook size E selection The performance of the proposed top-down pathway depends on code-book size E (Sec. 4.4), necessitating a principled selection method. We determine E automaticallyby monitoring the perplexity of code usage distribution during training, requiring only the trainingset without validation data. Perplexitythe exponent of entropyindicates how uniformly the codesare being used. While perplexity typically increases with codebook size, it plateaus when E exceedsthe number of distinct semantic patterns in the data, as some codes become unused . Tofind the optimal size, we start with E = 64 and double it until the perplexity plateaus after 250Kiterations. For example, on COCO, the perplexity when the codebook size is 256, 512, and 1024 are",
  "DINOSAUR reprod 34.10.931.40.539.50.129.40.627.03.241.23.448.23.839.03.6Ours37.40.033.00.340.30.231.2 0.326.74.743.92.651.02.542.02.8": "176.9, 253.9, and 242.8, respectively, where 512 was chosen as the final size. This procedure enablesefficient hyperparameter selection using only training data, eliminating the need for validation settuning. Following this approach, we set E = 128 for synthetic datasets (MOVI-C and MOVI-E) andE = 512 for real-world datasets (COCO and VOC).",
  "Quantitative Analysis": "DINOSAUR is the first successful OCL method that scales slot attention to real-world datasetsby introducing the use of a self-supervised image encoder , an autoregressive decoder, and afeature reconstruction objective. Notably, DINOSAUR uses the vanilla slot attention mechanismwithout modifications, making it a perfect baseline for validating the effectiveness of our proposedtop-down pathway. Thus, we adopt DINOSAUR as a baseline and compare its performance with andwithout our proposed method. For a fair comparison, we report both the reported and reproducedperformance of DINOSAUR. Tab. 1 demonstrates that incorporating the proposed top-down pathway into DINOSAUR largelyimproves performance in every metric. Specifically, our method improves FG-ARI by 5.9 on MOVI-E,which is the most challenging synthetic dataset. In Tab. 2, performances on authentic datasets, COCOand VOC, are reported. Our method largely surpasses the reproduced baseline on most metrics.The only metric for which our method does not show improvement is the FG-ARI on VOC. Wehypothesize that this is because VOC images frequently contain single objects only, and FG-ARIis computed solely with foreground pixels so that the performance is less affected by the top-downinformation. Tab. 3 presents a comparison between our method and recent state-of-the-art methods. It is notablethat our proposed method achieves competitive performance even to recent methods using advanceddiffusion-based decoders . Moreover, our approach focuses on incorporating top-downinformation into slot attention, which is orthogonal to the line of work advancing decoders to providebetter training signals to slot attention.",
  "Qualitative Results": "Codebook visualization To validate whether the proposed codebook learns meaningful semanticconcepts, we present visualizations of the codebook in . The index of the code and the maskprediction obtained from the slots modulated by the code are presented together, revealing thesemantic entity each code represents. Visualization demonstrates that the codebook successfullydiscovers and stores distinct semantic concepts without using any annotations. Moreover, the codesare mapped to objects with various appearances and layouts, which demonstrates that the codes learnhigh-level semantic information and not low-level structural or positional information.",
  "Code 343": ": Visualization of the codebook C on COCO . The results show that the codebook learnsto capture recurring semantic concepts in the dataset, such as pizza (code 124), sign (code 496),clock (code 235), zebra (code 207), motorcycle (code 341), surfer (code 352), dog (code 359),and skier (code 343). refines the attention maps, depending on how well they have captured the scene. For example, whenthe attention map is well-structured but coarse, the modulation process refines the boundaries of theattention map without changing the overall layout (first row). However, if the attention maps failto delineate objects, the modulation process recomposes the attention maps to differentiate objects(second to fourth row). By providing top-down semantic and spatial information via self-modulation,the attention maps are enhanced to capture the object within complex real-world environments.",
  "In-depth Analysis": "Effect of codebook size In our framework, vector quantization maps slots to distinct top-downsemantic information stored in the codebook, as shown in . In Tab. 4, we report the performancesacross different codebook sizes, E. The results show that a codebook size too large (E = 1024)or small (E 256) leads to performance degradation. When the codebook size is too small, thecodes cannot sufficiently learn distinct semantic information, which degrades the quality of thebootstrapped top-down semantic information. On the other hand, a codebook size too large may causethe codes to capture irrelevant details such as appearance variance or positional information ratherthan meaningful semantic concepts. However, we determine the optimal codebook size automaticallyusing the perplexity of codebook usage during training (Sec. 4.1), eliminating the need for extensivehyperparameter tuning using validation split and benchmark metrics.",
  "Ours37.332.7": "Impact of increased iterationsSince our method requires repeating slot attention with self-modulation, we investigate whether our performance improvement simply comes from the increasediterations of slot attention. In Tab. 5, we compare the results of our model with the DINOSAURbaseline model using six iterations of slot attention, which is twice as many iterations as the defaultsetting. Notably, both our model and the DINOSAUR 6-iteration model leverage the same numberof iterations. The results show that the DINOSAUR model with six iterations actually performsworse compared to the default 3 iterations. This indicates that merely increasing the number ofiterations does not guarantee improvement. Our methods superior performance is thus attributed tothe self-modulation mechanism rather than the increased number of iterations. Computation overhead of top-down pathway While our model requires one more forward pass forslot attention, the additional computation cost is negligible. In DINOSAUR, slot attention accountsfor only 0.64% of the total FLOPs, compared to 71.26% for the encoder and 28.10% for the decoder.Consequently, our model requires 47.62 GFLOPs versus 47.32 GFLOPs of DINOSAURa mereincrease below 1%. In practice, processing the entire COCO 2017 val split on a single NVIDIARTX 3090 GPU takes 71.4 seconds for our model compared to 70.5 seconds for DINOSAUR,demonstrating minimal impact on inference time. Codes representing broader semantics While most codes in our codebook consistently representsingle object categories, we observed interesting cases where codes capture broader concepts, asshown in . Some codes are trained to represent supercategories - for instance, grouping differentanimal species (code 468) or various human parts into shared codes (code 328). This suggests thecodebook can flexibly adapt to different levels of semantic abstraction when beneficial. We alsodiscovered an edge case where certain codes (e.g., code 223) specialize in capturing top-left patchesof images. This behavior appears to be influenced by the autoregressive decoding process, which",
  "Code 508": ": Visualization of the codebook C on COCO . The results show that the codebook learnsto capture broader semantics other than single object categories, such as supercategory (code 468,328), top-left patch (code 223), and background (code 236, 133, 508). must reconstruct the top-left patch first without surrounding context. However, these specializedpositional codes are rare (1-2 out of 512 codes) and have minimal impact on overall performance.Additionally, we found that certain codes specialize in capturing background elements common innatural scenes. For example, code 236 represents sky regions, code 133 captures sports fields, andcode 508 represents crowd scenes. : Ablation studies on COCO datasetfor each module consisting of the proposedtop-down pathway: channel-wise modula-tion (mc), vector quantization (VQ), spatial-wise modulation (ms), and shifting atten-tion map (shift).",
  "Conclusion": "In this paper, we introduced an OCL framework that incorporates top-down information into the slotattention mechanism through a top-down pathway. In this pathway, the output of the slot attentionis used to bootstrap high-level semantic knowledge and rough localization cues for existing objects.Using the bootstrapped top-down knowledge, slot attention is modulated to focus on features mostrelevant to the objects in the scene. Consequently, by incorporating the proposed top-down pathwayinto slot attention, we achieved state-of-the-art performance on various OCL benchmarks, includingchallenging synthetic and authentic datasets. Limitation The proposed top-down pathway has a limitation in that its overall performance relieson the quality of the codebook learned during training. As shown in Tab. 4, an incorrect choice ofcodebook size can result in the codes failing to learn distinct semantic concepts or capturing irrelevantdetails. While we mitigate this limitation through perplexity-based automatic codebook size tuning,the more principled codebook design that can eliminate the need for a pre-defined hyperparameter,such as dynamically expanding codebook during learning , will be promising future researchdirection.",
  "and Disclosure of Funding": "This work was supported by IITP grants funded by the Korea government (MSIT) (RS-2019-II191906Artificial Intelligence Graduate School Program (POSTECH); RS-2024-00457882 AI Research HubProject; RS-2024-00509258 Global AI Frontier Lab). P. Anderson, X. He, C. Buehler, D. Teney, M. Johnson, S. Gould, and L. Zhang. Bottom-up and top-downattention for image captioning and visual question answering. In IEEE Conference on Computer Visionand Pattern Recognition (CVPR), 2018.",
  "T. J. Buschman and E. K. Miller. Top-down versus bottom-up control of attention in the prefrontal andposterior parietal cortices. science, 315(5820):18601862, 2007": "M. Caron, H. Touvron, I. Misra, H. Jgou, J. Mairal, P. Bojanowski, and A. Joulin. Emerging Propertiesin Self-Supervised Vision Transformers. In IEEE International Conference on Computer Vision (ICCV),2021. M. Chang, T. Griffiths, and S. Levine. Object representations as fixed points: Training iterative refine-ment algorithms with implicit differentiation. In Conference on Neural Information Processing Systems(NeurIPS), 2022.",
  "M. Corbetta and G. L. Shulman. Control of goal-directed and stimulus-driven attention in the brain. Naturereviews neuroscience, 3(3):201215, 2002": "A. Dittadi, S. Papa, M. De Vita, B. Schlkopf, O. Winther, and F. Locatello. Generalization and robustnessimplications in object-centric learning. In International Conference on Machine Learning (ICML), 2022. A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Min-derer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers for image recognition atscale. In International Conference on Learning Representations (ICLR), 2021. G. Elsayed, A. Mahendran, S. Van Steenkiste, K. Greff, M. C. Mozer, and T. Kipf. Savi++: Towardsend-to-end object-centric learning from real-world videos. In Conference on Neural Information ProcessingSystems (NeurIPS), 2022. M. Engelcke, A. R. Kosiorek, O. P. Jones, and I. Posner. Genesis: Generative scene inference and samplingwith object-centric latent representations. In International Conference on Learning Representations (ICLR),2019.",
  "M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisserman. The pascal visual object classes(voc) challenge. In International Journal of Computer Vision (IJCV), 2010": "K. Greff, F. Belletti, L. Beyer, C. Doersch, Y. Du, D. Duckworth, D. J. Fleet, D. Gnanapragasam, F. Golemo,C. Herrmann, et al. Kubric: A scalable dataset generator. In IEEE Conference on Computer Vision andPattern Recognition (CVPR), 2022. M. Huh, B. Cheung, P. Agrawal, and P. Isola. Straightening out the straight-through estimator: Overcomingoptimization challenges in vector quantized networks. In International Conference on Machine Learning(ICML), 2023.",
  "J. Jiang, F. Deng, G. Singh, and S. Ahn. Object-centric slot diffusion. In Conference on Neural InformationProcessing Systems (NeurIPS), 2023": "J. Johnson, B. Hariharan, L. Van Der Maaten, L. Fei-Fei, C. Lawrence Zitnick, and R. Girshick. Clevr: Adiagnostic dataset for compositional language and elementary visual reasoning. In IEEE Conference onComputer Vision and Pattern Recognition (CVPR), 2017. I. Kakogeorgiou, S. Gidaris, K. Karantzalos, and N. Komodakis. Spot: Self-training with patch-orderpermutation for object-centric learning with autoregressive transformers. In IEEE Conference on ComputerVision and Pattern Recognition (CVPR), 2024.",
  "BDetails of Autoregressive Decoder": "The proposed top-down pathway is implemented based on the DINOSAUR baseline , using anautoregressive slot decoder. Singh et al. first proposed to use such an autoregressive decodingscheme for slot attention training. Autoregressive decoder is known to provide better training signalleading to improved performance, compared to the MLP-based broadcast decoder used by theslot attention originally. Autoregressive decoder is the simple variant of the transformer decoder ,which takes input visual feature x and slots S. The decoder consists of multiple decoding blocks. Letmulti-head attention be denoted as MHA(Q; K; V ), where Q, K, and V is for query, key, and value,respectively. Then, the decoding block can be represented as:",
  "DecBlock(x; S) = FFN(MHA(x; S; S)),(11)x = MHA<(x[BOS]; x[BOS]; x[BOS]),(12)": "where FFN denotes a feedforward layer with MLP and residual connection, MHA<() representsmulti-head self-attention with causal masking, and x[BOS] represents the visual feature sequence witha learnable [BOS] token appended at the start of the sequence. By using multiple decoding blocks,we can compute the autoregressive reconstruction of the visual feature x, which is consequentlyused for the computing reconstruction objective. Following DINOSAUR, we use an autoregressivedecoder with four decoding blocks. The number of heads for multi-head attention is set to 8.",
  "Figure A5: Visualization ofthe predicted object mask onCLEVR6": "To see if these settings are essential for the top-down pathway,we have implemented our self-modulation technique with the orig-inal slot attention setting , which includes training encodersfrom scratch and using an image reconstruction objective with spa-tial broadcast decoder . Tab. A7 summarizes the results of theCLEVR6 dataset. We observe a significant improvement in mBO,showing that our self-modulation technique is applicable to slot at-tention and provides complementary benefits. Although FG-ARIdecreased, mBO is considered more robust when evaluating modelperformance . We have also included qualitative re-sults in Fig. A5, which demonstrates that self-modulation markedlyimproves segmentation. These results indicate our methods effec-tiveness with different encoder configurations and training objec-tives. Comparison to MaskCut While object-centric learning (OCL) and unsupervised instancesegmentation share the goal of discovering objects without supervision, their ultimate objectivesdiffer. OCL aims to learn object-wise representations that support downstream tasks requiringcompositionality and systematic generalization, whereas unsupervised instance segmentation focusesprimarily on obtaining accurate object masks. Nevertheless, we can directly compare methodsfrom both tasks on their object discovery capabilities. We evaluate our method against MaskCut,the pseudo-mask generation algorithm underlying CutLER , on the COCO dataset. As shownin Tab. A8, our method significantly outperforms MaskCut across all metrics, demonstrating itseffectiveness for object discovery even when compared to specialized unsupervised segmentationapproaches.",
  "Ours37.40.033.30.326.74.743.92.658.95.146.82.459.73.139.31.8": "Comparison with SPOT In Tab. A9, we present the comparison with SPOT , a recent state-of-the-art object-centric learning method. SPOT proposed various ideas that can improve the qualityof object-centric representation, such as patch order permutation within autoregressive decoder andself-distillation. We want to emphasize that these ideas are all orthogonal to the proposed top-downpathway and can be used together for further improvement, which we will leave as future work."
}