{
  "Abstract": "Performative prediction aims to model scenarios where predictive outcomes sub-sequently influence the very systems they target. The pursuit of a performativeoptimum (PO)minimizing performative riskis generally reliant on modeling ofthe distribution map, which characterizes how a deployed ML model alters the datadistribution. Unfortunately, inevitable misspecification of the distribution map canlead to a poor approximation of the true PO. To address this issue, we introducea novel framework of distributionally robust performative prediction and studya new solution concept termed as distributionally robust performative optimum(DRPO). We show provable guarantees for DRPO as a robust approximation tothe true PO when the nominal distribution map is different from the actual one.Moreover, distributionally robust performative prediction can be reformulated asan augmented performative prediction problem, enabling efficient optimization.The experimental results demonstrate that DRPO offers potential advantages overtraditional PO approach when the distribution map is misspecified at either micro-or macro-level.",
  "Introduction": "In numerous fields where predictive analytics play an important role, decisions made on the basis ofmachine learning models do not just passively predict outcomes but actively influence future inputdata. Consider the domain of financial services, such as credit scoring and loan issuance, where amodels decision to grant or deny an application can affect the applicants future financial behaviorsand, consequently, the profile of future applicants. Similarly, in educational settings, the decisionprocess for school admissions can shape the applicant pool, as those who are accepted often sharetheir success strategies, indirectly influencing the preparation of future candidates. These exampleshighlight the study of performative prediction , a recent framework that facilitates a formalexamination of learning in the presence of performative distribution shift resulting from deployedML models. Delving deeper into the formulations of performative prediction, the concept of a distribution mapemerges as pivotal. This map characterizes the impact that a deployed ML model has on theunderlying data distribution, which is a crucial element in navigating performative effects. Theliterature primarily revolves around the pursuit of performative stability (PS), a model which isoptimal for the distribution it induces . However, a more ambitious target is theperformative optimum (PO), which seeks the minimization of performative risk, the risk of thedeployed model on the distribution it induces. Efficiently achieving PO typically necessitates amodeling of the distribution map . Practically, the precise influence of a model on thedata ecosystem is intricate and dynamic, making perfect specification an unattainable ideal. In this work, we propose a distributionally robust performative prediction framework that aimsto enhance robustness against a spectrum of distribution maps, thereby mitigating the issue ofmisspecification. Our contributions are summarized as follows: 1) in , we formalize the",
  "arXiv:2412.04346v1 [cs.LG] 5 Dec 2024": "DRPO concept, anchoring it within the performative prediction literature as a robust alternative; 2)in , we provide theoretical insights into the efficacy of DRPO, demonstrating its resiliencein the face of distribution map misspecification; 3) in , we recast distributionally robustperformative risk minimization as an augmented performative risk minimization problem, facilitatingefficient optimization; 4) in , we showcase DRPOs advatanges over conventional PO byempirical experiments. The paper concludes with a summary and discussion.",
  "Related Work": "Performative prediction. Performative prediction is an emerging framework for learning models thatinfluence the data they intend to predict. The majority of research focuses on performative stability, albeit with a few exceptions aiming at performative optimality. proposea two-stage plug-in method for finding the PO. propose to find the PO by a parametric model ofthe distribution map. This method is extended by to stateful performative prediction. solvesthe PO when the problem is outcome performative only. argue that the PO with a misspecifiednominal distribution map can still reasonably approximate the true PO, as long as the misspecificationlevel is not significant. This claim is also supported by our theory and experimental findings. Unlikethem, we demonstrate that the DRPO is comparable to the PO if the misspecification is small, whereasthe DRPO can offer substantial advantages over the PO if the misspecification is moderate to large.Moreover, the DRPO ensures reasonable performance for all distribution maps in an uncertaintycollection surrounding the nominal distribution map, rather than just a single distribution map. Distributionally robust optimization. DRO solves a stochastic optimization problem by optimizingunder the worst-case scenario over an uncertainty set of probability distributions. Most popular DROframeworks are based on -divergence and Wasserstein distance . The existing DRO literature pays no attention to performative prediction except for . study the repeated distributionally robust optimization algorithm, which repeatedly minimizesthe distributionally robust risk at the induced distribution. They show such repeated training algorithmyields a distributionally robust performative stable (DRPS) solution, assuming conditions analogousto the validity of repeated risk minimization in finding the performative stable (PS) solution.Nevertheless, the DRPO solution, which lies at the heart of the distributionally robust performativeprediction problem, is not the subject of their study. Furthermore, they lacks theoretical guaranteesregarding the proximity of the DRPS to either the PS or PO solution.",
  "Performative Prediction Essentials": "Let denote the (finite-dimensional) model parameter space, Z denote the data sample space, andP(Z) denote the set of probability measures supported on Z. In performative prediction, we aim tofind a that achieves low performative riskPRtrue() = EZDtrue()[(Z; )],(2.1) where : Z R is the (known) loss function, and Dtrue : P(Z) is the true distributionmap. The true performative optimum (true PO) PO,true is known to minimize the (true) performativerisk:PO,true arg min",
  "PRtrue() = EZDtrue()[(Z; )].(2.2)": "It is easy to see that if we know the true distribution map, then we can evaluate the true performativerisk and find the true performative optimum well up to some finite sample error which is negligible asthe sample size goes to infinity. However, the true map Dtrue() is unknown in general, thus posing asignificant obstacle in the pursuit of evaluating and optimizing the true performative risk. To enable the optimization of performative risk, it is necessary to have a known nominal distributionmap D() that is believed to closely approximate the unknown true distribution map Dtrue(). Thenone can find the performative optimum (PO) by minimizing the nominal performative risk:PO arg min",
  "Because the distribution map is inevitably misspecified, D() = Dtrue(), the true performative risk isgenerally not minimized by PO. Therefore, we treat (2.3) as a solution concept to approximately": "solve the true performative risk minimization problem (2.2), and refer (2.3) to standard performativeprediction. Now we provide several illustrative instances of potential sources that may lead to themisspecification of distribution map, i.e., D() = Dtrue(). Modeling error. The modeling of D() can be either a deterministic model of explicit form or astatistical model with model parameters to be estimated. The misspecification of D() may stem frommodeling error.Example 2.1 (Strategic classification). Let Z = X Y where X is the feature space and Y is thelabel space, so that we are in the supervised learning regime. Strategic classification relies on aworking model of individuals data manipulation strategy:",
  "Z D() Zd= Z0 + A where Z0 Dtrue(0),": "where A Rdim(Z)dim() is unknown and therefore must be estimated. If we observe the samplingdistributions of D(0), D(1) . . . , D(K), then A is partially identified up to a linear subspace ofRdim(Z)dim():A {M | M(k 0) = k 0 for k [K]},where k is the mean of D(k). In this example, the distribution map D() can be misspecifiedbecause the model parameter A is only partially identifiable. Distribution shift. Consider the training and test environments have different distribution maps,Dtrain() and Dtest(), respectively. We specify the nominal distribution map as the training distribu-tion map D() = Dtrain(). Then D() can be misspecified for the true distribution map Dtrue() =Dtest() due to the difference between the training and test environments Dtrain() = Dtest().Example 2.3 (Disparate impacts and fairness). A population is comprised of majority and minoritysubpopulations (e.g., by race or gender). The population distribution map is a mixture of thesubpopulation distribution maps: Dpop()d= Dmaj() + (1 )Dmin(). In fair machine learning,a theme is to check whether an ML model has disparate impacts on different subpopulations orbiased against the minority. Suppose that we work with D() = Dpop() and target at the minorityDtrue() = Dmin(). However, the population distribution map and the minority distribution mapmay differ. In this example, the distribution map D() is misspecified because of subpopulation shift.",
  "Distributionally Robust Performative Prediction": "From an intuitive perspective, the PO solution (2.3) has the potential to achieve low performative riskPRtrue(PO) when the nominal distribution map D() closely aligns with the true distribution mapDtrue(). However, D() and Dtrue() may be quite different. In such cases, the PO solution mayincur high performative risk. To address this issue, we propose a distributionally robust formulationfor performative prediction, where we explicitly incorporate into the learning phase the considerationthat the true distribution map Dtrue() is different from the nominal one D().",
  "U(D) = { D : P(Z) | D( D()D()) , }": "The radius reflects the the magnitude of shifts in distribution map we seek to be robust to. Weremark that the value of can be prescribed or be selected in data-driven ways. We postpone thediscussion on critical radius calibration to .3.Definition 2.4 (Distributionally robust performative risk). The distributionally robust performativerisk with the uncertainty collection U(D) is defined asDRPR() =supD: DU(D)EZ D()[(Z; )].(2.5) In other words, the distributionally robust performative risk DRPR() measures the worst possibleperformative risk incurred by the model parameterized by among the collection of all alternativedistribution maps that are -close to the nominal distribution map D. With this intuition, it is naturalto define an alternative solution concept which minimizes (2.5).Definition 2.5 (Distributionally robust performative optimum). The distributionally robust performa-tive optimum (DRPO) is defined asDRPO arg minDRPR().(2.6) We refer the method (2.6) to distributionally robust performative prediction. When comparing (2.6)and (2.3), we view the DRPO (2.6) as a competing solution concept to the PO (2.3), because both ofthem aim for achieving low performance risk (2.1).",
  "Generalization Principle of DRPO": "Distributionally robust performative prediction asks to not only perform well on a fixed performativeprediction problem (parameterized by the distribution map D), but simultaneously for a range ofperformative prediction problems, each determined by a distribution map in an uncertainty collectionU. This results in more robust solutions, that is, those DRPOs which are robust to misspecification ofdistribution map. The uncertainty collection plays a key role: it implicitly defines the induced notionof robustness. Moreover, distributionally robust performative prediction yields a natural approach forcertifying out-of-sample performance, which is summarized by the following principle.Proposition 2.6 (Generalization principle of distributionally robust performative prediction). Supposethat the uncertainty collection U contains the true distribution map Dtrue, then the true performativerisk is bounded by the distribution robust performative risk: PRtrue() DRPR() for any .In consequence, we have PRtrue(DRPO) DRPR(DRPO).",
  "where the penalty function Penalty() =": "22 || penalizes the deviation of from the origin0, and the critical radius tunes the level of regularization. That is to say, in this toy example,the distributionally robust performative risk minimization problem is essentially a L1-regularizedperformative risk minimization problem. Better worst-case control. To be more concrete, we let f() = a1 + a0 for some a1, a0 > 0. Forany D U(D), let the performative risk of D be PR D() = EZ D()[(Z; )]. If D is the truedistribution map, then PR D() is the true performative risk that we incur. Through direct calculation(see details in Appendix A), one can show that DRPO is more robust than PO in the sense of worst-case performative risk control, that is, sup DU(D) PR D(PO) sup DU(D) PR D(DRPO) + 2",
  "Excess Risk Bound": "For now, we are interested in bounding the excess risk: E() = PRtrue() min PRtrue() =PRtrue() PRtrue(PO,true), where is an approximate solution to the true PO PO,true, whichcould particularly be PO and DRPO. The excess risk captures the true performance of relative tothe oracle performative optimum PO,true, providing a direct measurement of the suboptimality of in terms of performative risk. As follows, we show the excess risk bounds of the PO solution and theDRPO solution, E(PO) and E(DRPO), and compare them.Proposition 3.2 (Excess risk bound of the PO). Suppose that we have bounded loss function|(z; )| B for any z Z, and some B > 0. Then we have",
  "VarZD(PO,true)[(Z; PO,true)] + o().(3.3)": "Comparing Proposition 3.2 and 3.3, we see that the excess risk bound of the DRPO can be localizedto the true PO while the excess risk bound of the PO is entangled with the full parametric space .Although we are comparing two upper bounds which can be not tight enough, the comparison shedslights to the potential benefits of using DRPO over PO solution. Even if in the case of no significantimprovement of using DRPO over PO solution, the excess risks of them are comparable, thus doingno harm. Our insight has been verified through a toy example in .4 and as well experimentalresults in . In Appendix C, we generalize Proposition 3.3 to the scenario when the uncertaintycollection doesnt cover the true distribution map, i.e., D(Dtrue(DRPO)D(DRPO)) > .",
  "Algorithms": "We recall a standard algorithm for performative risk minimization in Appendix E. In the followingsubsections, we will see that any off-the-shelf algorithms for finding the PO can be utilized as anintermediate algorithm for finding the DRPO by using our proposed algorithms. Moreover, weprovide practical considerations for the selection of a critical radius in the last subsection.",
  ": end while7: Return:": "The step of minimizing on with fixed (Line 4 in Algorithm 1) is itself a perfor-mative risk minimization problem, whichcan be solved by any suitable performa-tive risk minimization algorithm. Thestep of minimizing on with fixed (Line 5 in Algorithm 1) can be solved bythe line search or the NewtonRaphsonmethod since (, ) is convex in . Thetotal cost of Algorithm 1 is therefore com-parable to that of the performative risk minimization algorithm used in the intermediate step. Lastly,the alternating minimization algorithm in common practice guarantees global convergence (to station-ary point) regardless of how the optimization parameters are initialized. With the strong convexityassumption, the alternating minimization algorithm guarantees convergence to the global minimum.",
  "Tilted Performative Risk Minimization": "Treating as a hyperparameter which can be tuned by a practitioner, the solution of the dual problem(4.1) can be denoted by ((), ()). One can show that () is a decreasing function of . Anintuition is that as , we have arg min {(, )} arg minEZD()[(Z; )],which reduces to the original performative risk minimization problem, or the distributionally robustperformative risk minimization problem with = 0 (see an formal explanation in Appendix F).",
  ": Return:": "where TPR() stands for the tilted perfor-mative risk and TPO is the tilted performa-tive optimum, that is, the performative opti-mum of the tilted problem. In order to havestronger distributional robustness property,we tune to be larger. Given the correspondence (), we should have TPO with equals DRPOwith = ()1(1/). Therefore, the tilted performative risk minimization implicitly solves acorresponding DR performative risk minimization problem. Finally, we remark that exponentialtilting is a statistical method that has been around at least since the exponential family was firstinvented. More recently, it has been applied to operation research and machine learning .",
  "Calibration of Critical Radius": "The performance of the DRPO is contingent on the uncertainty collection radius , which is typicallydifficult to choose a priori without additional information. The greater the value of , the higher thelevel of distributional robustness, and thus the greater the tolerance for distribution map misspecifica-tion. Therefore, the selection of reflects a practitioners risk-aversion preference. In this subsection,we present two simple, yet effective calibration techniques for selecting . Post-fitting calibration. Without additional information, we can only hope to be robust to a prescribedset of distribution maps, say . The assumption of Proposition 3.3 requires only the uncertaintycollection at the DRPO, which reduces to an uncertainty ball centered at D(DRPO), encompassingthe true distribution Dtrue(DRPO). Therefore, in order to provide a provable guarantee for alldistribution maps in , the radius can be chosen based on the following criterion:",
  "D(DtrueD) := D(Dtrue(DRPO())D(DRPO()))": "Here D is the estimated KL divergence, and DRPO() is indexed by the radius to highlight itsdependence on as a tuning parameter to be calibrated. We implement the post-fitting calibrationapproach (with bisection search for ) in .1. Calibration set. With additional information, such as a small set of calibration data, we can pick (or if we use Algorithm 2) by evaluating the performance of DRPO() on the calibration set.Consider Example 2.3 where the training and test distribution maps may differ, we can conductthe following grid searching procedure to choose : 1) for a candidate set C of s, we compute{DRPO() : C} under Dtrain; 2) we obtain a few calibrating samples from Dtest, on whichwe evaluate the performance of DRPO(); 3) we select cal C with the best calibration setperformance. The calibration set approach is implemented in .3. To conclude this subsection, we discuss the computational costs of the proposed calibration methods.For general problems, these calibration methods necessitate a grid search, which may be computa-tionally expensive. Fortunately, for specific problems (for example, experiments in .1), wecan exploit the decreasing nature of the estimated KL divergence as a function in . As a result, wecan use bisection search rather than grid search to significantly reduce computational costs.",
  "Experiments": "We revisit Examples 2.1, 2.2, and 2.3 and compare the PO and the DRPO empirically. For a particular(true) distribution map, either the PO or the DRPO may have the potential to outperform the otherin terms of the performative risk evaluated at this particular distribution map. In contrast to thePO, however, the DRPO aims to guarantee reasonable performance for all distribution maps in anuncertainty collection around the nominal distribution map. To ensure the performance of the DRPOon a (set of) specific distribution map(s), the radius (or the tilt ) must be calibrated. Lastly, eachshaded region in figures below shows the curves standard error of the mean from 30 trials.",
  "Strategic Classification with Misspecified Cost Function": "In reference to Example 2.1, we examine strategic classification involving a cost function that ismisspecified. The experimental setup resembles that in . The task is credit scoring, specificallypredicting debt default. Individuals strategically manipulate their features to obtain a favorableclassification. Consider an instantiation of the response map (2.4) such that u(x) = x and c(x, x) =12 xstrat xstrat22 + xnon-strat xnon-strat22. Without loss of generality, let the firstm features be strategic features and the last d m features be non-strategic features. Let B =diag(1, . . . , 1, 0, . . . , 0) Rdd where the first m diagonal elements are 1s and the others are 0s.Then the best response function is (x) = x B. For the base distribution D(0), we use a class-balanced subset of a Kaggle credit scoring dataset (,CC-BY 4.0). Features encompass an individuals historical data, including their monthly incomeand credit line count. Labels are binary where the value of 1 represents a default on a debt and 0otherwise. There are a total of 3 strategic features and 6 non-strategic features. We use logistic modelfor the classifier and the cross-entropy loss with L2-regularization for the loss function . Consider the cost function is misspecified by its performativity level . We specify the cost functionwith the nominal performativity level = 0.5. However, the true performativity level true might notbe 0.5, but in the range of [0.5 0.5, 0.5 + 0.5] for some 0. The left plot of shows performative risk incurred by the PO and the DRPOs with variousradius s. Note that the PO can be understood as the DRPO with = 0. As increases, the DRPOaims to achieve more uniform performance across a wider range of true. The middle plot of shows relative improvement in worst-case performative risk1 of the DRPO to the PO as the radius",
  "Radius": "0.0% 2.5% 5.0% 7.5% 10.0% 12.5% 15.0% 17.5% Relative Worst-Case Improvement = 0.6 = 0.8 = 1.0 : Additional results of Experiment 5.1. Left: performative balanced error rate incurredby the PO and the DRPOs with various radius s. Right: relative improvement in worst-caseperformative balanced error rate of the DRPO to the PO as the radius increases, for different rangeof misspecification s.",
  "Partially Identifiable Distribution Map": "Recall Example 2.2, we examine a location model for distribution map where the mis-specification arises from the estimation error of the model parameter.Let V=[1 0 | 2 0 | | K 0] RdK and U = [1 0 | 2 0 | | K 0] RdKwhere d = dim(). The unknown parameter A can only be partially indentified through the equationAV = U when K < d. A particular estimate of A is A = UV = U(V V )1V , where V isthe Moore-Penrose inverse of V . In fact, the parameter A is only identifiable up to the subspaceW = {UV + E | span{E} N(V )}, where N(V ) is the left null space of V . Precicely, wehave AV = U if and only if A W. In this experiment, we still use the credit data. We observe sampling distribution of D(0), D(e1),and D(e2), where ei is the i-th canonical basis. For the true distribution map, the performativityof the first two features is 0.5, while the performativity of the other 7 features is true. In short,Atrue = diag(0.5, 0.5, true, . . . , true). We set the range of true to be [0.5, 0.5] for 0. Byusing the estiamte A, we model the performativity of the first two features correctly, but wronglymodel the other features as non-strategic. This time we fit TPO by Algorithm 2 instead of DRPO. The left plot of shows performative risk incurred by the PO and the TPOs with various tilts. As increases, the TPO performs more uniformly across a wider range of true. The middleplot of shows relative improvement in worst-case performative risk of the TPO to the POas the tilt increases, for different range of misspecification s. Without misspeicification, = 0,the PO is for sure better than the TPO. With moderate to large misspecification, {2/3, 4/3, 2},the TPO demonstrates certain advantages over the PO. The right of shows the relationship between the distributionally robust performative risk minimization and the tilted performative riskminimization: fitting DRPO with (which returns the optimal dual variable ) is equivalent tofitting TPO with = 1/.",
  ": Results of Experiment 5.3. Performa-tive risk of the population, the majority, and theminority, as the tilt increases. The vertical bandindicates the calibrated tilt cals": "Referencing to Example 2.3, we examine thescenario where the population distribution mapis a mixture of two subpopulation distributionmaps. We train a classifier using the populationdistribution map Dpop, but target at its perfor-mance on both the majority and minority, Dmajand Dmin. The distribution map is thereforemisspecified due to subpopulation shift. The experimental setup resembles that in .Note that the credit dataset used in the previ-ous experiments lacks demographic features. Toenable oracle access to demographic informa-tion, synthetic data is generated for a performa-tive classification task. The synthetic datasetexemplifies a scenario in which a linear decisionboundary is not able to effectively classify boththe majority and minority groups, necessitatinga trade-off between them. shows performative risk of the population, the majority, and the minority incurred by theTPO, as the tilt increases. The PO (which is TPO with = 0) exhibits the lowest performative riskat the population, but the greatest disparity between its performance for the majority and minoritygroups. As increases, the TPO reduces the performance gap between the two groups at theexpense of an increased population performative risk. This suggests that the distributionally robustperformative prediction framework has the potential to mitigate unfairness towards the minority group,even in the absence of demographic information. Using a small calibration set with demographics,we can calibrate the tilt cals via the calibration set approach described in .3. The goal is tocalibrate the tilt to satisfy a four-fifth rule2 with minimal population performative risk. The verticalbands in shows the calibrated tilt cals reasonably meet the goal.",
  "Summary and Discussion": "In this work, we present a distributionally robust performative prediction framework that aimsto improve robustness against a variety of distribution maps, thereby mitigating the problem ofdistribution map misspecification. We show provable guarantees for DRPO as a robust approximationto the true PO when the nominal distribution map differs from the actual one. We developed efficientalgorithms for minimizing the distributionally robust performative risk. Empirical experiments areconducted to support our theoretical findings and validate the proposed algorithms. The components of our approach are not new, but we are combining them in a novel way to solvea relevant problem. To be precise, the proposed approach is novel in the context to use the ideaof distributional robustness to solve the practical problem of distribution map misspecification inperformative prediction. In addition, it is novel to study the solution concept of distributionally robustperformative optimum (DRPO), both theoretically and algorithmically. In Appendix H, we extend the KL divergence distributionally robust performative prediction frame-work to a general -divergence distributionally robust performative prediction framework. Further-more, it is possible to go beyond general -divergence. An extension to a Wasserstein DRO versionis a natural direction for future research, calling for the development of new algorithms.3 2The minoritys performative risk is not 25% higher than that of the majority, as motivated by the four-fifthrule documented in Uniform Guidelines on Employment Selection Procedures, 29 C.F.R. 1607.4(D) (2015).3Due to space constraints, we provide additional materials for discussion (e.g., limitations) in Appendix I.",
  "Supplementary Materials forDistributionally Robust Performative Prediction": "This supplementary materials contain the omitted details, technical proofs, and additional resultspertaining to the main article Distributionally Robust Performative Prediction. In Section A, themissing deriving steps for the toy example in .4 are provided. In Section B, we provide acharacterization of the worst-case distribution map which attains the supremum in (2.5), and showthat the DR performative prediction regulates the right tail of the performative losses. In SectionC, we show a generalized excess risk bound for the DRPO. In Section D, all of the deferred proofsare presented. In Section E, we recall a standard algorithm for performative risk minimization. InSection F, we explain the claim in .2. In Section G, we provide omitted experimental detailsand additional empirical results. In Section H, we generalize the KL divergence DR performativeprediction framework to a general -divergence DR performative prediction version, and propose analgorithm to find the associated DRPO. In Section I, we provide additional materials for discussion.",
  "Proposition B.1 shows that the worst-case distribution map D is an exponentially tilted distributionmap with respect to the nominal distribution map D, where D puts more weights on the high ends": "shows histogram of performative loss for the PO, the DRPO with = 0.02, and the DRPOwith = 0.04, under the setup of Experiment 5.1 with true = 0.5. These plots displayed in aleft-to-right manner demonstrate that the DRPO regulates the right tail of the performative losses.Moreover, as the radius increases, there is a corresponding increase in the degree of regulationeffects.",
  "D(Dtrue(DRPO)P).(C.1)": "Comparing (C.1) to (3.3), we have an additional term in the excess risk bound which accommodatesand accounts for the misspecification of uncertainty set around D(DRPO), which doesnt necessarilycover Dtrue(DRPO). Furthermore, it is not difficult to see that if D(Dtrue(DRPO)D(DRPO)) ,then the last infimum term in the upper bound of (C.1) vanishes and (C.1) reduces to (3.3). Therefore,Proposition C.1 provides a generalized excess risk bound for the DRPO than Proposition 3.3.",
  "G.1Strategic Classification with Misspecified Cost Function in .1 (Continued)": "The data preprocessing procedure for the credit dataset follows the procedure documented in .After that procedure, the base distribution Dtrue(0) has n = 14878 data points with equal probabilitymass. We treat the distribution map associated with Dtrue(0) as the underlying test distribution mapwhich is unknown to us. We generate n IID samples from Dtrue(0) to get a training base distributionD(0), that is, D(0) Dtrue(0)n, and then we have a training nominal distribution map induced byD(0). This training procedure is repeated for 10 trials, and the shaded region in each of the figures inthis paper represents the standard error of the mean calculated from the 10 trials for the correspondingcurve. The left of shows performative balanced error rate, which refers to the balanced error rate(BER) on the test model-induced distribution, incurred by the PO and the DRPOs with various radiuss. Because the cross-entropy loss serves as a surrogate for classification error, we see patterns ofthese curves similar to those in the left of : as increases, the DRPO achieves more uniformperformance across a wider range of true. On the other hand, because the performative classificationerror is not exactly the criterion we minimize, different patterns are also observed: the DRPO withrelatively large radius = 0.04 outperforms the PO for all true . As indicated by the rightof , by increasing the DRPO constantly improves the relative worst-case performance interms of performative BER for {0.6, 0.8, 1.0}. Here the relative worst-case improvement inperformative BER of to PO is defined by maxtrue BERtrue(PO)maxtrue BERtrue()",
  "G.2Partially Identifiable Distribution Map in .2 (Continued)": "The data generating procedure is the same as that in Experiment 5.1. The left of showsperformative balanced error rate (BER) incurred by the PO and the TPOs with various tilt s.Similar to the left of , we observe that 1) as increases, the TPO achieves more uniformperformance across a wider range of true; 2) the TPO with relatively large tilt = 0.5 outperformsthe PO for all true . The right of shows relative improvement in worst-caseperformative balanced error rate of the TPO to the PO as the tilt increases, for different range ofmisspecification s. Without misspeicification, = 0, the TPO has comparable performance tothe PO in terms of performative BER. With moderate to large misspecification, {2/3, 4/3, 2},the TPO shows significant advantages over the PO in terms of relative improvement in worst-caseperformative BER. 1.00.50.00.51.0 True Performativity Level true 0.32 0.34 0.36 0.38 0.40 0.42",
  "Tilt": "0.0% 2.0% 4.0% 6.0% 8.0% 10.0% 12.0% 14.0% Relative Worst-Case Improvement = 0 = 2/3 = 4/3 = 2 : Additional results of Experiment 5.2. Left: performative balanced error rate incurred bythe PO and the TPOs with various tilt s. Right: relative improvement in worst-case performativebalanced error rate of the TPO to the PO as the tilt increases, for different range of misspecifications.",
  "G.3Fairness without Demographics in .3 (Continued)": "We adopt the following data generating process similar to that in . Let X N(A, A) +(1 )N(B, B). Let = 0.8 so that group A is the majority group and group B is the minoritygroup. Let A = 1 1d, B = 0.8 1d, and A = B = 0.1 Id. If X comes from group A,then label Y = 0 if X1d A1d. If X comes from group B, then label Y = 0 if X1d B1d.The distribution map follows:",
  "X1:d/2 X1:d/2 1:d/2X(d/2+1):d X(d/2+1):d": "so that the first d/2 features are strategic features, and controls the strength of performativity.We choose d = 10 and = 0.5 to wrap up the setup. Finally, we assume knowledge of the trueperformativity and observe IID samples of size n = 12500 from the population base distribution. Inshort, we eliminate the effect of population distribution map misspecification, and instead concentrateon the effect of subpopulation distribution map shift. shows the performative accuracy, which refers to the accuracy on the model-induceddistribution, of the population, the majority, and the minority incurred by the TPO, as the tilt increases. Because the cross-entropy loss serves as a surrogate for classification error, we see patternsof the three curves similar to those in . The PO (which is TPO with = 0) exhibits thehighest performative accuracy at the population, but the greatest disparity between its performancefor the majority and minority groups. As increases, the TPO reduces the performance gap betweenthe two groups at the expense of an decreased population performative accuracy. This suggests thatthe distributionally robust performative prediction framework has the potential to mitigate unfairnesstowards the minority group, even in the absence of demographic information.",
  "dP,": "where : R+ R+ and (1) = 0. Here dQ/dP is the RadonNikodym derivative, and weimplicitly require the probability measure Q to be absolutely continuous with respect to P. Note thatif we choose (t) = t log t, then we recover the framework presented in the main article. Now wekeep as a generic function and we will instantiate some popular families of -divergence after thepresentation of the general framework.",
  "where k is the conjugate number of k such that 1/k + 1/k = 1. Therefore, an algorithm parallel toAlgirthm 1 can be developed in a similar fashion based on the single-variable dual form (H.4)": "As a final remark, all of the theoretical results regarding excess risk bounds (see Proposition 3.3and Proposition C.1) are still valid for the general -divergence distributionally robust performativeprediction (which means the result statements wont change if one replaced the KL-divergence byany -divergence). On the other hand, it is possible to extend Proposition 3.2 to general -divergence, and the resultstatement needs a slight modification. By generalized Pinskers inequality, there is an increasingfunction F : R+ such that D(PQ) F(TV(PQ)). Then for general -divergence,Proposition 3.2 can be modified to E(PO) 2B sup F 1(D(Dtrue()D()) or E(PO) 2BF 1(sup D(Dtrue()D()) due to monotonicity of F(). For a concrete example, F(v) =v21{v < 1} +v",
  "In this appendix, we provide additional materials for discussion": "Extension to Wasserstein distance. It is possible to use Wasserstein distance to define the uncertaintycollection within our framework. Our algorithms can be modified to compute Wasserstein DRPO.For example, one can establish the strong duality of Wasserstein DRPO and develop an alternatingminimization algorithm similar to Algorithm 1. However, the new algorithm involves an additionalstep of transport cost-regularized loss maximization due to the more complex dual form of WassersteinDRO. For the corresponding theory, we expect that the square root of variance in (3.3) would bereplaced by the Lipschitz norm of (, PO,true). Conservativeness of cal and trade-off in selecting . The main text covers the discussion ofthe conservativeness of cal in two ways. Firstly, we show the trade-off in selecting . For valuesof ranging from small to moderate, DRPO outperforms PO in terms of performative risk (andsimilarly for worst-case performative risk). Conversely, for large values of , PO is better than DRPO.There exists an \"sweet spot\" of where DRPO yields maximal benefits over PO. This trade-offbetween DRPO and PO is demonstrated in any \"vertical slices\" of the left plot of (andsimilarly in the lines of the middle plot of for worst-case performance). Secondly, weshow the performance of the calibrated radius cal in the middle plot of , where the verticalbands indicate the calibrated radius cal. Although cal doesnt achieve the best possible worst-caseimprovement (which is an impossible oracle), it achieves a comparable performance, especially when {0.8, 1.0}. This demonstrates the effectiveness of cal chosen by our calibration method.",
  "Difference between TPO and TERM, and difference between DRPO and simple DRO. TPO (see.2 and Algorithm 2) differs from TERM in that it takes into account performativity,": "whereas TERM ignores. To be precise, TPO minimizes EZD()[e(Z;)] while TERM minimizesEZD(0)[e(Z;)], where D() is the distribution map and D(0) is the base distribution. Ananalogous explanation applies to the difference between DRPO and simple DRO. TERM (or implicitlyequivalently simple DRO) is ineffective in the context of this works problem setup because it fails toaccount for performativity. Note that our methods are not doing DRO because the uncertainty sethere depends on the model parameter . We borrow the idea of distributional robustness, but we havea fundamentally different problem at hand. Absence of small calibration set. The calibration set is not always available. Here we clarify thecalibration set method and briefly discuss a possible solution when there is no calibration set. In theexperiment of .3, we only need a small set of calibration data, which aligns with the regimeof \"weak group information\". It is generally difficult to calibrate radius for uncertainty set and mostdistributional robustness related work only concerns the effect of increasing and . We do moreto demonstrate that there are some practical calibration methods that work. In the absence of anycalibration data, one can specify the radius by some prior belief. For example, consider Dpop() =Dmajor() + (1 )Dminor() as in Example 2.3. If one believes that 1 2 for some0 < 1, 2 < 1, then one can upper bound the divergence D(Dmajor()Dpop()) log(1) andD(Dminor()Dpop()) log(1 2). Further, one can choose = max{ log(1), log(1 2)} when using DRPO. However, this choice of radius could be conservative. Algorithmic convergence. The convergence guarantees of the proposed algorithms can be establishedon a case-by-case basis, depending on the specific inner algorithm or solver employed for solvingperformative risk minimization. Existing algorithmic convergence results for model-based PO solvers can be taped into our algorithms naturally. Moreover, empirical results in validatethe effectiveness of our proposed algorithms. Broader impacts. This paper presents work whose goal is to advance the field of machine learning,especially the subfield of performative prediction. Despite the development of new algorithms, theirdirect societal impact are limited to those outlined in the original paper on performative prediction."
}