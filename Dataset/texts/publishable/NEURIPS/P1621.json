{
  "Abstract": "This work considers the problem of sampling from a probability distribution knownup to a normalization constant while satisfying a set of statistical constraintsspecified by the expected values of general nonlinear functions. This problemfinds applications in, e.g., Bayesian inference, where it can constrain moments toevaluate counterfactual scenarios or enforce desiderata such as prediction fairness.Methods developed to handle support constraints, such as those based on mirrormaps, barriers, and penalties, are not suited for this task. This work therefore relieson gradient descent-ascent dynamics in Wasserstein space to put forward a discrete-time primal-dual Langevin Monte Carlo algorithm (PD-LMC) that simultaneouslyconstrains the target distribution and samples from it. We analyze the convergenceof PD-LMC under standard assumptions on the target distribution and constraints,namely (strong) convexity and log-Sobolev inequalities. To do so, we bringclassical optimization arguments for saddle-point algorithms to the geometry ofWasserstein space. We illustrate the relevance and effectiveness of PD-LMC inseveral applications.",
  "Introduction": "Sampling is a fundamental task in statistics, with applications to estimation and decision making, andof growing interest in machine learning (ML), motivated by the need for uncertainty quantificationand its success in generative tasks . In these settings, the distribution we wish to samplefrom (target distribution) is often known only up to its normalization constant. This is the case, forinstance, of score functions learned from data or posterior distributions of complex Bayesian models.Markov Chain Monte Carlo (MCMC) algorithms can be used to tackle this problem andLangevin Monte Carlo (LMC), in particular, has attracted considerable attention due to its simplicity,theoretical grounding, and effectiveness in practice . These sampling algorithms, however, donot naturally incorporate requirements on the samples they generate. Specifically, standard MCMCmethods do not enforce restrictions on the target distributions, such as support (e.g., truncatedGaussian), conditional probabilities (e.g., fairness), or moments (e.g., portfolio return) constraints.This limitation is often addressed by post-processing, transforming variables, or by introducingpenalties in the target distribution. Though successful in specific settings, these approaches haveconsiderable downsides. Post-processing techniques such as rejection sampling (see, e.g., )may substantially reduce the effective number of samples (number of samples generated per iterationof the algorithm). Variable transformations based on link functions, projections, or mirror/proximalmaps (see, e.g., ) only accommodate (deterministic) support constraints and are not suited",
  "(PI)": "where P2(Rd) denotes the set of probability measures on Rd with bounded second moments andthe functions gi, hj represent the requirements. Note that (PI) only considers measures againstwhich gi, hj are integrable. Otherwise, the expectations are taken to be +, making the correspond-ing measure infeasible. Observe that (PI) is more general than the support-constrained samplingproblem considered in, e.g., . Indeed, it constrains the distribution rather than its sam-ples x (). Algorithms based on projections, barrier, or mirror maps are not suited for thistype of constraints (see .2 for more details). To tackle (PI), this paper instead derivesand analyzes a primal-dual LMC algorithm (PD-LMC) that is the sampling counterpart of gradientdescent-ascent (GDA) methods from (Euclidean) optimization. A dual ascent algorithm was pre-viously proposed to tackle (PI), but it requires the exact computation of expectations with respectto intractable distributions . This paper not only overcomes this limitation, but also providesconvergence guarantees for a broader class of constraint functions.",
  "t= (t)W2KL((t)),(2)": "for a d-dimensional Brownian motion W(t), where q denotes the divergence of q andW2 KL() denotes the Wasserstein-2 gradient of KL(|) at [26, Theorem 10.4.17] (seeAppendix B for more details). Indeed, the Langevin diffusion (2) brings the distribution (t) of x(t)progressively closer to the target . In fact, the Fokker-Planck equation can be interpreted as agradient flow of the KL divergence with respect to the Wasserstein-2 distance . However, computing the path of the stochastic differential equation in (2) is not practical anddiscretizations are used instead. Chief among them is the (forward) EulerMaruyama scheme, whichleads to the celebrated Langevin Monte Carlo (LMC) algorithm",
  "xk+1 = xk kf(xk) +": "2kk,kiid N(0, Id),(3)for a step size k > 0, where Id denotes the d-dimensional identity matrix. Notice that it is notnecessary to know Z in order to evaluate (3). This has made LMC and its variants widely popularin practice and the subject of extensive research. Despite (3) being a biased time-discretizationsof the Langevin diffusion in (2) , rates of convergence of LMC have been obtained for smoothand strongly convex or convex potentials or when the target distribution verifies anLSI .",
  "Constrained sampling": "Our goal, however, is not to sample from itself, but from a distribution close to that satisfies aset of statistical requirements. Explicitly, we wish to sample from a distribution that solves (PI).Since (PI) constrains the distribution rather than its samples x, it can accommodate more generalrequirements than the support constraints typically considered in constrained sampling (e.g., ).Next, we illustrate the wide range of practical problems that can be formulated as (PI). Theseexamples are further explored in and more details on their formulations are providedin Appendix E.",
  "subject toPx[x C] = 1,(PII)": "for a closed convex set C Rd. Indeed, let C be the intersection of the 0-sublevel sets of convexfunctions {si}i=1,...,I. Such a description always exists (see Appendix E). Then, (PII) can becast as (PI) using gi(x) = [si(x)]+ for [z]+ = max(0, z). Notice that the gi are convex andthat although they are not everywhere differentiable, I(si(x) > 0)si(x) is a subgradient of gi,where I(E) = 1 on the event E and 0 otherwise. Observe that support constraints can also beimposed using projections, mirror/proximal maps, and barriers as in . These methods,however, constrain the samples x rather than their distribution as in (PII). 2. Rate-constrained Bayesian models: rate constraints have garnered attention in ML due to theircentral role in fairness . Consider data pairs (x, y), where x X are features and y {0, 1} labels, and a protected (measurable) subgroup G X. Let be a Bayesian posterior of theparameters of a model q(; ) denoting the probability of a positive outcome (based, e.g., on abinomial model). We wish to enforce statistical parity, i.e., we wish the prevalence of positiveoutcomes within the protected group G to be close to or higher than in the whole population. Wecast this problem as (PI) by constraining the average probability of positive outcome as inP =minP2(Rd)KL()",
  "subject toEx,q(x; ) | G Ex,q(x; ) .(PIII)": "where > 0 denotes our tolerance . Naturally, multiple protected groups can be accommo-dated by incorporating additional constraints. Hence, constrained sampling provides a natural wayto encode fairness in Bayesian inference. 3. Counterfactual sampling: rather than imposing requirements on probabilistic models, constrainedsampling can also be used to probe them by evaluating counterfactual statements. Indeed, let denote a reference probabilistic model such that sampling from yields realizations of the realworld. Consider the counterfactual statement how would the world have been if E[g(x)] 0?Constrained sampling not only gives realizations of this alternative world, but it also indicates itscompatibility with the reference model, namely the value P of (PI).More concretely, consider a Bayesian stock market model. Here, is a posterior model for the(log-)returns of I assets, e.g., distributed as Gaussians N(, ). Here, the vector describes themean return of each stock and their covariance. We can investigate what the market would looklike if, e.g., the mean and variance of each stocks were to change by solving",
  "(PIV)": "Due to correlations in the market, certain choices of i or 2i may be more unrealistic thanothers. Additionally, it could be that some of these conditions are vacuous conditioned on theothers. As we show next, our approach to tackling (PI) effectively isolates the contribution of eachrequirement in the solution , thus enabling us to identify which are (conditionally) vacuous andwhich are most at odds with the reference model .",
  "Lagrangian duality and dual ascent algorithms": "Although directly sampling from does not appear straightforward, it admits a convenient character-ization based on convex duality that is amenable to be sampled using the LMC algorithm (3). Indeed,let g : Rd RI and h : Rd RJ be vector-valued functions collecting the constraint functions giand hj respectively. The Lagrangian of (PI) is then defined as",
  "d(, ) minP2(Rd) L(, , ).(6)": "Notice from (4) that the minimum in (6) is achieved for from (5), the Lagrangian minimizer,so that d(, ) = log(Z/Z). The solution of (6) is therefore a tilted version of , whose tilt iscontrolled by the dual variables (, ). Since (6) is a relaxation of (PI), it yields a lower bound on theprimal value, i.e., d(, ) P for all (, ) RI+ RJ. The dual problem seeks the tilts (, )that yield the best lower bound, i.e.,",
  "The set = argmax0, d(, ) of solutions of (DI) is called the set of Lagrange multipliers.Note from (6) that (DI) depends on the distributions and through its objective d": "The dual problem (DI) has several advantageous properties. Indeed, while the primal problem (PI)is an infinite dimensional, smooth optimization problem in probability space, the dual problem (DI)is a finite dimensional, non-smooth optimization problem in Euclidean space. What is more, it is aconcave problem regardless of the functions f, g, h, since the dual function (6) is the minimum of aset of affine functions in (, ) [30, Prop. 4.1.1]. These properties are all the more attractive giventhat, under mild conditions stated below, (DI) can be used to solve (PI).",
  "Then, (, ) are subgradients of P (0, 0) = P , i.e., P (u, v) P u v,and if P (u, v) is differentiable at (0, 0), then uP (u, v) = and vP (u, v) = at (0, 0)": "Proof. In finite dimensional settings, (i)(v) are well-known duality results (see, e.g., ). Whilethey also hold for infinite dimensional optimization problems, their proofs are slightly more scattered.We collect their reference below. The objective of (PI) is a convex function and its constraints arelinear functions of . Hence, (PI) is a convex program. Under Slaters condition (Assumption 2.1),it is (i) strongly dual (P = D) and (ii) there exists at least one solution (, ) of (DI) (see [31,Sec. 8.6, Thm. 1] or [32, Cor. 4.1]). This implies (iii) the existence of the saddle-point (7) [33,Prop. 2.156], (iv) that argmin L(, , ) = {}, since the KL divergence is stronglyconvex and its minimizer is unique [34, Thm. 7.3.7], and (v) that (, ) are subgradients of theperturbation function P (u, v) [33, Prop. 4.27]. Proposition 2.2 shows that given solutions (, ) of (DI), the constrained sampling problem (PI)reduces to sampling from eU(,,) (see Appendix F for an explicit example of thisresult). It is important to note that this results only relies on the KL divergence being (strongly)convex in the standard L2 geometry, i.e., along mixtures of the form t0 + (1 t)1 for t .This does not imply that it is (geodesically) convex in the Wasserstein sense [35, .1.2]. Thiswould require U in (5) to be convex for all 0 and RJ, i.e., for f, g to be convex and h to belinear. Hence, Proposition 2.2 reduces the constrained sampling problem (PI) to that of finding the Lagrangemultipliers (, ). Despite their finite dimensionality, however, computing these parameters isintricate. Indeed, since (DI) is a concave program, we could obtain (, ) using",
  "+andk+1 = k + k Ekk [h],(8)": "for k > 0, where we used the fact that E[g] and E[h] are (sub)gradients of the dual function (6)at (, ) [36, Thm. 2.87]. This procedure is known in optimization and game theory as dual ascentor best response . Notice, however, that (8) is not a practical algorithm as it requires explicitintegration with respect to the intractable distribution from (5).",
  "+,k = Law(xk).(9)": "Note that since J = 0, we omit the argument of U for clarity. Nevertheless, the updates in (9) stillrequire an explicit integration. While it is now possible to sample from k (namely, using the xk),empirical approximations of Ek[g] may not only require an exponential (in the dimension d) numberof samples (e.g., [39, Thm. 1.2]), but it introduces errors that are not taken into account in the analysisof . In the sequel, we address these drawbacks by replacing these dual ascent algorithms by asaddle-point one.",
  "t= jL(t), (t), (t)(10c)": "for the Lagrangian L defined in (4), where [z],+ = z for > 0 and [z],+ = max(a, 0) other-wise [40, Sec. 2.2]. Observe that iL(, , ) = E[gi] and jL(, , ) = E[hj]. Hence,the algorithm from described in (9) involves a deterministic implementation of (10b) that fullyintegrates over (t). In contrast, we consider a stochastic, single-particle implementation of (10) thatleads to the practical procedure in Algorithm 1. Explicitly, we also use an Euler-Maruyama time-discretization of the Langevin diffusion associatedto (10a) (step 3), but replace the expectations in (10b)(10c) by single-sample approximations (steps 45). Algorithm 1 can therefore be seen as a particle implementation of the deterministic WassersteinGDA algorithm (10). As such, it resembles a primal-dual counterpart of the LMC algorithm in (3),which is why we dub it primal-dual LMC (PD-LMC). Alternatively, Algorithm 1 can be interpretedas a stochastic approximation of the dual ascent method in (9). This suggests that the gradient approx-imations in steps 45 could be improved using mini-batches, which is in fact how approximatesthe expectation in (9). Our theoretical analysis and experiments show that these mini-batches areneither necessary nor always worth the additional computational cost (see .1 and ).Note that the stochastic approximations in Algorithm 1 refer to the dual updates (steps 45) ratherthan the LMC update (step 3) as in stochastic gradient Langevin . Though these methods couldbe combined, it is beyond the scope of this work. The remainder of this section is dedicated to analyzing the convergence properties of PD-LMCfor both stochastic dual gradients (as in Algorithm 1) and exact dual gradient (as in (9)). For thelatter, we obtain guarantees for the discrete implementation (9) under weaker assumptions than thecontinuous-time analysis of . We consider strongly log-concave target distributions in .1and those satisfying an LSI in .2.",
  "PD-LMC with (strongly) convex potentials": "As opposed to the traditional LMC algorithm (3) or the deterministic updates in (9), Algorithm 1involves three coupled random variables, namely, (xk, k, k). Hence, the LMC update (step 3) isbased on a stochastic potential U and the distribution k of xk is now a random measure. Our analysissidesteps this obstacle by using techniques from stochastic optimization. We also leverage techniquesfrom primal-dual algorithms in the Wasserstein space, in the spirit of works such as thatstudied the LMC (3) or alternative time-discretizations of gradient flows of the KL divergence assplitting schemes.",
  "maxf2L2(k), gi2L2(k), hj2L2(k) G2 and maxEk[g2], Ek[h2] G2": "Assumption 3.1 holds with m = 0 if f, g are convex and h is linear. If f is additionally stronglyconvex, then it holds with m > 0 [26, Prop. 9.3.2]. Assumption 3.2 is typical in (stochastic) non-smooth optimization analyses (see, e.g., ). Notice, however, that gradients are onlyrequired to be bounded along trajectories of Algorithm 1, a crucial distinction in the case of stronglyconvex functions whose gradients can only be bounded locally. Assumption 3.2 can be satisfied undermild conditions on f, g, h, such as local Lipschitz continuity or linear growth. The following theorem provides the first convergence analysis of the discrete-time PD-LMC.Theorem 3.3. Denote by k the distribution of xk in Algorithm 1. Under Assumptions 2.1, 3.1,and 3.2, there exists R20 such that, for k ,",
  "+ and k+1 = k+k Ek[h]": "Theorem 3.3, whose proof is deferred to Appendix C, implies rates similar to those for GDA schemesin finite-dimensional Euclidean optimization (see, e.g., ). To recover those rates, however, wemust bound the magnitudes of k, k. In , this is done by bounding the iterates in the algorithmitself, i.e., by projecting them onto the set Dr = {(, ) RI+ RJ | max(2 , 2) r} andchoosing r such that Dr (Proposition 2.2(ii) ensures this is possible). We then incur a bias onthe order of in (12) that vanishes in the decreasing step size setting of (13). Though convenient,this is not necessary since there exists a sequence of step sizes such that both E[k2] and E[k2]are bounded for all k 0. In the interest of generality, Theorem 3.3 holds without these hypotheses.It is worth noting that though faster rates and last iterates guarantees can be obtained for Euclideansaddle-point problems, they rely on more complex schemes than the GDA in Algorithm 1 involvingacceleration or proximal methods . The results in Theorem 3.3 are stated for the stochastic scheme in Algorithm 1.However,Theorem 3.3 yields the same rates (without expectations) for exact dual gradients, i.e., forthe dual ascent scheme (9).In this case, the second condition in Assumption 3.2 simplifiesto maxk( Ek[g]2, Ek[h]2) G2. Not only are these milder assumptions than [24, Eq. (16)],but the guarantees hold for discrete- rather than continuous-time dynamics. Finally, (12)(13) im-ply convergence with respect to the KL divergence for convex potentials (m = 0) with strongerguarantees in Wasserstein metric for strongly convex ones (m > 0). The convergence rates for distributions k from Theorem 3.3 also imply convergence rates forempirical averages across iterates xk of Algorithm 1. This corollary is obtained by combining (12)(13) with the following proposition. By taking to be the constraint functions g or h from (PI) yieldsfeasibility guarantees for PD-LMC .",
  "PD-LMC with LSI potentials": "In this section, we replace Assumption 3.1 on the convexity of the potential by an LSI common inthe sampling literature. We consider only inequality constraints (J = 0) here and omit the functionarguments , since accounting for equality constraints requires significant additional assumptions.Assumption 3.5. The distribution satisfies the LSI for bounded , i.e., there exists > 0 such that2 KL() log (d/d)2L2() for all P2(Rd). The LSI in Assumption 3.5 is often used in the analysis of the standard LMC algorithm . Itholds, e.g., when f is strongly convex and g is a (possibly non-convex) bounded function due to theHolley-Stroock perturbation theorem . In fact, if f is 1-strongly convex and |g| is bounded by 1,then Assumption 3.5 holds for e2 (see, e.g., [51, Prop. 5.1.6] or [52, Thm 1.1]). The LSI isakin to the Polyak-ojasiewicz (PL) condition from Euclidean optimization , which supposesissues with GDA methods such as Algorithm 1. Indeed, it is not enough for the Lagrangian (4) tosatisfy the PL condition in the primal variable to guarantee the convergence of GDA in Euclideanspaces. We must either modify Algorithm 1 using acceleration or proximal methods orimpose the PL condition also on . Since the Lagrangian 4 is linear in , it is clear thatAlgorithm 1 will not suffice to provide theoretical guarantees in the LSI case. We therefore consider the variant in Algorithm 2, where N 0k LMC iterations (step 3) are executedbefore updating the dual variables (step 4). This is akin to using different time-scales in continuous-time, a common technique for solving saddle-point problems . Since it resembles a dualascent counterpart of the LMC algorithm (3), we refer to it as (stochastic) dual LMC (DLMC). Asopposed to the dual ascent algorithm from in (9), however, Algorithm 2 does not require anyexplicit evaluation of expected values. The following theorem provides an analysis of its convergence.Theorem 3.6. Assume that the functions f, g are M-smooth, i.e., have M-Lipschitz continuousgradients, satisfy Assumption 3.5, and that E[g2] G2 for all P2(Rd). Let 0 < k ,0 < G2 < 1,",
  ": Sampling from a 2D truncated Gaussian (true mean in red and sample mean in orange)": "Theorem 3.6, whose proof is deferred to Appendix D, provides similar guarantees as (approxi-mate) subgradient methods in finite-dimensional optimization (see, e.g., ). This is notsurprising seen as k, N 0k in Theorem 3.6 are chosen to ensure that step 4 yields a sample xk ksuch that KL(kk) using [29, Theorem 1]. At this point, g(xN 0k) in step 5 is an approximate,stochastic subgradient of the dual function (6). Though it may appear from (12) and (14) that Al-gorithms 1 and 2 have the same convergence rates, an informal computation shows that the latterevaluates on the order of d2/ as many gradient per iteration, where = M/. Note that we canonce again apply Theorem 3.6 to derive ergodic average and feasibility guarantees for Algorithm 2.",
  "Experiments": "We now return to the applications described in .2 to showcase the behavior of PD-LMC.We defer implementation details and additional results to Appendix E. Code for these examples ispublicly available at 1. Sampling from convex sets. We cast the problem of sampling from a Gaussian distribution N(0, 1)truncated to C = as (PI) by taking f(x) = x2/2 and g(x) = [(x1)(x3)]+ (see .2). shows histograms for the samples obtained using PD-LMC, the projected LMC (Proj. LMC)from , and the mirror LMC from , all with the same step size. Both Proj. LMC and MirrorLMC generate an excess of samples close to the boundary (between 1.5 and 3 times more samplesthan expected). This leads to an underestimation of the mean (Proj. LMC: 1.488 / Mirror LMC: 1.470vs. true mean: 1.510). In contrast, PD-LMC provides a more accurate estimate (1.508). Yet, since itconstrains the distribution rather than its samples, it is not an interior-point method and can producesamples outside of C. Theorems 3.33.6 show that this becomes less frequent as the algorithmprogresses (in , only 2% of the samples are not in C). This occurs even without using mini-batches in steps 45 of Algorithm 1 as in . In fact, our experiments show that mini-batchesincrease the computational complexity with no performance benefit (Appendix E). These issuesare exacerbated in more challenging problems, such as sampling from a two-dimensional standardGaussian centered at restricted to an unit 2-norm ball (). In this case, Proj. LMCplaces almost 25% of its samples on the boundary (where only 0.14% of samples should be), whilePD-LMC only places 1.8% of its samples outside of the support. Mirror LMC provides a better meanestimation in this setting, although a bit more asymmetric than PD-LMC [Mirror LMC: (0.312, 0.418)vs. PD-LMC: (0.446, 0.444) vs. true mean: (0.368, 0.368)].",
  ": Counterfactual sampling of the stockmarket: dual variables": "makes more than $50k based on socioeconomic information (details on data pre-processing can befound in ). We consider a standard Gaussian prior on the parameters Rd+1 of the model,where d is the number of features. Using the LMC algorithm to sample from the posterior (i.e., noconstraints), we find that while the average probability of positive predictions is 19.1% over the wholetest set, it is 26.2% among males and 5% among females (Unconstrained in ). To overcomethis disparity, we take gender to be the protected class in (PIII), constraining both Gmale and Gfemalewith = 0.01. Using PD-LMC, we obtain a Bayesian model that leads to an average probability ofpositive outcomes of 18.1% and 15.1% for males and females respectively. In fact, we now observea substantial overlap of the distributions of positive predictions across genders for the constrainedposterior (Constrained ( = 0.01) in ). This substantial reduction of prediction disparitiescomes at only a minor decline in accuracy (unconstrained: 84% vs constrained: 82%). 3. Counterfactual sampling. Though the distribution of positive predictions changes considerablyfor both male and female individuals, the final dual variables (male = 0 and female 160) showthat these changes are due uniquely to the female group [as per Prop. 2.2(iv)]. This implies that thereference model is itself compatible with the requirement for the male group, but that reducing thedisparity for females requires considerable deviations from it. By examining female, we concludewithout recalculating that even small changes in the tolerance for the female constraint wouldsubstantially change the distribution of outcomes [Prop. 2.2(v)]. This is confirmed by Constrained( = 0.03) in . Notice that this is only possible due to the primal-dual nature of PD-LMC.This type of counterfactual analysis is even more beneficial in the presence of multiple requirements.Indeed, let be the posterior of a Bayesian model for the daily (log-)return of a set of assets (seeAppendix E for more details). Using (PIV), we consider how the market would look like if theaverage (log-)return of each asset were to have been (exactly) 20% higher. Inspecting the dualvariables (), we notice that this increased market return is essentially driven by two stocks: NVDAand LLY ( < 0). In fact, the reference model would be consistent with an even higher increasefor JNJ and GOOG ( > 0). We confirm these observations by constraining only NVDA and LLY, whichyields essentially the same (log-)return distribution for all assets.",
  "Conclusion": "We tackled the problem of sampling from a target distribution while satisfying a set of statisticalconstraints. Based on a GDA method in Wasserstein space, we put forward a fully stochastic, discrete-time primal-dual LMC algorithm (PD-LMC) that precludes any explicit integration in its updates.We analyze the behavior of PD-LMC for (strongly) convex and log-Sobolev potentials, proving thatthe distribution of its samples converges to the optimal constrained distribution. We illustrated theuse of PD-LMC for different constrained sampling applications. Future work include strengtheningthe convergence results to almost sure guarantees and improving the rates obtained using proximaland extra gradient methods, particularly in the LSI setting.",
  "T. Lin, C. Jin, and M. I. Jordan, Near-optimal algorithms for minimax optimization, inProceedings of Thirty Third Conference on Learning Theory, 2020, pp. 27382779": "A. Mokhtari, A. Ozdaglar, and S. Pattathil, A unified analysis of extra-gradient and optimisticgradient methods for saddle point problems: Proximal point approach, in Proceedings ofthe Twenty Third International Conference on Artificial Intelligence and Statistics, 2020, pp.14971507. S. Chewi, M. A. Erdogdu, M. Li, R. Shen, and S. Zhang, Analysis of Langevin Monte Carlo:from Poincar to Log-Sobolev, in Proceedings of Thirty Fifth Conference on Learning Theory,ser. Proceedings of Machine Learning Research, P.-L. Loh and M. Raginsky, Eds., vol. 178.PMLR, 2022, pp. 12.",
  "M. Sanjabi, J. Ba, M. Razaviyayn, and J. D. Lee, On the convergence and robustness of traininggans with regularized optimal transport, in Advances in Neural Information Processing Systems,2018": "J. Yang, N. Kiyavash, and N. He, Global convergence and variance reduction for a class ofnonconvex-nonconcave minimax problems, in Advances in Neural Information ProcessingSystems, 2020, pp. 11531165. T. Fiez, L. Ratliff, E. Mazumdar, E. Faulkner, and A. Narang, Global convergence to local min-max equilibrium in classes of nonconvex zero-sum games, in Advances in Neural InformationProcessing Systems, 2021, pp. 29 04929 063.",
  "ARelated work": "In constrained sampling, it is important to distinguish between two types of constraints: supportconstraints and statistical constraints. The former deals with sampling from a target distribution that is supported on a proper subset X Rd, which arises in applications such as latent Dirichletallocation and regularized regression . The latter is the problem tackled in the current work. A first family of constrained sampling methods relies on rejection sampling: it obtains samples viaany (unconstrained) method, rejecting those that violate the constraint (see, e.g., ). Thoughthis approach can handle constraints of any nature, it is often inefficient in terms of number of samplesgenerated per iteration of the method (effective number of samples), especially when confronted withintricate constraints and high dimensional problems. These drawbacks can be addressed for support constraints using techniques inspired by finite-dimensional constrained optimization. Projected LMC, for instance, deals with the problem ofsampling from a target distribution restricted to a convex set . Barrier methods have alsobeen used to tackle the same problem . Similarly, mirror and proximal descent versionsof popular sampling algorithms such as LMC and Stein Variational GradientDescent (SVGD) have been proposed. Mirror descent algorithms enforce constraints by map-ping (mirroring) the primal variables to a space with a different geometry (induced by a Bregmandivergence) over which the optimization is carried out. Alternatively, methods adapted to targetdistributions support on manifolds have also been put forward . In practice, these methodsrequire explicit expressions for the projections, barriers, and mirror maps describing the constraintsand are therefore not adapted to handle statistical requirements such as those considered in (PI).Langevin dynamics with constraint violation penalties were considered in , although they cannotenforce exact constraint satisfaction. Statistical (moment) constraints such as those considered in (PI) were investigated in . As wediscussed at the end of .3, this paper considers the combination of LMC and approximatedual ascent shown in (9). It also introduces a similar version of SVGD as well as algorithmsbased on barriers. Aside from requiring exact integration against intractable measures (namely, k),convergence guarantees for these methods hold under restrictive assumptions on the constraints gi.Additionally, guarantees are derived only for continuous-time (gradient flows) dynamics. This work is also closely related to saddle-point methods in finite-dimensional optimization.For the general problem of maxx miny f(x, y), the behavior of descent-ascent methods havebeen investigated under a myriad of scenarios, including for functions f that are (strongly) con-vex/(strongly) concave as well as non-convex/non-concave under, e.g., PL condi-tions . In general, convergence holds for the ergodic average of iterates .Last-iterate results often require different algorithms, involving proximal point or extra gradientmethods and time scale separation . In particular, guarantees for the GDA method usedin Algorithm 1 often requires stringent conditions that are hard to enforce for dual problems suchas (DI).",
  "x y2ds(x, y),": "where S(, ) is the set of couplings between and . The metric space (P2(Rd), W2) is referred toas the Wasserstein space . It can be equipped with a Riemannian structure . In this geometricinterpretation, the tangent space to P2(Rd) at is included in L2() and is equipped with a scalarproduct defined for f, g L2() as",
  "t+ ((t)v(t)) = 0": "that holds in the distributional sense, where v(t) is a subgradient of F at (t). Among the possibleprocesses (v(t))t, one has a minimal L2((t)) norm and is called the velocity field of ((t))t.In a Riemannian interpretation of the Wasserstein space , this minimality condition can becharacterized by v(t) belonging to the tangent space to P2(Rd) at (t) denoted T(t)P2(Rd), whichis a subset of L2((t)). The Wasserstein gradient is defined as this unique element, and is denotedW2F((t)). In particular, if P2(Rd) is absolutely continuous with respect to the Lebesguemeasure, with density in C1(Rd) and such that F() < , W2F()(x) = F()(x) for -a.e.x Rd, where F() denotes the first variation of F evaluated at , i.e. (if it exists) the uniquefunction F() : Rd R s.t.",
  "for all = , where P2(Rd)": "Now, we denote by T# the pushforward measure of P2(Rd) by the measurable map T. Werecall that the KL divergence of relative to can be decomposed as (1). The distribution k of xkin (3) is known to follow a forward-flow splitting scheme of the Fokker-Planck equation in (2),namelyk+1/2 =I kW2V(k)",
  "V (, , ) W 22 (, ) +min(,) 2 + 2,(17)": "where is the solution of the constrained sampling problem (PI) and is the set of solutionsof the dual problem (DI). Our goal is to show that V decreases (in some sense) along trajectoriesof Algorithm 1. We say in some sense because contrary to the standard LMC algorithm, thedistribution k of xk is now a random measure that depends on the random variables {k, k}.Explicitly, we consider the filtration Fk = (0, {, }k) and show that V decreases on averagewhen conditioned on Fk. This turns out to be enough to prove Theorem 3.3. Indeed, notice that k Fk. Hence, the potential energy E(k, k, k) Fk and the conditionallaw k = L(xk|Fk1) evolves as in the regular LMC algorithm (3). That is to say, conditioned onthe event Fk, step 5 of Algorithm 1 follows a splitting scheme as (16), i.e.,",
  "k": "K and Kk=1 1/k 1 + log(K). Notice that allinequalities in the proof continue to hold for deterministic {k, k}. The bounds in Theorem 3.3therefore also hold (without expectations) when using exact gradients to update the dual variables. Finally, we show there exists a sequence of step sizes k such that E[V (k1/2, k, k)] R20for all k 1, where the expectation is taken over the {k, k}.This immediately impliesthat W 22 (k, ) R20 and both E[k2] and E[k2] are bounded for all k. We proceedby induction. The base case is covered by Lemma C.2. Suppose now that there exists a se-quence {0, . . . , k1} such that E[V (k1/2, k, k)] R20. From the definition of V in (17)and the fact that the (, ) are bounded (Prop. 2.2), we then obtain that E[k], E[k] arebounded. Consequently, there exists k > 0 such that",
  "Proof of Lemma C.1. The proof proceeds by combining two inequalities bounding the primal anddual terms in (17)": "(i) W 22 (k+1/2, ). We proceed following a coupling argument. Let sk be an optimal couplingbetween the random variables Y k and Z , i.e., a coupling that achieves W 22 (k, ).Consider now the random variable T = Y kW2E(k, k, k) and observe from (18a) that itis distributed as k+1/2. Naturally, the coupling sk is no longer optimal for (T, Z), so that by thedefinition of the Wasserstein distance it follows that",
  "+ 2kW2E(k, k, k)2L2(k) + Exk g(x)2 + Exk h(x)2 .(29)": "To upper bound the term in brackets, we add and subtract log(Z) and use the decomposition of theLagrangian in terms of (11) to obtainE(, k, k) + H() E(k, , ) H(k) = L(, k, k) L(k, , ).Using the saddle-point property (7), we then getL(, k, k) L(k, , ) L(, , ) L(k, , ) KL(k ),We therefore conclude that",
  "The expectations are taken over the samples k k": "We conclude by combining Prop. D.1 with [29, Theorem 1], which characterizes the convergenceof the LMC algorithm (3) under Assumption 3.5. Indeed, using the k, N 0k from Theorem 3.6in Algorithm 1 guarantees that the law k of xk|k is such that KL(kk) , i.e., satisfiesthe conditions in Prop. D.1. We can then apply Jensens inequality to get that E[KL(k)] KL(E[k]) = KL(k), where k is the law of xN 0k in Algorithm 2.",
  "EApplications": "In this section, we provide further details on the example applications described in .2 aswell as additional results from the experiments in . In these experiments, we start all chainsat zero (unless stated otherwise) and use different step-sizes for each of the updates in steps 35 fromAlgorithm 1. We refer to them as x, , and . In contrast, we do not use diminishing step-sizes.",
  ": The effect of the mini-batch size Nb on PD-LMC for sampling from a 1D truncatedGaussian: Estimated mean vs. (a) iteration and (b) LMC evaluations": "(x 1)(x 3). In order to satisfy the assumptions of our convergence guarantees (particularly 2.1),we leave some slack in the constraints by considering E[[si(x)]+] 0.005. This also helpswith the numerical stability of the algorithm. shows histograms of the samples obtainedusing PD-LMC (Algorithm 1 with x = = 103), the projected LMC (Proj. LMC, = 103)from , and the mirror LMC ( = 103) from . In all cases, we take 5 106 samples andkeep only the second half. Observe that, due to the projection step, Proj. LMC generates an excess of samples close to theboundaries. In fact, it generates over three times more than required. This leads to an underestimationof the distribution mean and variance (). A similar effect is observed for mirror LMC. Incontrast, PD-LMC provides a more accurate estimate. Nevertheless, PD-LMC imposes constraintson the distribution rather than its samples. Indeed, note from (47) that its feasibility set is such thatsamples belong to C almost surely, which still allows for a (potentially infinite) number of realizationsoutside of C. Yet, though PD-LMC is not an interior-point method, Theorems 3.33.6 show thatexcursions of iterates outside of C become less frequent as the algorithm progresses. We can confirmthis is indeed the case in a, which shows the ergodic average of [s(x)]+ along the samples ofPD-LMC. Note that it almost vanishes by iteration 104 even though the dual variable only beginsto stabilize later (b). This is not surprising given that it is guaranteed by Prop. 3.4. In fact, onlyroughly 2% of the samples displayed in are not in C. Before proceeding, we examine whether the convergence of PD-LMC could be improved by averagingmore than one LMC samples when updating the dual variables, i.e., using mini-batches in steps 45of Algorithm 1. Mini-batches will reduce the variance of the dual updates, although at the cost ofadditional LMC steps per iteration. To compensate for this fact, b displays the evolution of theergodic average of PD-LMC samples as a function of the number of LMC evaluations rather than thenumber of iterations (as in a). Notice that, in this application, increasing the number of LMCsamples Nb does not lead to faster convergence. This illustrates that, though mini-batches could beuseful in some applications (particularly when the constraints are not convex, as in .2), it",
  ": Density estimate of two-dimensional truncated Gaussian using samples from (a) Proj. LMCand (b) PD-LMC": "is not immediate that their benefits always outweigh the increased computational cost. Oftentimes,using a single LMC sample is more than enough. It is worth noting that using PD-LMC with a largemini-batch Nb was suggested in to approximate the expectation needed by their continuous-timealgorithm. As we see here, this is neither necessary nor always beneficial. We now turn to a more challenging, two-dimensional applications. We seek to sample from a Gaussianlocated at with covariance diag() restricted to an 2-norm unit ball (). Specifically,we use f(x) = x2 /2 (i.e., ex2/2) and si(x) = x2 1. Once again, we leave someslack to the algorithm by taking the constraint in (PVI) to be E[[si(x)]+] 0.001. For reference,we also display samples from the real distribution obtained using rejection sampling. This is indeed a challenging problem. The boundary of C is 2 standard deviations away from themean of the target distribution, which means that samples from the target are extremely scarcethis region. Indeed, using the untruncated Gaussian as a proposal for rejection sampling yieldsan acceptance rate of approximately 1%. The strong push of the potential f towards the exteriorof C leads Proj. LMC ( = 103; the last 106 samples are used after running 5 106 iterations) tobe now even more concentrated around its boundary. In fact, almost 25% of its samples are in anannular region of radius [0.999, 1), where only 0.14% of the samples should be according to rejectionsampling. Indeed, note from a, that even as iterations advance, the samples of Proj. LMCcontinue concentrate close to the boundary. In contrast, PD-LMC (x = 103 and = 2 101) only place 1.8% of its samples outsideof C, mostly during the initial phase of the algorithm (a). Indeed, the average of the constraintfunction along samples from PD-LMC essentially vanishes around iteration 5 104. Achievingthis requires larger values of (on the order of 250, b) compared to the one-dimensionalcase (b). This reflects the difficulty of constraining samples to C in this instance, a statementformalized in the perturbation results of Prop. 2.2(iv). Due to the more amenable numerical propertiesof the barrier function, mirror LMC ( = 103) performs well without concentrating samples on theboundary (0.15% of the samples on the annular region of radius [0.999, 1)).",
  "E.2Rate-constrained Bayesian models": "While rate constraints have become popular in ML due to their central role in fairness (see, e.g., ),they find applications in robust control and to express other requirements on the confusionmatrix, such as precision, recall, and false negatives . For illustration, we consider here theproblem of fairness in Bayesian classification. Let q(x; ) = P[y = 1|x, ] denote the probability of a positive outcome (y = 1) given the observedfeatures x X and the parameters distributed according to the posterior . This posterior isdetermined, e.g., by some arbitrary Bayesian model based on observations {(xn, yn)}n=1,...,N.Hence, E[q(x; )] denotes the likelihood of a positive outcome for x. Consider now a protectedgroup, represented by a measurable subset G X, for which we wish to enforce statistical parity. Inother words, we would like the prevalence of positive outcomes to be roughly the same as that ofthe whole population. Thus, we now want to sample not from the posterior , but from a close-bydistribution of parameters that ensures this parity. Explicitly, for some tolerance > 0, we want tosample from",
  "Naturally, we can account for more than one protected by incorporating additional constraints": "In our experiments, we take to be the posterior of a Bayesian logistic regression model for theAdult dataset from (details on data pre-processing can be found in ). The N = 32561 datapoints in the training set are composed of d = 62 socio-economical features (x Rd, includingthe intercept) and the goal is to predict whether the individual makes more than US$ 50000 peryear (y {0, 1}). The posterior is obtained by combining a binomial log-likelihood with independentzero-mean Gaussian (log)priors (2 = 3) on each parameter of the model, i.e., we consider the",
  ".(48)": "We begin by using the LMC algorithm from (3) (i.e., we impose no constraints) to collect samplesof the coefficients from this posterior ( = 104; the last 104 samples are used after running2 104 iterations). We find that, while the probability of positive outputs is 19.1% across the wholetest set, it is 26.2% among males and 0.05% among females. Looking at the distribution of thisprobability over the unconstrained posterior (a), we see that this behavior goes beyondthe mean. The model effectively amplifies the inequality already present in the test set, where theprevalence of positive outputs is 30.6% among males and 10.9% among females. To overcome this disparity, we consider gender to be the protected class in (PVII), constrainingboth Gfemale and Gmale. We formulate the constraint of (PVII) using an empirical distribution inducedfrom the data. Explicitly, we consider constraints",
  "n=1Eq(xn; )": "where Gfemale, Gmale {1, . . . , N} partition the data set. For these experiments, we take = 0.01.Using PD-LMC (x = 104, = 5 103), we then obtain a new set of samples from the logisticregression parameters that lead to a prevalence of positive outcomes (in the test set) of 17.1%over the whole population, 18.1% for males, and 15.1% for females. In fact, we notice a substantialoverlap between the distributions of this probability across the constrained posterior for male andfemale (a). Additionally, this substantial improvement over the previously observed disparitycomes at only a minor reduction in accuracy. Though both distributions change considerably, noticefrom the value of the dual variables that these changes are completely guided by the female group.Indeed, male = 0 throughout the execution of PD-LMC (b). Before proceeding, we once again examine the effect of using multiple LMC samples to update thedual variables, i.e., using mini-batches in steps 45 of Algorithm 1. shows the distributionof the prevalence of positive predictions (> $50k) for different mini-batch sizes Nb. In all cases,we collect 2 104 samples, which means that we evaluate 2Nb 104 LMC updates (step 3 inAlgorithm 1). Same as in the 1D truncated Gaussian case, we notice no difference between theresulting distributions. This is to be expected given our results (Theorem 3.3). The computation time,on the other hand, increases considerably with the mini-batch size. Once again, we note that PD-LMCwith a large mini-batch Nb was used in the experiments of to overcome the challenge ofcomputing an expectation in their dual variable updates. In turns out that this computationallyintensive modification is not necessary.",
  "E.3Counterfactual sampling": "Previous applications were primarily interested in sampling from , the constrained version ofthe target distribution . The goal of counterfactual sampling, on the other hand, is to probe theprobabilistic model by evaluating its compatibility with a set of moment conditions. It is thereforeinterested not only in , but in how each condition affects the value P = KL(). We nextdescribe how constrained sampling can be used to tackle this problem. Let denote a reference probabilistic model, such as the posterior of the Bayesian logistic modelin (48). Standard Bayesian hypothesis tests can be used to evaluate the validity of actual statementssuch as is it true that Ex[g(x)] 0? or Ex[h(x)] = 0? Hence, we could check is morelikely to yield a positive output for a male than a female individual? (from the distributions underUnconstrained in a, this is probably the case). In contrast, counterfactual sampling is concernedwith counterfactual statements such as how would the world have been if E[g(x)] 0? In the caseof fairness, how would the model have been if it predicted positive outcomes more equitably? Constrained sampling evaluates these counterfactual statements in two ways. First, by providingrealizations of this alternative, counterfactual world (). For instance, we can inspect the differ-ence between realizations of , obtained using the traditional LMC in (3), and , obtained usingPD-LMC (Algorithm 1). In a, we show the mean of some coefficients of the Bayesian logisticmodels from Section E.2. Notice that it is not enough to normalize the Intercept and reduce theadvantage given to males (Female is encoded as Male = 0). This alternative model also compensatesfor other correlated features, such as education (Bachelor), profession (Adm/clerical), and age. Second, constrained sampling evaluates the compatibility of each counterfactual condition (con-straint) world with the reference model. While Algorithm 1 does not evaluate P explicitly, it providesmeasures of its sensitivity to perturbations of the constraints: the Lagrange multipliers (, ). In-deed, recall from Prop. 2.2 that",
  "n=1Eq(xn; ) ,": "i.e., the statement the model predicts positive outcomes for males on average at least as much as forthe whole population. In contrast, accommodating statistical parity for females requires considerabledeviations from the reference model (female 160). Without recalculating , we therefore knowthat even small changes in the tolerance for the female constraint would substantially change the",
  "E.3.1Stock market model": "Counterfactual analyses based on the dual variables become more powerful as the number of con-straints grow. To see this is the case, consider the Bayesian stock market model introduced in.2. Here, denotes the posterior model for the (log-)returns of 7 assets (TSLA, NVDA, JNJ,AAPL, GOOG, BRK-B, and LLY). The dataset is composed of 5 years of adjusted closing prices for atotal of 1260 points per asset. The posterior is obtained by combining a Gaussian likelihood N(, )with Gaussian prior on the mean [N(0, 3I)] and an inverse Wishart prior on the covariance (withparameters = I and = 12). Using the LMC algorithm ( = 103; the last 3 105 samples areused after running 6 105 iterations), we collect samples from this posterior and estimate the meanand variance of the (log-)return for each stock (). In this case, is initialized to 10 I. We might now be interested in understanding what the market would look like if all stocks wereto incur a 20% increase in their average (log-)returns. To do so, we use PD-LMC (x = 103and = 6 103) to solve the following constrained sampling problem",
  "model ()+20% (all stocks)+20% (LLY and NVDA)": "TSLA0.20 0.120.24 0.120.23 0.12NVDA0.25 0.090.31 0.090.31 0.09JNJ0.02 0.070.02 0.070.03 0.07AAPL0.11 0.060.14 0.060.14 0.06GOOG0.09 0.060.11 0.060.11 0.06BRK-B0.06 0.070.07 0.070.07 0.07LLY0.16 0.060.19 0.060.19 0.06 stocks. Said differently, their returns increasing 20% is consistent with the reference model con-ditioned on the other returns increasing. Proceeding, two stocks have negative dual variables (LLYand NVDA). This implies that bringing their constraints down to i would yield a decrease in P (dis-tance to the reference model ). This is in contrast to JNJ and GOOG, whose positive s imply that weshould should increase their returns to reduce P . Indeed, by inspecting the ergodic slacks (a)we see that all stocks approach zero (i.e., feasibility), but that JNJ and GOOG do so from above. Thisbehavior is expected according to Prop. 3.4. These observations show two things. First, that an increase in the average returns of LLY and NVDA isenough to drive up the returns of all other stocks. In fact, it leads to essentially the same distributionas if we had required the increase to affect all stocks (). Second, that the increase we wouldsee in JNJ (and to a lesser extent GOOG) would actually be larger than 20%. Once again, we reachthese conclusion without any additional computation. Their accuracy can be corroborated by theresults in .",
  "In this section we illustrate the result in Prop. 2.2, i.e., we show that given solutions (, ) of (DI),the constrained sampling problem (PI) reduces to sampling from eU(,,)": "Indeed, consider a standard Gaussian target, i.e., ex2/2, and the linear moment con-straint E[x] = b, for b Rd. This can be cast as (PI) with f(x) = x2/2 and h(x) = b x (noinequality constraints, i.e., I = 0). Clearly, the solution of (PI) in this case is = N(b, I), i.e., aGaussian distribution with mean b. What Prop 2.2 claims is that rather than directly solving (PI), wecan solve (DI) to obtain a Lagrange multiplier such that = for defined as in (4).",
  "The answer NA means that the paper has no limitation while the answer No means that thepaper has limitations, but those are not discussed in the paper": "The authors are encouraged to create a separate \"Limitations\" section in their paper. The paper should point out any strong assumptions and how robust the results are to violations ofthese assumptions (e.g., independence assumptions, noiseless settings, model well-specification,asymptotic approximations only holding locally). The authors should reflect on how theseassumptions might be violated in practice and what the implications would be. The authors should reflect on the scope of the claims made, e.g., if the approach was only testedon a few datasets or with a few runs. In general, empirical results often depend on implicitassumptions, which should be articulated. The authors should reflect on the factors that influence the performance of the approach. Forexample, a facial recognition algorithm may perform poorly when image resolution is low orimages are taken in low lighting. Or a speech-to-text system might not be used reliably toprovide closed captions for online lectures because it fails to handle technical jargon.",
  "If applicable, the authors should discuss possible limitations of their approach to addressproblems of privacy and fairness": "While the authors might fear that complete honesty about limitations might be used by reviewersas grounds for rejection, a worse outcome might be that reviewers discover limitations thatarent acknowledged in the paper. The authors should use their best judgment and recognizethat individual actions in favor of transparency play an important role in developing norms thatpreserve the integrity of the community. Reviewers will be specifically instructed to not penalizehonesty concerning limitations.",
  "If the contribution is a dataset and/or model, the authors should describe the steps taken to maketheir results reproducible or verifiable": "Depending on the contribution, reproducibility can be accomplished in various ways. Forexample, if the contribution is a novel architecture, describing the architecture fully mightsuffice, or if the contribution is a specific model and empirical evaluation, it may be necessaryto either make it possible for others to replicate the model with the same dataset, or provideaccess to the model. In general. releasing code and data is often one good way to accomplishthis, but reproducibility can also be provided via detailed instructions for how to replicate theresults, access to a hosted model (e.g., in the case of a large language model), releasing of amodel checkpoint, or other means that are appropriate to the research performed. While NeurIPS does not require releasing code, the conference does require all submissionsto provide some reasonable avenue for reproducibility, which may depend on the nature of thecontribution. For example",
  "(b) If the contribution is primarily a new model architecture, the paper should describe thearchitecture clearly and fully": "(c) If the contribution is a new model (e.g., a large language model), then there should eitherbe a way to access this model for reproducing the results or a way to reproduce the model(e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors arewelcome to describe the particular way they provide for reproducibility. In the case ofclosed-source models, it may be that access to the model is limited in some way (e.g.,to registered users), but it should be possible for other researchers to have some path toreproducing or verifying the results.",
  "The answer NA means that the paper does not include experiments": "The authors should answer \"Yes\" if the results are accompanied by error bars, confidenceintervals, or statistical significance tests, at least for the experiments that support the main claimsof the paper. The factors of variability that the error bars are capturing should be clearly stated (for example,train/test split, initialization, random drawing of some parameter, or overall run with givenexperimental conditions).",
  "Guidelines:": "The answer NA means that there is no societal impact of the work performed. If the authors answer NA or No, they should explain why their work has no societal impact orwhy the paper does not address societal impact. Examples of negative societal impacts include potential malicious or unintended uses (e.g.,disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deploy-ment of technologies that could make decisions that unfairly impact specific groups), privacyconsiderations, and security considerations. The conference expects that many papers will be foundational research and not tied to par-ticular applications, let alone deployments. However, if there is a direct path to any negativeapplications, the authors should point it out. For example, it is legitimate to point out thatan improvement in the quality of generative models could be used to generate deepfakes fordisinformation. On the other hand, it is not needed to point out that a generic algorithm foroptimizing neural networks could enable people to train models that generate Deepfakes faster. The authors should consider possible harms that could arise when the technology is being usedas intended and functioning correctly, harms that could arise when the technology is being usedas intended but gives incorrect results, and harms following from (intentional or unintentional)misuse of the technology. If there are negative societal impacts, the authors could also discuss possible mitigation strategies(e.g., gated release of models, providing defenses in addition to attacks, mechanisms formonitoring misuse, mechanisms to monitor how a system learns from feedback over time,improving the efficiency and accessibility of ML).",
  ". New Assets": "Question: Are new assets introduced in the paper well documented and is the documentationprovided alongside the assets?Answer: [Yes]Justification: we only have experiments in Python that will be made public.Guidelines: The answer NA means that the paper does not release new assets. Researchers should communicate the details of the dataset/code/model as part of their sub-missions via structured templates. This includes details about training, license, limitations,etc.",
  ". Crowdsourcing and Research with Human Subjects": "Question: For crowdsourcing experiments and research with human subjects, does the paperinclude the full text of instructions given to participants and screenshots, if applicable, as well asdetails about compensation (if any)?Answer: [NA]Justification: our experiments do not involve crowdsourcing.Guidelines: The answer NA means that the paper does not involve crowdsourcing nor research with humansubjects.",
  "According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or otherlabor should be paid at least the minimum wage in the country of the data collector": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Sub-jectsQuestion: Does the paper describe potential risks incurred by study participants, whether suchrisks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals(or an equivalent approval/review based on the requirements of your country or institution) wereobtained?Answer: [NA]Justification: our study do not involve risk for participants.Guidelines: The answer NA means that the paper does not involve crowdsourcing nor research with humansubjects. Depending on the country in which research is conducted, IRB approval (or equivalent) may berequired for any human subjects research. If you obtained IRB approval, you should clearlystate this in the paper. We recognize that the procedures for this may vary significantly between institutions andlocations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines fortheir institution."
}