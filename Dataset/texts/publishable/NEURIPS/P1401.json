{
  "Abstract": "Online free-viewpoint video (FVV) streaming is a challenging problem, which isrelatively under-explored. It requires incremental on-the-fly updates to a volumetricrepresentation, fast training and rendering to satisfy real-time constraints and asmall memory footprint for efficient transmission. If achieved, it can enhance userexperience by enabling novel applications, e.g., 3D video conferencing and livevolumetric video broadcast, among others. In this work, we propose a novel frame-work for QUantized and Efficient ENcoding (QUEEN) for streaming FVV using3D Gaussian Splatting (3D-GS). QUEEN directly learns Gaussian attribute residu-als between consecutive frames at each time-step without imposing any structuralconstraints on them, allowing for high quality reconstruction and generalizability.To efficiently store the residuals, we further propose a quantization-sparsity frame-work, which contains a learned latent-decoder for effectively quantizing attributeresiduals other than Gaussian positions and a learned gating module to sparsifyposition residuals. We propose to use the Gaussian viewspace gradient differencevector as a signal to separate the static and dynamic content of the scene. It actsas a guide for effective sparsity learning and speeds up training. On diverse FVVbenchmarks, QUEEN outperforms the state-of-the-art online FVV methods on allmetrics. Notably, for several highly dynamic scenes, it reduces the model size tojust 0.7 MB per frame while training in under 5 sec and rendering at 350 FPS.",
  "Introduction": "The dynamic world that we perceive around us is not 2D, but rather 3D. Unlike 2D videos, whichare ubiquitous, the question of how to effectively capture, encode and disseminate free-viewpointvideos (FVV) of dynamic 3D scenes, which can be viewed at any instance of time and from anyviewpoint, has intrigued computer vision and graphics researchers for much time. Free-viewpointvideo transmission, if achieved, has the potential to transform and enrich user experience in profoundways by offering novel immersive experiences, e.g., FVV video playback and live streaming, 3Dvideo conferencing and telepresence, gaming, virtual spatial tutoring and teleoperation, among others. The underlying problem of reconstructing FVV involves learning a 6D plenoptic function of a dynamicscene P(x, d, t) from sparse multiple views acquired over a window of time, with x R3 being aposition in 3D space, d = (, ) a viewing direction and t an instance of time. Neural volumetricrepresentations, which learn a 5D plenoptic function of a scene P(x, d) at a fixed time instance, e.g.,",
  "arXiv:2412.04469v1 [cs.CV] 5 Dec 2024": "neural radiance fields (NeRFs) and its variants present a compact and high-fidelityrepresentation for 3D scenes. NeRFs have also been extended to dynamic 4D scenes providing a powerful tool for reconstructing FVV. However, NeRFs require compositing denseinformation across a 3D volume and hence are slow to train and render. Recently, 3D GaussianSplatting (3D-GS) has emerged as a promising technique with significantly faster training andrendering speeds in comparison with NeRFs, and they have also been extended to dynamic 4Dscenes . While these representations accurately model 4D scenes, they are trained in anoffline fashion requiring full multi-view video sequences to learn temporal relationships betweenframes. They also require long training times to achieve high reconstruction quality and are mostlynot streamable. Online FVV, e.g., for broadcast and teleconferencing applications, presents additional challengesversus offline. It requires incremental on-the-fly updates to volumetric representation at each time-stepof the dynamic scene, fast training and rendering times to maintain real-time operation, and smallpacket sizes per frame to enable effective transmission on bandwidth-limited channels. Consequently,the more challenging problem of online FVV reconstruction remains relatively under-explored.Notable prior solutions are those based on NeRFs using voxel grids or triplanes to learn3D representations that are updated on-the-fly. Unsurprisingly, they suffer from slow rendering speeds.Recently, Sun et al. proposed 3DGStream , which uses 3D-GS to model a 3D scene along withInstantNGP to model its geometric transformation over time. It achieves high rendering speedsbut imposes heuristic structural constraints on the volumetric representation to achieve efficiency,which compromises model expressiveness and quality. In this work, we propose a novel QUantized and Efficient ENcoding (QUEEN) framework, whichuses 3D-GS for online FVV. Similarly to prior approaches , we also learn Gaussian attributeresiduals between consecutive time-steps. To reduce memory requirements, however, learnsonly a subset of the Gaussian attributes at each time-step, limiting model expressiveness. Our firstinsight, therefore, is to model residuals for all attributes instead, which does not compromise quality.However, encoding all Guassian attributes increases the per frame memory requirement and hencenecessitates a means to compress them more effectively. Our second insight, then, is to learn todirectly compress the Gaussian residuals in proportion to the real-time scene dynamics, e.g., motionand illumination changes. This contrasts with existing methods that employ a singlefixed-sized structure, e.g., a voxel-grid, a triplane, or hash encoding at all time-steps, and the resultis higher efficiency in terms of model size, training speeds, and rendering speeds. Lastly, we alsoexploit temporal redundancies across time-steps to limit computations to the highly dynamic parts ofthe scene only and achieve further efficiencies. Specifically, to achieve this, we propose a learned quantization-sparsity framework to simultaneouslylearn and compress Gaussian attribute residuals for each time-step. We quantize all attribute residuals,except Gaussian positions, via an end-to-end trainable integer-based latent-decoder. Once learned,we efficiently encode the integer latents via entropy coding to achieve high compression factors.For position residuals that exhibit greater sensitivity to quantization, we propose a learned gatingmechanism to sparsify them, which identifies the static (corresponding to 0 value) and dynamicGaussians and retains the sparse dynamic ones only at full precision. Finally, to achieve furtherefficiencies in terms of training time and storage, we utilize the differences between the 2D viewspaceGaussian gradients of consecutive frames to initialize our learnable gates, and to selectively renderlocal image regions corresponding to highly dynamic scene content. We evaluate our approach, QUEEN, on two benchmark datasets, containing diverse scenes withlarge geometric motion and illumination changes. QUEEN outperforms all prior state-of-the-artapproaches for online FVV and significantly reduces the per-frame memory cost (10), all whileachieving higher reconstruction quality, as well as faster training and rendering speeds. Extensiveablations show the efficacy of the various components of our approach.",
  "To summarize, our key contributions are:": "We propose a Gaussian residual-based framework to model 3D dynamic scenes for online FVVwithout any structural constraints, which allows free learning of all 3D-GS attribute residuals,resulting in higher model expressiveness. We introduce a learned quantization-sparsity framework for compressing per-frame residuals, andwe initialize and train it efficiently using viewspace gradient differences that separate the dynamicand static scene content.",
  "Traditional Free-viewpoint Video": "Ever since early FVV work such as , a series of geometry-based FVV methods has beenpushing for high reconstruction quality and streamable performance. However, their rendering andcompression quality rely on the accuracy of a sophisticated pipeline of geometry reconstruction , tracking , and texturing . They also require high-end hardware for capturing complexand dynamic appearance . Purely image-based rendering relaxes therequirement for geometric accuracy. Although methods such as support view interpolationwith layered representations in the dynamic setting, they require a high count of views as input toensure interpolation quality.",
  "Neural and Gaussian-based Free-viewpoint Video": "Offline Methods.Compared to the traditional representations, the emergence of neural rep-resentations opened a new door for capturing FVV for dynamic hu-mans and monocular videos . In this work,we focus on general dynamic scenes from multiple views to push the quality of streamable FVVwithout requiring a strong human prior or a very constrained input. model thescene dynamics via explicit deformation. Although suitable for motion analysis, they inevitably facea trade-off between motion accuracy and visual quality . To tackle this, use a spatial-temporal formulation via time-conditioned latent codes to implicitly encode the 4D scene, enablingreconstruction of topological changes and volumetric effects. factorize the 4D scene intomultiple space-time feature planes and achieves higher model compactness and training efficiency. decompose the 4D scene into static and dynamic volumes. incorporateefficient NeRF representations for higher fidelity. Although, these NeRF-based methodachieve high compactness, they suffer from low rendering efficiency, even when converted to a more efficient NeRF formulation . Seeing their great potential for efficiency, recentworks extend 3D Gaussian representations to dynamic scenes, with temporal attributes ,generalized 4D Gaussians and a hybrid representation . While these methods achieve highquality in modeling 3D dynamic scenes, they, together with the aforementioned NeRF-based methods,are mostly offline, i.e., they require all the input video and a long time for training, which is inherentlydifficult for streaming applications. Online Methods. Online reconstruction for FVVs is relatively under-explored, as it imposes addi-tional challenges of on-the-fly reconstruction using only local temporal information instead of thefull recordings. Furthermore, toward the goal of streamable FVVs, the encoding system is evaluatedby multiple metrics including compression rate, encoding and rendering speed and visual quality. tracks dense 3D Gaussians by solving their motion over time. Visual quality and dynamicappearance is not their focus. models motion by rendering scene dynamics, however theirmethod is not optimized for efficiency. focuses on generalizable NeRF reconstruction and showsgood promise to adapt to a new frame but has a high memory footprint due to an MVSNet-styleneural network . accelerates training and rendering speed with a special tuning strategyand sparse voxels, however, their representation still has high temporal redundancy. proposes anincremental training scheme with natural information partitioning and achieves high compression,but its encoding is slow. Several works use video codec-inspired encoding paradigms fordata efficiency. achieves a decent compression rate and near interactive rendering with compactmotion and residual grids. However, their training requires 10 minutes per frame. focuses onreal-time decoding, streaming and rendering instead of on-the-fly encoding. performs groupedtraining on a hybrid representation of triplanes and volume grid. While achieving high compressionrate, their fixed encoding paradigm and aggressive quantization limits their reconstruction qualityalong-with low rendering speeds. is the closest work to ours for streaming FVV via 3D-GS.They encode the position and rotation residuals via an Instant-NGP based transformation cache.While achieving faster training and rendering speeds than prior work, they have high data redundancydue to a fixed structured modeling. Additionally, they focus on geometric transformations only and",
  "Time": ": Overview of QUEEN for online FVV. We incrementally update Gaussian attributes at eachtime-step (gray block) by simultaneously learning and compressing residuals between consecutivetime-steps via a quantization (orange block) and sparsity (yellow block) framework. We additionallyrender only the dynamic Gaussians for masked regions to achieve faster convergence (green block). can approximate only small changes in new scene content or lighting variations. Our work updatesand compresses all 3D-GS attributes freely without any structural constraints while still obtainingmuch better memory costs via our quantization-sparsity framework along with better training times.",
  "D Scene Representation Compression": "Several works propose a variety of compression methodologies for reducing the memory, training time,or rendering speed of standard static scene 3D representations. decompose NeRFs via low-rank approximations. prune voxels along with vector quantization by . compressmulti-resolution feature-grids via codebook/vector quantization. A large number of approaches target3D-GS compression and acceleration via pruning. While these approaches can beapplied to static representations on a per-frame basis, their trivial frame-wise application would resultin extremely high training costs as well as large memory per frame. To enable streaming, our work,instead, explicitly focuses on effectively leveraging the temporal redundancies across frames bycompressing the residual information between them to achieve greater efficiency.",
  "QUEEN: Quantized Efficient Encoding for Streaming FVV": "A solution for streamable FVV must have low-latency encoding (training) and decoding (rendering),and low data bandwidth (memory) for transmission on a common network infrastructure. Motivatedby these constraints, we aim to generate streamable FVVs with compact representations that are fastto train and render incrementally. In this section, we first provide an overview of 3D-GS (Sec. 3.1).In Sec. 3.2, we propose a compression framework to efficiently represent and train Gaussian attributeresiduals at each time step. Sec. 3.3 discusses utilizing an approach based on viewspace gradientdifferences to achieve greater efficiencies. An overview of our method is shown in .",
  "Our efficient representation for dynamic scenes is based on 3D Gaussian Splatting (3DGS) .Given multi-view images I, a 3D scene is modeled by a set of Gaussians with attributes A": "Representation. The shape of each Gaussian i is defined by its mean pi R3 and covariancematrix i. The covariance matrix is represented by i = RiSiSTi RTi , where Ri is a rotationmatrix parameterized by a quaternion vector qi R4, and the scale matrix Si is a diagonal matrixwith elements si R3. Each Gaussian also contains opacity oi and spherical harmoniccoefficients hi for view-dependent appearance with dimensions based on the number of degrees.",
  "At = At1 + Rt,(4)": "where Rt consists of learnable residuals for each attribute (in (gray block)). For time-step t = 0,we perform vanilla Gaussian splatting training to obtain attributes A0. This sequential formulationallows us to freely and adaptively update the residuals Rt on-the-fly with incoming streaming trainingviews, without any structural constraints as in prior works . However, representing the 4Dscene with uncompressed residuals is still highly inefficient. As residuals have low magnitudes incomparison with the attributes themselves, they can be efficiently compressed, for which we proposea novel quantization-sparsity framework.",
  "Attribute Residual Quantization": "There exists spatial redundancy within the Gaussian attributes of the same time-step. NearbyGaussians have highly correlated residuals for shape, orientation and appearance. To reduce thestorage cost of the residuals, we propose to utilize a quantization framework during training . At each time-step t, we represent the residuals via quantized latents and a shared compact decoder.Specifically, to obtain the residuals for each category2 ri RM, we maintain corresponding quantizedinteger latents li ZL for each Gaussian i. These latents are passed through a shared linear decoderD with learnable parameters D RML to obtain the decoded attribute residual ri. Such a compactdecoder has small time and memory costs due to few parameters and arithmetic operations. Toallow differentiable training of the integer latents via gradient optimization, we use a continuousapproximation li RL instead. li are rounded to the nearest integer values for the forward pass butcan still receive backpropagated gradients via the Straight-Through Estimator (STE) :",
  "li = STE(li),ri = D(li; D) = D float(li).(5)": "The continuous latents L = {li}Ni=1, and the shared decoders parameters D are learnable duringtraining. After adding the decoded residuals to the previous time-steps attributes (Eq. 4), the standardrasterization process (Eq. 2) is used to obtain the rendered image. This differentiable quantizationmodule is trained end-to-end with the main training process by optimizing the reconstruction loss.Post-training, we entropy code the quantized latents L and directly store the decoder D. Entropycoding results in as much as 10 reduction in model size from 44 to 4 MB without quality degradation.",
  "Position Residual Gating": "Sparse Representation. While most of the attribute residuals can be quantized effectively with ourproposed method in Sec 3.2.1, we observe that the position residuals are sensitive to quantization andrequire high precision during rendering3. Storing all the full-precision position residuals, however,still results in high per-frame memory costs. To tackle this, we propose a learned gating methodology,which enforces sparsity in the residuals instead of quantization. This mechanism allows us to seta vast majority of the position residuals to zeros, while maintaining full-precision non-zero values.Specifically, we represent the positional residual for each Gaussian i as pi = gi lpi, where thescalar gi is the learnable gate variable and lpi R3 is the learnable pre-gated residual in full precisionduring training. After training, the sparse pi can be efficiently stored via sparse matrix formats to reduce memory costs. Thus, our goal is to encourage the sparsity for the variable gi across all",
  "Gaussians. This goal also aligns with the observation that a large portion of a dynamic scene is staticor nearly static, which can be leveraged to attain high compression performance": "Hard Concrete Gate. Sparsity can be induced via L0 or L1 norm regularization penalties. However,L1 norm induces shrinkage, i.e., lowers the magnitude of even non-zero values. L0 norm is theideal sparsity loss without shrinkage, but is computationally intractable with non-differentiability andcombinatorial complexity. To enforce sparsity, we instead propose to use the hard concrete gate .For each Gaussian i, the concrete gate is a continuous relaxation of the Bernoulli distribution:",
  "gi = Sigmoid( log i/),(6)": "where i is a learnable parameter and is the temperature parameter. Although the concrete gateapproximates the discrete Bernoulli gate, it does not include the end points {0, 1}, which does notdirectly result in sparsity. The hard concrete gate stretches the range of the concrete gate to theinterval (0, 1) and then applies a hard-sigmoid:",
  ": Viewspace Gradient Difference. We use the differenceof viewspace gradients between consecutive frames to identifydynamic scene content": "Real-world dynamic scenes con-tain high amounts of temporalredundancy with only a fractionof the content changing betweenconsecutive time-steps.Theproposed quantization-sparsityframework can learn to iden-tify Gaussians corresponding tostatic scene content and settheir residuals to 0.How-ever, they still forward/backwardpass through static regions result-ing in wasted training computa-tion.Additionally, initializingthe gates with 1s requires moreiterations for convergence. Wethus propose a proxy metric to identify Gaussians, which are static or dynamic at the start of training.We use this metric to initialize our gates while also identifying dynamic image regions to performlocal rendering in, during training. Viewspace Gradient Difference.The ground-truth (GT) training images contain information ofthe dynamic scene content, which we leverage to separate static and dynamic Gaussians. A simplepixel difference between consecutive frames does not account for illumination changes and is a noisysignal for the geometric position residuals. 3D-GS utilizes 2D viewspace gradients L",
  ",L(v)t1 = LI(v)t1, I(v)t1,L(v)t= LI(v)t, I(v)t1.(9)": "As shown in , dt identifies the dynamic scene regions while factoring out the noise fromimperfect reconstructions at time-step t 1. We use the norm of the score vector |dti| to initializethe gate parameters. We define the probability of a gate being active for Gaussian i at time-step t as",
  "We set pi in Eq. 8 to be dti to solve for the initial i. This initialization leads to better convergenceby identifying Gaussians corresponding to 0 position residuals at the start of training itself": "Adaptive Masked Training. In addition to gate initialization, we propose to utilize the score vectordt for an adaptive masked training scheme. We split the Gaussians into static or dynamic parts byapplying a threshold td on the norm of dt. We render dynamic Gaussians for each training viewto identify corresponding dynamic image regions. We then only render and backpropagate throughthese pixel locations. We perform this masked training for a fraction of the full training iterations,and find it to improve training speeds with little to no loss of reconstruction quality.",
  "Efficient End-to-end Learnable Residuals": "Initial Frame Reconstruction. For an incrementally updating online approach, it is importantto make sure the initial frame is well reconstructed. COLMAP used to initialize the positions ofGaussians can result in sparse 3D points for regions with sparse camera views. Hence, we use anoff-the-shelf monocular depth estimation network to estimate point locations in these empty regionsand predict a more complete initial point cloud. Further details and results are in the supplementary. End-to-end Training. We train separate decoders and quantized latents {Lc, Dc|c {q, s, o, h}}for all attributes except position. For position, we learn the gate parameters and positional residualsLp. All variables are end-to-end differentiable. The total loss function that we minimize is thereconstruction loss (Eq. 3) and the sparsity gate regularization loss (Eq. 8):",
  "Ltotal = L + regLreg,(11)": "where reg controls tradeoffs between memory and reconstruction quality. By simultaneously quan-tizing while training we achieve high compression while maintaining quality, unlike withpost-training compression that lead to quality degradations. We also apply the 3D-GS densificationstage at each time-step and is sufficient in modeling new or finer scene content. 3DGStream adds Gaussians relative to the first time-step only, which limits their approach to small scene changes.",
  "Datasets and Implementation": "We evaluate our method on two challenging FVV video datasets. (1) Neural 3D Videos (N3DV) consists of six indoor scenes with forward-facing 20-view videos. (2) Immersive Videos consistsof seven indoor and outdoor scenes captures with 46 cameras. In both datasets, the central viewis held out for testing. We implement QUEEN on . We train for 500 and 350 epochs for thefirst time-step, and for 10 and 15 epochs for the subsequent time-steps, for N3DV and Immersive,respectively, on an NVIDIA A100 GPU. One epoch contains all training views. We evaluate visualquality in terms of average frame-wise PSNR, SSIM, and LPIPS (VGG) across all videos. We alsocompute the average storage size and training time for each time-step, and the rendering speed.Additional details are provided in the supplementary materials.",
  "Quantitative Comparisons": "We compare QUEEN against state-of-the-art existing online FVV methods (3DGStream ,StreamRF and TeTriRF ) on N3DV and Immersive (Tab. 1). 3DGStream is the overallbest-performing prior method. Since 3DGStream was originally run on an older NVIDIA V100GPU on N3DV, we re-run 3DGStream on an NVIDIA A100 GPU on both N3DV and Immersiveand denote it as 3DGStream* in Tab. 1 for consistency with QUEEN. For brevity, in Tab. 1 weadditionally compare against only selected top-performing offline FVV methods. We include a moreextensive comparison to all existing offline FVV methods in the supplementary (Tab. 8). Lastly, weevaluate three variants of QUEEN: QUEEN-s (small), QUEEN-m (medium) and QUEEN-l (large),with residuals trained for 6, 8 and 10 epochs, respectively. : Quantitative Results. We compare QUEEN against state-of-the-art online and (a few forbrevity) offline FVV methods on N3DV and Immersive . We include many more offlinemethods in the supplementary (Tab. 8). 3DGStream* refers to our re-implementation on the sameNVIDIA A100 GPU used by QUEEN for fairness. Bold and underlined numbers indicate the bestand the second best results, respectively, within each category.",
  "PSNR(dB) Storage(MB) Training(sec) Rendering(FPS) PSNR(dB) Storage(MB) Training(sec) Rendering(FPS)": "Baseline31.6644.367.2921428.5478.420.85276+ Attribute Quantization32.044.187.2828529.014.5725.17199+ Position Gating32.050.726.9527428.992.0126.99190+ Gate Initialization32.140.607.9227129.081.3327.81177+ Masked Training32.190.747.8824829.221.7919.70183 From Tab. 1, on N3DV, QUEEN-l results in the best quality among all online FVV methods andachieves a 10 reduction in storage size compared to 3DGStream. Although TeTriRF requires lessmemory than QUEEN, it has much worse quality (1.5dB) and rendering speed (4FPS), and highertraining time (39 sec). On Immersive, which contains more pronounced scene changes than N3DV, welimit our comparisons to 3DGStream with longer iterations. TeTriRF requires long convergence timesto achieve reasonable reconstruction quality, limiting their training feasibility. QUEEN-l significantlyoutperforms 3DGStream, obtaining +4dB PSNR, 5 smaller size, and lower training times. Theseresults on the more challenging scenes from the Immersive datasets reveal the structural constraintsbrought by the heuristic compression design of 3DGStream. In contrast, our quantization-sparsityframework shows higher flexibility and quality in capturing changing appearances and scene densityas well as learning compact and effective representations.",
  "Qualitative Comparisons": "In we compare the reconstruction results of the various methods. On N3DV, we reconstructfiner details than 3DGStream, e.g., the hand and the dog, and minimize artifacts such as the tongs inthe top scene or the coffee and metal tumbler in the bottom scene. TeTriRF produces blurry outputs,e.g., the cap or metal tumbler in the bottom scene. On Immersive, we better model illuminationchanges and new scene content such as the person (first patch) and the flame (third patch) in the topscene or the face in the bottom scene (second patch) versus 3DGStream.",
  "Ablations": "Effect of Updating Appearance Attributes.To ablate the importance of updating the Gaussianappearance attributes (color and opacity) per frame in QUEEN, we run experiments on N3DV for2 settings: (1) learning only geometric attribute residuals (position, scale and covariance) withappearance residuals set to zero and (2) learning all residuals per frame (Tab. 3). Updating onlygeometric attributes results in a drop of 0.4 dB PSNR versus updating all attributes.Visually, for theFlame Steak scene in N3DV (), updating all attributes results in the highest quality, while fixingopacity introduces artifacts at the edge of the flamethrower. Fixing color, additionally, results in asignificant drop in PSNR (-0.6 dB) producing a discolored flame (rightmost column). Effect of Attribute Compression and Masked Training.We show results for five variants ofQUEEN with incrementally added sub-components: (1) a baseline with uncompressed residualtraining (Sec. 3.2), (2) adding quantization to all attributes except position (Sec. 3.2.1), (3) addinggating of position residuals (Sec. 3.2.2), (4) gate initialization with viewspace gradient differencesand (5) masked image training (Sec. 3.3). Results are summarized in Tab. 2 for both N3DV andImmersive datasets. Compressing attributes and gating position residuals results in significant modelsize reduction on both datasets (60, 40). This is further reduced by gate initialization with",
  ": Effect of Gating. While a large number of gates (47%) are active at start of training (a, c),they are pruned and only gates corresponding to changing scene content (2%) remain active (b, d)": "viewspace gradient differences due to faster convergence of the gates, without loss of quality. Bymasked training via localized image rendering, we reduce training time by 8 seconds for Immersiveand marginally for N3DV. Overall, from the baseline, we obtain significant model size reduction withequivalent or lower training and rendering speed. Attribute quantization framework even improvesPSNR compared to the baseline for both datasets. This largely stems from quantizing the scalingattribute leading to a more stable optimization with better reconstruction quality while reducingstorage size (Tab. 4). Effect of Gating. As shown in , more than half of the gates are set to be inactive at the start oftraining with viewspace gradient initialization (Sec. 3.3) and a large portion of the image is active.However, post-training, most gates become inactive while the remaining active gates successfullyfocus on the dynamic scene content, e.g., the persons hands or the dogs face. This validates that ourgating mechanism effectively separates static and dynamic scene content.",
  ": Adaptive Image Mask Visualization. We separate out the dynamic scene content atdifferent time-steps of the video through our viewspace gradient difference approach in Sec. 3.3": "Effect of Adaptive Image Mask.We visualize the masks obtained by our viewspace gradientdifference module in Sec. 3.3. Results on 2 scenes in the Immersive dataset are shown in . Forvarious time instance of the video (columns), we adaptively identify image regions corresponding tothe dynamic scene content. We can therefore perform local image rendering and backpropagation forfaster training skipping computation for the static parts of the scene such as the background.",
  "Conclusion": "We proposed QUEEN, a framework to model 3D dynamic scenes for online FVV using 3D-GS.We utilized an attribute residual framework, which freely updates all parameters leading to bettermodeling of complex scenes. We show that the residuals can be successfully compressed via ourlearned quantization-sparsity mechanism, which adapts to the dynamic scene content to achieve verysmall model sizes, improved training and rendering speeds, and improved visual quality. In futurework, we aim to extend QUEEN for sparse view reconstruction or sequences with long duration. Benjamin Attal, Jia-Bin Huang, Christian Richardt, Michael Zollhoefer, Johannes Kopf,Matthew OToole, and Changil Kim. Hyperreel: High-fidelity 6-dof video with ray-conditionedsampling. In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1661016620, 2023. Jonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla,and Pratul P Srinivasan. Mip-nerf: A multiscale representation for anti-aliasing neural radiancefields. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages58555864, 2021.",
  "Ang Cao and Justin Johnson. Hexplane: A fast representation for dynamic scenes. In Proceed-ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 130141,2023": "Charles-Flix Chabert, Per Einarsson, Andrew Jones, Bruce Lamond, Wan-Chun Ma, SebastianSylwan, Tim Hawkins, and Paul Debevec. Relighting human locomotion with flowed reflectancefields. In ACM SIGGRAPH 2006 Sketches, pages 76es. Association for Computing Machinery,2006. Eric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano, Boxiao Pan, Shalini De Mello,Orazio Gallo, Leonidas J Guibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient geometry-aware 3d generative adversarial networks. In Proceedings of the IEEE/CVF conference oncomputer vision and pattern recognition, pages 1612316133, 2022. Anpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang, Fanbo Xiang, Jingyi Yu, andHao Su. Mvsnerf: Fast generalizable radiance field reconstruction from multi-view stereo. InProceedings of the IEEE/CVF international conference on computer vision, pages 1412414133,2021.",
  "Abe Davis, Marc Levoy, and Fredo Durand. Unstructured light fields. Comput. Graph. Forum,31(2pt1):305314, may 2012. ISSN 0167-7055. doi: 10.1111/j.1467-8659.2012.03009.x. URL": "Paul Debevec, Tim Hawkins, Chris Tchou, Haarm-Pieter Duiker, Westley Sarokin, and MarkSagar. Acquiring the reflectance field of a human face. In Proceedings of the 27th annualconference on Computer graphics and interactive techniques, pages 145156, 2000. Chenxi Lola Deng and Enzo Tartaglione. Compressing explicit voxel grid representations: fastnerfs become also small. In Proceedings of the IEEE/CVF Winter Conference on Applicationsof Computer Vision, pages 12361245, 2023. Yilun Du, Yinan Zhang, Hong-Xing Yu, Joshua B Tenenbaum, and Jiajun Wu. Neural radianceflow for 4d view synthesis and video processing. In 2021 IEEE/CVF International Conferenceon Computer Vision (ICCV), pages 1430414314. IEEE Computer Society, 2021.",
  "Zhiwen Fan, Kevin Wang, Kairun Wen, Zehao Zhu, Dejia Xu, and Zhangyang Wang. Light-gaussian: Unbounded 3d gaussian compression with 15x reduction and 200+ fps. arXiv preprintarXiv:2311.17245, 2023": "Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and AngjooKanazawa. Plenoxels: Radiance fields without neural networks. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition, pages 55015510, 2022. Sara Fridovich-Keil, Giacomo Meanti, Frederik Rahbk Warburg, Benjamin Recht, and AngjooKanazawa. K-planes: Explicit radiance fields in space, time, and appearance. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1247912488,2023.",
  "Sharath Girish, Kamal Gupta, and Abhinav Shrivastava. Eagles: Efficient accelerated 3dgaussians with lightweight encodings. arXiv preprint arXiv:2312.04564, 2023": "Sharath Girish, Abhinav Shrivastava, and Kamal Gupta. Shacira: Scalable hash-grid com-pression for implicit neural representations. In Proceedings of the IEEE/CVF InternationalConference on Computer Vision (ICCV), pages 1751317524, October 2023. Kaiwen Guo, Peter Lincoln, Philip Davidson, Jay Busch, Xueming Yu, Matt Whalen, GeoffHarvey, Sergio Orts-Escolano, Rohit Pandey, Jason Dourgarian, et al. The relightables: Volu-metric performance capture of humans with realistic relighting. ACM Transactions on Graphics(ToG), 38(6):119, 2019.",
  "Lingzhi Li, Zhen Shen, Zhongshu Wang, Li Shen, and Ping Tan. Streaming radiance fieldsfor 3d video synthesis. Advances in Neural Information Processing Systems, 35:1348513498,2022": "Lingzhi Li, Zhen Shen, Zhongshu Wang, Li Shen, and Liefeng Bo. Compressing volumetricradiance fields to 1 mb. In Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 42224231, 2023. Ruilong Li, Julian Tanke, Minh Vo, Michael Zollhfer, Jrgen Gall, Angjoo Kanazawa, andChristoph Lassner. Tava: Template-free animatable volumetric actors. In European Conferenceon Computer Vision, pages 419436. Springer, 2022. Tianye Li, Timo Bolkart, Michael. J. Black, Hao Li, and Javier Romero. Learning a modelof facial shape and expression from 4D scans. ACM Transactions on Graphics, (Proc. SIG-GRAPH Asia), 36(6):194:1194:17, 2017. URL Tianye Li, Mira Slavcheva, Michael Zollhoefer, Simon Green, Christoph Lassner, ChangilKim, Tanner Schmidt, Steven Lovegrove, Michael Goesele, Richard Newcombe, et al. Neural3d video synthesis from multi-view video. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, pages 55215531, 2022. Zhan Li, Zhang Chen, Zhong Li, and Yi Xu. Spacetime gaussian feature splatting for real-timedynamic view synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition, 2024. Zhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang. Neural scene flow fields forspace-time view synthesis of dynamic scenes. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, pages 64986508, 2021. Zhengqi Li, Qianqian Wang, Forrester Cole, Richard Tucker, and Noah Snavely. Dynibar:Neural dynamic image-based rendering. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition (CVPR), 2023. Haotong Lin, Sida Peng, Zhen Xu, Yunzhi Yan, Qing Shuai, Hujun Bao, and Xiaowei Zhou.Efficient neural radiance fields for interactive free-viewpoint video. In SIGGRAPH Asia 2022Conference Papers, pages 19, 2022. Haotong Lin, Sida Peng, Zhen Xu, Tao Xie, Xingyi He, Hujun Bao, and Xiaowei Zhou. High-fidelity and real-time novel view synthesis for dynamic scenes. In SIGGRAPH Asia 2023Conference Papers, pages 19, 2023. Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel Schwartz, Andreas Lehrmann, andYaser Sheikh. Neural volumes: Learning dynamic renderable volumes from images. ACMTransactions on Graphics, 38(4):65:165:14, July 2019. Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J. Black.SMPL: A skinned multi-person linear model. ACM Trans. Graphics (Proc. SIGGRAPH Asia),34(6):248:1248:16, October 2015.",
  "Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuousrelaxation of discrete random variables. arXiv preprint arXiv:1611.00712, 2016": "Ricardo Martin-Brualla, Noha Radwan, Mehdi SM Sajjadi, Jonathan T Barron, Alexey Dosovit-skiy, and Daniel Duckworth. Nerf in the wild: Neural radiance fields for unconstrained photocollections. In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 72107219, 2021. Ben Mildenhall, Pratul P Srinivasan, Rodrigo Ortiz-Cayon, Nima Khademi Kalantari, RaviRamamoorthi, Ren Ng, and Abhishek Kar. Local light field fusion: Practical view synthesiswith prescriptive sampling guidelines. ACM Transactions on Graphics (TOG), 38(4):114,2019. Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi,and Ren Ng. NeRF: Representing scenes as neural radiance fields for view synthesis. InEuropean Conference on Computer Vision, 2020.",
  "Richard A Newcombe, Steven J Lovegrove, and Andrew J Davison. Dtam: Dense tracking andmapping in real-time. In 2011 international conference on computer vision, pages 23202327.IEEE, 2011": "Sergio Orts-Escolano, Christoph Rhemann, Sean Fanello, Wayne Chang, Adarsh Kowdle, YuryDegtyarev, David Kim, Philip L Davidson, Sameh Khamis, Mingsong Dou, et al. Holoportation:Virtual 3d teleportation in real-time. In Proceedings of the 29th annual symposium on userinterface software and technology, pages 741754, 2016. Panagiotis Papantonakis, Georgios Kopanas, Bernhard Kerbl, Alexandre Lanvin, and GeorgeDrettakis. Reducing the memory footprint of 3d gaussian splatting. In Proceedings of the ACMon Computer Graphics and Interactive Techniques, volume 7, 2024. Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T. Barron, Sofien Bouaziz, Dan BGoldman, Ricardo Martin-Brualla, and Steven M. Seitz. Hypernerf: A higher-dimensionalrepresentation for topologically varying neural radiance fields. ACM Trans. Graph., 40(6), dec2021.",
  "Fabin Prada, Misha Kazhdan, Ming Chuang, Alvaro Collet, and Hugues Hoppe. Spatiotemporalatlas parameterization for evolving meshes. ACM Transactions on Graphics (TOG), 36(4):112,2017": "Albert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. D-nerf:Neural radiance fields for dynamic scenes. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, pages 1031810327, 2021. Shenhan Qian, Tobias Kirschstein, Liam Schoneveld, Davide Davoli, Simon Giebenhain, andMatthias Niener. Gaussianavatars: Photorealistic head avatars with rigged 3d gaussians. arXivpreprint arXiv:2312.02069, 2023. Ren Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towardsrobust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. IEEETransactions on Pattern Analysis and Machine Intelligence, 44(3), 2022.",
  "Umme Sara, Morium Akter, and Mohammad Shorif Uddin. Image quality assessment throughfsim, ssim, mse and psnra comparative study. Journal of Computer and Communications, 7(3):818, 2019": "Ruizhi Shao, Zerong Zheng, Hanzhang Tu, Boning Liu, Hongwen Zhang, and Yebin Liu.Tensor4d: Efficient neural 4d decomposition for high-fidelity dynamic reconstruction andrendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1663216642, 2023. Vincent Sitzmann, Justus Thies, Felix Heide, Matthias Niener, Gordon Wetzstein, and MichaelZollhofer. Deepvoxels: Learning persistent 3d feature embeddings. In Proceedings of the IEEEConference on Computer Vision and Pattern Recognition, pages 24372446, 2019. Liangchen Song, Anpei Chen, Zhong Li, Zhang Chen, Lele Chen, Junsong Yuan, Yi Xu, andAndreas Geiger. Nerfplayer: A streamable dynamic scene representation with decomposedneural radiance fields. IEEE Transactions on Visualization and Computer Graphics, 29(5):27322742, 2023. doi: 10.1109/TVCG.2023.3247082. Jiakai Sun, Han Jiao, Guangyuan Li, Zhanjie Zhang, Lei Zhao, and Wei Xing. 3dgstream:On-the-fly training of 3d gaussians for efficient streaming of photo-realistic free-viewpointvideos. arXiv preprint arXiv:2403.01444, 2024. Towaki Takikawa, Alex Evans, Jonathan Tremblay, Thomas Mller, Morgan McGuire, AlecJacobson, and Sanja Fidler. Variable bitrate neural fields. In ACM SIGGRAPH 2022 ConferenceProceedings, pages 19, 2022.",
  "Jiaxiang Tang, Xiaokang Chen, Jingbo Wang, and Gang Zeng. Compressible-composable nerfvia rank-residual decomposition. Advances in Neural Information Processing Systems, 35:1479814809, 2022": "Ayush Tewari, Justus Thies, Ben Mildenhall, Pratul Srinivasan, Edgar Tretschk, Wang Yifan,Christoph Lassner, Vincent Sitzmann, Ricardo Martin-Brualla, Stephen Lombardi, et al. Ad-vances in neural rendering. In Computer Graphics Forum, volume 41, pages 703735. WileyOnline Library, 2022. Edgar Tretschk, Ayush Tewari, Vladislav Golyanik, Michael Zollhfer, Christoph Lassner, andChristian Theobalt. Non-rigid neural radiance fields: Reconstruction and novel view synthesisof a dynamic scene from monocular video. In IEEE International Conference on ComputerVision (ICCV). IEEE, 2021. Edith Tretschk, Vladislav Golyanik, Michael Zollhfer, Aljaz Bozic, Christoph Lassner, andChristian Theobalt. Scenerflow: Time-consistent reconstruction of general dynamic scenes. InInternational Conference on 3D Vision (3DV), 2024.",
  "Chaoyang Wang, Ben Eckart, Simon Lucey, and Orazio Gallo. Neural trajectory fields fordynamic novel view synthesis. arXiv preprint arXiv:2105.05994, 2021": "Feng Wang, Sinan Tan, Xinghang Li, Zeyue Tian, Yafei Song, and Huaping Liu. Mixed neuralvoxels for fast multi-view video synthesis. In Proceedings of the IEEE/CVF InternationalConference on Computer Vision, pages 1970619716, 2023. Liao Wang, Jiakai Zhang, Xinhang Liu, Fuqiang Zhao, Yanshun Zhang, Yingliang Zhang,Minye Wu, Jingyi Yu, and Lan Xu. Fourier plenoctrees for dynamic radiance field renderingin real-time. In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1352413534, 2022. Liao Wang, Qiang Hu, Qihan He, Ziyu Wang, Jingyi Yu, Tinne Tuytelaars, Lan Xu, and MinyeWu. Neural residual radiance fields for streamably free-viewpoint videos. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 7687,June 2023.",
  "Shengze Wang, Alexey Supikov, Joshua Ratcliff, Henry Fuchs, and Ronald Azuma. Inv:Towards streaming incremental neural videos. arXiv preprint arXiv:2302.01532, 2023": "Chung-Yi Weng, Brian Curless, Pratul P Srinivasan, Jonathan T Barron, and Ira Kemelmacher-Shlizerman. Humannerf: Free-viewpoint rendering of moving people from monocular video. InProceedings of the IEEE/CVF conference on computer vision and pattern Recognition, pages1621016220, 2022. Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu,Qi Tian, and Wang Xinggang. 4d gaussian splatting for real-time dynamic scene rendering. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. Minye Wu, Zehao Wang, Georgios Kouros, and Tinne Tuytelaars. Tetrirf: Temporal tri-planeradiance fields for efficient free-viewpoint video. In Proceedings of the IEEE/CVF conferenceon computer vision and pattern recognition, 2024. Wenqi Xian, Jia-Bin Huang, Johannes Kopf, and Changil Kim. Space-time neural irradiancefields for free-viewpoint video. In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 94219431, 2021. Yiheng Xie, Towaki Takikawa, Shunsuke Saito, Or Litany, Shiqin Yan, Numair Khan, FedericoTombari, James Tompkin, Vincent Sitzmann, and Srinath Sridhar. Neural fields in visualcomputing and beyond. In Computer Graphics Forum, volume 41, pages 641676. WileyOnline Library, 2022.",
  "Ziyi Yang, Xinyu Gao, Wen Zhou, Shaohui Jiao, Yuqing Zhang, and Xiaogang Jin. De-formable 3d gaussians for high-fidelity monocular dynamic scene reconstruction. arXiv preprintarXiv:2309.13101, 2023": "Yao Yao, Zixin Luo, Shiwei Li, Tian Fang, and Long Quan. Mvsnet: Depth inference forunstructured multi-view stereo. In Proceedings of the European conference on computer vision(ECCV), pages 767783, 2018. Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and Angjoo Kanazawa. Plenoctrees forreal-time rendering of neural radiance fields. In Proceedings of the IEEE/CVF InternationalConference on Computer Vision, pages 57525761, 2021. Raza Yunus, Jan Eric Lenssen, Michael Niemeyer, Yiyi Liao, Christian Rupprecht, ChristianTheobalt, Gerard Pons-Moll, Jia-Bin Huang, Vladislav Golyanik, and Eddy Ilg. Recent trendsin 3d reconstruction of general non-rigid scenes. In Computer Graphics Forum, page e15062.Wiley Online Library, 2024. Jiakai Zhang, Liao Wang, Xinhang Liu, Fuqiang Zhao, Minzhang Li, Haizhao Dai, BoyuanZhang, Wei Yang, Lan Xu, and Jingyi Yu. Neuvv: Neural volumetric videos with immersiverendering and editing. arXiv preprint arXiv:2202.06088, 2022. Fuqiang Zhao, Wei Yang, Jiakai Zhang, Pei Lin, Yingliang Zhang, Jingyi Yu, and Lan Xu.Humannerf: Efficiently generated human radiance field from sparse inputs. In Proceedingsof the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages77437753, June 2022. C Lawrence Zitnick, Sing Bing Kang, Matthew Uyttendaele, Simon Winder, and RichardSzeliski. High-quality video view interpolation using a layered representation. ACM transactionson graphics (TOG), 23(3):600608, 2004. Matthias Zwicker, Hanspeter Pfister, Jeroen Van Baar, and Markus Gross. Surface splatting. InProceedings of the 28th annual conference on Computer graphics and interactive techniques,pages 371378, 2001. We provide supplementary results (Appendix A), additional implementation details (Appendix B),discussion on limitations and future work (Appendix C) and the broader impact (Appendix D) ofour approach. We recommend the reader to watch the supplementary video hosted on our projectwebsite: for a visualcomparison of the results of the various methods as well as more details of this project.",
  "A.1Quantization vs. Gating": "We evaluate the effect of the gating framework in comparison to the quantization framework for theposition residuals. We perform gating or quantization on the position residuals while quantizing allother attributes. Results, averaged on the N3DV dataset, are shown in . We vary trainingepochs per frame from 6 to 15 for gating and 6 to 25 for quantization to obtain trade-off curves.Increased training epochs result in higher reconstruction quality (PSNR) but longer training timeor a larger model size. The left figure shows the PSNR versus size tradeoff while the right figureshows PSNR versus training time tradeoff. We see that, in both cases, the gating framework producesmuch better tradeoff curves than quantization, with PSNR values more than 0.2dB higher at similarsizes. When increasing the number of training iterations, quantization still improves in quality albeitat a slower rate requiring more training iterations for convergence. This demonstrates that positionattributes are more sensitive to quantization errors and require full precision. It justifies our choiceof learning to sparsify them as opposed to quantizing them. However, this does not translate to theother geometric attributes, scaling and rotation, where quantization is sufficient in compressing theattributes. This is seen in the results in on the Exhibit scene from the Immersive dataset.Quantizing both rotation and scaling results in the lowest storage memory per frame at a similarPSNR and slightly higher training time.",
  "A.2Accuracy-memory Tradeoff": "We consider the effect of varying different loss coefficients to trade off between accuracy and memory.In we explore the tradeoff between PSNR and size by varying the number of trainingiterations. We can also control the amount of sparsity in the scene by varying the reg loss coefficient.As visualized in (b), we find that increasing reg leads to higher sparsity or lower memory butalso lower reconstruction quality. We further experiment with an additional regularization loss to reduce the entropy of the latents. Weobserve that lower entropy corresponds to lower memory, but also lower reconstruction quality. Whilelearnable probability models can successfully reduce entropy, as shown by , these models havehigher time and memory costs during training. We instead observe that the probability distribution ofthe various attribute residuals at each time-step is unimodal and is close to a Laplacian or Gaussiandistribution. As a unimodal distribution has entropy proportional to the variance , we enforce aloss on the standard deviation of the latents with a tradeoff parameter std controlling the effect ofthis regularization loss. (a) shows results on the N3DV dataset by varying std. We observe thatincreasing std reduces the entropy costs, leading to lower memory costs, but lower reconstructionquality, and vice versa.",
  "A.3Effect of Quantization Latent Dimension": "We provide additional analysis on the effect of latent dimension for the various attributes in .In general, latent dimension does not have a significant effect on reconstruction quality or modelsize. Increasing the latent dimension can lead to lower per-dimension entropy due to our learnablequantization framework and hence still maintains the overall total size for the latent. We findthat varying the total number of iterations (Appendix A.1) or the entropy loss/variance coefficient(Appendix A.2) are more effective knobs for trading off between quality-memory or quality-time. 0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85 Model Size (MB)",
  "A.4Framewise PSNR and Size": "A key advantage of our quantization-sparsity framework is its adaptability to scene content. Wevisualize the per frame sizes for 2 scenes each from N3DV and Immersive along with the visualframe difference between consecutive frames in . We see that our approach allocates variablemodel sizes for each frame unlike 3DGStream , which uses a fixed-sized InstantNGP structure.Additionally, higher frame differences result in larger model sizes and vice versa, as seen in the toprow. This shows that our method is capable of allocating more bits to frames with large scene changes.This is especially evident from the spikes in Immersives scenes in the bottom row, which correlatewith the model size. Next, we show the stability of our approach at recovering from large scene variations correspondingto the frame difference spikes as mentioned above. We visualize the reconstructed test-view PSNR foreach frame, for 2 scenes each from N3DV and Immersive, along with the frame difference betweenconsecutive frames in . A large L1 error such as around frames 175 (top left), frames 225(top right), frames 75 (bottom left) or frames 90 and 290 (bottom right) does lead to drops in PSNR.However, our PSNR recovers in subsequent frames showing the stability of our framework with largescene variations present.",
  "A.5Effect of Improved Point Cloud Initialization": "Consistent geometry for the 3D scene in the first frame is important to learn accurate residuals forthe attributes of the subsequent frames. The COLMAP-generated point cloud initialization can beincomplete for regions that are textureless or are not sufficiently captured in multiple cameras. Thisis visualized in the top row of . The boundaries of the scene consist of limited trainingview cameras as shown by the white box leading to sparse or no points in these regions by COLMAPinitialization. The densification stage in 3DGS is unable to recover from this producing erroneousrendered depth or geometry and also leads to low quality image reconstruction. 32.5 33.0 33.5 34.0 34.5 PSNR (dB) PSNR 5.0 5.5 6.0 6.5 7.0 7.5",
  "Frame Difference": ": Per-frame Quality Evaluation. Our approach results in higher PSNR for large scenechanges corresponding to higher consecutive frame difference such as around frame 175 (top right)or the spikes in the bottom right scene. : Effect of Depth Initialization. Top row: (a) COLMAP produces sparse or no points forregions of the scene with limited texture, producing (b) erroneous image rendering and (c) incorrectgeometry or depth. Bottom row: initializing with depth maps predicted by an off-the-shelf monoculardepth network produces better reconstruction and consistent scene geometry. Therefore, we propose to use an off-the-shelf monocular depth estimation network to predicta more complete initial point cloud. However, due to the scale-shift ambiguity of monocular depthestimation, we align the predicted monocular depth with the true scene depth from existing COLMAPpoints. To do so, we estimate the 2D pixel locations pi of each COLMAP point i by projecting pointsfrom 3D world space to 2D screenspace.",
  "aligned with the GT depth value from the COLMAP 3D points zi by a least-squares optimization toobtain the scale and shift parameters , . We then obtain the aligned dense depth map zi +": "To identify regions with empty COLMAP initializations, we iterate over training views and renderthe corresponding image along with an alpha mask calculating the accumulated transmittance ateach pixel location. Mask values below a threshold tz (0.10 for N3DV and 0.03 for Immersive) areidentified to obtain the corresponding pixel locations containing few COLMAP points. We then usethe aligned depth values corresponding to these pixel locations to re-project back into the worldspace. As seen in the bottom row of , the depth map initialization produces more dense pointsin empty regions. These points maintain consistent depth with existing COLMAP points. Such aninitialization results in improved image reconstruction quality with 3dB improvement in PSNR forthe corresponding view while also producing better consistent depth or geometry. As we utilize thenetwork depth at initialization only, we do not require high-quality depth networks and a coarse depthis sufficient for sampling new points. The training can then learn to move the Gaussians to producefiner scene depth. This also results in minimal increases in training time as its a one-time operationat the initial time-step and produces a small number of additional points in empty regions. We show quantitative results for 2 configurations with and without depth map initialization for thedatasets of N3DV and Immersive in Tab. 7. We show PSNR for the central test view as well as for thefirst train view for all scenes. We also show the number of points at the end of training the first framewith and without initialization along with the average training time per frame for the full scene. Thedepth initialization does not significantly affect the test view PSNR on N3DV as it corresponds to thecentral view with a large overlap with many training views. In contrast, the PSNR for the trainingview (also in ) improves considerably (+1.3dB) with the depth initialization highlighting theefficacy of the approach in improving geometry for regions with sparse camera views. Additionally,there is almost no overhead cost as we obtain similar number of Gaussian points at the end of trainingthe first frame. Training time for the full video is also only marginally higher. On Immersive, weobserve a 0.5 dB improvement in PSNR for the test view while the train view PSNR shows minorimprovements. Again, the number of Gaussians and training time is not significantly affected by theinitialization with additional depth based points.",
  "A.6Additional Baseline Comparisons": "We show comparisons to additional offline FVV baselines on the N3DV dataset in for the sakeof completeness. This is a superset of the Tab. 2 in the main paper. A majority of the works computeSSIM using the scikit-image implementation, which tends to produce higher values different fromour SSIM implementation similar to MipNeRF . Since the two are not comparable, we excludeimplementation numbers computed with scikit-image. We also only show results for LPIPS on VGGfor the methods that use it. This is consistent with how we compute LPIPS. : Quantitative Comparisons on the N3DV and Immersive Datasets. We com-pare QUEEN against state-of-the-art online and include offline FVV methods for completeness.3DGStream* refers to our re-implementation on the same NVIDIA A100 GPU as used by QUEENfor fairness. is evaluated on the flame salmon scene only. Bold and underlined numbers indicate thebest and the second best results, respectively, within each category.",
  "A.8Perceptual quality: User study": "In addition to the extensive quantitative analysis in the paper as well as supplementary, we conductan A/B user study to measure the perceptual quality of our video reconstructions. For each vote, weshow a pair of randomly chosen rendering results (from a test view that is not used in training) byour method and one of the baseline methods (3DGStream and TeTriRF ). We also show theground truth video as a reference for the participants to make the decision. We ask the participantto choose the method that more faithfully matches the reference video. In total, we collected 285responses from 15 participants within the timeline of the rebuttal. For the N3DV dataset, 76.67% ofusers preferred our method over 3DGStream and 96.67% preferred our method over TeTriRF. On theGoogle Immersive dataset, 97.14% of users preferred the reconstructions from our approach overthat of 3DGStream. This showed that the participants strongly prefer our results in comparison to thebaseline methods for both datasets.",
  "BImplementation Details": "Training. Our implementation of QUEEN builds on that of . We train the Gaussians for 500 and350 epochs for first time-step, and for 10 and 15 epochs for the subsequent time-steps, on N3DV andImmersive, respectively, with each epoch consisting of all training views. We set the SH degree to 2for N3DV and 3 for Immersive. We set the score vector threshold td = 0.001 for all experiments. Weadditionally dilate the image mask by a 48 48 kernel to include neighboring image regions whilerendering as larger Gaussians can depend on multiple pixel locations. We perform masked trainingfor 30% of the iterations for N3DV and 65% for Immersive. We perform masked training for only afraction of iterations as updating Gaussians rendered only at masked locations can alter the renderedpixels at unmasked location as well. We thus allow fine-tuning on the full image to account for anychanges in the unmasked image regions.",
  "Sparse Gating. We learn the parameters with the Adam optimizer . The position residuallearning rate is set to 0.00016 for N3DV and 0.0005 for Immersive. Other hyperparameters areprovided in": "Quantization. For both datasets, we set the learning rate of the decoder parameters to be 0.001for the color and rotation attributes and 0.0001 for opacity and scaling, optimized with the Adamoptimizer. Other hyperparameters are provided in . Densification. For N3DV, we perform densification for subsequent frames from epoch 6 until 80%of the epochs with an interval of 2 epochs and a gradient threshold of 0.00125. For Immersive, weperform densification on the 8th epoch with a gradient threshold of 0.00125.",
  "N3DV0.10.01-0.51.010.3Immersive0.10.01-0.11.10.5": "First-frame Quantization. Uncompressed Gaussians have large memory costs even for the firstframe. However, quantizing all attributes results in quality degradations . We therefore applylearnable quantization only on the first frames high-frequency spherical harmonic coefficients(excluding the DC component) using the hyperparameters mentioned above. For example, on N3DV,we reduce the first frames size from 47 to 17 MB with no quality degradation. Storage. Post-training, we convert the learned parameters to the final compressed form. For thequantized residuals of rotation, scale, opacity and appearance, we convert the continuous latents Lcto the integer form and then apply entropy encoding and store this further-compressed representationEc as well as the decoder Dc for all four categories c {r, s, o, h}. Our entropy coding approachflattens our integer latent matrix for each attribute before encoding. For example, for L-dimensionallatent attributes for N gaussians, we flatten the matrix to obtain a vector with L*N elements. Thisinteger vector is then encoded using standard entropy coding approaches such as arithmetic coding.",
  "N3DV60.02580.0130.0580.012540.000625Immersive60.01580.00730.0580.0125120.000375": "The number of bits for storing each attribute residual matrix is therefore dependent on the scenecontent as it relies on the amount of motion. This number can be fractional, on average, which is thestandard for the entropy coding algorithm of arithmetic coding or Huffman coding . For example,for the Sear Steak scene in the N3DV dataset, on average, we require 0.68 bits for all the quantizedattributes (corresponding to 0.5MB/frame). This depends on the entropy of the latents itself, whichvaries with changing scene motion (). For the sparse gates, we store the positional residuals as a sparse matrix with the indices from binarizedgate variables I = {i|gi = 0}, and the full-precision residual vectors only if its corresponding gateis on, Ep = {lpi|i I}. Both operations add a negligible computation overhead. This correspondsto the coordinate format (COO) for storing sparse matrices where the non-zero values are stored inFP-32 precision along with their integer index locations.",
  "Ours (N3DV hyperparam.)32.140.60Ours (Immersive hyperparam.)32.061.493DGStream 31.587.80": "We set different hyperparameters for the two datasets in in Tables 11 and 12 to account for the widelyvarying amount of scene motion between N3DV and Immersive datasets. The Immersive datasetcontains larger and more complex scene motions (e.g., a person entering and leaving the scene)while N3DV contains relatively minor motions. We found that a higher learning rate for the positionresiduals allows Gaussians to adapt to the highly dynamic scenes. The gating hyperparametersin for N3DV are set to utilize this prior information about the dataset where the stretchhyperparameters 0 and 1 are set closer to 0 to enforce more sparsity in the position residuals.Additionally, the Immersive dataset itself consists of a wide variety of indoor/outdoor scenes atvarying scales/scene motion/illumination. We use the same set of hyperparameters for each sceneachieving good reconstruction quality for every scene () showing its generalization capability. Tables 11 and 12 list out the different hyperparameters for quantizing and sparsifying residuals for bothN3DV and Immersive datasets. To test the sensitivity of reconstruction quality to hyperparameters,we train our method on the N3DV dataset with two sets of hyperparameters. The first configurationuses the stated hyperparameters for N3DV from Tables 11 and 12, while the second utilizes thehyperparameters corresponding to Immersive while also matching the learning rate for the positionresiduals (0.0005). We show results on N3DV datasets in . We see that the Immersivedatasets hyperparameter configuration still achieves similar PSNR as the original hyperparametersfor N3DV. While the model size is higher (1.49 MB) with the Immersive configuration compared tothe original configuration (0.60 MB), it is still much lower than the prior state-of-the-art 3DGStream(7.8 MB) while maintaining higher reconstruction quality in terms of PSNR.",
  "B.2Evaluation": "Datasets. (1) Neural 3D Video (N3DV) Datasets consist of six indoor scenes with forward-facing multiview videos with up to 20 cameras at 2704 2028 resolution. Similar to prior work, wedownsample videos by a factor of 2 for training and testing, holding out the central view for testing.Each video consists of 300 frames at 30 FPS. (2) Immersive Video Datasets consist of light fieldvideos of indoor and outdoor scenes captured using a 46-camera rig with fisheye lenses. Following prior work, we downsample videos by a factor of 2 to obtain a resolution of 1280 960. We evaluateon 7 scenes (Welder, Flames, Truck, Exhibit, Face Paint 1, Face Paint 2 and Cave) with the centralview held out for testing. We extract the first 300 frames for all scenes except for Truck, whichconsists of 150 frames. We undistort the fisheye views into perspective views using the distortionparameters. We train and evaluate on the perspective views with pinhole camera parameters. Baselines. 3DGStream : We use the official codebase4 from 3DGStream . We use thesame default configuration for N3DV as provided by the authors. For Immersive, we reduce thegradient threshold for densification to 0.0075 to allow for more Gaussians while increasing thetraining iterations for Stage 1 and 2 to be 450 and 250 iterations, respectively. TeTriRF : We usethe official codebase5 from TeTriRF for all experiments on N3DV. Measuring FPS. We compute FPS for all frames of the video and report the median value in ourexperiments including the time taken to decode the residuals. Note that the decoding is a one-timeoperation per step and rendering for a preceding frame can be performed while learning the residualsof a subsequent ones to achieve even higher speeds.",
  "CLimitations and Future Work": "For efficiency and on-the-fly training, we encode sequences by learning inter-frame residuals. How-ever, for FVVs of long duration or drastic scene update, per-frame training will face challenges inreconstruction capability. Unlike offline reconstruction, per-frame training does not have access tofuture-frame information. This setup limits the capability to effectively reason about large scenechanges (e.g., topological changes and highly varying appearance) . If an object suddenly ap-pears or disappears, it is more difficult to (de-)allocate and update scene parameters to capture suchchanges. In the context of Gaussian splatting, it is challenging to schedule densification and pruningof the Gaussians. Future work could address this by designing an efficient keyframing technique foridentifying large changes in the scene and allocate longer training times accordingly. Furthermore, most current FVV encoding paradigms rely on the input multi-view videos for re-construction. Exploring a general prior of the dynamic scenes, e.g., generative video models, is apromising direction for reducing the dependence on coherent multi-view input, as the video priorcould regularize the reconstructed FVV to capture reasonable scene dynamics even if some inputviews or frames are missing. Moreover, extending our approach to a single or sparse view scenario isa challenging yet important problem for further democratizing streamable FVV. We will leave thesedirections for future work.",
  "DBroader Impacts": "We consider our work as a neutral technology. This proposed method reconstructs free-viewpointvideos from user-provided video inputs. As we highlighted in the introduction, this technology canimprove many aspect of peoples lives, such as through healthcare (tele-operation) and communica-tions (3D video conferencing). There is indeed a possibility that this work can be misused. Since ourreconstruction completely relies on the video inputs, the most likely cases of misuse are those wherethe input video (provided by the users) have negative impacts.",
  "The answer NA means that the paper has no limitation while the answer No means thatthe paper has limitations, but those are not discussed in the paper": "The authors are encouraged to create a separate \"Limitations\" section in their paper. The paper should point out any strong assumptions and how robust the results are toviolations of these assumptions (e.g., independence assumptions, noiseless settings,model well-specification, asymptotic approximations only holding locally). The authorsshould reflect on how these assumptions might be violated in practice and what theimplications would be. The authors should reflect on the scope of the claims made, e.g., if the approach wasonly tested on a few datasets or with a few runs. In general, empirical results oftendepend on implicit assumptions, which should be articulated. The authors should reflect on the factors that influence the performance of the approach.For example, a facial recognition algorithm may perform poorly when image resolutionis low or images are taken in low lighting. Or a speech-to-text system might not beused reliably to provide closed captions for online lectures because it fails to handletechnical jargon.",
  "If applicable, the authors should discuss possible limitations of their approach toaddress problems of privacy and fairness": "While the authors might fear that complete honesty about limitations might be used byreviewers as grounds for rejection, a worse outcome might be that reviewers discoverlimitations that arent acknowledged in the paper. The authors should use their bestjudgment and recognize that individual actions in favor of transparency play an impor-tant role in developing norms that preserve the integrity of the community. Reviewerswill be specifically instructed to not penalize honesty concerning limitations.",
  ". Experimental Result Reproducibility": "Question: Does the paper fully disclose all the information needed to reproduce the main ex-perimental results of the paper to the extent that it affects the main claims and/or conclusionsof the paper (regardless of whether the code and data are provided or not)?Answer: [Yes]Justification: We provide extensive explanations of each component of our work in detailin Sec. 3. Additional implementation details with corresponding hyperparameters are alsoprovided in Appendix B.Guidelines: The answer NA means that the paper does not include experiments. If the paper includes experiments, a No answer to this question will not be perceivedwell by the reviewers: Making the paper reproducible is important, regardless ofwhether the code and data are provided or not.",
  "If the contribution is a dataset and/or model, the authors should describe the steps takento make their results reproducible or verifiable": "Depending on the contribution, reproducibility can be accomplished in various ways.For example, if the contribution is a novel architecture, describing the architecture fullymight suffice, or if the contribution is a specific model and empirical evaluation, it maybe necessary to either make it possible for others to replicate the model with the samedataset, or provide access to the model. In general. releasing code and data is oftenone good way to accomplish this, but reproducibility can also be provided via detailedinstructions for how to replicate the results, access to a hosted model (e.g., in the caseof a large language model), releasing of a model checkpoint, or other means that areappropriate to the research performed. While NeurIPS does not require releasing code, the conference does require all submis-sions to provide some reasonable avenue for reproducibility, which may depend on thenature of the contribution. For example(a) If the contribution is primarily a new algorithm, the paper should make it clear howto reproduce that algorithm.",
  "(b) If the contribution is primarily a new model architecture, the paper should describethe architecture clearly and fully": "(c) If the contribution is a new model (e.g., a large language model), then there shouldeither be a way to access this model for reproducing the results or a way to reproducethe model (e.g., with an open-source dataset or instructions for how to constructthe dataset). (d) We recognize that reproducibility may be tricky in some cases, in which caseauthors are welcome to describe the particular way they provide for reproducibility.In the case of closed-source models, it may be that access to the model is limited insome way (e.g., to registered users), but it should be possible for other researchersto have some path to reproducing or verifying the results.",
  ". Experimental Setting/Details": "Question: Does the paper specify all the training and test details (e.g., data splits, hyper-parameters, how they were chosen, type of optimizer, etc.) necessary to understand theresults?Answer: [Yes]Justification: We follow benchmark and evaluation protocols that are widely used by existingwork in the area. Additional hyperparameters and experiment details are provided in thesupplementary materials B.Guidelines: The answer NA means that the paper does not include experiments. The experimental setting should be presented in the core of the paper to a level of detailthat is necessary to appreciate the results and make sense of them.",
  ". Experiment Statistical Significance": "Question: Does the paper report error bars suitably and correctly defined or other appropriateinformation about the statistical significance of the experiments?Answer: [No]Justification: We follow benchmark and evaluation protocols that are widely used by existingwork in the area. To our knowledge, most of the existing work in this area do not providestatistical significance.Guidelines: The answer NA means that the paper does not include experiments. The authors should answer \"Yes\" if the results are accompanied by error bars, confi-dence intervals, or statistical significance tests, at least for the experiments that supportthe main claims of the paper. The factors of variability that the error bars are capturing should be clearly stated (forexample, train/test split, initialization, random drawing of some parameter, or overallrun with given experimental conditions).",
  "Guidelines:": "The answer NA means that the paper poses no such risks. Released models that have a high risk for misuse or dual-use should be released withnecessary safeguards to allow for controlled use of the model, for example by requiringthat users adhere to usage guidelines or restrictions to access the model or implementingsafety filters.",
  "The answer NA means that the paper does not involve crowdsourcing nor research withhuman subjects": "Depending on the country in which research is conducted, IRB approval (or equivalent)may be required for any human subjects research. If you obtained IRB approval, youshould clearly state this in the paper. We recognize that the procedures for this may vary significantly between institutionsand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and theguidelines for their institution."
}