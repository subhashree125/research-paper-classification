{
  "Abstract": "To develop a preliminary understanding towards Graph Foundation Models, we study theextent to which pretrained Graph Neural Networks can be applied across datasets, an effortrequiring to be agnostic to dataset-specific features and their encodings. We build upon apurely structural pretraining approach and propose an extension to capture feature infor-mation while still being feature-agnostic. We evaluate pretrained models on downstreamtasks for varying amounts of training samples and choices of pretraining datasets. Our pre-liminary results indicate that embeddings from pretrained models improve generalizationonly with enough downstream data points and in a degree which depends on the quantityand properties of pretraining data. Feature information can lead to improvements, butcurrently requires some similarities between pretraining and downstream feature spaces.Keywords: Graph Neural Networks; Foundation Models; Transfer Learning.",
  ". Introduction": "Machine Learning is being revolutionized by Foundation Models (FMs), reference modelsthat can be effectively adapted to diverse downstream tasks with a relatively small amountof data. Successful FMs abound in the textual and visual domains, but the development ofFMs on graphs is still an open problem (Liu et al., 2023b; Mao et al., 2024) with a danglingfundamental question: How to design an architecture that can be pretrained and, later, suc-cessfully applied across graph datasets? This is challenging, as graphs representing differentobjects may exhibit different patterns in their structure and features; these last ones, acrossdomains, may even represent semantically different spaces with distinct encodings.If anything, such an architecture is required to be feature-agnostic: it may processfeatures, but in a way that is independent of dataset-specific encodings and that encourageslearning of transferable patterns. While researchers have recently started to develop feature-agnostic approaches, they are still plagued by various limitations (see Appendix A).Towards a more foundational understanding of the matter, we find a constructive ref-erence in the pipeline proposed by Canturk et al. (2024). The authors propose GPSE, amethod to learn graph positional and structural encodings (P/SEs) (Dwivedi et al., 2022a;",
  "Frasca Jogl Eliasof Ostrovsky Schonlieb Gartner Maron": "About molpcba.Without even training our models, it is clear that molpcba is a com-pletely different challenge than zinc and peptides. This is because (i) the dataset is bothsignificantly larger and (ii) the learning task is seemingly more difficult. For (i), molpcbacontains roughly 437, 929 graphs and is thus at least 28 times larger than zinc (12, 000graphs) and peptides (15, 535 graphs). Despite the abundance of data, it seems that it ismuch more difficult to train a model that generalizes well on molpcba. This is witnessedby the fact that strong models on molpcba achieve an Average Precision (AP) of 0.3compared to 0.7 on peptides (Canturk et al., 2024) (ii). Our preliminary results fromdownstream evaluation on molpcba are reported in and indeed showcase the inher-ent difficulty of the task. Training ratio 0.014 corresponds to using 5, 000 training graphs,and is then comparable with the 0.5 ratio in zinc and peptides. For this amount of data,however, the generalization performance is relatively much worse, as it only hardly reaches0.06 test AP. The only regime where GPSE pretrained models seem to be significantly ben-eficial is ratio 0.056, four times the size of the training data for which the same pretrainedmodels significantly outperformed the baseline on both zinc and peptides. Structuraliza-tion does seem to struggle even at this regime, but this may be due to a lower quality ofthe pretrained models (see Appendix C). In general, we believe that more extensive experi-mentation is due on molpcba in order to draw more solid conclusions. For example, it maybe required to explore substantially larger training ratios, an analysis which we could notcarry out at this time with the available resources.",
  "In this work, we analyze the aforementioned aspects. We propose Feature-Structuralization1": "to encode feature information in structural form in a way that is compatible with structuralpretraining strategies. Next, we consider three datasets that differ in their structural andfeature patterns. We analyze the behavior and performance of structuralization and theoriginal GPSE model on them, studying the impact of multi-dataset pretraining and offeature information in successfully transferring to downstream low-data regimes.",
  ": Left: graph from zinc; Right: structuralizedform. Colors represent node features": "Structuralization encodes featureinformation into additional struc-ture augmenting the original in-put graph: by only encoding ab-stract, logical relations induced byfeatures, the pretraining approachremains feature-agnostic while stillaccessing meaningful informationon how features interact with theoriginal graph structure. Consider a graph G with d different categorical node features2. For each channel i =1, . . . d, structuralization materializes an additional feature-node ui,j for any category jin i, and connects ui,j to the nodes v G attributed with category j in channel i. In thetransformed graph, all explicit features are discarded; nodes are only assigned a mark whichallows to disambiguate between original and feature-nodes. An example of a single-channelstructuralization is shown in where, on the transformed graph, feature-nodes arein green, original nodes in blue. 1. We may abbreviate this simply as structuralization or struct.2. Our approach can be extended to consider edge features and non-categorical ones. To represent edgefeatures, one could consider an incidence network representation of the graph (Albooyeh et al., 2019),where edges are effectively materialized into nodes. In that case, Feature-Structuralization would apply asdescribed above. As for non-categorical (continuous) features, we can represent them as edge attributes inthe structuralized form of the input graph. For this, we materialize a single feature-node with connectionsto all original nodes and encode the value of the continuous features as edge features thereon.",
  ". Experimental Setting": "We seek to address the following questions: (Q1) Are embeddings from pretrained modelsbeneficial in downstream low-data regimes? (Q2) How does the composition of the pre-training corpus affect downstream generalization performance? (Q3) Is pretraining withfeatures information beneficial when these are incorporated via structuralization?. We nowdescribe our experimental setting and refer to Appendix B for additional details. Datasets.We opt to use three datasets: ZINC-12k (Dwivedi et al., 2020), ogbg-molpcba (Huet al., 2020), and peptides-func (Dwivedi et al., 2022b)(resp. zinc, molpcba, peptides).These are from a similar domain, but with different degrees of similarity in structures andfeatures: peptidess graphs are aminoacid-chains, structurally dissimilar from the graphsin zinc and molpcba (small molecules); nodes in molpcba and peptides have the same 9SMILES-derived categories as features, nodes in zinc only one category (the atom type). Pretraining.All pretrainings use the same base model from GPSE (Canturk et al., 2024),with either the original graph structures or their feature-structuralized variants. Follow-ing Canturk et al. (2024), pretraining involves jointly predicting predefined P/SEs fromthe output node representations. For structuralization, the model predicts P/SEs for theoriginal nodes in both the original and transformed graphs. Pretraining is conducted oneach dataset and on all pairwise combinations, totaling 12 possibilities. We downsampledmolpcba to match the training set sizes of zinc and peptides.",
  ". Results and Discussion": "Pretraining. visualizes how pretrainedGPSEs generalize for one pretraining target (resultsfor other targets can be found in Appendix C). Thisshows that pretraining on multiple datasets gener-ally improves performance.3 Models not pretrainedon peptides perform near to chance on peptides,unlike zinc or molpcba. While the gap between in-and off-domain pretraining narrows with more data,it never fully closes (see Appendix E). Adding datafrom other sources can still be beneficial. Pretrain-ing on structuralized graphs slightly worsens in-domain and hampers out-of-dataset generalization.",
  "(v.iii)": ": Downstream test performance for different training fraction and pretraining data(z: zinc, p: peptides, m: molpcba). Left: Average Precision on peptides,higher is better. Right: Mean Absolute Error on zinc, lower is better. Downstream applications.We now present observations for downstream evaluationson zinc and peptides, which we visually summarize in . We also ran preliminaryexperiments on molpcba, but noticed an inherent task hardness beyond the aforementioneddatasets; our initial results are in Appendix D, where we also report an extended versionof . Next, when referring to a baseline, we intend the same downstream architec-ture deprived of embedding from pretrained models.(i) Pre-trained embeddings can be but are not always beneficial (Q1). Pre-trained embeddings improve generalization over the baseline with sufficient data, but canbe detrimental in data-scarce settings4. Notably, in zinc, pretrained models outperformthe baseline even with just 10% of the training data.(ii) When pretraining becomes beneficial, all pretraining mixes seem to providebetter generalization than the baseline (Q2). However, the composition of the corpusmay have a strong impact, which tend to reflect the similarity between source and targetdatasets. E.g., pretraining on peptides is less beneficial when transferring on zinc for bothGPSE and Struct, due to differences in both structure and features (see mark ii).(iii) Including datasets in pretraining other than the target does not generallydeteriorate performance (Q2). In general, the performance is at least as good as theone attained by the best of the two pretraining datasets.(iv) Pretraining mixes that do not include the target dataset can perform aswell as pretraining on the target (Q2). In some cases, off-dataset pretraining mayeven generalize slightly better than pretraining on the same dataset (see mark iv). Webelieve that this may be mostly due to an increase in pretraining data (see Appendix E).(v) Feature structuralization does not yield a consistent, significant improve-ment across settings (Q3).Structuralization improves performance when in-datasetpretraining, outperforming vanilla GPSE on zinc when pretrained on zinc (mark v.i) andon peptides when pretrained on peptides (mark v.i). It can also help when the pretraining",
  ". Conclusions": "We analyzed the extent to which pretrained GNNs can be transferred across datasets bymeasuring the impact of pretraining datasets on downstream generalization and the inclu-sion of feature information via structuralization. Our work can be extended in differentways by: studying non-molecular domains, e.g., social and collaboration networks; closelyenquiring into the underperformance of structuralization and designing more compatiblepretraining strategies and architectures; exploring other feature-agnostic pretraining ap-proaches, e.g., to natively handle continuous features for domains such as geometric graphs. The authors would like to thank Pascal Welke, David Penz, Sagar Malhotra, Marco Ci-ccone and Ethan Fetaya for helpful discussions. FF is funded by the Andrew and ErnaFinci Viterbi Post-Doctoral Fellowship. FF performed this work while visiting the MachineLearning Research Unit at TU Wien led by Prof. Thomas Gartner. FJ is funded by the Cen-ter for Artificial Intelligence and Machine Learning (CAIML) at TU Wien. ME is fundedby the Blavatnik-Cambridge fellowship, the Cambridge Accelerate Programme for ScientificDiscovery, and the Maths4DL EPSRC Programme. TG is supported by the Vienna Scienceand Technology Fund (WWTF) through project ICT22-059. HM is a Robert J. ShillmanFellow and is supported by the Israel Science Foundation through a personal grant (ISF264/23) and an equipment grant (ISF 532/23).",
  "Vijay Prakash Dwivedi, Chaitanya K. Joshi, Thomas Laurent, Yoshua Bengio, and XavierBresson. Benchmarking graph neural networks. arXiv:2003.00982, 2020. URL": "Vijay Prakash Dwivedi, Anh Tuan Luu, Thomas Laurent, Yoshua Bengio, and Xavier Bres-son. Graph neural networks with learnable structural and positional representations. InInternational Conference on Learning Representations, 2022a. Vijay Prakash Dwivedi, Ladislav Rampasek, Michael Galkin, Ali Parviz, Guy Wolf,Anh Tuan Luu, and Dominique Beaini. Long range graph benchmark. In S. Koyejo,S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in NeuralInformation Processing Systems, volume 35, pages 2232622340. Curran Associates, Inc.,2022b.",
  "Mikhail Galkin, Xinyu Yuan, Hesham Mostafa, Jian Tang, and Zhaocheng Zhu. TowardsFoundation Models for Knowledge Graph Reasoning.In International Conference onLearning Representations, 2024": "Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, MicheleCatasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning ongraphs. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Ad-vances in Neural Information Processing Systems, volume 33, pages 2211822133. CurranAssociates, Inc., 2020.",
  "Diederik P. Kingma and Jimmy Ba.Adam: A method for stochastic optimization.InInternational Conference on Learning Representations, 2015": "Hao Liu, Jiarui Feng, Lecheng Kong, Ningyue Liang, Dacheng Tao, Yixin Chen, and MuhanZhang. One for All: Towards Training One Graph Model for All Classification Tasks,December 2023a. URL arXiv:2310.00149 [cs]. Jiawei Liu, Cheng Yang, Zhiyuan Lu, Junze Chen, Yibo Li, Mengmei Zhang, Ting Bai, YuanFang, Lichao Sun, Philip S. Yu, and Chuan Shi. Towards Graph Foundation Models:A Survey and Beyond, December 2023b.URL [cs].",
  "An Analysis on Cross-Dataset Transfer of Pretrained GNNs": "improves to 0.15 in the case of (1) and 0.35 in the case of (2). This suggests that, as alreadymentioned in , more data can help in off-domain generalization, although asreasonably expected much less efficiently than in-domain additional data: 10k more(unlabeled) samples from the peptides training set enable median R2s of 0.91 and 0.64on, resp. Electrostatic Potential PEs and Laplacian Eigenvector PEs over the peptidestest set. Last we report that the in-dataset performance for these targets also improvewith more data. As for the Laplacian Eigenvector PEs, we report that the performance ofGPSE on the molpcbas test set improves from the median R2 of 0.77 to 0.85 of (1) and0.96 of (2). Downstream evaluation on peptides.We evaluated models (1) and (2) on the peptidesdownstream task, following the same setting and procedure described in Appendix B. As afirst important observation, we note that more pretraining data do not allow models (1), (2)to outperform the baseline in the data regimes 0.1, 0.25. We do notice, however, that morepretraining data closes the gap with models pretrained on data mixtures: (1), (2) performcomparably with, e.g., the model pretrained on zinc+molpcba. Last, we observe that, inthe 0.5 ratio setting, model (2) attains a test AP of 0.513 0.002, slightly outperformingthe best pretraining mix molpcba+peptides (0.508 0.010).",
  "Xingtong Yu, Chang Zhou, Yuan Fang, and Xinming Zhang. Text-Free Multi-domain GraphPre-training: Toward Graph Foundation Models, May 2024. URL arXiv:2405.13934 [cs]": "Haihong Zhao, Aochuan Chen, Xiangguo Sun, Hong Cheng, and Jia Li.All in oneand one for all: A simple yet effective method towards cross-domain graph pretrain-ing.In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discoveryand Data Mining, KDD 24, page 44434454, New York, NY, USA, 2024a. Associationfor Computing Machinery. ISBN 9798400704901. doi: 10.1145/3637528.3671913. URL",
  "A. Dealing with different features across datasets": "Pioneering approaches for feature-agnostic architectures have either resorted to (i) LargeLanguage Models (LLMs) to embed features in a shared semantic space (Liu et al., 2023a;Huang et al., 2023), (ii) adopted dimensionality reduction techniques (Zhao et al., 2024a;Yu et al., 2024), or (iii) assumed the architecture is parameterized by a bank of linearGNNs whose optimal predictions can be precalculated in closed form (Zhao et al., 2024b).As for (i), end users have little control over the semantic relations between the LLM-encoded features, which can also significantly depend on the prompting pattern and tech-nique. This is particularly relevant when working with abstract categorical features orcontinuous ones. Techniques in class (ii) leverage covariance information in the featuresto derive a certain number of principal components. This can be interpreted as a form ofcanonization, which, however, neglects structural information in the graph and does notnecessarily guarantee, in itself, a form of alignment of the semantic spaces across datasets.Importantly, we note that covariance information is trivial for purely categorical featurespaces, e.g., following a one-hot encoding scheme. Approach (iii) is particularly interest-ing as the learnable components of the approach effectively work in the prediction space,sidestepping the problem of aligning semantic spaces across datasets. However, it poses anarchitectural constraint, viz., the use of linear GNNs, that may be too limiting in applica-tions such as the ones considered in this study.Beyond our scope, in the specific context of Knowledge Graphs, Galkin et al. (2024) pro-pose to construct a graph between relation types based on concept of sharing head or tailentities. Message-passing on this graph allows the representation of any relation type andthus enables applications across Knowledge Graphs. Structuralization may share similari-ties with this approach in the high-level intuition; however, graphs generated by structural-ization jointly represent original and feature-induced nodes and edges. The model beingpretrained can access both and potentially (learn to) capture patterns in their interaction.",
  "B.1. Pretraining and pretraining target prediction": "In all pretrainings, we employ the same backbone architecture proposed by Canturk et al.(2024): a 20-layer GNN with 512-dimensional Residual Gated GCN message-passing lay-ers Bresson and Laurent (2017) and prediction heads constituted by 2-Layer Perceptronswith 32 hidden-dimensions, totaling slightly more than 2M parameters. The layers of thebackbone architecture are regularized with dropout, whose rate is set to 0.2. As in (Canturket al., 2024), input graphs are augmented with virtual nodes, and 20-dimensional randomnode features drawn from a standard Normal distribution.For optimization, we follow the setting proposed in (Canturk et al., 2024): we employthe AdamW optimizer (Loshchilov and Hutter, 2019) for 120 epochs with weight decayset to 0.00001, a base learning rate of 0.005 and a Cosine-decay scheduler with 5 warmupepochs. We use the same mae + cosine similarity loss (Canturk et al., 2024).The P/SE targets exactly correspond to those chosen in (Canturk et al., 2024), viz., 4Laplacian eigenvectors (taken in their absolute value) along with corresponding eigenvalues,7 Electrostatic PEs, 20 Random Walk SEs, 20 Heat Kernel Diag. SEs. There is only one",
  ": Architecure pipeline for downstream experiments": "exception: we do not consider Cycle Counting Graph Encodings as we observed they wouldhave led to intractable precomputation runtimes in the case of structuralization.We remark that, when pretraining a model on structuralized graphs, we ask it to predictthe above targets on the original nodes calculated for both the original and the structuralizedform of the input graph.This means having double the pretraining targets than whenpretraining a vanilla GPSE model.",
  "B.2. Downstream evaluation": "In all downstream evaluation experiments we follow the pipeline depicted in . Inparticular, component (b) is the pretrained model, from which we extract the generatednode representations upstream the prediction heads used in pretraining ((c)). These rep-resentations are linearly transformed by component (d), and then summed along with theencoding of the explicit, original node features in output from component (e).Theseconstitute the initial node representations updated and later pooled by the downstreamGNN depicted in (f). Component (a) is, effectively, a graph transformation which (i)either discard features or perform structuralization as required; (ii) inject random featuresas prescribed by the GPSE model (Canturk et al., 2024). Note that, during downstreamadaptation, only components (d), (e), (f) are trained.The GNN in (f) is, in all cases, a 3-layer Graph Isomorphism Network (Xu et al., 2019)employing a single linear pre-message-passing layer and a 2-Layer Perceptron post-message-passing and before graph readout. The hidden size of this architecture is fixed to 128. Nodropout or weight-decay regularization is applied.In accordance with Canturk et al. (2024), dropout is applied before and after component(d). We do not perform tuning of dropout rates, but rather apply the same exact valueschosen by the authors in (Canturk et al., 2024) for the most prominent models in therespective full-dataset adaptation experiments. In particular, for zinc the dropout hasa rate of 0.5 before (d) and is not applied after that component; as for peptides thedropout has a rate of 0.1 before (d) and, again, is not applied thereafter; for molpcba",
  "C. Results on pretraining target predictions": "Figures 5, 6 and 7 show the evaluation of all pretraining targets. In particular, we groupP/SEs by category and report the median R2 scores attained on the test sets of the respectiveevaluation datasets. Predicting the P/SE targets of the original graphs with GPSE.We observethat GPSE is able to fit most targets relatively well, although the prediction of LaplacianEigenvector PEs seems to be a harder task (). Either way, we note that achievingsatisfactory generalization on peptides seems to only be possible if peptides was used inthe pretraining. Predicting the P/SE targets of the original graphs after structuralization.ForGPSE on structuralized graphs (), we observe our model is only able to generalizeto other datasets if pretrained on multiple datasets.This indicates that the inclusionof feature information may render transfer more difficult unless the pretraining corpusis larger and/or diverse (and to enquire into the relative impact of size and diversitywould be an interesting future endeavor). As for the present results, we find interestinghow cross-dataset generalization is relatively better across molpcba and peptides whichshare the same feature encoding and is, instead, always problematic when transferringfrom molpcba+peptides to zinc which has a different feature encoding. We recall thattargets are purely structural in this setting and thus hypothesize that the model is harmfullyleveraging feature information. One last noteworthy observation is that the prediction ofEigenvalues is particularly hard for this model, even when pretraining in-dataset. Predicting the P/SE targets of structuralized graphs (after structuralization).Similar conclusions to the above can be drawn for this setting (), except for thefact that the prediction of targets seems generally more difficult and sometimes notpossible even when pretraining in-dataset (see, e.g., the prediction of Random WalkSEs on molpcba or Heat Kernel Diag. SEs on all datasets). These generally lower valuesmay contribute to explain the underperformance of structuralization in some downstreamevaluation settings, and suggests more focused efforts are required on designing betterpretraining strategies and/or constructing better pretraining corpora for our approach.",
  "We present the full results on all downstream datasets in Figures 8, 9 and 10": "About the lowest data regimes.In the aforementioned figures it is possible appreci-ate the behavior of the models in lower data regimes beyond what reported in .We observe that node representations from pretrained models are particularly harmful onpeptides in the 0.1 training ratio setting, while, intriguingly, they provide more benefits inthe lowest data regime (0.01 ratio, around 100 graphs). We hypothesize that, with such datascarcity, on peptides it may even be hard to learn a reasonable message-passing scheme,while pretrained embeddings already provide some ready-to-use representations that canmore immediately correlate with the targets being predicted. As the amount of trainingdata is increased up to the 0.25 ratio, the message-passing parameters can more reasonablybe fitted (see baseline), but it is likely that the models leverage spurious correlations inthe pretrained node embeddings. Finally, for the 0.5 ratio, enough data are available tooptimally make use of both the two sources of information. As for zinc, the only settingwhere embeddings from pretrained models are not beneficial is the one of most scarcity, andotherwise provide a significant positive impact. This clearer picture is coherent with theobservation that generalizing on zinc becomes relatively easier when the model has accessto cyclic-like structural information (as the one that is provided by the pretrained model)and beyond what can be captured by message-passing (Bouritsas et al., 2022).",
  "E. Using more pretraining data": "What is the impact of using more pretraining data? Let us first reason on the fact thatthis is not always possible. In some downstream applications one may only have access toa limited amount of samples, of which only a smaller subset is labeled. In our experimentswe have observed that, at least on zinc and peptides, it may be beneficial to augment theavailable in-domain data with additional data sources, and that this sometimes may evenimprove generalization (see ).On the other hand, it is important to notice that, throughout the experiments discussedso far, we have artificially sampled a small fraction of molpcbas training data to matchthe size of the training sets in the other datasets. This setting is different that the oneconsidered in (Canturk et al., 2024), where models were pretrained on the full set of non-isomorphic structures in molpcba. A complete analysis on how our full set of results wouldchange when considering the complete molpcba dataset is left for future work; nevertheless,we ran some preliminary experiments we will briefly discuss below. Pretraining target prediction.We pretrained two additional GPSE models: one withtwice the number of molpcba samples than in the rest of our experiments (around 20kgraphs) (1); one on the full molpcba training set (2). Then, we moved to evaluate theperformance of both (1) and (2) on the prediction of pretraining targets across the threedatasets. We mention noteworthy results for Electrostatic Potential PEs and LaplacianEigenvector PEs (the performance on the other targets is mostly saturated already with10k pretraining data points). As for the Electrostatic Potential PEs, we report that theperformance of GPSE on peptides improves from the median R2 of 0.32 to 0.41 of (1) and0.70 of (2). Concerning the Laplacian Eigenvector PEs, on peptides, the median R2 of 0.09"
}