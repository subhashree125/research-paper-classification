{
  "Abstract": "Since pioneering work of Hinton et al., knowledge distillation based on Kullback-Leibler Divergence (KL-Div) has been predominant, and recently its variants haveachieved compelling performance. However, KL-Div only compares probabilitiesof the corresponding category between the teacher and student while lacking amechanism for cross-category comparison. Besides, KL-Div is problematic whenapplied to intermediate layers, as it cannot handle non-overlapping distributionsand is unaware of geometry of the underlying manifold. To address these down-sides, we propose a methodology of Wasserstein Distance (WD) based knowledgedistillation. Specifically, we propose a logit distillation method called WKD-Lbased on discrete WD, which performs cross-category comparison of probabilitiesand thus can explicitly leverage rich interrelations among categories. Moreover,we introduce a feature distillation method called WKD-F, which uses a parametricmethod for modeling feature distributions and adopts continuous WD for transfer-ring knowledge from intermediate layers. Comprehensive evaluations on imageclassification and object detection have shown (1) for logit distillation WKD-Loutperforms very strong KL-Div variants; (2) for feature distillation WKD-F issuperior to the KL-Div counterparts and state-of-the-art competitors. The sourcecode is available at",
  "Introduction": "Knowledge distillation (KD) aims to transfer knowledge from a high-performance teacher modelwith large capacity to a lightweight student model. In the past years, it has attracted ever increasinginterest and made great advance in deep learning, enjoying widespread applications in visual recog-nition and object detection, among others . In their pioneering work, Hinton et al. introduceKullback-Leibler divergence (KL-Div) for knowledge distillation, where the prediction of categoryprobabilities of the student is constrained to be similar to that of the teacher. Since then, KL-Div hasbeen predominant in logit distillation and recently its variants [3; 4; 5] have achieved compellingperformance. In addition, such logit distillation methods are complementary to many state-of-the-artmethods that transfer knowledge from intermediate layers [6; 7; 8]. Despite the great success, KL-Div has two downsides that hinder fully transferring of the teachersknowledge. First, KL-Div only compares the probabilities of the corresponding category betweenthe teacher and student, lacking a mechanism to perform cross-category comparison. However,real-world categories exhibit varying degrees of visual resemblance, e.g., mammal species like dogand wolf look more similar to each other while visually very distinct from artifact such as car andbicycle. Deep neural networks (DNNs) can distinguish thousands of categories and thus are",
  "DogWolfCar": "(b) For logit distillation, discrete WD performs cross-category com-parison by exploiting pairwise IRs, in contrast to KL-Div that is acategory-to-category measure and lacks a mechanism to use suchIRs (cf. ). For feature distillation, we use Gaussians fordistribution modeling and continuous WD for knowledge transfer. : Our methodology of Wasserstein Distance (WD) based knowledge distillation. To effectivelyexploit rich category interrelations (a), we propose discrete WD based logit distillation (WKD-L)(b) that matches predicted distributions between the teacher and student. Besides, we introducea feature distillation method based on continuous WD (WKD-F) (b), where we let student mimicparametric feature distributions of the teacher. In (a), features of 100 categories are displayed by thecorresponding images as per their 2D embeddings obtained by t-SNE; refer to Section A.1 for detailson this visualization. well-informed of such complex relations among categories, as shown in a. Unfortunately,due to its category-to-category nature, the classical KD and its variants [3; 4; 5] are unable toexplicitly utilize this rich cross-category knowledge. Secondly, KL-Div is problematic for distilling knowledge from intermediate layers. Deep features ofan image are generally high-dimensional and of small size, so being populated very sparsely in thefeature space [10, Chap. 2]. This not only makes non-parametric density estimation (e.g., histogram)that KL-Div requires infeasible due to curse of dimensionality, but also leads to non-overlappingdiscrete distributions that KL-Div fails to deal with . One may turn to parametric, continuousmethods (e.g., Gaussian) for modeling feature distributions. However, KL-Div and its variants havelimited ability for measuring dis-similarity between continuous distributions, as it is not a metric and is unaware of geometric structure of the underlying manifold . The Wasserstein distance (WD) , also called Earth Movers Distance (EMD) or optimal transport,has the potential to address the limitations of KL-Div. The WD between two probability distributionsis generally defined as the minimal cost to transform one distribution to the other. Several works havemade exploration by using WD for knowledge transfer from intermediate layers [15; 16]. Specifically,they measure the dis-similarity of a mini-batch of images between the teacher and student based ondiscrete WD, which concerns comparison across instances in a soft manner, failing to make use ofrelations across categories. Moreover, they mainly quest for non-parametric method for modelingdistributions, behind in performance state-of-the-art KL-Div based methods. To address these problems, we propose a methodology of Wasserstein distance based knowledgedistillation, which we call WKD. This methodology is applicable to logits (WKD-L) as well as tointermediate layers (WKD-F) as shown in b. In WKD-L, we minimize the discrepancybetween the predicted probabilities of the teacher and student using discrete WD for knowledgetransfer. In this way, we perform cross-category comparison that effectively leverages interrelations(IRs) among categories, in stark contrast to category-to-category comparison in the classical KL-Div.We propose to use Centered Kernel Alignment (CKA) [17; 18] to quantify category IRs, whichmeasures the similarity of features between any pair of categories. For WKD-F, we introduce WD into intermediate layers to condense knowledge from features. Unlikethe logits, there is no class probability involved in the intermediate layers. Therefore, we let thestudent directly match the feature distributions of the teacher. As the dimensions of DNN features are high, the non-parametric methods (e.g., histogram) are infeasible due to curse of dimensionality [10,Chap. 2], we choose parametric methods for modeling distributions. Specifically, we utilize one ofthe most widely used continuous distributions (i.e., Gaussian), which is of maximal entropy given 1st-and 2nd-moments estimated from features [19, Chap. 1]. WD between Gaussians can be computedin closed form and is a Riemannian metric on the underlying manifold . We summarize our contributions in the following. We present a discrete WD based logit distillation method (WKD-L). It can leverage rich interrela-tions among classes via cross-category comparisons between predicted probabilities of the teacherand student, overcoming the downside of category-to-category KL divergence. We introduce continuous WD into intermediate layers for feature distillation (WKD-F). It can ef-fectively leverage geometric structure of the Riemannian space of Gaussians, better than geometry-unaware KL-divergence. On both image classification and object detection tasks, WKD-L perform better than very strongKL-Div based logit distillation methods, while WKD-F is supervisor to the KL-Div counterpartsand competitors of feature distillation. Their combination further improves the performance.",
  "WD for Knowledge Transfer": "Given a pre-trained, high-performance teacher model T, our task is to train a lightweight studentmodel S that can distill knowledge from the teacher. As such, supervisions of the student are fromboth the ground truth label with the cross entropy loss and from the teacher with distillation losses tobe described in the next two sections.",
  "Discrete WD for Logit Distillation": "Interrelations (IRs) among categories.As shown in Figures 1a and 4, real-world categoriesexhibit complex topological relations in the feature space. For instance, mammal species are nearereach other while being far away from artifact or food. Moreover, features of the same categorycluster and form a distribution while neighboring categories have overlapping features and cannotbe fully separated. As such, we propose to quantify category IRs based on CKA , which is anormalized Hilbert-Schmidt Independence Criterion (HSIC) that models statistical relations of twosets of features by mapping them into a Reproducing Kernel Hilbert Space (RKHS) . Given a set of b training examples of category Ci, we compute a matrix Xi Rub where the kthcolumn indicates the feature of example k that is output from the DNNs penultimate layer. Thenwe compute a kernel matrix Ki Rbb with some positive definite kernel, e.g., a linear kernel forwhich Ki = XTi Xi where T indicates matrix transpose. Besides the linear kernel, we can chooseother kernels such as polynomial kernel and RBF kernel (cf. Section A.1 for details). The IR betweenCi and Cj is defined as:",
  "Here H = I 1": "b11T is the centering matrix where I indicates the identity matrix and 1 indicatesan all-one vector; tr indicates matrix trace. IR(Ci, Cj) is invariant to isotropic scaling andorthogonal transformation. Note that the cost to compute the IRs can be neglected since we onlyneed to compute them once beforehand. As the teacher is more knowledgeable, we compute categoryinterrelations using the teacher model, which is indicated by IRT(Ci, Cj). Besides CKA, cosine similarity between the prototypes of two categories can also be used to quantifyIRs. In practice, the prototype of one category can be computed as the average of the features of thecategorys examples. Alternatively, the weight vectors associated with the softmax classifier of aDNN model can be regarded as prototypes of individual categories . Loss function.Given an input image (instance), we let z=ziRn be the corresponding logitsof a DNN model where i Sn= {1, , n} indicates the index of ith category. The predictedcategory probability p =piis computed via the softmax function with a temperature , i.e.,",
  "i pTi logpTi /pSi.(2)": "KL-Div (2) only compares predicted probabilities correspond-ing to the same category between the teacher and student, es-sentially short of a mechanism to perform cross-category com-parison, as shown in . Though during gradient back-propagation probability of one category affects probabilitiesof other categories due to the softmax function, this impliciteffect is insignificant and, above all, cannot explicitly exploitrich knowledge of pairwise interrelations as described in (1). In contrast to KL-Div, WD performs cross-category compar-ison and thus naturally makes use of category interrelations,as shown in b (left). We formulate discrete WD as anentropy regularized linear programming :",
  "i qij =pSj , i, j Sn,": "where cij and qij respectively indicate transport cost per mass and the transport amount while movingprobability mass from pTi to pSj ; is a regularization parameter. We define the cost cij by convertingthe similarity measure IRs to a distance measure according to the commonly used Gaussian kernel [10,Chap. 6], i.e., cij = 1 exp((1 IRT(Ci, Cj))), where is a parameter that can control thedegree of sharpening of IR. The smaller IRT(Ci, Cj) in the feature space, the less cost is needed fortransport between the two categories. As such, the loss function of WKD-L is",
  "LWKDL =DWD(pT, pS).(4)": "Recent work discloses target probability (i.e., the probability of the target category) and the non-target ones play different roles: the former concerns the difficulty of training examples while the lattercontaining prominent dark knowledge. It has been shown that this separation is helpful to balancetheir roles and improves greatly over the classical KD [3; 4]. Inspired by them, we also consider asimilar separation strategy. Let t be index of target category and zT\\t =zTiRn1, iSn\\{t} bethe teachers logits of non-target categories. We normalize zT\\t as previously, obtaining the teachersnon-target probabilities pT\\t =pTi. In this case, our loss functions consist of two terms:",
  "Continuous WD for Feature Distillation": "As the features output from intermediate layers of DNN are of high dimension and small size, thenon-parametric methods, e.g., histogram and kernel density estimation, are infeasible. Therefore, weuse one of the widely used parametric methods (i.e., Gaussian) for distribution modeling. Feature distribution modeling. Given an input image, let us consider feature maps output by someintermediate layer of a DNN model, whose spatial height, width and channel number are h, w andl, respectively. We reshape the feature maps to a matrix F Rlm where m = h w and theith column fi Rl indicates a spatial feature. For these features, we estimate the 1st-moment = 1",
  "where | | indicates matrix determinant": "We estimate Gaussian distribution of the teacher directly from its backbone network. For the student,as in previous arts [24; 25; 26], a projector is used to transform the features so that they are compatiblein size with the features of the teacher. Then the transformed features produced by the projector areused to compute the students distribution. We select Gaussian for distribution modeling as it is ofmaximal entropy for given the 1st- and 2nd-moments [19, Chap. 1] and has a closed form WD that isa Riemannian metric .",
  "DWD(N T, N S) = Dmean(T, S)+ Dcov(T, S).(8)": "Here Dmean(T, S) =T S2 and Dcov(T, S) = tr(T+ S 2((T)12 S(T)12 )12 )where superscript12 indicates matrix square root. As the covariance matrices estimated from high-dimensional features are often ill-conditioned , we add a small positive number (1e5) to thediagonals. We also consider diagonal covariance matrices, for which we have Dcov(T, S) =TS2, where T is a vector of standard variances formed by the square roots of the diagonals ofT. We later compare Gaussian (Full) and Gaussian (Diag) which have full and diagonal covariancematrices, respectively. To balance role of the mean and covariance, we introduce a mean-cov ratio and define the loss as",
  "LWKDF =Dmean(T, S)+Dcov(T, S).(9)": "We can use the strategy of spatial pyramid pooling [28; 29; 6] to enhance representation ability.Specifically, we partition the feature maps into a k k spatial grid, compute a Gaussian for each cellof the grid and then match per cell the Gaussians of the teacher and student. KL-Div and symmetric KL-Div (i.e., Jeffreys divergence) , both having closed form expres-sions for Gaussians , can be used for knowledge transfer. However, they are not metrics ,unaware of the geometry of the space of Gaussians that is a Riemannian space. Conversely, DWDis a Riemannian metric that measures the intrinsic distance . Note that G2DeNet proposesa metric between Gaussians that leverages the geometry based on Lie group, which can be used todefine distillation loss. Besides Gaussian, one can also use Laplace and exponential distributions formodeling feature distributions. Finally, though histogram or kernel density estimation are infeasible,one can still model feature distribution with probability mass function (PMF) and accordingly usediscrete WD to define the distillation loss. Details on these methods can be found in Section A.2.",
  "We summarize KD methods related to ours and show their connections and differences in": "KL-Div based knowledge distillation.Zhao et al. disclose the classical KD loss is a coupledformulation that limits its performance, and thereby propose a decoupled formulation (DKD) thatconsists of a binary logit loss for the target category and a multi-class logit loss for all non-targetcategories. Yang et al. propose a normalized KD (NKD) method, which decomposes the classicalKD loss into a combination of the target loss (like the widely used cross-entropy loss) and the lossof normalized non-target predictions. WTTM introduces Rnyi entropy regularizer withouttemperature scaling for student. In spite of competitive performance, they cannot explicitly exploitrelations among categories. By contrast, our Wasserstein distance (WD) based method can performcross-category comparison and thus exploit rich category interrelations. WD based knowledge distillation. The existing KD methods founded on WD [15; 16] mainlyconcern cross-instance matching for feature distillation, as shown in (left). Chen et al. propose a Wasserstein Contrastive Representation Distillation (WCoRD) framework which involves",
  ":Diagrams of WCoRD/EMD+IPOT and NST/ICKD-C": "a global and a local contrastive loss. The former loss minimizes mutual information (via dual formof WD) between the distributions of the teacher and student; the latter loss minimizes Wassersteindistance for the penultimate layer only, where the set of features of the mini-batch images are matchedbetween the teacher and student. Lohit et al. independently propose a similar cross-instancematching method called EMD+IPOT, in which knowledge is transferred from all intermediatelayers and discrete WD is computed via an inexact proximal optimal transport algorithm . Thedifferences of our work from them are twofold: (1) They fail to leverage category interrelations whichour WKD-L can make full use of; (2) They are concerned with cross-instance matching based ondiscrete WD, while our WKD-F involves instance-wise matching with continuous WD. Other arts based on statistical modeling. NST is among the first to formalize feature distilla-tion as a distribution matching problem, in which the student mimics the distributions of the teacherbased on Maximum Mean Discrepancy (MMD). They show that the 2nd-order polynomial kernelperforms best among the candidate kernels of MMD, and that the activation-based attention transfer(AT) is a special case of NST. Yang et al. propose a novel loss function, which transfersthe statistics learned by the student back to the teacher based on adaptive instance normalization.Liu et al. propose inter-channel correlations (ICKD-C) to model feature diversity and homologyfor better knowledge transfer. Both NST and ICKD-C can be regarded as Frobenius norm baseddistribution modeling via 2nd-moment along spatial- and channel-dimension, respectively, as shownin (right). However, they fail to utilize the geometric structure of the 2nd-moment matrices,which are symmetric positive definite (SPD) and form a Riemannian space [38; 39]. Ahn et al. introduce variational information distillation (VID) based on mutual information. VID assumes thatfeature distributions are Gaussians, and its loss boils down to mean square loss (i.e., FitNet ) ifGaussians are further assumed to have unit variance.",
  "Experiments": "We evaluate WKD for image classification on ImageNet and CIFAR-100 . Also, we evaluatethe effectiveness of WKD on self-knowledge distillation (Self-KD). Further, we extend WKD toobject detection and conduct experiment on MS-COCO . We train and test models with PyTorchframework , using a PC with an Intel Core i9-13900K CPU and GeForce RTX 4090 GPUs.",
  "Experiment Setting": "Image classification. ImageNet contains 1,000 categories with 1.28M images for training, 50Kimages for validation and 100K for testing. In accordance with , we train the models for 100epochs using SGD optimizer with a batch size of 256, a momentum of 0.9 and a weight decay of1e4. The initial learning rate is 0.1, divided by 10 at the 30th, 60th and 90th epochs, respectively.We use random resized crop and random horizontal clip for data augmentation. For WKD-L, we usePOT library for solving discrete WD with =0.05 and 9 iterations. For WKD-F, the projectorhas a bottleneck structure, i.e., a 11 Convolution (Conv) and a 33 Conv both with 256 filtersfollowed by a 11 Conv with BN and ReLU to match the size of teachers feature maps.",
  ": Ablation analysis of WKD-L for image classification (Acc, %) on ImageNet": "CIFAR-100 contains 60K images of 32 32 pixels from 100 categories with 50K for trainingand 10K for testing. Following the setting of OFA , we conduct experiments across ConvolutionalNeural Networks (CNNs) and vision transformers. Additional experiments within CNN architecturesare given in of Section C.6. The images are upsampled to the resolution of 224x224. Allmodels are trained for 300 epochs with a batch size of 512 and a cosine annealing schedule. ForCNN-based students, we use SGD optimizer with an initial learning rate of 2.5e-2 and a weight decayof 2e-3. For transformer-based students, we use AdamW optimizer with an initial learning rate of2.5e-4 and a weight decay of 2e-3. Object detection. MS-COCO is a commonly used benchmark for object detection that contains80 categories. Following the common practice, we use standard split of COCO 2017 with 118Kimages for training and 5K images for validation. As in ReviewKD , we employ the frameworkof Faster-RCNN with Feature Pyramid Network (FPN) on Detectron2 platform . As inprevious arts [50; 29; 51], we use the detection models officially trained and released as the teachers,while training the student models whose backbones are initialized with the weights pre-trained onImageNet. The student networks are trained in 180K iterations with a batch size of 8; the initiallearning rate is 0.01, decayed by a factor of 0.1 at 120K and 160K iterations.",
  "Ablation of WKD-L": "How WD performs against KL-Div? We compare WD to KL-Div with (w/) and without (w/o)separation of target probability in a. For the case of without separation, WD (w/o) improvesover KL-Div (w/o) by 1.0%; for the case of with separation, WD (w/) outperforms non-triviallyKL-Div (w/) based DKD and NKD. The comparison above clearly shows that (1) WD performs betterthan KL-Div in both cases and (2) the separation also matters for WD. As such, WD with separationof target probability is used across the paper. How to model category interrelations (IRs)? b compares two methods for IR modeling,i.e., CKA and cosine. For the former, we assess different kernels while for the latter we evaluateprototypes with classifier weights or class centroids. We note all WD-based methods perform muchbetter than the baseline of KL-Div. Overall, IR with CKA performs better than IR with cosine,indicating it has better capability to represent similarity among categories. For IR with CKA, RBFkernel is better than polynomial kernel, while linear kernel is the best so is used throughout.",
  "Ablation of WKD-F": "Full covariance matrix or diagonal one?As shown in a (3rd and 4th rows), for Gaussian(Full), WD performs better than G2DeNet , which suggests that the former metric is moreappropriate for feature distillation. When using WD, Gaussian (Diag) (5th row) produces higheraccuracy than Gaussian (Full). We conjecture the reason is that high dimensionality of features makesestimation of full covariance matrices not robust ; in contrast, for Gaussians (Diag), we only needto estimate 1D variances for univariate data of single dimension. Besides, Gaussian (Diag) is muchmore efficient than Gaussian (Full). So we use Gaussians (Diag) throughout the paper.",
  ": Ablation analysis of WKD-F for image classification (Acc, %) on ImageNet": "How to model distributions? In a, we compare different parametric methods for knowledgedistillation, including Gaussian, Laplace, exponential distribution, as well as separate 1stmomentand 2ndmoment. Note that NST adopts spatial 1st- and 2nd-moment while ICKD-C useschannel 2ndmoment. Besides, we compare to non-parametric method based on PMF. For Gaussians (Diag), KL-Div and symmetric (Sym) KL-Div produce similar accuracies which areboth lower than WD. The reason may be that KL-related divergences are not intrinsic distances,failing to exploit geometric structure of the manifold of Gaussians. For statistical moments, we seethat channel-wise moments perform better than spatial-wise ones. For channel-wise representations,1st-moment outperforms 2nd-moment, suggesting that the mean plays a more important role. At last,the non-parametric method of PMF underperforms the parametric method of Gaussians. Besides Gaussian (Diag), We can also use univariate Laplace or exponential functions to modeldistributions of each component of features. For them KL-Div can be computed in closed-form but WD is an unsolved problem. When using KL-Div, Gaussian (Diag) achieves better performancethan both Laplace and exponential distributions, highlighting Gaussian as a more suitable optionamong these parametric alternatives. Further, the Gaussian (Diag) combined with WD yields superiorperformance compared to KL-Div, suggesting advantage of the Riemannian metric. Instance-wise or cross-instance matching? Our WKD-F is an instance-wise matching methodbased on continuous WD, while WCoRD and EMD+IPOT concern cross-instance matching for amini-batch of images based on discrete WD. As seen in b, WCoRD produces an accuracymuch higher than EMD+IPOT, which may be attributed to its extra global contrast loss based onmutual information; WKD-F outperforms the two counterparts by a large margin of 1.0%, whichsuggests the advantage of our strategy. Note that our WKD-F runs remarkably faster than the twocounterparts that rely on optimization algorithm to solve discrete WD. Distillation position and grid scheme. We evaluate in c the effect of position at which weperform distribution matching and that of different grid schemes. From the 3rd and 4th rows, we seethat the last stage of Conv_5x performs much better than Conv_4x, indicating high-level features aremore suitable for knowledge transfer. Comparing the 4th and 5th rows, we see 22 grid does notimprove over 11 grid. Lastly, combination of features of Conv_4x and Conv_5x bring no furthergains. Therefore, we use features of Conv_5x and 11 grid for classification on ImageNet.",
  "Image Classification on ImageNet": "compares to existing works in two settings. Setting (a) involves homogeneous architecture,where the teacher and student networks are ResNet34 and ResNet18 , respectively; setting (b)concerns heterogeneous architecture, in which we set the teacher as ResNet50 and the student asMobileNetV1 . Refer to Section C.2 for hyper-parameters in Settings (a) and (b). For logit distillation, we compare our WKD-L with KD , DKD , NKD , CTKD andWTTM . Our WKD-L performs better than the classical KD and all its variants in both settings.Particularly, our WKD-L outperforms WTTM, a very strong variant of KD, which additionallyintroduces a sample-adaptive weighting method. This suggests Wasserstein distance that performs",
  "Top-5 92.86 88.76 89.80 91.0591.3290.1490.4191.0091.1391.3991.1791.2491.0591.63": ": Image classification results (Acc, %) on ImageNet. In setting (a), the teacher (T) and student(S) are ResNet34 and ResNet18, respectively, while setting (b) consists of a teacher of ResNet50and a student of MobileNetV1. We refer to in Section C.4 for additional comparison tocompetitors with different setups. cross-category comparison is superior to category-to-category KL-Div. For feature distillation, wecompare to FitNet , CRD , ReviewKD and CAT . Our WKD-F improves ReviewKD,previous top-performer, by 0.9% in the setting (a) and 0.6% in the setting (b) in terms of top-1accuracy; this comparison indicates that, for knowledge transfer, matching of Gaussian distributionsis better than matching of features. Finally, combination of WKD-L and WKD-F further improvesand outperforms strong competitors, including CRD+KD , DPK , FCFD and KD-Zero .More results of combination about WKD-L or WKD-F can be found in .",
  ": Training latency on ImageNet": "compares in Setting (a) latency of different meth-ods with a batch size of 256 using a GeForce RTX 4090.For logit distillation, the latency of WKD-L is 1.3 timeslarger than KL-Div based methods, due to the optimizationprocedure to solve discrete WD. WKD-F has a latency onpar with KL-Div based methods, while running 1.6 fasterthan ReviewKD and 1.2 faster than EMD+IPOT; this isbecause WKD-F only involves mean vectors and variancevectors, leading to negligibly additional cost. Finally, com-bination of WKD-L and WKD-F has larger latency butbetter performance than ICKD-C, and meanwhile is moreefficient than state-of-the-art FCFD.",
  "Image Classification on CIFAR-100": "We evaluate WKD in the settings where the teacher is a CNN and the student is a Transformeror vice versa. We use CNN models including ResNet (RN) , MobileNetV2 (MNV2) andConvNeXt , as well as vision transformers that involve ViT , DeiT , and Swin Trans-former . The setting of hyper-parameters can be found in Section C.5. For logit distillation, we compare WKD-L to KD , DKD , DIST and OFA . As shownin , WKD-L consistently outperforms state-of-the-art OFA for transferring knowledge fromTransformers to CNNs or vice versa. For feature distillation, we compare to FitNet , CC ,RKD and CRD . WKD-F ranks first across the board; notably, it significantly outperformsthe previous best competitors by 2.1%3.4% in four out of five settings. We attribute superiorityof WKD-F to our distribution modeling and matching strategies, i.e., Gaussians and Wassersteindistance. We posit that, for knowledge transfer across CNNs and Transformers that yield very distinctfeatures , WKD-F is more suitable than raw feature comparisons as in FitNet and CRD.",
  "Self-Knowledge Distillation on ImageNet": "We implement our WKD in the framework of Born-Again Network (BAN) for self-knowledgedistillation (Self-KD). Specifically, we first train an initial model S0 using ground truth labels. Thenwe distill, using WKD-L, the knowledge of S0 into a student model S1 with the same architectureas S0. For the sake of simplicity, we do not perform multi-generation distillation, such as training astudent model S2 with S1 as the teacher, etc.",
  ": Self-KD results (Acc, %) on Ima-geNet with ResNet18": "We conduct experiments with ResNet18 on Ima-geNet, where the hyper-parameters are consistentwith those in Setting (a). As shown in , BANachieves competitive accuracy that is comparable tostate-of-the-art results. Our method achieves the bestresult, outperforming BAN by 0.9% in Top-1 accu-racy and the second-best (i.e., USKD) by 0.6%. Thiscomparison demonstrates that our WKD can wellgeneralize to self-knowledge distillation.",
  "Object Detection on MS-COCO": "We extend WKD to object detection in the framework of Faster-RCNN . For WKD-L, we use theclassification branch in the detection head for logit distillation. For WKD-F, we transfer knowledgefrom features straightly fed to the classification branch, i.e., features output by the RoIAlign layer,and choose a 44 spatial grid for computing Gaussians. Implementation details, ablation of keycomponents, and extra experiments are given in Section E of Appendix.",
  ": Object detection results on MS-COCO.Additional bounding-box regression is used": "We compare with existing methods in two set-tings, as shown in . In RN101RN18setting, the teacher is ResNet101 and the studentis ResNet18; in RN50MNV2, the teacher andstudent are ResNet50 and MobileNetV2 ,respectively. For logit distillation, our WKD-Lsignificantly outperforms the classical KD and is slightly better than DKD . For featuredistillation, we compare with FitNet, FGFI ,ICD and ReviewKD ; our WKD-F im-proves ReviewKD, the previous top feature dis-tillation performer, by a non-trivial margin inboth settings. Finally, by combining WKD-Land WKD-F, we achieve performance betterthan DKD+ReviewKD . When additionalbounding-box regression is used for knowl-edge transfer, our WKD-L+WKD-F improvesfurther, outperforming previous state-of-the-artFCFD .",
  "Conclusion": "The Wasserstein distance (WD) has shown evident advantages over KL-Div in several fields such asgenerative models . However, in knowledge distillation, KL-Div is still dominant and it is unclearwhether WD will outperform. We argue that earlier attempts on knowledge distillation based on WDfail to unleash the potential of this metric. Hence, we propose a novel methodology of WD-basedknowledge distillation, which can transfer knowledge from both logits and features. Extensiveexperiments have demonstrated that discrete WD is a very promising alternative of predominant KL-Div in logit distillation, and that continuous WD can achieve compelling performance for transferringknowledge from intermediate layers. Nevertheless, our methods have limitations. Specifically,WKD-L is more expensive than KL-Div based logit distillation methods, while WKD-F assumesfeatures follow Gaussian distribution. We refer to Section F in Appendix for detailed discussion onlimitations and future research. Finally, we hope our work can shed light on the promise of WD andinspire further interest in this metric in knowledge distillation.",
  "Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network.arXiv preprint arXiv:1503.02531, 2015": "Borui Zhao, Quan Cui, Renjie Song, Yiyu Qiu, and Jiajun Liang. Decoupled knowledgedistillation. In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1195311962, 2022. Zhendong Yang, Ailing Zeng, Chun Yuan, and Yu Li. From knowledge distillation to self-knowledge distillation: A unified approach with normalized loss and customized soft labels.In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1718517194, 2023.",
  "Kaixiang Zheng and EN-HUI YANG. Knowledge distillation based on transformed teachermatching. In International Conference on Learning Representations, 2024": "Li Liu, Qingle Huang, Sihao Lin, Hongwei Xie, Bing Wang, Xiaojun Chang, and XiaodanLiang. Exploring inter-channel correlation for diversity-preserved knowledge distillation. InProceedings of the IEEE/CVF International Conference on Computer Vision, pages 82718280,2021. Martin Zong, Zengyu Qiu, Xinzhu Ma, Kunlin Yang, Chunya Liu, Jun Hou, Shuai Yi, and WanliOuyang. Better teacher better student: Dynamic prior knowledge for knowledge distillation. InInternational Conference on Learning Representations, 2023.",
  "Luigi Malago, Luigi Montrucchio, and Giovanni Pistone. Wasserstein Riemannian geometry ofGaussian densities. Information Geometry, 1(2):137179, 2018": "Arthur Gretton, Olivier Bousquet, Alex Smola, and Bernhard Schlkopf. Measuring statisticaldependence with hilbert-schmidt norms. In International Conference on Algorithmic LearningTheory, pages 6377, 2005. Vardan Papyan, Xuemei Han, and David L. Donoho. Prevalence of neural collapse during theterminal phase of deep learning training. Proceedings of the National Academy of Sciences,117(40):2465224663, 2020.",
  "Harold Jeffreys. An invariant form for the prior probability in estimation problems. Proceedingsof the Royal Society of London. Series A. Mathematical and Physical Sciences, 186(1007):453461, 1946": "Shaohua Kevin Zhou and Rama Chellappa. From sample similarity to ensemble similarity:probabilistic distance measures in reproducing kernel hilbert space. IEEE Transactions onPattern Analysis and Machine Intelligence, 28(6):917929, 2006. Qilong Wang, Jiangtao Xie, Wangmeng Zuo, Lei Zhang, and Peihua Li. Deep CNNs meetglobal covariance pooling: Better representation and generalization. IEEE Transactions onPattern Analysis and Machine Intelligence, 43(8):25822597, 2021.",
  "Rajendra Bhatia. Positive Definite Matrices. Princeton University Press, 2015": "Mehrtash Harandi, Mathieu Salzmann, and Richard Hartley. Dimensionality reduction on SPDmanifolds: The emergence of geometry-aware methods. IEEE Transactions on Pattern Analysisand Machine Intelligence, 40(1):4862, 2018. Sungsoo Ahn, Shell Xu Hu, Andreas Damianou, Neil D. Lawrence, and Zhenwen Dai. Vari-ational information distillation for knowledge transfer. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition, pages 91639171, 2019. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and PatternRecognition, pages 248255, 2009.",
  "Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.2009": "Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, PiotrDollr, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In EuropeanConference on Computer Vision, pages 740755, 2014. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, AndreasKopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style,high-performance deep learning library. In Advances in Neural Information Processing Systems,2019. Rmi Flamary, Nicolas Courty, Alexandre Gramfort, Mokhtar Z. Alaya, Aurlie Boisbunon,Stanislas Chambon, Laetitia Chapel, Adrien Corenflos, Kilian Fatras, Nemo Fournier, LoGautheron, Nathalie T.H. Gayraud, Hicham Janati, Alain Rakotomamonjy, Ievgen Redko,Antoine Rolet, Antony Schutz, Vivien Seguy, Danica J. Sutherland, Romain Tavenard, AlexanderTong, and Titouan Vayer. POT: Python optimal transport. The Journal of Machine LearningResearch, 22(78):18, 2021. Zhiwei Hao, Jianyuan Guo, Kai Han, Yehui Tang, Han Hu, Yunhe Wang, and Chang Xu.One-for-all: Bridge the gap between heterogeneous architectures in knowledge distillation. InAdvances in Neural Information Processing Systems, volume 36, pages 7957079582, 2023. Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster R-CNN: Towards real-timeobject detection with region proposal networks. IEEE Transactions on Pattern Analysis andMachine Intelligence, 39(6):11371149, 2017. Tsung-Yi Lin, Piotr Dollr, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie.Feature pyramid networks for object detection. In Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition, pages 21172125, 2017.",
  "Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, and Ross Girshick. Detectron2. 2019": "Tao Wang, Li Yuan, Xiaopeng Zhang, and Jiashi Feng. Distilling object detectors with fine-grained feature imitation. In Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 49334942, 2019. Zijian Kang, Peizhen Zhang, Xiangyu Zhang, Jian Sun, and Nanning Zheng.Instance-conditional knowledge distillation for object detection. In Advances in Neural InformationProcessing Systems, pages 1646816480, 2021. Eunho Yang, Aurelie Lozano, and Pradeep Ravikumar. Elementary estimators for sparsecovariance matrices and other structured moments. In International Conference on MachineLearning, pages 397405, 2014.",
  "Manuel Gil. On rnyi divergence measures for continuous alphabet sources. Master thesis,2011": "Zheng Li, Xiang Li, Lingfeng Yang, Borui Zhao, Renjie Song, Lei Luo, Jun Li, and Jian Yang.Curriculum temperature for knowledge distillation. In Proceedings of the AAAI Conference onArtificial Intelligence, volume 37, pages 15041512, 2023. Ziyao Guo, Haonan Yan, Hui Li, and Xiaodong Lin. Class attention transfer based knowledgedistillation. In Croceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1186811877, 2023. Lujun Li, Peijie Dong, Anggeng Li, Zimian Wei, and Ya Yang. Kd-zero: Evolving knowledgedistiller for any teacher-student pairs. Advances in Neural Information Processing Systems,pages 6949069504, 2023. Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, TobiasWeyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neuralnetworks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017. Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen.Mobilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition, pages 45104520, 2018. Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and SainingXie. A convnet for the 2020s. In Proceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 1197611986, 2022. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.An image is worth 16x16 words: Transformers for image recognition at scale. In InternationalConference on Learning Representations, 2020. Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, andHerv Jgou. Training data-efficient image transformers & distillation through attention. InInternational Conference on Machine Learning, pages 1034710357. PMLR, 2021. Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and BainingGuo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedingsof the IEEE/CVF International Conference on Computer Vision, pages 1001210022, 2021.",
  "Tommaso Furlanello, Zachary Lipton, Michael Tschannen, Laurent Itti, and Anima Anandkumar.Born again neural networks. In International conference on machine learning, pages 16071616.PMLR, 2018": "Li Yuan, Francis EH Tay, Guilin Li, Tao Wang, and Jiashi Feng.Revisiting knowledgedistillation via label smoothing regularization. In Proceedings of the IEEE/CVF conference oncomputer vision and pattern recognition, pages 39033911, 2020. Mingi Ji, Seungjae Shin, Seunghyun Hwang, Gibeom Park, and Il-Chul Moon. Refine myselfby teaching myself: Feature refinement via self-knowledge distillation. In Proceedings of theIEEE/CVF conference on computer vision and pattern recognition, pages 1066410673, 2021. Jiajun Liang, Linze Li, Zhaodong Bing, Borui Zhao, Yao Tang, Bo Lin, and Haoqiang Fan.Efficient one pass self-distillation with zipfs label smoothing. In European conference oncomputer vision, pages 104119. Springer, 2022. Jason Altschuler, Jonathan Niles-Weed, and Philippe Rigollet. Near-linear time approximationalgorithms for optimal transport via sinkhorn iteration. In Advances in Neural InformationProcessing Systems, volume 30, 2017.",
  "Ying Jin, Jiaqi Wang, and Dahua Lin. Multi-level logit distillation. In Proceedings of theIEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2427624285,2023": "Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practicalautomated data augmentation with a reduced search space. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition Workshops, pages 702703, 2020. Zhiwei Hao, Jianyuan Guo, Kai Han, Han Hu, Chang Xu, and Yunhe Wang. Revisit the powerof vanilla knowledge distillation: from small scale to large scale. In A. Oh, T. Naumann,A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural InformationProcessing Systems, volume 36, pages 1017010183. Curran Associates, Inc., 2023.",
  "Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scaleimage recognition. In International Conference on Learning Representations, 2015": "Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun. Shufflenet: An extremely efficientconvolutional neural network for mobile devices. In Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition, pages 68486856, 2018. Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, DeviParikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-basedlocalization. In Proceedings of the IEEE International Conference on Computer Vision, pages618626, 2017. Zhaohui Zheng, Ping Wang, Wei Liu, Jinze Li, Rongguang Ye, and Dongwei Ren. Distance-iouloss: Faster and better learning for bounding box regression. In Proceedings of the AAAIConference on Artificial Intelligence, pages 1299313000, 2020. Li Yuan, Francis EH Tay, Guilin Li, Tao Wang, and Jiashi Feng.Revisiting knowledgedistillation via label smoothing regularization. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, pages 39023910, 2020.",
  "A.1Interrelations (IRs) among Category for WKD-L": "Visualization of category interrelations.We select in random fifty images from each of onehundred categories randomly chosen on the training set of ImageNet. Then we feed them to apre-trained ResNet50 model and extract from the penultimate layer features that are projected to2D space using t-SNE. The 2D embedding points are shown in a where different categoriesare indicated by different colors. For intuitive understanding, inspired by karpathy1, we displayfeatures by the corresponding images at their nearest 2D embedding locations in b. It can beseen that the categories exhibit complex topological relations (distances) in the feature space, e.g.,mammal species are nearer each other while far from artifact or food. The relations encode abundantinformation and are beneficial for knowledge distillation. In addition, the features of the same categorycluster and form a (unknown) distribution that often overlaps with those of neighboring categories.The observation suggests that it is more desirable to model the interrelations with statistical method.",
  "(b) Features are displayed by the corresponding im-ages at their nearest 2D embedding locations": ": Visualization of interrelations among 100 categories in feature space. The categories exhibitcomplex topological relations, where features of the same category cluster and form a distributionthat often overlaps with those of neighboring categories. Quantization of IRs with CKA.We propose to use CKA for modeling category IRs as it caneffectively characterize similarity of deep representations . CKA is normalized HSIC thatmeasures statistical dependence of random variables (features) by mapping them into a RKHSwith some positive definite kernels. Recall that, for category Ci, we have a matrix Xi Rub ofu-dimensional features of b training images. As such, for the commonly used kernels, we havelinear kernel Klini= XTi Xi, the polynomial kernel Kpolyi= (XTi Xi +1)k where k {2, 3, 4},and RBF kernel Krbfi= exp(Di",
  "(A+AT )": "Quantization of IRs with cosine similarity.Besides CKA, cosine similarity between the prototypesof two categories is used to quantify category interrelations. The prototype of a category canbe naturally computed as feature centroid of this categorys training examples, i.e., xi = 1 bXi1.Alternatively, the weight vectors associated with the softmax classifier can be used as prototypes .Specifically, if the weight matrix of the last FC layer is WRun where n is number of total classes,then its ith column wi can be regarded as the prototype of category Ci, i.e., xi =wi.",
  "A.2Distributions Modeling for WKD-F": "Recall that, for an input image, we have 3D feature maps output by some layer of a DNN, whosespatial height, width and channel number are h, w and l, respectively. We reshape the feature mapsto a matrix F Rlm where m = h w; we denote the ith column as a feature fi Rl , whilethe jth row (after transpose) as a feature fj Rm. In WKD-F, we estimate channel 1st-moment Rl and 2nd-moment Rll:",
  "i=1(fi)(fi)T ,(10)": "which are used to construct a parametric Gaussian N(, ). For measuring difference betweenGaussians, we use Wasserstein distance (WD) that is a Riemannian metric. We prefer Gaussians(Diag) that have diagonal covariances to Gaussians (Full) that have full covariances, as they are muchmore efficient and have better performance as shown in a.",
  "m,(16)": "where (fi) denotes Kronecker function that is equal to one if f =fi and zero otherwise. Let pTf /pSfbe the PMFs of the teacher/student. We can use discrete WD to measure their discrepancy, i.e.,DWD(pTf , pSf ); the transport cost cij between two features f Ti and f Sj is computed as 1cos(f Ti , f Sj ).Note that KL-Div is inapplicable to PMF as it cannot handle non-overlapping distributions . It is worth noting that (1) the space of Gaussians is a Riemannian space, on which WD is an intrinsicdistance while KL-Div or its symmetric version are not and thus are unaware of thegeometry ; (2) the space of either channel or spatial 2nd-moments is a manifold of symmetric,positive definite matrices rather than a Euclidean space [38; 39], so the Frobenius norm is not anintrinsic distance [35; 6] and fails to exploit the geometric structure of the manifold.",
  "BComputational Complexity of WKD": "The logit-based WKD-L is formulated as an entropy regularized linear programming, which canbe solved fastly by Sinkhorn algorithm . Let n be the dimension of the predicted logits, thecomplexity of WKD-L can be written as O(Dn2 log n) . Here D = C3 is a constant, whereC indicates the infinity norm of the transportation cost matrix C = [cij], and > 0 indicatesa prescribed error. In contrast, the computational complexity of KL-Div is O(n). Despite its highcomplexity, WKD-L can be computed efficiently as Sinkhorn algorithm is highly suitable for parallelcomputation on GPU . For feature-based WKD-F, the dominant cost lies in computation ofmeans and variances. Given a set of m features fi of l-dimension, the means can be computed byglobal average pooling that takes O(ml) time; the complexity of variances is also O(ml), as it canbe obtained by element-wise square operations followed by global average pooling.",
  "We adopt the setting (a) for ablation on ImageNet, in which the teacher is ResNet34 and the studentis ResNet18": "Hyper-parameters of WKD-L.The total loss function of WKD-L consists of the cross-entropyloss LCE, the target loss Lt and WD-based logit loss LWKDL. Following [3; 4], we set the weightsof the two former losses to 1 across the paper. As such, our hyper-parameters includes the temperature, the weight of LWKDL, the sharpening parameter that controls smoothness of IRs for transportcost in WD, and the regularization parameter (set to 0.05) in discrete WD. As simultaneousoptimization of them is computationally infeasible, we first analyze the effect of temperature by",
  ": Analysis of hyper-parameters of WKD-L on ImageNet": "fixing the sharpening parameter and weight to 1 and 30, respectively. As seen in (top left),across all temperatures WKD-L performs better than the baseline of KD by large margins whileachieving the highest accuracy when the temperature is 2. Next, we fix the temperature and weight,analyzing the effect of sharpening parameter. From (top right) we see that performancevariation is not large against the sharpening parameter and the best performance is obtained when it isequal to 1. Subsequently, by fixing the best temperature and sharpening parameters, we evaluate theweight of WKD-L. As (bottom left) shows, its accuracy varies smoothly against the weightand achieves the best accuracy when the weight is 30. Finally, (bottom right) illustrates thatthe performance varies smoothly as a function of the regularization parameter over a reasonablywide range; for consistency, we set to 0.05 in all experiments throughout this paper. Hyper-parameters of WKD-F. The loss function of WKD-F method contains the cross-entropyloss LCE and WD-based feature loss LWKDF. As in previous work, we set the weight of thecross-entropy loss to 1. Therefore, there are two hyper-parameters, i.e., the mean-cov ratio andthe weight of LWKDF. We first analyze the effect of the former by fixing the latter to 2e-2. (left) shows the performance as a function of the mean-cov ratio. We see that in the whole rangeof the mean-cov ratio WKD-F is clearly better than the baseline of FitNet ; the best accuracyis obtained when its value is 2, which suggests that the means play a more important role than thecovariances. Next, we fix the mean-cov ratio and evaluate the effect of the weight. As seen in (right), WKD-F is much better than the baseline of FitNet across all weights and achieves the bestperformance when the weight is 0.02. Mean-cov ratio 70.0 70.5 71.0 71.5 72.0 72.5 73.0 Top-1 Acc (%) WKD-F (Ours)FitNet 0.000.020.040.060.080.10",
  "C.2Summary of Hyper-parameters on ImageNet": "The setting (a) involves homogeneous architecture, where the teacher and student networks areResNet34 and ResNet18, respectively. For WKD-L, the weights of LCE, Lt and LWKDL are 1, 1and 30, respectively; the temperature =2 and sharping parameter =1. For WKD-F, we set theweights of LCE and LWKDF to 1 and 2e2, respectively. We adopt features from Conv5_x andGaussian (Diag) with mean-cov ratio =2. The setting (b) concerns heterogeneous architecture, inwhich the teacher is ResNet50 and the student is MobileNetV1. In this setting, the weight of LWKDLis set to 25 while that of LWKDF is 1e3, and the remaining hyper-parameters are identical to thosein the setting (a).",
  "C.3More Experiments on Combination of WKD-L or WKD-F": "How WKD complements other KD methods? We combine WKD-F with a state-of-the-art logit-based knowledge distillation (i.e., NKD), and combine WKD-L with a state-of-the-art feature-basedmethod (i.e., ReviewKD). The results are shown in a and b, respectively. We cansee that NKD+WKD-F improves over individual NKD and WKD-F, which suggests our WKD-F iscomplementary to NKD. Notably, NKD+WKD-F is slightly inferior to WKD-L+WKD-F. In addition,WKD-L+ReviewKD improves over ReviewKD but underperforms WKD-L. We conjecture that, dueto the large gap ( 0.9%) between ReviewKD and WKD-L, the combination hurts WKD-L. Will separation of target probability in WKD-L still help when combined with WKD-F? Toanswer this question, we integrate WKD-L without separation into WKD-F, which is called WKD-L(w/o)+WKD-F. From c, we can see that it is slightly inferior to WKD-L (w/)+WKD-F inwhich the separation scheme is used in WKD-L. This suggests that the benefit due to the separationscheme decreases but still persists in the logit+feature approach.",
  "C.4Comparison to Competitors with Different Setups": "In of the main paper, we conduct comparison in the ordinary setting, which is formalized inCRD and adopted by most methods. However, there exist some works which adopt Settings (a)and (b) but with non-trivially different setups, e.g., DIST , NKD , MGD , and DiffKD .Specifically, their students and/or teachers have performance higher than those in the ordinarysettings, which make them somewhat advantageous when comparing with the metric of Top-1accuracy. For a fair comparison, we propose to use gains of the distilled student over the vanillastudent in Top-1 accuracy, which is denoted by . They can more faithfully indicate how muchknowledge the student has learned from the teacher. The comparison results are shown in . For logit distillation, WKD-L outperforms NKD, a strong KL-Div based variant, by 0.68% and 0.93%in setting (a) and setting (b), respectively. Instead of KL-Div, DIST matches predicted probabilitiesbased on Pearson correlation coefficients, which exploits both instance-wise and cross-instanceknowledge, in contrast to WKD-L that uses instance-wise knowledge only. Nevertheless, WKD-Lperforms better than it by 0.43% in setting (a) and a large margin of 1.19% in setting (b). For featuredistillation, we compare to MGD that randomly masks the students features and then forces thestudent to generate the teachers features. The accuracies of WKD-F are over 1.0% higher than thoseof MGD in both settings. Regarding logit+feature distillation, we contrast with MGD+WSLD andDiffKD. DiffKD denoises the student features through a diffusion model trained by teacher featureswhose computation cost is high. WKD-L+WKD-F surpasses DiffKD by 0.55% and 1.33% in setting(a) and setting (b), respectively.",
  "WKD-L+WKD-F (ours)73.3169.7572.76+3.0176.1668.8773.69+4.82": ": Image classification results (Top-1 Acc, %) on ImageNet between WKD and the competitorswith different setups. Red numbers indicate the teacher/student model has non-trivially higherperformance than the commonly used ones formalized in CRD . represents the gains of thedistilled student over the vanilla student. Additionally, MLKD makes use of a stronger image augmentation, i.e., RandAugment (cf.their implementation), for improving performance. Vanilla KD studies the great potential ofvanilla KD in a very different setting, in which optimizer with much longer epochs, diverse and verystrong augmentations along with more regularization methods are used. In contrast, we and many ofthe state-of-the-art methods follow the ordinary setting formalized in CRD .",
  "C.5Summary of Hyper-parameters for across CNNs and Transformers on CIFAR-100": "For a fair comparison, we follow OFA and separately tune the hyper-parameters for differentsettings. For WKD-L, we set the temperature to 2 and the sharpening parameter to 1, while searchingthe optimal weight of LWKDL in with a step of 50. For WKD-F, the projector is simplya linear layer. We set mean-cov ratio to 2, and perform grid search for the weight of LWKDF in{1, 2, 4, 8}1e2. The spatial grid for computing Gaussians (Diag) is set to 11. We report averageaccuracy and standard deviation after three runs in ; the results of all competing methods areduplicated from OFA.",
  "C.6Knowledge Distillation within CNN Architectures on CIFAR-100": "Experimental Setup.We follow the setting of CRD , where the networks cover ResNet ,Wide-ResNet (WRN) , VGG , MobileNetV2 , and ShuffleNetV1 (SNV1) . Allmodels are trained for 240 epochs via the SGD optimizer with a batch size of 64, a momentum of 0.9and a weight decay of 0.0005. The initial learning rate is 0.01 for MobileNetV2 and ShuffleNetV1 and0.05 for the remaining networks, divided by 10 at the 150th, 180th, and 210th epochs, respectively.As in CRD , the projector is a 11 Conv or 44 transposed Conv both with BN and ReLU. For a fair comparison, we follow the practice of DKD , CAT , ReviewKD , FCFD and WTTM , i.e., tuning hyper-parameters separately for different architectures. For WKD-L,we perform grid search for the weight of LWKDL in with a step of 50, the temperature in {4, 8} and the sharpening parameter in {0.5, 1}. For WKD-F, we perform grid search for theweight of LWKDF in {1, 2, . . . , 50}1e2 and mean-cov ratio in {2, 3, . . . , 8}. The spatial gridfor computing Gaussians (Diag) is searched in {11, 22, 44}. We report the average accuracyand standard deviation of our method after three runs. Results.The comparison results are shown in . For logit distillation, WKD-L outperformsthe classical KD by large margins in all settings, and surpasses the competitors in 5 out of 6 settingsacross homogeneous and heterogeneous architectures. For feature distillation, WKD-F invariablyachieves better results than EMD-based counterparts (i.e., WCoRD and EMD+IPOT) and 2nd-moment based counterpart (i.e., NST); meanwhile, WKD-F is also very competitive, compared toother state-of-the-art methods. For logit+feature distillation, WKD-L+WKD-F ranks first in 4 out of6 settings across the board. Overall, our methods have comparable or lower standard deviation, asopposed to the competing methods, which suggests that our method is statistically robust.",
  "D.1Visualization of Teacher-Student Discrepancies": "Following [25; 3], we visualize the difference of correlation matrices of student and teacher log-its.a shows visualization results in two settings on CIFAR-100 .In setting ofResNet32x4ResNet8x4, the teacher and student are ResNet32x4 and ResNet8x4, respectively; thesetting VGG13VGG8 consists of a teacher of VGG13 and a student of VGG8. On the whole, inboth settings the colors of WKD-L are much lighter than those of KD , which indicates WKD-Lproduces correlations matrices more similar to the teacher than KD. As the differences of correlationmatrices capture inter-class correlations , smaller differences of WKD-L suggest better-informedcross-category relations than KL divergence can be learned. In addition, we visualize whether WKD-F can learn distributions that are more similar to the teacher.To that end, we use high-level features output from the last Conv layer of a network model forcomputing distributions, as they encode the most discriminative information for classification. Foreach validation image of the ith category, we compute WD between feature distributions of theteacher and student. Hence, for n categories each with k validation images, we obtain a nk matrixof distribution matching. b shows visualization of two settings on CIFAR-100. We can seethat overall WKD-F demonstrates smaller discrepancies with the teacher than FitNet , suggestingit can better mimic the teachers distributions.",
  "D.2Visualization of Class Activation Maps (CAMs)": "We use Grad-CAM to compute class activation maps using features output from the last Convlayer. shows, for three example images, CAMs of different models, including the teacher,vanilla student, distilled models by KD, WKD-L, FitNet and WKD-F. It can be seen that WKD-L andWKD-F have more similar CAMs with the teacher than KD and FitNet, and localize more accuratelythe important regions of objects. The comparison suggests that WKD-L and WKD-F can learnfeatures with better representation capability.",
  "EExtra Experiment on Object Detection": "The framework of Faster-RCNN with Feature Pyramid Network (FPN) is much morecomplicated than that of image classification. It contains the backbone network, Region ProposalNetwork (RPN), Feature Pyramid Network (FPN), and detection head consisting of a classificationbranch and a localization branch. For feature distillation, besides RoIAlign layer, FPN can also beused for knowledge transfer .",
  "E.1Implementation details on COCO": "For WKD-L, we use discrete WD to match the probabilities predicted by the classification branchesof the teacher and student. We set the temperature =1 and sharpening parameter =2, and set allthe weights of LCE, Lt and LWKDL to 1 for RN101RN18. All hyper-parameters of RN50MV2are the same as RN101R18 but that is set to 1. For WKD-F, we transfer knowledge from featuresstraightly fed to the classification branch, i.e., features output by the RoIAlign layer. We let RoIAligngenerate a high resolution of 1818 feature maps to exploit more features, and choose a 44 spatialgrid for computing Gaussians. For both settings, we set the weight of LCE to 1, and that of LWKDFto 5e3 with mean-cov ratio =2; as in FCFD , we use a projector of 33 Conv with BN.",
  "We analyze key component of WKD-F specific to object detection on MS-COCO with ResNet101as the teacher and ResNet18 as the student": "Where to distill features: RoIAlign or FPN?Among five stages (P2P6) of FPN, we select asingle P3 with a spatial grid of 1616 for extracting Gaussians; this option produces the best resultamong the candidates, and combination of multiple stages brings us no benefit. We compare to thebaseline of FitNet and the results are shown in a. When distilling features of FPN,WKD-F significantly outperforms FitNet (over 2% mAP). By moving the distillation position toRoIAlign layer, the performance of both methods improve, while WKD-F still performs better thanFitNet by a non-trivial margin. How spatial size of RoIAlign features affect performance? In Faster-RCNN, the RoIAlign layeroutputs standard 77 feature maps. To exploit more spatial information, we let RoIAlign layer outputfeature maps of higher resolution. b shows effect of size of feature maps on performance. Itcan be seen that when the spatial size enlarges mAP increases accordingly, and the mAP tends tosaturate if the size is as large as 2828. The result suggests that larger size of RoIAlign featuresbenefit feature distillation.",
  "E.3Integration with State-of-the-art KD Methods": "We assess whether our methodology is complementary to two state-of-the-art methods, i.e., ReviewKDand FCFD. To this end, for ReviewKD, we add the losses of WKD-L and WKD-F into the originalloss functions; the weights of our two losses and hyper-parameters are same as those in Section E.1.For FCFD, we replace respectively the original KD loss and feature distillation loss (i.e., FitNet loss)by the losses of WKD-L and WKD-F whose weights are identical to those specified in the mainpaper. The results are shown in . We can see that, by integrating our methods, both FCFDand ReviewKD improve by non-trivial margins, which indicates that our methodology is parallel tothem and thus can enhance their performance.",
  "E.4Implementation Details on BB Regression for KD": "Besides common logit distillation and feature distillation, FCFD additionally uses BB regressionfor knowledge transfer and achieves state-of-the-art performance. Here we also introduce it into ourmethodology. Specifically, for each proposal from RPN of the student , both student and teacherperform BB regression, predicting separately a localization offset vector (LOV) o, from which weobtain predicted bounding box B of the target class. Then the distillation loss is written as",
  "LBB =L2oT, oS+LDIoUBT, BS,(17)": "where L2 indicates the square of Euclidean distance between the students LOV vector oS and theteachers one oT. We use the Distance-IoU loss LDIoU defined by Zheng et al. , which measuresthe Intersection over Union (IoU) between two bounding-boxes BS and BT with a penalty term. Theconstant is to balance the two losses and is set to 20 throughout.",
  "FLimitations and Future Research": "The cost of our WKD-L is higher than KL-Div based methods due to the regularized linear program-ming . However, it is affordable as shown in and will benefit from advance of fasteralgorithms for solving WD . Potentially, WKD-L can generalize to be a label smoothing regular-ization method. Specifically, besides the CE loss, one introduces an additional loss that measuresWD rather than KL-Div between the logits and a uniform distribution , while computing IRsusing embeddings of textual category names as prototypes via ready-made LLMs . BesidesBAN based Self-KD in .5, it is promising to further generalize WKD-L for teacher-freeknowledge distillation, e.g., by using customized soft labels and IRs based on weights of thesoftmax classifier. Our WKD-F models distribution of features with Gaussians. As deep features of DNNs are generallyof high-dimension and small-size, accurate estimation of covariance matrices is difficult .Therefore, exploration of robust and efficient methods for estimating Gaussian may further improvethe performance of WKD-F. Besides, the Gaussians may be sub-optimal for modeling featuredistributions. To the best of our knowledge, what distributions deep features may exactly followis an open problem. It is interesting to study other parametric distributions and the correspondingdis-similarities for knowledge distillation.",
  "GBroader Impact": "We address limitations of Kullback-Leibler Divergence for knowledge distillation (KD), and canobtain stronger, lightweight models suitable for resource-limited devices. Our methodology is verypromising, readily applicable to a variety of visual tasks, e.g., image classification and object detection.We hope our work sheds light on importance of WD and inspires future exploration of it in the fieldof knowledge distillation. With breakthroughs of large-scale pre-training, multimodal large language models (LLMs) like GPT-4 have excelled in many visual tasks. Our methodology can be potentially applied to transferknowledge from LLMs to smaller ones for specific visual or language tasks, allowing for improvedperformance while preserving fast inference cost. Consequently, researchers as well as practitionerscan more easily benefit from advanced technologies of LLMs, facilitating their widespread use andbroader accessibility. Currently, the theoretical quest for KD is limited. Due to the black-box nature of the distillationprocess, the student distilled using our methods may inevitably inherit harmful biases from theteacher . Moreover, the reduction in deployment cost may lead to more potential harms of modelabuse. This highlights the necessity for more widespread efforts to regulate the use of artificialintelligence techniques including knowledge distillation."
}