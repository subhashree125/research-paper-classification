{
  "Abstract": "As Large Language Models (LLMs) are increasingly deployed in specialized do-mains with continuously evolving knowledge, the need for timely and preciseknowledge injection has become essential. Fine-tuning with paraphrased data isa common approach to enhance knowledge injection, yet it faces two significantchallenges: high computational costs due to repetitive external model usage andlimited sample diversity. To this end, we introduce LaPael, a latent-level paraphras-ing method that applies input-dependent noise to early LLM layers. This approachenables diverse and semantically consistent augmentations directly within themodel. Furthermore, it eliminates the recurring costs of paraphrase generationfor each knowledge update. Our extensive experiments on question-answeringbenchmarks demonstrate that LaPael improves knowledge injection over standardfine-tuning and existing noise-based approaches. Additionally, combining LaPaelwith data-level paraphrasing further enhances performance.",
  "Introduction": "Pre-trained Large Language Models (LLMs) encode extensive factual information from their trainingdata, enabling them to answer factoid questions such as Who is the director of Dune: Part Two? . However, knowledge in LLMs is static, which can lead to outdated information as real-worldknowledge evolves. Additionally, LLMs often lack specificity for specialized or private domains. Toaddress this, it is common practice to fine-tune LLMs with updated or domain-specific documents,keeping the models knowledge up-to-date and enhancing expertise in particular domains . StreamingQA SQuADArchivalQA75 F1 Score QA Task Performance Fine-Tuning (FT)FT + Paraphrases OursOurs + Para.",
  ": Effect of paraphrasing datain knowledge injection": "However, does fine-tuning LLMs on a single document allowthem to fully internalize its knowledge? Even in pre-training,Kandpal et al. found that LLMs cannot perfectly learn all theinformation in the training data, particularly long-tail knowledgethat appears rarely or only once. Existing work has shownthat this issue persists with fine-tuning and suggested that dataaugmentation, such as paraphrasing, is a simple yet effective wayto enhance knowledge injection. As shown in , fine-tuningwith paraphrases enhances knowledge injection, as evidenced byimproved Question-Answering (QA) task performance. While data-augmented approach via paraphrasing is effective for knowledge learning, it has two mainlimitations: (1) High computational cost: Generating high-quality paraphrases requires significantcomputational resources. As shown in , paraphrasing models such as LLMs need to repeatedly generate paraphrases for each document with the new incoming knowledge. Thisleads to higher costs as the number of documents being learned continually increases; and (2) Limited",
  "User": "As of May 14, 2024, Dune: Part Two has grossed $282.1 million in the United States and Canada and $428.5 million in other territories, for a worldwide total of $710.6 million. A: $710.6 million As of May 14, 2024, Dune: Part Two has grossed $282.1 million in the United States and Canada and $428.5 million in other territories, for a worldwide total of $710.6 million. As of May 14, 2024, Dune: Part Two's revenue stands at $282.1 million in the United States and Canada and $428.5 million globally, with a combined total of $710.6 million.",
  "Paraphrasers": "BaseLLM : A conceptual illustration of the proposed approach. On the left, we show the existing method ofknowledge injection by paraphrasing each document for data-level augmentation. On the right, we present theconceptual illustration of LaPael with trained latent paraphrasers. Unlike the method on the left, LaPael caneliminate the need for users to repeatedly paraphrase using LLMs once latent paraphrasers are trained. diversity in augmented data: Although LLMs can produce varying high-quality paraphrases bysampling from the generative distribution, the diversity of the generated text is limited, resulting in anarrow range of augmented samples at the discrete data level. One way to overcome these issues is tointroduce noise into the token embedding. However, existing works do not consider the textsemantics when they perturb the latent features of LLMs with randomly generated noise. To address these issues, we take a distinct approach using an input-dependent noise generator namedlatent paraphraser learned from the paraphrases. Specifically, this function perturbs early layersto augment LLMs at the latent level while preserving the meaning of the text. To optimize thelatent paraphraser, we start by generating paraphrases of the documents. Then, we train the latentparaphrasers to ensure that the latent distribution of the LLMs with the original sentence is closeto the latent distribution with the paraphrased sentences. Once training is done, we can transfer thelatent paraphrasers to the documents from any domain that contains new knowledge. We refer to ourmethod as Latent Paraphrasing of Language Models (LaPael), as it learns the paraphrasing of textdata at the latent level. We validate our approach on diverse question-answering benchmark datasets designedto evaluate knowledge injection. These benchmarks involve fine-tuning LLMs on documents thatcontain the knowledge required to answer the questions in the datasets. Our results show that LaPaelsignificantly improves knowledge injection performance compared to standard fine-tuning. Moreover,LaPael outperforms fine-tuning with paraphrases, demonstrating that LaPael alone is sufficient fordata augmentation in knowledge injection scenarios, as illustrated in . As shown in ,we further find that using LaPael in combination with paraphrases further enhances performance,providing complementary benefits to data-level augmentations. Finally, LaPael surpasses existingnoise baselines , highlighting the importance of learning noise for effective augmentations. Our contributions are as follows: We introduce LaPael, a new method that applies learned perturbations to the layers of LLMs toenhance knowledge injection, addressing the limitations of data augmentations and noise baselines.",
  "Knowledge of Large Language ModelsLarge Language Models (LLMs) store vast amounts offactual knowledge in their pre-trained parameters . The straightforward way to extract the": "knowledge of LLMs is to ask the question that requires factual knowledge . Through askingquestions, Kandpal et al. have found that LLMs cannot perfectly memorize the entire knowledgein the pre-training corpora, especially for knowledge that appears rarely or only once. To make LLMsanswer the question requires under-represented or new knowledge, previous works have clusteredinto two different solutions. The first one is retrieval-augmented methods that retrieveknowledge from an external knowledge base and input the retrieved knowledge alongside the questioninto LLMs. The second one is fine-tuning where the parameters of pre-trained LLMs arecontinually updated by fine-tuned on the document containing knowledge in an unsupervised way asin pre-training . In our work, we focus on improving the fine-tuning-based solution, as storingnew knowledge in the parameters of LLMs is efficient since we can reduce the length of the inputprompt and do not need any extra module or memory in the deployment time . Knowledge Injection in LLMsIn this work, knowledge injection in LLMs denotes fine-tuningLLMs on the set of documents to inject new or under-represented knowledge into LLMs ,different from another task of injecting symbolic knowledge (e.g., knowledge graph) into LLMs . Among previous works, CaMeLS has introduced a meta-learning method for learnable lossscaling function that improves knowledge injection. As a concurrent work, MAC has proposedusing the memory of amortized context is highly effective in a knowledge injection. However, bothmethods have drawbacks like high computational costs for bi-level optimization or the need foradditional modules and memory. Recent works have shown that data augmentation whichparaphrases the knowledge-containing sentences helps language models memorize knowledge in amore extractable format (e.g., asking questions) after knowledge injection. Furthermore, Jiang et al. has shown that the instruction-tuned model is better at learning new knowledge. Compared toprevious works, we focus on developing an alternative method to data augmentation that perturbs thelatent representation of LLMs for better knowledge injection. Data Augmentation and Latent PerturbationThe usefulness of data augmentations for textdata was empirically observed in the literature. For instance, EDA has introduced simple dataaugmentation method which randomly deletes, swaps, replaces, and inserts the words. Other previousworks have utilized the trained LMs to augment the text data. Recently, Maini et al. has shown that adding data rephrased by LLMs into the pre-training corpus improves the performanceof LM pre-training. However, those methods require additional costs in the knowledge injection as itutilize the LLMs to rephrase the text. In contrast, the latent perturbations offer an orthogonal approachto improve the robustness of neural networks, complementing data augmentation. This techniquehas been employed in meta-learning and out-of-distribution generalization . For instance,NEFTune demonstrated that adding noise, randomly sampled from a uniform distribution, totoken embedding layers improves instruction tuning performance. Expanding on the concept of latentperturbations, our work introduces a novel approach that internalizes the effects of text paraphrasingby identifying optimal latent perturbations through training a small neural network within the LLMs.",
  "Problem Formulation": "In this work, we follow the knowledge injection setting outlined by Ovadia et al. . We are giventhree resources: (1) documents DK containing knowledge that we are interested to inject; (2) question& answering dataset DQA = {(q(i), a(i))}ni=1 for verifying injected knowledge from DK; and (3)a pre-trained Large Language Models (LLMs) p() parameterized by . Our objective is to find atransformation F that could enhance the knowledge about DQA:",
  "estimateestimate": ": (a) Illustration of the latent paraphraser. The linear layer embeds each tokens latent feature h into. We then sample stochastic noise from N(, I) and apply a mask mt to control the scale. (b) Trainingpipeline of LaPael. To train the latent paraphraser, we estimate the parameters of Gaussian distributions. Wethen minimize the KL divergence between these distributions to optimize the latent paraphrasers.",
  "Proposed Method": "We propose Latent Paraphrasing of Language Models (LaPael), a framework that perturbs the latentfeature of LLMs, to achieve the equivalent effect of data augmentation at the latent level. Knowledgeinjection using LaPael consists of the following four processes: paraphrasing the set of documentsto make the paraphrased data (.1), training the latent paraphrasers with paraphrased data(.2), fine-tuning LLMs with the trained latent paraphrasers on DK and evaluate the injectedknowledge of LLMs on DQA (.3).",
  "Data Augmentation: Paraphrasing": "To train the latent paraphrasers, we need a distinct set of training data Dtrain = {s(i)}Ni=1 whichconsists of documents having different knowledge with DK. As a preliminary, we formulate theparaphrasing of the text in terms of the knowledge equivalence, which is a narrower concept thansemantic equivalence where two different sentences can contain the same knowledge. Weconsider that each sentence s in Dtrain can be decomposed into words for the object (entity orattribute) of the knowledge (y) and others (x) where both are the sequence of tokens. For instance,given the sentence The capital of the United States is Washington D.C.,x = The capital of the United States is;y = Washington D.C.,represent the knowledge (United States, capital, Washington D.C.). Then, we paraphrase a sentences = (x, y) into a paraphrased sentence1. For the above sentence, a paraphrased sentence can bex = In the case of the United States, the designated capital city iswith the same y, which is knowledge equivalent to (x, y). For each knowledge K, we assume thatthere is a set of the knowledge equivalent sentences S(K) where (x, y) S(K). We generate K para-phrased sentence via a LLM: (x1, y), . . . , (xK, y) pLLM(x|prompt, x, y). Then, we have the setof paraphrased data {{(x(i)k , y(i))}Kk=1}Ni=1 of Dtrain. We define p(x|x) := pLLM(x|prompt, x, y)which denotes the probability distribution of paraphrases given the original sentence. 1One possible way is to prompt the LLM (e.g., gpt-3.5-turbo ) with instruction For the followingparagraph give me a paraphrase of the same in high-quality English language as in sentences on Wikipedia",
  "Introducing Latent Paraphraser": "Latent ParaphraserWe introduce a latent paraphraser within a transformer layer , whichaugments a latent feature and is expected to paraphrase the given input text within the latent space.As illustrated in (a), within the transformer architecture, we insert this new layer just beforethe Multi-layer Perceptron (MLP), using the output from the second LayerNorm as its input.",
  "where MLPz is a 2-layers MLP. We use the reparameterization trick to enable the back-propagation through the sampling from the Gaussian distribution: = + , where N(0, I)": "To modulate the scale of perturbation for individual tokens, we employ a learnable mask. It isimportant as too much noise on key tokens (e.g., United States) might hurt the semantics of thesequence. For learnable binary mask, we use concrete distribution to approximate the samplingdiscrete random variable from a Bernoulli distribution using continuous relaxation as follows:",
  "Datasets": "To follow the experimental setup in , we need (1) documents containing knowledge DKand (2) associated QA datasets DQA. We mainly use the test split of three QA datasets: SQuAD ,StreamingQA , and ArchivalQA for the source of DK and DQA in our main experiments.These datasets, previously used in Hu et al. , consist of documents paired with their correspondingQAs, making them well-suited to our experimental setup. While the questions in these datasets are ofdecent quality, a significant limitation lies in the documents provided. These documents are likely tohave been seen by LLMs during pre-training, making it difficult to accurately assess the performanceof methods on injecting new knowledge.",
  "Fine-Tuning (+ para.)68.5085.1280.5185.4593.6792.3264.9085.9281.24": "To mitigate this issue, we incorporate two datasets with synthetic QAs Films 2024 and Events2024. These are QA datasets generated from raw Wikipedia articles under the 2024 films categoryand from US events in May, June, and July 2024, in the 2024 events in the United States category.We generated question-answer pairs from these documents using GPT-4o following methods fromprevious works . Since the documents used to generate these datasets were not seen by theLLMs during pre-training, we can better evaluate the effectiveness of each method for knowledgeinjection especially on new knowledge. Datasets with Synthetic DocumentsThe raw documents from datasets are unsuitable for preciselymeasuring the knowledge injection performance. Specifically, fine-tuning LLMs on a document doesnot always ensure that LLMs can answer the associated questions, due to the reversal curse .Moreover, documents often contain irrelevant knowledge that may hinder the accurate assessment ofknowledge injection . To address these issues, we conduct evaluations under the setting of synthetic documents. Forgenerating synthetic documents, we construct DK by rephrasing each question and answer in DQAusing GPT-4-turbo , ensuring that fine-tuning on these synthetic documents guarantee that LLMsbecome answerable to the associated questions. Examples of questions, synthetic, and raw documentsare shown in . To make a difference, we denote the dataset under the synthetic documentsetting with the suffix -syn and the raw document setting with the suffix -raw. Datasets for Training Latent ParaphrasersFor training our latent paraphrasers, the set of trainingdata Dtrain is required in addition to DK. Therefore, we use GPT-3.5-turbo to generate the set ofsynthetic sentences from the subset of a training split of each QA dataset, where each sentence mustbe with the answer to questions, following the sentence format in .1.",
  "Experimental Details": "BaselinesWe compare our LaPael against several baselines. All models are fine-tuned on thedocuments in DK unless explicitly stated otherwise. (1) No Injection. We use the pre-trained LLMwithout any fine-tuning. (2) Fine-Tuning. We fine-tune the LLM on DK. (3) Fine-Tuning (seq). Wefirst fine-tune the LLM on the paraphrased documents of Dtrain. Then, we fine-tune the LLM on DK.(4) Fine-Tuning (+ para). We fine-tune LLM on the original and paraphrased documents of DK.(5) FreeLB . We add trained adversarial noise to the token embedding while fine-tuning. (6)NEFTune. We add random uniform noise to the token embedding while fine-tuning. (7) LaPael(ours). We train the latent paraphrasers on Dtrain and then fine-tune the model on DK.",
  "Ours (SQuAD )72.5089.3885.3484.3893.4492.1754.1769.4065.7263.7068.2867.98Ours (StreamingQA )72.8089.6585.9084.0693.7391.9054.5872.5868.1563.2068.0267.79": "Training & InferenceWe mainly use Vicuna-7b-v1.5 for fine-tuning, which is the instruction-tuned version of Llama-2-7b for our experiments. We fine-tune LLMs for 12 epochs with alearning rate of 0.00005 and step learning rate scheduler where we decay a learning rate by 0.85by every 4 epochs. For inference, we use in-context learning with 5 examples by prompting the 5examples in the prompt . To measure QA accuracy, we use Exact Match (EM), Recall (Rec.), andF1 score. More details on the experimental setting are provided in the Appendix C.",
  "Experimental Results": "Experiments with Synthetic DocumentsIn , we present the experimental results for thesynthetic documents setting. Fine-tuning does improve the QA performance of LLMs, but it does notlead to near-perfect scores even though the synthetic document contains the necessary knowledge foranswering the questions, as shown in . Our experiments show that paraphrasing documents for fine-tuning consistently improves QA per-formance across all three benchmarks. Notably, LaPael demonstrates performance comparable tofine-tuning with paraphrases on StreamingQA and even outperforms it on two other benchmarks.These findings suggest that the latent paraphrasers learn an effective noise distribution that aidsknowledge injection without additional data augmentation. We also compared LaPael with two other noise-based methods, FreeLB and NEFTune ,to validate that the latent-level noise generated by latent paraphrasers is more effective. As shownin , LaPael outperforms these baselines, confirming the strength of our approach. Experiments with Raw DocumentsWhile our method has proven effective for knowledge injectionwith synthetic documents, it is important to evaluate its performance on raw documents, whichrepresent a more realistic data format. To demonstrate the applicability of our method to real-worlddata, we conducted experiments in which we fine-tuned LLMs on raw documents for each dataset,using latent paraphrasers trained on Dtrain from SQuAD-syn. As shown in , our method outperforms both fine-tuning and noise-based baselines in thecontext of knowledge injection with raw documents. Considering that the latent paraphrasers weretrained on synthetic sentences from Dtrain, these results demonstrate their effectiveness on documentswith a different format than those used in training. Cross-domain TransferOnce trained, the latent paraphrasers can be applied to fine-tune LLMs ondocuments from any domain. To demonstrate this, we conducted cross-domain transfer experiments.Specifically, we trained latent paraphrasers on Dtrain from a source domain (e.g., SQuAD) and fine-tuned LLMs with the trained latent paraphrasers on DK from a target domain (e.g., StreamingQA). The Number of Paraphrases F1 Fine-tuningOurs",
  "(c) ArchivalQA-syn": ": Effect of the Number of Paraphrases. Each plot shows the relationship between the number ofparaphrases (x-axis) and F1 scores (y-axis) in knowledge injection. The F1 scores of both standard fine-tuningand our method improve as the number of paraphrases increases. Perturb Training Dataset Size (%) QA Accuracy (F1) NEFTuneFreeLBOurs (mean)",
  "(b) Varying position of latent paraphraser": ": (a) We conduct experiments varying the size of Dtrain on SQuAD-syn, where 100% indicates 1,000documents. We report mean and std. over three runs. (b) We conduct experiments on StreamingQA-syn varyingthe start position of latent paraphrasers where # layers denotes the number of latent paraphrasers. As shown in , our method successfully transfers across domains, with the latent paraphrasersenhancing the performance of the knowledge injection on NovelQA and MedMCQAtwo domainsdistinct from the source (see Appendix C.1 for details on these datasets). Even though both domainscontain specialized entities, our method consistently outperforms standard fine-tuning and othernoise-based baselines. Combining LaPael and ParaphrasesParaphrasing documents in DK has been shown to improveknowledge injection performance, as seen in . While LaPael significantly improves perfor-mance without requiring paraphrases, it is valuable to consider the effect of combining paraphraseswith the latent perturbations from LaPael. As illustrated in , LaPael consistently outperformsstandard fine-tuning, showing that LaPael provides advantages over data-level augmentations.",
  "Ablation Studies": "Effects of the Size of DtrainLaPael needs additional data Dtrain for training latent paraphrasers.Although only a small amount of data is required, it might be unclear how much is needed to makethe latent paraphrasers learn the useful noise distribution. As shown in a, LaPael works welleven with 50 sentences for Dtrain, while increasing the size of Dtrain ensures a steady performanceimprovement for LaPael. Effects of the Position of Latent ParaphrasersOur latent paraphrasers can be inserted into anylayer of the LLMs. The possible question is which position and how many layers are optimal forlatent paraphrasers to effectively learn noise for knowledge injection. To answer this, we analyzedthe position and number of latent paraphrasers. In b, we show the QA accuracy results, varying the start position and number of latentparaphrasers. The first layer is the closest layer to the input layer, and \"start position 1\" with \"#layers = 3\" means we insert the latent paraphrasers into the first, second, and third layers of the LLM.Results show that inserting three latent paraphrasers into the early layers of the LLM is effective. Thisis consistent with findings in previous works where using noisy token embeddings (thelowest layer) enhanced the generalization in LLMs. Furthermore, in , we empirically showthat positioning the latent paraphraser before the MLP layer within each transformer layer is the mosteffective choice over other positions.",
  "Learnable Mul.84.0693.7391.90Learnable Add.73.0583.2381.70Gaussian83.4690.3289.54Gaussian + mask82.8589.7088.87Uniform79.4887.1786.15Uniform + mask74.4381.0980.26": "Ablation Studies on ModulesLaPael has many design choices concerning the latent paraphraserarchitecture, noise type, and training. We conducted extensive ablation studies to empirically verifyeach design choice and provide guidance for future work. In summary, as shown in , alldesign choices are important for building the most effective latent paraphraser. Specifically, we usea trainable mask m in Equation (7) to regulate the perturbation depending on each token, whichis crucial, as the performance on StreamingQA drops significantly if we remove it from the latentparaphraser. Furthermore, using only the sigmoid function in Equation (7) instead of the concretedistribution also leads to much lower performance, as the mask is not properly trained. Regardingnoise training, using deterministic noise instead of stochastic noise by removing the noise drawnfrom a Gaussian distribution in Equation (5) also decreases performance. Additionally, replacingthe KL loss with Mean Squared Error loss between two means latent in Equation (13) and datain Equation (14) leads to a decrease in performance, confirming the importance of stochastic noisetrained with KL loss. Ablation Studies on Noise DistributionShould we train the latent paraphrasers to be effective,or can adding random noise in the early layers also be effective? Which is more important: thelearnable mask or the learnable noise? To answer these questions, we conducted ablation studieson the choice of noise distribution. In , Learnable Add. denotes the model with the additivenoise h + g(h) instead of Equation (4) without softplus from Equation (6). Gaussian is the use ofzero-mean Gaussian noise N(0, I) in Equation (6) without using MLPz. Uniform is the use of noisedrawn from the uniform distribution defined in NEFTune instead of z in Equation (6). As shown in , the learnable multiplicative noise described in .2 is the best design fornoise distribution used in the latent paraphraser. To analyze the effect of the learnable mask, we alsoadded the learnable mask to the Gaussian and Uniform noise settings and optimized only Wm andbm in Equation (7) with loss in Equation (15). Interestingly, the learnable mask is not effective for thefixed noise distribution, which contrasts with the results for learnable noise in . We conjecturethat using the learnable mask is important for input-dependent learnable noise, as it can allocatedifferent noise scales to different tokens, while this is not the case for static noise distribution.",
  "Conclusion": "We have introduced LaPael, a method for enhancing knowledge injection in Large Language Models(LLMs) by applying learned perturbations to their layers. Unlike traditional data-level augmentationsor noise-based approaches, LaPael operates at the latent level, preserving the semantic integrity of thetext while introducing meaningful variability. LaPael addresses key limitations of existing methods byreducing computational costs and increasing the diversity of augmented data. Our extensive validationacross diverse benchmark datasets demonstrates the superiority of our method in knowledge injection,as it significantly outperforms both standard fine-tuning and other noise-based baselines. Moreover,combining LaPael with paraphrases yields complementary benefits, further enhancing performance.We believe that LaPael, being simple yet effective, has the potential for significant practical impactand will encourage further research on applying perturbation within the latent space of LLMs. Discussions & LimitationsIn our work, the following points can be discussed further: (1) CostAnalysisWhile LaPael is effective, it incurs additional costs due to the need for training latentparaphrasers and fine-tuning LLMs with them. (2) Knowledge RetentionAlthough LaPael improvesknowledge injection, there may be trade-offs in terms of retaining the original knowledge thatthe LLM has memorized. (3) Comparison to Retrieval-Augmented Generation (RAG)While ourmethod improves knowledge injection, it is still less effective than RAG in terms of performance. Weprovide a detailed discussion of these points, along with other limitations, in the Appendix A.",
  "We sincerely thank Byeongju Kim, Jongwon Jeong, Jimin Hong, and Jongho Park for their insightfuldiscussion. This work was fully supported by the KRAFTON AI Research Center": "Marah I Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, HanyAwadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat S. Behl, Alon Benhaim, MishaBilenko, Johan Bjorck, Sbastien Bubeck, Martin Cai, Caio Csar Teodoro Mendes, WeizhuChen, Vishrav Chaudhary, Parul Chopra, Allie Del Giorno, Gustavo de Rosa, Matthew Dixon,Ronen Eldan, Dan Iter, Amit Garg, Abhishek Goswami, Suriya Gunasekar, Emman Haider,Junheng Hao, Russell J. Hewett, Jamie Huynh, Mojan Javaheripi, Xin Jin, Piero Kauffmann,Nikos Karampatziakis, Dongwoo Kim, Mahoud Khademi, Lev Kurilenko, James R. Lee, Yin TatLee, Yuanzhi Li, Chen Liang, Weishung Liu, Eric Lin, Zeqi Lin, Piyush Madan, Arindam Mitra,Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, ThomasPortet, Reid Pryzant, Heyang Qin, Marko Radmilac, Corby Rosset, Sambudha Roy, OlatunjiRuwase, Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang,Hiteshi Sharma, Xia Song, Masahiro Tanaka, Xin Wang, Rachel Ward, Guanhua Wang, PhilippWitte, Michael Wyatt, Can Xu, Jiahang Xu, Sonali Yadav, Fan Yang, Ziyi Yang, Donghan Yu,Chengruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang,Yunan Zhang, and Xiren Zhou. Phi-3 technical report: A highly capable language model locallyon your phone. arXiv preprint arXiv:2404.14219, 2024. doi: 10.48550/ARXIV.2404.14219.URL Monica Agrawal, Stefan Hegselmann, Hunter Lang, Yoon Kim, and David A. Sontag. Largelanguage models are few-shot clinical information extractors. In Yoav Goldberg, ZornitsaKozareva, and Yue Zhang, editors, Proceedings of the 2022 Conference on Empirical Methodsin Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December7-11, 2022, pages 19982022. Association for Computational Linguistics, 2022. URL Lukas Berglund, Meg Tong, Maximilian Kaufmann, Mikita Balesni, Asa Cooper Stickland,Tomasz Korbak, and Owain Evans. The reversal curse: LLMs trained on a is b fail to learnb is a. In The Twelfth International Conference on Learning Representations, 2024. URL Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agar-wal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh,Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler,Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCan-dlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learn-ers. In Hugo Larochelle, MarcAurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, andHsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: AnnualConference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL Hengyi Cai, Hongshen Chen, Yonghao Song, Cheng Zhang, Xiaofang Zhao, and Dawei Yin.Data manipulation: Towards effective instance learning for neural dialogue generation vialearning to augment and reweight. In Proceedings of the 58th Annual Meeting of the Associ-ation for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 63346343.Association for Computational Linguistics, 2020. URL Maria Angels de Luis Balaguer, Vinamra Benara, Renato Luiz de Freitas Cunha, Robertode M. Estevo Filho, Todd Hendry, Daniel Holstein, Jennifer Marsman, Nick Mecklenburg,Sara Malvar, Leonardo O. Nunes, Rafael Padilha, Morris Sharp, Bruno Silva, Swati Sharma,Vijay Aski, and Ranveer Chandra. RAG vs fine-tuning: Pipelines, tradeoffs, and a case study on",
  "Ronen Eldan and Yuanzhi Li. Tinystories: How small can language models be and still speakcoherent english? arXiv preprint arXiv:2305.07759, 2023. URL": "Yarin Gal, Jiri Hron, and Alex Kendall. Concrete dropout. In Isabelle Guyon, Ulrike vonLuxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and RomanGarnett, editors, Advances in Neural Information Processing Systems 30: Annual Conferenceon Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA,pages 35813590, 2017. URL Zorik Gekhman, Gal Yona, Roee Aharoni, Matan Eyal, Amir Feder, Roi Reichart, and JonathanHerzig. Does fine-tuning llms on new knowledge encourage hallucinations? arXiv preprintarXiv:2405.05904, abs/2405.05904, 2024. URL",
  "Olga Golovneva, Zeyuan Allen-Zhu, Jason Weston, and Sainbayar Sukhbaatar. Reverse trainingto nurse the reversal curse. arXiv preprint arXiv:2403.13799, 2024. URL": "Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio Csar Teodoro Mendes, Allie Del Giorno,Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, AdilSalim, Shital Shah, Harkirat Singh Behl, Xin Wang, Sbastien Bubeck, Ronen Eldan, Adam Tau-man Kalai, Yin Tat Lee, and Yuanzhi Li.Textbooks are all you need.arXiv preprintarXiv:2306.11644, 2023. doi: 10.48550/ARXIV.2306.11644. URL Suchin Gururangan, Ana Marasovic, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,and Noah A. Smith. Dont stop pretraining: Adapt language models to domains and tasks.In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault, editors, Proceedings ofthe 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online,July 5-10, 2020, pages 83428360. Association for Computational Linguistics, 2020. URL Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. InInternational Conference on Learning Representations, 2022. URL Nathan Hu, Eric Mitchell, Christopher D. Manning, and Chelsea Finn. Meta-learning on-line adaptation of language models. In Houda Bouamor, Juan Pino, and Kalika Bali, edi-tors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Pro-cessing, EMNLP 2023, Singapore, December 6-10, 2023, pages 44184432. Associationfor Computational Linguistics, 2023. doi: 10.18653/V1/2023.EMNLP-MAIN.268. URL Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qiang-long Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and Ting Liu. A survey on hallucinationin large language models: Principles, taxonomy, challenges, and open questions. arXiv preprintarXiv:2311.05232, 2023. URL Neel Jain, Ping yeh Chiang, Yuxin Wen, John Kirchenbauer, Hong-Min Chu, GowthamiSomepalli, Brian R. Bartoldson, Bhavya Kailkhura, Avi Schwarzschild, Aniruddha Saha, MicahGoldblum, Jonas Geiping, and Tom Goldstein. NEFTune: Noisy embeddings improve instruc-tion finetuning. In The Twelfth International Conference on Learning Representations, 2024.URL",
  "In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event,April 25-29, 2022. OpenReview.net, 2022. URL": "Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra SinghChaplot, Diego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, LucileSaulnier, Llio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, ThibautLavril, Thomas Wang, Timothe Lacroix, and William El Sayed. Mistral 7b. arXiv preprintarXiv:2310.06825, 2023. doi: 10.48550/ARXIV.2310.06825. URL Zhengbao Jiang, Zhiqing Sun, Weijia Shi, Pedro Rodriguez, Chunting Zhou, Graham Neubig,Xi Victoria Lin, Wen-tau Yih, and Srinivasan Iyer. Instruction-tuned language models are betterknowledge learners. arXiv preprint arXiv:2402.12847, 2024. doi: 10.48550/ARXIV.2402.12847.URL Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. Large languagemodels struggle to learn long-tail knowledge. In Andreas Krause, Emma Brunskill, KyunghyunCho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, International Conferenceon Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202of Proceedings of Machine Learning Research, pages 1569615707. PMLR, 2023. URL Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In Yoshua Bengioand Yann LeCun, editors, 2nd International Conference on Learning Representations, ICLR2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings, 2014. URL Sosuke Kobayashi. Contextual augmentation: Data augmentation by words with paradigmaticrelations. In Proceedings of the 2018 Conference of the North American Chapter of theAssociation for Computational Linguistics: Human Language Technologies, NAACL-HLT, NewOrleans, Louisiana, USA, June 1-6, 2018, Volume 2 (Short Papers), pages 452457. Associationfor Computational Linguistics, 2018. URL Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. Semantic uncertainty: Linguistic invariancesfor uncertainty estimation in natural language generation. In The Eleventh International Confer-ence on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net,2023. URL Haebeom Lee, Taewook Nam, Eunho Yang, and Sung Ju Hwang. Meta dropout: Learningto perturb latent features for generalization. In 8th International Conference on LearningRepresentations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020.URL Seanie Lee, Minki Kang, Juho Lee, and Sung Ju Hwang. Learning to perturb word embeddingsfor out-of-distribution QA. In Proceedings of the 59th Annual Meeting of the Association forComputational Linguistics and the 11th International Joint Conference on Natural LanguageProcessing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021,pages 55835595. Association for Computational Linguistics, 2021. URL Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin,Naman Goyal, Heinrich Kttler, Mike Lewis, Wen-tau Yih, Tim Rocktschel, Sebas-tian Riedel, and Douwe Kiela.Retrieval-augmented generation for knowledge-intensiveNLP tasks.In Advances in Neural Information Processing Systems 33: Annual Con-ference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL Adam Liska, Toms Kocisk, Elena Gribovskaya, Tayfun Terzi, Eren Sezener, Devang Agrawal,Cyprien de Masson dAutume, Tim Scholtes, Manzil Zaheer, Susannah Young, Ellen Gilsenan-McMahon, Sophia Austin, Phil Blunsom, and Angeliki Lazaridou. Streamingqa: A benchmark for adaptation to new knowledge over time in question answering models.In KamalikaChaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvri, Gang Niu, and Sivan Sabato, editors,International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore,Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pages 1360413622. PMLR, 2022. URL Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In 7th InternationalConference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.OpenReview.net, 2019. URL Pratyush Maini, Skyler Seto, He Bai, David Grangier, Yizhe Zhang, and Navdeep Jaitly. Rephras-ing the web: A recipe for compute and data-efficient language modeling. arXiv preprintarXiv:2401.16380, 2024. URL Nathan Ng, Kyunghyun Cho, and Marzyeh Ghassemi. SSMBA: self-supervised manifoldbased data augmentation for improving out-of-domain robustness. In Proceedings of the 2020Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online,November 16-20, 2020, pages 12681283. Association for Computational Linguistics, 2020.",
  "Oded Ovadia, Menachem Brief, Moshik Mishaeli, and Oren Elisha. Fine-tuning or retrieval?comparing knowledge injection in llms. arXiv preprint arXiv:2312.05934, 2023. URL": "Ankit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. Medmcqa: A large-scalemulti-subject multi-choice dataset for medical domain question answering. In Gerardo Flores,George H. Chen, Tom J. Pollard, Joyce C. Ho, and Tristan Naumann, editors, Conferenceon Health, Inference, and Learning, CHIL 2022, 7-8 April 2022, Virtual Event, volume 174of Proceedings of Machine Learning Research, pages 248260. PMLR, 2022. URL",
  "Keiran Paster, Marco Dos Santos, Zhangir Azerbayev, and Jimmy Ba. Openwebmath: An opendataset of high-quality mathematical web text. arXiv preprint arXiv:2310.06786, 2023. URL": "Fabio Petroni, Tim Rocktschel, Sebastian Riedel, Patrick S. H. Lewis, Anton Bakhtin, YuxiangWu, and Alexander H. Miller. Language models as knowledge bases? In Kentaro Inui, JingJiang, Vincent Ng, and Xiaojun Wan, editors, Proceedings of the 2019 Conference on EmpiricalMethods in Natural Language Processing and the 9th International Joint Conference on NaturalLanguage Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, pages24632473. Association for Computational Linguistics, 2019. URL",
  "Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Languagemodels are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019": "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100, 000+ questionsfor machine comprehension of text. In Jian Su, Xavier Carreras, and Kevin Duh, editors,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,EMNLP 2016, Austin, Texas, USA, November 1-4, 2016, pages 23832392. The Association forComputational Linguistics, 2016. URL Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham.In-context retrieval-augmented language models.Transac-tions of the Association for Computational Linguistics, 11:13161331, 2023. URL Jeongun Ryu, Jaewoong Shin, Haebeom Lee, and Sung Ju Hwang. Metaperturb: Transferableregularizer for heterogeneous tasks and architectures. In Advances in Neural InformationProcessing Systems 33: Annual Conference on Neural Information Processing Systems 2020,NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL Christopher Sciavolino, Zexuan Zhong, Jinhyuk Lee, and Danqi Chen. Simple entity-centricquestions challenge dense retrievers. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia,and Scott Wen-tau Yih, editors, Proceedings of the 2021 Conference on Empirical Methods inNatural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic,7-11 November, 2021, pages 61386148. Association for Computational Linguistics, 2021.doi: 10.18653/V1/2021.EMNLP-MAIN.496. URL",
  "Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, LukeZettlemoyer, and Wen-tau Yih. REPLUG: retrieval-augmented black-box language models.CoRR, abs/2301.12652, 2023. URL": "Karan Singhal, Shekoofeh Azizi, Tao Tu, S. Sara Mahdavi, Jason Wei, Hyung Won Chung,Nathan Scales, Ajay Kumar Tanwani, Heather Cole-Lewis, Stephen Pfohl, Perry Payne,Martin Seneviratne, Paul Gamble, Chris Kelly, Nathaneal Schrli, Aakanksha Chowdhery,Philip Andrew Mansfield, Blaise Agera y Arcas, Dale R. Webster, Gregory S. Corrado,Yossi Matias, Katherine Chou, Juraj Gottweis, Nenad Tomasev, Yun Liu, Alvin Rajkomar,Joelle K. Barral, Christopher Semturs, Alan Karthikesalingam, and Vivek Natarajan. Largelanguage models encode clinical knowledge. arXiv preprint arXiv:2212.13138, 2022. URL Kai Sun, Yifan Ethan Xu, Hanwen Zha, Yue Liu, and Xin Luna Dong. Head-to-tail: Howknowledgeable are large language models (llm)? A.K.A. will llms replace knowledge graphs?arXiv preprint arXiv:2308.10168, 2023. Jihoon Tack, Jaehyung Kim, Eric Mitchell, Jinwoo Shin, Yee Whye Teh, and Jonathan RichardSchwarz. Online adaptation of language models with a memory of amortized contexts. arXivpreprint arXiv:2403.04317, 2024. URL",
  "Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, PercyLiang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. 2023": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Tim-othe Lacroix, Baptiste Rozire, Naman Goyal, Eric Hambro, Faisal Azhar, Aurlien Ro-driguez, Armand Joulin, Edouard Grave, and Guillaume Lample.Llama: Open and effi-cient foundation language models. arXiv preprint arXiv:2302.13971, 2023. URL Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, LukasBlecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernan-des, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal,Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez,Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux,Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, TodorMihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein,Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Sub-ramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan,Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, SharanNarang, Aurlien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2:Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. URL",
  "Laurens van der Maaten and Geoffrey Hinton.Visualizing data using t-sne.Journal ofMachine Learning Research, 9(86):25792605, 2008. URL": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike vonLuxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and RomanGarnett, editors, Advances in Neural Information Processing Systems 30: Annual Conferenceon Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA,pages 59986008, 2017. Jiexin Wang, Adam Jatowt, and Masatoshi Yoshikawa. Archivalqa: A large-scale benchmarkdataset for open-domain question answering over historical news collections. In Enrique Amig,Pablo Castells, Julio Gonzalo, Ben Carterette, J. Shane Culpepper, and Gabriella Kazai, editors,SIGIR 22: The 45th International ACM SIGIR Conference on Research and Development inInformation Retrieval, Madrid, Spain, July 11 - 15, 2022, pages 30253035. ACM, 2022. URL Jason W. Wei and Kai Zou. EDA: easy data augmentation techniques for boosting performanceon text classification tasks. In Proceedings of the 2019 Conference on Empirical Methods inNatural Language Processing and the 9th International Joint Conference on Natural LanguageProcessing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, pages 63816387.Association for Computational Linguistics, 2019. URL",
  "Qinggang Zhang, Junnan Dong, Hao Chen, Xiao Huang, Daochen Zha, and ZailiangYu. Knowgpt: Black-box knowledge injection for large language models. arXiv preprintarXiv:2312.06185, 2023. URL": "Zhengyan Zhang, Zhiyuan Zeng, Yankai Lin, Huadong Wang, Deming Ye, Chaojun Xiao,Xu Han, Zhiyuan Liu, Peng Li, Maosong Sun, and Jie Zhou. Plug-and-play knowledge injectionfor pre-trained language models. In Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki,editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics(Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pages 1064110658.Association for Computational Linguistics, 2023. URL Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, YonghaoZhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez,and Ion Stoica.Judging llm-as-a-judge with mt-bench and chatbot arena.In Alice Oh,Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, edi-tors, Advances in Neural Information Processing Systems 36: Annual Conference on Neu-ral Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December10 - 16, 2023, 2023. URL Chen Zhu, Yu Cheng, Zhe Gan, Siqi Sun, Tom Goldstein, and Jingjing Liu. Freelb: Enhancedadversarial training for natural language understanding. In 8th International Conference onLearning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenRe-view.net, 2020. URL",
  "ADiscussions & Limitations": "Cost AnalysisOur method requires additional costs compared to the fine-tuning baseline. Specifi-cally, it involves two extra computational costs beyond standard fine-tuning. A comparison of theper-step computational cost (in GFLOPs) between the baseline and our proposed method is shownin , where we consider fine-tuning LLMs with 7B parameters. In detail, one forward passof a 7B parameter LLM requires 13.21 GFLOPs, and one backward pass costs twice as much as aforward pass. The latent paraphraser model we used in the experiments consists of 5 paraphrasers,each with 4 linear layers, totaling 250M parameters, which is 3.6% of the parameter size of the LLM.The total computational costs can vary depending on the hyperparameters (e.g., N in Equation (13))and the size of the dataset used. While training the latent paraphrasers requires an initial cost, this is a one-time expense. Once trained,these can be used repeatedly for knowledge injection without additional ongoing costs. This makesthe overall expense relatively low in the long term. Furthermore, incorporating latent paraphrasersduring fine-tuning adds only a minimal computational overhead, as their parameter size is just 3.6%of the size of LLM. Knowledge RetentionA common drawback of knowledge injection is the potential for LLMsto forget previously learned knowledge . To assess this issue, we used the EntityQuestionsdataset , which contains simple questions about entities. Specifically, we focused on \"place-of-birth\" questions for well-known entities (e.g., \"Where was Leonardo da Vinci born?\"), with 988questions in total. We fine-tune the Vicuna-7b-v1.5 on a synthetic SQuAD document set (DK)using each method, then measure its QA performance on the EntityQuestions. As in , the experimental results show that all fine-tuning approaches negatively impactknowledge retention, as observed in the previous work . Additionally, we observe that improvedknowledge injection often comes at the cost of greater knowledge forgetting. Although our primaryfocus is on enhancing knowledge injection, we acknowledge that addressing knowledge retention iscrucial and should be a focus of future research. Comparison to RAGThe primary advantages of fine-tuning methods, including ours, over retrieval-based approaches like Retrieval-Augmented Generation (RAG) , lie in their simplicity andreduced computational cost on the inference . Fine-tuning results in a self-contained model, whichsimplifies the system architecture by removing the need for additional components like documentretrieval and ranking during inference. This reduction in complexity leads to lower computationaloverhead, especially in terms of GPU memory usage due to the shorter length of the prompt, makingfine-tuning more suitable for an LLM deployment in resource-constrained environments. However, it is important to check the performance gap between them. Therefore, we experiment withRAG on the Events 2024 dataset with Vicuna-7b. For ours, we follow the same experimental settingwith . For RAG, we use the bge-large-en-v1.5 model for document and query embeddingfor retrieval. In , our experimental results indicate that the RAG approach outperformsfine-tuning methods including ours, as previously observed by de Luis Balaguer et al. . However,our LaPael method narrows the gap between the two approaches, suggesting that there is potential forfurther improvements in fine-tuning strategies.",
  "Reversal curse.The proposed method is unable to address the reversal curse, where the LargeLanguage Models (LLMs) trained on A is B\" fail to answer What is B?\" . As outlined in Berglund": "et al. , this phenomenon is mainly due to the format of data and the autoregressive nature of LLMsthat are trained in a way from left to right. Therefore, it is limited to improve the knowledge injectionperformance if the document does not contain a sentence having the reverse relationship, even withour method. Future work will need to explore the combining of our method with a recent solutionfor the reversal curse like reverse training . Otherwise, we can seek a solution that addresses thereversal curse at the latent level similar to LaPael, which can be an interesting direction for futurework. Limited scope of Task and Experiments.The scope of our method remains limited in the knowl-edge injection task. Specifically, there are challenges in applying LaPael for continual pre-trainingon large-scale corpora, such as the 15B OpenWebMath dataset , or for instruction tuning withdatasets like Alpaca . Addressing these challenges will require future work as a new approach fortraining latent paraphrasers tailored to other tasks. In terms of experiments, our experiments onlyfocus on the 7B LLMs, and do not conduct any experiment on larger LLMs of size with 13B or70B due to the limited computational budget for our experiments.",
  "BBroader Impact": "This work explores the knowledge injection in Large Language Models (LLMs), which are highlyrelated to hallucinations . While our method improves the addition of new knowledge to LLMs,it also increases the risk of introducing misinformation. Specifically, our method could enhancethe inaccuracies in LLMs when they are fine-tuned using documents that contain incorrect facts.Therefore, it is crucial to thoroughly check the documents used for fine-tuning LLMs before applyingour method to enhance knowledge injection.",
  "Dtrain1,0001,0001,000------DK1,0006531,0002401,0002,0671,6281,202175DQA1,0006531,0002401,00010,5701,6655,968865": "As briefly mentioned in .1, we generate the synthetic document from each question-answerpair using GPT-4-turbo model . To generate the documents from the question and answer pairs,we use the prompt in . To generate diverse paraphrases from Dtrain, we use the prompt in using GPT-3.5-turbo model. For cross-domain transfer experiments, we also use thesubset of MedMCQA and a synthetic NovelQA dataset based on the Les Misrables Wikipediapage, where we generate the synthetic document for each question. For MedMCQA , we use thesubset of the dataset where the domain of question corresponds to the anatomy. We summarize the statistics of the synthetic dataset used in our experiments in . We also plotthe distributions of token counts in documents, questions, and answers for each dataset used in ourexperiments in . We present the example of each dataset in .",
  "C.2Training Details": "As briefly mentioned in .1, we mainly use Vicuna-7b-v1.5 for fine-tuning. We fine-tuneLLMs for 12 epochs with a learning rate of 0.00005 and step learning rate scheduler where we decaya learning rate by 0.85 by every 4 epochs. For experiments in , we fine-tune for 3 epochswith a decaying period as 1 epoch. For optimizer, we use AdamW . For all experiments, we onlyupdate the parameters corresponding to the MLP layer of transformer . For Llama model ,it corresponds to linear layers named up_proj, gate_proj, and down_proj. We use 4 A100 GPUsfor fine-tuning LLMs. For inference, we use in-context learning with 5 examples by prompting the 5examples in the prompt . For training latent paraphrasers, we train them for 10 epochs with a learning rate of 1e 3 and cosinelearning rate scheduler where we linearly decay a learning rate to 10% of the initial learning ratewithout warmup. We use 5 latent paraphrasers on the 5 sequential early layers of LLMs. For Equa-tion (13), we use N = 4. For Equation (14), we use K = 10. For Equation (15), we set r = 0.5.For gold mask mt, we use a similar method to Agrawal et al. to find the named entities fromeach document using GPT-3.5-turbo. For fine-tuning with latent paraphrases (Equation (17)), we useN = 4. : Data Example. Example data from all datasets we used in experiments. Words in the yellow backgroundindicate the answer to the question. Hypen (-) in the original document column indicates the case where theoriginal document is not accessible.",
  ": Prompt for Synthetic Document Generation. An 1-shot prompt for generating the synthetic documentfrom the question": "Write a concise informative background sentence, that is directly helpful to answer the following question.The background sentence is the sentence that ends with a suffix. In other words, the answer entity should befollowed by the entities used in the question. ### QuestionQuestion: Who replaced Tim Sloan as CEO of Wells Fargo? Answer: Charles Scharf### SuffixCharles Scharf### Background SentenceTim Sloan was succeeded as CEO of Wells Fargo by Charles Scharf.",
  ": Prompt for Paraphrasing. A 2-shot prompt for paraphrasing. y indicates the answer for the questionand x denotes the remaining part of sentence, as introduced in .1": "For the following prefix, give me 2 highly diverse paraphrases of the same in high-quality English language asin sentences on Wikipedia. Ensure that the suffix is followed by a paraphrased prefix. Do not inclue numbering.Maintain the sentence structure.# SentenceIn infrainguinal bypass surgery, the preferred type of graft for optimal outcomes is an Autologous vein.# PrefixIn infrainguinal bypass surgery, the preferred type of graft for optimal outcomes is an# Suffix (PRESERVE AND KEEP LETTER CASE)Autologous vein.# Paraphrases (Prefix + Suffix)In infrainguinal bypass procedures, the graft type most recommended for the best results is an Autologousvein.During infrainguinal bypass operations, the optimal choice for a graft to achieve the best outcomes is anAutologous vein. For the following prefix, give me 2 highly diverse paraphrases of the same in high-quality English languageas in sentences on Wikipedia. Ensure that the suffix is followed by a paraphrased prefix. Do not includenumbering. Maintain the sentence structure.# SentenceDuring the embryonic development of the gastrointestinal tract, proper rotation of the gut is necessary for thecorrect placement of the caecum; an abnormality in this process can lead to Mixed rotation.# PrefixDuring the embryonic development of the gastrointestinal tract, proper rotation of the gut is necessary for thecorrect placement of the caecum; an abnormality in this process can lead to# Suffix (PRESERVE AND KEEP LETTER CASE)Mixed rotation.# Paraphrases (Prefix + Suffix)In the formation of the gastrointestinal system during embryonic growth, it is essential for the gut to rotatecorrectly to ensure the caecum is properly positioned; deviations in this mechanism may result in Mixedrotation.Throughout the development of the gastrointestinal tract in the embryo, the accurate rotation of the gut iscrucial for the appropriate localization of the caecum; any irregularities in this rotation can result in Mixedrotation. For the following prefix, give me 10 highly diverse paraphrases of the same in high-quality English languageas in sentences on Wikipedia. Ensure that the suffix is followed by a paraphrased prefix. Do not includenumbering. Maintain the sentence structure.",
  "D.1Experiments with Other Language Models": "Verifying whether the proposed method can be transferred to other Language Models (LMs) isimportant. First, we validate our LaPael with Llama-2-7B , a non-instruction-tuned version of theVicuna-7B we used in experiments. In , we present the experimental results with Llama-2-7B.The results show that our LaPael is effective even in the LM that is not instruction-tuned. In ,we also present the experimental results with Mistral-7B-Instruct-v0.2, which is an instruction-tunedmodel based on a different LLM Mistral-7B . The results indicate that our LaPael is applicablenot only to Llama-based models but also to LMs with different bases. Furthermore, in , wepresent the experimental results with Phi3-mini-4k-instruction, which is a pre-trained LLM with 3.8billion parameters . The results indicate that our LaPael is highly effective when applied to thePhi3-mini model, which has fewer parameters than other LLMs.",
  "D.2Experiments with Parameter-Efficient Fine-Tuning": "Parameter-efficient fine-tuning is a method that fine-tunes LLMs with minimal updates to theirparameters. It is of interest that our LaPael can be effective even with parameter-efficient fine-tuning.LoRA is a well-known method for parameter-efficient fine-tuning, which updates trainablerank decomposition matrices injected into the parameters of LLMs. In , we present theexperimental results with LoRA on Vicuna-7b-v1.5 where we update only the low-rank matrices ofup_proj, gate_proj, and down_proj layers. The results demonstrate that LaPael is also effectivein LoRA fine-tuning, highlighting its flexible applicability in diverse fine-tuning scenarios.",
  "D.3Visualization of Latent Features": "In , we display the latent features from the final layers of large language models (LLMs)with and without latent paraphrases, where we reduce the dimension using t-SNE . Crosses(x) mark the embeddings from LLMs with latent paraphrasers. As illustrated in , latentparaphrasers enable the generation of diverse data samples, enhancing the diversity compared todata-level paraphrases.",
  "Ours65.8082.1078.8080.0989.4388.0361.7079.2275.24": ": Visualization of Latent Features. We visualize the latent features from the last layers of LLMs using5 randomly sampled data from ArchivalQA dataset. Each color denotes the different data, circles denote theoriginal sentences, triangles denote the paraphrases, diamonds denote the questions, and crosses (x) denote theoriginal sentence with latent paraphrasing."
}