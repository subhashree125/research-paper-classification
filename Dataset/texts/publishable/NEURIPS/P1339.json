{
  "Abstract": "Safe reinforcement learning (RL) requires the agent to finish a given task whileobeying specific constraints. Giving constraints in natural language form hasgreat potential for practical scenarios due to its flexible transfer capability andaccessibility. Previous safe RL methods with natural language constraints typicallyneed to design cost functions manually for each constraint, which requires domainexpertise and lacks flexibility. In this paper, we harness the dual role of text inthis task, using it not only to provide constraint but also as a training signal. Weintroduce the Trajectory-level Textual Constraints Translator (TTCT) to replacethe manually designed cost function. Our empirical results demonstrate that TTCTeffectively comprehends textual constraint and trajectory, and the policies trainedby TTCT can achieve a lower violation rate than the standard cost function. Extrastudies are conducted to demonstrate that the TTCT has zero-shot transfer capabilityto adapt to constraint-shift environments.",
  "Introduction": "In recent years, reinforcement learning (RL) has achieved remarkable success in multiple domains,such as go game and robotic control . However, deploying RL in real-world scenariosstill remains challenging. Many real-world decision-making applications, such as autonomous driving require agents to obey certain constraints while achieving the desired goals. To learn a safeconstrained policy, some safe RL works have proposed methods to maximizethe reward while minimizing the constraint violations after training or during training. However, several limitations prevent the existing safe RL methods widespread use in real-worldapplications. Firstly, these methods often require mathematical or logical definitions of cost functions,which require domain expertise (Limitation 1). Secondly, their cost function definitions are frequentlyspecific to a particular context and cannot be easily generalized to new tasks with similar constraints(Limitation 2). Lastly, most current safe RL methods focus on constraints that are logically simple,typically involving only one single entity or one single state , which cant represent the real-worldsafety requirements and lack universality (Limitation 3). Using natural language to provide constraints is a promising approach to overcomeLimitation 1 and 2 because natural language allows for flexible, high-level expression of constraintsthat can easily adapt to different scenarios. Regarding Limitation 3, previous approaches primarilyemploy what we call the single state/entity textual constraint. The single-state/entity textual con-straint focuses solely on constraints related to one specific state or entity, limiting the ability to modelcomplex safety requirements in real-world scenarios. Many safety requirements involve interactionsand dependencies among multiple states or entities over time. By only addressing a single state or",
  "Dont step in the gassed area. (single state)Dont stay in the gas area for more than 5 min-utes, your gas mask will fail. (multi states)": "entity, these constraints fail to capture the dynamic relationships and temporal aspects that are crucialfor ensuring safety in complex environments. So we suggest using a more generalizable constrainttype trajectory-level textual constraint. The trajectory-level textual constraint is a more universalconstraint with complex logic semantics involving multiple states/entities. It is a nonlinear combina-tion of multiple entities or multiple environment states and can model any constraint requirements inreal-world scenarios. The trajectory-level constraints in natural language form are the highly abstractexpression of the agents behavior guidelines, serving as a more natural and straightforward way tointroduce constraints. Notably, the set of trajectory-level constraints encompasses single-state con-straints, as any single-state constraint can be reformulated as an equivalent trajectory-level constraint.Examples of trajectory-level textual constraint and single state/entity textual constraint are presentedin . Employing trajectory-level textual constraints across the entire trajectory poses two significantchallenges. Firstly, determining whether an RL agent violates textual constraint over a trajectoryis non-trivial, it needs to have the perception of the historical states and actions observed along thetrajectory (Challenge 1). Secondly, the trajectory-level safety problem is susceptible to sparse cost, where cost is only imposed when the agent violates textual constraints at the final time step,making it challenging for the agent to learn which actions contribute to a gradual escalation of risk(Challenge 2). For instance, the agent needs to learn constraints like, \"Dont touch lava after youtouch grass.\" Without intermediate feedback, however, it struggles to understand how early actions,such as stepping on grass, contribute to eventual violations. To address Challenge 1, we propose a general approach to align the trajectorys factual logic with thetexts semantic logic, eliminating the need for manual encoding or separate models for each type ofconstraint. Our method employs a sequence model for modeling agent historical interactionswith the environment, and a pre-trained language model (LM) to comprehend natural languageconstraints. We then maximize the embedding similarities between matching pairs (trajectory, text)and minimize the embedding similarities between non-matching pairs using a contrastive learningapproach similar to CLIP . By calculating the similarity between the textual embeddings ofthe constraint and the trajectory, we can predict whether a constraint is violated in this trajectory. Ourmethod uniquely leverages text as both a source of constraints and a unified supervisory signal fortrajectory encoding. In this dual role, text not only provides constraints but also guides the trainingprocess, enabling the model to naturally handle diverse semantic constraints without requiring specificmodel adjustments for each type. This design allows for a more flexible and generalizable system,significantly simplifying the handling of complex, multi-dimensional constraints. In addition, to address the issue of cost sparsity (Challenge 2), we introduce a method for temporalcredit assignment . The proposed approach involves decomposing the one episodic cost of thetextual constraint into multiple parts and allocating them to each state-action pair within the trajectory.This method offers denser cost signals regarding the relationship between the textual constraint andthe agents every action. It informs the agent which behaviors are risky and which are safer, therebyenhancing safety and aiding model performance. Our experiments demonstrate that the proposed method can effectively address Challenge 1 andChallenge 2. In both 3D navigation and 2D grid exploration tasks, agents trained usingour method achieve significantly lower violation rates (up to 4.0x) compared to agents trained withground-truth cost functions while maintaining comparable rewards and more importantly, our methodobtains the Pareto frontier . In addition to this, our method has zero-shot adaptation capability toadapt to constraint-shift environments without fine-tuning.",
  "Related Work": "Safe RL. Safe RL aims to train policies that maximize reward while minimizing constraint violations. In prior work, there are usually two ways to learn safe policies: (1) consider cost as one ofthe optimization objectives to achieve safety , and (2) achieve safety by leveragingexternal knowledge (e.g. expert demonstration) . These typical safe RL algorithmsrequire either human-defined cost functions or human-specified cost constraints which are unavailablein the tasks that constraints are given by natural language. RL with Natural Language. Prior works have integrated natural language into RL to improvegeneralization or learning efficiency in various ways. For example, Hermann et al. studied howto train an agent that can follow natural language instructions to reach a specific goal. Additionally,natural language has been used to constrain agents to behave safely. For instance, Prakash et al. trained a constraint checker to predict whether natural language constraints are violated. Yang etal. trained a constraint interpreter to predict which entities in the environment may be relevantto the constraint and used the interpreter to predict costs. Lou et al. used pre-trained languagemodels to predict the cost of specific states, avoiding the need for artificially designed cost functions.However, previous methods cannot uniformly handle textual constraints with one framework, whichlimits their applicability. Credit Assignment in RL. Credit assignment studies the problem of inferring the true reward fromthe designed reward. Prior works have studied improving sample efficiency of RL algorithms throughcredit assignment, for example by using information gain , as an intrinsic bonus reward to aidexploration. Goyal et al. proposed the use of natural language instructions to perform rewardshaping to improve the sample efficiency of RL algorithms. Liu et al. learned to decompose theepisodic return as the reward for policy optimization. However, to the best of our knowledge, ourwork is the first to apply credit assignment to safe RL.",
  "Preliminaries": "Problem formulation. Trajectory-level constraint problem can be formed as the ConstrainedNon-Markov Decision Process (CNMDP) , and it can be defined by the tuple<S,A,T,R,,C,Y, >. Here S represents the set of states, A represents the set of actions, Trepresents the state transition function, R represents the reward function, and (0,1) representsthe discount factor. In addition, Y represents the set of trajectory-level textual constraints (e.g., Youhave 10 HP, you will lose 3 HP every time you touch the lava, dont die.), which describes theconstraint that the agent needs to obey across the entire trajectory. C represents the cost functiondetermined by y Y . represents the set of historical trajectories. RL with constraints. The objective for the agent is to maximize reward while obeying the specifiedtextual constraint as much as possible . Thus, in our task setting, the agent needs to learn a policy: S Y P(A) which maps from the state space S, textual constraints Y and historicaltrajectories to the distributions over actions A. Given a y, we learn a policy that maximizes thecumulative discounted reward JR while keeping the cumulative discounted cost (average violationrate) JC below a constraint violation budget BC(y):",
  "Here BC(y) and C(st,at,y,t) are two functions both depending on textual constraint y. trepresents the historical trajectory at time step t": "Episodic RL. Similar to the task with episodic rewards , in our task setting, a cost is only givenat the end of each trajectory when the agent violates the textual constraint y. In other words, beforeviolating y, the cost C(st,at,y,t) = 0 for all t < T. For simplicity, we omit the discount factorand assume that the trajectory length is at most T so that we can denote aT as the final action thatcauses the agent to violate y without further confusion. Therefore, the constraint qualification ofthe objective in RL with constraints becomes JC() = E[C(sT ,aT ,y,T )] BC(y). Due to thesparsity of cost, a large amount of rollout trajectories are needed to help the agent distinguish thesubtle effects of actions on textual constraint . This situation will become more serious whentrajectory-level constraints are complex and difficult to understand.",
  "concatLH": ": TTCT overview. TTCT consists of two training components: (1) the text-trajectoryalignment component connects trajectory to text with multimodal architecture, and (2) the costassignment component assigns a cost value to each state-action based on its impact on satisfyingthe constraint. When training RL policy, the text-trajectory alignment component is used to predictwhether a trajectory violates a given constraint and the cost assignment component is used to assignnon-violation cost.",
  "TTCT: Trajectory-level Textual Constraints Translator": "In this section, we introduce our proposed framework TTCT ( Trajectory-level Textual ConstraintsTranslator) as shown in . TTCT has two key components: the text-trajectory alignmentcomponent and the cost assignment component. The text-trajectory alignment component is used toaddress the violations prediction problem. The cost assignment component is used to address thesparse cost problem.",
  "Text-Trajectory Alignment Component": "We propose a component to learn from offline data to predict whether a given trajectory violatestextual constraints. The core idea of this component is to learn trajectory representations under textualsupervision and connect trajectory representation to text representation. If the distance betweenthe two representations in the embedding space is sufficiently close, we can consider that the giventrajectory violates the constraint. Our approach does not require modeling entities of the environmentlike previous work, such as , which involves labeling hazardous items artificially in everyobservation. Instead, we model this task as a trajectory-text multimodal learning problem. Hence,our method can learn trajectory representations and text representations from the pairs (trajectory,trajectory-level textual constraint). We believe that learning from the supervision of natural languagecould not only enhance the representation power but also enable flexible zero-shot transfer . Formally, given a batch of N (trajectory , trajectory-level textual constraint y) pairs. For eachpair, the trajectory corresponds to the text, indicating that the given trajectory violates the giventextual constraint. The trajectory can be defined as = (s1,a1,s2,a2,...,sT 1,aT 1,sT ,aT ), whereT is the step at which the textual constraint y is first violated by the trajectory. Here st is a ds-dimensional observation vector. Each state in the trajectory is processed by a state encoder to obtaina representation vst , also action is processed by an action encoder to obtain a representation vat . Then,we concatenate vst and vat to obtain a vector representation vt for each state-action pair. After that,we learn separate unimodal encoders gT and gC for the trajectory and textual constraint, respectively.The trajectory encoder gT utilizes a causal transformer to extract the trajectory representation fromthe input state-action representation sequence {vt}Tt=1:",
  "H1,H2,H3,...,HT 1,HT = gT ({vt}Tt=1),(2)": "where Ht is a dH-dimensional vector. The final embedding HT is used as the representation forthe entire trajectory. Specifically, the causal Transformer processes the trajectory sequence bymaintaining a left-to-right context while generating embeddings. This allows the model to capturetemporal dependencies within the trajectory and obtain the embeddings for time steps before T. Thetextual constraint encoder gC is used to extract features that are related to the constraints and it couldbe one of a wide variety of language models:",
  "Nj=1 exp(simT (y,j)).(5)": "Let qy(),qy(y) indicate the ground-truth similarity scores, where the negative pair (trajectorydoesnt violate textual constraint) has a probability of 0 and the positive pair (trajectory violatetextual constraint) has a probability of 1. In our task setting, a trajectory can correspond to multipletextual constraints, and vice versa. For example, two textual constraints such as Do not touch lavaand After stepping on water, do not touch lava might both be violated by a single given trajectory.This many-to-many relationship between trajectories and textual constraints implies that the sametextual constraints (or different textual constraints with the same semantics) can apply to multipletrajectories, while a single trajectory may comply with several textual constraints. So there may bemore than one positive pair in qyi() and qyi(y). Therefore, we use KullbackLeibler (KL)divergence as the multimodal contrastive (MC) loss to optimize our encoder similar to :",
  "where D is the training set": "In addition to the multimodal contrastive (MC) loss, we also introduce a within-trajectory (WT) loss.Specifically, suppose we have a trajectorys representation sequence (H1,H2,H3,...,H(T 1),HT )and its corresponding textual constraint embedding L, we can calculate cosine similarity simt(,y)between embedding Ht and L using Equation 4. Then we can calculate similarity scores within thetrajectory:",
  "Tk=1 exp(simk(,y)).(7)": "Different from pyi(y) in Equation 5, which measures similarity scores across N trajectories,Equation 7 is used to measure the similarity scores of different time steps within a trajectory. Thereason for doing this is that the textual constraint is violated at time step T, while in the previous timesteps the constraint is not violated. Therefore, the instinct is to maximize the similarity score betweenthe final trajectory embedding HT and the textual constraint embedding L, while minimizing thesimilarity score between all previous time step embeddings (H1,H2,H3,...,H(T 1)) and the textualconstraint embedding L. Based on this instinct, we introduce within-trajectory (WT) loss:",
  "T (T 1t=1log(1 pt (y)) + log(pT (y)))],(8)": "where the first term is responsible for minimizing the similarity score for the embedding of timesteps before T in the trajectory sequence, while the second term is responsible for maximizing thesimilarity score for the embedding of time step T. By combining these two losses LW T ,LMC, we can train a text encoder to minimize the distancebetween embeddings of semantically similar texts, while simultaneously training a trajectory encoderto minimize the distance between embeddings of semantically similar trajectories. Crucially, thisapproach enables us to align the text and trajectory embeddings that correspond to the same semanticconstraint, fostering a cohesive representation of their shared meaning, and further determiningwhether the trajectory violates the constraint by calculating embedding similarity.",
  "a component to capture the relationship between the state-action pair and the textual constraint andassign a cost value to each state-action based on its impact on satisfying the constraint": "Specifically, suppose we have a (trajectory , textual constraint y) pair and its representation ({Ht}Tt=1,L) obtained from text-trajectory alignment component. The textual constraint representation Lis processed by an episodic-cost prediction layer F e to obtain a predicted episodic cost C(y) =sigmoid(F e(L)) for the entire trajectory. We expect the episodic cost can be considered as the sum ofcost on all non-violation state-action pairs: C(y) = T 1t=1 c(st,at,y,t). To evaluate the significanceof each timesteps action relative to textual constraints, we employ an attention mechanism:",
  "et = sigmoid(simt(,y)).(9)": "Here we regard the text representation as the query, each time steps representation in the trajectoryas the key, and compute the attention score et based on the cosine similarity metric. After that, weuse the sigmoid function to make sure the score falls within the range of 0 to 1. Each attention scoreet quantifies the degree of influence of the state-action pair (st,at) on violating the textual constraint.Then we obtain an influence-based representation Ht = etHt. To predict the cost c(st,at,y,t),we incorporate a feed-forward layer called the cost assignment layer F c and output the predictednon-violation single step cost as:",
  "LCA = E(,y)D[(T 1t=1c(st,at,y,t) C(y))2].(11)": "This mutual prediction loss function LCA is only used to update the episodic-cost prediction layerand cost assignment layer, and not to update the parameters of the trajectory encoder or text encoderduring backpropagation. This helps ensure the validity of the predictions by preventing overfitting orinterference from other parts of the model. The effectiveness of this component comes from two main sources. First, the text-trajectory alignmentcomponent projects semantically similar text representations to nearby points in the embedding space,allowing the episodic-cost prediction layer to assign similar values to embeddings with close distances.This aligns with the intuition that textual constraints with comparable violation difficulty should yieldsimilar episodic costs. Second, the cost assignment layer leverages the representational power of thetext-trajectory alignment component to capture complex relationships between state-action pairs andconstraints, enabling accurate single-step cost predictions.",
  "LT T CT = LMC + LW T + LCA.(12)": "By doing this, we can avoid the need for separate pre-training or fine-tuning steps, which can be time-consuming and require additional hyperparameter tuning. Also, this can enable the cost assignmentcomponent to gradually learn from the text-trajectory alignment component and make more accuratepredictions over time. In the test phase, at time step t we encode trajectory t and textual constraint y with Equation 3 andEquation 2 to obtain the entire trajectory embedding Ht and text embedding L. Then we calculatedistance score sim(t,y) using Equation 4. The predicted cost function c is given by:",
  "Policy Training": "Our Trajectory-level textual constraints Translator framework is a general method for integratingfree-form natural language into safe RL algorithms. In this section, we introduce how to integrateour TTCT into safe RL algorithms so that the agents can maximize rewards while avoiding earlytermination of the environment due to violation of textual constraints. To enable perception ofhistorical trajectory, the trajectory encoder and text encoder are not only used as frozen plugins gTand gC for cost prediction but also as trainable sequence models gT and gC for modeling historicaltrajectory. This allows the agent to take into account historical context when making decisions. Tofurther improve the ability to capture relevant information from the environment, we use LoRA to fine-tune both the gT and gC during policy training. The usage of gT , gC and gT , gC is illustratedin Appendix A.4 . Formally, lets assume we have a policy with parameter to gather transitions from environments.We maintain a vector to record the history state-action pairs sequence, and at time step t we use gTand gC to encode t1 and textual constraint y so that we can get historical context representationHt1 and textual constraint representation L. The policy selects an action at = (ot,Ht1,L) tointeract with environment to get a new observation ot+1. And we update t with the new state-actionpair (ot,at) to get t. With t and L, ct can be predicted according to Equation 13. Then we storethe transition into the buffer and keep interacting until the buffer is full. In the policy updating phase,after calculating the specific loss function for different safe RL algorithms, we update the policy with gradient descent and update gT , gC with LoRA. It is worth noting that gT and gC are not updatedduring the whole policy training phase, as they are only used for cost prediction. The pseudo-codeand more details of the policy training can be found in Appendix A.4.",
  "Experiments": "Our experiments aim to answer the following questions: (1) Can our TTCT accurately recognizewhether an agent violates the trajectory-level textual constraints? (2) Does the policy network, trainedwith predicted cost from TTCT, achieve fewer constraint violations than trained with the ground-truthcost function? (3) How much performance improvement can the cost assignment (CA) componentachieve? (4) Does our TTCT have zero-shot capability to be directly applicable to constraint-shiftenvironments without any fine-tuning? We adopt the following experiment setting to address thesequestions.",
  "(c) LavaWall": ": (a) One layout in Hazard-World-Grid , where orange tiles are lava, blue tiles arewater and green tiles are grass. Agents need to collect reward objects in the grid while avoidingviolating our designed textual constraint for the entire episode. (b) Robot navigation task SafetyGoalthat is built in Safety-Gymnasium , where there are multiple types of objects in the environment.Agents need to reach the goal while avoiding violating our designed textual constraint for the entireepisode. (c) LavaWall , a task has the same goal but different hazard objects compared toHazard-World-Grid.",
  "Setup": "Task. We evaluate TTCT on two tasks ( (a,b)): 2D grid exploration task Hazard-World-Grid (Grid) and 3D robot navigation task SafetyGoal (Goal) . And we designed over 200trajectory-level textual constraints which can be grouped into 4 categories, to constrain the agents.A detailed description of the categories of constraints will be given in Appendix A.1. Different from the default setting, in our task setting, when a trajectory-level textual constraint is violated, theenvironment is immediately terminated. This is a more difficult setup than the default. In this setup,the agents must collect as many rewards as possible while staying alive. Baselines.We consider the following baselines: PPO , PPO_Lagrangian(PPO_Lag) ,CPPO_PID , FOCOPS . PPO does not consider constraints and simply aims to maximize theaverage reward. We use PPO to compare the ability of our methods to obtain rewards. As for the lastthree algorithms, we design two training modes for them. One is trained with standard ground-truthcost, where the cost is given by the human-designed violation checking functions, and we call itground-truth (GC) mode. The other is trained with the predicted cost by our proposed TTCT, whichwe refer to as cost prediction (CP) mode. More information about the baselines and training modescan be found in Appendix A.1 A.2. Metrics. We take average episodic reward (Avg. R) and average episodic cost (Avg. C) as the maincomparison metrics. Average episodic cost can also be considered as the average probability ofviolating the constraints. The higher Avg. R, the better performance, and the lower Avg. C, the betterperformance. PPO_Lag CPPO_PID FOCOPS 0.0 0.5 1.0 1.5 2.0 2.5 Avg. R 2.7 2.01 1.52 2.71 2.0 1.53 2.55 PPOCP(our)GC PPO_Lag CPPO_PID FOCOPS 0.0 0.2 0.4 0.6 0.8 Avg. C 0.280.28 0.48 0.61 0.4 0.58 0.88PPOCP(our)GC",
  "Main Results and Analysis": "The evaluation results are shown in and the learning curves are shown in . We canobserve that in the Hazard-World-Grid task, compared with PPO, the policies trained with GC canreduce the probability of violating textual constraints to some extent, but not significantly. This isbecause the sparsity of the cost makes it difficult for an agent to learn the relevance of the behaviorto the textual constraints, further making it difficult to find risk-avoiding paths of action. In themore difficult SafetyGoal task, it is even more challenging for GC-trained agents to learn how toavoid violations. In the CPPO_PID and FOCOPS algorithms trained with GC mode, the probabilityof violations even rises gradually as the training progresses. In contrast, the agents trained withpredicted cost can achieve lower violation probabilities than GC-trained agents across all algorithmsand tasks and get rewards close to GC-trained agents. These results show that TTCT can give an accurate predicted episodic cost at the time step whenthe constraint is violated, it can also give timely cost feedback to non-violation actions throughthe cost assignment component so that the agents can find more risk-averse action paths. Andthese results answer questions (1) and (2). The discussion about the violation prediction capability ofthe text-trajectory component can be found in Appendix B.1. The interpretability and case study ofthe cost assignment component can be found in Appendix B.2.",
  "Ablation Study": "To study the influence of the cost assignment component. We conduct an ablation study by removingthe cost assignment component from the full TTCT. The results of the ablation study experiment areshown in . We can observe that even TTCT without cost assignment can achieve similarperformance as GC mode. And in most of the results if we remove the cost assignment component,the performance drops. This shows that our text trajectory alignment component can accuratelypredict the ground truth cost, and the use of the cost assignment component can further helpus learn a safer agent. These results answer questions (3).",
  "PPO_Lag": ": Learning curve of our proposed method TTCT. Each column is an algorithm. The sixfigures on the left show the results of experiments on the Hazard-World-Grid task and the six figureson the right show the results of experiments on the SafetyGoal task. The solid line is the mean value,and the light shade represents the area within one standard deviation. PPO_Lag CPPO_PID FOCOPS 0.0 0.5 1.0 1.5 2.0 2.5 Avg. R 2.7 2.01 1.52 2.57 2.0 0.79 2.55 PPOCP(Full)CP w/o CA PPO_Lag CPPO_PID FOCOPS 0.0 0.2 0.4 0.6 0.8 Avg. C 0.280.28 0.48 0.560.52 0.43 0.88PPOCP(Full)CP w/o CA",
  "(b) SafetyGoal": ": Ablation study of removing the cost assignment (CA) component. The blue bars arecost prediction (CP) mode performance with full TTCT and the orange bars is the cost prediction(CP) mode performance without CA component. The black dashed lines are PPO performance. (a)Ablation results on Hazard-World-Grid task. (b) Ablation results in SafetyGoal task.",
  "Further Results": "Pareto frontier. Multi-objective optimization typically involves finding the best trade-offs betweenmultiple objectives. In this context, it is important to evaluate the performance of different methodsbased on their Pareto frontier , which represents the set of optimal trade-offs between the rewardand cost objectives. We plot the Pareto frontier of policies trained with GC and policies trained withCP on a two-dimensional graph, with the vertical axis representing the reward objective and thehorizontal axis representing the cost objective as presented in . The solution that has thePareto frontier closer to the origin is generally considered more effective than those that have thePareto frontier farther from the origin. We can observe from the figure that the policies trained withpredicted cost by our TTCT have a Pareto frontier closer to the origin. This proves the effectivenessof our method and further answers Questions (1) and (2).",
  "(c) FOCOPS": ": Results of Pareto frontiers. We compare the performance of 200 policies trained usingcost prediction (CP) and 200 policies trained with ground-truth cost (GC). The symbol representsthe policy on the Pareto frontier. And we connect the Pareto-optimal policies with a curve. Zero-shot transfer capability. To explore whether our method has zero-shot transfer capability,we use the TTCT trained under the Hazard-World-Grid environment to apply directly to a newenvironment called LavaWall ( (c)) , without fine-tuning. The results are shown in . We can observe that the policy trained with cost prediction (CP) from TTCT trained under theHazard-World-Grid environment can still achieve a low violation rate comparable to the GC-trainedpolicy. This answers Question (4). epoch",
  "Conclusion and Future Work": "In this paper, we study the problem of safe RL with trajectory-level natural language constraints andpropose a method of trajectory-level textual constraints translator (TTCT) to translate constraints intoa cost function. By combining the text-trajectory alignment (CA) component and the cost assignment(CA) component, our method can elegantly solve the problems of predicting constraint violationsand cost sparsity. We demonstrated that our TTCT method achieves a lower violation probabilitycompared to the standard cost function. Thanks to its powerful multimodal representation capabilities,our method also has zero-shot transfer capability to help the agent safely explore the constraint-shiftenvironment. This work opens up new possibilities for training agents in safe RL tasks with totalfree-form and complex textual constraints. Our work still has room for improvement. The violation rate of our method is not absolute zero.In future work, we plan to investigate the application of TTCT in more complex environmentsand explore the integration of other techniques such as meta-learning to further improve theperformance and generalization capabilities of our method. This work was supported by the grants from the Natural Science Foundation of China (62225202,62202029), and Young Elite Scientists Sponsorship Program by CAST (No. 2023QNRC001). Thanksfor the computing infrastructure provided by Beijing Advanced Innovation Center for Big Data andBrain Computing. This work was also sponsored by CAAI-Huawei MindSpore Open Fund. JianxinLi is the corresponding author.",
  "B. Prakash, N. R. Waytowich, A. Ganesan, T. Oates, and T. Mohsenin, Guiding safe rein-forcement learning policies using structured language constraints. in SafeAI@ AAAI, 2020, pp.153161": "T.-Y. Yang, M. Y. Hu, Y. Chow, P. J. Ramadge, and K. Narasimhan, Safe reinforcement learningwith natural language constraints, Advances in Neural Information Processing Systems, vol. 34,pp. 13 79413 808, 2021. M. Riedmiller, R. Hafner, T. Lampe, M. Neunert, J. Degrave, T. Wiele, V. Mnih, N. Heess,and J. T. Springenberg, Learning by playing solving sparse reward tasks from scratch, inInternational conference on machine learning.PMLR, 2018, pp. 43444353.",
  "J. Garca and F. Fernndez, A comprehensive survey on safe reinforcement learning, Journalof Machine Learning Research, vol. 16, no. 1, pp. 14371480, 2015": "S. Ross, G. Gordon, and D. Bagnell, A reduction of imitation learning and structured predictionto no-regret online learning, in Proceedings of the fourteenth international conference onartificial intelligence and statistics.JMLR Workshop and Conference Proceedings, 2011, pp.627635. A. Rajeswaran, V. Kumar, A. Gupta, G. Vezzani, J. Schulman, E. Todorov, and S. Levine, Learn-ing complex dexterous manipulation with deep reinforcement learning and demonstrations,arXiv preprint arXiv:1709.10087, 2017.",
  "A.1Dataset": "In our task setting, humans need to provide high-level textual instruction to the agent for the entiretrajectory, and then TTCT can predict the cost based on the real-time state of the agents explorationso that the agent can learn a safe policy with the predicted cost. Thus our dataset is comprised of twoparts: the trajectory-level textual constraints and the environments. Trajectory-level textual constraints: To generate textual constraints, we first explore the environ-ment using a random policy, collecting a large amount of offline trajectory data. Then we design adescriptor that automatically analyzes trajectories and gives natural language descriptions based onpredefined templates. To validate whether our method can understand different difficulty levels oftextual constraints, we design four types of trajectory-level textual constraints. The four types oftextual constraints are: 1. Quantitative textual constraint describes a quantitative relationship in which an entity in theenvironment cannot be touched beyond a specific number of times, which can be interpretedas the entitys tolerance threshold, and when the threshold is exceeded, the entity mayexperience irrecoverable damage. 2. Sequential textual constraint describes a sequence-based relationship, where the occurrenceof two or more distinct actions independently may not pose a risk, but when they occur insequence, it does. For instance, its safe to drink or drive when they happen independently,but when they occur in sequence (i.e., drinking first and then driving), it becomes dangerous. 3. Relational textual constraint describes constraints on the relationships between an agentand entities in its environment, such as maintaining a certain distance, always being in frontof that entity, or not staying too far from it. 4. Mathematical textual constraints often do not provide explicit instructions to the agentregarding what actions to avoid, but rather present logical descriptions that demand themodels ability to reason mathematically. This type of constraint thereby presents a highercognitive burden for our TTCT to comprehend. Examples of four types of trajectory-level textual constraints are included in . Then werandomly split the (trajectory, textual constraint) pairs into 80% training and 20% test sets. And weuse the training set to train our TTCT end-to-end.",
  "Environments: We use two environments Hazard-World-Grid and SafetyGoal as main benchmarksand a environment LavaWall to evaluate the zero-shot transfer capability:": "1. Hazard-World-Grid. The environment is a 12 12 grid, with the gray walls surroundingthe perimeter. The agent can only explore within the grid. Inside the grid, some itemsprovide rewards: blue keys, red balls, and yellow boxes. Collecting all of these items willbe considered as completing the task. Additionally, there are hazardous materials in thegrid, where orange tiles are lava, cyan tiles are water, and green tiles are grass. Duringexploration, the agent can only see a range of 7 7 pixels ahead, resulting in an observationspace with size 7 7 3. 2. SafetyGoal. Robot navigation task, the environment has a navigation target, which iscompleted when the robot reaches the target. The environment also contains vases (cyansquares) and Hazards (Mediumslateblue circles). The vases can be moved by the robot andhazards are fixed. 3. LavaWall. The LavaWall environment shares the same task goal as Hazard-World-Grid butwith different hazardous materials. Whenever the environment is initialized, a random lavawall with only one entrance is generated in the grid, and the agent must learn to obtain thereward on the other side while avoiding stepping on lava. For each episode, we place the agent at a randomized start location, fill the environment with objects,and randomly select a textual constraint from the constraint pool. We manually design the violationchecking function for every constraint to determine whether the environment violates the textualconstraint, and when the textual constraint is violated, the environment will provide a cost with a",
  "Mathematical": "You only have 20 HP, grass and lava are very dangerous, you will lose3 and 2 HP respectively if you step on them, please dont die!You only have 25 HP, stepping on grass and lava will lose 3 and 2 HPrespectively, but water will help you regain 1 HP, please dont die!You only have 10 HP, and stepping on dangerous water and lava willcause you to lose 3 and 2 HP respectively, your fate hangs by a thread.dont let it snap!",
  "A.2Baselines": "(1) PPO : This algorithm does not consider constraints, and simply considers maximizing theaverage reward, which we use to compare the ability of our methods to obtain rewards.(2) PPO_Lagrangian(PPO_Lag) : This algorithm transforms a constrained optimization probleminto an unconstrained optimization problem via Lagrange multipliers(3) CPPO_PID : This algorithm PID to control Lagrange multiplier, which solves the cyclefluctuation problem of PPO-Lagrange.(4) FOCOPS : This algorithm finds the optimal update policy by solving a constrained optimizationproblem in the nonparameterized policy space, then projects the update policy back into theparametric policy space.",
  "A.4Policy Training Detail": "We provide pseudocode of training policy with predicted from TTCT in Algorithm 1. To enablethe agent to comply with diverse types of constraints, we launch async vectorized environmentswith different types of constrained environments during roll-out collection. The agent interacts withthese environments and collects transitions under different constraint restrictions to update its policy.During policy updates, we fine-tune the trajectory encoder gH and text encoder gL using LoRA at every epochs first iteration, while not updating encoder parameters in other iterations, which savestraining time.",
  ": Heatmap of cosine similarity between trajectory and text embeddings": "To further study the ability of our text-trajectory alignment component to predict violations, weconduct an experiment given a batch of trajectory-text pairs and we use the text-trajectory alignmentcomponent to encode the trajectory and textual constraint, and then calculate the cosine distancewith Equation 5 between every two embeddings across two modal. We plot a sample of heatmapof calculated cosine similarity and ground-truth as presented in 10. Further, We plot the receiveroperating characteristic (ROC) curve to evaluate the performance of the text-trajectory alignmentcomponent as presented in . AUC (Area Under the Curve) values indicate the areaunder the ROC curve. The AUC value of our violations prediction result is 0.98. Then We setthreshold equal to the best cutoff value of the ROC curve. We determine whether the trajectoryviolates a given textual constraint by:",
  "B.2Case Study of Cost Assignment (CA) Component": "We visualize the assigned cost for every state-action pair to demonstrate that the cost assignmentcomponent could capture the subtle relation between the state-action pair and textual constraint.Our intuition is that we should assign larger costs to state-action pairs that lead to violations oftrajectory-level textual constraints, and smaller or negative values to pairs that do not contribute toconstraint violations. Using the Hazard-World-Grid environment as an example, we choose threedifferent types of constraints to show our results in . The first row shows the textualconstraint, the second row shows the trajectory of the agent in the environment, and to make it easierto visualize, we simplify the observation st by representing it as a square, denoting the entity steppedon by the agent at time step t. The square emphasized by the red line indicates the final entity thatmakes the agent violate textual constraint at time step T. The third row shows the predicted cost ofthe agent at every time step t and deeper colors indicate larger cost values. The (a) constraint is mathematical textual constraint: You only have 20 HP. Lava and grassare dangerous, they will make you lose 3 and 2 HP, respectively. However, water can regenerate1 HP. Please dont die.. This constraint describes the two dangerous entities lava and grass, and 0.00.20.40.60.81.0",
  "Receiver Operating Characteristic": "ROC curve (area = 0.98) : ROC curve of text-trajectory alignment component. The x-axis represents the falsepositive rate, and the y-axis represents the true positive rate. The closer the AUC value is to 1, thebetter the performance of the model; conversely, the closer the AUC value is to 0, the worse theperformance of the model.",
  "= floor": ": Case study of cost assignment component on three types of textual constraints. Thefirst row of every case shows the textual constraint, the second row shows the trajectory of the agentin the environment and each square represents the object stepped on by the agent at that time step, thethird row shows the assigned cost of the agent at each time step, and the fourth row shows the timesteps. The red line indicates the final observation where the agent violates the textual constraint. the beneficial entity water. From the third-row heat map, we can observe that our cost assignmentcomponent assigns a high cost to the action that steps on lava or grass, with the cost increasing as theagent approaches the constraint-violating situation. Not only that, the CA component also recognizesthe different levels of danger posed by lava and grass. Since stepping on lava will deduct 3 HP whilestepping on grass will deduct 2 HP, the CA component assigns a larger cost value at time step 13 14compared to the cost value at time step 3 5. The (b) constraint is quantitative textual constraint: Lava is dangerous. Dont touch them morethan eight times!. When stepping on the floor, the CA component considers these actions to besafe and assigns a cost of nearly 0. However, when the agent steps onto lava, it assigns a highercost, especially when the agent steps on lava for the eighth time. At this point, our CA componentconcludes that the situation has become extremely dangerous, and one more step on lava will violatethe constraint, thus giving the highest cost compared to the before time steps. The (c) constraint is sequential textual constraint: After you touch lava, dont step on grass!.Our CA component captures two relevant entities: lava and grass, and understands the sequentialrelationship between entities. When the agent first steps onto the grass, the text-trajectory alignment component determines that this action does not violate the textual constraint. However, after theagent steps onto lava and then steps onto the grass for the second time, the text-trajectory alignmentcomponent detects the cosine similarity sim(,y) greater than the threshold , thereby violating thetextual constraint. And CA component captures that the key trigger condition for violating constraintis stepping onto the lava, therefore assigning a relatively larger cost to such actions at time step7 9. The cost assignment component also assigns relatively small costs to some safe actions thatare stepping on the floor, such as steps 13 15. This is because it detects that the agent, althoughhasnt stepped on grass yet, is trending towards approaching grass, which is a hazardous trend, thusproviding a series of gradually increasing and small costs. This demonstrates that our component notonly monitors key actions that lead to constraint violations but also monitors the hazardous trend andnudging the agent to choose relatively safer paths.",
  "B.3Results for Different Types of Constraints": "To evaluate the agents understanding of different types of trajectory-level textual constraints, weconducted an additional experiment using the CPPO_PID algorithm. During training, we separatelytracked the average episodic reward and the average episodic cost for three types of textual constraintsas presented in . From the learning curves, we can observe that for every type of constraint,our CP mode can achieve the lowest violation rate compared to CP without the CA component modeand ground-truth cost (GC) mode. epoch 0.5 1.0 1.5 2.0 2.5",
  "B.4Inference time": "We perform the trajectory length sensitivity analysis on Hazard-World-Grid. Since our framework ismainly used for reinforcement learning policy training where data is typically provided as input inbatches, we counted the inference time for different trajectory lengths with 64 as the batch size, usingthe hardware device V100-32G. shows that the average inference time per trajectory is10ms for trajectories of length 100.",
  "CBroader Impacts and Limitation": "Our method can help train agents in reinforcement learning tasks with total free-form natural languageconstraints, which can be useful in various real-world applications such as autonomous driving,robotics, and game playing. There are still limitations to our work. Our method may not be able tocompletely eliminate constraint violations. Our method has the contextual bottleneck as the length ofthe trajectory increases. We performed a trajectory length sensitivity analysis on Hazard-World-Grid.As shown in , initially increasing the trajectory length improves performance because longertrajectories may provide more dependencies. However, beyond a certain point, further increases intrajectory length result in a slight drop in AUC. This decline is because the trajectory encoder hasdifficulty capturing global information. We consider this bottleneck to be related to the transformersencoding capability."
}