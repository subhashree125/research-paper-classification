{
  "Abstract": "Climate changes destruction of marine biodiversity is threatening communities andeconomies around the world which rely on healthy oceans for their livelihoods. Thechallenge of applying computer vision to niche, real-world domains such as oceanconservation lies in the dynamic and diverse environments where traditional top-down learning struggle with long-tailed distributions, generalization, and domaintransfer. Scalable species identification for ocean monitoring is particularly difficultdue to the need to adapt models to new environments and identify rare or unseenspecies. To overcome these limitations, we propose leveraging bottom-up, open-domain learning frameworks as a resilient, scalable solution for image and videoanalysis in marine applications. Our preliminary demonstration uses pretrainedvision-language models (VLMs) combined with retrieval-augmented generation(RAG) as grounding, leaving the door open for numerous architectural, trainingand engineering optimizations. We validate this approach through a preliminaryapplication in classifying fish from video onboard fishing vessels, demonstratingimpressive emergent retrieval and prediction capabilities without domain-specifictraining or knowledge of the task itself.",
  "Introduction": "Reliable and timely data is critical for addressing climate challenges driving global biodiversitycollapse, and threatening 3 billion livelihoods that depend on healthy oceans . Thousands ofcameras are deployed around the world underwater, on drones, on fishing vessels, and via satelliteto monitor marine life and inform critical management tasks. The scale of the data they produce isalready impossible for humans to handle. In fisheries monitoring alone, thousands of hours of videoare generated per boat, making its manual review crushingly slow, not scalable, and unaffordable. . Automated processing of visual data brings key insights about marine activity at an unprecedentedscale. Traditional vision approaches, such as CNNs and basic vision transformers , requireextensive retraining for new environments and perform poorly when faced with rare or unseenspecies. These top-down methods struggle with generalization, domain transfer, and handlinglong-tailed distributions, and are infeasible for scalable ocean applications. Generalizable speciesidentification stands to revolutionize marine management by providing comprehensive data on marinelife distribution and their responses to climate change - vital for management of sustainable fisheries,invasive species, carbon sequestration, and ecosystem health. To overcome these challenges, we propose leveraging bottom-up, open-domain learning frameworksas a scalable and resilient solution, where knowledge about tuna species or oil spills are connected ina modular way, even after deployment. In this proposal, we show one accessible method for grounded,open-domain vision and highlight the promise of vision-language models (VLMs) combined withretrieval-augmented generation (RAG) . This approach enhances generalization",
  "Related Work": "Traditional vision methods such as convolutional neural networks (CNNs), single-stage detectors,and vision transformers have been the cornerstone of visual recognition. These top-downmethods have shown reliable performance in closed-set, uni-domain climate applications, includingenvironmental monitoring . However, their effectiveness diminishes in generalized climateapplications where data is often non-IID, and the distribution of objects and events is long-tailed orunexpected . Recent advances in bottom-up, multi-modal approaches offer promising solutions to these challenges.Vision-language models (VLMs) based on CLIP and BLIP-2 learn representations of theworld through contrastive learning and show emergent capabilities like detection, retrieval, question-answering and broader domain transfer. Note that this effectively bridges the modality gap betweenvisual and textual data. These models excel in open-domain tasks such as visual question-answering(VQA) and retrieval , which are crucial for climate monitoring and conservation . The integration of grounding and retrieval-augmented generation (RAG) into these models furtherenhances their performance by allowing them to access and incorporate external knowledge duringinference . For instance, grounded CLIP showcases emergent detection andidentification capabilities that compete and surpass SOTA methods. Interestingly, Shen et al. empirically indicates that biodiversity-related domains benefit most from transferable vision modelsinjected with external knowledge (e.g. Flowers102 and OxfordPets). Grounding is key for open-endedVQA tasks, where external knowledge is required , but can also alleviate difficulties of classimbalance, domain shift and transfer, and interpretability. These advances make VLMs combinedwith RAG particularly promising for addressing the complex challenges posed by climate-relatedmonitoring tasks. Despite these advancements, there is still limited work that systematically composes, refines, andscales these approaches across diverse real-world applications. Our work addresses this gap byexploring the application of these methods in marine conservation, where the ability to generalizeand adapt to new environments is critical for effective and scalable species identification and marinemonitoring.",
  "Method": "Current vision research has produced powerful methods that can extrapolate and perform new taskseven without explicit training on them. While extremely generalizable, they cannot yet producespecialized outputs they have never seen in training, such as object classification to an unseen class.Thus grounding vision is critical as it provides access to external knowledge, and this is our focus forthis preliminary work. We outline a framework for building generalizable models adaptable to unseen domains and tasks,with a focus on climate impact, and marine monitoring applications. We integrate key componentsincluding bottom-up learning and grounding with retrieval-augmented generation (RAG), to addressthe challenges of scalability, adaptability, and robustness in real-world environments. For groundingand RAG, we show a minimal setup using similarity search, opening the door for complete groundingand pipelines in future work. Design enhancements, fine-tuning, and extra tools (such as promptoptimization, multi-query search, and large context) can greatly improve RAG-based approaches (seeAppendix B). We propose a methodology which uses an open-domain, bottom-up, and task-free model, specificallycontrastive learning-style VLMs, combined with RAG to enhance model adaptability and performanceacross diverse and unseen domains. These models can easily transfer between domains, using modularRAG connections for grounding at any time and without model retraining. Similar to typical multi-modal retrieval-augmented transformer structures , our visual RAG pipeline has threecomponents as shown in : a CLIP visual encoder to generate image embeddings, the knowledgebase built with image embedding, and the backbone where we can evaluate various pre-trained",
  ": Architecture of visual RAG. The small pentagons with different colours represent tokens.They are concatenated as input into a language model to generate the final prediction": "language models. As this is preliminary work, we focus on demonstrating the value of informationretrieval on visual classification and direct our investigation using an open dataset for fisheriesmonitoring . For a comprehensive version of this method with attention to the entire pipeline,multiple domains, and multiple tasks, see Appendix B and future work.",
  "Image-based Vector Store": "In existing RAG-based vision work, vector stores use text-based keys . Our work is thefirst to our knowledge to build the knowledge base with an image embedding key. It is motivated byseveral reasons: a) Poor image quality () causes direct similarity search between these imagesfeatures and text to result in noise and confuses the language model; b) The image embeddingdatabase leverages the limited amount of labeled data, helping constrain knowledge retrieval; c) Thequery itself is static (e.g. object classification), hence adaptive retrieval according to user prompt isunnecessary. We produce image embeddings for a small set of reference species using the CLIP encoder and storethem as the key in a vector database. The descriptions of different species can then be retrieved bytheir corresponding image embedding and used as augmented context for answer generation in thefinal step. Specifically, we produce embeddings of images in the Fishnet validation set using theCLIP encoder and store them as the key in our vector database.",
  "Pre-trained Multi-modal LLM": "The LLaVA family is widely used as the backbone for multi-modal downstream tasks. Wemodify the CLIP model as visual encoder to generate the query for the similarity search, and usethe pre-trained LLaVA 1.5 weights for performance evaluation. To perform a new set of tasks (whilestill) in an emergent manner, VLMs greatly benefit from instruction fine-tuning , which can besupported with LoRA. We leave this for future work (see Appendix B) building on this proposal.",
  "As was selected for motivation and demonstration in the methods section, we investigate visual RAGon the sample visual task of classification for real-world climate applications": "We implemented our proposed method using the exact minimal architecture shown of . As ourdataset, we use Kay and Merrifield s Fishnet dataset, version 1.0.0, since it is the largest publicdataset of on-deck fisheries activity, and represents a challenge in many of the dimensions discussedin previous sections (data distribution, domain shifts, unseen knowledge).",
  "VLM-RAG (Ours, RAG Retrieval)0.86840.95270.9781": "retrieval step, investigating the performance of the encoder vector search separately. Completing ourpreliminary ablation, we also tested the pipeline without any grounding (see ). As our baseline,we used InceptionV3 pretrained on ImageNet with all the same parameters as provided in Kay andMerrifield using a single NVIDIA A100. For intuition on the task difficulty and influence of retrieved descriptions, shows typical inputimage, prompt, and output, using low resolution and partially occluded input, both detrimentalfor species classification. Without retrieval visually shows our ablation where the category list isexplicitly given to the model. Even with no re-ranking or other optimizations in this preliminary implementation, showsretrieval performance outperforming both baseline and final prediction. We predict that by advancingthe methods outlined in with steps described in Appendix B and high-quality embeddeddescriptions and sample images, the final prediction with outperform RAG retrieval while both see anadditional increase. In the appendix, we present top-k accuracy of the retrieval process (), capturing whether thelanguage model has enough information for accurate prediction. then show the precisionand recall of the final prediction under different RAG settings. Additionally, we give a visualisationof our embedding space (), to emphasize the task difficulty and the necessity of using visualRAG on on-deck fish images.",
  "Conclusion and Pathways to Climate Impact": "Our results demonstrate impressive retrieval and prediction capabilities, without any task or domain-specific training, highlighting the potential of bottom-up learning models to advance scalable marinemonitoring. Unblocked from the need for expensive domain adaptation, our continued collaborationwith fisheries and marine conservation partners will enable faster and more accessible deploymentsof marine life monitoring for significantly more informed responses to changing climates.",
  "Acknowledgements": "We would like to thank the entire OnDeck Fisheries AI team for supporting our research and theirhard work to translate research into real solutions, accessible and affordable around the world. Thankyou to Prof. Graham Taylors research group in 2023 for conversations about combining non-textmodalities with RAG for biodiversity science which encouraged us to conduct our initial literaturereview and begin this work.",
  "Haotian Liu et al. Visual Instruction Tuning. Dec. 11, 2023. arXiv: 2304.08485[cs]. URL:": "Kenneth Marino et al. KRISP: Integrating Implicit and Symbolic Knowledge for Open-Domain Knowledge-Based VQA. In: 2021 IEEE/CVF Conference on Computer Vision andPattern Recognition (CVPR). 2021 IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR). Nashville, TN, USA: IEEE, June 2021, pp. 1410614116. ISBN: 978-1-66544-509-2. URL: Kenneth Marino et al. OK-VQA: A Visual Question Answering Benchmark Requiring Exter-nal Knowledge. In: 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR) (June 2019). Conference Name: 2019 IEEE/CVF Conference on Computer Vision andPattern Recognition (CVPR) ISBN: 9781728132938 Place: Long Beach, CA, USA Publisher:IEEE, pp. 31903199. URL:",
  "Mohammad Sadegh Norouzzadeh et al. A deep active learning system for species identificationand counting in camera trap images. Oct. 21, 2019. arXiv: 1910.09716[cs,eess,stat].URL:": "Mohammad Sadegh Norouzzadeh et al. Automatically identifying, counting, and describingwild animals in camera-trap images with deep learning. In: Proceedings of the NationalAcademy of Sciences 115.25 (June 19, 2018). Publisher: Proceedings of the National Academyof Sciences, E5716E5725. URL: Alec Radford et al. Learning Transferable Visual Models From Natural Language Supervi-sion. In: Proceedings of the 38th International Conference on Machine Learning. InternationalConference on Machine Learning. ISSN: 2640-3498. PMLR, July 1, 2021, pp. 87488763.URL:",
  "Samuel Stevens et al. BioCLIP: A Vision Foundation Model for the Tree of Life. In:Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR). 2024, pp. 1941219424": "Grant Van Horn et al. The iNaturalist Species Classification and Detection Dataset. In:2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2018 IEEE/CVFConference on Computer Vision and Pattern Recognition (CVPR). Salt Lake City, UT: IEEE,June 2018, pp. 87698778. ISBN: 978-1-5386-6420-9. URL: Kate Wing and Benjamin Woodward. Advancing artificial intelligence in fisheries requiresnovel cross-sector collaborations. In: ICES Journal of Marine Science (Aug. 28, 2024). Ed. byHoward Browman, fsae118. ISSN: 1054-3139, 1095-9289. URL:",
  "LLaVA with retrieved description. Here the category limit is implicitly included by descriptionretrieval": "A.2Top-k Accuracy shows the top-k accuracy of the retrieval process. That is, we are measuring how oftenthe top k retrieved descriptions contain the correct category or species. Higher accuracy of theretrieval should typically mean the model has higher probability to make the correct prediction in thegeneration step, since it is provided with higher accuracy external knowledge. The experiments are conducted in two different granularities: category and species, where onecategory might contain several species that are hard to distinguish. For example, category Tunacontains species Albacore, Yellowfin tuna, Skipjack tuna, Bigeye tuna, and Tuna (whichthe dataset grouped due to ambiguity).",
  "BDiscussion and Future Work": "The promise of grounded visual reasoning is quite significant. This is even more pronounced withthe current momentum towards universal reasoners based on projections of scaling laws for visionmodels. Grounding these highly generalizable methods can allow us to reach human capacity foranalyzing imagery, where a model will look up information it does not have just as a human would.We also emphasize that this work does not explore the \"reasoning design\", in our case being thechoice of (static) pipeline steps and leading into a final VLM-as-predictor with a static query. Forexample, introducing guided reasoning would allow the architecture to be dynamically adjusted,potentially addressing multi-task capability, and recursive reasoning and attending to harder queries."
}