{
  "Abstract": "Recognizing the sounding objects in scenes is a longstanding objective in em-bodied AI, with diverse applications in robotics and AR/VR/MR. To that end,Audio-Visual Segmentation (AVS), taking as condition an audio signal to identifythe masks of the target sounding objects in an input image with synchronous cameraand microphone sensors, has been recently advanced. However, this paradigm isstill insufficient for real-world operation, as the mapping from 2D images to 3Dscenes is missing. To address this fundamental limitation, we introduce a novelresearch problem, 3D Audio-Visual Segmentation, extending the existing AVSto the 3D output space. This problem poses more challenges due to variationsin camera extrinsics, audio scattering, occlusions, and diverse acoustics acrosssounding object categories. To facilitate this research, we create the very firstsimulation based benchmark, 3DAVS-S34-O7, providing photorealistic 3D sceneenvironments with grounded spatial audio under single-instance and multi-instancesettings, across 34 scenes and 7 object categories. This is made possible by re-purposing the Habitat simulator to generate comprehensive annotations ofsounding object locations and corresponding 3D masks. Subsequently, we pro-pose a new approach, EchoSegnet, characterized by integrating the ready-to-useknowledge from pretrained 2D audio-visual foundation models synergistically with3D visual scene representation through spatial audio-aware mask alignment andrefinement. Extensive experiments demonstrate that EchoSegnet can effectivelysegment sounding objects in 3D space on our new benchmark, representing asignificant advancement in the field of embodied AI. Project page:",
  "Introduction": "Human perception of the real world, both visual and acoustic, predominantly occurs in three dimen-sions. Prior psychology literature has highlighted humans remarkable ability to correspondacross multiple modalities, often involving the association of events across these modalities. Forinstance, we can effortlessly ground emergent surround sound with its potential source in 3D visuals. Inspired by this capability, a crucial aspect in the development of embodied AI systems istheir ability to integrate cues from synchronous multimodal input streams and establish targets corre-sponding to their goals. In this work, we aim to build a machine model to achieve this multimodalcorrespondence, particularly targeted towards the task of audio-visual segmentation (AVS) in 3D. Albeit AVS has been widely explored within audio-visual scene analysis and correspondence learning,prominent research in this field has focused on 2D environments involving mono (single channel)sound sources, thus devoid of spatial presence entirely. In this paper, we take the first step towardsexploring 3D AVS and introduce a large benchmark, 3DAVS-S34-O7. Our exploration is rooted in afundamental grounding problem: given an embodied agent equipped with a camera and a binauralmicrophone, can we teach the agent to obtain fine-grained localization of potential sounding objects(generally by predicting a segment-level mask of the object in 3D) while also utilizing spatial audio",
  "arXiv:2411.02236v1 [cs.CV] 4 Nov 2024": "#! \"\"-)*((+,*'&)%&$, 01.01401: MI0DA@HH U[XY@ f U[XY@ A@HH MI0DA@HH 4A@HHk@g 4MI0DA@HH A@HH 4A@HH@gYww1 : Comparison of the existing 2D AVS task with our proposed 3D AVS. Former task utilizessingle channel audio to generate pixel-level masks of the potential sounding object in the input RGBframe. 3D AVS on the other hand is aimed at generating 3D masks (from which multi-view consistent2D masks can be rendered) while utilizing multichannel (spatial) audio. cues? (see ) Furthermore, we extend our benchmark to include a more competitive multi-instance setup where, although multiple instances of the same object might be present in the scene,the goal is to segment only the sounding instance. This setup helps us testify to the efficacy of spatialpresence harnessed from the input binaural audio samples. Recently, 3D Gaussian Splatting (3D-GS) has emerged as a prospective method for modelingstatic 3D scenes directly from input RGB frames. Owing to its explicit Gaussian based representation,it has paved a natural pathway for 3D visual segmentation . Deriving inspiration fromhuman spatial memory in indoor environments, we design EchoSegnet, a purely training-freepipeline for 3D AVS within a 3D-GS representation. EchoSegnet leverages 2D foundation models(namely SAM and ImageBind ) to first obtain 2D AVS masks on the input RGB frames. These2D AVS masks are further used to segment the Gaussians in the learned 3D-GS representation toobtain multi-view masks to achieve a consistent 3D segmentation. To summarize, we make the following contributions: (1) the first 3D audio-visual segmentationbenchmark composing of fairly complex indoor room scenes with integrated spatial sound cues;(2) a training-free AVS framework, EchoSegnet, capable of syncing across sequential frames from3D environments; (3) a novel Audio-Informed Spatial Refinement Module AISRM, designed toenhance 3D segmentation and resolve ambiguities in complex, multi-instance environments byleveraging spatial audio intensity maps. We perform a comprehensive evaluation of EchoSegneton the proposed 3DAVS-S34-O7 for both single-instance and multi-instance scenarios, along withan ablative comparison with existing 2D AVS models, highlighting their shortcomings in aligningaudio-visual cues within 3D scenes -establishing their adaptation to 3DAVS-S34-O7 as non-trivial.",
  "Related Work": "Audio-visual segmentation Existing AVS methods cater to 2D scenes with mono audio as inputsto identify audible visual pixels associated with a given audio signal andare typically trained on thousands of manually annotated 2D segmentation masks. Although therehave been recent improvements on reducing the dependence on annotated AV masks using weaklysupervised or entirely unsupervised methods , there has not been any effortextending the task of AVS particularly to 3D scenes, with spatial audio cues. To address this gap,we propose the first benchmark for 3D AVS harnessing existing embodied AI platforms (Habitatsimulator ) to capture visual and (binaural) acoustic cues for sounding objects placed in 3Dindoor scenes. We believe this lays a prominent groundwork for systematic evaluation of futureembodied systems for 3D segmentation. 3D scene representations Point-based rendering techniques, initiated by , utilize point-basedexplicit representation where each point affects a single pixel. Zwicker et al. advanced this withellipsoid-based rendering (splatting), allowing mutual overlap to fill image holes. In the absence of",
  "single-instance725multi-instance79": "given geometry, Mildenhall et al. explored neural implicit representation, NeRF, predicting view-dependent radiance via implicit density fields. 3D Gaussian Splatting (3D-GS) , a novel-viewsynthesis method, employs explicit point-based representation, contrasting with NeRFs volumetricrendering. Owing to its real-time high-quality rendering capabilities, 3D-GS has been applied tovarious domains, including simultaneous localization , content generation , and 4Ddynamic scenes , among others. In this work, we utilize the explicit representation fromlearned 3D-GS and decompose 2D masks of potential sounding objects to obtain consistent 3Dmasks.",
  "Dataset": "Our proposed 3DAVS-S34-O7 is profoundly motivated towards simulating real-world indoor scenes,in terms of the visual quality of the scenes as well as the acoustic response generated by the objectsplaced within it. In the context of our 3D AVS task, we define an observation as O={(vi, ai, mi)}ni=1,where vi, ai represent the visual (RGB view, R100810083) and acoustic (1 second binaural audio,at 44.1kHz) cues respectively, captured by the embodied agent at i-th time. mi represents a binarymask corresponding to vi highlighting the sounding object. To record an observation, we load arandomly sampled scene from the Habitat-Matterport3D dataset into the SoundSpaces 2.0 .Next, we place a semantically relevant sounding object (for instance, bathroomwashing machine,kitchenmicrowave, etc.) which emits a sound based on a mono audio (corresponding to the placedobject and sourced from ). We capture n = 120 frames at 1 fps symbolizing differentpositions along the moving agents path. Alongside the above single-instance setup, we also explore aslightly challenging multi-instance setup wherein, we place multiple instances of the sounding object,although only one instance is sound-emitting (). We split each observation into 7:1 for train:testsplit (following ). Details of the selected scanned spaces and sound-emitting objects are providedin Appendix A.3.",
  "Method": "Considering the complexity of the 3D AVS task and deriving inspiration from human spatial memory,we propose EchoSegnet (see ), a training-free pipeline leveraging 2D foundation models.For each input view, vi, we first obtain corresponding 2D AVS masks mi using OWOD-BIND. Particularly, OWOD-BIND prompts the SAM model using bounding boxes obtained fromclass-agnostic object detection (CAOD) . The mask proposals from SAM are further filteredbased on maximum cosine similarity with ais audio embedding generated using ImageBind . Please note, the masks mi are confined to vi however the main goal of the 3D AVS task is to obtainmulti-view consistent masks of the potential sounding object for novel viewing positions (beyondvi). Moreover, the sounding object may be fully or partly visible in the novel view. To achieve this,we propose to lift the 2D AVS masks mi within an explicit 3D scene representation G, generatedusing vanilla 3D-GS . Although similar approaches exist for salient 3D visual segmentation(such as ), the sounding object in the context of 3D AVS, may not always be salient (i.e in theforeground). As a result, unlike , we opt to exclude out-of-view projections from the votingprocess for selecting underlying Gaussians as well as directly lift mi (see Appendix A.5). &# &#4(.' @ @L L' (.'V_ #(L L_ @ . zxu_&# uL{ ' '&Vxu 4V(_& & : Overview of EchoSegnet: (a) 2D AVS pipeline OWOD-BIND generates 2D masks. (b)These masks are lifted into a 3D-GS scene representation using with a modified voting strategy.(d) The initial 3D segmentation may contain noise and ambiguities, as spatial relationships betweenobjects and sound were not considered. (c) To address this, we apply the novel Audio-InformedSpatial Refinement Module (AISRM). (e) In the refined 3D segmentation, only the sound-emittingobject instance is retained, and noise is filtered out. Audio-Informed Spatial Refinement Module (AISRM) Although the above lifting process yields3D segmentation masks, we observe certain ambiguities: (1) in the case of a multiple-instance setup,computation of mi, being devoid of spatial audio, is unable to accurately localize only the sound-emitting instance of the object (see (d)), and (2) due to errors in audio-visual alignment withinthe frozen ImageBind, mi often includes other (silent) objects in the vicinity of the sound-emittingobject. To handle both the ambiguities, we start with a 3D-GS-based Audio Intensity Map. Specifically, weintroduce additional labels Ig on every Gaussian g within our scene representation G by weighing theroot mean square (RMS) intensities on the agents left and right audio channels, Rl, Rr respectively.For each Gaussian g, we compute Ig = ti=1|RliRri | max(Rli,Rri ) IRMS(gcenter, ai), where IRMS(.) equals1 if the Gaussian center gcenter is located on the side with the greater RMS intensity based on thebinaural audio observation ai. proposed a similar intensity map but in two dimensions, and notgrounded within an underlying 3D-GS representation. We then perform an Audio-Informed Gaussian Refinement process through spatial clustering,guided by Ig. We cluster the segmented 3D Gaussians using DBSCAN and filter clusters withvolumes > v + 0.5v where v is the mean volume and v is the standard deviation of all clustervolumes. Next we localize the audio intensity center by computing an average of the Gaussian centercoordinates weighted by Ig (only Gaussians with Ig > ref are considered). We hypothesize thatthe cluster closest to the computed audio intensity center consists of Gaussians corresponding tothe sound-emitting object, effectively filtering out both the inclusion of silent objects, as well asnon-sound-emitting instances in the multi-instance setting.",
  "Experiments": "In this section, we demonstrate the effectiveness of EchoSegnet on the 3DAVS-S34-O7 benchmark,with a particular focus on the contribution of AISRM. Following , we adopt mIoU and F-Score asthe metrics to estimate the segmentation performance. For implementation details, please refer toAppendix A.4. From , it is evident that removing the AISRM module results in a performancedrop across both single-instance and multiple-instance settings. For the single-instance setting, mIoUdecreases by 0.06, and F-Score drops by 0.10. Similarly, in the multiple-instance setting, mIoUdrops by 0.04, and F-Score decreases by 0.11. As illustrated in (Left), omitting AISRMintroduces noisy Gaussians representing silent objects (e.g., the door, view 3), negatively impactingperformance across both subsets. In the multiple-instance setting, the inability to distinguish betweensound-emitting and non-sound-emitting instances of the same object further reduces segmentationaccuracy (e.g., both clocks are segmented, but only one is sound-emitting, view 1). Additionally,Gaussians in the vicinity of the sounding object (clock) are also incorrectly segmented (view 2). $*'&&(53'9/*& ACITWQROKVJGEDJBUGP?<V?KB<;GMOSG>GKVL@V?<VJGACGTODKKBO?KG=SHU:VF<BU?KN ]CGEXWWW^iTgG>GTcGMOKhACGEXW iV?JVHVJG]CG@OKhKGqUHG?UlVkGlBVnK & && & && &9&9 / /$/ : Left: (Scene a) Qualitative comparison of EchoSegnet performance with and withoutAISRM, illustrated through projected 3D-GS scene representation and renderings. Right: Comparisonbetween DenseAV (SSL), OWOD-BIND (2D AVS) and EchoSegnet. (Scene b) OWOD-BIND incorrectly segments the non-sound-emitting coffee machine. (Scene c) Both SSL and 2D AVS fail tohandle a complex scenario where only a small part of the sound-emitting telephone is present in theview, whereas EchoSegnet successfully addresses this challenge.",
  "DenseAV (2D SSL)0.4260.0230.4360.023OWOD-BIND (2D AVS)0.6930.5230.6960.502": "Comparison Between SSL, 2D, and 3D Audio-Visual Segmentation. We propose EchoSegnet asthe first approach towards the novel 3D AVS task. Naturally, comparing the performance of existing2D AVS (and Sound Source Localization (SSL)) approaches for the 3D AVS task is essential toestablish the efficacy of EchoSegnet. From , it can be clearly observed that EchoSegnetconsistently outperforms OWOD-BIND (a 2D AVS method) across both subsets, while DenseAV (aSSL method) shows significantly poorer and incomparable performance. The strength of EchoSegnetin performing 3D AVS lies in its ability to capture spatial relationships between objects and theirsounds, which the existing 2D AVS methods lack, often resulting in segmentation of all visibleinstances (, Right, Scene b).",
  "Conclusion": "In this work, we introduced 3D Audio-Visual segmentation (3D AVS) as a novel extension of theexisting 2D AVS paradigm. We presented the 3DAVS-S34-O7 benchmark, the first simulation-basedlarge dataset for 3D AVS, featuring photorealistic environments with spatial audio across 34 scenesand 7 object categories. Our proposed method, EchoSegnet, effectively segments sounding objects in3D scenes in a training-free pipeline leveraging 2D audio-visual foundation models and 3D GaussianSplatting. We believe this marks a significant advancement in bridging the gap between 2D and 3Daudio-visual understanding, with broader implications for embodied AI. Looking ahead, we aim toexplore diverse acoustic environments and dynamic objects as the future scope of this work.",
  "Swapnil Bhosale, Haosen Yang, Diptesh Kanojia, and Xiatian Zhu. Leveraging foundationmodels for unsupervised audio-visual segmentation. ArXiv, abs/2309.06728, 2023. URL": "Changan Chen, Carl Schissler, Sanchit Garg, Philip Kobernik, Alexander Clegg, Paul Calamia,Dhruv Batra, Philip W Robinson, and Kristen Grauman. Soundspaces 2.0: A simulationplatform for visual-acoustic learning. In NeurIPS 2022 Datasets and Benchmarks Track, 2022. Yuanhong Chen, Yuyuan Liu, Hu Wang, Fengbei Liu, Chong Wang, Helen Frazer, and GustavoCarneiro. Unraveling instance associations: A closer look for audio-visual segmentation. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages2649726507, 2024. Martin Ester, Hans-Peter Kriegel, Jrg Sander, and Xiaowei Xu. A density-based algorithmfor discovering clusters in large spatial databases with noise. In Proceedings of the SecondInternational Conference on Knowledge Discovery and Data Mining, KDD96, page 226231.AAAI Press, 1996.",
  "Jeffrey P Grossman and William J Dally. Point sample rendering. In Rendering Techniques 98:Proceedings of the Eurographics Workshop. Springer, 1998": "Mark Hamilton, Andrew Zisserman, John R. Hershey, and William T. Freeman. Separatingthe chirp from the chat: Self-supervised visual grounding of sound and language. 2024IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1311713127, 2024. URL Dawei Hao, Yuxin Mao, Bowen He, Xiaodong Han, Yuchao Dai, and Yiran Zhong. Improvingaudio-visual segmentation with bidirectional generation. In Proceedings of the AAAI Conferenceon Artificial Intelligence, volume 38, pages 20672075, 2024. Xu Hu, Yuxi Wang, Lue Fan, Junsong Fan, Junran Peng, Zhen Lei, Qing Li, and ZhaoxiangZhang. Sagd: Boundary-enhanced segment anything in 3d gaussian via gaussian decomposition,2024. URL arXiv preprint arXiv:2401.17857. Nikhil Keetha, Jay Karhade, Krishna Murthy Jatavallabhula, Gengshan Yang, Sebastian Scherer,Deva Ramanan, and Jonathon Luiten. Splatam: Splat, track & map 3d gaussians for dense rgb-dslam. arXiv preprint arXiv:2312.02126, 2023.",
  "Chen Liu, Peike Li, Hu Zhang, Lincheng Li, Zi Huang, Dadong Wang, and Xin Yu. Bavs: boot-strapping audio-visual segmentation by integrating foundation knowledge. IEEE Transactionson Multimedia, 2024": "Jinxiang Liu, Yu Wang, Chen Ju, Chaofan Ma, Ya Zhang, and Weidi Xie. Annotation-freeaudio-visual segmentation. In Proceedings of the IEEE/CVF Winter Conference on Applicationsof Computer Vision, pages 56045614, 2024. Muhammad Maaz, Hanoona Rasheed, Salman Khan, Fahad Shahbaz Khan, Rao MuhammadAnwer, and Ming-Hsuan Yang. Class-agnostic object detection with multi-modal transformer.In 17th European Conference on Computer Vision (ECCV). Springer, 2022.",
  "Shentong Mo and Yapeng Tian. Av-sam: Segment anything model meets audio-visual localiza-tion and segmentation. arXiv preprint arXiv:2305.01836, 2023": "Karol J. Piczak. ESC: Dataset for Environmental Sound Classification. In Proceedings of the23rd Annual ACM Conference on Multimedia, pages 10151018. ACM Press, 2015. ISBN978-1-4503-3459-4. doi: 10.1145/2733373.2806390. URL Santhosh K Ramakrishnan, Aaron Gokaslan, Erik Wijmans, Oleksandr Maksymets, Alex Clegg,John Turner, Eric Undersander, Wojciech Galuba, Andrew Westbury, Angel X Chang, et al.Habitat-matterport 3d dataset (hm3d): 1000 large-scale 3d environments for embodied ai. arXivpreprint arXiv:2109.08238, 2021. Zhongzheng Ren, Aseem Agarwala, Bryan Russell, Alexander G. Schwing, and OliverWang. Neural volumetric object selection. In IEEE/CVF Conference on Computer Vision andPattern Recognition (CVPR), 2022. ( alphabetic ordering). Jrg Sander, Martin Ester, Hans-Peter Kriegel, and Xiaowei Xu. Density-based clustering inspatial databases: The algorithm gdbscan and its applications. Data Mining and KnowledgeDiscovery, 2(2):169194, 1998. doi: 10.1023/A:1009745219419. Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, BhavanaJain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, Devi Parikh, and Dhruv Batra.Habitat: A Platform for Embodied AI Research. In Proceedings of the IEEE/CVF InternationalConference on Computer Vision (ICCV), 2019.",
  "Robert B Welch and David H Warren. Immediate perceptual response to intersensory discrep-ancy. Psychological bulletin, 88(3):638, 1980": "Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu,Qi Tian, and Xinggang Wang. 4d gaussian splatting for real-time dynamic scene rendering.arXiv preprint arXiv:2310.08528, 2023. Qi Yang, Xing Nie, Tong Li, Pengfei Gao, Ying Guo, Cheng Zhen, Pengfei Yan, and ShimingXiang. Cooperation does matter: Exploring multi-order bilateral relations for audio-visualsegmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 2713427143, 2024. Zeyuan Yang, Jiageng Liu, Peihao Chen, Anoop Cherian, Tim K. Marks, Jonathan Le Roux,and Chuang Gan. Rila: Reflective and imaginative language agent for zero-shot semanticaudio-visual navigation. In Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition (CVPR), pages 1625116261. IEEE, 2024.",
  "A.1Limitations & Failure Cases": "&%$$/3%.4&$ VRMPLFEJSTQIHAQDKINUBTUSDB@I>J=IGIST<;TUBTQIVRIPJASSDJUSI?=:K9TCBDKUSO bTUQT:TQI\\RI;JSYSIfK:IUK`TXI`DT^S $$x.43$nlpkjiz l$%.43$lpkjiz HNLb> . $ $ $. $ $ $. : Failure case: Due to the close proximity of the microwave and oven, the AISRM mistakenlyrefines the segmented Gaussians to those of the silent oven, discarding the Gaussians of the sound-emitting microwave. Despite the evident improvements brought by AISRM, it struggles when objects are positioned tooclosely, which consequently impacts the overall performance of EchoSegnet. In , both themicrowave and oven are initially segmented in 3D due to a misalignment in ImageBind , eventhough only the microwave is emitting sound. While AISRM typically resolves such ambiguities, inthis case, the 3D-GS-based Audio Intensity Map provided conflicting guidance due to the proximityof the objects. Consequently, the AISRM refinement process incorrectly retained the Gaussianscorresponding to the silent oven, rather than the sound-emitting microwave, as seen in the rendered2D masks for novel views.",
  "A.3Dataset: Sound-Emitting Objects and Scanned Spaces": "The 3DAVS-S34-O7 dataset focuses on indoor environments, using scanned spaces from the Habitat-Matterport3D dataset . Four scans were chosen based on their scanning quality and suitabilityfor audio rendering (shown in , Right). Seven commonly found sound-emitting objects wereselected: a washing machine, toilet, vacuum cleaner, microwave, coffee machine, clock, and telephone(shown in , Left). The 3D models of these objects were sourced from Sketchfab andselected for their realism.",
  "A.4Implementation Details": "In the OWOD-BIND pipeline, each 1-second audio clip is extended to 2 seconds by appending 0.5seconds of audio from neighboring clips before being input into the ImageBind audio encoder, andthe threshold BIND is set to 0.2. To construct the 3D Gaussians Splatting scene representation,the original image resolution of 1008x1008 is retained, and each scene is trained for 30,000 iterations.For the modified voting strategy in SAGD , the threshold voting is set to 0.3, with the intervalparameter for Gaussian Decomposition fixed at 4, as recommended by . During the DBSCAN clustering process, an epsilon value of 0.04 is used, with a minimum point count of 6, as sT4fr6TAbpF wc2JMjhGNzB5ZKStnWn8Zoq9vSo1VnCiC",
  "A.5Modified Voting Strategy for SAGD": "In contrast to SAGDs original voting strategy , which selects 3D Gaussians based on theirprojection into the 2D object mask more frequently than into the background or out of view, followedby thresholding, we exclude out-of-view projections from the voting process. Thresholding is appliedsolely based on the ratio of projections into the mask versus the background. This modification allowsus to lift object masks as long as the object is consistently segmented in 2D, even if it appears in onlya limited number of views. Since we apply additional refinement via AISRM, compared to the originalSAGD , it is reasonable to use a lower thresholding value voting. This approach prioritizessegmenting as many Gaussians as possible of the sound-emitting object, even if some Gaussiansrepresenting other objects are included, rather than risking the omission of Gaussians related to thesound-emitting object (demonstrated in ). Additionally, we omit SAGDs original 3D promptconstruction strategy, opting instead to directly lift the masks predicted by OWOD-BIND . : EchoSegnet performance on a sample single-instance scene (sT4fr6TAbpF, bathroom withsound-emitting vacuum cleaner) with varying voting thresholds. Values below 0.3 have little impacton accuracy due to AISRM, while higher values reduce performance."
}