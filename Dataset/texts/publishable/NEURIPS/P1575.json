{
  "Abstract": "Image fusion aims to integrate complementary information from multiple inputimages acquired through various sources to synthesize a new fused image. Existingmethods usually employ distinct constraint designs tailored to specific scenes,forming fixed fusion paradigms. However, this data-driven fusion approach ischallenging to deploy in varying scenarios, especially in rapidly changing environ-ments. To address this issue, we propose a conditional controllable fusion (CCF)framework for general image fusion tasks without specific training. Due to thedynamic differences of different samples, our CCF employs specific fusion con-straints for each individual in practice. Given the powerful generative capabilitiesof the denoising diffusion model, we first inject the specific constraints into thepre-trained DDPM as adaptive fusion conditions. The appropriate conditions aredynamically selected to ensure the fusion process remains responsive to the specificrequirements in each reverse diffusion stage. Thus, CCF enables conditionallycalibrating the fused images step by step. Extensive experiments validate oureffectiveness in general fusion tasks across diverse scenarios against the competingmethods without additional training. The code is publicly available.",
  "Introduction": "Image fusion aims at integrating complementary information from multi-source images, fusing anew composite image containing richer details . It has been applied in various scenarios thatsingle image contains incomplete information, such as multi-modal fusion (MMF) , multi-exposure fusion (MEF) , multi-focus fusion (MFF) , and remote sensing fusion . Thefused image inherits the strengths of both modalities, resulting in a composite with enhanced visualeffects . These fusion tasks have diverse downstream applications in computer vision, includingobject detection , semantic segmentation , and medical diagnosis because thecomprehensive representation of images with multi-scene information contributes to the improvedperformance of applications. Recently, numerous image fusion methods have been proposed, such as traditional fusionmethods , CNN-based fusion methods and GAN-based methods . While thesemethods produce acceptable fused images in certain scenarios, they are also accompanied by sig-nificant drawbacks and limitations: (i) They are often tailored for specific scenarios or individualtasks, limiting their adaptability across diverse applications; (ii) These methods necessitate trainingand consume substantial computational resources, posing limitations in terms of time and resourcerequirements. Lately, denoising diffusion probabilistic models (DDPM) have emerged as an iterative",
  "Detail": ":The conditions selection statistics during the sampling process of the LLVIP dataset.The distinct process of sampling has different favor of the conditions. The crucial role that diverseconditions play in controlling various image generation processes. Throughout the diffusion sampling,different conditions are dynamically selected to best suit the generation requirement at each stage.generation framework, showcasing impressive capabilities in unconditional generation. Inspiringly,numerous researchers explored its controllable aspects. ILVR proposed iterative latent variablerefinement with a reference image to control image translation. Some recent works employedthe diffusion model for image fusion, which fuses images in fixed fusion paradigms (fixed fusionconditions) by using its inherent reconstruction capacity. However, these approaches are not qualifiedfor sample-customized fusion with dynamic conditions. At present, general image fusion withcontrollable diffusion models is still a challenging problem, warranting further exploration. In this paper, we propose a diffusion-based controllable conditional image fusion (CCF) framework,which controls the fusion process by adaptively selecting optimization conditions. We construct acondition bank of generally used conditions, categorizing them into basic, enhanced, and task-specificconditions. CCF dynamically assigns fusion conditions from the condition bank and continuouslyinjects them into the sampling process of diffusion. To enable flexible integrated conditions, wefurther propose a sampling-adaptive condition selection (SCS) mechanism that tailors conditionselection at different denoising steps. The iterative refinements of the sampling are based on thepre-trained diffusion model without additional training. It is worth noting that the estimated fusedimages are conditionally controllable during the iterative denoising process. The diffusion processseamlessly integrates these conditions during the sampling process, decreasing potential impacts. Asillustrated in , the generation process emphasizes different aspects at various sampling steps.In the initial stages, the condition selection is influenced by random noise, resulting in a randomselection. During the intermediate stages, there is a shift towards content components. In the finalstage, the emphasis moves to generating and selecting texture details. These various conditionalfactors contribute to different aspects of fusion results and demonstrate the necessity and effectivenessof introducing specific conditions in different stages. To the best of our knowledge, we for the firsttime propose a conditional controllable framework for image fusion. The main contributions aresummarized as follows: We propose a pioneering conditional controllable image fusion (CCF) framework with acondition bank, achieving controllability in various image fusion scenarios and facilitatingthe capability of dynamic controllable image fusion. We propose a sampling-adaptive condition selection mechanism to subtly integrate thecondition bank into denoising steps, allowing adaptive condition selection on the fly withoutadditional training and ensuring the dynamic adaptability of the fusion process. Extensive experiments on various fusion tasks have confirmed our superior fusion perfor-mance against the competing methods. Furthermore, our approach qualifies for interactivemanipulation of the fusion results, demonstrating our applicability and efficacy.",
  "Image fusion focuses on producing a unified image that amalgamates complementary informationsourced from multiple source images": "Specialized. Focus on specialized tasks such as VIF, several early approaches relied onCNNs to address challenges across various scenarios. GTF defined the objective of image fusionas preserving both intensity information in infrared images and gradient information in visible images.Besides that, researchers started artificially incorporating prior knowledge to aid in the fusion process.CDDFuse introduced the concept of high and low-frequency decomposition with dual-branch asprior information. Diverging from approaches tailored to single scenarios, numerous methods arenow exploring the development of a unified fusion framework.DDFM represents the pioneeringtraining-free method that employs a diffusion model for multi-modal image fusion. Generalized. Not limited to specialized applications, researchers aim to extend its use to generalizedtasks. U2Fusion introduced a unified framework capable of adaptively preserving informationand facilitating joint training across various task scenarios. Additionally, SwinFusion proposeda cross-domain distance learning method that has been extended to form a unified frameworkencompassing diverse task scenarios. Defusion employs self-supervised learning techniquesto decompose images and subsequently adaptively fuse them. TC-MoA proposed a noveltask-customized mixture of adapters for generating image fusion with a unified model, enablingadaptive prompting for various fusion tasks. Nevertheless, these methods cannot control image fusion for adaptation to different scenarios. There-fore, we propose a method that enables image control, manipulating the fused image through existingconditions on our condition bank.",
  "Preliminary": "Denosing diffusion probabilistic models (DDPM) is a class of likelihood-based models that showsremarkable performance with a stable training objective in unconditional image generation. Thediffusion process entails incrementally introducing Gaussian noise to the data until it reaches a stateof random noise. For a clean sample x0 q(x0) each step within the diffusion process constitutes aMarkov Chain, encompassing a total of T steps, relying on the data derived from the preceding step.Gaussian noise is added as follows:",
  "txt1, tI),(1)": "where {t}Tt=1 is the variance schedule of each diffusion step which is fixed and predefined. Thegenerative process learns the inverse of the DDPM forward (diffusion) process, sampling from adistribution by reversing a gradual denoising process. We can directly sample xt at any t step basedon the original data xt q(xt|x0) and via the reparameterization, it can be redefined:",
  "t,(2)": "where defined t := 1 t and := ti=1 i. The diffusion process introduces noise to the data,whereas the inverse process represents a denoising procedure called sampling. In particular, statswith a noise xT N(0, I), the diffusion model learns to produce slightly less-noisy sample xT 1,the process can be formulate by:",
  "where, 2(t) = (t) = (1t)(1t1)": "1t, signifies the output of a neural network, commonly aU-Net. This neural network predicts the noise at each step, which is utilized for the denoisingprocedure. It can be observed that variance is a fixed quantity, because of diffusion process parametersbeing constant, whereas the mean is a function dependent on x0 and xt. However, the stochasticprocess poses challenges in controlling the generative process.",
  "Method": "Leveraging the reconstruction capability of unconditional DDPM, we introduced a new controllableconditional image fusion (CCF) framework. Our approach accomplishes dynamically controllableimage fusion via progressive condition embedding. In particular, we introduced a condition bankthat regulates the incorporation of fusion information using conditions. It allows for combining thedynamic selection of multiple conditions to achieve sampling-adaptive fusion effects. As shown in, we illustrate our CCF framework in detail with visible-infrared image fusion (VIF). The goal isto generate a fused image f RHW N from visible v RHW N and infrared i RHW Nimages, where H, W and N denote height, width, and channel numbers, respectively.",
  "Controllable Conditions": "Firstly, we provide the notation for the model formulation. For each sampling instance, a pre-trainedDDPM represents unconditional transition p(xt1|xt). Our method facilitates the inclusion ofconditional c during the sampling step of unconditional transformation, without no additional training.For this purpose, we sample images from the conditional distribution p(x0|c) given condition c:",
  "log p(c|xt) log p(c|x0) = f(xt, t) ||C A(x0)||2.(10)": "Here, A() can be linear or nonlinear operation. We represent ||C A(x0)|| as C. Now, the objectiveis to obtain x0 and incorporate the condition into it. We can minimize C to regulate the samplingwithin the diffusion process. In the following section, we will provide a detailed explanation of howto build a condition bank and how to select conditions.",
  "Condition Bank": "We empirically construct a condition bank and divide the image constraints into three categories:basic fusion conditions, enhanced fusion conditions, and task-specific fusion conditions. Basic fusionconditions are utilized throughout the entire sampling process, while enhanced fusion conditionsare dynamically selected. Task-specific fusion conditions are manually optional, tailored to specifictasks, and may possess unique attributes that can be customized for various task scenarios. Allconditions can be part of the enhanced condition set, enabling dynamic selection. The condition bankpresented in this paper includes some common conditions, but additional conditions can be exploredand utilized in other scenarios. In the above formulation, each conditional Markov transition with the given condition c is shownin Eq. 5. In particular, we constructed a condition bank that allows us to select required conditionsC = {c1, c2, ..., cn}, subsequently integrating them into the unconditional DDPM for executingconditional image fusion. Let C represent a condition bank comprising a series of conditions. Thefunction C represents the difference between the source images with the given condition. In everysampling step t, the difference function ci can be minimized using gradient descent. These conditionshelp regulate the image information within each modality involved in the fusion process. Basic Conditions. As shown in , basic conditions are essential to select for a basic generation.The basic conditions aim to synthesize a foundational fused image, offering an initial, coarserepresentation. The fused image serves as a primary fusion output, capturing essential features fromthe source images, though it may suffer from detail loss or texture blur. Notably, different scenariosmay require adjustments to the basic condition, as the specific requirements of each fusion task, suchas clarity, contrast, and other priorities, can influence its selection. Tailoring the basic condition toalign with the unique demands of each task thus ensures an effective fusion process. Enhanced Conditions. Besides basic conditions, we added enhancement conditions for refining theimage generation process. The condition bank contains a variety of enhanced conditions, inspiredby various Image Quality Assessments (IQA) such as SSIM, and standard deviation(SD). Theseconditions can be integrated into the CCF generation process to improve the quality of the generatedimages. The enhanced conditions can be selected with SCS algorithm, allowing different steps of thediffusion sampling process to be optimized with different conditions. This targeted approach ensures",
  "that each phase of the image generation is adapted to the specific requirements of that stage, resultingin higher quality fused images": "Task-specific Conditions. The purpose of image fusion is to facilitate various downstream tasks. Tomeet the specific requirements of these tasks, we offer task-specific conditions. These conditions canbe manually added to ensure that the fusion process is tailored to suit the downstream applicationsbetter. For example, a detection condition can be introduced by utilizing the feature extracted bya detection network F = D(x). The detection condition is formulated as ||F(x), F(M)||2, whereX {x0}mi and M is the set of m modalities. Other task-specific conditions can be similarly tailoredto optimize the fusion process for different tasks. By integrating these task-specific conditions, theimage fusion process can be precisely aligned with the demands of various applications, enhancingthe effectiveness and utility of the generated images.",
  "Sampling-adaptive Condition Selection": "During the diffusion sampling process, it is crucial to focus on generating distinct aspects of theimage. To address this, we designed an algorithm that dynamically selects the appropriate conditionfrom the condition bank to fit each sampling stage. This selection process can be denoted asCopt = Topk{Gate(C)}. The Copt is the selected condition, and Gate is the gate of selection. Wehypothesize that rapidly changing conditions during the sampling process should be prioritized asthey indicate greater significance at that generation stage. Inspired by multi-task learning , thegate of conditions can be calculated using the following formula:Gate = [1, ..c], i(t) = i(t 1) i.(11)The i(t 1) represents the condition gradient from the previous step, and the i is calculated as:",
  "Experiments": "Datasets. We conduct experiments in three image fusion scenarios: multi-modal, multi-focus, andmulti-exposure image fusion. For multi-modal image fusion task, we conducted experiments on theLLVIP dataset and referred to the test set outlined in Zhu et al. . For MEF and MFF, ourtesting procedure followed the test setting in MFFW dataset and MEFB dataset , respectively.Additionally, we test our method on the TNO dataset and Harvard medical dataset to assess ourmethods performance within the multi-modal fusion domain, detailed in App. B and H. Implementation Details. Our method utilizes a pre-trained diffusion model as our foundationalmodel . This model was directly applied without any subsequent fine-tuning for specific taskrequirements during our experiments. The experiments are conducted on Huawei Atlas 800 TrainingServer with CANN and NVIDIA RTX 3090 GPU. Experimental settings are shown in App. A. Evaluation Metrics. We evaluated the fusion results in both quantitative and qualitative. Qualitativeevaluation primarily hinges on subjective visual assessments conducted by individuals. We expectthat the fused image will exhibit rich texture details and abundant color saturability. Objectiveevaluation primarily focuses on measuring the quality assessments of individual fused images andtheir deviations from the source images. For different task scenarios, the different evaluation metricsused, specifically, we employ six metrics including Structural Similarity (SSIM), Mean squared error(MSE), correlation coefficient (CC), peak signal-to-noise ratio (PSNR), modified fusion artifactsmeasure (Nabf). In the MFF and MEF tasks, considering the different task scenarios, we employstandard deviation (SD), average gradient (AG), spatial frequency (SF), and sum of the correlationsof differences (SCD) for evaluation metrics.",
  "Evaluation on Multi-Modal Image Fusion": "For multi-modal image fusion, we compare our method with the state-of-the-art methods: Swin-Fusion , DIVFusion , MUFusion , CDDFuse , DDFM , Text-IF , andTC-MoA and on the LLVIP dataset. More datasets and comparison methods are shown inApp. B. Note that our method, like DDFM, does not require additional tuning.",
  "Text-IF 1.2021350.66931.970.023TC-MoA 1.2027900.66633.000.017CCF (ours)1.2216940.70532.710.005": "Quantitative Comparisons. We em-ploy five quantitative metrics to eval-uate our model, as shown in Ta-ble 1. Our method demonstrated ex-ceptional performance across variousevaluation metrics.On the LLVIPdataset, our method achieved the bestresults in SSIM, MSE, and CC indica-tors. Specifically, our method outper-formed others in SSIM and CC, withimprovements of 0.02 and 0.035 overthe second-best results, respectively.Additionally, lower MSE values indi-cate better performance, with our method showing a reduction of 362 compared to the second-bestmethods in these metrics. This indicates that our method retains more information from the sourceimages. The results show suboptimal performance in Nabf but are close to the best values, indicatingthe fused image with less noise. Notably, our method does not necessitate turning and holds itsground against methods requiring training. Compared to existing LLM tuning methods, our modelperforms slightly worse in terms of PSNR. This demonstrates the excellent performance of our model,achieving high performance across almost all indicators in a tuning-free model. Qualitative Comparisons. Furthermore, the incorporation of the basic condition and enhancedconditions enables effective preservation of the background and texture. This comparison underscoresour models efficacy in image fusion, resulting in outstanding visual outcomes. As shown in ,our method showcases superior visual quality compared to other approaches. Specifically, our methodexcels in preserving intricate texture details well lid in low light ( red box). Although TC-MoAand MUFusion approach our method in retaining details, they exhibit visible artifacts, blur, and lowcontrastcharacteristics absent in CCF (, green box). CCF exhibits the highest contrast, theclearest details, and the most information content, further highlighting its superiority in preservingtexture details. Its excellent detail retention and clear background generation further demonstrate theeffectiveness of our proposed method.",
  "Text-IF 66.277.7221.582.2762.514.7817.261.46TC-MoA 57.556.9520.671.0350.274.8215.640.42CCF (ours)79.028.1824.302.7871.885.7020.283.00": "contrast and clearer images. The SCD is 0.11 higher than the suboptimal value, suggesting a lowererror between source images and fused images. The AG and SF also rank first demonstrating theretention of more texture details. These results showcase that our method effectively preserves detailsfrom the source images and produces high-quality fused images. Qualitative Comparisons. As illustrated in , our proposed method demonstrates outstandingvisual performance, particularly in preserving intricate details. We have carefully selected specificconditions that allow our approach to effectively handle the blurring caused by multi-focus sceneswhile retaining the original images lighting and color information. In comparison, other DDPM-based methods such as DDFM are unable to achieve the same level of effectiveness as our approach.In (red), our method excels in preserving the details of the watch hand. The closest result toours is U2Fusion, but it loses texture and color fidelity, appearing blurry, in (green). In short,our method performs well in maintaining both color and authentic details.",
  "For multi-exposure image fusion, we compare our model with five general image fusion methods,i.e., U2Fusion , DeFusion , DDFM , Text-IF , and TC-MoA": "Quantitative Comparisons. As demonstrated in (right), our method achieved SOTA onthe MEF index, analogous to the results observed for the MFF task. Notably, the metrics SD, AG,and SF signify the highest image quality, while SCD exhibits the highest correlation. Each of thesemetrics attained state-of-the-art performance levels, underscoring the efficacy of our approach. Thisconsistent performance across multiple metrics illustrates the robustness and versatility of our methodin enhancing image quality and fidelity. Qualitative Comparisons. demonstrates the excellent visual performance of our method.Our approach effectively addresses the issue of overexposure while preserving crucial details. Incontrast, DDFM, which relies on finding the middle value of two images, struggles to maintain texturedetails. Similarly, Text-IF tends to result in higher average brightness, which can lead to contentloss in overexposed scenes. Defusion and TC-MoA exhibit similar visual performance, with moreblurred edges compared to our method. In comparison, our method strikes a balance between thesechallenges, resulting in superior visual fidelity and saturation compared to other existing methods.",
  ": The visualization of the w/o and withtask-specific conditions": "The task-specific conditions can be manually se-lected. In this section, we use the detection con-dition as an example. The detection model em-ployed is YOLOv5 , trained on the LLVIPdataset. We randomly select 287 images fromthe LLVIP test dataset to validate our methodwith the detection condition. Before adding thedetection condition, the fused image achievedmAP.5 = 0.737, mAP.5 : .95 = 0.509, and arecall of 0.737. After incorporating the detec-tion condition, the mAP.5 increased by 0.049 to0.907, mAP.5 : .95 increased by 0.054 to 0.563,and recall significantly improved to 0.832. visualizes several cases detected using YOLOv5.",
  ": Ablation study of condition bank. (a) is infrared image, (b) is visible image, (c) is basiccondition, (d) is CCF w/o SCS, (e) is CCF": "The fused images with the detection condition exhibit higher confidence (columns 1 and 2) and recall(columns 1, 2, and 3). This demonstrates that task-specific conditions can enhance the performanceof fused images in downstream tasks. By integrating task-specific conditions, the fused image can beprecisely aligned with the demands of various applications, enhancing the overall effectiveness andutility of the generated images. More detail shows in App. F.",
  "Ablation Study": "Numerous ablation experiments were conducted to assess the effectiveness of different componentsof our model. The aforementioned four metrics were utilized to evaluate fusion performance acrossdifferent experimental configurations. The quantitative results are presented in App. G. To validate the effectiveness of the condition bank, we systematically add basic and enhancedconditions individually and then verify the effectiveness of SCS. illustrates a gradual variationin performance metrics with the progressive addition of conditions. In (a), using only the basic condition results in image fusion. When all enhanced conditions areinjected together without SCS, the metrics all reduced ((b)), likely due to the conflict betweendifferent conditions, as evidenced by the messy lines and random noise in (d). This demonstratesthat injecting conditions unevenly without DCS leads to suboptimal results. After introducing DCS,both the metrics and visual results ((e)) are well-balanced, with conditions being injected asneeded during the generation process. This indicates the effectiveness of the condition bank and DCSin dynamically and appropriately selecting the conditions.",
  "T steps).As the process continues, and the fusion model tends to generate more texture details, conditionslike EI and edge become more dominant (around 3": "4T to T steps). SSIM also remains selected toconstrain the structure, whereas the frequency of SD selection decreases. This demonstrates thecrucial role of dynamically selecting conditions during the diffusion sampling process. Our CCFadaptively decomposes diverse conflict fusion conditions into different denoising steps, which issignificantly compatible with the reconstruction preferences of different steps in the denoising model.This dynamic fusion paradigm ensures appropriate condition selection at each stage of the sampling.",
  "Conclusion": "In this paper, we introduce a learning-free approach for conditional controllable image fusion (CCF),utilizing a condition bank to regulate joint information with a pre-trained DDPM. We capitalizeon the remarkable reconstruction abilities of DDPM and integrate them into the sampling steps.Sample-adaptive condition selection facilitates fusion in dynamic scenarios. Varied fusion imagescan personalize their conditions to emphasize different aspects. Empirical findings demonstrate thatCCF surpasses the competing methods in achieving superior performance for general image fusiontasks. In the future, we will further explore automatic manners to distinguish basic and enhancedconditions to reduce empirical intervention, thereby enabling more robust and reliable image fusion.",
  "Acknowledgements": "This work was sponsored by National Science and Technology Major Project (No. 2022ZD0116500),National Natural Science Foundation of China (No.s 62476198, 62106171, 61925602, U23B2049, and62222608), Tianjin Natural Science Funds for Distinguished Young Scholar (No. 23JCJQJC00270),the Zhejiang Provincial Natural Science Foundation of China (No. LD24F020004), and CCF-BaiduOpen Fund. This work was also sponsored by CAAI-CANN Open Fund, developed on OpenICommunity.",
  "Shutao Li, Xudong Kang, Leyuan Fang, Jianwen Hu, and Haitao Yin. Pixel-level image fusion:A survey of the state of the art. information Fusion, 33:100112, 2017": "Jiaxin Li, Danfeng Hong, Lianru Gao, Jing Yao, Ke Zheng, Bing Zhang, and Jocelyn Chanussot.Deep learning in multimodal remote sensing data fusion: A comprehensive review. InternationalJournal of Applied Earth Observation and Geoinformation, 112:102926, 2022. Xingchen Zhang, Ping Ye, and Gang Xiao. Vifb: A visible and infrared image fusion benchmark.In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern RecognitionWorkshops, pages 104105, 2020.",
  "Yu Liu, Lei Wang, Juan Cheng, Chang Li, and Xun Chen. Multi-focus image fusion: A surveyof the state of the art. Information Fusion, 64:7191, 2020": "Bing Cao, Yiming Sun, Pengfei Zhu, and Qinghua Hu. Multi-modal gated mixture of local-to-global experts for dynamic image fusion. In Proceedings of the IEEE/CVF InternationalConference on Computer Vision (ICCV), pages 2355523564, October 2023. Yiming Sun, Bing Cao, Pengfei Zhu, and Qinghua Hu. Detfusion: A detection-driven infraredand visible image fusion network. In Proceedings of the 30th ACM International Conference onMultimedia, pages 40034011, 2022. Chenxiao Zhang, Peng Yue, Deodato Tapete, Liangcun Jiang, Boyi Shangguan, Li Huang,and Guangchao Liu. A deeply supervised image fusion network for change detection in highresolution bi-temporal remote sensing images. ISPRS Journal of Photogrammetry and RemoteSensing, 166:183200, 2020. Jinyuan Liu, Xin Fan, Zhanbo Huang, Guanyao Wu, Risheng Liu, Wei Zhong, and ZhongxuanLuo. Target-aware dual adversarial learning and a multi-scenario multi-modality benchmark tofuse infrared and visible for object detection. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, pages 58025811, 2022.",
  "Jiayi Ma, Wei Yu, Chen Chen, Pengwei Liang, Xiaojie Guo, and Junjun Jiang. Pan-gan: Anunsupervised pan-sharpening method for remote sensing image fusion. Information Fusion, 62:110120, 2020": "Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune Gwon, and Sungroh Yoon. Ilvr:Conditioning method for denoising diffusion probabilistic models. In 2021 IEEE/CVF Interna-tional Conference on Computer Vision (ICCV), 2021. Mining Li, Ronghao Pei, Tianyou Zheng, Yang Zhang, and Weiwei Fu. Fusiondiff: Multi-focusimage fusion using denoising diffusion probabilistic models. Expert Systems with Applications,238:121664, 2024. Zixiang Zhao, Haowen Bai, Yuanzhi Zhu, Jiangshe Zhang, Shuang Xu, Yulun Zhang, KaiZhang, Deyu Meng, Radu Timofte, and Luc Van Gool. Ddfm: denoising diffusion model formulti-modality image fusion. arXiv preprint arXiv:2303.06840, 2023. Jinyuan Liu, Xin Fan, Ji Jiang, Risheng Liu, and Zhongxuan Luo. Learning a deep multi-scalefeature ensemble and an edge-attention guidance for image fusion. IEEE Transactions onCircuits and Systems for Video Technology, 32(1):105119, 2021. Hui Li, Xiao-Jun Wu, and Josef Kittler. Infrared and visible image fusion using a deep learningframework. In 2018 24th international conference on pattern recognition (ICPR), pages27052710. IEEE, 2018.",
  "Jiayi Ma, Chen Chen, Chang Li, and Jun Huang. Infrared and visible image fusion via gradienttransfer and total variation minimization. Information Fusion, page 100109, Sep 2016": "Zixiang Zhao, Haowen Bai, Jiangshe Zhang, Yulun Zhang, Shuang Xu, Zudi Lin, Radu Timofte,and Luc Van Gool. Cddfuse: Correlation-driven dual-branch feature decomposition for multi-modality image fusion. In Proceedings of the IEEE/CVF conference on computer vision andpattern recognition, pages 59065916, 2023. Han Xu, Jiayi Ma, Junjun Jiang, Xiaojie Guo, and Haibin Ling. U2fusion: A unified unsuper-vised image fusion network. IEEE Transactions on Pattern Analysis and Machine Intelligence,44(1):502518, 2020. Jiayi Ma, Linfeng Tang, Fan Fan, Jun Huang, Xiaoguang Mei, and Yong Ma. Swinfusion:Cross-domain long-range learning for general image fusion via swin transformer. IEEE/CAAJournal of Automatica Sinica, 9(7):12001217, 2022. Pengwei Liang, Junjun Jiang, Xianming Liu, and Jiayi Ma. Fusion from decomposition: A self-supervised decomposition approach for image fusion. In European Conference on ComputerVision, pages 719735. Springer, 2022.",
  "Hyungjin Chung, Jeongsol Kim, Michael T Mccann, Marc L Klasky, and Jong Chul Ye. Diffu-sion posterior sampling for general noisy inverse problems. arXiv preprint arXiv:2209.14687,2022": "Hyungjin Chung, Jeongsol Kim, MichaelT. Mccann, MarcL. Klasky, and JongChul Ye. Dif-fusion posterior sampling for general noisy inverse problems. The Eleventh InternationalConference on Learning Representations, Sep 2023. Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and Andrew Rabinovich. Gradnorm: Gra-dient normalization for adaptive loss balancing in deep multitask networks. In Internationalconference on machine learning, pages 794803. PMLR, 2018. Xinyu Jia, Chuang Zhu, Minzhen Li, Wenqi Tang, and Wenli Zhou. Llvip: A visible-infraredpaired dataset for low-light vision. In Proceedings of the IEEE/CVF international conferenceon computer vision, pages 34963504, 2021.",
  "Wenlong Zhang, Xiaolin Liu, Wuchao Wang, and Yujun Zeng. Multi-exposure image fu-sion based on wavelet transform. International Journal of Advanced Robotic Systems, 15(2):1729881418768939, 2018": "Jianming Zhang, Mingshuang Wu, Wei Cao, and Zi Xing. Partition-based image exposurecorrection via wavelet-based high frequency restoration.In International Conference onIntelligent Computing, pages 452463. Springer, 2024. Linfeng Tang, Hao Zhang, Han Xu, and Jiayi Ma. Rethinking the necessity of image fusionin high-level vision tasks: A practical infrared and visible image fusion network based onprogressive semantic injection and scene fidelity. Information Fusion, page 101870, 2023.",
  "Linfeng Tang, Jiteng Yuan, Hao Zhang, Xingyu Jiang, and Jiayi Ma. Piafusion: A progressiveinfrared and visible image fusion network based on illumination aware. Information Fusion, 83:7992, 2022": "Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P Kingma. Pixelcnn++: Improving thepixelcnn with discretized logistic mixture likelihood and other modifications. arXiv preprintarXiv:1701.05517, 2017. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural informationprocessing systems, 30, 2017.",
  "AExperimental Settings": "Our method not only excels in individual settings to generate customized images but also demonstratesrobust performance in general settings. In this section, we elaborate on numerous experiments forimage fusion tasks to demonstrate the superiority of our method. For different tasks, the basicconditions vary. Specifically, in the VIF task, MSE serves as the basic condition,where cMSE can be expressed as ||MSE(i, x0) + MSE(v, x0)||. In contrast, for the MEF and MFFtasks, to achieve clearer and higher fidelity images and inspired by , the basic conditionsinclude MSE, frequency and edge conditions, as detailed in the App. C. To ensure convenience andfairness, we select eight enhanced conditions from the condition bank: SSIM, MSE, Edge, Low-frequency, High-frequency, Spatial Frequency, Edge Intensity, and Standard Deviation while settingk = 3 across all tasks.",
  "BExperiments on More Comparisons": "In our evaluation using the TNO dataset, we referred to the test set outlined in the work by Tang etal. We evaluated the fusion results in both quantitative and qualitative. Qualitative evaluationprimarily hinges on subjective visual assessments conducted by individuals. We expect that the fusedimage will exhibit rich texture details and abundant color saturability. For different task scenarios, thedifferent evaluation metrics used, specifically, we employ six metrics including standard deviation(SD), entropy (EN), spatial frequency (SF), sum of the correlations of differences (SCD), StructuralSimilarity (SSIM), and edge intensity (EI). In the MFF and MEF tasks, considering the different taskscenarios, we employ SD, AG, SF, SCD, and MSE for evaluation metrics. For multi-modal imagefusion, we compare our method with four task-specific methods: DenseFuse , RFN-Nest ,UMF-CMGR , YDTR , and three general approaches U2Fusion , DeFusion andDDFM on the LLVIP dataset and TNO dataset. Quantitative Comparisons. We employ 6 quantitative metrics to evaluate our model, as shownin . Our method demonstrated exceptional performance across various evaluation metrics.On the TNO dataset, our method achieves the best result on the SD, EN, and SCD indicators. Theresult shows suboptimal performance in SF and is close to the best values. Notably, our method doesnot necessitate turning and holds its ground against methods requiring training. Compared with thelearning-free DDFM and DeFusion, our method shows better results. This demonstrates the excellentperformance of our model, achieving high performance across almost all indicators in a tuning-freemodel. Qualitative Comparisons. The incorporation of both basic and enhanced conditions enables effectivepreservation of background and texture. This comparison underscores our models efficacy in imagefusion, resulting in outstanding visual outcomes. As shown in , our method demonstratessuperior visual quality compared to other approaches. Specifically, it excels in preserving intricatetexture details and color fidelity, such as license plate numbers (highlighted in the red box in ).The DDFM and DeFusion approaches retain fewer texture details. As depicted in , CCF exhibitsthe highest contrast, the clearest details, and the most comprehensive information content, furtherhighlighting its superiority in preserving texture details. Its excellent detail retention ability and clearbackground generation further prove the effectiveness of our proposed method.",
  "CBasic Conditions of the MEF and MFF": "High-frequency Its commonly understood that the high-frequency information in both modalitiesis distinctive to each modality. For instance, texture and detailed information are specific to visibleimages, while thermal radiation information pertains to infrared images. Specifically, we utilizeswavelet transform to extract high-frequency information from the image. For example, 2D discretewavelet transformation with Haar wavelets to transform the input image (IM) into four sub-bands canbe expressed as:{LLk, HLk, LHk, HHk} = Wk(LLk1)(14)",
  "HF = HF (x0) F(h|HF (i)|, (1 h)|HF (v)|)(15)": "where || stands for the absolute operation and F(, ) can represent a variety of customized functions.In this paper, specifically, we employ the max function. The restored average coefficient and thehigh-frequency coefficient at scale k are correspondingly converted into the output at scale k 1 byemploying W1 the 2D inverse discrete wavelet transform:",
  ": end for": "In the sampling process, the neural network ar-chitecture is similar to PixelCNN++ , con-sisting of a U-Net backbone with group nor-malization. The diffusion step t is defined byincorporating the Transformer sinusoidalposition embedding into each residual block. Asshown in , six feature map resolutions areutilized, with two convolutional residual blocksper resolution level. Additionally, self-attentionblocks are incorporated at the 1616 resolutionbetween the convolutional blocks. Representing a process of reverse diffusion (sam-pling), starting from a standard normal distribu-tion N(0, I), T sampling steps are performedto generate the final fused image. However, the generation with unconditional DDPM is uncon-trollable. Therefore, conditions selected from the condition bank we constructed are introduced tocontrol the sampling process. More precisely, the conditions can rectify each steps estimation of x0.Finally, after the T step, the final fused image can be generated. The algorithm of CCF is presentedin Algorithm 1.",
  "We follow the recent works , and adopt the eight most commonly used constraints asconditions in this paper. However, as shown in , our approach is not restricted to these": ": Comparison across different numbers of enhancement conditions. For the 4 conditions,conditions include SSIM, MSE, Edge, and SD. The 8 conditions expand to include SSIM, MSE,Edge, SD, Low-frequency, High-frequency, SF, and EI. The 12 conditions incorporate the previous 8conditions with four additional conditions: CC, MMS-SSIM, SCD, and VIF. Bold indicates the bestresults",
  ": Example of enhanced conditions selection in denoising iteration for rapidly changingscenarios": "constraints; fewer conditions can be randomly selected (here we select SSIM, MSE, Edge, and SD),and additional enhanced conditions (Correlation Coefficient (CC), Multi-Scale StructuralSimilarity (MS-SSIM), the Sum of the Correlations of Differences (SCD), and the Visual InformationFidelity (VIF)) can also be incorporated. Additionally, we evaluate the runtime across differentnumbers of conditions to assess their impact on efficiency. While adding more conditions slightlyimproves performance, it also increases inference runtime. illustrates the selection of enhanced conditions at each denoising step in the same locationunder both daylight and nighttime scenarios, representing a typical rapidly changing environment. Itdemonstrates that CCF can adapt to these changing environments by selecting different conditions atvarious denoising steps to respond effectively to dynamic environmental characteristics.",
  "FMore About Task-specific Conditions": "The task-specific conditions are incorporated to guide the fusion process throughout the denoisingprocedure. For instance, we take the Euclidean distance of features extracted by the object detectionmodel as the detection condition. In our experiments, we employ YOLOv5, which is pretrainedon the visible modality, to extract features from both the estimated x0 at each step and the visibleimage. The Euclidean distance is minimized in the inverse diffusion process iteratively. Consequently,the final fused image progressively integrates the object-specific information, enhancing the fusionperformance. We provide additional visualizations of the detection conditions, as shown in ,directly fused images appear to have a lot of missed detection in raw 1-4 and the detection scores aregenerally higher. The last row of demonstrates a reduction in false-positive boxes when thedetection condition is applied. also supports this, indicating that metrics such as mAP.5, mAP.5:95, and recall have increasedremarkably, showing that our method can customize conditions to fit the downstream task effectively.By tailoring conditions to specific tasks, our approach effectively improves the applicability offused images for particular applications, as evidenced by the improved detection metrics. Wefurther explored the use of different levels of features obtained with YoloV5 as detection conditions.Specifically, we evaluate in three scales and each of the scales is evaluated individually, and allfeatures combined involve using all three scale features simultaneously, averaged, to add task-specificperception to enhance task performance. Furthermore, indicates that there is no significant",
  "GAblation": "The quantitative results of the ablation study compare different scenarios: without DDPM, withonly basic conditions, with enhanced conditions but without SCS, and the complete CCF results.With the reconstruction capability of DDPM, the fusion performance significantly improved after theconditions were integrated. Some metrics for the enhanced conditions without SCS show a declinedue to conflicts among the various focusing aspect conditions. However, after incorporating SCS,most metrics improve, demonstrating that SCS can enhance the image fusion process, producinghigh-quality images.",
  "HExperiments on Medical Image Fusion": "Medical image fusion (MIF) experiments were conducted to validate the efficacy of our proposedmethod using the Harvard Medical Image Dataset. We visualized fused medical images, focusingon MRI-SPECT, MRI-CT, and MRI-PET fusion, with the results presented in . Observingour fused images, it is evident that the preservation of both the skulls shape and intensity is notablywell-maintained. Within the brain region, the CCF method effectively combines intricate details andstructures from the two modalities.",
  "ILimitations and Broader Impacts": "Even though the proposed CCF model achieves superior performance over existing methods anddemonstrates advanced generalization with dynamic condition selection, there are still some potentiallimitations. We provide a condition bank, but it needs to be constructed empirically. Most ofthe conditions are inspired by Image Quality Assessment (IQA), but classifying these conditionsis challenging due to the complexity of IQA. Therefore, it is necessary to propose a method toautomatically classify the conditions, reducing empirical intervention. Additionally, our method relieson a pre-trained diffusion model, which limits its efficiency and makes the generation process time-consuming. Exploring more effective sampling processes would be valuable to improve efficiencyand further enhance the models performance. For potential social impact, it is difficult to ensure theeffectiveness of selected conditions for all scenarios, which may be risky in high-risk scenarios suchas medical imaging and autonomous driving."
}